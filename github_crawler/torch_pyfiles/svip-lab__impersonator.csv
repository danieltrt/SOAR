file_path,api_count,code
demo_imitator.py,0,"b'import numpy as np\nfrom tqdm import tqdm\nimport os\nimport glob\n\nfrom models.imitator import Imitator\nfrom options.test_options import TestOptions\nfrom utils.util import mkdir\nimport pickle\nfrom utils.video import make_video\n\n\nfrom run_imitator import adaptive_personalize\n\nmixamo_root_path = \'./assets/samples/refs/mixamo\'\n# MIXAMO_DANCE_ACTION_IDX_LIST = [102]\n# MIXAMO_BASE_ACTION_IDX_LIST = [20, 22, 32, 70]\n# MIXAMO_ACROBAT_ACTION_IDX_LIST = [7, 31, 83, 131, 145]\n\nMIXAMO_DANCE_ACTION_IDX_LIST = [102]\nMIXAMO_BASE_ACTION_IDX_LIST = [20]\nMIXAMO_ACROBAT_ACTION_IDX_LIST = [7]\n\n\ndef load_mixamo_smpl(mixamo_idx):\n    global mixamo_root_path\n\n    dir_name = \'%.4d\' % mixamo_idx\n    pkl_path = os.path.join(mixamo_root_path, dir_name, \'result.pkl\')\n\n    with open(pkl_path, \'rb\') as f:\n        result = pickle.load(f)\n\n    anim_len = result[\'anim_len\']\n    pose_array = result[\'smpl_array\'].reshape(anim_len, -1)\n    cam_array = result[\'cam_array\']\n    shape_array = np.ones((anim_len, 10))\n    smpl_array = np.concatenate((cam_array, pose_array, shape_array), axis=1)\n\n    return smpl_array\n\n\ndef generate_actor_result(test_opt, src_img_path):\n    imitator = Imitator(test_opt)\n    src_img_name = os.path.split(src_img_path)[-1][:-4]\n    test_opt.src_path = src_img_path\n\n    if test_opt.post_tune:\n        adaptive_personalize(test_opt, imitator, visualizer=None)\n    else:\n        imitator.personalize(test_opt.src_path, visualizer=None)\n\n    action_list_dict = {\'dance\': MIXAMO_DANCE_ACTION_IDX_LIST,\n                        \'base\': MIXAMO_BASE_ACTION_IDX_LIST,\n                        \'acrobat\': MIXAMO_ACROBAT_ACTION_IDX_LIST}\n\n    for action_type in [\'dance\', \'base\', \'acrobat\']:\n        for i in action_list_dict[action_type]:\n            if test_opt.output_dir:\n                pred_output_dir = os.path.join(test_opt.output_dir, \'mixamo_preds\')\n                if os.path.exists(pred_output_dir):\n                    os.system(""rm -r %s"" % pred_output_dir)\n                mkdir(pred_output_dir)\n            else:\n                pred_output_dir = None\n\n            print(pred_output_dir)\n            tgt_smpls = load_mixamo_smpl(i)\n\n            imitator.inference_by_smpls(tgt_smpls, cam_strategy=\'smooth\', output_dir=pred_output_dir, visualizer=None)\n\n            save_dir = os.path.join(test_opt.output_dir, src_img_name, action_type)\n            mkdir(save_dir)\n\n            output_mp4_path = os.path.join(save_dir, \'mixamo_%.4d_%s.mp4\' % (i, src_img_name))\n            img_path_list = sorted(glob.glob(\'%s/*.jpg\' % pred_output_dir))\n            make_video(output_mp4_path, img_path_list, save_frames_dir=None, fps=30)\n\n\ndef clean(output_dir):\n\n    for item in [\'imgs\', \'pairs\', \'mixamo_preds\', \'pairs_meta.pkl\']:\n        filepath = os.path.join(output_dir, item)\n        if os.path.exists(filepath):\n            os.system(""rm -r %s"" % filepath)\n\n\ndef main():\n    # meta imitator\n    test_opt = TestOptions().parse()\n    test_opt.bg_ks = 25\n    test_opt.front_warp = False\n    test_opt.post_tune = True\n\n    test_opt.output_dir = mkdir(\'./outputs/results/demos/imitators\')\n    # source images from iPER\n    images_paths = [\'./assets/src_imgs/imper_A_Pose/009_5_1_000.jpg\',\n                    \'./assets/src_imgs/imper_A_Pose/024_8_2_0000.jpg\',\n                    \'./assets/src_imgs/fashion_woman/Sweaters-id_0000088807_4_full.jpg\']\n\n    for src_img_path in tqdm(images_paths):\n        generate_actor_result(test_opt, src_img_path)\n\n    # clean other files\n    clean(test_opt.output_dir)\n\n    print(\'Completed! All demo videos are save in {}\'.format(test_opt.output_dir))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
demo_swap.py,0,"b'import cv2\nimport os\nimport numpy as np\n\nfrom models.swapper import Swapper\nfrom options.test_options import TestOptions\nfrom utils.visdom_visualizer import VisdomVisualizer\nfrom utils.util import mkdir\n\n\ndef tensor2cv2(img_tensor):\n    img = (img_tensor[0].cpu().numpy().transpose(1, 2, 0) + 1) / 2\n    img = img[:, :, ::-1]\n    img = (img * 255).astype(np.uint8)\n\n    return img\n\n\nif __name__ == ""__main__"":\n\n    opt = TestOptions().parse()\n    opt.bg_ks = 25\n    opt.front_warp = True\n    opt.post_tune = True\n\n    src_path_list = [(\'iPER\',    \'./assets/src_imgs/imper_A_Pose/009_5_1_000.jpg\'),\n                     (\'Fashion\', \'./assets/src_imgs/fashion_man/Jackets_Vests-id_0000008408_4_full.jpg\'),\n                     (\'Fashion\', \'./assets/src_imgs/fashion_woman/Sweaters-id_0000088807_4_full.jpg\')]\n\n    tgt_path_list = [\'./assets/src_imgs/fashion_woman/fashionWOMENBlouses_Shirtsid0000666802_4full.jpg\',\n                     \'./assets/src_imgs/fashion_man/Sweatshirts_Hoodies-id_0000680701_4_full.jpg\',\n                     \'./assets/src_imgs/fashion_man/Sweatshirts_Hoodies-id_0000097801_4_full.jpg\']\n\n    for (dataset, src_path) in src_path_list:\n        opt.src_path = src_path\n\n        for tgt_path in tgt_path_list:\n            opt.tgt_path = tgt_path\n\n            # set imitator\n            swapper = Swapper(opt)\n\n            if opt.ip:\n                visualizer = VisdomVisualizer(env=opt.name, ip=opt.ip, port=opt.port)\n            else:\n                visualizer = None\n\n            src_path = opt.src_path\n            tgt_path = opt.tgt_path\n\n            swapper.swap_setup(src_path, tgt_path)\n\n            if opt.post_tune:\n                print(\'\\n\\t\\t\\tPersonalization: meta cycle finetune...\')\n                swapper.post_personalize(opt.output_dir, visualizer=visualizer, verbose=True)\n\n            print(\'\\n\\t\\t\\tPersonalization: completed...\')\n\n            # if a->b\n            print(\'\\n\\t\\t\\tSwapping: {} wear the clothe of {}...\'.format(src_path, tgt_path))\n            result = swapper.swap(src_info=swapper.src_info, tgt_info=swapper.tsf_info, target_part=opt.swap_part,\n                                  visualizer=visualizer)\n            # else b->a\n            # swapper.swap(src_info=swapper.tsf_info, tgt_info=swapper.src_info, target_part=opt.swap_part,\n            #              visualizer=visualizer)\n\n            src_img_true_name = os.path.split(opt.src_path)[-1][:-4]\n\n            save_dir = mkdir(\'./outputs/results/demos/swappers/%s\' % src_img_true_name)\n            save_img_name = \'%s.%s\' % (os.path.split(opt.src_path)[-1], os.path.split(opt.tgt_path)[-1])\n\n            cv2.imwrite(\'%s/%s\' % (save_dir, save_img_name), tensor2cv2(result))\n'"
demo_view.py,0,"b'import numpy as np\nfrom tqdm import tqdm\nimport cv2\nimport os\nimport glob\n\nfrom models.imitator import Imitator\nfrom models.viewer import Viewer\nfrom options.test_options import TestOptions\n\nfrom utils.visdom_visualizer import VisdomVisualizer\nfrom utils.video import make_video\nfrom utils.util import mkdir\n\nfrom run_imitator import adaptive_personalize\n\n\ndef clean(output_dir):\n\n    for item in [\'imgs\', \'pairs\', \'mixamo_preds\', \'pairs_meta.pkl\', \'T_novel_view_preds\']:\n        filepath = os.path.join(output_dir, item)\n        if os.path.exists(filepath):\n            os.system(""rm -r %s"" % filepath)\n\n\ndef tensor2cv2(img_tensor):\n    img = (img_tensor[0].detach().cpu().numpy().transpose(1, 2, 0) + 1) / 2\n    img = img[:, :, ::-1]\n    img = (img * 255).astype(np.uint8)\n\n    return img\n\n\ndef parse_view_params(view_params):\n    """"""\n    :param view_params: R=xxx,xxx,xxx/t=xxx,xxx,xxx\n    :return:\n        -R: np.ndarray, (3,)\n        -t: np.ndarray, (3,)\n    """"""\n\n    params = dict()\n    for segment in view_params.split(\'/\'):\n        # R=xxx,xxx,xxx -> (name, xxx,xxx,xxx)\n        name, params_str = segment.split(\'=\')\n\n        vals = [float(val) for val in params_str.split(\',\')]\n\n        params[name] = np.array(vals, dtype=np.float32)\n\n    params[\'R\'] = params[\'R\'] / 180 * np.pi\n    return params\n\n\ndef create_T_pose_novel_view_smpl():\n    from scipy.spatial.transform import Rotation as R\n    # cam + pose + shape\n    smpls = np.zeros((180, 75))\n\n    for i in range(180):\n        r1 = R.from_rotvec([0, 0, 0])\n        r2 = R.from_euler(""xyz"", [180, i * 2, 0], degrees=True)\n        r = (r1 * r2).as_rotvec()\n\n        smpls[i, 3:6] = r\n\n    return smpls\n\n\ndef generate_T_pose_novel_view_result(test_opt, src_img_path):\n    imitator = Imitator(test_opt)\n    src_img_name = os.path.split(src_img_path)[-1][:-4]\n    test_opt.src_path = src_img_path\n\n    if test_opt.post_tune:\n        adaptive_personalize(test_opt, imitator, visualizer=None)\n    else:\n        imitator.personalize(test_opt.src_path, visualizer=None)\n\n    if test_opt.output_dir:\n        pred_output_dir = os.path.join(test_opt.output_dir, \'T_novel_view_preds\')\n        if os.path.exists(pred_output_dir):\n            os.system(""rm -r %s"" % pred_output_dir)\n        mkdir(pred_output_dir)\n    else:\n        pred_output_dir = None\n\n    print(pred_output_dir)\n    tgt_smpls = create_T_pose_novel_view_smpl()\n\n    imitator.inference_by_smpls(tgt_smpls, cam_strategy=\'smooth\', output_dir=pred_output_dir, visualizer=None)\n\n    save_dir = os.path.join(test_opt.output_dir, src_img_name)\n    mkdir(save_dir)\n\n    output_mp4_path = os.path.join(save_dir, \'T_novel_view_%s.mp4\' % src_img_name)\n    img_path_list = sorted(glob.glob(\'%s/*.jpg\' % pred_output_dir))\n    make_video(output_mp4_path, img_path_list, save_frames_dir=None, fps=30)\n\n    # clean other left\n    clean(test_opt.output_dir)\n\n\ndef generate_orig_pose_novel_view_result(opt, src_path):\n    opt.src_path = src_path\n    # set imitator\n    viewer = Viewer(opt)\n\n    if opt.ip:\n        visualizer = VisdomVisualizer(env=opt.name, ip=opt.ip, port=opt.port)\n    else:\n        visualizer = None\n\n    if opt.post_tune:\n        adaptive_personalize(opt, viewer, visualizer)\n\n    viewer.personalize(opt.src_path, visualizer=visualizer)\n    print(\'\\n\\t\\t\\tPersonalization: completed...\')\n\n    view_params = opt.view_params\n    params = parse_view_params(view_params)\n\n    length = 180\n    delta = 360 / length\n    logger = tqdm(range(length))\n\n    src_img_true_name = os.path.split(opt.src_path)[-1][:-4]\n    save_dir = os.path.join(opt.output_dir, src_img_true_name)\n    mkdir(os.path.join(save_dir, \'imgs\'))\n\n    print(\'\\n\\t\\t\\tSynthesizing {} novel views\'.format(length))\n    for i in logger:\n        params[\'R\'][0] = 0\n        params[\'R\'][1] = delta * i / 180.0 * np.pi\n        params[\'R\'][2] = 0\n\n        preds = viewer.view(params[\'R\'], params[\'t\'], visualizer=None, name=str(i))\n        # pred_outs.append(preds)\n\n        save_img_name = \'%s.%d.jpg\' % (os.path.split(opt.src_path)[-1], delta * i)\n\n        cv2.imwrite(\'%s/imgs/%s\' % (save_dir, save_img_name), tensor2cv2(preds))\n\n    """"""\n    make video\n    """"""\n    img_path_list = glob.glob(""%s/imgs/*.jpg"" % save_dir)\n    output_mp4_path = \'%s/%s.mp4\' % (save_dir, src_img_true_name)\n    make_video(output_mp4_path, img_path_list, save_frames_dir=None, fps=30)\n\n    clean(opt.output_dir)\n    clean(save_dir)\n\n\nif __name__ == ""__main__"":\n\n    opt = TestOptions().parse()\n    opt.bg_ks = 31\n    opt.T_pose = False\n    opt.front_warp = False\n    opt.bg_replace = True\n    opt.post_tune = True\n    opt.output_dir = \'./outputs/results/demos/viewers\'\n\n    src_path_list = [\n        (\'iPER\', \'./assets/src_imgs/imper_Random_Pose/novel_3.jpg\'),\n        (\'Fashion\', \'./assets/src_imgs/fashion_woman/fashionWOMENDressesid0000271801_4full.jpg\'),\n        (\'Fashion\', \'./assets/src_imgs/fashion_man/Jackets_Vests-id_0000071603_4_full.jpg\')\n    ]\n\n    for dataset, src_path in src_path_list:\n        if dataset == \'Fashion\':\n            opt.T_pose = True\n            generate_T_pose_novel_view_result(opt, src_path)\n        else:\n            opt.T_pose = False\n            generate_orig_pose_novel_view_result(opt, src_path)\n'"
evaluate.py,1,"b'import os\nimport torch\nimport numpy as np\nfrom typing import Dict, Any, List\n# evaluations\nfrom his_evaluators import MotionImitationModel, IPERMotionImitationEvaluator\n\nfrom models.imitator import Imitator\nfrom options.test_options import TestOptions\nfrom utils.visdom_visualizer import VisdomVisualizer\nfrom run_imitator import adaptive_personalize\nfrom utils import cv_utils\n\n\nclass LWGEvaluatorModel(MotionImitationModel):\n\n    def __init__(self, opt, output_dir):\n        super().__init__(output_dir)\n\n        self.opt = opt\n\n        if self.opt.ip:\n            visualizer = VisdomVisualizer(env=self.opt.name, ip=self.opt.ip, port=self.opt.port)\n        else:\n            visualizer = None\n\n        self.visualizer = visualizer\n        self.model = None\n\n    def imitate(self, src_infos: Dict[str, Any], ref_infos: Dict[str, Any]) -> List[str]:\n        """"""\n            Running the motion imitation of the self.model, based on the source information with respect to the\n            provided reference information. It returns the full paths of synthesized images.\n        Args:\n            src_infos (dict): the source information contains:\n                --images (list of str): the list of full paths of source images (the length is 1)\n                --smpls (np.ndarray): (length of images, 85)\n                --kps (np.ndarray): (length of images, 19, 2)\n            ref_infos (dict): the reference information contains:\n                --images (list of str): the list of full paths of reference images.\n                --smpls (np.ndarray): (length of images, 85)\n                --kps (np.ndarray): (length of images, 19, 2)\n                --self_imitation (bool): the flag indicates whether it is self-imitation or not.\n\n        Returns:\n            preds_files (list of str): full paths of synthesized images with respects to the images in ref_infos.\n        """"""\n\n        tgt_paths = ref_infos[""images""]\n        tgt_smpls = np.copy(ref_infos[""smpls""])\n        self_imitation = ref_infos[""self_imitation""]\n        if self_imitation:\n            cam_strategy = ""copy""\n            out_dir = self.si_out_dir\n            count = self.num_preds_si\n            self.num_preds_si += len(tgt_paths)\n        else:\n            cam_strategy = ""smooth""\n            out_dir = self.ci_out_dir\n            count = self.num_preds_ci\n            self.num_preds_ci += len(tgt_paths)\n        # outputs = self.model.inference(tgt_paths, tgt_smpls=tgt_smpls, cam_strategy=cam_strategy,\n        #                                visualizer=None, verbose=True)\n        #\n        # all_preds_files = []\n        # for i, preds in enumerate(outputs):\n        #     filename = ""{:0>8}.jpg"".format(count)\n        #     pred_file = os.path.join(out_dir, \'pred_\' + filename)\n        #     count += 1\n        #\n        #     cv_utils.save_cv2_img(preds, pred_file, normalize=True)\n        #     all_preds_files.append(pred_file)\n\n        all_preds_files = []\n        for i in range(len(tgt_smpls)):\n            filename = ""{:0>8}.jpg"".format(count)\n            pred_file = os.path.join(out_dir, \'pred_\' + filename)\n            count += 1\n\n            all_preds_files.append(pred_file)\n\n        return all_preds_files\n\n    def build_model(self):\n        """"""\n            You must define your model in this function, including define the graph and allocate GPU.\n            This function will be called in @see `MotionImitationRunnerProcessor.run()`.\n        Returns:\n            None\n        """"""\n        # set imitator\n        self.model = Imitator(self.opt)\n\n    def personalization(self, src_infos):\n        """"""\n            some task/method specific data pre-processing or others.\n        Args:\n            src_infos (dict): the source information contains:\n                --images (list of str): the list of full paths of source images (the length is 1)\n                --smpls (np.ndarray): (length of images, 85)\n                --kps (np.ndarray): (length of images, 19, 2)\n\n        Returns:\n            processed_src_infos (dict): the source information contains:\n                --images (list of str): the list of full paths of source images (the length is 1)\n                --smpls (np.ndarray): (length of images, 85)\n                --kps (np.ndarray): (length of images, 19, 2)\n                ...\n        """"""\n\n        # 1. load the pretrain model\n        self.model._load_params(self.model.generator, self.opt.load_path)\n        self.opt.src_path = src_infos[""images""][0]\n\n        # 2. post personalization\n        if self.opt.post_tune:\n            adaptive_personalize(self.opt, self.model, self.visualizer)\n\n        self.model.personalize(self.opt.src_path, src_smpl=np.copy(src_infos[""smpls""][0]), visualizer=None)\n        processed_src_infos = src_infos\n        return processed_src_infos\n\n    def terminate(self):\n        """"""\n            Close the model session, like if the model is based on TensorFlow, it needs to call sess.close() to\n            dealloc the resources.\n        Returns:\n\n        """"""\n        pass\n\n\nif __name__ == ""__main__"":\n    opt = TestOptions().parse()\n\n    model = LWGEvaluatorModel(opt, output_dir=opt.output_dir)\n    # iPER_MI_evaluator = IPERMotionImitationEvaluator(dataset=""iPER"", data_dir=opt.data_dir)\n    iPER_MI_evaluator = IPERMotionImitationEvaluator(dataset=""iPER_ICCV"", data_dir=opt.data_dir)\n\n    iPER_MI_evaluator.evaluate(\n        model=model,\n        image_size=opt.image_size,\n        pair_types=(""ssim"", ""psnr"", ""lps"", ""face-CS"", ""OS-CS-reid""),\n        unpair_types=(""is"", ""fid"", ""OS-CS-reid"", ""OS-freid"", ""face-CS"", ""face-FD"", ""PCB-CS-reid"", ""PCB-freid""),\n        device=torch.device(""cuda:0"")\n    )\n\n'"
run_imitator.py,15,"b'import torch\nimport torch.nn\nimport torch.utils.data\nfrom tqdm import tqdm\nimport os\nimport glob\n\nfrom data.dataset import PairSampleDataset\nfrom models.imitator import Imitator\nfrom options.test_options import TestOptions\nfrom utils.visdom_visualizer import VisdomVisualizer\nfrom utils.util import load_pickle_file, write_pickle_file, mkdirs, mkdir, clear_dir\nimport utils.cv_utils as cv_utils\n\n\n__all__ = [\'write_pair_info\', \'scan_tgt_paths\', \'meta_imitate\',\n           \'MetaCycleDataSet\', \'make_dataset\', \'adaptive_personalize\']\n\n\n@torch.no_grad()\ndef write_pair_info(src_info, tsf_info, out_file, imitator, only_vis):\n    """"""\n    Args:\n        src_info:\n        tsf_info:\n        out_file:\n        imitator:\n    Returns:\n\n    """"""\n    pair_data = dict()\n\n    pair_data[\'from_face_index_map\'] = src_info[\'fim\'][0][:, :, None].cpu().numpy()\n    pair_data[\'to_face_index_map\'] = tsf_info[\'fim\'][0][:, :, None].cpu().numpy()\n    pair_data[\'T\'] = tsf_info[\'T\'][0].cpu().numpy()\n    pair_data[\'warp\'] = tsf_info[\'tsf_img\'][0].cpu().numpy()\n    pair_data[\'smpls\'] = torch.cat([src_info[\'theta\'], tsf_info[\'theta\']], dim=0).cpu().numpy()\n    pair_data[\'j2d\'] = torch.cat([src_info[\'j2d\'], tsf_info[\'j2d\']], dim=0).cpu().numpy()\n\n    tsf_f2verts, tsf_fim, tsf_wim = imitator.render.render_fim_wim(tsf_info[\'cam\'], tsf_info[\'verts\'])\n    tsf_p2verts = tsf_f2verts[:, :, :, 0:2]\n    tsf_p2verts[:, :, :, 1] *= -1\n\n    T_cycle = imitator.render.cal_bc_transform(tsf_p2verts, src_info[\'fim\'], src_info[\'wim\'])\n    pair_data[\'T_cycle\'] = T_cycle[0].cpu().numpy()\n\n    # back_face_ids = mesh.get_part_face_ids(part_type=\'head_back\')\n    # tsf_p2verts[:, back_face_ids] = -2\n    # T_cycle_vis = imitator.render.cal_bc_transform(tsf_p2verts, src_info[\'fim\'], src_info[\'wim\'])\n    # pair_data[\'T_cycle_vis\'] = T_cycle_vis[0].cpu().numpy()\n\n    # for key, val in pair_data.items():\n    #     print(key, val.shape)\n\n    write_pickle_file(out_file, pair_data)\n\n\ndef scan_tgt_paths(tgt_path, itv=20):\n    if os.path.isdir(tgt_path):\n        all_tgt_paths = glob.glob(os.path.join(tgt_path, \'*\'))\n        all_tgt_paths.sort()\n        all_tgt_paths = all_tgt_paths[::itv]\n    else:\n        all_tgt_paths = [tgt_path]\n\n    return all_tgt_paths\n\n\ndef meta_imitate(opt, imitator, prior_tgt_path, save_imgs=True, visualizer=None):\n    src_path = opt.src_path\n\n    all_tgt_paths = scan_tgt_paths(prior_tgt_path, itv=40)\n    output_dir = opt.output_dir\n\n    out_img_dir, out_pair_dir = mkdirs([os.path.join(output_dir, \'imgs\'), os.path.join(output_dir, \'pairs\')])\n\n    img_pair_list = []\n\n    for t in tqdm(range(len(all_tgt_paths))):\n        tgt_path = all_tgt_paths[t]\n        preds = imitator.inference([tgt_path], visualizer=visualizer, cam_strategy=opt.cam_strategy, verbose=False)\n\n        tgt_name = os.path.split(tgt_path)[-1]\n        out_path = os.path.join(out_img_dir, \'pred_\' + tgt_name)\n\n        if save_imgs:\n            cv_utils.save_cv2_img(preds[0], out_path, normalize=True)\n            write_pair_info(imitator.src_info, imitator.tsf_info,\n                            os.path.join(out_pair_dir, \'{:0>8}.pkl\'.format(t)), imitator=imitator,\n                            only_vis=opt.only_vis)\n\n            img_pair_list.append((src_path, tgt_path))\n\n    if save_imgs:\n        write_pickle_file(os.path.join(output_dir, \'pairs_meta.pkl\'), img_pair_list)\n\n\nclass MetaCycleDataSet(PairSampleDataset):\n    def __init__(self, opt):\n        super(MetaCycleDataSet, self).__init__(opt, True)\n        self._name = \'MetaCycleDataSet\'\n\n    def _read_dataset_paths(self):\n        # read pair list\n        self._dataset_size = 0\n        self._read_samples_info(None, self._opt.pkl_dir, self._opt.pair_ids_filepath)\n\n    def _read_samples_info(self, im_dir, pkl_dir, pair_ids_filepath):\n        """"""\n        Args:\n            im_dir:\n            pkl_dir:\n            pair_ids_filepath:\n\n        Returns:\n\n        """"""\n        # 1. load image pair list\n        self.im_pair_list = load_pickle_file(pair_ids_filepath)\n\n        # 2. load pkl file paths\n        self.all_pkl_paths = sorted(glob.glob((os.path.join(pkl_dir, \'*.pkl\'))))\n\n        assert len(self.im_pair_list) == len(self.all_pkl_paths), \'{} != {}\'.format(\n            len(self.im_pair_list), len(self.all_pkl_paths)\n        )\n        self._dataset_size = len(self.im_pair_list)\n\n    def __getitem__(self, item):\n        """"""\n        Args:\n            item (int):  index of self._dataset_size\n\n        Returns:\n            new_sample (dict): items contain\n                --src_inputs (torch.FloatTensor): (3+3, h, w)\n                --tsf_inputs (torch.FloatTensor): (3+3, h, w)\n                --T (torch.FloatTensor): (h, w, 2)\n                --head_bbox (torch.IntTensor): (4), hear 4 = [lt_x, lt_y, rt_x, rt_y]\n                --valid_bbox (torch.FloatTensor): (1), 1.0 valid and 0.0 invalid.\n                --images (torch.FloatTensor): (2, 3, h, w)\n                --pseudo_masks (torch.FloatTensor) : (2, 1, h, w)\n                --bg_inputs (torch.FloatTensor): (3+1, h, w) or (2, 3+1, h, w) if self.is_both is True\n        """"""\n        im_pairs = self.im_pair_list[item]\n        pkl_path = self.all_pkl_paths[item]\n\n        sample = self.load_sample(im_pairs, pkl_path)\n        sample = self.preprocess(sample)\n\n        sample[\'preds\'] = torch.tensor(self.load_init_preds(im_pairs[1])).float()\n\n        return sample\n\n    def load_init_preds(self, pred_path):\n        pred_img_name = os.path.split(pred_path)[-1]\n        pred_img_path = os.path.join(self._opt.preds_img_folder, \'pred_\' + pred_img_name)\n\n        img = cv_utils.read_cv2_img(pred_img_path)\n        img = cv_utils.transform_img(img, self._opt.image_size, transpose=True)\n        img = img * 2 - 1\n\n        return img\n\n\ndef make_dataset(opt):\n    import platform\n\n    class Config(object):\n        pass\n\n    config = Config()\n\n    output_dir = opt.output_dir\n\n    config.pair_ids_filepath = os.path.join(output_dir, \'pairs_meta.pkl\')\n    config.pkl_dir = os.path.join(output_dir, \'pairs\')\n    config.preds_img_folder = os.path.join(output_dir, \'imgs\')\n    config.image_size = opt.image_size\n    config.map_name = opt.map_name\n    config.uv_mapping = opt.uv_mapping\n    config.is_both = False\n    config.bg_ks = opt.bg_ks\n    config.ft_ks = opt.ft_ks\n\n    meta_cycle_ds = MetaCycleDataSet(opt=config)\n    length = len(meta_cycle_ds)\n\n    data_loader = torch.utils.data.DataLoader(\n        meta_cycle_ds,\n        batch_size=min(length, opt.batch_size),\n        shuffle=False,\n        num_workers=0 if platform.system() == \'Windows\' else 4,\n        drop_last=True)\n\n    return data_loader\n\n\ndef adaptive_personalize(opt, imitator, visualizer):\n    output_dir = opt.output_dir\n    mkdirs([os.path.join(output_dir, \'imgs\'), os.path.join(output_dir, \'pairs\')])\n\n    # TODO check if it has been computed.\n    print(\'\\n\\t\\t\\tPersonalization: meta imitation...\')\n    imitator.personalize(opt.src_path, visualizer=None)\n    meta_imitate(opt, imitator, prior_tgt_path=opt.pri_path, visualizer=None, save_imgs=True)\n\n    # post tune\n    print(\'\\n\\t\\t\\tPersonalization: meta cycle finetune...\')\n    loader = make_dataset(opt)\n    imitator.post_personalize(opt.output_dir, loader, visualizer=None, verbose=False)\n\n\nif __name__ == ""__main__"":\n    # meta imitator\n    test_opt = TestOptions().parse()\n\n    if test_opt.ip:\n        visualizer = VisdomVisualizer(env=test_opt.name, ip=test_opt.ip, port=test_opt.port)\n    else:\n        visualizer = None\n\n    # set imitator\n    imitator = Imitator(test_opt)\n\n    if test_opt.post_tune:\n        adaptive_personalize(test_opt, imitator, visualizer)\n\n    imitator.personalize(test_opt.src_path, visualizer=visualizer)\n    print(\'\\n\\t\\t\\tPersonalization: completed...\')\n\n    if test_opt.save_res:\n        pred_output_dir = mkdir(os.path.join(test_opt.output_dir, \'imitators\'))\n        pred_output_dir = clear_dir(pred_output_dir)\n    else:\n        pred_output_dir = None\n\n    print(\'\\n\\t\\t\\tImitating `{}`\'.format(test_opt.tgt_path))\n    tgt_paths = scan_tgt_paths(test_opt.tgt_path, itv=1)\n    imitator.inference(tgt_paths, tgt_smpls=None, cam_strategy=\'smooth\',\n                       output_dir=pred_output_dir, visualizer=visualizer, verbose=True)\n\n\n\n'"
run_swap.py,0,"b'import os\nfrom models.swapper import Swapper\nfrom options.test_options import TestOptions\nfrom utils.visdom_visualizer import VisdomVisualizer\nfrom utils.util import mkdir\n\n\ndef get_img_name(img_path: str):\n    """"""\n        Get the name from the image path.\n\n    Args:\n        img_path (str): a/b.jpg or a/b.png ...\n\n    Returns:\n        name (str): a/b.jpg -> b\n    """"""\n    image_name = os.path.split(img_path)[-1].split(\'.\')[0]\n    return image_name\n\n\ndef save_results(src_path, tgt_path, output_dir, preds):\n    """"""\n        Save the results.\n    """"""\n    import utils.cv_utils as cv_utils\n\n    src_name = get_img_name(src_path)\n    tgt_name = get_img_name(tgt_path)\n\n    preds = preds[0].permute(1, 2, 0)\n    preds = preds.cpu().numpy()\n\n    filepath = os.path.join(output_dir, \'{}->{}.png\'.format(src_name, tgt_name))\n    cv_utils.save_cv2_img(preds, filepath, normalize=True)\n    print(\'\\n\\t\\t\\tSaving results to {}\'.format(filepath))\n\n\nif __name__ == ""__main__"":\n    opt = TestOptions().parse()\n\n    # set imitator\n    swapper = Swapper(opt=opt)\n\n    if opt.ip:\n        visualizer = VisdomVisualizer(env=opt.name, ip=opt.ip, port=opt.port)\n    else:\n        visualizer = None\n\n    src_path = opt.src_path\n    tgt_path = opt.tgt_path\n\n    swapper.swap_setup(src_path, tgt_path)\n\n    if opt.post_tune:\n        print(\'\\n\\t\\t\\tPersonalization: meta cycle finetune...\')\n        swapper.post_personalize(opt.output_dir, visualizer=None, verbose=False)\n        # swapper.post_personalize(opt.output_dir, visualizer=visualizer, verbose=True, bidirection=opt.bidirection)\n\n    print(\'\\n\\t\\t\\tPersonalization: completed...\')\n\n    # if a->b\n    print(\'\\n\\t\\t\\tSwapping: {} wear the clothe of {}...\'.format(src_path, tgt_path))\n    preds = swapper.swap(src_info=swapper.src_info, tgt_info=swapper.tsf_info,\n                         target_part=opt.swap_part, visualizer=visualizer)\n\n    if opt.save_res:\n        pred_output_dir = mkdir(os.path.join(opt.output_dir, \'swappers\'))\n        save_results(src_path, tgt_path, pred_output_dir, preds)\n\n    # # else b->a\n    # preds = swapper.swap(src_info=swapper.tgt_info, tgt_info=swapper.src_info,\n    #                      target_part=opt.swap_part, visualizer=visualizer)\n\n'"
run_view.py,2,"b'import torch.utils.data\nimport torchvision.utils\nimport numpy as np\nfrom tqdm import tqdm\nimport os\n\nfrom models.viewer import Viewer\nfrom options.test_options import TestOptions\nfrom utils.visdom_visualizer import VisdomVisualizer\nfrom utils.util import mkdir\n\nfrom run_imitator import adaptive_personalize\n\n\ndef parse_view_params(view_params):\n    """"""\n    :param view_params: R=xxx,xxx,xxx/t=xxx,xxx,xxx\n    :return:\n        -R: np.ndarray, (3,)\n        -t: np.ndarray, (3,)\n    """"""\n\n    params = dict()\n    for segment in view_params.split(\'/\'):\n        # R=xxx,xxx,xxx -> (name, xxx,xxx,xxx)\n        name, params_str = segment.split(\'=\')\n\n        vals = [float(val) for val in params_str.split(\',\')]\n\n        params[name] = np.array(vals, dtype=np.float32)\n\n    params[\'R\'] = params[\'R\'] / 180 * np.pi\n    return params\n\n\nif __name__ == ""__main__"":\n\n    opt = TestOptions().parse()\n\n    # set imitator\n    viewer = Viewer(opt=opt)\n\n    if opt.ip:\n        visualizer = VisdomVisualizer(env=opt.name, ip=opt.ip, port=opt.port)\n    else:\n        visualizer = None\n\n    if opt.post_tune:\n        adaptive_personalize(opt, viewer, visualizer)\n\n    viewer.personalize(opt.src_path, visualizer=visualizer)\n    print(\'\\n\\t\\t\\tPersonalization: completed...\')\n\n    src_path = opt.src_path\n    view_params = opt.view_params\n    params = parse_view_params(view_params)\n\n    length = 16\n    delta = 360 / length\n    pred_outs = []\n    logger = tqdm(range(length))\n\n    print(\'\\n\\t\\t\\tSynthesizing {} novel views\'.format(length))\n    for i in logger:\n        params[\'R\'][0] = 10 / 180 * np.pi\n        params[\'R\'][1] = delta * i / 180.0 * np.pi\n        params[\'R\'][2] = 10 / 180 * np.pi\n\n        preds = viewer.view(params[\'R\'], params[\'t\'], visualizer=None, name=str(i))\n        pred_outs.append(preds)\n\n        logger.set_description(\n            \'view = ({:.3f}, {:.3f}, {:.3f})\'.format(params[\'R\'][0], params[\'R\'][1], params[\'R\'][2])\n        )\n\n    pred_outs = torch.cat(pred_outs, dim=0)\n    pred_outs = (pred_outs + 1) / 2.0\n\n    if opt.ip:\n        visualizer.vis_named_img(\'preds\', pred_outs, denormalize=False)\n\n    if opt.save_res:\n        pred_output_dir = mkdir(os.path.join(opt.output_dir, \'viewers\'))\n        filepath = os.path.join(pred_output_dir, src_path.split(\'/\')[-1])\n        torchvision.utils.save_image(pred_outs, filepath)\n\n    # def process(x):\n    #     return float(x) / 180 * np.pi\n    #\n    # while True:\n    #     inputs = input(\'input thetas: \')\n    #     if inputs == \'q\':\n    #         break\n    #     thetas = list(map(process, inputs.split(\' \')))\n    #\n    #     preds = viewer.view(thetas, params[\'t\'], visualizer=None, name=\'0\')\n    #     visualizer.vis_named_img(\'pred\', preds)\n\n\n\n\n'"
train.py,0,"b'import time\nfrom options.train_options import TrainOptions\nfrom data.custom_dataset_data_loader import CustomDatasetDataLoader\nfrom models.models import ModelsFactory\nfrom utils.tb_visualizer import TBVisualizer\nfrom collections import OrderedDict\n\n\nclass Train(object):\n    def __init__(self):\n        self._opt = TrainOptions().parse()\n        data_loader_train = CustomDatasetDataLoader(self._opt, is_for_train=True)\n        data_loader_test = CustomDatasetDataLoader(self._opt, is_for_train=False)\n\n        self._dataset_train = data_loader_train.load_data()\n        self._dataset_test = data_loader_test.load_data()\n\n        self._dataset_train_size = len(data_loader_train)\n        self._dataset_test_size = len(data_loader_test)\n        print(\'#train video clips = %d\' % self._dataset_train_size)\n        print(\'#test video clips = %d\' % self._dataset_test_size)\n\n        self._model = ModelsFactory.get_by_name(self._opt.model, self._opt)\n        self._tb_visualizer = TBVisualizer(self._opt)\n\n        self._train()\n\n    def _train(self):\n        self._total_steps = self._opt.load_epoch * self._dataset_train_size\n        self._iters_per_epoch = self._dataset_train_size / self._opt.batch_size\n        self._last_display_time = None\n        self._last_save_latest_time = None\n        self._last_print_time = time.time()\n\n        for i_epoch in range(self._opt.load_epoch + 1, self._opt.nepochs_no_decay + self._opt.nepochs_decay + 1):\n            epoch_start_time = time.time()\n\n            # train epoch\n            self._train_epoch(i_epoch)\n\n            # save model\n            print(\'saving the model at the end of epoch %d, iters %d\' % (i_epoch, self._total_steps))\n            self._model.save(i_epoch)\n\n            # print epoch info\n            time_epoch = time.time() - epoch_start_time\n            print(\'End of epoch %d / %d \\t Time Taken: %d sec (%d min or %d h)\' %\n                  (i_epoch, self._opt.nepochs_no_decay + self._opt.nepochs_decay, time_epoch,\n                   time_epoch / 60, time_epoch / 3600))\n\n            # update learning rate\n            if i_epoch > self._opt.nepochs_no_decay:\n                self._model.update_learning_rate()\n\n    def _train_epoch(self, i_epoch):\n        epoch_iter = 0\n        self._model.set_train()\n        for i_train_batch, train_batch in enumerate(self._dataset_train):\n            iter_start_time = time.time()\n\n            # display flags\n            do_visuals = self._last_display_time is None or time.time() - self._last_display_time > self._opt.display_freq_s\n            do_print_terminal = time.time() - self._last_print_time > self._opt.print_freq_s or do_visuals\n\n            # train model\n            self._model.set_input(train_batch)\n            trainable = ((i_train_batch+1) % self._opt.train_G_every_n_iterations == 0) or do_visuals\n            self._model.optimize_parameters(keep_data_for_visuals=do_visuals, trainable=trainable)\n\n            # update epoch info\n            self._total_steps += self._opt.batch_size\n            epoch_iter += self._opt.batch_size\n\n            # display terminal\n            if do_print_terminal:\n                self._display_terminal(iter_start_time, i_epoch, i_train_batch, do_visuals)\n                self._last_print_time = time.time()\n\n            # display visualizer\n            if do_visuals:\n                self._display_visualizer_train(self._total_steps)\n                self._display_visualizer_val(i_epoch, self._total_steps)\n                self._last_display_time = time.time()\n\n            # save model\n            if self._last_save_latest_time is None or time.time() - self._last_save_latest_time > self._opt.save_latest_freq_s:\n                print(\'saving the latest model (epoch %d, total_steps %d)\' % (i_epoch, self._total_steps))\n                self._model.save(i_epoch)\n                self._last_save_latest_time = time.time()\n\n    def _display_terminal(self, iter_start_time, i_epoch, i_train_batch, visuals_flag):\n        errors = self._model.get_current_errors()\n        t = (time.time() - iter_start_time) / self._opt.batch_size\n        self._tb_visualizer.print_current_train_errors(i_epoch, i_train_batch, self._iters_per_epoch, errors, t, visuals_flag)\n\n    def _display_visualizer_train(self, total_steps):\n        self._tb_visualizer.display_current_results(self._model.get_current_visuals(), total_steps, is_train=True)\n        self._tb_visualizer.plot_scalars(self._model.get_current_errors(), total_steps, is_train=True)\n        self._tb_visualizer.plot_scalars(self._model.get_current_scalars(), total_steps, is_train=True)\n\n    def _display_visualizer_val(self, i_epoch, total_steps):\n        val_start_time = time.time()\n\n        # set model to eval\n        self._model.set_eval()\n\n        # evaluate self._opt.num_iters_validate epochs\n        val_errors = OrderedDict()\n        for i_val_batch, val_batch in enumerate(self._dataset_test):\n            if i_val_batch == self._opt.num_iters_validate:\n                break\n\n            # evaluate model\n            self._model.set_input(val_batch)\n            self._model.forward(keep_data_for_visuals=(i_val_batch == 0))\n            errors = self._model.get_current_errors()\n\n            # store current batch errors\n            for k, v in errors.items():\n                if k in val_errors:\n                    val_errors[k] += v\n                else:\n                    val_errors[k] = v\n\n        # normalize errors\n        for k in val_errors:\n            val_errors[k] /= self._opt.num_iters_validate\n\n        # visualize\n        t = (time.time() - val_start_time)\n        self._tb_visualizer.print_current_validate_errors(i_epoch, val_errors, t)\n        self._tb_visualizer.plot_scalars(val_errors, total_steps, is_train=False)\n        self._tb_visualizer.display_current_results(self._model.get_current_visuals(), total_steps, is_train=False)\n\n        # set model back to train\n        self._model.set_train()\n\n\nif __name__ == ""__main__"":\n    Train()\n'"
data/__init__.py,0,b''
data/custom_dataset_data_loader.py,2,"b'import torch.utils.data\nfrom data.dataset import DatasetFactory\n\n\nclass CustomDatasetDataLoader(object):\n    def __init__(self, opt, is_for_train=True):\n        self._opt = opt\n        self._is_for_train = is_for_train\n        self._num_threds = opt.n_threads_train if is_for_train else opt.n_threads_test\n        self._create_dataset()\n\n    def _create_dataset(self):\n        self._dataset = DatasetFactory.get_by_name(self._opt.dataset_mode, self._opt, self._is_for_train)\n        self._dataloader = torch.utils.data.DataLoader(\n            self._dataset,\n            batch_size=self._opt.batch_size,\n            shuffle=not self._opt.serial_batches,\n            num_workers=int(self._num_threds),\n            drop_last=True)\n\n    def load_data(self):\n        return self._dataloader\n\n    def __len__(self):\n        return len(self._dataset)\n\n'"
data/dataset.py,43,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport glob\n\n\nfrom utils.util import load_pickle_file, morph\nimport utils.cv_utils as cv_utils\nimport utils.mesh as mesh\n\n\nclass DatasetFactory(object):\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_by_name(dataset_name, opt, is_for_train):\n        if dataset_name == \'iPER\':\n            from data.imper_dataset import ImPerDataset\n            dataset = ImPerDataset(opt, is_for_train)\n\n        elif dataset_name == \'fashion\':\n            from data.fashion_dataset import FashionPairDataset\n            dataset = FashionPairDataset(opt, is_for_train)\n\n        elif dataset_name == \'iPER_place\':\n            from data.imper_fashion_place_dataset import ImPerPlaceDataset\n            dataset = ImPerPlaceDataset(opt, is_for_train)\n\n        elif dataset_name == \'iPER_fashion_place\':\n            from data.imper_fashion_place_dataset import ImPerFashionPlaceDataset\n            dataset = ImPerFashionPlaceDataset(opt, is_for_train)\n\n        else:\n            raise ValueError(""Dataset [%s] not recognized."" % dataset_name)\n\n        print(\'Dataset {} was created\'.format(dataset.name))\n        return dataset\n\n\nclass DatasetBase(data.Dataset):\n    def __init__(self, opt, is_for_train):\n        super(DatasetBase, self).__init__()\n        self._name = \'BaseDataset\'\n        self._root = None\n        self._opt = opt\n        self._is_for_train = is_for_train\n        self._create_transform()\n\n        self._IMG_EXTENSIONS = [\n            \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n            \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n        ]\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def path(self):\n        return self._root\n\n    def _create_transform(self):\n        self._transform = transforms.Compose([])\n\n    def get_transform(self):\n        return self._transform\n\n    def _is_image_file(self, filename):\n        return any(filename.endswith(extension) for extension in self._IMG_EXTENSIONS)\n\n    def _is_csv_file(self, filename):\n        return filename.endswith(\'.csv\')\n\n    def _get_all_files_in_subfolders(self, dir, is_file):\n        images = []\n        assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n        for root, _, fnames in sorted(os.walk(dir)):\n            for fname in fnames:\n                if is_file(fname):\n                    path = os.path.join(root, fname)\n                    images.append(path)\n\n        return images\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __getitem__(self, item):\n        raise NotImplementedError\n\n\nclass PairSampleDataset(DatasetBase):\n    def __init__(self, opt, is_for_train):\n        super(PairSampleDataset, self).__init__(opt, is_for_train)\n        self._name = \'PairSampleDataset\'\n\n        self.is_both = self._opt.is_both\n        self.bg_ks = self._opt.bg_ks\n        self.ft_ks = self._opt.ft_ks\n\n        # read dataset\n        self._read_dataset_paths()\n\n        # prepare mapping function\n        self.map_fn = mesh.create_mapping(map_name=opt.map_name, mapping_path=opt.uv_mapping,\n                                          contain_bg=True, fill_back=False)\n        # prepare head mapping function\n        # self.head_fn = mesh.create_mapping(\'head\', head_info=\'assets/pretrains/head.json\',\n        #                                    contain_bg=True, fill_back=False)\n\n        self.bg_kernel = torch.ones(1, 1, self.bg_ks, self.bg_ks, dtype=torch.float32)\n        self.ft_kernel = torch.ones(1, 1, self.ft_ks, self.ft_ks, dtype=torch.float32)\n\n    def _read_dataset_paths(self):\n        # /public/liuwen/p300/deep_fashion\n        self._root = self._opt.data_dir\n\n        # read pair list\n        self._dataset_size = 0\n        self._sample_files = []\n\n        pair_ids_filename = self._opt.train_ids_file if self._is_for_train else self._opt.test_ids_file\n        pair_ids_filepath = os.path.join(self._root, pair_ids_filename)\n\n        pkl_filename = self._opt.train_pkl_folder if self._is_for_train else self._opt.test_pkl_folder\n        pkl_dir = os.path.join(self._root, pkl_filename)\n\n        im_dir = os.path.join(self._root, self._opt.images_folder)\n        if not os.path.exists(im_dir):\n            vid_name = \'train_256\' if self._is_for_train else \'test_256\'\n            im_dir = os.path.join(self._root, vid_name)\n        self._read_samples_info(im_dir, pkl_dir, pair_ids_filepath)\n\n    def _read_samples_info(self, im_dir, pkl_dir, pair_ids_filepath):\n        """"""\n        Args:\n            im_dir:\n            pkl_dir:\n            pair_ids_filepath:\n\n        Returns:\n\n        """"""\n        # 1. load image pair list\n        self.im_pair_list = self._read_pair_list(im_dir, pair_ids_filepath)\n\n        # 2. load pkl file paths\n        self.all_pkl_paths = sorted(glob.glob((os.path.join(pkl_dir, \'*.pkl\'))))\n\n        assert len(self.im_pair_list) == len(self.all_pkl_paths), \'{} != {}\'.format(\n            len(self.im_pair_list), len(self.all_pkl_paths)\n        )\n        self._dataset_size = len(self.im_pair_list)\n\n    def _read_pair_list(self, im_dir, pair_pkl_path):\n        pair_list = load_pickle_file(pair_pkl_path)\n        new_pair_list = []\n\n        for i, pairs in enumerate(pair_list):\n            src_path = os.path.join(im_dir, pairs[0])\n            dst_path = os.path.join(im_dir, pairs[1])\n\n            new_pair_list.append((src_path, dst_path))\n\n        return new_pair_list\n\n    def __len__(self):\n        return self._dataset_size\n\n    def __getitem__(self, item):\n        """"""\n        Args:\n            item (int):  index of self._dataset_size\n\n        Returns:\n            new_sample (dict): items contain\n                --src_inputs (torch.FloatTensor): (3+3, h, w)\n                --tsf_inputs (torch.FloatTensor): (3+3, h, w)\n                --T (torch.FloatTensor): (h, w, 2)\n                --head_bbox (torch.IntTensor): (4), hear 4 = [lt_x, lt_y, rt_x, rt_y]\n                --valid_bbox (torch.FloatTensor): (1), 1.0 valid and 0.0 invalid.\n                --images (torch.FloatTensor): (2, 3, h, w)\n                --pseudo_masks (torch.FloatTensor) : (2, 1, h, w)\n                --bg_inputs (torch.FloatTensor): (3+1, h, w) or (2, 3+1, h, w) if self.is_both is True\n        """"""\n        im_pairs = self.im_pair_list[item]\n        pkl_path = self.all_pkl_paths[item]\n\n        sample = self.load_sample(im_pairs, pkl_path)\n        sample = self.preprocess(sample)\n\n        return sample\n\n    def load_images(self, im_pairs):\n        imgs = []\n        for im_path in im_pairs:\n            img = cv_utils.read_cv2_img(im_path)\n            img = cv_utils.transform_img(img, self._opt.image_size, transpose=True)\n            img = img * 2 - 1\n            imgs.append(img)\n        imgs = np.stack(imgs)\n        return imgs\n\n    def load_sample(self, im_pairs, pkl_path):\n        # 1. load images\n        imgs = self.load_images(im_pairs)\n        # 2.load pickle data\n        pkl_data = load_pickle_file(pkl_path)\n        src_fim = pkl_data[\'from_face_index_map\'][:, :, 0]  # (img_size, img_size)\n        dst_fim = pkl_data[\'to_face_index_map\'][:, :, 0]  # (img_size, img_size)\n        T = pkl_data[\'T\']  # (img_size, img_size, 2)\n        fims = np.stack([src_fim, dst_fim], axis=0)\n\n        fims_enc = self.map_fn[fims]  # (2, h, w, c)\n        fims_enc = np.transpose(fims_enc, axes=(0, 3, 1, 2))  # (2, c, h, w)\n\n        sample = {\n            \'images\': torch.tensor(imgs).float(),\n            \'src_fim\': torch.tensor(src_fim).float(),\n            \'tsf_fim\': torch.tensor(dst_fim).float(),\n            \'fims\': torch.tensor(fims_enc).float(),\n            \'T\': torch.tensor(T).float(),\n            \'j2d\': torch.tensor(pkl_data[\'j2d\']).float()\n        }\n\n        if \'warp\' in pkl_data:\n            if len(pkl_data[\'warp\'].shape) == 4:\n                sample[\'warp\'] = torch.tensor(pkl_data[\'warp\'][0], dtype=torch.float32)\n            else:\n                sample[\'warp\'] = torch.tensor(pkl_data[\'warp\'], dtype=torch.float32)\n        elif \'warp_R\' in pkl_data:\n            sample[\'warp\'] = torch.tensor(pkl_data[\'warp_R\'][0], dtype=torch.float32)\n        elif \'warp_T\' in pkl_data:\n            sample[\'warp\'] = torch.tensor(pkl_data[\'warp_T\'][0], dtype=torch.float32)\n\n        if \'T_cycle\' in pkl_data:\n            sample[\'T_cycle\'] = torch.tensor(pkl_data[\'T_cycle\']).float()\n\n        if \'T_cycle_vis\' in pkl_data:\n            sample[\'T_cycle_vis\'] = torch.tensor(pkl_data[\'T_cycle_vis\']).float()\n\n        return sample\n\n    def preprocess(self, sample):\n        """"""\n        Args:\n           sample (dict): items contain\n                --images (torch.FloatTensor): (2, 3, h, w)\n                --fims (torch.FloatTensor): (2, 3, h, w)\n                --T (torch.FloatTensor): (h, w, 2)\n                --warp (torch.FloatTensor): (3, h, w)\n                --head_bbox (torch.FloatTensor): (4), hear 4 = [lt_x, lt_y, rt_x, rt_y]\n\n        Returns:\n            new_sample (dict): items contain\n                --src_inputs (torch.FloatTensor): (3+3, h, w)\n                --tsf_inputs (torch.FloatTensor): (3+3, h, w)\n                --T (torch.FloatTensor): (h, w, 2)\n                --head_bbox (torch.FloatTensor): (4), hear 4 = [lt_x, lt_y, rt_x, rt_y]\n                --images (torch.FloatTensor): (2, 3, h, w)\n                --pseudo_masks (torch.FloatTensor) : (2, 1, h, w)\n                --bg_inputs (torch.FloatTensor): (3+1, h, w) or (2, 3+1, h, w) if self.is_both is True\n        """"""\n        with torch.no_grad():\n            images = sample[\'images\']\n            fims = sample[\'fims\']\n\n            # 1. process the bg inputs\n            src_fim = fims[0]\n            src_img = images[0]\n            src_mask = src_fim[None, -1:, :, :]   # (1, h, w)\n            src_bg_mask = morph(src_mask, ks=self.bg_ks, mode=\'erode\', kernel=self.bg_kernel)[0]  # bg is 0, front is 1\n            src_bg_inputs = torch.cat([src_img * src_bg_mask, src_bg_mask], dim=0)\n\n            # 2. process the src inputs\n            src_crop_mask = morph(src_mask, ks=self.ft_ks, mode=\'erode\', kernel=self.ft_kernel)[0]\n            src_inputs = torch.cat([src_img * (1 - src_crop_mask), src_fim])\n\n            # 3. process the tsf inputs\n            tsf_fim = fims[1]\n            tsf_mask = tsf_fim[None, -1:, :, :]     # (1, h, w), bg is 0, front is 1\n            tsf_crop_mask = morph(tsf_mask, ks=self.ft_ks, mode=\'erode\', kernel=self.ft_kernel)[0]\n\n            if \'warp\' not in sample:\n                warp = F.grid_sample(src_img[None], sample[\'T\'][None])[0]\n            else:\n                warp = sample[\'warp\']\n            tsf_inputs = torch.cat([warp, tsf_fim], dim=0)\n\n            if self.is_both:\n                tsf_img = images[1]\n                tsf_bg_mask = morph(tsf_mask, ks=self.bg_ks, mode=\'dilate\', kernel=self.bg_kernel)[0]  # bg is 0, front is 1\n                tsf_bg_inputs = torch.cat([tsf_img * (1 - tsf_bg_mask), tsf_bg_mask], dim=0)\n                bg_inputs = torch.stack([src_bg_inputs, tsf_bg_inputs], dim=0)\n            else:\n                bg_inputs = src_bg_inputs\n\n            # 4. concat pseudo mask\n            pseudo_masks = torch.stack([src_crop_mask, tsf_crop_mask], dim=0)\n\n            new_sample = {\n                \'images\': images,\n                \'pseudo_masks\': pseudo_masks,\n                \'j2d\': sample[\'j2d\'],\n                \'T\': sample[\'T\'],\n                \'bg_inputs\': bg_inputs,\n                \'src_inputs\': src_inputs,\n                \'tsf_inputs\': tsf_inputs,\n                \'src_fim\': sample[\'src_fim\'],\n                \'tsf_fim\': sample[\'tsf_fim\']\n            }\n\n            if \'T_cycle\' in sample:\n                new_sample[\'T_cycle\'] = sample[\'T_cycle\']\n\n            if \'T_cycle_vis\' in sample:\n                new_sample[\'T_cycle_vis\'] = sample[\'T_cycle_vis\']\n\n            return new_sample\n\n\n'"
data/fashion_dataset.py,38,"b'import glob\nimport os.path\nimport torch\nimport torch.nn.functional as F\nfrom data.dataset import DatasetBase\nimport numpy as np\nfrom utils import cv_utils\nfrom utils.util import load_pickle_file, morph, cal_mask_bbox\n\nimport utils.mesh as mesh\n\n\nclass FashionPairDataset(DatasetBase):\n    def __init__(self, opt, is_for_train):\n        super(FashionPairDataset, self).__init__(opt, is_for_train)\n        self._name = \'FashionPairDataset\'\n\n        self.use_src_bg = False\n        self.bg_ks = 21\n        self.ft_ks = 7\n\n        # read dataset\n        self._read_dataset_paths()\n\n        # prepare mapping function\n        self.map_fn = mesh.create_mapping(map_name=opt.map_name, mapping_path=opt.uv_mapping,\n                                          contain_bg=True, fill_back=False)\n        # prepare head mapping function\n        self.head_fn = mesh.create_mapping(\'head\', head_info=\'assets/pretrains/head.json\',\n                                           contain_bg=True, fill_back=False)\n\n        self.bg_kernel = torch.ones(1, 1, self.bg_ks, self.bg_ks, dtype=torch.float32)\n        self.ft_kernel = torch.ones(1, 1, self.ft_ks, self.ft_ks, dtype=torch.float32)\n\n    def _read_dataset_paths(self):\n        # /public/liuwen/p300/deep_fashion\n        self._root = self._opt.fashion_dir\n\n        # read pair list\n        self._dataset_size = 0\n        self._sample_files = []\n\n        pair_ids_filename = \'pairs_train.pkl\' if self._is_for_train else \'pairs_test.pkl\'\n        pair_ids_filepath = os.path.join(self._root, pair_ids_filename)\n\n        pkl_filename = \'train_256_v2_max_bbox_dp_hmr_pairs_results\' if self._is_for_train \\\n            else \'test_256_v2_max_bbox_dp_hmr_pairs_results\'\n        pkl_dir = os.path.join(self._root, pkl_filename)\n\n        im_dir = os.path.join(self._root, \'img_256\')\n        self._read_samples_info(im_dir, pkl_dir, pair_ids_filepath)\n\n    def _read_samples_info(self, im_dir, pkl_dir, pair_ids_filepath):\n        """"""\n        Args:\n            im_dir:\n            pkl_dir:\n            pair_ids_filepath:\n\n        Returns:\n\n        """"""\n        # 1. load image pair list\n        im_pair_list = load_pickle_file(pair_ids_filepath)\n\n        # 2. load pkl file paths\n        all_pkl_paths = sorted(glob.glob((os.path.join(pkl_dir, \'*.pkl\'))))\n\n        # 3. filters the source image is not front\n        self.im_pair_list = []\n        self.all_pkl_paths = []\n\n        for pairs, pkl_path in zip(im_pair_list, all_pkl_paths):\n            src_path = os.path.join(im_dir, pairs[0])\n            dst_path = os.path.join(im_dir, pairs[1])\n\n            if \'side\' in src_path or \'back\' in src_path:\n                continue\n\n            src_path = os.path.join(im_dir, src_path)\n            dst_path = os.path.join(im_dir, dst_path)\n\n            self.im_pair_list.append((src_path, dst_path))\n            self.all_pkl_paths.append(pkl_path)\n\n        assert len(self.im_pair_list) == len(self.all_pkl_paths), \'{} != {}\'.format(\n            len(self.im_pair_list), len(self.all_pkl_paths)\n        )\n        self._dataset_size = len(self.im_pair_list)\n\n        del im_pair_list\n        del all_pkl_paths\n\n    def __len__(self):\n        return self._dataset_size\n\n    def __getitem__(self, item):\n        """"""\n        Args:\n            item (int):  index of self._dataset_size\n\n        Returns:\n            new_sample (dict): items contain\n                --src_inputs (torch.FloatTensor): (3+3, h, w)\n                --tsf_inputs (torch.FloatTensor): (3+3, h, w)\n                --T (torch.FloatTensor): (h, w, 2)\n                --head_bbox (torch.IntTensor): (4), hear 4 = [lt_x, lt_y, rt_x, rt_y]\n                --valid_bbox (torch.FloatTensor): (1), 1.0 valid and 0.0 invalid.\n                --images (torch.FloatTensor): (2, 3, h, w)\n                --pseudo_masks (torch.FloatTensor) : (2, 1, h, w)\n                --bg_inputs (torch.FloatTensor): (3+1, h, w) or (2, 3+1, h, w) if self.is_both is True\n        """"""\n\n        item = item % self._dataset_size\n        im_pairs = self.im_pair_list[item]\n        pkl_path = self.all_pkl_paths[item]\n\n        sample = self.load_sample(im_pairs, pkl_path)\n        sample = self.preprocess(sample)\n\n        return sample\n\n    def load_images(self, im_pairs):\n        imgs = []\n        for im_path in im_pairs:\n            img = cv_utils.read_cv2_img(im_path)\n            img = cv_utils.transform_img(img, self._opt.image_size, transpose=True)\n            img = img * 2 - 1\n            imgs.append(img)\n        imgs = np.stack(imgs)\n        return imgs\n\n    def load_sample(self, im_pairs, pkl_path):\n        # 1. load images\n        imgs = self.load_images(im_pairs)\n        # 2.load pickle data\n        pkl_data = load_pickle_file(pkl_path)\n        src_fim = pkl_data[\'from_face_index_map\'][:, :, 0]  # (img_size, img_size)\n        dst_fim = pkl_data[\'to_face_index_map\'][:, :, 0]  # (img_size, img_size)\n        T = pkl_data[\'T\']  # (img_size, img_size, 2)\n        fims = np.stack([src_fim, dst_fim], axis=0)\n\n        fims_enc = self.map_fn[fims]  # (2, h, w, c)\n        fims_enc = np.transpose(fims_enc, axes=(0, 3, 1, 2))  # (2, c, h, w)\n\n        heads_mask = self.head_fn[fims[1:]]  # (1, h, w, 1)\n        heads_mask = np.transpose(heads_mask, axes=(0, 3, 1, 2))    # (1, 1, h, w)\n        head_bbox, _ = cal_mask_bbox(heads_mask, factor=1.05)\n        body_bbox, _ = cal_mask_bbox(1 - fims_enc[1:, -1:], factor=1.2)\n\n        # print(head_bbox.shape, valid_bbox.shape)\n        sample = {\n            \'images\': torch.tensor(imgs).float(),\n            \'fims\': torch.tensor(fims_enc).float(),\n            \'T\': torch.tensor(T).float(),\n            \'head_bbox\': torch.tensor(head_bbox[0]).long(),\n            \'body_bbox\': torch.tensor(body_bbox[0]).long()\n        }\n\n        if \'warp\' in pkl_data:\n            if len(pkl_data[\'warp\'].shape) == 4:\n                sample[\'warp\'] = torch.tensor(pkl_data[\'warp\'][0], dtype=torch.float32)\n            else:\n                sample[\'warp\'] = torch.tensor(pkl_data[\'warp\'], dtype=torch.float32)\n        elif \'warp_R\' in pkl_data:\n            sample[\'warp\'] = torch.tensor(pkl_data[\'warp_R\'][0], dtype=torch.float32)\n        elif \'warp_T\' in pkl_data:\n            sample[\'warp\'] = torch.tensor(pkl_data[\'warp_T\'][0], dtype=torch.float32)\n\n        return sample\n\n    @torch.no_grad()\n    def preprocess(self, sample):\n        """"""\n        Args:\n           sample (dict): items contain\n                --images (torch.FloatTensor): (2, 3, h, w)\n                --fims (torch.FloatTensor): (2, 3, h, w)\n                --T (torch.FloatTensor): (h, w, 2)\n                --warp (torch.FloatTensor): (3, h, w)\n                --head_bbox (torch.FloatTensor): (4), hear 4 = [lt_x, lt_y, rt_x, rt_y]\n\n        Returns:\n            new_sample (dict): items contain\n                --src_inputs (torch.FloatTensor): (3+3, h, w)\n                --tsf_inputs (torch.FloatTensor): (3+3, h, w)\n                --T (torch.FloatTensor): (h, w, 2)\n                --head_bbox (torch.FloatTensor): (4), hear 4 = [lt_x, lt_y, rt_x, rt_y]\n                --images (torch.FloatTensor): (2, 3, h, w)\n                --pseudo_masks (torch.FloatTensor) : (2, 1, h, w)\n                --bg_inputs (torch.FloatTensor): (3+1, h, w) or (2, 3+1, h, w) if self.is_both is True\n        """"""\n        images = sample[\'images\']\n        fims = sample[\'fims\']\n\n        # 1. process the bg inputs\n        src_fim = fims[0]\n        src_img = images[0]\n        src_mask = src_fim[None, -1:, :, :]   # (1, h, w)\n\n        # 2. process the src inputs\n        src_crop_mask = morph(src_mask, ks=self.ft_ks, mode=\'erode\', kernel=self.ft_kernel)[0]\n        src_inputs = torch.cat([src_img * (1 - src_crop_mask), src_fim])\n\n        # 3. process the tsf inputs\n        tsf_fim = fims[1]\n        tsf_mask = tsf_fim[None, -1:, :, :]     # (1, h, w), bg is 0, front is 1\n        tsf_crop_mask = morph(tsf_mask, ks=self.ft_ks, mode=\'erode\', kernel=self.ft_kernel)[0]\n\n        if \'warp\' not in sample:\n            warp = F.grid_sample(src_img[None], sample[\'T\'][None])[0]\n        else:\n            warp = sample[\'warp\']\n\n        tsf_inputs = torch.cat([warp, tsf_fim], dim=0)\n\n        if self.use_src_bg:\n            src_bg_mask = morph(src_mask, ks=self.bg_ks, mode=\'erode\', kernel=self.bg_kernel)[0]  # bg is 0, front is 1\n            bg_inputs = torch.cat([src_img * src_bg_mask, src_bg_mask], dim=0)\n        else:\n            tsf_img = images[1]\n            tsf_bg_mask = morph(tsf_mask, ks=self.bg_ks, mode=\'erode\', kernel=self.bg_kernel)[0]\n            bg_inputs = torch.cat([tsf_img * tsf_bg_mask, tsf_bg_mask], dim=0)\n\n        # 4. concat pseudo mask\n        pseudo_masks = torch.stack([src_crop_mask, tsf_crop_mask], dim=0)\n\n        new_sample = {\n            \'images\': images,\n            \'pseudo_masks\': pseudo_masks,\n            \'head_bbox\': sample[\'head_bbox\'],\n            \'body_bbox\': sample[\'body_bbox\'],\n            \'bg_inputs\': bg_inputs,\n            \'src_inputs\': src_inputs,\n            \'tsf_inputs\': tsf_inputs,\n            \'T\': sample[\'T\'],\n        }\n\n        return new_sample\n'"
data/imper_dataset.py,0,"b""import os.path\nimport torchvision.transforms as transforms\nfrom data.dataset import DatasetBase\nimport numpy as np\nfrom utils import cv_utils\nfrom utils.util import load_pickle_file, ToTensor, ImageTransformer\nimport glob\n\n\n__all__ = ['ImPerBaseDataset', 'ImPerDataset']\n\n\nclass ImPerBaseDataset(DatasetBase):\n\n    def __init__(self, opt, is_for_train):\n        super(ImPerBaseDataset, self).__init__(opt, is_for_train)\n        self._name = 'ImPerBaseDataset'\n\n        self._intervals = opt.intervals\n\n        # read dataset\n        self._read_dataset_paths()\n\n    def __getitem__(self, index):\n        # assert (index < self._dataset_size)\n\n        # start_time = time.time()\n        # get sample data\n        v_info = self._vids_info[index % self._num_videos]\n        images, smpls = self._load_pairs(v_info)\n\n        # pack data\n        sample = {\n            'images': images,\n            'smpls': smpls\n        }\n\n        sample = self._transform(sample)\n        # print(time.time() - start_time)\n\n        return sample\n\n    def __len__(self):\n        return self._dataset_size\n\n    def _read_dataset_paths(self):\n        self._root = self._opt.data_dir\n        self._vids_dir = os.path.join(self._root, self._opt.images_folder)\n        self._smpls_dir = os.path.join(self._root, self._opt.smpls_folder)\n\n        # read video list\n        self._num_videos = 0\n        self._dataset_size = 0\n        use_ids_filename = self._opt.train_ids_file if self._is_for_train else self._opt.test_ids_file\n        use_ids_filepath = os.path.join(self._root, use_ids_filename)\n        self._vids_info = self._read_vids_info(use_ids_filepath)\n\n    def _read_vids_info(self, file_path):\n        vids_info = []\n        with open(file_path, 'r') as reader:\n\n            lines = []\n            for line in reader:\n                line = line.rstrip()\n                lines.append(line)\n\n            total = len(lines)\n            for i, line in enumerate(lines):\n                images_path = glob.glob(os.path.join(self._vids_dir, line, '*'))\n                images_path.sort()\n                smpl_data = load_pickle_file(os.path.join(self._smpls_dir, line, 'pose_shape.pkl'))\n                cams = smpl_data['cams']\n                # kps_data = load_pickle_file(os.path.join(self._smpls_dir, line, 'kps.pkl'))\n                # kps = (kps_data['kps'] + 1) / 2.0 * 1024\n\n                assert len(images_path) == len(cams), '{} != {}'.format(len(images_path), len(cams))\n\n                info = {\n                    'images': images_path,\n                    'cams': cams,\n                    'thetas': smpl_data['pose'],\n                    'betas': smpl_data['shape'],\n                    'length': len(images_path)\n                }\n                vids_info.append(info)\n                self._dataset_size += info['length'] // self._intervals\n                # self._dataset_size += info['length']\n                self._num_videos += 1\n                print('loading video = {}, {} / {}'.format(line, i, total))\n\n                if self._opt.debug:\n                    if i > 1:\n                        break\n\n        return vids_info\n\n    @property\n    def video_info(self):\n        return self._vids_info\n\n    def _load_pairs(self, vid_info):\n        length = vid_info['length']\n        pair_ids = np.random.choice(length, size=2, replace=False)\n\n        smpls = np.concatenate((vid_info['cams'][pair_ids],\n                                vid_info['thetas'][pair_ids],\n                                vid_info['betas'][pair_ids]), axis=1)\n\n        images = []\n        images_paths = vid_info['images']\n        for t in pair_ids:\n            image_path = images_paths[t]\n            image = cv_utils.read_cv2_img(image_path)\n\n            images.append(image)\n\n        return images, smpls\n\n    def _create_transform(self):\n        transform_list = [\n            ImageTransformer(output_size=self._opt.image_size),\n            ToTensor()]\n        self._transform = transforms.Compose(transform_list)\n\n\nclass ImPerDataset(ImPerBaseDataset):\n\n    def __init__(self, opt, is_for_train):\n        super(ImPerDataset, self).__init__(opt, is_for_train)\n        self._name = 'ImPerDataset'\n\n    def _load_pairs(self, vid_info):\n        length = vid_info['length']\n\n        start = np.random.randint(0, 15)\n        end = np.random.randint(0, length)\n        pair_ids = np.array([start, end], dtype=np.int32)\n\n        smpls = np.concatenate((vid_info['cams'][pair_ids],\n                                vid_info['thetas'][pair_ids],\n                                vid_info['betas'][pair_ids]), axis=1)\n\n        images = []\n        images_paths = vid_info['images']\n        for t in pair_ids:\n            image_path = images_paths[t]\n            image = cv_utils.read_cv2_img(image_path)\n\n            images.append(image)\n\n        return images, smpls\n\n\n"""
data/imper_fashion_place_dataset.py,0,"b""import numpy as np\n\nfrom .dataset import DatasetBase\nfrom .imper_dataset import ImPerDataset\nfrom .place_dataset import PlaceDataset\nfrom .fashion_dataset import FashionPairDataset\n\n\nclass ImPerPlaceDataset(DatasetBase):\n\n    def __init__(self, opt, is_for_train):\n        super(ImPerPlaceDataset, self).__init__(opt, is_for_train)\n        self._name = 'ImPerPlaceDataset'\n\n        # self.mi = FastLoadMIDataset(opt, is_for_train)\n        self.mi = ImPerDataset(opt, is_for_train)\n        self.place = PlaceDataset(opt, is_for_train)\n\n        self._dataset_size = len(self.mi)\n        num_places = len(self.place)\n        interval = num_places // self._dataset_size\n        self.sample_ids = np.arange(0, num_places, interval)[0:self._dataset_size]\n\n        print(self._dataset_size, len(self.sample_ids))\n\n    def __len__(self):\n        return self._dataset_size\n\n    def __getitem__(self, item):\n        sample = self.mi[item]\n        bg = self.place[self.sample_ids[item]]\n\n        sample['bg'] = bg\n        return sample\n\n\nclass ImPerFashionPlaceDataset(DatasetBase):\n\n    def __init__(self, opt, is_for_train):\n        super(ImPerFashionPlaceDataset, self).__init__(opt, is_for_train)\n        self._name = 'ImPerFashionPlaceDataset'\n\n        # self.mi = FastLoadMIDataset(opt, is_for_train)\n        self.mi = ImPerDataset(opt, is_for_train)\n        self.place = PlaceDataset(opt, is_for_train)\n        self.fashion = FashionPairDataset(opt, is_for_train)\n\n        self._dataset_size = len(self.mi)\n        num_places = len(self.place)\n        interval = num_places // self._dataset_size\n        self.sample_ids = np.arange(0, num_places, interval)[0:self._dataset_size]\n\n        print(self._dataset_size, len(self.sample_ids))\n\n    def __len__(self):\n        return self._dataset_size\n\n    def __getitem__(self, item):\n        sample = self.mi[item]\n        bg = self.place[self.sample_ids[item]]\n        fashion = self.fashion[item]\n\n        sample['bg'] = bg\n\n        sample['fashion_images'] = fashion['images']\n        sample['fashion_masks'] = fashion['pseudo_masks']\n        sample['fashion_head_bbox'] = fashion['head_bbox']\n        sample['fashion_body_bbox'] = fashion['body_bbox']\n        sample['fashion_bg_inputs'] = fashion['bg_inputs']\n        sample['fashion_src_inputs'] = fashion['src_inputs']\n        sample['fashion_tsf_inputs'] = fashion['tsf_inputs']\n        sample['fashion_T'] = fashion['T']\n\n        return sample\n\n\n\n\n"""
data/place_dataset.py,0,"b""import os.path\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom data.dataset import DatasetBase\nfrom utils.util import ImageNormalizeToTensor\n\n\nclass PlaceDataset(DatasetBase):\n\n    def __init__(self, opt, is_for_train):\n        super(PlaceDataset, self).__init__(opt, is_for_train)\n        self._name = 'PlaceDataset'\n\n        self._read_dataset_paths()\n\n    def _read_dataset_paths(self):\n        if self._is_for_train:\n            sub_folder = 'train'\n        else:\n            sub_folder = 'val'\n\n        self._data_dir = os.path.join(self._opt.place_dir, sub_folder)\n\n        self.dataset = datasets.ImageFolder(self._data_dir, transform=self._transform)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, item):\n        return self.dataset[item][0]\n\n    def _create_transform(self):\n        transforms.ToTensor()\n        transform_list = [\n            transforms.RandomResizedCrop(self._opt.image_size),\n            transforms.RandomHorizontalFlip(),\n            ImageNormalizeToTensor()]\n        self._transform = transforms.Compose(transform_list)\n\n\n\n"""
models/__init__.py,0,b''
models/animator.py,24,"b""import os\nimport torch\nimport torch.nn.functional as F\nfrom .models import BaseModel\nfrom networks.networks import NetworksFactory, HumanModelRecovery\nfrom utils.nmr import SMPLRenderer\nfrom utils.util import to_tensor\nimport utils.cv_utils as cv_utils\n\n\nclass Animator(BaseModel):\n\n    PART_IDS = {\n        'body': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n        'upper_body': [1, 2, 3, 4, 9],\n        'lower_body': [4, 5],\n        'torso': [9],\n        'all': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    }\n\n    def __init__(self, opt):\n        super(Animator, self).__init__(opt)\n        self._name = 'Animator'\n\n        # create networks\n        self._init_create_networks()\n\n        # prefetch variables\n        self.src_info = None\n        self.ref_info = None\n        self.tsf_info = None\n        self.first_cam = None\n\n        # initialize T\n        self.initial_T = torch.zeros(opt.image_size, opt.image_size, 2, dtype=torch.float32).cuda() - 1.0\n        self.initial_T_grid = self._make_grid()\n\n    def _make_grid(self):\n        # initialize T\n        image_size = self._opt.image_size\n        xy = torch.arange(0, image_size, dtype=torch.float32) / (image_size - 1) * 2 - 1.0\n        grid_y, grid_x = torch.meshgrid(xy, xy)\n\n        # 1. make mesh grid\n        T = torch.stack([grid_x, grid_y], dim=-1).cuda()  # (image_size, image_size, 2)\n\n        return T\n\n    def _create_generator(self):\n        net = NetworksFactory.get_by_name(self._opt.gen_name, bg_dim=4, src_dim=3+self._G_cond_nc,\n                                          tsf_dim=3+self._G_cond_nc, repeat_num=6).cuda()\n\n        if self._opt.load_path:\n            self._load_params(net, self._opt.load_path)\n        elif self._opt.load_epoch > 0:\n            self._load_network(net, 'G', self._opt.load_epoch)\n        else:\n            raise ValueError('load_path {} is empty and load_epoch {} is 0'.format(\n                self._opt.load_path, self._opt.load_epoch))\n\n        net.eval()\n        return net\n\n    def _create_mesh_model(self):\n        hmr = HumanModelRecovery(smpl_pkl_path=self._opt.smpl_model).cuda()\n        saved_data = torch.load(self._opt.hmr_model)\n        hmr.load_state_dict(saved_data)\n\n        hmr.eval()\n        print('load hmr model from {}'.format(self._opt.hmr_model))\n        return hmr\n\n    def _create_render(self, faces):\n        render = SMPLRenderer(faces=faces, map_name=self._opt.map_name, uv_map_path=self._opt.uv_mapping,\n                              tex_size=self._opt.tex_size, image_size=self._opt.image_size, fill_back=True,\n                              anti_aliasing=True, background_color=(0, 0, 0), has_front_map=True).cuda()\n        return render\n\n    def _init_create_networks(self):\n        # generator network\n        self.model = self._create_generator()\n        self.hmr = self._create_mesh_model()\n        self.render = self._create_render(self.hmr.smpl.faces)\n\n    def _extract_smpls(self, input_file):\n        img = cv_utils.read_cv2_img(input_file)\n        img = cv_utils.transform_img(img, image_size=224) * 2 - 1.0  # hmr receive [-1, 1]\n        img = img.transpose((2, 0, 1))\n        img = torch.FloatTensor(img).cuda()[None, ...]\n        theta = self.hmr(img)[-1]\n\n        return theta\n\n    def swap_smpl(self, src_cam, src_shape, tgt_smpl, cam_strategy='smooth'):\n        tgt_cam = tgt_smpl[:, 0:3].contiguous()\n        pose = tgt_smpl[:, 3:75].contiguous()\n\n        # TODO, need more tricky ways\n        if cam_strategy == 'smooth':\n\n            cam = src_cam.clone()\n            delta_xy = tgt_cam[:, 1:] - self.first_cam[:, 1:]\n            cam[:, 1:] += delta_xy\n\n        elif cam_strategy == 'source':\n            cam = src_cam\n        else:\n            cam = tgt_cam\n\n        tsf_smpl = torch.cat([cam, pose, src_shape], dim=1)\n\n        return tsf_smpl\n\n    def calculate_trans(self, bc_f2pts, src_fim, tsf_dim, mask):\n        bs = src_fim.shape[0]\n        T = self.initial_T.repeat(bs, 1, 1, 1)   # (bs, image_size, image_size, 2)\n\n        for i in range(bs):\n            Ti = T[i]\n\n            tsf_mask = mask[i]\n            tsf_i = tsf_dim[i, tsf_mask].long()\n\n            # (nf, 2)\n            tsf_flows = bc_f2pts[i, tsf_i]      # (nt, 2)\n            Ti[tsf_mask] = tsf_flows\n\n        return T\n\n    def transfer(self, tgt_path, tgt_smpl=None, cam_strategy='smooth', t=0, visualizer=None):\n        with torch.no_grad():\n            # 1. get source info\n            src_info = self.src_info\n\n            ori_img = cv_utils.read_cv2_img(tgt_path)\n            if tgt_smpl is None:\n                img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n                img_hmr = torch.FloatTensor(img_hmr).cuda()[None, ...]\n                tgt_smpl = self.hmr(img_hmr)[-1]\n            else:\n                tgt_smpl = to_tensor(tgt_smpl).cuda()[None, ...]\n\n            if t == 0 and cam_strategy == 'smooth':\n                self.first_cam = tgt_smpl[:, 0:3].clone()\n\n            # 2. compute tsf smpl\n            tsf_smpl = self.swap_smpl(src_info['cam'], src_info['shape'], tgt_smpl, cam_strategy=cam_strategy)\n            tsf_info = self.hmr.get_details(tsf_smpl)\n            # add pose condition and face index map into source info\n            tsf_info['cond'], tsf_info['fim'] = self.render.encode_fim(tsf_info['cam'],\n                                                                       tsf_info['verts'], transpose=True)\n            # add part condition into source info\n            tsf_info['part'] = self.render.encode_front_fim(tsf_info['fim'], transpose=True)\n\n            # 3. calculate syn front image and transformation flows\n            ref_info = self.ref_info\n            selected_part_id = self.PART_IDS['body']\n            left_id = [i for i in self.PART_IDS['all'] if i not in selected_part_id]\n\n            src_part_mask = (torch.sum(tsf_info['part'][:, left_id, ...], dim=1) != 0).byte()\n            ref_part_mask = (torch.sum(tsf_info['part'][:, selected_part_id, ...], dim=1) != 0).byte()\n\n            T_s = self.calculate_trans(src_info['bc_f2pts'], src_info['fim'], tsf_info['fim'], src_part_mask)\n            T_r = self.calculate_trans(ref_info['bc_f2pts'], ref_info['fim'], tsf_info['fim'], ref_part_mask)\n\n            tsf_s = self.model.transform(src_info['image'], T_s)\n            tsf_r = self.model.transform(ref_info['image'], T_r)\n\n            tsf_img = tsf_s * src_part_mask.float() + tsf_r * ref_part_mask.float()\n            tsf_inputs = torch.cat([tsf_img, tsf_info['cond']], dim=1)\n\n            preds = self.forward2(tsf_inputs, src_info['feats'], T_s, ref_info['feats'], T_r, src_info['bg'])\n\n            if visualizer is not None:\n                visualizer.vis_named_img('src', src_info['image'])\n                visualizer.vis_named_img('ref', ref_info['image'])\n                visualizer.vis_named_img('src_cond', src_info['cond'])\n                visualizer.vis_named_img('ref_cond', ref_info['cond'])\n                visualizer.vis_named_img('tsf_cond', tsf_info['cond'])\n                visualizer.vis_named_img('tsf_s', tsf_s)\n                visualizer.vis_named_img('tsf_r', tsf_r)\n                visualizer.vis_named_img('tsf_img', tsf_img)\n                visualizer.vis_named_img('preds', preds)\n                visualizer.vis_named_img('src_part_mask', src_part_mask)\n                visualizer.vis_named_img('ref_part_mask', ref_part_mask)\n\n            return preds\n\n    def animate_setup(self, src_path, ref_path, src_smpl=None, ref_smpl=None, output_dir=''):\n\n        with torch.no_grad():\n            self.src_info = self.personalize(src_path, src_smpl)\n            self.ref_info = self.personalize(ref_path, ref_smpl)\n\n    def animate(self, img_paths, smpls=None, cam_strategy='smooth', output_dir='', visualizer=None):\n        length = len(img_paths)\n\n        for t in range(length):\n            img_path = img_paths[t]\n            smpl = smpls[t] if smpls is not None else None\n\n            preds = self.transfer(img_path, smpl, cam_strategy=cam_strategy, t=t, visualizer=visualizer)\n\n    def get_src_bc_f2pts(self, src_cams, src_verts):\n\n        points = self.render.batch_orth_proj_idrot(src_cams, src_verts)\n        f2pts = self.render.points_to_faces(points)\n        bc_f2pts = self.render.compute_barycenter(f2pts)  # (bs, nf, 2)\n\n        return bc_f2pts\n\n    def personalize(self, src_path, src_smpl=None):\n\n        with torch.no_grad():\n            ori_img = cv_utils.read_cv2_img(src_path)\n\n            # resize image and convert the color space from [0, 255] to [-1, 1]\n            img = cv_utils.transform_img(ori_img, self._opt.image_size, transpose=True) * 2 - 1.0\n            img = torch.FloatTensor(img).cuda()[None, ...]\n\n            if src_smpl is None:\n                img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n                img_hmr = torch.FloatTensor(img_hmr).cuda()[None, ...]\n                src_smpl = self.hmr(img_hmr)[-1]\n            else:\n                src_smpl = to_tensor(src_smpl).cuda()[None, ...]\n\n            # source process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n            src_info = self.hmr.get_details(src_smpl)\n\n            # add source bary-center points\n            src_info['bc_f2pts'] = self.get_src_bc_f2pts(src_info['cam'], src_info['verts'])\n\n            # add image to source info\n            src_info['image'] = img\n\n            # add texture into source info\n            _, src_info['tex'] = self.render.forward(src_info['cam'], src_info['verts'],\n                                                     img, is_uv_sampler=False, reverse_yz=True, get_fim=False)\n\n            # add pose condition and face index map into source info\n            src_info['cond'], src_info['fim'] = self.render.encode_fim(src_info['cam'],\n                                                                       src_info['verts'], transpose=True)\n\n            # add part condition into source info\n            src_info['part'] = self.render.encode_front_fim(src_info['fim'], transpose=True)\n\n            # bg input and inpaiting background\n            src_bg_mask = self.morph(src_info['cond'][:, -1:, :, :], ks=15, mode='erode')\n            bg_inputs = torch.cat([img * src_bg_mask, src_bg_mask], dim=1)\n            src_info['bg'] = self.model.bg_model(bg_inputs)\n            #\n            # source identity\n            src_crop_mask = self.morph(src_info['cond'][:, -1:, :, :], ks=3, mode='erode')\n            src_inputs = torch.cat([img * (1 - src_crop_mask), src_info['cond']], dim=1)\n            src_info['feats'] = self.model.src_model.inference(src_inputs)\n            #\n            # self.src_info = src_info\n\n            return src_info\n\n    def morph(self, src_bg_mask, ks, mode='erode'):\n        n_ks = ks ** 2\n        kernel = torch.ones(1, 1, ks, ks, dtype=torch.float32).cuda()\n        out = F.conv2d(src_bg_mask, kernel, padding=ks // 2)\n\n        if mode == 'erode':\n            out = (out == n_ks).float()\n        else:\n            out = (out >= 1).float()\n\n        return out\n\n    def forward(self, tsf_inputs, feats, T, bg):\n        with torch.no_grad():\n            # generate fake images\n            src_encoder_outs, src_resnet_outs = feats\n\n            tsf_color, tsf_mask = self.model.inference(src_encoder_outs, src_resnet_outs, tsf_inputs, T)\n            tsf_mask = self._do_if_necessary_saturate_mask(tsf_mask, saturate=self._opt.do_saturate_mask)\n            pred_imgs = tsf_mask * bg + (1 - tsf_mask) * tsf_color\n\n        return pred_imgs\n\n    def forward2(self, tsf_inputs, feats21, T21, feats11, T11, bg):\n        with torch.no_grad():\n            # generate fake images\n            src_encoder_outs21, src_resnet_outs21 = feats21\n            src_encoder_outs11, src_resnet_outs11 = feats11\n\n            tsf_color, tsf_mask = self.model.swap(tsf_inputs, src_encoder_outs21, src_encoder_outs11,\n                                                  src_resnet_outs21, src_resnet_outs11, T21, T11)\n            tsf_mask = self._do_if_necessary_saturate_mask(tsf_mask, saturate=self._opt.do_saturate_mask)\n            pred_imgs = tsf_mask * bg + (1 - tsf_mask) * tsf_color\n\n        return pred_imgs\n\n    def auto_encoder(self, tsf_inputs, bg_img):\n        with torch.no_grad():\n            tsf_color, tsf_mask = self.model.src_model(tsf_inputs)\n            tsf_mask = self._do_if_necessary_saturate_mask(tsf_mask, saturate=self._opt.do_saturate_mask)\n            pred_imgs = tsf_mask * bg_img + (1 - tsf_mask) * tsf_color\n\n        return pred_imgs\n\n    def _do_if_necessary_saturate_mask(self, m, saturate=False):\n        return torch.clamp(0.55*torch.tanh(3*(m-0.5))+0.5, 0, 1) if saturate else m\n"""
models/baseline.py,81,"b""import os\nimport torch\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport utils.util as util\nimport utils.cv_utils as cv_utils\nfrom .models import BaseModel\nfrom networks.networks import NetworksFactory, HMRLoss, VGGLoss, SphereFaceLoss\nfrom utils.nmr import SMPLRenderer\nimport ipdb\n\n\nclass ConcatBaseline(BaseModel):\n    def __init__(self, opt):\n        super(ConcatBaseline, self).__init__(opt)\n        self._name = 'ConcatBaseline'\n\n        # create networks\n        self._init_create_networks()\n\n        # init train variables and losses\n        if self._is_train:\n            self._init_train_vars()\n            self._init_losses()\n\n        # load networks and optimizers\n        if not self._is_train or self._opt.load_epoch > 0:\n            self.load()\n\n        # prefetch variables\n        self._init_prefetch_inputs()\n\n    def _init_create_networks(self):\n        # generator network\n        self._G = self._create_generator()\n        self._G.init_weights()\n        if len(self._gpu_ids) > 1:\n            self._G = torch.nn.DataParallel(self._G)\n        self._G.cuda()\n\n        # discriminator network\n        self._D = self._create_discriminator()\n        self._D.init_weights()\n        if len(self._gpu_ids) > 1:\n            self._D = torch.nn.DataParallel(self._D)\n        self._D.cuda()\n\n        self._criterion_hmr = HMRLoss(pretrain_model=self._opt.hmr_model, smpl_pkl_path=self._opt.smpl_model).cuda()\n\n        self._render = SMPLRenderer(faces=self._criterion_hmr.hmr.smpl.faces,\n                                    map_name=self._opt.map_name,\n                                    uv_map_path=self._opt.uv_mapping,\n                                    tex_size=self._opt.tex_size,\n                                    image_size=self._opt.image_size, fill_back=True,\n                                    anti_aliasing=True, background_color=(0, 0, 0), has_front_map=False)\n\n    def _create_generator(self):\n        return NetworksFactory.get_by_name('concat', bg_dim=4, src_dim=self._G_cond_nc, tsf_dim=self._G_cond_nc, repeat_num=6)\n\n    def _create_discriminator(self):\n        return NetworksFactory.get_by_name('discriminator_patch_gan', input_nc=3 + self._D_cond_nc,\n                                           ndf=64, n_layers=4, use_sigmoid=False)\n\n    def _init_train_vars(self):\n        self._current_lr_G = self._opt.lr_G\n        self._current_lr_D = self._opt.lr_D\n\n        # initialize optimizers\n        self._optimizer_G = torch.optim.Adam(self._G.parameters(), lr=self._current_lr_G,\n                                             betas=[self._opt.G_adam_b1, self._opt.G_adam_b2])\n        self._optimizer_D = torch.optim.Adam(self._D.parameters(), lr=self._current_lr_D,\n                                             betas=[self._opt.D_adam_b1, self._opt.D_adam_b2])\n\n    def _init_prefetch_inputs(self):\n        self._input_src_img = None\n        self._input_src_smpl = None\n        self._input_src_cond = None\n\n        self._input_desired_img = None\n        self._input_desired_smpl = None\n        self._input_desired_cond = None\n\n        self._input_real_imgs = None\n\n        self._bg_mask = None\n        self._input_G_bg = None\n        self._input_G_fg = None\n\n        self._input_D = None\n        self._src_info = None\n        self._tgt_info = None\n\n    def _init_losses(self):\n        # define loss functions\n        self._criterion_l1 = torch.nn.L1Loss().cuda()\n\n        if self._opt.use_vgg:\n            self._criterion_vgg = VGGLoss().cuda()\n\n        if self._opt.use_face:\n            self._criterion_face = SphereFaceLoss().cuda()\n\n        # init losses G\n        self._loss_g_l1 = self._Tensor([0])\n        self._loss_g_vgg = self._Tensor([0])\n        self._loss_g_face = self._Tensor([0])\n        self._loss_g_adv = self._Tensor([0])\n        self._loss_g_smooth = self._Tensor([0])\n        self._loss_g_mask = self._Tensor([0])\n        self._loss_g_mask_smooth = self._Tensor([0])\n\n        # init losses D\n        self._loss_d_real = self._Tensor([0])\n        self._loss_d_fake = self._Tensor([0])\n\n    def morph(self, src_bg_mask, ks, mode='erode'):\n        n_ks = ks ** 2\n        kernel = torch.ones(1, 1, ks, ks, dtype=torch.float32).cuda()\n        out = F.conv2d(src_bg_mask, kernel, padding=ks // 2)\n\n        if mode == 'erode':\n            out = (out == n_ks).float()\n        else:\n            out = (out >= 1).float()\n\n        return out\n\n    def set_input_cond(self, is_train=True):\n        # source process\n        # source process\n        src_info = self._criterion_hmr.hmr.get_details(self._input_src_smpl)\n        src_rd, src_info['tex'] = self._render.forward(src_info['cam'], src_info['verts'],\n                                                       self._input_src_img, get_fim=False)\n\n        self._input_src_cond, src_info['fim'] = self._render.encode_fim(src_info['cam'], src_info['verts'], transpose=True)\n        src_bg_mask = self.morph(self._input_src_cond[:, -1:, :, :], ks=15, mode='erode')\n\n        # bg input\n        self._input_G_bg = torch.cat([self._input_src_img * src_bg_mask, src_bg_mask], dim=1)\n\n        # transfer\n        tgt_info = self._criterion_hmr.hmr.get_details(self._input_desired_smpl)\n        self._input_desired_cond, tgt_info['fim'] = self._render.encode_fim(tgt_info['cam'], tgt_info['verts'], transpose=True)\n\n        self._input_G_fg = torch.cat([self._input_src_img, self._input_src_cond, self._input_desired_cond], dim=1)\n\n        if is_train:\n            tsf_crop_mask = self.morph(self._input_desired_cond[:, -1:, :, :], ks=3, mode='erode')\n            self._bg_mask = tsf_crop_mask\n            self._input_D = torch.cat([self._input_desired_img, self._input_src_cond, self._input_desired_cond], dim=1)\n            self._input_real_imgs = self._input_desired_img\n\n        self._src_info = src_info\n        self._tgt_info = tgt_info\n\n    def set_input(self, input):\n\n        with torch.no_grad():\n            images = input['images']\n            smpls = input['smpls']\n            self._input_src_img = images[:, 0, ...].contiguous().cuda()\n            self._input_src_smpl = smpls[:, 0, ...].contiguous().cuda()\n            self._input_desired_img = images[:, 1, ...].contiguous().cuda()\n            self._input_desired_smpl = smpls[:, 1, ...].contiguous().cuda()\n\n            self.set_input_cond()\n\n    def set_test_input(self, input):\n        with torch.no_grad():\n            self._input_src_img = input['src_img']\n            self._input_src_smpl = input['src_smpl']\n            self._input_desired_smpl = input['desired_smpl']\n\n            self.set_input_cond(is_train=False)\n\n    def set_train(self):\n        self._G.train()\n        self._D.train()\n        self._is_train = True\n\n    def set_eval(self):\n        self._G.eval()\n        self._is_train = False\n\n    def forward(self, keep_data_for_visuals=False, return_estimates=False):\n        # generate fake images\n        fake_bg, fake_tsf_color, fake_tsf_mask = self._G.forward(self._input_G_bg, self._input_G_fg)\n        fake_tsf_mask = self._do_if_necessary_saturate_mask(fake_tsf_mask, saturate=self._opt.do_saturate_mask)\n        fake_tsf_imgs = fake_tsf_mask * fake_bg + (1 - fake_tsf_mask) * fake_tsf_color\n\n        fake_masks = fake_tsf_mask\n        fake_imgs =  fake_tsf_imgs\n\n        # keep data for visualization\n        if keep_data_for_visuals:\n            self.transfer_imgs(fake_bg, fake_imgs, fake_tsf_color, fake_masks)\n\n        return fake_tsf_imgs, fake_imgs, fake_masks\n\n    def optimize_parameters(self, train_generator=True, keep_data_for_visuals=False):\n        if self._is_train:\n            # convert tensor to variables\n            self._B = self._input_src_img.size(0)\n\n            # run\n            fake_tsf_imgs, fake_imgs, fake_masks = self.forward(keep_data_for_visuals=keep_data_for_visuals)\n\n            loss_G = self._optimize_G(fake_tsf_imgs, fake_imgs, fake_masks)\n\n            self._optimizer_G.zero_grad()\n            loss_G.backward()\n            self._optimizer_G.step()\n\n            # train D\n            if train_generator:\n                loss_D = self._optimize_D(fake_tsf_imgs)\n                self._optimizer_D.zero_grad()\n                loss_D.backward()\n                self._optimizer_D.step()\n\n    def _optimize_G(self, fake_tsf_imgs, fake_imgs, fake_masks):\n        # D(G(Ic1, c2)*M, c2) masked\n        fake_input_D = torch.cat([fake_tsf_imgs, self._input_src_cond, self._input_desired_cond], dim=1)\n        d_fake_outs = self._D.forward(fake_input_D)\n        self._loss_g_adv = self._compute_loss_D(d_fake_outs, 0) * self._opt.lambda_D_prob\n\n        # l_cyc(G(Ic1,c2)*M, Ic2)\n        self._loss_g_l1 = self._criterion_l1(fake_imgs, self._input_real_imgs) * self._opt.lambda_rec\n\n        if self._opt.use_vgg:\n            self._loss_g_vgg = self._criterion_vgg(fake_imgs, self._input_real_imgs) * self._opt.lambda_tsf\n\n        if self._opt.use_face:\n            self._loss_g_face = self._criterion_face(fake_tsf_imgs, self._input_desired_img,\n                                                     self._tgt_info['j2d'], self._tgt_info['j2d']) * self._opt.lambda_face\n        # loss mask\n        self._loss_g_mask = torch.mean((fake_masks - self._bg_mask) ** 2) * self._opt.lambda_mask\n        self._loss_g_mask_smooth = self._compute_loss_smooth(fake_masks) * self._opt.lambda_mask_smooth\n\n        # combine losses\n        return self._loss_g_adv + self._loss_g_l1 + self._loss_g_vgg + self._loss_g_face + \\\n               self._loss_g_mask + self._loss_g_mask_smooth\n\n    def _optimize_D(self, fake_tsf_imgs):\n        fake_input_D = torch.cat([fake_tsf_imgs.detach(), self._input_src_cond, self._input_desired_cond], dim=1)\n\n        d_real_outs = self._D.forward(self._input_D)\n        d_fake_outs = self._D.forward(fake_input_D)\n\n        self._loss_d_real = self._compute_loss_D(d_real_outs, 1) * self._opt.lambda_D_prob\n        self._loss_d_fake = self._compute_loss_D(d_fake_outs, -1) * self._opt.lambda_D_prob\n\n        # combine losses\n        return self._loss_d_real + self._loss_d_fake\n\n    def _compute_loss_D(self, x, y):\n        return torch.mean((x - y) ** 2)\n\n    def _compute_loss_smooth(self, mat):\n        return torch.sum(torch.abs(mat[:, :, :, :-1] - mat[:, :, :, 1:])) + \\\n               torch.sum(torch.abs(mat[:, :, :-1, :] - mat[:, :, 1:, :]))\n\n    def get_current_errors(self):\n        loss_dict = OrderedDict([('g_l1', self._loss_g_l1.item()),\n                                 ('g_vgg', self._loss_g_vgg.item()),\n                                 ('g_face', self._loss_g_face.item()),\n                                 ('g_adv', self._loss_g_adv.item()),\n                                 ('g_mask', self._loss_g_mask.item()),\n                                 ('g_mask_smooth', self._loss_g_mask_smooth.item()),\n                                 ('d_real', self._loss_d_real.item()),\n                                 ('d_fake', self._loss_d_fake.item())])\n\n        return loss_dict\n\n    def get_current_scalars(self):\n        return OrderedDict([('lr_G', self._current_lr_G), ('lr_D', self._current_lr_D)])\n\n    def get_current_visuals(self):\n        # visuals return dictionary\n        visuals = OrderedDict()\n\n        # inputs\n        visuals['1_real_img'] = self._vis_real_img\n        visuals['3_fake_bg'] = self._vis_fake_bg\n\n        # outputs\n        visuals['4_fake_img'] = self._vis_fake_img\n        visuals['5_fake_color'] = self._vis_fake_color\n        visuals['6_fake_mask'] = self._vis_fake_mask\n\n        # batch outputs\n        visuals['7_batch_real_img'] = self._vis_batch_real_img\n        visuals['8_batch_fake_img'] = self._vis_batch_fake_img\n\n        return visuals\n\n    def transfer_imgs(self, fake_bg, fake_imgs, fake_color, fake_masks):\n        self._vis_real_img = util.tensor2im(self._input_real_imgs)\n\n        ids = fake_imgs.shape[0] // 2\n        self._vis_fake_bg = util.tensor2im(fake_bg.data)\n        self._vis_fake_color = util.tensor2im(fake_color.data)\n        self._vis_fake_img = util.tensor2im(fake_imgs[ids].data)\n        self._vis_fake_mask = util.tensor2maskim(fake_masks[ids].data)\n\n        self._vis_batch_real_img = util.tensor2im(self._input_real_imgs, idx=-1)\n        self._vis_batch_fake_img = util.tensor2im(fake_imgs.data, idx=-1)\n\n    def save(self, label):\n        # save networks\n        self._save_network(self._G, 'G', label)\n        self._save_network(self._D, 'D', label)\n\n        # save optimizers\n        self._save_optimizer(self._optimizer_G, 'G', label)\n        self._save_optimizer(self._optimizer_D, 'D', label)\n\n    def load(self):\n        load_epoch = self._opt.load_epoch\n\n        # load G\n        self._load_network(self._G, 'G', load_epoch)\n\n        if self._is_train:\n            # load D\n            self._load_network(self._D, 'D', load_epoch)\n\n            # load optimizers\n            self._load_optimizer(self._optimizer_G, 'G', load_epoch)\n            self._load_optimizer(self._optimizer_D, 'D', load_epoch)\n\n    def update_learning_rate(self):\n        # updated learning rate G\n        final_lr = self._opt.final_lr\n\n        lr_decay_G = (self._opt.lr_G - final_lr) / self._opt.nepochs_decay\n        self._current_lr_G -= lr_decay_G\n        for param_group in self._optimizer_G.param_groups:\n            param_group['lr'] = self._current_lr_G\n        print('update G learning rate: %f -> %f' % (self._current_lr_G + lr_decay_G, self._current_lr_G))\n\n        # update learning rate D\n        lr_decay_D = (self._opt.lr_D - final_lr) / self._opt.nepochs_decay\n        self._current_lr_D -= lr_decay_D\n        for param_group in self._optimizer_D.param_groups:\n            param_group['lr'] = self._current_lr_D\n        print('update D learning rate: %f -> %f' % (self._current_lr_D + lr_decay_D, self._current_lr_D))\n\n    def _do_if_necessary_saturate_mask(self, m, saturate=False):\n        return torch.clamp(0.55*torch.tanh(3*(m-0.5))+0.5, 0, 1) if saturate else m\n\n    def personalize(self, src_path, src_smpl=None, output_path=''):\n\n        with torch.no_grad():\n            ori_img = cv_utils.read_cv2_img(src_path)\n\n            # resize image and convert the color space from [0, 255] to [-1, 1]\n            img = cv_utils.transform_img(ori_img, self._opt.image_size, transpose=True) * 2 - 1.0\n            img = torch.FloatTensor(img).cuda()[None, ...]\n\n            if src_smpl is None:\n                img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n                img_hmr = torch.FloatTensor(img_hmr).cuda()[None, ...]\n                src_smpl = self._criterion_hmr.hmr(img_hmr)[-1]\n            else:\n                src_smpl = util.to_tensor(src_smpl).cuda()[None, ...]\n\n            # source process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n            src_info = self._criterion_hmr.hmr.get_details(src_smpl)\n\n            # add image to source info\n            src_info['image'] = img\n\n            # add texture into source info\n            _, src_info['tex'] = self._render.forward(src_info['cam'], src_info['verts'],\n                                                      img, is_uv_sampler=False, reverse_yz=True, get_fim=False)\n\n            # add pose condition and face index map into source info\n            src_info['cond'], src_info['fim'] = self._render.encode_fim(src_info['cam'],\n                                                                        src_info['verts'], transpose=True)\n\n            src_bg_mask = self.morph(src_info['cond'][:, -1:, :, :], ks=15, mode='erode')\n\n            # bg input\n            bg_inputs = torch.cat([img * src_bg_mask, src_bg_mask], dim=1)\n            src_info['bg'] = self._G.bg_model(bg_inputs)\n\n            self.src_info = src_info\n\n            if output_path:\n                cv_utils.save_cv2_img(ori_img, output_path, image_size=self._opt.image_size)\n\n    def transfer(self, tgt_path, tgt_smpl=None):\n        with torch.no_grad():\n            # get source info\n            src_info = self.src_info\n\n            ori_img = cv_utils.read_cv2_img(tgt_path)\n            if tgt_smpl is None:\n                img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n                img_hmr = torch.FloatTensor(img_hmr).cuda()[None, ...]\n                tgt_smpl = self._criterion_hmr.hmr(img_hmr)[-1]\n            else:\n                tgt_smpl = util.to_tensor(tgt_smpl).cuda()[None, ...]\n\n            tgt_info = self._criterion_hmr.hmr.get_details(tgt_smpl)\n            tgt_info['cond'], tgt_info['fim'] = self._render.encode_fim(tgt_info['cam'], tgt_info['verts'],\n                                                                        transpose=True)\n\n            tsf_inputs = torch.cat([src_info['image'], src_info['cond'], tgt_info['cond']], dim=1)\n            tsf_color, tsf_mask = self._G.inference(tsf_inputs)\n            pred_imgs = tsf_mask * src_info['bg'] + (1 - tsf_mask) * tsf_color\n\n            return pred_imgs, ori_img\n\n    def imitate(self, tgt_paths, tgt_smpls=None, output_dir='', visualizer=None, cam_strategy=None):\n        length = len(tgt_paths)\n\n        outputs = []\n        for t in range(length):\n\n            tgt_path = tgt_paths[t]\n            tgt_smpl = tgt_smpls[t] if tgt_smpls is not None else None\n\n            preds, ori_img = self.transfer(tgt_path, tgt_smpl)\n\n            if visualizer is not None:\n                visualizer.vis_named_img('pred_2', preds)\n\n            if output_dir:\n                preds = preds[0].permute(1, 2, 0)\n                preds = preds.cpu().numpy()\n                filename = os.path.split(tgt_path)[-1]\n\n                cv_utils.save_cv2_img(preds, os.path.join(output_dir, 'pred_' + filename), normalize=True)\n                cv_utils.save_cv2_img(ori_img, os.path.join(output_dir, 'gt_' + filename),\n                                      image_size=self._opt.image_size)\n            outputs.append(preds)\n            print('{} / {}'.format(t, length))\n\n        return outputs\n\n\nclass TextureWarpingBaseline(BaseModel):\n    def __init__(self, opt):\n        super(TextureWarpingBaseline, self).__init__(opt)\n        self._name = 'TextureWarpingBaseline'\n\n        # create networks\n        self._init_create_networks()\n\n        # init train variables and losses\n        if self._is_train:\n            self._init_train_vars()\n            self._init_losses()\n\n        # load networks and optimizers\n        if not self._is_train or self._opt.load_epoch > 0:\n            self.load()\n\n        # prefetch variables\n        self._init_prefetch_inputs()\n\n    def _init_create_networks(self):\n        # generator network\n        self._G = self._create_generator()\n        self._G.init_weights()\n        if len(self._gpu_ids) > 1:\n            self._G = torch.nn.DataParallel(self._G)\n        self._G.cuda()\n\n        # discriminator network\n        self._D = self._create_discriminator()\n        self._D.init_weights()\n        if len(self._gpu_ids) > 1:\n            self._D = torch.nn.DataParallel(self._D)\n        self._D.cuda()\n\n        self._criterion_hmr = HMRLoss(pretrain_model=self._opt.hmr_model, smpl_pkl_path=self._opt.smpl_model).cuda()\n\n        self._render = SMPLRenderer(faces=self._criterion_hmr.hmr.smpl.faces,\n                                    map_name=self._opt.map_name,\n                                    uv_map_path=self._opt.uv_mapping,\n                                    tex_size=self._opt.tex_size,\n                                    image_size=self._opt.image_size, fill_back=True,\n                                    anti_aliasing=True, background_color=(0, 0, 0), has_front_map=False)\n\n    def _create_generator(self):\n        return NetworksFactory.get_by_name('concat', bg_dim=4, src_dim=0, tsf_dim=self._G_cond_nc,\n                                           repeat_num=6)\n\n    def _create_discriminator(self):\n        return NetworksFactory.get_by_name('discriminator_patch_gan', input_nc=3 + self._D_cond_nc,\n                                           ndf=64, n_layers=4, use_sigmoid=False)\n\n    def _init_train_vars(self):\n        self._current_lr_G = self._opt.lr_G\n        self._current_lr_D = self._opt.lr_D\n\n        # initialize optimizers\n        self._optimizer_G = torch.optim.Adam(self._G.parameters(), lr=self._current_lr_G,\n                                             betas=[self._opt.G_adam_b1, self._opt.G_adam_b2])\n        self._optimizer_D = torch.optim.Adam(self._D.parameters(), lr=self._current_lr_D,\n                                             betas=[self._opt.D_adam_b1, self._opt.D_adam_b2])\n\n    def _init_prefetch_inputs(self):\n        self._input_src_img = None\n        self._input_src_smpl = None\n        self._input_src_cond = None\n\n        self._input_desired_img = None\n        self._input_desired_smpl = None\n        self._input_desired_cond = None\n\n        self._input_real_imgs = None\n\n        self._bg_mask = None\n        self._input_src = None\n        self._input_tsf = None\n        self._input_G_bg = None\n        self._input_G_src = None\n        self._input_G_tsf = None\n        self._input_D = None\n        self._src_info = None\n        self._tgt_info = None\n\n    def _init_losses(self):\n        # define loss functions\n        self._criterion_l1 = torch.nn.L1Loss().cuda()\n\n        if self._opt.use_vgg:\n            self._criterion_vgg = VGGLoss().cuda()\n\n        if self._opt.use_face:\n            self._criterion_face = SphereFaceLoss().cuda()\n\n        # init losses G\n        self._loss_g_l1 = self._Tensor([0])\n        self._loss_g_vgg = self._Tensor([0])\n        self._loss_g_face = self._Tensor([0])\n        self._loss_g_adv = self._Tensor([0])\n        self._loss_g_smooth = self._Tensor([0])\n        self._loss_g_mask = self._Tensor([0])\n        self._loss_g_mask_smooth = self._Tensor([0])\n\n        # init losses D\n        self._loss_d_real = self._Tensor([0])\n        self._loss_d_fake = self._Tensor([0])\n\n    def morph(self, src_bg_mask, ks, mode='erode'):\n        n_ks = ks ** 2\n        kernel = torch.ones(1, 1, ks, ks, dtype=torch.float32).cuda()\n        out = F.conv2d(src_bg_mask, kernel, padding=ks // 2)\n\n        if mode == 'erode':\n            out = (out == n_ks).float()\n        else:\n            out = (out >= 1).float()\n\n        return out\n\n    def set_input_cond(self, is_train=True):\n        # source process\n        # source process\n        src_info = self._criterion_hmr.hmr.get_details(self._input_src_smpl)\n        src_rd, src_info['tex'] = self._render.forward(src_info['cam'], src_info['verts'],\n                                                       self._input_src_img, is_uv_sampler=False,\n                                                       reverse_yz=True, get_fim=False)\n\n        self._input_src_cond, src_info['fim'] = self._render.encode_fim(src_info['cam'], src_info['verts'], transpose=True)\n        src_bg_mask = self.morph(self._input_src_cond[:, -1:, :, :], ks=15, mode='erode')\n        src_crop_mask = self.morph(self._input_src_cond[:, -1:, :, :], ks=3, mode='erode')\n\n        # bg input\n        self._input_G_bg = torch.cat([self._input_src_img * src_bg_mask, src_bg_mask], dim=1)\n\n        # transfer\n        tgt_info = self._criterion_hmr.hmr.get_details(self._input_desired_smpl)\n        self._input_tsf, _ = self._render.render(tgt_info['cam'], tgt_info['verts'], src_info['tex'], reverse_yz=True, get_fim=False)\n        self._input_desired_cond, tgt_info['fim'] = self._render.encode_fim(tgt_info['cam'], tgt_info['verts'], transpose=True)\n        self._input_G_tsf = torch.cat([self._input_tsf, self._input_desired_cond], dim=1)\n\n        if is_train:\n            tsf_crop_mask = self.morph(self._input_desired_cond[:, -1:, :, :], ks=3, mode='erode')\n            self._bg_mask = tsf_crop_mask\n            self._input_D = torch.cat([self._input_desired_img, self._input_src_cond, self._input_desired_cond], dim=1)\n            self._input_real_imgs = self._input_desired_img\n\n        self._src_info = src_info\n        self._tgt_info = tgt_info\n\n    def set_input(self, input):\n\n        with torch.no_grad():\n            images = input['images']\n            smpls = input['smpls']\n            self._input_src_img = images[:, 0, ...].contiguous().cuda()\n            self._input_src_smpl = smpls[:, 0, ...].contiguous().cuda()\n            self._input_desired_img = images[:, 1, ...].contiguous().cuda()\n            self._input_desired_smpl = smpls[:, 1, ...].contiguous().cuda()\n\n            self.set_input_cond()\n\n    def set_test_input(self, input):\n        with torch.no_grad():\n            self._input_src_img = input['src_img']\n            self._input_src_smpl = input['src_smpl']\n            self._input_desired_smpl = input['desired_smpl']\n\n            self.set_input_cond(is_train=False)\n\n    def set_train(self):\n        self._G.train()\n        self._D.train()\n        self._is_train = True\n\n    def set_eval(self):\n        self._G.eval()\n        self._is_train = False\n\n    def forward(self, keep_data_for_visuals=False, return_estimates=False):\n        # generate fake images\n        fake_bg, fake_tsf_color, fake_tsf_mask = self._G.forward(self._input_G_bg, self._input_G_tsf)\n        fake_tsf_mask = self._do_if_necessary_saturate_mask(fake_tsf_mask, saturate=self._opt.do_saturate_mask)\n        fake_tsf_imgs = fake_tsf_mask * fake_bg + (1 - fake_tsf_mask) * fake_tsf_color\n\n        fake_masks = fake_tsf_mask\n        fake_imgs = fake_tsf_imgs\n\n        # keep data for visualization\n        if keep_data_for_visuals:\n            self.transfer_imgs(fake_bg, fake_imgs, fake_tsf_color, fake_masks)\n\n        return fake_tsf_imgs, fake_imgs, fake_masks\n\n    def optimize_parameters(self, train_generator=True, keep_data_for_visuals=False):\n        if self._is_train:\n            # convert tensor to variables\n            self._B = self._input_src_img.size(0)\n\n            # run\n            fake_tsf_imgs, fake_imgs, fake_masks = self.forward(keep_data_for_visuals=keep_data_for_visuals)\n\n            loss_G = self._optimize_G(fake_tsf_imgs, fake_imgs, fake_masks)\n\n            self._optimizer_G.zero_grad()\n            loss_G.backward()\n            self._optimizer_G.step()\n\n            # train D\n            if train_generator:\n                loss_D = self._optimize_D(fake_tsf_imgs)\n                self._optimizer_D.zero_grad()\n                loss_D.backward()\n                self._optimizer_D.step()\n\n    def _optimize_G(self, fake_tsf_imgs, fake_imgs, fake_masks):\n        # D(G(Ic1, c2)*M, c2) masked\n        fake_input_D = torch.cat([fake_tsf_imgs, self._input_src_cond, self._input_desired_cond], dim=1)\n        d_fake_outs = self._D.forward(fake_input_D)\n        self._loss_g_adv = self._compute_loss_D(d_fake_outs, 0) * self._opt.lambda_D_prob\n\n        # l_cyc(G(Ic1,c2)*M, Ic2)\n        self._loss_g_l1 = self._criterion_l1(fake_imgs, self._input_real_imgs) * self._opt.lambda_rec\n\n        if self._opt.use_vgg:\n            self._loss_g_vgg = self._criterion_vgg(fake_imgs, self._input_real_imgs) * self._opt.lambda_tsf\n\n        if self._opt.use_face:\n            self._loss_g_face = self._criterion_face(fake_tsf_imgs, self._input_desired_img,\n                                                     self._tgt_info['j2d'], self._tgt_info['j2d']) * self._opt.lambda_face\n        # loss mask\n        self._loss_g_mask = torch.mean((fake_masks - self._bg_mask) ** 2) * self._opt.lambda_mask\n        self._loss_g_mask_smooth = self._compute_loss_smooth(fake_masks) * self._opt.lambda_mask_smooth\n\n        # combine losses\n        return self._loss_g_adv + self._loss_g_l1 + self._loss_g_vgg + self._loss_g_face + \\\n               self._loss_g_mask + self._loss_g_mask_smooth\n\n    def _optimize_D(self, fake_tsf_imgs):\n        fake_input_D = torch.cat([fake_tsf_imgs.detach(), self._input_src_cond, self._input_desired_cond], dim=1)\n\n        d_real_outs = self._D.forward(self._input_D)\n        d_fake_outs = self._D.forward(fake_input_D)\n\n        self._loss_d_real = self._compute_loss_D(d_real_outs, 1) * self._opt.lambda_D_prob\n        self._loss_d_fake = self._compute_loss_D(d_fake_outs, -1) * self._opt.lambda_D_prob\n\n        # combine losses\n        return self._loss_d_real + self._loss_d_fake\n\n    def _compute_loss_D(self, x, y):\n        return torch.mean((x - y) ** 2)\n\n    def _compute_loss_smooth(self, mat):\n        return torch.sum(torch.abs(mat[:, :, :, :-1] - mat[:, :, :, 1:])) + \\\n               torch.sum(torch.abs(mat[:, :, :-1, :] - mat[:, :, 1:, :]))\n\n    def get_current_errors(self):\n        loss_dict = OrderedDict([('g_l1', self._loss_g_l1.item()),\n                                 ('g_vgg', self._loss_g_vgg.item()),\n                                 ('g_face', self._loss_g_face.item()),\n                                 ('g_adv', self._loss_g_adv.item()),\n                                 ('g_mask', self._loss_g_mask.item()),\n                                 ('g_mask_smooth', self._loss_g_mask_smooth.item()),\n                                 ('d_real', self._loss_d_real.item()),\n                                 ('d_fake', self._loss_d_fake.item())])\n\n        return loss_dict\n\n    def get_current_scalars(self):\n        return OrderedDict([('lr_G', self._current_lr_G), ('lr_D', self._current_lr_D)])\n\n    def get_current_visuals(self):\n        # visuals return dictionary\n        visuals = OrderedDict()\n\n        # inputs\n        visuals['1_real_img'] = self._vis_real_img\n        # visuals['2_input_tsf'] = self._vis_tsf\n        visuals['3_fake_bg'] = self._vis_fake_bg\n\n        # outputs\n        visuals['4_fake_img'] = self._vis_fake_img\n        visuals['5_fake_color'] = self._vis_fake_color\n        visuals['6_fake_mask'] = self._vis_fake_mask\n\n        # batch outputs\n        visuals['7_batch_real_img'] = self._vis_batch_real_img\n        visuals['8_batch_fake_img'] = self._vis_batch_fake_img\n\n        return visuals\n\n    def transfer_imgs(self, fake_bg, fake_imgs, fake_color, fake_masks):\n        self._vis_real_img = util.tensor2im(self._input_real_imgs)\n\n        ids = fake_imgs.shape[0] // 2\n        # self._vis_tsf = util.tensor2im(self._input_tsf.data)\n        self._vis_fake_bg = util.tensor2im(fake_bg.data)\n        self._vis_fake_color = util.tensor2im(fake_color.data)\n        self._vis_fake_img = util.tensor2im(fake_imgs[ids].data)\n        self._vis_fake_mask = util.tensor2maskim(fake_masks[ids].data)\n\n        self._vis_batch_real_img = util.tensor2im(self._input_real_imgs, idx=-1)\n        self._vis_batch_fake_img = util.tensor2im(fake_imgs.data, idx=-1)\n\n    def save(self, label):\n        # save networks\n        self._save_network(self._G, 'G', label)\n        self._save_network(self._D, 'D', label)\n\n        # save optimizers\n        self._save_optimizer(self._optimizer_G, 'G', label)\n        self._save_optimizer(self._optimizer_D, 'D', label)\n\n    def load(self):\n        load_epoch = self._opt.load_epoch\n\n        # load G\n        self._load_network(self._G, 'G', load_epoch)\n\n        if self._is_train:\n            # load D\n            self._load_network(self._D, 'D', load_epoch)\n\n            # load optimizers\n            self._load_optimizer(self._optimizer_G, 'G', load_epoch)\n            self._load_optimizer(self._optimizer_D, 'D', load_epoch)\n\n    def update_learning_rate(self):\n        # updated learning rate G\n        final_lr = self._opt.final_lr\n\n        lr_decay_G = (self._opt.lr_G - final_lr) / self._opt.nepochs_decay\n        self._current_lr_G -= lr_decay_G\n        for param_group in self._optimizer_G.param_groups:\n            param_group['lr'] = self._current_lr_G\n        print('update G learning rate: %f -> %f' % (self._current_lr_G + lr_decay_G, self._current_lr_G))\n\n        # update learning rate D\n        lr_decay_D = (self._opt.lr_D - final_lr) / self._opt.nepochs_decay\n        self._current_lr_D -= lr_decay_D\n        for param_group in self._optimizer_D.param_groups:\n            param_group['lr'] = self._current_lr_D\n        print('update D learning rate: %f -> %f' % (self._current_lr_D + lr_decay_D, self._current_lr_D))\n\n    def _do_if_necessary_saturate_mask(self, m, saturate=False):\n        return torch.clamp(0.55*torch.tanh(3*(m-0.5))+0.5, 0, 1) if saturate else m\n\n    def swap_smpl(self, src_cam, src_shape, tgt_smpl, preserve_scale=True):\n        cam = tgt_smpl[:, 0:3].contiguous()\n        pose = tgt_smpl[:, 3:75].contiguous()\n\n        if preserve_scale:\n            cam[:, 0] = src_cam[:, 0]\n            # cam[:, 1:] = (src_cam[:, 0] / cam[:, 0]) * cam[:, 1:] + src_cam[:, 1:]\n            # cam[:, 0] = src_cam[:, 0]\n        else:\n            cam = src_cam\n\n        tsf_smpl = torch.cat([cam, pose, src_shape], dim=1)\n\n        return tsf_smpl\n\n    def personalize(self, src_path, src_smpl=None, output_path=''):\n\n        with torch.no_grad():\n            ori_img = cv_utils.read_cv2_img(src_path)\n\n            # resize image and convert the color space from [0, 255] to [-1, 1]\n            img = cv_utils.transform_img(ori_img, self._opt.image_size, transpose=True) * 2 - 1.0\n            img = torch.FloatTensor(img).cuda()[None, ...]\n\n            if src_smpl is None:\n                img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n                img_hmr = torch.FloatTensor(img_hmr).cuda()[None, ...]\n                src_smpl = self._criterion_hmr.hmr(img_hmr)[-1]\n            else:\n                src_smpl = util.to_tensor(src_smpl).cuda()[None, ...]\n\n            # source process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n            src_info = self._criterion_hmr.hmr.get_details(src_smpl)\n\n            # add image to source info\n            src_info['image'] = img\n\n            # add texture into source info\n            _, src_info['tex'] = self._render.forward(src_info['cam'], src_info['verts'],\n                                                      img, is_uv_sampler=False, reverse_yz=True, get_fim=False)\n\n            # add pose condition and face index map into source info\n            src_info['cond'], src_info['fim'] = self._render.encode_fim(src_info['cam'],\n                                                                        src_info['verts'], transpose=True)\n\n            src_bg_mask = self.morph(src_info['cond'][:, -1:, :, :], ks=15, mode='erode')\n\n            # bg input\n            bg_inputs = torch.cat([img * src_bg_mask, src_bg_mask], dim=1)\n            src_info['bg'] = self._G.bg_model(bg_inputs)\n\n            self.src_info = src_info\n\n            if output_path:\n                cv_utils.save_cv2_img(ori_img, output_path, image_size=self._opt.image_size)\n\n    def transfer(self, tgt_path, tgt_smpl=None):\n        with torch.no_grad():\n            # get source info\n            src_info = self.src_info\n\n            ori_img = cv_utils.read_cv2_img(tgt_path)\n            if tgt_smpl is None:\n                img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n                img_hmr = torch.FloatTensor(img_hmr).cuda()[None, ...]\n                tgt_smpl = self._criterion_hmr.hmr(img_hmr)[-1]\n            else:\n                tgt_smpl = util.to_tensor(tgt_smpl).cuda()[None, ...]\n\n            tsf_smpl = self.swap_smpl(src_info['cam'], src_info['shape'], tgt_smpl, preserve_scale=True)\n\n            tsf_info = self._criterion_hmr.hmr.get_details(tsf_smpl)\n            tsf_img, _ = self._render.render(tsf_info['cam'], tsf_info['verts'], src_info['tex'],\n                                             reverse_yz=True, get_fim=False)\n            tsf_info['cond'], tsf_info['fim'] = self._render.encode_fim(tsf_info['cam'], tsf_info['verts'],\n                                                                        transpose=True)\n            tsf_inputs = torch.cat([tsf_img, tsf_info['cond']], dim=1)\n            tsf_color, tsf_mask = self._G.inference(tsf_inputs)\n            pred_imgs = tsf_mask * src_info['bg'] + (1 - tsf_mask) * tsf_color\n\n            return pred_imgs, ori_img\n\n    def imitate(self, tgt_paths, tgt_smpls=None, output_dir='', visualizer=None, cam_strategy=None):\n        length = len(tgt_paths)\n\n        for t in range(length):\n\n            tgt_path = tgt_paths[t]\n            tgt_smpl = tgt_smpls[t] if tgt_smpls is not None else None\n\n            preds, ori_img = self.transfer(tgt_path, tgt_smpl)\n\n            if visualizer is not None:\n                visualizer.vis_named_img('pred_2', preds)\n\n            if output_dir:\n                preds = preds[0].permute(1, 2, 0)\n                preds = preds.cpu().numpy()\n                filename = os.path.split(tgt_path)[-1]\n\n                cv_utils.save_cv2_img(preds, os.path.join(output_dir, 'pred_' + filename), normalize=True)\n                cv_utils.save_cv2_img(ori_img, os.path.join(output_dir, 'gt_' + filename),\n                                      image_size=self._opt.image_size)\n\n            print('{} / {}'.format(t, length))\n\n\nclass FeatureWarpingBaseline(BaseModel):\n    def __init__(self, opt):\n        super(FeatureWarpingBaseline, self).__init__(opt)\n        self._name = 'FeatureWarpingBaseline'\n\n        # create networks\n        self._init_create_networks()\n\n        # init train variables and losses\n        if self._is_train:\n            self._init_train_vars()\n            self._init_losses()\n\n        # load networks and optimizers\n        if not self._is_train or self._opt.load_epoch > 0:\n            self.load()\n\n        # prefetch variables\n        self._init_prefetch_inputs()\n\n    def _init_create_networks(self):\n        # generator network\n        self._G = self._create_generator()\n        self._G.init_weights()\n        if len(self._gpu_ids) > 1:\n            self._G = torch.nn.DataParallel(self._G)\n        self._G.cuda()\n\n        # discriminator network\n        self._D = self._create_discriminator()\n        self._D.init_weights()\n        if len(self._gpu_ids) > 1:\n            self._D = torch.nn.DataParallel(self._D)\n        self._D.cuda()\n\n        self._criterion_hmr = HMRLoss(pretrain_model=self._opt.hmr_model, smpl_pkl_path=self._opt.smpl_model).cuda()\n\n        self._render = SMPLRenderer(faces=self._criterion_hmr.hmr.smpl.faces,\n                                    map_name=self._opt.map_name,\n                                    uv_map_path=self._opt.uv_mapping,\n                                    tex_size=self._opt.tex_size,\n                                    image_size=self._opt.image_size, fill_back=True,\n                                    anti_aliasing=True, background_color=(0, 0, 0), has_front_map=False)\n\n    def _create_generator(self):\n        return NetworksFactory.get_by_name(self._opt.gen_name, bg_dim=4, src_dim=3+self._G_cond_nc,\n                                           tsf_dim=self._G_cond_nc, repeat_num=6)\n\n    def _create_discriminator(self):\n        return NetworksFactory.get_by_name('discriminator_patch_gan', input_nc=3 + self._D_cond_nc,\n                                           ndf=64, n_layers=4, use_sigmoid=False)\n\n    def _init_train_vars(self):\n        self._current_lr_G = self._opt.lr_G\n        self._current_lr_D = self._opt.lr_D\n\n        # initialize optimizers\n        self._optimizer_G = torch.optim.Adam(self._G.parameters(), lr=self._current_lr_G,\n                                             betas=[self._opt.G_adam_b1, self._opt.G_adam_b2])\n        self._optimizer_D = torch.optim.Adam(self._D.parameters(), lr=self._current_lr_D,\n                                             betas=[self._opt.D_adam_b1, self._opt.D_adam_b2])\n\n    def _init_prefetch_inputs(self):\n        self._input_src_img = None\n        self._input_src_smpl = None\n        self._input_src_cond = None\n\n        self._input_desired_img = None\n        self._input_desired_smpl = None\n        self._input_desired_cond = None\n\n        self._input_real_imgs = None\n\n        self._bg_mask = None\n        self._input_src = None\n        self._input_tsf = None\n        self._input_G_bg = None\n        self._input_G_src = None\n        self._input_G_tsf = None\n        self._input_D = None\n        self._src_info = None\n        self._tgt_info = None\n\n        self._initial_T = None\n        self._T = None\n\n        self._initialize_T()\n\n    def _init_losses(self):\n        # define loss functions\n        self._criterion_l1 = torch.nn.L1Loss().cuda()\n\n        if self._opt.use_vgg:\n            self._criterion_vgg = VGGLoss().cuda()\n\n        if self._opt.use_face:\n            self._criterion_face = SphereFaceLoss().cuda()\n\n        # init losses G\n        self._loss_g_l1 = self._Tensor([0])\n        self._loss_g_vgg = self._Tensor([0])\n        self._loss_g_face = self._Tensor([0])\n        self._loss_g_adv = self._Tensor([0])\n        self._loss_g_smooth = self._Tensor([0])\n        self._loss_g_mask = self._Tensor([0])\n        self._loss_g_mask_smooth = self._Tensor([0])\n\n        # init losses D\n        self._loss_d_real = self._Tensor([0])\n        self._loss_d_fake = self._Tensor([0])\n\n    def _initialize_T(self):\n        # initialize T\n        image_size = self._opt.image_size\n        T = torch.zeros(image_size, image_size, 2, dtype=torch.float32).cuda() - 1.0\n        self._initial_T = T\n\n    def _transformer(self, src_cams, src_verts, src_fim, tgt_fim):\n        bs = src_fim.shape[0]\n\n        T = self._initial_T.repeat(bs, 1, 1, 1)   # (bs, image_size, image_size, 2)\n\n        # 2. calculate occlusion flows, (bs, no, 2)\n        tgt_ids = tgt_fim != -1\n\n        # 3. calculate tgt flows, (bs, nt, 2)\n        points = self._render.batch_orth_proj_idrot(src_cams, src_verts)\n        f2pts = self._render.points_to_faces(points)\n        bc_f2pts = self._render.compute_barycenter(f2pts)  # (bs, nf, 2)\n\n        for i in range(bs):\n            Ti = T[i]\n\n            tgt_i = tgt_ids[i]\n\n            # (nf, 2)\n            tgt_flows = bc_f2pts[i, tgt_fim[i, tgt_i].long()]      # (nt, 2)\n            Ti[tgt_i] = tgt_flows\n\n        return T\n\n    def morph(self, src_bg_mask, ks, mode='erode'):\n        n_ks = ks ** 2\n        kernel = torch.ones(1, 1, ks, ks, dtype=torch.float32).cuda()\n        out = F.conv2d(src_bg_mask, kernel, padding=ks // 2)\n\n        if mode == 'erode':\n            out = (out == n_ks).float()\n        else:\n            out = (out >= 1).float()\n\n        return out\n\n    def set_input_cond(self, is_train=True):\n        # source process\n        # source process\n        src_info = self._criterion_hmr.hmr.get_details(self._input_src_smpl)\n        src_rd, src_info['tex'] = self._render.forward(src_info['cam'], src_info['verts'],\n                                                       self._input_src_img, is_uv_sampler=False,\n                                                       reverse_yz=True, get_fim=False)\n\n        self._input_src_cond, src_info['fim'] = self._render.encode_fim(src_info['cam'], src_info['verts'], transpose=True)\n        src_bg_mask = self.morph(self._input_src_cond[:, -1:, :, :], ks=15, mode='erode')\n        src_crop_mask = self.morph(self._input_src_cond[:, -1:, :, :], ks=3, mode='erode')\n\n        # bg input\n        self._input_G_bg = torch.cat([self._input_src_img * src_bg_mask, src_bg_mask], dim=1)\n\n        # src input\n        self._input_src = self._input_src_img * (1 - src_crop_mask)\n        self._input_G_src = torch.cat([self._input_src, self._input_src_cond], dim=1)\n\n        # transfer\n        tgt_info = self._criterion_hmr.hmr.get_details(self._input_desired_smpl)\n        self._input_tsf, _ = self._render.render(tgt_info['cam'], tgt_info['verts'], src_info['tex'], reverse_yz=True, get_fim=False)\n        self._input_desired_cond, tgt_info['fim'] = self._render.encode_fim(tgt_info['cam'], tgt_info['verts'], transpose=True)\n        self._input_G_tsf = self._input_desired_cond\n\n        if is_train:\n            tsf_crop_mask = self.morph(self._input_desired_cond[:, -1:, :, :], ks=3, mode='erode')\n            self._bg_mask = tsf_crop_mask\n            self._input_D = torch.cat([self._input_desired_img, self._input_src_cond, self._input_desired_cond], dim=1)\n            self._input_real_imgs = self._input_desired_img\n\n        self._src_info = src_info\n        self._tgt_info = tgt_info\n\n        # set transformation matrix\n        self._T = self._transformer(src_info['cam'], src_info['verts'], src_fim=src_info['fim'], tgt_fim=tgt_info['fim'])\n\n    def set_input(self, input):\n\n        with torch.no_grad():\n            images = input['images']\n            smpls = input['smpls']\n            self._input_src_img = images[:, 0, ...].contiguous().cuda()\n            self._input_src_smpl = smpls[:, 0, ...].contiguous().cuda()\n            self._input_desired_img = images[:, 1, ...].contiguous().cuda()\n            self._input_desired_smpl = smpls[:, 1, ...].contiguous().cuda()\n\n            self.set_input_cond()\n\n    def set_test_input(self, input):\n        with torch.no_grad():\n            self._input_src_img = input['src_img']\n            self._input_src_smpl = input['src_smpl']\n            self._input_desired_smpl = input['desired_smpl']\n\n            self.set_input_cond(is_train=False)\n\n    def set_train(self):\n        self._G.train()\n        self._D.train()\n        self._is_train = True\n\n    def set_eval(self):\n        self._G.eval()\n        self._is_train = False\n\n    def forward(self, keep_data_for_visuals=False, return_estimates=False):\n        # generate fake images\n        fake_bg, fake_src_color, fake_src_mask, fake_tsf_color, fake_tsf_mask = \\\n            self._G.forward(self._input_G_bg, self._input_G_src, self._input_G_tsf, T=self._T)\n        fake_src_mask = self._do_if_necessary_saturate_mask(fake_src_mask, saturate=self._opt.do_saturate_mask)\n        fake_tsf_mask = self._do_if_necessary_saturate_mask(fake_tsf_mask, saturate=self._opt.do_saturate_mask)\n\n        fake_src_imgs = fake_src_mask * fake_bg + (1 - fake_src_mask) * fake_src_color\n        fake_tsf_imgs = fake_tsf_mask * fake_bg + (1 - fake_tsf_mask) * fake_tsf_color\n\n        fake_masks = torch.cat([fake_src_mask, fake_tsf_mask], dim=0)\n        fake_imgs = torch.cat([fake_src_imgs, fake_tsf_imgs], dim=0)\n\n        # keep data for visualization\n        if keep_data_for_visuals:\n            self.transfer_imgs(fake_bg, fake_imgs, fake_tsf_color, fake_masks)\n\n        return fake_tsf_imgs, fake_tsf_imgs, fake_tsf_mask\n\n    def optimize_parameters(self, train_generator=True, keep_data_for_visuals=False):\n        if self._is_train:\n            # convert tensor to variables\n            self._B = self._input_src_img.size(0)\n\n            # run\n            fake_tsf_imgs, fake_imgs, fake_masks = self.forward(keep_data_for_visuals=keep_data_for_visuals)\n\n            loss_G = self._optimize_G(fake_tsf_imgs, fake_imgs, fake_masks)\n\n            self._optimizer_G.zero_grad()\n            loss_G.backward()\n            self._optimizer_G.step()\n\n            # train D\n            if train_generator:\n                loss_D = self._optimize_D(fake_tsf_imgs)\n                self._optimizer_D.zero_grad()\n                loss_D.backward()\n                self._optimizer_D.step()\n\n    def _optimize_G(self, fake_tsf_imgs, fake_imgs, fake_masks):\n        # D(G(Ic1, c2)*M, c2) masked\n        fake_input_D = torch.cat([fake_tsf_imgs, self._input_src_cond, self._input_desired_cond], dim=1)\n        d_fake_outs = self._D.forward(fake_input_D)\n        self._loss_g_adv = self._compute_loss_D(d_fake_outs, 0) * self._opt.lambda_D_prob\n\n        # l_cyc(G(Ic1,c2)*M, Ic2)\n        self._loss_g_l1 = self._criterion_l1(fake_imgs, self._input_real_imgs) * self._opt.lambda_rec\n\n        if self._opt.use_vgg:\n            self._loss_g_vgg = self._criterion_vgg(fake_imgs, self._input_real_imgs) * self._opt.lambda_tsf\n\n        if self._opt.use_face:\n            self._loss_g_face = self._criterion_face(fake_tsf_imgs, self._input_desired_img,\n                                                     self._tgt_info['j2d'], self._tgt_info['j2d']) * self._opt.lambda_face\n        # loss mask\n        self._loss_g_mask = torch.mean((fake_masks - self._bg_mask) ** 2) * self._opt.lambda_mask\n        self._loss_g_mask_smooth = self._compute_loss_smooth(fake_masks) * self._opt.lambda_mask_smooth\n\n        # combine losses\n        return self._loss_g_adv + self._loss_g_l1 + self._loss_g_vgg + self._loss_g_face + \\\n               self._loss_g_mask + self._loss_g_mask_smooth\n\n    def _optimize_D(self, fake_tsf_imgs):\n        fake_input_D = torch.cat([fake_tsf_imgs.detach(), self._input_src_cond, self._input_desired_cond], dim=1)\n\n        d_real_outs = self._D.forward(self._input_D)\n        d_fake_outs = self._D.forward(fake_input_D)\n\n        self._loss_d_real = self._compute_loss_D(d_real_outs, 1) * self._opt.lambda_D_prob\n        self._loss_d_fake = self._compute_loss_D(d_fake_outs, -1) * self._opt.lambda_D_prob\n\n        # combine losses\n        return self._loss_d_real + self._loss_d_fake\n\n    def _compute_loss_D(self, x, y):\n        return torch.mean((x - y) ** 2)\n\n    def _compute_loss_smooth(self, mat):\n        return torch.sum(torch.abs(mat[:, :, :, :-1] - mat[:, :, :, 1:])) + \\\n               torch.sum(torch.abs(mat[:, :, :-1, :] - mat[:, :, 1:, :]))\n\n    def get_current_errors(self):\n        loss_dict = OrderedDict([('g_l1', self._loss_g_l1.item()),\n                                 ('g_vgg', self._loss_g_vgg.item()),\n                                 ('g_face', self._loss_g_face.item()),\n                                 ('g_adv', self._loss_g_adv.item()),\n                                 ('g_mask', self._loss_g_mask.item()),\n                                 ('g_mask_smooth', self._loss_g_mask_smooth.item()),\n                                 ('d_real', self._loss_d_real.item()),\n                                 ('d_fake', self._loss_d_fake.item())])\n\n        return loss_dict\n\n    def get_current_scalars(self):\n        return OrderedDict([('lr_G', self._current_lr_G), ('lr_D', self._current_lr_D)])\n\n    def get_current_visuals(self):\n        # visuals return dictionary\n        visuals = OrderedDict()\n\n        # inputs\n        visuals['1_real_img'] = self._vis_real_img\n        # visuals['2_input_tsf'] = self._vis_tsf\n        visuals['3_fake_bg'] = self._vis_fake_bg\n\n        # outputs\n        visuals['4_fake_img'] = self._vis_fake_img\n        visuals['5_fake_color'] = self._vis_fake_color\n        visuals['6_fake_mask'] = self._vis_fake_mask\n\n        # batch outputs\n        visuals['7_batch_real_img'] = self._vis_batch_real_img\n        visuals['8_batch_fake_img'] = self._vis_batch_fake_img\n\n        return visuals\n\n    def transfer_imgs(self, fake_bg, fake_imgs, fake_color, fake_masks):\n        self._vis_real_img = util.tensor2im(self._input_real_imgs)\n\n        ids = fake_imgs.shape[0] // 2\n        # self._vis_tsf = util.tensor2im(self._input_tsf.data)\n        self._vis_fake_bg = util.tensor2im(fake_bg.data)\n        self._vis_fake_color = util.tensor2im(fake_color.data)\n        self._vis_fake_img = util.tensor2im(fake_imgs[ids].data)\n        self._vis_fake_mask = util.tensor2maskim(fake_masks[ids].data)\n\n        self._vis_batch_real_img = util.tensor2im(self._input_real_imgs, idx=-1)\n        self._vis_batch_fake_img = util.tensor2im(fake_imgs.data, idx=-1)\n\n    def save(self, label):\n        # save networks\n        self._save_network(self._G, 'G', label)\n        self._save_network(self._D, 'D', label)\n\n        # save optimizers\n        self._save_optimizer(self._optimizer_G, 'G', label)\n        self._save_optimizer(self._optimizer_D, 'D', label)\n\n    def load(self):\n        load_epoch = self._opt.load_epoch\n\n        # load G\n        self._load_network(self._G, 'G', load_epoch)\n\n        if self._is_train:\n            # load D\n            self._load_network(self._D, 'D', load_epoch)\n\n            # load optimizers\n            self._load_optimizer(self._optimizer_G, 'G', load_epoch)\n            self._load_optimizer(self._optimizer_D, 'D', load_epoch)\n\n    def update_learning_rate(self):\n        # updated learning rate G\n        final_lr = self._opt.final_lr\n\n        lr_decay_G = (self._opt.lr_G - final_lr) / self._opt.nepochs_decay\n        self._current_lr_G -= lr_decay_G\n        for param_group in self._optimizer_G.param_groups:\n            param_group['lr'] = self._current_lr_G\n        print('update G learning rate: %f -> %f' % (self._current_lr_G + lr_decay_G, self._current_lr_G))\n\n        # update learning rate D\n        lr_decay_D = (self._opt.lr_D - final_lr) / self._opt.nepochs_decay\n        self._current_lr_D -= lr_decay_D\n        for param_group in self._optimizer_D.param_groups:\n            param_group['lr'] = self._current_lr_D\n        print('update D learning rate: %f -> %f' % (self._current_lr_D + lr_decay_D, self._current_lr_D))\n\n    def _do_if_necessary_saturate_mask(self, m, saturate=False):\n        return torch.clamp(0.55*torch.tanh(3*(m-0.5))+0.5, 0, 1) if saturate else m\n\n    def swap_smpl(self, src_cam, src_shape, tgt_smpl, preserve_scale=True):\n        cam = tgt_smpl[:, 0:3].contiguous()\n        pose = tgt_smpl[:, 3:75].contiguous()\n\n        if preserve_scale:\n            cam[:, 0] = src_cam[:, 0]\n            # cam[:, 1:] = (src_cam[:, 0] / cam[:, 0]) * cam[:, 1:] + src_cam[:, 1:]\n            # cam[:, 0] = src_cam[:, 0]\n        else:\n            cam = src_cam\n\n        tsf_smpl = torch.cat([cam, pose, src_shape], dim=1)\n\n        return tsf_smpl\n\n    def personalize(self, src_path, src_smpl=None, output_path=''):\n\n        with torch.no_grad():\n            ori_img = cv_utils.read_cv2_img(src_path)\n\n            # resize image and convert the color space from [0, 255] to [-1, 1]\n            img = cv_utils.transform_img(ori_img, self._opt.image_size, transpose=True) * 2 - 1.0\n            img = torch.FloatTensor(img).cuda()[None, ...]\n\n            if src_smpl is None:\n                img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n                img_hmr = torch.FloatTensor(img_hmr).cuda()[None, ...]\n                src_smpl = self._criterion_hmr.hmr(img_hmr)[-1]\n            else:\n                src_smpl = util.to_tensor(src_smpl).cuda()[None, ...]\n\n            # source process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n            src_info = self._criterion_hmr.hmr.get_details(src_smpl)\n\n            # add image to source info\n            src_info['image'] = img\n\n            # add texture into source info\n            _, src_info['tex'] = self._render.forward(src_info['cam'], src_info['verts'],\n                                                      img, is_uv_sampler=False, reverse_yz=True, get_fim=False)\n\n            # add pose condition and face index map into source info\n            src_info['cond'], src_info['fim'] = self._render.encode_fim(src_info['cam'],\n                                                                        src_info['verts'], transpose=True)\n\n            src_bg_mask = self.morph(src_info['cond'][:, -1:, :, :], ks=15, mode='erode')\n\n            # bg input\n            bg_inputs = torch.cat([img * src_bg_mask, src_bg_mask], dim=1)\n            src_info['bg'] = self._G.bg_model(bg_inputs)\n\n            # src input\n            src_crop_mask = self.morph(src_info['cond'][:, -1:, :, :], ks=3, mode='erode')\n            input_src = img * (1 - src_crop_mask)\n            input_G_src = torch.cat([input_src, src_info['cond']], dim=1)\n\n            src_info['feats'] = self._G.src_model.inference(input_G_src)\n\n            self.src_info = src_info\n\n            if output_path:\n                cv_utils.save_cv2_img(ori_img, output_path, image_size=self._opt.image_size)\n\n    def transfer(self, tgt_path, tgt_smpl=None):\n        with torch.no_grad():\n            # get source info\n            src_info = self.src_info\n\n            ori_img = cv_utils.read_cv2_img(tgt_path)\n            if tgt_smpl is None:\n                img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n                img_hmr = torch.FloatTensor(img_hmr).cuda()[None, ...]\n                tgt_smpl = self._criterion_hmr.hmr(img_hmr)[-1]\n            else:\n                tgt_smpl = util.to_tensor(tgt_smpl).cuda()[None, ...]\n\n            tsf_smpl = self.swap_smpl(src_info['cam'], src_info['shape'], tgt_smpl, preserve_scale=True)\n\n            tsf_info = self._criterion_hmr.hmr.get_details(tsf_smpl)\n            tsf_img, _ = self._render.render(tsf_info['cam'], tsf_info['verts'], src_info['tex'],\n                                             reverse_yz=True, get_fim=False)\n            tsf_info['cond'], tsf_info['fim'] = self._render.encode_fim(tsf_info['cam'], tsf_info['verts'],\n                                                                        transpose=True)\n\n            # transfer\n            tsf_img, _ = self._render.render(tsf_info['cam'], tsf_info['verts'], src_info['tex'],\n                                             reverse_yz=True, get_fim=False)\n            tsf_inputs, tsf_info['fim'] = self._render.encode_fim(tsf_info['cam'], tsf_info['verts'],\n                                                                  transpose=True)\n            T = self._transformer(src_info['cam'], src_info['verts'], src_info['fim'], tsf_info['fim'])\n            src_encoder_outs, src_resnet_outs = src_info['feats']\n            tsf_color, tsf_mask = self._G.inference(src_encoder_outs, src_resnet_outs, tsf_inputs, T)\n            pred_imgs = tsf_mask * src_info['bg'] + (1 - tsf_mask) * tsf_color\n\n            return pred_imgs, ori_img\n\n    def imitate(self, tgt_paths, tgt_smpls=None, output_dir='', visualizer=None, cam_strategy=None):\n        length = len(tgt_paths)\n\n        for t in range(length):\n\n            tgt_path = tgt_paths[t]\n            tgt_smpl = tgt_smpls[t] if tgt_smpls is not None else None\n\n            preds, ori_img = self.transfer(tgt_path, tgt_smpl)\n\n            if visualizer is not None:\n                visualizer.vis_named_img('pred_2', preds)\n\n            if output_dir:\n                preds = preds[0].permute(1, 2, 0)\n                preds = preds.cpu().numpy()\n                filename = os.path.split(tgt_path)[-1]\n\n                cv_utils.save_cv2_img(preds, os.path.join(output_dir, 'pred_' + filename), normalize=True)\n                cv_utils.save_cv2_img(ori_img, os.path.join(output_dir, 'gt_' + filename),\n                                      image_size=self._opt.image_size)\n\n            print('{} / {}'.format(t, length))\n"""
models/imitator.py,32,"b""import os\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm\nfrom .models import BaseModel\nfrom networks.networks import NetworksFactory, HumanModelRecovery\nfrom utils.nmr import SMPLRenderer\nfrom utils.detectors import PersonMaskRCNNDetector\nimport utils.cv_utils as cv_utils\nimport utils.util as util\n\n\nclass Imitator(BaseModel):\n    def __init__(self, opt):\n        super(Imitator, self).__init__(opt)\n        self._name = 'Imitator'\n\n        self._create_networks()\n\n        # prefetch variables\n        self.src_info = None\n        self.tsf_info = None\n        self.first_cam = None\n\n    def _create_networks(self):\n        # 0. create generator\n        self.generator = self._create_generator().cuda()\n\n        # 0. create bgnet\n        if self._opt.bg_model != 'ORIGINAL':\n            self.bgnet = self._create_bgnet().cuda()\n        else:\n            self.bgnet = self.generator.bg_model\n\n        # 2. create hmr\n        self.hmr = self._create_hmr().cuda()\n\n        # 3. create render\n        self.render = SMPLRenderer(image_size=self._opt.image_size, tex_size=self._opt.tex_size,\n                                   has_front=self._opt.front_warp, fill_back=False).cuda()\n        # 4. pre-processor\n        if self._opt.has_detector:\n            self.detector = PersonMaskRCNNDetector(ks=self._opt.bg_ks, threshold=0.5, to_gpu=True)\n        else:\n            self.detector = None\n\n    def _create_bgnet(self):\n        net = NetworksFactory.get_by_name('deepfillv2', c_dim=4)\n        self._load_params(net, self._opt.bg_model, need_module=False)\n        net.eval()\n        return net\n\n    def _create_generator(self):\n        net = NetworksFactory.get_by_name(self._opt.gen_name, bg_dim=4, src_dim=3+self._G_cond_nc,\n                                          tsf_dim=3+self._G_cond_nc, repeat_num=self._opt.repeat_num)\n\n        if self._opt.load_path:\n            self._load_params(net, self._opt.load_path)\n        elif self._opt.load_epoch > 0:\n            self._load_network(net, 'G', self._opt.load_epoch)\n        else:\n            raise ValueError('load_path {} is empty and load_epoch {} is 0'.format(\n                self._opt.load_path, self._opt.load_epoch))\n\n        net.eval()\n        return net\n\n    def _create_hmr(self):\n        hmr = HumanModelRecovery(self._opt.smpl_model)\n        saved_data = torch.load(self._opt.hmr_model)\n        hmr.load_state_dict(saved_data)\n        hmr.eval()\n        return hmr\n\n    def visualize(self, *args, **kwargs):\n        visualizer = args[0]\n        if visualizer is not None:\n            for key, value in kwargs.items():\n                visualizer.vis_named_img(key, value)\n\n    @torch.no_grad()\n    def personalize(self, src_path, src_smpl=None, output_path='', visualizer=None):\n\n        ori_img = cv_utils.read_cv2_img(src_path)\n\n        # resize image and convert the color space from [0, 255] to [-1, 1]\n        img = cv_utils.transform_img(ori_img, self._opt.image_size, transpose=True) * 2 - 1.0\n        img = torch.tensor(img, dtype=torch.float32).cuda()[None, ...]\n\n        if src_smpl is None:\n            img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n            img_hmr = torch.tensor(img_hmr, dtype=torch.float32).cuda()[None, ...]\n            src_smpl = self.hmr(img_hmr)\n        else:\n            src_smpl = torch.tensor(src_smpl, dtype=torch.float32).cuda()[None, ...]\n\n        # source process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n        src_info = self.hmr.get_details(src_smpl)\n        src_f2verts, src_fim, src_wim = self.render.render_fim_wim(src_info['cam'], src_info['verts'])\n        # src_f2pts = src_f2verts[:, :, :, 0:2]\n        src_info['fim'] = src_fim\n        src_info['wim'] = src_wim\n        src_info['cond'], _ = self.render.encode_fim(src_info['cam'], src_info['verts'], fim=src_fim, transpose=True)\n        src_info['f2verts'] = src_f2verts\n        src_info['p2verts'] = src_f2verts[:, :, :, 0:2]\n        src_info['p2verts'][:, :, :, 1] *= -1\n\n        if self._opt.only_vis:\n            src_info['p2verts'] = self.render.get_vis_f2pts(src_info['p2verts'], src_fim)\n        # add image to source info\n        src_info['img'] = img\n        src_info['image'] = ori_img\n\n        # 2. process the src inputs\n        if self.detector is not None:\n            bbox, body_mask = self.detector.inference(img[0])\n            bg_mask = 1 - body_mask\n        else:\n            # bg is 1, ft is 0\n            bg_mask = util.morph(src_info['cond'][:, -1:, :, :], ks=self._opt.bg_ks, mode='erode')\n            body_mask = 1 - bg_mask\n\n        if self._opt.bg_model != 'ORIGINAL':\n            src_info['bg'] = self.bgnet(img, masks=body_mask, only_x=True)\n        else:\n            incomp_img = img * bg_mask\n            bg_inputs = torch.cat([incomp_img, bg_mask], dim=1)\n            img_bg = self.bgnet(bg_inputs)\n            # src_info['bg'] = bg_inputs[:, 0:3] + img_bg * bg_inputs[:, -1:]\n            src_info['bg'] = img_bg\n\n        ft_mask = 1 - util.morph(src_info['cond'][:, -1:, :, :], ks=self._opt.ft_ks, mode='erode')\n        src_inputs = torch.cat([img * ft_mask, src_info['cond']], dim=1)\n\n        src_info['feats'] = self.generator.encode_src(src_inputs)\n\n        self.src_info = src_info\n\n        if visualizer is not None:\n            visualizer.vis_named_img('src', img)\n            visualizer.vis_named_img('bg', src_info['bg'])\n\n        if output_path:\n            cv_utils.save_cv2_img(src_info['image'], output_path, image_size=self._opt.image_size)\n\n    @torch.no_grad()\n    def _extract_smpls(self, input_file):\n        img = cv_utils.read_cv2_img(input_file)\n        img = cv_utils.transform_img(img, image_size=224) * 2 - 1.0  # hmr receive [-1, 1]\n        img = img.transpose((2, 0, 1))\n        img = torch.tensor(img, dtype=torch.float32).cuda()[None, ...]\n        theta = self.hmr(img)[-1]\n\n        return theta\n\n    @torch.no_grad()\n    def inference(self, tgt_paths, tgt_smpls=None, cam_strategy='smooth',\n                  output_dir='', visualizer=None, verbose=True):\n\n        length = len(tgt_paths)\n\n        outputs = []\n        process_bar = tqdm(range(length)) if verbose else range(length)\n\n        for t in process_bar:\n            tgt_path = tgt_paths[t]\n            tgt_smpl = tgt_smpls[t] if tgt_smpls is not None else None\n\n            tsf_inputs = self.transfer_params(tgt_path, tgt_smpl, cam_strategy, t=t)\n            preds = self.forward(tsf_inputs, self.tsf_info['T'])\n\n            if visualizer is not None:\n                gt = cv_utils.transform_img(self.tsf_info['image'], image_size=self._opt.image_size, transpose=True)\n                visualizer.vis_named_img('pred_' + cam_strategy, preds)\n                visualizer.vis_named_img('gt', gt[None, ...], denormalize=False)\n\n            preds = preds[0].permute(1, 2, 0)\n            preds = preds.cpu().numpy()\n            outputs.append(preds)\n\n            if output_dir:\n                filename = os.path.split(tgt_path)[-1]\n\n                cv_utils.save_cv2_img(preds, os.path.join(output_dir, 'pred_' + filename), normalize=True)\n                cv_utils.save_cv2_img(self.tsf_info['image'], os.path.join(output_dir, 'gt_' + filename),\n                                      image_size=self._opt.image_size)\n\n        return outputs\n    \n    @torch.no_grad()\n    def inference_by_smpls(self, tgt_smpls, cam_strategy='smooth', output_dir='', visualizer=None):\n        length = len(tgt_smpls)\n\n        outputs = []\n        for t in tqdm(range(length)):\n            tgt_smpl = tgt_smpls[t] if tgt_smpls is not None else None\n\n            tsf_inputs = self.transfer_params_by_smpl(tgt_smpl, cam_strategy, t=t)\n            preds = self.forward(tsf_inputs, self.tsf_info['T'])\n\n            if visualizer is not None:\n                gt = cv_utils.transform_img(self.tsf_info['image'], image_size=self._opt.image_size, transpose=True)\n                visualizer.vis_named_img('pred_' + cam_strategy, preds)\n                visualizer.vis_named_img('gt', gt[None, ...], denormalize=False)\n\n            preds = preds[0].permute(1, 2, 0)\n            preds = preds.cpu().numpy()\n            outputs.append(preds)\n\n            if output_dir:\n                cv_utils.save_cv2_img(preds, os.path.join(output_dir, 'pred_%.8d.jpg' % t), normalize=True)\n\n        return outputs\n\n    def swap_smpl(self, src_cam, src_shape, tgt_smpl, cam_strategy='smooth'):\n        tgt_cam = tgt_smpl[:, 0:3].contiguous()\n        pose = tgt_smpl[:, 3:75].contiguous()\n\n        # TODO, need more tricky ways\n        if cam_strategy == 'smooth':\n\n            cam = src_cam.clone()\n            delta_xy = tgt_cam[:, 1:] - self.first_cam[:, 1:]\n            cam[:, 1:] += delta_xy\n\n        elif cam_strategy == 'source':\n            cam = src_cam\n        else:\n            cam = tgt_cam\n\n        tsf_smpl = torch.cat([cam, pose, src_shape], dim=1)\n\n        return tsf_smpl\n\n    def transfer_params_by_smpl(self, tgt_smpl, cam_strategy='smooth', t=0):\n        # get source info\n        src_info = self.src_info\n\n        if isinstance(tgt_smpl, np.ndarray):\n            tgt_smpl = torch.tensor(tgt_smpl).float().cuda()[None, ...]\n\n        if t == 0 and cam_strategy == 'smooth':\n            self.first_cam = tgt_smpl[:, 0:3].clone()\n\n        # get transfer smpl\n        tsf_smpl = self.swap_smpl(src_info['cam'], src_info['shape'], tgt_smpl, cam_strategy=cam_strategy)\n        # transfer process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n        tsf_info = self.hmr.get_details(tsf_smpl)\n\n        tsf_f2verts, tsf_fim, tsf_wim = self.render.render_fim_wim(tsf_info['cam'], tsf_info['verts'])\n        # src_f2pts = src_f2verts[:, :, :, 0:2]\n        tsf_info['fim'] = tsf_fim\n        tsf_info['wim'] = tsf_wim\n        tsf_info['cond'], _ = self.render.encode_fim(tsf_info['cam'], tsf_info['verts'], fim=tsf_fim, transpose=True)\n        # tsf_info['sil'] = util.morph((tsf_fim != -1).float(), ks=self._opt.ft_ks, mode='dilate')\n\n        T = self.render.cal_bc_transform(src_info['p2verts'], tsf_fim, tsf_wim)\n        tsf_img = F.grid_sample(src_info['img'], T)\n        tsf_inputs = torch.cat([tsf_img, tsf_info['cond']], dim=1)\n\n        # add target image to tsf info\n        tsf_info['tsf_img'] = tsf_img\n        tsf_info['T'] = T\n\n        self.tsf_info = tsf_info\n\n        return tsf_inputs\n\n    def transfer_params(self, tgt_path, tgt_smpl=None, cam_strategy='smooth', t=0):\n        ori_img = cv_utils.read_cv2_img(tgt_path)\n        if tgt_smpl is None:\n            img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n            img_hmr = torch.tensor(img_hmr, dtype=torch.float32).cuda()[None, ...]\n            tgt_smpl = self.hmr(img_hmr)\n        else:\n            if isinstance(tgt_smpl, np.ndarray):\n                tgt_smpl = torch.tensor(tgt_smpl, dtype=torch.float32).cuda()[None, ...]\n\n        tsf_inputs = self.transfer_params_by_smpl(tgt_smpl=tgt_smpl, cam_strategy=cam_strategy, t=t)\n        self.tsf_info['image'] = ori_img\n\n        return tsf_inputs\n\n    # @torch.no_grad()\n    # def transfer_params(self, tgt_path, tgt_smpl=None, cam_strategy='smooth', t=0):\n    #     # get source info\n    #     src_info = self.src_info\n    #\n    #     ori_img = cv_utils.read_cv2_img(tgt_path)\n    #     if tgt_smpl is None:\n    #         img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n    #         img_hmr = torch.tensor(img_hmr, dtype=torch.float32).cuda()[None, ...]\n    #         tgt_smpl = self.hmr(img_hmr)\n    #     else:\n    #         tgt_smpl = torch.tensor(tgt_smpl, dtype=torch.float32).cuda()[None, ...]\n    #\n    #     if t == 0 and cam_strategy == 'smooth':\n    #         self.first_cam = tgt_smpl[:, 0:3].clone()\n    #\n    #     # get transfer smpl\n    #     tsf_smpl = self.swap_smpl(src_info['cam'], src_info['shape'], tgt_smpl, cam_strategy=cam_strategy)\n    #     # transfer process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n    #     tsf_info = self.hmr.get_details(tsf_smpl)\n    #\n    #     tsf_f2verts, tsf_fim, tsf_wim = self.render.render_fim_wim(tsf_info['cam'], tsf_info['verts'])\n    #     # src_f2pts = src_f2verts[:, :, :, 0:2]\n    #     tsf_info['fim'] = tsf_fim\n    #     tsf_info['wim'] = tsf_wim\n    #     tsf_info['cond'], _ = self.render.encode_fim(tsf_info['cam'], tsf_info['verts'], fim=tsf_fim, transpose=True)\n    #     # tsf_info['sil'] = util.morph((tsf_fim != -1).float(), ks=self._opt.ft_ks, mode='dilate')\n    #\n    #     T = self.render.cal_bc_transform(src_info['p2verts'], tsf_fim, tsf_wim)\n    #     tsf_img = F.grid_sample(src_info['img'], T)\n    #     tsf_inputs = torch.cat([tsf_img, tsf_info['cond']], dim=1)\n    #\n    #     # add target image to tsf info\n    #     tsf_info['tsf_img'] = tsf_img\n    #     tsf_info['image'] = ori_img\n    #     tsf_info['T'] = T\n    #\n    #     self.tsf_info = tsf_info\n    #\n    #     return tsf_inputs\n\n    def forward(self, tsf_inputs, T):\n        bg_img = self.src_info['bg']\n        src_encoder_outs, src_resnet_outs = self.src_info['feats']\n\n        tsf_color, tsf_mask = self.generator.inference(src_encoder_outs, src_resnet_outs, tsf_inputs, T)\n        pred_imgs = tsf_mask * bg_img + (1 - tsf_mask) * tsf_color\n\n        if self._opt.front_warp:\n            pred_imgs = self.warp_front(pred_imgs, tsf_mask)\n\n        return pred_imgs\n\n    def warp_front(self, preds, mask):\n        front_mask = self.render.encode_front_fim(self.tsf_info['fim'], transpose=True, front_fn=True)\n        preds = (1 - front_mask) * preds + self.tsf_info['tsf_img'] * front_mask * (1 - mask)\n        # preds = torch.clamp(preds + self.tsf_info['tsf_img'] * front_mask, -1, 1)\n        return preds\n\n    def post_personalize(self, out_dir, data_loader, visualizer, verbose=True):\n        from networks.networks import FaceLoss\n\n        bg_inpaint = self.src_info['bg']\n\n        @torch.no_grad()\n        def set_gen_inputs(sample):\n            j2ds = sample['j2d'].cuda()  # (N, 4)\n            T = sample['T'].cuda()  # (N, h, w, 2)\n            T_cycle = sample['T_cycle'].cuda()  # (N, h, w, 2)\n            src_inputs = sample['src_inputs'].cuda()  # (N, 6, h, w)\n            tsf_inputs = sample['tsf_inputs'].cuda()  # (N, 6, h, w)\n            src_fim = sample['src_fim'].cuda()\n            tsf_fim = sample['tsf_fim'].cuda()\n            init_preds = sample['preds'].cuda()\n            images = sample['images']\n            images = torch.cat([images[:, 0, ...], images[:, 1, ...]], dim=0).cuda()  # (2N, 3, h, w)\n            pseudo_masks = sample['pseudo_masks']\n            pseudo_masks = torch.cat([pseudo_masks[:, 0, ...], pseudo_masks[:, 1, ...]],\n                                     dim=0).cuda()  # (2N, 1, h, w)\n\n            return src_fim, tsf_fim, j2ds, T, T_cycle, \\\n                   src_inputs, tsf_inputs, images, init_preds, pseudo_masks\n\n        def set_cycle_inputs(fake_tsf_imgs, src_inputs, tsf_inputs, T_cycle):\n            # set cycle src inputs\n            cycle_src_inputs = torch.cat([fake_tsf_imgs * tsf_inputs[:, -1:, ...], tsf_inputs[:, 3:]], dim=1)\n\n            # set cycle tsf inputs\n            cycle_tsf_img = F.grid_sample(fake_tsf_imgs, T_cycle)\n            cycle_tsf_inputs = torch.cat([cycle_tsf_img, src_inputs[:, 3:]], dim=1)\n\n            return cycle_src_inputs, cycle_tsf_inputs\n\n        def warp(preds, tsf, fim, fake_tsf_mask):\n            front_mask = self.render.encode_front_fim(fim, transpose=True)\n            preds = (1 - front_mask) * preds + tsf * front_mask * (1 - fake_tsf_mask)\n            # preds = torch.clamp(preds + tsf * front_mask, -1, 1)\n            return preds\n\n        def inference(src_inputs, tsf_inputs, T, T_cycle, src_fim, tsf_fim):\n            fake_src_color, fake_src_mask, fake_tsf_color, fake_tsf_mask = \\\n                self.generator.infer_front(src_inputs, tsf_inputs, T=T)\n\n            fake_src_imgs = fake_src_mask * bg_inpaint + (1 - fake_src_mask) * fake_src_color\n            fake_tsf_imgs = fake_tsf_mask * bg_inpaint + (1 - fake_tsf_mask) * fake_tsf_color\n\n            if self._opt.front_warp:\n                fake_tsf_imgs = warp(fake_tsf_imgs, tsf_inputs[:, 0:3], tsf_fim, fake_tsf_mask)\n\n            cycle_src_inputs, cycle_tsf_inputs = set_cycle_inputs(\n                fake_tsf_imgs, src_inputs, tsf_inputs, T_cycle)\n\n            cycle_src_color, cycle_src_mask, cycle_tsf_color, cycle_tsf_mask = \\\n                self.generator.infer_front(cycle_src_inputs, cycle_tsf_inputs, T=T_cycle)\n\n            cycle_src_imgs = cycle_src_mask * bg_inpaint + (1 - cycle_src_mask) * cycle_src_color\n            cycle_tsf_imgs = cycle_tsf_mask * bg_inpaint + (1 - cycle_tsf_mask) * cycle_tsf_color\n\n            if self._opt.front_warp:\n                cycle_tsf_imgs = warp(cycle_tsf_imgs, src_inputs[:, 0:3], src_fim, fake_src_mask)\n\n            return fake_src_imgs, fake_tsf_imgs, cycle_src_imgs, cycle_tsf_imgs, fake_src_mask, fake_tsf_mask\n\n        def create_criterion():\n            face_criterion = FaceLoss(pretrained_path=self._opt.face_model).cuda()\n            idt_criterion = torch.nn.L1Loss()\n            mask_criterion = torch.nn.BCELoss()\n\n            return face_criterion, idt_criterion, mask_criterion\n\n        init_lr = 0.0002\n        nodecay_epochs = 5\n        optimizer = torch.optim.Adam(self.generator.parameters(), lr=init_lr, betas=(0.5, 0.999))\n        face_cri, idt_cri, msk_cri = create_criterion()\n\n        step = 0\n        logger = tqdm(range(nodecay_epochs))\n        for epoch in logger:\n            for i, sample in enumerate(data_loader):\n                src_fim, tsf_fim, j2ds, T, T_cycle, src_inputs, tsf_inputs, \\\n                images, init_preds, pseudo_masks = set_gen_inputs(sample)\n\n                # print(bg_inputs.shape, src_inputs.shape, tsf_inputs.shape)\n                bs = tsf_inputs.shape[0]\n                src_imgs = images[0:bs]\n                fake_src_imgs, fake_tsf_imgs, cycle_src_imgs, cycle_tsf_imgs, fake_src_mask, fake_tsf_mask = inference(\n                    src_inputs, tsf_inputs, T, T_cycle, src_fim, tsf_fim)\n\n                # cycle reconstruction loss\n                cycle_loss = idt_cri(src_imgs, fake_src_imgs) + idt_cri(src_imgs, cycle_tsf_imgs)\n\n                # structure loss\n                bg_mask = src_inputs[:, -1:]\n                body_mask = 1 - bg_mask\n                str_src_imgs = src_imgs * body_mask\n                cycle_warp_imgs = F.grid_sample(fake_tsf_imgs, T_cycle)\n                back_head_mask = 1 - self.render.encode_front_fim(tsf_fim, transpose=True, front_fn=False)\n                struct_loss = idt_cri(init_preds, fake_tsf_imgs) + \\\n                              2 * idt_cri(str_src_imgs * back_head_mask, cycle_warp_imgs * back_head_mask)\n\n                fid_loss = face_cri(src_imgs, cycle_tsf_imgs, kps1=j2ds[:, 0], kps2=j2ds[:, 0]) + \\\n                           face_cri(init_preds, fake_tsf_imgs, kps1=j2ds[:, 1], kps2=j2ds[:, 1])\n\n                # mask loss\n                # mask_loss = msk_cri(fake_tsf_mask, tsf_inputs[:, -1:]) + msk_cri(fake_src_mask, src_inputs[:, -1:])\n                mask_loss = msk_cri(torch.cat([fake_src_mask, fake_tsf_mask], dim=0), pseudo_masks)\n\n                loss = 10 * cycle_loss + 10 * struct_loss + fid_loss + 5 * mask_loss\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                if verbose:\n                    logger.set_description(\n                        (\n                            f'epoch: {epoch + 1}; step: {step}; '\n                            f'total: {loss.item():.6f}; cyc: {cycle_loss.item():.6f}; '\n                            f'str: {struct_loss.item():.6f}; fid: {fid_loss.item():.6f}; '\n                            f'msk: {mask_loss.item():.6f}'\n                        )\n                    )\n\n                if verbose and step % 5 == 0:\n                    self.visualize(visualizer, input_imgs=images, tsf_imgs=fake_tsf_imgs, cyc_imgs=cycle_tsf_imgs)\n\n                step += 1\n\n        self.generator.eval()\n"""
models/impersonator_trainer.py,58,"b'import os\nimport torch\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport utils.util as util\nfrom .models import BaseModel\nfrom networks.networks import NetworksFactory, HumanModelRecovery, Vgg19, VGGLoss, FaceLoss, StyleLoss\nfrom utils.nmr import SMPLRenderer\nimport ipdb\n\n\nclass BodyRecoveryFlow(torch.nn.Module):\n\n    def __init__(self, opt):\n        super(BodyRecoveryFlow, self).__init__()\n        self._name = \'BodyRecoveryFlow\'\n        self._opt = opt\n\n        # create networks\n        self._init_create_networks()\n\n    def _create_hmr(self):\n        hmr = HumanModelRecovery(smpl_pkl_path=self._opt.smpl_model)\n        saved_data = torch.load(self._opt.hmr_model)\n        hmr.load_state_dict(saved_data)\n        hmr.eval()\n        return hmr\n\n    def _create_render(self):\n        render = SMPLRenderer(map_name=self._opt.map_name,\n                              uv_map_path=self._opt.uv_mapping,\n                              tex_size=self._opt.tex_size,\n                              image_size=self._opt.image_size, fill_back=False,\n                              anti_aliasing=True, background_color=(0, 0, 0), has_front=False)\n\n        return render\n\n    def _init_create_networks(self):\n        # hmr and render\n        self._hmr = self._create_hmr()\n        self._render = self._create_render()\n\n    def forward(self, src_img, ref_img, src_smpl, ref_smpl):\n        # get smpl information\n        src_info = self._hmr.get_details(src_smpl)\n        ref_info = self._hmr.get_details(ref_smpl)\n\n        # process source inputs\n        src_f2verts, src_fim, _ = self._render.render_fim_wim(src_info[\'cam\'], src_info[\'verts\'])\n        src_f2verts = src_f2verts[:, :, :, 0:2]\n        src_f2verts[:, :, :, 1] *= -1\n        src_cond, _ = self._render.encode_fim(src_info[\'cam\'], src_info[\'verts\'], fim=src_fim, transpose=True)\n        src_crop_mask = util.morph(src_cond[:, -1:, :, :], ks=3, mode=\'erode\')\n\n        _, ref_fim, ref_wim = self._render.render_fim_wim(ref_info[\'cam\'], ref_info[\'verts\'])\n        ref_cond, _ = self._render.encode_fim(ref_info[\'cam\'], ref_info[\'verts\'], fim=ref_fim, transpose=True)\n        T = self._render.cal_bc_transform(src_f2verts, ref_fim, ref_wim)\n        syn_img = F.grid_sample(src_img, T)\n\n        # src input\n        input_G_src = torch.cat([src_img * (1 - src_crop_mask), src_cond], dim=1)\n\n        # tsf input\n        input_G_tsf = torch.cat([syn_img, ref_cond], dim=1)\n\n        # bg input\n        src_bg_mask = util.morph(src_cond[:, -1:, :, :], ks=15, mode=\'erode\')\n        input_G_src_bg = torch.cat([src_img * src_bg_mask, src_bg_mask], dim=1)\n\n        if self._opt.bg_both:\n            ref_bg_mask = util.morph(ref_cond[:, -1:, :, :], ks=15, mode=\'erode\')\n            input_G_tsf_bg = torch.cat([ref_img * ref_bg_mask, ref_bg_mask], dim=1)\n        else:\n            input_G_tsf_bg = None\n\n        # masks\n        tsf_crop_mask = util.morph(ref_cond[:, -1:, :, :], ks=3, mode=\'erode\')\n\n        head_bbox = self.cal_head_bbox(ref_info[\'j2d\'])\n        body_bbox = self.cal_body_bbox(ref_info[\'j2d\'])\n\n        return input_G_src_bg, input_G_tsf_bg, input_G_src, input_G_tsf, \\\n               T, src_crop_mask, tsf_crop_mask, head_bbox, body_bbox\n\n    def cal_head_bbox(self, kps):\n        """"""\n        Args:\n            kps: (N, 19, 2)\n\n        Returns:\n            bbox: (N, 4)\n        """"""\n        NECK_IDS = 12\n\n        image_size = self._opt.image_size\n\n        kps = (kps + 1) / 2.0\n\n        necks = kps[:, NECK_IDS, 0]\n        zeros = torch.zeros_like(necks)\n        ones = torch.ones_like(necks)\n\n        # min_x = int(max(0.0, np.min(kps[HEAD_IDS:, 0]) - 0.1) * image_size)\n        min_x, _ = torch.min(kps[:, NECK_IDS:, 0] - 0.05, dim=1)\n        min_x = torch.max(min_x, zeros)\n\n        max_x, _ = torch.max(kps[:, NECK_IDS:, 0] + 0.05, dim=1)\n        max_x = torch.min(max_x, ones)\n\n        # min_x = int(max(0.0, np.min(kps[HEAD_IDS:, 0]) - 0.1) * image_size)\n        min_y, _ = torch.min(kps[:, NECK_IDS:, 1] - 0.05, dim=1)\n        min_y = torch.max(min_y, zeros)\n\n        max_y, _ = torch.max(kps[:, NECK_IDS:, 1], dim=1)\n        max_y = torch.min(max_y, ones)\n\n        min_x = (min_x * image_size).long()  # (T, 1)\n        max_x = (max_x * image_size).long()  # (T, 1)\n        min_y = (min_y * image_size).long()  # (T, 1)\n        max_y = (max_y * image_size).long()  # (T, 1)\n\n        # print(min_x.shape, max_x.shape, min_y.shape, max_y.shape)\n        rects = torch.stack((min_x, max_x, min_y, max_y), dim=1)\n        # import ipdb\n        # ipdb.set_trace()\n        return rects\n\n    def cal_body_bbox(self, kps, factor=1.2):\n        """"""\n        Args:\n            kps (torch.cuda.FloatTensor): (N, 19, 2)\n            factor (float):\n\n        Returns:\n            bbox: (N, 4)\n        """"""\n        image_size = self._opt.image_size\n        bs = kps.shape[0]\n        kps = (kps + 1) / 2.0\n        zeros = torch.zeros((bs,), device=kps.device)\n        ones = torch.ones((bs,), device=kps.device)\n\n        min_x, _ = torch.min(kps[:, :, 0], dim=1)\n        max_x, _ = torch.max(kps[:, :, 0], dim=1)\n        middle_x = (min_x + max_x) / 2\n        width = (max_x - min_x) * factor\n        min_x = torch.max(zeros, middle_x - width / 2)\n        max_x = torch.min(ones, middle_x + width / 2)\n\n        min_y, _ = torch.min(kps[:, :, 1], dim=1)\n        max_y, _ = torch.max(kps[:, :, 1], dim=1)\n        middle_y = (min_y + max_y) / 2\n        height = (max_y - min_y) * factor\n        min_y = torch.max(zeros, middle_y - height / 2)\n        max_y = torch.min(ones, middle_y + height / 2)\n\n        min_x = (min_x * image_size).long()  # (T,)\n        max_x = (max_x * image_size).long()  # (T,)\n        min_y = (min_y * image_size).long()  # (T,)\n        max_y = (max_y * image_size).long()  # (T,)\n\n        # print(min_x.shape, max_x.shape, min_y.shape, max_y.shape)\n        bboxs = torch.stack((min_x, max_x, min_y, max_y), dim=1)\n\n        return bboxs\n\n\nclass Impersonator(BaseModel):\n    def __init__(self, opt):\n        super(Impersonator, self).__init__(opt)\n        self._name = \'Impersonator\'\n\n        # create networks\n        self._init_create_networks()\n\n        # init train variables and losses\n        if self._is_train:\n            self._init_train_vars()\n            self._init_losses()\n\n        # load networks and optimizers\n        if not self._is_train or self._opt.load_epoch > 0:\n            self.load()\n        elif self._opt.load_path != \'None\':\n            # ipdb.set_trace()\n            self._load_params(self._G, self._opt.load_path, need_module=len(self._gpu_ids) > 1)\n\n        # prefetch variables\n        self._init_prefetch_inputs()\n\n    def _init_create_networks(self):\n        multi_gpus = len(self._gpu_ids) > 1\n\n        # body recovery Flow\n        self._bdr = BodyRecoveryFlow(opt=self._opt)\n        if multi_gpus:\n            self._bdr = torch.nn.DataParallel(self._bdr)\n\n        self._bdr.eval()\n        self._bdr.cuda()\n\n        # generator network\n        self._G = self._create_generator()\n        self._G.init_weights()\n        if multi_gpus:\n            self._G = torch.nn.DataParallel(self._G)\n        self._G.cuda()\n\n        # discriminator network\n        self._D = self._create_discriminator()\n        self._D.init_weights()\n        if multi_gpus:\n            self._D = torch.nn.DataParallel(self._D)\n        self._D.cuda()\n\n    def _create_generator(self):\n        return NetworksFactory.get_by_name(self._opt.gen_name, bg_dim=4, src_dim=3+self._G_cond_nc,\n                                           tsf_dim=3+self._G_cond_nc, repeat_num=self._opt.repeat_num)\n\n    def _create_discriminator(self):\n        return NetworksFactory.get_by_name(\'discriminator_patch_gan\', input_nc=3 + self._D_cond_nc,\n                                           norm_type=self._opt.norm_type, ndf=64, n_layers=4, use_sigmoid=False)\n\n    def _init_train_vars(self):\n        self._current_lr_G = self._opt.lr_G\n        self._current_lr_D = self._opt.lr_D\n\n        # initialize optimizers\n        self._optimizer_G = torch.optim.Adam(self._G.parameters(), lr=self._current_lr_G,\n                                             betas=(self._opt.G_adam_b1, self._opt.G_adam_b2))\n        self._optimizer_D = torch.optim.Adam(self._D.parameters(), lr=self._current_lr_D,\n                                             betas=(self._opt.D_adam_b1, self._opt.D_adam_b2))\n\n    def _init_prefetch_inputs(self):\n        self._real_src = None\n        self._real_tsf = None\n        self._bg_mask = None\n        self._input_src = None\n        self._input_G_bg = None\n        self._input_G_src = None\n        self._input_G_tsf = None\n        self._T = None\n        self._body_bbox = None\n        self._head_bbox = None\n\n    def _init_losses(self):\n        # define loss functions\n        multi_gpus = len(self._gpu_ids) > 1\n        self._crt_l1 = torch.nn.L1Loss()\n\n        if self._opt.mask_bce:\n            self._crt_mask = torch.nn.BCELoss()\n        else:\n            self._crt_mask = torch.nn.MSELoss()\n\n        vgg_net = Vgg19()\n        if self._opt.use_vgg:\n            self._crt_tsf = VGGLoss(vgg=vgg_net)\n            if multi_gpus:\n                self._crt_tsf = torch.nn.DataParallel(self._crt_tsf)\n            self._crt_tsf.cuda()\n\n        if self._opt.use_style:\n            self._crt_style = StyleLoss(feat_extractors=vgg_net)\n            if multi_gpus:\n                self._crt_style = torch.nn.DataParallel(self._crt_style)\n            self._crt_style.cuda()\n\n        if self._opt.use_face:\n            self._criterion_face = FaceLoss(pretrained_path=self._opt.face_model)\n            if multi_gpus:\n                self._criterion_face = torch.nn.DataParallel(self._criterion_face)\n            self._criterion_face.cuda()\n\n        # init losses G\n        self._loss_g_rec = self._Tensor([0])\n        self._loss_g_tsf = self._Tensor([0])\n        self._loss_g_style = self._Tensor([0])\n        self._loss_g_face = self._Tensor([0])\n        self._loss_g_adv = self._Tensor([0])\n        self._loss_g_smooth = self._Tensor([0])\n        self._loss_g_mask = self._Tensor([0])\n        self._loss_g_mask_smooth = self._Tensor([0])\n\n        # init losses D\n        self._d_real = self._Tensor([0])\n        self._d_fake = self._Tensor([0])\n\n    def set_input(self, input):\n\n        with torch.no_grad():\n            images = input[\'images\']\n            smpls = input[\'smpls\']\n            src_img = images[:, 0, ...].cuda()\n            src_smpl = smpls[:, 0, ...].cuda()\n            tsf_img = images[:, 1, ...].cuda()\n            tsf_smpl = smpls[:, 1, ...].cuda()\n\n            input_G_src_bg, input_G_tsf_bg, input_G_src, input_G_tsf, T, src_crop_mask, \\\n                tsf_crop_mask, head_bbox, body_bbox = self._bdr(src_img, tsf_img, src_smpl, tsf_smpl)\n\n            self._real_src = src_img\n            self._real_tsf = tsf_img\n\n            self._bg_mask = torch.cat((src_crop_mask, tsf_crop_mask), dim=0)\n            if self._opt.bg_both:\n                self._input_G_bg = torch.cat([input_G_src_bg, input_G_tsf_bg], dim=0)\n            else:\n                self._input_G_bg = input_G_src_bg\n            self._input_G_src = input_G_src\n            self._input_G_tsf = input_G_tsf\n            self._T = T\n            self._head_bbox = head_bbox\n            self._body_bbox = body_bbox\n\n    def set_train(self):\n        self._G.train()\n        self._D.train()\n        self._is_train = True\n\n    def set_eval(self):\n        self._G.eval()\n        self._is_train = False\n\n    def forward(self, keep_data_for_visuals=False, return_estimates=False):\n        # generate fake images\n        fake_bg, fake_src_color, fake_src_mask, fake_tsf_color, fake_tsf_mask = \\\n            self._G.forward(self._input_G_bg, self._input_G_src, self._input_G_tsf, T=self._T)\n\n        bs = fake_src_color.shape[0]\n        fake_src_bg = fake_bg[0:bs]\n        if self._opt.bg_both:\n            fake_tsf_bg = fake_bg[bs:]\n            fake_src_imgs = fake_src_mask * fake_src_bg + (1 - fake_src_mask) * fake_src_color\n            fake_tsf_imgs = fake_tsf_mask * fake_tsf_bg + (1 - fake_tsf_mask) * fake_tsf_color\n        else:\n            fake_src_imgs = fake_src_mask * fake_src_bg + (1 - fake_src_mask) * fake_src_color\n            fake_tsf_imgs = fake_tsf_mask * fake_src_bg + (1 - fake_tsf_mask) * fake_tsf_color\n\n        fake_masks = torch.cat([fake_src_mask, fake_tsf_mask], dim=0)\n\n        # keep data for visualization\n        if keep_data_for_visuals:\n            self.visual_imgs(fake_bg, fake_src_imgs, fake_tsf_imgs, fake_masks)\n\n        return fake_bg, fake_src_imgs, fake_tsf_imgs, fake_masks\n\n    def optimize_parameters(self, trainable=True, keep_data_for_visuals=False):\n        if self._is_train:\n\n            # run inference\n            fake_bg, fake_src_imgs, fake_tsf_imgs, fake_masks = self.forward(keep_data_for_visuals=keep_data_for_visuals)\n\n            loss_G = self._optimize_G(fake_bg, fake_src_imgs, fake_tsf_imgs, fake_masks)\n\n            self._optimizer_G.zero_grad()\n            loss_G.backward()\n            self._optimizer_G.step()\n\n            # train D\n            if trainable:\n                loss_D = self._optimize_D(fake_tsf_imgs)\n                self._optimizer_D.zero_grad()\n                loss_D.backward()\n                self._optimizer_D.step()\n\n    def _optimize_G(self, fake_bg, fake_src_imgs, fake_tsf_imgs, fake_masks):\n        fake_input_D = torch.cat([fake_tsf_imgs, self._input_G_tsf[:, 3:]], dim=1)\n        d_fake_outs = self._D.forward(fake_input_D)\n        self._loss_g_adv = self._compute_loss_D(d_fake_outs, 0) * self._opt.lambda_D_prob\n\n        self._loss_g_rec = self._crt_l1(fake_src_imgs, self._real_src) * self._opt.lambda_rec\n\n        if self._opt.use_vgg:\n            self._loss_g_tsf = torch.mean(self._crt_tsf(fake_tsf_imgs, self._real_tsf)) * self._opt.lambda_tsf\n        else:\n            self._loss_g_tsf = torch.mean(self._crt_tsf(fake_tsf_imgs, self._real_tsf)) * self._opt.lambda_tsf\n\n        if self._opt.use_style:\n            self._loss_g_style = torch.mean(self._crt_style(\n                fake_tsf_imgs, self._real_tsf)) * self._opt.lambda_style\n\n        if self._opt.use_face:\n            self._loss_g_face = torch.mean(self._criterion_face(\n                fake_tsf_imgs, self._real_tsf, bbox1=self._head_bbox, bbox2=self._head_bbox)) * self._opt.lambda_face\n        # loss mask\n        self._loss_g_mask = self._crt_mask(fake_masks, self._bg_mask) * self._opt.lambda_mask\n\n        if self._opt.lambda_mask_smooth != 0:\n            self._loss_g_mask_smooth = self._compute_loss_smooth(fake_masks) * self._opt.lambda_mask_smooth\n\n        # combine losses\n        return self._loss_g_adv + self._loss_g_rec + self._loss_g_tsf + self._loss_g_style + self._loss_g_face + \\\n               self._loss_g_mask + self._loss_g_mask_smooth\n\n    def _optimize_D(self, fake_tsf_imgs):\n        tsf_cond = self._input_G_tsf[:, 3:]\n        fake_input_D = torch.cat([fake_tsf_imgs.detach(), tsf_cond], dim=1)\n        real_input_D = torch.cat([self._real_tsf, tsf_cond], dim=1)\n\n        d_real_outs = self._D.forward(real_input_D)\n        d_fake_outs = self._D.forward(fake_input_D)\n\n        _loss_d_real = self._compute_loss_D(d_real_outs, 1) * self._opt.lambda_D_prob\n        _loss_d_fake = self._compute_loss_D(d_fake_outs, -1) * self._opt.lambda_D_prob\n\n        self._d_real = torch.mean(d_real_outs)\n        self._d_fake = torch.mean(d_fake_outs)\n\n        # combine losses\n        return _loss_d_real + _loss_d_fake\n\n    def _compute_loss_D(self, x, y):\n        return torch.mean((x - y) ** 2)\n\n    def _compute_loss_smooth(self, mat):\n        return torch.mean(torch.abs(mat[:, :, :, :-1] - mat[:, :, :, 1:])) + \\\n               torch.mean(torch.abs(mat[:, :, :-1, :] - mat[:, :, 1:, :]))\n\n    def get_current_errors(self):\n        loss_dict = OrderedDict([(\'g_rec\', self._loss_g_rec.item()),\n                                 (\'g_tsf\', self._loss_g_tsf.item()),\n                                 (\'g_style\', self._loss_g_style.item()),\n                                 (\'g_face\', self._loss_g_face.item()),\n                                 (\'g_adv\', self._loss_g_adv.item()),\n                                 (\'g_mask\', self._loss_g_mask.item()),\n                                 (\'g_mask_smooth\', self._loss_g_mask_smooth.item()),\n                                 (\'d_real\', self._d_real.item()),\n                                 (\'d_fake\', self._d_fake.item())])\n\n        return loss_dict\n\n    def get_current_scalars(self):\n        return OrderedDict([(\'lr_G\', self._current_lr_G), (\'lr_D\', self._current_lr_D)])\n\n    def get_current_visuals(self):\n        # visuals return dictionary\n        visuals = OrderedDict()\n\n        # inputs\n        visuals[\'1_real_img\'] = self._vis_input\n        visuals[\'2_input_tsf\'] = self._vis_tsf\n        visuals[\'3_fake_bg\'] = self._vis_fake_bg\n\n        # outputs\n        visuals[\'4_fake_tsf\'] = self._vis_fake_tsf\n        visuals[\'5_fake_src\'] = self._vis_fake_src\n        visuals[\'6_fake_mask\'] = self._vis_mask\n\n        # batch outputs\n        visuals[\'7_batch_real_img\'] = self._vis_batch_real\n        visuals[\'8_batch_fake_img\'] = self._vis_batch_fake\n\n        return visuals\n\n    @torch.no_grad()\n    def visual_imgs(self, fake_bg, fake_src_imgs, fake_tsf_imgs, fake_masks):\n        ids = fake_masks.shape[0] // 2\n        self._vis_input = util.tensor2im(self._real_src)\n        self._vis_tsf = util.tensor2im(self._input_G_tsf[0, 0:3])\n        self._vis_fake_bg = util.tensor2im(fake_bg)\n        self._vis_fake_src = util.tensor2im(fake_src_imgs)\n        self._vis_fake_tsf = util.tensor2im(fake_tsf_imgs)\n        self._vis_mask = util.tensor2maskim(fake_masks[ids])\n\n        self._vis_batch_real = util.tensor2im(self._real_tsf, idx=-1)\n        self._vis_batch_fake = util.tensor2im(fake_tsf_imgs, idx=-1)\n\n    def save(self, label):\n        # save networks\n        self._save_network(self._G, \'G\', label)\n        self._save_network(self._D, \'D\', label)\n\n        # save optimizers\n        self._save_optimizer(self._optimizer_G, \'G\', label)\n        self._save_optimizer(self._optimizer_D, \'D\', label)\n\n    def load(self):\n        load_epoch = self._opt.load_epoch\n\n        # load G\n        self._load_network(self._G, \'G\', load_epoch, need_module=True)\n\n        if self._is_train:\n            # load D\n            self._load_network(self._D, \'D\', load_epoch, need_module=True)\n\n            # load optimizers\n            self._load_optimizer(self._optimizer_G, \'G\', load_epoch)\n            self._load_optimizer(self._optimizer_D, \'D\', load_epoch)\n\n    def update_learning_rate(self):\n        # updated learning rate G\n        final_lr = self._opt.final_lr\n\n        lr_decay_G = (self._opt.lr_G - final_lr) / self._opt.nepochs_decay\n        self._current_lr_G -= lr_decay_G\n        for param_group in self._optimizer_G.param_groups:\n            param_group[\'lr\'] = self._current_lr_G\n        print(\'update G learning rate: %f -> %f\' % (self._current_lr_G + lr_decay_G, self._current_lr_G))\n\n        # update learning rate D\n        lr_decay_D = (self._opt.lr_D - final_lr) / self._opt.nepochs_decay\n        self._current_lr_D -= lr_decay_D\n        for param_group in self._optimizer_D.param_groups:\n            param_group[\'lr\'] = self._current_lr_D\n        print(\'update D learning rate: %f -> %f\' % (self._current_lr_D + lr_decay_D, self._current_lr_D))\n\n'"
models/impersonator_trainer_aug.py,74,"b'import torch\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport utils.util as util\nfrom .models import BaseModel\nfrom networks.networks import NetworksFactory, HumanModelRecovery, Vgg19, VGGLoss, FaceLoss, StyleLoss\nfrom utils.nmr import SMPLRenderer\nimport ipdb\n\n\nclass BodyRecoveryFlow(torch.nn.Module):\n\n    def __init__(self, opt):\n        super(BodyRecoveryFlow, self).__init__()\n        self._name = \'BodyRecoveryFlow\'\n        self._opt = opt\n\n        # create networks\n        self._init_create_networks()\n\n    def _create_hmr(self):\n        hmr = HumanModelRecovery(smpl_pkl_path=self._opt.smpl_model)\n        saved_data = torch.load(self._opt.hmr_model)\n        hmr.load_state_dict(saved_data)\n        hmr.eval()\n        print(\'hmr load model from {} successfully.\'.format(self._opt.smpl_model))\n        return hmr\n\n    def _create_render(self):\n        render = SMPLRenderer(map_name=self._opt.map_name,\n                              uv_map_path=self._opt.uv_mapping,\n                              tex_size=self._opt.tex_size,\n                              image_size=self._opt.image_size, fill_back=False,\n                              anti_aliasing=True, background_color=(0, 0, 0), has_front=False)\n\n        return render\n\n    def _init_create_networks(self):\n        # hmr and render\n        self._hmr = self._create_hmr()\n        self._render = self._create_render()\n\n    def forward(self, aug_img, src_img, ref_img, src_smpl, ref_smpl):\n        # get smpl information\n        src_info = self._hmr.get_details(src_smpl)\n        ref_info = self._hmr.get_details(ref_smpl)\n\n        # process source inputs\n        src_f2verts, src_fim, _ = self._render.render_fim_wim(src_info[\'cam\'], src_info[\'verts\'])\n        src_f2verts = src_f2verts[:, :, :, 0:2]\n        src_f2verts[:, :, :, 1] *= -1\n        src_cond, _ = self._render.encode_fim(src_info[\'cam\'], src_info[\'verts\'], fim=src_fim, transpose=True)\n        src_crop_mask = util.morph(src_cond[:, -1:, :, :], ks=3, mode=\'erode\')\n\n        _, ref_fim, ref_wim = self._render.render_fim_wim(ref_info[\'cam\'], ref_info[\'verts\'])\n        ref_cond, _ = self._render.encode_fim(ref_info[\'cam\'], ref_info[\'verts\'], fim=ref_fim, transpose=True)\n        T = self._render.cal_bc_transform(src_f2verts, ref_fim, ref_wim)\n        syn_img = F.grid_sample(src_img, T)\n\n        # src input\n        input_G_src = torch.cat([src_img * (1 - src_crop_mask), src_cond], dim=1)\n\n        # tsf input\n        input_G_tsf = torch.cat([syn_img, ref_cond], dim=1)\n\n        # bg input\n        src_bg_mask = util.morph(src_cond[:, -1:, :, :], ks=15, mode=\'erode\')\n        input_G_aug_bg = torch.cat([aug_img * src_bg_mask, src_bg_mask], dim=1)\n        input_G_src_bg = torch.cat([src_img * src_bg_mask, src_bg_mask], dim=1)\n\n        if self._opt.bg_both:\n            ref_bg_mask = util.morph(ref_cond[:, -1:, :, :], ks=25, mode=\'erode\')\n            input_G_tsf_bg = torch.cat([ref_img * ref_bg_mask, ref_bg_mask], dim=1)\n        else:\n            input_G_tsf_bg = None\n\n        # masks\n        tsf_crop_mask = util.morph(ref_cond[:, -1:, :, :], ks=3, mode=\'erode\')\n\n        head_bbox = self.cal_head_bbox(ref_info[\'j2d\'])\n        body_bbox = self.cal_body_bbox(ref_info[\'j2d\'])\n\n        return input_G_aug_bg, input_G_src_bg, input_G_tsf_bg, input_G_src, input_G_tsf, \\\n               T, src_crop_mask, tsf_crop_mask, head_bbox, body_bbox\n\n    def cal_head_bbox(self, kps):\n        """"""\n        Args:\n            kps: (N, 19, 2)\n\n        Returns:\n            bbox: (N, 4)\n        """"""\n        NECK_IDS = 12\n\n        image_size = self._opt.image_size\n\n        kps = (kps + 1) / 2.0\n\n        necks = kps[:, NECK_IDS, 0]\n        zeros = torch.zeros_like(necks)\n        ones = torch.ones_like(necks)\n\n        # min_x = int(max(0.0, np.min(kps[HEAD_IDS:, 0]) - 0.1) * image_size)\n        min_x, _ = torch.min(kps[:, NECK_IDS:, 0] - 0.05, dim=1)\n        min_x = torch.max(min_x, zeros)\n\n        max_x, _ = torch.max(kps[:, NECK_IDS:, 0] + 0.05, dim=1)\n        max_x = torch.min(max_x, ones)\n\n        # min_x = int(max(0.0, np.min(kps[HEAD_IDS:, 0]) - 0.1) * image_size)\n        min_y, _ = torch.min(kps[:, NECK_IDS:, 1] - 0.05, dim=1)\n        min_y = torch.max(min_y, zeros)\n\n        max_y, _ = torch.max(kps[:, NECK_IDS:, 1], dim=1)\n        max_y = torch.min(max_y, ones)\n\n        min_x = (min_x * image_size).long()  # (T, 1)\n        max_x = (max_x * image_size).long()  # (T, 1)\n        min_y = (min_y * image_size).long()  # (T, 1)\n        max_y = (max_y * image_size).long()  # (T, 1)\n\n        # print(min_x.shape, max_x.shape, min_y.shape, max_y.shape)\n        rects = torch.stack((min_x, max_x, min_y, max_y), dim=1)\n        # import ipdb\n        # ipdb.set_trace()\n        return rects\n\n    def cal_body_bbox(self, kps, factor=1.2):\n        """"""\n        Args:\n            kps (torch.cuda.FloatTensor): (N, 19, 2)\n            factor (float):\n\n        Returns:\n            bbox: (N, 4)\n        """"""\n        image_size = self._opt.image_size\n        bs = kps.shape[0]\n        kps = (kps + 1) / 2.0\n        zeros = torch.zeros((bs,), device=kps.device)\n        ones = torch.ones((bs,), device=kps.device)\n\n        min_x, _ = torch.min(kps[:, :, 0], dim=1)\n        max_x, _ = torch.max(kps[:, :, 0], dim=1)\n        middle_x = (min_x + max_x) / 2\n        width = (max_x - min_x) * factor\n        min_x = torch.max(zeros, middle_x - width / 2)\n        max_x = torch.min(ones, middle_x + width / 2)\n\n        min_y, _ = torch.min(kps[:, :, 1], dim=1)\n        max_y, _ = torch.max(kps[:, :, 1], dim=1)\n        middle_y = (min_y + max_y) / 2\n        height = (max_y - min_y) * factor\n        min_y = torch.max(zeros, middle_y - height / 2)\n        max_y = torch.min(ones, middle_y + height / 2)\n\n        min_x = (min_x * image_size).long()  # (T,)\n        max_x = (max_x * image_size).long()  # (T,)\n        min_y = (min_y * image_size).long()  # (T,)\n        max_y = (max_y * image_size).long()  # (T,)\n\n        # print(min_x.shape, max_x.shape, min_y.shape, max_y.shape)\n        bboxs = torch.stack((min_x, max_x, min_y, max_y), dim=1)\n\n        return bboxs\n\n\nclass Impersonator(BaseModel):\n    def __init__(self, opt):\n        super(Impersonator, self).__init__(opt)\n        self._name = \'Impersonator\'\n\n        # create networks\n        self._init_create_networks()\n\n        # init train variables and losses\n        if self._is_train:\n            self._init_train_vars()\n            self._init_losses()\n\n        # load networks and optimizers\n        if not self._is_train or self._opt.load_epoch > 0:\n            self.load()\n        elif self._opt.load_path != \'None\':\n            self._load_params(self._G, self._opt.load_path, need_module=len(self._gpu_ids) > 1)\n\n        # prefetch variables\n        self._init_prefetch_inputs()\n\n    def _init_create_networks(self):\n        multi_gpus = len(self._gpu_ids) > 1\n\n        # body recovery Flow\n        self._bdr = BodyRecoveryFlow(opt=self._opt)\n        if multi_gpus:\n            self._bdr = torch.nn.DataParallel(self._bdr)\n\n        self._bdr.eval()\n        self._bdr.cuda()\n\n        # generator network\n        self._G = self._create_generator()\n        self._G.init_weights()\n        if multi_gpus:\n            self._G = torch.nn.DataParallel(self._G)\n        self._G.cuda()\n\n        # discriminator network\n        self._D = self._create_discriminator()\n        self._D.init_weights()\n        if multi_gpus:\n            self._D = torch.nn.DataParallel(self._D)\n        self._D.cuda()\n\n    def _create_generator(self):\n        return NetworksFactory.get_by_name(self._opt.gen_name, bg_dim=4, src_dim=3 + self._G_cond_nc,\n                                           tsf_dim=3+self._G_cond_nc, repeat_num=self._opt.repeat_num)\n\n    def _create_discriminator(self):\n        return NetworksFactory.get_by_name(\'global_local\', input_nc=3 + self._D_cond_nc,\n                                           norm_type=self._opt.norm_type, ndf=64, n_layers=4, use_sigmoid=False)\n\n    def _init_train_vars(self):\n        self._current_lr_G = self._opt.lr_G\n        self._current_lr_D = self._opt.lr_D\n\n        # initialize optimizers\n        self._optimizer_G = torch.optim.Adam(self._G.parameters(), lr=self._current_lr_G,\n                                             betas=(self._opt.G_adam_b1, self._opt.G_adam_b2))\n        self._optimizer_D = torch.optim.Adam(self._D.parameters(), lr=self._current_lr_D,\n                                             betas=(self._opt.D_adam_b1, self._opt.D_adam_b2))\n\n    def _init_prefetch_inputs(self):\n\n        self._real_bg = None\n        self._real_src = None\n        self._real_tsf = None\n\n        self._bg_mask = None\n        self._input_G_bg = None\n        self._input_G_src = None\n        self._input_G_tsf = None\n        self._head_bbox = None\n        self._body_bbox = None\n\n        self._T = None\n\n    def _init_losses(self):\n        # define loss functions\n        multi_gpus = len(self._gpu_ids) > 1\n        self._crt_l1 = torch.nn.L1Loss()\n\n        if self._opt.mask_bce:\n            self._crt_mask = torch.nn.BCELoss()\n        else:\n            self._crt_mask = torch.nn.MSELoss()\n\n        vgg_net = Vgg19()\n        if self._opt.use_vgg:\n            self._crt_vgg = VGGLoss(vgg=vgg_net)\n            if multi_gpus:\n                self._crt_vgg = torch.nn.DataParallel(self._crt_vgg)\n            self._crt_vgg.cuda()\n\n        if self._opt.use_style:\n            self._crt_sty = StyleLoss(feat_extractors=vgg_net)\n            if multi_gpus:\n                self._crt_sty = torch.nn.DataParallel(self._crt_sty)\n            self._crt_sty.cuda()\n\n        if self._opt.use_face:\n            self._crt_face = FaceLoss(pretrained_path=self._opt.face_model)\n            if multi_gpus:\n                self._criterion_face = torch.nn.DataParallel(self._crt_face)\n            self._crt_face.cuda()\n\n        # init losses G\n        self._g_rec = self._Tensor([0])\n        self._g_tsf = self._Tensor([0])\n        self._g_style = self._Tensor([0])\n        self._g_face = self._Tensor([0])\n        self._g_adv = self._Tensor([0])\n        self._g_smooth = self._Tensor([0])\n        self._g_mask = self._Tensor([0])\n        self._g_mask_smooth = self._Tensor([0])\n\n        # init losses D\n        self._d_real = self._Tensor([0])\n        self._d_fake = self._Tensor([0])\n\n    @torch.no_grad()\n    def set_input(self, input):\n\n        images = input[\'images\'].cuda()\n        smpls = input[\'smpls\'].cuda()\n        aug_bg = input[\'bg\'].cuda()\n        src_img = images[:, 0, ...]\n        src_smpl = smpls[:, 0, ...]\n        tsf_img = images[:, 1, ...]\n        tsf_smpl = smpls[:, 1, ...]\n\n        # print(src_img.shape, src_smpl.shape, tsf_img.shape, tsf_smpl.shape)\n        input_G_aug_bg, input_G_src_bg, input_G_tsf_bg, input_G_src, input_G_tsf, T, src_crop_mask, tsf_crop_mask, \\\n        head_bbox, body_bbox = self._bdr(aug_bg, src_img, tsf_img, src_smpl, tsf_smpl)\n\n        if self._opt.bg_both:\n            self._input_G_bg = torch.cat([input_G_src_bg, input_G_aug_bg, input_G_tsf_bg], dim=0)\n        else:\n            self._input_G_bg = torch.cat([input_G_src_bg, input_G_aug_bg], dim=0)\n\n        self._bg_mask = torch.cat((src_crop_mask, tsf_crop_mask), dim=0)\n        self._input_G_src = input_G_src\n        self._input_G_tsf = input_G_tsf\n        self._T = T\n        self._head_bbox = head_bbox\n        self._body_bbox = body_bbox\n        self._real_src = src_img\n        self._real_tsf = tsf_img\n        self._real_bg = aug_bg\n\n    def set_train(self):\n        self._G.train()\n        self._D.train()\n        self._is_train = True\n\n    def set_eval(self):\n        self._G.eval()\n        self._is_train = False\n\n    def forward(self, keep_data_for_visuals=False, return_estimates=False):\n        # generate fake images\n        fake_bg, fake_src_color, fake_src_mask, fake_tsf_color, fake_tsf_mask = \\\n            self._G.forward(self._input_G_bg, self._input_G_src, self._input_G_tsf, T=self._T)\n\n        bs = fake_src_color.shape[0]\n        fake_src_bg = fake_bg[0:bs]\n        fake_aug_bg = fake_bg[bs:2 * bs]\n\n        if self._opt.bg_both:\n            fake_tsf_bg = fake_bg[2*bs:3*bs]\n            fake_src_imgs = fake_src_mask * fake_src_bg + (1 - fake_src_mask) * fake_src_color\n            fake_tsf_imgs = fake_tsf_mask * fake_tsf_bg + (1 - fake_tsf_mask) * fake_tsf_color\n        else:\n            fake_src_imgs = fake_src_mask * fake_src_bg + (1 - fake_src_mask) * fake_src_color\n            fake_tsf_imgs = fake_tsf_mask * fake_src_bg + (1 - fake_tsf_mask) * fake_tsf_color\n\n        fake_masks = torch.cat([fake_src_mask, fake_tsf_mask], dim=0)\n\n        # keep data for visualization\n        if keep_data_for_visuals:\n            self.visual_imgs(fake_bg, fake_aug_bg, fake_src_imgs, fake_tsf_imgs, fake_masks)\n\n        return fake_aug_bg, fake_src_imgs, fake_tsf_imgs, fake_masks\n\n    def optimize_parameters(self, trainable=True, keep_data_for_visuals=False):\n        if self._is_train:\n            # convert tensor to variables\n            fake_aug_bg, fake_src_imgs, fake_tsf_imgs, fake_masks = self.forward(\n                keep_data_for_visuals=keep_data_for_visuals)\n\n            loss_G = self._optimize_G(fake_aug_bg, fake_src_imgs, fake_tsf_imgs, fake_masks)\n\n            self._optimizer_G.zero_grad()\n            loss_G.backward()\n            self._optimizer_G.step()\n\n            # train D\n            if trainable:\n                loss_D = self._optimize_D(fake_aug_bg, fake_tsf_imgs)\n                self._optimizer_D.zero_grad()\n                loss_D.backward()\n                self._optimizer_D.step()\n\n    def _optimize_G(self, fake_aug_bg, fake_src_imgs, fake_tsf_imgs, fake_masks):\n        bs = fake_tsf_imgs.shape[0]\n\n        fake_global = torch.cat([fake_aug_bg, self._input_G_bg[bs:2*bs, -1:]], dim=1)\n        fake_local = torch.cat([fake_tsf_imgs, self._input_G_tsf[:, 3:]], dim=1)\n        d_fake_outs = self._D.forward(fake_global, fake_local, self._body_bbox)\n        self._g_adv = self._compute_loss_D(d_fake_outs, 0) * self._opt.lambda_D_prob\n\n        self._g_rec = self._crt_l1(fake_src_imgs, self._real_src) * self._opt.lambda_rec\n\n        if self._opt.use_vgg:\n            self._g_tsf = torch.mean(self._crt_vgg(fake_tsf_imgs, self._real_tsf)\n                                     + self._crt_vgg(fake_aug_bg, self._real_bg)) * self._opt.lambda_tsf\n\n        if self._opt.use_style:\n            self._g_style = torch.mean(self._crt_sty(fake_tsf_imgs, self._real_tsf)\n                                       + self._crt_sty(fake_aug_bg, self._real_bg)) * self._opt.lambda_style\n\n        if self._opt.use_face:\n            self._g_face = torch.mean(self._crt_face(fake_tsf_imgs, self._real_tsf, bbox1=self._head_bbox,\n                                                     bbox2=self._head_bbox)) * self._opt.lambda_face\n        # loss mask\n        self._g_mask = self._crt_mask(fake_masks, self._bg_mask) * self._opt.lambda_mask\n\n        if self._opt.lambda_mask_smooth != 0:\n            self._g_mask_smooth = self._compute_loss_smooth(fake_masks) * self._opt.lambda_mask_smooth\n\n        # combine losses\n        return self._g_adv + self._g_rec + self._g_tsf + self._g_style + self._g_face + self._g_mask + self._g_mask_smooth\n\n    def _optimize_D(self, fake_aug_bg, fake_tsf_imgs):\n        bs = fake_tsf_imgs.shape[0]\n        fake_global = torch.cat([fake_aug_bg.detach(), self._input_G_bg[bs:2*bs, -1:]], dim=1)\n        fake_local = torch.cat([fake_tsf_imgs.detach(), self._input_G_tsf[:, 3:]], dim=1)\n        real_global = torch.cat([self._real_bg, self._input_G_bg[bs:2*bs, -1:]], dim=1)\n        real_local = torch.cat([self._real_tsf, self._input_G_tsf[:, 3:]], dim=1)\n\n        d_real_outs = self._D.forward(real_global, real_local, self._body_bbox)\n        d_fake_outs = self._D.forward(fake_global, fake_local, self._body_bbox)\n\n        _loss_d_real = self._compute_loss_D(d_real_outs, 1) * self._opt.lambda_D_prob\n        _loss_d_fake = self._compute_loss_D(d_fake_outs, -1) * self._opt.lambda_D_prob\n\n        self._d_real = torch.mean(d_real_outs)\n        self._d_fake = torch.mean(d_fake_outs)\n\n        # combine losses\n        return _loss_d_real + _loss_d_fake\n\n    def _compute_loss_D(self, x, y):\n        return torch.mean((x - y) ** 2)\n\n    def _compute_loss_smooth(self, mat):\n        return torch.mean(torch.abs(mat[:, :, :, :-1] - mat[:, :, :, 1:])) + \\\n               torch.mean(torch.abs(mat[:, :, :-1, :] - mat[:, :, 1:, :]))\n\n    def get_current_errors(self):\n        loss_dict = OrderedDict([(\'g_rec\', self._g_rec.item()),\n                                 (\'g_tsf\', self._g_tsf.item()),\n                                 (\'g_style\', self._g_style.item()),\n                                 (\'g_face\', self._g_face.item()),\n                                 (\'g_adv\', self._g_adv.item()),\n                                 (\'g_mask\', self._g_mask.item()),\n                                 (\'g_mask_smooth\', self._g_mask_smooth.item()),\n                                 (\'d_real\', self._d_real.item()),\n                                 (\'d_fake\', self._d_fake.item())])\n\n        return loss_dict\n\n    def get_current_scalars(self):\n        return OrderedDict([(\'lr_G\', self._current_lr_G), (\'lr_D\', self._current_lr_D)])\n\n    def get_current_visuals(self):\n        # visuals return dictionary\n        visuals = OrderedDict()\n\n        # inputs\n        visuals[\'1_real_img\'] = self._vis_input\n        visuals[\'2_input_tsf\'] = self._vis_tsf\n        visuals[\'3_fake_bg\'] = self._vis_fake_bg\n\n        # outputs\n        visuals[\'4_fake_tsf\'] = self._vis_fake_tsf\n        visuals[\'5_fake_src\'] = self._vis_fake_src\n        visuals[\'6_fake_mask\'] = self._vis_mask\n\n        # batch outputs\n        visuals[\'7_batch_real_img\'] = self._vis_batch_real\n        visuals[\'8_batch_fake_img\'] = self._vis_batch_fake\n\n        return visuals\n\n    @torch.no_grad()\n    def visual_imgs(self, fake_bg, fake_aug_bg, fake_src_imgs, fake_tsf_imgs, fake_masks):\n        ids = fake_masks.shape[0] // 2\n        self._vis_input = util.tensor2im(self._real_src)\n        self._vis_tsf = util.tensor2im(self._input_G_tsf[0, 0:3])\n        self._vis_fake_bg = util.tensor2im(fake_bg)\n        self._vis_fake_src = util.tensor2im(fake_src_imgs)\n        self._vis_fake_tsf = util.tensor2im(fake_tsf_imgs)\n        self._vis_mask = util.tensor2maskim(fake_masks[ids])\n\n        self._vis_batch_real = util.tensor2im(torch.cat([self._real_tsf, self._real_bg], dim=0), idx=-1)\n        self._vis_batch_fake = util.tensor2im(torch.cat([fake_tsf_imgs, fake_aug_bg], dim=0), idx=-1)\n\n    def save(self, label):\n        # save networks\n        self._save_network(self._G, \'G\', label)\n        self._save_network(self._D, \'D\', label)\n\n        # save optimizers\n        self._save_optimizer(self._optimizer_G, \'G\', label)\n        self._save_optimizer(self._optimizer_D, \'D\', label)\n\n    def load(self):\n        load_epoch = self._opt.load_epoch\n\n        # load G\n        self._load_network(self._G, \'G\', load_epoch, need_module=True)\n\n        if self._is_train:\n            # load D\n            self._load_network(self._D, \'D\', load_epoch, need_module=True)\n\n            # load optimizers\n            self._load_optimizer(self._optimizer_G, \'G\', load_epoch)\n            self._load_optimizer(self._optimizer_D, \'D\', load_epoch)\n\n    def update_learning_rate(self):\n        # updated learning rate G\n        final_lr = self._opt.final_lr\n\n        lr_decay_G = (self._opt.lr_G - final_lr) / self._opt.nepochs_decay\n        self._current_lr_G -= lr_decay_G\n        for param_group in self._optimizer_G.param_groups:\n            param_group[\'lr\'] = self._current_lr_G\n        print(\'update G learning rate: %f -> %f\' % (self._current_lr_G + lr_decay_G, self._current_lr_G))\n\n        # update learning rate D\n        lr_decay_D = (self._opt.lr_D - final_lr) / self._opt.nepochs_decay\n        self._current_lr_D -= lr_decay_D\n        for param_group in self._optimizer_D.param_groups:\n            param_group[\'lr\'] = self._current_lr_D\n        print(\'update D learning rate: %f -> %f\' % (self._current_lr_D + lr_decay_D, self._current_lr_D))\n\n    def debug(self, visualizer):\n        visualizer.vis_named_img(\'bg_inputs\', self._input_G_bg[:, 0:3])\n        visualizer.vis_named_img(\'tsf_imgs\', self._input_G_tsf[:, 0:3])\n        # ipdb.set_trace()\n\n\nclass ImpersonatorAllSetTrain(Impersonator):\n    def __init__(self, opt):\n        super(ImpersonatorAllSetTrain, self).__init__(opt)\n        self._name = \'ImpersonatorAllSetTrain\'\n\n    @torch.no_grad()\n    def set_input(self, input):\n\n        images = input[\'images\']\n        smpls = input[\'smpls\']\n        aug_bg = input[\'bg\'].cuda()\n        src_img = images[:, 0, ...].contiguous().cuda()\n        src_smpl = smpls[:, 0, ...].contiguous().cuda()\n        tsf_img = images[:, 1, ...].contiguous().cuda()\n        tsf_smpl = smpls[:, 1, ...].contiguous().cuda()\n\n        input_G_aug_bg, input_G_bg, input_G_src, input_G_tsf, T, bg_mask, head_bbox, body_bbox = \\\n            self._bdr(aug_bg, src_img, src_smpl, tsf_smpl)\n        bs = input_G_bg.shape[0]\n\n        fashion_images = input[\'fashion_images\'].cuda()\n        fashion_G_bg = input[\'fashion_bg_inputs\'].cuda()\n        fashion_G_src = input[\'fashion_src_inputs\'].cuda()\n        fashion_G_tsf = input[\'fashion_tsf_inputs\'].cuda()\n        fashion_T = input[\'fashion_T\'].cuda()\n        fashion_head_bbox = input[\'fashion_head_bbox\'].cuda()\n        fashion_body_bbox = input[\'fashion_body_bbox\'].cuda()\n        fashion_bg_mask = input[\'fashion_masks\'].cuda()\n\n        self._input_G_aug_bg = torch.cat([input_G_bg, fashion_G_bg, input_G_aug_bg], dim=0)\n        self._input_G_src = torch.cat([input_G_src, fashion_G_src], dim=0)\n        self._input_G_tsf = torch.cat([input_G_tsf, fashion_G_tsf], dim=0)\n        self._bg_mask = torch.cat([bg_mask[0:bs], fashion_bg_mask[:, 0],\n                                   bg_mask[bs:], fashion_bg_mask[:, 1]], dim=0)\n        self._T = torch.cat([T, fashion_T], dim=0)\n\n        self._head_bbox = torch.cat([head_bbox, fashion_head_bbox], dim=0)\n        self._body_bbox = torch.cat([body_bbox, fashion_body_bbox], dim=0)\n        self._real_src = torch.cat([src_img, fashion_images[:, 0]], dim=0)\n        self._real_tsf = torch.cat([tsf_img, fashion_images[:, 1]], dim=0)\n        self._real_bg = aug_bg\n\n    def debug(self, visualizer):\n        T = self._T\n        tsf_imgs = F.grid_sample(self._real_src, T)\n\n        visualizer.vis_named_img(\'bg_inputs\', self._input_G_aug_bg[:, 0:3])\n        visualizer.vis_named_img(\'src_inputs\', self._input_G_src[:, 0:3])\n        visualizer.vis_named_img(\'tsf_inputs\', self._input_G_tsf[:, 0:3])\n        visualizer.vis_named_img(\'tsf_imgs\', tsf_imgs)\n        visualizer.vis_named_img(\'real_src\', self._real_src)\n        visualizer.vis_named_img(\'real_tsf\', self._real_tsf)\n        visualizer.vis_named_img(\'real_bg\', self._real_bg)\n        visualizer.vis_named_img(\'bg_masks\', self._bg_mask)\n\n        print(self._head_bbox)\n        print()\n        print(self._body_bbox)\n        ipdb.set_trace()\n\n\n\n\n\n'"
models/models.py,6,"b'import os\nimport torch\nfrom torch.optim import lr_scheduler\nfrom collections import OrderedDict\n\n\nclass ModelsFactory(object):\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_by_name(model_name, *args, **kwargs):\n        model = None\n\n        if model_name == \'concat\':\n            from .baseline import ConcatBaseline\n            model = ConcatBaseline(*args, **kwargs)\n\n        elif model_name == \'texture_warping\':\n            from .baseline import TextureWarpingBaseline\n            model = TextureWarpingBaseline(*args, **kwargs)\n\n        elif model_name == \'feature_warping\':\n            from .baseline import FeatureWarpingBaseline\n            model = FeatureWarpingBaseline(*args, **kwargs)\n\n        elif model_name == \'imitator\':\n            from .imitator import Imitator\n            model = Imitator(*args, **kwargs)\n\n        elif model_name == \'swapper\':\n            from .swapper import Swapper\n            model = Swapper(*args, **kwargs)\n\n        elif model_name == \'viewer\':\n            from .viewer import Viewer\n            model = Viewer(*args, **kwargs)\n\n        elif model_name == \'animator\':\n            raise NotImplementedError\n            # from .animator import Animator\n            # model = Animator(*args, **kwargs)\n\n        elif model_name == \'impersonator_trainer\':\n            from .impersonator_trainer import Impersonator\n            model = Impersonator(*args, **kwargs)\n\n        elif model_name == \'impersonator_trainer_aug\':\n            from .impersonator_trainer_aug import Impersonator\n            model = Impersonator(*args, **kwargs)\n\n        elif model_name == \'impersonator_all_set_trainer_aug\':\n            from .impersonator_trainer_aug import ImpersonatorAllSetTrain\n            model = ImpersonatorAllSetTrain(*args, **kwargs)\n\n        else:\n            raise ValueError(""Model %s not recognized."" % model_name)\n\n        print(""Model %s was created"" % model.name)\n        return model\n\n\nclass BaseModel(object):\n\n    def __init__(self, opt):\n        self._name = \'BaseModel\'\n\n        self._opt = opt\n        self._gpu_ids = opt.gpu_ids\n        self._is_train = opt.is_train\n\n        self._Tensor = torch.cuda.FloatTensor if self._gpu_ids else torch.Tensor\n        self._save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n        self._G_cond_nc, self._D_cond_nc = self.cond_nc()\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def is_train(self):\n        return self._is_train\n\n    def cond_nc(self):\n        from utils.mesh import get_map_fn_dim\n        if self._opt.map_name:\n            nc = get_map_fn_dim(self._opt.map_name)\n            _G_cond_nc, _D_cond_nc = nc, nc\n        else:\n            _G_cond_nc = self._opt.cond_nc\n            _D_cond_nc = self._opt.cond_nc\n\n        return _G_cond_nc, _D_cond_nc\n\n    def set_input(self, input):\n        assert False, ""set_input not implemented""\n\n    def set_train(self):\n        assert False, ""set_train not implemented""\n\n    def set_eval(self):\n        assert False, ""set_eval not implemented""\n\n    def forward(self, *input):\n        assert False, ""forward not implemented""\n\n    # used in test time, no backprop\n    def test(self):\n        assert False, ""test not implemented""\n\n    def get_image_paths(self):\n        return {}\n\n    def optimize_parameters(self):\n        assert False, ""optimize_parameters not implemented""\n\n    def get_current_visuals(self):\n        return {}\n\n    def get_current_errors(self):\n        return {}\n\n    def get_current_scalars(self):\n        return {}\n\n    def save(self, label):\n        assert False, ""save not implemented""\n\n    def load(self):\n        assert False, ""load not implemented""\n\n    def _save_optimizer(self, optimizer, optimizer_label, epoch_label):\n        save_filename = \'opt_epoch_%s_id_%s.pth\' % (epoch_label, optimizer_label)\n        save_path = os.path.join(self._save_dir, save_filename)\n        torch.save(optimizer.state_dict(), save_path)\n\n    def _load_optimizer(self, optimizer, optimizer_label, epoch_label):\n        load_filename = \'opt_epoch_%s_id_%s.pth\' % (epoch_label, optimizer_label)\n        load_path = os.path.join(self._save_dir, load_filename)\n        assert os.path.exists(load_path), \'Weights file not found. %s \' \\\n                                          \'Have you trained a model!? We are not providing one\' % load_path\n\n        optimizer.load_state_dict(torch.load(load_path))\n        print(\'loaded optimizer: %s\' % load_path)\n\n    def _save_network(self, network, network_label, epoch_label):\n        save_filename = \'net_epoch_%s_id_%s.pth\' % (epoch_label, network_label)\n        save_path = os.path.join(self._save_dir, save_filename)\n        torch.save(network.state_dict(), save_path)\n        print(\'saved net: %s\' % save_path)\n\n    def _load_network(self, network, network_label, epoch_label, need_module=False):\n        load_filename = \'net_epoch_%s_id_%s.pth\' % (epoch_label, network_label)\n        load_path = os.path.join(self._save_dir, load_filename)\n\n        self._load_params(network, load_path, need_module)\n\n    def _load_params(self, network, load_path, need_module=False):\n        assert os.path.exists(\n            load_path), \'Weights file not found. Have you trained a model!? We are not providing one %s\' % load_path\n\n        def load(model, orig_state_dict):\n            state_dict = OrderedDict()\n            for k, v in orig_state_dict.items():\n                # remove \'module\'\n                name = k[7:] if \'module\' in k else k\n                state_dict[name] = v\n\n            # load params\n            model.load_state_dict(state_dict)\n\n        save_data = torch.load(load_path)\n        if need_module:\n            network.load_state_dict(save_data)\n        else:\n            load(network, save_data)\n\n        print(\'Loading net: %s\' % load_path)\n\n    def update_learning_rate(self):\n        pass\n\n    def print_network(self, network):\n        num_params = 0\n        for param in network.parameters():\n            num_params += param.numel()\n        print(network)\n        print(\'Total number of parameters: %d\' % num_params)\n\n    def _get_scheduler(self, optimizer, opt):\n        if opt.lr_policy == \'lambda\':\n            def lambda_rule(epoch):\n                lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n                return lr_l\n            scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n        elif opt.lr_policy == \'step\':\n            scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n        elif opt.lr_policy == \'plateau\':\n            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\', factor=0.2, threshold=0.01, patience=5)\n        else:\n            return NotImplementedError(\'learning rate policy [%s] is not implemented\', opt.lr_policy)\n        return scheduler\n'"
models/swapper.py,63,"b""import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom .models import BaseModel\nfrom networks.networks import NetworksFactory, HumanModelRecovery\nfrom utils.detectors import PersonMaskRCNNDetector\nfrom utils.nmr import SMPLRenderer\nimport utils.cv_utils as cv_utils\nimport utils.util as util\nimport utils.mesh as mesh\n\nimport ipdb\n\n\nclass Swapper(BaseModel):\n\n    PART_IDS = {\n        'body': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n        'all': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    }\n\n    def __init__(self, opt):\n        super(Swapper, self).__init__(opt)\n        self._name = 'Swapper'\n\n        self._create_networks()\n\n        # prefetch variables\n        self.src_info = None\n        self.tsf_info = None\n        self.T = None\n        self.T12 = None\n        self.T21 = None\n        self.grid = self.render.create_meshgrid(self._opt.image_size).cuda()\n        self.part_fn = torch.tensor(mesh.create_mapping('par', self._opt.uv_mapping,\n                                                        contain_bg=True, fill_back=False)).float().cuda()\n        self.part_faces_dict = mesh.get_part_face_ids(part_type='par', fill_back=False)\n        self.part_faces = list(self.part_faces_dict.values())\n\n    def _create_networks(self):\n        # 0. create generator\n        self.generator = self._create_generator().cuda()\n\n        # 0. create bgnet\n        if self._opt.bg_model != 'ORIGINAL':\n            self.bgnet = self._create_bgnet().cuda()\n        else:\n            self.bgnet = self.generator.bg_model\n\n        # 2. create hmr\n        self.hmr = self._create_hmr().cuda()\n\n        # 3. create render\n        self.render = SMPLRenderer(image_size=self._opt.image_size, tex_size=self._opt.tex_size,\n                                   has_front=self._opt.front_warp, fill_back=False).cuda()\n        # 4. pre-processor\n        if self._opt.has_detector:\n            self.detector = PersonMaskRCNNDetector(ks=self._opt.bg_ks, threshold=0.5, to_gpu=True)\n        else:\n            self.detector = None\n\n    def _create_bgnet(self):\n        net = NetworksFactory.get_by_name('deepfillv2', c_dim=4)\n        self._load_params(net, self._opt.bg_model, need_module=False)\n        net.eval()\n        return net\n\n    def _create_generator(self):\n        net = NetworksFactory.get_by_name(self._opt.gen_name, bg_dim=4, src_dim=3+self._G_cond_nc,\n                                          tsf_dim=3+self._G_cond_nc, repeat_num=self._opt.repeat_num)\n\n        if self._opt.load_path:\n            self._load_params(net, self._opt.load_path)\n        elif self._opt.load_epoch > 0:\n            self._load_network(net, 'G', self._opt.load_epoch)\n        else:\n            raise ValueError('load_path {} is empty and load_epoch {} is 0'.format(\n                self._opt.load_path, self._opt.load_epoch))\n\n        net.eval()\n        return net\n\n    def _create_hmr(self):\n        hmr = HumanModelRecovery(self._opt.smpl_model)\n        saved_data = torch.load(self._opt.hmr_model)\n        hmr.load_state_dict(saved_data)\n        hmr.eval()\n        return hmr\n\n    @staticmethod\n    def visualize(*args, **kwargs):\n        visualizer = args[0]\n        if visualizer is not None:\n            for key, value in kwargs.items():\n                visualizer.vis_named_img(key, value)\n\n    # TODO it dose not support mini-batch inputs currently.\n    @torch.no_grad()\n    def personalize(self, src_path, src_smpl=None, output_path='', visualizer=None):\n\n        ori_img = cv_utils.read_cv2_img(src_path)\n\n        # resize image and convert the color space from [0, 255] to [-1, 1]\n        img = cv_utils.transform_img(ori_img, self._opt.image_size, transpose=True) * 2 - 1.0\n        img = torch.tensor(img, dtype=torch.float32).cuda()[None, ...]\n\n        if src_smpl is None:\n            img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n            img_hmr = torch.tensor(img_hmr, dtype=torch.float32).cuda()[None, ...]\n            src_smpl = self.hmr(img_hmr)\n        else:\n            src_smpl = torch.tensor(src_smpl, dtype=torch.float32).cuda()[None, ...]\n\n        # source process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n        src_info = self.hmr.get_details(src_smpl)\n        src_f2verts, src_fim, src_wim = self.render.render_fim_wim(src_info['cam'], src_info['verts'])\n        # src_f2pts = src_f2verts[:, :, :, 0:2]\n        src_info['fim'] = src_fim\n        src_info['wim'] = src_wim\n        src_info['cond'], _ = self.render.encode_fim(src_info['cam'], src_info['verts'], fim=src_fim, transpose=True)\n        src_info['f2verts'] = src_f2verts\n        src_info['p2verts'] = src_f2verts[:, :, :, 0:2]\n        src_info['p2verts'][:, :, :, 1] *= -1\n\n        if self._opt.only_vis:\n            src_info['p2verts'] = self.render.get_vis_f2pts(src_info['p2verts'], src_fim)\n\n        src_info['part'], _ = self.render.encode_fim(src_info['cam'], src_info['verts'],\n                                                     fim=src_fim, transpose=True, map_fn=self.part_fn)\n        # add image to source info\n        src_info['img'] = img\n        src_info['image'] = ori_img\n\n        # 2. process the src inputs\n        if self.detector is not None:\n            bbox, body_mask = self.detector.inference(img[0])\n            bg_mask = 1 - body_mask\n        else:\n            bg_mask = util.morph(src_info['cond'][:, -1:, :, :], ks=self._opt.bg_ks, mode='erode')\n            body_mask = 1 - bg_mask\n\n        if self._opt.bg_model != 'ORIGINAL':\n            src_info['bg'] = self.bgnet(img, masks=body_mask, only_x=True)\n        else:\n            incomp_img = img * bg_mask\n            bg_inputs = torch.cat([incomp_img, bg_mask], dim=1)\n            img_bg = self.bgnet(bg_inputs)\n            # src_info['bg_inputs'] = bg_inputs\n            src_info['bg'] = img_bg\n            # src_info['bg'] = incomp_img + img_bg * body_mask\n\n        ft_mask = 1 - util.morph(src_info['cond'][:, -1:, :, :], ks=self._opt.ft_ks, mode='erode')\n        src_inputs = torch.cat([img * ft_mask, src_info['cond']], dim=1)\n\n        src_info['feats'] = self.generator.encode_src(src_inputs)\n        src_info['src_inputs'] = src_inputs\n\n        src_info = src_info\n\n        # if visualizer is not None:\n        #     self.visualize(visualizer, src=img, bg=src_info['bg'])\n\n        if output_path:\n            cv_utils.save_cv2_img(src_info['image'], output_path, image_size=self._opt.image_size)\n\n        return src_info\n\n    def _extract_smpls(self, input_file):\n        img = cv_utils.read_cv2_img(input_file)\n        img = cv_utils.transform_img(img, image_size=224) * 2 - 1.0  # hmr receive [-1, 1]\n        img = img.transpose((2, 0, 1))\n        img = torch.FloatTensor(img).cuda()[None, ...]\n        theta = self.hmr(img)[-1]\n\n        return theta\n\n    @torch.no_grad()\n    def swap_smpl(self, src_cam, src_shape, tgt_smpl, preserve_scale=True):\n        cam = tgt_smpl[:, 0:3].contiguous()\n        pose = tgt_smpl[:, 3:75].contiguous()\n\n        if preserve_scale:\n            cam[:, 0] = src_cam[:, 0]\n            cam[:, 1:] = (src_cam[:, 0] / cam[:, 0]) * cam[:, 1:] + src_cam[:, 1:]\n            cam[:, 0] = src_cam[:, 0]\n        else:\n            cam[: 0] = src_cam[:, 0]\n\n        tsf_smpl = torch.cat([cam, pose, src_shape], dim=1)\n\n        return tsf_smpl\n\n    @torch.no_grad()\n    def swap_setup(self, src_path, tgt_path, src_smpl=None, tgt_smpl=None, output_dir=''):\n        self.src_info = self.personalize(src_path, src_smpl)\n        self.tsf_info = self.personalize(tgt_path, tgt_smpl)\n\n    @torch.no_grad()\n    def swap(self, src_info, tgt_info, target_part='body', visualizer=None):\n        assert target_part in self.PART_IDS.keys()\n\n        def merge_list(part_ids):\n            faces = set()\n            for i in part_ids:\n                fs = set(self.part_faces[i])\n                faces |= fs\n            return list(faces)\n\n        # get target selected face index map\n        selected_ids = self.PART_IDS[target_part]\n        left_ids = [i for i in self.PART_IDS['all'] if i not in selected_ids]\n\n        src_part_mask = (torch.sum(src_info['part'][:, selected_ids, ...], dim=1) != 0).bool()\n        src_left_mask = torch.sum(src_info['part'][:, left_ids, ...], dim=1).bool()\n\n        # selected_faces = merge_list(selected_ids)\n        left_faces = merge_list(left_ids)\n\n        T11, T21 = self.calculate_trans(src_left_mask, left_faces)\n\n        tsf21 = self.generator.transform(tgt_info['img'], T21)\n        tsf11 = self.generator.transform(src_info['img'], T11)\n\n        src_part_mask = src_part_mask[:, None, :, :].float()\n        src_left_mask = src_left_mask[:, None, :, :].float()\n        tsf_img = tsf21 * src_part_mask + tsf11 * src_left_mask\n\n        tsf_inputs = torch.cat([tsf_img, src_info['cond']], dim=1)\n\n        preds, tsf_mask = self.forward(tsf_inputs, tgt_info['feats'], T21, src_info['feats'], T11, src_info['bg'])\n\n        if self._opt.front_warp:\n            # preds = tsf11 * src_left_mask + (1 - src_left_mask) * preds\n            preds = self.warp(preds, src_info['img'], src_info['fim'], tsf_mask)\n\n        if visualizer is not None:\n            self.visualize(visualizer, src_img=src_info['img'], tgt_img=tgt_info['img'], preds=preds)\n\n        return preds\n\n    # TODO it dose not support mini-batch inputs currently.\n    def calculate_trans(self, src_left_mask, left_faces):\n        # calculate T11\n        T11 = self.grid.clone()\n        T11[~src_left_mask[0]] = -2\n        T11.unsqueeze_(0)\n\n        # calculate T21\n        tsf_f2p = self.tsf_info['p2verts'].clone()\n        tsf_f2p[0, left_faces] = -2\n        T21 = self.render.cal_bc_transform(tsf_f2p, self.src_info['fim'], self.src_info['wim'])\n        T21.clamp_(-2, 2)\n        return T11, T21\n\n    def warp(self, preds, tsf, fim, fake_tsf_mask):\n        front_mask = self.render.encode_front_fim(fim, transpose=True)\n        preds = (1 - front_mask) * preds + tsf * front_mask * (1 - fake_tsf_mask)\n        # preds = torch.clamp(preds + tsf * front_mask, -1, 1)\n        return preds\n\n    def forward(self, tsf_inputs, feats21, T21, feats11, T11, bg):\n        with torch.no_grad():\n            # generate fake images\n            src_encoder_outs21, src_resnet_outs21 = feats21\n            src_encoder_outs11, src_resnet_outs11 = feats11\n\n            tsf_color, tsf_mask = self.generator.swap(tsf_inputs, src_encoder_outs21, src_encoder_outs11,\n                                                      src_resnet_outs21, src_resnet_outs11, T21, T11)\n            pred_imgs = tsf_mask * bg + (1 - tsf_mask) * tsf_color\n\n        return pred_imgs, tsf_mask\n\n    def post_personalize(self, out_dir, visualizer, verbose=True):\n        from networks.networks import FaceLoss\n        bs = 2 if self._opt.batch_size > 1 else 1\n\n        init_bg = torch.cat([self.src_info['bg'], self.tsf_info['bg']], dim=0)\n\n        @torch.no_grad()\n        def initialize(src_info, tsf_info):\n            src_encoder_outs, src_resnet_outs = src_info['feats']\n            src_f2p = src_info['p2verts']\n\n            tsf_fim = tsf_info['fim']\n            tsf_wim = tsf_info['wim']\n            tsf_cond = tsf_info['cond']\n\n            T = self.render.cal_bc_transform(src_f2p, tsf_fim, tsf_wim)\n            tsf_img = F.grid_sample(src_info['img'], T)\n            tsf_inputs = torch.cat([tsf_img, tsf_cond], dim=1)\n\n            tsf_color, tsf_mask = self.generator.inference(\n                src_encoder_outs, src_resnet_outs, tsf_inputs, T)\n\n            preds = src_info['bg'] * tsf_mask + tsf_color * (1 - tsf_mask)\n\n            if self._opt.front_warp:\n                preds = self.warp(preds, tsf_img, tsf_fim, tsf_mask)\n\n            return preds, T, tsf_inputs\n\n        @torch.no_grad()\n        def set_inputs(src_info, tsf_info):\n            s2t_init_preds, s2t_T, s2t_tsf_inputs = initialize(src_info, tsf_info)\n            t2s_init_preds, t2s_T, t2s_tsf_inputs = initialize(tsf_info, src_info)\n\n            s2t_j2d = torch.cat([src_info['j2d'], tsf_info['j2d']], dim=0)\n            t2s_j2d = torch.cat([tsf_info['j2d'], src_info['j2d']], dim=0)\n            j2ds = torch.stack([s2t_j2d, t2s_j2d], dim=0)\n\n            init_preds = torch.cat([s2t_init_preds, t2s_init_preds], dim=0)\n            images = torch.cat([src_info['img'], tsf_info['img']], dim=0)\n            T = torch.cat([s2t_T, t2s_T], dim=0)\n            T_cycle = torch.cat([t2s_T, s2t_T], dim=0)\n            tsf_inputs = torch.cat([s2t_tsf_inputs, t2s_tsf_inputs], dim=0)\n            src_fim = torch.cat([src_info['fim'], tsf_info['fim']], dim=0)\n            tsf_fim = torch.cat([tsf_info['fim'], src_info['fim']], dim=0)\n\n            s2t_inputs = src_info['src_inputs']\n            t2s_inputs = tsf_info['src_inputs']\n\n            src_inputs = torch.cat([s2t_inputs, t2s_inputs], dim=0)\n\n            src_mask = util.morph(src_inputs[:, -1:, ], ks=self._opt.ft_ks, mode='erode')\n            tsf_mask = util.morph(tsf_inputs[:, -1:, ], ks=self._opt.ft_ks, mode='erode')\n\n            pseudo_masks = torch.cat([src_mask, tsf_mask], dim=0)\n\n            return src_fim, tsf_fim, j2ds, T, T_cycle, src_inputs, tsf_inputs, images, init_preds, pseudo_masks\n\n        def set_cycle_inputs(fake_tsf_imgs, src_inputs, tsf_inputs, T_cycle):\n            # set cycle bg inputs\n            tsf_bg_mask = tsf_inputs[:, -1:, ...]\n\n            # set cycle src inputs\n            cycle_src_inputs = torch.cat([fake_tsf_imgs * tsf_bg_mask, tsf_inputs[:, 3:]], dim=1)\n\n            # set cycle tsf inputs\n            cycle_tsf_img = F.grid_sample(fake_tsf_imgs, T_cycle)\n            cycle_tsf_inputs = torch.cat([cycle_tsf_img, src_inputs[:, 3:]], dim=1)\n\n            return cycle_src_inputs, cycle_tsf_inputs\n\n        def inference(bg, src_inputs, tsf_inputs, T, T_cycle, src_fim, tsf_fim):\n            fake_src_color, fake_src_mask, fake_tsf_color, fake_tsf_mask = \\\n                self.generator.infer_front(src_inputs, tsf_inputs, T=T)\n            fake_src_imgs = fake_src_mask * bg + (1 - fake_src_mask) * fake_src_color\n            fake_tsf_imgs = fake_tsf_mask * bg + (1 - fake_tsf_mask) * fake_tsf_color\n\n            if self._opt.front_warp:\n                fake_tsf_imgs = self.warp(fake_tsf_imgs, tsf_inputs[:, 0:3], tsf_fim, fake_tsf_mask)\n\n            cycle_src_inputs, cycle_tsf_inputs = set_cycle_inputs(\n                fake_tsf_imgs, src_inputs, tsf_inputs, T_cycle)\n\n            cycle_src_color, cycle_src_mask, cycle_tsf_color, cycle_tsf_mask = \\\n                self.generator.infer_front(cycle_src_inputs, cycle_tsf_inputs, T=T_cycle)\n\n            cycle_src_imgs = cycle_src_mask * bg + (1 - cycle_src_mask) * cycle_src_color\n            cycle_tsf_imgs = cycle_tsf_mask * bg + (1 - cycle_tsf_mask) * cycle_tsf_color\n\n            if self._opt.front_warp:\n                cycle_tsf_imgs = self.warp(cycle_tsf_imgs, src_inputs[:, 0:3], src_fim, fake_src_mask)\n\n            return fake_src_imgs, fake_tsf_imgs, cycle_src_imgs, cycle_tsf_imgs, fake_src_mask, fake_tsf_mask, cycle_tsf_inputs\n\n        def create_criterion():\n            face_criterion = FaceLoss(pretrained_path=self._opt.face_model).cuda()\n            idt_criterion = torch.nn.L1Loss()\n            mask_criterion = torch.nn.BCELoss()\n\n            return face_criterion, idt_criterion, mask_criterion\n\n        def print_losses(*args, **kwargs):\n\n            print('step = {}'.format(kwargs['step']))\n            for key, value in kwargs.items():\n                if key == 'step':\n                    continue\n                print('\\t{}, {:.6f}'.format(key, value.item()))\n\n        def update_learning_rate(optimizer, current_lr, init_lr, final_lr, nepochs_decay):\n            # updated learning rate G\n            lr_decay = (init_lr - final_lr) / nepochs_decay\n            current_lr -= lr_decay\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = current_lr\n            # print('update G learning rate: %f -> %f' % (current_lr + lr_decay, current_lr))\n            return current_lr\n\n        init_lr = 0.0002\n        cur_lr = init_lr\n        final_lr = 0.00001\n        fix_iters = 25\n        total_iters = 50\n        # fix_iters = int(50 / bs)\n        # total_iters = int(100 / bs)\n        optimizer = torch.optim.Adam(self.generator.parameters(), lr=init_lr, betas=(0.5, 0.999))\n        face_cri, idt_cri, msk_cri = create_criterion()\n\n        # set up inputs\n        src_fim, tsf_fim, j2ds, T, T_cycle, src_inputs, tsf_inputs, \\\n        src_imgs, init_preds, pseudo_masks = set_inputs(\n            src_info=self.src_info, tsf_info=self.tsf_info\n        )\n\n        logger = tqdm(range(total_iters))\n        for step in logger:\n            if bs == 1:\n                i = step % 2\n                _bg = init_bg[i][None]\n                _init_preds = init_preds[i][None]\n                _src_imgs = src_imgs[i][None]\n                _src_inputs = src_inputs[i][None]\n                _tsf_inputs = tsf_inputs[i][None]\n                _T = T[i][None]\n                _T_cycle = T_cycle[i][None]\n                _src_fim = src_fim[i][None]\n                _tsf_fim = tsf_fim[i][None]\n\n                _pseudo_masks = pseudo_masks[i:4:2]\n            else:\n                _bg = init_bg\n                _src_imgs = src_imgs\n                _init_preds = init_preds\n                _pseudo_masks = pseudo_masks\n                _src_inputs, _tsf_inputs, _T, _T_cycle, _src_fim, _tsf_fim = \\\n                    src_inputs, tsf_inputs, T, T_cycle, src_fim, tsf_fim\n            fake_src_imgs, fake_tsf_imgs, cycle_src_imgs, cycle_tsf_imgs, fake_src_mask, fake_tsf_mask, \\\n            cycle_tsf_inputs = inference(_bg, _src_inputs, _tsf_inputs, _T, _T_cycle, _src_fim, _tsf_fim)\n\n            # cycle reconstruction loss\n            cycle_loss = idt_cri(_src_imgs, fake_src_imgs) + idt_cri(_src_imgs, cycle_tsf_imgs)\n\n            # structure loss\n            bg_mask = _src_inputs[:, -1:]\n            body_mask = 1.0 - bg_mask\n            str_src_imgs = _src_imgs * body_mask\n            cycle_warp_imgs = cycle_tsf_inputs[:, 0:3]\n\n            struct_loss = idt_cri(_init_preds, fake_tsf_imgs) + \\\n                          2 * idt_cri(str_src_imgs, cycle_warp_imgs)\n\n            fid_loss = face_cri(_src_imgs, cycle_tsf_imgs, kps1=j2ds[:, 0], kps2=j2ds[:, 0]) + \\\n                       face_cri(_tsf_inputs[:, 0:3], fake_tsf_imgs, kps1=j2ds[:, 1], kps2=j2ds[:, 1])\n\n            # mask loss\n            # mask_loss = msk_cri(fake_tsf_mask, tsf_inputs[:, -1:]) + msk_cri(fake_src_mask, src_inputs[:, -1:])\n            mask_loss = msk_cri(torch.cat([fake_src_mask, fake_tsf_mask], dim=0), _pseudo_masks)\n\n            loss = 10 * cycle_loss + 10 * struct_loss + fid_loss + 5 * mask_loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if verbose:\n                logger.set_description(\n                    (\n                        f'step: {step}; '\n                        f'total: {loss.item():.6f}; cyc: {cycle_loss.item():.6f}; '\n                        f'str: {struct_loss.item():.6f}; fid: {fid_loss.item():.6f}; '\n                        f'msk: {mask_loss.item():.6f}'\n                    )\n                )\n\n            if step % 10 == 0:\n                self.visualize(visualizer, input_imgs=src_imgs, tsf_imgs=fake_tsf_imgs,\n                               cyc_imgs=cycle_tsf_imgs, fake_tsf_mask=fake_tsf_mask,\n                               init_preds=init_preds,\n                               str_src_imgs=str_src_imgs,\n                               cycle_warp_imgs=cycle_warp_imgs)\n\n            if step > fix_iters:\n                cur_lr = update_learning_rate(optimizer, cur_lr, init_lr, final_lr, fix_iters)\n\n        self.generator.eval()\n\n    # def post_personalize_previous(self, out_dir, visualizer, verbose=True):\n    #     from networks.networks import FaceLoss\n    #     bs = 2 if self._opt.batch_size > 1 else 1\n    #\n    #     init_bg = torch.cat([self.src_info['bg'], self.tsf_info['bg']], dim=0)\n    #\n    #     @torch.no_grad()\n    #     def initialize(src_info, tsf_info):\n    #         src_encoder_outs, src_resnet_outs = src_info['feats']\n    #         src_f2p = src_info['p2verts']\n    #\n    #         tsf_fim = tsf_info['fim']\n    #         tsf_wim = tsf_info['wim']\n    #         tsf_cond = tsf_info['cond']\n    #\n    #         T = self.render.cal_bc_transform(src_f2p, tsf_fim, tsf_wim)\n    #         tsf_img = F.grid_sample(src_info['img'], T)\n    #         tsf_inputs = torch.cat([tsf_img, tsf_cond], dim=1)\n    #\n    #         tsf_color, tsf_mask = self.generator.inference(\n    #             src_encoder_outs, src_resnet_outs, tsf_inputs, T)\n    #\n    #         preds = src_info['bg'] * tsf_mask + tsf_color * (1 - tsf_mask)\n    #\n    #         if self._opt.front_warp:\n    #             preds = self.warp(preds, tsf_img, tsf_fim, tsf_mask)\n    #\n    #         return preds, T, tsf_inputs\n    #\n    #     @torch.no_grad()\n    #     def set_inputs(src_info, tsf_info):\n    #         s2t_init_preds, s2t_T, s2t_tsf_inputs = initialize(src_info, tsf_info)\n    #         t2s_init_preds, t2s_T, t2s_tsf_inputs = initialize(tsf_info, src_info)\n    #\n    #         s2t_j2d = torch.cat([src_info['j2d'], tsf_info['j2d']], dim=0)\n    #         t2s_j2d = torch.cat([tsf_info['j2d'], src_info['j2d']], dim=0)\n    #         j2ds = torch.stack([s2t_j2d, t2s_j2d], dim=0)\n    #\n    #         init_preds = torch.cat([s2t_init_preds, t2s_init_preds], dim=0)\n    #         images = torch.cat([src_info['img'], tsf_info['img']], dim=0)\n    #         T = torch.cat([s2t_T, t2s_T], dim=0)\n    #         T_cycle = torch.cat([t2s_T, s2t_T], dim=0)\n    #         tsf_inputs = torch.cat([s2t_tsf_inputs, t2s_tsf_inputs], dim=0)\n    #         src_fim = torch.cat([src_info['fim'], tsf_info['fim']], dim=0)\n    #         tsf_fim = torch.cat([tsf_info['fim'], src_info['fim']], dim=0)\n    #\n    #         s2t_inputs = src_info['src_inputs']\n    #         t2s_inputs = tsf_info['src_inputs']\n    #\n    #         src_inputs = torch.cat([s2t_inputs, t2s_inputs], dim=0)\n    #\n    #         src_mask = util.morph(src_inputs[:, -1:, ], ks=self._opt.ft_ks, mode='erode')\n    #         tsf_mask = util.morph(tsf_inputs[:, -1:, ], ks=self._opt.ft_ks, mode='erode')\n    #\n    #         pseudo_masks = torch.cat([src_mask, tsf_mask], dim=0)\n    #\n    #         return src_fim, tsf_fim, j2ds, T, T_cycle, src_inputs, tsf_inputs, images, init_preds, pseudo_masks\n    #\n    #     def set_cycle_inputs(fake_tsf_imgs, src_inputs, tsf_inputs, T_cycle):\n    #         # set cycle bg inputs\n    #         tsf_bg_mask = tsf_inputs[:, -1:, ...]\n    #\n    #         # set cycle src inputs\n    #         cycle_src_inputs = torch.cat([fake_tsf_imgs * tsf_bg_mask, tsf_inputs[:, 3:]], dim=1)\n    #\n    #         # set cycle tsf inputs\n    #         cycle_tsf_img = F.grid_sample(fake_tsf_imgs, T_cycle)\n    #         cycle_tsf_inputs = torch.cat([cycle_tsf_img, src_inputs[:, 3:]], dim=1)\n    #\n    #         return cycle_src_inputs, cycle_tsf_inputs\n    #\n    #     def inference(src_inputs, tsf_inputs, T, T_cycle, src_fim, tsf_fim):\n    #         fake_src_color, fake_src_mask, fake_tsf_color, fake_tsf_mask = \\\n    #             self.generator.infer_front(src_inputs, tsf_inputs, T=T)\n    #\n    #         fake_src_imgs = fake_src_mask * init_bg + (1 - fake_src_mask) * fake_src_color\n    #         fake_tsf_imgs = fake_tsf_mask * init_bg + (1 - fake_tsf_mask) * fake_tsf_color\n    #\n    #         if self._opt.front_warp:\n    #             fake_tsf_imgs = self.warp(fake_tsf_imgs, tsf_inputs[:, 0:3], tsf_fim, fake_tsf_mask)\n    #\n    #         cycle_src_inputs, cycle_tsf_inputs = set_cycle_inputs(\n    #             fake_tsf_imgs, src_inputs, tsf_inputs, T_cycle)\n    #\n    #         cycle_src_color, cycle_src_mask, cycle_tsf_color, cycle_tsf_mask = \\\n    #             self.generator.infer_front(cycle_src_inputs, cycle_tsf_inputs, T=T_cycle)\n    #\n    #         cycle_src_imgs = cycle_src_mask * init_bg + (1 - cycle_src_mask) * cycle_src_color\n    #         cycle_tsf_imgs = cycle_tsf_mask * init_bg + (1 - cycle_tsf_mask) * cycle_tsf_color\n    #\n    #         if self._opt.front_warp:\n    #             cycle_tsf_imgs = self.warp(cycle_tsf_imgs, src_inputs[:, 0:3], src_fim, fake_src_mask)\n    #\n    #         return fake_src_imgs, fake_tsf_imgs, cycle_src_imgs, cycle_tsf_imgs, fake_src_mask, fake_tsf_mask, cycle_tsf_inputs\n    #\n    #     def create_criterion():\n    #         face_criterion = FaceLoss(pretrained_path=self._opt.face_model).cuda()\n    #         idt_criterion = torch.nn.L1Loss()\n    #         mask_criterion = torch.nn.BCELoss()\n    #\n    #         return face_criterion, idt_criterion, mask_criterion\n    #\n    #     def print_losses(*args, **kwargs):\n    #\n    #         print('step = {}'.format(kwargs['step']))\n    #         for key, value in kwargs.items():\n    #             if key == 'step':\n    #                 continue\n    #             print('\\t{}, {:.6f}'.format(key, value.item()))\n    #\n    #     def update_learning_rate(optimizer, current_lr, init_lr, final_lr, nepochs_decay):\n    #         # updated learning rate G\n    #         lr_decay = (init_lr - final_lr) / nepochs_decay\n    #         current_lr -= lr_decay\n    #         for param_group in optimizer.param_groups:\n    #             param_group['lr'] = current_lr\n    #         # print('update G learning rate: %f -> %f' % (current_lr + lr_decay, current_lr))\n    #         return current_lr\n    #\n    #     init_lr = 0.0002\n    #     cur_lr = init_lr\n    #     final_lr = 0.00001\n    #     fix_iters = 25\n    #     total_iters = 50\n    #     optimizer = torch.optim.Adam(self.generator.parameters(), lr=init_lr, betas=(0.5, 0.999))\n    #     face_cri, idt_cri, msk_cri = create_criterion()\n    #\n    #     # set up inputs\n    #     src_fim, tsf_fim, j2ds, T, T_cycle, src_inputs, tsf_inputs, \\\n    #     src_imgs, init_preds, pseudo_masks = set_inputs(\n    #         src_info=self.src_info, tsf_info=self.tsf_info\n    #     )\n    #\n    #     logger = tqdm(range(total_iters))\n    #     for step in logger:\n    #\n    #         fake_src_imgs, fake_tsf_imgs, cycle_src_imgs, cycle_tsf_imgs, \\\n    #         fake_src_mask, fake_tsf_mask, cycle_tsf_inputs = inference(src_inputs, tsf_inputs,\n    #                                                                    T, T_cycle, src_fim, tsf_fim)\n    #\n    #         # cycle reconstruction loss\n    #         cycle_loss = idt_cri(src_imgs, fake_src_imgs) + idt_cri(src_imgs, cycle_tsf_imgs)\n    #\n    #         # structure loss\n    #         bg_mask = src_inputs[:, -1:]\n    #         body_mask = 1.0 - bg_mask\n    #         str_src_imgs = src_imgs * body_mask\n    #         cycle_warp_imgs = cycle_tsf_inputs[:, 0:3]\n    #         # back_head_mask = 1 - self.render.encode_front_fim(tsf_fim, transpose=True, front_fn=False)\n    #         # struct_loss = idt_cri(init_preds, fake_tsf_imgs) + \\\n    #         #               2 * idt_cri(str_src_imgs * back_head_mask, cycle_warp_imgs * back_head_mask)\n    #\n    #         struct_loss = idt_cri(init_preds, fake_tsf_imgs) + \\\n    #                       2 * idt_cri(str_src_imgs, cycle_warp_imgs)\n    #\n    #         # fid_loss = face_cri(src_imgs, cycle_tsf_imgs, kps1=j2ds[:, 0], kps2=j2ds[:, 0]) + \\\n    #         #            face_cri(init_preds, fake_tsf_imgs, kps1=j2ds[:, 1], kps2=j2ds[:, 1])\n    #\n    #         fid_loss = face_cri(src_imgs, cycle_tsf_imgs, kps1=j2ds[:, 0], kps2=j2ds[:, 0]) + \\\n    #                    face_cri(tsf_inputs[:, 0:3], fake_tsf_imgs, kps1=j2ds[:, 1], kps2=j2ds[:, 1])\n    #\n    #         # mask loss\n    #         # mask_loss = msk_cri(fake_tsf_mask, tsf_inputs[:, -1:]) + msk_cri(fake_src_mask, src_inputs[:, -1:])\n    #         mask_loss = msk_cri(torch.cat([fake_src_mask, fake_tsf_mask], dim=0), pseudo_masks)\n    #\n    #         loss = 10 * cycle_loss + 10 * struct_loss + fid_loss + 5 * mask_loss\n    #         optimizer.zero_grad()\n    #         loss.backward()\n    #         optimizer.step()\n    #\n    #         # print_losses(step=step, total=loss, cyc=cycle_loss,\n    #         #              str=struct_loss, fid=fid_loss, msk=mask_loss)\n    #\n    #         if verbose:\n    #             logger.set_description(\n    #                 (\n    #                     f'step: {step}; '\n    #                     f'total: {loss.item():.6f}; cyc: {cycle_loss.item():.6f}; '\n    #                     f'str: {struct_loss.item():.6f}; fid: {fid_loss.item():.6f}; '\n    #                     f'msk: {mask_loss.item():.6f}'\n    #                 )\n    #             )\n    #\n    #         if step % 10 == 0:\n    #             self.visualize(visualizer, input_imgs=src_imgs, tsf_imgs=fake_tsf_imgs,\n    #                            cyc_imgs=cycle_tsf_imgs, fake_tsf_mask=fake_tsf_mask,\n    #                            init_preds=init_preds,\n    #                            str_src_imgs=str_src_imgs,\n    #                            cycle_warp_imgs=cycle_warp_imgs)\n    #\n    #         if step > fix_iters:\n    #             cur_lr = update_learning_rate(optimizer, cur_lr, init_lr, final_lr, fix_iters)\n    #\n    #     self.generator.eval()\n"""
models/viewer.py,33,"b""import os\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom .models import BaseModel\nfrom networks.networks import NetworksFactory, HumanModelRecovery\nfrom utils.nmr import SMPLRenderer\nfrom utils.detectors import PersonMaskRCNNDetector\nimport utils.cv_utils as cv_utils\nimport utils.util as util\n\n\nclass Viewer(BaseModel):\n\n    def __init__(self, opt):\n        super(Viewer, self).__init__(opt)\n        self._name = 'Viewer'\n\n        self._create_networks()\n\n        # prefetch variables\n        self.src_info = None\n        self.tsf_info = None\n        self.first_cam = None\n\n    def _create_networks(self):\n        # 0. create generator\n        self.generator = self._create_generator().cuda()\n\n        # 0. create bgnet\n        if self._opt.bg_model != 'ORIGINAL':\n            self.bgnet = self._create_bgnet().cuda()\n        else:\n            self.bgnet = self.generator.bg_model\n\n        # 2. create hmr\n        self.hmr = self._create_hmr().cuda()\n\n        # 3. create render\n        self.render = SMPLRenderer(image_size=self._opt.image_size, tex_size=self._opt.tex_size,\n                                   has_front=self._opt.front_warp, fill_back=False).cuda()\n        # 4. pre-processor\n        if self._opt.has_detector:\n            self.detector = PersonMaskRCNNDetector(ks=self._opt.bg_ks, threshold=0.5, to_gpu=True)\n        else:\n            self.detector = None\n\n    def _create_bgnet(self):\n        net = NetworksFactory.get_by_name('deepfillv2', c_dim=4)\n        self._load_params(net, self._opt.bg_model, need_module=False)\n        net.eval()\n        return net\n\n    def _create_generator(self):\n        net = NetworksFactory.get_by_name(self._opt.gen_name, bg_dim=4, src_dim=3+self._G_cond_nc,\n                                          tsf_dim=3+self._G_cond_nc, repeat_num=self._opt.repeat_num)\n\n        if self._opt.load_path:\n            self._load_params(net, self._opt.load_path)\n        elif self._opt.load_epoch > 0:\n            self._load_network(net, 'G', self._opt.load_epoch)\n        else:\n            raise ValueError('load_path {} is empty and load_epoch {} is 0'.format(\n                self._opt.load_path, self._opt.load_epoch))\n\n        net.eval()\n        return net\n\n    def _create_hmr(self):\n        hmr = HumanModelRecovery(self._opt.smpl_model)\n        saved_data = torch.load(self._opt.hmr_model)\n        hmr.load_state_dict(saved_data)\n        hmr.eval()\n        return hmr\n\n    def visualize(self, *args, **kwargs):\n        visualizer = args[0]\n        if visualizer is not None:\n            for key, value in kwargs.items():\n                visualizer.vis_named_img(key, value)\n\n    @torch.no_grad()\n    def personalize(self, src_path, src_smpl=None, output_path='', visualizer=None):\n\n        ori_img = cv_utils.read_cv2_img(src_path)\n\n        # resize image and convert the color space from [0, 255] to [-1, 1]\n        img = cv_utils.transform_img(ori_img, self._opt.image_size, transpose=True) * 2 - 1.0\n        img = torch.tensor(img, dtype=torch.float32).cuda()[None, ...]\n\n        if src_smpl is None:\n            img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n            img_hmr = torch.tensor(img_hmr, dtype=torch.float32).cuda()[None, ...]\n            src_smpl = self.hmr(img_hmr)\n        else:\n            src_smpl = torch.tensor(src_smpl, dtype=torch.float32).cuda()[None, ...]\n\n        # source process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n        src_info = self.hmr.get_details(src_smpl)\n        src_f2verts, src_fim, src_wim = self.render.render_fim_wim(src_info['cam'], src_info['verts'])\n        # src_f2pts = src_f2verts[:, :, :, 0:2]\n        src_info['fim'] = src_fim\n        src_info['wim'] = src_wim\n        src_info['cond'], _ = self.render.encode_fim(src_info['cam'], src_info['verts'], fim=src_fim, transpose=True)\n        src_info['f2verts'] = src_f2verts\n        src_info['p2verts'] = src_f2verts[:, :, :, 0:2]\n        src_info['p2verts'][:, :, :, 1] *= -1\n\n        if self._opt.only_vis:\n            src_info['p2verts'] = self.render.get_vis_f2pts(src_info['p2verts'], src_fim)\n        # add image to source info\n        src_info['img'] = img\n        src_info['image'] = ori_img\n\n        # 2. process the src inputs\n        if self.detector is not None:\n            bbox, body_mask = self.detector.inference(img[0])\n            bg_mask = 1 - body_mask\n        else:\n            bg_mask = util.morph(src_info['cond'][:, -1:, :, :], ks=self._opt.bg_ks, mode='erode')\n            body_mask = 1 - bg_mask\n\n        if self._opt.bg_model != 'ORIGINAL':\n            src_info['bg'] = self.bgnet(img, masks=body_mask, only_x=True)\n        else:\n            incomp_img = img * bg_mask\n            bg_inputs = torch.cat([incomp_img, bg_mask], dim=1)\n            img_bg = self.bgnet(bg_inputs)\n            src_info['bg'] = bg_inputs[:, 0:3] + img_bg * bg_inputs[:, -1:]\n\n        ft_mask = 1 - util.morph(src_info['cond'][:, -1:, :, :], ks=self._opt.ft_ks, mode='erode')\n        src_inputs = torch.cat([img * ft_mask, src_info['cond']], dim=1)\n\n        src_info['feats'] = self.generator.encode_src(src_inputs)\n\n        self.src_info = src_info\n\n        if visualizer is not None:\n            visualizer.vis_named_img('src', img)\n            visualizer.vis_named_img('bg', src_info['bg'])\n\n        if output_path:\n            cv_utils.save_cv2_img(src_info['image'], output_path, image_size=self._opt.image_size)\n\n    @torch.no_grad()\n    def _extract_smpls(self, input_file):\n        img = cv_utils.read_cv2_img(input_file)\n        img = cv_utils.transform_img(img, image_size=224) * 2 - 1.0  # hmr receive [-1, 1]\n        img = img.transpose((2, 0, 1))\n        img = torch.tensor(img, dtype=torch.float32).cuda()[None, ...]\n        theta = self.hmr(img)[-1]\n\n        return theta\n\n    @torch.no_grad()\n    def inference(self, tgt_paths, tgt_smpls=None, cam_strategy='smooth', output_dir='', visualizer=None, verbose=True):\n        length = len(tgt_paths)\n\n        outputs = []\n        bg_img = self.src_info['bg']\n        src_encoder_outs, src_resnet_outs = self.src_info['feats']\n\n        process_bar = tqdm(range(length)) if verbose else range(length)\n        for t in process_bar:\n            tgt_path = tgt_paths[t]\n            tgt_smpl = tgt_smpls[t] if tgt_smpls is not None else None\n\n            tsf_inputs = self.transfer_params(tgt_path, tgt_smpl, cam_strategy, t=t)\n\n            tsf_color, tsf_mask = self.generator.inference(src_encoder_outs, src_resnet_outs,\n                                                           tsf_inputs, self.tsf_info['T'])\n            preds = tsf_mask * bg_img + (1 - tsf_mask) * tsf_color\n\n            if self._opt.front_warp:\n                preds = self.warp_front(preds, self.tsf_info['tsf_img'], self.tsf_info['fim'], tsf_mask)\n\n            if visualizer is not None:\n                gt = cv_utils.transform_img(self.tsf_info['image'], image_size=self._opt.image_size, transpose=True)\n                visualizer.vis_named_img('pred_' + cam_strategy, preds)\n                visualizer.vis_named_img('gt', gt[None, ...], denormalize=False)\n\n            preds = preds[0].permute(1, 2, 0)\n            preds = preds.cpu().numpy()\n            outputs.append(preds)\n\n            if output_dir:\n                filename = os.path.split(tgt_path)[-1]\n\n                cv_utils.save_cv2_img(preds, os.path.join(output_dir, 'pred_' + filename), normalize=True)\n                cv_utils.save_cv2_img(self.tsf_info['image'], os.path.join(output_dir, 'gt_' + filename),\n                                      image_size=self._opt.image_size)\n\n        return outputs\n\n    @torch.no_grad()\n    def swap_smpl(self, src_cam, src_shape, tgt_smpl, cam_strategy='smooth'):\n        tgt_cam = tgt_smpl[:, 0:3].contiguous()\n        pose = tgt_smpl[:, 3:75].contiguous()\n\n        # TODO, need more tricky ways\n        if cam_strategy == 'smooth':\n\n            cam = src_cam.clone()\n            delta_xy = tgt_cam[:, 1:] - self.first_cam[:, 1:]\n            cam[:, 1:] += delta_xy\n\n        elif cam_strategy == 'source':\n            cam = src_cam\n        else:\n            cam = tgt_cam\n\n        tsf_smpl = torch.cat([cam, pose, src_shape], dim=1)\n\n        return tsf_smpl\n\n    @torch.no_grad()\n    def transfer_params(self, tgt_path, tgt_smpl=None, cam_strategy='smooth', t=0):\n        # get source info\n        src_info = self.src_info\n\n        ori_img = cv_utils.read_cv2_img(tgt_path)\n        if tgt_smpl is None:\n            img_hmr = cv_utils.transform_img(ori_img, 224, transpose=True) * 2 - 1.0\n            img_hmr = torch.tensor(img_hmr, dtype=torch.float32).cuda()[None, ...]\n            tgt_smpl = self.hmr(img_hmr)\n        else:\n            tgt_smpl = torch.tensor(tgt_smpl, dtype=torch.float32).cuda()[None, ...]\n\n        if t == 0 and cam_strategy == 'smooth':\n            self.first_cam = tgt_smpl[:, 0:3].clone()\n\n        # get transfer smpl\n        tsf_smpl = self.swap_smpl(src_info['cam'], src_info['shape'], tgt_smpl, cam_strategy=cam_strategy)\n        # transfer process, {'theta', 'cam', 'pose', 'shape', 'verts', 'j2d', 'j3d'}\n        tsf_info = self.hmr.get_details(tsf_smpl)\n\n        tsf_f2verts, tsf_fim, tsf_wim = self.render.render_fim_wim(tsf_info['cam'], tsf_info['verts'])\n        # src_f2pts = src_f2verts[:, :, :, 0:2]\n        tsf_info['fim'] = tsf_fim\n        tsf_info['wim'] = tsf_wim\n        tsf_info['cond'], _ = self.render.encode_fim(tsf_info['cam'], tsf_info['verts'], fim=tsf_fim, transpose=True)\n        # tsf_info['sil'] = util.morph((tsf_fim != -1).float(), ks=self._opt.ft_ks, mode='dilate')\n\n        T = self.render.cal_bc_transform(src_info['p2verts'], tsf_fim, tsf_wim)\n        tsf_img = F.grid_sample(src_info['img'], T)\n        tsf_inputs = torch.cat([tsf_img, tsf_info['cond']], dim=1)\n\n        # add target image to tsf info\n        tsf_info['tsf_img'] = tsf_img\n        tsf_info['image'] = ori_img\n        tsf_info['T'] = T\n\n        self.T = T\n        self.tsf_info = tsf_info\n\n        return tsf_inputs\n\n    def warp_front(self, preds, tsf_img, fim, mask):\n        front_mask = self.render.encode_front_fim(fim, transpose=True, front_fn=True)\n        preds = (1 - front_mask) * preds + tsf_img * front_mask * (1 - mask)\n        # preds = torch.clamp(preds + self.tsf_info['tsf_img'] * front_mask, -1, 1)\n        return preds\n\n    def rotate_trans(self, rt, t, X):\n        R = cv_utils.euler2matrix(rt)    # (3 x 3)\n\n        R = torch.FloatTensor(R)[None, :, :].cuda()\n        t = torch.FloatTensor(t)[None, None, :].cuda()\n\n        # (bs, Nv, 3) + (bs, 1, 3)\n        return torch.bmm(X, R) + t\n\n    def view(self, rt, t, visualizer=None, name='1'):\n\n        src_info = self.src_info\n        src_mesh = self.src_info['verts']\n        tsf_mesh = self.rotate_trans(rt, t, src_mesh)\n\n        tsf_f2verts, tsf_fim, tsf_wim = self.render.render_fim_wim(src_info['cam'], tsf_mesh)\n        tsf_cond, _ = self.render.encode_fim(src_info['cam'], tsf_mesh, fim=tsf_fim, transpose=True)\n\n        T = self.render.cal_bc_transform(src_info['p2verts'], tsf_fim, tsf_wim)\n        tsf_img = F.grid_sample(src_info['img'], T)\n        tsf_inputs = torch.cat([tsf_img, tsf_cond], dim=1)\n\n        if not self._opt.bg_replace:\n            bg = torch.zeros_like(src_info['bg'])\n        else:\n            bg = src_info['bg']\n        preds, tsf_mask = self.forward(tsf_inputs, src_info['feats'], T, bg)\n\n        if self._opt.front_warp:\n            preds = self.warp_front(preds, tsf_img, tsf_fim, tsf_mask)\n\n        if visualizer is not None:\n            # self.render.set_ambient_light()\n            # textures = self.render.debug_textures()[None, ...].cuda()\n            # src_mesh, _ = self.render.render(src_info['cam'], src_X, textures, reverse_yz=True, get_fim=False)\n            # tsf_mesh, _ = self.render.render(src_info['cam'], tsf_X, textures, reverse_yz=True, get_fim=False)\n\n            visualizer.vis_named_img('src_img', src_info['img'])\n            visualizer.vis_named_img('pred_' + name, preds)\n            visualizer.vis_named_img('cond_' + name, tsf_cond)\n\n        return preds\n\n    def forward(self, tsf_inputs, feats, T, bg):\n        # generate fake images\n        src_encoder_outs, src_resnet_outs = feats\n\n        tsf_color, tsf_mask = self.generator.inference(src_encoder_outs, src_resnet_outs, tsf_inputs, T)\n        pred_imgs = tsf_mask * bg + (1 - tsf_mask) * tsf_color\n\n        return pred_imgs, tsf_mask\n\n    def post_personalize(self, out_dir, data_loader, visualizer, verbose=True):\n        from networks.networks import FaceLoss\n\n        bg_inpaint = self.src_info['bg']\n\n        @torch.no_grad()\n        def set_gen_inputs(sample):\n            j2ds = sample['j2d'].cuda()  # (N, 4)\n            T = sample['T'].cuda()  # (N, h, w, 2)\n            T_cycle = sample['T_cycle'].cuda()  # (N, h, w, 2)\n            src_inputs = sample['src_inputs'].cuda()  # (N, 6, h, w)\n            tsf_inputs = sample['tsf_inputs'].cuda()  # (N, 6, h, w)\n            src_fim = sample['src_fim'].cuda()\n            tsf_fim = sample['tsf_fim'].cuda()\n            init_preds = sample['preds'].cuda()\n            images = sample['images']\n            images = torch.cat([images[:, 0, ...], images[:, 1, ...]], dim=0).cuda()  # (2N, 3, h, w)\n            pseudo_masks = sample['pseudo_masks']\n            pseudo_masks = torch.cat([pseudo_masks[:, 0, ...], pseudo_masks[:, 1, ...]],\n                                     dim=0).cuda()  # (2N, 1, h, w)\n\n            return src_fim, tsf_fim, j2ds, T, T_cycle, \\\n                   src_inputs, tsf_inputs, images, init_preds, pseudo_masks\n\n        def set_cycle_inputs(fake_tsf_imgs, src_inputs, tsf_inputs, T_cycle):\n            # set cycle src inputs\n            cycle_src_inputs = torch.cat([fake_tsf_imgs * tsf_inputs[:, -1:, ...], tsf_inputs[:, 3:]], dim=1)\n\n            # set cycle tsf inputs\n            cycle_tsf_img = F.grid_sample(fake_tsf_imgs, T_cycle)\n            cycle_tsf_inputs = torch.cat([cycle_tsf_img, src_inputs[:, 3:]], dim=1)\n\n            return cycle_src_inputs, cycle_tsf_inputs\n\n        def warp(preds, tsf, fim, fake_tsf_mask):\n            front_mask = self.render.encode_front_fim(fim, transpose=True)\n            preds = (1 - front_mask) * preds + tsf * front_mask * (1 - fake_tsf_mask)\n            # preds = torch.clamp(preds + tsf * front_mask, -1, 1)\n            return preds\n\n        def inference(src_inputs, tsf_inputs, T, T_cycle, src_fim, tsf_fim):\n            fake_src_color, fake_src_mask, fake_tsf_color, fake_tsf_mask = \\\n                self.generator.infer_front(src_inputs, tsf_inputs, T=T)\n\n            fake_src_imgs = fake_src_mask * bg_inpaint + (1 - fake_src_mask) * fake_src_color\n            fake_tsf_imgs = fake_tsf_mask * bg_inpaint + (1 - fake_tsf_mask) * fake_tsf_color\n\n            if self._opt.front_warp:\n                fake_tsf_imgs = warp(fake_tsf_imgs, tsf_inputs[:, 0:3], tsf_fim, fake_tsf_mask)\n\n            cycle_src_inputs, cycle_tsf_inputs = set_cycle_inputs(\n                fake_tsf_imgs, src_inputs, tsf_inputs, T_cycle)\n\n            cycle_src_color, cycle_src_mask, cycle_tsf_color, cycle_tsf_mask = \\\n                self.generator.infer_front(cycle_src_inputs, cycle_tsf_inputs, T=T_cycle)\n\n            cycle_src_imgs = cycle_src_mask * bg_inpaint + (1 - cycle_src_mask) * cycle_src_color\n            cycle_tsf_imgs = cycle_tsf_mask * bg_inpaint + (1 - cycle_tsf_mask) * cycle_tsf_color\n\n            if self._opt.front_warp:\n                cycle_tsf_imgs = warp(cycle_tsf_imgs, src_inputs[:, 0:3], src_fim, fake_src_mask)\n\n            return fake_src_imgs, fake_tsf_imgs, cycle_src_imgs, cycle_tsf_imgs, fake_src_mask, fake_tsf_mask\n\n        def create_criterion():\n            face_criterion = FaceLoss(pretrained_path=self._opt.face_model).cuda()\n            idt_criterion = torch.nn.L1Loss()\n            mask_criterion = torch.nn.BCELoss()\n\n            return face_criterion, idt_criterion, mask_criterion\n\n        init_lr = 0.0002\n        nodecay_epochs = 5\n        optimizer = torch.optim.Adam(self.generator.parameters(), lr=init_lr, betas=(0.5, 0.999))\n        face_cri, idt_cri, msk_cri = create_criterion()\n\n        step = 0\n        logger = tqdm(range(nodecay_epochs))\n        for epoch in logger:\n            for i, sample in enumerate(data_loader):\n                src_fim, tsf_fim, j2ds, T, T_cycle, src_inputs, tsf_inputs, \\\n                images, init_preds, pseudo_masks = set_gen_inputs(sample)\n\n                # print(bg_inputs.shape, src_inputs.shape, tsf_inputs.shape)\n                bs = tsf_inputs.shape[0]\n                src_imgs = images[0:bs]\n                fake_src_imgs, fake_tsf_imgs, cycle_src_imgs, cycle_tsf_imgs, fake_src_mask, fake_tsf_mask = inference(\n                    src_inputs, tsf_inputs, T, T_cycle, src_fim, tsf_fim)\n\n                # cycle reconstruction loss\n                cycle_loss = idt_cri(src_imgs, fake_src_imgs) + idt_cri(src_imgs, cycle_tsf_imgs)\n\n                # structure loss\n                bg_mask = src_inputs[:, -1:]\n                body_mask = 1 - bg_mask\n                str_src_imgs = src_imgs * body_mask\n                cycle_warp_imgs = F.grid_sample(fake_tsf_imgs, T_cycle)\n                back_head_mask = 1 - self.render.encode_front_fim(tsf_fim, transpose=True, front_fn=False)\n                struct_loss = idt_cri(init_preds, fake_tsf_imgs) + \\\n                              2 * idt_cri(str_src_imgs * back_head_mask, cycle_warp_imgs * back_head_mask)\n\n                fid_loss = face_cri(src_imgs, cycle_tsf_imgs, kps1=j2ds[:, 0], kps2=j2ds[:, 0]) + \\\n                           face_cri(init_preds, fake_tsf_imgs, kps1=j2ds[:, 1], kps2=j2ds[:, 1])\n\n                # mask loss\n                # mask_loss = msk_cri(fake_tsf_mask, tsf_inputs[:, -1:]) + msk_cri(fake_src_mask, src_inputs[:, -1:])\n                mask_loss = msk_cri(torch.cat([fake_src_mask, fake_tsf_mask], dim=0), pseudo_masks)\n\n                loss = 10 * cycle_loss + 10 * struct_loss + fid_loss + 5 * mask_loss\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                if verbose:\n                    logger.set_description(\n                        (\n                            f'epoch: {epoch + 1}; step: {step}; '\n                            f'total: {loss.item():.6f}; cyc: {cycle_loss.item():.6f}; '\n                            f'str: {struct_loss.item():.6f}; fid: {fid_loss.item():.6f}; '\n                            f'msk: {mask_loss.item():.6f}'\n                        )\n                    )\n\n                if verbose and step % 5 == 0:\n                    self.visualize(visualizer, input_imgs=images, tsf_imgs=fake_tsf_imgs, cyc_imgs=cycle_tsf_imgs)\n\n                step += 1\n\n        self.generator.eval()\n"""
networks/__init__.py,0,b''
networks/baseline.py,6,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom .networks import NetworkBase\nimport torch\nimport ipdb\n\n\nclass ResidualBlock(nn.Module):\n    """"""Residual Block.""""""\n    def __init__(self, dim_in, dim_out):\n        super(ResidualBlock, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(dim_out, affine=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(dim_out, affine=True))\n\n    def forward(self, x):\n        return x + self.main(x)\n\n\nclass ResNetGenerator(NetworkBase):\n    """"""Generator. Encoder-Decoder Architecture.""""""\n    def __init__(self, conv_dim=64, c_dim=5, repeat_num=9, k_size=4, n_down=2):\n        super(ResNetGenerator, self).__init__()\n        self._name = \'resnet_generator\'\n\n        layers = []\n        layers.append(nn.Conv2d(c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.InstanceNorm2d(conv_dim, affine=True))\n        layers.append(nn.ReLU(inplace=True))\n\n        # Down-Sampling\n        curr_dim = conv_dim\n        for i in range(n_down):\n            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=k_size, stride=2, padding=1, bias=False))\n            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True))\n            layers.append(nn.ReLU(inplace=True))\n            curr_dim = curr_dim * 2\n\n        # Bottleneck\n        for i in range(repeat_num):\n            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n\n        # Up-Sampling\n        for i in range(n_down):\n            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=k_size, stride=2, padding=1, output_padding=1, bias=False))\n            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True))\n            layers.append(nn.ReLU(inplace=True))\n            curr_dim = curr_dim // 2\n\n        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.Tanh())\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, c=None):\n        if c is not None:\n            # replicate spatially and concatenate domain information\n            c = c.unsqueeze(2).unsqueeze(3)\n            c = c.expand(c.size(0), c.size(1), x.size(2), x.size(3))\n            x = torch.cat([x, c], dim=1)\n        return self.model(x)\n\n\nclass ResUnetGenerator(NetworkBase):\n    """"""Generator. Encoder-Decoder Architecture.""""""\n    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6, k_size=4, n_down=2):\n        super(ResUnetGenerator, self).__init__()\n        self._name = \'resunet_generator\'\n\n        self.repeat_num = repeat_num\n        self.n_down = n_down\n\n        encoders = []\n\n        encoders.append(nn.Sequential(\n            nn.Conv2d(c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False),\n            nn.InstanceNorm2d(conv_dim, affine=True),\n            nn.ReLU(inplace=True)\n        ))\n\n        # Down-Sampling\n        curr_dim = conv_dim\n        for i in range(n_down):\n            encoders.append(nn.Sequential(\n                nn.Conv2d(curr_dim, curr_dim*2, kernel_size=k_size, stride=2, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim*2, affine=True),\n                nn.ReLU(inplace=True)\n            ))\n\n            curr_dim = curr_dim * 2\n\n        self.encoders = nn.Sequential(*encoders)\n\n        # Bottleneck\n        resnets = []\n        for i in range(repeat_num):\n            resnets.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n\n        self.resnets = nn.Sequential(*resnets)\n\n        # Up-Sampling\n        decoders = []\n        skippers = []\n        for i in range(n_down):\n            decoders.append(nn.Sequential(\n                nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=k_size, stride=2, padding=1, output_padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim//2, affine=True),\n                nn.ReLU(inplace=True)\n            ))\n\n            skippers.append(nn.Sequential(\n                nn.Conv2d(curr_dim, curr_dim//2, kernel_size=k_size, stride=1, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim//2, affine=True),\n                nn.ReLU(inplace=True)\n            ))\n\n            curr_dim = curr_dim // 2\n\n        self.decoders = nn.Sequential(*decoders)\n        self.skippers = nn.Sequential(*skippers)\n\n        layers = []\n        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.Tanh())\n        self.img_reg = nn.Sequential(*layers)\n\n        layers = []\n        layers.append(nn.Conv2d(curr_dim, 1, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.Sigmoid())\n        self.attetion_reg = nn.Sequential(*layers)\n\n    def inference(self, x):\n        # encoder, 0, 1, 2, 3 -> [256, 128, 64, 32]\n        encoder_outs = self.encode(x)\n\n        # resnet, 32\n        resnet_outs = []\n        src_x = encoder_outs[-1]\n        for i in range(self.repeat_num):\n            src_x = self.resnets[i](src_x)\n            resnet_outs.append(src_x)\n\n        return encoder_outs, resnet_outs\n\n    def forward(self, x):\n\n        # encoder, 0, 1, 2, 3 -> [256, 128, 64, 32]\n        encoder_outs = self.encode(x)\n\n        # resnet, 32\n        resnet_outs = self.resnets(encoder_outs[-1])\n\n        # decoder, 0, 1, 2 -> [64, 128, 256]\n        d_out = self.decode(resnet_outs, encoder_outs)\n\n        img_outs, mask_outs = self.regress(d_out)\n        return img_outs, mask_outs\n\n    def encode(self, x):\n        e_out = self.encoders[0](x)\n\n        encoder_outs = [e_out]\n        for i in range(1, self.n_down + 1):\n            e_out = self.encoders[i](e_out)\n            encoder_outs.append(e_out)\n            # print(i, e_out.shape)\n        return encoder_outs\n\n    def decode(self, x, encoder_outs):\n        d_out = x\n        for i in range(self.n_down):\n            d_out = self.decoders[i](d_out)  # x * 2\n            skip = encoder_outs[self.n_down - 1 - i]\n            d_out = torch.cat([skip, d_out], dim=1)\n            d_out = self.skippers[i](d_out)\n            # print(i, d_out.shape)\n        return d_out\n\n    def regress(self, x):\n        return self.img_reg(x), self.attetion_reg(x)\n\n\nclass ConcatGenerator(NetworkBase):\n    """"""Generator. Encoder-Decoder Architecture.""""""\n    def __init__(self, bg_dim, src_dim, tsf_dim, conv_dim=64, repeat_num=6):\n        super(ConcatGenerator, self).__init__()\n        self._name = \'concat_generator\'\n\n        self.n_down = 3\n        self.repeat_num = repeat_num\n        # background generator\n        self.bg_model = ResNetGenerator(conv_dim=conv_dim, c_dim=bg_dim, repeat_num=repeat_num, k_size=3, n_down=self.n_down)\n\n        # transfer generator\n        self.tsf_model = ResUnetGenerator(conv_dim=conv_dim, c_dim=3 + src_dim + tsf_dim, repeat_num=repeat_num, k_size=3, n_down=self.n_down)\n\n    def forward(self, bg_inputs, inputs):\n\n        img_bg = self.bg_model(bg_inputs)\n\n        tsf_img, tsf_mask = self.tsf_model(inputs)\n\n        # print(front_rgb.shape, front_mask.shape)\n        return img_bg, tsf_img, tsf_mask\n\n    def inference(self, inputs):\n        tsf_img, tsf_mask = self.tsf_model(inputs)\n\n        # print(front_rgb.shape, front_mask.shape)\n        return tsf_img, tsf_mask\n\n\nif __name__ == \'__main__\':\n    imitator = ConcatGenerator(bg_dim=4, src_dim=3, tsf_dim=3, conv_dim=64, repeat_num=6)\n\n    bg_x = torch.rand(2, 4, 256, 256)\n    x = torch.rand(2, 9, 256, 256)\n\n    img_bg, tsf_img, tsf_mask = imitator(bg_x, x)\n\n    ipdb.set_trace()\n'"
networks/batch_smpl.py,45,"b'"""""" \nTensorflow SMPL implementation as batch.\nSpecify joint types:\n\'coco\': Returns COCO+ 19 joints\n\'lsp\': Returns H3.6M-LSP 14 joints\nNote: To get original smpl joints, use self.J_transformed\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom utils.util import load_pickle_file\n\nVERT_NOSE = 331\nVERT_EAR_L = 3485\nVERT_EAR_R = 6880\nVERT_EYE_L = 2802\nVERT_EYE_R = 6262\n\n\ndef batch_skew(vec, batch_size=None, device=""cpu""):\n    """"""\n    vec is N x 3, batch_size is int.\n\n    e.g. r = [rx, ry, rz]\n        skew(r) = [[ 0,    -rz,      ry],\n                   [ rz,     0,     -rx],\n                   [-ry,    rx,       0]]\n\n    returns N x 3 x 3. Skew_sym version of each matrix.\n    """"""\n\n    if batch_size is None:\n        batch_size = vec.shape[0]\n\n    col_inds = np.array([1, 2, 3, 5, 6, 7], dtype=np.int64)\n\n    # indices = torch.from_numpy(np.reshape(\n    #     np.reshape(np.arange(0, batch_size) * 9, [-1, 1]) + col_inds,\n    #     newshape=(-1,))).to(device).long()\n\n    # For better compatibility\xef\xbc\x8c since if indices is torch.tensor, it must be long dtype.\n    # For fixed index, np.ndarray might be better.\n    indices = np.reshape(np.reshape(\n        np.arange(0, batch_size) * 9, [-1, 1]) + col_inds, newshape=(-1, )).astype(np.int64)\n\n    updates = torch.stack(\n        [\n            -vec[:, 2], vec[:, 1], vec[:, 2],\n            -vec[:, 0], -vec[:, 1], vec[:, 0]\n        ],\n        dim=1\n    ).view(-1).to(device)\n\n    res = torch.zeros(batch_size * 9, dtype=vec.dtype).to(device)\n    res[indices] = updates\n    res = res.view(batch_size, 3, 3)\n\n    return res\n\n\ndef batch_rodrigues(theta, device=""cpu""):\n    """"""\n    Theta is N x 3\n\n    rodrigues (from cv2.rodrigues):\n    source: https://docs.opencv.org/3.0-beta/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html\n    input: r (3 x 1)\n    output: R (3 x 3)\n\n        angle = norm(r)\n        r = r / angle\n\n        skew(r) = [[ 0,    -rz,      ry],\n                   [ rz,     0,     -rx],\n                   [-ry,    rx,       0]]\n\n        R = cos(theta * eye(3) + (1 - cos(theta)) * r * r.T + sin(theta) *  skew(r)\n    """"""\n    batch_size = theta.shape[0]\n\n    # angle (batch_size, 1), r (batch_size, 3)\n    angle = torch.norm(theta + 1e-8, p=2, dim=1, keepdim=True)\n    r = torch.div(theta, angle)\n\n    # angle (batch_size, 1, 1), r (batch_size, 3, 1)\n    angle = angle.unsqueeze(-1)\n    r = r.unsqueeze(-1)\n\n    cos = torch.cos(angle)\n    sin = torch.sin(angle)\n\n    # outer (batch_size, 3, 3)\n    outer = torch.matmul(r, r.permute(0, 2, 1))\n    eyes = torch.eye(3, dtype=torch.float32).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n\n    R = cos * eyes + (1 - cos) * outer + sin * batch_skew(r, batch_size=batch_size, device=device)\n\n    return R\n\n\ndef batch_lrotmin(theta, device=""cpu""):\n    """""" NOTE: not used bc I want to reuse R and this is simple.\n    Output of this is used to compute joint-to-pose blend shape mapping.\n    Equation 9 in SMPL paper.\n\n\n    Args:\n      pose: `Tensor`, N x 72 vector holding the axis-angle rep of K joints.\n            This includes the global rotation so K=24\n\n    Returns\n      diff_vec : `Tensor`: N x 207 rotation matrix of 23=(K-1) joints with identity subtracted.,\n    """"""\n\n    # ignore global, N x 72\n    theta = theta[:, 3:]\n    # (N*23) x 3 x 3\n    # reshape = contiguous + view\n    Rs = batch_rodrigues(theta.reshape(-1, 3))\n    eye = torch.eye(3).to(torch.eye(3))\n    lrotmin = (Rs - eye).view(-1, 207)\n\n    return lrotmin\n\n\ndef batch_global_rigid_transformation(Rs, Js, parent, rotate_base=False, device=""cpu""):\n    """"""\n    Computes absolute joint locations given pose.\n\n    rotate_base: if True, rotates the global rotation by 90 deg in x axis.\n    if False, this is the original SMPL coordinate.\n\n    Args:\n      Rs: N x 24 x 3 x 3 rotation vector of K joints\n      Js: N x 24 x 3, joint locations before posing\n      parent: 24 holding the parent id for each index\n\n    Returns\n      new_J : `Tensor`: N x 24 x 3 location of absolute joints\n      A     : `Tensor`: N x 24 x 4 x 4 relative joint transformations for LBS.\n    """"""\n\n    N = Rs.shape[0]\n    if rotate_base:\n        # print(\'Flipping the SMPL coordinate frame!!!!\')\n        # rot_x = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]], dtype=Rs.dtype)\n        # rot_x = np.reshape(np.tile(rot_x, [N, 1]), (N, 3, 3))\n        # root_rotation = np.matmul(Rs[:, 0, :, :], rot_x)\n\n        rot_x = torch.from_numpy(np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]],\n                                          dtype=np.float32)).type(Rs.dtype).to(device)\n\n        # rot_x = torch.from_numpy(np.array([[-1, 0, 0], [0, 1, 0], [0, 0, -1]],\n        #                                   dtype=np.float32)).type(Rs.dtype).to(device)\n\n        rot_x = rot_x.repeat(N, 1).view(N, 3, 3)\n        root_rotation = torch.matmul(Rs[:, 0, :, :], rot_x)\n    else:\n        root_rotation = Rs[:, 0, :, :]\n\n    # Now Js is N x 24 x 3 x 1\n    Js = Js.unsqueeze(-1)\n\n    def make_A(R, t):\n        """"""\n        Composite homogeneous matrix.\n        Args:\n            R: N x 3 x 3 rotation matrix.\n            t: N x 3 x 1 translation vector.\n\n        Returns:\n            homogeneous matrix N x 4 x 4.\n        """"""\n\n        # # Rs is N x 3 x 3, ts is N x 3 x 1\n        # R_homo = np.pad(R, [[0, 0], [0, 1], [0, 0]], mode=\'constant\')\n        # t_homo = np.concatenate([t, np.ones((N, 1, 1))], 1)\n        # return np.concatenate([R_homo, t_homo], 2)\n\n        # Pad to (N, 4, 3)\n        R_homo = F.pad(R, (0, 0, 0, 1, 0, 0), mode=\'constant\', value=0)\n        # Concatenate to (N, 4, 1)\n        t_homo = torch.cat([t, torch.ones(N, 1, 1, dtype=Rs.dtype).to(device)], dim=1)\n        return torch.cat([R_homo, t_homo], dim=2)\n\n    # root_rotation: (N, 3, 3), Js[:, 0]: (N, 3, 1)\n    # ---------- A0: (N, 4, 4)\n    A0 = make_A(root_rotation, Js[:, 0])\n    results = [A0]\n    for i in range(1, parent.shape[0]):\n        j_here = Js[:, i] - Js[:, parent[i]]\n        A_here = make_A(Rs[:, i], j_here)\n        res_here = torch.matmul(results[parent[i]], A_here)\n        results.append(res_here)\n\n    # N x 24 x 4 x 4\n    results = torch.stack(results, dim=1)\n\n    new_J = results[:, :, :3, 3]\n\n    # --- Compute relative A: Skinning is based on\n    # how much the bone moved (not the final location of the bone)\n    # but (final_bone - init_bone)\n    # ---\n\n    # Js_w0: (N, 24, 4, 1)\n    Js_w0 = torch.cat([Js, torch.zeros(N, 24, 1, 1, dtype=Rs.dtype).to(device)], dim=2)\n\n    # init_bone: (N, 24, 4, 1) = (N, 24, 4, 4) x (N, 24, 4, 1)\n    init_bone = torch.matmul(results, Js_w0)\n    # Append empty 4 x 3:\n    init_bone = F.pad(init_bone, (3, 0, 0, 0, 0, 0, 0, 0), mode=\'constant\', value=0)\n    A = results - init_bone\n\n    return new_J, A\n\n\ndef batch_orth_proj_idrot(X, camera, device=""cpu""):\n    """"""\n    X is N x num_points x 3\n    camera is N x 3\n    same as applying orth_proj_idrot to each N\n    """"""\n\n    # TODO check X dim size.\n\n    # X_trans is (N, num_points, 2)\n    X_trans = X[:, :, :2] + camera[:, None, 1:]\n    return camera[:, None, 0:1] * X_trans\n\n\nclass SMPL(nn.Module):\n    def __init__(self, pkl_path, rotate=False):\n        """"""\n        pkl_path is the path to a SMPL model\n        """"""\n        super(SMPL, self).__init__()\n        self.rotate = rotate\n\n        # -- Load SMPL params --\n        dd = load_pickle_file(pkl_path)\n\n        # define faces\n        # self.register_buffer(\'faces\', torch.from_numpy(undo_chumpy(dd[\'f\']).astype(np.int32)).type(dtype=torch.int32))\n        self.faces = torch.from_numpy(dd[\'f\'].astype(np.int32)).type(dtype=torch.int32)\n\n        # Mean template vertices\n        self.register_buffer(\'v_template\', torch.FloatTensor(dd[\'v_template\']))\n        # Size of mesh [Number of vertices, 3], (6890, 3)\n        self.size = [self.v_template.shape[0], 3]\n        self.num_betas = dd[\'shapedirs\'].shape[-1]\n        # Shape blend shape basis (shapedirs): (6980, 3, 10)\n        # reshaped to (6980*3, 10), transposed to (10, 6980*3)\n        self.register_buffer(\'shapedirs\', torch.FloatTensor(np.reshape(\n            dd[\'shapedirs\'], [-1, self.num_betas]).T))\n\n        # Regressor for joint locations given shape -> (24, 6890)\n        # Transpose to shape (6890, 24)\n        self.register_buffer(\'J_regressor\', torch.FloatTensor(\n            np.asarray(dd[\'J_regressor\'].T.todense())))\n\n        # Pose blend shape basis: (6890, 3, 207)\n        num_pose_basis = dd[\'posedirs\'].shape[-1]\n\n        # Pose blend pose basis is reshaped to (6890*3, 207)\n        # posedirs is transposed into (207, 6890*3)\n        self.register_buffer(\'posedirs\', torch.FloatTensor(np.reshape(\n            dd[\'posedirs\'], [-1, num_pose_basis]).T))\n\n        # indices of parents for each joints\n        self.parents = np.array(dd[\'kintree_table\'][0].astype(np.int32))\n\n        # LBS weights (6890, 24)\n        self.register_buffer(\'weights\', torch.FloatTensor(dd[\'weights\']))\n\n        # This returns 19 keypoints: 6890 x 19\n        joint_regressor = torch.FloatTensor(\n            np.asarray(dd[\'cocoplus_regressor\'].T.todense()))\n\n        self.register_buffer(\'joint_regressor\', joint_regressor)\n\n    def forward(self, beta, theta, get_skin=False):\n        """"""\n        Obtain SMPL with shape (beta) & pose (theta) inputs.\n        Theta includes the global rotation.\n        Args:\n          beta: N x 10\n          theta: N x 72 (with 3-D axis-angle rep)\n          get_skin: boolean, return skin or not\n\n        Updates:\n        self.J_transformed: N x 24 x 3 joint location after shaping\n                 & posing with beta and theta\n        Returns:\n          - joints: N x 19 or 14 x 3 joint locations depending on joint_type\n        If get_skin is True, also returns\n          - Verts: N x 6980 x 3\n        """"""\n        device = beta.device\n\n        num_batch = beta.shape[0]\n\n        # 1. Add shape blend shapes\n        #       matmul  : (N, 10) x (10, 6890*3) = (N, 6890*3)\n        #       reshape : (N, 6890*3) -> (N, 6890, 3)\n        #       v_shaped: (N, 6890, 3)\n        v_shaped = torch.matmul(beta, self.shapedirs).view(-1, self.size[0], self.size[1]) + self.v_template\n\n        # 2. Infer shape-dependent joint locations.\n        # ----- J_regressor: (6890, 24)\n        # ----- Jx (Jy, Jz): (N, 6890) x (6890, 24) = (N, 24)\n        # --------------- J: (N, 24, 3)\n        Jx = torch.matmul(v_shaped[:, :, 0], self.J_regressor)\n        Jy = torch.matmul(v_shaped[:, :, 1], self.J_regressor)\n        Jz = torch.matmul(v_shaped[:, :, 2], self.J_regressor)\n        J = torch.stack([Jx, Jy, Jz], dim=2)\n\n        # 3. Add pose blend shapes\n        # ------- theta    : (N, 72)\n        # ------- reshape  : (N*24, 3)\n        # ------- rodrigues: (N*24, 9)\n        # -- Rs = reshape  : (N, 24, 3, 3)\n        Rs = batch_rodrigues(theta.view(-1, 3), device=device).view(-1, 24, 3, 3)\n        # Ignore global rotation.\n        #       Rs[:, 1:, :, :]: (N, 23, 3, 3)\n        #           - np.eye(3): (N, 23, 3, 3)\n        #          pose_feature: (N, 207)\n        pose_feature = (Rs[:, 1:, :, :] - torch.eye(3).to(device)).view(-1, 207)\n\n        # (N, 207) x (207, 6890*3) -> (N, 6890, 3)\n        v_posed = torch.matmul(pose_feature, self.posedirs).view(-1, self.size[0], self.size[1]) + v_shaped\n\n        # 4. Get the global joint location\n        # ------- Rs is (N, 24, 3, 3),         J is (N, 24, 3)\n        # ------- J_transformed is (N, 24, 3), A is (N, 24, 4, 4)\n        J_transformed, A = batch_global_rigid_transformation(Rs, J, self.parents, device=device,\n                                                             rotate_base=self.rotate)\n\n        # 5. Do skinning:\n        # ------- weights is (6890, 24)\n        # ---------- tile is (N*6890, 24)\n        # --- W = reshape is (N, 6890, 24)\n        W = self.weights.repeat(num_batch, 1).view(num_batch, -1, 24)\n\n        # ------ reshape A is (N, 24, 16)\n        # --------- matmul is (N, 6890, 24) x (N, 24, 16) -> (N, 6890, 16)\n        # -------- reshape is (N, 6890, 4, 4)\n        T = torch.matmul(W, A.view(num_batch, 24, 16)).view(num_batch, -1, 4, 4)\n\n        # axis is 2, (N, 6890, 3) concatenate (N, 6890, 1) -> (N, 6890, 4)\n        v_posed_homo = torch.cat(\n            [v_posed, torch.ones(num_batch, v_posed.shape[1], 1, dtype=torch.float32).to(device)], dim=2)\n\n        # -unsqueeze_ is (N, 6890, 4, 1)\n        # --------- T is (N, 6890, 4, 4)\n        # ---- matmul is (N, 6890, 4, 4) x (N, 6890, 4, 1) -> (N, 6890, 4, 1)\n        v_posed_homo = v_posed_homo.unsqueeze(-1)\n        v_homo = torch.matmul(T, v_posed_homo)\n\n        # (N, 6890, 3)\n        verts = v_homo[:, :, :3, 0]\n\n        # Get cocoplus or lsp joints: (N, 6890) x (6890, 19)\n        joint_x = torch.matmul(verts[:, :, 0], self.joint_regressor)\n        joint_y = torch.matmul(verts[:, :, 1], self.joint_regressor)\n        joint_z = torch.matmul(verts[:, :, 2], self.joint_regressor)\n        joints = torch.stack([joint_x, joint_y, joint_z], dim=2)\n\n        if get_skin:\n            return verts, joints, Rs\n        else:\n            return joints\n'"
networks/discriminator.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport functools\nfrom .networks import NetworkBase\n\n\nclass PatchDiscriminator(NetworkBase):\n    """"""Defines a PatchGAN discriminator""""""\n\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_type=\'batch\', use_sigmoid=False):\n        """"""Construct a PatchGAN discriminator\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator\n            norm_layer      -- normalization layer\n        """"""\n        super(PatchDiscriminator, self).__init__()\n\n        norm_layer = self._get_norm_layer(norm_type)\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func != nn.BatchNorm2d\n        else:\n            use_bias = norm_layer != nn.BatchNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):  # gradually increase the number of filters\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        """"""Standard forward.""""""\n        return self.model(input)\n\n\nclass GlobalLocalDiscriminator(NetworkBase):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_type=\'batch\', use_sigmoid=False):\n        super(GlobalLocalDiscriminator, self).__init__()\n        self.global_model = PatchDiscriminator(4, ndf=ndf, n_layers=n_layers,\n                                               norm_type=norm_type, use_sigmoid=use_sigmoid)\n        self.local_model = PatchDiscriminator(input_nc, ndf=ndf, n_layers=n_layers,\n                                              norm_type=norm_type, use_sigmoid=use_sigmoid)\n        # from utils.demo_visualizer import MotionImitationVisualizer\n        # self._visualizer = MotionImitationVisualizer(\'debug\', ip=\'http://10.10.10.100\', port=31102)\n\n    def forward(self, global_x, local_x, local_rects):\n        glocal_outs = self.global_model(global_x)\n        crop_imgs = self.crop_body(local_x, local_rects)\n        local_outs = self.local_model(crop_imgs)\n\n        # self._visualizer.vis_named_img(\'body_imgs\', crop_imgs[:, 0:3])\n\n        return torch.cat([glocal_outs, local_outs], dim=0)\n\n    @staticmethod\n    def crop_body(imgs, rects):\n        """"""\n        :param imgs: (N, C, H, W)\n        :return:\n        """"""\n        bs, _, ori_h, ori_w = imgs.shape\n        head_imgs = []\n\n        for i in range(bs):\n            min_x, max_x, min_y, max_y = rects[i].detach()\n            head = imgs[i:i+1, :, min_y:max_y, min_x:max_x]  # (1, c, h\', w\')\n            head = F.interpolate(head, size=(ori_h, ori_w), mode=\'bilinear\', align_corners=True)\n            head_imgs.append(head)\n\n        head_imgs = torch.cat(head_imgs, dim=0)\n\n        return head_imgs\n\n\nclass MultiScaleDiscriminator(NetworkBase):\n\n    def __init__(self, input_nc, n_scales=5, ndf=64, n_layers=3, use_sigmoid=False):\n        super(MultiScaleDiscriminator, self).__init__()\n\n        # low-res to high-res\n        scale_models = nn.ModuleList()\n        # scale_n_layers = [1, 1, 1, 1, 1]\n        for i in range(n_scales):\n            # n_layers = scale_n_layers[i]\n            model = PatchDiscriminator(input_nc, ndf, n_layers, use_sigmoid)\n            scale_models.append(model)\n\n        self.n_scales = n_scales\n        self.scale_models = scale_models\n\n    def forward(self, x, is_detach=False):\n        scale_outs = []\n        for i in range(0, self.n_scales):\n            x_scale = x[i]\n            if is_detach:\n                x_scale = x_scale.detach()\n\n            outs = self.scale_models[i](x_scale)\n            # print(i, x[i].shape, outs.shape)\n\n            scale_outs.append(outs)\n\n        return scale_outs\n\n\n\n'"
networks/facenet.py,2,"b'import torch.nn as nn\nimport math\nimport torch.nn.functional as F\n\n__all__ = [\'SENet\', \'Sphere20a\', \'senet50\']\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\n# This SEModule is not used.\nclass SEModule(nn.Module):\n\n    def __init__(self, planes, compress_rate):\n        super(SEModule, self).__init__()\n        self.conv1 = nn.Conv2d(planes, planes // compress_rate, kernel_size=1, stride=1, bias=True)\n        self.conv2 = nn.Conv2d(planes // compress_rate, planes, kernel_size=1, stride=1, bias=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = F.avg_pool2d(module_input, kernel_size=module_input.size(2))\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n        # SENet\n        compress_rate = 16\n        # self.se_block = SEModule(planes * 4, compress_rate)  # this is not used.\n        self.conv4 = nn.Conv2d(planes * 4, planes * 4 // compress_rate, kernel_size=1, stride=1, bias=True)\n        self.conv5 = nn.Conv2d(planes * 4 // compress_rate, planes * 4, kernel_size=1, stride=1, bias=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        ## senet\n        out2 = F.avg_pool2d(out, kernel_size=out.size(2))\n        out2 = self.conv4(out2)\n        out2 = self.relu(out2)\n        out2 = self.conv5(out2)\n        out2 = self.sigmoid(out2)\n        # out2 = self.se_block.forward(out)  # not used\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out2 * out + residual\n        # out = out2 + residual  # not used\n        out = self.relu(out)\n        return out\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=8631, include_top=True):\n        self.inplanes = 64\n        super(SENet, self).__init__()\n        self.include_top = include_top\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n\n        if self.include_top:\n            self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x, get_feat=True):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x0 = self.maxpool(x)\n\n        x1 = self.layer1(x0)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n        x_avg = self.avgpool(x4)\n\n        if not self.include_top:\n            if get_feat:\n                return [x0, x1, x2, x3, x4]\n            else:\n                return x_avg\n\n        else:\n            x_fc = x_avg.view(x_avg.size(0), -1)\n            x_fc = self.fc(x_fc)\n\n            if get_feat:\n                return [x0, x1, x2, x3, x4]\n            else:\n                return x_fc\n\n\ndef senet50(**kwargs):\n    """"""Constructs a SENet-50 model.\n    """"""\n    model = SENet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\n\nclass Sphere20a(nn.Module):\n    def __init__(self,classnum=10574,feature=False):\n        super(Sphere20a, self).__init__()\n        self.classnum = classnum\n        self.feature = feature\n        #input = B*3*112*96\n        self.conv1_1 = nn.Conv2d(3,64,3,2,1) #=>B*64*56*48\n        self.relu1_1 = nn.PReLU(64)\n        self.conv1_2 = nn.Conv2d(64,64,3,1,1)\n        self.relu1_2 = nn.PReLU(64)\n        self.conv1_3 = nn.Conv2d(64,64,3,1,1)\n        self.relu1_3 = nn.PReLU(64)\n\n        self.conv2_1 = nn.Conv2d(64,128,3,2,1) #=>B*128*28*24\n        self.relu2_1 = nn.PReLU(128)\n        self.conv2_2 = nn.Conv2d(128,128,3,1,1)\n        self.relu2_2 = nn.PReLU(128)\n        self.conv2_3 = nn.Conv2d(128,128,3,1,1)\n        self.relu2_3 = nn.PReLU(128)\n\n        self.conv2_4 = nn.Conv2d(128,128,3,1,1) #=>B*128*28*24\n        self.relu2_4 = nn.PReLU(128)\n        self.conv2_5 = nn.Conv2d(128,128,3,1,1)\n        self.relu2_5 = nn.PReLU(128)\n\n        self.conv3_1 = nn.Conv2d(128,256,3,2,1) #=>B*256*14*12\n        self.relu3_1 = nn.PReLU(256)\n        self.conv3_2 = nn.Conv2d(256,256,3,1,1)\n        self.relu3_2 = nn.PReLU(256)\n        self.conv3_3 = nn.Conv2d(256,256,3,1,1)\n        self.relu3_3 = nn.PReLU(256)\n\n        self.conv3_4 = nn.Conv2d(256,256,3,1,1) #=>B*256*14*12\n        self.relu3_4 = nn.PReLU(256)\n        self.conv3_5 = nn.Conv2d(256,256,3,1,1)\n        self.relu3_5 = nn.PReLU(256)\n\n        self.conv3_6 = nn.Conv2d(256,256,3,1,1) #=>B*256*14*12\n        self.relu3_6 = nn.PReLU(256)\n        self.conv3_7 = nn.Conv2d(256,256,3,1,1)\n        self.relu3_7 = nn.PReLU(256)\n\n        self.conv3_8 = nn.Conv2d(256,256,3,1,1) #=>B*256*14*12\n        self.relu3_8 = nn.PReLU(256)\n        self.conv3_9 = nn.Conv2d(256,256,3,1,1)\n        self.relu3_9 = nn.PReLU(256)\n\n        self.conv4_1 = nn.Conv2d(256,512,3,2,1) #=>B*512*7*6\n        self.relu4_1 = nn.PReLU(512)\n        self.conv4_2 = nn.Conv2d(512,512,3,1,1)\n        self.relu4_2 = nn.PReLU(512)\n        self.conv4_3 = nn.Conv2d(512,512,3,1,1)\n        self.relu4_3 = nn.PReLU(512)\n\n        self.fc5 = nn.Linear(512*7*6, 512)\n\n    def forward(self, x):\n        feat_outs = []\n        x = self.relu1_1(self.conv1_1(x))\n        x = x + self.relu1_3(self.conv1_3(self.relu1_2(self.conv1_2(x))))\n        feat_outs.append(x)\n\n        x = self.relu2_1(self.conv2_1(x))\n        x = x + self.relu2_3(self.conv2_3(self.relu2_2(self.conv2_2(x))))\n        x = x + self.relu2_5(self.conv2_5(self.relu2_4(self.conv2_4(x))))\n        feat_outs.append(x)\n\n        x = self.relu3_1(self.conv3_1(x))\n        x = x + self.relu3_3(self.conv3_3(self.relu3_2(self.conv3_2(x))))\n        x = x + self.relu3_5(self.conv3_5(self.relu3_4(self.conv3_4(x))))\n        x = x + self.relu3_7(self.conv3_7(self.relu3_6(self.conv3_6(x))))\n        x = x + self.relu3_9(self.conv3_9(self.relu3_8(self.conv3_8(x))))\n        feat_outs.append(x)\n\n        x = self.relu4_1(self.conv4_1(x))\n        x = x + self.relu4_3(self.conv4_3(self.relu4_2(self.conv4_2(x))))\n        feat_outs.append(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc5(x)\n        feat_outs.append(x)\n\n        return feat_outs\n'"
networks/generator.py,8,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom .networks import NetworkBase\nimport torch\nimport ipdb\n\n\nclass ResidualBlock(nn.Module):\n    """"""Residual Block.""""""\n    def __init__(self, dim_in, dim_out):\n        super(ResidualBlock, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(dim_out, affine=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(dim_out, affine=True))\n\n    def forward(self, x):\n        return x + self.main(x)\n\n\nclass ResNetGenerator(NetworkBase):\n    """"""Generator. Encoder-Decoder Architecture.""""""\n    def __init__(self, conv_dim=64, c_dim=5, repeat_num=9, k_size=4, n_down=2):\n        super(ResNetGenerator, self).__init__()\n        self._name = \'resnet_generator\'\n\n        layers = []\n        layers.append(nn.Conv2d(c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.InstanceNorm2d(conv_dim, affine=True))\n        layers.append(nn.ReLU(inplace=True))\n\n        # Down-Sampling\n        curr_dim = conv_dim\n        for i in range(n_down):\n            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=k_size, stride=2, padding=1, bias=False))\n            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True))\n            layers.append(nn.ReLU(inplace=True))\n            curr_dim = curr_dim * 2\n\n        # Bottleneck\n        for i in range(repeat_num):\n            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n\n        # Up-Sampling\n        for i in range(n_down):\n            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=k_size, stride=2, padding=1,\n                                             output_padding=1, bias=False))\n            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True))\n            layers.append(nn.ReLU(inplace=True))\n            curr_dim = curr_dim // 2\n\n        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.Tanh())\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, c=None):\n        if c is not None:\n            # replicate spatially and concatenate domain information\n            c = c.unsqueeze(2).unsqueeze(3)\n            c = c.expand(c.size(0), c.size(1), x.size(2), x.size(3))\n            x = torch.cat([x, c], dim=1)\n        return self.model(x)\n\n\nclass ResUnetGenerator(NetworkBase):\n    """"""Generator. Encoder-Decoder Architecture.""""""\n    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6, k_size=4, n_down=2):\n        super(ResUnetGenerator, self).__init__()\n        self._name = \'resunet_generator\'\n\n        self.repeat_num = repeat_num\n        self.n_down = n_down\n\n        encoders = []\n\n        encoders.append(nn.Sequential(\n            nn.Conv2d(c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False),\n            nn.InstanceNorm2d(conv_dim, affine=True),\n            nn.ReLU(inplace=True)\n        ))\n\n        # Down-Sampling\n        curr_dim = conv_dim\n        for i in range(n_down):\n            encoders.append(nn.Sequential(\n                nn.Conv2d(curr_dim, curr_dim*2, kernel_size=k_size, stride=2, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim*2, affine=True),\n                nn.ReLU(inplace=True)\n            ))\n\n            curr_dim = curr_dim * 2\n\n        self.encoders = nn.Sequential(*encoders)\n\n        # Bottleneck\n        resnets = []\n        for i in range(repeat_num):\n            resnets.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n\n        self.resnets = nn.Sequential(*resnets)\n\n        # Up-Sampling\n        decoders = []\n        skippers = []\n        for i in range(n_down):\n            decoders.append(nn.Sequential(\n                nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=k_size, stride=2, padding=1, output_padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim//2, affine=True),\n                nn.ReLU(inplace=True)\n            ))\n\n            skippers.append(nn.Sequential(\n                nn.Conv2d(curr_dim, curr_dim//2, kernel_size=k_size, stride=1, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim//2, affine=True),\n                nn.ReLU(inplace=True)\n            ))\n\n            curr_dim = curr_dim // 2\n\n        self.decoders = nn.Sequential(*decoders)\n        self.skippers = nn.Sequential(*skippers)\n\n        layers = []\n        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.Tanh())\n        self.img_reg = nn.Sequential(*layers)\n\n        layers = []\n        layers.append(nn.Conv2d(curr_dim, 1, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.Sigmoid())\n        self.attetion_reg = nn.Sequential(*layers)\n\n    def inference(self, x):\n        # encoder, 0, 1, 2, 3 -> [256, 128, 64, 32]\n        encoder_outs = self.encode(x)\n\n        # resnet, 32\n        resnet_outs = []\n        src_x = encoder_outs[-1]\n        for i in range(self.repeat_num):\n            src_x = self.resnets[i](src_x)\n            resnet_outs.append(src_x)\n\n        return encoder_outs, resnet_outs\n\n    def forward(self, x):\n\n        # encoder, 0, 1, 2, 3 -> [256, 128, 64, 32]\n        encoder_outs = self.encode(x)\n\n        # resnet, 32\n        resnet_outs = self.resnets(encoder_outs[-1])\n\n        # decoder, 0, 1, 2 -> [64, 128, 256]\n        d_out = self.decode(resnet_outs, encoder_outs)\n\n        img_outs, mask_outs = self.regress(d_out)\n        return img_outs, mask_outs\n\n    def encode(self, x):\n        e_out = self.encoders[0](x)\n\n        encoder_outs = [e_out]\n        for i in range(1, self.n_down + 1):\n            e_out = self.encoders[i](e_out)\n            encoder_outs.append(e_out)\n            #print(i, e_out.shape)\n        return encoder_outs\n\n    def decode(self, x, encoder_outs):\n        d_out = x\n        for i in range(self.n_down):\n            d_out = self.decoders[i](d_out)  # x * 2\n            skip = encoder_outs[self.n_down - 1 - i]\n            d_out = torch.cat([skip, d_out], dim=1)\n            d_out = self.skippers[i](d_out)\n            # print(i, d_out.shape)\n        return d_out\n\n    def regress(self, x):\n        return self.img_reg(x), self.attetion_reg(x)\n\n\nclass ImpersonatorGenerator(NetworkBase):\n    """"""Generator. Encoder-Decoder Architecture.""""""\n    def __init__(self, bg_dim, src_dim, tsf_dim, conv_dim=64, repeat_num=6):\n        super(ImpersonatorGenerator, self).__init__()\n        self._name = \'impersonator_generator\'\n\n        self.n_down = 3\n        self.repeat_num = repeat_num\n        # background generator\n        self.bg_model = ResNetGenerator(conv_dim=conv_dim, c_dim=bg_dim, repeat_num=repeat_num, k_size=3, n_down=self.n_down)\n\n        # source generator\n        self.src_model = ResUnetGenerator(conv_dim=conv_dim, c_dim=src_dim, repeat_num=repeat_num, k_size=3, n_down=self.n_down)\n\n        # transfer generator\n        self.tsf_model = ResUnetGenerator(conv_dim=conv_dim, c_dim=tsf_dim, repeat_num=repeat_num, k_size=3, n_down=self.n_down)\n\n    def forward(self, bg_inputs, src_inputs, tsf_inputs, T):\n\n        img_bg = self.bg_model(bg_inputs)\n\n        src_img, src_mask, tsf_img, tsf_mask = self.infer_front(src_inputs, tsf_inputs, T)\n\n        # print(front_rgb.shape, front_mask.shape)\n        return img_bg, src_img, src_mask, tsf_img, tsf_mask\n\n    def encode_src(self, src_inputs):\n        return self.src_model.inference(src_inputs)\n\n    def infer_front(self, src_inputs, tsf_inputs, T):\n        # encoder\n        src_x = self.src_model.encoders[0](src_inputs)\n        tsf_x = self.tsf_model.encoders[0](tsf_inputs)\n\n        src_encoder_outs = [src_x]\n        tsf_encoder_outs = [tsf_x]\n        for i in range(1, self.n_down + 1):\n            src_x = self.src_model.encoders[i](src_x)\n            warp = self.transform(src_x, T)\n            tsf_x = self.tsf_model.encoders[i](tsf_x) + warp\n\n            src_encoder_outs.append(src_x)\n            tsf_encoder_outs.append(tsf_x)\n\n        # resnets\n        T_scale = self.resize_trans(src_x, T)\n        for i in range(self.repeat_num):\n            src_x = self.src_model.resnets[i](src_x)\n            warp = self.stn(src_x, T_scale)\n            tsf_x = self.tsf_model.resnets[i](tsf_x) + warp\n\n        # decoders\n        src_img, src_mask = self.src_model.regress(self.src_model.decode(src_x, src_encoder_outs))\n        tsf_img, tsf_mask = self.tsf_model.regress(self.tsf_model.decode(tsf_x, tsf_encoder_outs))\n\n        # print(front_rgb.shape, front_mask.shape)\n        return src_img, src_mask, tsf_img, tsf_mask\n\n    def swap(self, tsf_inputs, src_encoder_outs12, src_encoder_outs21, src_resnet_outs12, src_resnet_outs21, T12, T21):\n        # encoder\n        src_x12 = src_encoder_outs12[0]\n        src_x21 = src_encoder_outs21[0]\n        tsf_x = self.tsf_model.encoders[0](tsf_inputs)\n\n        tsf_encoder_outs = [tsf_x]\n        for i in range(1, self.n_down + 1):\n            src_x12 = src_encoder_outs12[i]\n            src_x21 = src_encoder_outs21[i]\n            warp12 = self.transform(src_x12, T12)\n            warp21 = self.transform(src_x21, T21)\n\n            tsf_x = self.tsf_model.encoders[i](tsf_x) + warp12 + warp21\n            tsf_encoder_outs.append(tsf_x)\n\n        # resnets\n        T_scale12 = self.resize_trans(src_x12, T12)\n        T_scale21 = self.resize_trans(src_x21, T21)\n        for i in range(self.repeat_num):\n            src_x12 = src_resnet_outs12[i]\n            src_x21 = src_resnet_outs21[i]\n            warp12 = self.stn(src_x12, T_scale12)\n            warp21 = self.stn(src_x21, T_scale21)\n            tsf_x = self.tsf_model.resnets[i](tsf_x) + warp12 + warp21\n\n        # decoders\n        tsf_img, tsf_mask = self.tsf_model.regress(self.tsf_model.decode(tsf_x, tsf_encoder_outs))\n\n        # print(front_rgb.shape, front_mask.shape)\n        return tsf_img, tsf_mask\n\n    def inference(self, src_encoder_outs, src_resnet_outs, tsf_inputs, T):\n        # encoder\n        src_x = src_encoder_outs[0]\n        tsf_x = self.tsf_model.encoders[0](tsf_inputs)\n\n        tsf_encoder_outs = [tsf_x]\n        for i in range(1, self.n_down + 1):\n            src_x = src_encoder_outs[i]\n            warp = self.transform(src_x, T)\n\n            tsf_x = self.tsf_model.encoders[i](tsf_x) + warp\n            tsf_encoder_outs.append(tsf_x)\n\n        # resnets\n        T_scale = self.resize_trans(src_x, T)\n        for i in range(self.repeat_num):\n            src_x = src_resnet_outs[i]\n            warp = self.stn(src_x, T_scale)\n            tsf_x = self.tsf_model.resnets[i](tsf_x) + warp\n\n        # decoders\n        tsf_img, tsf_mask = self.tsf_model.regress(self.tsf_model.decode(tsf_x, tsf_encoder_outs))\n\n        # print(front_rgb.shape, front_mask.shape)\n        return tsf_img, tsf_mask\n\n    def resize_trans(self, x, T):\n        _, _, h, w = x.shape\n\n        T_scale = T.permute(0, 3, 1, 2)  # (bs, 2, h, w)\n        T_scale = F.interpolate(T_scale, size=(h, w), mode=\'bilinear\', align_corners=True)\n        T_scale = T_scale.permute(0, 2, 3, 1)  # (bs, h, w, 2)\n\n        return T_scale\n\n    def stn(self, x, T):\n        x_trans = F.grid_sample(x, T)\n\n        return x_trans\n\n    def transform(self, x, T):\n        T_scale = self.resize_trans(x, T)\n        x_trans = self.stn(x, T_scale)\n        return x_trans\n\n\nif __name__ == \'__main__\':\n    imitator = ImpersonatorGenerator(bg_dim=4, src_dim=6, tsf_dim=6, conv_dim=64, repeat_num=6)\n\n    bg_x = torch.rand(2, 4, 256, 256)\n    src_x = torch.rand(2, 6, 256, 256)\n    tsf_x = torch.rand(2, 6, 256, 256)\n    T = torch.rand(2, 256, 256, 2)\n\n    img_bg, src_img, src_mask, tsf_img, tsf_mask = imitator(bg_x, src_x, tsf_x, T)\n\n    ipdb.set_trace()\n'"
networks/hmr.py,4,"b'# -*- coding: utf-8 -*-\n# @Time    : 2018/10/29 8:20 PM\n# @Author  : Zhixin Piao\n# @Email   : piaozhx@shanghaitech.edu.cn\n\n\'\'\'Pre-activation ResNet in PyTorch.\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport numpy as np\nimport h5py\n\nfrom .batch_smpl import SMPL, batch_orth_proj_idrot\n\n\ndef subsample(inputs, factor):\n    """"""Subsamples the input along the spatial dimensions.\n\n    Args:\n      inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n      factor: The subsampling factor.\n\n    Returns:\n      output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n        input, either intact (if factor == 1) or subsampled (if factor > 1).\n    """"""\n    if factor == 1:\n        return inputs\n    else:\n        return F.max_pool2d(inputs, [1, 1], stride=factor)\n\n\nclass PreActBlock(nn.Module):\n    \'\'\'Pre-activation version of the BasicBlock.\'\'\'\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        self.stride = stride\n\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    \'\'\'Pre-activation version of the original Bottleneck module.\'\'\'\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        # tf implementation there needs bias\n        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=True)\n\n        self.stride = stride\n\n        # if stride != 1 or in_planes != self.expansion * planes:\n        #     self.shortcut = nn.Sequential(\n        #         # tf implementation there needs bias\n        #         nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=True)\n        #     )\n        if in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                # tf implementation there needs bias\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=True)\n            )\n\n    def forward(self, x, layer_id=\'layer1\', block_id=\'0\', results_outs=OrderedDict()):\n        # out = F.relu(self.bn1(x))\n        # shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        # out = self.conv1(out)\n        # out = self.conv2(F.relu(self.bn2(out)))\n        # out = self.conv3(F.relu(self.bn3(out)))\n        # out += shortcut\n\n        # layer1.0.bn1\n        out_name = \'{}.{}\'.format(layer_id, block_id)\n\n        preact = F.relu(self.bn1(x))\n        shortcut_out = self.shortcut(preact) if hasattr(self, \'shortcut\') else subsample(x, factor=self.stride)\n        conv1_out = F.relu(self.bn2(self.conv1(preact)))\n        conv2_out = F.relu(self.bn3(self.conv2(conv1_out)))\n        conv3_out = self.conv3(conv2_out)\n        conv3_out_add = conv3_out + shortcut_out\n\n        results_outs[out_name + \'.shortcut.0\'] = shortcut_out\n        results_outs[out_name + \'.conv1\'] = conv1_out\n        results_outs[out_name + \'.conv2\'] = conv2_out\n        results_outs[out_name + \'.conv3\'] = conv3_out\n        results_outs[out_name] = conv3_out_add\n\n        return conv3_out_add\n\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks):\n        super(PreActResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=True)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=2)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=1)\n        self.post_bn = nn.BatchNorm2d(2048)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        # layers = []\n        # layers.append(block(self.in_planes, planes, stride))\n        #\n        # self.in_planes = planes * block.expansion\n        # for i in range(1, num_blocks):\n        #     layers.append(block(self.in_planes, planes))\n        # return nn.Sequential(*layers)\n\n        layers = []\n        layers.append(block(self.in_planes, planes, 1))\n\n        self.in_planes = planes * block.expansion\n        for i in range(1, num_blocks):\n            s = stride if i == num_blocks - 1 else 1\n            layers.append(block(self.in_planes, planes, stride=s))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        # here need padding =1\n        # out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n        out = F.max_pool2d(out, kernel_size=3, stride=2, ceil_mode=True)\n\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.relu(self.post_bn(out))\n\n        # need global avg_pooling\n        out = F.avg_pool2d(out, 7)\n\n        out = out.view(out.size(0), -1)\n        return out\n\n\ndef preActResNet18():\n    return PreActResNet(PreActBlock, [2, 2, 2, 2])\n\n\ndef preActResNet34():\n    return PreActResNet(PreActBlock, [3, 4, 6, 3])\n\n\ndef preActResNet50():\n    return PreActResNet(PreActBottleneck, [3, 4, 6, 3])\n\n\ndef preActResNet101():\n    return PreActResNet(PreActBottleneck, [3, 4, 23, 3])\n\n\ndef preActResNet152():\n    return PreActResNet(PreActBottleneck, [3, 8, 36, 3])\n\n\ndef load_mean_theta(smpl_mean_theta_path, total_theta_count=85):\n    mean = np.zeros(total_theta_count, dtype=np.float)\n\n    if smpl_mean_theta_path:\n        mean_values = h5py.File(smpl_mean_theta_path, \'r\')\n        mean_pose = mean_values[\'pose\'][...]\n        # Ignore the global rotation.\n        mean_pose[:3] = 0\n        mean_shape = mean_values[\'shape\'][...]\n\n        # This initializes the global pose to be up-right when projected\n        mean_pose[0] = np.pi\n\n        # Initialize scale at 0.9\n        mean[0] = 0.9\n        mean[3:75] = mean_pose[:]\n        mean[75:] = mean_shape[:]\n\n        mean_values.close()\n    else:\n        mean[0] = 0.9\n\n    return mean\n\n\nclass ThetaRegressor(nn.Module):\n    def __init__(self, input_dim, out_dim, iterations=3):\n        super(ThetaRegressor, self).__init__()\n        self.iterations = iterations\n\n        # register mean theta\n        self.register_buffer(\'mean_theta\', torch.rand(out_dim, dtype=torch.float32))\n\n        fc_blocks = OrderedDict()\n\n        fc_blocks[\'fc1\'] = nn.Linear(input_dim, 1024, bias=True)\n        fc_blocks[\'relu1\'] = nn.ReLU()\n        fc_blocks[\'dropout1\'] = nn.Dropout(p=0.5)\n\n        fc_blocks[\'fc2\'] = nn.Linear(1024, 1024, bias=True)\n        fc_blocks[\'relu2\'] = nn.ReLU()\n\n        fc_blocks[\'dropout2\'] = nn.Dropout(p=0.5)\n\n        fc_blocks[\'fc3\'] = nn.Linear(1024, out_dim, bias=True)\n        # small_xavier initialization\n        nn.init.xavier_normal_(fc_blocks[\'fc3\'].weight, gain=0.1)\n        nn.init.zeros_(fc_blocks[\'fc3\'].bias)\n\n        self.fc_blocks = nn.Sequential(fc_blocks)\n\n    def forward(self, x):\n        """"""\n        :param x: the output of encoder, 2048 dim\n        :return: a list contains [[theta1, theta1, ..., theta1],\n                                 [theta2, theta2, ..., theta2], ... , ],\n                shape is iterations X N X 85(or other theta count)\n        """"""\n        batch_size = x.shape[0]\n        theta = self.mean_theta.repeat(batch_size, 1)\n        for _ in range(self.iterations):\n            total_inputs = torch.cat([x, theta], dim=1)\n            theta = theta + self.fc_blocks(total_inputs)\n\n        return theta\n\n\nclass HumanModelRecovery(nn.Module):\n    """"""\n        regressor can predict betas(include beta and theta which needed by SMPL) from coder\n        extracted from encoder in a iteration way\n    """"""\n\n    def __init__(self, smpl_pkl_path, feature_dim=2048, theta_dim=85, iterations=3):\n        super(HumanModelRecovery, self).__init__()\n\n        # define resnet50_v2\n        self.resnet = preActResNet50()\n\n        # define smpl\n        self.smpl = SMPL(pkl_path=smpl_pkl_path)\n\n        self.feature_dim = feature_dim\n        self.theta_dim = theta_dim\n\n        self.regressor = ThetaRegressor(feature_dim + theta_dim, theta_dim, iterations)\n        self.iterations = iterations\n\n    def forward(self, inputs):\n        out = self.resnet.conv1(inputs)\n\n        # here need padding =1\n        # out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n        out = F.max_pool2d(out, kernel_size=3, stride=2, ceil_mode=True)\n\n        out = self.resnet.layer1(out)\n\n        out = self.resnet.layer2(out)\n\n        out = self.resnet.layer3(out)\n\n        out = self.resnet.layer4(out)\n\n        out = F.relu(self.resnet.post_bn(out))\n        # need global avg_pooling\n        out = F.avg_pool2d(out, 7)\n\n        features = out.view(out.size(0), -1)\n\n        # regressor\n        thetas = self.regressor(features)\n\n        return thetas\n\n    def get_details(self, theta):\n        """"""\n            purpose:\n                calc verts, joint2d, joint3d, Rotation matrix\n\n            inputs:\n                theta: N X (3 + 72 + 10)\n\n            return:\n                thetas, verts, j2d, j3d, Rs\n        """"""\n\n        cam = theta[:, 0:3].contiguous()\n        pose = theta[:, 3:75].contiguous()\n        shape = theta[:, 75:].contiguous()\n        verts, j3d, rs = self.smpl(beta=shape, theta=pose, get_skin=True)\n        j2d = batch_orth_proj_idrot(j3d, cam)\n\n        detail_info = {\n            \'theta\': theta,\n            \'cam\': cam,\n            \'pose\': pose,\n            \'shape\': shape,\n            \'verts\': verts,\n            \'j2d\': j2d,\n            \'j3d\': j3d\n        }\n\n        return detail_info\n'"
networks/inpaintor.py,17,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\ndef get_pad(in_,  ksize, stride, atrous=1):\n    out_ = np.ceil(float(in_)/stride)\n    return int(((out_ - 1) * stride + atrous*(ksize-1) + 1 - in_)/2)\n\n\nclass GatedConv2dWithActivation(nn.Module):\n    """"""\n    Gated Convlution layer with activation (default activation:LeakyReLU)\n    Params: same as conv2d\n    Input: The feature from last layer ""I""\n    Output:\\phi(f(I))*\\sigmoid(g(I))\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n        super(GatedConv2dWithActivation, self).__init__()\n        self.batch_norm = batch_norm\n        self.activation = activation\n        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n        self.mask_conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n        self.batch_norm2d = torch.nn.BatchNorm2d(out_channels)\n        self.sigmoid = torch.nn.Sigmoid()\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n\n    def gated(self, mask):\n        # return torch.clamp(mask, -1, 1)\n        return self.sigmoid(mask)\n\n    def forward(self, input):\n        x = self.conv2d(input)\n        mask = self.mask_conv2d(input)\n        if self.activation is not None:\n            x = self.activation(x) * self.gated(mask)\n        else:\n            x = x * self.gated(mask)\n        if self.batch_norm:\n            return self.batch_norm2d(x)\n        else:\n            return x\n\n\nclass GatedDeConv2dWithActivation(nn.Module):\n    """"""\n    Gated DeConvlution layer with activation (default activation:LeakyReLU)\n    resize + conv\n    Params: same as conv2d\n    Input: The feature from last layer ""I""\n    Output:\\phi(f(I))*\\sigmoid(g(I))\n    """"""\n    def __init__(self, scale_factor, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, bias=True, batch_norm=True, activation=torch.nn.LeakyReLU(0.2, inplace=True)):\n        super(GatedDeConv2dWithActivation, self).__init__()\n        self.conv2d = GatedConv2dWithActivation(in_channels, out_channels, kernel_size, stride, padding,\n                                                dilation, groups, bias, batch_norm, activation)\n        self.scale_factor = scale_factor\n\n    def forward(self, input):\n        # print(input.size())\n        x = F.interpolate(input, scale_factor=2)\n        return self.conv2d(x)\n\n\nclass SelfAttention(nn.Module):\n    """""" Self attention Layer""""""\n    def __init__(self, in_dim, activation, with_attn=False):\n        super(SelfAttention, self).__init__()\n        self.chanel_in = in_dim\n        self.activation = activation\n        self.with_attn = with_attn\n        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        """"""\n            inputs :\n                x : input feature maps( B X C X W X H)\n            returns :\n                out : self attention value + input feature\n                attention: B X N X N (N is Width*Height)\n        """"""\n        m_batchsize, C, width, height = x.size()\n        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)  # B X CX(N)\n        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)   # B X C x (*W*H)\n        energy = torch.bmm(proj_query, proj_key)   # transpose check\n        attention = self.softmax(energy)   # BX (N) X (N)\n        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)   # B X C X N\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(m_batchsize, C, width, height)\n\n        out = self.gamma * out + x\n        if self.with_attn:\n            return out, attention\n        else:\n            return out\n\n\nclass InpaintSANet(torch.nn.Module):\n    """"""\n    Inpaint generator, input should be 5*256*256, where 3*256*256 is the masked image, 1*256*256 for mask, 1*256*256 is the guidence\n    """"""\n    def __init__(self, c_dim=5):\n        super(InpaintSANet, self).__init__()\n        cnum = 32\n        self.coarse_net = nn.Sequential(\n            # input is 5*256*256, but it is full convolution network, so it can be larger than 256\n            GatedConv2dWithActivation(c_dim, cnum, 5, 1, padding=get_pad(256, 5, 1)),\n            # downsample 128\n            GatedConv2dWithActivation(cnum, 2*cnum, 4, 2, padding=get_pad(256, 4, 2)),\n            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n            # downsample to 64\n            GatedConv2dWithActivation(2*cnum, 4*cnum, 4, 2, padding=get_pad(128, 4, 2)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n            # atrous convlution\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=2, padding=get_pad(64, 3, 1, 2)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=4, padding=get_pad(64, 3, 1, 4)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=8, padding=get_pad(64, 3, 1, 8)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=16, padding=get_pad(64, 3, 1, 16)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n            # Self_Attn(4*cnum, \'relu\'),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n            # upsample\n            GatedDeConv2dWithActivation(2, 4*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n            # Self_Attn(2*cnum, \'relu\'),\n            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n            GatedDeConv2dWithActivation(2, 2*cnum, cnum, 3, 1, padding=get_pad(256, 3, 1)),\n\n            GatedConv2dWithActivation(cnum, cnum//2, 3, 1, padding=get_pad(256, 3, 1)),\n            # Self_Attn(cnum//2, \'relu\'),\n            GatedConv2dWithActivation(cnum//2, 3, 3, 1, padding=get_pad(128, 3, 1), activation=None)\n        )\n\n        self.refine_conv_net = nn.Sequential(\n            # input is 5*256*256\n            GatedConv2dWithActivation(c_dim, cnum, 5, 1, padding=get_pad(256, 5, 1)),\n            # downsample\n            GatedConv2dWithActivation(cnum, cnum, 4, 2, padding=get_pad(256, 4, 2)),\n            GatedConv2dWithActivation(cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n            # downsample\n            GatedConv2dWithActivation(2*cnum, 2*cnum, 4, 2, padding=get_pad(128, 4, 2)),\n            GatedConv2dWithActivation(2*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=2, padding=get_pad(64, 3, 1, 2)),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=4, padding=get_pad(64, 3, 1, 4)),\n            # Self_Attn(4*cnum, \'relu\'),\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=8, padding=get_pad(64, 3, 1, 8)),\n\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, dilation=16, padding=get_pad(64, 3, 1, 16))\n        )\n        self.refine_attn = SelfAttention(4 * cnum, \'relu\', with_attn=False)\n        self.refine_upsample_net = nn.Sequential(\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n\n            GatedConv2dWithActivation(4*cnum, 4*cnum, 3, 1, padding=get_pad(64, 3, 1)),\n            GatedDeConv2dWithActivation(2, 4*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n            GatedConv2dWithActivation(2*cnum, 2*cnum, 3, 1, padding=get_pad(128, 3, 1)),\n            GatedDeConv2dWithActivation(2, 2*cnum, cnum, 3, 1, padding=get_pad(256, 3, 1)),\n\n            GatedConv2dWithActivation(cnum, cnum//2, 3, 1, padding=get_pad(256, 3, 1)),\n            # Self_Attn(cnum, \'relu\'),\n            GatedConv2dWithActivation(cnum//2, 3, 3, 1, padding=get_pad(256, 3, 1), activation=None),\n        )\n\n    def forward(self, imgs, masks, only_out=False, only_x=False):\n        # Coarse\n        masked_imgs = imgs * (1 - masks) + masks\n        input_imgs = torch.cat([masked_imgs, masks], dim=1)\n        # print(input_imgs.size(), imgs.size(), masks.size())\n        x = self.coarse_net(input_imgs)\n        x = torch.clamp(x, -1., 1.)\n        coarse_x = x\n        # Refine\n        masked_imgs = imgs * (1 - masks) + coarse_x * masks\n        input_imgs = torch.cat([masked_imgs, masks], dim=1)\n        x = self.refine_conv_net(input_imgs)\n        x = self.refine_attn(x)\n        # print(x.size(), attention.size())\n        x = self.refine_upsample_net(x)\n        x = torch.clamp(x, -1., 1.)\n\n        comp_imgs = x * masks + imgs * (1 - masks)\n\n        if only_out:\n            return comp_imgs\n        elif only_x:\n            return x\n        else:\n            return coarse_x, x, comp_imgs\n'"
networks/networks.py,41,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nimport functools\nfrom .hmr import HumanModelRecovery\nfrom .facenet import Sphere20a, senet50\n\n\nclass NetworksFactory(object):\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_by_name(network_name, *args, **kwargs):\n\n        if network_name == \'impersonator\':\n            from .generator import ImpersonatorGenerator\n            network = ImpersonatorGenerator(*args, **kwargs)\n\n        elif network_name == \'deepfillv2\':\n            from .inpaintor import InpaintSANet\n            network = InpaintSANet(*args, **kwargs)\n\n        elif network_name == \'concat\':\n            from .baseline import ConcatGenerator\n            network = ConcatGenerator(*args, **kwargs)\n\n        elif network_name == \'discriminator_patch_gan\':\n            from .discriminator import PatchDiscriminator\n            network = PatchDiscriminator(*args, **kwargs)\n\n        elif network_name == \'global_local\':\n            from .discriminator import GlobalLocalDiscriminator\n            network = GlobalLocalDiscriminator(*args, **kwargs)\n\n        else:\n            raise ValueError(""Network %s not recognized."" % network_name)\n\n        print(""Network %s was created"" % network_name)\n\n        return network\n\n\nclass NetworkBase(nn.Module):\n    def __init__(self):\n        super(NetworkBase, self).__init__()\n        self._name = \'BaseNetwork\'\n\n    @property\n    def name(self):\n        return self._name\n\n    def init_weights(self):\n        self.apply(self._weights_init_fn)\n\n    def _weights_init_fn(self, m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv\') != -1:\n            m.weight.data.normal_(0.0, 0.02)\n            if hasattr(m.bias, \'data\'):\n                m.bias.data.fill_(0)\n        elif classname.find(\'BatchNorm2d\') != -1:\n            m.weight.data.normal_(1.0, 0.02)\n            m.bias.data.fill_(0)\n\n    def _get_norm_layer(self, norm_type=\'batch\'):\n        if norm_type == \'batch\':\n            norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n        elif norm_type == \'instance\':\n            norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n        elif norm_type == \'batchnorm2d\':\n            norm_layer = nn.BatchNorm2d\n        else:\n            raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_type)\n\n        return norm_layer\n\n    def forward(self, *input):\n        raise NotImplementedError\n\n\nclass Vgg19(nn.Module):\n    """"""\n    Sequential(\n          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\n          (1): ReLU(inplace)\n          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (3): ReLU(inplace)\n          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\n          (6): ReLU(inplace)\n          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (8): ReLU(inplace)\n          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\n          (11): ReLU(inplace)\n          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (13): ReLU(inplace)\n          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (15): ReLU(inplace)\n          (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (17): ReLU(inplace)\n          (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\n          (20): ReLU(inplace)\n          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (22): ReLU(inplace)\n          (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (24): ReLU(inplace)\n          (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (26): ReLU(inplace)\n          (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n\n          (29): ReLU(inplace)\n          xxxx(30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          xxxx(31): ReLU(inplace)\n          xxxx(32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          xxxx(33): ReLU(inplace)\n          xxxx(34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          xxxx(35): ReLU(inplace)\n          xxxx(36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    """"""\n\n    def __init__(self, requires_grad=False, before_relu=False):\n        super(Vgg19, self).__init__()\n        vgg_pretrained_features = models.vgg19(pretrained=True).features\n        print(\'loading vgg19 ...\')\n\n        if before_relu:\n            slice_ids = [1, 6, 11, 20, 29]\n        else:\n            slice_ids = [2, 7, 12, 21, 30]\n\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(slice_ids[0]):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(slice_ids[0], slice_ids[1]):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(slice_ids[1], slice_ids[2]):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(slice_ids[2], slice_ids[3]):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(slice_ids[3], slice_ids[4]):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h_out1 = self.slice1(X)\n        h_out2 = self.slice2(h_out1)\n        h_out3 = self.slice3(h_out2)\n        h_out4 = self.slice4(h_out3)\n        h_out5 = self.slice5(h_out4)\n        out = [h_out1, h_out2, h_out3, h_out4, h_out5]\n        return out\n\n\nclass VGGLoss(nn.Module):\n    def __init__(self, vgg=None, before_relu=False):\n        super(VGGLoss, self).__init__()\n        if vgg is None:\n            self.vgg = Vgg19(before_relu=before_relu).cuda()\n        else:\n            self.vgg = vgg\n        self.criterion = nn.L1Loss()\n        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n\n    def forward(self, x, y):\n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n        loss = 0\n        for i in range(len(x_vgg)):\n            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n        return loss\n\n\nclass HMRLoss(nn.Module):\n    def __init__(self, pretrain_model, smpl_pkl_path):\n        super(HMRLoss, self).__init__()\n        self.hmr = HumanModelRecovery(smpl_pkl_path=smpl_pkl_path)\n        self.load_model(pretrain_model)\n        self.criterion = nn.L1Loss()\n        self.eval()\n\n    def forward(self, x, y):\n        x_hmr, y_hmr = self.hmr(x), self.hmr(y)\n        loss = 0.0\n        for i in range(len(x_hmr)):\n            loss += self.criterion(x_hmr[i], y_hmr[i].detach())\n            # loss += self.criterion(x_hmr[i], y_hmr[i])\n        return loss\n\n    def load_model(self, pretrain_model):\n        saved_data = torch.load(pretrain_model)\n        self.hmr.load_state_dict(saved_data)\n        print(\'load hmr model from {}\'.format(pretrain_model))\n\n\nclass FaceLoss(nn.Module):\n    def __init__(self, pretrained_path=\'asset/spretrains/sphere20a_20171020.pth\'):\n        super(FaceLoss, self).__init__()\n        if \'senet\' in pretrained_path:\n            self.net = senet50(include_top=False)\n            self.load_senet_model(pretrained_path)\n            self.height, self.width = 224, 224\n        else:\n            self.net = Sphere20a()\n            self.load_sphere_model(pretrained_path)\n            self.height, self.width = 112, 96\n\n        self.net.eval()\n        self.criterion = nn.L1Loss()\n        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n\n        # from utils.demo_visualizer import MotionImitationVisualizer\n        # self._visualizer = MotionImitationVisualizer(\'debug\', ip=\'http://10.10.10.100\', port=31102)\n\n    def forward(self, imgs1, imgs2, kps1=None, kps2=None, bbox1=None, bbox2=None):\n        """"""\n        Args:\n            imgs1:\n            imgs2:\n            kps1:\n            kps2:\n            bbox1:\n            bbox2:\n\n        Returns:\n\n        """"""\n        if kps1 is not None:\n            head_imgs1 = self.crop_head_kps(imgs1, kps1)\n        elif bbox1 is not None:\n            head_imgs1 = self.crop_head_bbox(imgs1, bbox1)\n        elif self.check_need_resize(imgs1):\n            head_imgs1 = F.interpolate(imgs1, size=(self.height, self.width), mode=\'bilinear\', align_corners=True)\n        else:\n            head_imgs1 = imgs1\n\n        if kps2 is not None:\n            head_imgs2 = self.crop_head_kps(imgs2, kps2)\n        elif bbox2 is not None:\n            head_imgs2 = self.crop_head_bbox(imgs2, bbox2)\n        elif self.check_need_resize(imgs2):\n            head_imgs2 = F.interpolate(imgs1, size=(self.height, self.width), mode=\'bilinear\', align_corners=True)\n        else:\n            head_imgs2 = imgs2\n\n        loss = self.compute_loss(head_imgs1, head_imgs2)\n\n        # self._visualizer.vis_named_img(\'img2\', imgs2)\n        # self._visualizer.vis_named_img(\'head imgs2\', head_imgs2)\n        #\n        # self._visualizer.vis_named_img(\'img1\', imgs1)\n        # self._visualizer.vis_named_img(\'head imgs1\', head_imgs1)\n        # import ipdb\n        # ipdb.set_trace()\n\n        return loss\n\n    def compute_loss(self, img1, img2):\n        """"""\n        :param img1: (n, 3, 112, 96), [-1, 1]\n        :param img2: (n, 3, 112, 96), [-1, 1], if it is used in training,\n                     img2 is reference image (GT), use detach() to stop backpropagation.\n        :return:\n        """"""\n        f1, f2 = self.net(img1), self.net(img2)\n\n        loss = 0.0\n        for i in range(len(f1)):\n            loss += self.criterion(f1[i], f2[i].detach())\n\n        return loss\n\n    def check_need_resize(self, img):\n        return img.shape[2] != self.height or img.shape[3] != self.width\n\n    def crop_head_bbox(self, imgs, bboxs):\n        """"""\n        Args:\n            bboxs: (N, 4), 4 = [lt_x, lt_y, rt_x, rt_y]\n\n        Returns:\n            resize_image:\n        """"""\n        bs, _, ori_h, ori_w = imgs.shape\n\n        head_imgs = []\n\n        for i in range(bs):\n            min_x, max_x, min_y, max_y = bboxs[i]\n            head = imgs[i:i+1, :, min_y:max_y, min_x:max_x]  # (1, c, h\', w\')\n            head = F.interpolate(head, size=(self.height, self.width), mode=\'bilinear\', align_corners=True)\n            head_imgs.append(head)\n\n        head_imgs = torch.cat(head_imgs, dim=0)\n\n        return head_imgs\n\n    def crop_head_kps(self, imgs, kps):\n        """"""\n        :param imgs: (N, C, H, W)\n        :param kps: (N, 19, 2)\n        :return:\n        """"""\n        bs, _, ori_h, ori_w = imgs.shape\n\n        rects = self.find_head_rect(kps, ori_h, ori_w)\n        head_imgs = []\n\n        for i in range(bs):\n            min_x, max_x, min_y, max_y = rects[i]\n            head = imgs[i:i+1, :, min_y:max_y, min_x:max_x]  # (1, c, h\', w\')\n            head = F.interpolate(head, size=(self.height, self.width), mode=\'bilinear\', align_corners=True)\n            head_imgs.append(head)\n\n        head_imgs = torch.cat(head_imgs, dim=0)\n\n        return head_imgs\n\n    @staticmethod\n    @torch.no_grad()\n    def find_head_rect(kps, height, width):\n        NECK_IDS = 12\n\n        kps = (kps + 1) / 2.0\n\n        necks = kps[:, NECK_IDS, 0]\n        zeros = torch.zeros_like(necks)\n        ones = torch.ones_like(necks)\n\n        # min_x = int(max(0.0, np.min(kps[HEAD_IDS:, 0]) - 0.1) * image_size)\n        min_x, _ = torch.min(kps[:, NECK_IDS:, 0] - 0.05, dim=1)\n        min_x = torch.max(min_x, zeros)\n\n        max_x, _ = torch.max(kps[:, NECK_IDS:, 0] + 0.05, dim=1)\n        max_x = torch.min(max_x, ones)\n\n        # min_x = int(max(0.0, np.min(kps[HEAD_IDS:, 0]) - 0.1) * image_size)\n        min_y, _ = torch.min(kps[:, NECK_IDS:, 1] - 0.05, dim=1)\n        min_y = torch.max(min_y, zeros)\n\n        max_y, _ = torch.max(kps[:, NECK_IDS:, 1], dim=1)\n        max_y = torch.min(max_y, ones)\n\n        min_x = (min_x * width).long()      # (T, 1)\n        max_x = (max_x * width).long()      # (T, 1)\n        min_y = (min_y * height).long()     # (T, 1)\n        max_y = (max_y * height).long()     # (T, 1)\n\n        # print(min_x.shape, max_x.shape, min_y.shape, max_y.shape)\n        rects = torch.stack((min_x, max_x, min_y, max_y), dim=1)\n\n        # import ipdb\n        # ipdb.set_trace()\n\n        return rects\n\n    def load_senet_model(self, pretrain_model):\n        # saved_data = torch.load(pretrain_model, encoding=\'latin1\')\n        from utils.util import load_pickle_file\n        saved_data = load_pickle_file(pretrain_model)\n        save_weights_dict = dict()\n\n        for key, val in saved_data.items():\n            if key.startswith(\'fc\'):\n                continue\n            save_weights_dict[key] = torch.from_numpy(val)\n\n        self.net.load_state_dict(save_weights_dict)\n\n        print(\'load face model from {}\'.format(pretrain_model))\n\n    def load_sphere_model(self, pretrain_model):\n        saved_data = torch.load(pretrain_model)\n        save_weights_dict = dict()\n\n        for key, val in saved_data.items():\n            if key.startswith(\'fc6\'):\n                continue\n            save_weights_dict[key] = val\n\n        self.net.load_state_dict(save_weights_dict)\n\n        print(\'load face model from {}\'.format(pretrain_model))\n\n\nclass StyleLoss(nn.Module):\n    """"""\n    Use vgg or inception for style loss, compute the feature distance, (todo)\n    """"""\n    def __init__(self, weight=1, feat_extractors=None):\n        super(StyleLoss, self).__init__()\n        self.weight = weight\n        self.feat_extractors = feat_extractors\n\n    def gram(self, x):\n        gram_x = x.view(x.size(0), x.size(1), x.size(2)*x.size(3))\n        return torch.bmm(gram_x, torch.transpose(gram_x, 1, 2))\n\n    def forward(self, imgs, recon_imgs):\n        imgs = F.interpolate(imgs, (224, 224))\n        recon_imgs = F.interpolate(recon_imgs, (224, 224))\n        feats = self.feat_extractors(imgs)\n        recon_feats = self.feat_extractors(recon_imgs)\n        loss = 0\n        for feat, recon_feat in zip(feats, recon_feats):\n            loss = loss + torch.mean(torch.abs(self.gram(feat) - self.gram(recon_feat))) / (feat.size(2) * feat.size(3))\n        return self.weight*loss\n\n\n# class SphereFaceLoss(nn.Module):\n#\n#     def __init__(self, pretrained_path=\'assets/pretrains/sphere20a_20171020.pth\', height=112, width=96):\n#         super(SphereFaceLoss, self).__init__()\n#         self.net = Sphere20a()\n#         self.load_model(pretrained_path)\n#         self.eval()\n#         self.criterion = nn.L1Loss()\n#         self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n#\n#         self.height, self.width = height, width\n#\n#         # from utils.demo_visualizer import MotionImitationVisualizer\n#         # self._visualizer = MotionImitationVisualizer(\'debug\', ip=\'http://10.10.10.100\', port=31100)\n#\n#     def forward(self, imgs1, imgs2, kps1=None, kps2=None):\n#         """"""\n#         :param imgs1:\n#         :param imgs2:\n#         :param kps1:\n#         :param kps2:\n#         :return:\n#         """"""\n#         if kps1 is not None:\n#             head_imgs1 = self.crop_resize_head(imgs1, kps1)\n#         elif self.check_need_resize(imgs1):\n#             head_imgs1 = F.interpolate(imgs1, size=(self.height, self.width), mode=\'bilinear\', align_corners=True)\n#         else:\n#             head_imgs1 = imgs1\n#\n#         if kps2 is not None:\n#             head_imgs2 = self.crop_resize_head(imgs2, kps2)\n#         elif self.check_need_resize(imgs2):\n#             head_imgs2 = F.interpolate(imgs1, size=(self.height, self.width), mode=\'bilinear\', align_corners=True)\n#         else:\n#             head_imgs2 = imgs2\n#\n#         loss = self.compute_loss(head_imgs1, head_imgs2)\n#\n#         # self._visualizer.vis_named_img(\'img2\', imgs2)\n#         # self._visualizer.vis_named_img(\'head imgs2\', head_imgs2)\n#         #\n#         # self._visualizer.vis_named_img(\'img1\', imgs1)\n#         # self._visualizer.vis_named_img(\'head imgs1\', head_imgs1)\n#\n#         return loss\n#\n#     def compute_loss(self, img1, img2):\n#         """"""\n#         :param img1: (n, 3, 112, 96), [-1, 1]\n#         :param img2: (n, 3, 112, 96), [-1, 1], if it is used in training,\n#                      img2 is reference image (GT), use detach() to stop backpropagation.\n#         :return:\n#         """"""\n#         f1, f2 = self.net(img1), self.net(img2)\n#\n#         loss = 0.0\n#         for i in range(len(f1)):\n#             loss += self.criterion(f1[i], f2[i].detach())\n#\n#         return loss\n#\n#     def check_need_resize(self, img):\n#         return img.shape[2] != self.height or img.shape[3] != self.width\n#\n#     def crop_resize_head(self, imgs, kps):\n#         """"""\n#         :param imgs: (N, C, H, W)\n#         :param kps: (N, 19, 2)\n#         :return:\n#         """"""\n#         bs, _, ori_h, ori_w = imgs.shape\n#\n#         rects = self.find_head_rect(kps, ori_h, ori_w)\n#         head_imgs = []\n#\n#         for i in range(bs):\n#             min_x, max_x, min_y, max_y = rects[i].detach()\n#             head = imgs[i:i+1, :, min_y:max_y, min_x:max_x]  # (1, c, h\', w\')\n#             head = F.interpolate(head, size=(self.height, self.width), mode=\'bilinear\', align_corners=True)\n#             head_imgs.append(head)\n#\n#         head_imgs = torch.cat(head_imgs, dim=0)\n#\n#         return head_imgs\n#\n#     @staticmethod\n#     @torch.no_grad()\n#     def find_head_rect(kps, height, width):\n#         NECK_IDS = 12\n#\n#         kps = (kps + 1) / 2.0\n#\n#         necks = kps[:, NECK_IDS, 0]\n#         zeros = torch.zeros_like(necks)\n#         ones = torch.ones_like(necks)\n#\n#         # min_x = int(max(0.0, np.min(kps[HEAD_IDS:, 0]) - 0.1) * image_size)\n#         min_x, _ = torch.min(kps[:, NECK_IDS:, 0] - 0.05, dim=1)\n#         min_x = torch.max(min_x, zeros)\n#\n#         max_x, _ = torch.max(kps[:, NECK_IDS:, 0] + 0.05, dim=1)\n#         max_x = torch.min(max_x, ones)\n#\n#         # min_x = int(max(0.0, np.min(kps[HEAD_IDS:, 0]) - 0.1) * image_size)\n#         min_y, _ = torch.min(kps[:, NECK_IDS:, 1] - 0.05, dim=1)\n#         min_y = torch.max(min_y, zeros)\n#\n#         max_y, _ = torch.max(kps[:, NECK_IDS:, 1], dim=1)\n#         max_y = torch.min(max_y, ones)\n#\n#         min_x = (min_x * width).long()      # (T, 1)\n#         max_x = (max_x * width).long()      # (T, 1)\n#         min_y = (min_y * height).long()     # (T, 1)\n#         max_y = (max_y * height).long()     # (T, 1)\n#\n#         # print(min_x.shape, max_x.shape, min_y.shape, max_y.shape)\n#         rects = torch.stack((min_x, max_x, min_y, max_y), dim=1)\n#\n#         # import ipdb\n#         # ipdb.set_trace()\n#\n#         return rects\n#\n#     def load_model(self, pretrain_model):\n#         saved_data = torch.load(pretrain_model)\n#         save_weights_dict = dict()\n#\n#         for key, val in saved_data.items():\n#             if key.startswith(\'fc6\'):\n#                 continue\n#             save_weights_dict[key] = val\n#\n#         self.net.load_state_dict(save_weights_dict)\n#\n#         print(\'load face model from {}\'.format(pretrain_model))\n\n\nif __name__ == \'__main__\':\n    # model = Vgg19(before_relu=True)\n    face_loss = FaceLoss()\n'"
options/__init__.py,0,b''
options/base_options.py,0,"b'import argparse\nimport os\nfrom utils import util\n\n\nclass BaseOptions(object):\n    def __init__(self):\n        self._parser = argparse.ArgumentParser()\n        self._initialized = False\n\n    def initialize(self):\n        self._parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./outputs/checkpoints/\',\n                                  help=\'models are saved here\')\n\n        self._parser.add_argument(\'--data_dir\', type=str, default=\'/p300/datasets/iPER\', help=\'path to dataset\')\n        self._parser.add_argument(\'--dataset_mode\', type=str, default=\'iPER\', help=\'chooses dataset to be used\')\n        self._parser.add_argument(\'--train_ids_file\', type=str, default=\'train.txt\', help=\'file containing train ids\')\n        self._parser.add_argument(\'--test_ids_file\', type=str, default=\'val.txt\', help=\'file containing test ids\')\n        self._parser.add_argument(\'--images_folder\', type=str, default=\'images_HD\', help=\'images folder\')\n        self._parser.add_argument(\'--smpls_folder\', type=str, default=\'smpls\', help=\'smpls folder\')\n\n        self._parser.add_argument(\'--map_name\', type=str, default=\'uv_seg\', help=\'mapping function\')\n        self._parser.add_argument(\'--part_info\', type=str, default=\'assets/pretrains/smpl_part_info.json\',\n                                  help=\'smpl part info path.\')\n        self._parser.add_argument(\'--uv_mapping\', type=str, default=\'assets/pretrains/mapper.txt\',\n                                  help=\'uv mapping.\')\n        self._parser.add_argument(\'--hmr_model\', type=str, default=\'assets/pretrains/hmr_tf2pt.pth\',\n                                  help=\'pretrained hmr model path.\')\n        self._parser.add_argument(\'--smpl_model\', type=str, default=\'assets/pretrains/smpl_model.pkl\',\n                                  help=\'pretrained smpl model path.\')\n        self._parser.add_argument(\'--face_model\', type=str, default=\'assets/pretrains/sphere20a_20171020.pth\',\n                                  help=\'pretrained face model path.\')\n\n        self._parser.add_argument(\'--load_epoch\', type=int, default=-1,\n                                  help=\'which epoch to load? set to -1 to use latest cached model\')\n        self._parser.add_argument(\'--load_path\', type=str,\n                                  default=\'./outputs/checkpoints/lwb_imper_fashion_place/net_epoch_30_id_G.pth\',\n                                  help=\'pretrained model path\')\n        self._parser.add_argument(\'--batch_size\', type=int, default=4, help=\'input batch size\')\n        self._parser.add_argument(\'--time_step\', type=int, default=10, help=\'time step size\')\n        self._parser.add_argument(\'--tex_size\', type=int, default=3, help=\'input tex size\')\n        self._parser.add_argument(\'--image_size\', type=int, default=256, help=\'input image size\')\n        self._parser.add_argument(\'--repeat_num\', type=int, default=6, help=\'number of residual blocks.\')\n        self._parser.add_argument(\'--cond_nc\', type=int, default=3, help=\'# of conditions\')\n        self._parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self._parser.add_argument(\'--model\', type=str, default=\'impersonator\', help=\'model to run\')\n        self._parser.add_argument(\'--name\', type=str, default=\'running\',\n                                  help=\'name of the experiment. It decides where to store samples and models\')\n        self._parser.add_argument(\'--gen_name\', type=str, default=\'impersonator\',\n                                  help=\'chooses generator to be used, resnet or unet\')\n        self._parser.add_argument(\'--norm_type\', type=str, default=\'instance\',\n                                  help=\'choose use what norm layer in discriminator\')\n        self._parser.add_argument(\'--n_threads_test\', default=2, type=int, help=\'# threads for loading data\')\n        self._parser.add_argument(\'--serial_batches\', action=\'store_true\',\n                                  help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        self._parser.add_argument(\'--do_saturate_mask\', action=""store_true"",\n                                  default=False, help=\'do use mask_fake for mask_cyc\')\n        self._parser.add_argument(\'--bg_replace\', action=""store_true"",\n                                  default=False, help=\'replace original pixels or not\')\n        self._parser.add_argument(\'--debug\', action=""store_true"",\n                                  default=False, help=\'debug or not\')\n        self._initialized = True\n\n    def set_zero_thread_for_Win(self):\n        import platform\n        if platform.system() == \'Windows\':\n            if \'n_threads_test\' in self._opt.__dict__:\n                self._opt.__setattr__(\'n_threads_test\', 0)\n\n            if \'n_threads_train\' in self._opt.__dict__:\n                self._opt.__setattr__(\'n_threads_train\', 0)\n\n    def parse(self):\n        if not self._initialized:\n            self.initialize()\n        self._opt = self._parser.parse_args()\n\n        self.set_zero_thread_for_Win()\n\n        # set is train or set\n        self._opt.is_train = self.is_train\n\n        # set and check load_epoch\n        self._set_and_check_load_epoch()\n\n        # get and set gpus\n        self._get_set_gpus()\n\n        args = vars(self._opt)\n\n        # print in terminal args\n        self._print(args)\n\n        # save args to file\n        self._save(args)\n\n        return self._opt\n\n    def _set_and_check_load_epoch(self):\n        models_dir = os.path.join(self._opt.checkpoints_dir, self._opt.name)\n        if os.path.exists(models_dir):\n            if self._opt.load_epoch == -1:\n                load_epoch = 0\n                for file in os.listdir(models_dir):\n                    if file.startswith(""net_epoch_""):\n                        load_epoch = max(load_epoch, int(file.split(\'_\')[2]))\n                self._opt.load_epoch = load_epoch\n            else:\n                found = False\n                for file in os.listdir(models_dir):\n                    if file.startswith(""net_epoch_""):\n                        found = int(file.split(\'_\')[2]) == self._opt.load_epoch\n                        if found: break\n                assert found, \'Model for epoch %i not found\' % self._opt.load_epoch\n        else:\n            assert self._opt.load_epoch < 1, \'Model for epoch %i not found\' % self._opt.load_epoch\n            self._opt.load_epoch = 0\n\n    def _get_set_gpus(self):\n        os.environ[\'CUDA_DEVICES_ORDER\'] = ""PCI_BUS_ID""\n\n        if len(self._opt.gpu_ids) > 0:\n            os.environ[\'CUDA_VISIBLE_DEVICES\'] = self._opt.gpu_ids\n        else:\n            os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\n\n    def _print(self, args):\n        print(\'------------ Options -------------\')\n        for k, v in sorted(args.items()):\n            print(\'%s: %s\' % (str(k), str(v)))\n        print(\'-------------- End ----------------\')\n\n    def _save(self, args):\n        expr_dir = os.path.join(self._opt.checkpoints_dir, self._opt.name)\n        print(expr_dir)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt_%s.txt\' % (\'train\' if self.is_train else \'test\'))\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(\'------------ Options -------------\\n\')\n            for k, v in sorted(args.items()):\n                opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n            opt_file.write(\'-------------- End ----------------\\n\')\n'"
options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self._parser.add_argument(\'--output_dir\', type=str, default=\'./outputs/results/\',\n                                  help=\'output directory to save the results\')\n        self._parser.add_argument(\'--src_path\', type=str, default=\'\', help=\'source image path\')\n        self._parser.add_argument(\'--tgt_path\', type=str, default=\'\', help=\'target image path\')\n        self._parser.add_argument(\'--pri_path\', type=str, default=\'./assets/samples/A_priors/imgs\',\n                                  help=\'prior image path\')\n\n        self._parser.add_argument(\'--bg_model\', type=str,\n                                  default=\'./outputs/checkpoints/deepfillv2/net_epoch_50_id_G.pth\',\n                                  help=\'if it is `ORIGINAL`, it will use the \'\n                                       \'original BGNet of the generator of LiquidWarping GAN, \'\n                                       \'otherwise, set it as `./outputs/checkpoints/deepfillv2/net_epoch_50_id_G.pth`, \'\n                                       \'and it will use a pretrained deepfillv2 background inpaintor (default).\')\n\n        self._parser.add_argument(\'--bg_ks\', default=13, type=int, help=\'dilate kernel size of background mask.\')\n        self._parser.add_argument(\'--ft_ks\', default=3, type=int, help=\'dilate kernel size of front mask.\')\n        self._parser.add_argument(\'--only_vis\', action=""store_true"", default=False, help=\'only visible or not\')\n        self._parser.add_argument(\'--has_detector\', action=\'store_true\', help=\'use mask rcnn or not\')\n        self._parser.add_argument(\'--body_seg\', action=""store_true"", default=False,\n                                  help=\'use the body segmentation estimated by mask rcnn.\')\n        self._parser.add_argument(\'--front_warp\', action=""store_true"", default=False, help=\'front warp or not\')\n        self._parser.add_argument(\'--post_tune\', action=""store_true"", default=False, help=\'post tune or not\')\n\n        # Human motion imitation\n        self._parser.add_argument(\'--cam_strategy\', type=str, default=\'smooth\', choices=[\'smooth\', \'source\', \'copy\'],\n                                  help=\'the strategy to control the camera pameters (s, x, y) \'\n                                       \'betwwen the source and reference image.\')\n\n        # Human appearance transfer\n        self._parser.add_argument(\'--swap_part\', type=str, default=\'body\', help=\'part to swap\')\n\n        # Novel view synthesis\n        self._parser.add_argument(\'--T_pose\', action=""store_true"", default=False, help=\'view as T pose or not.\')\n        self._parser.add_argument(\'--view_params\', type=str, default=\'R=0,90,0/t=0,0,0\', help=\'params of novel view.\')\n\n        # visualizer\n        self._parser.add_argument(\'--ip\', type=str, default=\'\', help=\'visdom ip\')\n        self._parser.add_argument(\'--port\', type=int, default=31100, help=\'visdom port\')\n\n        # save results or not\n        self._parser.add_argument(\'--save_res\', action=\'store_true\', default=False,\n                                  help=\'save images or not, if true, the results are saved in `${output_dir}/preds`.\')\n\n        self.is_train = False\n'"
options/train_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n\n        # use place dataset if need\n        self._parser.add_argument(\'--place_dir\', type=str, default=\'/p300/places365_standard\', help=\'place folder\')\n        self._parser.add_argument(\'--place_bs\', type=int, default=4, help=\'input batch size of place dataset\')\n\n        # use deep fashion dataset if need\n        self._parser.add_argument(\'--fashion_dir\', type=str, default=\'/public/deep_fashion/intrinsic\', help=\'place folder\')\n        self._parser.add_argument(\'--fashion_bs\', type=int, default=4, help=\'input batch size of fashion dataset\')\n\n        self._parser.add_argument(\'--intervals\', type=int, default=10, help=\'the interval between frames.\')\n        self._parser.add_argument(\'--n_threads_train\', default=4, type=int, help=\'# threads for loading data\')\n        self._parser.add_argument(\'--num_iters_validate\', default=1, type=int, help=\'# batches to use when validating\')\n        self._parser.add_argument(\'--print_freq_s\', type=int, default=60, help=\'frequency of showing training results on console\')\n        self._parser.add_argument(\'--display_freq_s\', type=int, default=300, help=\'frequency [s] of showing training results on screen\')\n        self._parser.add_argument(\'--save_latest_freq_s\', type=int, default=3600, help=\'frequency of saving the latest results\')\n\n        self._parser.add_argument(\'--bg_both\', action=""store_true"", help=\'inpainting both source and target background or not.\')\n        self._parser.add_argument(\'--use_vgg\', action=\'store_true\', help=\'whether to use VGG loss or L1 loss, if true use VGG, other use L1, default is L1\')\n        self._parser.add_argument(\'--use_style\', action=\'store_true\', help=\'whether to use style loss or not\')\n        self._parser.add_argument(\'--use_face\', action=\'store_true\', help=\'whether to use face loss or not\')\n        self._parser.add_argument(\'--mask_bce\', action=\'store_true\', help=\'whether to use CrossEntropyLoss or L1 loss in mask or not.\')\n        self._parser.add_argument(\'--nepochs_no_decay\', type=int, default=10, help=\'# of epochs at starting learning rate\')\n        self._parser.add_argument(\'--nepochs_decay\', type=int, default=20, help=\'# of epochs to linearly decay learning rate to zero\')\n\n        self._parser.add_argument(\'--train_G_every_n_iterations\', type=int, default=1, help=\'train G every n interations\')\n        self._parser.add_argument(\'--final_lr\', type=float, default=0.000002, help=\'final learning rate\')\n        self._parser.add_argument(\'--lr_G\', type=float, default=0.0002, help=\'initial learning rate for G adam\')\n        self._parser.add_argument(\'--G_adam_b1\', type=float, default=0.5, help=\'beta1 for G adam\')\n        self._parser.add_argument(\'--G_adam_b2\', type=float, default=0.999, help=\'beta2 for G adam\')\n        self._parser.add_argument(\'--lr_D\', type=float, default=0.0002, help=\'initial learning rate for D adam\')\n        self._parser.add_argument(\'--D_adam_b1\', type=float, default=0.5, help=\'beta1 for D adam\')\n        self._parser.add_argument(\'--D_adam_b2\', type=float, default=0.999, help=\'beta2 for D adam\')\n        self._parser.add_argument(\'--lambda_D_prob\', type=float, default=1, help=\'lambda for real/fake discriminator loss\')\n        self._parser.add_argument(\'--lambda_rec\', type=float, default=10, help=\'lambda SID loss\')\n        self._parser.add_argument(\'--lambda_tsf\', type=float, default=10, help=\'lambda TSF loss\')\n        self._parser.add_argument(\'--lambda_style\', type=float, default=5, help=\'lambda style loss\')\n        self._parser.add_argument(\'--lambda_face\', type=float, default=1, help=\'lambda face loss\')\n        self._parser.add_argument(\'--lambda_mask\', type=float, default=0.1, help=\'lambda mask loss\')\n        self._parser.add_argument(\'--lambda_mask_smooth\', type=float, default=1e-5, help=\'lambda mask smooth loss\')\n\n        self.is_train = True\n'"
tools/unzip_iPER.py,0,"b'import os\nimport glob\nfrom tqdm import tqdm\n\n\n# Replacing them as your own folder\ndataset_video_root_path = \'/p300/tpami/iPER_examples/iPER_256_video_release\'\nsave_images_root_path = \'/p300/tpami/iPER_examples/images\'\n\n\ndef extract_one_video(video_path, save_dir):\n    os.makedirs(save_dir, exist_ok=True)\n    os.system(""ffmpeg -i %s -start_number 0 %s/frame%%08d.png > /dev/null 2>&1"" % (video_path, save_dir))\n\n\ndef main():\n    global dataset_video_root_path, save_images_root_path\n\n    video_path_list = sorted(glob.glob(""%s/*.mp4"" % dataset_video_root_path))\n\n    for video_path in tqdm(video_path_list):\n        video_name = os.path.split(video_path)[-1][:-4]\n        actor_id, cloth_id, action_type = video_name.split(\'_\')\n\n        video_images_dir = os.path.join(save_images_root_path, actor_id, cloth_id, action_type)\n        extract_one_video(video_path, video_images_dir)\n\n        # import ipdb\n        # ipdb.set_trace()\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
tools/visual_iPER.py,5,"b""import cv2\nimport torch\nimport numpy as np\nimport os\nimport os.path as osp\nimport glob\nfrom tqdm import tqdm\nimport time\nfrom utils.visdom_visualizer import VisdomVisualizer\nfrom utils.nmr import SMPLRenderer\nfrom networks.hmr import HumanModelRecovery\n\nos.environ['CUDA_DEVICES_ORDER'] = 'PCI_BUS_ID'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n\nIMG_SIZE = 256\nvisualizer = VisdomVisualizer(env='visual', ip='http://10.10.10.100', port=31102)\n\n\ndef visual(model, out_dir):\n    global visualizer\n\n    render = SMPLRenderer(image_size=IMG_SIZE).cuda()\n\n    texs = render.debug_textures().cuda()[None]\n\n    with h5py.File(osp.join(out_dir, 'smpl_infos.h5'), 'r') as reader:\n        cams_crop = reader['cam_crop']\n        poses = reader['pose']\n        shapes = reader['shape']\n        frame_ids = reader['f_id']\n\n        scan_image_paths = sorted(glob.glob(osp.join(out_dir, 'cropped_frames', '*.png')))\n\n        for i in range(len(frame_ids) - 1):\n            assert frame_ids[i] < frame_ids[i + 1]\n\n        image_paths = [scan_image_paths[f_id] for f_id in frame_ids]\n\n        for i in tqdm(range(len(image_paths))):\n            im_path = image_paths[i]\n            image = cv2.imread(im_path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = np.transpose(image, (2, 0, 1))\n            image = image.astype(np.float) / 255\n            image = torch.tensor(image).float()[None].cuda()\n\n            cams = torch.tensor(cams_crop[i]).float()[None].cuda()\n            pose = torch.tensor(poses[i]).float()[None].cuda()\n            shape = torch.tensor(shapes[i]).float()[None].cuda()\n            verts, _, _ = model.smpl(beta=shape, theta=pose, get_skin=True)\n            rd_imgs, _ = render.render(cams, verts, texs.clone())\n            sil = render.render_silhouettes(cams, verts)\n\n            masked_img = image * sil[:, None, :, :]\n\n            visualizer.vis_named_img('rd_imgs', rd_imgs, denormalize=False)\n            visualizer.vis_named_img('masked_img', masked_img, denormalize=False)\n            visualizer.vis_named_img('imgs', image, denormalize=False)\n\n            time.sleep(1)\n\n\nif __name__ == '__main__':\n\n    hmr = HumanModelRecovery(smpl_pkl_path='./assets/pretrains/smpl_model.pkl')\n    hmr.load_state_dict(torch.load('./assets/pretrains/hmr_tf2pt.pth'))\n    hmr = hmr.eval().cuda()\n\n    ## process neuralAvatar\n    src_path = '/p300/tpami/neuralAvatar/original/wenliu_fps_30.mp4'\n    out_dir = '/p300/tpami/neuralAvatar/processed/wenliu_fps_30'\n\n    # src_path = '/p300/tpami/neuralAvatar/processed/wenliu_fps_30/frames/frame00000000.png'\n    # out_dir = '/p300/tpami/neuralAvatar/processed/frame00000000'\n    rescale = None\n    is_visual = True\n\n    process(hmr,\n            src_path=src_path,\n            output_dir=out_dir,\n            save_crop=True,\n            rescale=rescale)\n\n    if is_visual:\n        visual(hmr, out_dir=out_dir)\n"""
utils/__init__.py,0,b''
utils/cv_utils.py,0,"b'import cv2\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nHMR_IMG_SIZE = 224\nIMG_SIZE = 256\n\n\ndef read_cv2_img(path):\n    """"""\n    Read color images\n    :param path: Path to image\n    :return: Only returns color images\n    """"""\n    img = cv2.imread(path, -1)\n\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    return img\n\n\ndef save_cv2_img(img, path, image_size=None, normalize=False):\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\n    # print(\'normalize = {}\'.format(normalize))\n\n    if image_size is not None:\n        img = cv2.resize(img, (image_size, image_size))\n\n    if normalize:\n        img = (img + 1) / 2.0 * 255\n        img = img.astype(np.uint8)\n\n    cv2.imwrite(path, img)\n    return img\n\n\ndef transform_img(image, image_size, transpose=False):\n    image = cv2.resize(image, (image_size, image_size))\n    image = image.astype(np.float32)\n    image /= 255.0\n\n    if transpose:\n        image = image.transpose((2, 0, 1))\n\n    return image\n\n\ndef resize_img_with_scale(img, scale_factor):\n    new_size = (np.floor(np.array(img.shape[0:2]) * scale_factor)).astype(int)\n    new_img = cv2.resize(img, (new_size[1], new_size[0]))\n    # This is scale factor of [height, width] i.e. [y, x]\n    actual_factor = [\n        new_size[0] / float(img.shape[0]), new_size[1] / float(img.shape[1])\n    ]\n    return new_img, actual_factor\n\n\ndef kp_to_bbox_param(kp, vis_thresh=0, diag_len=150.0):\n    """"""\n    Finds the bounding box parameters from the 2D keypoints.\n\n    Args:\n        kp (Kx3): 2D Keypoints.\n        vis_thresh (float): Threshold for visibility.\n        diag_len(float): diagonal length of bbox of each person\n\n    Returns:\n        [center_x, center_y, scale]\n    """"""\n    if kp is None:\n        return\n\n    if kp.shape[1] == 3:\n        vis = kp[:, 2] > vis_thresh\n        if not np.any(vis):\n            return\n        min_pt = np.min(kp[vis, :2], axis=0)\n        max_pt = np.max(kp[vis, :2], axis=0)\n    else:\n        min_pt = np.min(kp, axis=0)\n        max_pt = np.max(kp, axis=0)\n\n    person_height = np.linalg.norm(max_pt - min_pt)\n    if person_height < 0.5:\n        return\n    center = (min_pt + max_pt) / 2.\n    scale = diag_len / person_height\n\n    return np.append(center, scale)\n\n\ndef cal_process_params(im_path, bbox_param, rescale=None, image=None, image_size=IMG_SIZE, proc=False):\n    """"""\n    Args:\n        im_path (str): the path of image.\n        image (np.ndarray or None): if it is None, then loading the im_path, else use image.\n        bbox_param (3,) : [cx, cy, scale].\n        rescale (float, np.ndarray or None): rescale factor.\n        proc (bool): the flag to return processed image or not.\n        image_size (int): the cropped image.\n\n    Returns:\n        proc_img (np.ndarray): if proc is True, return the process image, else return the original image.\n    """"""\n    if image is None:\n        image = read_cv2_img(im_path)\n\n    orig_h, orig_w = image.shape[0:2]\n    center = bbox_param[:2]\n    scale = bbox_param[2]\n    if rescale is not None:\n        scale = rescale\n\n    if proc:\n        image_scaled, scale_factors = resize_img_with_scale(image, scale)\n        resized_h, resized_w = image_scaled.shape[:2]\n    else:\n        scale_factors = [scale, scale]\n        resized_h = orig_h * scale\n        resized_w = orig_w * scale\n\n    center_scaled = np.round(center * scale_factors).astype(np.int)\n\n    if proc:\n        # Make sure there is enough space to crop image_size x image_size.\n        image_padded = np.pad(\n            array=image_scaled,\n            pad_width=((image_size,), (image_size,), (0,)),\n            mode=\'edge\'\n        )\n        padded_h, padded_w = image_padded.shape[0:2]\n    else:\n        padded_h = resized_h + image_size * 2\n        padded_w = resized_w + image_size * 2\n\n    center_scaled += image_size\n\n    # Crop image_size x image_size around the center.\n    margin = image_size // 2\n    start_pt = (center_scaled - margin).astype(int)\n    end_pt = (center_scaled + margin).astype(int)\n    end_pt[0] = min(end_pt[0], padded_w)\n    end_pt[1] = min(end_pt[1], padded_h)\n\n    if proc:\n        proc_img = image_padded[start_pt[1]:end_pt[1], start_pt[0]:end_pt[0], :]\n        height, width = image_scaled.shape[:2]\n    else:\n        height, width = end_pt[1] - start_pt[1], end_pt[0] - start_pt[0]\n        proc_img = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n        # proc_img = None\n\n    center_scaled -= start_pt\n    im_shape = [height, width]\n\n    return {\n        # return original too with info.\n        \'image\': proc_img,\n        \'im_path\': im_path,\n        \'im_shape\': im_shape,\n        \'orig_im_shape\': [orig_h, orig_w],\n        \'center\': center_scaled,\n        \'scale\': scale,\n        \'start_pt\': start_pt,\n    }\n\n\ndef cam_denormalize(cam, N):\n    # This is camera in crop image coord.\n    new_cam = np.hstack([N * cam[0] * 0.5, cam[1:] + (2. / cam[0]) * 0.5])\n    return new_cam\n\n\ndef cam_init2orig(cam, scale, start_pt, N=HMR_IMG_SIZE):\n    """"""\n    Args:\n        cam (3,): (s, tx, ty)\n        scale (float): scale = resize_h / orig_h\n        start_pt (2,): (lt_x, lt_y)\n        N (int): hmr_image_size (224) or IMG_SIZE\n\n    Returns:\n        cam_orig (3,): (s, tx, ty), camera in original image coordinates.\n\n    """"""\n    # This is camera in crop image coord.\n    cam_crop = np.hstack([N * cam[0] * 0.5, cam[1:] + (2. / cam[0]) * 0.5])\n\n    print(\'cam_init\', cam)\n    print(\'cam_crop\', cam_crop)\n\n    # This is camera in orig image coord\n    cam_orig = np.hstack([\n        cam_crop[0] / scale,\n        cam_crop[1:] + (start_pt - N) / cam_crop[0]\n    ])\n    print(\'cam_orig\', cam_orig)\n    return cam_orig\n\n\ndef cam_orig2crop(cam, scale, start_pt, N=IMG_SIZE, normalize=True):\n    """"""\n    Args:\n        cam (3,): (s, tx, ty), camera in orginal image coordinates.\n        scale (float): scale = resize_h / orig_h or (resize_w / orig_w)\n        start_pt (2,): (lt_x, lt_y)\n        N (int): hmr_image_size (224) or IMG_SIZE\n        normalize (bool)\n\n    Returns:\n\n    """"""\n    cam_recrop = np.hstack([\n        cam[0] * scale,\n        cam[1:] + (N - start_pt) / (scale * cam[0])\n    ])\n    if normalize:\n        cam_norm = np.hstack([\n            cam_recrop[0] * (2. / N),\n            cam_recrop[1:] - N / (2 * cam_recrop[0])\n        ])\n    else:\n        cam_norm = cam_recrop\n    return cam_norm\n\n\ndef cam_process(cam_init, scale_150, start_pt_150, scale_proc, start_pt_proc, image_size):\n    """"""\n    Args:\n        cam_init:\n        scale_150:\n        start_pt_150:\n        scale_proc:\n        start_pt_proc:\n        image_size\n\n    Returns:\n\n    """"""\n    cam_orig = cam_init2orig(cam_init, scale=scale_150, start_pt=start_pt_150, N=HMR_IMG_SIZE)\n    cam_crop = cam_orig2crop(cam_orig, scale=scale_proc, start_pt=start_pt_proc, N=image_size, normalize=True)\n\n    return cam_crop\n\n\ndef show_cv2_img(img, title=\'img\'):\n    \'\'\'\n    Display cv2 image\n    :param img: cv::mat\n    :param title: title\n    :return: None\n    \'\'\'\n    plt.imshow(img)\n    plt.title(title)\n    plt.axis(\'off\')\n    plt.show()\n\n\ndef show_images_row(imgs, titles, rows=1):\n    """"""\n       Display grid of cv2 images image\n       :param img: list [cv::mat]\n       :param title: titles\n       :return: None\n    """"""\n    assert ((titles is None) or (len(imgs) == len(titles)))\n    num_images = len(imgs)\n\n    if titles is None:\n        titles = [\'Image (%d)\' % i for i in range(1, num_images + 1)]\n\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(imgs, titles)):\n        ax = fig.add_subplot(rows, np.ceil(num_images / float(rows)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image)\n        ax.set_title(title)\n        plt.axis(\'off\')\n    plt.show()\n\n\ndef intrinsic_mtx(f, c):\n    """"""\n    Obtain intrisic camera matrix.\n    Args:\n        f: np.array, 1 x 2, the focus lenth of camera, (fx, fy)\n        c: np.array, 1 x 2, the center of camera, (px, py)\n    Returns:\n        - cam_mat: np.array, 3 x 3, the intrisic camera matrix.\n    """"""\n    return np.array([[f[1], 0, c[1]],\n                     [0, f[0], c[0]],\n                     [0, 0, 1]], dtype=np.float32)\n\n\ndef extrinsic_mtx(rt, t):\n    """"""\n    Obtain extrinsic matrix of camera.\n    Args:\n        rt: np.array, 1 x 3, the angle of rotations.\n        t: np.array, 1 x 3, the translation of camera center.\n    Returns:\n        - ext_mat: np.array, 3 x 4, the extrinsic matrix of camera.\n    """"""\n    # R is (3, 3)\n    R = cv2.Rodrigues(rt)[0]\n    t = np.reshape(t, newshape=(3, 1))\n    Rc = np.dot(R, t)\n    ext_mat = np.hstack((R, -Rc))\n    ext_mat = np.vstack((ext_mat, [0, 0, 0, 1]))\n    ext_mat = ext_mat.astype(np.float32)\n    return ext_mat\n\n\ndef extrinsic(rt, t):\n    """"""\n    Obtain extrinsic matrix of camera.\n    Args:\n        rt: np.array, 1 x 3, the angle of rotations.\n        t: np.array, 1 x 3, or (3,) the translation of camera center.\n    Returns:\n        - R: np.ndarray, 3 x 3\n        - t: np.ndarray, 1 x 3\n    """"""\n    R = cv2.Rodrigues(rt)[0]\n    t = np.reshape(t, newshape=(1, 3))\n    return R, t\n\n\ndef euler2matrix(rt):\n    """"""\n    Obtain rotation matrix from euler angles\n    Args:\n        rt: np.array, (3,)\n    Returns:\n        R: np.array, (3,3)\n    """"""\n    Rx = np.array([[1, 0,             0],\n                   [0, np.cos(rt[0]), -np.sin(rt[0])],\n                   [0, np.sin(rt[0]), np.cos(rt[0])]], dtype=np.float32)\n\n    Ry = np.array([[np.cos(rt[1]),     0,       np.sin(rt[1])],\n                   [0,                 1,       0],\n                   [-np.sin(rt[1]),    0,       np.cos(rt[1])]], dtype=np.float32)\n\n    Rz = np.array([[np.cos(rt[2]),     -np.sin(rt[2]),       0],\n                   [np.sin(rt[2]),      np.cos(rt[2]),       0],\n                   [0,                              0,       1]], dtype=np.float32)\n\n    return np.dot(Rz, np.dot(Ry, Rx))\n\n\ndef get_rotated_smpl_pose(pose, theta):\n    """"""\n    :param pose: (72,)\n    :param theta: rotation angle of y axis\n    :return:\n    """"""\n    global_pose = pose[:3]\n    R, _ = cv2.Rodrigues(global_pose)\n    Ry = np.array([\n        [np.cos(theta), 0, np.sin(theta)],\n        [0, 1, 0],\n        [-np.sin(theta), 0, np.cos(theta)]\n    ])\n    new_R = np.matmul(R, Ry)\n    new_global_pose, _ = cv2.Rodrigues(new_R)\n    new_global_pose = new_global_pose.reshape(3)\n\n    rotated_pose = pose.copy()\n    rotated_pose[:3] = new_global_pose\n\n    return rotated_pose\n\n\nif __name__ == \'__main__\':\n    # Checks if a matrix is a valid rotation matrix.\n    def isRotationMatrix(R):\n        Rt = np.transpose(R)\n        shouldBeIdentity = np.dot(Rt, R)\n        I = np.identity(3, dtype=R.dtype)\n        n = np.linalg.norm(I - shouldBeIdentity)\n        return n < 1e-6\n\n    R = euler2matrix(np.array([0, 90, 0], dtype=np.float32))\n\n    print(isRotationMatrix(R))\n\n'"
utils/detectors.py,3,"b'import torch\nimport torchvision\n\nfrom utils.util import morph\n\n\nclass PersonMaskRCNNDetector(object):\n    COCO_INSTANCE_CATEGORY_NAMES = [\n        \'__background__\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\',\n        \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\', \'N/A\', \'stop sign\',\n        \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\', \'sheep\', \'cow\',\n        \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'N/A\', \'backpack\', \'umbrella\', \'N/A\', \'N/A\',\n        \'handbag\', \'tie\', \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\',\n        \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\', \'surfboard\', \'tennis racket\',\n        \'bottle\', \'N/A\', \'wine glass\', \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\',\n        \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\',\n        \'donut\', \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\', \'N/A\', \'dining table\',\n        \'N/A\', \'N/A\', \'toilet\', \'N/A\', \'tv\', \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell phone\',\n        \'microwave\', \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'N/A\', \'book\',\n        \'clock\', \'vase\', \'scissors\', \'teddy bear\', \'hair drier\', \'toothbrush\'\n    ]\n\n    PERSON_IDS = 1\n\n    def __init__(self, ks=3, threshold=0.5, to_gpu=True):\n        super(PersonMaskRCNNDetector, self).__init__()\n\n        self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n        self.model.eval()\n\n        self.threshold = threshold\n        self.ks = ks\n        self.kernel = torch.ones(1, 1, ks, ks, dtype=torch.float32)\n\n        if to_gpu:\n            self.model = self.model.cuda()\n            self.kernel = self.kernel.cuda()\n\n    def forward(self, images):\n        predictions = self.model(images)\n        return predictions\n\n    def get_bbox_max_ids(self, labels, bboxs):\n        """"""\n        Args:\n            labels:\n            bboxs: [N, 4], [x0, y0, x1, y1]\n\n        Returns:\n\n        """"""\n\n        cur_pid = -1\n        cur_bbox_area = -1\n        for i, label in enumerate(labels):\n            if label == self.PERSON_IDS:\n                x0, y0, x1, y1 = bboxs[i]\n                cur_area = torch.abs((x1 - x0) * (y1 - y0))\n\n                if cur_area > cur_bbox_area:\n                    cur_bbox_area = cur_area\n                    cur_pid = i\n\n        return cur_pid\n\n    def inference(self, img):\n        img_list = [(img + 1) / 2.0]\n\n        with torch.no_grad():\n            predictions = self.forward(img_list)[0]\n            labels = predictions[\'labels\']\n            bboxs = predictions[\'boxes\']\n            masks = predictions[\'masks\']\n\n            pid = self.get_bbox_max_ids(labels, bboxs)\n\n            pid_bboxs = bboxs[pid]\n            pid_masks = masks[pid]\n\n            final_masks = (pid_masks > self.threshold).float()\n\n            if self.ks > 0:\n                final_masks = morph(final_masks[None], self.ks, mode=\'dilate\', kernel=self.kernel)\n\n            return pid_bboxs, final_masks\n\n\n\n\n\n\n\n\n'"
utils/mesh.py,16,"b'import itertools\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport json\n\n\ndef save_to_obj(verts, faces, path):\n    """"""\n    Save the SMPL model into .obj file.\n\n    Parameter:\n    ---------\n    path: Path to save.\n\n    """"""\n\n    with open(path, \'w\') as fp:\n        fp.write(\'g\\n\')\n        for v in verts:\n            fp.write(\'v %f %f %f\\n\' % (v[0], v[1], v[2]))\n        for f in faces + 1:\n            fp.write(\'f %d %d %d\\n\' % (f[0], f[1], f[2]))\n        fp.write(\'s off\\n\')\n\n\ndef load_obj(obj_file):\n    with open(obj_file, \'r\') as fp:\n        verts = []\n        faces = []\n        vts = []\n        vns = []\n        faces_vts = []\n        faces_vns = []\n\n        for line in fp:\n            line = line.rstrip()\n            line_splits = line.split()\n            prefix = line_splits[0]\n\n            if prefix == \'v\':\n                verts.append(np.array([line_splits[1], line_splits[2], line_splits[3]], dtype=np.float32))\n\n            elif prefix == \'vn\':\n                vns.append(np.array([line_splits[1], line_splits[2], line_splits[3]], dtype=np.float32))\n\n            elif prefix == \'vt\':\n                vts.append(np.array([line_splits[1], line_splits[2]], dtype=np.float32))\n\n            elif prefix == \'f\':\n                f = []\n                f_vt = []\n                f_vn = []\n                for p_str in line_splits[1:4]:\n                    p_split = p_str.split(\'/\')\n                    f.append(p_split[0])\n                    f_vt.append(p_split[1])\n                    f_vn.append(p_split[2])\n\n                faces.append(np.array(f, dtype=np.int32) - 1)\n                faces_vts.append(np.array(f_vt, dtype=np.int32) - 1)\n                faces_vns.append(np.array(f_vn, dtype=np.int32) - 1)\n\n            else:\n                raise ValueError(prefix)\n\n        obj_dict = {\n            \'vertices\': np.array(verts, dtype=np.float32),\n            \'faces\': np.array(faces, dtype=np.int32),\n            \'vts\': np.array(vts, dtype=np.float32),\n            \'vns\': np.array(vns, dtype=np.float32),\n            \'faces_vts\': np.array(faces_vts, dtype=np.int32),\n            \'faces_vns\': np.array(faces_vns, dtype=np.int32)\n        }\n\n        return obj_dict\n\n\ndef sample_textures(texture_flow, images):\n    """"""\n    texture_flow: B x F x T x T x 2\n    (In normalized coordinate [-1, 1])\n    images: B x 3 x N x N\n\n    output: B x F x T x T x 3\n    """"""\n    # Reshape into B x F x T*T x 2\n    T = texture_flow.size(-2)\n    F = texture_flow.size(1)\n    flow_grid = texture_flow.view(-1, F, T * T, 2)\n    # B x 3 x F x T*T\n    samples = torch.nn.functional.grid_sample(images, flow_grid)\n    # B x 3 x F x T x T\n    samples = samples.view(-1, 3, F, T, T)\n    # B x F x T x T x 3\n    return samples.permute(0, 2, 3, 4, 1)\n\n\ndef get_spherical_coords(X):\n    # X is N x 3\n    rad = np.linalg.norm(X, axis=1)\n    # Inclination\n    theta = np.arccos(X[:, 2] / rad)\n    # Azimuth\n    phi = np.arctan2(X[:, 1], X[:, 0])\n\n    # Normalize both to be between [-1, 1]\n    vv = (theta / np.pi) * 2 - 1\n    uu = ((phi + np.pi) / (2 * np.pi)) * 2 - 1\n    # Return N x 2\n    return np.stack([uu, vv], 1)\n\n\ndef compute_coords(tex_size):\n    """"""\n    :param tex_size:\n    :return: (2, T*T)\n    """"""\n    alpha = np.arange(tex_size, dtype=np.float) / (tex_size - 1)\n    beta = np.arange(tex_size, dtype=np.float) / (tex_size - 1)\n    # Barycentric coordinate values\n    coords = np.stack([p for p in itertools.product(*[alpha, beta])])  # T*T x 2\n    coords = torch.FloatTensor(coords.T)  # (2, T*T)\n    return coords\n\n\ndef compute_uvsampler(verts, faces, tex_size=2):\n    """"""\n    For this mesh, pre-computes the UV coordinates for\n    F x T x T points.\n    Returns F x T x T x 2\n    """"""\n    alpha = np.arange(tex_size, dtype=np.float) / (tex_size - 1)\n    beta = np.arange(tex_size, dtype=np.float) / (tex_size - 1)\n    # Barycentric coordinate values\n    coords = np.stack([p for p in itertools.product(*[alpha, beta])])  # 36 x 2\n    vs = verts[faces]\n    # Compute alpha, beta (this is the same order as NMR)\n    v2 = vs[:, 2]  # (656, 3)\n    v0v2 = vs[:, 0] - vs[:, 2]  # (656, 3)\n    v1v2 = vs[:, 1] - vs[:, 2]  # (656, 3)\n    # F x 3 x T*2\n    samples = np.dstack([v0v2, v1v2]).dot(coords.T) + v2.reshape(-1, 3, 1)\n    # F x T*2 x 3 points on the sphere\n    samples = np.transpose(samples, (0, 2, 1))\n\n    # Now convert these to uv.\n    uv = get_spherical_coords(samples.reshape(-1, 3))\n    # uv = uv.reshape(-1, len(coords), 2)\n\n    uv = uv.reshape(-1, tex_size, tex_size, 2)\n    return uv\n\n\ndef compute_barycenter(f2vts):\n    """"""\n\n    :param f2vts:  F x 3 x 2\n    :return: F x 2\n    """"""\n\n    # Compute alpha, beta (this is the same order as NMR)\n    v2 = f2vts[:, 2]  # (nf, 2)\n    v0v2 = f2vts[:, 0] - f2vts[:, 2]  # (nf, 2)\n    v1v2 = f2vts[:, 1] - f2vts[:, 2]  # (nf, 2)\n\n    fbc = v2 + 0.5 * v0v2 + 0.5 * v1v2\n\n    return fbc\n\n\ndef get_f2vts(uv_mapping_path, fill_back=False):\n    """"""\n    For this mesh, pre-computes the bary-center coords.\n    Returns F x 2\n    """"""\n    obj_info = load_obj(uv_mapping_path)\n\n    vts = obj_info[\'vts\']\n    vts[:, 1] = 1 - vts[:, 1]\n    # vts = vts * 2 - 1\n\n    # F x (2 + 1) = F x 3\n    vts = np.concatenate([vts, np.zeros((vts.shape[0], 1), dtype=np.float32)], axis=-1)\n    faces = obj_info[\'faces_vts\']\n\n    if fill_back:\n        faces = np.concatenate((faces, faces[:, ::-1]), axis=0)\n\n    # F x 3 x 3\n    f2vts = vts[faces]\n\n    return f2vts\n\n\ndef create_uvf2vts(uv_mapping_path, add_z=True):\n    obj_info = load_obj(uv_mapping_path)\n\n    vts = obj_info[\'vts\']\n    vts[:, 1] = 1 - vts[:, 1]\n    # vts = vts * 2 - 1\n\n    # F x (2 + 1) = F x 3\n    if add_z:\n        vts = np.concatenate([vts, np.zeros((vts.shape[0], 1), dtype=np.float32) + 5], axis=-1)\n    faces_vts = obj_info[\'faces_vts\']\n\n    # F x 3 x 3 or F x 3 x2\n    uvf2vts = vts[faces_vts]\n    return uvf2vts\n\n\ndef get_head_front_ids(nf, front_face_info, fill_back=False):\n\n    if fill_back:\n        half_nf = nf // 2\n\n    with open(front_face_info, \'r\') as reader:\n        front_data = json.load(reader)\n\n        faces = front_data[\'face\']\n\n        if fill_back:\n            faces = faces + [f + half_nf for f in faces]\n\n    return faces\n\n\ndef get_head_back_ids(nf, head_face_info, front_face_info, fill_back=False):\n\n    if fill_back:\n        half_nf = nf // 2\n\n    with open(head_face_info, \'r\') as reader:\n        head_faces = set(json.load(reader)[\'face\'])\n        with open(front_face_info, \'r\') as front_reader:\n            front_faces = set(json.load(front_reader)[\'face\'])\n\n        faces = list(head_faces - front_faces)\n        if fill_back:\n            faces = faces + [f + half_nf for f in faces]\n\n    return faces\n\n\ndef get_part_ids(nf, part_info, fill_back=False):\n    if fill_back:\n        half_nf = nf // 2\n    with open(part_info, \'r\') as reader:\n        part_data = json.load(reader)\n\n        part_names = sorted(part_data.keys())\n\n        total_faces = set()\n        ordered_faces = dict()\n        for i, part_name in enumerate(part_names):\n            part_vals = part_data[part_name]\n            faces = part_vals[\'face\']\n            if fill_back:\n                faces = faces + [f + half_nf for f in faces]\n            ordered_faces[part_name] = faces\n            total_faces |= set(faces)\n\n        nf_counter = len(total_faces)\n        assert nf_counter == nf, \'nf_counter = {}, nf = {}\'.format(nf_counter, nf)\n\n    return ordered_faces\n\n\ndef binary_mapping(nf):\n\n    width = len(np.binary_repr(nf))\n    map_fn = [np.array(list(map(int, np.binary_repr(i, width=width)))) for i in range(nf)]\n    map_fn = np.stack(map_fn, axis=0)\n\n    bg = np.zeros((1, width), dtype=np.float32) - 1.0\n\n    return map_fn, bg\n\n\ndef ids_mapping(nf):\n    map_fn = np.arange(0, 1, 1/nf, dtype=np.float32)\n    bg = np.array([[-1]], dtype=np.float32)\n    return map_fn, bg\n\n\ndef par_mapping(nf, part_info, fill_back=False):\n\n    if fill_back:\n        half_nf = nf // 2\n    with open(part_info, \'r\') as reader:\n        part_data = json.load(reader)\n\n        ndim = len(part_data) + 1 # 10\n        map_fn = np.zeros((nf, ndim), dtype=np.float32)\n\n        part_names = sorted(part_data.keys())\n\n        total_faces = set()\n        for i, part_name in enumerate(part_names):\n            part_vals = part_data[part_name]\n            faces = part_vals[\'face\']\n\n            if fill_back:\n                faces = faces + [f + half_nf for f in faces]\n\n            map_fn[faces, i] = 1.0\n            total_faces |= set(faces)\n\n        nf_counter = len(total_faces)\n        assert nf_counter == nf, \'nf_counter = {}, nf = {}\'.format(nf_counter, nf)\n\n        # add bg\n        bg = np.zeros((1, ndim), dtype=np.float32)\n        bg[0, -1] = 1\n\n        return map_fn, bg\n\n\ndef front_face_mapping(nf, front_face_info, fill_back=False):\n\n    if fill_back:\n        half_nf = nf // 2\n\n    map_fn = np.zeros((nf, 1), dtype=np.float32)\n\n    with open(front_face_info, \'r\') as reader:\n        front_data = json.load(reader)\n\n        faces = front_data[\'face\']\n\n        if fill_back:\n            faces = faces + [f + half_nf for f in faces]\n\n        map_fn[faces] = 1.0\n\n    # add bg\n    bg = np.zeros((1, 1), dtype=np.float32)\n\n    return map_fn, bg\n\n\ndef back_face_mapping(nf, head_face_info, front_face_info, fill_back=False):\n\n    if fill_back:\n        half_nf = nf // 2\n\n    map_fn = np.zeros((nf, 1), dtype=np.float32)\n\n    with open(head_face_info, \'r\') as reader:\n        head_faces = set(json.load(reader)[\'face\'])\n        with open(front_face_info, \'r\') as front_reader:\n            front_faces = set(json.load(front_reader)[\'face\'])\n\n        faces = list(head_faces - front_faces)\n        if fill_back:\n            faces = faces + [f + half_nf for f in faces]\n\n        map_fn[faces] = 1.0\n\n    # add bg\n    bg = np.zeros((1, 1), dtype=np.float32)\n\n    return map_fn, bg\n\n\ndef create_mapping(map_name, mapping_path=\'assets/pretrains/mapper.txt\',\n                   part_info=\'assets/pretrains/smpl_part_info.json\',\n                   front_info=\'assets/pretrains/front_facial.json\',\n                   head_info=\'assets/pretrains/head.json\',\n                   contain_bg=True, fill_back=False):\n    """"""\n    :param mapping_path:\n    :param map_name:\n            \'uv\'     -> (F + 1) x 2  (bg as -1)\n            \'ids\'    -> (F + 1) x 1  (bg as -1)\n            \'binary\' -> (F + 1) x 14 (bs as -1)\n            \'seg\'    -> (F + 1) x 1  (bs as 0)\n            \'par\'    -> (F + 1) x (10 + 1)\n    :param part_info:\n    :param front_info:\n    :param contain_bg:\n    :param fill_back:\n    :return:\n    """"""\n\n    # F x C\n    f2vts = get_f2vts(mapping_path, fill_back=fill_back)\n    nf = f2vts.shape[0]\n\n    if map_name == \'uv\':\n        fbc = compute_barycenter(f2vts)\n        map_fn = fbc[:, 0:2]    # F x 2\n        bg = np.array([[-1, -1]], dtype=np.float32)\n    elif map_name == \'seg\':\n        map_fn = np.ones((nf, 1), dtype=np.float32)\n        bg = np.array([[0]], dtype=np.float32)\n    elif map_name == \'uv_seg\':\n        fbc = compute_barycenter(f2vts)\n        map_fn = fbc    # F x 3\n        bg = np.array([[0, 0, 1]], dtype=np.float32)\n    elif map_name == \'par\':\n        map_fn, bg = par_mapping(nf, part_info)\n    elif map_name == \'front\':\n        map_fn, bg = front_face_mapping(nf, front_info, fill_back=fill_back)\n    elif map_name == \'head\':\n        map_fn, bg = front_face_mapping(nf, head_info, fill_back=fill_back)\n    elif map_name == \'back\':\n        map_fn, bg = back_face_mapping(nf, head_info, front_info, fill_back=fill_back)\n    elif map_name == \'ids\':\n        map_fn, bg = ids_mapping(nf)\n    elif map_name == \'binary\':\n        map_fn, bg = binary_mapping(nf)\n    else:\n        raise ValueError(\'map name error {}\'.format(map_name))\n\n    if contain_bg:\n        map_fn = np.concatenate([map_fn, bg], axis=0)\n\n    return map_fn\n\n\ndef get_part_face_ids(part_type, mapping_path=\'assets/pretrains/mapper.txt\',\n                      part_info=\'assets/pretrains/smpl_part_info.json\',\n                      front_info=\'assets/pretrains/front_face_1.json\',\n                      head_info=\'assets/pretrains/head.json\',\n                      fill_back=False):\n    # F x C\n    f2vts = get_f2vts(mapping_path, fill_back=fill_back)\n    nf = f2vts.shape[0]\n    if part_type == \'head_front\':\n        faces = get_head_front_ids(nf, front_info, fill_back=fill_back)\n    elif part_type == \'head_back\':\n        faces = get_head_back_ids(nf, head_info, front_info, fill_back=fill_back)\n    elif part_type == \'head\':\n        raise NotImplementedError\n    elif part_type == \'par\':\n        faces = get_part_ids(nf, part_info, fill_back=fill_back)\n    else:\n        raise ValueError(\'map name error {}\'.format(part_type))\n\n    return faces\n\n\ndef get_map_fn_dim(map_name):\n    """"""\n    :param map_name:\n        \'seg\'    -> (F + 1) x 1  (bs as -1 or 0)\n        \'uv\'     -> (F + 1) x 2  (bg as -1)\n        \'uv_seg\' -> (F + 1) x 3  (bg as -1)\n        \'ids\'    -> (F + 1) x 1  (bg as -1)\n        \'binary\' -> (F + 1) x 15 (bs as -1)\n        \'par\'    -> (F + 1) x (10 + 1)\n    :return:\n    """"""\n    # F x C\n    if map_name == \'seg\':\n        dim = 1\n    elif map_name == \'uv\':\n        dim = 2\n    elif map_name == \'uv_seg\':\n        dim = 3\n    elif map_name == \'par\':\n        dim = 11\n    elif map_name == \'ids\':\n        dim = 1\n    elif map_name == \'binary\':\n        dim = 15\n    else:\n        raise ValueError(\'map name error {}\'.format(map_name))\n\n    return dim\n\n\ndef cvt_fim_enc(fim_enc, map_name):\n\n    h, w, c = fim_enc.shape\n\n    if map_name == \'uv\':\n        # (H, W, 2), bg is -1, -> (H, W, 3)\n        img = np.ones((h, w, 3), dtype=np.float32)\n        # print(fim_enc.shape)\n        img[:, :, 0:2] = fim_enc[:, :, 0:2]\n        img = np.transpose(img, axes=(2, 0, 1))\n\n    elif map_name == \'seg\':\n        # (H, W, 1), bg is -1  -> (H, W)\n        img = fim_enc[:, :, 0]\n\n    elif map_name == \'uv_seg\':\n        # (H, W, 3) -> (H, W, 3)\n        img = fim_enc.copy()\n        img = np.transpose(img, axes=(2, 0, 1))\n\n    elif map_name == \'par\':\n        # (H, W, C) -> (H, W)\n        img = fim_enc.argmax(axis=-1)\n        img = img.astype(np.float32)\n        img /= img.max()\n\n    elif map_name == \'ids\':\n        # (H, W, 1), bg is -1  -> (H, W)\n        img = fim_enc[:, :, 0]\n\n    elif map_name == \'binary\':\n        img = np.zeros((h, w), dtype=np.float32)\n\n        def bin2int(bits):\n            total = 0\n            for shift, j in enumerate(bits[::-1]):\n                if j:\n                    total += 1 << shift\n            return total\n\n        for i in range(h):\n            for j in range(w):\n                val = bin2int(fim_enc[i, j, :])\n                img[i, j] = val\n\n        img /= img.max()\n    else:\n        raise ValueError(map_name)\n    img = img.astype(np.float32)\n    return img\n\n\ndef create_uvsampler(uv_mapping_path=\'data/uv_mappings.txt\', tex_size=2):\n    """"""\n    For this mesh, pre-computes the UV coordinates for\n    F x T x T points.\n    Returns F x T*T x 2\n    """"""\n    alpha = np.arange(tex_size, dtype=np.float32) / (tex_size - 1)\n    beta = np.arange(tex_size, dtype=np.float32) / (tex_size - 1)\n    # Barycentric coordinate values\n    coords = np.stack([p for p in itertools.product(*[alpha, beta])])  # T*2 x 2\n\n    obj_info = load_obj(uv_mapping_path)\n\n    vts = obj_info[\'vts\']\n    vts[:, 1] = 1 - vts[:, 1]\n    faces_vts = obj_info[\'faces_vts\']\n\n    # F x 3 x 2\n    f2vts = vts[faces_vts]\n\n    # print(f2vts.shape, f2vts.max(), f2vts.min())\n\n    # Compute alpha, beta (this is the same order as NMR)\n    v2 = f2vts[:, 2]  # (nf, 2)\n    v0v2 = f2vts[:, 0] - f2vts[:, 2]  # (nf, 2)\n    v1v2 = f2vts[:, 1] - f2vts[:, 2]  # (nf, 2)\n\n    # F x 2 x T*2\n    samples = np.dstack([v0v2, v1v2]).dot(coords.T) + v2.reshape(-1, 2, 1)\n    samples = np.clip(samples, a_min=0.0, a_max=1.0)\n\n    # F x T*2 x 2 points on the sphere\n    uv = np.transpose(samples, (0, 2, 1))\n\n    # uv = uv.reshape(-1, tex_size, tex_size, 2)\n    # normalize to [-1, 1]\n    uv = uv * 2 - 1\n\n    return uv\n\n\ndef vertices_to_faces(vertices, faces):\n    """"""\n    :param vertices: [batch size, number of vertices, 2]\n    :param faces: [batch size, number of faces, 3]\n    :return: [batch size, number of faces, 3, 2]\n    """"""\n    bs, nv = vertices.shape[:2]\n    device = vertices.device\n\n    # print(vertices.shape, faces.shape)\n    faces = faces + (torch.arange(bs, dtype=torch.int32).to(device) * nv)[:, None, None]\n    vertices = vertices.reshape((bs * nv, 2))\n    # pytorch only supports long and byte tensors for indexing\n    return vertices[faces.long()]\n\n\ndef faces_to_sampler(coords, faces):\n    """"""\n    :param coords: [T*T, 3]\n    :param faces: [batch size, number of vertices, 3, 2]\n    :return: [batch_size, number of vertices, T*T, 2]\n    """"""\n\n    # Compute alpha, beta (this is the same order as NMR)\n    nf = faces.shape[1]\n    v2 = faces[:, :, 2]  # (bs, nf, 2)\n    v0v2 = faces[:, :, 0] - faces[:, :, 2]  # (bs, nf, 2)\n    v1v2 = faces[:, :, 1] - faces[:, :, 2]  # (bs, nf, 2)\n\n    # bs x  F x 2 x T*2\n    samples = torch.matmul(torch.stack((v0v2, v1v2), dim=-1), coords) + v2.view(-1, nf, 2, 1)\n    # bs x F x T*2 x 2 points on the sphere\n    samples = samples.permute(0, 1, 3, 2)\n    samples = torch.clamp(samples, min=-1.0, max=1.0)\n    return samples\n\n\nclass UVImageModel(nn.Module):\n    def __init__(self, uv, image_size):\n        super(UVImageModel, self).__init__()\n\n        # (1, 3, image_size, image_size)\n        # self.weight = nn.Parameter(torch.zeros(1, 3, image_size, image_size) - 1.0)\n        self.weight = nn.Parameter(torch.zeros(1, 3, image_size, image_size) - 1.0)\n\n        # (f, t, t, 2)\n        self.f, self.t = uv.shape[:2]\n        # (1, f, t*t, 2)\n        uv = uv.reshape(1, self.f, self.t * self.t, 2)\n        self.uv = torch.FloatTensor(uv).cuda()\n\n    def forward(self):\n        uv_image = torch.tanh(self.weight)\n        texture = F.grid_sample(uv_image, self.uv)\n        # (1,3,f,t,t)\n        texture = texture.view(1, 3, self.f, self.t, self.t)\n        # (1,f,t,t,3)\n        texture = texture.permute(0, 2, 3, 4, 1)\n\n        return texture\n\n    def get_uv_image(self):\n        return torch.tanh(self.weight)\n        # return 2 * torch.sigmoid(self.weight) - 1\n\n\ndef compute_uv_image(uv, texture, uv_size=224):\n    """"""\n    :param uv: (f, t, t, 2)\n    :param texture: torch.Tensor [f, t, t, 3]\n    :param uv_size: int, default is 224\n    :return: uv_image (3,h,w) rgb(-1,1)\n    """"""\n    with torch.enable_grad():\n        uv_image_model = UVImageModel(uv, image_size=uv_size).cuda()\n        opt = torch.optim.Adam(uv_image_model.parameters(), lr=1e-2)\n        for epoch in range(2000):\n            pred_texture = uv_image_model()\n\n            loss = ((pred_texture - texture) ** 2).mean()\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n            # if epoch % 10 == 0:\n            #     print(epoch, loss.item())\n\n    return uv_image_model.get_uv_image()[0]\n'"
utils/nmr.py,33,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nimport neural_renderer as nr\nfrom . import mesh\n\n\ndef orthographic_proj_withz_idrot(X, cam, offset_z=0.):\n    """"""\n    X: B x N x 3\n    cam: B x 3: [sc, tx, ty]\n    No rotation!\n    Orth preserving the z.\n    sc * ( x + [tx; ty])\n    as in HMR..\n    """"""\n    scale = cam[:, 0].contiguous().view(-1, 1, 1)\n    trans = cam[:, 1:3].contiguous().view(cam.size(0), 1, -1)\n\n    # proj = scale * X\n    proj = X\n\n    proj_xy = scale * (proj[:, :, :2] + trans)\n    proj_z = proj[:, :, 2, None] + offset_z\n\n    return torch.cat((proj_xy, proj_z), 2)\n\n\ndef orthographic_proj_withz(X, cam, offset_z=0.):\n    """"""\n    X: B x N x 3\n    cam: B x 7: [sc, tx, ty, quaternions]\n    Orth preserving the z.\n    sc * ( x + [tx; ty])\n    as in HMR..\n    """"""\n    quat = cam[:, -4:]\n    X_rot = quat_rotate(X, quat)\n\n    scale = cam[:, 0].contiguous().view(-1, 1, 1)\n    trans = cam[:, 1:3].contiguous().view(cam.size(0), 1, -1)\n\n    # proj = scale * X_rot\n    proj = X_rot\n\n    proj_xy = scale * (proj[:, :, :2] + trans)\n    proj_z = proj[:, :, 2, None] + offset_z\n\n    return torch.cat((proj_xy, proj_z), 2)\n\n\ndef quat_rotate(X, q):\n    """"""Rotate points by quaternions.\n\n    Args:\n        X: B X N X 3 points\n        q: B X 4 quaternions\n\n    Returns:\n        X_rot: B X N X 3 (rotated points)\n    """"""\n    # repeat q along 2nd dim\n    ones_x = X[[0], :, :][:, :, [0]] * 0 + 1\n    q = torch.unsqueeze(q, 1) * ones_x\n\n    q_conj = torch.cat([q[:, :, [0]], -1 * q[:, :, 1:4]], dim=-1)\n    X = torch.cat([X[:, :, [0]] * 0, X], dim=-1)\n\n    X_rot = hamilton_product(q, hamilton_product(X, q_conj))\n    return X_rot[:, :, 1:4]\n\n\ndef hamilton_product(qa, qb):\n    """"""Multiply qa by qb.\n\n    Args:\n        qa: B X N X 4 quaternions\n        qb: B X N X 4 quaternions\n    Returns:\n        q_mult: B X N X 4\n    """"""\n    qa_0 = qa[:, :, 0]\n    qa_1 = qa[:, :, 1]\n    qa_2 = qa[:, :, 2]\n    qa_3 = qa[:, :, 3]\n\n    qb_0 = qb[:, :, 0]\n    qb_1 = qb[:, :, 1]\n    qb_2 = qb[:, :, 2]\n    qb_3 = qb[:, :, 3]\n\n    # See https://en.wikipedia.org/wiki/Quaternion#Hamilton_product\n    q_mult_0 = qa_0 * qb_0 - qa_1 * qb_1 - qa_2 * qb_2 - qa_3 * qb_3\n    q_mult_1 = qa_0 * qb_1 + qa_1 * qb_0 + qa_2 * qb_3 - qa_3 * qb_2\n    q_mult_2 = qa_0 * qb_2 - qa_1 * qb_3 + qa_2 * qb_0 + qa_3 * qb_1\n    q_mult_3 = qa_0 * qb_3 + qa_1 * qb_2 - qa_2 * qb_1 + qa_3 * qb_0\n\n    return torch.stack([q_mult_0, q_mult_1, q_mult_2, q_mult_3], dim=-1)\n\n\nclass SMPLRenderer(nn.Module):\n    def __init__(self, face_path=\'assets/pretrains/smpl_faces.npy\',\n                 uv_map_path=\'assets/pretrains/mapper.txt\', map_name=\'uv_seg\', tex_size=3, image_size=256,\n                 anti_aliasing=True, fill_back=False, background_color=(0, 0, 0), viewing_angle=30, near=0.1, far=25.0,\n                 has_front=False):\n        """"""\n        Args:\n            face_path:\n            uv_map_path:\n            map_name:\n            tex_size:\n            image_size:\n            anti_aliasing:\n            fill_back:\n            background_color:\n            viewing_angle:\n            near:\n            far:\n            has_front:\n        """"""\n\n        super(SMPLRenderer, self).__init__()\n\n        self.background_color = background_color\n        self.anti_aliasing = anti_aliasing\n        self.image_size = image_size\n        self.fill_back = fill_back\n        self.map_name = map_name\n\n        faces = np.load(face_path)\n        self.tex_size = tex_size\n        self.base_nf = faces.shape[0]\n        self.register_buffer(\'coords\', self.create_coords(tex_size))\n\n        # fill back\n        if self.fill_back:\n            faces = np.concatenate((faces, faces[:, ::-1]), axis=0)\n\n        faces = torch.tensor(faces.astype(np.int32)).int()\n        self.nf = faces.shape[0]\n        self.register_buffer(\'faces\', faces)\n\n        # (nf, T*T, 2)\n        img2uv_sampler = torch.tensor(mesh.create_uvsampler(uv_map_path, tex_size=tex_size)).float()\n        map_fn = torch.tensor(mesh.create_mapping(map_name, uv_map_path, contain_bg=True,\n                                                  fill_back=fill_back)).float()\n        self.register_buffer(\'img2uv_sampler\', img2uv_sampler)\n        self.register_buffer(\'map_fn\', map_fn)\n\n        back_map_fn = torch.tensor(mesh.create_mapping(\'back\', uv_map_path, contain_bg=True,\n                                                       fill_back=fill_back)).float()\n        self.register_buffer(\'back_map_fn\', back_map_fn)\n\n        if has_front:\n            front_map_fn = torch.tensor(mesh.create_mapping(\'front\', uv_map_path, contain_bg=True,\n                                                            fill_back=fill_back)).float()\n            self.register_buffer(\'front_map_fn\', front_map_fn)\n        else:\n            self.front_map_fn = None\n\n        # light\n        self.light_intensity_ambient = 1\n        self.light_intensity_directional = 0\n        self.light_color_ambient = [1, 1, 1]\n        self.light_color_directional = [1, 1, 1]\n        self.light_direction = [0, 1, 0]\n\n        self.rasterizer_eps = 1e-3\n\n        # project function and camera\n        self.near = near\n        self.far = far\n        self.proj_func = orthographic_proj_withz_idrot\n        self.viewing_angle = viewing_angle\n        self.eye = [0, 0, -(1. / np.tan(np.radians(self.viewing_angle)) + 1)]\n\n    def set_ambient_light(self, int_dir=0.3, int_amb=0.7, direction=(1, 0.5, 1)):\n        self.light_intensity_directional = int_dir\n        self.light_intensity_ambient = int_amb\n        if direction is not None:\n            self.light_direction = direction\n\n    def set_bgcolor(self, color=(-1, -1, -1)):\n        self.background_color = color\n\n    def set_tex_size(self, tex_size):\n        del self.coords\n        self.coords = self.create_coords(tex_size).cuda()\n\n    def forward(self, cam, vertices, uv_imgs, dynamic=True, get_fim=False):\n        bs = cam.shape[0]\n        faces = self.faces.repeat(bs, 1, 1)\n\n        if dynamic:\n            samplers = self.dynamic_sampler(cam, vertices, faces)\n        else:\n            samplers = self.img2uv_sampler.repeat(bs, 1, 1, 1)\n\n        textures = self.extract_tex(uv_imgs, samplers)\n\n        images, fim = self.render(cam, vertices, textures, faces, get_fim=get_fim)\n\n        if get_fim:\n            return images, textures, fim\n        else:\n            return images, textures\n\n    def render(self, cam, vertices, textures, faces=None, get_fim=False):\n        if faces is None:\n            bs = cam.shape[0]\n            faces = self.faces.repeat(bs, 1, 1)\n\n        # lighting is inplace operation\n        textures = textures.clone()\n        # lighting\n        faces_lighting = nr.vertices_to_faces(vertices, faces)\n        textures = nr.lighting(\n            faces_lighting,\n            textures,\n            self.light_intensity_ambient,\n            self.light_intensity_directional,\n            self.light_color_ambient,\n            self.light_color_directional,\n            self.light_direction)\n\n        # set offset_z for persp proj\n        proj_verts = self.proj_func(vertices, cam)\n        # flipping the y-axis here to make it align with the image coordinate system!\n        proj_verts[:, :, 1] *= -1\n        # calculate the look_at vertices.\n        vertices = nr.look_at(proj_verts, self.eye)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        images = nr.rasterize(faces, textures, self.image_size, self.anti_aliasing,\n                              self.near, self.far, self.rasterizer_eps, self.background_color)\n        fim = None\n        if get_fim:\n            fim = nr.rasterize_face_index_map(faces, image_size=self.image_size, anti_aliasing=False,\n                                              near=self.near, far=self.far, eps=self.rasterizer_eps)\n\n        return images, fim\n\n    def render_fim(self, cam, vertices, faces=None):\n        if faces is None:\n            bs = cam.shape[0]\n            faces = self.faces.repeat(bs, 1, 1)\n\n        # set offset_z for persp proj\n        proj_verts = self.proj_func(vertices, cam)\n        # flipping the y-axis here to make it align with the image coordinate system!\n        proj_verts[:, :, 1] *= -1\n        # calculate the look_at vertices.\n        vertices = nr.look_at(proj_verts, self.eye)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        fim = nr.rasterize_face_index_map(faces, self.image_size, False)\n        return fim\n\n    def render_fim_wim(self, cam, vertices, faces=None):\n        if faces is None:\n            bs = cam.shape[0]\n            faces = self.faces.repeat(bs, 1, 1)\n\n        # set offset_z for persp proj\n        proj_verts = self.proj_func(vertices, cam)\n        # flipping the y-axis here to make it align with the image coordinate system!\n        proj_verts[:, :, 1] *= -1\n        # calculate the look_at vertices.\n        vertices = nr.look_at(proj_verts, self.eye)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        fim, wim = nr.rasterize_face_index_map_and_weight_map(faces, self.image_size, False)\n        return faces, fim, wim\n\n    def render_depth(self, cam, vertices):\n        raise NotImplementedError\n\n        # bs = cam.shape[0]\n        # faces = self.faces.repeat(bs, 1, 1)\n        # # if self.fill_back:\n        # #     faces = torch.cat((faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n        #\n        # vertices = self.weak_projection(cam, vertices)\n        #\n        # # rasterization\n        # faces = self.vertices_to_faces(vertices, faces)\n        # images = nr.rasterize_depth(faces, self.image_size, self.anti_aliasing)\n        # return images\n\n    def render_silhouettes(self, cam, vertices, faces=None):\n        if faces is None:\n            bs = cam.shape[0]\n            faces = self.faces.repeat(bs, 1, 1)\n\n        # set offset_z for persp proj\n        proj_verts = self.proj_func(vertices, cam)\n        # flipping the y-axis here to make it align with the image coordinate system!\n        proj_verts[:, :, 1] *= -1\n        # calculate the look_at vertices.\n        vertices = nr.look_at(proj_verts, self.eye)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        images = nr.rasterize_silhouettes(faces, self.image_size, self.anti_aliasing)\n        return images\n\n    def infer_face_index_map(self, cam, vertices):\n        raise NotImplementedError\n        # bs = cam.shape[0]\n        # faces = self.faces.repeat(bs, 1, 1)\n        #\n        # # if self.fill_back:\n        # #     faces = torch.cat((faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n        #\n        # vertices = self.weak_projection(cam, vertices)\n        #\n        # # rasterization\n        # faces = nr.vertices_to_faces(vertices, faces)\n        # fim = nr.rasterize_face_index_map(faces, self.image_size, False)\n\n        # return fim\n\n    def encode_fim(self, cam, vertices, fim=None, transpose=True, map_fn=None):\n\n        if fim is None:\n            fim = self.infer_face_index_map(cam, vertices)\n\n        if map_fn is not None:\n            fim_enc = map_fn[fim.long()]\n        else:\n            fim_enc = self.map_fn[fim.long()]\n\n        if transpose:\n            fim_enc = fim_enc.permute(0, 3, 1, 2)\n\n        return fim_enc, fim\n\n    def encode_front_fim(self, fim, transpose=True, front_fn=True):\n        if front_fn:\n            fim_enc = self.front_map_fn[fim.long()]\n        else:\n            fim_enc = self.back_map_fn[fim.long()]\n\n        if transpose:\n            fim_enc = fim_enc.permute(0, 3, 1, 2)\n\n        return fim_enc\n\n    def extract_tex_from_image(self, images, cam, vertices):\n        bs = images.shape[0]\n        faces = self.faces.repeat(bs, 1, 1)\n\n        sampler = self.dynamic_sampler(cam, vertices, faces)  # (bs, nf, T*T, 2)\n\n        tex = self.extract_tex(images, sampler)\n\n        return tex\n\n    def extract_tex(self, uv_img, uv_sampler):\n        """"""\n        :param uv_img: (bs, 3, h, w)\n        :param uv_sampler: (bs, nf, T*T, 2)\n        :return:\n        """"""\n\n        # (bs, 3, nf, T*T)\n        tex = F.grid_sample(uv_img, uv_sampler)\n        # (bs, 3, nf, T, T)\n        tex = tex.view(-1, 3, self.nf, self.tex_size, self.tex_size)\n        # (bs, nf, T, T, 3)\n        tex = tex.permute(0, 2, 3, 4, 1)\n        # (bs, nf, T, T, T, 3)\n        tex = tex.unsqueeze(4).repeat(1, 1, 1, 1, self.tex_size, 1)\n\n        return tex\n\n    def dynamic_sampler(self, cam, vertices, faces):\n        # ipdb.set_trace()\n        points = self.batch_orth_proj_idrot(cam, vertices)  # (bs, nf, 2)\n        faces_points = self.points_to_faces(points, faces)   # (bs, nf, 3, 2)\n        # print(faces_points.shape)\n        sampler = self.points_to_sampler(self.coords, faces_points)  # (bs, nf, T*T, 2)\n        return sampler\n\n    def project_to_image(self, cam, vertices):\n        # set offset_z for persp proj\n        proj_verts = self.proj_func(vertices, cam)\n        # flipping the y-axis here to make it align with the image coordinate system!\n        # proj_verts[:, :, 1] *= -1\n        proj_verts = proj_verts[:, :, 0:2]\n        return proj_verts\n\n    def points_to_faces(self, points, faces=None):\n        """"""\n        :param points:\n        :param faces\n        :return:\n        """"""\n        bs, nv = points.shape[:2]\n        device = points.device\n\n        if faces is None:\n            faces = self.faces.repeat(bs, 1, 1)\n            # if self.fill_back:\n            #     faces = torch.cat((faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n\n        faces = faces + (torch.arange(bs, dtype=torch.int32).to(device) * nv)[:, None, None]\n        points = points.reshape((bs * nv, 2))\n        # pytorch only supports long and byte tensors for indexing\n        return points[faces.long()]\n\n    @staticmethod\n    def compute_barycenter(f2vts):\n        """"""\n\n        :param f2vts:  N x F x 3 x 2\n        :return: N x F x 2\n        """"""\n\n        # Compute alpha, beta (this is the same order as NMR)\n        v2 = f2vts[:, :, 2]  # (nf, 2)\n        v0v2 = f2vts[:, :, 0] - f2vts[:, :, 2]  # (nf, 2)\n        v1v2 = f2vts[:, :, 1] - f2vts[:, :, 2]  # (nf, 2)\n\n        fbc = v2 + 0.5 * v0v2 + 0.5 * v1v2\n\n        return fbc\n\n    @staticmethod\n    def batch_orth_proj_idrot(camera, X):\n        """"""\n        X is N x num_points x 3\n        camera is N x 3\n        same as applying orth_proj_idrot to each N\n        """"""\n\n        # TODO check X dim size.\n        # X_trans is (N, num_points, 2)\n        X_trans = X[:, :, :2] + camera[:, None, 1:]\n        # reshape X_trans, (N, num_points * 2)\n        # --- * operation, (N, 1) x (N, num_points * 2) -> (N, num_points * 2)\n        # ------- reshape, (N, num_points, 2)\n\n        return camera[:, None, 0:1] * X_trans\n\n    @staticmethod\n    def points_to_sampler(coords, faces):\n        """"""\n        :param coords: [2, T*T]\n        :param faces: [batch size, number of vertices, 3, 2]\n        :return: [batch_size, number of vertices, T*T, 2]\n        """"""\n\n        # Compute alpha, beta (this is the same order as NMR)\n        nf = faces.shape[1]\n        v2 = faces[:, :, 2]  # (bs, nf, 2)\n        v0v2 = faces[:, :, 0] - faces[:, :, 2]  # (bs, nf, 2)\n        v1v2 = faces[:, :, 1] - faces[:, :, 2]  # (bs, nf, 2)\n\n        # bs x  F x 2 x T*2\n        samples = torch.matmul(torch.stack((v0v2, v1v2), dim=-1), coords) + v2.view(-1, nf, 2, 1)\n        # bs x F x T*2 x 2 points on the sphere\n        samples = samples.permute(0, 1, 3, 2)\n        samples = torch.clamp(samples, min=-1.0, max=1.0)\n        return samples\n\n    @staticmethod\n    def create_coords(tex_size=3):\n        """"""\n        :param tex_size: int\n        :return: 2 x (tex_size * tex_size)\n        """"""\n        if tex_size == 1:\n            step = 1\n        else:\n            step = 1 / (tex_size - 1)\n\n        alpha_beta = torch.arange(0, 1+step, step, dtype=torch.float32).cuda()\n        xv, yv = torch.meshgrid([alpha_beta, alpha_beta])\n\n        coords = torch.stack([xv.flatten(), yv.flatten()], dim=0)\n\n        return coords\n\n    @staticmethod\n    def create_meshgrid(image_size):\n        """"""\n        Args:\n            image_size:\n\n        Returns:\n            (image_size, image_size, 2)\n        """"""\n        factor = torch.arange(0, image_size, dtype=torch.float32) / (image_size - 1)   # [0, 1]\n        factor = (factor - 0.5) * 2\n        xv, yv = torch.meshgrid([factor, factor])\n        # grid = torch.stack([xv, yv], dim=-1)\n        grid = torch.stack([yv, xv], dim=-1)\n        return grid\n\n    @staticmethod\n    def get_vis_f2pts(f2pts, fims):\n        """"""\n        Args:\n            f2pts: (bs, f, 3, 2) or (bs, f, 3, 3)\n            fims:  (bs, 256, 256)\n\n        Returns:\n\n        """"""\n\n        def get_vis(orig_f2pts, fim):\n            """"""\n            Args:\n                orig_f2pts: (f, 3, 2) or (f, 3, 3)\n                fim: (256, 256)\n\n            Returns:\n                vis_f2pts: (f, 3, 2)\n            """"""\n            vis_f2pts = torch.zeros_like(orig_f2pts) - 2.0\n            # 0 is -1\n            face_ids = fim.unique()[1:].long()\n            vis_f2pts[face_ids] = orig_f2pts[face_ids]\n\n            return vis_f2pts\n\n        # import ipdb\n        # ipdb.set_trace()\n        if f2pts.dim() == 4:\n            all_vis_f2pts = []\n            bs = f2pts.shape[0]\n            for i in range(bs):\n                all_vis_f2pts.append(get_vis(f2pts[i], fims[i]))\n\n            all_vis_f2pts = torch.stack(all_vis_f2pts, dim=0)\n\n        else:\n            all_vis_f2pts = get_vis(f2pts, fims)\n\n        return all_vis_f2pts\n\n    @staticmethod\n    def set_null_f2pts(f2pts, fims):\n        """"""\n        Args:\n            f2pts: (bs, f, 3, 2) or (bs, f, 3, 3)\n            fims:  (bs, 256, 256)\n\n        Returns:\n\n        """"""\n\n        def get_vis(orig_f2pts, fim):\n            """"""\n            Args:\n                orig_f2pts: (f, 3, 2) or (f, 3, 3)\n                fim: (256, 256)\n\n            Returns:\n                vis_f2pts: (f, 3, 2)\n            """"""\n            # 0 is -1\n            face_ids = fim.unique()[1:].long()\n            orig_f2pts[face_ids] = -2.0\n\n            return orig_f2pts\n\n        if f2pts.dim() == 4:\n            all_vis_f2pts = []\n            bs = f2pts.shape[0]\n            for i in range(bs):\n                all_vis_f2pts.append(get_vis(f2pts[i], fims[i]))\n\n            all_vis_f2pts = torch.stack(all_vis_f2pts, dim=0)\n\n        else:\n            all_vis_f2pts = get_vis(f2pts, fims)\n\n        return all_vis_f2pts\n\n    def cal_transform(self, bc_f2pts, src_fim, dst_fim):\n        """"""\n        Args:\n            bc_f2pts:\n            src_fim:\n            dst_fim:\n\n        Returns:\n\n        """"""\n        device = bc_f2pts.device\n        bs = src_fim.shape[0]\n        # T = renderer.init_T.repeat(bs, 1, 1, 1)    # (bs, image_size, image_size, 2)\n        T = (torch.zeros(bs, self.image_size, self.image_size, 2, device=device) - 2)\n        # 2. calculate occlusion flows, (bs, no, 2)\n        dst_ids = dst_fim != -1\n\n        # 3. calculate tgt flows, (bs, nt, 2)\n\n        for i in range(bs):\n            Ti = T[i]\n\n            tgt_i = dst_ids[i]\n\n            # (nf, 2)\n            tgt_flows = bc_f2pts[i, dst_fim[i, tgt_i].long()]  # (nt, 2)\n            Ti[tgt_i] = tgt_flows\n\n        return T\n\n    def cal_bc_transform(self, src_f2pts, dst_fims, dst_wims):\n        """"""\n        Args:\n            src_f2pts: (bs, 13776, 3, 2)\n            dst_fims:  (bs, 256, 256)\n            dst_wims:  (bs, 256, 256, 3)\n        Returns:\n\n        """"""\n        bs = src_f2pts.shape[0]\n        T = -2 * torch.ones((bs, self.image_size * self.image_size, 2), dtype=torch.float32, device=src_f2pts.device)\n\n        for i in range(bs):\n            # (13776, 3, 2)\n            from_faces_verts_on_img = src_f2pts[i]\n\n            # to_face_index_map\n            to_face_index_map = dst_fims[i]\n\n            # to_weight_map\n            to_weight_map = dst_wims[i]\n\n            # (256, 256) -> (256*256, )\n            to_face_index_map = to_face_index_map.long().reshape(-1)\n            # (256, 256, 3) -> (256*256, 3)\n            to_weight_map = to_weight_map.reshape(-1, 3)\n\n            to_exist_mask = (to_face_index_map != -1)\n            # (exist_face_num,)\n            to_exist_face_idx = to_face_index_map[to_exist_mask]\n            # (exist_face_num, 3)\n            to_exist_face_weights = to_weight_map[to_exist_mask]\n\n            # (exist_face_num, 3, 2) * (exist_face_num, 3) -> sum -> (exist_face_num, 2)\n            exist_smpl_T = (from_faces_verts_on_img[to_exist_face_idx] * to_exist_face_weights[:, :, None]).sum(dim=1)\n            # (256, 256, 2)\n            T[i, to_exist_mask] = exist_smpl_T\n\n        T = T.view(bs, self.image_size, self.image_size, 2)\n\n        # T = torch.clamp(-2, 2)\n\n        return T\n\n    def debug_textures(self):\n        return torch.ones((self.nf, self.tex_size, self.tex_size, self.tex_size, 3), dtype=torch.float32)\n'"
utils/tb_visualizer.py,0,"b'import numpy as np\nimport os\nimport time\nfrom . import util\nfrom tensorboardX import SummaryWriter\nimport ipdb\n\n\nclass TBVisualizer(object):\n    def __init__(self, opt):\n        self._opt = opt\n        self._save_path = os.path.join(opt.checkpoints_dir, opt.name)\n\n        self._log_path = os.path.join(self._save_path, \'loss_log2.txt\')\n        self._tb_path = os.path.join(self._save_path, \'summary.json\')\n        self._writer = SummaryWriter(self._save_path)\n\n        with open(self._log_path, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    def __del__(self):\n        self._writer.close()\n\n    def display_current_results(self, visuals, it, is_train, save_visuals=False):\n        for label, image_numpy in visuals.items():\n            # ipdb.set_trace()\n            sum_name = \'{}/{}\'.format(\'Train\' if is_train else \'Test\', label)\n            self._writer.add_image(sum_name, image_numpy, it)\n\n            if save_visuals:\n                util.save_image(image_numpy,\n                                os.path.join(self._opt.checkpoints_dir, self._opt.name,\n                                             \'event_imgs\', sum_name, \'%08d.png\' % it))\n\n        self._writer.export_scalars_to_json(self._tb_path)\n\n    def plot_scalars(self, scalars, it, is_train):\n        for label, scalar in scalars.items():\n            sum_name = \'{}/{}\'.format(\'Train\' if is_train else \'Test\', label)\n            self._writer.add_scalar(sum_name, scalar, it)\n\n    def print_current_train_errors(self, epoch, i, iters_per_epoch, errors, t, visuals_were_stored):\n        log_time = time.strftime(""[%d/%m/%Y %H:%M:%S]"")\n        visuals_info = ""v"" if visuals_were_stored else """"\n        message = \'%s (T%s, epoch: %d, it: %d/%d, t/smpl: %.3fs)\\n\' % (log_time, visuals_info, epoch, i, iters_per_epoch, t)\n        for k, v in errors.items():\n            msg = \'\\t%s:%.3f\\n\' % (k, v)\n            message += msg\n        print(message)\n        with open(self._log_path, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    def print_current_validate_errors(self, epoch, errors, t):\n        log_time = time.strftime(""[%d/%m/%Y %H:%M:%S]"")\n        message = \'%s (V, epoch: %d, time_to_val: %ds)\\n\' % (log_time, epoch, t)\n        for k, v in errors.items():\n            message += \'\\t%s:%.3f\\n\' % (k, v)\n\n        print(message)\n        with open(self._log_path, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    def save_images(self, visuals):\n        for label, image_numpy in visuals.items():\n            image_name = \'%s.png\' % label\n            save_path = os.path.join(self._save_path, ""samples"", image_name)\n            util.save_image(image_numpy, save_path)\n\n\nif __name__ == \'__main__\':\n    from options.train_options import TrainOptions\n    from data.imper_dataset import SeqMIDataset\n    from utils.util import tensor2im\n\n    opts = TrainOptions().parse()\n\n    pair_dataset = SeqMIDataset(opts, is_for_train=True)\n    tb_visualizer = TBVisualizer(opts)\n\n    for i in range(0, 100):\n        sample = pair_dataset[i]\n        images = sample[\'images\']\n        print(images.shape)\n        visuals = {\n            \'images\': tensor2im(sample[\'images\'])\n        }\n\n        scalars = {\n            \'loss\': np.random.rand()\n        }\n\n        tb_visualizer.display_current_results(visuals, i, is_train=True)\n        tb_visualizer.plot_scalars(scalars, i, is_train=True)\n\n        time.sleep(1)\n'"
utils/util.py,5,"b'from __future__ import print_function\nfrom PIL import Image\nimport numpy as np\nimport os\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms.functional as TF\nimport math\nimport pickle\n\n\nclass ImageTransformer(object):\n    """"""\n    Rescale the image in a sample to a given size.\n    """"""\n\n    def __init__(self, output_size):\n        """"""\n        Args:\n            output_size (tuple or int): Desired output size. If tuple, output is matched to output_size.\n                            If int, smaller of image edges is matched to output_size keeping aspect ratio the same.\n        """"""\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        images = sample[\'images\']\n        resized_images = []\n\n        for image in images:\n            image = cv2.resize(image, (self.output_size, self.output_size))\n            image = image.astype(np.float32)\n            image /= 255.0\n            image = image * 2 - 1\n\n            image = np.transpose(image, (2, 0, 1))\n\n            resized_images.append(image)\n\n        resized_images = np.stack(resized_images, axis=0)\n\n        sample[\'images\'] = resized_images\n        return sample\n\n\nclass ImageNormalizeToTensor(object):\n    """"""\n    Rescale the image in a sample to a given size.\n    """"""\n\n    def __call__(self, image):\n        # image = F.to_tensor(image)\n        image = TF.to_tensor(image)\n        image.mul_(2.0)\n        image.sub_(1.0)\n        return image\n\n\nclass ToTensor(object):\n    """"""\n    Convert ndarrays in sample to Tensors.\n    """"""\n\n    def __call__(self, sample):\n        sample[\'images\'] = torch.Tensor(sample[\'images\']).float()\n        sample[\'smpls\'] = torch.Tensor(sample[\'smpls\']).float()\n\n        return sample\n\n\ndef morph(src_bg_mask, ks, mode=\'erode\', kernel=None):\n    n_ks = ks ** 2\n    pad_s = ks // 2\n\n    if kernel is None:\n        kernel = torch.ones(1, 1, ks, ks, dtype=torch.float32, device=src_bg_mask.device)\n\n    if mode == \'erode\':\n        src_bg_mask_pad = F.pad(src_bg_mask, [pad_s, pad_s, pad_s, pad_s], value=1.0)\n        out = F.conv2d(src_bg_mask_pad, kernel)\n        out = (out == n_ks).float()\n    else:\n        src_bg_mask_pad = F.pad(src_bg_mask, [pad_s, pad_s, pad_s, pad_s], value=0.0)\n        out = F.conv2d(src_bg_mask_pad, kernel)\n        out = (out >= 1).float()\n\n    return out\n\n\ndef cal_mask_bbox(head_mask, factor=1.3):\n    """"""\n    Args:\n        head_mask (np.ndarray): (N, 1, 256, 256).\n        factor (float): the factor to enlarge the bbox of head.\n\n    Returns:\n        bbox (np.ndarray.int32): (N, 4), hear, 4 = (left_top_x, right_top_x, left_top_y, right_top_y)\n\n    """"""\n    bs, _, height, width = head_mask.shape\n\n    bbox = np.zeros((bs, 4), dtype=np.int32)\n    valid = np.ones((bs,), dtype=np.float32)\n\n    for i in range(bs):\n        mask = head_mask[i, 0]\n        ys, xs = np.where(mask == 1)\n\n        if len(ys) == 0:\n            valid[i] = 0.0\n            bbox[i, 0] = 0\n            bbox[i, 1] = width\n            bbox[i, 2] = 0\n            bbox[i, 3] = height\n            continue\n\n        lt_y = np.min(ys)   # left top of Y\n        lt_x = np.min(xs)   # left top of X\n\n        rt_y = np.max(ys)   # right top of Y\n        rt_x = np.max(xs)   # right top of X\n\n        h = rt_y - lt_y     # height of head\n        w = rt_x - lt_x     # width of head\n\n        cy = (lt_y + rt_y) // 2    # (center of y)\n        cx = (lt_x + rt_x) // 2    # (center of x)\n\n        _h = h * factor\n        _w = w * factor\n\n        _lt_y = max(0, int(cy - _h / 2))\n        _lt_x = max(0, int(cx - _w / 2))\n\n        _rt_y = min(height, int(cy + _h / 2))\n        _rt_x = min(width, int(cx + _w / 2))\n\n        if (_lt_x == _rt_x) or (_lt_y == _rt_y):\n            valid[i] = 0.0\n            bbox[i, 0] = 0\n            bbox[i, 1] = width\n            bbox[i, 2] = 0\n            bbox[i, 3] = height\n        else:\n            bbox[i, 0] = _lt_x\n            bbox[i, 1] = _rt_x\n            bbox[i, 2] = _lt_y\n            bbox[i, 3] = _rt_y\n\n    return bbox, valid\n\n\ndef to_tensor(tensor):\n    if isinstance(tensor, np.ndarray):\n        tensor = torch.FloatTensor(tensor)\n    return tensor\n\n\ndef plot_fim_enc(fim_enc, map_name):\n    # import matplotlib.pyplot as plt\n    import utils.mesh as mesh\n    if not isinstance(fim_enc, np.ndarray):\n        fim_enc = fim_enc.cpu().numpy()\n\n    if fim_enc.ndim != 4:\n        fim_enc = fim_enc[np.newaxis, ...]\n\n    fim_enc = np.transpose(fim_enc, axes=(0, 2, 3, 1))\n\n    imgs = []\n    for fim_i in fim_enc:\n        img = mesh.cvt_fim_enc(fim_i, map_name)\n        imgs.append(img)\n\n    return np.stack(imgs, axis=0)\n\n\ndef tensor2im(img, imtype=np.uint8, unnormalize=True, idx=0, nrows=None):\n    # select a sample or create grid if img is a batch\n    if len(img.shape) == 4:\n        nrows = nrows if nrows is not None else int(math.sqrt(img.size(0)))\n        img = img[idx] if idx >= 0 else torchvision.utils.make_grid(img, nrows)\n\n    img = img.cpu().float()\n    if unnormalize:\n        img += 1.0\n        img /= 2.0\n\n    image_numpy = img.numpy()\n    # image_numpy = np.transpose(image_numpy, (1, 2, 0))\n    image_numpy *= 255.0\n\n    return image_numpy.astype(imtype)\n\n\ndef tensor2maskim(mask, imtype=np.uint8, idx=0, nrows=1):\n    im = tensor2im(mask, imtype=imtype, idx=idx, unnormalize=False, nrows=nrows)\n    if im.shape[2] == 1:\n        im = np.repeat(im, 3, axis=-1)\n    return im\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n    return paths\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return path\n\n\ndef clear_dir(path):\n    import shutil\n    if os.path.exists(path):\n        shutil.rmtree(path)\n\n    return mkdir(path)\n\n\ndef save_image(image_numpy, image_path):\n    mkdir(os.path.dirname(image_path))\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\n\ndef load_pickle_file(pkl_path):\n    with open(pkl_path, \'rb\') as f:\n        data = pickle.load(f, encoding=\'latin1\')\n\n    return data\n\n\ndef write_pickle_file(pkl_path, data_dict):\n    with open(pkl_path, \'wb\') as fp:\n        pickle.dump(data_dict, fp, protocol=2)\n\n'"
utils/video.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 2019-08-02 18:31\n# @Author  : Zhixin Piao \n# @Email   : piaozhx@shanghaitech.edu.cn\n\nimport os\nimport glob\nimport cv2\nimport shutil\nfrom multiprocessing import Pool\nfrom functools import partial\nfrom tqdm import tqdm\nimport numpy as np\n\n\ndef auto_unzip_fun(x, f):\n    return f(*x)\n\n\ndef render_and_save(pose_visualizer, bg_img, smpl_pose, smpl_cam, save_path):\n    render_img = pose_visualizer.render_one_image(bg_img, smpl_pose, smpl_cam, ret_tensor=False)\n    cv2.imwrite(save_path, render_img)\n\n\ndef make_video(output_mp4_path, img_path_list, save_frames_dir=None, fps=24):\n    """"""\n    output_path is the final mp4 name\n    img_dir is where the images to make into video are saved.\n    """"""\n\n    first_img = cv2.imread(img_path_list[0])\n    h, w = first_img.shape[:2]\n\n    pool_size = 40\n    tmp_avi_video_path = \'%s.avi\' % output_mp4_path\n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n\n    videoWriter = cv2.VideoWriter(tmp_avi_video_path, fourcc, fps, (w, h))\n    args_list = [(img_path,) for img_path in img_path_list]\n    with Pool(pool_size) as p:\n        for img in tqdm(p.imap(partial(auto_unzip_fun, f=cv2.imread), args_list), total=len(args_list)):\n            videoWriter.write(img)\n    videoWriter.release()\n\n    if save_frames_dir:\n        for i, img_path in enumerate(img_path_list):\n            shutil.copy(img_path, \'%s/%.8d.jpg\' % (save_frames_dir, i))\n\n    os.system(""ffmpeg -y -i %s -vcodec h264 %s > /dev/null 2>&1"" % (tmp_avi_video_path, output_mp4_path))\n    os.system(""rm %s"" % (tmp_avi_video_path))\n\n\ndef fuse_image(img_path_list, row_num, col_num):\n    assert len(img_path_list) == row_num * col_num\n\n    img_list = [cv2.imread(img_path) for img_path in img_path_list]\n\n    row_imgs = []\n    for i in range(row_num):\n        col_imgs = img_list[i * col_num: (i + 1) * col_num]\n        col_img = np.concatenate(col_imgs, axis=1)\n        row_imgs.append(col_img)\n\n    fused_img = np.concatenate(row_imgs, axis=0)\n    return fused_img\n\n\ndef fuse_video(video_frames_path_list, output_mp4_path, row_num, col_num, fps=24):\n    assert len(video_frames_path_list) == row_num * col_num\n\n    frame_num = len(video_frames_path_list[0])\n    first_img = cv2.imread(video_frames_path_list[0][0])\n    h, w = first_img.shape[:2]\n    fused_h, fused_w = h * row_num, w * col_num\n\n    args_list = []\n    for frame_idx in range(frame_num):\n        fused_frame_path_list = [video_frames[frame_idx] for video_frames in video_frames_path_list]\n        args_list.append((fused_frame_path_list, row_num, col_num))\n\n    pool_size = 40\n    tmp_avi_video_path = \'%s.avi\' % output_mp4_path\n    fourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n\n    # for args in args_list:\n    #     fuse_image(*args)\n    # exit()\n\n    videoWriter = cv2.VideoWriter(tmp_avi_video_path, fourcc, fps, (fused_w, fused_h))\n    with Pool(pool_size) as p:\n        for img in tqdm(p.imap(partial(auto_unzip_fun, f=fuse_image), args_list), total=len(args_list)):\n            videoWriter.write(img)\n    videoWriter.release()\n\n    os.system(""ffmpeg -y -i %s -vcodec h264 %s > /dev/null 2>&1"" % (tmp_avi_video_path, output_mp4_path))\n    os.system(""rm %s"" % (tmp_avi_video_path))\n'"
utils/visdom_visualizer.py,3,"b'from __future__ import absolute_import, division, print_function\nfrom visdom import Visdom\nimport numpy as np\n\n\nclass BaseVisualizer(object):\n    """"""\n        Base class of visualizer.\n    """"""\n\n    def __init__(self, env, ip, port):\n        print(\'ip = {}, port = {}\'.format(ip, port))\n        self.env = env\n\n        if ip and port:\n            self.vis = Visdom(server=ip,\n                              endpoint=\'events\',\n                              port=port,\n                              env=self.env)\n            # self.vis.close(env=self.env)\n\n\nclass VisdomVisualizer(BaseVisualizer):\n\n    def __init__(self, env, time_step=1, num_points=18,\n                 ip=None, port=None):\n        super(VisdomVisualizer, self).__init__(env, ip=ip, port=port)\n\n        self.time_step = time_step\n        self.num_points = num_points\n\n    def vis_keypoints(self, preds, gts):\n        """"""\n        :type preds:  torch.tensor, (self.time_step, num_points, 2)\n        :param preds: the time series of predicted keypoints.\n\n        :type gts:  torch.tensor, (self.time_step, num_points, 2)\n        :param gts: the time series of ground truth keypoints.\n        """"""\n\n        lsp_key_points_name = [\'Right ankle\', \'Right knee\', \'Right hip\', \'Left hip\', \'Left knee\', \'Left ankle\', \'Right wrist\', \'Right elbow\', \'Right shoulder\',\n                               \'Left shoulder\', \'Left elbow\', \'Left wrist\', \'Neck\', \'Head top\']\n        lsp_plus_key_points_name = lsp_key_points_name + [\'Left ear\', \'Left eye\', \'Nose\', \'Right ear\', \'Right eye\']\n\n        preds = preds.clone()\n        preds[:, :, 1] = - preds[:, :, 1]\n        gts = gts.clone()\n        gts[:, :, 1] = - gts[:, :, 1]\n\n        for i in range(self.time_step):\n            win = \'pred_keypoints_\' + str(i)\n            self.draw_skeleton(preds[i], win, plus=True)\n\n        for i in range(self.time_step):\n            win = \'gt_keypoints_\' + str(i)\n            self.draw_skeleton(gts[i], win, plus=False)\n\n    def draw_skeleton(self, key_points, win_name, plus=False):\n        """"""\n        :param key_points: coco format [14 or 19, 2]\n        :return:\n        """"""\n        lsp_key_points_name = [\'Right ankle\', \'Right knee\', \'Right hip\', \'Left hip\', \'Left knee\', \'Left ankle\', \'Right wrist\', \'Right elbow\', \'Right shoulder\',\n                               \'Left shoulder\', \'Left elbow\', \'Left wrist\', \'Neck\', \'Head top\']\n\n        lsp_plus_key_points_name = lsp_key_points_name + [\'Nose\', \'Left eye\', \'Right eye\', \'Left ear\', \'Right ear\']\n\n        # start from 1\n        lsp_kintree_table = [(14, 13), (13, 10), (10, 11), (11, 12), (13, 9), (9, 8), (8, 7), (13, 4), (13, 3), (4, 5), (5, 6), (3, 2), (2, 1)]\n        lsp_plus_kintree_table = lsp_kintree_table + [(18, 16), (16, 15), (15, 17), (17, 19)]\n\n        # minus 1 to start from 0\n        lsp_kintree_table = [(k0 - 1, k1 - 1) for k0, k1 in lsp_kintree_table]\n        lsp_plus_kintree_table = [(k0 - 1, k1 - 1) for k0, k1 in lsp_plus_kintree_table]\n\n        if plus:\n            key_points_name = lsp_plus_key_points_name\n            kintree_table = lsp_plus_kintree_table\n        else:\n            key_points_name = lsp_key_points_name\n            kintree_table = lsp_kintree_table\n\n        X = np.array([[key_points[k0][0], key_points[k1][0]] for k0, k1 in kintree_table]).T\n        Y = np.array([[key_points[k0][1], key_points[k1][1]] for k0, k1 in kintree_table]).T\n\n        self.vis.line(Y, X, win=win_name, opts=dict(xtickmin=-1, xtickmax=1, xtickstep=0.2,\n                                                    ytickmin=-1, ytickmax=1, ytickstep=0.2,\n                                                    markers=True, title=win_name))\n\n    def vis_named_img(self, name, imgs, denormalize=True, transpose=False):\n        """"""\n        :param name: str, window name\n        :param imgs: np.ndarray or torch.tensor, (self.time_step, 1, self.image_size, self.image_size)\n        :param denormalize: True, [-1, 1] -> [0, 1]\n        :param transpose: False\n        :return:\n        """"""\n        if isinstance(imgs, np.ndarray):\n            if imgs.ndim == 3:\n                imgs = imgs[:, np.newaxis, :, :]\n\n            if transpose:\n                imgs = np.transpose(imgs, (0, 3, 1, 2))\n\n        else:\n            if imgs.ndimension() == 3:\n                imgs = imgs[:, None, :, :]\n\n            if transpose:\n                imgs = imgs.permute(0, 3, 1, 2)\n\n        if denormalize:\n            imgs = (imgs + 1) / 2.0\n\n        self.vis.images(\n            tensor=imgs,\n            win=name,\n            opts={\'title\': name}\n        )\n\n    def vis_preds_gts(self, preds=None, gts=None):\n        """"""\n        :type preds: np.ndarray, (self.time_step, 1, self.image_size, self.image_size) or\n                        (self.time_step, self.image_size, self.image_size)\n        :param preds: the time series of predicted silhouettes,\n                      if preds.ndim == 3, (self.time_step, self.image_size, self.image_size)\n                      then, it will reshape to (self.time_step, 1, self.image_size, self.image_size)\n\n        :type gts: np.ndarray, (self.time_step, 1, self.image_size, self.image_size) or\n                        (self.time_step, self.image_size, self.image_size)\n        :param gts: the time series of ground truth silhouettes,\n                      if preds.ndim == 3, (self.time_step, self.image_size, self.image_size)\n                      then, it will reshape to (self.time_step, 1, self.image_size, self.image_size)\n        """"""\n        if preds is not None:\n            if type(preds) == np.ndarray:\n                if preds.ndim == 3:\n                    preds = preds[:, np.newaxis, :, :]\n            else:\n                if preds.ndimension() == 3:\n                    preds = preds[:, None, :, :]\n\n            preds = (preds + 1.0) / 2.0\n            self.vis.images(\n                tensor=preds,\n                win=\'predicted images\',\n                opts={\'title\': \'predicted images\'}\n            )\n\n        if gts is not None:\n            if type(gts) == np.ndarray:\n                if gts.ndim == 3:\n                    gts = gts[:, np.newaxis, :, :]\n            else:\n                if gts.ndimension() == 3:\n                    gts = gts[:, None, :, :]\n\n            gts = (gts + 1.0) / 2.0\n            self.vis.images(\n                tensor=gts,\n                win=\'ground truth images\',\n                opts={\'title\': \'ground truth images\'}\n            )\n'"
thirdparty/his_evaluators/__init__.py,0,b'\n'
thirdparty/his_evaluators/setup.py,0,"b'from setuptools import setup\nimport unittest\n\n\ndef test_all():\n    test_loader = unittest.TestLoader()\n    test_suite = test_loader.discover(\'tests\', pattern=\'*_test.py\')\n    return test_suite\n\n\nsetup(\n    description=\'evaluators of Human Image Synthesize (HIS), including \'\n                \'Motion Imitation(MI), Appearance Transfer (AT), Novel View Synthesize(NVS)\',\n    name=\'his_evaluators\',\n    version=\'0.1.0\',\n    author=\'liuwen\',\n    author_email=\'liuwen@shanghaitech.edu.cn\',\n    packages=[\'his_evaluators\'],\n    package_data={\n        \'./data\': [\'*.json\'],\n    },\n    license=\'MIT License\',\n    test_suite=\'setup.test_all\',\n    install_requires=[\n        ""scikit_image >= 0.16.2"",\n        ""torchvision >= 0.4.2"",\n        ""scipy >= 1.2.1"",\n        ""opencv_contrib_python >= 3.2"",\n        ""tqdm >= 4.38.0"",\n        ""numpy >= 1.18.1"",\n        ""torch >= 1.0"",\n        ""setuptools >= 39.1.0"",\n        ""Pillow == 6.2.0"",\n        ""typing >= 3.7.4.1""\n    ],\n)\n'"
thirdparty/neural_renderer/__init__.py,0,b''
thirdparty/neural_renderer/setup.py,1,"b'from setuptools import setup, find_packages\nimport unittest\n\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nCUDA_FLAGS = []\n\ndef test_all():\n    test_loader = unittest.TestLoader()\n    test_suite = test_loader.discover(\'tests\', pattern=\'test_*.py\')\n    return test_suite\n\next_modules=[\n    CUDAExtension(\'neural_renderer.cuda.load_textures\', [\n        \'neural_renderer/cuda/load_textures_cuda.cpp\',\n        \'neural_renderer/cuda/load_textures_cuda_kernel.cu\',\n        ]),\n    CUDAExtension(\'neural_renderer.cuda.rasterize\', [\n        \'neural_renderer/cuda/rasterize_cuda.cpp\',\n        \'neural_renderer/cuda/rasterize_cuda_kernel.cu\',\n        ]),\n    CUDAExtension(\'neural_renderer.cuda.create_texture_image\', [\n        \'neural_renderer/cuda/create_texture_image_cuda.cpp\',\n        \'neural_renderer/cuda/create_texture_image_cuda_kernel.cu\',\n        ]),\n    ]\n\nINSTALL_REQUIREMENTS = [\'numpy\', \'torch\', \'torchvision\', \'scikit-image\', \'tqdm\', \'imageio\']\n\nsetup(\n    description=\'PyTorch implementation of ""A 3D mesh renderer for neural networks""\',\n    author=\'Nikolaos Kolotouros\',\n    author_email=\'nkolot@seas.upenn.edu\',\n    license=\'MIT License\',\n    version=\'1.1.3\',\n    name=\'neural_renderer\',\n    test_suite=\'setup.test_all\',\n    packages=[\'neural_renderer\', \'neural_renderer.cuda\'],\n    install_requires=INSTALL_REQUIREMENTS,\n    ext_modules=ext_modules,\n    cmdclass = {\'build_ext\': BuildExtension}\n)\n'"
thirdparty/his_evaluators/his_evaluators/__init__.py,0,"b'from .evaluators.motion_imitation import MotionImitationModel, IPERMotionImitationEvaluator\n'"
thirdparty/his_evaluators/tests/MotionSynthetic_test.py,0,"b'import unittest\nimport torch\nimport numpy as np\nimport cv2\nimport os\nimport shutil\nfrom tqdm import tqdm\n\nfrom his_evaluators.protocols.iPER import IPERProtocol\nfrom his_evaluators.protocols.MotionSynthetic import MotionSyntheticProtocol\n\n\nclass ProtocolTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls.iPER_Protocol = IPERProtocol(data_dir=""/p300/tpami/iPER"")\n        cls.MS_Protocol = MotionSyntheticProtocol(data_dir=""/p300/tpami/datasets/motionSynthetic"")\n\n    def test_01_MS_Protocol(self):\n        for vid_info in self.MS_Protocol:\n            print(vid_info)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n\n\n\n'"
thirdparty/his_evaluators/tests/__init__.py,0,b''
thirdparty/his_evaluators/tests/human_detector_test.py,4,"b'import cv2\nimport os\nimport numpy as np\nimport torch\n\nfrom his_evaluators.metrics.yolov3 import YoLov3HumanDetector, pad_to_square, resize\n\n\nif __name__ == ""__main__"":\n\n    # device = torch.device(""cuda:0"")\n    device = torch.device(""cpu"")\n    sample_dir = ""./data""\n    img_names = os.listdir(sample_dir)\n\n    detector = YoLov3HumanDetector(\n        weights_path=""../data/yolov3-spp.weights"",\n        device=device\n    )\n\n    original_imgs = []\n    input_imgs = []\n    input_shapes = []\n\n    img_names = [\n        ""pred_00000000.jpg"",\n        ""pred_00000114.jpg"",\n        ""pred_00000175.jpg"",\n        ""pred_00000423.jpg"",\n    ]\n    for name in img_names:\n        orig_img = cv2.imread(os.path.join(sample_dir, name))\n        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n        original_imgs.append(orig_img)\n        input_shapes.append(orig_img.shape[0:2])\n\n        img = orig_img.astype(np.float32)\n        img = img / 255\n        img = np.transpose(img, (2, 0, 1))\n        img = torch.tensor(img).float()\n        img, _ = pad_to_square(img, 0)\n        img = resize(img, detector.img_size)\n        input_imgs.append(img)\n\n    input_imgs = torch.stack(input_imgs).to(device)\n\n    boxes = detector.forward(input_imgs, input_shapes)\n\n    for name, box in zip(img_names, boxes):\n        if box is None:\n            continue\n\n        orig_img = cv2.imread(os.path.join(sample_dir, name))\n        x1, y1, x2, y2 = box\n\n        print(orig_img.shape)\n        crop = orig_img[y1:y2, x1:x2, :]\n\n        print(crop.shape)\n        crop_path = ""./data/crop_{}"".format(name)\n        print(crop_path)\n        cv2.imwrite(crop_path, crop)\n\n'"
thirdparty/his_evaluators/tests/metric_runner_test.py,1,"b'import unittest\nimport torch\nimport numpy as np\nimport cv2\nimport os\nimport shutil\nfrom tqdm import tqdm\n\n\nfrom his_evaluators.evaluators.base import PairedMetricRunner, UnpairedMetricRunner\n\n\nIMAGE_SIZE = 512\nDEVICE = torch.device(""cuda:0"")\nTEMPLATE_DIR = ""./template_dir""\n\n\ndef mkdir(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    return directory\n\n\ndef clean_dir(directory):\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n\n\ndef make_template_files(number=100):\n    global TEMPLATE_DIR\n\n    TEMPLATE_DIR = mkdir(TEMPLATE_DIR)\n    file_paths = []\n\n    print(""preparing {} samples and saving them into the template directory {}."".format(number, TEMPLATE_DIR))\n    for i in tqdm(range(number)):\n        pred = np.random.rand(IMAGE_SIZE, IMAGE_SIZE, 3)\n        pred *= 255\n        pred = pred.astype(np.uint8)\n\n        # ref = np.random.rand(IMAGE_SIZE, IMAGE_SIZE, 3)\n        ref = np.ones((IMAGE_SIZE, IMAGE_SIZE, 3))\n        ref *= 255\n        ref = ref.astype(np.uint8)\n\n        pred_path = os.path.join(TEMPLATE_DIR, ""{:0>8}.png"".format(i))\n        ref_path = os.path.join(TEMPLATE_DIR, ""{:0>8}.png"".format(i))\n\n        cv2.imwrite(pred_path, pred)\n        cv2.imwrite(ref_path, ref)\n\n        file_paths.append((pred_path, ref_path))\n\n    return file_paths\n\n\nclass MetricRunnerTest(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.paired_metric_runner = PairedMetricRunner(\n            metric_types=(""ssim"", ""psnr"", ""lps"", ""OS-CS-reid"", ""face-CS""), device=DEVICE\n        )\n\n        cls.unpaired_metric_runner = UnpairedMetricRunner(\n            metric_types=(""is"", ""fid"", ""OS-CS-reid"", ""OS-freid"", ""face-CS"", ""face-FD"", ""SSPE""), device=DEVICE\n        )\n\n        # cls.unpaired_metric_runner = UnpairedMetricRunner(\n        #     metric_types=(""is"", ""fid"", ""PCB-CS-reid"", ""PCB-freid"", ""OS-CS-reid"", ""OS-freid""), device=DEVICE\n        # )\n\n        # cls.unpaired_metric_runner = UnpairedMetricRunner(\n        #     metric_types=(""is"", ""fid"", ""PCB-CS-reid"", ""PCB-freid""), device=DEVICE\n        # )\n\n        cls.file_paths = make_template_files(number=100)\n\n    def test_01_paired_runner(self):\n        self.paired_metric_runner.evaluate(file_paths=self.file_paths, image_size=IMAGE_SIZE, batch_size=16)\n\n    def test_02_unpaired_runner(self):\n        self.unpaired_metric_runner.evaluate(file_paths=self.file_paths, image_size=IMAGE_SIZE, batch_size=16)\n\n    def test_04_SSPE(self):\n        from his_evaluators.evaluators.base import PairedEvaluationDataset, build_data_loader\n\n        sample_dir = ""./data""\n        img_names = [\n            ""pred_00000000.jpg"",\n            ""pred_00000114.jpg"",\n            ""pred_00000175.jpg"",\n            ""pred_00000423.jpg"",\n        ]\n        all_img_paths = []\n        for name in img_names:\n            img_path = os.path.join(sample_dir, name)\n            all_img_paths.append(img_path)\n\n        dataset = PairedEvaluationDataset(list(zip(all_img_paths, all_img_paths)), image_size=512)\n        dataloader = build_data_loader(dataset, batch_size=4)\n        sample = next(iter(dataloader))\n\n        pred = sample[""pred""]\n        ref = sample[""ref""]\n        ids = np.random.permutation(len(ref))\n        ref = ref[ids]\n        sspe = self.unpaired_metric_runner.metric_dict[""SSPE""].calculate_score(pred, ref)\n        print(""sspe = {}"".format(sspe))\n\n    def test_03_face_detector(self):\n        from his_evaluators.evaluators.base import PairedEvaluationDataset, build_data_loader\n\n        sample_dir = ""./data""\n        img_names = [\n            ""pred_00000000.jpg"",\n            ""pred_00000114.jpg"",\n            ""pred_00000175.jpg"",\n            ""pred_00000423.jpg"",\n        ]\n        all_img_paths = []\n        for name in img_names:\n            img_path = os.path.join(sample_dir, name)\n            all_img_paths.append(img_path)\n\n        dataset = PairedEvaluationDataset(list(zip(all_img_paths, all_img_paths)), image_size=512)\n        dataloader = build_data_loader(dataset, batch_size=4)\n        sample = next(iter(dataloader))\n\n        ref = sample[""ref""]\n        face_cropped, valid_ids = self.paired_metric_runner.metric_dict[""face-CS""].detect_face(ref)\n        print(valid_ids)\n        has_detected_face = face_cropped[valid_ids]\n        print(face_cropped.shape, has_detected_face.shape)\n        face_cropped = face_cropped.cpu().numpy()\n        for i, face in enumerate(face_cropped):\n            face = (face + 1) / 2 * 255\n            face = face.astype(np.uint8, copy=False)\n            face = np.transpose(face, (1, 2, 0))\n            face = cv2.cvtColor(face, cv2.COLOR_RGB2BGR)\n\n            print(face.shape, face.max(), face.min())\n            cv2.imwrite(os.path.join(sample_dir, ""face_{}"".format(img_names[i])), face)\n\n        print(face_cropped.shape, face_cropped.max(), face_cropped.min())\n\n    @classmethod\n    def tearDownClass(cls):\n        global TEMPLATE_DIR\n\n        clean_dir(TEMPLATE_DIR)\n\n        print(""clean the template directory {}"".format(TEMPLATE_DIR))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
thirdparty/his_evaluators/tests/metric_test.py,8,"b'import torch\nimport numpy as np\nimport unittest\nimport cv2\n\n\nfrom his_evaluators.metrics import register_metrics\n\n\nDEVICE = torch.device(""cuda:0"")\n\n\ndef load_image(img_path):\n    image = cv2.imread(img_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = np.transpose(image, (2, 0, 1))\n    image = image.astype(np.float32, copy=False)\n    image /= 255\n    image = torch.as_tensor(image)\n    return image\n\n\nclass MetricTestCase(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls.paired_metric_dict = register_metrics(types=(""ssim"", ""psnr"", ""lps""), device=DEVICE)\n        # cls.unpaired_metric_dict = register_metrics(\n        #     types=(""is"", ""fid"", ""PCB-CS-reid"", ""PCB-freid"", ""OS-CS-reid"", ""OS-freid""),\n        #     device=DEVICE\n        # )\n\n        cls.unpaired_metric_dict = register_metrics(\n            types=(""is"", ""fid"", ""SSPE"", ""OS-CS-reid"", ""OS-freid""),\n            device=DEVICE\n        )\n\n        cls.face_metric_dict = register_metrics(\n            types=(""face-CS"", ),\n            device=DEVICE\n        )\n\n    # def test_01_paired_metrics(self):\n    #     bs = 5\n    #     image_size = 512\n    #     preds_imgs = np.random.rand(bs, 3, image_size, image_size)\n    #     preds_imgs *= 255\n    #     preds_imgs = preds_imgs.astype(np.uint8)\n    #     ref_imgs = np.copy(preds_imgs)\n    #\n    #     preds_imgs = torch.tensor(preds_imgs).float()\n    #     ref_imgs = torch.tensor(ref_imgs).float()\n    #     ssim_score = self.paired_metric_dict[""ssim""].calculate_score(preds_imgs, ref_imgs)\n    #     psnr_score = self.paired_metric_dict[""psnr""].calculate_score(preds_imgs, ref_imgs)\n    #     lps_score = self.paired_metric_dict[""lps""].calculate_score(preds_imgs, ref_imgs)\n    #\n    #     print(""ssim score = {}"".format(ssim_score))\n    #     print(""psnr score = {}"".format(psnr_score))\n    #     print(""lps score = {}"".format(lps_score))\n    #\n    #     self.assertEqual(ssim_score, 1.0)\n    #     self.assertEqual(psnr_score, np.inf)\n    #     self.assertEqual(lps_score, 0.0)\n    #\n    # def test_02_unpaired_metrics(self):\n    #     bs = 5\n    #     image_size = 512\n    #     preds_imgs = np.random.rand(bs, 3, image_size, image_size)\n    #     preds_imgs *= 255\n    #     preds_imgs = preds_imgs.astype(np.uint8)\n    #\n    #     ref_imgs = np.random.rand(bs, 3, image_size, image_size)\n    #     ref_imgs *= 255\n    #     ref_imgs = ref_imgs.astype(np.uint8)\n    #\n    #     preds_imgs = torch.tensor(preds_imgs).float()\n    #     ref_imgs = torch.tensor(ref_imgs).float()\n    #\n    #     inception_score = self.unpaired_metric_dict[""is""].calculate_score(preds_imgs)\n    #     fid_score = self.unpaired_metric_dict[""fid""].calculate_score(preds_imgs, ref_imgs)\n    #     sspe = self.unpaired_metric_dict[""SSPE""].calculate_score(preds_imgs, ref_imgs)\n    #     os_cs_reid = self.unpaired_metric_dict[""OS-CS-reid""].calculate_score(preds_imgs, ref_imgs)\n    #     os_freid = self.unpaired_metric_dict[""OS-freid""].calculate_score(preds_imgs, ref_imgs)\n    #\n    #     # pcb_cs_reid = self.unpaired_metric_dict[""PCB-CS-reid""].calculate_score(preds_imgs, ref_imgs)\n    #     # pcb_freid = self.unpaired_metric_dict[""PCB-freid""].calculate_score(preds_imgs, ref_imgs)\n    #\n    #     print(""inception score = {}"".format(inception_score))\n    #     print(""fid score = {}"".format(fid_score))\n    #     print(""ssp error = {}"".format(sspe))\n    #     print(""OS-Cosine Similarity = {}"".format(os_cs_reid))\n    #     print(""OS-freid = {}"".format(os_freid))\n    #\n    #     # print(""PCB-Cosine Similarity = {}"".format(pcb_cs_reid))\n    #     # print(""PCB-freid = {}"".format(pcb_freid))\n\n    def test_03_face_metric_all_have_face(self):\n        pred_img_list = [\n            ""./data/pred_00000000.jpg"",\n            ""./data/pred_00000114.jpg"",\n            ""./data/pred_00000423.jpg"",\n            ""./data/pred_00000175.jpg""\n        ]\n\n        ref_img_list = [\n            ""./data/pred_00000423.jpg"",\n            ""./data/pred_00000114.jpg"",\n            ""./data/pred_00000175.jpg"",\n            ""./data/pred_00000000.jpg""\n        ]\n\n        pred_imgs = []\n        for img_path in pred_img_list:\n            img = load_image(img_path)\n            pred_imgs.append(img)\n        pred_imgs = torch.stack(pred_imgs)\n\n        ref_imgs = []\n        for img_path in ref_img_list:\n            img = load_image(img_path)\n            ref_imgs.append(img)\n        ref_imgs = torch.stack(ref_imgs)\n\n        face_cs = self.face_metric_dict[""face-CS""].calculate_score(pred_imgs, ref_imgs)\n\n        print(""face-cs"", face_cs)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
thirdparty/neural_renderer/examples/__init__.py,0,b''
thirdparty/neural_renderer/examples/example1.py,1,"b'""""""\nExample 1. Drawing a teapot from multiple viewpoints.\n""""""\nimport os\nimport argparse\n\nimport torch\nimport numpy as np\nimport tqdm\nimport imageio\n\nimport neural_renderer as nr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-i\', \'--filename_input\', type=str, default=os.path.join(data_dir, \'teapot.obj\'))\n    parser.add_argument(\'-o\', \'--filename_output\', type=str, default=os.path.join(data_dir, \'example1.gif\'))\n    parser.add_argument(\'-g\', \'--gpu\', type=int, default=0)\n    args = parser.parse_args()\n\n    # other settings\n    camera_distance = 2.732\n    elevation = 30\n    texture_size = 2\n\n    # load .obj\n    vertices, faces = nr.load_obj(args.filename_input)\n    vertices = vertices[None, :, :]  # [num_vertices, XYZ] -> [batch_size=1, num_vertices, XYZ]\n    faces = faces[None, :, :]  # [num_faces, 3] -> [batch_size=1, num_faces, 3]\n\n    # create texture [batch_size=1, num_faces, texture_size, texture_size, texture_size, RGB]\n    textures = torch.ones(1, faces.shape[1], texture_size, texture_size, texture_size, 3, dtype=torch.float32).cuda()\n\n    # to gpu\n\n    # create renderer\n    renderer = nr.Renderer(camera_mode=\'look_at\')\n\n    # draw object\n    loop = tqdm.tqdm(range(0, 360, 4))\n    writer = imageio.get_writer(args.filename_output, mode=\'I\')\n    for num, azimuth in enumerate(loop):\n        loop.set_description(\'Drawing\')\n        renderer.eye = nr.get_points_from_angles(camera_distance, elevation, azimuth)\n        images = renderer(vertices, faces, textures)  # [batch_size, RGB, image_size, image_size]\n        image = images.detach().cpu().numpy()[0].transpose((1, 2, 0))  # [image_size, image_size, RGB]\n        writer.append_data((255*image).astype(np.uint8))\n    writer.close()\n\nif __name__ == \'__main__\':\n    main()\n'"
thirdparty/neural_renderer/examples/example2.py,5,"b'""""""\nExample 2. Optimizing vertices.\n""""""\nfrom __future__ import division\nimport os\nimport argparse\nimport glob\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom skimage.io import imread, imsave\nimport tqdm\nimport imageio\n\nimport neural_renderer as nr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\n\nclass Model(nn.Module):\n    def __init__(self, filename_obj, filename_ref):\n        super(Model, self).__init__()\n\n        # load .obj\n        vertices, faces = nr.load_obj(filename_obj)\n        self.vertices = nn.Parameter(vertices[None, :, :])\n        self.register_buffer(\'faces\', faces[None, :, :])\n\n        # create textures\n        texture_size = 2\n        textures = torch.ones(1, self.faces.shape[1], texture_size, texture_size, texture_size, 3, dtype=torch.float32)\n        self.register_buffer(\'textures\', textures)\n\n        # load reference image\n        image_ref = torch.from_numpy(imread(filename_ref).astype(np.float32).mean(-1) / 255.)[None, ::]\n        self.register_buffer(\'image_ref\', image_ref)\n\n        # setup renderer\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        self.renderer = renderer\n\n    def forward(self):\n        self.renderer.eye = nr.get_points_from_angles(2.732, 0, 90)\n        image = self.renderer(self.vertices, self.faces, mode=\'silhouettes\')\n        loss = torch.sum((image - self.image_ref[None, :, :])**2)\n        return loss\n\n\ndef make_gif(filename):\n    with imageio.get_writer(filename, mode=\'I\') as writer:\n        for filename in sorted(glob.glob(\'/tmp/_tmp_*.png\')):\n            writer.append_data(imageio.imread(filename))\n            os.remove(filename)\n    writer.close()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-io\', \'--filename_obj\', type=str, default=os.path.join(data_dir, \'teapot.obj\'))\n    parser.add_argument(\'-ir\', \'--filename_ref\', type=str, default=os.path.join(data_dir, \'example2_ref.png\'))\n    parser.add_argument(\n        \'-oo\', \'--filename_output_optimization\', type=str, default=os.path.join(data_dir, \'example2_optimization.gif\'))\n    parser.add_argument(\n        \'-or\', \'--filename_output_result\', type=str, default=os.path.join(data_dir, \'example2_result.gif\'))\n    parser.add_argument(\'-g\', \'--gpu\', type=int, default=0)\n    args = parser.parse_args()\n\n    model = Model(args.filename_obj, args.filename_ref)\n    model.cuda()\n\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n    # optimizer.setup(model)\n    loop = tqdm.tqdm(range(300))\n    for i in loop:\n        loop.set_description(\'Optimizing\')\n        # optimizer.target.cleargrads()\n        optimizer.zero_grad()\n        loss = model()\n        loss.backward()\n        optimizer.step()\n        images = model.renderer(model.vertices, model.faces, mode=\'silhouettes\')\n        image = images.detach().cpu().numpy()[0]\n        imsave(\'/tmp/_tmp_%04d.png\' % i, image)\n    make_gif(args.filename_output_optimization)\n\n    # draw object\n    loop = tqdm.tqdm(range(0, 360, 4))\n    for num, azimuth in enumerate(loop):\n        loop.set_description(\'Drawing\')\n        model.renderer.eye = nr.get_points_from_angles(2.732, 0, azimuth)\n        images = model.renderer(model.vertices, model.faces, model.textures)\n        image = images.detach().cpu().numpy()[0].transpose((1, 2, 0))\n        imsave(\'/tmp/_tmp_%04d.png\' % num, image)\n    make_gif(args.filename_output_result)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
thirdparty/neural_renderer/examples/example3.py,7,"b'""""""\nExample 3. Optimizing textures.\n""""""\nfrom __future__ import division\nimport os\nimport argparse\nimport glob\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nimport tqdm\nimport imageio\n\nimport neural_renderer as nr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\n\n\nclass Model(nn.Module):\n    def __init__(self, filename_obj, filename_ref):\n        super(Model, self).__init__()\n        vertices, faces = nr.load_obj(filename_obj)\n        self.register_buffer(\'vertices\', vertices[None, :, :])\n        self.register_buffer(\'faces\', faces[None, :, :])\n\n        # create textures\n        texture_size = 4\n        textures = torch.zeros(1, self.faces.shape[1], texture_size, texture_size, texture_size, 3, dtype=torch.float32)\n        self.textures = nn.Parameter(textures)\n\n        # load reference image\n        image = cv2.imread(filename_ref)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = np.transpose(image, (2, 0, 1))\n\n        image_ref = torch.from_numpy(image.astype(\'float32\') / 255.)[None, ::]\n        self.register_buffer(\'image_ref\', image_ref)\n\n        # setup renderer\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.perspective = False\n        renderer.light_intensity_directional = 0.0\n        renderer.light_intensity_ambient = 1.0\n        self.renderer = renderer\n\n    def forward(self):\n        self.renderer.eye = nr.get_points_from_angles(2.732, 0, np.random.uniform(0, 360))\n        image = self.renderer(self.vertices, self.faces, torch.tanh(self.textures))\n        loss = torch.sum((image - self.image_ref) ** 2)\n        return loss\n\n\ndef make_gif(filename):\n    with imageio.get_writer(filename, mode=\'I\') as writer:\n        for filename in sorted(glob.glob(\'/tmp/_tmp_*.png\')):\n            writer.append_data(imageio.imread(filename))\n            os.remove(filename)\n    writer.close()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-io\', \'--filename_obj\', type=str, default=os.path.join(data_dir, \'teapot.obj\'))\n    parser.add_argument(\'-ir\', \'--filename_ref\', type=str, default=os.path.join(data_dir, \'example3_ref.png\'))\n    parser.add_argument(\'-or\', \'--filename_output\', type=str, default=os.path.join(data_dir, \'example3_result.gif\'))\n    parser.add_argument(\'-g\', \'--gpu\', type=int, default=0)\n    args = parser.parse_args()\n\n    model = Model(args.filename_obj, args.filename_ref)\n    model.cuda()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1, betas=(0.5,0.999))\n    loop = tqdm.tqdm(range(300))\n    for _ in loop:\n        loop.set_description(\'Optimizing\')\n        optimizer.zero_grad()\n        loss = model()\n        loss.backward()\n        optimizer.step()\n\n    # draw object\n    loop = tqdm.tqdm(range(0, 360, 4))\n    for num, azimuth in enumerate(loop):\n        loop.set_description(\'Drawing\')\n        model.renderer.eye = nr.get_points_from_angles(2.732, 0, azimuth)\n        images = model.renderer(model.vertices, model.faces, torch.tanh(model.textures))\n        image = images.detach().cpu().numpy()[0].transpose((1, 2, 0))\n        # cv2.imwrite(\'/tmp/_tmp_%04d.png\' % num, image)\n        cv2.imshow(\'test\', image)\n        cv2.waitKey(1000)\n\n    make_gif(args.filename_output)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
thirdparty/neural_renderer/examples/example4.py,8,"b'""""""\nExample 4. Finding camera parameters.\n""""""\nimport os\nimport argparse\nimport glob\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom skimage.io import imread, imsave\nimport tqdm\nimport imageio\n\nimport neural_renderer as nr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\n\nclass Model(nn.Module):\n    def __init__(self, filename_obj, filename_ref=None):\n        super(Model, self).__init__()\n        # load .obj\n        vertices, faces = nr.load_obj(filename_obj)\n        self.register_buffer(\'vertices\', vertices[None, :, :])\n        self.register_buffer(\'faces\', faces[None, :, :])\n\n        # create textures\n        texture_size = 2\n        textures = torch.ones(1, self.faces.shape[1], texture_size, texture_size, texture_size, 3, dtype=torch.float32)\n        self.register_buffer(\'textures\', textures)\n\n        # load reference image\n        image_ref = torch.from_numpy((imread(filename_ref).max(-1) != 0).astype(np.float32))\n        self.register_buffer(\'image_ref\', image_ref)\n\n        # camera parameters\n        self.camera_position = nn.Parameter(torch.from_numpy(np.array([6, 10, -14], dtype=np.float32)))\n\n        # setup renderer\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.eye = self.camera_position\n        self.renderer = renderer\n\n    def forward(self):\n        image = self.renderer(self.vertices, self.faces, mode=\'silhouettes\')\n        loss = torch.sum((image - self.image_ref[None, :, :]) ** 2)\n        return loss\n\n\ndef make_gif(filename):\n    with imageio.get_writer(filename, mode=\'I\') as writer:\n        for filename in sorted(glob.glob(\'/tmp/_tmp_*.png\')):\n            writer.append_data(imread(filename))\n            os.remove(filename)\n    writer.close()\n\n\ndef make_reference_image(filename_ref, filename_obj):\n    model = Model(filename_obj)\n    model.cuda()\n\n    model.renderer.eye = nr.get_points_from_angles(2.732, 30, -15)\n    images = model.renderer.render(model.vertices, model.faces, torch.tanh(model.textures))\n    image = images.detach().cpu().numpy()[0]\n    imsave(filename_ref, image)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-io\', \'--filename_obj\', type=str, default=os.path.join(data_dir, \'teapot.obj\'))\n    parser.add_argument(\'-ir\', \'--filename_ref\', type=str, default=os.path.join(data_dir, \'example4_ref.png\'))\n    parser.add_argument(\'-or\', \'--filename_output\', type=str, default=os.path.join(data_dir, \'example4_result.gif\'))\n    parser.add_argument(\'-mr\', \'--make_reference_image\', type=int, default=0)\n    parser.add_argument(\'-g\', \'--gpu\', type=int, default=0)\n    args = parser.parse_args()\n\n    if args.make_reference_image:\n        make_reference_image(args.filename_ref, args.filename_obj)\n\n    model = Model(args.filename_obj, args.filename_ref)\n    model.cuda()\n\n    # optimizer = chainer.optimizers.Adam(alpha=0.1)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n    loop = tqdm.tqdm(range(1000))\n    for i in loop:\n        optimizer.zero_grad()\n        loss = model()\n        loss.backward()\n        optimizer.step()\n        images = model.renderer(model.vertices, model.faces, torch.tanh(model.textures))\n        image = images.detach().cpu().numpy()[0].transpose(1,2,0)\n        imsave(\'/tmp/_tmp_%04d.png\' % i, image)\n        loop.set_description(\'Optimizing (loss %.4f)\' % loss.data)\n        if loss.item() < 70:\n            break\n    make_gif(args.filename_output)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
thirdparty/neural_renderer/neural_renderer/__init__.py,0,"b""from .get_points_from_angles import get_points_from_angles\nfrom .lighting import lighting\nfrom .load_obj import load_obj\nfrom .look import look\nfrom .look_at import look_at\nfrom .mesh import Mesh\nfrom .perspective import perspective\nfrom .projection import projection, projection_by_params\nfrom .rasterize import (rasterize_rgbad, rasterize, rasterize_rgb_and_face_index_map, rasterize_silhouettes,\n                        rasterize_depth, rasterize_face_index_map, rasterize_face_index_map_and_weight_map,\n                        rasterize_weight_map, Rasterize)\nfrom .renderer import Renderer\nfrom .save_obj import save_obj\nfrom .vertices_to_faces import vertices_to_faces\n\n__version__ = '1.1.3'\n"""
thirdparty/neural_renderer/neural_renderer/get_points_from_angles.py,4,"b'from __future__ import division\nimport math\n\nimport torch\n\ndef get_points_from_angles(distance, elevation, azimuth, degrees=True):\n    if isinstance(distance, float) or isinstance(distance, int):\n        if degrees:\n            elevation = math.radians(elevation)\n            azimuth = math.radians(azimuth)\n        return (\n            distance * math.cos(elevation) * math.sin(azimuth),\n            distance * math.sin(elevation),\n            -distance * math.cos(elevation) * math.cos(azimuth))\n    else:\n        if degrees:\n            elevation = math.pi/180. * elevation\n            azimuth = math.pi/180. * azimuth\n    #\n        return torch.stack([\n            distance * torch.cos(elevation) * torch.sin(azimuth),\n            distance * torch.sin(elevation),\n            -distance * torch.cos(elevation) * torch.cos(azimuth)\n            ]).transpose(1,0)\n'"
thirdparty/neural_renderer/neural_renderer/lighting.py,10,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\n\n\ndef lighting(faces, textures, intensity_ambient=0.5, intensity_directional=0.5,\n             color_ambient=(1, 1, 1), color_directional=(1, 1, 1), direction=(0, 1, 0)):\n\n    bs, nf = faces.shape[:2]\n    device = faces.device\n\n    # arguments\n    # make sure to convert all inputs to float tensors\n    if isinstance(color_ambient, tuple) or isinstance(color_ambient, list):\n        color_ambient = torch.tensor(color_ambient, dtype=torch.float32, device=device)\n    elif isinstance(color_ambient, np.ndarray):\n        color_ambient = torch.from_numpy(color_ambient).float().to(device)\n    if isinstance(color_directional, tuple) or isinstance(color_directional, list):\n        color_directional = torch.tensor(color_directional, dtype=torch.float32, device=device)\n    elif isinstance(color_directional, np.ndarray):\n        color_directional = torch.from_numpy(color_directional).float().to(device)\n    if isinstance(direction, tuple) or isinstance(direction, list):\n        direction = torch.tensor(direction, dtype=torch.float32, device=device)\n    elif isinstance(direction, np.ndarray):\n        direction = torch.from_numpy(direction).float().to(device)\n    if color_ambient.ndimension() == 1:\n        color_ambient = color_ambient[None, :]\n    if color_directional.ndimension() == 1:\n        color_directional = color_directional[None, :]\n    if direction.ndimension() == 1:\n        direction = direction[None, :]\n\n    # create light\n    light = torch.zeros(bs, nf, 3, dtype=torch.float32).to(device)\n\n    # ambient light\n    if intensity_ambient != 0:\n        light += intensity_ambient * color_ambient[:, None, :]\n\n    # directional light\n    if intensity_directional != 0:\n        faces = faces.reshape((bs * nf, 3, 3))\n        v10 = faces[:, 0] - faces[:, 1]\n        v12 = faces[:, 2] - faces[:, 1]\n        # pytorch normalize divides by max(norm, eps) instead of (norm+eps) in chainer\n        normals = F.normalize(torch.cross(v10, v12), eps=1e-5)\n        normals = normals.reshape((bs, nf, 3))\n\n        if direction.ndimension() == 2:\n            direction = direction[:, None, :]\n        cos = F.relu(torch.sum(normals * direction, dim=2))\n        # may have to verify that the next line is correct\n        light += intensity_directional * (color_directional[:, None, :] * cos[:, :, None])\n\n    # apply\n    light = light[:,:,None, None, None, :]\n    textures *= light\n    return textures\n'"
thirdparty/neural_renderer/neural_renderer/load_obj.py,8,"b'from __future__ import division\nimport os\n\nimport torch\nimport numpy as np\nimport cv2\n\n\nimport neural_renderer.cuda.load_textures as load_textures_cuda\n\ndef load_mtl(filename_mtl):\n    \'\'\'\n    load color (Kd) and filename of textures from *.mtl\n    \'\'\'\n    texture_filenames = {}\n    colors = {}\n    material_name = \'\'\n    with open(filename_mtl) as f:\n        for line in f.readlines():\n            if len(line.split()) != 0:\n                if line.split()[0] == \'newmtl\':\n                    material_name = line.split()[1]\n                if line.split()[0] == \'map_Kd\':\n                    texture_filenames[material_name] = line.split()[1]\n                if line.split()[0] == \'Kd\':\n                    colors[material_name] = np.array(list(map(float, line.split()[1:4])))\n    return colors, texture_filenames\n\n\ndef load_textures(filename_obj, filename_mtl, texture_size):\n    # load vertices\n    vertices = []\n    with open(filename_obj) as f:\n        lines = f.readlines()\n    for line in lines:\n        if len(line.split()) == 0:\n            continue\n        if line.split()[0] == \'vt\':\n            vertices.append([float(v) for v in line.split()[1:3]])\n    vertices = np.vstack(vertices).astype(np.float32)\n\n    # load faces for textures\n    faces = []\n    material_names = []\n    material_name = \'\'\n    for line in lines:\n        if len(line.split()) == 0:\n            continue\n        if line.split()[0] == \'f\':\n            vs = line.split()[1:]\n            nv = len(vs)\n            if \'/\' in vs[0]:\n                v0 = int(vs[0].split(\'/\')[1])\n            else:\n                v0 = 0\n            for i in range(nv - 2):\n                if \'/\' in vs[i + 1]:\n                    v1 = int(vs[i + 1].split(\'/\')[1])\n                else:\n                    v1 = 0\n                if \'/\' in vs[i + 2]:\n                    v2 = int(vs[i + 2].split(\'/\')[1])\n                else:\n                    v2 = 0\n                faces.append((v0, v1, v2))\n                material_names.append(material_name)\n        if line.split()[0] == \'usemtl\':\n            material_name = line.split()[1]\n    faces = np.vstack(faces).astype(np.int32) - 1\n    faces = vertices[faces]\n    faces = torch.from_numpy(faces).cuda()\n    faces[1 < faces] = faces[1 < faces] % 1\n\n    colors, texture_filenames = load_mtl(filename_mtl)\n\n    textures = torch.zeros(faces.shape[0], texture_size, texture_size, texture_size, 3, dtype=torch.float32) + 0.5\n    textures = textures.cuda()\n\n    #\n    for material_name, color in colors.items():\n        color = torch.from_numpy(color).cuda()\n        for i, material_name_f in enumerate(material_names):\n            if material_name == material_name_f:\n                textures[i, :, :, :, :] = color[None, None, None, :]\n\n    for material_name, filename_texture in texture_filenames.items():\n        filename_texture = os.path.join(os.path.dirname(filename_obj), filename_texture)\n        image = cv2.imread(filename_texture).astype(np.float32) / 255.0\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # pytorch does not support negative slicing for the moment\n        image = image[::-1, :, :]\n        image = torch.from_numpy(image.copy()).cuda()\n        is_update = (np.array(material_names) == material_name).astype(np.int32)\n        is_update = torch.from_numpy(is_update).cuda()\n        textures = load_textures_cuda.load_textures(image, faces, textures, is_update)\n    return textures\n\n\ndef load_obj(filename_obj, normalization=True, texture_size=4, load_texture=False):\n    """"""\n    Load Wavefront .obj file.\n    This function only supports vertices (v x x x) and faces (f x x x).\n    """"""\n\n    # load vertices\n    vertices = []\n    with open(filename_obj) as f:\n        lines = f.readlines()\n\n    for line in lines:\n        if len(line.split()) == 0:\n            continue\n        if line.split()[0] == \'v\':\n            vertices.append([float(v) for v in line.split()[1:4]])\n    vertices = torch.from_numpy(np.vstack(vertices).astype(np.float32)).cuda()\n\n    # load faces\n    faces = []\n    for line in lines:\n        if len(line.split()) == 0:\n            continue\n        if line.split()[0] == \'f\':\n            vs = line.split()[1:]\n            nv = len(vs)\n            v0 = int(vs[0].split(\'/\')[0])\n            for i in range(nv - 2):\n                v1 = int(vs[i + 1].split(\'/\')[0])\n                v2 = int(vs[i + 2].split(\'/\')[0])\n                faces.append((v0, v1, v2))\n    faces = torch.from_numpy(np.vstack(faces).astype(np.int32)).cuda() - 1\n\n    # load textures\n    textures = None\n    if load_texture:\n        for line in lines:\n            if line.startswith(\'mtllib\'):\n                filename_mtl = os.path.join(os.path.dirname(filename_obj), line.split()[1])\n                textures = load_textures(filename_obj, filename_mtl, texture_size)\n        if textures is None:\n            raise Exception(\'Failed to load textures.\')\n\n    # normalize into a unit cube centered zero\n    if normalization:\n        vertices -= vertices.min(0)[0][None, :]\n        vertices /= torch.abs(vertices).max()\n        vertices *= 2\n        vertices -= vertices.max(0)[0][None, :] / 2\n\n    if load_texture:\n        return vertices, faces, textures\n    else:\n        return vertices, faces\n'"
thirdparty/neural_renderer/neural_renderer/look.py,14,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\ndef look(vertices, eye, direction=[0, 1, 0], up=[0, 1, 0]):\n    """"""\n    ""Look"" transformation of vertices.\n    """"""\n    if (vertices.ndimension() != 3):\n        raise ValueError(\'vertices Tensor should have 3 dimensions\')\n\n    device = vertices.device\n\n    if isinstance(direction, list) or isinstance(direction, tuple):\n        direction = torch.tensor(direction, dtype=torch.float32, device=device)\n    elif isinstance(direction, np.ndarray):\n        direction = torch.from_numpy(direction).to(device)\n    elif torch.is_tensor(direction):\n        direction.to(device)\n\n    if isinstance(eye, list) or isinstance(eye, tuple):\n        eye = torch.tensor(eye, dtype=torch.float32, device=device)\n    elif isinstance(eye, np.ndarray):\n        eye = torch.from_numpy(eye).to(device)\n    elif torch.is_tensor(eye):\n        eye = eye.to(device)\n\n    if isinstance(up, list) or isinstance(up, tuple):\n        up = torch.tensor(up, dtype=torch.float32, device=device)\n    elif isinstance(up, np.ndarray):\n        up = torch.from_numpy(up).to(device)\n    elif torch.is_tensor(up):\n        up.to(device)\n\n    if eye.ndimension() == 1:\n        eye = eye[None, :]\n    if direction.ndimension() == 1:\n        direction = direction[None, :]\n    if up.ndimension() == 1:\n        up = up[None, :]\n\n    # create new axes\n    z_axis = F.normalize(direction, eps=1e-5)\n    x_axis = F.normalize(torch.cross(up, z_axis), eps=1e-5)\n    y_axis = F.normalize(torch.cross(z_axis, x_axis), eps=1e-5)\n\n    # create rotation matrix: [bs, 3, 3]\n    r = torch.cat((x_axis[:, None, :], y_axis[:, None, :], z_axis[:, None, :]), dim=1)\n\n    # apply\n    # [bs, nv, 3] -> [bs, nv, 3] -> [bs, nv, 3]\n    if vertices.shape != eye.shape:\n        eye = eye[:, None, :]\n    vertices = vertices - eye\n    vertices = torch.matmul(vertices, r.transpose(1,2))\n\n    return vertices\n'"
thirdparty/neural_renderer/neural_renderer/look_at.py,14,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\ndef look_at(vertices, eye, at=[0, 0, 0], up=[0, 1, 0]):\n    """"""\n    ""Look at"" transformation of vertices.\n    """"""\n    if (vertices.ndimension() != 3):\n        raise ValueError(\'vertices Tensor should have 3 dimensions\')\n\n    device = vertices.device\n\n    # if list or tuple convert to numpy array\n    if isinstance(at, list) or isinstance(at, tuple):\n        at = torch.tensor(at, dtype=torch.float32, device=device)\n    # if numpy array convert to tensor\n    elif isinstance(at, np.ndarray):\n        at = torch.from_numpy(at).to(device)\n    elif torch.is_tensor(at):\n        at.to(device)\n\n    if isinstance(up, list) or isinstance(up, tuple):\n        up = torch.tensor(up, dtype=torch.float32, device=device)\n    elif isinstance(up, np.ndarray):\n        up = torch.from_numpy(up).to(device)\n    elif torch.is_tensor(up):\n        up.to(device)\n\n    if isinstance(eye, list) or isinstance(eye, tuple):\n        eye = torch.tensor(eye, dtype=torch.float32, device=device)\n    elif isinstance(eye, np.ndarray):\n        eye = torch.from_numpy(eye).to(device)\n    elif torch.is_tensor(eye):\n        eye = eye.to(device)\n\n    batch_size = vertices.shape[0]\n    if eye.ndimension() == 1:\n        eye = eye[None, :].repeat(batch_size, 1)\n    if at.ndimension() == 1:\n        at = at[None, :].repeat(batch_size, 1)\n    if up.ndimension() == 1:\n        up = up[None, :].repeat(batch_size, 1)\n\n    # create new axes\n    # eps is chosen as 0.5 to match the chainer version\n    z_axis = F.normalize(at - eye, eps=1e-5)\n    x_axis = F.normalize(torch.cross(up, z_axis), eps=1e-5)\n    y_axis = F.normalize(torch.cross(z_axis, x_axis), eps=1e-5)\n\n    # create rotation matrix: [bs, 3, 3]\n    r = torch.cat((x_axis[:, None, :], y_axis[:, None, :], z_axis[:, None, :]), dim=1)\n\n    # apply\n    # [bs, nv, 3] -> [bs, nv, 3] -> [bs, nv, 3]\n    if vertices.shape != eye.shape:\n        eye = eye[:, None, :]\n    vertices = vertices - eye\n    vertices = torch.matmul(vertices, r.transpose(1,2))\n\n    return vertices\n'"
thirdparty/neural_renderer/neural_renderer/mesh.py,2,"b""import torch\nimport torch.nn as nn\n\nimport neural_renderer as nr\n\nclass Mesh(object):\n    '''\n    A simple class for creating and manipulating trimesh objects\n    '''\n    def __init__(self, vertices, faces, textures=None, texture_size=4):\n        '''\n        vertices, faces and textures(if not None) are expected to be Tensor objects\n        '''\n        self.vertices = vertices\n        self.faces = faces\n        self.num_vertices = self.vertices.shape[0]\n        self.num_faces = self.faces.shape[0]\n\n        # create textures\n        if textures is None:\n            shape = (self.num_faces, texture_size, texture_size, texture_size, 3)\n            self.textures = nn.Parameter(0.05*torch.randn(*shape))\n            self.texture_size = texture_size\n        else:\n            self.texture_size = textures.shape[0]\n\n    @classmethod\n    def fromobj(cls, filename_obj, normalization=True, load_texture=False, texture_size=4):\n        '''\n        Create a Mesh object from a .obj file\n        '''\n        if load_texture:\n            vertices, faces, textures = nr.load_obj(filename_obj,\n                                                    normalization=normalization,\n                                                    texture_size=texture_size,\n                                                    load_texture=True)\n        else:\n            vertices, faces = nr.load_obj(filename_obj,\n                                          normalization=normalization,\n                                          texture_size=texture_size,\n                                          load_texture=False)\n            textures = None\n        return cls(vertices, faces, textures, texture_size)\n"""
thirdparty/neural_renderer/neural_renderer/perspective.py,3,"b""from __future__ import division\nimport math\n\nimport torch\n\ndef perspective(vertices, angle=30.):\n    '''\n    Compute perspective distortion from a given angle\n    '''\n    if (vertices.ndimension() != 3):\n        raise ValueError('vertices Tensor should have 3 dimensions')\n    device = vertices.device\n    angle = torch.tensor(angle / 180 * math.pi, dtype=torch.float32, device=device)\n    angle = angle[None]\n    width = torch.tan(angle)\n    width = width[:, None] \n    z = vertices[:, :, 2]\n    x = vertices[:, :, 0] / z / width\n    y = vertices[:, :, 1] / z / width\n    vertices = torch.stack((x,y,z), dim=2)\n    return vertices\n"""
thirdparty/neural_renderer/neural_renderer/projection.py,17,"b'from __future__ import division\n\nimport torch\nimport numpy as np\n\n\ndef batch_skew(vec, batch_size=None, device=""cpu""):\n    """"""\n    vec is N x 3, batch_size is int.\n\n    e.g. r = [rx, ry, rz]\n        skew(r) = [[ 0,    -rz,      ry],\n                   [ rz,     0,     -rx],\n                   [-ry,    rx,       0]]\n\n    returns N x 3 x 3. Skew_sym version of each matrix.\n    """"""\n\n    if batch_size is None:\n        batch_size = vec.shape[0]\n\n    col_inds = np.array([1, 2, 3, 5, 6, 7], dtype=np.int32)\n\n    indices = torch.from_numpy(np.reshape(\n        np.reshape(np.arange(0, batch_size) * 9, [-1, 1]) + col_inds,\n        newshape=(-1,))).to(device)\n\n    updates = torch.stack(\n        [\n            -vec[:, 2], vec[:, 1], vec[:, 2],\n            -vec[:, 0], -vec[:, 1], vec[:, 0]\n        ],\n        dim=1\n    ).view(-1).to(device)\n\n    res = torch.zeros(batch_size * 9, dtype=vec.dtype).to(device)\n    res[indices] = updates\n    res = res.view(batch_size, 3, 3)\n\n    return res\n\n\ndef batch_rodrigues(theta, device=""cpu""):\n    """"""\n    Theta is N x 3\n\n    rodrigues (from cv2.rodrigues):\n    source: https://docs.opencv.org/3.0-beta/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html\n    input: r (3 x 1)\n    output: R (3 x 3)\n\n        angle = norm(r)\n        r = r / angle\n\n        skew(r) = [[ 0,    -rz,      ry],\n                   [ rz,     0,     -rx],\n                   [-ry,    rx,       0]]\n\n        R = cos(theta * eye(3) + (1 - cos(theta)) * r * r.T + sin(theta) *  skew(r)\n    """"""\n    batch_size = theta.shape[0]\n\n    # angle (batch_size, 1), r (batch_size, 3)\n    angle = torch.norm(theta + 1e-8, p=2, dim=1, keepdim=True)\n    r = torch.div(theta, angle)\n\n    # angle (batch_size, 1, 1), r (batch_size, 3, 1)\n    angle = angle.unsqueeze(-1)\n    r = r.unsqueeze(-1)\n\n    cos = torch.cos(angle)\n    sin = torch.sin(angle)\n\n    # outer (batch_size, 3, 3)\n    outer = torch.matmul(r, r.permute(0, 2, 1))\n    eyes = torch.eye(3).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n\n    R = cos * eyes + (1 - cos) * outer + sin * batch_skew(r, batch_size=batch_size, device=device)\n\n    return R\n\n\ndef projection_by_params(vertices, camera_f, camera_c, rt, t, dist_coeffs, orig_size, get_image_points=False):\n    """"""\n    Calculate projective transformation of vertices given a projection matrix\n\n    Args:\n        vertices: [batch_size, N, 3]\n        camera_f: focal length of camera, [batch_size, 2]\n        camera_c: camera center, [batch_size, 2]\n        rt: rotation vector of (each-axis), [batch_size, 3]\n        t: translation vector, [batch_size, 3]\n        dist_coeffs: vector of distortion coefficients, [batch_size, 5]\n        orig_size: original size of image captured by the camera\n        get_image_points: the flag to control whether return image points or not, if True, it will return image points.\n\n    Returns:\n        projected vertices: [batch_size, N, 3]\n        image points (optional): [batch_size, N, 2]\n    """"""\n    # R is (batch_size, 3, 3)\n    R = batch_rodrigues(rt, device=vertices.device)\n\n    vertices = torch.bmm(vertices, R) + t[:, None, :]\n\n    x, y, z = vertices[:, :, 0], vertices[:, :, 1], vertices[:, :, 2]\n    x_ = x / (z + 1e-5)\n    y_ = y / (z + 1e-5)\n\n    # Get distortion coefficients from vector\n    k1 = dist_coeffs[:, None, 0]\n    k2 = dist_coeffs[:, None, 1]\n    p1 = dist_coeffs[:, None, 2]\n    p2 = dist_coeffs[:, None, 3]\n    k3 = dist_coeffs[:, None, 4]\n\n    # we use x_ for x\' and x__ for x\'\' etc.\n    r = torch.sqrt(x_ ** 2 + y_ ** 2)\n    x__ = x_*(1 + k1*(r**2) + k2*(r**4) + k3*(r**6)) + 2*p1*x_*y_ + p2*(r**2 + 2*x_**2)\n    y__ = y_*(1 + k1*(r**2) + k2*(r**4) + k3 *(r**6)) + p1*(r**2 + 2*y_**2) + 2*p2*x_*y_\n    \n    u = camera_f[:, 0] * x__ + camera_c[:, 0]\n    v = camera_f[:, 1] * y__ + camera_c[:, 1]\n\n    x__ = 2 * (u - orig_size / 2.) / orig_size\n    y__ = 2 * (v - orig_size / 2.) / orig_size\n\n    vertices = torch.stack([x__, y__, z], dim=-1)\n    if get_image_points:\n        points = torch.stack([u, v], dim=-1)\n        return vertices, points\n    else:\n        return vertices\n\n\ndef projection(vertices, P, dist_coeffs, orig_size):\n    \'\'\'\n    Calculate projective transformation of vertices given a projection matrix\n    P: 3x4 projection matrix\n    dist_coeffs: vector of distortion coefficients\n    orig_size: original size of image captured by the camera\n    \'\'\'\n    vertices = torch.cat([vertices, torch.ones_like(vertices[:, :, None, 0])], dim=-1)\n    vertices = torch.bmm(vertices, P.transpose(2,1))\n    x, y, z = vertices[:, :, 0], vertices[:, :, 1], vertices[:, :, 2]\n    x_ = x / (z + 1e-5)\n    y_ = y / (z + 1e-5)\n\n    # Get distortion coefficients from vector\n    k1 = dist_coeffs[:, None, 0]\n    k2 = dist_coeffs[:, None, 1]\n    p1 = dist_coeffs[:, None, 2]\n    p2 = dist_coeffs[:, None, 3]\n    k3 = dist_coeffs[:, None, 4]\n\n    # we use x_ for x\' and x__ for x\'\' etc.\n    r = torch.sqrt(x_ ** 2 + y_ ** 2)\n    x__ = x_*(1 + k1*(r**2) + k2*(r**4) + k3*(r**6)) + 2*p1*x_*y_ + p2*(r**2 + 2*x_**2)\n    y__ = y_*(1 + k1*(r**2) + k2*(r**4) + k3 *(r**6)) + p1*(r**2 + 2*y_**2) + 2*p2*x_*y_\n    x__ = 2 * (x__ - orig_size / 2.) / orig_size\n    y__ = 2 * (y__ - orig_size / 2.) / orig_size\n    vertices = torch.stack([x__,y__,z], dim=-1)\n    return vertices\n\n\n'"
thirdparty/neural_renderer/neural_renderer/rasterize.py,48,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\nimport neural_renderer.cuda.rasterize as rasterize_cuda\n\nDEFAULT_IMAGE_SIZE = 256\nDEFAULT_ANTI_ALIASING = True\nDEFAULT_NEAR = 0.1\nDEFAULT_FAR = 100\nDEFAULT_EPS = 1e-4\nDEFAULT_BACKGROUND_COLOR = (0, 0, 0)\n\n\nclass RasterizeFunction(Function):\n    \'\'\'\n    Definition of differentiable rasterize operation\n    Some parts of the code are implemented in CUDA\n    Currently implemented only for cuda Tensors\n    \'\'\'\n    @staticmethod\n    def forward(ctx, faces, textures, image_size, near, far, eps, background_color,\n                return_rgb=False, return_alpha=False, return_depth=False):\n        \'\'\'\n        Forward pass\n        \'\'\'\n        ctx.image_size = image_size\n        ctx.near = near\n        ctx.far = far\n        ctx.eps = eps\n        ctx.background_color = background_color\n        ctx.return_rgb = return_rgb\n        ctx.return_alpha = return_alpha\n        ctx.return_depth = return_depth\n\n        faces = faces.clone()\n\n        ctx.device = faces.device\n        ctx.batch_size, ctx.num_faces = faces.shape[:2]\n\n        if ctx.return_rgb:\n            textures = textures.contiguous()\n            ctx.texture_size = textures.shape[2]\n        else:\n            # initializing with dummy values\n            textures = torch.cuda.FloatTensor(1).fill_(0)\n            ctx.texture_size = None\n\n        face_index_map = torch.cuda.IntTensor(ctx.batch_size, ctx.image_size, ctx.image_size).fill_(-1)\n        weight_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 3).fill_(0.0)\n        depth_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size).fill_(ctx.far)\n\n        if ctx.return_rgb:\n            rgb_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 3).fill_(0)\n            sampling_index_map = torch.cuda.IntTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 8).fill_(0)\n            sampling_weight_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 8).fill_(0)\n        else:\n            rgb_map = torch.cuda.FloatTensor(1).fill_(0)\n            sampling_index_map = torch.cuda.FloatTensor(1).fill_(0)\n            sampling_weight_map = torch.cuda.FloatTensor(1).fill_(0)\n        if ctx.return_alpha:\n            alpha_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size).fill_(0)\n        else:\n            alpha_map = torch.cuda.FloatTensor(1).fill_(0)\n        if ctx.return_depth:\n            face_inv_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 3, 3).fill_(0)\n        else:\n            face_inv_map = torch.cuda.FloatTensor(1).fill_(0)\n\n        face_index_map, weight_map, depth_map, face_inv_map = \\\n            RasterizeFunction.forward_face_index_map(ctx, faces, face_index_map,\n                                                     weight_map, depth_map,\n                                                     face_inv_map)\n\n        rgb_map, sampling_index_map, sampling_weight_map = \\\n            RasterizeFunction.forward_texture_sampling(ctx, faces, textures,\n                                                       face_index_map, weight_map,\n                                                       depth_map, rgb_map,\n                                                       sampling_index_map,\n                                                       sampling_weight_map)\n\n        rgb_map = RasterizeFunction.forward_background(ctx, face_index_map, rgb_map)\n        alpha_map = RasterizeFunction.forward_alpha_map(ctx, alpha_map, face_index_map)\n\n        ctx.save_for_backward(faces, textures, face_index_map, weight_map,\n                              depth_map, rgb_map, alpha_map, face_inv_map,\n                              sampling_index_map, sampling_weight_map)\n\n        rgb_r, alpha_r, depth_r = torch.tensor([]), torch.tensor([]), torch.tensor([])\n        if ctx.return_rgb:\n            rgb_r = rgb_map\n        if ctx.return_alpha:\n            alpha_r = alpha_map.clone()\n        if ctx.return_depth:\n            depth_r = depth_map.clone()\n\n        return rgb_r, alpha_r, depth_r, face_index_map, weight_map\n\n    # TODO check is correct or not, when adding grad_face_index_map\n    @staticmethod\n    def backward(ctx, grad_rgb_map, grad_alpha_map, grad_depth_map,\n                 grad_face_index_map, grad_weight_map):\n        \'\'\'\n        Backward pass\n        \'\'\'\n        faces, textures, face_index_map, weight_map, \\\n        depth_map, rgb_map, alpha_map, face_inv_map, \\\n        sampling_index_map, sampling_weight_map = \\\n            ctx.saved_tensors\n        # initialize output buffers\n        # no need for explicit allocation of cuda.FloatTensor because zeros_like does it automatically\n        # grad_faces = torch.zeros_like(faces, dtype=torch.float32).to(ctx.device).contiguous()\n        grad_faces = torch.zeros_like(faces, dtype=torch.float32).to(ctx.device).contiguous()\n        if ctx.return_rgb:\n            grad_textures = torch.zeros_like(textures, dtype=torch.float32).to(ctx.device).contiguous()\n        else:\n            grad_textures = torch.cuda.FloatTensor(1).fill_(0.0)\n\n        # get grad_outputs\n        if ctx.return_rgb:\n            if grad_rgb_map is not None:\n                grad_rgb_map = grad_rgb_map.contiguous()\n            else:\n                grad_rgb_map = torch.zeros_like(rgb_map)\n        else:\n            grad_rgb_map = torch.cuda.FloatTensor(1).fill_(0.0)\n        if ctx.return_alpha:\n            if grad_alpha_map is not None:\n                grad_alpha_map = grad_alpha_map.contiguous()\n            else:\n                grad_alpha_map = torch.zeros_like(alpha_map)\n        else:\n            grad_alpha_map = torch.cuda.FloatTensor(1).fill_(0.0)\n        if ctx.return_depth:\n            if grad_depth_map is not None:\n                grad_depth_map = grad_depth_map.contiguous()\n            else:\n                grad_depth_map = torch.zeros_like(ctx.depth_map)\n        else:\n            grad_depth_map = torch.cuda.FloatTensor(1).fill_(0.0)\n\n        # backward pass\n        grad_faces = RasterizeFunction.backward_pixel_map(\n            ctx, faces, face_index_map, rgb_map,\n            alpha_map, grad_rgb_map, grad_alpha_map,\n            grad_faces)\n        grad_textures = RasterizeFunction.backward_textures(\n            ctx, face_index_map, sampling_weight_map,\n            sampling_index_map, grad_rgb_map, grad_textures)\n        grad_faces = RasterizeFunction.backward_depth_map(\n            ctx, faces, depth_map, face_index_map,\n            face_inv_map, weight_map, grad_depth_map,\n            grad_faces)\n\n        if not textures.requires_grad:\n            grad_textures = None\n\n        return grad_faces, grad_textures, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def forward_face_index_map(ctx, faces, face_index_map, weight_map,\n                               depth_map, face_inv_map):\n        faces_inv = torch.zeros_like(faces)\n        return rasterize_cuda.forward_face_index_map(faces, face_index_map, weight_map,\n                                                     depth_map, face_inv_map, faces_inv,\n                                                     ctx.image_size, ctx.near, ctx.far,\n                                                     ctx.return_rgb, ctx.return_alpha,\n                                                     ctx.return_depth)\n\n    @staticmethod\n    def forward_texture_sampling(ctx, faces, textures, face_index_map,\n                                 weight_map, depth_map, rgb_map,\n                                 sampling_index_map, sampling_weight_map):\n        if not ctx.return_rgb:\n            return rgb_map, sampling_index_map, sampling_weight_map\n        else:\n            return rasterize_cuda.forward_texture_sampling(faces, textures, face_index_map,\n                                                           weight_map, depth_map, rgb_map,\n                                                           sampling_index_map, sampling_weight_map,\n                                                           ctx.image_size, ctx.eps)\n\n    @staticmethod\n    def forward_alpha_map(ctx, alpha_map, face_index_map):\n        if ctx.return_alpha:\n            alpha_map[face_index_map >= 0] = 1\n        return alpha_map\n\n    @staticmethod\n    def forward_background(ctx, face_index_map, rgb_map):\n        if ctx.return_rgb:\n            background_color = torch.cuda.FloatTensor(ctx.background_color)\n            mask = (face_index_map >= 0).float()[:, :, :, None]\n            if background_color.ndimension() == 1:\n                rgb_map = rgb_map * mask + (1-mask) * background_color[None, None, None, :]\n            elif background_color.ndimension() == 2:\n                rgb_map = rgb_map * mask + (1-mask) * background_color[:, None, None, :]\n        return rgb_map\n\n    @staticmethod\n    def backward_pixel_map(ctx, faces, face_index_map, rgb_map,\n                           alpha_map, grad_rgb_map, grad_alpha_map, grad_faces):\n        if (not ctx.return_rgb) and (not ctx.return_alpha):\n            return grad_faces\n        else:\n            return rasterize_cuda.backward_pixel_map(faces, face_index_map, rgb_map,\n                                                     alpha_map, grad_rgb_map, grad_alpha_map,\n                                                     grad_faces, ctx.image_size, ctx.eps, ctx.return_rgb,\n                                                     ctx.return_alpha)\n\n    @staticmethod\n    def backward_textures(ctx, face_index_map, sampling_weight_map,\n                          sampling_index_map, grad_rgb_map, grad_textures):\n        if not ctx.return_rgb:\n            return grad_textures\n        else:\n            return rasterize_cuda.backward_textures(face_index_map, sampling_weight_map,\n                                                    sampling_index_map, grad_rgb_map,\n                                                    grad_textures, ctx.num_faces)\n\n    @staticmethod\n    def backward_depth_map(ctx, faces, depth_map, face_index_map,\n                           face_inv_map, weight_map, grad_depth_map, grad_faces):\n        if not ctx.return_depth:\n            return grad_faces\n        else:\n            return rasterize_cuda.backward_depth_map(faces, depth_map, face_index_map,\n                                                     face_inv_map, weight_map,\n                                                     grad_depth_map, grad_faces, ctx.image_size)\n\nclass Rasterize(nn.Module):\n    \'\'\'\n    Wrapper around the autograd function RasterizeFunction\n    Currently implemented only for cuda Tensors\n    \'\'\'\n    def __init__(self, image_size, near, far, eps, background_color,\n                 return_rgb=False, return_alpha=False, return_depth=False):\n        super(Rasterize, self).__init__()\n        self.image_size = image_size\n        self.image_size = image_size\n        self.near = near\n        self.far = far\n        self.eps = eps\n        self.background_color = background_color\n        self.return_rgb = return_rgb\n        self.return_alpha = return_alpha\n        self.return_depth = return_depth\n\n    def forward(self, faces, textures):\n        if faces.device == ""cpu"" or (textures is not None and textures.device == ""cpu""):\n            raise TypeError(\'Rasterize module supports only cuda Tensors\')\n        return RasterizeFunction.apply(faces, textures, self.image_size, self.near, self.far,\n                                       self.eps, self.background_color,\n                                       self.return_rgb, self.return_alpha, self.return_depth)\n\n\ndef rasterize_rgbad(\n        faces,\n        textures=None,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n        return_rgb=True,\n        return_alpha=True,\n        return_depth=True,\n        return_fim=True,\n        return_weight=True\n):\n    """"""\n    Generate RGB, alpha channel, and depth images from faces and textures (for RGB).\n\n    Args:\n        faces (torch.Tensor): Faces. The shape is [batch size, number of faces, 3 (vertices), 3 (XYZ)].\n        textures (torch.Tensor): Textures.\n            The shape is [batch size, number of faces, texture size, texture size, texture size, 3 (RGB)].\n        image_size (int): Width and height of rendered images.\n        anti_aliasing (bool): do anti-aliasing by super-sampling.\n        near (float): nearest z-coordinate to draw.\n        far (float): farthest z-coordinate to draw.\n        eps (float): small epsilon for approximated differentiation.\n        background_color (tuple): background color of RGB images.\n        return_rgb (bool): generate RGB images or not.\n        return_alpha (bool): generate alpha channels or not.\n        return_depth (bool): generate depth images or not.\n        return_fim (bool): generate face index map or not.\n        return_weight (bool): generate weight map or not\n\n    Returns:\n        dict:\n            {\n                \'rgb\': RGB images. The shape is [batch size, 3, image_size, image_size].\n                \'alpha\': Alpha channels. The shape is [batch size, image_size, image_size].\n                \'depth\': Depth images. The shape is [batch size, image_size, image_size].\n                \'face_index_map\': Face Index Map. The shape is [batch_size, image_size, image_size]\n                \'weight_map\': Weight Map. The shape is [batch_size, image_size, image_size, 3]\n            }\n\n    """"""\n    if textures is None:\n        inputs = [faces, None]\n    else:\n        inputs = [faces, textures]\n\n    if anti_aliasing:\n        # 2x super-sampling\n        # rgb, alpha, depth, face_index_map = Rasterize(\n        #     image_size * 2, near, far, eps, background_color, return_rgb, return_alpha, return_depth)(*inputs)\n        # 2x super-sampling\n        rgb, alpha, depth, face_index_map, weight_map = Rasterize(\n            image_size * 2, near, far, eps, background_color, return_rgb, return_alpha, return_depth)(*inputs)\n    else:\n        rgb, alpha, depth, face_index_map, weight_map = Rasterize(\n            image_size, near, far, eps, background_color, return_rgb, return_alpha, return_depth)(*inputs)\n\n    # transpose & vertical flip\n    if return_rgb:\n        rgb = rgb.permute((0, 3, 1, 2))\n        # pytorch does not support negative slicing for the moment\n        # may need to look at this again because it seems to be very slow\n        # rgb = rgb[:, :, ::-1, :]\n        rgb = rgb[:, :, list(reversed(range(rgb.shape[2]))), :]\n        # rgb = torch.flip(rgb, dims=(2,))\n    if return_alpha:\n        # alpha = alpha[:, ::-1, :]\n        # alpha = alpha[:, list(reversed(range(alpha.shape[1]))), :]\n        alpha = torch.flip(alpha, dims=(1,))\n    if return_depth:\n        # depth = depth[:, ::-1, :]\n        # depth = depth[:, list(reversed(range(depth.shape[1]))), :]\n        depth = torch.flip(depth, dims=(1,))\n    if return_fim:\n        face_index_map = torch.flip(face_index_map, dims=(1,))\n\n    if return_weight:\n        weight_map = torch.flip(weight_map, dims=(1,))\n\n    if anti_aliasing:\n        # 0.5x down-sampling\n        if return_rgb:\n            rgb = F.avg_pool2d(rgb, kernel_size=(2, 2))\n        if return_alpha:\n            alpha = F.avg_pool2d(alpha[:, None, :, :], kernel_size=(2, 2))[:, 0]\n        if return_depth:\n            depth = F.avg_pool2d(depth[:, None, :, :], kernel_size=(2, 2))[:, 0]\n\n\n    ret = {\n        \'rgb\': rgb if return_rgb else None,\n        \'alpha\': alpha if return_alpha else None,\n        \'depth\': depth if return_depth else None,\n        \'face_index_map\': face_index_map if return_fim else None,\n        \'weight_map\': weight_map if return_fim else None\n    }\n\n    return ret\n\n\ndef rasterize(\n        faces,\n        textures,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n):\n    """"""\n    Generate RGB images from faces and textures.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        textures: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n        background_color: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: RGB images. The shape is [batch size, 3, image_size, image_size].\n\n    """"""\n\n    return rasterize_rgbad(\n        faces, textures, image_size, anti_aliasing, near, far, eps, background_color, True, False, False, False)[\'rgb\']\n\n\ndef rasterize_rgb_and_face_index_map(\n        faces,\n        textures,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n):\n    """"""\n    Generate RGB images from faces and textures.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        textures: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n        background_color: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: RGB images. The shape is [batch size, 3, image_size, image_size].\n        ~torch.Tensor: face index map\n\n    """"""\n\n    res = rasterize_rgbad(faces, textures, image_size, anti_aliasing, near, far, eps,\n                          background_color, True, False, False, True)\n\n    return res[\'rgb\'], res[\'face_index_map\']\n\n\ndef rasterize_silhouettes(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate alpha channels from faces.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: Alpha channels. The shape is [batch size, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(faces, None, image_size, anti_aliasing,\n                           near, far, eps, None, False, True, False, False)[\'alpha\']\n\n\ndef rasterize_depth(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate depth images from faces.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: Depth images. The shape is [batch size, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(faces, None, image_size, anti_aliasing,\n                           near, far, eps, None, False, False, True, False)[\'depth\']\n\n\ndef rasterize_face_index_map(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate RGB images from faces and textures.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.rasterize_face_index_map\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: RGB images. The shape is [batch size, 3, image_size, image_size].\n        ~torch.Tensor: face index map\n\n    """"""\n\n    res = rasterize_rgbad(faces, None, image_size, anti_aliasing, near, far, eps, None, False, False, False, True)\n\n    return res[\'face_index_map\']\n\n\ndef rasterize_weight_map(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate RGB images from faces and textures.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.rasterize_face_index_map\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: RGB images. The shape is [batch size, 3, image_size, image_size].\n        ~torch.Tensor: face index map\n\n    """"""\n\n    res = rasterize_rgbad(faces, None, image_size, anti_aliasing, near, far, eps, None,\n                          False, False, False, return_fim=False, return_weight=True)\n\n    return res[\'weight_map\']\n\n\ndef rasterize_face_index_map_and_weight_map(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate RGB images from faces and textures.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.rasterize_face_index_map\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: RGB images. The shape is [batch size, 3, image_size, image_size].\n        ~torch.Tensor: face index map\n\n    """"""\n\n    res = rasterize_rgbad(faces, None, image_size, anti_aliasing, near, far, eps, None,\n                          False, False, False, return_fim=True, return_weight=True)\n\n    return res[\'face_index_map\'], res[\'weight_map\']\n\n'"
thirdparty/neural_renderer/neural_renderer/rasterize_test.py,37,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\nimport neural_renderer.cuda.rasterize as rasterize_cuda\n\nDEFAULT_IMAGE_SIZE = 256\nDEFAULT_ANTI_ALIASING = True\nDEFAULT_NEAR = 0.1\nDEFAULT_FAR = 100\nDEFAULT_EPS = 1e-4\nDEFAULT_BACKGROUND_COLOR = (0, 0, 0)\n\nclass RasterizeFunction(Function):\n    \'\'\'\n    Definition of differentiable rasterize operation\n    Some parts of the code are implemented in CUDA\n    Currently implemented only for cuda Tensors\n    \'\'\'\n    @staticmethod\n    def forward(ctx, faces, textures, image_size, near, far, eps, background_color,\n                return_rgb=False, return_alpha=False, return_depth=False):\n        \'\'\'\n        Forward pass\n        \'\'\'\n        ctx.image_size = image_size\n        ctx.near = near\n        ctx.far = far\n        ctx.eps = eps\n        ctx.background_color = background_color\n        ctx.return_rgb = return_rgb\n        ctx.return_alpha = return_alpha\n        ctx.return_depth = return_depth\n\n        faces = faces.clone()\n\n        ctx.device = faces.device\n        ctx.batch_size, ctx.num_faces = faces.shape[:2]\n\n        if ctx.return_rgb:\n            textures = textures.contiguous()\n            ctx.texture_size = textures.shape[2]\n        else:\n            # initializing with dummy values\n            textures = torch.cuda.FloatTensor(1).fill_(0)\n            ctx.texture_size = None\n\n        face_index_map = torch.cuda.IntTensor(ctx.batch_size, ctx.image_size, ctx.image_size).fill_(-1)\n        weight_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 3).fill_(0.0)\n        depth_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size).fill_(ctx.far)\n\n        if ctx.return_rgb:\n            rgb_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 3).fill_(0)\n            sampling_index_map = torch.cuda.IntTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 8).fill_(0)\n            sampling_weight_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 8).fill_(0)\n        else:\n            rgb_map = torch.cuda.FloatTensor(1).fill_(0)\n            sampling_index_map = torch.cuda.FloatTensor(1).fill_(0)\n            sampling_weight_map = torch.cuda.FloatTensor(1).fill_(0)\n        if ctx.return_alpha:\n            alpha_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size).fill_(0)\n        else:\n            alpha_map = torch.cuda.FloatTensor(1).fill_(0)\n        if ctx.return_depth:\n            face_inv_map = torch.cuda.FloatTensor(ctx.batch_size, ctx.image_size, ctx.image_size, 3, 3).fill_(0)\n        else:\n            face_inv_map = torch.cuda.FloatTensor(1).fill_(0)\n\n\n        face_index_map, weight_map, depth_map, face_inv_map =\\\n            RasterizeFunction.forward_face_index_map(ctx, faces, face_index_map,\n                                                     weight_map, depth_map,\n                                                     face_inv_map)\n\n        rgb_map, sampling_index_map, sampling_weight_map =\\\n                RasterizeFunction.forward_texture_sampling(ctx, faces, textures,\n                                                           face_index_map, weight_map,\n                                                           depth_map, rgb_map,\n                                                           sampling_index_map,\n                                                           sampling_weight_map)\n\n        rgb_map = RasterizeFunction.forward_background(ctx, face_index_map, rgb_map)\n        alpha_map = RasterizeFunction.forward_alpha_map(ctx, alpha_map, face_index_map)\n\n        ctx.save_for_backward(faces, textures, face_index_map, weight_map,\n                              depth_map, rgb_map, alpha_map, face_inv_map,\n                              sampling_index_map, sampling_weight_map)\n\n\n        rgb_r, alpha_r, depth_r = torch.tensor([]), torch.tensor([]), torch.tensor([])\n        if ctx.return_rgb:\n            rgb_r = rgb_map\n        if ctx.return_alpha:\n            alpha_r = alpha_map.clone()\n        if ctx.return_depth:\n            depth_r = depth_map.clone()\n\n        return rgb_r, alpha_r, depth_r, face_index_map\n\n    @staticmethod\n    def backward(ctx, grad_rgb_map, grad_alpha_map, grad_depth_map, grad_face_index_map):\n        \'\'\'\n        Backward pass\n        \'\'\'\n        faces, textures, face_index_map, weight_map,\\\n        depth_map, rgb_map, alpha_map, face_inv_map,\\\n        sampling_index_map, sampling_weight_map = \\\n                ctx.saved_tensors\n        # initialize output buffers\n        # no need for explicit allocation of cuda.FloatTensor because zeros_like does it automatically\n        # grad_faces = torch.zeros_like(faces, dtype=torch.float32).to(ctx.device).contiguous()\n        grad_faces = torch.zeros_like(faces, dtype=torch.float32).to(ctx.device).contiguous()\n        if ctx.return_rgb:\n            grad_textures = torch.zeros_like(textures, dtype=torch.float32).to(ctx.device).contiguous()\n        else:\n            grad_textures = torch.cuda.FloatTensor(1).fill_(0.0)\n\n        # get grad_outputs\n        if ctx.return_rgb:\n            if grad_rgb_map is not None:\n                grad_rgb_map = grad_rgb_map.contiguous()\n            else:\n                grad_rgb_map = torch.zeros_like(rgb_map)\n        else:\n            grad_rgb_map = torch.cuda.FloatTensor(1).fill_(0.0)\n        if ctx.return_alpha:\n            if grad_alpha_map is not None:\n                grad_alpha_map = grad_alpha_map.contiguous()\n            else:\n                grad_alpha_map = torch.zeros_like(alpha_map)\n        else:\n            grad_alpha_map = torch.cuda.FloatTensor(1).fill_(0.0)\n        if ctx.return_depth:\n            if grad_depth_map is not None:\n                grad_depth_map = grad_depth_map.contiguous()\n            else:\n                grad_depth_map = torch.zeros_like(ctx.depth_map)\n        else:\n            grad_depth_map = torch.cuda.FloatTensor(1).fill_(0.0)\n\n        # backward pass\n        grad_faces = RasterizeFunction.backward_pixel_map(\n                                        ctx, faces, face_index_map, rgb_map,\n                                        alpha_map, grad_rgb_map, grad_alpha_map,\n                                        grad_faces)\n        grad_textures = RasterizeFunction.backward_textures(\n                                        ctx, face_index_map, sampling_weight_map,\n                                        sampling_index_map, grad_rgb_map, grad_textures)\n        grad_faces = RasterizeFunction.backward_depth_map(\n                                        ctx, faces, depth_map, face_index_map,\n                                        face_inv_map, weight_map, grad_depth_map,\n                                        grad_faces)\n\n        if not textures.requires_grad:\n            grad_textures = None\n\n        return grad_faces, grad_textures, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def forward_face_index_map(ctx, faces, face_index_map, weight_map,\n                               depth_map, face_inv_map):\n        faces_inv = torch.zeros_like(faces)\n        return rasterize_cuda.forward_face_index_map(faces, face_index_map, weight_map,\n                                        depth_map, face_inv_map, faces_inv,\n                                        ctx.image_size, ctx.near, ctx.far,\n                                        ctx.return_rgb, ctx.return_alpha,\n                                        ctx.return_depth)\n\n    @staticmethod\n    def forward_texture_sampling(ctx, faces, textures, face_index_map,\n                                 weight_map, depth_map, rgb_map,\n                                 sampling_index_map, sampling_weight_map):\n        if not ctx.return_rgb:\n            return rgb_map, sampling_index_map, sampling_weight_map\n        else:\n            return rasterize_cuda.forward_texture_sampling(faces, textures, face_index_map,\n                                           weight_map, depth_map, rgb_map,\n                                           sampling_index_map, sampling_weight_map,\n                                           ctx.image_size, ctx.eps)\n\n    @staticmethod\n    def forward_alpha_map(ctx, alpha_map, face_index_map):\n        if ctx.return_alpha:\n            alpha_map[face_index_map >= 0] = 1\n        return alpha_map\n\n    @staticmethod\n    def forward_background(ctx, face_index_map, rgb_map):\n        if ctx.return_rgb:\n            background_color = torch.cuda.FloatTensor(ctx.background_color)\n            mask = (face_index_map >= 0).float()[:, :, :, None]\n            if background_color.ndimension() == 1:\n                rgb_map = rgb_map * mask + (1-mask) * background_color[None, None, None, :]\n            elif background_color.ndimension() == 2:\n                rgb_map = rgb_map * mask + (1-mask) * background_color[:, None, None, :]\n        return rgb_map\n\n    @staticmethod\n    def backward_pixel_map(ctx, faces, face_index_map, rgb_map,\n                           alpha_map, grad_rgb_map, grad_alpha_map, grad_faces):\n        if (not ctx.return_rgb) and (not ctx.return_alpha):\n            return grad_faces\n        else:\n            return rasterize_cuda.backward_pixel_map(faces, face_index_map, rgb_map,\n                                     alpha_map, grad_rgb_map, grad_alpha_map,\n                                     grad_faces, ctx.image_size, ctx.eps, ctx.return_rgb,\n                                     ctx.return_alpha)\n\n    @staticmethod\n    def backward_textures(ctx, face_index_map, sampling_weight_map,\n                          sampling_index_map, grad_rgb_map, grad_textures):\n        if not ctx.return_rgb:\n            return grad_textures\n        else:\n            return rasterize_cuda.backward_textures(face_index_map, sampling_weight_map,\n                                                    sampling_index_map, grad_rgb_map,\n                                                    grad_textures, ctx.num_faces)\n\n    @staticmethod\n    def backward_depth_map(ctx, faces, depth_map, face_index_map,\n                           face_inv_map, weight_map, grad_depth_map, grad_faces):\n        if not ctx.return_depth:\n            return grad_faces\n        else:\n            return rasterize_cuda.backward_depth_map(faces, depth_map, face_index_map,\n                                     face_inv_map, weight_map,\n                                     grad_depth_map, grad_faces, ctx.image_size)\n\nclass Rasterize(nn.Module):\n    \'\'\'\n    Wrapper around the autograd function RasterizeFunction\n    Currently implemented only for cuda Tensors\n    \'\'\'\n    def __init__(self, image_size, near, far, eps, background_color,\n                 return_rgb=False, return_alpha=False, return_depth=False):\n        super(Rasterize, self).__init__()\n        self.image_size = image_size\n        self.image_size = image_size\n        self.near = near\n        self.far = far\n        self.eps = eps\n        self.background_color = background_color\n        self.return_rgb = return_rgb\n        self.return_alpha = return_alpha\n        self.return_depth = return_depth\n\n    def forward(self, faces, textures):\n        if faces.device == ""cpu"" or (textures is not None and textures.device == ""cpu""):\n            raise TypeError(\'Rasterize module supports only cuda Tensors\')\n        return RasterizeFunction.apply(faces, textures, self.image_size, self.near, self.far,\n                                       self.eps, self.background_color,\n                                       self.return_rgb, self.return_alpha, self.return_depth)\n\ndef rasterize_rgbad(\n        faces,\n        textures=None,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n        return_rgb=True,\n        return_alpha=True,\n        return_depth=True,\n):\n    """"""\n    Generate RGB, alpha channel, and depth images from faces and textures (for RGB).\n\n    Args:\n        faces (torch.Tensor): Faces. The shape is [batch size, number of faces, 3 (vertices), 3 (XYZ)].\n        textures (torch.Tensor): Textures.\n            The shape is [batch size, number of faces, texture size, texture size, texture size, 3 (RGB)].\n        image_size (int): Width and height of rendered images.\n        anti_aliasing (bool): do anti-aliasing by super-sampling.\n        near (float): nearest z-coordinate to draw.\n        far (float): farthest z-coordinate to draw.\n        eps (float): small epsilon for approximated differentiation.\n        background_color (tuple): background color of RGB images.\n        return_rgb (bool): generate RGB images or not.\n        return_alpha (bool): generate alpha channels or not.\n        return_depth (bool): generate depth images or not.\n\n    Returns:\n        dict:\n            {\n                \'rgb\': RGB images. The shape is [batch size, 3, image_size, image_size].\n                \'alpha\': Alpha channels. The shape is [batch size, image_size, image_size].\n                \'depth\': Depth images. The shape is [batch size, image_size, image_size].\n            }\n\n    """"""\n    if textures is None:\n        inputs = [faces, None]\n    else:\n        inputs = [faces, textures]\n\n    if anti_aliasing:\n        # 2x super-sampling\n        rgb, alpha, depth, face_index_map = Rasterize(\n            image_size * 2, near, far, eps, background_color, return_rgb, return_alpha, return_depth)(*inputs)\n    else:\n        rgb, alpha, depth, face_index_map = Rasterize(\n            image_size, near, far, eps, background_color, return_rgb, return_alpha, return_depth)(*inputs)\n\n    # transpose & vertical flip\n    if return_rgb:\n        rgb = rgb.permute((0, 3, 1, 2))\n        # pytorch does not support negative slicing for the moment\n        # may need to look at this again because it seems to be very slow\n        # rgb = rgb[:, :, ::-1, :]\n        rgb = rgb[:, :, list(reversed(range(rgb.shape[2]))), :]\n    if return_alpha:\n        # alpha = alpha[:, ::-1, :]\n        alpha = alpha[:, list(reversed(range(alpha.shape[1]))), :]\n    if return_depth:\n        # depth = depth[:, ::-1, :]\n        depth = depth[:, list(reversed(range(depth.shape[1]))), :]\n\n    if anti_aliasing:\n        # 0.5x down-sampling\n        if return_rgb:\n            rgb = F.avg_pool2d(rgb, kernel_size=(2,2))\n        if return_alpha:\n            alpha = F.avg_pool2d(alpha[:, None, :, :], kernel_size=(2, 2))[:, 0]\n        if return_depth:\n            depth = F.avg_pool2d(depth[:, None, :, :], kernel_size=(2, 2))[:, 0]\n\n    ret = {\n        \'rgb\': rgb if return_rgb else None,\n        \'alpha\': alpha if return_alpha else None,\n        \'depth\': depth if return_depth else None,\n        \'face_index_map\':face_index_map\n    }\n\n    return ret\n\n\ndef rasterize(\n        faces,\n        textures,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n):\n    """"""\n    Generate RGB images from faces and textures.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        textures: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n        background_color: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: RGB images. The shape is [batch size, 3, image_size, image_size].\n\n    """"""\n\n    return rasterize_rgbad(\n        faces, textures, image_size, anti_aliasing, near, far, eps, background_color, True, False, False)[\'rgb\']\n\n\ndef rasterize_rgb_and_face_index_map(\n        faces,\n        textures,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n):\n    """"""\n    Generate RGB images from faces and textures.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        textures: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n        background_color: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: RGB images. The shape is [batch size, 3, image_size, image_size].\n        ~torch.Tensor: face index map\n\n    """"""\n\n    res = rasterize_rgbad(faces, textures, image_size, anti_aliasing, near, far, eps, background_color, True, False, False)\n\n    return res[\'rgb\'], res[\'face_index_map\']\n\n\ndef rasterize_silhouettes(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate alpha channels from faces.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: Alpha channels. The shape is [batch size, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(faces, None, image_size, anti_aliasing, near, far, eps, None, False, True, False)[\'alpha\']\n\n\ndef rasterize_depth(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate depth images from faces.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: Depth images. The shape is [batch size, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(faces, None, image_size, anti_aliasing, near, far, eps, None, False, False, True)[\'depth\']\n'"
thirdparty/neural_renderer/neural_renderer/renderer.py,14,"b'from __future__ import division\nimport math\n\nimport torch\nimport torch.nn as nn\nimport numpy\n\nimport neural_renderer as nr\n\n\nclass Renderer(nn.Module):\n    def __init__(self, image_size=256, anti_aliasing=True, background_color=[0, 0, 0],\n                 fill_back=True, camera_mode=\'projection\',\n                 P=None, dist_coeffs=None, orig_size=1024,\n                 perspective=True, viewing_angle=30, camera_direction=[0, 0, 1],\n                 near=0.1, far=100,\n                 light_intensity_ambient=0.5, light_intensity_directional=0.5,\n                 light_color_ambient=[1, 1, 1], light_color_directional=[1, 1, 1],\n                 light_direction=[0, 1, 0]):\n        super(Renderer, self).__init__()\n        # rendering\n        self.image_size = image_size\n        self.anti_aliasing = anti_aliasing\n        self.background_color = background_color\n        self.fill_back = fill_back\n\n        # camera\n        self.camera_mode = camera_mode\n        if self.camera_mode == \'projection\':\n            self.P = P\n            if isinstance(self.P, numpy.ndarray):\n                self.P = torch.from_numpy(self.P).cuda()\n            if self.P is None or P.ndimension() != 3 or self.P.shape[1] != 3 or self.P.shape[2] != 4:\n                raise ValueError(\'You need to provide a valid (batch_size)x3x4 projection matrix\')\n            self.dist_coeffs = dist_coeffs\n            if dist_coeffs is None:\n                self.dist_coeffs = torch.cuda.FloatTensor([[0., 0., 0., 0., 0.]]).repeat(P.shape[0], 1)\n            self.orig_size = orig_size\n        elif self.camera_mode in [\'look\', \'look_at\']:\n            self.perspective = perspective\n            self.viewing_angle = viewing_angle\n            self.eye = [0, 0, -(1. / math.tan(math.radians(self.viewing_angle)) + 1)]\n            self.camera_direction = [0, 0, 1]\n        else:\n            raise ValueError(\'Camera mode has to be one of projection, look or look_at\')\n\n        self.near = near\n        self.far = far\n\n        # light\n        self.light_intensity_ambient = light_intensity_ambient\n        self.light_intensity_directional = light_intensity_directional\n        self.light_color_ambient = light_color_ambient\n        self.light_color_directional = light_color_directional\n        self.light_direction = light_direction\n\n        # rasterization\n        self.rasterizer_eps = 1e-3\n\n    def forward(self, vertices, faces, textures=None, mode=None):\n        \'\'\'\n        Implementation of forward rendering method\n        The old API is preserved for back-compatibility with the Chainer implementation\n        \'\'\'\n\n        if mode is None:\n            return self.render(vertices, faces, textures)\n        elif mode == \'silhouettes\':\n            return self.render_silhouettes(vertices, faces)\n        elif mode == \'depth\':\n            return self.render_depth(vertices, faces)\n        else:\n            raise ValueError(""mode should be one of None, \'silhouettes\' or \'depth\'"")\n\n    def render_silhouettes(self, vertices, faces):\n        # fill back\n        if self.fill_back:\n            faces = torch.cat((faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1)\n\n        # viewpoint transformation\n        if self.camera_mode == \'look_at\':\n            vertices = nr.look_at(vertices, self.eye)\n            # perspective transformation\n            if self.perspective:\n                vertices = nr.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'look\':\n            vertices = nr.look(vertices, self.eye, self.camera_direction)\n            # perspective transformation\n            if self.perspective:\n                vertices = nr.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'projection\':\n            vertices = nr.projection(vertices, self.P, self.dist_coeffs, self.orig_size)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        images = nr.rasterize_silhouettes(faces, self.image_size, self.anti_aliasing)\n        return images\n\n    def render_depth(self, vertices, faces):\n        # fill back\n        if self.fill_back:\n            faces = torch.cat((faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n\n        # viewpoint transformation\n        if self.camera_mode == \'look_at\':\n            vertices = nr.look_at(vertices, self.eye)\n            # perspective transformation\n            if self.perspective:\n                vertices = nr.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'look\':\n            vertices = nr.look(vertices, self.eye, self.camera_direction)\n            # perspective transformation\n            if self.perspective:\n                vertices = nr.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'projection\':\n            vertices = nr.projection(vertices, self.P, self.dist_coeffs, self.orig_size)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        images = nr.rasterize_depth(faces, self.image_size, self.anti_aliasing)\n        return images\n\n    def render(self, vertices, faces, textures):\n        # fill back\n        if self.fill_back:\n            faces = torch.cat((faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n            textures = torch.cat((textures, textures.permute((0, 1, 4, 3, 2, 5))), dim=1)\n\n        # lighting\n        faces_lighting = nr.vertices_to_faces(vertices, faces)\n        textures = nr.lighting(\n            faces_lighting,\n            textures,\n            self.light_intensity_ambient,\n            self.light_intensity_directional,\n            self.light_color_ambient,\n            self.light_color_directional,\n            self.light_direction)\n\n        # viewpoint transformation\n        if self.camera_mode == \'look_at\':\n            vertices = nr.look_at(vertices, self.eye)\n            # perspective transformation\n            if self.perspective:\n                vertices = nr.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'look\':\n            vertices = nr.look(vertices, self.eye, self.camera_direction)\n            # perspective transformation\n            if self.perspective:\n                vertices = nr.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'projection\':\n            vertices = nr.projection(vertices, self.P, self.dist_coeffs, self.orig_size)\n\n        # rasterization\n        faces = nr.vertices_to_faces(vertices, faces)\n        images = nr.rasterize(\n            faces, textures, self.image_size, self.anti_aliasing, self.near, self.far, self.rasterizer_eps,\n            self.background_color)\n        return images\n\n\n# class Renderer(nn.Module):\n#     def __init__(self, image_size=256, anti_aliasing=True, background_color=[0,0,0],\n#                  fill_back=True, camera_mode=\'projection\',\n#                  camera_f=None, camera_c=None, camera_rt=None, camera_t=None,\n#                  P=None, dist_coeffs=None, orig_size=1024,\n#                  perspective=True, viewing_angle=30, camera_direction=[0,0,1],\n#                  near=0.1, far=100,\n#                  light_intensity_ambient=0.5, light_intensity_directional=0.5,\n#                  light_color_ambient=[1,1,1], light_color_directional=[1,1,1],\n#                  light_direction=[0,1,0]):\n#         super(Renderer, self).__init__()\n#         # rendering\n#         self.image_size = image_size\n#         self.anti_aliasing = anti_aliasing\n#         self.background_color = background_color\n#         self.fill_back = fill_back\n#\n#         # camera\n#         self.camera_mode = camera_mode\n#         if self.camera_mode == \'projection\':\n#             self.P = P\n#             if isinstance(self.P, numpy.ndarray):\n#                 self.P = torch.from_numpy(self.P).cuda()\n#             if self.P is None or P.ndimension() != 3 or self.P.shape[1] != 3 or self.P.shape[2] != 4:\n#                 raise ValueError(\'You need to provide a valid (batch_size)x3x4 projection matrix\')\n#\n#             self.dist_coeffs = dist_coeffs\n#             if dist_coeffs is None:\n#                 self.dist_coeffs = torch.cuda.FloatTensor([[0., 0., 0., 0., 0.]]).repeat(P.shape[0], 1)\n#             self.orig_size = orig_size\n#         elif self.camera_mode == \'projection_by_params\':\n#             self.camera_f = camera_f\n#             self.camera_c = camera_c\n#             self.camera_rt = camera_rt\n#             self.camera_t = camera_t\n#\n#             self.dist_coeffs = dist_coeffs\n#             if dist_coeffs is None:\n#                 self.dist_coeffs = torch.cuda.FloatTensor([[0., 0., 0., 0., 0.]]).repeat(camera_f.shape[0], 1)\n#             self.orig_size = orig_size\n#         elif self.camera_mode in [\'look\', \'look_at\']:\n#             self.perspective = perspective\n#             self.viewing_angle = viewing_angle\n#             self.eye = [0, 0, -(1. / math.tan(math.radians(self.viewing_angle)) + 1)]\n#             self.camera_direction = [0, 0, 1]\n#         else:\n#             raise ValueError(\'Camera mode has to be one of projection, look or look_at\')\n#\n#         self.near = near\n#         self.far = far\n#\n#         # light\n#         self.light_intensity_ambient = light_intensity_ambient\n#         self.light_intensity_directional = light_intensity_directional\n#         self.light_color_ambient = light_color_ambient\n#         self.light_color_directional = light_color_directional\n#         self.light_direction = light_direction\n#\n#         # rasterization\n#         self.rasterizer_eps = 1e-3\n#\n#     def forward(self, vertices, faces, textures=None, mode=None):\n#         \'\'\'\n#         Implementation of forward rendering method\n#         The old API is preserved for back-compatibility with the Chainer implementation\n#         \'\'\'\n#\n#         if mode is None:\n#             return self.render(vertices, faces, textures)\n#         elif mode == \'silhouettes\':\n#             return self.render_silhouettes(vertices, faces)\n#         elif mode == \'depth\':\n#             return self.render_depth(vertices, faces)\n#         else:\n#             raise ValueError(""mode should be one of None, \'silhouettes\' or \'depth\'"")\n#\n#     def render_silhouettes(self, vertices, faces):\n#         # fill back\n#         if self.fill_back:\n#             faces = torch.cat((faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1)\n#\n#         # viewpoint transformation\n#         if self.camera_mode == \'look_at\':\n#             vertices = nr.look_at(vertices, self.eye)\n#             # perspective transformation\n#             if self.perspective:\n#                 vertices = nr.perspective(vertices, angle=self.viewing_angle)\n#         elif self.camera_mode == \'look\':\n#             vertices = nr.look(vertices, self.eye, self.camera_direction)\n#             # perspective transformation\n#             if self.perspective:\n#                 vertices = nr.perspective(vertices, angle=self.viewing_angle)\n#         elif self.camera_mode == \'projection\':\n#             vertices = nr.projection(vertices, self.P, self.dist_coeffs, self.orig_size)\n#         elif self.camera_mode == \'projection_by_params\':\n#             vertices = nr.projection_by_params(vertices,\n#                                                self.camera_f, self.camera_c,\n#                                                self.camera_rt, self.camera_t,\n#                                                self.dist_coeffs, self.orig_size)\n#\n#         # rasterization\n#         faces = nr.vertices_to_faces(vertices, faces)\n#         images = nr.rasterize_silhouettes(faces, self.image_size, self.anti_aliasing)\n#         return images\n#\n#     def render_depth(self, vertices, faces):\n#         # fill back\n#         if self.fill_back:\n#             faces = torch.cat((faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n#\n#         # viewpoint transformation\n#         if self.camera_mode == \'look_at\':\n#             vertices = nr.look_at(vertices, self.eye)\n#             # perspective transformation\n#             if self.perspective:\n#                 vertices = nr.perspective(vertices, angle=self.viewing_angle)\n#         elif self.camera_mode == \'look\':\n#             vertices = nr.look(vertices, self.eye, self.camera_direction)\n#             # perspective transformation\n#             if self.perspective:\n#                 vertices = nr.perspective(vertices, angle=self.viewing_angle)\n#         elif self.camera_mode == \'projection\':\n#             vertices = nr.projection(vertices, self.P, self.dist_coeffs, self.orig_size)\n#         elif self.camera_mode == \'projection_by_params\':\n#             vertices = nr.projection_by_params(vertices,\n#                                                self.camera_f, self.camera_c,\n#                                                self.camera_rt, self.camera_t,\n#                                                self.dist_coeffs, self.orig_size)\n#\n#         # rasterization\n#         faces = nr.vertices_to_faces(vertices, faces)\n#         images = nr.rasterize_depth(faces, self.image_size, self.anti_aliasing)\n#         return images\n#\n#     def render(self, vertices, faces, textures):\n#         # fill back\n#         if self.fill_back:\n#             faces = torch.cat((faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n#             textures = torch.cat((textures, textures.permute((0, 1, 4, 3, 2, 5))), dim=1)\n#\n#         # lighting\n#         faces_lighting = nr.vertices_to_faces(vertices, faces)\n#         textures = nr.lighting(\n#             faces_lighting,\n#             textures,\n#             self.light_intensity_ambient,\n#             self.light_intensity_directional,\n#             self.light_color_ambient,\n#             self.light_color_directional,\n#             self.light_direction)\n#\n#         # viewpoint transformation\n#         if self.camera_mode == \'look_at\':\n#             vertices = nr.look_at(vertices, self.eye)\n#             # perspective transformation\n#             if self.perspective:\n#                 vertices = nr.perspective(vertices, angle=self.viewing_angle)\n#         elif self.camera_mode == \'look\':\n#             vertices = nr.look(vertices, self.eye, self.camera_direction)\n#             # perspective transformation\n#             if self.perspective:\n#                 vertices = nr.perspective(vertices, angle=self.viewing_angle)\n#         elif self.camera_mode == \'projection\':\n#             vertices = nr.projection(vertices, self.P, self.dist_coeffs, self.orig_size)\n#         elif self.camera_mode == \'projection_by_params\':\n#             vertices = nr.projection_by_params(vertices,\n#                                                self.camera_f, self.camera_c,\n#                                                self.camera_rt, self.camera_t,\n#                                                self.dist_coeffs, self.orig_size)\n#\n#         # rasterization\n#         faces = nr.vertices_to_faces(vertices, faces)\n#         images = nr.rasterize(\n#             faces, textures, self.image_size, self.anti_aliasing, self.near, self.far, self.rasterizer_eps,\n#             self.background_color)\n#         return images\n'"
thirdparty/neural_renderer/neural_renderer/save_obj.py,4,"b""from __future__ import division\nimport os\n\nimport torch\nimport cv2\nimport numpy as np\n\n\nimport neural_renderer.cuda.create_texture_image as create_texture_image_cuda\n\n\ndef create_texture_image(textures, texture_size_out=16):\n    num_faces, texture_size_in = textures.shape[:2]\n    tile_width = int((num_faces - 1.) ** 0.5) + 1\n    tile_height = int((num_faces - 1.) / tile_width) + 1\n    image = torch.zeros(tile_height * texture_size_out, tile_width * texture_size_out, 3, dtype=torch.float32)\n    vertices = torch.zeros((num_faces, 3, 2), dtype=torch.float32)  # [:, :, XY]\n    face_nums = torch.arange(num_faces)\n    column = face_nums % tile_width\n    row = face_nums / tile_width\n    vertices[:, 0, 0] = column * texture_size_out\n    vertices[:, 0, 1] = row * texture_size_out\n    vertices[:, 1, 0] = column * texture_size_out\n    vertices[:, 1, 1] = (row + 1) * texture_size_out - 1\n    vertices[:, 2, 0] = (column + 1) * texture_size_out - 1\n    vertices[:, 2, 1] = (row + 1) * texture_size_out - 1\n    image = image.cuda()\n    vertices = vertices.cuda()\n    textures = textures.cuda()\n    image = create_texture_image_cuda.create_texture_image(vertices, textures, image, 1e-5)\n    # image = torch.ones_like(image)\n\n    print(image.dtype, image.min(), image.max())\n\n    vertices[:, :, 0] /= (image.shape[1] - 1)\n    vertices[:, :, 1] /= (image.shape[0] - 1)\n\n    image = image.detach().cpu().numpy()\n    vertices = vertices.detach().cpu().numpy()\n    image = image[::-1, ::1]\n\n    return image, vertices\n\n\ndef save_obj(filename, vertices, faces, textures=None):\n    assert vertices.ndimension() == 2\n    assert faces.ndimension() == 2\n\n    if textures is not None:\n        filename_mtl = filename[:-4] + '.mtl'\n        filename_texture = filename[:-4] + '.png'\n        material_name = 'material_1'\n        texture_image, vertices_textures = create_texture_image(textures)\n        # imsave(filename_texture, texture_image)\n\n        print(texture_image.dtype, texture_image.max(), texture_image.min())\n        texture_image = (texture_image + 1) / 2.0 * 255.0\n        texture_image = texture_image.astype(np.uint8)\n        cv2.imwrite(filename_texture, texture_image)\n\n    faces = faces.detach().cpu().numpy()\n\n    with open(filename, 'w') as f:\n        f.write('# %s\\n' % os.path.basename(filename))\n        f.write('#\\n')\n        f.write('g\\n')\n\n        if textures is not None:\n            f.write('mtllib %s\\n\\n' % os.path.basename(filename_mtl))\n\n        for vertex in vertices:\n            f.write('v %.8f %.8f %.8f\\n' % (vertex[0], vertex[1], vertex[2]))\n        f.write('\\n')\n\n        if textures is not None:\n            for vertex in vertices_textures.reshape((-1, 2)):\n                f.write('vt %.8f %.8f\\n' % (vertex[0], vertex[1]))\n            f.write('\\n')\n\n            f.write('usemtl %s\\n' % material_name)\n            for i, face in enumerate(faces):\n                f.write('f %d/%d %d/%d %d/%d\\n' % (\n                    face[0] + 1, 3 * i + 1, face[1] + 1, 3 * i + 2, face[2] + 1, 3 * i + 3))\n            f.write('\\n')\n        else:\n            for face in faces:\n                f.write('f %d %d %d\\n' % (face[0] + 1, face[1] + 1, face[2] + 1))\n\n        f.write('s off\\n')\n        print('save obj to {}'.format(filename))\n\n    if textures is not None:\n        with open(filename_mtl, 'w') as f:\n            f.write('newmtl %s\\n' % material_name)\n            f.write('map_Kd %s\\n' % os.path.basename(filename_texture))"""
thirdparty/neural_renderer/neural_renderer/vertices_to_faces.py,1,"b'import torch\n\n\ndef vertices_to_faces(vertices, faces):\n    """"""\n    :param vertices: [batch size, number of vertices, 3]\n    :param faces: [batch size, number of faces, 3)\n    :return: [batch size, number of faces, 3, 3]\n    """"""\n    assert (vertices.ndimension() == 3)\n    assert (faces.ndimension() == 3)\n    assert (vertices.shape[0] == faces.shape[0])\n    assert (vertices.shape[2] == 3)\n    assert (faces.shape[2] == 3)\n\n    bs, nv = vertices.shape[:2]\n    bs, nf = faces.shape[:2]\n    device = vertices.device\n    faces = faces + (torch.arange(bs, dtype=torch.int32).to(device) * nv)[:, None, None]\n    vertices = vertices.reshape((bs * nv, 3))\n    # pytorch only supports long and byte tensors for indexing\n    return vertices[faces.long()]\n'"
thirdparty/neural_renderer/tests/__init__.py,0,b''
thirdparty/neural_renderer/tests/test_get_points_from_angles.py,0,b'# TODO\n'
thirdparty/neural_renderer/tests/test_lighting.py,2,"b'import unittest\n\nimport torch\n\nimport neural_renderer as nr\n\nclass TestLighting(unittest.TestCase):\n    \n    def test_case1(self):\n        """"""Test whether it is executable.""""""\n        faces = torch.randn(64, 16, 3, 3, dtype=torch.float32)\n        textures = torch.randn(64, 16, 8, 8, 8, 3, dtype=torch.float32)\n        nr.lighting(faces, textures)\n\nif __name__ == \'__main__\':\n    unittest.main()\n\n\n\n'"
thirdparty/neural_renderer/tests/test_load_obj.py,0,"b""import unittest\nimport os\n\nimport numpy as np\nfrom skimage.io import imsave\n\nimport neural_renderer as nr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, 'data')\n\nclass TestCore(unittest.TestCase):\n    def test_tetrahedron(self):\n        vertices_ref = np.array(\n            [\n                [1., 0., 0.],\n                [0., 1., 0.],\n                [0., 0., 1.],\n                [0., 0., 0.]],\n            'float32')\n        faces_ref = np.array(\n            [\n                [1, 3, 2],\n                [3, 1, 0],\n                [2, 0, 1],\n                [0, 2, 3]],\n            'int32')\n\n        obj_file = os.path.join(data_dir, 'tetrahedron.obj')\n        vertices, faces = nr.load_obj(obj_file, False)\n        assert (np.allclose(vertices_ref, vertices))\n        assert (np.allclose(faces_ref, faces))\n        vertices, faces = nr.load_obj(obj_file, True)\n        assert (np.allclose(vertices_ref * 2 - 1.0, vertices))\n        assert (np.allclose(faces_ref, faces))\n\n    def test_teapot(self):\n        obj_file = os.path.join(data_dir, 'teapot.obj')\n        vertices, faces = nr.load_obj(obj_file)\n        assert (faces.shape[0] == 2464)\n        assert (vertices.shape[0] == 1292)\n\n    def test_texture(self):\n        renderer = nr.Renderer(camera_mode='look_at')\n\n        vertices, faces, textures = nr.load_obj(\n            os.path.join(data_dir, '1cde62b063e14777c9152a706245d48/model.obj'), load_texture=True)\n\n        renderer.eye = nr.get_points_from_angles(2, 15, 30)\n        images = renderer.render(vertices[None, :, :], faces[None, :, :], textures[None, :, :, :, :, :]).permute(0,2,3,1).detach().cpu().numpy()\n        imsave(os.path.join(data_dir, 'car.png'), images[0])\n\n        vertices, faces, textures = nr.load_obj(\n            os.path.join(data_dir, '4e49873292196f02574b5684eaec43e9/model.obj'), load_texture=True, texture_size=16)\n        renderer.eye = nr.get_points_from_angles(2, 15, -90)\n        images = renderer.render(vertices[None, :, :], faces[None, :, :], textures[None, :, :, :, :, :]).permute(0,2,3,1).detach().cpu().numpy()\n        imsave(os.path.join(data_dir, 'display.png'), images[0])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
thirdparty/neural_renderer/tests/test_look.py,0,b'# TODO\n'
thirdparty/neural_renderer/tests/test_look_at.py,1,"b""import unittest\n\nimport torch\nimport numpy as np\n\nimport neural_renderer as nr\n\nclass TestLookAt(unittest.TestCase):\n    def test_case1(self):\n        eyes = [\n            [1, 0, 1],\n            [0, 0, -10],\n            [-1, 1, 0],\n        ]\n        answers = [\n            [-np.sqrt(2) / 2, 0, np.sqrt(2) / 2],\n            [1, 0, 10],\n            [0, np.sqrt(2) / 2, 3. / 2. * np.sqrt(2)],\n        ]\n        vertices = torch.from_numpy(np.array([1, 0, 0], np.float32))\n        vertices = vertices[None, None, :]\n        for e, a in zip(eyes, answers):\n            eye = np.array(e, np.float32)\n            transformed = nr.look_at(vertices, eye)\n            assert(np.allclose(transformed.data.squeeze().numpy(), np.array(a)))\n\nif __name__ == '__main__':\n    unittest.main()\n"""
thirdparty/neural_renderer/tests/test_perspective.py,1,"b""import unittest\n\nimport torch\nimport numpy as np\n\nimport neural_renderer as nr\n\nclass TestPerspective(unittest.TestCase):\n    def test_case1(self):\n        vertices = torch.from_numpy(np.array([1,2,10], np.float32))\n        v_out = np.array([np.sqrt(3) / 10, 2 * np.sqrt(3) / 10, 10], np.float32)\n        vertices = vertices[None, None, :]\n        transformer = nr.perspective(vertices)\n        assert(np.allclose(transformer.data.squeeze().numpy(), v_out))\n\nif __name__ == '__main__':\n    unittest.main()\n"""
thirdparty/neural_renderer/tests/test_rasterize.py,14,"b'import unittest\nimport os\n\nimport torch\nimport numpy as np\nfrom skimage.io import imread, imsave\n\nimport neural_renderer as nr\nimport utils\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\n\nclass TestRasterize(unittest.TestCase):\n    def test_forward_case1(self):\n        """"""Rendering a teapot without anti-aliasing.""""""\n\n        # load teapot\n        vertices, faces, textures = utils.load_teapot_batch()\n        vertices = vertices.cuda()\n        faces = faces.cuda()\n        textures = textures.cuda()\n\n        # create renderer\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 256\n        renderer.anti_aliasing = False\n\n        # render\n        images = renderer(vertices, faces, textures)\n        images = images.detach().cpu().numpy()\n        image = images[2]\n        image = image.transpose((1, 2, 0))\n\n        imsave(os.path.join(data_dir, \'test_rasterize1.png\'), image)\n\n    def test_forward_case2(self):\n        """"""Rendering a teapot with anti-aliasing and another viewpoint.""""""\n\n        # load teapot\n        vertices, faces, textures = utils.load_teapot_batch()\n        vertices = vertices.cuda()\n        faces = faces.cuda()\n        textures = textures.cuda()\n\n        # create renderer\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.eye = [1, 1, -2.7]\n\n        # render\n        images = renderer(vertices, faces, textures)\n        images = images.detach().cpu().numpy()\n        image = images[2]\n        image = image.transpose((1, 2, 0))\n\n        imsave(os.path.join(data_dir, \'test_rasterize2.png\'), image)\n\n    def test_forward_case3(self):\n        """"""Whether a silhouette by neural renderer matches that by Blender.""""""\n\n        # load teapot\n        vertices, faces, textures = utils.load_teapot_batch()\n        vertices = vertices.cuda()\n        faces = faces.cuda()\n        textures = textures.cuda()\n\n        # create renderer\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 256\n        renderer.anti_aliasing = False\n        renderer.light_intensity_ambient = 1.0\n        renderer.light_intensity_directional = 0.0\n\n        images = renderer(vertices, faces, textures)\n        images = images.detach().cpu().numpy()\n        image = images[2].mean(0)\n\n        # load reference image by blender\n        ref = imread(os.path.join(data_dir, \'teapot_blender.png\'))\n        ref = (ref.min(axis=-1) != 255).astype(np.float32)\n\n        assert(np.allclose(ref, image))\n\n    def test_backward_case1(self):\n        """"""Backward if non-zero gradient is out of a face.""""""\n\n        vertices = [\n            [0.8, 0.8, 1.],\n            [0.0, -0.5, 1.],\n            [0.2, -0.4, 1.]]\n        faces = [[0, 1, 2]]\n        pxi = 35\n        pyi = 25\n        grad_ref = [\n            [1.6725862, -0.26021874, 0.],\n            [1.41986704, -1.64284933, 0.],\n            [0., 0., 0.],\n        ]\n\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 64\n        renderer.anti_aliasing = False\n        renderer.perspective = False\n        renderer.light_intensity_ambient = 1.0\n        renderer.light_intensity_directional = 0.0\n\n        vertices = torch.from_numpy(np.array(vertices, dtype=np.float32)).cuda()\n        faces = torch.from_numpy(np.array(faces, dtype=np.int32)).cuda()\n        textures = torch.ones(faces.shape[0], 4, 4, 4, 3, dtype=torch.float32).cuda()\n        grad_ref = torch.from_numpy(np.array(grad_ref, dtype=np.float32)).cuda()\n        vertices, faces, textures, grad_ref = utils.to_minibatch((vertices, faces, textures, grad_ref))\n        vertices, faces, textures, grad_ref = vertices.cuda(), faces.cuda(), textures.cuda(), grad_ref.cuda()\n        vertices.requires_grad = True\n        images = renderer(vertices, faces, textures)\n        images = torch.mean(images, dim=1)\n        loss = torch.sum(torch.abs(images[:, pyi, pxi] - 1))\n        loss.backward()\n\n        assert(torch.allclose(vertices.grad, grad_ref, rtol=1e-2))\n\n    def test_backward_case2(self):\n        """"""Backward if non-zero gradient is on a face.""""""\n\n        vertices = [\n            [0.8, 0.8, 1.],\n            [-0.5, -0.8, 1.],\n            [0.8, -0.8, 1.]]\n        faces = [[0, 1, 2]]\n        pyi = 40\n        pxi = 50\n        grad_ref = [\n            [0.98646867, 1.04628897, 0.],\n            [-1.03415668, - 0.10403691, 0.],\n            [3.00094461, - 1.55173182, 0.],\n        ]\n\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 64\n        renderer.anti_aliasing = False\n        renderer.perspective = False\n        renderer.light_intensity_ambient = 1.0\n        renderer.light_intensity_directional = 0.0\n\n        vertices = torch.from_numpy(np.array(vertices, dtype=np.float32)).cuda()\n        faces = torch.from_numpy(np.array(faces, dtype=np.int32)).cuda()\n        textures = torch.ones(faces.shape[0], 4, 4, 4, 3, dtype=torch.float32).cuda()\n        grad_ref = torch.from_numpy(np.array(grad_ref, dtype=np.float32)).cuda()\n        vertices, faces, textures, grad_ref = utils.to_minibatch((vertices, faces, textures, grad_ref))\n        vertices.requires_grad=True\n\n        images = renderer(vertices, faces, textures)\n        images = torch.mean(images, dim=1)\n        loss = torch.sum(torch.abs(images[:, pyi, pxi]))\n        loss.backward()\n\n        assert(torch.allclose(vertices.grad, grad_ref, rtol=1e-2))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
thirdparty/neural_renderer/tests/test_rasterize_depth.py,4,"b'import unittest\nimport os\n\nimport torch\nimport numpy as np\nfrom skimage.io import imread\n\nimport neural_renderer as nr\nimport utils\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\n\nclass TestRasterizeDepth(unittest.TestCase):\n    def test_forward_case1(self):\n        """"""Whether a silhouette by neural renderer matches that by Blender.""""""\n\n        # load teapot\n        vertices, faces, _ = utils.load_teapot_batch()\n\n        # create renderer\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 256\n        renderer.anti_aliasing = False\n\n        images = renderer(vertices, faces, mode=\'depth\')\n        images = images.detach().cpu().numpy()\n        image = images[2]\n        image = image != image.max()\n\n        # load reference image by blender\n        ref = imread(os.path.join(data_dir, \'teapot_blender.png\'))\n        ref = (ref.min(axis=-1) != 255).astype(np.float32)\n\n        assert(np.allclose(ref, image))\n\n    def test_forward_case2(self):\n        # load teapot\n        vertices, faces, _ = utils.load_teapot_batch()\n\n        # create renderer\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 256\n        renderer.anti_aliasing = False\n\n        images = renderer(vertices, faces, mode=\'depth\')\n        images = images.detach().cpu().numpy()\n        image = images[2]\n        image[image == image.max()] = image.min()\n        image = (image - image.min()) / (image.max() - image.min())\n\n        ref = imread(os.path.join(data_dir, \'test_depth.png\')).astype(np.float32) / 255.\n\n        assert(np.allclose(image, ref, atol=1e-2))\n\n    def test_backward_case1(self):\n        vertices = [\n            [-0.9, -0.9, 2.],\n            [-0.8, 0.8, 1.],\n            [0.8, 0.8, 0.5]]\n        faces = [[0, 1, 2]]\n\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 64\n        renderer.anti_aliasing = False\n        renderer.perspective = False\n        renderer.camera_mode = \'none\'\n\n        vertices = torch.from_numpy(np.array(vertices, np.float32)).cuda()\n        faces = torch.from_numpy(np.array(faces, np.int32)).cuda()\n        vertices, faces = utils.to_minibatch((vertices, faces))\n        vertices.requires_grad = True\n\n        images = renderer(vertices, faces, mode=\'depth\')\n        loss = torch.sum((images[0, 15, 20] - 1)**2)\n        loss.backward()\n        grad = vertices.grad.clone()\n        grad2 = np.zeros_like(grad)\n\n        for i in range(3):\n            for j in range(3):\n                eps = 1e-3\n                vertices2 = vertices.clone()\n                vertices2[i, j] += eps\n                images = renderer.render_depth(vertices2, faces)\n                loss2 = torch.sum((images[0, 15, 20] - 1)**2)\n                grad2[i, j] = ((loss2 - loss) / eps).item()\n\n        assert(np.allclose(grad, grad2, atol=1e-3))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
thirdparty/neural_renderer/tests/test_rasterize_silhouettes.py,8,"b'import unittest\nimport os\n\nimport torch\nimport numpy as np\nfrom skimage.io import imread\n\nimport neural_renderer as nr\nimport utils\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\n\n\nclass TestRasterizeSilhouettes(unittest.TestCase):\n    def test_case1(self):\n        """"""Whether a silhouette by neural renderer matches that by Blender.""""""\n\n        # load teapot\n        vertices, faces, _ = utils.load_teapot_batch()\n\n        # create renderer\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 256\n        renderer.anti_aliasing = False\n\n        images = renderer(vertices, faces, mode=\'silhouettes\')\n        images = images.detach().cpu().numpy()\n        image = images[2]\n\n        # load reference image by blender\n        ref = imread(os.path.join(data_dir, \'teapot_blender.png\'))\n        ref = (ref.min(-1) != 255).astype(np.float32)\n\n        assert(np.allclose(ref, image))\n\n    def test_backward_case1(self):\n        """"""Backward if non-zero gradient is out of a face.""""""\n\n        vertices = [\n            [0.8, 0.8, 1.],\n            [0.0, -0.5, 1.],\n            [0.2, -0.4, 1.]]\n        faces = [[0, 1, 2]]\n        pxi = 35\n        pyi = 25\n        grad_ref = [\n            [1.6725862, -0.26021874, 0.],\n            [1.41986704, -1.64284933, 0.],\n            [0., 0., 0.],\n        ]\n\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 64\n        renderer.anti_aliasing = False\n        renderer.perspective = False\n\n        vertices = torch.from_numpy(np.array(vertices, np.float32)).cuda()\n        faces = torch.from_numpy(np.array(faces, np.int32)).cuda()\n        grad_ref = torch.from_numpy(np.array(grad_ref, np.float32)).cuda()\n        vertices, faces, grad_ref = utils.to_minibatch((vertices, faces, grad_ref))\n        vertices.requires_grad = True\n        images = renderer(vertices, faces, mode=\'silhouettes\')\n        loss = torch.sum(torch.abs(images[:, pyi, pxi] - 1))\n        loss.backward()\n\n        assert(np.allclose(vertices.grad, grad_ref, rtol=1e-2))\n\n    def test_backward_case2(self):\n        """"""Backward if non-zero gradient is on a face.""""""\n\n        vertices = [\n            [0.8, 0.8, 1.],\n            [-0.5, -0.8, 1.],\n            [0.8, -0.8, 1.]]\n        faces = [[0, 1, 2]]\n        pyi = 40\n        pxi = 50\n        grad_ref = [\n            [0.98646867, 1.04628897, 0.],\n            [-1.03415668, - 0.10403691, 0.],\n            [3.00094461, - 1.55173182, 0.],\n        ]\n\n        renderer = nr.Renderer(camera_mode=\'look_at\')\n        renderer.image_size = 64\n        renderer.anti_aliasing = False\n        renderer.perspective = False\n\n        vertices = torch.from_numpy(np.array(vertices, np.float32)).cuda()\n        faces = torch.from_numpy(np.array(faces, np.int32)).cuda()\n        grad_ref = torch.from_numpy(np.array(grad_ref, np.float32)).cuda()\n        vertices, faces, grad_ref = utils.to_minibatch((vertices, faces, grad_ref))\n        vertices.requires_grad = True\n        images = renderer(vertices, faces, mode=\'silhouettes\')\n        loss = torch.sum(torch.abs(images[:, pyi, pxi]))\n        loss.backward()\n\n        assert(np.allclose(vertices.grad, grad_ref, rtol=1e-2))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
thirdparty/neural_renderer/tests/test_renderer.py,0,"b""# TODO\n'''\nMight have to do some refactoring because the tests for Renderer are included in the test_rasterize* unit tests\n'''\n"""
thirdparty/neural_renderer/tests/test_save_obj.py,0,"b""import unittest\nimport os\n\nimport numpy as np\n\nimport neural_renderer as nr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, 'data')\n\nclass TestCore(unittest.TestCase):\n    def test_save_obj(self):\n        teapot = os.path.join(data_dir, 'teapot.obj')\n        teapot2 = os.path.join(data_dir, 'teapot2.obj')\n        vertices, faces = nr.load_obj(teapot)\n        nr.save_obj(teapot2, vertices, faces)\n        vertices2, faces2 = nr.load_obj(teapot2)\n        os.remove(teapot2)\n        assert np.allclose(vertices, vertices2)\n        assert np.allclose(faces, faces2)\n\n    def test_texture(self):\n        pass\n\nif __name__ == '__main__':\n    unittest.main()\n"""
thirdparty/neural_renderer/tests/test_vertices_to_faces.py,0,b'# TODO\n'
thirdparty/neural_renderer/tests/utils.py,3,"b""import os\n\nimport torch\n\nimport neural_renderer as nr\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, 'data')\n\n\ndef to_minibatch(data, batch_size=4, target_num=2):\n    ret = []\n    for d in data:\n        device = d.device\n        d2 = torch.unsqueeze(torch.zeros_like(d), 0)\n        r = [1 for _ in d2.shape]\n        r[0] = batch_size\n        d2 = torch.unsqueeze(torch.zeros_like(d), 0).repeat(*r).to(device)\n        d2[target_num] = d\n        ret.append(d2)\n    return ret\n\ndef load_teapot_batch(batch_size=4, target_num=2):\n    vertices, faces = nr.load_obj(os.path.join(data_dir, 'teapot.obj'))\n    textures = torch.ones((faces.shape[0], 4, 4, 4, 3), dtype=torch.float32)\n    vertices, faces, textures = to_minibatch((vertices, faces, textures), batch_size, target_num)\n    return vertices, faces, textures\n"""
thirdparty/his_evaluators/his_evaluators/evaluators/__init__.py,0,b''
thirdparty/his_evaluators/his_evaluators/evaluators/base.py,3,"b'import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport argparse\nfrom tqdm import tqdm\nfrom scipy.special import softmax\n\nfrom his_evaluators.utils.io import load_img\n\nfrom his_evaluators.metrics import TYPES_QUALITIES, BaseMetric, register_metrics\nfrom his_evaluators.protocols import create_dataset_protocols\n\n\ndef parse_arg():\n    parser = argparse.ArgumentParser(description=""Evaluate Motion Imitation."")\n    parser.add_argument(""--gpu"", type=str, default=""0"", help=""gpu id"")\n    parser.add_argument(""--data_dir"", type=str, help=""data directory"")\n    parser.add_argument(""--output_dir"", type=str, help=""output directory"")\n\n    args = parser.parse_args()\n\n    return args\n\n\nclass PairedEvaluationDataset(Dataset):\n    def __init__(self, pair_file_list, image_size=512):\n        self.image_size = image_size\n        self.pair_file_list = pair_file_list\n\n    def __len__(self):\n        return len(self.pair_file_list)\n\n    def __getitem__(self, item):\n        pred_file, ref_file = self.pair_file_list[item]\n\n        pred_img = load_img(pred_file, self.image_size)\n        ref_img = load_img(ref_file, self.image_size)\n\n        sample = {\n            ""pred"": pred_img,\n            ""ref"": ref_img\n        }\n\n        return sample\n\n\ndef build_data_loader(dataset, batch_size=32):\n    data_loader = DataLoader(\n        dataset=dataset, batch_size=batch_size,\n        num_workers=4, shuffle=False, drop_last=False,\n        pin_memory=True\n    )\n\n    return data_loader\n\n\nclass PairedMetricRunner(object):\n    def __init__(self,\n                 metric_types=(""ssim"", ""psnr"", ""lps""),\n                 device=torch.device(""cuda:0"")):\n\n        self.metric_types = metric_types\n        self.metric_dict = register_metrics(metric_types, device)\n\n    def build_metric_results(self, metric_types):\n        metric_results = dict()\n\n        for name in metric_types:\n            metric_results[name] = []\n\n        return metric_results\n\n    def evaluate(self, file_paths, image_size=512, batch_size=16):\n        dataset = PairedEvaluationDataset(file_paths, image_size=image_size)\n        dataloader = build_data_loader(dataset, batch_size=batch_size)\n\n        metric_results = self.build_metric_results(self.metric_types)\n        print(""running {} metrics with paired samples = {}"".format(self.metric_types, len(dataset)))\n\n        for sample in tqdm(dataloader):\n            pred_imgs = sample[""pred""]\n            ref_imgs = sample[""ref""]\n\n            for name in self.metric_types:\n                score = self.metric_dict[name].calculate_score(pred_imgs, ref_imgs)\n                metric_results[name].append(score)\n\n        return self.post_process_results(metric_results)\n\n    def post_process_results(self, metric_results):\n        for name in metric_results:\n            metric_results[name] = np.mean(metric_results[name])\n\n            print(""{} = {}, quality = {}"".format(name, metric_results[name], self.metric_dict[name].quality()))\n\n        return metric_results\n\n\nclass UnpairedMetricRunner(object):\n    def __init__(self,\n                 metric_types=(""is"", ""fid"", ""OS-CS-reID"", ""OS-freid"", ""face-CS"", ""face-FD""),\n                 device=torch.device(""cpu"")):\n\n        metric_types = set(metric_types)\n\n        if ""is"" in metric_types and ""fid"" not in metric_types:\n            metric_types.add(""fid"")\n\n        add_PCB = False\n        add_OSNET = False\n        add_FACE = False\n        for m_t in metric_types:\n            if ""PCB"" in m_t:\n                add_PCB = True\n            if ""OS"" in m_t:\n                add_OSNET = True\n            if ""face"" in m_t:\n                add_FACE = True\n\n        if add_PCB:\n            metric_types.add(""PCB-freid"")\n\n        if add_OSNET:\n            metric_types.add(""OS-freid"")\n\n        if add_FACE:\n            metric_types.add(""face-CS"")\n\n        self.metric_types = tuple(metric_types)\n        self.metric_dict = register_metrics(self.metric_types, device, has_detector=True)\n\n        self.get_is_feats = False\n        self.get_fid_feats = False\n        self.get_osnet_feats = False\n        self.get_pcb_feats = False\n        self.get_cs_reid = False\n        self.get_face_feats = False\n        self.get_face_cs = False\n        self.get_sspe = False\n\n    def build_metric_results(self, metric_types):\n        metric_results = dict()\n\n        for m_t in metric_types:\n            if m_t == ""is"":\n                self.get_is_feats = True\n                metric_results[""inception_softmax""] = []\n            elif m_t == ""fid"":\n                self.get_fid_feats = True\n                metric_results[""inception_feats""] = {\n                    ""pred"": [],\n                    ""ref"": []\n                }\n            elif ""PCB"" in m_t:\n                self.get_pcb_feats = True\n                metric_results[""pcb_feats""] = {\n                    ""pred"": [],\n                    ""ref"": [],\n                    ""CS"": []\n                }\n                if ""-CS"" in m_t:\n                    self.get_cs_reid = True\n\n            elif ""OS"" in m_t:\n                self.get_osnet_feats = True\n                metric_results[""osnet_feats""] = {\n                    ""pred"": [],\n                    ""ref"": [],\n                    ""CS"": []\n                }\n                if ""-CS"" in m_t:\n                    self.get_cs_reid = True\n\n            elif ""face"" in m_t:\n                self.get_face_feats = True\n                metric_results[""face_feats""] = {\n                    ""pred"": [],\n                    ""ref"": [],\n                    ""CS"": []\n                }\n                if ""-CS"" in m_t:\n                    self.get_face_cs = True\n\n            elif ""SSPE"" in m_t:\n                self.get_sspe = True\n                metric_results[""SSPE""] = []\n\n        return metric_results\n\n    def evaluate(self, file_paths, image_size=512, batch_size=16):\n        """"""\n        Args:\n            file_paths:\n            image_size:\n            batch_size:\n\n        Returns:\n\n        """"""\n\n        dataset = PairedEvaluationDataset(file_paths, image_size=image_size)\n        dataloader = build_data_loader(dataset, batch_size=batch_size)\n\n        metric_results = self.build_metric_results(self.metric_types)\n\n        print(""running {} metrics with unpaired samples = {}"".format(self.metric_types, len(dataset)))\n        for sample in tqdm(dataloader):\n            pred_imgs = sample[""pred""]\n            ref_imgs = sample[""ref""]\n\n            if self.get_fid_feats:\n                inception_preds = self.metric_dict[""fid""].forward(pred_imgs)\n                inception_refs = self.metric_dict[""fid""].forward(ref_imgs)\n                metric_results[""inception_feats""][""pred""].append(inception_preds)\n                metric_results[""inception_feats""][""ref""].append(inception_refs)\n\n                if self.get_is_feats:\n                    is_softmax = softmax(inception_preds, axis=1)\n                    metric_results[""inception_softmax""].append(is_softmax)\n\n            if self.get_osnet_feats:\n                osnet_preds = self.metric_dict[""OS-freid""].forward(pred_imgs)\n                osnet_refs = self.metric_dict[""OS-freid""].forward(ref_imgs)\n                metric_results[""osnet_feats""][""pred""].append(osnet_preds)\n                metric_results[""osnet_feats""][""ref""].append(osnet_refs)\n\n                if self.get_cs_reid:\n                    cs_score = self.metric_dict[""OS-freid""].cosine_similarity(osnet_preds, osnet_refs)\n                    metric_results[""osnet_feats""][""CS""].append(cs_score)\n\n            if self.get_pcb_feats:\n                pcb_preds = self.metric_dict[""PCB-freid""].forward(pred_imgs)\n                pcb_refs = self.metric_dict[""PCB-freid""].forward(ref_imgs)\n                metric_results[""pcb_feats""][""pred""].append(pcb_preds)\n                metric_results[""pcb_feats""][""ref""].append(pcb_refs)\n\n                if self.get_cs_reid:\n                    cs_score = self.metric_dict[""PCB-freid""].cosine_similarity(pcb_preds, pcb_refs)\n                    metric_results[""pcb_feats""][""CS""].append(cs_score)\n\n            if self.get_face_feats:\n                face_preds, _ = self.metric_dict[""face-CS""].forward(pred_imgs)\n                face_refs, valid_ids = self.metric_dict[""face-CS""].forward(ref_imgs)\n\n                metric_results[""face_feats""][""pred""].append(face_preds[valid_ids])\n                metric_results[""face_feats""][""ref""].append(face_refs[valid_ids])\n\n                if self.get_face_cs:\n                    cs_score = self.metric_dict[""face-CS""].cosine_similarity(face_preds, face_refs)\n                    metric_results[""face_feats""][""CS""].append(cs_score)\n\n            if self.get_sspe:\n                sspe = self.metric_dict[""SSPE""].calculate_score(pred_imgs, ref_imgs)\n                metric_results[""SSPE""].append(sspe)\n\n        results = self.post_process_results(metric_results)\n\n        return results\n\n    def template_results_dict(self):\n        results = dict()\n\n        for key in self.metric_types:\n            results[key] = []\n\n        return results\n\n    def post_process_results(self, metric_results):\n\n        for key in metric_results:\n\n            if key == ""SSPE"":\n                continue\n\n            if key == ""inception_softmax"" :\n                metric_results[key] = np.concatenate(metric_results[key], axis=0)\n                continue\n\n            for sub_key in metric_results[key]:\n                if sub_key == ""CS"":\n                    continue\n                feats = metric_results[key][sub_key]\n                metric_results[key][sub_key] = np.concatenate(feats, axis=0)\n\n        results = dict()\n        if self.get_is_feats:\n            feats_norm = metric_results[""inception_softmax""]\n            results[""is""] = self.metric_dict[""is""].is_score_func(feats_norm)\n\n            print(""is = {}, quality = {}"".format(results[""is""], TYPES_QUALITIES[""is""]))\n\n        if self.get_fid_feats:\n            pred_feats = metric_results[""inception_feats""][""pred""]\n            ref_feats = metric_results[""inception_feats""][""ref""]\n            results[""fid""] = BaseMetric.fid_score_func(pred_feats, ref_feats)\n\n            print(""fid = {}, quality = {}"".format(results[""fid""], TYPES_QUALITIES[""fid""]))\n\n        if self.get_osnet_feats:\n            pred_feats = metric_results[""osnet_feats""][""pred""]\n            ref_feats = metric_results[""osnet_feats""][""ref""]\n            results[""OS-freid""] = BaseMetric.fid_score_func(pred_feats, ref_feats)\n\n            print(""OS-freid = {}, quality = {}"".format(results[""OS-freid""], TYPES_QUALITIES[""OS-freid""]))\n\n            if self.get_cs_reid:\n                results[""OS-CS-reid""] = np.mean(metric_results[""osnet_feats""][""CS""])\n\n                print(""OS-CS-reid = {}, quality = {}"".format(results[""OS-CS-reid""], TYPES_QUALITIES[""OS-CS-reid""]))\n\n        if self.get_pcb_feats:\n            pred_feats = metric_results[""pcb_feats""][""pred""]\n            ref_feats = metric_results[""pcb_feats""][""ref""]\n            results[""PCB-freid""] = BaseMetric.fid_score_func(pred_feats, ref_feats)\n\n            print(""PCB-freid = {}, quality = {}"".format(results[""PCB-freid""], TYPES_QUALITIES[""PCB-freid""]))\n\n            if self.get_cs_reid:\n                results[""PCB-CS-reid""] = np.mean(metric_results[""pcb_feats""][""CS""])\n\n                print(""PCB-CS-reid = {}, quality = {}"".format(results[""PCB-CS-reid""], TYPES_QUALITIES[""PCB-CS-reid""]))\n\n        if self.get_face_feats:\n            pred_feats = metric_results[""face_feats""][""pred""]\n            ref_feats = metric_results[""face_feats""][""ref""]\n\n            length = len(ref_feats)\n            if length == 0:\n                print(""there is no face detected! We can not compute face-FD and face-CS."")\n            else:\n                results[""face-FD""] = BaseMetric.fid_score_func(pred_feats, ref_feats)\n                print(""face-FD = {}, quality = {}"".format(results[""face-FD""], TYPES_QUALITIES[""face-FD""]))\n                if self.get_face_cs:\n                    results[""face-CS""] = np.mean(metric_results[""face_feats""][""CS""])\n                    print(""face-CS = {}, quality = {}"".format(results[""face-CS""], TYPES_QUALITIES[""face-CS""]))\n\n        if self.get_sspe:\n            results[""SSPE""] = np.mean(metric_results[""SSPE""])\n            print(""SSPE = {}, quality = {}"".format(results[""SSPE""], TYPES_QUALITIES[""SSPE""]))\n\n        return results\n\n\nclass Evaluator(object):\n    def __init__(self, dataset, data_dir):\n        self.dataset = dataset\n        self.protocols = create_dataset_protocols(dataset, data_dir)\n\n    def build_metrics(self):\n        raise NotImplementedError\n\n    def run_inference(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def run_metrics(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def evaluate(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def preprocess(self, *args, **kwargs):\n        raise NotImplementedError\n'"
thirdparty/his_evaluators/his_evaluators/evaluators/motion_imitation.py,2,"b'from abc import ABC\nimport torch\nfrom multiprocessing import Process, Manager\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any\nimport os\n\nfrom his_evaluators.metrics import TYPES_QUALITIES\n\nfrom .base import PairedMetricRunner, UnpairedMetricRunner, Evaluator\nfrom ..utils.io import mkdir\n\n\nclass MotionImitationModel(object):\n\n    def __init__(self, output_dir):\n        """"""\n\n        Args:\n            output_dir:\n        """"""\n\n        self.output_dir = mkdir(output_dir)\n        self.si_out_dir = mkdir(os.path.join(output_dir, ""self_imitation""))\n        self.ci_out_dir = mkdir(os.path.join(output_dir, ""cross_imitation""))\n        self.num_preds_si = 0\n        self.num_preds_ci = 0\n\n    def imitate(self, src_infos: Dict[str, Any], ref_infos: Dict[str, Any]) -> List[str]:\n        """"""\n            Running the motion imitation of the self.model, based on the source information with respect to the\n            provided reference information. It returns the full paths of synthesized images.\n        Args:\n            src_infos (dict): the source information contains:\n                --images (list of str): the list of full paths of source images (the length is 1)\n                --smpls (np.ndarray): (length of images, 85)\n                --kps (np.ndarray): (length of images, 19, 2)\n            ref_infos (dict): the reference information contains:\n                --images (list of str): the list of full paths of reference images.\n                --smpls (np.ndarray): (length of images, 85)\n                --kps (np.ndarray): (length of images, 19, 2)\n                --self_imitation (bool): the flag indicates whether it is self-imitation or not.\n\n        Returns:\n            preds_files (list of str): full paths of synthesized images with respects to the images in ref_infos.\n        """"""\n        raise NotImplementedError\n\n    def build_model(self):\n        """"""\n            You must define your model in this function, including define the graph and allocate GPU.\n            This function will be called in @see `MotionImitationRunnerProcessor.run()`.\n        Returns:\n            None\n        """"""\n        raise NotImplementedError\n\n    def terminate(self):\n        """"""\n            Close the model session, like if the model is based on TensorFlow, it needs to call sess.close() to\n            dealloc the resources.\n        Returns:\n\n        """"""\n        raise NotImplementedError\n\n    def personalization(self, src_infos):\n        """"""\n            some task/method specific data pre-processing or others.\n        Args:\n            src_infos (dict): the source information contains:\n                --images (list of str): the list of full paths of source images (the length is 1)\n                --smpls (np.ndarray): (length of images, 85)\n                --kps (np.ndarray): (length of images, 19, 2)\n\n        Returns:\n            processed_src_infos (dict): the source information contains:\n                --images (list of str): the list of full paths of source images (the length is 1)\n                --smpls (np.ndarray): (length of images, 85)\n                --kps (np.ndarray): (length of images, 19, 2)\n                ...\n        """"""\n\n        processed_src_infos = src_infos\n        return processed_src_infos\n\n\nclass MotionImitationRunnerProcessor(Process):\n    def __init__(self, model, protocols, return_dict: Manager):\n        """"""\n            The processor of running motion imitation models.\n        Args:\n            model (MotionImitationModel):\n            protocols (Protocols):\n            return_dict (Manager)\n        """"""\n        self.model = model\n        self.protocols = protocols\n        self.return_dict = return_dict\n\n        super().__init__()\n\n    def run(self):\n        self.model.build_model()\n\n        # si means self-imitation\n        all_si_preds_ref_file_list = []\n        # ci means cross-imitation\n        all_ci_preds_ref_file_list = []\n\n        for vid_info in tqdm(self.protocols):\n            # source information, contains {""images"", ""smpls"", ""kps""},\n            # here ""images"" are the list of full paths of source images (the length is 1)\n            src_infos = vid_info[""source""]\n\n            # run personalization\n            src_infos = self.model.personalization(src_infos)\n\n            # si means (self-imitation)\n            si_infos = vid_info[""self_imitation""]\n            si_pred_files = self.model.imitate(src_infos, si_infos)\n\n            # ci means (cross-imitation)\n            ci_infos = vid_info[""cross_imitation""]\n            ci_pred_files = self.model.imitate(src_infos, ci_infos)\n\n            si_pred_ref_files, ci_pred_ref_files = self.post_format_metric_file_list(\n                si_pred_files, si_infos[""images""],\n                ci_pred_files, vid_info[""flag""]\n            )\n\n            all_si_preds_ref_file_list.extend(si_pred_ref_files)\n            all_ci_preds_ref_file_list.extend(ci_pred_ref_files)\n\n            # break\n\n        self.return_dict[""all_si_preds_ref_file_list""] = all_si_preds_ref_file_list\n        self.return_dict[""all_ci_preds_ref_file_list""] = all_ci_preds_ref_file_list\n\n    def terminate(self) -> None:\n        self.model.terminate()\n\n    def post_format_metric_file_list(self, si_preds_files, si_ref_files, ci_preds_files, ci_ref_files):\n        """"""\n            make [(si_pred, si_ref), ...], and [(ci_pred, ci_ref), ...]\n        Args:\n            si_preds_files:\n            si_ref_files:\n            ci_preds_files:\n            ci_ref_files:\n\n        Returns:\n            si_preds_ref_files:\n            ci_preds_ref_files:\n        """"""\n        si_preds_ref_files = list(zip(si_preds_files, si_ref_files))\n        ci_preds_ref_files = list(zip(ci_preds_files, ci_ref_files))\n\n        return si_preds_ref_files, ci_preds_ref_files\n\n\nclass MotionImitationEvaluator(Evaluator, ABC):\n    def __init__(self, dataset, data_dir):\n        super().__init__(dataset, data_dir)\n\n        # please call `build_metrics` to instantiate these two runners.\n        self.paired_metrics_runner = None\n        self.unpaired_metrics_runner = None\n\n    def reset_dataset(self, dataset, data_dir):\n        super().__init__(dataset, data_dir)\n\n    def build_metrics(\n        self,\n        pair_types=(""ssim"", ""psnr"", ""lps""),\n        unpair_types=(""is"", ""fid"", ""PCB-freid"", ""PCB-CS-reid""),\n        device=torch.device(""cpu"")\n    ):\n        paired_metrics_runner = PairedMetricRunner(metric_types=pair_types, device=device)\n        unpaired_metrics_runner = UnpairedMetricRunner(metric_types=unpair_types, device=device)\n\n        self.paired_metrics_runner = paired_metrics_runner\n        self.unpaired_metrics_runner = unpaired_metrics_runner\n\n    def run_metrics(self, self_imitation_files, cross_imitation_files, image_size=512):\n        assert (self.paired_metrics_runner is not None or self.unpaired_metrics_runner is not None), \\\n            ""please call `build_metrics(pair_types, unpair_types)` to instantiate metrics runners "" \\\n            ""before calling this function.""\n\n        si_results = self.paired_metrics_runner.evaluate(self_imitation_files, image_size)\n        ci_results = self.unpaired_metrics_runner.evaluate(cross_imitation_files, image_size)\n\n        return si_results, ci_results\n\n    def evaluate(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def run_inference(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass IPERMotionImitationEvaluator(MotionImitationEvaluator):\n\n    def __init__(self, data_dir, dataset=""iPER""):\n        super().__init__(dataset=dataset, data_dir=data_dir)\n\n    def run_inference(self, model, src_infos, ref_infos):\n        """"""\n        Args:\n            model (MotionImitationModel): the model object, it must define and implements the function\n                            `imitate(src_infos, ref_infos, is_self_imitation) -> List[str]`\n            src_infos (dict): the source information contains:\n                --images (list of str): the list of full paths of source images (the length is 1)\n                --smpls (np.ndarray):\n                --kps (np.ndarray):\n            ref_infos (dict): the reference information contains:\n                --images (list of str):\n                --smpls (np.ndarray):\n                --kps (np.ndarray):\n                --self_imitation (bool):\n\n        Returns:\n            file_paths (list of str): [pred_img_path_0, pred_img_path_1, ..., pred_img_path_i, ..., pred_img_path_n)]\n\n        """"""\n\n        assert hasattr(model, ""imitate""), \'{} must implement imitate(src_infos, ref_infos) -> List[str]\'\n\n        file_paths = model.imitate(src_infos, ref_infos)\n\n        return file_paths\n\n    def evaluate(self, model, num_sources=1, image_size=512,\n                 pair_types=(""ssim"", ""psnr"", ""lps""),\n                 unpair_types=(""is"", ""fid"", ""PCB-freid"", ""PCB-CS-reid""),\n                 device=torch.device(""cpu"")):\n        # 1. setup protocols\n        self.protocols.setup(num_sources=num_sources, load_smpls=True, load_kps=True)\n\n        # 2. declare runner processor for inference\n        return_dict = Manager().dict({})\n        runner = MotionImitationRunnerProcessor(model, self.protocols, return_dict)\n        runner.start()\n        runner.join()\n\n        del model\n\n        all_si_preds_ref_file_list = return_dict[""all_si_preds_ref_file_list""]\n        all_ci_preds_ref_file_list = return_dict[""all_ci_preds_ref_file_list""]\n\n        # run metrics\n        self.build_metrics(pair_types, unpair_types, device)\n        si_results, ci_results = self.run_metrics(all_si_preds_ref_file_list, all_ci_preds_ref_file_list, image_size)\n\n        return si_results, ci_results\n\n    def preprocess(self, *args, **kwargs):\n        pass\n\n    def save_results(self, out_path, si_results, ci_result):\n        """"""\n            save the the results into the out_path.\n        Args:\n            out_path (str): the full path to save the results.\n            si_results (dict): the self-imitation results.\n            ci_result (dict): the cross-imitation results.\n\n        Returns:\n            None\n        """"""\n\n        with open(out_path, ""w"") as writer:\n            writer.write(""########################Self-imitation Results########################\\n"")\n            for key, val in si_results.items():\n                writer.write(""{} = {}, quality = {}\\n"".format(key, val, TYPES_QUALITIES[key]))\n\n            writer.write(""########################Cross-imitation Results########################\\n"")\n            for key, val in ci_result.items():\n                writer.write(""{} = {}, quality = {}\\n"".format(key, val, TYPES_QUALITIES[key]))\n\n'"
thirdparty/his_evaluators/his_evaluators/metrics/__init__.py,0,"b'from .metrics import BaseMetric, PerceptualMetric, SSIMMetric, PSNRMetric, \\\n    InceptionScoreMetric, FIDMetric, FreIDMetric, ReIDScore, ScaleShapePoseError\n\n\nTYPES = [\n    ""ssim"",\n    ""psnr"",\n    ""lps"",\n    ""is"",\n    ""fid"",\n    ""OS-CS-reid"",\n    ""OS-freid"",\n    ""PCB-CS-reid"",\n    ""PCB-freid"",\n    ""SSPE"",\n    ""face-CS"",\n    ""face-FD""\n]\n\nTYPES_RESULTS_MAP = {\n    ""ssim"": ""ssim"",\n    ""psnr"": ""psnr"",\n    ""lps"": ""lps"",\n    ""is"": ""inception_feats"",\n    ""fid"": ""inception_feats"",\n    ""OS-CS-reid"": ""osnet_feats"",\n    ""OS-freid"": ""osnet_feats"",\n    ""PCB-CS-reid"": ""pcb_feats"",\n    ""PCB-freid"": ""pcb_feats"",\n    ""SSPE"": ""SSPE"",\n    ""face-CS"": ""face-CS"",\n    ""face-FD"": ""face_feats""\n}\n\nTYPES_QUALITIES = {\n    ""ssim"": BaseMetric.LOWER,\n    ""psnr"": BaseMetric.HIGHER,\n    ""lps"": BaseMetric.LOWER,\n    ""is"": BaseMetric.HIGHER,\n    ""fid"": BaseMetric.LOWER,\n    ""OS-CS-reid"": BaseMetric.HIGHER,\n    ""OS-freid"": BaseMetric.LOWER,\n    ""PCB-CS-reid"": BaseMetric.HIGHER,\n    ""PCB-freid"": BaseMetric.LOWER,\n    ""SSPE"": BaseMetric.LOWER,\n    ""face-CS"": BaseMetric.HIGHER,\n    ""face-FD"": BaseMetric.LOWER\n}\n\nMETRIC_DICT = dict()\n\n\ndef register_metrics(types, device, has_detector=True):\n    global TYPES, METRIC_DICT\n\n    metric_dict = dict()\n\n    for name in types:\n        assert name in TYPES\n\n        if name in METRIC_DICT:\n            metric_dict[name] = METRIC_DICT[name]\n            continue\n\n        if name == ""ssim"":\n            metric_dict[name] = SSIMMetric()\n        elif name == ""psnr"":\n            metric_dict[name] = PSNRMetric()\n        elif name == ""lps"":\n            metric_dict[name] = PerceptualMetric(device)\n        elif name == ""is"":\n            metric_dict[name] = InceptionScoreMetric(device)\n        elif name == ""fid"":\n            metric_dict[name] = FIDMetric(device)\n        elif name == ""OS-CS-reid"":\n            metric_dict[name] = ReIDScore(device, reid_name=BaseMetric.OSreID, has_detector=has_detector)\n        elif name == ""OS-freid"":\n            metric_dict[name] = FreIDMetric(device, reid_name=BaseMetric.OSreID, has_detector=has_detector)\n        elif name == ""PCB-CS-reid"":\n            metric_dict[name] = ReIDScore(device, reid_name=BaseMetric.PCBreID, has_detector=has_detector)\n        elif name == ""PCB-freid"":\n            metric_dict[name] = FreIDMetric(device, reid_name=BaseMetric.PCBreID, has_detector=has_detector)\n        elif name == ""SSPE"":\n            metric_dict[name] = ScaleShapePoseError(device)\n        elif name == ""face-CS"":\n            from .metrics import FaceSimilarityScore\n            metric_dict[name] = FaceSimilarityScore(device=device, has_detector=has_detector)\n        elif name == ""face-FD"":\n            from .metrics import FaceFrechetDistance\n            metric_dict[name] = FaceFrechetDistance(device=device, has_detector=has_detector)\n        else:\n            raise ValueError(name)\n\n        METRIC_DICT[name] = metric_dict[name]\n\n    return metric_dict\n'"
thirdparty/his_evaluators/his_evaluators/metrics/metrics.py,63,"b'from __future__ import division\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models.inception import inception_v3\nimport numpy as np\nimport skimage.metrics\nfrom scipy import linalg\n\n\nMODEL_ZOOS = dict()\n\n\nclass InceptionV3(nn.Module):\n    """"""Pretrained InceptionV3 network returning feature maps""""""\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 4\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,    # First max pooling features\n        192: 1,   # Second max pooling featurs\n        768: 2,   # Pre-aux classifier features\n        2048: 3,  # Final average pooling features\n        1000: 4   # Final classifier score\n    }\n\n    def __init__(self,\n                 output_blocks=(DEFAULT_BLOCK_INDEX,),\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False):\n        """"""Build pretrained InceptionV3\n        Parameters\n        ----------\n        output_blocks : list of int\n            Indices of blocks to return features of. Possible values are:\n                - 0: corresponds to output of first max pooling\n                - 1: corresponds to output of second max pooling\n                - 2: corresponds to output which is fed to aux classifier\n                - 3: corresponds to output of final average pooling\n        resize_input : bool\n            If true, bilinearly resizes input to width and height 299 before\n            feeding input to model. As the network without fully connected\n            layers is fully convolutional, it should be able to handle inputs\n            of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n            If true, scales the input from range (0, 1) to the range the\n            pretrained Inception network expects, namely (-1, 1)\n        requires_grad : bool\n            If true, parameters of the model require gradient. Possibly useful\n            for finetuning the network\n        """"""\n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 4, \\\n            \'Last possible output block index is 4\'\n\n        self.blocks = nn.ModuleList()\n\n        inception = inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        # Block 4: 1000 classifier\n        if self.last_needed_block >= 4:\n            self.blocks.append(inception.fc)\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        """"""Get Inception feature maps\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in\n            range (0, 1)\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output\n        block, sorted ascending by index\n        """"""\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.interpolate(x, size=(299, 299), mode=\'bilinear\', align_corners=False)\n\n        if self.normalize_input:\n            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n\n            if idx == 3:\n                x = x[:, :, 0, 0]\n\n            if idx in self.output_blocks:\n                if len(x.shape) == 4 and (x.size(2) != 1 or x.size(3) != 1):\n                    preds = F.adaptive_avg_pool2d(x, output_size=(1, 1)).squeeze_(-1).squeeze_(-1)\n                else:\n                    preds = x\n                outp.append(preds)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp\n\n\nclass BaseMetric(object):\n\n    INCEPTION_V3 = \'inception_v3\'\n    PERCEPTUAL = \'perceptual\'\n    HMR = ""hmr""\n    FACE = ""sphereface""\n    FACE_DETECTOR = ""MTCNN""\n    FACE_FD = ""inception_resnet_v1""\n    FACE_RECOGNITION = ""inception_resnet_v1""\n    PERSON_DETECTOR = ""YOLOv3""\n    OSreID = \'OS-reid\'\n    PCBreID = \'PCB-reid\'\n\n    FACE_RECOGNITION_SIZE = 160\n\n    MODEL_KEYS = [INCEPTION_V3, PERCEPTUAL, HMR, FACE, PERSON_DETECTOR,\n                  OSreID, PCBreID, FACE_DETECTOR, FACE_RECOGNITION]\n\n    LOWER = \'lower score is better.\'\n    HIGHER = \'higher score is better\'\n\n    def __init__(self, device=torch.device(""cpu""), size=(512, 512)):\n        global MODEL_ZOOS\n\n        super(BaseMetric, self).__init__()\n\n        self.device = device\n        self.size = size\n        self.model_zoos = MODEL_ZOOS\n\n    def forward(self, *input):\n        raise NotImplementedError\n\n    def calculate_score(self, *input):\n        raise NotImplementedError\n\n    def register_model(self, key):\n        assert key in self.MODEL_KEYS, \'{} must in {}\'.format(key, self.MODEL_KEYS)\n\n        if key not in self.model_zoos.keys():\n            if key == self.INCEPTION_V3:\n                self.model_zoos[key] = InceptionV3(output_blocks=[3], resize_input=False,\n                                                   normalize_input=False, requires_grad=False)\n                self.model_zoos[key] = self.model_zoos[key].to(self.device)\n                self.model_zoos[key].eval()\n\n            elif key == self.PERCEPTUAL:\n                from .lpips import PerceptualLoss\n\n                use_gpu = self.device != ""cpu""\n                model = PerceptualLoss(model=\'net-lin\', net=\'alex\', use_gpu=use_gpu)\n\n                self.model_zoos[key] = model\n\n            elif key == self.PERSON_DETECTOR:\n                from .yolov3 import YoLov3HumanDetector\n\n                data_dir = self.resource_dir\n\n                detector = YoLov3HumanDetector(\n                    weights_path=os.path.join(data_dir, ""yolov3-spp.weights""),\n                    device=self.device\n                )\n                self.model_zoos[key] = detector\n\n            elif key == self.OSreID:\n                from .OSreid import OsNetEncoder\n\n                data_dir = self.resource_dir\n\n                model = OsNetEncoder(\n                    # input_width=704,\n                    # input_height=480,\n                    # weight_filepath=""weights/model_weights.pth.tar-40"",\n                    weight_filepath=os.path.join(data_dir, ""osnet_ibn_x1_0_imagenet.pth""),\n                    input_height=self.size[0],\n                    input_width=self.size[1],\n                    batch_size=32,\n                    num_classes=2022,\n                    patch_height=256,\n                    patch_width=128,\n                    norm_mean=[0.485, 0.456, 0.406],\n                    norm_std=[0.229, 0.224, 0.225],\n                    GPU=True)\n                self.model_zoos[key] = model\n\n            elif key == self.PCBreID:\n                from .PCBreid import PCBReIDMetric\n\n                data_dir = self.resource_dir\n\n                model = PCBReIDMetric(name=""PCB"", pretrain_path=os.path.join(data_dir, ""pcb_net_last.pth""))\n                model = model.to(self.device)\n\n                self.model_zoos[key] = model\n\n            elif key == self.HMR:\n                from .bodynets import HumanModelRecovery\n\n                data_dir = self.resource_dir\n\n                model = HumanModelRecovery(smpl_pkl_path=os.path.join(data_dir, ""smpl_model.pkl""))\n                model_dict = torch.load(os.path.join(data_dir, ""hmr_tf2pt.pth""))\n                model.load_state_dict(model_dict)\n                model = model.to(self.device)\n                model.eval()\n\n                self.model_zoos[key] = model\n\n            elif key == self.FACE_DETECTOR:\n                from .facenet_pytorch import MTCNN\n\n                mtcnn = MTCNN(image_size=self.FACE_RECOGNITION_SIZE, device=self.device)\n                self.model_zoos[key] = mtcnn\n\n            elif key == self.FACE_RECOGNITION:\n                from .facenet_pytorch import InceptionResnetV1\n\n                resnet = InceptionResnetV1(pretrained=\'vggface2\', classify=False, device=self.device).eval()\n                self.model_zoos[key] = resnet\n\n            else:\n                raise ValueError(key)\n\n    @property\n    def resource_dir(self):\n        dirpath = os.path.abspath(os.path.dirname(__file__))\n        data_dir = os.path.dirname(os.path.dirname(dirpath))\n        data_dir = os.path.join(data_dir, ""data"")\n        return data_dir\n\n    def preprocess(self, x):\n        raise NotImplementedError\n\n    @staticmethod\n    def to_numpy(x, transpose=False):\n        if not isinstance(x, np.ndarray):\n            x = x.cpu().numpy()\n\n        if transpose:\n            # (3, 256, 256) -> (256, 256, 3)\n            x = np.transpose(x, (1, 2, 0))\n\n        return x\n\n    def quality(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n        """"""Numpy implementation of the Frechet Distance.\n        The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n        and X_2 ~ N(mu_2, C_2) is\n                d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n        Stable version by Dougal J. Sutherland.\n        Params:\n        -- mu1   : Numpy array containing the activations of a layer of the\n                   inception net (like returned by the function \'get_predictions\')\n                   for generated samples.\n        -- mu2   : The sample mean over activations, precalculated on an\n                   representative data set.\n        -- sigma1: The covariance matrix over activations for generated samples.\n        -- sigma2: The covariance matrix over activations, precalculated on an\n                   representative data set.\n        Returns:\n        --   : The Frechet Distance.\n        """"""\n\n        mu1 = np.atleast_1d(mu1)\n        mu2 = np.atleast_1d(mu2)\n\n        sigma1 = np.atleast_2d(sigma1)\n        sigma2 = np.atleast_2d(sigma2)\n\n        assert mu1.shape == mu2.shape, \\\n            \'Training and test mean vectors have different lengths\'\n        assert sigma1.shape == sigma2.shape, \\\n            \'Training and test covariances have different dimensions\'\n\n        diff = mu1 - mu2\n\n        # Product might be almost singular\n        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n        if not np.isfinite(covmean).all():\n            msg = (\'fid calculation produces singular product; \'\n                   \'adding %s to diagonal of cov estimates\') % eps\n            print(msg)\n            offset = np.eye(sigma1.shape[0]) * eps\n            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n        # Numerical error might give slight imaginary component\n        if np.iscomplexobj(covmean):\n            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n                m = np.max(np.abs(covmean.imag))\n                raise ValueError(\'Imaginary component {}\'.format(m))\n            covmean = covmean.real\n\n        tr_covmean = np.trace(covmean)\n\n        return (diff.dot(diff) + np.trace(sigma1) +\n                np.trace(sigma2) - 2 * tr_covmean)\n\n    @staticmethod\n    def fid_score_func(pred_feats, gt_feats):\n        """"""\n\n        Args:\n            pred_feats:\n            gt_feats:\n\n        Returns:\n\n        """"""\n        if len(pred_feats) == 0 or len(gt_feats) == 0:\n            return 0\n        else:\n            m1, s1 = np.mean(pred_feats, axis=0), np.cov(pred_feats, rowvar=False)\n            m2, s2 = np.mean(gt_feats, axis=0), np.cov(gt_feats, rowvar=False)\n            return BaseMetric.calculate_frechet_distance(m1, s1, m2, s2)\n\n    @staticmethod\n    def is_score_func(feats_softmax):\n        """"""\n            inception score function.\n\n        Args:\n            feats_softmax (np.ndarray): features after softmax\n\n        Returns:\n            score (float)\n        """"""\n        kl = feats_softmax * (np.log(feats_softmax) - np.log(np.expand_dims(np.mean(feats_softmax, 0), 0)))\n        kl = np.mean(np.sum(kl, 1))\n        score = np.exp(kl)\n        return score\n\n    @staticmethod\n    def ssp_abs_err_score_func(src_smpls, ref_smpls):\n        """"""\n            The function of Scale-Shape-Pose absolution error score.\n        Args:\n            src_smpls (np.ndarray): (bs, 85),\n            ref_smpls (np.ndarray): (bs, 85).\n\n        Returns:\n            error (float): the absolution Scale-Shape-Pose error between the source and the reference smpls .\n        """"""\n\n        src_scale = src_smpls[:, 0]\n        ref_scale = ref_smpls[:, 0]\n\n        scale_error = np.mean(np.abs(src_scale - ref_scale))\n        shape_error = np.mean(np.sum(np.abs(src_smpls[:, -10:] - ref_smpls[:, -10:]), axis=1))\n        pose_error = np.mean(np.sum(np.abs(src_smpls[:, 0:-10] - ref_smpls[:, 0:-10]), axis=1))\n\n        error = scale_error + shape_error + pose_error\n\n        return error\n\n    @staticmethod\n    def normalize_feats(feats):\n        """"""\n\n        Args:\n            feats (np.ndarray): (bs, dim)\n\n        Returns:\n            feats (np.ndarray): (bs, dim)\n        """"""\n\n        norm = np.sqrt(np.sum(feats ** 2, axis=1, keepdims=True))\n        feats = feats / (norm + 1e-6)\n        return feats\n\n    def cosine_similarity(self, pred_feats, ref_feats, norm_first=True):\n\n        if len(pred_feats) == 0 or len(ref_feats) == 0:\n            return 0\n        else:\n            if norm_first:\n                pred_norm = self.normalize_feats(pred_feats)\n                ref_norm = self.normalize_feats(ref_feats)\n            else:\n                pred_norm = pred_feats\n                ref_norm = ref_feats\n\n            return np.mean(np.sum(pred_norm * ref_norm, axis=1))\n\n\nclass SSIMMetric(BaseMetric):\n\n    def __init__(self):\n        super(SSIMMetric, self).__init__()\n        BaseMetric.__init__(self)\n\n    def preprocess(self, x):\n        """"""\n            normalize x from [0, 1] color intensity with np.float32 to [-1, 1] with np.float32,\n        Args:\n            x (np.ndarray or torch.tensor): [0, 1] color intensity, np.float32 (torch.tensor),\n            shape = (3, image_size, image_size)\n\n        Returns:\n            out (np.ndarray): [-1, 1] color intensity, np.float32, (image_size, image_size, 3)\n        """"""\n        if isinstance(x, np.ndarray):\n            out = x.astype(np.float32, copy=True)\n        else:\n            out = x.numpy().astype(np.float32, copy=True)\n\n        out *= 2\n        out -= 1\n\n        out = np.transpose(out, (1, 2, 0))\n\n        return out\n\n    def forward(self, pred, ref):\n        """"""\n\n        Args:\n            pred (np.ndarray): color intensity is [-1, 1]\n            ref (np.ndarray): color intensity is [-1, 1]\n\n        Returns:\n\n        """"""\n        # print(pred.shape, img.shape)\n        # score = measure.compare_ssim(pred, ref, multichannel=True)\n        pred = self.preprocess(pred)\n        ref = self.preprocess(ref)\n        score = skimage.metrics.structural_similarity(pred, ref, multichannel=True)\n        return score\n\n    def calculate_score(self, preds, gts):\n        scores = []\n        length = len(preds)\n\n        assert length == len(gts)\n\n        for i, (pred, ref) in enumerate(zip(preds, gts)):\n            scores.append(self.forward(pred, ref))\n\n        return np.mean(scores)\n\n    def quality(self):\n        return self.HIGHER\n\n\nclass PSNRMetric(BaseMetric):\n    def __init__(self):\n        super(PSNRMetric, self).__init__()\n        BaseMetric.__init__(self)\n\n    def preprocess(self, x):\n        """"""\n            normalize x from [0, 1] color intensity with np.uint8 to [-1, 1] with np.float32,\n        Args:\n            x (np.ndarray or torch.tensor): [0, 1] color intensity, np.float32 (torch.tensor),\n            shape = (3, image_size, image_size)\n\n        Returns:\n            out (np.ndarray): [-1, 1] color intensity, np.float32, (image_size, image_size, 3)\n        """"""\n        if isinstance(x, np.ndarray):\n            out = x.astype(np.float32, copy=True)\n        else:\n            out = x.numpy().astype(np.float32, copy=True)\n\n        out *= 2\n        out -= 1\n\n        out = np.transpose(out, (1, 2, 0))\n\n        return out\n\n    def forward(self, pred, ref):\n        """"""\n\n        Args:\n            pred (np.ndarray): color intensity is [-1, 1]\n            ref (np.ndarray): color intensity is [-1, 1]\n\n        Returns:\n            score (np.float32): the ssim score, higher is better.\n        """"""\n\n        # score = measure.compare_psnr(pred, ref)\n        pred = self.preprocess(pred)\n        ref = self.preprocess(ref)\n        score = skimage.metrics.peak_signal_noise_ratio(image_true=ref, image_test=pred)\n        return score\n\n    def calculate_score(self, preds, gts):\n        scores = []\n        length = len(preds)\n\n        assert length == len(gts)\n\n        for i, (pred, ref) in enumerate(zip(preds, gts)):\n            scores.append(self.forward(pred, ref))\n\n        return np.mean(scores)\n\n    def quality(self):\n        return self.HIGHER\n\n\nclass PerceptualMetric(BaseMetric):\n    def __init__(self, device):\n\n        BaseMetric.__init__(self, device=device)\n        self.register_model(self.PERCEPTUAL)\n\n    def preprocess(self, x):\n        """"""\n\n        Args:\n            x (np.ndarray (or torch.tensor)): np.ndarray (or torch.tensor),\n            each element is [bs, 3, image_size, image_size] wth np.float32 (torch.float32) and color intensity [0, 1];\n\n        Returns:\n            out (torch.tensor): (bs, 3, image_size, image_size), color intensity [-1, 1]\n        """"""\n        if isinstance(x, np.ndarray):\n            out = torch.tensor(x).float()\n        else:\n            out = x.clone().float()\n\n        out *= 2\n        out -= 1\n        # print(out.shape, out.max(), out.min(), out.device)\n        # print(self.device)\n        out = out.to(self.device)\n        return out\n\n    def forward(self, pred, ref):\n        """"""\n\n        Args:\n            pred (torch.tensor): color intensity is [-1, 1]\n            ref (torch.tensor): color intensity is [-1, 1]\n\n        Returns:\n            score (torch.tensor):\n        """"""\n\n        with torch.no_grad():\n            pred = self.preprocess(pred)\n            ref = self.preprocess(ref)\n            # return 1.0 - torch.mean(self.model_zoos[self.PERCEPTUAL].forward(pred, target))\n            return torch.mean(self.model_zoos[self.PERCEPTUAL].forward(pred, ref))\n\n    def calculate_score(self, preds, gts, batch_size=32):\n        assert len(preds) == len(gts)\n\n        length = len(preds)\n\n        scores = []\n\n        for i in range(int(math.ceil((length / batch_size)))):\n            pred_batch = preds[i * batch_size: (i + 1) * batch_size]\n            gt_batch = gts[i * batch_size: (i + 1) * batch_size]\n\n            s = self.forward(pred_batch, gt_batch)\n            scores.append(s)\n        scores = torch.stack(scores)\n        return torch.mean(scores).cpu().numpy()\n\n    def quality(self):\n        return self.LOWER\n\n\nclass InceptionScoreMetric(BaseMetric):\n\n    def __init__(self, device=torch.device(""cpu"")):\n        BaseMetric.__init__(self, device)\n\n        self.register_model(self.INCEPTION_V3)\n\n        self.height, self.width = 299, 299\n        # self.mean = torch.tensor([0.485, 0.456, 0.406]).float().to(self.device).view(1, 3, 1, 1)\n        # self.std = torch.tensor([0.229, 0.224, 0.225]).float().to(self.device).view(1, 3, 1, 1)\n\n    def preprocess(self, x):\n        """"""\n\n        Args:\n            x (np.ndarray or torch.tensor): np.ndarray or torch.tensor, each element is [3, image_size, image_size] wth np.uint8 and color intensity [0, 255];\n\n        Returns:\n            out (torch.tensor): (bs, 3, 299, 299), color intensity [0, 1] and normalized using\n            mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n        """"""\n\n        if isinstance(x, np.ndarray):\n            out = torch.tensor(x)\n        else:\n            out = x.clone()\n\n        out *= 2\n        out -= 1\n        out = out.to(self.device)\n\n        with torch.no_grad():\n            out = F.interpolate(out, size=(self.height, self.width), mode=\'bilinear\', align_corners=False)\n            # out = (out - self.mean) / self.std\n\n        return out\n\n    def forward(self, imgs):\n        """"""\n\n        Args:\n            imgs (torch.tensor): (bs, 3, image_size, image_size), color intensity [0, 1] and normalized using\n            mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n\n        Returns:\n            probs (torch.tensor): (bs, number of feature outputs)\n        """"""\n\n        imgs = self.preprocess(imgs)\n        with torch.no_grad():\n            feats = self.model_zoos[self.INCEPTION_V3](imgs)[-1]\n            probs = F.softmax(feats, dim=1)\n\n        probs = probs.cpu().numpy()\n        return probs\n\n    def calculate_score(self, preds, batch_size=32):\n        scores = []\n        length = len(preds)\n        for i in range(int(math.ceil((length / batch_size)))):\n            pred_batch = preds[i * batch_size: (i + 1) * batch_size]\n\n            part = self.forward(pred_batch)\n            scores.append(self.is_score_func(part))\n        return np.mean(scores), np.std(scores)\n\n    def quality(self):\n        return self.HIGHER\n\n\nclass FIDMetric(BaseMetric):\n\n    def __init__(self, device=torch.device(""cpu"")):\n        BaseMetric.__init__(self, device)\n\n        self.register_model(self.INCEPTION_V3)\n\n        self.height, self.width = 299, 299\n        # self.mean = torch.tensor([0.485, 0.456, 0.406]).float().to(self.device).view(1, 3, 1, 1)\n        # self.std = torch.tensor([0.229, 0.224, 0.225]).float().to(self.device).view(1, 3, 1, 1)\n\n    def preprocess(self, x):\n        """"""\n\n        Args:\n            x (np.ndarray or torch.tensor): np.ndarray or torch.tensor, each element is\n                            [3, image_size, image_size] wth np.float32 and color intensity [0, 1];\n\n        Returns:\n            out (torch.tensor): (bs, 3, 299, 299), color intensity [0, 1] and normalized using\n            mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n        """"""\n\n        if isinstance(x, np.ndarray):\n            out = torch.tensor(x)\n        else:\n            out = x.clone()\n\n        out *= 2\n        out -= 1\n        out = out.to(self.device)\n\n        with torch.no_grad():\n            out = F.interpolate(out, size=(self.height, self.width), mode=\'bilinear\', align_corners=False)\n            # out = (out - self.mean) / self.std\n\n        return out\n\n    def forward(self, imgs):\n        """"""\n\n        Args:\n            imgs (torch.tensor): (bs, 3, image_size, image_size), color intensity [0, 1] and normalized using\n            mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n\n        Returns:\n            probs (torch.tensor): (bs, number of feature outputs)\n        """"""\n\n        imgs = self.preprocess(imgs)\n        with torch.no_grad():\n            feats = self.model_zoos[self.INCEPTION_V3](imgs)[-1]\n            # print(feats.shape)\n            feats = feats.cpu().numpy()\n        return feats\n\n    def calculate_score(self, preds, gts, batch_size=32):\n        pred_feats = []\n        gt_feats = []\n\n        length = len(preds)\n        for i in range(int(math.ceil((length / batch_size)))):\n            pred_batch = preds[i * batch_size: (i + 1) * batch_size]\n            gt_batch = gts[i * batch_size: (i + 1) * batch_size]\n\n            pred_f = self.forward(pred_batch)\n            gt_f = self.forward(gt_batch)\n\n            pred_feats.append(pred_f)\n            gt_feats.append(gt_f)\n\n        pred_feats = np.concatenate(pred_feats, axis=0)\n        gt_feats = np.concatenate(gt_feats, axis=0)\n\n        return self.fid_score_func(pred_feats, gt_feats)\n\n    def quality(self):\n        return self.LOWER\n\n\nclass FreIDMetric(BaseMetric):\n    def __init__(self, device=torch.device(""cpu""), reid_name=""PCB-reid"", has_detector=True):\n\n        BaseMetric.__init__(self, device)\n\n        if reid_name == ""PCB-reid"":\n            self.REID = self.PCBreID\n        else:\n            self.REID = self.OSreID\n        self.register_model(self.REID)\n\n        self.has_detector = has_detector\n        if has_detector:\n            self.register_model(self.PERSON_DETECTOR)\n\n    def preprocess(self, x):\n        """"""\n\n        Args:\n            x (torch.tensor): (bs, 3, height, width) is in the range of [0, 1] with torch.float32.\n\n        Returns:\n            x (torch.tensor): (bs, 3, height, width) is in the range of [0, 1] with torch.float32.\n        """"""\n        x = x.clone()\n        x = x.to(self.device)\n        return x\n\n    def forward(self, pred):\n        """"""\n\n        Args:\n            x (torch.tensor): np.ndarray or torch.tensor, each element is\n                [bs, 3, image_size, image_size] wth torch.uint8 and color intensity [0, 255];\n\n        Returns:\n            feat (np.ndarray): [bs, C]\n        """"""\n\n        with torch.no_grad():\n            pred = self.preprocess(pred)\n\n            if self.has_detector:\n                img_shapes = [pred.shape[2:]] * pred.shape[0]\n                boxes = self.model_zoos[self.PERSON_DETECTOR](pred, img_shapes, factor=1.05)\n            else:\n                boxes = None\n            feat = self.model_zoos[self.REID](pred, boxes)\n            feat = feat.cpu().numpy()\n        return feat\n\n    def calculate_score(self, preds, gts, batch_size=32):\n        pred_feats = []\n        gt_feats = []\n\n        length = len(preds)\n        for i in range(int(math.ceil((length / batch_size)))):\n            pred_batch = preds[i * batch_size: (i + 1) * batch_size]\n            gt_batch = gts[i * batch_size: (i + 1) * batch_size]\n\n            pred_f = self.forward(pred_batch)\n            gt_f = self.forward(gt_batch)\n\n            pred_feats.append(pred_f)\n            gt_feats.append(gt_f)\n\n        pred_feats = np.concatenate(pred_feats, axis=0)\n        gt_feats = np.concatenate(gt_feats, axis=0)\n\n        return self.fid_score_func(pred_feats, gt_feats)\n\n    def quality(self):\n        return self.LOWER\n\n\nclass ReIDScore(FreIDMetric):\n\n    def __init__(self, device=torch.device(""cpu""), reid_name=""PCB-reid"", has_detector=True):\n        super().__init__(device, reid_name, has_detector)\n\n    def calculate_score(self, preds, gts, batch_size=32):\n        assert len(preds) == len(gts)\n\n        length = len(preds)\n\n        scores = []\n\n        for i in range(int(math.ceil((length / batch_size)))):\n            pred_batch = preds[i * batch_size: (i + 1) * batch_size]\n            gt_batch = gts[i * batch_size: (i + 1) * batch_size]\n\n            pred_f = self.forward(pred_batch)\n            gt_f = self.forward(gt_batch)\n\n            s = self.cosine_similarity(pred_f, gt_f)\n            scores.append(s)\n\n        return np.mean(scores)\n\n    def quality(self):\n        return self.HIGHER\n\n\nclass FaceSimilarityScore(BaseMetric):\n\n    def __init__(self, device=torch.device(""cpu""), has_detector=True):\n        """"""\n\n        Args:\n            device (torch.device):\n            has_detector (bool): use detector or not, default is True.\n        """"""\n\n        super().__init__(device=device)\n        self.has_detector = has_detector\n\n        if has_detector:\n            self.register_model(self.FACE_DETECTOR)\n\n        self.register_model(self.FACE_RECOGNITION)\n\n    def preprocess(self, x):\n        """"""\n\n        Args:\n            x (np.ndarray or torch.tensor): each element is [bs, 3, image_size, image_size]\n                        with float32 and color intensity [0, 1];\n\n        Returns:\n            out (np.ndarray or torch.tensor): (bs, image_size, image_size, 3),\n                 if self.has_detector is True, then it is in the range of [0, 255] with np.uint8;\n                 otherwise, it is the torch.tensor and its color is in the range of [-1, 1] with torch.float32.\n        """"""\n\n        x = x.clone()\n        if self.has_detector:\n            if not isinstance(x, np.ndarray):\n                x = x.numpy()\n                x *= 255\n                x = x.astype(np.uint8)\n            x = np.transpose(x, (0, 2, 3, 1))\n        else:\n            x *= 2\n            x -= 1\n            x = x.to(self.device)\n            x = F.interpolate(x, size=(self.FACE_RECOGNITION_SIZE, self.FACE_RECOGNITION_SIZE), mode=""area"")\n        return x\n\n    def detect_face(self, img):\n        """"""\n\n        Args:\n            img (torch.tensor): (bs, 3, height, width) is in the range of [0, 1] with torch.float32.\n\n        Returns:\n            face_cropped (torch.tensor): (bs, 3, face_size, face_size) is in the range of [-1, 1] with torch.float32.\n        """"""\n\n        # Get cropped and prewhitened image tensor\n        # img_cropped = mtcnn(img, save_path=<optional save path>)\n        proc_img = img.clone()\n        proc_img = proc_img.numpy()\n        proc_img *= 255\n        proc_img = proc_img.astype(np.uint8)\n        proc_img = np.transpose(proc_img, (0, 2, 3, 1))\n\n        img_cropped = self.model_zoos[self.FACE_DETECTOR](proc_img)\n\n        face_cropped = []\n        valid_ids = []\n        for i, cropped in enumerate(img_cropped):\n            if cropped is None:\n                cropped = F.interpolate(\n                    img[i:i + 1] * 2 - 1,\n                    size=(self.FACE_RECOGNITION_SIZE, self.FACE_RECOGNITION_SIZE),\n                    mode=""area""\n                )[0]\n            else:\n                valid_ids.append(i)\n\n            # print(cropped.shape, cropped.max(), cropped.min(), img[i:i+1].max(), img[i:i+1].min())\n            face_cropped.append(cropped)\n\n        face_cropped = torch.stack(face_cropped).to(self.device)\n\n        return face_cropped, valid_ids\n\n    def forward(self, img):\n        """"""\n\n        Args:\n            img:\n\n        Returns:\n\n        """"""\n\n        with torch.no_grad():\n            if self.has_detector:\n                face_cropped, valid_ids = self.detect_face(img)\n            else:\n                face_cropped = self.preprocess(img)\n                valid_ids = list(range(img.shape[0]))\n\n            # print(face_cropped.shape, face_cropped.max(), face_cropped.min())\n            # Calculate embedding (unsqueeze to add batch dimension)\n            img_embedding = self.model_zoos[self.FACE_RECOGNITION](face_cropped, normalize=False)\n\n            img_embedding = img_embedding.cpu().numpy()\n\n        return img_embedding, valid_ids\n\n    def calculate_score(self, preds, gts, batch_size=32):\n        pred_feats = []\n        gt_feats = []\n\n        length = len(preds)\n        for i in range(int(math.ceil((length / batch_size)))):\n            pred_batch = preds[i * batch_size: (i + 1) * batch_size]\n            gt_batch = gts[i * batch_size: (i + 1) * batch_size]\n\n            pred_f, _ = self.forward(pred_batch)\n            gt_f, valid_ids = self.forward(gt_batch)\n\n            pred_feats.append(pred_f[valid_ids])\n            gt_feats.append(gt_f[valid_ids])\n\n        pred_feats = np.concatenate(pred_feats, axis=0)\n        gt_feats = np.concatenate(gt_feats, axis=0)\n\n        return self.cosine_similarity(pred_feats, gt_feats, norm_first=True)\n\n    def quality(self):\n        return self.HIGHER\n\n\nclass FaceFrechetDistance(FaceSimilarityScore):\n    def __init__(self, device=torch.device(""cpu""), has_detector=True):\n        super().__init__(device, has_detector)\n\n    def calculate_score(self, preds, gts, batch_size=32):\n        pred_feats = []\n        gt_feats = []\n\n        length = len(preds)\n        for i in range(int(math.ceil((length / batch_size)))):\n            pred_batch = preds[i * batch_size: (i + 1) * batch_size]\n            gt_batch = gts[i * batch_size: (i + 1) * batch_size]\n\n            pred_f = self.forward(pred_batch)\n            gt_f = self.forward(gt_batch)\n\n            pred_feats.append(pred_f)\n            gt_feats.append(gt_f)\n\n        pred_feats = np.concatenate(pred_feats, axis=0)\n        gt_feats = np.concatenate(gt_feats, axis=0)\n\n        return self.fid_score_func(pred_feats, gt_feats)\n\n    def quality(self):\n        return self.LOWER\n\n\nclass ScaleShapePoseError(BaseMetric):\n    def __init__(self, device=torch.device(""cpu"")):\n        super().__init__(device)\n        self.register_model(self.HMR)\n\n        self.height, self.width = 224, 224\n\n    def quality(self):\n        return self.LOWER\n\n    def preprocess(self, x):\n        """"""\n\n        Args:\n            x (np.ndarray or torch.tensor): np.ndarray or torch.tensor, each element is [3, image_size, image_size]\n                        wth np.float32 and color intensity [0, 1];\n\n        Returns:\n            out (torch.tensor): (bs, 3, 224, 224) is in the range of [-1, 1].\n        """"""\n\n        x = x.clone()\n        x *= 2\n        x -= 1\n        x = F.interpolate(x, (self.height, self.width), mode=""bilinear"", align_corners=False)\n        x = x.to(self.device)\n        return x\n\n    def forward(self, pred):\n        """"""\n\n        Args:\n            x (torch.tensor): np.ndarray or torch.tensor, each element is\n                [bs, 3, image_size, image_size] wth torch.uint8 and color intensity [0, 255];\n\n        Returns:\n            smpls (np.ndarray): (bs, 85).\n        """"""\n\n        with torch.no_grad():\n            pred = self.preprocess(pred)\n            smpls = self.model_zoos[self.HMR](pred)\n            smpls = smpls.cpu().numpy()\n        return smpls\n\n    def calculate_score(self, preds, gts, batch_size=32):\n        pred_smpls = []\n        gt_smpls = []\n\n        length = len(preds)\n        for i in range(int(math.ceil((length / batch_size)))):\n            pred_batch = preds[i * batch_size: (i + 1) * batch_size]\n            gt_batch = gts[i * batch_size: (i + 1) * batch_size]\n\n            pred_f = self.forward(pred_batch)\n            gt_f = self.forward(gt_batch)\n\n            pred_smpls.append(pred_f)\n            gt_smpls.append(gt_f)\n\n        pred_smpls = np.concatenate(pred_smpls, axis=0)\n        gt_smpls = np.concatenate(gt_smpls, axis=0)\n\n        return self.ssp_abs_err_score_func(pred_smpls, gt_smpls)\n'"
thirdparty/his_evaluators/his_evaluators/protocols/MotionSynthetic.py,0,"b'import os\nimport glob\nimport numpy as np\n\nfrom .protocol import Protocol\nfrom ..utils.io import load_json_file, load_pickle_file\n\n\nclass MotionSyntheticProtocol(Protocol):\n\n    def __init__(self, data_dir=""/home/piaozx/liuwen/p300/human_pose/processed""):\n        super().__init__()\n\n        # the root directory of iPER, need to be replaced!\n        self.data_dir = data_dir\n        self.processed_dir = os.path.join(data_dir, ""processed"")\n        self.train_ids_file = ""train.txt""\n        self.test_ids_file = ""val.txt""\n        self.eval_path = ""MS_protocol.json""\n\n        """"""\n        ""001/9/1"": {\n          ""source"": [""000.jpg"", ""035.jpg"", ""091.jpg"", ""120.jpg"", ""155.jpg"", ""195.jpg"", ""219.jpg"", ""251.jpg""],\n          ""view angle"": [0, 45, 90, 135, 180, 225, 270, 315],\n          ""s_n"" : {\n              ""1"": [""000.jpg""],\n              ""2"": [""000.jpg"", ""155.jpg""],\n              ""4"": [""000.jpg"", ""091.jpg"", ""155.jpg"", ""219.jpg""],\n              ""8"": [""000.jpg"", ""035.jpg"", ""091.jpg"", ""120.jpg"", ""155.jpg"", ""195.jpg"", ""219.jpg"", ""251.jpg""]\n          },\n          ""mask"": [],\n          ""novel view"": false,\n          ""self_imitation"": {\n            ""target"": ""001/9/1"",\n            ""range"": [0, 300]\n          },\n          ""cross_imitation"": {\n            ""target"": ""007/1/2"",\n            ""range"": [180, 255]\n          },\n          ""flag"": [180, 255]\n        },\n        \n        """"""\n\n        full_eval_path = os.path.join(\n            os.path.abspath(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))),\n            ""data"", ""MS_protocol.json""\n        )\n\n        self.eval_info = load_json_file(full_eval_path)[""val""]\n        self.vid_names = list(self.eval_info.keys())\n\n        self._all_vid_smpls = {}\n        self._all_vid_offsets = {}\n        self._all_vid_kps = {}\n\n        # setups\n        self._num_source = 1\n        self._load_smpls = False\n        self._load_kps = False\n\n    def __len__(self):\n        return len(self.vid_names)\n\n    def take_images_paths(self, vid_name, start, end):\n        """"""\n        Args:\n            vid_name:\n            start:\n            end:\n\n        Returns:\n\n        """"""\n        vid_path = os.path.join(self.processed_dir, vid_name, ""images"")\n        vid_images_paths = glob.glob(os.path.join(vid_path, ""*""))\n        vid_images_paths.sort()\n        images_paths = vid_images_paths[start: end + 1]\n        return images_paths\n\n    def setup(self, num_sources=1, load_smpls=False, load_kps=False):\n        self._num_source = num_sources\n        self._load_smpls = load_smpls\n        self._load_kps = load_kps\n\n    def __getitem__(self, item):\n        """"""\n\n        Args:\n            item:\n\n        Returns:\n            eval_info (dict): the information for evaluation, it contains:\n\n                --source (dict):\n                    --s_n (str): dict map of from number of source (s_n) to source images\n                    --name (str): the video name of source (`001/9/1`)\n                    --formated_name (str): the formated video name of source (`001_9_1`);\n                    --vid_path (str):\n                    --images (list of str):\n                    --smpls (np.ndarray or None):\n                    --kps (np.ndarray or None):\n\n                --self_imitation (dict):\n                    --images (list of str):\n                    --smpls (np.ndarray or None):\n                    --kps (np.ndarray or None):\n                    --self_imitation (bool): True\n\n                --cross_imitation (dict):\n                    --images (list of str):\n                    --smpls (np.ndarray or None):\n                    --kps (np.ndarray or None):\n                    --self_imitation (bool): False\n\n                --flag (list of str):\n        """"""\n\n        num_sources = self._num_source\n        load_smpls = self._load_smpls\n        load_kps = self._load_kps\n\n        vid_name = self.vid_names[item]\n        vid_info = self.eval_info[vid_name]\n\n        eval_info = dict()\n\n        # 1. source information\n        src_vid_smpls = self.get_smpls(vid_name)\n        src_vid_kps = self.get_kps(vid_name)\n\n        src_vid_path = os.path.join(self.processed_dir, vid_name, ""images"")\n        src_img_paths = glob.glob(os.path.join(src_vid_path, ""*""))\n        src_img_paths.sort()\n\n        src_img_names = vid_info[""s_n""][str(num_sources)]\n\n        # ""frame_00000000.png"" -> ""00000000"" -> int(""00000000"")\n        src_img_ids = [int(t.split(""."")[0].split(\'_\')[-1]) for t in src_img_names]\n        eval_info[""source""] = {\n            ""s_n"": num_sources,\n            ""name"": vid_name,\n            ""formated_name"": self.format_name(vid_name),\n            ""vid_path"": os.path.join(self.processed_dir, vid_name, ""images""),\n            ""images"": [src_img_paths[t] for t in src_img_ids],\n            ""smpls"": src_vid_smpls[src_img_ids] if load_smpls else None,\n            ""kps"": src_vid_kps[src_img_ids] if load_kps else None\n        }\n\n        # 2. self-imitation\n        self_imitation = vid_info[""self_imitation""]\n\n        eval_info[""self_imitation""] = {\n            ""name"": self_imitation[""target""],\n            ""formated_name"": self.format_name(self_imitation[""target""]),\n            ""images"": src_img_paths[self_imitation[""range""][0]: self_imitation[""range""][1] + 1],\n            ""smpls"": src_vid_smpls[self_imitation[""range""][0]: self_imitation[""range""][1] + 1] if load_smpls else None,\n            ""kps"": src_vid_kps[self_imitation[""range""][0]: self_imitation[""range""][1] + 1] if load_kps else None,\n            ""self_imitation"": True\n        }\n\n        # 2. cross-imitation\n        cross_imitation = vid_info[""cross_imitation""]\n        target_vid_name = cross_imitation[""target""]\n        target_vid_smpls = self.get_smpls(target_vid_name)\n        target_vid_kps = self.get_kps(target_vid_name)\n        cross_images_paths = self.take_images_paths(\n            vid_name=target_vid_name,\n            start=cross_imitation[""range""][0],\n            end=cross_imitation[""range""][1]\n        )\n        eval_info[""cross_imitation""] = {\n            ""name"": target_vid_name,\n            ""formated_name"": self.format_name(target_vid_name),\n            ""images"": cross_images_paths,\n            ""smpls"": target_vid_smpls[\n                     cross_imitation[""range""][0]: cross_imitation[""range""][1] + 1] if load_smpls else None,\n            ""kps"": target_vid_kps[\n                   cross_imitation[""range""][0]: cross_imitation[""range""][1] + 1] if load_kps else None,\n            ""self_imitation"": False\n        }\n\n        eval_info[""flag""] = self.take_images_paths(\n            vid_name=vid_name,\n            start=vid_info[""flag""][0],\n            end=vid_info[""flag""][1]\n        )\n\n        # print(vid_name, cross_imitation[""range""][1] - cross_imitation[""range""][0],\n        #       vid_info[""flag""][1] - vid_info[""flag""][0])\n        assert cross_imitation[""range""][1] - cross_imitation[""range""][0] == vid_info[""flag""][1] - vid_info[""flag""][0]\n\n        return eval_info\n\n    def get_smpl_path(self, name):\n        """"""\n\n        Args:\n            name (str):\n\n        Returns:\n            smpl_path (str):\n        """"""\n\n        smpl_path = os.path.join(self.processed_dir, name, ""pose_shape.pkl"")\n        return smpl_path\n\n    def get_kps_path(self, name):\n        """"""\n\n        Args:\n            name (str):\n\n        Returns:\n            kps_path (str):\n        """"""\n        smpl_path = os.path.join(self.processed_dir, name, ""kps.pkl"")\n        return smpl_path\n\n    def get_smpls(self, name):\n        smpls = None\n        if name in self.eval_info:\n            if name not in self._all_vid_smpls:\n                smpl_path = self.get_smpl_path(name)\n                smpl_data = load_pickle_file(smpl_path)\n                cams = smpl_data[\'cams\']\n                thetas = smpl_data[\'pose\']\n                betas = np.repeat(smpl_data[""shape""], cams.shape[0], axis=0)\n                smpls = np.concatenate([cams, thetas, betas], axis=1)\n                self._all_vid_smpls[name] = smpls\n            else:\n                smpls = self._all_vid_smpls[name]\n\n        return smpls\n\n    def get_kps(self, name):\n        kps = None\n        if name in self.eval_info:\n            if name not in self._all_vid_kps:\n                kps_path = self.get_kps_path(name)\n                kps = load_pickle_file(kps_path)\n                self._all_vid_kps[name] = kps\n            else:\n                kps = self._all_vid_kps[name]\n        return kps\n\n    @property\n    def total_frames(self):\n        total = 0\n        for vid_name, vid_info in self.eval_info.items():\n            src_vid = os.path.join(self.processed_dir, vid_name)\n            length = len(os.listdir(src_vid))\n            total += length\n        return total\n\n'"
thirdparty/his_evaluators/his_evaluators/protocols/__init__.py,0,"b'\n\nVALID_DATASET = [""iPER"", ""iPER_ICCV"", ""MotionSynthetic""]\n\n\ndef create_dataset_protocols(dataset, data_dir):\n    assert dataset in VALID_DATASET\n\n    if dataset == ""iPER"":\n        from .iPER import IPERProtocol\n        return IPERProtocol(data_dir)\n\n    elif dataset == ""iPER_ICCV"":\n        from .iPER import ICCVIPERProtocol\n        return ICCVIPERProtocol(data_dir)\n\n    elif dataset == ""MotionSynthetic"":\n        from .MotionSynthetic import MotionSyntheticProtocol\n        return MotionSyntheticProtocol(data_dir)\n\n    else:\n        raise ValueError(""{} must be in {}"".format(dataset, VALID_DATASET))\n'"
thirdparty/his_evaluators/his_evaluators/protocols/iPER.py,0,"b'import os\nimport glob\nimport numpy as np\n\nfrom .protocol import Protocol\nfrom ..utils.io import load_json_file, load_pickle_file\n\n\nclass IPERProtocol(Protocol):\n\n    def __init__(self, data_dir=""/p300/iPER""):\n        super().__init__()\n\n        # the root directory of iPER, need to be replaced!\n        self.data_dir = data_dir\n        self.train_ids_file = ""train.txt""\n        self.test_ids_file = ""val.txt""\n        self.eval_path = ""iPER_protocol.json""\n        self.images_folder = ""images_HD""\n        self.smpls_folder = ""smpls""\n\n        """"""\n        ""001/9/1"": {\n          ""source"": [""000.jpg"", ""035.jpg"", ""091.jpg"", ""120.jpg"", ""155.jpg"", ""195.jpg"", ""219.jpg"", ""251.jpg""],\n          ""view angle"": [0, 45, 90, 135, 180, 225, 270, 315],\n          ""s_n"" : {\n              ""1"": [""000.jpg""],\n              ""2"": [""000.jpg"", ""155.jpg""],\n              ""4"": [""000.jpg"", ""091.jpg"", ""155.jpg"", ""219.jpg""],\n              ""8"": [""000.jpg"", ""035.jpg"", ""091.jpg"", ""120.jpg"", ""155.jpg"", ""195.jpg"", ""219.jpg"", ""251.jpg""]\n          },\n          ""mask"": [],\n          ""novel view"": false,\n          ""self_imitation"": {\n            ""target"": ""001/9/1"",\n            ""range"": [0, 300]\n          },\n          ""cross_imitation"": {\n            ""target"": ""007/1/2"",\n            ""range"": [180, 255]\n          },\n          ""flag"": [180, 255]\n        },\n        \n        """"""\n\n        full_eval_path = os.path.join(\n            os.path.abspath(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))),\n            ""data"", ""iPER_protocol.json""\n        )\n\n        self.eval_info = load_json_file(full_eval_path)[""val""]\n        self.vid_names = list(self.eval_info.keys())\n\n        self._all_vid_smpls = {}\n        self._all_vid_kps = {}\n\n        # setups\n        self._num_source = 1\n        self._load_smpls = False\n        self._load_kps = False\n\n    def __len__(self):\n        return len(self.vid_names)\n\n    def take_images_paths(self, vid_name, start, end):\n        """"""\n        Args:\n            vid_name:\n            start:\n            end:\n\n        Returns:\n\n        """"""\n        vid_path = os.path.join(self.data_dir, self.images_folder, vid_name)\n        vid_images_paths = glob.glob(os.path.join(vid_path, ""*""))\n        vid_images_paths.sort()\n        images_paths = vid_images_paths[start: end + 1]\n        return images_paths\n\n    def setup(self, num_sources=1, load_smpls=False, load_kps=False):\n        self._num_source = num_sources\n        self._load_smpls = load_smpls\n        self._load_kps = load_kps\n\n    def __getitem__(self, item):\n        """"""\n\n        Args:\n            item:\n\n        Returns:\n            eval_info (dict): the information for evaluation, it contains:\n\n                --source (dict):\n                    --s_n (str): dict map of from number of source (s_n) to source images\n                    --name (str): the video name of source (`001/9/1`)\n                    --formated_name (str): the formated video name of source (`001_9_1`);\n                    --vid_path (str):\n                    --images (list of str):\n                    --smpls (np.ndarray or None):\n                    --kps (np.ndarray or None):\n\n                --self_imitation (dict):\n                    --images (list of str):\n                    --smpls (np.ndarray or None):\n                    --kps (np.ndarray or None):\n                    --self_imitation (bool): True\n\n                --cross_imitation (dict):\n                    --images (list of str):\n                    --smpls (np.ndarray or None):\n                    --kps (np.ndarray or None):\n                    --self_imitation (bool): False\n\n                --flag (list of str):\n        """"""\n\n        num_sources = self._num_source\n        load_smpls = self._load_smpls\n        load_kps = self._load_kps\n\n        vid_name = self.vid_names[item]\n        vid_info = self.eval_info[vid_name]\n\n        eval_info = dict()\n\n        # 1. source information\n        src_vid_smpls = self.get_smpls(vid_name)\n        src_vid_kps = self.get_kps(vid_name)\n\n        src_vid_path = os.path.join(self.data_dir, self.images_folder, vid_name)\n        src_img_paths = glob.glob(os.path.join(src_vid_path, ""*""))\n        src_img_paths.sort()\n\n        src_img_names = vid_info[""s_n""][str(num_sources)]\n        src_img_ids = [int(t.split(""."")[0]) for t in src_img_names]\n        eval_info[""source""] = {\n            ""s_n"": num_sources,\n            ""name"": vid_name,\n            ""formated_name"": self.format_name(vid_name),\n            ""vid_path"": os.path.join(self.data_dir, self.images_folder, vid_name),\n            ""images"": [src_img_paths[t] for t in src_img_ids],\n            ""smpls"": src_vid_smpls[src_img_ids] if load_smpls else None,\n            ""kps"": src_vid_kps[src_img_ids] if load_kps else None\n        }\n\n        # 2. self-imitation\n        self_imitation = vid_info[""self_imitation""]\n\n        eval_info[""self_imitation""] = {\n            ""name"": self_imitation[""target""],\n            ""formated_name"": self.format_name(self_imitation[""target""]),\n            ""images"": src_img_paths[self_imitation[""range""][0]: self_imitation[""range""][1] + 1],\n            ""smpls"": src_vid_smpls[self_imitation[""range""][0]: self_imitation[""range""][1] + 1] if load_smpls else None,\n            ""kps"": src_vid_kps[self_imitation[""range""][0]: self_imitation[""range""][1] + 1] if load_kps else None,\n            ""self_imitation"": True\n        }\n\n        # 2. cross-imitation\n        cross_imitation = vid_info[""cross_imitation""]\n        target_vid_name = cross_imitation[""target""]\n        target_vid_smpls = self.get_smpls(target_vid_name)\n        target_vid_kps = self.get_kps(target_vid_name)\n        cross_images_paths = self.take_images_paths(\n            vid_name=target_vid_name,\n            start=cross_imitation[""range""][0],\n            end=cross_imitation[""range""][1]\n        )\n        eval_info[""cross_imitation""] = {\n            ""name"": target_vid_name,\n            ""formated_name"": self.format_name(target_vid_name),\n            ""images"": cross_images_paths,\n            ""smpls"": target_vid_smpls[\n                     cross_imitation[""range""][0]: cross_imitation[""range""][1] + 1] if load_smpls else None,\n            ""kps"": target_vid_kps[\n                   cross_imitation[""range""][0]: cross_imitation[""range""][1] + 1] if load_kps else None,\n            ""self_imitation"": False\n        }\n\n        eval_info[""flag""] = self.take_images_paths(\n            vid_name=vid_name,\n            start=vid_info[""flag""][0],\n            end=vid_info[""flag""][1]\n        )\n\n        # print(vid_name, cross_imitation[""range""][1] - cross_imitation[""range""][0],\n        #       vid_info[""flag""][1] - vid_info[""flag""][0])\n        assert cross_imitation[""range""][1] - cross_imitation[""range""][0] == vid_info[""flag""][1] - vid_info[""flag""][0]\n        return eval_info\n\n    def format_name(self, name):\n        """"""\n            convert `001/9/1` to `001_9_1`\n        Args:\n            name (str): such as `001/9/1`.\n\n        Returns:\n            formated_name (str): such as `001_9_1`\n        """"""\n\n        formated_name = ""_"".join(name.split(""/""))\n        return formated_name\n\n    def original_name(self, formated_name):\n        """"""\n\n        Args:\n            formated_name:\n\n        Returns:\n\n        """"""\n        original_name = ""/"".join(formated_name.split(""_""))\n\n        return original_name\n\n    def get_smpl_path(self, name):\n        """"""\n\n        Args:\n            name (str): such as `001/9/1`.\n\n        Returns:\n            smpl_path (str):\n        """"""\n\n        smpl_path = os.path.join(self.data_dir, self.smpls_folder, name, ""pose_shape.pkl"")\n        return smpl_path\n\n    def get_kps_path(self, name):\n        """"""\n\n        Args:\n            name (str): such as `001/9/1`.\n\n        Returns:\n            kps_path (str):\n        """"""\n        smpl_path = os.path.join(self.data_dir, self.smpls_folder, name, ""kps.pkl"")\n        return smpl_path\n\n    def get_smpls(self, name):\n        smpls = None\n        if name in self.eval_info:\n            if name not in self._all_vid_smpls:\n                smpl_path = self.get_smpl_path(name)\n                smpl_data = load_pickle_file(smpl_path)\n                cams = smpl_data[\'cams\']\n                thetas = smpl_data[\'pose\']\n                betas = smpl_data[""shape""]\n                smpls = np.concatenate([cams, thetas, betas], axis=1)\n                self._all_vid_smpls[name] = smpls\n            else:\n                smpls = self._all_vid_smpls[name]\n\n        return smpls\n\n    def get_kps(self, name):\n        kps = None\n        if name in self.eval_info:\n            if name not in self._all_vid_kps:\n                kps_path = self.get_kps_path(name)\n                kps = load_pickle_file(kps_path)[""kps""]\n                self._all_vid_kps[name] = kps\n            else:\n                kps = self._all_vid_kps[name]\n        return kps\n\n    @property\n    def total_frames(self):\n        total = 0\n        for vid_name, vid_info in self.eval_info.items():\n            src_vid = os.path.join(self.data_dir, self.images_folder, vid_name)\n            length = len(os.listdir(src_vid))\n            total += length\n        return total\n\n\nclass ICCVIPERProtocol(IPERProtocol):\n    def __init__(self, data_dir=""/p300/iPER""):\n        super().__init__(data_dir)\n\n        self.NUM_SOURCES = 3\n\n    def __len__(self):\n        return len(self.vid_names) * self.NUM_SOURCES\n\n    def __getitem__(self, item):\n        """"""\n\n        Args:\n            item:\n\n        Returns:\n            eval_info (dict): the information for evaluation, it contains:\n\n                --source (dict):\n                    --s_n (str): dict map of from number of source (s_n) to source images\n                    --name (str): the video name of source (`001/9/1`)\n                    --formated_name (str): the formated video name of source (`001_9_1`);\n                    --vid_path (str):\n                    --images (list of str):\n                    --smpls (np.ndarray or None):\n                    --kps (np.ndarray or None):\n\n                --self_imitation (dict):\n                    --images (list of str):\n                    --smpls (np.ndarray or None):\n                    --kps (np.ndarray or None):\n                    --self_imitation (bool): True\n\n                --cross_imitation (dict):\n                    --images (list of str):\n                    --smpls (np.ndarray or None):\n                    --kps (np.ndarray or None):\n                    --self_imitation (bool): False\n\n                --flag (list of str):\n        """"""\n        self._num_source = 1\n        self._load_smpls = True\n        self._load_kps = True\n        num_sources = self._num_source\n        load_smpls = self._load_smpls\n        load_kps = self._load_kps\n\n        vid_ids = item // 3\n        src_ids = item % 3\n\n        vid_name = self.vid_names[vid_ids]\n        vid_info = self.eval_info[vid_name]\n\n        eval_info = dict()\n\n        # 1. source information\n        src_vid_smpls = self.get_smpls(vid_name)\n        src_vid_kps = self.get_kps(vid_name)\n\n        src_vid_path = os.path.join(self.data_dir, self.images_folder, vid_name)\n        src_img_paths = glob.glob(os.path.join(src_vid_path, ""*""))\n        src_img_paths.sort()\n\n        # num_source = 3\n        src_img_names = vid_info[""s_n""][str(self.NUM_SOURCES)][src_ids:src_ids+1]\n        src_img_ids = [int(t.split(""."")[0]) for t in src_img_names]\n        eval_info[""source""] = {\n            ""s_n"": num_sources,\n            ""name"": vid_name,\n            ""formated_name"": self.format_name(vid_name),\n            ""vid_path"": os.path.join(self.data_dir, self.images_folder, vid_name),\n            ""images"": [src_img_paths[t] for t in src_img_ids],\n            ""smpls"": src_vid_smpls[src_img_ids] if load_smpls else None,\n            ""kps"": src_vid_kps[src_img_ids] if load_kps else None\n        }\n\n        # 2. self-imitation\n        self_imitation = vid_info[""self_imitation""]\n\n        eval_info[""self_imitation""] = {\n            ""name"": self_imitation[""target""],\n            ""formated_name"": self.format_name(self_imitation[""target""]),\n            ""images"": src_img_paths[self_imitation[""range""][0]: self_imitation[""range""][1] + 1],\n            ""smpls"": src_vid_smpls[self_imitation[""range""][0]: self_imitation[""range""][1] + 1] if load_smpls else None,\n            ""kps"": src_vid_kps[self_imitation[""range""][0]: self_imitation[""range""][1] + 1] if load_kps else None,\n            ""self_imitation"": True\n        }\n\n        # 2. cross-imitation\n        cross_imitation = vid_info[""cross_imitation""]\n        target_vid_name = cross_imitation[""target""]\n        target_vid_smpls = self.get_smpls(target_vid_name)\n        target_vid_kps = self.get_kps(target_vid_name)\n        cross_images_paths = self.take_images_paths(\n            vid_name=target_vid_name,\n            start=cross_imitation[""range""][0],\n            end=cross_imitation[""range""][1]\n        )\n        eval_info[""cross_imitation""] = {\n            ""name"": target_vid_name,\n            ""formated_name"": self.format_name(target_vid_name),\n            ""images"": cross_images_paths,\n            ""smpls"": target_vid_smpls[\n                     cross_imitation[""range""][0]: cross_imitation[""range""][1] + 1] if load_smpls else None,\n            ""kps"": target_vid_kps[\n                   cross_imitation[""range""][0]: cross_imitation[""range""][1] + 1] if load_kps else None,\n            ""self_imitation"": False\n        }\n\n        eval_info[""flag""] = self.take_images_paths(\n            vid_name=vid_name,\n            start=vid_info[""flag""][0],\n            end=vid_info[""flag""][1]\n        )\n\n        # print(vid_name, cross_imitation[""range""][1] - cross_imitation[""range""][0],\n        #       vid_info[""flag""][1] - vid_info[""flag""][0])\n        assert cross_imitation[""range""][1] - cross_imitation[""range""][0] == vid_info[""flag""][1] - vid_info[""flag""][0]\n        return eval_info\n'"
thirdparty/his_evaluators/his_evaluators/protocols/protocol.py,0,"b""class Protocol(object):\n\n    def __str__(self):\n        _str = '<================ Constants information ================>\\n'\n        for name, value in self.__dict__.items():\n            print(name, value)\n            _str += '\\t{}\\t{}\\n'.format(name, value)\n\n        return _str\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __getitem__(self, item):\n        raise NotImplementedError\n\n    def format_name(self, name):\n        return name\n\n    def original_name(self, name):\n        return name\n"""
thirdparty/his_evaluators/his_evaluators/utils/__init__.py,0,b''
thirdparty/his_evaluators/his_evaluators/utils/io.py,0,"b'import json\nimport cv2\nimport numpy as np\nimport pickle\nimport os\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return path\n\n\ndef load_json_file(json_file):\n    with open(json_file, \'r\') as f:\n        data = json.load(f)\n    return data\n\n\ndef load_pickle_file(pkl_path):\n    with open(pkl_path, \'rb\') as f:\n        data = pickle.load(f, encoding=\'latin1\')\n\n    return data\n\n\ndef load_img(img_path, image_size):\n    """"""\n        load image from `img_path` and resize it to (image_size, image_size), convert to RGB color space.\n    Args:\n        img_path:\n        image_size:\n\n    Returns:\n        img (np.ndarray): [3, image_size, image_size], np.float32, RGB channel, [0, 1] intensity.\n    """"""\n    img = cv2.imread(img_path)\n\n    if img.shape[0] != image_size or img.shape[1] != image_size:\n        img = cv2.resize(img, (image_size, image_size))\n\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = np.transpose(img, (2, 0, 1))\n    img = img.astype(np.float32, copy=False)\n    img /= 255\n    return img\n'"
thirdparty/neural_renderer/neural_renderer/cuda/__init__.py,0,b''
thirdparty/neural_renderer/neural_renderer/cuda/create_texture_image.py,0,"b""def __bootstrap__():\n    global __bootstrap__, __loader__, __file__\n    import sys, pkg_resources, imp\n    __file__ = pkg_resources.resource_filename(__name__, 'create_texture_image.cpython-36m-x86_64-linux-gnu.so')\n    __loader__ = None; del __bootstrap__, __loader__\n    imp.load_dynamic(__name__,__file__)\n__bootstrap__()\n"""
thirdparty/neural_renderer/neural_renderer/cuda/load_textures.py,0,"b""def __bootstrap__():\n    global __bootstrap__, __loader__, __file__\n    import sys, pkg_resources, imp\n    __file__ = pkg_resources.resource_filename(__name__, 'load_textures.cpython-36m-x86_64-linux-gnu.so')\n    __loader__ = None; del __bootstrap__, __loader__\n    imp.load_dynamic(__name__,__file__)\n__bootstrap__()\n"""
thirdparty/neural_renderer/neural_renderer/cuda/rasterize.py,0,"b""def __bootstrap__():\n    global __bootstrap__, __loader__, __file__\n    import sys, pkg_resources, imp\n    __file__ = pkg_resources.resource_filename(__name__, 'rasterize.cpython-36m-x86_64-linux-gnu.so')\n    __loader__ = None; del __bootstrap__, __loader__\n    imp.load_dynamic(__name__,__file__)\n__bootstrap__()\n"""
thirdparty/his_evaluators/his_evaluators/metrics/OSreid/OSNet.py,1,"b'__all__ = [\'osnet_x1_0\', \'osnet_x0_75\', \'osnet_x0_5\', \'osnet_x0_25\', \'osnet_ibn_x1_0\']\n\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\n##########\n# Basic layers\n##########\nclass ConvLayer(nn.Module):\n    """"""Convolution layer (conv + bn + relu).""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, IN=False):\n        super(ConvLayer, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride,\n                              padding=padding, bias=False, groups=groups)\n        if IN:\n            self.bn = nn.InstanceNorm2d(out_channels, affine=True)\n        else:\n            self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Conv1x1(nn.Module):\n    """"""1x1 convolution + bn + relu.""""""\n\n    def __init__(self, in_channels, out_channels, stride=1, groups=1):\n        super(Conv1x1, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 1, stride=stride, padding=0,\n                              bias=False, groups=groups)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Conv1x1Linear(nn.Module):\n    """"""1x1 convolution + bn (w/o non-linearity).""""""\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(Conv1x1Linear, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 1, stride=stride, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass Conv3x3(nn.Module):\n    """"""3x3 convolution + bn + relu.""""""\n\n    def __init__(self, in_channels, out_channels, stride=1, groups=1):\n        super(Conv3x3, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1,\n                              bias=False, groups=groups)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass LightConv3x3(nn.Module):\n    """"""Lightweight 3x3 convolution.\n\n    1x1 (linear) + dw 3x3 (nonlinear).\n    """"""\n\n    def __init__(self, in_channels, out_channels):\n        super(LightConv3x3, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False, groups=out_channels)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\n##########\n# Building blocks for omni-scale feature learning\n##########\nclass ChannelGate(nn.Module):\n    """"""A mini-network that generates channel-wise gates conditioned on input tensor.""""""\n\n    def __init__(self, in_channels, num_gates=None, return_gates=False,\n                 gate_activation=\'sigmoid\', reduction=16, layer_norm=False):\n        super(ChannelGate, self).__init__()\n        if num_gates is None:\n            num_gates = in_channels\n        self.return_gates = return_gates\n        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(in_channels, in_channels//reduction, kernel_size=1, bias=True, padding=0)\n        self.norm1 = None\n        if layer_norm:\n            self.norm1 = nn.LayerNorm((in_channels//reduction, 1, 1))\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(in_channels//reduction, num_gates, kernel_size=1, bias=True, padding=0)\n        if gate_activation == \'sigmoid\':\n            self.gate_activation = nn.Sigmoid()\n        elif gate_activation == \'relu\':\n            self.gate_activation = nn.ReLU(inplace=True)\n        elif gate_activation == \'linear\':\n            self.gate_activation = None\n        else:\n            raise RuntimeError(""Unknown gate activation: {}"".format(gate_activation))\n\n    def forward(self, x):\n        input = x\n        x = self.global_avgpool(x)\n        x = self.fc1(x)\n        if self.norm1 is not None:\n            x = self.norm1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        if self.gate_activation is not None:\n            x = self.gate_activation(x)\n        if self.return_gates:\n            return x\n        return input * x\n\n\nclass OSBlock(nn.Module):\n    """"""Omni-scale feature learning block.""""""\n\n    def __init__(self, in_channels, out_channels, IN=False, bottleneck_reduction=4, **kwargs):\n        super(OSBlock, self).__init__()\n        mid_channels = out_channels // bottleneck_reduction\n        self.conv1 = Conv1x1(in_channels, mid_channels)\n        self.conv2a = LightConv3x3(mid_channels, mid_channels)\n        self.conv2b = nn.Sequential(\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n        )\n        self.conv2c = nn.Sequential(\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n        )\n        self.conv2d = nn.Sequential(\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n            LightConv3x3(mid_channels, mid_channels),\n        )\n        self.gate = ChannelGate(mid_channels)\n        self.conv3 = Conv1x1Linear(mid_channels, out_channels)\n        self.downsample = None\n        if in_channels != out_channels:\n            self.downsample = Conv1x1Linear(in_channels, out_channels)\n        self.IN = None\n        if IN:\n            self.IN = nn.InstanceNorm2d(out_channels, affine=True)\n\n    def forward(self, x):\n        residual = x\n        x1 = self.conv1(x)\n        x2a = self.conv2a(x1)\n        x2b = self.conv2b(x1)\n        x2c = self.conv2c(x1)\n        x2d = self.conv2d(x1)\n        x2 = self.gate(x2a) + self.gate(x2b) + self.gate(x2c) + self.gate(x2d)\n        x3 = self.conv3(x2)\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n        out = x3 + residual\n        if self.IN is not None:\n            out = self.IN(out)\n        return F.relu(out)\n\n\n##########\n# Network architecture\n##########\nclass OSNet(nn.Module):\n    """"""Omni-Scale Network.\n\n    Reference:\n        - Zhou et al. Omni-Scale Feature Learning for Person Re-Identification. ArXiv preprint, 2019.\n          https://arxiv.org/abs/1905.00953\n    """"""\n\n    def __init__(self, num_classes, blocks, layers, channels, feature_dim=512, loss=\'softmax\', IN=False, **kwargs):\n        super(OSNet, self).__init__()\n        num_blocks = len(blocks)\n        assert num_blocks == len(layers)\n        assert num_blocks == len(channels) - 1\n        self.loss = loss\n\n        # convolutional backbone\n        self.conv1 = ConvLayer(3, channels[0], 7, stride=2, padding=3, IN=IN)\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n        self.conv2 = self._make_layer(blocks[0], layers[0], channels[0], channels[1], reduce_spatial_size=True, IN=IN)\n        self.conv3 = self._make_layer(blocks[1], layers[1], channels[1], channels[2], reduce_spatial_size=True)\n        self.conv4 = self._make_layer(blocks[2], layers[2], channels[2], channels[3], reduce_spatial_size=False)\n        self.conv5 = Conv1x1(channels[3], channels[3])\n        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n        # fully connected layer\n        self.fc = self._construct_fc_layer(feature_dim, channels[3], dropout_p=None)\n        # identity classification layer\n        self.classifier = nn.Linear(self.feature_dim, num_classes)\n\n        self._init_params()\n\n    def _make_layer(self, block, layer, in_channels, out_channels, reduce_spatial_size, IN=False):\n        layers = []\n\n        layers.append(block(in_channels, out_channels, IN=IN))\n        for i in range(1, layer):\n            layers.append(block(out_channels, out_channels, IN=IN))\n\n        if reduce_spatial_size:\n            layers.append(\n                nn.Sequential(\n                    Conv1x1(out_channels, out_channels),\n                    nn.AvgPool2d(2, stride=2)\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def _construct_fc_layer(self, fc_dims, input_dim, dropout_p=None):\n        if fc_dims is None or fc_dims<0:\n            self.feature_dim = input_dim\n            return None\n\n        if isinstance(fc_dims, int):\n            fc_dims = [fc_dims]\n\n        layers = []\n        for dim in fc_dims:\n            layers.append(nn.Linear(input_dim, dim))\n            layers.append(nn.BatchNorm1d(dim))\n            layers.append(nn.ReLU(inplace=True))\n            if dropout_p is not None:\n                layers.append(nn.Dropout(p=dropout_p))\n            input_dim = dim\n\n        self.feature_dim = fc_dims[-1]\n\n        return nn.Sequential(*layers)\n\n    def _init_params(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def featuremaps(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        return x\n\n    def forward(self, x):\n        x = self.featuremaps(x)\n        v = self.global_avgpool(x)\n        v = v.view(v.size(0), -1)\n        if self.fc is not None:\n            v = self.fc(v)\n            # print(""in fc"")\n        if not self.training:\n            return v\n        # print(""before classifier"")\n        y = self.classifier(v)\n        if self.loss == \'softmax\':\n            return y\n        elif self.loss == \'triplet\':\n            return y, v\n        else:\n            raise KeyError(""Unsupported loss: {}"".format(self.loss))\n\n\n##########\n# Instantiation\n##########\ndef osnet_x1_0(num_classes=1000, loss=\'softmax\', **kwargs):\n    # standard size (width x1.0)\n    return OSNet(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                 channels=[64, 256, 384, 512], loss=loss, **kwargs)\n\n\ndef osnet_x0_75(num_classes=1000, loss=\'softmax\', **kwargs):\n    # medium size (width x0.75)\n    return OSNet(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                 channels=[48, 192, 288, 384], loss=loss, **kwargs)\n\n\ndef osnet_x0_5(num_classes=1000, loss=\'softmax\', **kwargs):\n    # tiny size (width x0.5)\n    return OSNet(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                 channels=[32, 128, 192, 256], loss=loss, **kwargs)\n\n\ndef osnet_x0_25(num_classes=1000, loss=\'softmax\', **kwargs):\n    # very tiny size (width x0.25)\n    return OSNet(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                 channels=[16, 64, 96, 128], loss=loss, **kwargs)\n\n\ndef osnet_ibn_x1_0(num_classes=1000, loss=\'softmax\', **kwargs):\n    # standard size (width x1.0) + IBN layer\n    # Ref: Pan et al. Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net. ECCV, 2018.\n    return OSNet(num_classes, blocks=[OSBlock, OSBlock, OSBlock], layers=[2, 2, 2],\n                 channels=[64, 256, 384, 512], loss=loss, IN=True, **kwargs)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/OSreid/__init__.py,0,b'from .encoder import OsNetEncoder\n'
thirdparty/his_evaluators/his_evaluators/metrics/OSreid/encoder.py,11,"b'from typing import List, Union\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms.functional as transform_func\n\nfrom .OSNet import osnet_ibn_x1_0\nfrom .utils import load_pretrained_weights\nfrom .image_handler import normalize, resize, ndarray_to_tensor\n\n\nclass OsNetEncoder(object):\n\n    # Encoder constants\n    PRETRAINED_MODEL = False\n    LOSS = \'softmax\'\n\n    def __init__(self, input_width: int, input_height: int, weight_filepath: str, batch_size: int, num_classes: int, patch_height: int, patch_width: int, norm_mean: List[float], norm_std: List[float], GPU: bool):\n\n        self._input_width = input_width\n        self._input_height = input_height\n        self._weight_filepath = weight_filepath\n        self.batch_size = batch_size\n        self.num_classes = num_classes\n        self.patch_size = (patch_height, patch_width)\n        self.norm_mean = norm_mean\n        self.norm_std = norm_std\n        self.GPU = GPU\n        self._model = osnet_ibn_x1_0(\n            num_classes=self.num_classes,\n            loss=OsNetEncoder.LOSS,\n            pretrained=OsNetEncoder.PRETRAINED_MODEL,\n            use_gpu=self.GPU\n        )\n        self._model.eval()  # Set the torch model for evaluation\n        self.weights_loaded = load_pretrained_weights(\n            model=self._model,\n            weight_path=self._weight_filepath\n        )\n        if self.GPU:\n            self._model = self._model.cuda()\n\n    def load_image(self, patch: np.ndarray) -> torch.Tensor:\n        \'\'\' load image involves three processes: resizing, normalising and translating\n        the np.ndarray into a torch.Tensor ready for GPU.\n\n        :param patch: single detection patch, in np.ndarray format\n        :return: resized and normalised single detection tensor\n        \'\'\'\n\n        if self.GPU:\n            device = torch.device(\'cuda\')\n        else:\n            device = torch.device(\'cpu\')\n\n        resized_patch = resize(\n            img=patch,\n            size=self.patch_size\n        )\n        torch_tensor = ndarray_to_tensor(pic=resized_patch)\n\n        normalized_tensor = normalize(\n            tensor=torch_tensor,\n            mean=self.norm_mean,\n            std=self.norm_std,\n            inplace=False\n        )\n\n        # Transforms the normalised tensor to a cuda tensor or a cpu tensor wrt which device is available\n        gpu_tensor = normalized_tensor.to(device)\n\n        return gpu_tensor\n\n    def __call__(self, images: torch.tensor, bboxes: Union[torch.tensor, np.ndarray, List] = None):\n        """"""\n\n        Args:\n            images (torch.tensor): (bs, 3, height, width) is in the range [0, 1] with torch.float32.\n            bboxes (torch.tensor, np.ndarray, list or None): [(4,), (4,), ..., (4,)], (4,) = (x0, y0, x1, y1)\n\n        Returns:\n            feats (torch.tensor): (bs, dim)\n        """"""\n\n        if bboxes is None:\n            crop_imgs = F.interpolate(images, size=self.patch_size, mode=""bilinear"", align_corners=True)\n        else:\n            bs = images.shape[0]\n            crop_imgs = []\n            for i in range(bs):\n                x = images[i]\n                box = bboxes[i]\n                if box is not None:\n                    x0, y0, x1, y1 = box\n                    crop = x[:, y0:y1, x0:x1]\n                else:\n                    crop = x\n                crop = transform_func.normalize(crop, mean=self.norm_mean, std=self.norm_std)\n                crop.unsqueeze_(0)\n                crop = F.interpolate(crop, size=self.patch_size, mode=""bilinear"", align_corners=True)\n                crop_imgs.append(crop)\n            crop_imgs = torch.cat(crop_imgs, dim=0)\n\n        feats = self._model(crop_imgs)\n        return feats\n\n    def get_features(self, image_patches: List[np.ndarray]) -> List[np.ndarray]:\n\n        \'\'\' Extract the 512 features associated to each detection\n        :param image_patches: List[np.ndarray] of detections\n        :return features: List[np.ndarray] of features associated to each detection\n        \'\'\'\n\n        features = list()\n\n        for patch in image_patches:\n            if patch is not None:\n                # initial_time = time.time()\n                patch_gpu_tensor = self.load_image(patch)\n                patch_features = self._model(patch_gpu_tensor)\n                # Translating to np.ndarray avoids further issues with deepcopying torch.Tensors (in ""tracker"")\n                numpy_features = patch_features.cpu().detach().numpy()\n                features.append(numpy_features)\n\n                # numpy_features returns a list of a list of features, so we get the first entry as an action of flattening\n                # features.append(compress_feature_vector(numpy_features[0]))\n                # print(list(numpy_features[0])) # Debugging console out\n                # final_time = time.time()\n                # print(f""[INFO] {numpy_features} appended"")\n                # print(f""[PERFORMANCE] Features extracted: {1/(final_time-initial_time)} Hz"")\n            else:\n                features.append(None)\n                # print(""[INFO] None appended"")\n\n        return features\n\n'"
thirdparty/his_evaluators/his_evaluators/metrics/OSreid/image_handler.py,10,"b'from typing import List, Tuple\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.autograd import Variable\n\n\ndef resize(img: np.ndarray, size: Tuple[int, int], interpolation=Image.BILINEAR) -> Image:\n    """"""Resize the input PIL Image to the given size.\n\n    Args:\n        img ( np.ndarray Image): Image to be resized.\n        size (tuple of int): Desired output size. The size is a sequence with the structure\n            (h, w). The output size will be matched to this. `\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    Returns:\n        PIL Image: Resized image.\n    """"""\n    # Resizing needs ""PIL.Image"" objects, as scipy method was deprecated\n    # and numpy does not support these sort of image transformations\n    pil_img = Image.fromarray(img)\n    pil_img = pil_img.resize(size[::-1], interpolation)\n    return np.array(pil_img)\n\ndef ndarray_to_tensor(pic: np.ndarray) -> torch.Tensor:\n    """"""Translates the ndarray to a torch Tensor\n    :param pic: PIL Image: Resized image\n    :return: transformed torch.Tensor, required by the ""normalize()"" method\n    """"""\n\n    # Define constants\n    RGB_VALUES = 255\n\n    # handle numpy array\n    if pic.ndim == 2:\n        pic = pic[:, :, None]\n    tensor = torch.from_numpy(pic.transpose((2, 0, 1)) / RGB_VALUES)\n    return tensor\n\n\ndef normalize(tensor: torch.Tensor, mean: List[float], std: List[float], inplace=False) -> torch.Tensor:\n    """"""Normalize a tensor image with mean and standard deviation.\n\n    .. note::\n        This transform acts out of place by default, i.e., it does not mutates the input tensor.\n\n    See :class:`~torchvision.transforms.Normalize` for more details.\n\n    Args:\n        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n        inplace(bool,optional): Bool to make this operation inplace.\n\n    Returns:\n        Tensor: Normalized Tensor image.\n    """"""\n\n    if not inplace:\n        tensor = tensor.clone()\n    # Issues leaving a tensor as torch.uint8, which is the exit of the method ndarray_to_tensor(),\n    # as tensor.sub_(mean[:, None, None]).div_(std[:, None, None]) presented an issue of division by 0.\n    # Output of torchvision.transforms.compose() is a torch.float32\n    tensor = tensor.type(torch.float32)\n    dtype = tensor.dtype\n    # Transform mean and std in torch tensors\n    mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n    std = torch.as_tensor(std, dtype=dtype, device=tensor.device)\n    # Normalization is implemented\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n    # unsqueeze_() adds another dimension to the tensor, with shape  (:,:,:).\n    # The input of our net requires a (1,:,:,:) tensor.\n    tensor.unsqueeze_(0)\n    # ""autograd.Variable()"" creates tensors that support gradient calculations.\n    # Might be redundant as no backpropagation is computed in ""test"" mode.\n    return Variable(tensor)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/OSreid/main.py,0,"b'import numpy as np\nimport os\n\nfrom .encoder import OsNetEncoder\n\n\ndirpath = os.path.abspath(os.path.dirname(__file__))\n\n# Declare an encoder object\nencoder = OsNetEncoder(\n    # input_width=704,\n    # input_height=480,\n    # weight_filepath=""weights/model_weights.pth.tar-40"",\n    weight_filepath=os.path.join(dirpath, ""osnet_ibn_x1_0_imagenet.pth""),\n    input_width=512,\n    input_height=512,\n    batch_size=32,\n    num_classes=2022,\n    patch_height=256,\n    patch_width=128,\n    norm_mean=[0.485, 0.456, 0.406],\n    norm_std=[0.229, 0.224, 0.225],\n    GPU=True)\n\n\nif __name__ == \'__main__\':\n    pred_imgs = np.random.rand(5, 512, 512, 3)\n    pred_imgs = pred_imgs * 255\n    pred_imgs = pred_imgs.astype(np.uint8)\n    features = encoder.get_features(pred_imgs)\n\n\n\n'"
thirdparty/his_evaluators/his_evaluators/metrics/OSreid/utils.py,3,"b'from collections import OrderedDict\nimport pickle\nfrom functools import partial\nimport warnings\nimport os.path as osp\nimport numpy as np\nimport zlib\nimport ast\nimport struct\nimport torch\n\nfrom .OSNet import OSNet\n\n\ndef load_checkpoint(fpath: str):\n    \'\'\'\n    Brought from torchreid utils.py .\n    More information: https://kaiyangzhou.github.io/deep-person-reid/_modules/torchreid/utils/torchtools.html\n    \'\'\'\n    if fpath is None:\n        raise ValueError(\'File path is None\')\n    if not osp.exists(fpath):\n        raise FileNotFoundError(\'File is not found at ""{}""\'.format(fpath))\n    map_location = None if torch.cuda.is_available() else \'cpu\'\n    try:\n        checkpoint = torch.load(fpath, map_location=map_location)\n    except UnicodeDecodeError:\n        pickle.load = partial(pickle.load, encoding=""latin1"")\n        pickle.Unpickler = partial(pickle.Unpickler, encoding=""latin1"")\n        checkpoint = torch.load(fpath, pickle_module=pickle, map_location=map_location)\n    except Exception:\n        print(\'Unable to load checkpoint from ""{}""\'.format(fpath))\n        raise\n    return checkpoint\n\n\ndef load_pretrained_weights(model: OSNet, weight_path: str):\n    \'\'\'\n    Brought from torchreid utils.py .\n    More information: https://kaiyangzhou.github.io/deep-person-reid/_modules/torchreid/utils/torchtools.html\n    \'\'\'\n    checkpoint = load_checkpoint(weight_path)\n\n    if \'state_dict\' in checkpoint:\n        state_dict = checkpoint[\'state_dict\']\n    else:\n        state_dict = checkpoint\n\n    model_dict = model.state_dict()\n    new_state_dict = OrderedDict()\n    matched_layers, discarded_layers = [], []\n\n    for k, v in state_dict.items():\n        if k.startswith(\'module.\'):\n            k = k[7:]  # discard module.\n\n        if k in model_dict and model_dict[k].size() == v.size():\n            new_state_dict[k] = v\n            matched_layers.append(k)\n        else:\n            discarded_layers.append(k)\n\n    model_dict.update(new_state_dict)\n    model.load_state_dict(model_dict)\n\n    if len(matched_layers) == 0:\n        warnings.warn(\n            \'[OSNET info] The pretrained weights ""{}"" cannot be loaded, \'\n            \'please check the key names manually \'\n            \'(** ignored and continue **)\'.format(weight_path))\n    else:\n        print(\'[OSNET info] Successfully loaded pretrained weights from ""{}""\'.format(weight_path))\n        if len(discarded_layers) > 0:\n            print(\'** The following layers are discarded \'\n                  \'due to unmatched keys or layer size: {}\'.format(discarded_layers))\n\n\ndef compress_feature_vector(feature_vector: np.ndarray) -> str:\n    \'\'\'\n    :param feature_vector: (np.ndarray) the OSNet 512 dimensional feature vector.\n    :return: (string) compressed feature vector\n    \'\'\'\n    # The input has to be a 1-dimensional ndarray\n    vector = list(feature_vector)\n    packed = struct.pack(\n        f""{len(vector)}f"",\n        *vector\n    )\n    zlibed = zlib.compress(packed)\n    return zlibed.hex()\n\ndef compress_bytes_image(cropped_image: bytes, image_width: int, image_height: int, colors: int) -> str:\n    \'\'\'\n    [INFO] -> This method is attached to this script to show how the image is compressed upstream.\n    :param cropped_image: (bytes) a cropped bounding box containing the image of a person.\n    :param image_width: (int) information about the bounding box width\n    :param image_height: (int) information about the bounding box height\n    :param colors: (int) number of channels of the bounding box\n    :return: (str) the dictionary as a string\n    \'\'\'\n    # Debugging: information of the image size\n    # print(""The size of the input image is: "", sys.getsizeof(cropped_image), "" bytes"")\n    zlibed = zlib.compress(cropped_image)\n    # print(""The size of the compressed image is: "", sys.getsizeof(zlibed), "" bytes"")\n    img_dict = {\n        ""width"": image_width,\n        ""height"": image_height,\n        ""colors"": colors,\n        ""image"": zlibed.hex()\n    }\n    return img_dict.__str__()\n\ndef uncompress_string_image(compresed_cropped_image: str) -> bytes:\n    \'\'\'\n    [INFO] -> This method uncompresses the bytes image compressed as shown in the method ""compress_bytes_image"".\n    :param compresed_cropped_image: (str) a dictionary as a string, that contains the crop information.\n    :return: (bytes) A bytes image with the visual info about the detection.\n    \'\'\'\n\n    # Defensive programming: an empty field can be provided: If so, return an None value\n    if compresed_cropped_image is not np.nan:\n        compresed_dict = ast.literal_eval(compresed_cropped_image)\n        # Debugging: information of the image size\n        # print(""The size of the compressed input image is: "", sys.getsizeof(compresed_cropped_image), "" bytes"")\n        unhexed = bytes.fromhex(compresed_dict[""image""])\n        unzlibed = zlib.decompress(unhexed)\n        patch_shape = (compresed_dict[""height""], compresed_dict[""width""], compresed_dict[""colors""])\n        # print(""The size of the uncompressed image is: "", sys.getsizeof(unzlibed), "" bytes"")\n        image_array = np.frombuffer(unzlibed, dtype=\'uint8\').reshape(patch_shape)\n        return image_array\n    else:\n        return None\n\n\n\n'"
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/__init__.py,0,b'from .person_dist import PCBReIDMetric\n'
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/demo.py,4,"b'import argparse\nimport scipy.io\nimport torch\nimport numpy as np\nimport os\nfrom torchvision import datasets\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\n#######################################################################\n# Evaluate\nparser = argparse.ArgumentParser(description=\'Demo\')\nparser.add_argument(\'--query_index\', default=777, type=int, help=\'test_image_index\')\nparser.add_argument(\'--test_dir\',default=\'../Market/pytorch\',type=str, help=\'./test_data\')\nopts = parser.parse_args()\n\ndata_dir = opts.test_dir\nimage_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ) for x in [\'gallery\',\'query\']}\n\n#####################################################################\n#Show result\ndef imshow(path, title=None):\n    """"""Imshow for Tensor.""""""\n    im = plt.imread(path)\n    plt.imshow(im)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n######################################################################\nresult = scipy.io.loadmat(\'pytorch_result.mat\')\nquery_feature = torch.FloatTensor(result[\'query_f\'])\nquery_cam = result[\'query_cam\'][0]\nquery_label = result[\'query_label\'][0]\ngallery_feature = torch.FloatTensor(result[\'gallery_f\'])\ngallery_cam = result[\'gallery_cam\'][0]\ngallery_label = result[\'gallery_label\'][0]\n\nmulti = os.path.isfile(\'multi_query.mat\')\n\nif multi:\n    m_result = scipy.io.loadmat(\'multi_query.mat\')\n    mquery_feature = torch.FloatTensor(m_result[\'mquery_f\'])\n    mquery_cam = m_result[\'mquery_cam\'][0]\n    mquery_label = m_result[\'mquery_label\'][0]\n    mquery_feature = mquery_feature.cuda()\n\nquery_feature = query_feature.cuda()\ngallery_feature = gallery_feature.cuda()\n\n#######################################################################\n# sort the images\ndef sort_img(qf, ql, qc, gf, gl, gc):\n    query = qf.view(-1,1)\n    # print(query.shape)\n    score = torch.mm(gf,query)\n    score = score.squeeze(1).cpu()\n    score = score.numpy()\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    # index = index[0:2000]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    #same camera\n    camera_index = np.argwhere(gc==qc)\n\n    #good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) \n\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n    return index\n\ni = opts.query_index\nindex = sort_img(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n\n########################################################################\n# Visualize the rank result\n\nquery_path, _ = image_datasets[\'query\'].imgs[i]\nquery_label = query_label[i]\nprint(query_path)\nprint(\'Top 10 images are as follow:\')\ntry: # Visualize Ranking Result \n    # Graphical User Interface is needed\n    fig = plt.figure(figsize=(16,4))\n    ax = plt.subplot(1,11,1)\n    ax.axis(\'off\')\n    imshow(query_path,\'query\')\n    for i in range(10):\n        ax = plt.subplot(1,11,i+2)\n        ax.axis(\'off\')\n        img_path, _ = image_datasets[\'gallery\'].imgs[index[i]]\n        label = gallery_label[index[i]]\n        imshow(img_path)\n        if label == query_label:\n            ax.set_title(\'%d\'%(i+1), color=\'green\')\n        else:\n            ax.set_title(\'%d\'%(i+1), color=\'red\')\n        print(img_path)\nexcept RuntimeError:\n    for i in range(10):\n        img_path = image_datasets.imgs[index[i]]\n        print(img_path[0])\n    print(\'If you want to see the visualization of the ranking result, graphical user interface is needed.\')\n\nfig.savefig(""show.png"")\n'"
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/evaluate.py,3,"b""import scipy.io\nimport torch\nimport numpy as np\n#import time\nimport os\n\n#######################################################################\n# Evaluate\ndef evaluate(qf,ql,qc,gf,gl,gc):\n    query = qf\n    score = np.dot(gf,query)\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    #index = index[0:2000]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = result['query_f']\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = result['gallery_f']\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nmulti = os.path.isfile('multi_query.mat')\n\nif multi:\n    m_result = scipy.io.loadmat('multi_query.mat')\n    mquery_feature = m_result['mquery_f']\n    mquery_cam = m_result['mquery_cam'][0]\n    mquery_label = m_result['mquery_label'][0]\n    \nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\n#print(query_label)\nfor i in range(len(query_label)):\n    ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n    if CMC_tmp[0]==-1:\n        continue\n    CMC = CMC + CMC_tmp\n    ap += ap_tmp\n    print(i, CMC_tmp[0])\n\nCMC = CMC.float()\nCMC = CMC/len(query_label) #average CMC\nprint('Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n\n# multiple-query\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\nif multi:\n    for i in range(len(query_label)):\n        mquery_index1 = np.argwhere(mquery_label==query_label[i])\n        mquery_index2 = np.argwhere(mquery_cam==query_cam[i])\n        mquery_index =  np.intersect1d(mquery_index1, mquery_index2)\n        mq = np.mean(mquery_feature[mquery_index,:], axis=0)\n        ap_tmp, CMC_tmp = evaluate(mq,query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n        if CMC_tmp[0]==-1:\n            continue\n        CMC = CMC + CMC_tmp\n        ap += ap_tmp\n        #print(i, CMC_tmp[0])\n    CMC = CMC.float()\n    CMC = CMC/len(query_label) #average CMC\n    print('multi Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"""
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/evaluate_gpu.py,8,"b""import scipy.io\nimport torch\nimport numpy as np\n#import time\nimport os\n\n#######################################################################\n# Evaluate\ndef evaluate(qf,ql,qc,gf,gl,gc):\n    query = qf.view(-1,1)\n    # print(query.shape)\n    score = torch.mm(gf,query)\n    score = score.squeeze(1).cpu()\n    score = score.numpy()\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    # index = index[0:2000]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = torch.FloatTensor(result['query_f'])\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = torch.FloatTensor(result['gallery_f'])\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nmulti = os.path.isfile('multi_query.mat')\n\nif multi:\n    m_result = scipy.io.loadmat('multi_query.mat')\n    mquery_feature = torch.FloatTensor(m_result['mquery_f'])\n    mquery_cam = m_result['mquery_cam'][0]\n    mquery_label = m_result['mquery_label'][0]\n    mquery_feature = mquery_feature.cuda()\n\nquery_feature = query_feature.cuda()\ngallery_feature = gallery_feature.cuda()\n\nprint(query_feature.shape)\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\n#print(query_label)\nfor i in range(len(query_label)):\n    ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n    if CMC_tmp[0]==-1:\n        continue\n    CMC = CMC + CMC_tmp\n    ap += ap_tmp\n    #print(i, CMC_tmp[0])\n\nCMC = CMC.float()\nCMC = CMC/len(query_label) #average CMC\nprint('Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n\n# multiple-query\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\nif multi:\n    for i in range(len(query_label)):\n        mquery_index1 = np.argwhere(mquery_label==query_label[i])\n        mquery_index2 = np.argwhere(mquery_cam==query_cam[i])\n        mquery_index =  np.intersect1d(mquery_index1, mquery_index2)\n        mq = torch.mean(mquery_feature[mquery_index,:], dim=0)\n        ap_tmp, CMC_tmp = evaluate(mq,query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n        if CMC_tmp[0]==-1:\n            continue\n        CMC = CMC + CMC_tmp\n        ap += ap_tmp\n        #print(i, CMC_tmp[0])\n    CMC = CMC.float()\n    CMC = CMC/len(query_label) #average CMC\n    print('multi Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"""
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/evaluate_rerank.py,2,"b""import scipy.io\nimport torch\nimport numpy as np\nimport time\nfrom  re_ranking import re_ranking\n#######################################################################\n# Evaluate\ndef evaluate(score,ql,qc,gl,gc):\n    index = np.argsort(score)  #from small to large\n    #index = index[::-1]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = result['query_f']\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = result['gallery_f']\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\n#re-ranking\nprint('calculate initial distance')\nq_g_dist = np.dot(query_feature, np.transpose(gallery_feature))\nq_q_dist = np.dot(query_feature, np.transpose(query_feature))\ng_g_dist = np.dot(gallery_feature, np.transpose(gallery_feature))\nsince = time.time()\nre_rank = re_ranking(q_g_dist, q_q_dist, g_g_dist)\ntime_elapsed = time.time() - since\nprint('Reranking complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\nfor i in range(len(query_label)):\n    ap_tmp, CMC_tmp = evaluate(re_rank[i,:],query_label[i],query_cam[i],gallery_label,gallery_cam)\n    if CMC_tmp[0]==-1:\n        continue\n    CMC = CMC + CMC_tmp\n    ap += ap_tmp\n    #print(i, CMC_tmp[0])\n\nCMC = CMC.float()\nCMC = CMC/len(query_label) #average CMC\nprint('top1:%f top5:%f top10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"""
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/model.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torchvision import models\nfrom torch.autograd import Variable\n\n\n######################################################################\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')  # For old pytorch, you may use kaiming_normal.\n    elif classname.find(\'Linear\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_out\')\n        init.constant_(m.bias.data, 0.0)\n    elif classname.find(\'BatchNorm1d\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Linear\') != -1:\n        init.normal_(m.weight.data, std=0.001)\n        init.constant_(m.bias.data, 0.0)\n\n\n# Defines the new fc layer and classification layer\n# |--Linear--|--bn--|--relu--|--Linear--|\nclass ClassBlock(nn.Module):\n    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True,\n                 return_f=False):\n        super(ClassBlock, self).__init__()\n        self.return_f = return_f\n        add_block = []\n        if linear:\n            add_block += [nn.Linear(input_dim, num_bottleneck)]\n        else:\n            num_bottleneck = input_dim\n        if bnorm:\n            add_block += [nn.BatchNorm1d(num_bottleneck)]\n        if relu:\n            add_block += [nn.LeakyReLU(0.1)]\n        if droprate > 0:\n            add_block += [nn.Dropout(p=droprate)]\n        add_block = nn.Sequential(*add_block)\n        add_block.apply(weights_init_kaiming)\n\n        classifier = []\n        classifier += [nn.Linear(num_bottleneck, class_num)]\n        classifier = nn.Sequential(*classifier)\n        classifier.apply(weights_init_classifier)\n\n        self.add_block = add_block\n        self.classifier = classifier\n\n    def forward(self, x):\n        x = self.add_block(x)\n        if self.return_f:\n            f = x\n            x = self.classifier(x)\n            return x, f\n        else:\n            x = self.classifier(x)\n            return x\n\n\n# Define the ResNet50-based Model\nclass ft_net(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5, stride=2):\n        super(ft_net, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        if stride == 1:\n            model_ft.layer4[0].downsample[0].stride = (1, 1)\n            model_ft.layer4[0].conv2.stride = (1, 1)\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.model.avgpool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the DenseNet121-based Model\nclass ft_net_dense(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5):\n        super().__init__()\n        model_ft = models.densenet121(pretrained=True)\n        model_ft.features.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        model_ft.fc = nn.Sequential()\n        self.model = model_ft\n        # For DenseNet, the feature dim is 1024 \n        self.classifier = ClassBlock(1024, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.features(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the ResNet50-based Model (Middle-Concat)\n# In the spirit of ""The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching."" Yu, Qian, et al. arXiv:1711.08106 (2017).\nclass ft_net_middle(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5):\n        super(ft_net_middle, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048 + 1024, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        # x0  n*1024*1*1\n        x0 = self.model.avgpool(x)\n        x = self.model.layer4(x)\n        # x1  n*2048*1*1\n        x1 = self.model.avgpool(x)\n        x = torch.cat((x0, x1), 1)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Part Model proposed in Yifan Sun etal. (2018)\nclass PCB(nn.Module):\n    def __init__(self, class_num):\n        super(PCB, self).__init__()\n\n        self.part = 6  # We cut the pool5 to 6 parts\n        model_ft = models.resnet50(pretrained=True)\n        self.model = model_ft\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part, 1))\n        self.dropout = nn.Dropout(p=0.5)\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1, 1)\n        self.model.layer4[0].conv2.stride = (1, 1)\n        # define 6 classifiers\n        for i in range(self.part):\n            name = \'classifier\' + str(i)\n            setattr(self, name, ClassBlock(2048, class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        part = {}\n        predict = {}\n        # get six part feature batchsize*2048*6\n        for i in range(self.part):\n            part[i] = torch.squeeze(x[:, :, i])\n            name = \'classifier\' + str(i)\n            c = getattr(self, name)\n            predict[i] = c(part[i])\n\n        # sum prediction\n        # y = predict[0]\n        # for i in range(self.part-1):\n        #    y += predict[i+1]\n        y = []\n        for i in range(self.part):\n            y.append(predict[i])\n        return y\n\n\nclass PCB_test(nn.Module):\n    def __init__(self, model):\n        super(PCB_test, self).__init__()\n        self.part = 6\n        self.model = model.model\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part, 1))\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1, 1)\n        self.model.layer4[0].conv2.stride = (1, 1)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        y = x.view(x.size(0), x.size(1), x.size(2))\n        return y\n\n\n\'\'\'\n# debug model structure\n# Run this code with:\npython model.py\n\'\'\'\nif __name__ == \'__main__\':\n    # Here I left a simple forward function.\n    # Test the model, before you train it.\n    net = ft_net(751, stride=1)\n    net.classifier = nn.Sequential()\n    print(net)\n    input = Variable(torch.FloatTensor(8, 3, 256, 128))\n    output = net(input)\n    print(\'net output size:\')\n    print(output.shape)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/person_dist.py,14,"b'import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms.functional as transform_func\nimport yaml\nimport cv2\nimport numpy as np\n\nfrom typing import List, Union\n\nfrom .model import ft_net, ft_net_dense, PCB, PCB_test\n\n\nclass Config(object):\n    pass\n\n\ndef create_model(name=\'PCB\', pretrain_path=\'./model\'):\n    cfg = Config()\n\n    config_dir = os.path.dirname(os.path.abspath(__file__))\n    config_path = os.path.join(config_dir, \'model\', name, \'opts.yaml\')\n    with open(config_path, \'r\') as stream:\n        config = yaml.load(stream)\n\n        # PCB: false\n        # batchsize: 32\n        # color_jitter: false\n        # data_dir:../ Market / pytorch\n        # droprate: 0.5\n        # erasing_p: 0\n        # fp16: true\n        # gpu_ids: \'0\'\n        # lr: 0.05\n        # name: ft_ResNet50\n        # stride: 2\n        # train_all: false\n        # use_dense: false\n        # nclasses: 751\n\n        cfg.PCB = config[\'PCB\']\n        cfg.name = config[\'name\']\n        cfg.stride = config[\'stride\']\n        cfg.use_dense = config[\'use_dense\']\n        cfg.nclasses = config[\'nclasses\']\n\n    if cfg.use_dense:\n        model_structure = ft_net_dense(cfg.nclasses)\n    else:\n        model_structure = ft_net(cfg.nclasses, stride=cfg.stride)\n\n    if cfg.PCB:\n        model_structure = PCB(cfg.nclasses)\n\n    model_structure.load_state_dict(torch.load(pretrain_path))\n\n    if cfg.PCB:\n        model_structure = PCB_test(model_structure)\n        cfg.height, cfg.width = 384, 192\n    else:\n        cfg.height, cfg.width = 256, 128\n\n    return model_structure, cfg\n\n\nclass PCBReIDMetric(nn.Module):\n\n    def __init__(self, name, pretrain_path):\n        super(PCBReIDMetric, self).__init__()\n        self.model, self.opt = create_model(name, pretrain_path)\n        self.model.eval()\n\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n\n    def forward(self, x: List[np.ndarray], bboxs: Union[List[np.ndarray], None] = None, ignore_head: bool = False):\n        """"""\n\n        Args:\n            x (list of np.ndarray): [(3, height, width), (3, height, width), ..., (3, height, width)], each item of\n                                (3, height, width) is in the range of [0, 1.0], with np.float32.\n            bboxs (list of np.ndarray or None): [(4,), (4,), ..., (4,)] with np.int32\n            ignore_head (bool):\n\n        Returns:\n            feats (torch.tensor): (bs, dim)\n        """"""\n        x = self.crop_patch(x, bboxs)\n        feats = self.extract_feat(x, bboxs, ignore_head)\n        return feats\n\n    def crop_patch(self, images: torch.tensor, bboxs: Union[torch.tensor, None] = None):\n        """"""\n\n        Args:\n            images (torch.tensor): [bs, 3, height, width] is in range of [0, 255] with torch.float32\n            bboxs (torch.tensor or None): [(4,), (4,), ..., (4,)] with np.int32\n\n        Returns:\n            crop (torch.tensor): (bs, 3, resize_height, resize_width)\n        """"""\n\n        if bboxs is None:\n            crop_imgs = F.interpolate(\n                images, size=(self.opt.height, self.opt.width),\n                mode=""bilinear"", align_corners=True\n            )\n        else:\n            bs = len(images)\n            crop_imgs = []\n            for i in range(bs):\n                x = images[i]\n                box = bboxs[i]\n                if box is not None:\n                    x0, y0, x1, y1 = box\n                    crop = x[:, y0:y1, x0:x1]\n                else:\n                    crop = x\n                crop = transform_func.normalize(crop, mean=self.mean, std=self.std)\n                crop.unsqueeze_(0)\n                crop = F.interpolate(crop, size=(self.opt.height, self.opt.width), mode=""bilinear"", align_corners=True)\n                crop_imgs.append(crop)\n            crop_imgs = torch.cat(crop_imgs, dim=0)\n        return crop_imgs\n\n    def extract_feat(self, x: torch.tensor,\n                     bboxs: Union[List[np.ndarray], None] = None,\n                     ignore_head: bool = False):\n\n        with torch.no_grad():\n            ff = self.model(x)\n            sqrt_num = 6\n            # ipdb.set_trace()\n            if ignore_head:\n                ff = ff[:, :, 1:]\n                sqrt_num = 5\n\n            if self.opt.PCB:\n                fnorm = torch.norm(ff, p=2, dim=1, keepdim=True) * np.sqrt(sqrt_num)\n                ff = ff.div(fnorm.expand_as(ff))\n                ff = ff.view(ff.size(0), -1)\n            else:\n                fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n                ff = ff.div(fnorm.expand_as(ff))\n\n            return ff\n\n    def load_img(self, img_path):\n        image = cv2.imread(img_path)\n        image = cv2.resize(image, (self.opt.width, self.opt.height))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = image.astype(np.float32)\n        image /= 255.0\n\n        image = self.crop_patch(image, bboxs=None)\n        return image\n\n\nif __name__ == \'__main__\':\n    import ipdb\n\n    pReIdMetric = PCBReIDMetric(\'PCB\')\n    print(pReIdMetric)\n\n    norm = True\n\n    src_path = \'./examples/gt.jpg\'\n    src_img = pReIdMetric.load_img(src_path)\n    src_out = pReIdMetric.extract_feat(src_img, norm=norm)\n\n    print(src_img.shape, src_img.max(), src_img.min())\n    for img_name in [\'source.jpg\', \'gt.jpg\', \'ours.jpg\', \'pG2.jpg\', \'SHUP.jpg\', \'DSC.jpg\']:\n        img_path = os.path.join(\'./examples\', img_name)\n\n        test_img = pReIdMetric.load_img(img_path)\n        test_out = pReIdMetric.extract_feat(test_img, norm=norm)\n        print(img_name, torch.sum((src_out * test_out)))\n\n    ipdb.set_trace()\n'"
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/prepare.py,0,"b""import os\nfrom shutil import copyfile\n\n# You only need to change this line to your dataset download path\ndownload_path = '../Market'\n\nif not os.path.isdir(download_path):\n    print('please change the download_path')\n\nsave_path = download_path + '/pytorch'\nif not os.path.isdir(save_path):\n    os.mkdir(save_path)\n#-----------------------------------------\n#query\nquery_path = download_path + '/query'\nquery_save_path = download_path + '/pytorch/query'\nif not os.path.isdir(query_save_path):\n    os.mkdir(query_save_path)\n\nfor root, dirs, files in os.walk(query_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = query_path + '/' + name\n        dst_path = query_save_path + '/' + ID[0] \n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n\n#-----------------------------------------\n#multi-query\nquery_path = download_path + '/gt_bbox'\n# for dukemtmc-reid, we do not need multi-query\nif os.path.isdir(query_path):\n    query_save_path = download_path + '/pytorch/multi-query'\n    if not os.path.isdir(query_save_path):\n        os.mkdir(query_save_path)\n\n    for root, dirs, files in os.walk(query_path, topdown=True):\n        for name in files:\n            if not name[-3:]=='jpg':\n                continue\n            ID  = name.split('_')\n            src_path = query_path + '/' + name\n            dst_path = query_save_path + '/' + ID[0]\n            if not os.path.isdir(dst_path):\n                os.mkdir(dst_path)\n            copyfile(src_path, dst_path + '/' + name)\n\n#-----------------------------------------\n#gallery\ngallery_path = download_path + '/bounding_box_test'\ngallery_save_path = download_path + '/pytorch/gallery'\nif not os.path.isdir(gallery_save_path):\n    os.mkdir(gallery_save_path)\n\nfor root, dirs, files in os.walk(gallery_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = gallery_path + '/' + name\n        dst_path = gallery_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n\n#---------------------------------------\n#train_all\ntrain_path = download_path + '/bounding_box_train'\ntrain_save_path = download_path + '/pytorch/train_all'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n\n\n#---------------------------------------\n#train_val\ntrain_path = download_path + '/bounding_box_train'\ntrain_save_path = download_path + '/pytorch/train'\nval_save_path = download_path + '/pytorch/val'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n    os.mkdir(val_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n            dst_path = val_save_path + '/' + ID[0]  #first image is used as val image\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n"""
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/prepare_static.py,8,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nfrom torchvision import datasets, transforms\nimport time\nimport os\n\nversion =  torch.__version__\n\n######################################################################\n# Options\n# --------\nparser = argparse.ArgumentParser(description='Training')\nparser.add_argument('--data_dir',default='/home/zzd/Market/pytorch',type=str, help='training dir path')\nparser.add_argument('--train_all', action='store_true', help='use all training data' )\nparser.add_argument('--color_jitter', action='store_true', help='use color jitter in training' )\nparser.add_argument('--batchsize', default=128, type=int, help='batchsize')\nopt = parser.parse_args()\n\ndata_dir = opt.data_dir\n\n######################################################################\n# Load Data\n# ---------\n#\n\ntransform_train_list = [\n        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n        transforms.Resize((288,144), interpolation=3),\n        #transforms.RandomCrop((256,128)),\n        #transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\ntransform_val_list = [\n        transforms.Resize(size=(256,128),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\n\nprint(transform_train_list)\ndata_transforms = {\n    'train': transforms.Compose( transform_train_list ),\n    'val': transforms.Compose(transform_val_list),\n}\n\n\ntrain_all = ''\nif opt.train_all:\n     train_all = '_all'\n\nimage_datasets = {}\nimage_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train' + train_all),\n                                          data_transforms['train'])\nimage_datasets['val'] = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n                                          data_transforms['val'])\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=True, num_workers=16)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\nuse_gpu = torch.cuda.is_available()\n\n######################################################################\n# prepare_dataset\n# ------------------\n#\n# Now, let's write a general function to train a model. Here, we will\n# illustrate:\n#\n# -  Scheduling the learning rate\n# -  Saving the best model\n#\n# In the following, parameter ``scheduler`` is an LR scheduler object from\n# ``torch.optim.lr_scheduler``.\n\ndef prepare_model():\n    since = time.time()\n\n    num_epochs = 1\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train']:\n\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            # Iterate over data.\n            for data in dataloaders[phase]:\n                # get the inputs\n                inputs, labels = data\n                now_batch_size,c,h,w = inputs.shape\n                mean += torch.sum(torch.mean(torch.mean(inputs,dim=3),dim=2),dim=0)\n                std += torch.sum(torch.std(inputs.view(now_batch_size,c,h*w),dim=2),dim=0)\n                \n            print(mean/dataset_sizes['train'])\n            print(std/dataset_sizes['train'])\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    return \n\n\n\nprepare_model()\n"""
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/random_erasing.py,0,"b'from __future__ import absolute_import\n\nfrom torchvision.transforms import *\n\n#from PIL import Image\nimport random\nimport math\n#import numpy as np\n#import torch\n\nclass RandomErasing(object):\n    """""" Randomly selects a rectangle region in an image and erases its pixels.\n        \'Random Erasing Data Augmentation\' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n    Args:\n         probability: The probability that the Random Erasing operation will be performed.\n         sl: Minimum proportion of erased area against input image.\n         sh: Maximum proportion of erased area against input image.\n         r1: Minimum aspect ratio of erased area.\n         mean: Erasing value. \n    """"""\n    \n    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n       \n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                return img\n\n        return img\n'"
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/re_ranking.py,0,"b'#!/usr/bin/env python2/python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Jun 26 14:46:56 2017\n@author: luohao\nModified by Houjing Huang, 2017-12-22. \n- This version accepts distance matrix instead of raw features. \n- The difference of `/` division between python 2 and 3 is handled.\n- numpy.float16 is replaced by numpy.float32 for numerical precision.\n\nModified by Zhedong Zheng, 2018-1-12.\n- replace sort with topK, which save about 30s.\n""""""\n\n""""""\nCVPR2017 paper:Zhong Z, Zheng L, Cao D, et al. Re-ranking Person Re-identification with k-reciprocal Encoding[J]. 2017.\nurl:http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf\nMatlab version: https://github.com/zhunzhong07/person-re-ranking\n""""""\n\n""""""\nAPI\nq_g_dist: query-gallery distance matrix, numpy array, shape [num_query, num_gallery]\nq_q_dist: query-query distance matrix, numpy array, shape [num_query, num_query]\ng_g_dist: gallery-gallery distance matrix, numpy array, shape [num_gallery, num_gallery]\nk1, k2, lambda_value: parameters, the original paper is (k1=20, k2=6, lambda_value=0.3)\nReturns:\n  final_dist: re-ranked distance, numpy array, shape [num_query, num_gallery]\n""""""\n\n\nimport numpy as np\n\ndef k_reciprocal_neigh( initial_rank, i, k1):\n    forward_k_neigh_index = initial_rank[i,:k1+1]\n    backward_k_neigh_index = initial_rank[forward_k_neigh_index,:k1+1]\n    fi = np.where(backward_k_neigh_index==i)[0]\n    return forward_k_neigh_index[fi]\n\ndef re_ranking(q_g_dist, q_q_dist, g_g_dist, k1=20, k2=6, lambda_value=0.3):\n    # The following naming, e.g. gallery_num, is different from outer scope.\n    # Don\'t care about it.\n    original_dist = np.concatenate(\n      [np.concatenate([q_q_dist, q_g_dist], axis=1),\n       np.concatenate([q_g_dist.T, g_g_dist], axis=1)],\n      axis=0)\n    original_dist = 2. - 2 * original_dist   # change the cosine similarity metric to euclidean similarity metric\n    original_dist = np.power(original_dist, 2).astype(np.float32)\n    original_dist = np.transpose(1. * original_dist/np.max(original_dist,axis = 0))\n    V = np.zeros_like(original_dist).astype(np.float32)\n    #initial_rank = np.argsort(original_dist).astype(np.int32)\n    # top K1+1\n    initial_rank = np.argpartition( original_dist, range(1,k1+1) )\n\n    query_num = q_g_dist.shape[0]\n    all_num = original_dist.shape[0]\n\n    for i in range(all_num):\n        # k-reciprocal neighbors\n        k_reciprocal_index = k_reciprocal_neigh( initial_rank, i, k1)\n        k_reciprocal_expansion_index = k_reciprocal_index\n        for j in range(len(k_reciprocal_index)):\n            candidate = k_reciprocal_index[j]\n            candidate_k_reciprocal_index = k_reciprocal_neigh( initial_rank, candidate, int(np.around(k1/2)))\n            if len(np.intersect1d(candidate_k_reciprocal_index,k_reciprocal_index))> 2./3*len(candidate_k_reciprocal_index):\n                k_reciprocal_expansion_index = np.append(k_reciprocal_expansion_index,candidate_k_reciprocal_index)\n\n        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)\n        weight = np.exp(-original_dist[i,k_reciprocal_expansion_index])\n        V[i,k_reciprocal_expansion_index] = 1.*weight/np.sum(weight)\n\n    original_dist = original_dist[:query_num,]\n    if k2 != 1:\n        V_qe = np.zeros_like(V,dtype=np.float32)\n        for i in range(all_num):\n            V_qe[i,:] = np.mean(V[initial_rank[i,:k2],:],axis=0)\n        V = V_qe\n        del V_qe\n    del initial_rank\n    invIndex = []\n    for i in range(all_num):\n        invIndex.append(np.where(V[:,i] != 0)[0])\n\n    jaccard_dist = np.zeros_like(original_dist,dtype = np.float32)\n\n    for i in range(query_num):\n        temp_min = np.zeros(shape=[1,all_num],dtype=np.float32)\n        indNonZero = np.where(V[i,:] != 0)[0]\n        indImages = []\n        indImages = [invIndex[ind] for ind in indNonZero]\n        for j in range(len(indNonZero)):\n            temp_min[0,indImages[j]] = temp_min[0,indImages[j]]+ np.minimum(V[i,indNonZero[j]],V[indImages[j],indNonZero[j]])\n        jaccard_dist[i] = 1-temp_min/(2.-temp_min)\n\n    final_dist = jaccard_dist*(1-lambda_value) + original_dist*lambda_value\n    del original_dist\n    del V\n    del jaccard_dist\n    final_dist = final_dist[:query_num,query_num:]\n    return final_dist\n'"
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/test.py,21,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nfrom torchvision import datasets, transforms\nimport os\nimport scipy.io\nimport yaml\nfrom .model import ft_net, ft_net_dense, PCB, PCB_test\n\n\n#fp16\ntry:\n    from apex.fp16_utils import *\nexcept ImportError: # will be 3.x series\n    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')\n######################################################################\n# Options\n# --------\n\nparser = argparse.ArgumentParser(description='Training')\nparser.add_argument('--gpu_ids', default='0', type=str,help='gpu_ids: e.g. 0  0,1,2  0,2')\nparser.add_argument('--which_epoch', default='last', type=str, help='0,1,2,3...or last')\nparser.add_argument('--test_dir', default='/home/piaozx/liuwen/p300/human_pose/processed/motion_transfer_HD/001/1/2',\n                    type=str, help='./test_data')\nparser.add_argument('--name', default='ft_ResNet50', type=str, help='save model path')\nparser.add_argument('--batchsize', default=256, type=int, help='batchsize')\nparser.add_argument('--use_dense', action='store_true', help='use densenet121' )\nparser.add_argument('--PCB', action='store_true', help='use PCB' )\nparser.add_argument('--multi', action='store_true', help='use multiple query' )\nparser.add_argument('--fp16', action='store_true', help='use fp16.' )\n\nopt = parser.parse_args()\n###load config###\n# load the training config\nconfig_path = os.path.join('./model', opt.name, 'opts.yaml')\nwith open(config_path, 'r') as stream:\n        config = yaml.load(stream)\nopt.fp16 = config['fp16'] \nopt.PCB = config['PCB']\nopt.use_dense = config['use_dense']\nopt.stride = config['stride']\n\nif 'nclasses' in config: # tp compatible with old config files\n    opt.nclasses = config['nclasses']\nelse: \n    opt.nclasses = 751 \n\nstr_ids = opt.gpu_ids.split(',')\n#which_epoch = opt.which_epoch\nname = opt.name\ntest_dir = opt.test_dir\n\ngpu_ids = []\nfor str_id in str_ids:\n    id = int(str_id)\n    if id >= 0:\n        gpu_ids.append(id)\n\n# set gpu ids\nif len(gpu_ids) > 0:\n    torch.cuda.set_device(gpu_ids[0])\n    cudnn.benchmark = True\n\n######################################################################\n# Load Data\n# ---------\n#\n# We will use torchvision and torch.utils.data packages for loading the\n# data.\n#\ndata_transforms = transforms.Compose([\n        transforms.Resize((256,128), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n############### Ten Crop        \n        #transforms.TenCrop(224),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.ToTensor()(crop) \n          #      for crop in crops]\n           # )),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(crop)\n          #       for crop in crops]\n          # ))\n])\n\nif opt.PCB:\n    data_transforms = transforms.Compose([\n        transforms.Resize((384,192), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n    ])\n\n\ndata_dir = test_dir\n\nif opt.multi:\n    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms) for x in ['gallery','query','multi-query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query','multi-query']}\nelse:\n    image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query']}\nclass_names = image_datasets['query'].classes\nuse_gpu = torch.cuda.is_available()\n\n\n######################################################################\n# Load model\n#---------------------------\ndef load_network(network):\n    save_path = os.path.join('./model', name, 'net_%s.pth' % opt.which_epoch)\n    network.load_state_dict(torch.load(save_path))\n    return network\n\n\n######################################################################\n# Extract feature\n# ----------------------\n#\n# Extract feature from  a trained model.\n#\ndef fliplr(img):\n    '''flip horizontal'''\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\n\ndef extract_feature(model, dataloaders):\n    features = torch.FloatTensor()\n    count = 0\n    for data in dataloaders:\n        img, label = data\n        n, c, h, w = img.size()\n        count += n\n        print(count)\n        ff = torch.FloatTensor(n, 512).zero_()\n\n        if opt.PCB:\n            ff = torch.FloatTensor(n, 2048, 6).zero_() # we have six parts\n        for i in range(2):\n            if i == 1:\n                img = fliplr(img)\n            input_img = Variable(img.cuda())\n            #if opt.fp16:\n            #    input_img = input_img.half()\n            outputs = model(input_img) \n            f = outputs.data.cpu().float()\n            ff = ff+f\n        # norm feature\n        if opt.PCB:\n            # feature size (n,2048,6)\n            # 1. To treat every part equally, I calculate the norm for every 2048-dim part feature.\n            # 2. To keep the cosine score==1, sqrt(6) is added to norm the whole feature (2048*6).\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True) * np.sqrt(6) \n            ff = ff.div(fnorm.expand_as(ff))\n            ff = ff.view(ff.size(0), -1)\n        else:\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n            ff = ff.div(fnorm.expand_as(ff))\n\n        features = torch.cat((features, ff), 0)\n    return features\n\n\ndef get_id(img_path):\n    camera_id = []\n    labels = []\n    for path, v in img_path:\n        #filename = path.split('/')[-1]\n        filename = os.path.basename(path)\n        label = filename[0:4]\n        camera = filename.split('c')[1]\n        if label[0:2]=='-1':\n            labels.append(-1)\n        else:\n            labels.append(int(label))\n        camera_id.append(int(camera[0]))\n    return camera_id, labels\n\ngallery_path = image_datasets['gallery'].imgs\nquery_path = image_datasets['query'].imgs\n\ngallery_cam,gallery_label = get_id(gallery_path)\nquery_cam,query_label = get_id(query_path)\n\nif opt.multi:\n    mquery_path = image_datasets['multi-query'].imgs\n    mquery_cam,mquery_label = get_id(mquery_path)\n\n######################################################################\n# Load Collected data Trained model\nprint('-------test-----------')\nif opt.use_dense:\n    model_structure = ft_net_dense(opt.nclasses)\nelse:\n    model_structure = ft_net(opt.nclasses, stride = opt.stride)\n\nif opt.PCB:\n    model_structure = PCB(opt.nclasses)\n\n#if opt.fp16:\n#    model_structure = network_to_half(model_structure)\n\nmodel = load_network(model_structure)\n\n# Remove the final fc layer and classifier layer\nif opt.PCB:\n    #if opt.fp16:\n    #    model = PCB_test(model[1])\n    #else:\n        model = PCB_test(model)\nelse:\n    #if opt.fp16:\n        #model[1].model.fc = nn.Sequential()\n        #model[1].classifier = nn.Sequential()\n    #else:\n        model.model.fc = nn.Sequential()\n        model.classifier.classifier = nn.Sequential()\n\n# Change to test mode\nmodel = model.eval()\nif use_gpu:\n    model = model.cuda()\n\n# Extract feature\nwith torch.no_grad():\n    gallery_feature = extract_feature(model, dataloaders['gallery'])\n    query_feature = extract_feature(model, dataloaders['query'])\n    if opt.multi:\n        mquery_feature = extract_feature(model, dataloaders['multi-query'])\n    \n# Save to Matlab for check\nresult = {'gallery_f':gallery_feature.numpy(),'gallery_label':gallery_label,'gallery_cam':gallery_cam,'query_f':query_feature.numpy(),'query_label':query_label,'query_cam':query_cam}\nscipy.io.savemat('pytorch_result.mat',result)\nif opt.multi:\n    result = {'mquery_f':mquery_feature.numpy(),'mquery_label':mquery_label,'mquery_cam':mquery_cam}\n    scipy.io.savemat('multi_query.mat',result)\n"""
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/train.py,16,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nimport torch.backends.cudnn as cudnn\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\n#from PIL import Image\nimport time\nimport os\nfrom model import ft_net, ft_net_dense, PCB\nfrom random_erasing import RandomErasing\nimport yaml\nfrom shutil import copyfile\n\nversion =  torch.__version__\n#fp16\ntry:\n    from apex.fp16_utils import *\n    from apex import amp, optimizers\nexcept ImportError: # will be 3.x series\n    print(\'This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\')\n######################################################################\n# Options\n# --------\nparser = argparse.ArgumentParser(description=\'Training\')\nparser.add_argument(\'--gpu_ids\',default=\'0\', type=str,help=\'gpu_ids: e.g. 0  0,1,2  0,2\')\nparser.add_argument(\'--name\',default=\'ft_ResNet50\', type=str, help=\'output model name\')\nparser.add_argument(\'--data_dir\',default=\'../Market/pytorch\',type=str, help=\'training dir path\')\nparser.add_argument(\'--train_all\', action=\'store_true\', help=\'use all training data\' )\nparser.add_argument(\'--color_jitter\', action=\'store_true\', help=\'use color jitter in training\' )\nparser.add_argument(\'--batchsize\', default=32, type=int, help=\'batchsize\')\nparser.add_argument(\'--stride\', default=2, type=int, help=\'stride\')\nparser.add_argument(\'--erasing_p\', default=0, type=float, help=\'Random Erasing probability, in [0,1]\')\nparser.add_argument(\'--use_dense\', action=\'store_true\', help=\'use densenet121\' )\nparser.add_argument(\'--lr\', default=0.05, type=float, help=\'learning rate\')\nparser.add_argument(\'--droprate\', default=0.5, type=float, help=\'drop rate\')\nparser.add_argument(\'--PCB\', action=\'store_true\', help=\'use PCB+ResNet50\' )\nparser.add_argument(\'--fp16\', action=\'store_true\', help=\'use float16 instead of float32, which will save about 50% memory\' )\nopt = parser.parse_args()\n\nfp16 = opt.fp16\ndata_dir = opt.data_dir\nname = opt.name\nstr_ids = opt.gpu_ids.split(\',\')\ngpu_ids = []\nfor str_id in str_ids:\n    gid = int(str_id)\n    if gid >=0:\n        gpu_ids.append(gid)\n\n# set gpu ids\nif len(gpu_ids)>0:\n    torch.cuda.set_device(gpu_ids[0])\n    cudnn.benchmark = True\n######################################################################\n# Load Data\n# ---------\n#\n\ntransform_train_list = [\n        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n        transforms.Resize((256,128), interpolation=3),\n        transforms.Pad(10),\n        transforms.RandomCrop((256,128)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\ntransform_val_list = [\n        transforms.Resize(size=(256,128),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.PCB:\n    transform_train_list = [\n        transforms.Resize((384,192), interpolation=3),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n    transform_val_list = [\n        transforms.Resize(size=(384,192),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.erasing_p>0:\n    transform_train_list = transform_train_list +  [RandomErasing(probability = opt.erasing_p, mean=[0.0, 0.0, 0.0])]\n\nif opt.color_jitter:\n    transform_train_list = [transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0)] + transform_train_list\n\nprint(transform_train_list)\ndata_transforms = {\n    \'train\': transforms.Compose( transform_train_list ),\n    \'val\': transforms.Compose(transform_val_list),\n}\n\n\ntrain_all = \'\'\nif opt.train_all:\n     train_all = \'_all\'\n\nimage_datasets = {}\nimage_datasets[\'train\'] = datasets.ImageFolder(os.path.join(data_dir, \'train\' + train_all),\n                                          data_transforms[\'train\'])\nimage_datasets[\'val\'] = datasets.ImageFolder(os.path.join(data_dir, \'val\'),\n                                          data_transforms[\'val\'])\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=True, num_workers=8, pin_memory=True) # 8 workers may work faster\n              for x in [\'train\', \'val\']}\ndataset_sizes = {x: len(image_datasets[x]) for x in [\'train\', \'val\']}\nclass_names = image_datasets[\'train\'].classes\n\nuse_gpu = torch.cuda.is_available()\n\nsince = time.time()\ninputs, classes = next(iter(dataloaders[\'train\']))\nprint(time.time()-since)\n######################################################################\n# Training the model\n# ------------------\n#\n# Now, let\'s write a general function to train a model. Here, we will\n# illustrate:\n#\n# -  Scheduling the learning rate\n# -  Saving the best model\n#\n# In the following, parameter ``scheduler`` is an LR scheduler object from\n# ``torch.optim.lr_scheduler``.\n\ny_loss = {} # loss history\ny_loss[\'train\'] = []\ny_loss[\'val\'] = []\ny_err = {}\ny_err[\'train\'] = []\ny_err[\'val\'] = []\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    #best_model_wts = model.state_dict()\n    #best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(\'Epoch {}/{}\'.format(epoch, num_epochs - 1))\n        print(\'-\' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in [\'train\', \'val\']:\n            if phase == \'train\':\n                scheduler.step()\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0.0\n            # Iterate over data.\n            for data in dataloaders[phase]:\n                # get the inputs\n                inputs, labels = data\n                now_batch_size,c,h,w = inputs.shape\n                if now_batch_size<opt.batchsize: # skip the last batch\n                    continue\n                #print(inputs.shape)\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = Variable(inputs.cuda().detach())\n                    labels = Variable(labels.cuda().detach())\n                else:\n                    inputs, labels = Variable(inputs), Variable(labels)\n                # if we use low precision, input also need to be fp16\n                #if fp16:\n                #    inputs = inputs.half()\n \n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                if phase == \'val\':\n                    with torch.no_grad():\n                        outputs = model(inputs)\n                else:\n                    outputs = model(inputs)\n\n                if not opt.PCB:\n                    _, preds = torch.max(outputs.data, 1)\n                    loss = criterion(outputs, labels)\n                else:\n                    part = {}\n                    sm = nn.Softmax(dim=1)\n                    num_part = 6\n                    for i in range(num_part):\n                        part[i] = outputs[i]\n\n                    score = sm(part[0]) + sm(part[1]) +sm(part[2]) + sm(part[3]) +sm(part[4]) +sm(part[5])\n                    _, preds = torch.max(score.data, 1)\n\n                    loss = criterion(part[0], labels)\n                    for i in range(num_part-1):\n                        loss += criterion(part[i+1], labels)\n\n                # backward + optimize only if in training phase\n                if phase == \'train\':\n                    if fp16: # we use optimier to backward loss\n                        with amp.scale_loss(loss, optimizer) as scaled_loss:\n                            scaled_loss.backward()\n                    else:\n                        loss.backward()\n                    optimizer.step()\n\n                # statistics\n                if int(version[0])>0 or int(version[2]) > 3: # for the new version like 0.4.0, 0.5.0 and 1.0.0\n                    running_loss += loss.item() * now_batch_size\n                else :  # for the old version like 0.3.0 and 0.3.1\n                    running_loss += loss.data[0] * now_batch_size\n                running_corrects += float(torch.sum(preds == labels.data))\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n            \n            print(\'{} Loss: {:.4f} Acc: {:.4f}\'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            y_loss[phase].append(epoch_loss)\n            y_err[phase].append(1.0-epoch_acc)            \n            # deep copy the model\n            if phase == \'val\':\n                last_model_wts = model.state_dict()\n                if epoch%10 == 9:\n                    save_network(model, epoch)\n                draw_curve(epoch)\n\n        time_elapsed = time.time() - since\n        print(\'Training complete in {:.0f}m {:.0f}s\'.format(\n            time_elapsed // 60, time_elapsed % 60))\n        print()\n\n    time_elapsed = time.time() - since\n    print(\'Training complete in {:.0f}m {:.0f}s\'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    #print(\'Best val Acc: {:4f}\'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(last_model_wts)\n    save_network(model, \'last\')\n    return model\n\n\n######################################################################\n# Draw Curve\n#---------------------------\nx_epoch = []\nfig = plt.figure()\nax0 = fig.add_subplot(121, title=""loss"")\nax1 = fig.add_subplot(122, title=""top1err"")\ndef draw_curve(current_epoch):\n    x_epoch.append(current_epoch)\n    ax0.plot(x_epoch, y_loss[\'train\'], \'bo-\', label=\'train\')\n    ax0.plot(x_epoch, y_loss[\'val\'], \'ro-\', label=\'val\')\n    ax1.plot(x_epoch, y_err[\'train\'], \'bo-\', label=\'train\')\n    ax1.plot(x_epoch, y_err[\'val\'], \'ro-\', label=\'val\')\n    if current_epoch == 0:\n        ax0.legend()\n        ax1.legend()\n    fig.savefig( os.path.join(\'./model\',name,\'train.jpg\'))\n\n######################################################################\n# Save model\n#---------------------------\ndef save_network(network, epoch_label):\n    save_filename = \'net_%s.pth\'% epoch_label\n    save_path = os.path.join(\'./model\',name,save_filename)\n    torch.save(network.cpu().state_dict(), save_path)\n    if torch.cuda.is_available():\n        network.cuda(gpu_ids[0])\n\n\n######################################################################\n# Finetuning the convnet\n# ----------------------\n#\n# Load a pretrainied model and reset final fully connected layer.\n#\n\nif opt.use_dense:\n    model = ft_net_dense(len(class_names), opt.droprate)\nelse:\n    model = ft_net(len(class_names), opt.droprate, opt.stride)\n\nif opt.PCB:\n    model = PCB(len(class_names))\n\nopt.nclasses = len(class_names)\n\nprint(model)\n\nif not opt.PCB:\n    ignored_params = list(map(id, model.model.fc.parameters() )) + list(map(id, model.classifier.parameters() ))\n    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n    optimizer_ft = optim.SGD([\n             {\'params\': base_params, \'lr\': 0.1*opt.lr},\n             {\'params\': model.model.fc.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier.parameters(), \'lr\': opt.lr}\n         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\nelse:\n    ignored_params = list(map(id, model.model.fc.parameters() ))\n    ignored_params += (list(map(id, model.classifier0.parameters() )) \n                     +list(map(id, model.classifier1.parameters() ))\n                     +list(map(id, model.classifier2.parameters() ))\n                     +list(map(id, model.classifier3.parameters() ))\n                     +list(map(id, model.classifier4.parameters() ))\n                     +list(map(id, model.classifier5.parameters() ))\n                     #+list(map(id, model.classifier6.parameters() ))\n                     #+list(map(id, model.classifier7.parameters() ))\n                      )\n    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n    optimizer_ft = optim.SGD([\n             {\'params\': base_params, \'lr\': 0.1*opt.lr},\n             {\'params\': model.model.fc.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier0.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier1.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier2.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier3.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier4.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier5.parameters(), \'lr\': opt.lr},\n             #{\'params\': model.classifier6.parameters(), \'lr\': 0.01},\n             #{\'params\': model.classifier7.parameters(), \'lr\': 0.01}\n         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\n\n# Decay LR by a factor of 0.1 every 40 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=40, gamma=0.1)\n\n######################################################################\n# Train and evaluate\n# ^^^^^^^^^^^^^^^^^^\n#\n# It should take around 1-2 hours on GPU. \n#\ndir_name = os.path.join(\'./model\',name)\nif not os.path.isdir(dir_name):\n    os.mkdir(dir_name)\n#record every run\ncopyfile(\'./train.py\', dir_name+\'/train.py\')\ncopyfile(\'./model.py\', dir_name+\'/model.py\')\n\n# save opts\nwith open(\'%s/opts.yaml\'%dir_name,\'w\') as fp:\n    yaml.dump(vars(opt), fp, default_flow_style=False)\n\n# model to gpu\nmodel = model.cuda()\nif fp16:\n    #model = network_to_half(model)\n    #optimizer_ft = FP16_Optimizer(optimizer_ft, static_loss_scale = 128.0)\n    model, optimizer_ft = amp.initialize(model, optimizer_ft, opt_level = ""O1"")\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=60)\n\n'"
thirdparty/his_evaluators/his_evaluators/metrics/bodynets/__init__.py,0,b'from .batch_smpl import SMPL\nfrom .hmr import HumanModelRecovery\n'
thirdparty/his_evaluators/his_evaluators/metrics/bodynets/batch_smpl.py,87,"b'"""""" \nTensorflow SMPL implementation as batch.\nSpecify joint types:\n\'coco\': Returns COCO+ 19 joints\n\'lsp\': Returns H3.6M-LSP 14 joints\nNote: To get original smpl joints, use self.J_transformed\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nVERT_NOSE = 331\nVERT_EAR_L = 3485\nVERT_EAR_R = 6880\nVERT_EYE_L = 2802\nVERT_EYE_R = 6262\n\n\ndef load_pickle_file(pkl_path):\n    import pickle\n    with open(pkl_path, \'rb\') as f:\n        data = pickle.load(f, encoding=\'latin1\')\n\n    return data\n\n\ndef batch_skew(vec, batch_size=None, device=""cpu""):\n    """"""\n    vec is N x 3, batch_size is int.\n\n    e.g. r = [rx, ry, rz]\n        skew(r) = [[ 0,    -rz,      ry],\n                   [ rz,     0,     -rx],\n                   [-ry,    rx,       0]]\n\n    returns N x 3 x 3. Skew_sym version of each matrix.\n    """"""\n\n    if batch_size is None:\n        batch_size = vec.shape[0]\n\n    col_inds = np.array([1, 2, 3, 5, 6, 7], dtype=np.int64)\n\n    # indices = torch.from_numpy(np.reshape(\n    #     np.reshape(np.arange(0, batch_size) * 9, [-1, 1]) + col_inds,\n    #     newshape=(-1,))).to(device)\n\n    # For better compatibility\xef\xbc\x8c since if indices is torch.tensor, it must be long dtype.\n    # For fixed index, np.ndarray might be better.\n    indices = np.reshape(np.reshape(\n        np.arange(0, batch_size) * 9, [-1, 1]) + col_inds, newshape=(-1, )).astype(np.int64)\n\n    updates = torch.stack(\n        [\n            -vec[:, 2], vec[:, 1], vec[:, 2],\n            -vec[:, 0], -vec[:, 1], vec[:, 0]\n        ],\n        dim=1\n    ).view(-1).to(device)\n\n    res = torch.zeros(batch_size * 9, dtype=vec.dtype).to(device)\n    res[indices] = updates\n    res = res.view(batch_size, 3, 3)\n\n    return res\n\n\ndef batch_rodrigues(theta):\n    """"""\n    Theta is N x 3\n\n    rodrigues (from cv2.rodrigues):\n    source: https://docs.opencv.org/3.0-beta/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html\n    input: r (3 x 1)\n    output: R (3 x 3)\n\n        angle = norm(r)\n        r = r / angle\n\n        skew(r) = [[ 0,    -rz,      ry],\n                   [ rz,     0,     -rx],\n                   [-ry,    rx,       0]]\n\n        R = cos(theta * eye(3) + (1 - cos(theta)) * r * r.T + sin(theta) *  skew(r)\n    """"""\n    batch_size = theta.shape[0]\n    device = theta.device\n\n    # angle (batch_size, 1), r (batch_size, 3)\n    angle = torch.norm(theta + 1e-8, p=2, dim=1, keepdim=True)\n    r = torch.div(theta, angle)\n\n    # angle (batch_size, 1, 1), r (batch_size, 3, 1)\n    angle = angle.unsqueeze(-1)\n    r = r.unsqueeze(-1)\n\n    cos = torch.cos(angle)\n    sin = torch.sin(angle)\n\n    # outer (batch_size, 3, 3)\n    outer = torch.matmul(r, r.permute(0, 2, 1))\n    eyes = torch.eye(3, dtype=torch.float32).unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n\n    R = cos * eyes + (1 - cos) * outer + sin * batch_skew(r, batch_size=batch_size, device=device)\n\n    return R\n\n\ndef batch_lrotmin(theta):\n    """""" NOTE: not used bc I want to reuse R and this is simple.\n    Output of this is used to compute joint-to-pose blend shape mapping.\n    Equation 9 in SMPL paper.\n\n\n    Args:\n      pose: `Tensor`, N x 72 vector holding the axis-angle rep of K joints.\n            This includes the global rotation so K=24\n\n    Returns\n      diff_vec : `Tensor`: N x 207 rotation matrix of 23=(K-1) joints with identity subtracted.,\n    """"""\n\n    # ignore global, N x 72\n    theta = theta[:, 3:]\n    # (N*23) x 3 x 3\n    # reshape = contiguous + view\n    Rs = batch_rodrigues(theta.reshape(-1, 3))\n    eye = torch.eye(3).to(torch.eye(3))\n    lrotmin = (Rs - eye).view(-1, 207)\n\n    return lrotmin\n\n\ndef batch_global_rigid_transformation(Rs, Js, parent, rotate_base=False, device=""cpu""):\n    """"""\n    Computes absolute joint locations given pose.\n\n    rotate_base: if True, rotates the global rotation by 90 deg in x axis.\n    if False, this is the original SMPL coordinate.\n\n    Args:\n      Rs: N x 24 x 3 x 3 rotation vector of K joints\n      Js: N x 24 x 3, joint locations before posing\n      parent: 24 holding the parent id for each index\n\n    Returns\n      new_J : `Tensor`: N x 24 x 3 location of absolute joints\n      A     : `Tensor`: N x 24 x 4 x 4 relative joint transformations for LBS.\n    """"""\n\n    N = Rs.shape[0]\n    if rotate_base:\n        # print(\'Flipping the SMPL coordinate frame!!!!\')\n        # rot_x = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]], dtype=Rs.dtype)\n        # rot_x = np.reshape(np.tile(rot_x, [N, 1]), (N, 3, 3))\n        # root_rotation = np.matmul(Rs[:, 0, :, :], rot_x)\n\n        rot_x = torch.from_numpy(np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]],\n                                          dtype=np.float32)).type(Rs.dtype).to(device)\n\n        # rot_x = torch.from_numpy(np.array([[-1, 0, 0], [0, 1, 0], [0, 0, -1]],\n        #                                   dtype=np.float32)).type(Rs.dtype).to(device)\n\n        rot_x = rot_x.repeat(N, 1).view(N, 3, 3)\n        root_rotation = torch.matmul(Rs[:, 0, :, :], rot_x)\n    else:\n        root_rotation = Rs[:, 0, :, :]\n\n    # Now Js is N x 24 x 3 x 1\n    Js = Js.unsqueeze(-1)\n\n    def make_A(R, t):\n        """"""\n        Composite homogeneous matrix.\n        Args:\n            R: N x 3 x 3 rotation matrix.\n            t: N x 3 x 1 translation vector.\n\n        Returns:\n            homogeneous matrix N x 4 x 4.\n        """"""\n\n        # # Rs is N x 3 x 3, ts is N x 3 x 1\n        # R_homo = np.pad(R, [[0, 0], [0, 1], [0, 0]], mode=\'constant\')\n        # t_homo = np.concatenate([t, np.ones((N, 1, 1))], 1)\n        # return np.concatenate([R_homo, t_homo], 2)\n\n        # Pad to (N, 4, 3)\n        R_homo = F.pad(R, (0, 0, 0, 1, 0, 0), mode=\'constant\', value=0)\n        # Concatenate to (N, 4, 1)\n        t_homo = torch.cat([t, torch.ones(N, 1, 1, dtype=Rs.dtype).to(device)], dim=1)\n        return torch.cat([R_homo, t_homo], dim=2)\n\n    # root_rotation: (N, 3, 3), Js[:, 0]: (N, 3, 1)\n    # ---------- A0: (N, 4, 4)\n    A0 = make_A(root_rotation, Js[:, 0])\n    results = [A0]\n    for i in range(1, parent.shape[0]):\n        j_here = Js[:, i] - Js[:, parent[i]]\n        A_here = make_A(Rs[:, i], j_here)\n        res_here = torch.matmul(results[parent[i]], A_here)\n        results.append(res_here)\n\n    # N x 24 x 4 x 4\n    results = torch.stack(results, dim=1)\n\n    new_J = results[:, :, :3, 3]\n\n    # --- Compute relative A: Skinning is based on\n    # how much the bone moved (not the final location of the bone)\n    # but (final_bone - init_bone)\n    # ---\n\n    # Js_w0: (N, 24, 4, 1)\n    Js_w0 = torch.cat([Js, torch.zeros(N, 24, 1, 1, dtype=Rs.dtype).to(device)], dim=2)\n\n    # init_bone: (N, 24, 4, 1) = (N, 24, 4, 4) x (N, 24, 4, 1)\n    init_bone = torch.matmul(results, Js_w0)\n    # Append empty 4 x 3:\n    init_bone = F.pad(init_bone, (3, 0, 0, 0, 0, 0, 0, 0), mode=\'constant\', value=0)\n    A = results - init_bone\n\n    return new_J, A\n\n\ndef batch_orth_proj_idrot(X, camera, device=""cpu""):\n    """"""\n    X is N x num_points x 3\n    camera is N x 3\n    same as applying orth_proj_idrot to each N\n    """"""\n\n    # TODO check X dim size.\n\n    # X_trans is (N, num_points, 2)\n    X_trans = X[:, :, :2] + camera[:, None, 1:]\n    return camera[:, None, 0:1] * X_trans\n\n\ndef batch_quat_rotation(theta_quat):\n    """"""\n    Args:\n        theta_quat: (N, 4)\n\n    Returns:\n        rotations: (N, num_joint, 3, 3)\n    """"""\n\n    x = theta_quat[:, 0]\n    y = theta_quat[:, 1]\n    z = theta_quat[:, 2]\n    w = theta_quat[:, 3]\n\n    x2 = x * x\n    y2 = y * y\n    z2 = z * z\n    w2 = w * w\n\n    xy = x * y\n    zw = z * w\n    xz = x * z\n    yw = y * w\n    yz = y * z\n    xw = x * w\n\n    res_R = torch.stack([\n        x2 - y2 - z2 + w2, 2 * (xy - zw), 2 * (xz + yw),\n        2 * (xy + zw), - x2 + y2 - z2 + w2, 2 * (yz - xw),\n        2 * (xz - yw), 2 * (yz + xw), - x2 - y2 + z2 + w2\n    ], dim=1).view(-1, 3, 3)\n\n    return res_R\n\n\nclass SMPL(nn.Module):\n    def __init__(self, pkl_path, rotate=False):\n        """"""\n        pkl_path is the path to a SMPL model\n        """"""\n        super(SMPL, self).__init__()\n        self.rotate = rotate\n\n        # -- Load SMPL params --\n        dd = load_pickle_file(pkl_path)\n\n        # define faces\n        # self.register_buffer(\'faces\', torch.from_numpy(undo_chumpy(dd[\'f\']).astype(np.int32)).type(dtype=torch.int32))\n        # self.faces = torch.from_numpy(dd[\'f\'].astype(np.int32)).type(dtype=torch.int32)\n\n        # Mean template vertices\n        self.register_buffer(\'v_template\', torch.FloatTensor(dd[\'v_template\']))\n        # Size of mesh [Number of vertices, 3], (6890, 3)\n        self.size = [self.v_template.shape[0], 3]\n        self.num_betas = dd[\'shapedirs\'].shape[-1]\n        # Shape blend shape basis (shapedirs): (6980, 3, 10)\n        # reshaped to (6980*3, 10), transposed to (10, 6980*3)\n        self.register_buffer(\'shapedirs\', torch.FloatTensor(np.reshape(\n            dd[\'shapedirs\'], [-1, self.num_betas]).T))\n\n        # Regressor for joint locations given shape -> (24, 6890)\n        # Transpose to shape (6890, 24)\n        self.register_buffer(\'J_regressor\', torch.FloatTensor(\n            np.asarray(dd[\'J_regressor\'].T.todense())))\n\n        # Pose blend shape basis: (6890, 3, 207)\n        num_pose_basis = dd[\'posedirs\'].shape[-1]\n\n        # Pose blend pose basis is reshaped to (6890*3, 207)\n        # posedirs is transposed into (207, 6890*3)\n        self.register_buffer(\'posedirs\', torch.FloatTensor(np.reshape(\n            dd[\'posedirs\'], [-1, num_pose_basis]).T))\n\n        # indices of parents for each joints\n        self.parents = np.array(dd[\'kintree_table\'][0].astype(np.int32))\n\n        # LBS weights (6890, 24)\n        self.register_buffer(\'weights\', torch.FloatTensor(dd[\'weights\']))\n\n        # This returns 19 keypoints: 6890 x 19\n        joint_regressor = torch.FloatTensor(\n            np.asarray(dd[\'cocoplus_regressor\'].T.todense()))\n\n        self.register_buffer(\'joint_regressor\', joint_regressor)\n\n    def forward(self, beta, theta, offsets=0, get_skin=False):\n        """"""\n        Obtain SMPL with shape (beta) & pose (theta) inputs.\n        Theta includes the global rotation.\n        Args:\n          beta: N x 10\n          theta: N x 72 (with 3-D axis-angle rep)\n          offsets: N x 6890 x 3\n          get_skin: boolean, return skin or not\n\n        Updates:\n        self.J_transformed: N x 24 x 3 joint location after shaping\n                 & posing with beta and theta\n        Returns:\n          - joints: N x 19 or 14 x 3 joint locations depending on joint_type\n        If get_skin is True, also returns\n          - Verts: N x 6980 x 3\n        """"""\n        device = beta.device\n\n        num_batch = beta.shape[0]\n\n        # 1. Add shape blend shapes\n        #       matmul  : (N, 10) x (10, 6890*3) = (N, 6890*3)\n        #       reshape : (N, 6890*3) -> (N, 6890, 3)\n        #       v_shaped: (N, 6890, 3)\n        v_shaped = torch.matmul(beta, self.shapedirs).view(-1, self.size[0], self.size[1]) + self.v_template + offsets\n\n        # 2. Infer shape-dependent joint locations.\n        # ----- J_regressor: (6890, 24)\n        # ----- Jx (Jy, Jz): (N, 6890) x (6890, 24) = (N, 24)\n        # --------------- J: (N, 24, 3)\n        Jx = torch.matmul(v_shaped[:, :, 0], self.J_regressor)\n        Jy = torch.matmul(v_shaped[:, :, 1], self.J_regressor)\n        Jz = torch.matmul(v_shaped[:, :, 2], self.J_regressor)\n        J = torch.stack([Jx, Jy, Jz], dim=2)\n\n        # 3. Add pose blend shapes\n        # ------- theta    : (N, 72)   or (N, 96)\n        # ------- reshape  : (N*24, 3) or (N * 24, 4)\n        # ------- rodrigues: (N*24, 9) or quat rotations (N * 24, 9)\n        # -- Rs = reshape  : (N, 24, 3, 3)\n        if theta.shape[-1] == 72:  # Euler angles\n            Rs = batch_rodrigues(theta.view(-1, 3)).view(-1, 24, 3, 3)\n        else:   # quat angles\n            Rs = batch_quat_rotation(theta.view(-1, 4)).view(-1, 24, 3, 3)\n        # Ignore global rotation.\n        #       Rs[:, 1:, :, :]: (N, 23, 3, 3)\n        #           - np.eye(3): (N, 23, 3, 3)\n        #          pose_feature: (N, 207)\n        pose_feature = (Rs[:, 1:, :, :] - torch.eye(3).to(device)).view(-1, 207)\n\n        # (N, 207) x (207, 6890*3) -> (N, 6890, 3)\n        v_posed = torch.matmul(pose_feature, self.posedirs).view(-1, self.size[0], self.size[1]) + v_shaped\n\n        # 4. Get the global joint location\n        # ------- Rs is (N, 24, 3, 3),         J is (N, 24, 3)\n        # ------- J_transformed is (N, 24, 3), A is (N, 24, 4, 4)\n        J_transformed, A = batch_global_rigid_transformation(Rs, J, self.parents, device=device,\n                                                             rotate_base=self.rotate)\n\n        # 5. Do skinning:\n        # ------- weights is (6890, 24)\n        # ---------- tile is (N*6890, 24)\n        # --- W = reshape is (N, 6890, 24)\n        W = self.weights.repeat(num_batch, 1).view(num_batch, -1, 24)\n\n        # ------ reshape A is (N, 24, 16)\n        # --------- matmul is (N, 6890, 24) x (N, 24, 16) -> (N, 6890, 16)\n        # -------- reshape is (N, 6890, 4, 4)\n        T = torch.matmul(W, A.view(num_batch, 24, 16)).view(num_batch, -1, 4, 4)\n\n        # axis is 2, (N, 6890, 3) concatenate (N, 6890, 1) -> (N, 6890, 4)\n        v_posed_homo = torch.cat(\n            [v_posed, torch.ones(num_batch, v_posed.shape[1], 1, dtype=torch.float32).to(device)], dim=2)\n\n        # -unsqueeze_ is (N, 6890, 4, 1)\n        # --------- T is (N, 6890, 4, 4)\n        # ---- matmul is (N, 6890, 4, 4) x (N, 6890, 4, 1) -> (N, 6890, 4, 1)\n        v_posed_homo = v_posed_homo.unsqueeze(-1)\n        v_homo = torch.matmul(T, v_posed_homo)\n\n        # (N, 6890, 3)\n        verts = v_homo[:, :, :3, 0]\n\n        # Get cocoplus or lsp joints: (N, 6890) x (6890, 19)\n        joint_x = torch.matmul(verts[:, :, 0], self.joint_regressor)\n        joint_y = torch.matmul(verts[:, :, 1], self.joint_regressor)\n        joint_z = torch.matmul(verts[:, :, 2], self.joint_regressor)\n        joints = torch.stack([joint_x, joint_y, joint_z], dim=2)\n\n        if get_skin:\n            return verts, joints, Rs\n        else:\n            return joints\n\n    def skinning(self, theta, offsets=0) -> dict:\n        """"""\n        Args:\n            theta: (N, 3 + 72 + 10)\n            offsets (torch.tensor) : (N, nv, 3) or 0\n        Returns:\n\n        """"""\n\n        cam = theta[:, 0:3]\n        pose = theta[:, 3:-10].contiguous()\n        shape = theta[:, -10:].contiguous()\n        verts, j3d, rs = self.forward(beta=shape, theta=pose, offsets=offsets, get_skin=True)\n\n        detail_info = {\n            \'cam\': cam,\n            \'pose\': pose,\n            \'shape\': shape,\n            \'verts\': verts,\n            \'theta\': theta\n        }\n\n        return detail_info\n\n    def get_details(self, theta, offsets=0) -> dict:\n        """"""\n            calc verts, joint2d, joint3d, Rotation matrix\n        Args:\n            theta (torch.tensor): (N, 85) = (N , 3 + 72 + 10)\n            offsets (torch.tensor) : (N, nv, 3) or 0\n\n        Returns:\n            detail_info (dict): the details information of smpl, including\n                --theta (torch.tensor): (N, 85),\n                --cam (torch.tensor):   (N, 3),\n                --pose (torch.tensor):  (N, 72),\n                --shape (torch.tensor): (N, 10),\n                --verts (torch.tensor): (N, 6890, 3),\n                --j2d (torch.tensor):   (N, 19, 2),\n                --j3d (torch.tensor):   (N, 19, 3)\n\n        """"""\n\n        cam = theta[:, 0:3]\n        pose = theta[:, 3:-10].contiguous()\n        shape = theta[:, -10:].contiguous()\n        verts, j3d, rs = self.forward(beta=shape, theta=pose, offsets=offsets, get_skin=True)\n        j2d = batch_orth_proj_idrot(j3d, cam)\n\n        detail_info = {\n            \'theta\': theta,\n            \'cam\': cam,\n            \'pose\': pose,\n            \'shape\': shape,\n            \'verts\': verts,\n            \'j2d\': j2d,\n            \'j3d\': j3d\n        }\n\n        return detail_info\n\n\n# class SMPL(nn.Module):\n#     def __init__(self, pkl_path, rotate=False):\n#         """"""\n#         pkl_path is the path to a SMPL model\n#         """"""\n#         super(SMPL, self).__init__()\n#         self.rotate = rotate\n#\n#         # -- Load SMPL params --\n#         dd = load_pickle_file(pkl_path)\n#\n#         # define faces\n#         # self.register_buffer(\'faces\', torch.from_numpy(undo_chumpy(dd[\'f\']).astype(np.int32)).type(dtype=torch.int32))\n#         # self.faces = torch.from_numpy(dd[\'f\'].astype(np.int32)).type(dtype=torch.int32)\n#\n#         # Mean template vertices\n#         self.register_buffer(\'v_template\', torch.FloatTensor(dd[\'v_template\']))\n#         # Size of mesh [Number of vertices, 3], (6890, 3)\n#         self.size = [self.v_template.shape[0], 3]\n#         self.num_betas = dd[\'shapedirs\'].shape[-1]\n#         # Shape blend shape basis (shapedirs): (6980, 3, 10)\n#         # reshaped to (6980*3, 10), transposed to (10, 6980*3)\n#         self.register_buffer(\'shapedirs\', torch.FloatTensor(np.reshape(\n#             dd[\'shapedirs\'], [-1, self.num_betas]).T))\n#\n#         # Regressor for joint locations given shape -> (24, 6890)\n#         # Transpose to shape (6890, 24)\n#         self.register_buffer(\'J_regressor\', torch.FloatTensor(\n#             np.asarray(dd[\'J_regressor\'].T.todense())))\n#\n#         # Pose blend shape basis: (6890, 3, 207)\n#         num_pose_basis = dd[\'posedirs\'].shape[-1]\n#\n#         # Pose blend pose basis is reshaped to (6890*3, 207)\n#         # posedirs is transposed into (207, 6890*3)\n#         self.register_buffer(\'posedirs\', torch.FloatTensor(np.reshape(\n#             dd[\'posedirs\'], [-1, num_pose_basis]).T))\n#\n#         # indices of parents for each joints\n#         self.parents = np.array(dd[\'kintree_table\'][0].astype(np.int32))\n#\n#         # LBS weights (6890, 24)\n#         self.register_buffer(\'weights\', torch.FloatTensor(dd[\'weights\']))\n#\n#         # This returns 19 keypoints: 6890 x 19\n#         joint_regressor = torch.FloatTensor(\n#             np.asarray(dd[\'cocoplus_regressor\'].T.todense()))\n#\n#         self.register_buffer(\'joint_regressor\', joint_regressor)\n#\n#     def forward(self, beta, theta, get_skin=False):\n#         """"""\n#         Obtain SMPL with shape (beta) & pose (theta) inputs.\n#         Theta includes the global rotation.\n#         Args:\n#           beta: N x 10\n#           theta: N x 72 (with 3-D axis-angle rep)\n#           get_skin: boolean, return skin or not\n#\n#         Updates:\n#         self.J_transformed: N x 24 x 3 joint location after shaping\n#                  & posing with beta and theta\n#         Returns:\n#           - joints: N x 19 or 14 x 3 joint locations depending on joint_type\n#         If get_skin is True, also returns\n#           - Verts: N x 6980 x 3\n#         """"""\n#         device = beta.device\n#\n#         num_batch = beta.shape[0]\n#\n#         # 1. Add shape blend shapes\n#         #       matmul  : (N, 10) x (10, 6890*3) = (N, 6890*3)\n#         #       reshape : (N, 6890*3) -> (N, 6890, 3)\n#         #       v_shaped: (N, 6890, 3)\n#         v_shaped = torch.matmul(beta, self.shapedirs).view(-1, self.size[0], self.size[1]) + self.v_template\n#\n#         # 2. Infer shape-dependent joint locations.\n#         # ----- J_regressor: (6890, 24)\n#         # ----- Jx (Jy, Jz): (N, 6890) x (6890, 24) = (N, 24)\n#         # --------------- J: (N, 24, 3)\n#         Jx = torch.matmul(v_shaped[:, :, 0], self.J_regressor)\n#         Jy = torch.matmul(v_shaped[:, :, 1], self.J_regressor)\n#         Jz = torch.matmul(v_shaped[:, :, 2], self.J_regressor)\n#         J = torch.stack([Jx, Jy, Jz], dim=2)\n#\n#         # 3. Add pose blend shapes\n#         # ------- theta    : (N, 72)   or (N, 96)\n#         # ------- reshape  : (N*24, 3) or (N * 24, 4)\n#         # ------- rodrigues: (N*24, 9) or quat rotations (N * 24, 9)\n#         # -- Rs = reshape  : (N, 24, 3, 3)\n#         if theta.shape[-1] == 72:  # Euler angles\n#             Rs = batch_rodrigues(theta.view(-1, 3)).view(-1, 24, 3, 3)\n#         else:   # quat angles\n#             Rs = batch_quat_rotation(theta.view(-1, 4)).view(-1, 24, 3, 3)\n#         # Ignore global rotation.\n#         #       Rs[:, 1:, :, :]: (N, 23, 3, 3)\n#         #           - np.eye(3): (N, 23, 3, 3)\n#         #          pose_feature: (N, 207)\n#         pose_feature = (Rs[:, 1:, :, :] - torch.eye(3).to(device)).view(-1, 207)\n#\n#         # (N, 207) x (207, 6890*3) -> (N, 6890, 3)\n#         v_posed = torch.matmul(pose_feature, self.posedirs).view(-1, self.size[0], self.size[1]) + v_shaped\n#\n#         # 4. Get the global joint location\n#         # ------- Rs is (N, 24, 3, 3),         J is (N, 24, 3)\n#         # ------- J_transformed is (N, 24, 3), A is (N, 24, 4, 4)\n#         J_transformed, A = batch_global_rigid_transformation(Rs, J, self.parents, device=device,\n#                                                              rotate_base=self.rotate)\n#\n#         # 5. Do skinning:\n#         # ------- weights is (6890, 24)\n#         # ---------- tile is (N*6890, 24)\n#         # --- W = reshape is (N, 6890, 24)\n#         W = self.weights.repeat(num_batch, 1).view(num_batch, -1, 24)\n#\n#         # ------ reshape A is (N, 24, 16)\n#         # --------- matmul is (N, 6890, 24) x (N, 24, 16) -> (N, 6890, 16)\n#         # -------- reshape is (N, 6890, 4, 4)\n#         T = torch.matmul(W, A.view(num_batch, 24, 16)).view(num_batch, -1, 4, 4)\n#\n#         # axis is 2, (N, 6890, 3) concatenate (N, 6890, 1) -> (N, 6890, 4)\n#         v_posed_homo = torch.cat(\n#             [v_posed, torch.ones(num_batch, v_posed.shape[1], 1, dtype=torch.float32).to(device)], dim=2)\n#\n#         # -unsqueeze_ is (N, 6890, 4, 1)\n#         # --------- T is (N, 6890, 4, 4)\n#         # ---- matmul is (N, 6890, 4, 4) x (N, 6890, 4, 1) -> (N, 6890, 4, 1)\n#         v_posed_homo = v_posed_homo.unsqueeze(-1)\n#         v_homo = torch.matmul(T, v_posed_homo)\n#\n#         # (N, 6890, 3)\n#         verts = v_homo[:, :, :3, 0]\n#\n#         # Get cocoplus or lsp joints: (N, 6890) x (6890, 19)\n#         joint_x = torch.matmul(verts[:, :, 0], self.joint_regressor)\n#         joint_y = torch.matmul(verts[:, :, 1], self.joint_regressor)\n#         joint_z = torch.matmul(verts[:, :, 2], self.joint_regressor)\n#         joints = torch.stack([joint_x, joint_y, joint_z], dim=2)\n#\n#         if get_skin:\n#             return verts, joints, Rs\n#         else:\n#             return joints\n#\n#     def skinning(self, theta) -> dict:\n#         """"""\n#         Args:\n#             theta: (N, 3 + 72 + 10)\n#\n#         Returns:\n#\n#         """"""\n#\n#         cam = theta[:, 0:3]\n#         pose = theta[:, 3:-10].contiguous()\n#         shape = theta[:, -10:].contiguous()\n#         verts, j3d, rs = self.forward(beta=shape, theta=pose, get_skin=True)\n#\n#         detail_info = {\n#             \'cam\': cam,\n#             \'pose\': pose,\n#             \'shape\': shape,\n#             \'verts\': verts,\n#             \'theta\': theta\n#         }\n#\n#         return detail_info\n#\n#     def get_details(self, theta) -> dict:\n#         """"""\n#             calc verts, joint2d, joint3d, Rotation matrix\n#         Args:\n#             theta (torch.tensor): (N, 85) = (N , 3 + 72 + 10)\n#\n#         Returns:\n#             detail_info (dict): the details information of smpl, including\n#                 --theta (torch.tensor): (N, 85),\n#                 --cam (torch.tensor):   (N, 3),\n#                 --pose (torch.tensor):  (N, 72),\n#                 --shape (torch.tensor): (N, 10),\n#                 --verts (torch.tensor): (N, 6890, 3),\n#                 --j2d (torch.tensor):   (N, 19, 2),\n#                 --j3d (torch.tensor):   (N, 19, 3)\n#\n#         """"""\n#\n#         cam = theta[:, 0:3]\n#         pose = theta[:, 3:-10].contiguous()\n#         shape = theta[:, -10:].contiguous()\n#         verts, j3d, rs = self.forward(beta=shape, theta=pose, get_skin=True)\n#         j2d = batch_orth_proj_idrot(j3d, cam)\n#\n#         detail_info = {\n#             \'theta\': theta,\n#             \'cam\': cam,\n#             \'pose\': pose,\n#             \'shape\': shape,\n#             \'verts\': verts,\n#             \'j2d\': j2d,\n#             \'j3d\': j3d\n#         }\n#\n#         return detail_info\n'"
thirdparty/his_evaluators/his_evaluators/metrics/bodynets/hmr.py,4,"b'# -*- coding: utf-8 -*-\n# @Time    : 2018/10/29 8:20 PM\n# @Author  : Zhixin Piao\n# @Email   : piaozhx@shanghaitech.edu.cn\n# @Update  : Wen Liu (liuwen@shanghaitech.edu.cn)\n\n""""""Pre-activation ResNet and HumanMeshRecovery (`HumanModelRecovery`) in PyTorch.\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n[2] Human Mesh Recovery, CVPR 2018\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport numpy as np\nimport h5py\n\nfrom .batch_smpl import SMPL\n\n\ndef subsample(inputs, factor):\n    """"""Subsamples the input along the spatial dimensions.\n\n    Args:\n      inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n      factor: The subsampling factor.\n\n    Returns:\n      output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n        input, either intact (if factor == 1) or subsampled (if factor > 1).\n    """"""\n    if factor == 1:\n        return inputs\n    else:\n        return F.max_pool2d(inputs, [1, 1], stride=factor)\n\n\nclass PreActBlock(nn.Module):\n    \'\'\'Pre-activation version of the BasicBlock.\'\'\'\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        self.stride = stride\n\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    \'\'\'Pre-activation version of the original Bottleneck module.\'\'\'\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        # tf implementation there needs bias\n        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=True)\n\n        self.stride = stride\n\n        # if stride != 1 or in_planes != self.expansion * planes:\n        #     self.shortcut = nn.Sequential(\n        #         # tf implementation there needs bias\n        #         nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=True)\n        #     )\n        if in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                # tf implementation there needs bias\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=True)\n            )\n\n    def forward(self, x, layer_id=\'layer1\', block_id=\'0\', results_outs=OrderedDict()):\n        # out = F.relu(self.bn1(x))\n        # shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        # out = self.conv1(out)\n        # out = self.conv2(F.relu(self.bn2(out)))\n        # out = self.conv3(F.relu(self.bn3(out)))\n        # out += shortcut\n\n        # layer1.0.bn1\n        out_name = \'{}.{}\'.format(layer_id, block_id)\n\n        preact = F.relu(self.bn1(x))\n        shortcut_out = self.shortcut(preact) if hasattr(self, \'shortcut\') else subsample(x, factor=self.stride)\n        conv1_out = F.relu(self.bn2(self.conv1(preact)))\n        conv2_out = F.relu(self.bn3(self.conv2(conv1_out)))\n        conv3_out = self.conv3(conv2_out)\n        conv3_out_add = conv3_out + shortcut_out\n\n        results_outs[out_name + \'.shortcut.0\'] = shortcut_out\n        results_outs[out_name + \'.conv1\'] = conv1_out\n        results_outs[out_name + \'.conv2\'] = conv2_out\n        results_outs[out_name + \'.conv3\'] = conv3_out\n        results_outs[out_name] = conv3_out_add\n\n        return conv3_out_add\n\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks):\n        super(PreActResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=True)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=2)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=1)\n        self.post_bn = nn.BatchNorm2d(2048)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        # layers = []\n        # layers.append(block(self.in_planes, planes, stride))\n        #\n        # self.in_planes = planes * block.expansion\n        # for i in range(1, num_blocks):\n        #     layers.append(block(self.in_planes, planes))\n        # return nn.Sequential(*layers)\n\n        layers = []\n        layers.append(block(self.in_planes, planes, 1))\n\n        self.in_planes = planes * block.expansion\n        for i in range(1, num_blocks):\n            s = stride if i == num_blocks - 1 else 1\n            layers.append(block(self.in_planes, planes, stride=s))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        # here need padding =1\n        # out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n        out = F.max_pool2d(out, kernel_size=3, stride=2, ceil_mode=True)\n\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.relu(self.post_bn(out))\n\n        # need global avg_pooling\n        out = F.avg_pool2d(out, 7)\n\n        out = out.view(out.size(0), -1)\n        return out\n\n\ndef preActResNet18():\n    return PreActResNet(PreActBlock, [2, 2, 2, 2])\n\n\ndef preActResNet34():\n    return PreActResNet(PreActBlock, [3, 4, 6, 3])\n\n\ndef preActResNet50():\n    return PreActResNet(PreActBottleneck, [3, 4, 6, 3])\n\n\ndef preActResNet101():\n    return PreActResNet(PreActBottleneck, [3, 4, 23, 3])\n\n\ndef preActResNet152():\n    return PreActResNet(PreActBottleneck, [3, 8, 36, 3])\n\n\ndef load_mean_theta(smpl_mean_theta_path, total_theta_count=85):\n    mean = np.zeros(total_theta_count, dtype=np.float)\n\n    if smpl_mean_theta_path:\n        mean_values = h5py.File(smpl_mean_theta_path, \'r\')\n        mean_pose = mean_values[\'pose\'][...]\n        # Ignore the global rotation.\n        mean_pose[:3] = 0\n        mean_shape = mean_values[\'shape\'][...]\n\n        # This initializes the global pose to be up-right when projected\n        mean_pose[0] = np.pi\n\n        # Initialize scale at 0.9\n        mean[0] = 0.9\n        mean[3:75] = mean_pose[:]\n        mean[75:] = mean_shape[:]\n\n        mean_values.close()\n    else:\n        mean[0] = 0.9\n\n    return mean\n\n\nclass ThetaRegressor(nn.Module):\n    def __init__(self, input_dim, out_dim, iterations=3):\n        super(ThetaRegressor, self).__init__()\n        self.iterations = iterations\n\n        # register mean theta\n        self.register_buffer(\'mean_theta\', torch.rand(out_dim, dtype=torch.float32))\n\n        fc_blocks = OrderedDict()\n\n        fc_blocks[\'fc1\'] = nn.Linear(input_dim, 1024, bias=True)\n        fc_blocks[\'relu1\'] = nn.ReLU()\n        fc_blocks[\'dropout1\'] = nn.Dropout(p=0.5)\n\n        fc_blocks[\'fc2\'] = nn.Linear(1024, 1024, bias=True)\n        fc_blocks[\'relu2\'] = nn.ReLU()\n\n        fc_blocks[\'dropout2\'] = nn.Dropout(p=0.5)\n\n        fc_blocks[\'fc3\'] = nn.Linear(1024, out_dim, bias=True)\n        # small_xavier initialization\n        nn.init.xavier_normal_(fc_blocks[\'fc3\'].weight, gain=0.1)\n        nn.init.zeros_(fc_blocks[\'fc3\'].bias)\n\n        self.fc_blocks = nn.Sequential(fc_blocks)\n\n    def forward(self, x):\n        """"""\n        :param x: the output of encoder, 2048 dim\n        :return: a list contains [[theta1, theta1, ..., theta1],\n                                 [theta2, theta2, ..., theta2], ... , ],\n                shape is iterations X N X 85(or other theta count)\n        """"""\n        batch_size = x.shape[0]\n        theta = self.mean_theta.repeat(batch_size, 1)\n        for _ in range(self.iterations):\n            total_inputs = torch.cat([x, theta], dim=1)\n            theta = theta + self.fc_blocks(total_inputs)\n\n        return theta\n\n\nclass HumanModelRecovery(nn.Module):\n    """"""\n        regressor can predict betas(include beta and theta which needed by SMPL) from coder\n        extracted from encoder in a iteration way\n    """"""\n\n    def __init__(self, smpl_pkl_path, feature_dim=2048, theta_dim=85, iterations=3):\n        super(HumanModelRecovery, self).__init__()\n\n        # define resnet50_v2\n        self.resnet = preActResNet50()\n\n        # define smpl\n        self.smpl = SMPL(pkl_path=smpl_pkl_path)\n\n        self.feature_dim = feature_dim\n        self.theta_dim = theta_dim\n\n        self.regressor = ThetaRegressor(feature_dim + theta_dim, theta_dim, iterations)\n        self.iterations = iterations\n\n    def forward(self, inputs):\n        out = self.resnet.conv1(inputs)\n\n        # here need padding =1\n        # out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n        out = F.max_pool2d(out, kernel_size=3, stride=2, ceil_mode=True)\n\n        out = self.resnet.layer1(out)\n\n        out = self.resnet.layer2(out)\n\n        out = self.resnet.layer3(out)\n\n        out = self.resnet.layer4(out)\n\n        out = F.relu(self.resnet.post_bn(out))\n        # need global avg_pooling\n        out = F.avg_pool2d(out, 7)\n\n        features = out.view(out.size(0), -1)\n\n        # regressor\n        thetas = self.regressor(features)\n\n        return thetas\n\n    def split(self, theta):\n        """"""\n        Args:\n            theta: (N, 3 + 72 + 10)\n\n        Returns:\n\n        """"""\n        cam = theta[:, 0:3]\n        pose = theta[:, 3:-10].contiguous()\n        shape = theta[:, -10:].contiguous()\n\n        detail_info = {\n            \'cam\': cam,\n            \'pose\': pose,\n            \'shape\': shape,\n            \'theta\': theta\n        }\n\n        return detail_info\n\n    def skinning(self, theta):\n        """"""\n        Args:\n            theta: (N, 3 + 72 + 10)\n\n        Returns:\n\n        """"""\n        return self.smpl.skinning(theta)\n\n    def get_details(self, theta):\n        """"""\n            purpose:\n                calc verts, joint2d, joint3d, Rotation matrix\n\n            inputs:\n                theta: N X (3 + 72 + 10)\n\n            return:\n                thetas, verts, j2d, j3d, Rs\n        """"""\n\n        return self.smpl.get_details(theta)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/__init__.py,0,"b'from .models.inception_resnet_v1 import InceptionResnetV1\nfrom .models.mtcnn import MTCNN, PNet, RNet, ONet, prewhiten, fixed_image_standardization\nfrom .models.utils.detect_face import extract_face\nfrom .models.utils import training\n'"
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/setup.py,3,"b'import setuptools, os\n\nPACKAGE_NAME = \'facenet-pytorch\'\nVERSION = \'2.2.9\'\nAUTHOR = \'Tim Esler\'\nEMAIL = \'tim.esler@gmail.com\'\nDESCRIPTION = \'Pretrained Pytorch face detection and recognition models\'\nGITHUB_URL = \'https://github.com/timesler/facenet-pytorch\'\n\nparent_dir = os.path.dirname(os.path.realpath(__file__))\nimport_name = os.path.basename(parent_dir)\n\nwith open(\'{}/README.md\'.format(parent_dir), \'r\') as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name=PACKAGE_NAME,\n    version=VERSION,\n    author=AUTHOR,\n    author_email=EMAIL,\n    description=DESCRIPTION,\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    url=GITHUB_URL,\n    packages=[\n        \'facenet_pytorch\',\n        \'facenet_pytorch.models\',\n        \'facenet_pytorch.models.utils\',\n        \'facenet_pytorch.data\',\n    ],\n    package_dir={\'facenet_pytorch\':\'.\'},\n    package_data={\'\': [\'*net.pt\']},\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n    install_requires=[\n        \'numpy\',\n        \'requests\',\n    ],\n)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/lpips/__init__.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch.nn\nfrom .models import dist_model\n\n\nclass PerceptualLoss(torch.nn.Module):\n    def __init__(self, model=\'net-lin\', net=\'vgg\', use_gpu=True):\n        # VGG using our perceptually-learned weights (LPIPS metric)\n        # def __init__(self, model=\'net\', net=\'vgg\', use_gpu=True): ""default"" way of using VGG\n        super(PerceptualLoss, self).__init__()\n        print(\'Setting up Perceptual loss...\')\n        self.model = dist_model.DistModel()\n        self.model.initialize(model=model, net=net, use_gpu=use_gpu)\n        print(\'...Done\')\n\n    def forward(self, pred, target, normalize=False):\n        """"""\n        Pred and target are Variables.\n        If normalize is on, assumes the images are between [0,1] and then scales thembetween [-1, 1]\n        If normalize is false, assumes the images are already between [-1,+1]\n\n        Inputs pred and target are Nx3xHxW\n        Output pytorch Variable N long\n        """"""\n        if normalize:\n            target = 2 * target - 1\n            pred = 2 * pred - 1\n\n        dist = self.model.forward_pair(target, pred)\n\n        return dist\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/__init__.py,0,"b'from .human_detector import YoLov3HumanDetector\nfrom .utils.datasets import pad_to_square, resize\n\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/detect.py,6,"b'from __future__ import division\n\nfrom .models import *\nfrom .utils.utils import *\nfrom .utils.datasets import *\n\nimport os\nimport time\nimport datetime\nimport argparse\n\nfrom PIL import Image\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.ticker import NullLocator\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--image_folder"", type=str, default=""data/samples"", help=""path to dataset"")\n    parser.add_argument(""--model_def"", type=str, default=""config/yolov3.cfg"", help=""path to model definition file"")\n    parser.add_argument(""--weights_path"", type=str, default=""weights/yolov3.weights"", help=""path to weights file"")\n    parser.add_argument(""--class_path"", type=str, default=""data/coco.names"", help=""path to class label file"")\n    parser.add_argument(""--conf_thres"", type=float, default=0.8, help=""object confidence threshold"")\n    parser.add_argument(""--nms_thres"", type=float, default=0.4, help=""iou thresshold for non-maximum suppression"")\n    parser.add_argument(""--batch_size"", type=int, default=1, help=""size of the batches"")\n    parser.add_argument(""--n_cpu"", type=int, default=0, help=""number of cpu threads to use during batch generation"")\n    parser.add_argument(""--img_size"", type=int, default=416, help=""size of each image dimension"")\n    parser.add_argument(""--checkpoint_model"", type=str, help=""path to checkpoint model"")\n    opt = parser.parse_args()\n    print(opt)\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    os.makedirs(""output"", exist_ok=True)\n\n    # Set up model\n    model = Darknet(opt.model_def, img_size=opt.img_size).to(device)\n\n    if opt.weights_path.endswith("".weights""):\n        # Load darknet weights\n        model.load_darknet_weights(opt.weights_path)\n    else:\n        # Load checkpoint weights\n        model.load_state_dict(torch.load(opt.weights_path))\n\n    model.eval()  # Set in evaluation mode\n\n    dataloader = DataLoader(\n        ImageFolder(opt.image_folder, img_size=opt.img_size),\n        batch_size=opt.batch_size,\n        shuffle=False,\n        num_workers=opt.n_cpu,\n    )\n\n    classes = load_classes(opt.class_path)  # Extracts class labels from file\n\n    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n\n    imgs = []  # Stores image paths\n    img_detections = []  # Stores detections for each image index\n\n    print(""\\nPerforming object detection:"")\n    prev_time = time.time()\n    for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n        # Configure input\n        input_imgs = Variable(input_imgs.type(Tensor))\n\n        # Get detections\n        with torch.no_grad():\n            detections = model(input_imgs)\n            detections = non_max_suppression(detections, opt.conf_thres, opt.nms_thres)\n\n        # Log progress\n        current_time = time.time()\n        inference_time = datetime.timedelta(seconds=current_time - prev_time)\n        prev_time = current_time\n        print(""\\t+ Batch %d, Inference Time: %s"" % (batch_i, inference_time))\n\n        # Save image and detections\n        imgs.extend(img_paths)\n        img_detections.extend(detections)\n\n    # Bounding-box colors\n    cmap = plt.get_cmap(""tab20b"")\n    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n    print(""\\nSaving images:"")\n    # Iterate through images and save plot of detections\n    for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n\n        print(""(%d) Image: \'%s\'"" % (img_i, path))\n\n        # Create plot\n        img = np.array(Image.open(path))\n        plt.figure()\n        fig, ax = plt.subplots(1)\n        ax.imshow(img)\n\n        # Draw bounding boxes and labels of detections\n        if detections is not None:\n            # Rescale boxes to original image\n            detections = rescale_boxes(detections, opt.img_size, img.shape[:2])\n            unique_labels = detections[:, -1].cpu().unique()\n            n_cls_preds = len(unique_labels)\n            bbox_colors = random.sample(colors, n_cls_preds)\n            for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n\n                print(""\\t+ Label: %s, Conf: %.5f"" % (classes[int(cls_pred)], cls_conf.item()))\n\n                box_w = x2 - x1\n                box_h = y2 - y1\n\n                color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n                # Create a Rectangle patch\n                bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=""none"")\n                # Add the bbox to the plot\n                ax.add_patch(bbox)\n                # Add label\n                plt.text(\n                    x1,\n                    y1,\n                    s=classes[int(cls_pred)],\n                    color=""white"",\n                    verticalalignment=""top"",\n                    bbox={""color"": color, ""pad"": 0},\n                )\n\n        # Save generated image with detections\n        plt.axis(""off"")\n        plt.gca().xaxis.set_major_locator(NullLocator())\n        plt.gca().yaxis.set_major_locator(NullLocator())\n        filename = path.split(""/"")[-1].split(""."")[0]\n        plt.savefig(f""output/{filename}.png"", bbox_inches=""tight"", pad_inches=0.0)\n        plt.close()\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/human_detector.py,5,"b'import torch\nimport torch.nn as nn\nimport os\n\nfrom .models import Darknet\nfrom .utils.utils import non_max_suppression, rescale_boxes\n\n\nclass YoLov3HumanDetector(nn.Module):\n    def __init__(self, weights_path=""weights/yolov3.weights"",\n                 conf_thres=0.8, nms_thres=0.4, img_size=416, device=torch.device(""cpu"")):\n        super().__init__()\n\n        self.conf_thres = conf_thres\n        self.nms_thres = nms_thres\n        self.img_size = img_size\n\n        # Set up model\n        model_def = os.path.abspath(os.path.dirname(__file__))\n        # model_def = os.path.join(model_def, ""config"", ""yolov3.cfg"")\n        model_def = os.path.join(model_def, ""config"", ""yolov3-spp.cfg"")\n        model = Darknet(model_def, img_size=img_size).to(device)\n        if weights_path.endswith("".weights""):\n            # Load darknet weights\n            model.load_darknet_weights(weights_path)\n        else:\n            # Load checkpoint weights\n            model.load_state_dict(torch.load(weights_path))\n\n        model.eval()\n\n        self.device = device\n        self.model = model.to(device)\n\n    def forward(self, input_imgs, input_shapes, factor=1.05):\n        """"""\n        Run YOLOv3 on input_imgs and return the largest bounding boxes of the person in input_imgs.\n\n        Args:\n            input_imgs (torch.tensor): (bs, 3, height, width) is in the range of [0, 1],\n            input_shapes (list[tuple]): [(height, width), (height, width), ...],\n            factor (float): the factor to enlarge the original boxes, e.g [x0, y0, x1, y1] -> [xx0, yy0, xx1, yy1],\n                    here (xx1 - xx0) / (x1 - x0) = factor and (yy1 - yy0) / (y1 - y0) = factor.\n\n        Returns:\n            boxes_list (list[tuple or None]): (x1, y1, x2, y2) or None\n        """"""\n\n        # Get detections\n        with torch.no_grad():\n            # img, _ = pad_to_square(input_imgs, 0)\n            # Resize\n            img_detections = self.model(input_imgs)\n            img_detections = non_max_suppression(img_detections, self.conf_thres, self.nms_thres)\n\n        bs = len(img_detections)\n\n        boxes_list = [None for _ in range(bs)]\n        # Draw bounding boxes and labels of detections\n\n        for i, (detections, img_shape) in enumerate(zip(img_detections, input_shapes)):\n            if detections is not None:\n                # Rescale boxes to original image\n                detections = rescale_boxes(detections, self.img_size, img_shape)\n\n                max_area = 0\n                boxes = None\n                for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n                    # if is `person`\n                    if int(cls_pred) != 0:\n                        continue\n\n                    box_w = x2 - x1\n                    box_h = y2 - y1\n                    area = box_h * box_w\n\n                    if area > max_area:\n                        max_area = area\n                        boxes = (x1, y1, x2, y2)\n\n                if boxes is not None:\n                    boxes_list[i] = self.enlarge_boxes(boxes, img_shape, factor=factor)\n\n        return boxes_list\n\n    @staticmethod\n    def enlarge_boxes(boxes, orig_shape, factor=1.0):\n        """"""\n\n        Args:\n            boxes (list or tuple): (x0, y0, x1, y1),\n            orig_shape (tuple or list): (height, width),\n            factor (float): the factor to enlarge the original boxes, e.g [x0, y0, x1, y1] -> [xx0, yy0, xx1, yy1],\n                    here (xx1 - xx0) / (x1 - x0) = factor and (yy1 - yy0) / (y1 - y0) = factor.\n\n        Returns:\n            new_boxes (list of tuple): (xx0, yy0, xx1, yy1),\n                here (xx1 - xx0) / (x1 - x0) = factor and (yy1 - yy0) / (y1 - y0) = factor.\n        """"""\n\n        height, width = orig_shape\n\n        x0, y0, x1, y1 = boxes\n\n        w = x1 - x0\n        h = y1 - y0\n\n        cx = (x1 + x0) / 2\n        cy = (y1 + y0) / 2\n\n        half_new_w = w * factor / 2\n        half_new_h = h * factor / 2\n\n        xx0 = int(max(0, cx - half_new_w))\n        yy0 = int(max(0, cy - half_new_h))\n\n        xx1 = int(min(width, cx + half_new_w))\n        yy1 = int(min(height, cy + half_new_h))\n\n        new_boxes = (xx0, yy0, xx1, yy1)\n        return new_boxes\n\n\n\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/models.py,26,"b'from __future__ import division\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom .utils.parse_config import *\nfrom .utils.utils import build_targets, to_cpu, non_max_suppression\n\n\ndef create_modules(module_defs):\n    """"""\n    Constructs module list of layer blocks from module configuration in module_defs\n    """"""\n    hyperparams = module_defs.pop(0)\n    output_filters = [int(hyperparams[""channels""])]\n    module_list = nn.ModuleList()\n    for module_i, module_def in enumerate(module_defs):\n        modules = nn.Sequential()\n\n        if module_def[""type""] == ""convolutional"":\n            bn = int(module_def[""batch_normalize""])\n            filters = int(module_def[""filters""])\n            kernel_size = int(module_def[""size""])\n            pad = (kernel_size - 1) // 2\n            modules.add_module(\n                f""conv_{module_i}"",\n                nn.Conv2d(\n                    in_channels=output_filters[-1],\n                    out_channels=filters,\n                    kernel_size=kernel_size,\n                    stride=int(module_def[""stride""]),\n                    padding=pad,\n                    bias=not bn,\n                ),\n            )\n            if bn:\n                modules.add_module(f""batch_norm_{module_i}"", nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))\n            if module_def[""activation""] == ""leaky"":\n                modules.add_module(f""leaky_{module_i}"", nn.LeakyReLU(0.1))\n\n        elif module_def[""type""] == ""maxpool"":\n            kernel_size = int(module_def[""size""])\n            stride = int(module_def[""stride""])\n            if kernel_size == 2 and stride == 1:\n                modules.add_module(f""_debug_padding_{module_i}"", nn.ZeroPad2d((0, 1, 0, 1)))\n            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n            modules.add_module(f""maxpool_{module_i}"", maxpool)\n\n        elif module_def[""type""] == ""upsample"":\n            upsample = Upsample(scale_factor=int(module_def[""stride""]), mode=""nearest"")\n            modules.add_module(f""upsample_{module_i}"", upsample)\n\n        elif module_def[""type""] == ""route"":\n            layers = [int(x) for x in module_def[""layers""].split("","")]\n            filters = sum([output_filters[1:][i] for i in layers])\n            modules.add_module(f""route_{module_i}"", EmptyLayer())\n\n        elif module_def[""type""] == ""shortcut"":\n            filters = output_filters[1:][int(module_def[""from""])]\n            modules.add_module(f""shortcut_{module_i}"", EmptyLayer())\n\n        elif module_def[""type""] == ""yolo"":\n            anchor_idxs = [int(x) for x in module_def[""mask""].split("","")]\n            # Extract anchors\n            anchors = [int(x) for x in module_def[""anchors""].split("","")]\n            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n            anchors = [anchors[i] for i in anchor_idxs]\n            num_classes = int(module_def[""classes""])\n            img_size = int(hyperparams[""height""])\n            # Define detection layer\n            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n            modules.add_module(f""yolo_{module_i}"", yolo_layer)\n        # Register module list and number of output filters\n        module_list.append(modules)\n        output_filters.append(filters)\n\n    return hyperparams, module_list\n\n\nclass Upsample(nn.Module):\n    """""" nn.Upsample is deprecated """"""\n\n    def __init__(self, scale_factor, mode=""nearest""):\n        super(Upsample, self).__init__()\n        self.scale_factor = scale_factor\n        self.mode = mode\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n        return x\n\n\nclass EmptyLayer(nn.Module):\n    """"""Placeholder for \'route\' and \'shortcut\' layers""""""\n\n    def __init__(self):\n        super(EmptyLayer, self).__init__()\n\n\nclass YOLOLayer(nn.Module):\n    """"""Detection layer""""""\n\n    def __init__(self, anchors, num_classes, img_dim=416):\n        super(YOLOLayer, self).__init__()\n        self.anchors = anchors\n        self.num_anchors = len(anchors)\n        self.num_classes = num_classes\n        self.ignore_thres = 0.5\n        self.mse_loss = nn.MSELoss()\n        self.bce_loss = nn.BCELoss()\n        self.obj_scale = 1\n        self.noobj_scale = 100\n        self.metrics = {}\n        self.img_dim = img_dim\n        self.grid_size = 0  # grid size\n\n    def compute_grid_offsets(self, grid_size, cuda=True):\n        self.grid_size = grid_size\n        g = self.grid_size\n        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n        self.stride = self.img_dim / self.grid_size\n        # Calculate offsets for each grid\n        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n\n    def forward(self, x, targets=None, img_dim=None):\n\n        # Tensors for cuda support\n        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n\n        self.img_dim = img_dim\n        num_samples = x.size(0)\n        grid_size = x.size(2)\n\n        prediction = (\n            x.view(num_samples, self.num_anchors, self.num_classes + 5, grid_size, grid_size)\n            .permute(0, 1, 3, 4, 2)\n            .contiguous()\n        )\n\n        # Get outputs\n        x = torch.sigmoid(prediction[..., 0])  # Center x\n        y = torch.sigmoid(prediction[..., 1])  # Center y\n        w = prediction[..., 2]  # Width\n        h = prediction[..., 3]  # Height\n        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n\n        # If grid size does not match current we compute new offsets\n        if grid_size != self.grid_size:\n            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n\n        # Add offset and scale with anchors\n        pred_boxes = FloatTensor(prediction[..., :4].shape)\n        pred_boxes[..., 0] = x.data + self.grid_x\n        pred_boxes[..., 1] = y.data + self.grid_y\n        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n\n        output = torch.cat(\n            (\n                pred_boxes.view(num_samples, -1, 4) * self.stride,\n                pred_conf.view(num_samples, -1, 1),\n                pred_cls.view(num_samples, -1, self.num_classes),\n            ),\n            -1,\n        )\n\n        if targets is None:\n            return output, 0\n        else:\n            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(\n                pred_boxes=pred_boxes,\n                pred_cls=pred_cls,\n                target=targets,\n                anchors=self.scaled_anchors,\n                ignore_thres=self.ignore_thres,\n            )\n\n            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n\n            # Metrics\n            cls_acc = 100 * class_mask[obj_mask].mean()\n            conf_obj = pred_conf[obj_mask].mean()\n            conf_noobj = pred_conf[noobj_mask].mean()\n            conf50 = (pred_conf > 0.5).float()\n            iou50 = (iou_scores > 0.5).float()\n            iou75 = (iou_scores > 0.75).float()\n            detected_mask = conf50 * class_mask * tconf\n            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)\n            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)\n            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)\n\n            self.metrics = {\n                ""loss"": to_cpu(total_loss).item(),\n                ""x"": to_cpu(loss_x).item(),\n                ""y"": to_cpu(loss_y).item(),\n                ""w"": to_cpu(loss_w).item(),\n                ""h"": to_cpu(loss_h).item(),\n                ""conf"": to_cpu(loss_conf).item(),\n                ""cls"": to_cpu(loss_cls).item(),\n                ""cls_acc"": to_cpu(cls_acc).item(),\n                ""recall50"": to_cpu(recall50).item(),\n                ""recall75"": to_cpu(recall75).item(),\n                ""precision"": to_cpu(precision).item(),\n                ""conf_obj"": to_cpu(conf_obj).item(),\n                ""conf_noobj"": to_cpu(conf_noobj).item(),\n                ""grid_size"": grid_size,\n            }\n\n            return output, total_loss\n\n\nclass Darknet(nn.Module):\n    """"""YOLOv3 object detection model""""""\n\n    def __init__(self, config_path, img_size=416):\n        super(Darknet, self).__init__()\n        self.module_defs = parse_model_config(config_path)\n        self.hyperparams, self.module_list = create_modules(self.module_defs)\n        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], ""metrics"")]\n        self.img_size = img_size\n        self.seen = 0\n        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n\n    def forward(self, x, targets=None):\n        img_dim = x.shape[2]\n        loss = 0\n        layer_outputs, yolo_outputs = [], []\n        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n            if module_def[""type""] in [""convolutional"", ""upsample"", ""maxpool""]:\n                x = module(x)\n            elif module_def[""type""] == ""route"":\n                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[""layers""].split("","")], 1)\n            elif module_def[""type""] == ""shortcut"":\n                layer_i = int(module_def[""from""])\n                x = layer_outputs[-1] + layer_outputs[layer_i]\n            elif module_def[""type""] == ""yolo"":\n                x, layer_loss = module[0](x, targets, img_dim)\n                loss += layer_loss\n                yolo_outputs.append(x)\n            layer_outputs.append(x)\n        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))\n        return yolo_outputs if targets is None else (loss, yolo_outputs)\n\n    def load_darknet_weights(self, weights_path):\n        """"""Parses and loads the weights stored in \'weights_path\'""""""\n\n        # Open the weights file\n        with open(weights_path, ""rb"") as f:\n            header = np.fromfile(f, dtype=np.int32, count=5)  # First five are header values\n            self.header_info = header  # Needed to write header when saving weights\n            self.seen = header[3]  # number of images seen during training\n            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n\n        # Establish cutoff for loading backbone weights\n        cutoff = None\n        if ""darknet53.conv.74"" in weights_path:\n            cutoff = 75\n\n        ptr = 0\n        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n            if i == cutoff:\n                break\n            if module_def[""type""] == ""convolutional"":\n                conv_layer = module[0]\n                if module_def[""batch_normalize""]:\n                    # Load BN bias, weights, running mean and running variance\n                    bn_layer = module[1]\n                    num_b = bn_layer.bias.numel()  # Number of biases\n                    # Bias\n                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n                    bn_layer.bias.data.copy_(bn_b)\n                    ptr += num_b\n                    # Weight\n                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n                    bn_layer.weight.data.copy_(bn_w)\n                    ptr += num_b\n                    # Running Mean\n                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n                    bn_layer.running_mean.data.copy_(bn_rm)\n                    ptr += num_b\n                    # Running Var\n                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n                    bn_layer.running_var.data.copy_(bn_rv)\n                    ptr += num_b\n                else:\n                    # Load conv. bias\n                    num_b = conv_layer.bias.numel()\n                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n                    conv_layer.bias.data.copy_(conv_b)\n                    ptr += num_b\n                # Load conv. weights\n                num_w = conv_layer.weight.numel()\n                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n                conv_layer.weight.data.copy_(conv_w)\n                ptr += num_w\n\n    def save_darknet_weights(self, path, cutoff=-1):\n        """"""\n            @:param path    - path of the new weights file\n            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n        """"""\n        fp = open(path, ""wb"")\n        self.header_info[3] = self.seen\n        self.header_info.tofile(fp)\n\n        # Iterate through layers\n        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n            if module_def[""type""] == ""convolutional"":\n                conv_layer = module[0]\n                # If batch norm, load bn first\n                if module_def[""batch_normalize""]:\n                    bn_layer = module[1]\n                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n                # Load conv bias\n                else:\n                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n                # Load conv weights\n                conv_layer.weight.data.cpu().numpy().tofile(fp)\n\n        fp.close()\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/test.py,7,"b'from __future__ import division\n\nfrom .models import *\nfrom .utils.utils import *\nfrom .utils.datasets import *\nfrom .utils.parse_config import *\n\nimport argparse\nimport tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\n\n\ndef evaluate(model, path, iou_thres, conf_thres, nms_thres, img_size, batch_size):\n    model.eval()\n\n    # Get dataloader\n    dataset = ListDataset(path, img_size=img_size, augment=False, multiscale=False)\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=False, num_workers=1, collate_fn=dataset.collate_fn\n    )\n\n    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n\n    labels = []\n    sample_metrics = []  # List of tuples (TP, confs, pred)\n    for batch_i, (_, imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=""Detecting objects"")):\n\n        # Extract labels\n        labels += targets[:, 1].tolist()\n        # Rescale target\n        targets[:, 2:] = xywh2xyxy(targets[:, 2:])\n        targets[:, 2:] *= img_size\n\n        imgs = Variable(imgs.type(Tensor), requires_grad=False)\n\n        with torch.no_grad():\n            outputs = model(imgs)\n            outputs = non_max_suppression(outputs, conf_thres=conf_thres, nms_thres=nms_thres)\n\n        sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)\n\n    # Concatenate sample statistics\n    true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n    precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)\n\n    return precision, recall, AP, f1, ap_class\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--batch_size"", type=int, default=8, help=""size of each image batch"")\n    parser.add_argument(""--model_def"", type=str, default=""config/yolov3.cfg"", help=""path to model definition file"")\n    parser.add_argument(""--data_config"", type=str, default=""config/coco.data"", help=""path to data config file"")\n    parser.add_argument(""--weights_path"", type=str, default=""weights/yolov3.weights"", help=""path to weights file"")\n    parser.add_argument(""--class_path"", type=str, default=""data/coco.names"", help=""path to class label file"")\n    parser.add_argument(""--iou_thres"", type=float, default=0.5, help=""iou threshold required to qualify as detected"")\n    parser.add_argument(""--conf_thres"", type=float, default=0.001, help=""object confidence threshold"")\n    parser.add_argument(""--nms_thres"", type=float, default=0.5, help=""iou thresshold for non-maximum suppression"")\n    parser.add_argument(""--n_cpu"", type=int, default=8, help=""number of cpu threads to use during batch generation"")\n    parser.add_argument(""--img_size"", type=int, default=416, help=""size of each image dimension"")\n    opt = parser.parse_args()\n    print(opt)\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    data_config = parse_data_config(opt.data_config)\n    valid_path = data_config[""valid""]\n    class_names = load_classes(data_config[""names""])\n\n    # Initiate model\n    model = Darknet(opt.model_def).to(device)\n    if opt.weights_path.endswith("".weights""):\n        # Load darknet weights\n        model.load_darknet_weights(opt.weights_path)\n    else:\n        # Load checkpoint weights\n        model.load_state_dict(torch.load(opt.weights_path))\n\n    print(""Compute mAP..."")\n\n    precision, recall, AP, f1, ap_class = evaluate(\n        model,\n        path=valid_path,\n        iou_thres=opt.iou_thres,\n        conf_thres=opt.conf_thres,\n        nms_thres=opt.nms_thres,\n        img_size=opt.img_size,\n        batch_size=8,\n    )\n\n    print(""Average Precisions:"")\n    for i, c in enumerate(ap_class):\n        print(f""+ Class \'{c}\' ({class_names[c]}) - AP: {AP[i]}"")\n\n    print(f""mAP: {AP.mean()}"")\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/train.py,8,"b'from __future__ import division\n\nfrom models import *\nfrom utils.logger import *\nfrom utils.utils import *\nfrom utils.datasets import *\nfrom utils.parse_config import *\nfrom test import evaluate\n\nfrom terminaltables import AsciiTable\n\nimport os\nimport sys\nimport time\nimport datetime\nimport argparse\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nimport torch.optim as optim\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--epochs"", type=int, default=100, help=""number of epochs"")\n    parser.add_argument(""--batch_size"", type=int, default=8, help=""size of each image batch"")\n    parser.add_argument(""--gradient_accumulations"", type=int, default=2, help=""number of gradient accums before step"")\n    parser.add_argument(""--model_def"", type=str, default=""config/yolov3.cfg"", help=""path to model definition file"")\n    parser.add_argument(""--data_config"", type=str, default=""config/coco.data"", help=""path to data config file"")\n    parser.add_argument(""--pretrained_weights"", type=str, help=""if specified starts from checkpoint model"")\n    parser.add_argument(""--n_cpu"", type=int, default=8, help=""number of cpu threads to use during batch generation"")\n    parser.add_argument(""--img_size"", type=int, default=416, help=""size of each image dimension"")\n    parser.add_argument(""--checkpoint_interval"", type=int, default=1, help=""interval between saving model weights"")\n    parser.add_argument(""--evaluation_interval"", type=int, default=1, help=""interval evaluations on validation set"")\n    parser.add_argument(""--compute_map"", default=False, help=""if True computes mAP every tenth batch"")\n    parser.add_argument(""--multiscale_training"", default=True, help=""allow for multi-scale training"")\n    opt = parser.parse_args()\n    print(opt)\n\n    logger = Logger(""logs"")\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    os.makedirs(""output"", exist_ok=True)\n    os.makedirs(""checkpoints"", exist_ok=True)\n\n    # Get data configuration\n    data_config = parse_data_config(opt.data_config)\n    train_path = data_config[""train""]\n    valid_path = data_config[""valid""]\n    class_names = load_classes(data_config[""names""])\n\n    # Initiate model\n    model = Darknet(opt.model_def).to(device)\n    model.apply(weights_init_normal)\n\n    # If specified we start from checkpoint\n    if opt.pretrained_weights:\n        if opt.pretrained_weights.endswith("".pth""):\n            model.load_state_dict(torch.load(opt.pretrained_weights))\n        else:\n            model.load_darknet_weights(opt.pretrained_weights)\n\n    # Get dataloader\n    dataset = ListDataset(train_path, augment=True, multiscale=opt.multiscale_training)\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=opt.batch_size,\n        shuffle=True,\n        num_workers=opt.n_cpu,\n        pin_memory=True,\n        collate_fn=dataset.collate_fn,\n    )\n\n    optimizer = torch.optim.Adam(model.parameters())\n\n    metrics = [\n        ""grid_size"",\n        ""loss"",\n        ""x"",\n        ""y"",\n        ""w"",\n        ""h"",\n        ""conf"",\n        ""cls"",\n        ""cls_acc"",\n        ""recall50"",\n        ""recall75"",\n        ""precision"",\n        ""conf_obj"",\n        ""conf_noobj"",\n    ]\n\n    for epoch in range(opt.epochs):\n        model.train()\n        start_time = time.time()\n        for batch_i, (_, imgs, targets) in enumerate(dataloader):\n            batches_done = len(dataloader) * epoch + batch_i\n\n            imgs = Variable(imgs.to(device))\n            targets = Variable(targets.to(device), requires_grad=False)\n\n            loss, outputs = model(imgs, targets)\n            loss.backward()\n\n            if batches_done % opt.gradient_accumulations:\n                # Accumulates gradient before each step\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # ----------------\n            #   Log progress\n            # ----------------\n\n            log_str = ""\\n---- [Epoch %d/%d, Batch %d/%d] ----\\n"" % (epoch, opt.epochs, batch_i, len(dataloader))\n\n            metric_table = [[""Metrics"", *[f""YOLO Layer {i}"" for i in range(len(model.yolo_layers))]]]\n\n            # Log metrics at each YOLO layer\n            for i, metric in enumerate(metrics):\n                formats = {m: ""%.6f"" for m in metrics}\n                formats[""grid_size""] = ""%2d""\n                formats[""cls_acc""] = ""%.2f%%""\n                row_metrics = [formats[metric] % yolo.metrics.get(metric, 0) for yolo in model.yolo_layers]\n                metric_table += [[metric, *row_metrics]]\n\n                # Tensorboard logging\n                tensorboard_log = []\n                for j, yolo in enumerate(model.yolo_layers):\n                    for name, metric in yolo.metrics.items():\n                        if name != ""grid_size"":\n                            tensorboard_log += [(f""{name}_{j+1}"", metric)]\n                tensorboard_log += [(""loss"", loss.item())]\n                logger.list_of_scalars_summary(tensorboard_log, batches_done)\n\n            log_str += AsciiTable(metric_table).table\n            log_str += f""\\nTotal loss {loss.item()}""\n\n            # Determine approximate time left for epoch\n            epoch_batches_left = len(dataloader) - (batch_i + 1)\n            time_left = datetime.timedelta(seconds=epoch_batches_left * (time.time() - start_time) / (batch_i + 1))\n            log_str += f""\\n---- ETA {time_left}""\n\n            print(log_str)\n\n            model.seen += imgs.size(0)\n\n        if epoch % opt.evaluation_interval == 0:\n            print(""\\n---- Evaluating Model ----"")\n            # Evaluate the model on the validation set\n            precision, recall, AP, f1, ap_class = evaluate(\n                model,\n                path=valid_path,\n                iou_thres=0.5,\n                conf_thres=0.5,\n                nms_thres=0.5,\n                img_size=opt.img_size,\n                batch_size=8,\n            )\n            evaluation_metrics = [\n                (""val_precision"", precision.mean()),\n                (""val_recall"", recall.mean()),\n                (""val_mAP"", AP.mean()),\n                (""val_f1"", f1.mean()),\n            ]\n            logger.list_of_scalars_summary(evaluation_metrics, epoch)\n\n            # Print class APs and mAP\n            ap_table = [[""Index"", ""Class name"", ""AP""]]\n            for i, c in enumerate(ap_class):\n                ap_table += [[c, class_names[c], ""%.5f"" % AP[i]]]\n            print(AsciiTable(ap_table).table)\n            print(f""---- mAP {AP.mean()}"")\n\n        if epoch % opt.checkpoint_interval == 0:\n            torch.save(model.state_dict(), f""checkpoints/yolov3_ckpt_%d.pth"" % epoch)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/models/__init__.py,0,b''
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/models/inception_resnet_v1.py,12,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport requests\nfrom requests.adapters import HTTPAdapter\nimport os\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_planes, out_planes,\n            kernel_size=kernel_size, stride=stride,\n            padding=padding, bias=False\n        ) # verify bias false\n        self.bn = nn.BatchNorm2d(\n            out_planes,\n            eps=0.001, # value found in tensorflow\n            momentum=0.1, # default pytorch value\n            affine=True\n        )\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Block35(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super().__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(256, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(256, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(256, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(96, 256, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Block17(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super().__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(896, 128, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(896, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 128, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(128, 128, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.conv2d = nn.Conv2d(256, 896, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Block8(nn.Module):\n\n    def __init__(self, scale=1.0, noReLU=False):\n        super().__init__()\n\n        self.scale = scale\n        self.noReLU = noReLU\n\n        self.branch0 = BasicConv2d(1792, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1792, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=(1,3), stride=1, padding=(0,1)),\n            BasicConv2d(192, 192, kernel_size=(3,1), stride=1, padding=(1,0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1792, kernel_size=1, stride=1)\n        if not self.noReLU:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.branch0 = BasicConv2d(256, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(256, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(192, 256, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(896, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(896, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(896, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionResnetV1(nn.Module):\n    """"""Inception Resnet V1 model with optional loading of pretrained weights.\n\n    Model parameters can be loaded based on pretraining on the VGGFace2 or CASIA-Webface\n    datasets. Pretrained state_dicts are automatically downloaded on model instantiation if\n    requested and cached in the torch cache. Subsequent instantiations use the cache rather than\n    redownloading.\n\n    Keyword Arguments:\n        pretrained {str} -- Optional pretraining dataset. Either \'vggface2\' or \'casia-webface\'.\n            (default: {None})\n        classify {bool} -- Whether the model should output classification probabilities or feature\n            embeddings. (default: {False})\n        num_classes {int} -- Number of output classes. If \'pretrained\' is set and num_classes not\n            equal to that used for the pretrained model, the final linear layer will be randomly\n            initialized. (default: {None})\n        dropout_prob {float} -- Dropout probability. (default: {0.6})\n    """"""\n    def __init__(self, pretrained=None, classify=False, num_classes=None, dropout_prob=0.6, device=None):\n        super().__init__()\n\n        # Set simple attributes\n        self.pretrained = pretrained\n        self.classify = classify\n        self.num_classes = num_classes\n\n        if pretrained == \'vggface2\':\n            tmp_classes = 8631\n        elif pretrained == \'casia-webface\':\n            tmp_classes = 10575\n        elif pretrained is None and self.num_classes is None:\n            raise Exception(\'At least one of ""pretrained"" or ""num_classes"" must be specified\')\n        else:\n            tmp_classes = self.num_classes\n\n        # Define layers\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n        self.conv2d_4b = BasicConv2d(192, 256, kernel_size=3, stride=2)\n        self.repeat_1 = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n        )\n        self.mixed_6a = Mixed_6a()\n        self.repeat_2 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n        )\n        self.mixed_7a = Mixed_7a()\n        self.repeat_3 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n        )\n        self.block8 = Block8(noReLU=True)\n        self.avgpool_1a = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.last_linear = nn.Linear(1792, 512, bias=False)\n        self.last_bn = nn.BatchNorm1d(512, eps=0.001, momentum=0.1, affine=True)\n        self.logits = nn.Linear(512, tmp_classes)\n\n        if pretrained is not None:\n            # load_weights(self, pretrained)\n            load_pretrain(self, pretrained)\n\n        if self.num_classes is not None:\n            self.logits = nn.Linear(512, self.num_classes)\n\n        self.device = torch.device(\'cpu\')\n        if device is not None:\n            self.device = device\n            self.to(device)\n\n    def forward(self, x, normalize=True):\n        """"""Calculate embeddings or logits given a batch of input image tensors.\n\n        Arguments:\n            x {torch.tensor} -- Batch of image tensors representing faces.\n            normalize {bool} -- Normalize the final features or not.\n\n        Returns:\n            torch.tensor -- Batch of embedding vectors or multinomial logits.\n        """"""\n        x = self.conv2d_1a(x)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.conv2d_4b(x)\n        x = self.repeat_1(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_2(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_3(x)\n        x = self.block8(x)\n        x = self.avgpool_1a(x)\n        x = self.dropout(x)\n        x = self.last_linear(x.view(x.shape[0], -1))\n        x = self.last_bn(x)\n        if self.classify:\n            x = self.logits(x)\n        else:\n            if normalize:\n                x = F.normalize(x, p=2, dim=1)\n        return x\n\n\ndef load_pretrain(mdl, name):\n    dirpath = os.path.abspath(os.path.dirname(__file__))\n    dirpath = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(dirpath))))\n    dirpath = os.path.join(dirpath, ""data"")\n\n    state_dict = {}\n    if name == \'vggface2\':\n        for name in [""20180402-114759-vggface2-features.pt"", ""20180402-114759-vggface2-logits.pt""]:\n            ckpt_path = os.path.join(dirpath, name)\n            state_dict.update(torch.load(ckpt_path))\n    else:\n        raise ValueError(\'Pretrained models only exist for ""vggface2"" and ""casia-webface""\')\n\n    mdl.load_state_dict(state_dict)\n\n\n\ndef load_weights(mdl, name):\n    """"""Download pretrained state_dict and load into model.\n\n    Arguments:\n        mdl {torch.nn.Module} -- Pytorch model.\n        name {str} -- Name of dataset that was used to generate pretrained state_dict.\n\n    Raises:\n        ValueError: If \'pretrained\' not equal to \'vggface2\' or \'casia-webface\'.\n    """"""\n    if name == \'vggface2\':\n        features_path = \'https://drive.google.com/uc?export=download&id=1cWLH_hPns8kSfMz9kKl9PsG5aNV2VSMn\'\n        logits_path = \'https://drive.google.com/uc?export=download&id=1mAie3nzZeno9UIzFXvmVZrDG3kwML46X\'\n    elif name == \'casia-webface\':\n        features_path = \'https://drive.google.com/uc?export=download&id=1LSHHee_IQj5W3vjBcRyVaALv4py1XaGy\'\n        logits_path = \'https://drive.google.com/uc?export=download&id=1QrhPgn1bGlDxAil2uc07ctunCQoDnCzT\'\n    else:\n        raise ValueError(\'Pretrained models only exist for ""vggface2"" and ""casia-webface""\')\n\n    model_dir = os.path.join(get_torch_home(), \'checkpoints\')\n    os.makedirs(model_dir, exist_ok=True)\n\n    state_dict = {}\n    for i, path in enumerate([features_path, logits_path]):\n        cached_file = os.path.join(model_dir, \'{}_{}.pt\'.format(name, path[-10:]))\n        if not os.path.exists(cached_file):\n            print(\'Downloading parameters ({}/2)\'.format(i+1))\n            s = requests.Session()\n            s.mount(\'https://\', HTTPAdapter(max_retries=10))\n            r = s.get(path, allow_redirects=True)\n            with open(cached_file, \'wb\') as f:\n                f.write(r.content)\n        state_dict.update(torch.load(cached_file))\n\n    mdl.load_state_dict(state_dict)\n\n\ndef get_torch_home():\n    torch_home = os.path.expanduser(\n        os.getenv(\n            \'TORCH_HOME\',\n            os.path.join(os.getenv(\'XDG_CACHE_HOME\', \'~/.cache\'), \'torch\')\n        )\n    )\n    return torch_home\n'"
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/models/mtcnn.py,9,"b'import torch\nfrom torch import nn\nimport numpy as np\nimport os\n\nfrom .utils.detect_face import detect_face, extract_face\n\n\nclass PNet(nn.Module):\n    """"""MTCNN PNet.\n    \n    Keyword Arguments:\n        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\n    """"""\n\n    def __init__(self, pretrained=True):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=3)\n        self.prelu1 = nn.PReLU(10)\n        self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True)\n        self.conv2 = nn.Conv2d(10, 16, kernel_size=3)\n        self.prelu2 = nn.PReLU(16)\n        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)\n        self.prelu3 = nn.PReLU(32)\n        self.conv4_1 = nn.Conv2d(32, 2, kernel_size=1)\n        self.softmax4_1 = nn.Softmax(dim=1)\n        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1)\n\n        self.training = False\n\n        if pretrained:\n            state_dict_path = os.path.join(os.path.dirname(__file__), \'../data/pnet.pt\')\n            state_dict = torch.load(state_dict_path)\n            self.load_state_dict(state_dict)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.prelu1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.prelu2(x)\n        x = self.conv3(x)\n        x = self.prelu3(x)\n        a = self.conv4_1(x)\n        a = self.softmax4_1(a)\n        b = self.conv4_2(x)\n        return b, a\n\n\nclass RNet(nn.Module):\n    """"""MTCNN RNet.\n    \n    Keyword Arguments:\n        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\n    """"""\n\n    def __init__(self, pretrained=True):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(3, 28, kernel_size=3)\n        self.prelu1 = nn.PReLU(28)\n        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)\n        self.conv2 = nn.Conv2d(28, 48, kernel_size=3)\n        self.prelu2 = nn.PReLU(48)\n        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)\n        self.conv3 = nn.Conv2d(48, 64, kernel_size=2)\n        self.prelu3 = nn.PReLU(64)\n        self.dense4 = nn.Linear(576, 128)\n        self.prelu4 = nn.PReLU(128)\n        self.dense5_1 = nn.Linear(128, 2)\n        self.softmax5_1 = nn.Softmax(dim=1)\n        self.dense5_2 = nn.Linear(128, 4)\n\n        self.training = False\n\n        if pretrained:\n            state_dict_path = os.path.join(os.path.dirname(__file__), \'../data/rnet.pt\')\n            state_dict = torch.load(state_dict_path)\n            self.load_state_dict(state_dict)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.prelu1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.prelu2(x)\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = self.prelu3(x)\n        x = x.permute(0, 3, 2, 1).contiguous()\n        x = self.dense4(x.view(x.shape[0], -1))\n        x = self.prelu4(x)\n        a = self.dense5_1(x)\n        a = self.softmax5_1(a)\n        b = self.dense5_2(x)\n        return b, a\n\n\nclass ONet(nn.Module):\n    """"""MTCNN ONet.\n    \n    Keyword Arguments:\n        pretrained {bool} -- Whether or not to load saved pretrained weights (default: {True})\n    """"""\n\n    def __init__(self, pretrained=True):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n        self.prelu1 = nn.PReLU(32)\n        self.pool1 = nn.MaxPool2d(3, 2, ceil_mode=True)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.prelu2 = nn.PReLU(64)\n        self.pool2 = nn.MaxPool2d(3, 2, ceil_mode=True)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)\n        self.prelu3 = nn.PReLU(64)\n        self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)\n        self.prelu4 = nn.PReLU(128)\n        self.dense5 = nn.Linear(1152, 256)\n        self.prelu5 = nn.PReLU(256)\n        self.dense6_1 = nn.Linear(256, 2)\n        self.softmax6_1 = nn.Softmax(dim=1)\n        self.dense6_2 = nn.Linear(256, 4)\n        self.dense6_3 = nn.Linear(256, 10)\n\n        self.training = False\n\n        if pretrained:\n            state_dict_path = os.path.join(os.path.dirname(__file__), \'../data/onet.pt\')\n            state_dict = torch.load(state_dict_path)\n            self.load_state_dict(state_dict)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.prelu1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.prelu2(x)\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = self.prelu3(x)\n        x = self.pool3(x)\n        x = self.conv4(x)\n        x = self.prelu4(x)\n        x = x.permute(0, 3, 2, 1).contiguous()\n        x = self.dense5(x.view(x.shape[0], -1))\n        x = self.prelu5(x)\n        a = self.dense6_1(x)\n        a = self.softmax6_1(a)\n        b = self.dense6_2(x)\n        c = self.dense6_3(x)\n        return b, c, a\n\n\nclass MTCNN(nn.Module):\n    """"""MTCNN face detection module.\n\n    This class loads pretrained P-, R-, and O-nets and returns images cropped to include the face\n    only, given raw input images of one of the following types:\n        - PIL image or list of PIL images\n        - numpy.ndarray (uint8) representing either a single image (3D) or a batch of images (4D).\n    Cropped faces can optionally be saved to file\n    also.\n    \n    Keyword Arguments:\n        image_size {int} -- Output image size in pixels. The image will be square. (default: {160})\n        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \n            Note that the application of the margin differs slightly from the davidsandberg/facenet\n            repo, which applies the margin to the original image before resizing, making the margin\n            dependent on the original image size (this is a bug in davidsandberg/facenet).\n            (default: {0})\n        min_face_size {int} -- Minimum face size to search for. (default: {20})\n        thresholds {list} -- MTCNN face detection thresholds (default: {[0.6, 0.7, 0.7]})\n        factor {float} -- Factor used to create a scaling pyramid of face sizes. (default: {0.709})\n        post_process {bool} -- Whether or not to post process images tensors before returning.\n            (default: {True})\n        select_largest {bool} -- If True, if multiple faces are detected, the largest is returned.\n            If False, the face with the highest detection probability is returned.\n            (default: {True})\n        keep_all {bool} -- If True, all detected faces are returned, in the order dictated by the\n            select_largest parameter. If a save_path is specified, the first face is saved to that\n            path and the remaining faces are saved to <save_path>1, <save_path>2 etc.\n        device {torch.device} -- The device on which to run neural net passes. Image tensors and\n            models are copied to this device before running forward passes. (default: {None})\n    """"""\n\n    def __init__(\n        self, image_size=160, margin=0, min_face_size=20,\n        thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n        select_largest=True, keep_all=False, device=None\n    ):\n        super().__init__()\n\n        self.image_size = image_size\n        self.margin = margin\n        self.min_face_size = min_face_size\n        self.thresholds = thresholds\n        self.factor = factor\n        self.post_process = post_process\n        self.select_largest = select_largest\n        self.keep_all = keep_all\n\n        self.pnet = PNet()\n        self.rnet = RNet()\n        self.onet = ONet()\n\n        self.device = torch.device(\'cpu\')\n        if device is not None:\n            self.device = device\n            self.to(device)\n\n    def forward(self, img, save_path=None, return_prob=False):\n        """"""Run MTCNN face detection on a PIL image or numpy array. This method performs both\n        detection and extraction of faces, returning tensors representing detected faces rather\n        than the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\n        \n        Arguments:\n            img {PIL.Image, np.ndarray, or list} -- A PIL image, np.ndarray, or list.\n        \n        Keyword Arguments:\n            save_path {str} -- An optional save path for the cropped image. Note that when\n                self.post_process=True, although the returned tensor is post processed, the saved\n                face image is not, so it is a true representation of the face in the input image.\n                If `img` is a list of images, `save_path` should be a list of equal length.\n                (default: {None})\n            return_prob {bool} -- Whether or not to return the detection probability.\n                (default: {False})\n        \n        Returns:\n            Union[torch.Tensor, tuple(torch.tensor, float)] -- If detected, cropped image of a face\n                with dimensions 3 x image_size x image_size. Optionally, the probability that a\n                face was detected. If self.keep_all is True, n detected faces are returned in an\n                n x 3 x image_size x image_size tensor with an optional list of detection\n                probabilities. If `img` is a list of images, the item(s) returned have an extra \n                dimension (batch) as the first dimension.\n\n        Example:\n        >>> from facenet_pytorch import MTCNN\n        >>> mtcnn = MTCNN()\n        >>> face_tensor, prob = mtcnn(img, save_path=\'face.png\', return_prob=True)\n        """"""\n\n        # Detect faces\n        with torch.no_grad():\n            batch_boxes, batch_probs = self.detect(img)\n\n        # Determine if a batch or single image was passed\n        batch_mode = True\n        if not isinstance(img, (list, tuple)) and not (isinstance(img, np.ndarray) and len(img.shape) == 4):\n            img = [img]\n            batch_boxes = [batch_boxes]\n            batch_probs = [batch_probs]\n            batch_mode = False\n\n        # Parse save path(s)\n        if save_path is not None:\n            if isinstance(save_path, str):\n                save_path = [save_path]\n        else:\n            save_path = [None for _ in range(len(img))]\n        \n        # Process all bounding boxes and probabilities\n        faces, probs = [], []\n        for im, box_im, prob_im, path_im in zip(img, batch_boxes, batch_probs, save_path):\n            if box_im is None:\n                faces.append(None)\n                probs.append([None] if self.keep_all else None)\n                continue\n\n            if not self.keep_all:\n                box_im = box_im[[0]]\n\n            faces_im = []\n            for i, box in enumerate(box_im):\n                face_path = path_im\n                if path_im is not None and i > 0:\n                    save_name, ext = os.path.splitext(path_im)\n                    face_path = save_name + \'_\' + str(i + 1) + ext\n\n                face = extract_face(im, box, self.image_size, self.margin, face_path)\n                if self.post_process:\n                    face = fixed_image_standardization(face)\n                faces_im.append(face)\n\n            if self.keep_all:\n                faces_im = torch.stack(faces_im)\n            else:\n                faces_im = faces_im[0]\n                prob_im = prob_im[0]\n            \n            faces.append(faces_im)\n            probs.append(prob_im)\n    \n        if not batch_mode:\n            faces = faces[0]\n            probs = probs[0]\n\n        if return_prob:\n            return faces, probs\n        else:\n            return faces\n\n    def detect(self, img, landmarks=False):\n        """"""Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\n\n        This method is used by the forward method and is also useful for face detection tasks\n        that require lower-level handling of bounding boxes and facial landmarks (e.g., face\n        tracking). The functionality of the forward function can be emulated by using this method\n        followed by the extract_face() function.\n        \n        Arguments:\n            img {PIL.Image, np.ndarray, or list} -- A PIL image or a list of PIL images.\n\n        Keyword Arguments:\n            landmarks {bool} -- Whether to return facial landmarks in addition to bounding boxes.\n                (default: {False})\n        \n        Returns:\n            tuple(numpy.ndarray, list) -- For N detected faces, a tuple containing an\n                Nx4 array of bounding boxes and a length N list of detection probabilities.\n                Returned boxes will be sorted in descending order by detection probability if\n                self.select_largest=False, otherwise the largest face will be returned first.\n                If `img` is a list of images, the items returned have an extra dimension\n                (batch) as the first dimension. Optionally, a third item, the facial landmarks,\n                are returned if `landmarks=True`.\n\n        Example:\n        >>> from PIL import Image, ImageDraw\n        >>> from facenet_pytorch import MTCNN, extract_face\n        >>> mtcnn = MTCNN(keep_all=True)\n        >>> boxes, probs, points = mtcnn.detect(img, landmarks=True)\n        >>> # Draw boxes and save faces\n        >>> img_draw = img.copy()\n        >>> draw = ImageDraw.Draw(img_draw)\n        >>> for i, (box, point) in enumerate(zip(boxes, points)):\n        ...     draw.rectangle(box.tolist(), width=5)\n        ...     for p in point:\n        ...         draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=10)\n        ...     extract_face(img, box, save_path=\'detected_face_{}.png\'.format(i))\n        >>> img_draw.save(\'annotated_faces.png\')\n        """"""\n\n        with torch.no_grad():\n            batch_boxes, batch_points = detect_face(\n                img, self.min_face_size,\n                self.pnet, self.rnet, self.onet,\n                self.thresholds, self.factor,\n                self.device\n            )\n\n        boxes, probs, points = [], [], []\n        for box, point in zip(batch_boxes, batch_points):\n            box = np.array(box)\n            point = np.array(point)\n            if len(box) == 0:\n                boxes.append(None)\n                probs.append([None])\n                points.append(None)\n            elif self.select_largest:\n                box_order = np.argsort((box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1]))[::-1]\n                box = box[box_order]\n                point = point[box_order]\n                boxes.append(box[:, :4])\n                probs.append(box[:, 4])\n                points.append(point)\n            else:\n                boxes.append(box[:, :4])\n                probs.append(box[:, 4])\n                points.append(point)\n        boxes = np.array(boxes)\n        probs = np.array(probs)\n        points = np.array(points)\n\n        if not isinstance(img, (list, tuple)) and not (isinstance(img, np.ndarray) and len(img.shape) == 4):\n            boxes = boxes[0]\n            probs = probs[0]\n            points = points[0]\n\n        if landmarks:\n            return boxes, probs, points\n\n        return boxes, probs\n\n\ndef fixed_image_standardization(image_tensor):\n    processed_tensor = (image_tensor - 127.5) / 128.0\n    return processed_tensor\n\ndef prewhiten(x):\n    mean = x.mean()\n    std = x.std()\n    std_adj = std.clamp(min=1.0/(float(x.numel())**0.5))\n    y = (x - mean) / std_adj\n    return y\n'"
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/tests/perf_test.py,2,"b'from facenet_pytorch import MTCNN, training\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom tqdm import tqdm\nimport time\n\n\ndef main():\n    device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    print(f\'Running on device ""{device}""\')\n\n    mtcnn = MTCNN(device=device)\n\n    batch_size = 32\n\n    # Generate data loader\n    ds = datasets.ImageFolder(\n        root=\'data/test_images/\',\n        transform=transforms.Resize((512, 512))\n    )\n    dl = DataLoader(\n        dataset=ds,\n        num_workers=4,\n        collate_fn=training.collate_pil,\n        batch_size=batch_size,\n        sampler=RandomSampler(ds, replacement=True, num_samples=960),\n    )\n\n    start = time.time()\n    faces = []\n    for x, _ in tqdm(dl):\n        faces.extend(mtcnn(x))\n    elapsed = time.time() - start\n    print(f\'Elapsed: {elapsed} | EPS: {len(dl) * batch_size / elapsed}\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/tests/travis_test.py,6,"b'""""""\nThe following code is intended to be run only by travis for continuius intengration and testing\npurposes. For implementation examples see notebooks in the examples folder.\n""""""\n\nfrom PIL import Image, ImageDraw\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport sys, os\nimport glob\n\nfrom models.mtcnn import MTCNN, fixed_image_standardization\nfrom models.inception_resnet_v1 import InceptionResnetV1, get_torch_home\n\n\n#### CLEAR ALL OUTPUT FILES ####\n\n# checkpoints = glob.glob(os.path.join(get_torch_home(), \'checkpoints/*\'))\n# for c in checkpoints:\n#     print(\'Removing {}\'.format(c))\n#     os.remove(c)\n\ncrop_files = glob.glob(\'data/test_images_aligned/**/*.png\')\nfor c in crop_files:\n    print(\'Removing {}\'.format(c))\n    os.remove(c)\n\n\n#### TEST EXAMPLE IPYNB\'S ####\n\nos.system(\'jupyter nbconvert --to script --stdout examples/infer.ipynb examples/finetune.ipynb > examples/tmptest.py\')\nos.chdir(\'examples\')\nimport examples.tmptest\nos.chdir(\'..\')\n\n\n#### TEST MTCNN ####\n\ndef get_image(path, trans):\n    img = Image.open(path)\n    img = trans(img)\n    return img\n\ntrans = transforms.Compose([\n    transforms.Resize(512)\n])\n\ntrans_cropped = transforms.Compose([\n    np.float32,\n    transforms.ToTensor(),\n    fixed_image_standardization\n])\n\ndataset = datasets.ImageFolder(\'data/test_images\', transform=trans)\ndataset.idx_to_class = {k: v for v, k in dataset.class_to_idx.items()}\nloader = DataLoader(dataset, collate_fn=lambda x: x[0])\n\nmtcnn_pt = MTCNN(device=torch.device(\'cpu\'))\n\nnames = []\naligned = []\naligned_fromfile = []\nfor img, idx in loader:\n    name = dataset.idx_to_class[idx]\n    start = time()\n    img_align = mtcnn_pt(img, save_path=\'data/test_images_aligned/{}/1.png\'.format(name))\n    print(\'MTCNN time: {:6f} seconds\'.format(time() - start))\n\n    if img_align is not None:\n        names.append(name)\n        aligned.append(img_align)\n        aligned_fromfile.append(get_image(\'data/test_images_aligned/{}/1.png\'.format(name), trans_cropped))\n\naligned = torch.stack(aligned)\naligned_fromfile = torch.stack(aligned_fromfile)\n\n\n#### TEST EMBEDDINGS ####\n\nexpected = [\n    [\n        [0.000000, 1.482895, 0.886342, 1.438450, 1.437583],\n        [1.482895, 0.000000, 1.345686, 1.029880, 1.061939],\n        [0.886342, 1.345686, 0.000000, 1.363125, 1.338803],\n        [1.438450, 1.029880, 1.363125, 0.000000, 1.066040],\n        [1.437583, 1.061939, 1.338803, 1.066040, 0.000000]\n    ],\n    [\n        [0.000000, 1.430769, 0.992931, 1.414197, 1.329544],\n        [1.430769, 0.000000, 1.253911, 1.144899, 1.079755],\n        [0.992931, 1.253911, 0.000000, 1.358875, 1.337322],\n        [1.414197, 1.144899, 1.358875, 0.000000, 1.204118],\n        [1.329544, 1.079755, 1.337322, 1.204118, 0.000000]\n    ]\n]\n\nfor i, ds in enumerate([\'vggface2\', \'casia-webface\']):\n    resnet_pt = InceptionResnetV1(pretrained=ds).eval()\n\n    start = time()\n    embs = resnet_pt(aligned)\n    print(\'\\nResnet time: {:6f} seconds\\n\'.format(time() - start))\n\n    embs_fromfile = resnet_pt(aligned_fromfile)\n\n    dists = [[(emb - e).norm().item() for e in embs] for emb in embs]\n    dists_fromfile = [[(emb - e).norm().item() for e in embs_fromfile] for emb in embs_fromfile]\n\n    print(\'\\nOutput:\')\n    print(pd.DataFrame(dists, columns=names, index=names))\n    print(\'\\nOutput (from file):\')\n    print(pd.DataFrame(dists_fromfile, columns=names, index=names))\n    print(\'\\nExpected:\')\n    print(pd.DataFrame(expected[i], columns=names, index=names))\n\n    total_error = (torch.tensor(dists) - torch.tensor(expected[i])).norm()\n    total_error_fromfile = (torch.tensor(dists_fromfile) - torch.tensor(expected[i])).norm()\n\n    print(\'\\nTotal error: {}, {}\'.format(total_error, total_error_fromfile))\n\n    if sys.platform != \'win32\':\n        assert total_error < 1e-4\n        assert total_error_fromfile < 1e-4\n\n\n#### TEST CLASSIFICATION ####\n\nresnet_pt = InceptionResnetV1(pretrained=ds, classify=True).eval()\nprob = resnet_pt(aligned)\n\n\n#### MULTI-FACE TEST ####\n\nmtcnn = MTCNN(keep_all=True)\nimg = Image.open(\'data/multiface.jpg\')\nboxes, probs = mtcnn.detect(img)\n\ndraw = ImageDraw.Draw(img)\nfor i, box in enumerate(boxes):\n    draw.rectangle(box.tolist())\n\nmtcnn(img, save_path=\'data/tmp.png\')\n\n\n#### MULTI-IMAGE TEST ####\n\nmtcnn = MTCNN(keep_all=True)\nimg = [\n    Image.open(\'data/multiface.jpg\'),\n    Image.open(\'data/multiface.jpg\')\n]\nbatch_boxes, batch_probs = mtcnn.detect(img)\n\nmtcnn(img, save_path=[\'data/tmp1.png\', \'data/tmp1.png\'])\ntmp_files = glob.glob(\'data/tmp*\')\nfor f in tmp_files:\n    os.remove(f)\n\n\n#### NO-FACE TEST ####\n\nimg = Image.new(\'RGB\', (512, 512))\nmtcnn(img)\nmtcnn(img, return_prob=True)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/lpips/models/__init__.py,0,b''
thirdparty/his_evaluators/his_evaluators/metrics/lpips/models/base_model.py,3,"b""import os\nimport torch\nimport numpy as np\n\n\nclass BaseModel(object):\n    def __init__(self):\n        pass\n        \n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, use_gpu=True):\n        self.use_gpu = use_gpu\n        self.Tensor = torch.cuda.FloatTensor if self.use_gpu else torch.Tensor\n        # self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n    def forward(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, path, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(path, save_filename)\n        torch.save(network.state_dict(), save_path)\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label):\n        # embed()\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        print('Loading network from %s'%save_path)\n        network.load_state_dict(torch.load(save_path))\n\n    def update_learning_rate(self):\n        pass\n\n    def get_image_paths(self):\n        return self.image_paths\n\n    def save_done(self, flag=False):\n        np.save(os.path.join(self.save_dir, 'done_flag'),flag)\n        np.savetxt(os.path.join(self.save_dir, 'done_flag'),[flag,],fmt='%i')\n\n"""
thirdparty/his_evaluators/his_evaluators/metrics/lpips/models/dist_model.py,7,"b'from __future__ import absolute_import\n\n# import sys\n# sys.path.append(\'..\')\n# sys.path.append(\'.\')\n\nimport numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nfrom .base_model import BaseModel\nfrom scipy.ndimage import zoom\nimport skimage.transform\n\nfrom . import networks_basic as networks\nfrom .. import util\n\n\nclass DistModel(BaseModel):\n    def name(self):\n        return self.model_name\n\n    def initialize(self, model=\'net-lin\', net=\'alex\', pnet_rand=False, pnet_tune=False,\n                   model_path=None, colorspace=\'Lab\', use_gpu=True, printNet=False, spatial=False, spatial_shape=None, spatial_order=1,\n                   spatial_factor=None, is_train=False, lr=.0001, beta1=0.5, version=\'0.1\'):\n        \'\'\'\n        INPUTS\n            model - [\'net-lin\'] for linearly calibrated network\n                    [\'net\'] for off-the-shelf network\n                    [\'L2\'] for L2 distance in Lab colorspace\n                    [\'SSIM\'] for ssim in RGB colorspace\n            net - [\'squeeze\',\'alex\',\'vgg\']\n            model_path - if None, will look in weights/[NET_NAME].pth\n            colorspace - [\'Lab\',\'RGB\'] colorspace to use for L2 and SSIM\n            use_gpu - bool - whether or not to use a GPU\n            printNet - bool - whether or not to print network architecture out\n            spatial - bool - whether to output an array containing varying distances across spatial dimensions\n            spatial_shape - if given, output spatial shape. if None then spatial shape is determined automatically via spatial_factor (see below).\n            spatial_factor - if given, specifies upsampling factor relative to the largest spatial extent of a convolutional layer. if None then resized to size of input images.\n            spatial_order - spline order of filter for upsampling in spatial mode, by default 1 (bilinear).\n            is_train - bool - [True] for training mode\n            lr - float - initial learning rate\n            beta1 - float - initial momentum term for adam\n            version - 0.1 for latest, 0.0 was original\n        \'\'\'\n        BaseModel.initialize(self, use_gpu=use_gpu)\n\n        self.model = model\n        self.net = net\n        self.use_gpu = use_gpu\n        self.is_train = is_train\n        self.spatial = spatial\n        self.spatial_shape = spatial_shape\n        self.spatial_order = spatial_order\n        self.spatial_factor = spatial_factor\n\n        self.model_name = \'%s [%s]\' % (model, net)\n        if (self.model == \'net-lin\'):  # pretrained net + linear layer\n            self.net = networks.PNetLin(use_gpu=use_gpu, pnet_rand=pnet_rand, pnet_tune=pnet_tune, pnet_type=net,\n                                        use_dropout=True, spatial=spatial, version=version)\n            kw = {}\n            if not use_gpu:\n                kw[\'map_location\'] = \'cpu\'\n            if (model_path is None):\n                import inspect\n                # model_path = \'./PerceptualSimilarity/weights/v%s/%s.pth\'%(version,net)\n                model_path = os.path.abspath(\n                    os.path.join(inspect.getfile(self.initialize), \'..\', \'..\', \'weights/v%s/%s.pth\' % (version, net)))\n\n            if (not is_train):\n                print(\'Loading model from: %s\' % model_path)\n                self.net.load_state_dict(torch.load(model_path, **kw))\n\n        elif (self.model == \'net\'):  # pretrained network\n            assert not self.spatial, \'spatial argument not supported yet for uncalibrated networks\'\n            self.net = networks.PNet(use_gpu=use_gpu, pnet_type=net)\n            self.is_fake_net = True\n        elif (self.model in [\'L2\', \'l2\']):\n            self.net = networks.L2(use_gpu=use_gpu, colorspace=colorspace)  # not really a network, only for testing\n            self.model_name = \'L2\'\n        elif (self.model in [\'DSSIM\', \'dssim\', \'SSIM\', \'ssim\']):\n            self.net = networks.DSSIM(use_gpu=use_gpu, colorspace=colorspace)\n            self.model_name = \'SSIM\'\n        else:\n            raise ValueError(""Model [%s] not recognized."" % self.model)\n\n        self.parameters = list(self.net.parameters())\n\n        if self.is_train:  # training mode\n            # extra network on top to go from distances (d0,d1) => predicted human judgment (h*)\n            self.rankLoss = networks.BCERankingLoss(use_gpu=use_gpu)\n            self.parameters += self.rankLoss.parameters\n            self.lr = lr\n            self.old_lr = lr\n            self.optimizer_net = torch.optim.Adam(self.parameters, lr=lr, betas=(beta1, 0.999))\n        else:  # test mode\n            self.net.eval()\n\n        if (printNet):\n            print(\'---------- Networks initialized -------------\')\n            networks.print_network(self.net)\n            print(\'-----------------------------------------------\')\n\n    def forward_pair(self, in1, in2, retPerLayer=False):\n        if (retPerLayer):\n            return self.net.forward(in1, in2, retPerLayer=True)\n        else:\n            return self.net.forward(in1, in2)\n\n    def forward(self, in0, in1, retNumpy=True):\n        \'\'\' Function computes the distance between image patches in0 and in1\n        INPUTS\n            in0, in1 - torch.Tensor object of shape Nx3xXxY - image patch scaled to [-1,1]\n            retNumpy - [False] to return as torch.Tensor, [True] to return as numpy array\n        OUTPUT\n            computed distances between in0 and in1\n        \'\'\'\n\n        self.input_ref = in0\n        self.input_p0 = in1\n\n        if (self.use_gpu):\n            self.input_ref = self.input_ref.cuda()\n            self.input_p0 = self.input_p0.cuda()\n\n        self.var_ref = Variable(self.input_ref, requires_grad=True)\n        self.var_p0 = Variable(self.input_p0, requires_grad=True)\n\n        self.d0 = self.forward_pair(self.var_ref, self.var_p0)\n        self.loss_total = self.d0\n\n        def convert_output(d0):\n            if (retNumpy):\n                ans = d0.cpu().data.numpy()\n                if not self.spatial:\n                    ans = ans.flatten()\n                else:\n                    assert (ans.shape[0] == 1 and len(ans.shape) == 4)\n                    return ans[0, ...].transpose(\n                        [1, 2, 0])  # Reshape to usual numpy image format: (height, width, channels)\n                return ans\n            else:\n                return d0\n\n        if self.spatial:\n            L = [convert_output(x) for x in self.d0]\n            spatial_shape = self.spatial_shape\n            if spatial_shape is None:\n                if (self.spatial_factor is None):\n                    spatial_shape = (in0.size()[2], in0.size()[3])\n                else:\n                    spatial_shape = (max([x.shape[0] for x in L]) * self.spatial_factor,\n                                     max([x.shape[1] for x in L]) * self.spatial_factor)\n\n            L = [skimage.transform.resize(x, spatial_shape, order=self.spatial_order, mode=\'edge\') for x in L]\n\n            L = np.mean(np.concatenate(L, 2) * len(L), 2)\n            return L\n        else:\n            return convert_output(self.d0)\n\n    # ***** TRAINING FUNCTIONS *****\n    def optimize_parameters(self):\n        self.forward_train()\n        self.optimizer_net.zero_grad()\n        self.backward_train()\n        self.optimizer_net.step()\n        self.clamp_weights()\n\n    def clamp_weights(self):\n        for module in self.net.modules():\n            if (hasattr(module, \'weight\') and module.kernel_size == (1, 1)):\n                module.weight.data = torch.clamp(module.weight.data, min=0)\n\n    def set_input(self, data):\n        self.input_ref = data[\'ref\']\n        self.input_p0 = data[\'p0\']\n        self.input_p1 = data[\'p1\']\n        self.input_judge = data[\'judge\']\n\n        if (self.use_gpu):\n            self.input_ref = self.input_ref.cuda()\n            self.input_p0 = self.input_p0.cuda()\n            self.input_p1 = self.input_p1.cuda()\n            self.input_judge = self.input_judge.cuda()\n\n        self.var_ref = Variable(self.input_ref, requires_grad=True)\n        self.var_p0 = Variable(self.input_p0, requires_grad=True)\n        self.var_p1 = Variable(self.input_p1, requires_grad=True)\n\n    def forward_train(self):  # run forward pass\n        self.d0 = self.forward_pair(self.var_ref, self.var_p0)\n        self.d1 = self.forward_pair(self.var_ref, self.var_p1)\n        self.acc_r = self.compute_accuracy(self.d0, self.d1, self.input_judge)\n\n        # var_judge\n        self.var_judge = Variable(1. * self.input_judge).view(self.d0.size())\n\n        self.loss_total = self.rankLoss.forward(self.d0, self.d1, self.var_judge * 2. - 1.)\n        return self.loss_total\n\n    def backward_train(self):\n        torch.mean(self.loss_total).backward()\n\n    def compute_accuracy(self, d0, d1, judge):\n        \'\'\' d0, d1 are Variables, judge is a Tensor \'\'\'\n        d1_lt_d0 = (d1 < d0).cpu().data.numpy().flatten()\n        judge_per = judge.cpu().numpy().flatten()\n        return d1_lt_d0 * judge_per + (1 - d1_lt_d0) * (1 - judge_per)\n\n    def get_current_errors(self):\n        retDict = OrderedDict([(\'loss_total\', self.loss_total.data.cpu().numpy()),\n                               (\'acc_r\', self.acc_r)])\n\n        for key in retDict.keys():\n            retDict[key] = np.mean(retDict[key])\n\n        return retDict\n\n    def get_current_visuals(self):\n        zoom_factor = 256 / self.var_ref.data.size()[2]\n\n        ref_img = util.tensor2im(self.var_ref.data)\n        p0_img = util.tensor2im(self.var_p0.data)\n        p1_img = util.tensor2im(self.var_p1.data)\n\n        ref_img_vis = zoom(ref_img, [zoom_factor, zoom_factor, 1], order=0)\n        p0_img_vis = zoom(p0_img, [zoom_factor, zoom_factor, 1], order=0)\n        p1_img_vis = zoom(p1_img, [zoom_factor, zoom_factor, 1], order=0)\n\n        return OrderedDict([(\'ref\', ref_img_vis),\n                            (\'p0\', p0_img_vis),\n                            (\'p1\', p1_img_vis)])\n\n    def save(self, path, label):\n        self.save_network(self.net, path, \'\', label)\n        self.save_network(self.rankLoss.net, path, \'rank\', label)\n\n    def update_learning_rate(self, nepoch_decay):\n        lrd = self.lr / nepoch_decay\n        lr = self.old_lr - lrd\n\n        for param_group in self.optimizer_net.param_groups:\n            param_group[\'lr\'] = lr\n\n        print(\'update lr [%s] decay: %f -> %f\' % (type, self.old_lr, lr))\n        self.old_lr = lr\n\n\ndef score_2afc_dataset(data_loader, func):\n    \'\'\' Function computes Two Alternative Forced Choice (2AFC) score using\n        distance function \'func\' in dataset \'data_loader\'\n    INPUTS\n        data_loader - CustomDatasetDataLoader object - contains a TwoAFCDataset inside\n        func - callable distance function - calling d=func(in0,in1) should take 2\n            pytorch tensors with shape Nx3xXxY, and return numpy array of length N\n    OUTPUTS\n        [0] - 2AFC score in [0,1], fraction of time func agrees with human evaluators\n        [1] - dictionary with following elements\n            d0s,d1s - N arrays containing distances between reference patch to perturbed patches \n            gts - N array in [0,1], preferred patch selected by human evaluators\n                (closer to ""0"" for left patch p0, ""1"" for right patch p1,\n                ""0.6"" means 60pct people preferred right patch, 40pct preferred left)\n            scores - N array in [0,1], corresponding to what percentage function agreed with humans\n    CONSTS\n        N - number of test triplets in data_loader\n    \'\'\'\n\n    d0s = []\n    d1s = []\n    gts = []\n\n    # bar = pb.ProgressBar(max_value=data_loader.load_data().__len__())\n    for (i, data) in enumerate(data_loader.load_data()):\n        d0s += func(data[\'ref\'], data[\'p0\']).tolist()\n        d1s += func(data[\'ref\'], data[\'p1\']).tolist()\n        gts += data[\'judge\'].cpu().numpy().flatten().tolist()\n        # bar.update(i)\n\n    d0s = np.array(d0s)\n    d1s = np.array(d1s)\n    gts = np.array(gts)\n    scores = (d0s < d1s) * (1. - gts) + (d1s < d0s) * gts + (d1s == d0s) * .5\n\n    return (np.mean(scores), dict(d0s=d0s, d1s=d1s, gts=gts, scores=scores))\n\n\ndef score_jnd_dataset(data_loader, func):\n    \'\'\' Function computes JND score using distance function \'func\' in dataset \'data_loader\'\n    INPUTS\n        data_loader - CustomDatasetDataLoader object - contains a JNDDataset inside\n        func - callable distance function - calling d=func(in0,in1) should take 2\n            pytorch tensors with shape Nx3xXxY, and return numpy array of length N\n    OUTPUTS\n        [0] - JND score in [0,1], mAP score (area under precision-recall curve)\n        [1] - dictionary with following elements\n            ds - N array containing distances between two patches shown to human evaluator\n            sames - N array containing fraction of people who thought the two patches were identical\n    CONSTS\n        N - number of test triplets in data_loader\n    \'\'\'\n\n    ds = []\n    gts = []\n\n    # bar = pb.ProgressBar(max_value=data_loader.load_data().__len__())\n    for (i, data) in enumerate(data_loader.load_data()):\n        ds += func(data[\'p0\'], data[\'p1\']).tolist()\n        gts += data[\'same\'].cpu().numpy().flatten().tolist()\n        # bar.update(i)\n\n    sames = np.array(gts)\n    ds = np.array(ds)\n\n    sorted_inds = np.argsort(ds)\n    ds_sorted = ds[sorted_inds]\n    sames_sorted = sames[sorted_inds]\n\n    TPs = np.cumsum(sames_sorted)\n    FPs = np.cumsum(1 - sames_sorted)\n    FNs = np.sum(sames_sorted) - TPs\n\n    precs = TPs / (TPs + FPs)\n    recs = TPs / (TPs + FNs)\n    score = util.voc_ap(recs, precs)\n\n    return score, dict(ds=ds, sames=sames)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/lpips/models/networks_basic.py,18,"b""from __future__ import absolute_import\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom . import pretrained_networks as pn\nfrom .. import util\n\n\n# Off-the-shelf deep network\nclass PNet(nn.Module):\n    '''Pre-trained network with all channels equally weighted by default'''\n    def __init__(self, pnet_type='vgg', pnet_rand=False, use_gpu=True):\n        super(PNet, self).__init__()\n\n        self.use_gpu = use_gpu\n\n        self.pnet_type = pnet_type\n        self.pnet_rand = pnet_rand\n\n        self.shift = torch.autograd.Variable(torch.Tensor([-.030, -.088, -.188]).view(1,3,1,1))\n        self.scale = torch.autograd.Variable(torch.Tensor([.458, .448, .450]).view(1,3,1,1))\n        \n        if(self.pnet_type in ['vgg','vgg16']):\n            self.net = pn.vgg16(pretrained=not self.pnet_rand,requires_grad=False)\n        elif(self.pnet_type=='alex'):\n            self.net = pn.alexnet(pretrained=not self.pnet_rand,requires_grad=False)\n        elif(self.pnet_type[:-2]=='resnet'):\n            self.net = pn.resnet(pretrained=not self.pnet_rand,requires_grad=False, num=int(self.pnet_type[-2:]))\n        elif(self.pnet_type=='squeeze'):\n            self.net = pn.squeezenet(pretrained=not self.pnet_rand,requires_grad=False)\n\n        self.L = self.net.N_slices\n\n        if(use_gpu):\n            self.net.cuda()\n            self.shift = self.shift.cuda()\n            self.scale = self.scale.cuda()\n\n    def forward(self, in0, in1, retPerLayer=False):\n        in0_sc = (in0 - self.shift.expand_as(in0))/self.scale.expand_as(in0)\n        in1_sc = (in1 - self.shift.expand_as(in0))/self.scale.expand_as(in0)\n\n        outs0 = self.net.forward(in0_sc)\n        outs1 = self.net.forward(in1_sc)\n\n        if(retPerLayer):\n            all_scores = []\n        for (kk,out0) in enumerate(outs0):\n            cur_score = (1. - util.cos_sim(outs0[kk], outs1[kk]))\n            if(kk==0):\n                val = 1.*cur_score\n            else:\n                # val = val + self.lambda_feat_layers[kk]*cur_score\n                val = val + cur_score\n            if(retPerLayer):\n                all_scores+=[cur_score]\n\n        if(retPerLayer):\n            return (val, all_scores)\n        else:\n            return val\n\n# Learned perceptual metric\nclass PNetLin(nn.Module):\n    def __init__(self, pnet_type='vgg', pnet_rand=False, pnet_tune=False, use_dropout=True, use_gpu=True, spatial=False, version='0.1'):\n        super(PNetLin, self).__init__()\n\n        self.use_gpu = use_gpu\n        self.pnet_type = pnet_type\n        self.pnet_tune = pnet_tune\n        self.pnet_rand = pnet_rand\n        self.spatial = spatial\n        self.version = version\n\n        if(self.pnet_type in ['vgg','vgg16']):\n            net_type = pn.vgg16\n            self.chns = [64,128,256,512,512]\n        elif(self.pnet_type=='alex'):\n            net_type = pn.alexnet\n            self.chns = [64,192,384,256,256]\n        elif(self.pnet_type=='squeeze'):\n            net_type = pn.squeezenet\n            self.chns = [64,128,256,384,384,512,512]\n\n        if(self.pnet_tune):\n            self.net = net_type(pretrained=not self.pnet_rand,requires_grad=True)\n        else:\n            self.net = [net_type(pretrained=not self.pnet_rand,requires_grad=False),]\n\n        self.lin0 = NetLinLayer(self.chns[0],use_dropout=use_dropout)\n        self.lin1 = NetLinLayer(self.chns[1],use_dropout=use_dropout)\n        self.lin2 = NetLinLayer(self.chns[2],use_dropout=use_dropout)\n        self.lin3 = NetLinLayer(self.chns[3],use_dropout=use_dropout)\n        self.lin4 = NetLinLayer(self.chns[4],use_dropout=use_dropout)\n        self.lins = [self.lin0,self.lin1,self.lin2,self.lin3,self.lin4]\n        if(self.pnet_type=='squeeze'): # 7 layers for squeezenet\n            self.lin5 = NetLinLayer(self.chns[5],use_dropout=use_dropout)\n            self.lin6 = NetLinLayer(self.chns[6],use_dropout=use_dropout)\n            self.lins+=[self.lin5,self.lin6]\n\n        self.shift = torch.autograd.Variable(torch.Tensor([-.030, -.088, -.188]).view(1,3,1,1))\n        self.scale = torch.autograd.Variable(torch.Tensor([.458, .448, .450]).view(1,3,1,1))\n\n        if(use_gpu):\n            if(self.pnet_tune):\n                self.net.cuda()\n            else:\n                self.net[0].cuda()\n            self.shift = self.shift.cuda()\n            self.scale = self.scale.cuda()\n            self.lin0.cuda()\n            self.lin1.cuda()\n            self.lin2.cuda()\n            self.lin3.cuda()\n            self.lin4.cuda()\n            if(self.pnet_type=='squeeze'):\n                self.lin5.cuda()\n                self.lin6.cuda()\n\n    def forward(self, in0, in1):\n        in0_sc = (in0 - self.shift.expand_as(in0))/self.scale.expand_as(in0)\n        in1_sc = (in1 - self.shift.expand_as(in0))/self.scale.expand_as(in0)\n\n        if(self.version=='0.0'):\n            # v0.0 - original release had a bug, where input was not scaled\n            in0_input = in0\n            in1_input = in1\n        else:\n            # v0.1\n            in0_input = in0_sc\n            in1_input = in1_sc\n\n        if(self.pnet_tune):\n            outs0 = self.net.forward(in0_input)\n            outs1 = self.net.forward(in1_input)\n        else:\n            outs0 = self.net[0].forward(in0_input)\n            outs1 = self.net[0].forward(in1_input)\n\n        feats0 = {}\n        feats1 = {}\n        diffs = [0]*len(outs0)\n\n        for (kk,out0) in enumerate(outs0):\n            feats0[kk] = util.normalize_tensor(outs0[kk])\n            feats1[kk] = util.normalize_tensor(outs1[kk])\n            diffs[kk] = (feats0[kk]-feats1[kk])**2\n\n        if self.spatial:\n            lin_models = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n            if(self.pnet_type=='squeeze'):\n                lin_models.extend([self.lin5, self.lin6])\n            res = [lin_models[kk].model(diffs[kk]) for kk in range(len(diffs))]\n            return res\n\t\t\t\n        val = torch.mean(torch.mean(self.lin0.model(diffs[0]),dim=3),dim=2)\n        val = val + torch.mean(torch.mean(self.lin1.model(diffs[1]),dim=3),dim=2)\n        val = val + torch.mean(torch.mean(self.lin2.model(diffs[2]),dim=3),dim=2)\n        val = val + torch.mean(torch.mean(self.lin3.model(diffs[3]),dim=3),dim=2)\n        val = val + torch.mean(torch.mean(self.lin4.model(diffs[4]),dim=3),dim=2)\n        if(self.pnet_type=='squeeze'):\n            val = val + torch.mean(torch.mean(self.lin5.model(diffs[5]),dim=3),dim=2)\n            val = val + torch.mean(torch.mean(self.lin6.model(diffs[6]),dim=3),dim=2)\n\n        val = val.view(val.size()[0],val.size()[1],1,1)\n\n        return val\n\nclass Dist2LogitLayer(nn.Module):\n    ''' takes 2 distances, puts through fc layers, spits out value between [0,1] (if use_sigmoid is True) '''\n    def __init__(self, chn_mid=32,use_sigmoid=True):\n        super(Dist2LogitLayer, self).__init__()\n        layers = [nn.Conv2d(5, chn_mid, 1, stride=1, padding=0, bias=True),]\n        layers += [nn.LeakyReLU(0.2,True),]\n        layers += [nn.Conv2d(chn_mid, chn_mid, 1, stride=1, padding=0, bias=True),]\n        layers += [nn.LeakyReLU(0.2,True),]\n        layers += [nn.Conv2d(chn_mid, 1, 1, stride=1, padding=0, bias=True),]\n        if(use_sigmoid):\n            layers += [nn.Sigmoid(),]\n        self.model = nn.Sequential(*layers)\n\n    def forward(self,d0,d1,eps=0.1):\n        return self.model.forward(torch.cat((d0,d1,d0-d1,d0/(d1+eps),d1/(d0+eps)),dim=1))\n\nclass BCERankingLoss(nn.Module):\n    def __init__(self, use_gpu=True, chn_mid=32):\n        super(BCERankingLoss, self).__init__()\n        self.use_gpu = use_gpu\n        self.net = Dist2LogitLayer(chn_mid=chn_mid)\n        self.parameters = list(self.net.parameters())\n        self.loss = torch.nn.BCELoss()\n        self.model = nn.Sequential(*[self.net])\n\n        if(self.use_gpu):\n            self.net.cuda()\n\n    def forward(self, d0, d1, judge):\n        per = (judge+1.)/2.\n        if(self.use_gpu):\n            per = per.cuda()\n        self.logit = self.net.forward(d0,d1)\n        return self.loss(self.logit, per)\n\nclass NetLinLayer(nn.Module):\n    ''' A single linear layer which does a 1x1 conv '''\n    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n        super(NetLinLayer, self).__init__()\n\n        layers = [nn.Dropout(),] if(use_dropout) else []\n        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False),]\n        self.model = nn.Sequential(*layers)\n\n\n# L2, DSSIM metrics\nclass FakeNet(nn.Module):\n    def __init__(self, use_gpu=True, colorspace='Lab'):\n        super(FakeNet, self).__init__()\n        self.use_gpu = use_gpu\n        self.colorspace=colorspace\n\nclass L2(FakeNet):\n\n    def forward(self, in0, in1):\n        assert(in0.size()[0]==1) # currently only supports batchSize 1\n\n        if(self.colorspace=='RGB'):\n            (N,C,X,Y) = in0.size()\n            value = torch.mean(torch.mean(torch.mean((in0-in1)**2,dim=1).view(N,1,X,Y),dim=2).view(N,1,1,Y),dim=3).view(N)\n            return value\n        elif(self.colorspace=='Lab'):\n            value = util.l2(util.tensor2np(util.tensor2tensorlab(in0.data, to_norm=False)),\n                            util.tensor2np(util.tensor2tensorlab(in1.data, to_norm=False)), range=100.).astype('float')\n            ret_var = Variable( torch.Tensor((value,) ) )\n            if(self.use_gpu):\n                ret_var = ret_var.cuda()\n            return ret_var\n\nclass DSSIM(FakeNet):\n\n    def forward(self, in0, in1):\n        assert(in0.size()[0]==1) # currently only supports batchSize 1\n\n        if(self.colorspace=='RGB'):\n            value = util.dssim(1. * util.tensor2im(in0.data), 1. * util.tensor2im(in1.data), range=255.).astype('float')\n        elif(self.colorspace=='Lab'):\n            value = util.dssim(util.tensor2np(util.tensor2tensorlab(in0.data, to_norm=False)),\n                               util.tensor2np(util.tensor2tensorlab(in1.data, to_norm=False)), range=100.).astype('float')\n        ret_var = Variable( torch.Tensor((value,) ) )\n        if(self.use_gpu):\n            ret_var = ret_var.cuda()\n        return ret_var\n\ndef print_network(net):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print('Network',net)\n    print('Total number of parameters: %d' % num_params)\n"""
thirdparty/his_evaluators/his_evaluators/metrics/lpips/models/pretrained_networks.py,22,"b'from collections import namedtuple\nimport torch\nimport torch.nn\nfrom torchvision import models\n\n\nclass squeezenet(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True):\n        super(squeezenet, self).__init__()\n        pretrained_features = models.squeezenet1_1(pretrained=pretrained).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.slice6 = torch.nn.Sequential()\n        self.slice7 = torch.nn.Sequential()\n        self.N_slices = 7\n        for x in range(2):\n            self.slice1.add_module(str(x), pretrained_features[x])\n        for x in range(2,5):\n            self.slice2.add_module(str(x), pretrained_features[x])\n        for x in range(5, 8):\n            self.slice3.add_module(str(x), pretrained_features[x])\n        for x in range(8, 10):\n            self.slice4.add_module(str(x), pretrained_features[x])\n        for x in range(10, 11):\n            self.slice5.add_module(str(x), pretrained_features[x])\n        for x in range(11, 12):\n            self.slice6.add_module(str(x), pretrained_features[x])\n        for x in range(12, 13):\n            self.slice7.add_module(str(x), pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1 = h\n        h = self.slice2(h)\n        h_relu2 = h\n        h = self.slice3(h)\n        h_relu3 = h\n        h = self.slice4(h)\n        h_relu4 = h\n        h = self.slice5(h)\n        h_relu5 = h\n        h = self.slice6(h)\n        h_relu6 = h\n        h = self.slice7(h)\n        h_relu7 = h\n        vgg_outputs = namedtuple(""SqueezeOutputs"", [\'relu1\',\'relu2\',\'relu3\',\'relu4\',\'relu5\',\'relu6\',\'relu7\'])\n        out = vgg_outputs(h_relu1,h_relu2,h_relu3,h_relu4,h_relu5,h_relu6,h_relu7)\n\n        return out\n\n\nclass alexnet(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True):\n        super(alexnet, self).__init__()\n        alexnet_pretrained_features = models.alexnet(pretrained=pretrained).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.N_slices = 5\n        for x in range(2):\n            self.slice1.add_module(str(x), alexnet_pretrained_features[x])\n        for x in range(2, 5):\n            self.slice2.add_module(str(x), alexnet_pretrained_features[x])\n        for x in range(5, 8):\n            self.slice3.add_module(str(x), alexnet_pretrained_features[x])\n        for x in range(8, 10):\n            self.slice4.add_module(str(x), alexnet_pretrained_features[x])\n        for x in range(10, 12):\n            self.slice5.add_module(str(x), alexnet_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1 = h\n        h = self.slice2(h)\n        h_relu2 = h\n        h = self.slice3(h)\n        h_relu3 = h\n        h = self.slice4(h)\n        h_relu4 = h\n        h = self.slice5(h)\n        h_relu5 = h\n        alexnet_outputs = namedtuple(""AlexnetOutputs"", [\'relu1\', \'relu2\', \'relu3\', \'relu4\', \'relu5\'])\n        out = alexnet_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n\n        return out\n\n\nclass vgg16(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True):\n        super(vgg16, self).__init__()\n        vgg_pretrained_features = models.vgg16(pretrained=pretrained).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.N_slices = 5\n        for x in range(4):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(9, 16):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(16, 23):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(23, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        h = self.slice5(h)\n        h_relu5_3 = h\n        vgg_outputs = namedtuple(""VggOutputs"", [\'relu1_2\', \'relu2_2\', \'relu3_3\', \'relu4_3\', \'relu5_3\'])\n        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n\n        return out\n\n\nclass resnet(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True, num=18):\n        super(resnet, self).__init__()\n        if(num==18):\n            self.net = models.resnet18(pretrained=pretrained)\n        elif(num==34):\n            self.net = models.resnet34(pretrained=pretrained)\n        elif(num==50):\n            self.net = models.resnet50(pretrained=pretrained)\n        elif(num==101):\n            self.net = models.resnet101(pretrained=pretrained)\n        elif(num==152):\n            self.net = models.resnet152(pretrained=pretrained)\n        self.N_slices = 5\n\n        self.conv1 = self.net.conv1\n        self.bn1 = self.net.bn1\n        self.relu = self.net.relu\n        self.maxpool = self.net.maxpool\n        self.layer1 = self.net.layer1\n        self.layer2 = self.net.layer2\n        self.layer3 = self.net.layer3\n        self.layer4 = self.net.layer4\n\n    def forward(self, X):\n        h = self.conv1(X)\n        h = self.bn1(h)\n        h = self.relu(h)\n        h_relu1 = h\n        h = self.maxpool(h)\n        h = self.layer1(h)\n        h_conv2 = h\n        h = self.layer2(h)\n        h_conv3 = h\n        h = self.layer3(h)\n        h_conv4 = h\n        h = self.layer4(h)\n        h_conv5 = h\n\n        outputs = namedtuple(""Outputs"", [\'relu1\',\'conv2\',\'conv3\',\'conv4\',\'conv5\'])\n        out = outputs(h_relu1, h_conv2, h_conv3, h_conv4, h_conv5)\n\n        return out\n'"
thirdparty/his_evaluators/his_evaluators/metrics/lpips/util/__init__.py,0,b'from .util import *\n'
thirdparty/his_evaluators/his_evaluators/metrics/lpips/util/util.py,6,"b'from __future__ import print_function\n\nfrom PIL import Image\nimport inspect\nimport re\nimport numpy as np\nimport os\nimport collections\nfrom scipy.ndimage.interpolation import zoom\nfrom skimage.measure import compare_ssim\nimport torch\nfrom datetime import datetime\n\n\ndef datetime_str():\n    now = datetime.now()\n    return \'%04d-%02d-%02d-%02d-%02d-%02d\' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n\n\ndef read_text_file(in_path):\n    fid = open(in_path, \'r\')\n\n    vals = []\n    cur_line = fid.readline()\n    while (cur_line != \'\'):\n        vals.append(float(cur_line))\n        cur_line = fid.readline()\n\n    fid.close()\n    return np.array(vals)\n\n\ndef rand_flip(input1, input2):\n    if (np.random.binomial(1, .5) == 1):\n        return (input1, input2)\n    else:\n        return (input2, input1)\n\n\ndef l2(p0, p1, range=255.):\n    return .5 * np.mean((p0 / range - p1 / range) ** 2)\n\n\ndef psnr(p0, p1, peak=255.):\n    return 10 * np.log10(peak ** 2 / np.mean((1. * p0 - 1. * p1) ** 2))\n\n\ndef dssim(p0, p1, range=255.):\n    # embed()\n    return (1 - compare_ssim(p0, p1, data_range=range, multichannel=True)) / 2.\n\n\ndef rgb2lab_with_mean_center(in_img, mean_cent=False):\n    from skimage import color\n    img_lab = color.rgb2lab(in_img)\n    if mean_cent:\n        img_lab[:, :, 0] = img_lab[:, :, 0] - 50\n    return img_lab\n\n\ndef normalize_blob(in_feat, eps=1e-10):\n    norm_factor = np.sqrt(np.sum(in_feat ** 2, axis=1, keepdims=True))\n    return in_feat / (norm_factor + eps)\n\n\ndef cos_sim_blob(in0, in1):\n    in0_norm = normalize_blob(in0)\n    in1_norm = normalize_blob(in1)\n    (N, C, X, Y) = in0_norm.shape\n\n    return np.mean(np.mean(np.sum(in0_norm * in1_norm, axis=1), axis=1), axis=1)\n\n\ndef normalize_tensor(in_feat, eps=1e-10):\n    # norm_factor = torch.sqrt(torch.sum(in_feat**2,dim=1)).view(in_feat.size()[0],1,in_feat.size()[2],in_feat.size()[3]).repeat(1,in_feat.size()[1],1,1)\n    norm_factor = torch.sqrt(torch.sum(in_feat ** 2, dim=1)).view(in_feat.size()[0], 1, in_feat.size()[2],\n                                                                  in_feat.size()[3])\n    return in_feat / (norm_factor.expand_as(in_feat) + eps)\n\n\ndef cos_sim(in0, in1):\n    in0_norm = normalize_tensor(in0)\n    in1_norm = normalize_tensor(in1)\n    N = in0.size()[0]\n    X = in0.size()[2]\n    Y = in0.size()[3]\n\n    return torch.mean(torch.mean(torch.sum(in0_norm * in1_norm, dim=1).view(N, 1, X, Y), dim=2).view(N, 1, 1, Y),\n                      dim=3).view(N)\n\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the conve\ndef tensor2np(tensor_obj):\n    # change dimension of a tensor object into a numpy array\n    return tensor_obj[0].cpu().float().numpy().transpose((1, 2, 0))\n\n\ndef np2tensor(np_obj):\n    # change dimenion of np array into tensor array\n    return torch.Tensor(np_obj[:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n\n\ndef tensor2tensorlab(image_tensor, to_norm=True, mc_only=False):\n    # image tensor to lab tensor\n    from skimage import color\n\n    img = tensor2im(image_tensor)\n    # print(\'img_rgb\',img.flatten())\n    img_lab = color.rgb2lab(img)\n    # print(\'img_lab\',img_lab.flatten())\n    if (mc_only):\n        img_lab[:, :, 0] = img_lab[:, :, 0] - 50\n    if (to_norm and not mc_only):\n        img_lab[:, :, 0] = img_lab[:, :, 0] - 50\n        img_lab = img_lab / 100.\n\n    return np2tensor(img_lab)\n\n\ndef tensorlab2tensor(lab_tensor, return_inbnd=False):\n    from skimage import color\n    import warnings\n    warnings.filterwarnings(""ignore"")\n\n    lab = tensor2np(lab_tensor) * 100.\n    lab[:, :, 0] = lab[:, :, 0] + 50\n    # print(\'lab\',lab)\n\n    rgb_back = 255. * np.clip(color.lab2rgb(lab.astype(\'float\')), 0, 1)\n    # print(\'rgb\',rgb_back)\n    if (return_inbnd):\n        # convert back to lab, see if we match\n        lab_back = color.rgb2lab(rgb_back.astype(\'uint8\'))\n        # print(\'lab_back\',lab_back)\n        # print(\'lab==lab_back\',np.isclose(lab_back,lab,atol=1.))\n        # print(\'lab-lab_back\',np.abs(lab-lab_back))\n        mask = 1. * np.isclose(lab_back, lab, atol=2.)\n        mask = np2tensor(np.prod(mask, axis=2)[:, :, np.newaxis])\n        return (im2tensor(rgb_back), mask)\n    else:\n        return im2tensor(rgb_back)\n\n\ndef tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255. / 2.):\n    # def tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=1.):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n    return image_numpy.astype(imtype)\n\n\ndef im2tensor(image, imtype=np.uint8, cent=1., factor=255. / 2.):\n    # def im2tensor(image, imtype=np.uint8, cent=1., factor=1.):\n    return torch.Tensor((image / factor - cent)\n                        [:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n\n\ndef tensor2vec(vector_tensor):\n    return vector_tensor.data.cpu().numpy()[:, :, 0, 0]\n\n\ndef diagnose_network(net, name=\'network\'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef grab_patch(img_in, P, yy, xx):\n    return img_in[yy:yy + P, xx:xx + P, :]\n\n\ndef load_image(path):\n    import cv2\n    return cv2.imread(path)[:, :, ::-1]\n\n\ndef resize_image(img, max_size=256):\n    [Y, X] = img.shape[:2]\n\n    # resize\n    max_dim = max([Y, X])\n    zoom_factor = 1. * max_size / max_dim\n    img = zoom(img, [zoom_factor, zoom_factor, 1])\n\n    return img\n\n\ndef resize_image_zoom(img, zoom_factor=1., order=3):\n    if (zoom_factor == 1):\n        return img\n    else:\n        return zoom(img, [zoom_factor, zoom_factor, 1], order=order)\n\n\ndef save_image(image_numpy, image_path, ):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\n\ndef prep_display_image(img, dtype=\'uint8\'):\n    if (dtype == \'uint8\'):\n        return np.clip(img, 0, 255).astype(\'uint8\')\n    else:\n        return np.clip(img, 0, 1.)\n\n\ndef info(object, spacing=10, collapse=1):\n    """"""Print methods and doc strings.\n    Takes module, class, list, dictionary, or string.""""""\n    methodList = [\n        e for e in dir(object) if isinstance(\n            getattr(\n                object,\n                e),\n            collections.Callable)]\n    processFunc = collapse and (lambda s: "" "".join(s.split())) or (lambda s: s)\n    print(""\\n"".join([""%s %s"" %\n                     (method.ljust(spacing),\n                      processFunc(str(getattr(object, method).__doc__)))\n                     for method in methodList]))\n\n\ndef varname(p):\n    for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n        m = re.search(r\'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)\', line)\n        if m:\n            return m.group(1)\n\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print(\'shape,\', x.shape)\n    if val:\n        x = x.flatten()\n        print(\n            \'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f\' %\n            (np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef rgb2lab(input):\n    from skimage import color\n    return color.rgb2lab(input / 255.)\n\n\ndef montage(\n    imgs,\n    PAD=5,\n    RATIO=16 / 9.,\n    EXTRA_PAD=(\n        False,\n        False),\n    MM=-1,\n    NN=-1,\n    primeDir=0,\n    verbose=False,\n    returnGridPos=False,\n    backClr=np.array(\n        (0,\n         0,\n         0))):\n    # INPUTS\n    #   imgs        YxXxMxN or YxXxN\n    #   PAD         scalar              number of pixels in between\n    #   RATIO       scalar              target ratio of cols/rows\n    #   MM          scalar              # rows, if specified, overrides RATIO\n    #   NN          scalar              # columns, if specified, overrides RATIO\n    #   primeDir    scalar              0 for top-to-bottom, 1 for left-to-right\n    # OUTPUTS\n    #   mont_imgs   MM*Y x NN*X x M     big image with everything montaged\n    # def montage(imgs, PAD=5, RATIO=16/9., MM=-1, NN=-1, primeDir=0,\n    # verbose=False, forceFloat=False):\n    if (imgs.ndim == 3):\n        toExp = True\n        imgs = imgs[:, :, np.newaxis, :]\n    else:\n        toExp = False\n\n    Y = imgs.shape[0]\n    X = imgs.shape[1]\n    M = imgs.shape[2]\n    N = imgs.shape[3]\n\n    PADS = np.array((PAD))\n    if (PADS.flatten().size == 1):\n        PADY = PADS\n        PADX = PADS\n    else:\n        PADY = PADS[0]\n        PADX = PADS[1]\n\n    if (MM == -1 and NN == -1):\n        NN = np.ceil(np.sqrt(1.0 * N * RATIO))\n        MM = np.ceil(1.0 * N / NN)\n        NN = np.ceil(1.0 * N / MM)\n    elif (MM == -1):\n        MM = np.ceil(1.0 * N / NN)\n    elif (NN == -1):\n        NN = np.ceil(1.0 * N / MM)\n\n    if (primeDir == 0):  # write top-to-bottom\n        [grid_mm, grid_nn] = np.meshgrid(\n            np.arange(MM, dtype=\'uint\'), np.arange(NN, dtype=\'uint\'))\n    elif (primeDir == 1):  # write left-to-right\n        [grid_nn, grid_mm] = np.meshgrid(\n            np.arange(NN, dtype=\'uint\'), np.arange(MM, dtype=\'uint\'))\n\n    grid_mm = np.uint(grid_mm.flatten()[0:N])\n    grid_nn = np.uint(grid_nn.flatten()[0:N])\n\n    EXTRA_PADY = EXTRA_PAD[0] * PADY\n    EXTRA_PADX = EXTRA_PAD[0] * PADX\n\n    # mont_imgs = np.zeros(((Y+PAD)*MM-PAD, (X+PAD)*NN-PAD, M), dtype=use_dtype)\n    mont_imgs = np.zeros(\n        (np.uint(\n            (Y + PADY) * MM - PADY + EXTRA_PADY),\n         np.uint(\n             (X + PADX) * NN - PADX + EXTRA_PADX),\n         M),\n        dtype=imgs.dtype)\n    mont_imgs = mont_imgs + \\\n                backClr.flatten()[np.newaxis, np.newaxis, :].astype(mont_imgs.dtype)\n\n    for ii in np.random.permutation(N):\n        # print imgs[:,:,:,ii].shape\n        # mont_imgs[grid_mm[ii]*(Y+PAD):(grid_mm[ii]*(Y+PAD)+Y), grid_nn[ii]*(X+PAD):(grid_nn[ii]*(X+PAD)+X),:]\n        mont_imgs[np.uint(grid_mm[ii] *\n                          (Y +\n                           PADY)):np.uint((grid_mm[ii] *\n                                           (Y +\n                                            PADY) +\n                                           Y)), np.uint(grid_nn[ii] *\n                                                        (X +\n                                                         PADX)):np.uint((grid_nn[ii] *\n                                                                         (X +\n                                                                          PADX) +\n                                                                         X)), :] = imgs[:, :, :, ii]\n\n    if (M == 1):\n        imgs = imgs.reshape(imgs.shape[0], imgs.shape[1], imgs.shape[3])\n\n    if (toExp):\n        mont_imgs = mont_imgs[:, :, 0]\n\n    if (returnGridPos):\n        # return (mont_imgs,np.concatenate((grid_mm[:,:,np.newaxis]*(Y+PAD),\n        # grid_nn[:,:,np.newaxis]*(X+PAD)),axis=2))\n        return (mont_imgs, np.concatenate(\n            (grid_mm[:, np.newaxis] * (Y + PADY), grid_nn[:, np.newaxis] * (X + PADX)), axis=1))\n        # return (mont_imgs, (grid_mm,grid_nn))\n    else:\n        return mont_imgs\n\n\ndef flatten_nested_list(nested_list):\n    # only works for list of list\n    accum = []\n    for sublist in nested_list:\n        for item in sublist:\n            accum.append(item)\n    return accum\n\n\ndef read_file(in_path, list_lines=False):\n    agg_str = \'\'\n    f = open(in_path, \'r\')\n    cur_line = f.readline()\n    while (cur_line != \'\'):\n        agg_str += cur_line\n        cur_line = f.readline()\n    f.close()\n    if (list_lines == False):\n        return agg_str.replace(\'\\n\', \'\')\n    else:\n        line_list = agg_str.split(\'\\n\')\n        ret_list = []\n        for item in line_list:\n            if (item != \'\'):\n                ret_list.append(item)\n        return ret_list\n\n\ndef read_csv_file_as_text(in_path):\n    agg_str = []\n    f = open(in_path, \'r\')\n    cur_line = f.readline()\n    while (cur_line != \'\'):\n        agg_str.append(cur_line)\n        cur_line = f.readline()\n    f.close()\n    return agg_str\n\n\ndef random_swap(obj0, obj1):\n    if (np.random.rand() < .5):\n        return (obj0, obj1, 0)\n    else:\n        return (obj1, obj0, 1)\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/utils/__init__.py,0,b''
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/utils/augmentations.py,1,"b'import torch\n\n\ndef horisontal_flip(images, targets):\n    images = torch.flip(images, [-1])\n    targets[:, 2] = 1 - targets[:, 2]\n    return images, targets\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/utils/datasets.py,6,"b'import glob\nimport random\nimport os\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\n\n\ndef pad_to_square(img, pad_value):\n    c, h, w = img.shape\n    dim_diff = np.abs(h - w)\n    # (upper / left) padding and (lower / right) padding\n    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n    # Determine padding\n    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n    # Add padding\n    img = F.pad(img, pad, ""constant"", value=pad_value)\n\n    return img, pad\n\n\ndef resize(image, size):\n    image = F.interpolate(image.unsqueeze(0), size=size, mode=""nearest"").squeeze(0)\n    return image\n\n\ndef random_resize(images, min_size=288, max_size=448):\n    new_size = random.sample(list(range(min_size, max_size + 1, 32)), 1)[0]\n    images = F.interpolate(images, size=new_size, mode=""nearest"")\n    return images\n\n\nclass ImageFolder(Dataset):\n    def __init__(self, folder_path, img_size=416):\n        self.files = sorted(glob.glob(""%s/*.*"" % folder_path))\n        self.img_size = img_size\n\n    def __getitem__(self, index):\n        img_path = self.files[index % len(self.files)]\n        # Extract image as PyTorch tensor\n        img = transforms.ToTensor()(Image.open(img_path))\n        # Pad to square resolution\n        img, _ = pad_to_square(img, 0)\n        # Resize\n        img = resize(img, self.img_size)\n\n        return img_path, img\n\n    def __len__(self):\n        return len(self.files)\n\n\nclass ListDataset(Dataset):\n    def __init__(self, list_path, img_size=416, augment=True, multiscale=True, normalized_labels=True):\n        with open(list_path, ""r"") as file:\n            self.img_files = file.readlines()\n\n        self.label_files = [\n            path.replace(""images"", ""labels"").replace("".png"", "".txt"").replace("".jpg"", "".txt"")\n            for path in self.img_files\n        ]\n        self.img_size = img_size\n        self.max_objects = 100\n        self.augment = augment\n        self.multiscale = multiscale\n        self.normalized_labels = normalized_labels\n        self.min_size = self.img_size - 3 * 32\n        self.max_size = self.img_size + 3 * 32\n        self.batch_count = 0\n\n    def __getitem__(self, index):\n\n        # ---------\n        #  Image\n        # ---------\n\n        img_path = self.img_files[index % len(self.img_files)].rstrip()\n\n        # Extract image as PyTorch tensor\n        img = transforms.ToTensor()(Image.open(img_path).convert(\'RGB\'))\n\n        # Handle images with less than three channels\n        if len(img.shape) != 3:\n            img = img.unsqueeze(0)\n            img = img.expand((3, img.shape[1:]))\n\n        _, h, w = img.shape\n        h_factor, w_factor = (h, w) if self.normalized_labels else (1, 1)\n        # Pad to square resolution\n        img, pad = pad_to_square(img, 0)\n        _, padded_h, padded_w = img.shape\n\n        # ---------\n        #  Label\n        # ---------\n\n        label_path = self.label_files[index % len(self.img_files)].rstrip()\n\n        targets = None\n        if os.path.exists(label_path):\n            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n            # Extract coordinates for unpadded + unscaled image\n            x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\n            y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\n            x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\n            y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\n            # Adjust for added padding\n            x1 += pad[0]\n            y1 += pad[2]\n            x2 += pad[1]\n            y2 += pad[3]\n            # Returns (x, y, w, h)\n            boxes[:, 1] = ((x1 + x2) / 2) / padded_w\n            boxes[:, 2] = ((y1 + y2) / 2) / padded_h\n            boxes[:, 3] *= w_factor / padded_w\n            boxes[:, 4] *= h_factor / padded_h\n\n            targets = torch.zeros((len(boxes), 6))\n            targets[:, 1:] = boxes\n\n        # Apply augmentations\n        if self.augment:\n            from .augmentations import horisontal_flip\n            if np.random.random() < 0.5:\n                img, targets = horisontal_flip(img, targets)\n\n        return img_path, img, targets\n\n    def collate_fn(self, batch):\n        paths, imgs, targets = list(zip(*batch))\n        # Remove empty placeholder targets\n        targets = [boxes for boxes in targets if boxes is not None]\n        # Add sample index to targets\n        for i, boxes in enumerate(targets):\n            boxes[:, 0] = i\n        targets = torch.cat(targets, 0)\n        # Selects new image size every tenth batch\n        if self.multiscale and self.batch_count % 10 == 0:\n            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n        # Resize images to input shape\n        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n        self.batch_count += 1\n        return paths, imgs, targets\n\n    def __len__(self):\n        return len(self.img_files)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/utils/logger.py,0,"b'import tensorflow as tf\n\n\nclass Logger(object):\n    def __init__(self, log_dir):\n        """"""Create a summary writer logging to log_dir.""""""\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def scalar_summary(self, tag, value, step):\n        """"""Log a scalar variable.""""""\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def list_of_scalars_summary(self, tag_value_pairs, step):\n        """"""Log scalar variables.""""""\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value) for tag, value in tag_value_pairs])\n        self.writer.add_summary(summary, step)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/utils/parse_config.py,0,"b'\n\ndef parse_model_config(path):\n    """"""Parses the yolo-v3 layer configuration file and returns module definitions""""""\n    file = open(path, \'r\')\n    lines = file.read().split(\'\\n\')\n    lines = [x for x in lines if x and not x.startswith(\'#\')]\n    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n    module_defs = []\n    for line in lines:\n        if line.startswith(\'[\'): # This marks the start of a new block\n            module_defs.append({})\n            module_defs[-1][\'type\'] = line[1:-1].rstrip()\n            if module_defs[-1][\'type\'] == \'convolutional\':\n                module_defs[-1][\'batch_normalize\'] = 0\n        else:\n            key, value = line.split(""="")\n            value = value.strip()\n            module_defs[-1][key.rstrip()] = value.strip()\n    file.close()\n    return module_defs\n\ndef parse_data_config(path):\n    """"""Parses the data configuration file""""""\n    options = dict()\n    options[\'gpus\'] = \'0,1,2,3\'\n    options[\'num_workers\'] = \'10\'\n    with open(path, \'r\') as fp:\n        lines = fp.readlines()\n    for line in lines:\n        line = line.strip()\n        if line == \'\' or line.startswith(\'#\'):\n            continue\n        key, value = line.split(\'=\')\n        options[key.strip()] = value.strip()\n    return options\n'"
thirdparty/his_evaluators/his_evaluators/metrics/yolov3/utils/utils.py,19,"b'from __future__ import division\nimport math\nimport time\nimport tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\ndef to_cpu(tensor):\n    return tensor.detach().cpu()\n\n\ndef load_classes(path):\n    """"""\n    Loads class labels at \'path\'\n    """"""\n    fp = open(path, ""r"")\n    names = fp.read().split(""\\n"")[:-1]\n    return names\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(""Conv"") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(""BatchNorm2d"") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\n\ndef rescale_boxes(boxes, current_dim, original_shape):\n    """""" Rescales bounding boxes to the original shape """"""\n    orig_h, orig_w = original_shape\n    # The amount of padding that was added\n    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n    # Image height and width after padding is removed\n    unpad_h = current_dim - pad_y\n    unpad_w = current_dim - pad_x\n    # Rescale bounding boxes to dimension of original image\n    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n    return boxes\n\n\ndef xywh2xyxy(x):\n    y = x.new(x.shape)\n    y[..., 0] = x[..., 0] - x[..., 2] / 2\n    y[..., 1] = x[..., 1] - x[..., 3] / 2\n    y[..., 2] = x[..., 0] + x[..., 2] / 2\n    y[..., 3] = x[..., 1] + x[..., 3] / 2\n    return y\n\n\ndef ap_per_class(tp, conf, pred_cls, target_cls):\n    """""" Compute the average precision, given the recall and precision curves.\n    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n    # Arguments\n        tp:    True positives (list).\n        conf:  Objectness value from 0-1 (list).\n        pred_cls: Predicted object classes (list).\n        target_cls: True object classes (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n\n    # Sort by objectness\n    i = np.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n    # Find unique classes\n    unique_classes = np.unique(target_cls)\n\n    # Create Precision-Recall curve and compute AP for each class\n    ap, p, r = [], [], []\n    for c in tqdm.tqdm(unique_classes, desc=""Computing AP""):\n        i = pred_cls == c\n        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n        n_p = i.sum()  # Number of predicted objects\n\n        if n_p == 0 and n_gt == 0:\n            continue\n        elif n_p == 0 or n_gt == 0:\n            ap.append(0)\n            r.append(0)\n            p.append(0)\n        else:\n            # Accumulate FPs and TPs\n            fpc = (1 - tp[i]).cumsum()\n            tpc = (tp[i]).cumsum()\n\n            # Recall\n            recall_curve = tpc / (n_gt + 1e-16)\n            r.append(recall_curve[-1])\n\n            # Precision\n            precision_curve = tpc / (tpc + fpc)\n            p.append(precision_curve[-1])\n\n            # AP from recall-precision curve\n            ap.append(compute_ap(recall_curve, precision_curve))\n\n    # Compute F1 score (harmonic mean of precision and recall)\n    p, r, ap = np.array(p), np.array(r), np.array(ap)\n    f1 = 2 * p * r / (p + r + 1e-16)\n\n    return p, r, ap, f1, unique_classes.astype(""int32"")\n\n\ndef compute_ap(recall, precision):\n    """""" Compute the average precision, given the recall and precision curves.\n    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n\n    # Arguments\n        recall:    The recall curve (list).\n        precision: The precision curve (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef get_batch_statistics(outputs, targets, iou_threshold):\n    """""" Compute true positives, predicted scores and predicted labels per sample """"""\n    batch_metrics = []\n    for sample_i in range(len(outputs)):\n\n        if outputs[sample_i] is None:\n            continue\n\n        output = outputs[sample_i]\n        pred_boxes = output[:, :4]\n        pred_scores = output[:, 4]\n        pred_labels = output[:, -1]\n\n        true_positives = np.zeros(pred_boxes.shape[0])\n\n        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n        target_labels = annotations[:, 0] if len(annotations) else []\n        if len(annotations):\n            detected_boxes = []\n            target_boxes = annotations[:, 1:]\n\n            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n\n                # If targets are found break\n                if len(detected_boxes) == len(annotations):\n                    break\n\n                # Ignore if label is not one of the target labels\n                if pred_label not in target_labels:\n                    continue\n\n                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)\n                if iou >= iou_threshold and box_index not in detected_boxes:\n                    true_positives[pred_i] = 1\n                    detected_boxes += [box_index]\n        batch_metrics.append([true_positives, pred_scores, pred_labels])\n    return batch_metrics\n\n\ndef bbox_wh_iou(wh1, wh2):\n    wh2 = wh2.t()\n    w1, h1 = wh1[0], wh1[1]\n    w2, h2 = wh2[0], wh2[1]\n    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n    return inter_area / union_area\n\n\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    """"""\n    Returns the IoU of two bounding boxes\n    """"""\n    if not x1y1x2y2:\n        # Transform from center and width to exact coordinates\n        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n    else:\n        # Get the coordinates of bounding boxes\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n\n    # get the corrdinates of the intersection rectangle\n    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n    # Intersection area\n    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n        inter_rect_y2 - inter_rect_y1 + 1, min=0\n    )\n    # Union Area\n    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n\n    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n\n    return iou\n\n\ndef non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n    """"""\n    Removes detections with lower object confidence score than \'conf_thres\' and performs\n    Non-Maximum Suppression to further filter detections.\n    Returns detections with shape:\n        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n    """"""\n\n    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n    output = [None for _ in range(len(prediction))]\n    for image_i, image_pred in enumerate(prediction):\n        # Filter out confidence scores below threshold\n        image_pred = image_pred[image_pred[:, 4] >= conf_thres]\n        # If none are remaining => process next image\n        if not image_pred.size(0):\n            continue\n        # Object confidence times class confidence\n        score = image_pred[:, 4] * image_pred[:, 5:].max(1)[0]\n        # Sort by it\n        image_pred = image_pred[(-score).argsort()]\n        class_confs, class_preds = image_pred[:, 5:].max(1, keepdim=True)\n        detections = torch.cat((image_pred[:, :5], class_confs.float(), class_preds.float()), 1)\n        # Perform non-maximum suppression\n        keep_boxes = []\n        while detections.size(0):\n            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n            label_match = detections[0, -1] == detections[:, -1]\n            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n            invalid = large_overlap & label_match\n            weights = detections[invalid, 4:5]\n            # Merge overlapping bboxes by order of confidence\n            detections[0, :4] = (weights * detections[invalid, :4]).sum(0) / weights.sum()\n            keep_boxes += [detections[0]]\n            detections = detections[~invalid]\n        if keep_boxes:\n            output[image_i] = torch.stack(keep_boxes)\n\n    return output\n\n\ndef build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n\n    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n\n    nB = pred_boxes.size(0)\n    nA = pred_boxes.size(1)\n    nC = pred_cls.size(-1)\n    nG = pred_boxes.size(2)\n\n    # Output tensors\n    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n\n    # Convert to position relative to box\n    target_boxes = target[:, 2:6] * nG\n    gxy = target_boxes[:, :2]\n    gwh = target_boxes[:, 2:]\n    # Get anchors with best iou\n    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n    best_ious, best_n = ious.max(0)\n    # Separate target values\n    b, target_labels = target[:, :2].long().t()\n    gx, gy = gxy.t()\n    gw, gh = gwh.t()\n    gi, gj = gxy.long().t()\n    # Set masks\n    obj_mask[b, best_n, gj, gi] = 1\n    noobj_mask[b, best_n, gj, gi] = 0\n\n    # Set noobj mask to zero where iou exceeds ignore threshold\n    for i, anchor_ious in enumerate(ious.t()):\n        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n\n    # Coordinates\n    tx[b, best_n, gj, gi] = gx - gx.floor()\n    ty[b, best_n, gj, gi] = gy - gy.floor()\n    # Width and height\n    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n    # One-hot encoding of label\n    tcls[b, best_n, gj, gi, target_labels] = 1\n    # Compute label correctness and iou at best anchor\n    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n\n    tconf = obj_mask.float()\n    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf\n'"
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/model/PCB/model.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torchvision import models\nfrom torch.autograd import Variable\n\n\n######################################################################\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')  # For old pytorch, you may use kaiming_normal.\n    elif classname.find(\'Linear\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_out\')\n        init.constant_(m.bias.data, 0.0)\n    elif classname.find(\'BatchNorm1d\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Linear\') != -1:\n        init.normal_(m.weight.data, std=0.001)\n        init.constant_(m.bias.data, 0.0)\n\n\n# Defines the new fc layer and classification layer\n# |--Linear--|--bn--|--relu--|--Linear--|\nclass ClassBlock(nn.Module):\n    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True,\n                 return_f=False):\n        super(ClassBlock, self).__init__()\n        self.return_f = return_f\n        add_block = []\n        if linear:\n            add_block += [nn.Linear(input_dim, num_bottleneck)]\n        else:\n            num_bottleneck = input_dim\n        if bnorm:\n            add_block += [nn.BatchNorm1d(num_bottleneck)]\n        if relu:\n            add_block += [nn.LeakyReLU(0.1)]\n        if droprate > 0:\n            add_block += [nn.Dropout(p=droprate)]\n        add_block = nn.Sequential(*add_block)\n        add_block.apply(weights_init_kaiming)\n\n        classifier = []\n        classifier += [nn.Linear(num_bottleneck, class_num)]\n        classifier = nn.Sequential(*classifier)\n        classifier.apply(weights_init_classifier)\n\n        self.add_block = add_block\n        self.classifier = classifier\n\n    def forward(self, x):\n        x = self.add_block(x)\n        if self.return_f:\n            f = x\n            x = self.classifier(x)\n            return x, f\n        else:\n            x = self.classifier(x)\n            return x\n\n\n# Define the ResNet50-based Model\nclass ft_net(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5, stride=2):\n        super(ft_net, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        if stride == 1:\n            self.model.layer4[0].downsample[0].stride = (1, 1)\n            self.model.layer4[0].conv2.stride = (1, 1)\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.model.avgpool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the DenseNet121-based Model\nclass ft_net_dense(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5):\n        super().__init__()\n        model_ft = models.densenet121(pretrained=True)\n        model_ft.features.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        model_ft.fc = nn.Sequential()\n        self.model = model_ft\n        # For DenseNet, the feature dim is 1024 \n        self.classifier = ClassBlock(1024, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.features(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Define the ResNet50-based Model (Middle-Concat)\n# In the spirit of ""The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching."" Yu, Qian, et al. arXiv:1711.08106 (2017).\nclass ft_net_middle(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5):\n        super(ft_net_middle, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048 + 1024, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        # x0  n*1024*1*1\n        x0 = self.model.avgpool(x)\n        x = self.model.layer4(x)\n        # x1  n*2048*1*1\n        x1 = self.model.avgpool(x)\n        x = torch.cat((x0, x1), 1)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n\n# Part Model proposed in Yifan Sun etal. (2018)\nclass PCB(nn.Module):\n    def __init__(self, class_num):\n        super(PCB, self).__init__()\n\n        self.part = 6  # We cut the pool5 to 6 parts\n        model_ft = models.resnet50(pretrained=True)\n        self.model = model_ft\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part, 1))\n        self.dropout = nn.Dropout(p=0.5)\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1, 1)\n        self.model.layer4[0].conv2.stride = (1, 1)\n        # define 6 classifiers\n        for i in range(self.part):\n            name = \'classifier\' + str(i)\n            setattr(self, name, ClassBlock(2048, class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        part = {}\n        predict = {}\n        # get six part feature batchsize*2048*6\n        for i in range(self.part):\n            part[i] = torch.squeeze(x[:, :, i])\n            name = \'classifier\' + str(i)\n            c = getattr(self, name)\n            predict[i] = c(part[i])\n\n        # sum prediction\n        # y = predict[0]\n        # for i in range(self.part-1):\n        #    y += predict[i+1]\n        y = []\n        for i in range(self.part):\n            y.append(predict[i])\n        return y\n\n\nclass PCB_test(nn.Module):\n    def __init__(self, model):\n        super(PCB_test, self).__init__()\n        self.part = 6\n        self.model = model.model\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part, 1))\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1, 1)\n        self.model.layer4[0].conv2.stride = (1, 1)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        y = x.view(x.size(0), x.size(1), x.size(2))\n        return y\n\n\n\'\'\'\n# debug model structure\n# Run this code with:\npython model.py\n\'\'\'\nif __name__ == \'__main__\':\n    # Here I left a simple forward function.\n    # Test the model, before you train it.\n    net = ft_net(751)\n    print(net)\n    input = Variable(torch.FloatTensor(8, 3, 224, 224))\n    output = net(input)\n    print(\'net output size:\')\n    print(output.shape)\n'"
thirdparty/his_evaluators/his_evaluators/metrics/PCBreid/model/PCB/train.py,16,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nimport torch.backends.cudnn as cudnn\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\n#from PIL import Image\nimport time\nimport os\nfrom model import ft_net, ft_net_dense, PCB\nfrom random_erasing import RandomErasing\nimport yaml\nfrom shutil import copyfile\n\nversion =  torch.__version__\n#fp16\ntry:\n    from apex.fp16_utils import *\nexcept ImportError: # will be 3.x series\n    print(\'This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\')\n######################################################################\n# Options\n# --------\nparser = argparse.ArgumentParser(description=\'Training\')\nparser.add_argument(\'--gpu_ids\',default=\'0\', type=str,help=\'gpu_ids: e.g. 0  0,1,2  0,2\')\nparser.add_argument(\'--name\',default=\'ft_ResNet50\', type=str, help=\'output model name\')\nparser.add_argument(\'--data_dir\',default=\'../Market/pytorch\',type=str, help=\'training dir path\')\nparser.add_argument(\'--train_all\', action=\'store_true\', help=\'use all training data\' )\nparser.add_argument(\'--color_jitter\', action=\'store_true\', help=\'use color jitter in training\' )\nparser.add_argument(\'--batchsize\', default=32, type=int, help=\'batchsize\')\nparser.add_argument(\'--stride\', default=2, type=int, help=\'stride\')\nparser.add_argument(\'--erasing_p\', default=0, type=float, help=\'Random Erasing probability, in [0,1]\')\nparser.add_argument(\'--use_dense\', action=\'store_true\', help=\'use densenet121\' )\nparser.add_argument(\'--lr\', default=0.1, type=float, help=\'learning rate\')\nparser.add_argument(\'--droprate\', default=0.5, type=float, help=\'drop rate\')\nparser.add_argument(\'--PCB\', action=\'store_true\', help=\'use PCB+ResNet50\' )\nparser.add_argument(\'--fp16\', action=\'store_true\', help=\'use float16 instead of float32, which will save about 50% memory\' )\nopt = parser.parse_args()\n\nfp16 = opt.fp16\ndata_dir = opt.data_dir\nname = opt.name\nstr_ids = opt.gpu_ids.split(\',\')\ngpu_ids = []\nfor str_id in str_ids:\n    gid = int(str_id)\n    if gid >=0:\n        gpu_ids.append(gid)\n\n# set gpu ids\nif len(gpu_ids)>0:\n    torch.cuda.set_device(gpu_ids[0])\n    cudnn.benchmark = True\n######################################################################\n# Load Data\n# ---------\n#\n\ntransform_train_list = [\n        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n        transforms.Resize((256,128), interpolation=3),\n        transforms.Pad(10),\n        transforms.RandomCrop((256,128)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\ntransform_val_list = [\n        transforms.Resize(size=(256,128),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.PCB:\n    transform_train_list = [\n        transforms.Resize((384,192), interpolation=3),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n    transform_val_list = [\n        transforms.Resize(size=(384,192),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.erasing_p>0:\n    transform_train_list = transform_train_list +  [RandomErasing(probability = opt.erasing_p, mean=[0.0, 0.0, 0.0])]\n\nif opt.color_jitter:\n    transform_train_list = [transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0)] + transform_train_list\n\nprint(transform_train_list)\ndata_transforms = {\n    \'train\': transforms.Compose( transform_train_list ),\n    \'val\': transforms.Compose(transform_val_list),\n}\n\n\ntrain_all = \'\'\nif opt.train_all:\n     train_all = \'_all\'\n\nimage_datasets = {}\nimage_datasets[\'train\'] = datasets.ImageFolder(os.path.join(data_dir, \'train\' + train_all),\n                                          data_transforms[\'train\'])\nimage_datasets[\'val\'] = datasets.ImageFolder(os.path.join(data_dir, \'val\'),\n                                          data_transforms[\'val\'])\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=True, num_workers=8, pin_memory=True) # 8 workers may work faster\n              for x in [\'train\', \'val\']}\ndataset_sizes = {x: len(image_datasets[x]) for x in [\'train\', \'val\']}\nclass_names = image_datasets[\'train\'].classes\n\nuse_gpu = torch.cuda.is_available()\n\nsince = time.time()\ninputs, classes = next(iter(dataloaders[\'train\']))\nprint(time.time()-since)\n######################################################################\n# Training the model\n# ------------------\n#\n# Now, let\'s write a general function to train a model. Here, we will\n# illustrate:\n#\n# -  Scheduling the learning rate\n# -  Saving the best model\n#\n# In the following, parameter ``scheduler`` is an LR scheduler object from\n# ``torch.optim.lr_scheduler``.\n\ny_loss = {} # loss history\ny_loss[\'train\'] = []\ny_loss[\'val\'] = []\ny_err = {}\ny_err[\'train\'] = []\ny_err[\'val\'] = []\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    #best_model_wts = model.state_dict()\n    #best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(\'Epoch {}/{}\'.format(epoch, num_epochs - 1))\n        print(\'-\' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in [\'train\', \'val\']:\n            if phase == \'train\':\n                scheduler.step()\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0.0\n            # Iterate over data.\n            for data in dataloaders[phase]:\n                # get the inputs\n                inputs, labels = data\n                now_batch_size,c,h,w = inputs.shape\n                if now_batch_size<opt.batchsize: # skip the last batch\n                    continue\n                #print(inputs.shape)\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = Variable(inputs.cuda().detach())\n                    labels = Variable(labels.cuda().detach())\n                else:\n                    inputs, labels = Variable(inputs), Variable(labels)\n                # if we use low precision, input also need to be fp16\n                if fp16:\n                    inputs = inputs.half()\n \n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                if phase == \'val\':\n                    with torch.no_grad():\n                        outputs = model(inputs)\n                else:\n                    outputs = model(inputs)\n\n                if not opt.PCB:\n                    _, preds = torch.max(outputs.data, 1)\n                    loss = criterion(outputs, labels)\n                else:\n                    part = {}\n                    sm = nn.Softmax(dim=1)\n                    num_part = 6\n                    for i in range(num_part):\n                        part[i] = outputs[i]\n\n                    score = sm(part[0]) + sm(part[1]) +sm(part[2]) + sm(part[3]) +sm(part[4]) +sm(part[5])\n                    _, preds = torch.max(score.data, 1)\n\n                    loss = criterion(part[0], labels)\n                    for i in range(num_part-1):\n                        loss += criterion(part[i+1], labels)\n\n                # backward + optimize only if in training phase\n                if phase == \'train\':\n                    if fp16: # we use optimier to backward loss\n                        optimizer.backward(loss)\n                    else:\n                        loss.backward()\n                    optimizer.step()\n\n                # statistics\n                if int(version[0])>0 or int(version[2]) > 3: # for the new version like 0.4.0, 0.5.0 and 1.0.0\n                    running_loss += loss.item() * now_batch_size\n                else :  # for the old version like 0.3.0 and 0.3.1\n                    running_loss += loss.data[0] * now_batch_size\n                running_corrects += float(torch.sum(preds == labels.data))\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n            \n            print(\'{} Loss: {:.4f} Acc: {:.4f}\'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            y_loss[phase].append(epoch_loss)\n            y_err[phase].append(1.0-epoch_acc)            \n            # deep copy the model\n            if phase == \'val\':\n                last_model_wts = model.state_dict()\n                if epoch%10 == 9:\n                    save_network(model, epoch)\n                draw_curve(epoch)\n\n        time_elapsed = time.time() - since\n        print(\'Training complete in {:.0f}m {:.0f}s\'.format(\n            time_elapsed // 60, time_elapsed % 60))\n        print()\n\n    time_elapsed = time.time() - since\n    print(\'Training complete in {:.0f}m {:.0f}s\'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    #print(\'Best val Acc: {:4f}\'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(last_model_wts)\n    save_network(model, \'last\')\n    return model\n\n\n######################################################################\n# Draw Curve\n#---------------------------\nx_epoch = []\nfig = plt.figure()\nax0 = fig.add_subplot(121, title=""loss"")\nax1 = fig.add_subplot(122, title=""top1err"")\ndef draw_curve(current_epoch):\n    x_epoch.append(current_epoch)\n    ax0.plot(x_epoch, y_loss[\'train\'], \'bo-\', label=\'train\')\n    ax0.plot(x_epoch, y_loss[\'val\'], \'ro-\', label=\'val\')\n    ax1.plot(x_epoch, y_err[\'train\'], \'bo-\', label=\'train\')\n    ax1.plot(x_epoch, y_err[\'val\'], \'ro-\', label=\'val\')\n    if current_epoch == 0:\n        ax0.legend()\n        ax1.legend()\n    fig.savefig( os.path.join(\'./model\',name,\'train.jpg\'))\n\n######################################################################\n# Save model\n#---------------------------\ndef save_network(network, epoch_label):\n    save_filename = \'net_%s.pth\'% epoch_label\n    save_path = os.path.join(\'./model\',name,save_filename)\n    torch.save(network.cpu().state_dict(), save_path)\n    if torch.cuda.is_available():\n        network.cuda(gpu_ids[0])\n\n\n######################################################################\n# Finetuning the convnet\n# ----------------------\n#\n# Load a pretrainied model and reset final fully connected layer.\n#\n\nif opt.use_dense:\n    model = ft_net_dense(len(class_names), opt.droprate)\nelse:\n    model = ft_net(len(class_names), opt.droprate, opt.stride)\n\nif opt.PCB:\n    model = PCB(len(class_names))\n\nprint(model)\n\nif not opt.PCB:\n    ignored_params = list(map(id, model.model.fc.parameters() )) + list(map(id, model.classifier.parameters() ))\n    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n    optimizer_ft = optim.SGD([\n             {\'params\': base_params, \'lr\': 0.1*opt.lr},\n             {\'params\': model.model.fc.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier.parameters(), \'lr\': opt.lr}\n         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\nelse:\n    ignored_params = list(map(id, model.model.fc.parameters() ))\n    ignored_params += (list(map(id, model.classifier0.parameters() )) \n                     +list(map(id, model.classifier1.parameters() ))\n                     +list(map(id, model.classifier2.parameters() ))\n                     +list(map(id, model.classifier3.parameters() ))\n                     +list(map(id, model.classifier4.parameters() ))\n                     +list(map(id, model.classifier5.parameters() ))\n                     #+list(map(id, model.classifier6.parameters() ))\n                     #+list(map(id, model.classifier7.parameters() ))\n                      )\n    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n    optimizer_ft = optim.SGD([\n             {\'params\': base_params, \'lr\': 0.1*opt.lr},\n             {\'params\': model.model.fc.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier0.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier1.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier2.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier3.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier4.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier5.parameters(), \'lr\': opt.lr},\n             #{\'params\': model.classifier6.parameters(), \'lr\': 0.01},\n             #{\'params\': model.classifier7.parameters(), \'lr\': 0.01}\n         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\n\n# Decay LR by a factor of 0.1 every 40 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=40, gamma=0.1)\n\n######################################################################\n# Train and evaluate\n# ^^^^^^^^^^^^^^^^^^\n#\n# It should take around 1-2 hours on GPU. \n#\ndir_name = os.path.join(\'./model\',name)\nif not os.path.isdir(dir_name):\n    os.mkdir(dir_name)\n#record every run\ncopyfile(\'./train.py\', dir_name+\'/train.py\')\ncopyfile(\'./model.py\', dir_name+\'/model.py\')\n\n# save opts\nwith open(\'%s/opts.yaml\'%dir_name,\'w\') as fp:\n    yaml.dump(vars(opt), fp, default_flow_style=False)\n\n# model to gpu\nmodel = model.cuda()\nif fp16:\n    model = network_to_half(model)\n    optimizer_ft = FP16_Optimizer(optimizer_ft, static_loss_scale = 128.0)\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=60)\n\n'"
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/models/utils/__init__.py,0,b''
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/models/utils/detect_face.py,21,"b'import torch\nfrom torch.nn.functional import interpolate\nfrom torchvision.transforms import functional as F\nfrom torchvision.ops.boxes import batched_nms\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport os\n\n\ndef detect_face(imgs, minsize, pnet, rnet, onet, threshold, factor, device):\n    if isinstance(imgs, (np.ndarray, torch.Tensor)):\n        imgs = torch.as_tensor(imgs, device=device)\n        if len(imgs.shape) == 3:\n            imgs = imgs.unsqueeze(0)\n    else:\n        if not isinstance(imgs, (list, tuple)):\n            imgs = [imgs]\n        if any(img.size != imgs[0].size for img in imgs):\n            raise Exception(""MTCNN batch processing only compatible with equal-dimension images."")\n        imgs = np.stack([np.uint8(img) for img in imgs])\n\n    imgs = torch.as_tensor(imgs, device=device)\n\n    imgs = imgs.permute(0, 3, 1, 2).float()\n\n    batch_size = len(imgs)\n    h, w = imgs.shape[2:4]\n    m = 12.0 / minsize\n    minl = min(h, w)\n    minl = minl * m\n\n    # Create scale pyramid\n    scale_i = m\n    scales = []\n    while minl >= 12:\n        scales.append(scale_i)\n        scale_i = scale_i * factor\n        minl = minl * factor\n\n    # First stage\n    boxes = []\n    image_inds = []\n    all_inds = []\n    all_i = 0\n    for scale in scales:\n        im_data = imresample(imgs, (int(h * scale + 1), int(w * scale + 1)))\n        im_data = (im_data - 127.5) * 0.0078125\n        reg, probs = pnet(im_data)\n    \n        boxes_scale, image_inds_scale = generateBoundingBox(reg, probs[:, 1], scale, threshold[0])\n        boxes.append(boxes_scale)\n        image_inds.append(image_inds_scale)\n        all_inds.append(all_i + image_inds_scale)\n        all_i += batch_size\n\n    boxes = torch.cat(boxes, dim=0)\n    image_inds = torch.cat(image_inds, dim=0).cpu()\n    all_inds = torch.cat(all_inds, dim=0)\n\n    # NMS within each scale + image\n    pick = batched_nms(boxes[:, :4], boxes[:, 4], all_inds, 0.5)\n    boxes, image_inds = boxes[pick], image_inds[pick]\n    \n    # NMS within each image\n    pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n    boxes, image_inds = boxes[pick], image_inds[pick]\n\n    regw = boxes[:, 2] - boxes[:, 0]\n    regh = boxes[:, 3] - boxes[:, 1]\n    qq1 = boxes[:, 0] + boxes[:, 5] * regw\n    qq2 = boxes[:, 1] + boxes[:, 6] * regh\n    qq3 = boxes[:, 2] + boxes[:, 7] * regw\n    qq4 = boxes[:, 3] + boxes[:, 8] * regh\n    boxes = torch.stack([qq1, qq2, qq3, qq4, boxes[:, 4]]).permute(1, 0)\n    boxes = rerec(boxes)\n    y, ey, x, ex = pad(boxes, w, h)\n    \n    # Second stage\n    if len(boxes) > 0:\n        im_data = []\n        for k in range(len(y)):\n            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\n                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\n                im_data.append(imresample(img_k, (24, 24)))\n        im_data = torch.cat(im_data, dim=0)\n        im_data = (im_data - 127.5) * 0.0078125\n        out = rnet(im_data)\n\n        out0 = out[0].permute(1, 0)\n        out1 = out[1].permute(1, 0)\n        score = out1[1, :]\n        ipass = score > threshold[1]\n        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\n        image_inds = image_inds[ipass]\n        mv = out0[:, ipass].permute(1, 0)\n\n        # NMS within each image\n        pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n        boxes, image_inds, mv = boxes[pick], image_inds[pick], mv[pick]\n        boxes = bbreg(boxes, mv)\n        boxes = rerec(boxes)\n\n    # Third stage\n    points = torch.zeros(0, 5, 2, device=device)\n    if len(boxes) > 0:\n        y, ey, x, ex = pad(boxes, w, h)\n        im_data = []\n        for k in range(len(y)):\n            if ey[k] > (y[k] - 1) and ex[k] > (x[k] - 1):\n                img_k = imgs[image_inds[k], :, (y[k] - 1):ey[k], (x[k] - 1):ex[k]].unsqueeze(0)\n                im_data.append(imresample(img_k, (48, 48)))\n        im_data = torch.cat(im_data, dim=0)\n        im_data = (im_data - 127.5) * 0.0078125\n        out = onet(im_data)\n\n        out0 = out[0].permute(1, 0)\n        out1 = out[1].permute(1, 0)\n        out2 = out[2].permute(1, 0)\n        score = out2[1, :]\n        points = out1\n        ipass = score > threshold[2]\n        points = points[:, ipass]\n        boxes = torch.cat((boxes[ipass, :4], score[ipass].unsqueeze(1)), dim=1)\n        image_inds = image_inds[ipass]\n        mv = out0[:, ipass].permute(1, 0)\n\n        w_i = boxes[:, 2] - boxes[:, 0] + 1\n        h_i = boxes[:, 3] - boxes[:, 1] + 1\n        points_x = w_i.repeat(5, 1) * points[:5, :] + boxes[:, 0].repeat(5, 1) - 1\n        points_y = h_i.repeat(5, 1) * points[5:10, :] + boxes[:, 1].repeat(5, 1) - 1\n        points = torch.stack((points_x, points_y)).permute(2, 1, 0)\n        boxes = bbreg(boxes, mv)\n\n        # NMS within each image using ""Min"" strategy\n        # pick = batched_nms(boxes[:, :4], boxes[:, 4], image_inds, 0.7)\n        pick = batched_nms_numpy(boxes[:, :4], boxes[:, 4], image_inds, 0.7, \'Min\')\n        boxes, image_inds, points = boxes[pick], image_inds[pick], points[pick]\n\n    boxes = boxes.cpu().numpy()\n    points = points.cpu().numpy()\n\n    batch_boxes = []\n    batch_points = []\n    for b_i in range(batch_size):\n        b_i_inds = np.where(image_inds == b_i)\n        batch_boxes.append(boxes[b_i_inds].copy())\n        batch_points.append(points[b_i_inds].copy())\n\n    batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n\n    return batch_boxes, batch_points\n\n\ndef bbreg(boundingbox, reg):\n    if reg.shape[1] == 1:\n        reg = torch.reshape(reg, (reg.shape[2], reg.shape[3]))\n\n    w = boundingbox[:, 2] - boundingbox[:, 0] + 1\n    h = boundingbox[:, 3] - boundingbox[:, 1] + 1\n    b1 = boundingbox[:, 0] + reg[:, 0] * w\n    b2 = boundingbox[:, 1] + reg[:, 1] * h\n    b3 = boundingbox[:, 2] + reg[:, 2] * w\n    b4 = boundingbox[:, 3] + reg[:, 3] * h\n    boundingbox[:, :4] = torch.stack([b1, b2, b3, b4]).permute(1, 0)\n\n    return boundingbox\n\n\ndef generateBoundingBox(reg, probs, scale, thresh):\n    stride = 2\n    cellsize = 12\n\n    reg = reg.permute(1, 0, 2, 3)\n\n    mask = probs >= thresh\n    mask_inds = mask.nonzero()\n    image_inds = mask_inds[:, 0]\n    score = probs[mask]\n    reg = reg[:, mask].permute(1, 0)\n    bb = mask_inds[:, 1:].float().flip(1)\n    q1 = ((stride * bb + 1) / scale).floor()\n    q2 = ((stride * bb + cellsize - 1 + 1) / scale).floor()\n    boundingbox = torch.cat([q1, q2, score.unsqueeze(1), reg], dim=1)\n    return boundingbox, image_inds\n\n\ndef nms_numpy(boxes, scores, threshold, method):\n    if boxes.size == 0:\n        return np.empty((0, 3))\n\n    x1 = boxes[:, 0].copy()\n    y1 = boxes[:, 1].copy()\n    x2 = boxes[:, 2].copy()\n    y2 = boxes[:, 3].copy()\n    s = scores\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n    I = np.argsort(s)\n    pick = np.zeros_like(s, dtype=np.int16)\n    counter = 0\n    while I.size > 0:\n        i = I[-1]\n        pick[counter] = i\n        counter += 1\n        idx = I[0:-1]\n\n        xx1 = np.maximum(x1[i], x1[idx]).copy()\n        yy1 = np.maximum(y1[i], y1[idx]).copy()\n        xx2 = np.minimum(x2[i], x2[idx]).copy()\n        yy2 = np.minimum(y2[i], y2[idx]).copy()\n\n        w = np.maximum(0.0, xx2 - xx1 + 1).copy()\n        h = np.maximum(0.0, yy2 - yy1 + 1).copy()\n\n        inter = w * h\n        if method is ""Min"":\n            o = inter / np.minimum(area[i], area[idx])\n        else:\n            o = inter / (area[i] + area[idx] - inter)\n        I = I[np.where(o <= threshold)]\n\n    pick = pick[:counter].copy()\n    return pick\n\n\ndef batched_nms_numpy(boxes, scores, idxs, threshold, method):\n    device = boxes.device\n    if boxes.numel() == 0:\n        return torch.empty((0,), dtype=torch.int64, device=device)\n    # strategy: in order to perform NMS independently per class.\n    # we add an offset to all the boxes. The offset is dependent\n    # only on the class idx, and is large enough so that boxes\n    # from different classes do not overlap\n    max_coordinate = boxes.max()\n    offsets = idxs.to(boxes) * (max_coordinate + 1)\n    boxes_for_nms = boxes + offsets[:, None]\n    boxes_for_nms = boxes_for_nms.cpu().numpy()\n    scores = scores.cpu().numpy()\n    keep = nms_numpy(boxes_for_nms, scores, threshold, method)\n    return torch.as_tensor(keep, dtype=torch.long, device=device)\n\n\ndef pad(boxes, w, h):\n    boxes = boxes.trunc().int().cpu().numpy()\n    x = boxes[:, 0]\n    y = boxes[:, 1]\n    ex = boxes[:, 2]\n    ey = boxes[:, 3]\n\n    x[x < 1] = 1\n    y[y < 1] = 1\n    ex[ex > w] = w\n    ey[ey > h] = h\n\n    return y, ey, x, ex\n\n\ndef rerec(bboxA):\n    h = bboxA[:, 3] - bboxA[:, 1]\n    w = bboxA[:, 2] - bboxA[:, 0]\n    \n    l = torch.max(w, h)\n    bboxA[:, 0] = bboxA[:, 0] + w * 0.5 - l * 0.5\n    bboxA[:, 1] = bboxA[:, 1] + h * 0.5 - l * 0.5\n    bboxA[:, 2:4] = bboxA[:, :2] + l.repeat(2, 1).permute(1, 0)\n\n    return bboxA\n\n\ndef imresample(img, sz):\n    im_data = interpolate(img, size=sz, mode=""area"")\n    return im_data\n\n\ndef crop_resize(img, box, image_size):\n    if isinstance(img, np.ndarray):\n        out = cv2.resize(\n            img[box[1]:box[3], box[0]:box[2]],\n            (image_size, image_size),\n            interpolation=cv2.INTER_AREA\n        ).copy()\n    else:\n        out = img.crop(box).copy().resize((image_size, image_size), Image.BILINEAR)\n    return out\n\n\ndef save_img(img, path):\n    if isinstance(img, np.ndarray):\n        cv2.imwrite(path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n    else:\n        img.save(path)\n\n\ndef get_size(img):\n    if isinstance(img, np.ndarray):\n        return img.shape[1::-1]\n    else:\n        return img.size\n\n\ndef extract_face(img, box, image_size=160, margin=0, save_path=None):\n    """"""Extract face + margin from PIL Image given bounding box.\n    \n    Arguments:\n        img {PIL.Image} -- A PIL Image.\n        box {numpy.ndarray} -- Four-element bounding box.\n        image_size {int} -- Output image size in pixels. The image will be square.\n        margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \n            Note that the application of the margin differs slightly from the davidsandberg/facenet\n            repo, which applies the margin to the original image before resizing, making the margin\n            dependent on the original image size.\n        save_path {str} -- Save path for extracted face image. (default: {None})\n    \n    Returns:\n        torch.tensor -- tensor representing the extracted face.\n    """"""\n    margin = [\n        margin * (box[2] - box[0]) / (image_size - margin),\n        margin * (box[3] - box[1]) / (image_size - margin),\n    ]\n    raw_image_size = get_size(img)\n    box = [\n        int(max(box[0] - margin[0] / 2, 0)),\n        int(max(box[1] - margin[1] / 2, 0)),\n        int(min(box[2] + margin[0] / 2, raw_image_size[0])),\n        int(min(box[3] + margin[1] / 2, raw_image_size[1])),\n    ]\n\n    face = crop_resize(img, box, image_size)\n\n    if save_path is not None:\n        os.makedirs(os.path.dirname(save_path) + ""/"", exist_ok=True)\n        save_img(face, save_path)\n\n    face = F.to_tensor(np.float32(face))\n\n    return face\n'"
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/models/utils/tensorflow2pytorch.py,34,"b'import tensorflow as tf\nimport torch\nimport json\nimport os, sys\n\nfrom dependencies.facenet.src import facenet\nfrom dependencies.facenet.src.models import inception_resnet_v1 as tf_mdl\nfrom dependencies.facenet.src.align import detect_face\n\nfrom models.inception_resnet_v1 import InceptionResnetV1\nfrom models.mtcnn import PNet, RNet, ONet\n\n\ndef import_tf_params(tf_mdl_dir, sess):\n    """"""Import tensorflow model from save directory.\n    \n    Arguments:\n        tf_mdl_dir {str} -- Location of protobuf, checkpoint, meta files.\n        sess {tensorflow.Session} -- Tensorflow session object.\n    \n    Returns:\n        (list, list, list) -- Tuple of lists containing the layer names,\n            parameter arrays as numpy ndarrays, parameter shapes.\n    """"""\n    print(\'\\nLoading tensorflow model\\n\')\n    if callable(tf_mdl_dir):\n        tf_mdl_dir(sess)\n    else:\n        facenet.load_model(tf_mdl_dir)\n\n    print(\'\\nGetting model weights\\n\')\n    tf_layers = tf.trainable_variables()\n    tf_params = sess.run(tf_layers)\n\n    tf_shapes = [p.shape for p in tf_params]\n    tf_layers = [l.name for l in tf_layers]\n\n    if not callable(tf_mdl_dir):\n        path = os.path.join(tf_mdl_dir, \'layer_description.json\')\n    else:\n        path = \'data/layer_description.json\'\n    with open(path, \'w\') as f:\n        json.dump({l: s for l, s in zip(tf_layers, tf_shapes)}, f)\n\n    return tf_layers, tf_params, tf_shapes\n\n\ndef get_layer_indices(layer_lookup, tf_layers):\n    """"""Giving a lookup of model layer attribute names and tensorflow variable names,\n    find matching parameters.\n    \n    Arguments:\n        layer_lookup {dict} -- Dictionary mapping pytorch attribute names to (partial)\n            tensorflow variable names. Expects dict of the form {\'attr\': [\'tf_name\', ...]}\n            where the \'...\'s are ignored.\n        tf_layers {list} -- List of tensorflow variable names.\n    \n    Returns:\n        list -- The input dictionary with the list of matching inds appended to each item.\n    """"""\n    layer_inds = {}\n    for name, value in layer_lookup.items():\n        layer_inds[name] = value + [[i for i, n in enumerate(tf_layers) if value[0] in n]]\n    return layer_inds\n\n\ndef load_tf_batchNorm(weights, layer):\n    """"""Load tensorflow weights into nn.BatchNorm object.\n    \n    Arguments:\n        weights {list} -- Tensorflow parameters.\n        layer {torch.nn.Module} -- nn.BatchNorm.\n    """"""\n    layer.bias.data = torch.tensor(weights[0]).view(layer.bias.data.shape)\n    layer.weight.data = torch.ones_like(layer.weight.data)\n    layer.running_mean = torch.tensor(weights[1]).view(layer.running_mean.shape)\n    layer.running_var = torch.tensor(weights[2]).view(layer.running_var.shape)\n\n\ndef load_tf_conv2d(weights, layer, transpose=False):\n    """"""Load tensorflow weights into nn.Conv2d object.\n    \n    Arguments:\n        weights {list} -- Tensorflow parameters.\n        layer {torch.nn.Module} -- nn.Conv2d.\n    """"""\n    if isinstance(weights, list):\n        if len(weights) == 2:\n            layer.bias.data = (\n                torch.tensor(weights[1])\n                    .view(layer.bias.data.shape)\n            )\n        weights = weights[0]\n    \n    if transpose:\n        dim_order = (3, 2, 1, 0)\n    else:\n        dim_order = (3, 2, 0, 1)\n\n    layer.weight.data = (\n        torch.tensor(weights)\n            .permute(dim_order)\n            .view(layer.weight.data.shape)\n    )\n\n\ndef load_tf_conv2d_trans(weights, layer):\n    return load_tf_conv2d(weights, layer, transpose=True)\n\n\ndef load_tf_basicConv2d(weights, layer):\n    """"""Load tensorflow weights into grouped Conv2d+BatchNorm object.\n    \n    Arguments:\n        weights {list} -- Tensorflow parameters.\n        layer {torch.nn.Module} -- Object containing Conv2d+BatchNorm.\n    """"""\n    load_tf_conv2d(weights[0], layer.conv)\n    load_tf_batchNorm(weights[1:], layer.bn)\n\n\ndef load_tf_linear(weights, layer):\n    """"""Load tensorflow weights into nn.Linear object.\n    \n    Arguments:\n        weights {list} -- Tensorflow parameters.\n        layer {torch.nn.Module} -- nn.Linear.\n    """"""\n    if isinstance(weights, list):\n        if len(weights) == 2:\n            layer.bias.data = (\n                torch.tensor(weights[1])\n                    .view(layer.bias.data.shape)\n            )\n        weights = weights[0]\n    layer.weight.data = (\n        torch.tensor(weights)\n            .transpose(-1, 0)\n            .view(layer.weight.data.shape)\n    )\n\n\n# High-level parameter-loading functions:\n\ndef load_tf_block35(weights, layer):\n    load_tf_basicConv2d(weights[:4], layer.branch0)\n    load_tf_basicConv2d(weights[4:8], layer.branch1[0])\n    load_tf_basicConv2d(weights[8:12], layer.branch1[1])\n    load_tf_basicConv2d(weights[12:16], layer.branch2[0])\n    load_tf_basicConv2d(weights[16:20], layer.branch2[1])\n    load_tf_basicConv2d(weights[20:24], layer.branch2[2])\n    load_tf_conv2d(weights[24:26], layer.conv2d)\n\n\ndef load_tf_block17_8(weights, layer):\n    load_tf_basicConv2d(weights[:4], layer.branch0)\n    load_tf_basicConv2d(weights[4:8], layer.branch1[0])\n    load_tf_basicConv2d(weights[8:12], layer.branch1[1])\n    load_tf_basicConv2d(weights[12:16], layer.branch1[2])\n    load_tf_conv2d(weights[16:18], layer.conv2d)\n\n\ndef load_tf_mixed6a(weights, layer):\n    if len(weights) != 16:\n        raise ValueError(f\'Number of weight arrays ({len(weights)}) not equal to 16\')\n    load_tf_basicConv2d(weights[:4], layer.branch0)\n    load_tf_basicConv2d(weights[4:8], layer.branch1[0])\n    load_tf_basicConv2d(weights[8:12], layer.branch1[1])\n    load_tf_basicConv2d(weights[12:16], layer.branch1[2])\n\n\ndef load_tf_mixed7a(weights, layer):\n    if len(weights) != 28:\n        raise ValueError(f\'Number of weight arrays ({len(weights)}) not equal to 28\')\n    load_tf_basicConv2d(weights[:4], layer.branch0[0])\n    load_tf_basicConv2d(weights[4:8], layer.branch0[1])\n    load_tf_basicConv2d(weights[8:12], layer.branch1[0])\n    load_tf_basicConv2d(weights[12:16], layer.branch1[1])\n    load_tf_basicConv2d(weights[16:20], layer.branch2[0])\n    load_tf_basicConv2d(weights[20:24], layer.branch2[1])\n    load_tf_basicConv2d(weights[24:28], layer.branch2[2])\n\n\ndef load_tf_repeats(weights, layer, rptlen, subfun):\n    if len(weights) % rptlen != 0:\n        raise ValueError(f\'Number of weight arrays ({len(weights)}) not divisible by {rptlen}\')\n    weights_split = [weights[i:i+rptlen] for i in range(0, len(weights), rptlen)]\n    for i, w in enumerate(weights_split):\n        subfun(w, getattr(layer, str(i)))\n\n\ndef load_tf_repeat_1(weights, layer):\n    load_tf_repeats(weights, layer, 26, load_tf_block35)\n\n\ndef load_tf_repeat_2(weights, layer):\n    load_tf_repeats(weights, layer, 18, load_tf_block17_8)\n\n\ndef load_tf_repeat_3(weights, layer):\n    load_tf_repeats(weights, layer, 18, load_tf_block17_8)\n\n\ndef test_loaded_params(mdl, tf_params, tf_layers):\n    """"""Check each parameter in a pytorch model for an equivalent parameter\n    in a list of tensorflow variables.\n    \n    Arguments:\n        mdl {torch.nn.Module} -- Pytorch model.\n        tf_params {list} -- List of ndarrays representing tensorflow variables.\n        tf_layers {list} -- Corresponding list of tensorflow variable names.\n    """"""\n    tf_means = torch.stack([torch.tensor(p).mean() for p in tf_params])\n    for name, param in mdl.named_parameters():\n        pt_mean = param.data.mean()\n        matching_inds = ((tf_means - pt_mean).abs() < 1e-8).nonzero()\n        print(f\'{name} equivalent to {[tf_layers[i] for i in matching_inds]}\')\n\n\ndef compare_model_outputs(pt_mdl, sess, test_data):\n    """"""Given some testing data, compare the output of pytorch and tensorflow models.\n    \n    Arguments:\n        pt_mdl {torch.nn.Module} -- Pytorch model.\n        sess {tensorflow.Session} -- Tensorflow session object.\n        test_data {torch.Tensor} -- Pytorch tensor.\n    """"""\n    print(\'\\nPassing test data through TF model\\n\')\n    if isinstance(sess, tf.Session):\n        images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")\n        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(""phase_train:0"")\n        embeddings = tf.get_default_graph().get_tensor_by_name(""embeddings:0"")\n        feed_dict = {images_placeholder: test_data.numpy(), phase_train_placeholder: False}\n        tf_output = torch.tensor(sess.run(embeddings, feed_dict=feed_dict))\n    else:\n        tf_output = sess(test_data)\n\n    print(tf_output)\n\n    print(\'\\nPassing test data through PT model\\n\')\n    pt_output = pt_mdl(test_data.permute(0, 3, 1, 2))\n    print(pt_output)\n\n    distance = (tf_output - pt_output).norm()\n    print(f\'\\nDistance {distance}\\n\')\n\n\ndef compare_mtcnn(pt_mdl, tf_fun, sess, ind, test_data):\n    tf_mdls = tf_fun(sess)\n    tf_mdl = tf_mdls[ind]\n\n    print(\'\\nPassing test data through TF model\\n\')\n    tf_output = tf_mdl(test_data.numpy())\n    tf_output = [torch.tensor(out) for out in tf_output]\n    print(\'\\n\'.join([str(o.view(-1)[:10]) for o in tf_output]))\n\n    print(\'\\nPassing test data through PT model\\n\')\n    with torch.no_grad():\n        pt_output = pt_mdl(test_data.permute(0, 3, 2, 1))\n    pt_output = [torch.tensor(out) for out in pt_output]\n    for i in range(len(pt_output)):\n        if len(pt_output[i].shape) == 4:\n            pt_output[i] = pt_output[i].permute(0, 3, 2, 1).contiguous()\n    print(\'\\n\'.join([str(o.view(-1)[:10]) for o in pt_output]))\n\n    distance = [(tf_o - pt_o).norm() for tf_o, pt_o in zip(tf_output, pt_output)]\n    print(f\'\\nDistance {distance}\\n\')\n\n\ndef load_tf_model_weights(mdl, layer_lookup, tf_mdl_dir, is_resnet=True, arg_num=None):\n    """"""Load tensorflow parameters into a pytorch model.\n    \n    Arguments:\n        mdl {torch.nn.Module} -- Pytorch model.\n        layer_lookup {[type]} -- Dictionary mapping pytorch attribute names to (partial)\n            tensorflow variable names, and a function suitable for loading weights.\n            Expects dict of the form {\'attr\': [\'tf_name\', function]}. \n        tf_mdl_dir {str} -- Location of protobuf, checkpoint, meta files.\n    """"""\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        tf_layers, tf_params, tf_shapes = import_tf_params(tf_mdl_dir, sess)\n        layer_info = get_layer_indices(layer_lookup, tf_layers)\n\n        for layer_name, info in layer_info.items():\n            print(f\'Loading {info[0]}/* into {layer_name}\')\n            weights = [tf_params[i] for i in info[2]]\n            layer = getattr(mdl, layer_name)\n            info[1](weights, layer)\n\n        test_loaded_params(mdl, tf_params, tf_layers)\n\n        if is_resnet:\n            compare_model_outputs(mdl, sess, torch.randn(5, 160, 160, 3).detach())\n\n\ndef tensorflow2pytorch():\n    lookup_inception_resnet_v1 = {\n        \'conv2d_1a\': [\'InceptionResnetV1/Conv2d_1a_3x3\', load_tf_basicConv2d],\n        \'conv2d_2a\': [\'InceptionResnetV1/Conv2d_2a_3x3\', load_tf_basicConv2d],\n        \'conv2d_2b\': [\'InceptionResnetV1/Conv2d_2b_3x3\', load_tf_basicConv2d],\n        \'conv2d_3b\': [\'InceptionResnetV1/Conv2d_3b_1x1\', load_tf_basicConv2d],\n        \'conv2d_4a\': [\'InceptionResnetV1/Conv2d_4a_3x3\', load_tf_basicConv2d],\n        \'conv2d_4b\': [\'InceptionResnetV1/Conv2d_4b_3x3\', load_tf_basicConv2d],\n        \'repeat_1\': [\'InceptionResnetV1/Repeat/block35\', load_tf_repeat_1],\n        \'mixed_6a\': [\'InceptionResnetV1/Mixed_6a\', load_tf_mixed6a],\n        \'repeat_2\': [\'InceptionResnetV1/Repeat_1/block17\', load_tf_repeat_2],\n        \'mixed_7a\': [\'InceptionResnetV1/Mixed_7a\', load_tf_mixed7a],\n        \'repeat_3\': [\'InceptionResnetV1/Repeat_2/block8\', load_tf_repeat_3],\n        \'block8\': [\'InceptionResnetV1/Block8\', load_tf_block17_8],\n        \'last_linear\': [\'InceptionResnetV1/Bottleneck/weights\', load_tf_linear],\n        \'last_bn\': [\'InceptionResnetV1/Bottleneck/BatchNorm\', load_tf_batchNorm],\n        \'logits\': [\'Logits\', load_tf_linear],\n    }\n\n    print(\'\\nLoad VGGFace2-trained weights and save\\n\')\n    mdl = InceptionResnetV1(num_classes=8631).eval()\n    tf_mdl_dir = \'data/20180402-114759\'\n    data_name = \'vggface2\'\n    load_tf_model_weights(mdl, lookup_inception_resnet_v1, tf_mdl_dir)\n    state_dict = mdl.state_dict()\n    torch.save(state_dict, f\'{tf_mdl_dir}-{data_name}.pt\')    \n    torch.save(\n        {\n            \'logits.weight\': state_dict[\'logits.weight\'],\n            \'logits.bias\': state_dict[\'logits.bias\'],\n        },\n        f\'{tf_mdl_dir}-{data_name}-logits.pt\'\n    )\n    state_dict.pop(\'logits.weight\')\n    state_dict.pop(\'logits.bias\')\n    torch.save(state_dict, f\'{tf_mdl_dir}-{data_name}-features.pt\')\n    \n    print(\'\\nLoad CASIA-Webface-trained weights and save\\n\')\n    mdl = InceptionResnetV1(num_classes=10575).eval()\n    tf_mdl_dir = \'data/20180408-102900\'\n    data_name = \'casia-webface\'\n    load_tf_model_weights(mdl, lookup_inception_resnet_v1, tf_mdl_dir)\n    state_dict = mdl.state_dict()\n    torch.save(state_dict, f\'{tf_mdl_dir}-{data_name}.pt\')    \n    torch.save(\n        {\n            \'logits.weight\': state_dict[\'logits.weight\'],\n            \'logits.bias\': state_dict[\'logits.bias\'],\n        },\n        f\'{tf_mdl_dir}-{data_name}-logits.pt\'\n    )\n    state_dict.pop(\'logits.weight\')\n    state_dict.pop(\'logits.bias\')\n    torch.save(state_dict, f\'{tf_mdl_dir}-{data_name}-features.pt\')\n    \n    lookup_pnet = {\n        \'conv1\': [\'pnet/conv1\', load_tf_conv2d_trans],\n        \'prelu1\': [\'pnet/PReLU1\', load_tf_linear],\n        \'conv2\': [\'pnet/conv2\', load_tf_conv2d_trans],\n        \'prelu2\': [\'pnet/PReLU2\', load_tf_linear],\n        \'conv3\': [\'pnet/conv3\', load_tf_conv2d_trans],\n        \'prelu3\': [\'pnet/PReLU3\', load_tf_linear],\n        \'conv4_1\': [\'pnet/conv4-1\', load_tf_conv2d_trans],\n        \'conv4_2\': [\'pnet/conv4-2\', load_tf_conv2d_trans],\n    }\n    lookup_rnet = {\n        \'conv1\': [\'rnet/conv1\', load_tf_conv2d_trans],\n        \'prelu1\': [\'rnet/prelu1\', load_tf_linear],\n        \'conv2\': [\'rnet/conv2\', load_tf_conv2d_trans],\n        \'prelu2\': [\'rnet/prelu2\', load_tf_linear],\n        \'conv3\': [\'rnet/conv3\', load_tf_conv2d_trans],\n        \'prelu3\': [\'rnet/prelu3\', load_tf_linear],\n        \'dense4\': [\'rnet/conv4\', load_tf_linear],\n        \'prelu4\': [\'rnet/prelu4\', load_tf_linear],\n        \'dense5_1\': [\'rnet/conv5-1\', load_tf_linear],\n        \'dense5_2\': [\'rnet/conv5-2\', load_tf_linear],\n    }\n    lookup_onet = {\n        \'conv1\': [\'onet/conv1\', load_tf_conv2d_trans],\n        \'prelu1\': [\'onet/prelu1\', load_tf_linear],\n        \'conv2\': [\'onet/conv2\', load_tf_conv2d_trans],\n        \'prelu2\': [\'onet/prelu2\', load_tf_linear],\n        \'conv3\': [\'onet/conv3\', load_tf_conv2d_trans],\n        \'prelu3\': [\'onet/prelu3\', load_tf_linear],\n        \'conv4\': [\'onet/conv4\', load_tf_conv2d_trans],\n        \'prelu4\': [\'onet/prelu4\', load_tf_linear],\n        \'dense5\': [\'onet/conv5\', load_tf_linear],\n        \'prelu5\': [\'onet/prelu5\', load_tf_linear],\n        \'dense6_1\': [\'onet/conv6-1\', load_tf_linear],\n        \'dense6_2\': [\'onet/conv6-2\', load_tf_linear],\n        \'dense6_3\': [\'onet/conv6-3\', load_tf_linear],\n    }\n\n    print(\'\\nLoad PNet weights and save\\n\')\n    tf_mdl_dir = lambda sess: detect_face.create_mtcnn(sess, None)\n    mdl = PNet()\n    data_name = \'pnet\'\n    load_tf_model_weights(mdl, lookup_pnet, tf_mdl_dir, is_resnet=False, arg_num=0)\n    torch.save(mdl.state_dict(), f\'data/{data_name}.pt\')\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        compare_mtcnn(mdl, tf_mdl_dir, sess, 0, torch.randn(1, 256, 256, 3).detach())\n\n    print(\'\\nLoad RNet weights and save\\n\')\n    mdl = RNet()\n    data_name = \'rnet\'\n    load_tf_model_weights(mdl, lookup_rnet, tf_mdl_dir, is_resnet=False, arg_num=1)\n    torch.save(mdl.state_dict(), f\'data/{data_name}.pt\')\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        compare_mtcnn(mdl, tf_mdl_dir, sess, 1, torch.randn(1, 24, 24, 3).detach())\n\n    print(\'\\nLoad ONet weights and save\\n\')\n    mdl = ONet()\n    data_name = \'onet\'\n    load_tf_model_weights(mdl, lookup_onet, tf_mdl_dir, is_resnet=False, arg_num=2)\n    torch.save(mdl.state_dict(), f\'data/{data_name}.pt\')\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        compare_mtcnn(mdl, tf_mdl_dir, sess, 2, torch.randn(1, 48, 48, 3).detach())\n'"
thirdparty/his_evaluators/his_evaluators/metrics/facenet_pytorch/models/utils/training.py,9,"b'import torch\nimport numpy as np\nimport time\n\n\nclass Logger(object):\n\n    def __init__(self, mode, length, calculate_mean=False):\n        self.mode = mode\n        self.length = length\n        self.calculate_mean = calculate_mean\n        if self.calculate_mean:\n            self.fn = lambda x, i: x / (i + 1)\n        else:\n            self.fn = lambda x, i: x\n\n    def __call__(self, loss, metrics, i):\n        track_str = \'\\r{} | {:5d}/{:<5d}| \'.format(self.mode, i + 1, self.length)\n        loss_str = \'loss: {:9.4f} | \'.format(self.fn(loss, i))\n        metric_str = \' | \'.join(\'{}: {:9.4f}\'.format(k, self.fn(v, i)) for k, v in metrics.items())\n        print(track_str + loss_str + metric_str + \'   \', end=\'\')\n        if i + 1 == self.length:\n            print(\'\')\n\n\nclass BatchTimer(object):\n    """"""Batch timing class.\n    Use this class for tracking training and testing time/rate per batch or per sample.\n    \n    Keyword Arguments:\n        rate {bool} -- Whether to report a rate (batches or samples per second) or a time (seconds\n            per batch or sample). (default: {True})\n        per_sample {bool} -- Whether to report times or rates per sample or per batch.\n            (default: {True})\n    """"""\n\n    def __init__(self, rate=True, per_sample=True):\n        self.start = time.time()\n        self.end = None\n        self.rate = rate\n        self.per_sample = per_sample\n\n    def __call__(self, y_pred, y):\n        self.end = time.time()\n        elapsed = self.end - self.start\n        self.start = self.end\n        self.end = None\n\n        if self.per_sample:\n            elapsed /= len(y_pred)\n        if self.rate:\n            elapsed = 1 / elapsed\n\n        return torch.tensor(elapsed)\n\n\ndef accuracy(logits, y):\n    _, preds = torch.max(logits, 1)\n    return (preds == y).float().mean()\n\n\ndef pass_epoch(\n    model, loss_fn, loader, optimizer=None, scheduler=None,\n    batch_metrics={\'time\': BatchTimer()}, show_running=True,\n    device=\'cpu\', writer=None\n):\n    """"""Train or evaluate over a data epoch.\n    \n    Arguments:\n        model {torch.nn.Module} -- Pytorch model.\n        loss_fn {callable} -- A function to compute (scalar) loss.\n        loader {torch.utils.data.DataLoader} -- A pytorch data loader.\n    \n    Keyword Arguments:\n        optimizer {torch.optim.Optimizer} -- A pytorch optimizer.\n        scheduler {torch.optim.lr_scheduler._LRScheduler} -- LR scheduler (default: {None})\n        batch_metrics {dict} -- Dictionary of metric functions to call on each batch. The default\n            is a simple timer. A progressive average of these metrics, along with the average\n            loss, is printed every batch. (default: {{\'time\': iter_timer()}})\n        show_running {bool} -- Whether or not to print losses and metrics for the current batch\n            or rolling averages. (default: {False})\n        device {str or torch.device} -- Device for pytorch to use. (default: {\'cpu\'})\n        writer {torch.utils.tensorboard.SummaryWriter} -- Tensorboard SummaryWriter. (default: {None})\n    \n    Returns:\n        tuple(torch.Tensor, dict) -- A tuple of the average loss and a dictionary of average\n            metric values across the epoch.\n    """"""\n    \n    mode = \'Train\' if model.training else \'Valid\'\n    logger = Logger(mode, length=len(loader), calculate_mean=show_running)\n    loss = 0\n    metrics = {}\n\n    for i_batch, (x, y) in enumerate(loader):\n        x = x.to(device)\n        y = y.to(device)\n        y_pred = model(x)\n        loss_batch = loss_fn(y_pred, y)\n\n        if model.training:\n            loss_batch.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        metrics_batch = {}\n        for metric_name, metric_fn in batch_metrics.items():\n            metrics_batch[metric_name] = metric_fn(y_pred, y).detach().cpu()\n            metrics[metric_name] = metrics.get(metric_name, 0) + metrics_batch[metric_name]\n            \n        if writer is not None and model.training:\n            if writer.iteration % writer.interval == 0:\n                writer.add_scalars(\'loss\', {mode: loss_batch.detach().cpu()}, writer.iteration)\n                for metric_name, metric_batch in metrics_batch.items():\n                    writer.add_scalars(metric_name, {mode: metric_batch}, writer.iteration)\n            writer.iteration += 1\n        \n        loss_batch = loss_batch.detach().cpu()\n        loss += loss_batch\n        if show_running:\n            logger(loss, metrics, i_batch)\n        else:\n            logger(loss_batch, metrics_batch, i_batch)\n    \n    if model.training and scheduler is not None:\n        scheduler.step()\n\n    loss = loss / (i_batch + 1)\n    metrics = {k: v / (i_batch + 1) for k, v in metrics.items()}\n            \n    if writer is not None and not model.training:\n        writer.add_scalars(\'loss\', {mode: loss.detach()}, writer.iteration)\n        for metric_name, metric in metrics.items():\n            writer.add_scalars(metric_name, {mode: metric})\n\n    return loss, metrics\n\n\ndef collate_pil(x): \n    out_x, out_y = [], [] \n    for xx, yy in x: \n        out_x.append(xx) \n        out_y.append(yy) \n    return out_x, out_y \n'"
