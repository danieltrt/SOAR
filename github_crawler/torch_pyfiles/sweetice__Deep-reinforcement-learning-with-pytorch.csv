file_path,api_count,code
Char00 Conventional Algorithms/Q-learning.py,0,"b'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\nALPHA = 0.1\nGAMMA = 0.95\nEPSILION = 0.9\nN_STATE = 20\nACTIONS = [\'left\', \'right\']\nMAX_EPISODES = 200\nFRESH_TIME = 0.1\n\ndef build_q_table(n_state, actions):\n    q_table = pd.DataFrame(\n    np.zeros((n_state, len(actions))),\n    np.arange(n_state),\n    actions\n    )\n    return q_table\n\ndef choose_action(state, q_table):\n    #epslion - greedy policy\n    state_action = q_table.loc[state,:]\n    if np.random.uniform()>EPSILION or (state_action==0).all():\n        action_name = np.random.choice(ACTIONS)\n    else:\n        action_name = state_action.idxmax()\n    return action_name\n\ndef get_env_feedback(state, action):\n    if action==\'right\':\n        if state == N_STATE-2:\n            next_state = \'terminal\'\n            reward = 1\n        else:\n            next_state = state+1\n            reward = -0.5\n    else:\n        if state == 0:\n            next_state = 0\n            \n        else:\n            next_state = state-1\n        reward = -0.5\n    return next_state, reward\n\ndef update_env(state,episode, step_counter):\n    env = [\'-\'] *(N_STATE-1)+[\'T\']\n    if state ==\'terminal\':\n        print(""Episode {}, the total step is {}"".format(episode+1, step_counter))\n        final_env = [\'-\'] *(N_STATE-1)+[\'T\']\n        return True, step_counter\n    else:\n        env[state]=\'*\'\n        env = \'\'.join(env)\n        print(env)\n        time.sleep(FRESH_TIME)\n        return False, step_counter\n        \n    \ndef q_learning():\n    q_table = build_q_table(N_STATE, ACTIONS)\n    step_counter_times = []\n    for episode in range(MAX_EPISODES):\n        state = 0\n        is_terminal = False\n        step_counter = 0\n        update_env(state, episode, step_counter)\n        while not is_terminal:\n            action = choose_action(state,q_table)\n            next_state, reward = get_env_feedback(state, action)\n            next_q = q_table.loc[state, action]\n            if next_state == \'terminal\':\n                is_terminal = True\n                q_target = reward\n            else:\n                delta = reward + GAMMA*q_table.iloc[next_state,:].max()-q_table.loc[state, action]\n                q_table.loc[state, action] += ALPHA*delta\n            state = next_state\n            is_terminal,steps = update_env(state, episode, step_counter+1)\n            step_counter+=1\n            if is_terminal:\n                step_counter_times.append(steps)\n                \n    return q_table, step_counter_times\n\ndef main():\n    q_table, step_counter_times= q_learning()\n    print(""Q table\\n{}\\n"".format(q_table))\n    print(\'end\')\n    \n    plt.plot(step_counter_times,\'g-\')\n    plt.ylabel(""steps"")\n    plt.show()\n    print(""The step_counter_times is {}"".format(step_counter_times))\n\nmain() \n'"
Char00 Conventional Algorithms/Sarsa.py,0,"b'\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\nALPHA = 0.1\nGAMMA = 0.95\nEPSILION = 0.9\nN_STATE = 6\nACTIONS = [\'left\', \'right\']\nMAX_EPISODES = 200\nFRESH_TIME = 0.1\n\ndef build_q_table(n_state, actions):\n    q_table = pd.DataFrame(\n    np.zeros((n_state, len(actions))),\n    np.arange(n_state),\n    actions\n    )\n    return q_table\n\ndef choose_action(state, q_table):\n    #epslion - greedy policy\n    state_action = q_table.loc[state,:]\n    if np.random.uniform()>EPSILION or (state_action==0).all():\n        action_name = np.random.choice(ACTIONS)\n    else:\n        action_name = state_action.idxmax()\n    return action_name\n\ndef get_env_feedback(state, action):\n    if action==\'right\':\n        if state == N_STATE-2:\n            next_state = \'terminal\'\n            reward = 1\n        else:\n            next_state = state+1\n            reward = -0.5\n    else:\n        if state == 0:\n            next_state = 0\n            \n        else:\n            next_state = state-1\n        reward = -0.5\n    return next_state, reward\n\ndef update_env(state,episode, step_counter):\n    env = [\'-\'] *(N_STATE-1)+[\'T\']\n    if state ==\'terminal\':\n        print(""Episode {}, the total step is {}"".format(episode+1, step_counter))\n        final_env = [\'-\'] *(N_STATE-1)+[\'T\']\n        return True, step_counter\n    else:\n        env[state]=\'*\'\n        env = \'\'.join(env)\n        print(env)\n        time.sleep(FRESH_TIME)\n        return False, step_counter\n        \n    \ndef sarsa_learning():\n    q_table = build_q_table(N_STATE, ACTIONS)\n    step_counter_times = []\n    for episode in range(MAX_EPISODES):\n        state = 0\n        is_terminal = False\n        step_counter = 0\n        update_env(state, episode, step_counter)\n        while not is_terminal:\n            action = choose_action(state,q_table)\n            next_state, reward = get_env_feedback(state, action)\n            if next_state != \'terminal\':\n                next_action = choose_action(next_state, q_table) #sarsa update method\n            else:\n                next_action = action\n            next_q = q_table.loc[state, action]\n\n            if next_state == \'terminal\':\n                is_terminal = True\n                q_target = reward\n            else:\n                delta = reward + GAMMA*q_table.loc[next_state,next_action]-q_table.loc[state, action]\n                q_table.loc[state, action] += ALPHA*delta\n            state = next_state\n            is_terminal,steps = update_env(state, episode, step_counter+1)\n            step_counter+=1\n            if is_terminal:\n                step_counter_times.append(steps)\n                \n    return q_table, step_counter_times\n\ndef main():\n    q_table, step_counter_times= sarsa_learning()\n    print(""Q table\\n{}\\n"".format(q_table))\n    print(\'end\')\n    \n    plt.plot(step_counter_times,\'g-\')\n    plt.ylabel(""steps"")\n    plt.show()\n    print(""The step_counter_times is {}"".format(step_counter_times))\n\nmain() \n'"
Char00 Conventional Algorithms/gridworld.py,0,"b'\nimport numpy as np\n\nclass GridWorld:\n\n    def __init__(self, tot_row, tot_col):\n        self.action_space_size = 4\n        self.world_row = tot_row\n        self.world_col = tot_col\n        #The world is a matrix of size row x col x 2\n        #The first layer contains the obstacles\n        #The second layer contains the rewards\n        #self.world_matrix = np.zeros((tot_row, tot_col, 2))\n        self.transition_matrix = np.ones((self.action_space_size, self.action_space_size))/ self.action_space_size\n        #self.transition_array = np.ones(self.action_space_size) / self.action_space_size\n        self.reward_matrix = np.zeros((tot_row, tot_col))\n        self.state_matrix = np.zeros((tot_row, tot_col))\n        self.position = [np.random.randint(tot_row), np.random.randint(tot_col)]\n\n    #def setTransitionArray(self, transition_array):\n        #if(transition_array.shape != self.transition_array):\n            #raise ValueError(\'The shape of the two matrices must be the same.\') \n        #self.transition_array = transition_array        \n\n    def setTransitionMatrix(self, transition_matrix):\n        \'\'\'Set the reward matrix.\n\n        The transition matrix here is intended as a matrix which has a line\n        for each action and the element of the row are the probabilities to\n        executes each action when a command is given. For example:\n        [[0.55, 0.25, 0.10, 0.10]\n         [0.25, 0.25, 0.25, 0.25]\n         [0.30, 0.20, 0.40, 0.10]\n         [0.10, 0.20, 0.10, 0.60]]\n\n        This matrix defines the transition rules for all the 4 possible actions.\n        The first row corresponds to the probabilities of executing each one of\n        the 4 actions when the policy orders to the robot to go UP. In this case\n        the transition model says that with a probability of 0.55 the robot will\n        go UP, with a probaiblity of 0.25 RIGHT, 0.10 DOWN and 0.10 LEFT.\n        \'\'\'\n        if(transition_matrix.shape != self.transition_matrix.shape):\n            raise ValueError(\'The shape of the two matrices must be the same.\') \n        self.transition_matrix = transition_matrix\n\n    def setRewardMatrix(self, reward_matrix):\n        \'\'\'Set the reward matrix.\n\n        \'\'\'\n        if(reward_matrix.shape != self.reward_matrix.shape):\n            raise ValueError(\'The shape of the matrix does not match with the shape of the world.\')\n        self.reward_matrix = reward_matrix\n\n    def setStateMatrix(self, state_matrix):\n        \'\'\'Set the obstacles in the world.\n\n        The input to the function is a matrix with the\n        same size of the world \n        -1 for states which are not walkable.\n        +1 for terminal states\n         0 for all the walkable states (non terminal)\n        The following matrix represents the 4x3 world\n        used in the series ""dissecting reinforcement learning""\n        [[0,  0,  0, +1]\n         [0, -1,  0, +1]\n         [0,  0,  0,  0]]\n        \'\'\'\n        if(state_matrix.shape != self.state_matrix.shape):\n            raise ValueError(\'The shape of the matrix does not match with the shape of the world.\')\n        self.state_matrix = state_matrix\n\n    def setPosition(self, index_row=None, index_col=None):\n        \'\'\' Set the position of the robot in a specific state.\n\n        \'\'\'\n        if(index_row is None or index_col is None): self.position = [np.random.randint(tot_row), np.random.randint(tot_col)]\n        else: self.position = [index_row, index_col]\n\n    def render(self):\n        \'\'\' Print the current world in the terminal.\n\n        O represents the robot position\n        - respresent empty states.\n        # represents obstacles\n        * represents terminal states\n        \'\'\'\n        graph = """"\n        for row in range(self.world_row):\n            row_string = """"\n            for col in range(self.world_col):\n                if(self.position == [row, col]): row_string += u"" \\u25CB "" # u"" \\u25CC ""\n                else:\n                    if(self.state_matrix[row, col] == 0): row_string += \' - \'\n                    elif(self.state_matrix[row, col] == -1): row_string += \' # \'\n                    elif(self.state_matrix[row, col] == +1): row_string += \' * \'\n            row_string += \'\\n\'\n            graph += row_string \n        print(graph)            \n\n    def reset(self, exploring_starts=False):\n        \'\'\' Set the position of the robot in the bottom left corner.\n\n        It returns the first observation\n        \'\'\'\n        if exploring_starts:\n            while(True):\n                row = np.random.randint(0, self.world_row)\n                col = np.random.randint(0, self.world_col)\n                if(self.state_matrix[row, col] == 0): break\n            self.position = [row, col]\n        else:\n            self.position = [self.world_row-1, 0]\n        #reward = self.reward_matrix[self.position[0], self.position[1]]\n        return self.position\n\n    def step(self, action):\n        \'\'\' One step in the world.\n\n        [observation, reward, done = env.step(action)]\n        The robot moves one step in the world based on the action given.\n        The action can be 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n        @return observation the position of the robot after the step\n        @return reward the reward associated with the next state\n        @return done True if the state is terminal  \n        \'\'\'\n        if(action >= self.action_space_size): \n            raise ValueError(\'The action is not included in the action space.\')\n\n        #Based on the current action and the probability derived\n        #from the trasition model it chooses a new actio to perform\n        action = np.random.choice(4, 1, p=self.transition_matrix[int(action),:])\n        #action = self.transition_model(action)\n\n        #Generating a new position based on the current position and action\n        if(action == 0): new_position = [self.position[0]-1, self.position[1]]   #UP\n        elif(action == 1): new_position = [self.position[0], self.position[1]+1] #RIGHT\n        elif(action == 2): new_position = [self.position[0]+1, self.position[1]] #DOWN\n        elif(action == 3): new_position = [self.position[0], self.position[1]-1] #LEFT\n        else: raise ValueError(\'The action is not included in the action space.\')\n\n        #Check if the new position is a valid position\n        #print(self.state_matrix)\n        if (new_position[0]>=0 and new_position[0]<self.world_row):\n            if(new_position[1]>=0 and new_position[1]<self.world_col):\n                if(self.state_matrix[new_position[0], new_position[1]] != -1):\n                    self.position = new_position\n\n        reward = self.reward_matrix[self.position[0], self.position[1]]\n        #Done is True if the state is a terminal state\n        done = bool(self.state_matrix[self.position[0], self.position[1]])\n        return self.position, reward, done\n\n'"
Char01 DQN/DQN.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport gym\nimport matplotlib.pyplot as plt\nimport copy\n\n# hyper-parameters\nBATCH_SIZE = 128\nLR = 0.01\nGAMMA = 0.90\nEPISILO = 0.9\nMEMORY_CAPACITY = 2000\nQ_NETWORK_ITERATION = 100\n\nenv = gym.make(""CartPole-v0"")\nenv = env.unwrapped\nNUM_ACTIONS = env.action_space.n\nNUM_STATES = env.observation_space.shape[0]\nENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample.shape\n\nclass Net(nn.Module):\n    """"""docstring for Net""""""\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(NUM_STATES, 50)\n        self.fc1.weight.data.normal_(0,0.1)\n        self.fc2 = nn.Linear(50,30)\n        self.fc2.weight.data.normal_(0,0.1)\n        self.out = nn.Linear(30,NUM_ACTIONS)\n        self.out.weight.data.normal_(0,0.1)\n\n    def forward(self,x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        action_prob = self.out(x)\n        return action_prob\n\nclass DQN():\n    """"""docstring for DQN""""""\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.eval_net, self.target_net = Net(), Net()\n\n        self.learn_step_counter = 0\n        self.memory_counter = 0\n        self.memory = np.zeros((MEMORY_CAPACITY, NUM_STATES * 2 + 2))\n        # why the NUM_STATE*2 +2\n        # When we store the memory, we put the state, action, reward and next_state in the memory\n        # here reward and action is a number, state is a ndarray\n        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n        self.loss_func = nn.MSELoss()\n\n    def choose_action(self, state):\n        state = torch.unsqueeze(torch.FloatTensor(state), 0) # get a 1D array\n        if np.random.randn() <= EPISILO:# greedy policy\n            action_value = self.eval_net.forward(state)\n            action = torch.max(action_value, 1)[1].data.numpy()\n            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n        else: # random policy\n            action = np.random.randint(0,NUM_ACTIONS)\n            action = action if ENV_A_SHAPE ==0 else action.reshape(ENV_A_SHAPE)\n        return action\n\n\n    def store_transition(self, state, action, reward, next_state):\n        transition = np.hstack((state, [action, reward], next_state))\n        index = self.memory_counter % MEMORY_CAPACITY\n        self.memory[index, :] = transition\n        self.memory_counter += 1\n\n\n    def learn(self):\n\n        #update the parameters\n        if self.learn_step_counter % Q_NETWORK_ITERATION ==0:\n            self.target_net.load_state_dict(self.eval_net.state_dict())\n        self.learn_step_counter+=1\n\n        #sample batch from memory\n        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n        batch_memory = self.memory[sample_index, :]\n        batch_state = torch.FloatTensor(batch_memory[:, :NUM_STATES])\n        batch_action = torch.LongTensor(batch_memory[:, NUM_STATES:NUM_STATES+1].astype(int))\n        batch_reward = torch.FloatTensor(batch_memory[:, NUM_STATES+1:NUM_STATES+2])\n        batch_next_state = torch.FloatTensor(batch_memory[:,-NUM_STATES:])\n\n        #q_eval\n        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n        q_next = self.target_net(batch_next_state).detach()\n        q_target = batch_reward + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n        loss = self.loss_func(q_eval, q_target)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\ndef reward_func(env, x, x_dot, theta, theta_dot):\n    r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.5\n    r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n    reward = r1 + r2\n    return reward\n\ndef main():\n    dqn = DQN()\n    episodes = 400\n    print(""Collecting Experience...."")\n    reward_list = []\n    plt.ion()\n    fig, ax = plt.subplots()\n    for i in range(episodes):\n        state = env.reset()\n        ep_reward = 0\n        while True:\n            env.render()\n            action = dqn.choose_action(state)\n            next_state, _ , done, info = env.step(action)\n            x, x_dot, theta, theta_dot = next_state\n            reward = reward_func(env, x, x_dot, theta, theta_dot)\n\n            dqn.store_transition(state, action, reward, next_state)\n            ep_reward += reward\n\n            if dqn.memory_counter >= MEMORY_CAPACITY:\n                dqn.learn()\n                if done:\n                    print(""episode: {} , the episode reward is {}"".format(i, round(ep_reward, 3)))\n            if done:\n                break\n            state = next_state\n        r = copy.copy(reward)\n        reward_list.append(r)\n        ax.set_xlim(0,300)\n        #ax.cla()\n        ax.plot(reward_list, \'g-\', label=\'total_loss\')\n        plt.pause(0.001)\n        \n\nif __name__ == \'__main__\':\n    main()\n'"
Char01 DQN/DQN_CartPole-v0.py,13,"b'import argparse\nimport pickle\nfrom collections import namedtuple\nfrom itertools import count\n\nimport os, time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal, Categorical\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\nfrom tensorboardX import SummaryWriter\n\n# Hyper-parameters\nseed = 1\nrender = False\nnum_episodes = 2000\nenv = gym.make(\'CartPole-v0\').unwrapped\nnum_state = env.observation_space.shape[0]\nnum_action = env.action_space.n\ntorch.manual_seed(seed)\nenv.seed(seed)\n\nTransition = namedtuple(\'Transition\', [\'state\', \'action\', \'reward\', \'next_state\'])\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(num_state, 100)\n        self.fc2 = nn.Linear(100, num_action)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        action_value = self.fc2(x)\n        return action_value\n\nclass DQN():\n\n    capacity = 8000\n    learning_rate = 1e-3\n    memory_count = 0\n    batch_size = 256\n    gamma = 0.995\n    update_count = 0\n\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.target_net, self.act_net = Net(), Net()\n        self.memory = [None]*self.capacity\n        self.optimizer = optim.Adam(self.act_net.parameters(), self.learning_rate)\n        self.loss_func = nn.MSELoss()\n        self.writer = SummaryWriter(\'./DQN/logs\')\n\n\n    def select_action(self,state):\n        state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n        value = self.act_net(state)\n        action_max_value, index = torch.max(value, 1)\n        action = index.item()\n        if np.random.rand(1) >= 0.9: # epslion greedy\n            action = np.random.choice(range(num_action), 1).item()\n        return action\n\n    def store_transition(self,transition):\n        index = self.memory_count % self.capacity\n        self.memory[index] = transition\n        self.memory_count += 1\n        return self.memory_count >= self.capacity\n\n    def update(self):\n        if self.memory_count >= self.capacity:\n            state = torch.tensor([t.state for t in self.memory]).float()\n            action = torch.LongTensor([t.action for t in self.memory]).view(-1,1).long()\n            reward = torch.tensor([t.reward for t in self.memory]).float()\n            next_state = torch.tensor([t.next_state for t in self.memory]).float()\n\n            reward = (reward - reward.mean()) / (reward.std() + 1e-7)\n            with torch.no_grad():\n                target_v = reward + self.gamma * self.target_net(next_state).max(1)[0]\n\n            #Update...\n            for index in BatchSampler(SubsetRandomSampler(range(len(self.memory))), batch_size=self.batch_size, drop_last=False):\n                v = (self.act_net(state).gather(1, action))[index]\n                loss = self.loss_func(target_v[index].unsqueeze(1), (self.act_net(state).gather(1, action))[index])\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                self.writer.add_scalar(\'loss/value_loss\', loss, self.update_count)\n                self.update_count +=1\n                if self.update_count % 100 ==0:\n                    self.target_net.load_state_dict(self.act_net.state_dict())\n        else:\n            print(""Memory Buff is too less"")\ndef main():\n\n    agent = DQN()\n    for i_ep in range(num_episodes):\n        state = env.reset()\n        if render: env.render()\n        for t in range(10000):\n            action = agent.select_action(state)\n            next_state, reward, done, info = env.step(action)\n            if render: env.render()\n            transition = Transition(state, action, reward, next_state)\n            agent.store_transition(transition)\n            state = next_state\n            if done or t >=9999:\n                agent.writer.add_scalar(\'live/finish_step\', t+1, global_step=i_ep)\n                agent.update()\n                if i_ep % 10 == 0:\n                    print(""episodes {}, step is {} "".format(i_ep, t))\n                break\n\nif __name__ == \'__main__\':\n    main()\n'"
Char01 DQN/DQN_MountainCar-v0.py,13,"b'import argparse\nimport pickle\nfrom collections import namedtuple\nfrom itertools import count\n\nimport os, time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal, Categorical\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\nfrom tensorboardX import SummaryWriter\n\n# Hyper-parameters\nseed = 1\nrender = False\nnum_episodes = 400000\nenv = gym.make(\'MountainCar-v0\').unwrapped\nnum_state = env.observation_space.shape[0]\nnum_action = env.action_space.n\ntorch.manual_seed(seed)\nenv.seed(seed)\n\nTransition = namedtuple(\'Transition\', [\'state\', \'action\', \'reward\', \'next_state\'])\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(num_state, 100)\n        self.fc2 = nn.Linear(100, num_action)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        action_prob = self.fc2(x)\n        return action_prob\n\nclass DQN():\n\n    capacity = 8000\n    learning_rate = 1e-3\n    memory_count = 0\n    batch_size = 256\n    gamma = 0.995\n    update_count = 0\n\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.target_net, self.act_net = Net(), Net()\n        self.memory = [None]*self.capacity\n        self.optimizer = optim.Adam(self.act_net.parameters(), self.learning_rate)\n        self.loss_func = nn.MSELoss()\n        self.writer = SummaryWriter(\'./DQN/logs\')\n\n\n    def select_action(self,state):\n        state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n        value = self.act_net(state)\n        action_max_value, index = torch.max(value, 1)\n        action = index.item()\n        if np.random.rand(1) >= 0.9: # epslion greedy\n            action = np.random.choice(range(num_action), 1).item()\n        return action\n\n    def store_transition(self,transition):\n        index = self.memory_count % self.capacity\n        self.memory[index] = transition\n        self.memory_count += 1\n        return self.memory_count >= self.capacity\n\n    def update(self):\n        if self.memory_count >= self.capacity:\n            state = torch.tensor([t.state for t in self.memory]).float()\n            action = torch.LongTensor([t.action for t in self.memory]).view(-1,1).long()\n            reward = torch.tensor([t.reward for t in self.memory]).float()\n            next_state = torch.tensor([t.next_state for t in self.memory]).float()\n\n            reward = (reward - reward.mean()) / (reward.std() + 1e-7)\n            with torch.no_grad():\n                target_v = reward + self.gamma * self.target_net(next_state).max(1)[0]\n\n            #Update...\n            for index in BatchSampler(SubsetRandomSampler(range(len(self.memory))), batch_size=self.batch_size, drop_last=False):\n                v = (self.act_net(state).gather(1, action))[index]\n                loss = self.loss_func(target_v[index].unsqueeze(1), (self.act_net(state).gather(1, action))[index])\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n                self.writer.add_scalar(\'loss/value_loss\', loss, self.update_count)\n                self.update_count +=1\n                if self.update_count % 100 ==0:\n                    self.target_net.load_state_dict(self.act_net.state_dict())\n        else:\n            print(""Memory Buff is too less"")\ndef main():\n\n    agent = DQN()\n    for i_ep in range(num_episodes):\n        state = env.reset()\n        if render: env.render()\n        for t in range(10000):\n            action = agent.select_action(state)\n            next_state, reward, done, info = env.step(action)\n            if render: env.render()\n            transition = Transition(state, action, reward, next_state)\n            agent.store_transition(transition)\n            state = next_state\n            if done or t >=9999:\n                agent.writer.add_scalar(\'live/finish_step\', t+1, global_step=i_ep)\n                agent.update()\n                if i_ep % 10 == 0:\n                    print(""episodes {}, step is {} "".format(i_ep, t))\n                break\n\nif __name__ == \'__main__\':\n    main()\n'"
Char01 DQN/DQN_mountain_car_v1.py,8,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nimport matplotlib.pyplot as plt\nimport gym\n\n\n#hyper parameters\nEPSILON = 0.9\nGAMMA = 0.9\nLR = 0.01\nMEMORY_CAPACITY = 2000\nQ_NETWORK_ITERATION = 100\nBATCH_SIZE = 32\n\nEPISODES = 400\nenv = gym.make(\'MountainCar-v0\')\nenv = env.unwrapped\nNUM_STATES = env.observation_space.shape[0] # 2\nNUM_ACTIONS = env.action_space.n\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.fc1 = nn.Linear(NUM_STATES, 30)\n        self.fc1.weight.data.normal_(0, 0.1)\n        self.fc2 = nn.Linear(30, NUM_ACTIONS)\n        self.fc2.weight.data.normal_(0, 0.1)\n\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n\n        return x\n\nclass Dqn():\n    def __init__(self):\n        self.eval_net, self.target_net = Net(), Net()\n        self.memory = np.zeros((MEMORY_CAPACITY, NUM_STATES *2 +2))\n        # state, action ,reward and next state\n        self.memory_counter = 0\n        self.learn_counter = 0\n        self.optimizer = optim.Adam(self.eval_net.parameters(), LR)\n        self.loss = nn.MSELoss()\n\n        self.fig, self.ax = plt.subplots()\n\n    def store_trans(self, state, action, reward, next_state):\n        if self.memory_counter % 500 ==0:\n            print(""The experience pool collects {} time experience"".format(self.memory_counter))\n        index = self.memory_counter % MEMORY_CAPACITY\n        trans = np.hstack((state, [action], [reward], next_state))\n        self.memory[index,] = trans\n        self.memory_counter += 1\n\n    def choose_action(self, state):\n        # notation that the function return the action\'s index nor the real action\n        # EPSILON\n        state = torch.unsqueeze(torch.FloatTensor(state) ,0)\n        if np.random.randn() <= EPSILON:\n            action_value = self.eval_net.forward(state)\n            action = torch.max(action_value, 1)[1].data.numpy() # get action whose q is max\n            action = action[0] #get the action index\n        else:\n            action = np.random.randint(0,NUM_ACTIONS)\n        return action\n\n    def plot(self, ax, x):\n        ax.cla()\n        ax.set_xlabel(""episode"")\n        ax.set_ylabel(""total reward"")\n        ax.plot(x, \'b-\')\n        plt.pause(0.000000000000001)\n\n    def learn(self):\n        # learn 100 times then the target network update\n        if self.learn_counter % Q_NETWORK_ITERATION ==0:\n            self.target_net.load_state_dict(self.eval_net.state_dict())\n        self.learn_counter+=1\n\n        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n        batch_memory = self.memory[sample_index, :]\n        batch_state = torch.FloatTensor(batch_memory[:, :NUM_STATES])\n        #note that the action must be a int\n        batch_action = torch.LongTensor(batch_memory[:, NUM_STATES:NUM_STATES+1].astype(int))\n        batch_reward = torch.FloatTensor(batch_memory[:, NUM_STATES+1: NUM_STATES+2])\n        batch_next_state = torch.FloatTensor(batch_memory[:, -NUM_STATES:])\n\n        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n        q_next = self.target_net(batch_next_state).detach()\n        q_target = batch_reward + GAMMA*q_next.max(1)[0].view(BATCH_SIZE, 1)\n\n        loss = self.loss(q_eval, q_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n\n\ndef main():\n    net = Dqn()\n    print(""The DQN is collecting experience..."")\n    step_counter_list = []\n    for episode in range(EPISODES):\n        state = env.reset()\n        step_counter = 0\n        while True:\n            step_counter +=1\n            env.render()\n            action = net.choose_action(state)\n            next_state, reward, done, info = env.step(action)\n            reward = reward * 100 if reward >0 else reward * 5\n            net.store_trans(state, action, reward, next_state)\n\n            if net.memory_counter >= MEMORY_CAPACITY:\n                net.learn()\n                if done:\n                    print(""episode {}, the reward is {}"".format(episode, round(reward, 3)))\n            if done:\n                step_counter_list.append(step_counter)\n                net.plot(net.ax, step_counter_list)\n                break\n\n            state = next_state\n\nif __name__ == \'__main__\':\n    main()\n'"
Char01 DQN/naiveDQN.py,9,"b'# Nota that this network won\'t work because the reward is always 1\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport gym\n\n# hyper-parameters\nBATCH_SIZE = 128\nLR = 0.01\nGAMMA = 0.90\nEPISILO = 0.9\nMEMORY_CAPACITY = 20000\nQ_NETWORK_ITERATION = 100\n\nenv = gym.make(""CartPole-v0"")\nenv = env.unwrapped\nNUM_ACTIONS = env.action_space.n\nNUM_STATES = env.observation_space.shape[0]\nENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample.shape\nclass Net(nn.Module):\n    """"""docstring for Net""""""\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(NUM_STATES, 50)\n        self.fc1.weight.data.normal_(0,0.1)\n        self.fc2 = nn.Linear(50,30)\n        self.fc2.weight.data.normal_(0,0.1)\n        self.out = nn.Linear(30,NUM_ACTIONS)\n        self.out.weight.data.normal_(0,0.1)\n\n    def forward(self,x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        action_prob = self.out(x)\n        return action_prob\n\nclass DQN():\n    """"""docstring for DQN""""""\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.eval_net, self.target_net = Net(), Net()\n\n        self.learn_step_counter = 0\n        self.memory_counter = 0\n        self.memory = np.zeros((MEMORY_CAPACITY, NUM_STATES * 2 + 2))\n        # why the NUM_STATE*2 +2\n        # When we store the memory, we put the state, action, reward and next_state in the memory\n        # here reward and action is a number, state is a ndarray\n        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n        self.loss_func = nn.MSELoss()\n\n    def choose_action(self, state):\n        state = torch.unsqueeze(torch.FloatTensor(state), 0) # get a 1D array\n        if np.random.randn() <= EPISILO:# greedy policy\n            action_value = self.eval_net.forward(state)\n            action = torch.max(action_value, 1)[1].data.numpy()\n            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n        else: # random policy\n            action = np.random.randint(0,NUM_ACTIONS)\n            action = action if ENV_A_SHAPE ==0 else action.reshape(ENV_A_SHAPE)\n        return action\n\n\n    def store_transition(self, state, action, reward, next_state):\n        transition = np.hstack((state, [action, reward], next_state))\n        index = self.memory_counter % MEMORY_CAPACITY\n        self.memory[index, :] = transition\n        self.memory_counter += 1\n\n\n    def learn(self):\n\n        #update the parameters\n        if self.learn_step_counter % Q_NETWORK_ITERATION ==0:\n            self.target_net.load_state_dict(self.eval_net.state_dict())\n        self.learn_step_counter+=1\n\n        #sample batch from memory\n        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n        batch_memory = self.memory[sample_index, :]\n        batch_state = torch.FloatTensor(batch_memory[:, :NUM_STATES])\n        batch_action = torch.LongTensor(batch_memory[:, NUM_STATES:NUM_STATES+1].astype(int))\n        batch_reward = torch.FloatTensor(batch_memory[:, NUM_STATES+1:NUM_STATES+2])\n        batch_next_state = torch.FloatTensor(batch_memory[:,-NUM_STATES:])\n\n        #q_eval\n        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n        q_next = self.target_net(batch_next_state).detach()\n        q_target = batch_reward + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n        loss = self.loss_func(q_eval, q_target)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\ndef main():\n    dqn = DQN()\n    episodes = 400\n    print(""Collecting Experience...."")\n    for i in range(episodes):\n        state = env.reset()\n        ep_reward = 0\n        while True:\n            env.render()\n            action = dqn.choose_action(state)\n            next_state, reward, done, info = env.step(action)\n\n            dqn.store_transition(state, action, reward, next_state)\n            ep_reward += reward\n\n            if dqn.memory_counter >= MEMORY_CAPACITY:\n                dqn.learn()\n                if done:\n                    print(""episode: {} , the episode reward is {}"".format(i, round(ep_reward, 3)))\n            if done:\n                break\n            state = next_state\n\nif __name__ == \'__main__\':\n    main()\n'"
Char02 Policy Gradient/PolicyGradient.py,8,"b'\nimport argparse\nimport gym\nimport numpy as np\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torch.distributions import Categorical\n\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch REINFORCE example\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor (default: 0.99)\')\nparser.add_argument(\'--seed\', type=int, default=543, metavar=\'N\',\n                    help=\'random seed (default: 543)\')\nparser.add_argument(\'--render\', action=\'store_true\',\n                    help=\'render the environment\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                    help=\'interval between training status logs (default: 10)\')\nargs = parser.parse_args()\n\n\nenv = gym.make(\'CartPole-v0\')\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\n\n\nclass Policy(nn.Module):\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.affine2 = nn.Linear(128, 2)\n\n        self.saved_log_probs = []\n        self.rewards = []\n\n    def forward(self, x):\n        x = F.relu(self.affine1(x))\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=1)\n\n\npolicy = Policy()\noptimizer = optim.Adam(policy.parameters(), lr=1e-2)\neps = np.finfo(np.float32).eps.item()\n\n\ndef select_action(state):\n    state = torch.from_numpy(state).float().unsqueeze(0)\n    probs = policy(state)\n    m = Categorical(probs)\n    action = m.sample()\n    policy.saved_log_probs.append(m.log_prob(action))\n    return action.item()\n\n\ndef finish_episode():\n    R = 0\n    policy_loss = []\n    rewards = []\n    for r in policy.rewards[::-1]:\n        R = r + args.gamma * R\n        rewards.insert(0, R)\n    rewards = torch.tensor(rewards)\n    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n        policy_loss.append(-log_prob * reward)\n    optimizer.zero_grad()\n    policy_loss = torch.cat(policy_loss).sum()\n    policy_loss.backward()\n    optimizer.step()\n    del policy.rewards[:]\n    del policy.saved_log_probs[:]\n\n\ndef main():\n    running_reward = 10\n    for i_episode in count(1):\n        state = env.reset()\n        for t in range(10000):  # Don\'t infinite loop while learning\n            action = select_action(state)\n            state, reward, done, _ = env.step(action)\n            if args.render:\n                env.render()\n            policy.rewards.append(reward)\n            if done:\n                break\n\n        running_reward = running_reward * 0.99 + t * 0.01\n        finish_episode()\n        if i_episode % args.log_interval == 0:\n            print(\'Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}\'.format(\n                i_episode, t, running_reward))\n        if running_reward > env.spec.reward_threshold:\n            print(""Solved! Running reward is now {} and ""\n                  ""the last episode runs to {} time steps!"".format(running_reward, t))\n            break\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Char02 Policy Gradient/REINFORCE.py,8,"b'import argparse\nimport gym\nimport numpy as np\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch REINFORCE example\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor (default: 0.99)\')\nparser.add_argument(\'--seed\', type=int, default=543, metavar=\'N\',\n                    help=\'random seed (default: 543)\')\nparser.add_argument(\'--render\', action=\'store_true\',\n                    help=\'render the environment\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                    help=\'interval between training status logs (default: 10)\')\nargs = parser.parse_args()\n\n\nenv = gym.make(\'CartPole-v0\')\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\n\n\nclass Policy(nn.Module):\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.affine2 = nn.Linear(128, 2)\n\n        self.saved_log_probs = []\n        self.rewards = []\n\n    def forward(self, x):\n        x = F.relu(self.affine1(x))\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=1)\n\n\npolicy = Policy()\noptimizer = optim.Adam(policy.parameters(), lr=1e-2)\neps = np.finfo(np.float32).eps.item()\n\n\ndef select_action(state):\n    state = torch.from_numpy(state).float().unsqueeze(0)\n    probs = policy(state)\n    m = Categorical(probs)\n    action = m.sample()\n    policy.saved_log_probs.append(m.log_prob(action))\n    return action.item()\n\n\ndef finish_episode():\n    R = 0\n    policy_loss = []\n    rewards = []\n    for r in policy.rewards[::-1]:\n        R = r + args.gamma * R\n        rewards.insert(0, R)\n    rewards = torch.tensor(rewards)\n    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n        policy_loss.append(-log_prob * reward)\n    optimizer.zero_grad()\n    policy_loss = torch.cat(policy_loss).sum()\n    policy_loss.backward()\n    optimizer.step()\n    del policy.rewards[:]\n    del policy.saved_log_probs[:]\n\n\ndef main():\n    running_reward = 10\n    for i_episode in count(1):\n        state = env.reset()\n        for t in range(10000):  # Don\'t infinite loop while learning\n            action = select_action(state)\n            state, reward, done, _ = env.step(action)\n            if args.render:\n                env.render()\n            policy.rewards.append(reward)\n            if done:\n                break\n\n        running_reward = running_reward * 0.99 + t * 0.01\n        finish_episode()\n        if i_episode % args.log_interval == 0:\n            print(\'Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}\'.format(\n                i_episode, t, running_reward))\n        if running_reward > env.spec.reward_threshold:\n            print(""Solved! Running reward is now {} and ""\n                  ""the last episode runs to {} time steps!"".format(running_reward, t))\n            break\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Char02 Policy Gradient/REINFORCE_with_Baseline.py,8,"b""\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport gym\nimport torch\nfrom torch.distributions import Categorical\nimport torch.optim as optim\nfrom copy import deepcopy\nimport argparse\nimport matplotlib.pyplot as plt\nfrom tensorboardX import SummaryWriter\nfrom torch.nn.utils import clip_grad_norm_\nrender = False\n\n#parser = argparse.ArgumentParser(description='PyTorch REINFORCE example with baseline')\n# parser.add_argument('--render', action='store_true', default=True,\n#                     help='render the environment')\n#args = parser.parse_args()\n\n#\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x9a Net1\xe8\xbe\x93\xe5\x85\xa5state\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xbaaction_prob; Net2 \xe8\xbe\x93\xe5\x85\xa5state,\xe8\xbe\x93\xe5\x87\xba\xe5\x90\x84\xe4\xb8\xaaaction_reward\nclass Policy(nn.Module):\n    def __init__(self,n_states, n_hidden, n_output):\n        super(Policy, self).__init__()\n        self.linear1 = nn.Linear(n_states, n_hidden)\n        self.linear2 = nn.Linear(n_hidden, n_output)\n\n #\xe8\xbf\x99\xe6\x98\xafpolicy\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n        self.reward = []\n        self.log_act_probs = []\n        self.Gt = []\n        self.sigma = []\n#\xe8\xbf\x99\xe6\x98\xafstate_action_func\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n        # self.Reward = []\n        # self.s_value = []\n\n    def forward(self, x):\n        x = F.relu(self.linear1(x))\n        output = F.softmax(self.linear2(x), dim= 1)\n        # self.act_probs.append(action_probs)\n        return output\n\n\n\nenv = gym.make('CartPole-v0')\n# writer = SummaryWriter('./yingyingying')\n# state = env.reset()\nn_states = env.observation_space.shape[0]\nn_actions = env.action_space.n\n\npolicy = Policy(n_states, 128, n_actions)\ns_value_func = Policy(n_states, 128, 1)\n\n\nalpha_theta = 1e-3\noptimizer_theta = optim.Adam(policy.parameters(), lr=alpha_theta)\n# alpha_w = 1e-3  #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n# optimizer_w = optim.Adam(policy.parameters(), lr=alpha_w)\ngamma = 0.99\n\n\n\nseed = 1\nenv.seed(seed)\ntorch.manual_seed(seed)\nlive_time = []\n\ndef loop_episode():\n\n    state = env.reset()\n    if render: env.render()\n    policy_loss = []\n    s_value = []\n    state_sequence = []\n    log_act_prob = []\n    for t in range(1000):\n        state = torch.from_numpy(state).unsqueeze(0).float()  # \xe5\x9c\xa8\xe7\xac\xac0\xe7\xbb\xb4\xe5\xa2\x9e\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x84\xe7\xbb\x87\xe6\x88\x90[N , .....] \xe5\xbd\xa2\xe5\xbc\x8f\n        state_sequence.append(deepcopy(state))\n        action_probs = policy(state)\n        m = Categorical(action_probs)\n        action = m.sample()\n        m_log_prob = m.log_prob(action)\n        log_act_prob.append(m_log_prob)\n        # policy.log_act_probs.append(m_log_prob)\n        action = action.item()\n        next_state, re, done, _ = env.step(action)\n        if render: env.render()\n        policy.reward.append(re)\n        if done:\n            live_time.append(t)\n            break\n        state = next_state\n\n    R = 0\n    Gt = []\n\n    # get Gt value\n    for r in policy.reward[::-1]:\n        R = r + gamma * R\n        Gt.insert(0, R)\n        # s_value_func.sigma.insert(0,sigma)\n        # policy.Gt.insert(0,R)\n\n\n    # update step by step\n    for i in range(len(Gt)):\n\n\n\n        G = Gt[i]\n        V = s_value_func(state_sequence[i])\n        delta = G - V\n\n        # update value network\n        alpha_w = 1e-3  # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n\n        optimizer_w = optim.Adam(policy.parameters(), lr=alpha_w)\n        optimizer_w.zero_grad()\n        policy_loss_w =-delta\n        policy_loss_w.backward(retain_graph = True)\n        clip_grad_norm_(policy_loss_w, 0.1)\n        optimizer_w.step()\n\n        # update policy network\n        optimizer_theta.zero_grad()\n        policy_loss_theta = - log_act_prob[i] * delta\n        policy_loss_theta.backward(retain_graph = True)\n        clip_grad_norm_(policy_loss_theta, 0.1)\n        optimizer_theta.step()\n\n    del policy.log_act_probs[:]\n    del policy.reward[:]\n\n\ndef plot(live_time):\n    plt.ion()\n    plt.grid()\n    plt.plot(live_time, 'g-')\n    plt.xlabel('running step')\n    plt.ylabel('live time')\n    plt.pause(0.000001)\n\n\n\nif __name__ == '__main__':\n\n    #\xe7\x94\x9f\xe6\x88\x90\xe8\x8b\xa5\xe5\xb9\xb2episode\n    # graph_data = torch.autograd.Variable(torch.ones(1,4))\n    # writer.add_graph(policy, (graph_data, ))\n    for i_episode in range(1000):\n        loop_episode()\n        plot(live_time)\n    #policy.plot(live_time)\n"""
Char02 Policy Gradient/Run_Model.py,8,"b'# MountainCar V0\n#\nimport numpy as np\nimport gym\nimport matplotlib.pyplot as plt\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.optim import adam\nfrom torch.distributions import Categorical\n\nenv = gym.make(\'MountainCar-v0\')\nenv = env.unwrapped\nenv.seed(1)\n\ntorch.manual_seed(1)\nplt.ion()\n\n\n#Hyperparameters\nlearning_rate = 0.02\ngamma = 0.995\nepisodes = 1000\n\neps = np.finfo(np.float32).eps.item()\n\naction_space = env.action_space.n\nstate_space = env.observation_space.shape[0]\n\nclass Policy(nn.Module):\n    def __init__(self):\n        super(Policy, self).__init__()\n\n        self.fc1 = nn.Linear(state_space, 20)\n        #self.fc2 = nn.Linear(128,64)\n        self.fc3 = nn.Linear(20, action_space)\n\n        self.gamma = gamma\n        self.saved_log_probs = []\n        self.rewards = []\n\n    def forward(self, x):\n\n        x = F.relu(self.fc1(x))\n        #x = F.relu(self.fc2(x))\n        x = F.softmax(self.fc3(x), dim=1)\n\n        return x\n\npolicy = torch.load(\'policyNet.pkl\')\n\ndef plot(steps):\n    ax = plt.subplot(111)\n    ax.cla()\n    ax.set_title(\'Training\')\n    ax.set_xlabel(\'Episode\')\n    ax.set_ylabel(\'Run Time\')\n    ax.plot(steps)\n    RunTime = len(steps)\n    path =  \'./PG_MountainCar-v0/\'+\'RunTime\'+str(RunTime)+\'.jpg\'\n    if len(steps) % 100 == 0:\n        #plt.savefig(path)\n        pass\n    plt.pause(0.0000001)\n\ndef selct_action(state):\n    state = torch.from_numpy(state).float().unsqueeze(0)\n    probs = policy(state)\n    c = Categorical(probs)\n    action = c.sample()\n\n\n    #policy.saved_log_probs.append(c.log_prob(action))\n    action = action.item()\n    return action\n\ndef run_Model():\n    running_reward = 0\n    steps = []\n    for episode in count(60000):\n        state = env.reset()\n\n        for t in range(10000):\n            action = selct_action(state)\n            state, reward ,done, info = env.step(action)\n            env.render()\n            #policy.rewards.append(reward)\n\n            if done:\n                print(""Episode {}, live time = {}"".format(episode, t))\n                steps.append(t)\n                plot(steps)\n                break\n        if episode % 50 == 0:\n            pass\n            #torch.save(policy, \'policyNet.pkl\')\n\n        running_reward = running_reward * policy.gamma - t*0.01\n        #finish_episode()\n\nif __name__ == \'__main__\':\n    run_Model()'"
Char02 Policy Gradient/naive-policy-gradient.py,10,"b""import numpy as np\nimport gym\nimport matplotlib.pyplot as plt\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.distributions import Categorical\n\n\n#Hyperparameters\nlearning_rate = 0.01\ngamma = 0.98\n\nnum_episode = 5000\nbatch_size = 32\n\n\nenv = gym.make('CartPole-v0')\nstate_space = env.observation_space.shape[0]\naction_space = env.action_space.n\n\ndef plot_durations(episode_durations):\n    plt.ion()\n    plt.figure(2)\n    plt.clf()\n    duration_t = torch.FloatTensor(episode_durations)\n    plt.title('Training')\n    plt.xlabel('Episodes')\n    plt.ylabel('Duration')\n    plt.plot(duration_t.numpy())\n\n    if len(duration_t) >= 100:\n        means = duration_t.unfold(0,100,1).mean(1).view(-1)\n        means = torch.cat((torch.zeros(99), means))\n        plt.plot(means.numpy())\n\n    plt.pause(0.00001)\n\nclass Policy(nn.Module):\n\n    def __init__(self):\n        super(Policy, self).__init__()\n\n        self.state_space = state_space\n        self.action_space = action_space\n\n        self.fc1 = nn.Linear(self.state_space, 128)\n        self.fc2 = nn.Linear(128, self.action_space)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        #x = F.dropout(x, 0.5)\n        x = F.relu(x)\n        x = F.softmax(self.fc2(x), dim=-1)\n\n        return x\n\npolicy = Policy()\noptimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate)\n\n\n\ndef train():\n\n    episode_durations = []\n    #Batch_history\n    state_pool = []\n    action_pool = []\n    reward_pool = []\n    steps = 0\n\n    for episode in range(num_episode):\n        state = env.reset()\n        state = torch.from_numpy(state).float()\n        state = Variable(state)\n\n        env.render()\n\n        for t in count():\n            probs = policy(state)\n            c = Categorical(probs)\n            action = c.sample()\n\n            action = action.data.numpy().astype('int32')\n            next_state, reward, done, info = env.step(action)\n            reward = 0 if done else reward # correct the reward\n            env.render()\n\n            state_pool.append(state)\n            action_pool.append(float(action))\n            reward_pool.append(reward)\n\n            state = next_state\n            state = torch.from_numpy(state).float()\n            state = Variable(state)\n\n            steps += 1\n\n            if done:\n                episode_durations.append(t+1)\n                plot_durations(episode_durations)\n                break\n\n        # update policy\n        if episode >0 and episode % batch_size == 0:\n\n            r = 0\n            '''\n            for i in reversed(range(steps)):\n                if reward_pool[i] == 0:\n                    running_add = 0\n                else:\n                    running_add = running_add * gamma +reward_pool[i]\n                    reward_pool[i] = running_add\n            '''\n            for i in reversed(range(steps)):\n                if reward_pool[i] == 0:\n                    r = 0\n                else:\n                    r = r * gamma + reward_pool[i]\n                    reward_pool[i] = r\n\n            #Normalize reward\n            reward_mean = np.mean(reward_pool)\n            reward_std = np.std(reward_pool)\n            reward_pool = (reward_pool-reward_mean)/reward_std\n\n            #gradiend desent\n            optimizer.zero_grad()\n\n            for i in range(steps):\n                state = state_pool[i]\n                action = Variable(torch.FloatTensor([action_pool[i]]))\n                reward = reward_pool[i]\n\n                probs = policy(state)\n                c = Categorical(probs)\n\n                loss = -c.log_prob(action) * reward\n                loss.backward()\n\n            optimizer.step()\n\n            # clear the batch pool\n            state_pool = []\n            action_pool = []\n            reward_pool = []\n            steps = 0\n\ntrain()\n"""
Char02 Policy Gradient/pytorch_MountainCar-v0.py,9,"b'# MountainCar V0\n\nimport numpy as np\nimport gym\nimport matplotlib.pyplot as plt\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.optim import adam\nfrom torch.distributions import Categorical\n\nenv = gym.make(\'MountainCar-v0\')\nenv = env.unwrapped\nenv.seed(1)\n\ntorch.manual_seed(1)\nplt.ion()\n\n\n#Hyperparameters\nlearning_rate = 0.02\ngamma = 0.995\nepisodes = 1000\n\neps = np.finfo(np.float32).eps.item()\n\naction_space = env.action_space.n\nstate_space = env.observation_space.shape[0]\n\n\nclass Policy(nn.Module):\n    def __init__(self):\n        super(Policy, self).__init__()\n\n        self.fc1 = nn.Linear(state_space, 20)\n        #self.fc2 = nn.Linear(128,64)\n        self.fc3 = nn.Linear(20, action_space)\n\n        self.gamma = gamma\n        self.saved_log_probs = []\n        self.rewards = []\n\n    def forward(self, x):\n\n        x = F.relu(self.fc1(x))\n        #x = F.relu(self.fc2(x))\n        x = F.softmax(self.fc3(x), dim=1)\n\n        return x\n\npolicy = Policy()\noptimizer = adam.Adam(policy.parameters(), lr=learning_rate)\n\ndef selct_action(state):\n    state = torch.from_numpy(state).float().unsqueeze(0)\n    probs = policy(state)\n    c = Categorical(probs)\n    action = c.sample()\n\n\n    policy.saved_log_probs.append(c.log_prob(action))\n    action = action.item()\n    return action\n\ndef finish_episode():\n    R = 0\n    policy_loss = []\n    rewards = []\n\n    for r in policy.rewards[::-1]:\n        R = r + policy.gamma * R\n        rewards.insert(0, R)\n\n    # Formalize reward\n    rewards = torch.tensor(rewards)\n    rewards = (rewards - rewards.mean())/(rewards.std() + eps)\n\n    # get loss\n    for reward, log_prob in zip(rewards, policy.saved_log_probs):\n        policy_loss.append(-log_prob * reward)\n\n    optimizer.zero_grad()\n    policy_loss = torch.cat(policy_loss).sum()\n    policy_loss.backward()\n    optimizer.step()\n\n\n\n    del policy.rewards[:]\n    del policy.saved_log_probs[:]\n\ndef plot(steps):\n    ax = plt.subplot(111)\n    ax.cla()\n    ax.set_title(\'Training\')\n    ax.set_xlabel(\'Episode\')\n    ax.set_ylabel(\'Run Time\')\n    ax.plot(steps)\n    RunTime = len(steps)\n    path =  \'./PG_MountainCar-v0/\'+\'RunTime\'+str(RunTime)+\'.jpg\'\n    if len(steps) % 100 == 0:\n        plt.savefig(path)\n    plt.pause(0.0000001)\n\n\n\ndef main():\n\n    running_reward = 0\n    steps = []\n    for episode in count(60000):\n        state = env.reset()\n\n        for t in range(10000):\n            action = selct_action(state)\n            state, reward ,done, info = env.step(action)\n            env.render()\n            policy.rewards.append(reward)\n\n            if done:\n                print(""Episode {}, live time = {}"".format(episode, t))\n                steps.append(t)\n                plot(steps)\n                break\n        if episode % 50 == 0:\n            torch.save(policy, \'policyNet.pkl\')\n\n        running_reward = running_reward * policy.gamma - t*0.01\n        finish_episode()\n\nif __name__ == \'__main__\':\n    main()\n'"
Char03 Actor-Critic/AC_CartPole-v0.py,10,"b""import gym, os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import count\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n\n#Parameters\nenv = gym.make('CartPole-v0')\nenv = env.unwrapped\n\nenv.seed(1)\ntorch.manual_seed(1)\n\nstate_space = env.observation_space.shape[0]\naction_space = env.action_space.n\n\n\n#Hyperparameters\nlearning_rate = 0.01\ngamma = 0.99\nepisodes = 20000\nrender = False\neps = np.finfo(np.float32).eps.item()\nSavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n\nclass Policy(nn.Module):\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(state_space, 32)\n\n        self.action_head = nn.Linear(32, action_space)\n        self.value_head = nn.Linear(32, 1) # Scalar Value\n\n        self.save_actions = []\n        self.rewards = []\n        os.makedirs('./AC_CartPole-v0', exist_ok=True)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        action_score = self.action_head(x)\n        state_value = self.value_head(x)\n\n        return F.softmax(action_score, dim=-1), state_value\n\nmodel = Policy()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\ndef plot(steps):\n    ax = plt.subplot(111)\n    ax.cla()\n    ax.grid()\n    ax.set_title('Training')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Run Time')\n    ax.plot(steps)\n    RunTime = len(steps)\n\n    path = './AC_CartPole-v0/' + 'RunTime' + str(RunTime) + '.jpg'\n    if len(steps) % 200 == 0:\n        plt.savefig(path)\n    plt.pause(0.0000001)\n\ndef select_action(state):\n    state = torch.from_numpy(state).float()\n    probs, state_value = model(state)\n    m = Categorical(probs)\n    action = m.sample()\n    model.save_actions.append(SavedAction(m.log_prob(action), state_value))\n\n    return action.item()\n\n\ndef finish_episode():\n    R = 0\n    save_actions = model.save_actions\n    policy_loss = []\n    value_loss = []\n    rewards = []\n\n    for r in model.rewards[::-1]:\n        R = r + gamma * R\n        rewards.insert(0, R)\n\n    rewards = torch.tensor(rewards)\n    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n\n    for (log_prob , value), r in zip(save_actions, rewards):\n        reward = r - value.item()\n        policy_loss.append(-log_prob * reward)\n        value_loss.append(F.smooth_l1_loss(value, torch.tensor([r])))\n\n    optimizer.zero_grad()\n    loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n    loss.backward()\n    optimizer.step()\n\n    del model.rewards[:]\n    del model.save_actions[:]\n\ndef main():\n    running_reward = 10\n    live_time = []\n    for i_episode in count(episodes):\n        state = env.reset()\n        for t in count():\n            action = select_action(state)\n            state, reward, done, info = env.step(action)\n            if render: env.render()\n            model.rewards.append(reward)\n\n            if done or t >= 1000:\n                break\n        running_reward = running_reward * 0.99 + t * 0.01\n        live_time.append(t)\n        plot(live_time)\n        if i_episode % 100 == 0:\n            modelPath = './AC_CartPole_Model/ModelTraing'+str(i_episode)+'Times.pkl'\n            torch.save(model, modelPath)\n        finish_episode()\n\nif __name__ == '__main__':\n    main()\n"""
Char03 Actor-Critic/AC_MountainCar-v0.py,11,"b'import gym, os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import count\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch.distributions import Categorical\nfrom torch.nn.functional import  smooth_l1_loss\n\n#Hyperparameters\nLEARNING_RATE = 0.01\nGAMMA = 0.995\nNUM_EPISODES = 50000\nRENDER = False\n#env info\nenv = gym.make(\'MountainCar-v0\')\nenv = env.unwrapped\n\nenv.seed(1)\ntorch.manual_seed(1)\n\nnum_state = env.observation_space.shape[0]\nnum_action = env.action_space.n\neps = np.finfo(np.float32).eps.item()\nplt.ion()\nsaveAction = namedtuple(\'SavedActions\',[\'probs\', \'action_values\'])\n\nclass Module(nn.Module):\n    def __init__(self):\n        super(Module, self).__init__()\n        self.fc1 = nn.Linear(num_state, 128)\n        #self.fc2 = nn.Linear(64, 128)\n\n        self.action_head = nn.Linear(128, num_action)\n        self.value_head = nn.Linear(128, 1)\n        self.policy_action_value = []\n        self.rewards = []\n\n        self.gamma = GAMMA\n        os.makedirs(\'/AC_MountainCar-v0_Model/\', exist_ok=True)\n\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        #x = F.relu(self.fc2(x))\n\n        probs = F.softmax(self.action_head(x))\n        value = self.value_head(x)\n        return probs, value\n\npolicy = Module()\noptimizer = Adam(policy.parameters(), lr=LEARNING_RATE)\n\ndef plot(steps):\n    ax = plt.subplot(111)\n    ax.cla()\n    ax.grid()\n    ax.set_title(\'Training\')\n    ax.set_xlabel(\'Episode\')\n    ax.set_ylabel(\'Run Time\')\n    ax.plot(steps)\n    RunTime = len(steps)\n    path = \'./AC_MountainCar-v0/\' + \'RunTime\' + str(RunTime) + \'.jpg\'\n    if len(steps) % 200 == 0:\n        plt.savefig(path)\n    plt.pause(0.0000001)\n\n\ndef select_action(state):\n    state = torch.from_numpy(state).float()\n    probs, value = policy(state)\n    c = Categorical(probs)\n    action = c.sample()\n    log_prob = c.log_prob(action)\n\n\n    policy.policy_action_value.append(saveAction(log_prob, value))\n    action = action.item()\n    return action\n\n\ndef finish_episode():\n    rewards = []\n    saveActions = policy.policy_action_value\n    policy_loss = []\n    value_loss = []\n    R = 0\n\n    for r in policy.rewards[::-1]:\n        R = r + policy.gamma * R\n        rewards.insert(0, R)\n\n    # Normalize the reward\n    rewards = torch.tensor(rewards)\n    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n\n    #Figure out loss\n    for (log_prob, value), r in zip(saveActions, rewards):\n        reward = r - value.item()\n        policy_loss.append(-log_prob * reward)\n        value_loss.append(smooth_l1_loss(value, torch.tensor([r]) ))\n\n    optimizer.zero_grad()\n    loss = torch.stack(policy_loss).sum() + torch.stack(policy_loss).sum()\n    loss.backward()\n    optimizer.step()\n\n    del policy.rewards[:]\n    del policy.policy_action_value[:]\n\n\ndef main():\n    run_steps = []\n    for i_episode in range(NUM_EPISODES):\n        state = env.reset()\n        if RENDER: env.render()\n\n        for t in count():\n            action = select_action(state)\n            state , reward, done, _ = env.step(action)\n            reward = state[0] + reward\n            if RENDER: env.render()\n            policy.rewards.append(reward)\n\n            if done:\n                run_steps.append(t)\n                print(""Epiosde {} , run step is {} "".format(i_episode+1 , t+1))\n                break\n\n        finish_episode()\n        plot(run_steps)\n\n        if i_episode % 100 == 0 and i_episode !=0:\n\n            modelPath = \'./AC_MountainCar-v0_Model/ModelTraing\' + str(i_episode) + \'Times.pkl\'\n            torch.save(policy, modelPath)\n\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Char04 A2C/A2C.py,14,"b'\nimport math\nimport random\n\nimport gym\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n\nimport matplotlib.pyplot as plt\n\nuse_cuda = torch.cuda.is_available()\ndevice   = torch.device(""cuda"" if use_cuda else ""cpu"")\n\nfrom multiprocessing_env import SubprocVecEnv\n\nnum_envs = 8\nenv_name = ""CartPole-v0""\n\ndef make_env():\n    def _thunk():\n        env = gym.make(env_name)\n        return env\n    return _thunk\n\nplt.ion()\nenvs = [make_env() for i in range(num_envs)]\nenvs = SubprocVecEnv(envs) # 8 env\n\nenv = gym.make(env_name) # a single env\n\nclass ActorCritic(nn.Module):\n    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n        super(ActorCritic, self).__init__()\n        \n        self.critic = nn.Sequential(\n            nn.Linear(num_inputs, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1)\n        )\n        \n        self.actor = nn.Sequential(\n            nn.Linear(num_inputs, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, num_outputs),\n            nn.Softmax(dim=1),\n        )\n        \n    def forward(self, x):\n        value = self.critic(x)\n        probs = self.actor(x)\n        dist  = Categorical(probs)\n        return dist, value\n\n\ndef test_env(vis=False):\n    state = env.reset()\n    if vis: env.render()\n    done = False\n    total_reward = 0\n    while not done:\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        dist, _ = model(state)\n        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n        state = next_state\n        if vis: env.render()\n        total_reward += reward\n    return total_reward\n\n\ndef compute_returns(next_value, rewards, masks, gamma=0.99):\n    R = next_value\n    returns = []\n    for step in reversed(range(len(rewards))):\n        R = rewards[step] + gamma * R * masks[step]\n        returns.insert(0, R)\n    return returns\n\ndef plot(frame_idx, rewards):\n    plt.plot(rewards,\'b-\')\n    plt.title(\'frame %s. reward: %s\' % (frame_idx, rewards[-1]))\n    plt.pause(0.0001)\n\n\nnum_inputs  = envs.observation_space.shape[0]\nnum_outputs = envs.action_space.n\n\n#Hyper params:\nhidden_size = 256\nlr          = 1e-3\nnum_steps   = 5\n\nmodel = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\noptimizer = optim.Adam(model.parameters())\n\n\nmax_frames   = 20000\nframe_idx    = 0\ntest_rewards = []\n\n\nstate = envs.reset()\n\nwhile frame_idx < max_frames:\n\n    log_probs = []\n    values    = []\n    rewards   = []\n    masks     = []\n    entropy = 0\n\n    # rollout trajectory\n    for _ in range(num_steps):\n        state = torch.FloatTensor(state).to(device)\n        dist, value = model(state)\n\n        action = dist.sample()\n        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n\n        log_prob = dist.log_prob(action)\n        entropy += dist.entropy().mean()\n        \n        log_probs.append(log_prob)\n        values.append(value)\n        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n        \n        state = next_state\n        frame_idx += 1\n        \n        if frame_idx % 100 == 0:\n            test_rewards.append(np.mean([test_env() for _ in range(10)]))\n            plot(frame_idx, test_rewards)\n            \n    next_state = torch.FloatTensor(next_state).to(device)\n    _, next_value = model(next_state)\n    returns = compute_returns(next_value, rewards, masks)\n    \n    log_probs = torch.cat(log_probs)\n    returns   = torch.cat(returns).detach()\n    values    = torch.cat(values)\n\n    advantage = returns - values\n\n    actor_loss  = -(log_probs * advantage.detach()).mean()\n    critic_loss = advantage.pow(2).mean()\n\n    loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n#test_env(True)\n'"
Char04 A2C/multiprocessing_env.py,0,"b'#This code is from openai baseline\n#https://github.com/openai/baselines/tree/master/baselines/common/vec_env\n\nimport numpy as np\nfrom multiprocessing import Process, Pipe\n\ndef worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.x()\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \'step\':\n            ob, reward, done, info = env.step(data)\n            if done:\n                ob = env.reset()\n            remote.send((ob, reward, done, info))\n        elif cmd == \'reset\':\n            ob = env.reset()\n            remote.send(ob)\n        elif cmd == \'reset_task\':\n            ob = env.reset_task()\n            remote.send(ob)\n        elif cmd == \'close\':\n            remote.close()\n            break\n        elif cmd == \'get_spaces\':\n            remote.send((env.observation_space, env.action_space))\n        else:\n            raise NotImplementedError\n\nclass VecEnv(object):\n    """"""\n    An abstract asynchronous, vectorized environment.\n    """"""\n    def __init__(self, num_envs, observation_space, action_space):\n        self.num_envs = num_envs\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    def reset(self):\n        """"""\n        Reset all the environments and return an array of\n        observations, or a tuple of observation arrays.\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n        """"""\n        pass\n\n    def step_async(self, actions):\n        """"""\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n        You should not call this if a step_async run is\n        already pending.\n        """"""\n        pass\n\n    def step_wait(self):\n        """"""\n        Wait for the step taken with step_async().\n        Returns (obs, rews, dones, infos):\n         - obs: an array of observations, or a tuple of\n                arrays of observations.\n         - rews: an array of rewards\n         - dones: an array of ""episode done"" booleans\n         - infos: a sequence of info objects\n        """"""\n        pass\n\n    def close(self):\n        """"""\n        Clean up the environments\' resources.\n        """"""\n        pass\n\n    def step(self, actions):\n        self.step_async(actions)\n        return self.step_wait()\n\n    \nclass CloudpickleWrapper(object):\n    """"""\n    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n    """"""\n    def __init__(self, x):\n        self.x = x\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n\n        \nclass SubprocVecEnv(VecEnv):\n    def __init__(self, env_fns, spaces=None):\n        """"""\n        envs: list of gym environments to run in subprocesses\n        """"""\n        self.waiting = False\n        self.closed = False\n        nenvs = len(env_fns)\n        self.nenvs = nenvs\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n        for p in self.ps:\n            p.daemon = True # if the main process crashes, we should not cause things to hang\n            p.start()\n        for remote in self.work_remotes:\n            remote.close()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        observation_space, action_space = self.remotes[0].recv()\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n\n    def step_async(self, actions):\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        self.waiting = True\n\n    def step_wait(self):\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def reset_task(self):\n        for remote in self.remotes:\n            remote.send((\'reset_task\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def close(self):\n        if self.closed:\n            return\n        if self.waiting:\n            for remote in self.remotes:            \n                remote.recv()\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n            self.closed = True\n            \n    def __len__(self):\n        return self.nenvs\n'"
Char05 DDPG/DDPG.py,19,"b'import argparse\nfrom itertools import count\n\nimport os, sys, random\nimport numpy as np\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nfrom tensorboardX import SummaryWriter\n\n\'\'\'\nImplementation of Deep Deterministic Policy Gradients (DDPG) with pytorch \nriginal paper: https://arxiv.org/abs/1509.02971\nNot the author\'s implementation !\n\'\'\'\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--mode\', default=\'train\', type=str) # mode = \'train\' or \'test\'\n# OpenAI gym environment name, # [\'BipedalWalker-v2\', \'Pendulum-v0\'] or any continuous environment\n# Note that DDPG is feasible about hyper-parameters.\n# You should fine-tuning if you change to another environment.\nparser.add_argument(""--env_name"", default=""Pendulum-v0"")\nparser.add_argument(\'--tau\',  default=0.005, type=float) # target smoothing coefficient\nparser.add_argument(\'--target_update_interval\', default=1, type=int)\nparser.add_argument(\'--test_iteration\', default=10, type=int)\n\nparser.add_argument(\'--learning_rate\', default=1e-4, type=float)\nparser.add_argument(\'--gamma\', default=0.99, type=int) # discounted factor\nparser.add_argument(\'--capacity\', default=1000000, type=int) # replay buffer size\nparser.add_argument(\'--batch_size\', default=100, type=int) # mini batch size\nparser.add_argument(\'--seed\', default=False, type=bool)\nparser.add_argument(\'--random_seed\', default=9527, type=int)\n# optional parameters\n\nparser.add_argument(\'--sample_frequency\', default=2000, type=int)\nparser.add_argument(\'--render\', default=False, type=bool) # show UI or not\nparser.add_argument(\'--log_interval\', default=50, type=int) #\nparser.add_argument(\'--load\', default=False, type=bool) # load model\nparser.add_argument(\'--render_interval\', default=100, type=int) # after render_interval, the env.render() will work\nparser.add_argument(\'--exploration_noise\', default=0.1, type=float)\nparser.add_argument(\'--max_episode\', default=100000, type=int) # num of games\nparser.add_argument(\'--print_log\', default=5, type=int)\nparser.add_argument(\'--update_iteration\', default=200, type=int)\nargs = parser.parse_args()\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nscript_name = os.path.basename(__file__)\nenv = gym.make(args.env_name)\n\nif args.seed:\n    env.seed(args.random_seed)\n    torch.manual_seed(args.random_seed)\n    np.random.seed(args.random_seed)\n\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.shape[0]\nmax_action = float(env.action_space.high[0])\nmin_Val = torch.tensor(1e-7).float().to(device) # min value\n\ndirectory = \'./exp\' + script_name + args.env_name +\'./\'\n\nclass Replay_buffer():\n    \'\'\'\n    Code based on:\n    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n    Expects tuples of (state, next_state, action, reward, done)\n    \'\'\'\n    def __init__(self, max_size=args.capacity):\n        self.storage = []\n        self.max_size = max_size\n        self.ptr = 0\n\n    def push(self, data):\n        if len(self.storage) == self.max_size:\n            self.storage[int(self.ptr)] = data\n            self.ptr = (self.ptr + 1) % self.max_size\n        else:\n            self.storage.append(data)\n\n    def sample(self, batch_size):\n        ind = np.random.randint(0, len(self.storage), size=batch_size)\n        x, y, u, r, d = [], [], [], [], []\n\n        for i in ind:\n            X, Y, U, R, D = self.storage[i]\n            x.append(np.array(X, copy=False))\n            y.append(np.array(Y, copy=False))\n            u.append(np.array(U, copy=False))\n            r.append(np.array(R, copy=False))\n            d.append(np.array(D, copy=False))\n\n        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1)\n\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n\n        self.l1 = nn.Linear(state_dim, 400)\n        self.l2 = nn.Linear(400, 300)\n        self.l3 = nn.Linear(300, action_dim)\n\n        self.max_action = max_action\n\n    def forward(self, x):\n        x = F.relu(self.l1(x))\n        x = F.relu(self.l2(x))\n        x = self.max_action * torch.tanh(self.l3(x))\n        return x\n\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n\n        self.l1 = nn.Linear(state_dim + action_dim, 400)\n        self.l2 = nn.Linear(400 , 300)\n        self.l3 = nn.Linear(300, 1)\n\n    def forward(self, x, u):\n        x = F.relu(self.l1(torch.cat([x, u], 1)))\n        x = F.relu(self.l2(x))\n        x = self.l3(x)\n        return x\n\n\nclass DDPG(object):\n    def __init__(self, state_dim, action_dim, max_action):\n        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n\n        self.critic = Critic(state_dim, action_dim).to(device)\n        self.critic_target = Critic(state_dim, action_dim).to(device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n        self.replay_buffer = Replay_buffer()\n        self.writer = SummaryWriter(directory)\n\n        self.num_critic_update_iteration = 0\n        self.num_actor_update_iteration = 0\n        self.num_training = 0\n\n    def select_action(self, state):\n        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n        return self.actor(state).cpu().data.numpy().flatten()\n\n    def update(self):\n\n        for it in range(args.update_iteration):\n            # Sample replay buffer\n            x, y, u, r, d = self.replay_buffer.sample(args.batch_size)\n            state = torch.FloatTensor(x).to(device)\n            action = torch.FloatTensor(u).to(device)\n            next_state = torch.FloatTensor(y).to(device)\n            done = torch.FloatTensor(1-d).to(device)\n            reward = torch.FloatTensor(r).to(device)\n\n            # Compute the target Q value\n            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n            target_Q = reward + (done * args.gamma * target_Q).detach()\n\n            # Get current Q estimate\n            current_Q = self.critic(state, action)\n\n            # Compute critic loss\n            critic_loss = F.mse_loss(current_Q, target_Q)\n            self.writer.add_scalar(\'Loss/critic_loss\', critic_loss, global_step=self.num_critic_update_iteration)\n            # Optimize the critic\n            self.critic_optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic_optimizer.step()\n\n            # Compute actor loss\n            actor_loss = -self.critic(state, self.actor(state)).mean()\n            self.writer.add_scalar(\'Loss/actor_loss\', actor_loss, global_step=self.num_actor_update_iteration)\n\n            # Optimize the actor\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            # Update the frozen target models\n            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n\n            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n\n            self.num_actor_update_iteration += 1\n            self.num_critic_update_iteration += 1\n\n    def save(self):\n        torch.save(self.actor.state_dict(), directory + \'actor.pth\')\n        torch.save(self.critic.state_dict(), directory + \'critic.pth\')\n        # print(""===================================="")\n        # print(""Model has been saved..."")\n        # print(""===================================="")\n\n    def load(self):\n        self.actor.load_state_dict(torch.load(directory + \'actor.pth\'))\n        self.critic.load_state_dict(torch.load(directory + \'critic.pth\'))\n        print(""===================================="")\n        print(""model has been loaded..."")\n        print(""===================================="")\n\ndef main():\n    agent = DDPG(state_dim, action_dim, max_action)\n    ep_r = 0\n    if args.mode == \'test\':\n        agent.load()\n        for i in range(args.test_iteration):\n            state = env.reset()\n            for t in count():\n                action = agent.select_action(state)\n                next_state, reward, done, info = env.step(np.float32(action))\n                ep_r += reward\n                env.render()\n                if done or t >= args.max_length_of_trajectory:\n                    print(""Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}"".format(i, ep_r, t))\n                    ep_r = 0\n                    break\n                state = next_state\n\n    elif args.mode == \'train\':\n        if args.load: agent.load()\n        total_step = 0\n        for i in range(args.max_episode):\n            total_reward = 0\n            step =0\n            state = env.reset()\n            for t in count():\n                action = agent.select_action(state)\n                action = (action + np.random.normal(0, args.exploration_noise, size=env.action_space.shape[0])).clip(\n                    env.action_space.low, env.action_space.high)\n\n                next_state, reward, done, info = env.step(action)\n                if args.render and i >= args.render_interval : env.render()\n                agent.replay_buffer.push((state, next_state, action, reward, np.float(done)))\n\n                state = next_state\n                if done:\n                    break\n                step += 1\n                total_reward += reward\n            total_step += step+1\n            print(""Total T:{} Episode: \\t{} Total Reward: \\t{:0.2f}"".format(total_step, i, total_reward))\n            agent.update()\n           # ""Total T: %d Episode Num: %d Episode T: %d Reward: %f\n\n            if i % args.log_interval == 0:\n                agent.save()\n    else:\n        raise NameError(""mode wrong!!!"")\n\nif __name__ == \'__main__\':\n    main()\n'"
Char07 PPO/PPO2.py,21,"b'\nimport argparse\nimport pickle\nfrom collections import namedtuple\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n\n# Parameters\nparser = argparse.ArgumentParser(description=\'Solve the Pendulum-v0 with PPO\')\nparser.add_argument(\n    \'--gamma\', type=float, default=0.9, metavar=\'G\', help=\'discount factor (default: 0.9)\')\nparser.add_argument(\'--seed\', type=int, default=0, metavar=\'N\', help=\'random seed (default: 0)\')\nparser.add_argument(\'--render\', action=\'store_true\', default=True, help=\'render the environment\')\nparser.add_argument(\n    \'--log-interval\',\n    type=int,\n    default=10,\n    metavar=\'N\',\n    help=\'interval between training status logs (default: 10)\')\nargs = parser.parse_args()\n\nenv = gym.make(\'Pendulum-v0\').unwrapped\nnum_state = env.observation_space.shape[0]\nnum_action = env.action_space.shape[0]\ntorch.manual_seed(args.seed)\nenv.seed(args.seed)\n\nTransition = namedtuple(\'Transition\',[\'state\', \'aciton\', \'reward\', \'a_log_prob\', \'next_state\'])\nTrainRecord = namedtuple(\'TrainRecord\',[\'episode\', \'reward\'])\n\nclass Actor(nn.Module):\n    def __init__(self):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_state, 64)\n        self.fc2 = nn.Linear(64,8)\n        self.mu_head = nn.Linear(8, 1)\n        self.sigma_head = nn.Linear(8, 1)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x))\n        x = F.leaky_relu(self.fc2(x))\n\n        mu = self.mu_head(x)\n        sigma = self.sigma_head(x)\n\n        return mu, sigma\n\nclass Critic(nn.Module):\n    def __init__(self):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(num_state, 64)\n        self.fc2 = nn.Linear(64, 8)\n        self.state_value= nn.Linear(8, 1)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x))\n        x = F.leaky_relu(self.fc2(x))\n        value = self.state_value(x)\n        return value\n\nclass PPO():\n    clip_param = 0.2\n    max_grad_norm = 0.5\n    ppo_epoch = 10\n    buffer_capacity = 1000\n    batch_size = 8\n\n    def __init__(self):\n        super(PPO, self).__init__()\n        self.actor_net = Actor().float()\n        self.critic_net = Critic().float()\n        self.buffer = []\n        self.counter = 0\n        self.training_step = 0\n\n        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), 1e-3)\n        self.critic_net_optimizer = optim.Adam(self.critic_net.parameters(), 4e-3)\n        if not os.path.exists(\'../param\'):\n            os.makedirs(\'../param/net_param\')\n            os.makedirs(\'../param/img\')\n\n    def select_action(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        with torch.no_grad():\n            mu, sigma = self.actor_net(state)\n        dist = Normal(mu, sigma)\n        action = dist.sample()\n        action_log_prob = dist.log_prob(action)\n        action = action.clamp(-2, 2)\n        return action.item(), action_log_prob.item()\n\n\n    def get_value(self, state):\n        state = torch.from_numpy(state)\n        with torch.no_grad():\n            value = self.critic_net(state)\n        return value.item()\n\n    def save_param(self):\n        torch.save(self.actor_net.state_dict(), \'../param/net_param/actor_net\'+str(time.time())[:10],+\'.pkl\')\n        torch.save(self.critic_net.state_dict(), \'../param/net_param/critic_net\'+str(time.time())[:10],+\'.pkl\')\n\n    def store_transition(self, transition):\n        self.buffer.append(transition)\n        self.counter+=1\n        return counter % self.buffer_capacity == 0\n\n    def update(self):\n        self.training_step +=1\n\n        state = torch.tensor([t.state for t in self.buffer ], dtype=torch.float)\n        action = torch.tensor([t.action for t in self.buffer], dtype=torch.float).view(-1, 1)\n        reward = torch.tensor([t.reward for t in self.buffer], dtype=torch.float).view(-1, 1)\n        next_state = torch.tensor([t.next_state for t in self.buffer], dtype=torch.float)\n        old_action_log_prob = torch.tensor([t.a_log_prob for t in self.buffer], dtype=torch.float).view(-1, 1)\n\n        reward = (reward - reward.mean())/(reward.std() + 1e-10)\n        with torch.no_grad():\n            target_v = reward + args.gamma * self.critic_net(next_state)\n\n        advantage = (target_v - self.critic_net(state)).detach()\n        for _ in range(self.ppo_epoch): # iteration ppo_epoch \n            for index in BatchSampler(SubsetRandomSampler(range(self.buffer_capacity), self.batch_size, True)):\n                # epoch iteration, PPO core!!!\n                mu, sigma = self.actor_net(state[index])\n                n = Normal(mu, sigma)\n                action_log_prob = n.log_prob(action[index])\n                ratio = torch.exp(action_log_prob - old_action_log_prob)\n                \n                L1 = ratio * advantage[index]\n                L2 = torch.clamp(ratio, 1-self.clip_param, 1+self.clip_param) * advantage[index]\n                action_loss = -torch.min(L1, L2).mean() # MAX->MIN desent\n                self.actor_optimizer.zero_grad()\n                action_loss.backward()\n                nn.utils.clip_grad_norm_(self.actor_net.parameters(), self.max_grad_norm)\n                self.actor_optimizer.step()\n\n                value_loss = F.smooth_l1_loss(self.critic_net(state[index]), target_v[index])\n                self.critic_net_optimizer.zero_grad()\n                value_loss.backward()\n                nn.utils.clip_grad_norm_(self.critic_net.parameters(), self.max_grad_norm)\n                self.critic_net_optimizer.step()\n\n        del self.buffer[:]\n\ndef main():\n\n    agent = PPO()\n\n    training_records = []\n    running_reward = -1000\n\n    for i_epoch in range(1000):\n        score = 0\n        state = env.reset()\n        if args.render: env.render()\n        for t in range(200):\n            action, action_log_prob = agent.select_action(state)\n            next_state, reward, done, info = env.step(action)\n            trans = Transition(state, action, reward, action_log_prob, next_state)\n            if args.render: env.render()\n            if agent.store_transition(trans):\n                agent.update()\n            score += reward\n            state = next_state\n\n        running_reward = running_reward * 0.9 + score * 0.1\n        training_records.append(TrainingRecord(i_epoch, running_reward))\n        if i_epoch % 10 ==0:\n            print(""Epoch {}, Moving average score is: {:.2f} "".format(i_epoch, running_reward))\n        if running_reward > -200:\n            print(""Solved! Moving average score is now {}!"".format(running_reward))\n            env.close()\n            agent.save_param()\n            break\n\nif __name__ == \'__main__\':\n    main()\n'"
Char07 PPO/PPO_CartPole_v0.py,21,"b'import argparse\nimport pickle\nfrom collections import namedtuple\nfrom itertools import count\n\nimport os, time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal, Categorical\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\nfrom tensorboardX import SummaryWriter\n\n# Parameters\ngamma = 0.99\nrender = False\nseed = 1\nlog_interval = 10\n\nenv = gym.make(\'CartPole-v0\').unwrapped\nnum_state = env.observation_space.shape[0]\nnum_action = env.action_space.n\ntorch.manual_seed(seed)\nenv.seed(seed)\nTransition = namedtuple(\'Transition\', [\'state\', \'action\',  \'a_log_prob\', \'reward\', \'next_state\'])\n\nclass Actor(nn.Module):\n    def __init__(self):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_state, 100)\n        self.action_head = nn.Linear(100, num_action)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        action_prob = F.softmax(self.action_head(x), dim=1)\n        return action_prob\n\n\nclass Critic(nn.Module):\n    def __init__(self):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(num_state, 100)\n        self.state_value = nn.Linear(100, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        value = self.state_value(x)\n        return value\n\n\nclass PPO():\n    clip_param = 0.2\n    max_grad_norm = 0.5\n    ppo_update_time = 10\n    buffer_capacity = 1000\n    batch_size = 32\n\n    def __init__(self):\n        super(PPO, self).__init__()\n        self.actor_net = Actor()\n        self.critic_net = Critic()\n        self.buffer = []\n        self.counter = 0\n        self.training_step = 0\n        self.writer = SummaryWriter(\'../exp\')\n\n        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), 1e-3)\n        self.critic_net_optimizer = optim.Adam(self.critic_net.parameters(), 3e-3)\n        if not os.path.exists(\'../param\'):\n            os.makedirs(\'../param/net_param\')\n            os.makedirs(\'../param/img\')\n\n    def select_action(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        with torch.no_grad():\n            action_prob = self.actor_net(state)\n        c = Categorical(action_prob)\n        action = c.sample()\n        return action.item(), action_prob[:,action.item()].item()\n\n    def get_value(self, state):\n        state = torch.from_numpy(state)\n        with torch.no_grad():\n            value = self.critic_net(state)\n        return value.item()\n\n    def save_param(self):\n        torch.save(self.actor_net.state_dict(), \'../param/net_param/actor_net\' + str(time.time())[:10], +\'.pkl\')\n        torch.save(self.critic_net.state_dict(), \'../param/net_param/critic_net\' + str(time.time())[:10], +\'.pkl\')\n\n    def store_transition(self, transition):\n        self.buffer.append(transition)\n        self.counter += 1\n\n\n    def update(self, i_ep):\n        state = torch.tensor([t.state for t in self.buffer], dtype=torch.float)\n        action = torch.tensor([t.action for t in self.buffer], dtype=torch.long).view(-1, 1)\n        reward = [t.reward for t in self.buffer]\n        # update: don\'t need next_state\n        #reward = torch.tensor([t.reward for t in self.buffer], dtype=torch.float).view(-1, 1)\n        #next_state = torch.tensor([t.next_state for t in self.buffer], dtype=torch.float)\n        old_action_log_prob = torch.tensor([t.a_log_prob for t in self.buffer], dtype=torch.float).view(-1, 1)\n\n        R = 0\n        Gt = []\n        for r in reward[::-1]:\n            R = r + gamma * R\n            Gt.insert(0, R)\n        Gt = torch.tensor(Gt, dtype=torch.float)\n        #print(""The agent is updateing...."")\n        for i in range(self.ppo_update_time):\n            for index in BatchSampler(SubsetRandomSampler(range(len(self.buffer))), self.batch_size, False):\n                if self.training_step % 1000 ==0:\n                    print(\'I_ep {} \xef\xbc\x8ctrain {} times\'.format(i_ep,self.training_step))\n                #with torch.no_grad():\n                Gt_index = Gt[index].view(-1, 1)\n                V = self.critic_net(state[index])\n                delta = Gt_index - V\n                advantage = delta.detach()\n                # epoch iteration, PPO core!!!\n                action_prob = self.actor_net(state[index]).gather(1, action[index]) # new policy\n\n                ratio = (action_prob/old_action_log_prob[index])\n                surr1 = ratio * advantage\n                surr2 = torch.clamp(ratio, 1 - self.clip_param, 1 + self.clip_param) * advantage\n\n                # update actor network\n                action_loss = -torch.min(surr1, surr2).mean()  # MAX->MIN desent\n                self.writer.add_scalar(\'loss/action_loss\', action_loss, global_step=self.training_step)\n                self.actor_optimizer.zero_grad()\n                action_loss.backward()\n                nn.utils.clip_grad_norm_(self.actor_net.parameters(), self.max_grad_norm)\n                self.actor_optimizer.step()\n\n                #update critic network\n                value_loss = F.mse_loss(Gt_index, V)\n                self.writer.add_scalar(\'loss/value_loss\', value_loss, global_step=self.training_step)\n                self.critic_net_optimizer.zero_grad()\n                value_loss.backward()\n                nn.utils.clip_grad_norm_(self.critic_net.parameters(), self.max_grad_norm)\n                self.critic_net_optimizer.step()\n                self.training_step += 1\n\n        del self.buffer[:] # clear experience\n\n    \ndef main():\n    agent = PPO()\n    for i_epoch in range(1000):\n        state = env.reset()\n        if render: env.render()\n\n        for t in count():\n            action, action_prob = agent.select_action(state)\n            next_state, reward, done, _ = env.step(action)\n            trans = Transition(state, action, action_prob, reward, next_state)\n            if render: env.render()\n            agent.store_transition(trans)\n            state = next_state\n\n            if done :\n                if len(agent.buffer) >= agent.batch_size:agent.update(i_epoch)\n                agent.writer.add_scalar(\'liveTime/livestep\', t, global_step=i_epoch)\n                break\n\nif __name__ == \'__main__\':\n    main()\n    print(""end"")\n'"
Char07 PPO/PPO_MountainCar-v0.py,21,"b'import argparse\nimport pickle\nfrom collections import namedtuple\nfrom itertools import count\n\nimport os, time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal, Categorical\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\nfrom tensorboardX import SummaryWriter\n\n# Parameters\nenv_name = \'MountainCar-v0\'\ngamma = 0.99\nrender = False\nseed = 1\nlog_interval = 10\n\nenv = gym.make(env_name).unwrapped\nnum_state = env.observation_space.shape[0]\nnum_action = env.action_space.n\ntorch.manual_seed(seed)\nenv.seed(seed)\nTransition = namedtuple(\'Transition\', [\'state\', \'action\', \'a_log_prob\', \'reward\', \'next_state\'])\n\n\nclass Actor(nn.Module):\n    def __init__(self):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(num_state, 128)\n        self.action_head = nn.Linear(128, num_action)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        action_prob = F.softmax(self.action_head(x), dim=1)\n        return action_prob\n\n\nclass Critic(nn.Module):\n    def __init__(self):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(num_state, 128)\n        self.state_value = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        value = self.state_value(x)\n        return value\n\n\nclass PPO():\n    clip_param = 0.2\n    max_grad_norm = 0.5\n    ppo_update_time = 10\n    buffer_capacity = 8000\n    batch_size = 32\n\n    def __init__(self):\n        super(PPO, self).__init__()\n        self.actor_net = Actor()\n        self.critic_net = Critic()\n        self.buffer = []\n        self.counter = 0\n        self.training_step = 0\n        self.writer = SummaryWriter(\'../exp\')\n\n        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), 1e-3)\n        self.critic_net_optimizer = optim.Adam(self.critic_net.parameters(), 3e-3)\n        if not os.path.exists(\'../param\'):\n            os.makedirs(\'../param/net_param\')\n            os.makedirs(\'../param/img\')\n\n    def select_action(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        with torch.no_grad():\n            action_prob = self.actor_net(state)\n        c = Categorical(action_prob)\n        action = c.sample()\n        return action.item(), action_prob[:, action.item()].item()\n\n    def get_value(self, state):\n        state = torch.from_numpy(state)\n        with torch.no_grad():\n            value = self.critic_net(state)\n        return value.item()\n\n    def save_param(self):\n        torch.save(self.actor_net.state_dict(), \'../param/net_param/actor_net\' + str(time.time())[:10], +\'.pkl\')\n        torch.save(self.critic_net.state_dict(), \'../param/net_param/critic_net\' + str(time.time())[:10], +\'.pkl\')\n\n    def store_transition(self, transition):\n        self.buffer.append(transition)\n        self.counter += 1\n\n    def update(self, i_ep):\n        state = torch.tensor([t.state for t in self.buffer], dtype=torch.float)\n        action = torch.tensor([t.action for t in self.buffer], dtype=torch.long).view(-1, 1)\n        reward = [t.reward for t in self.buffer]\n        # update: don\'t need next_state\n        # reward = torch.tensor([t.reward for t in self.buffer], dtype=torch.float).view(-1, 1)\n        # next_state = torch.tensor([t.next_state for t in self.buffer], dtype=torch.float)\n        old_action_log_prob = torch.tensor([t.a_log_prob for t in self.buffer], dtype=torch.float).view(-1, 1)\n\n        R = 0\n        Gt = []\n        for r in reward[::-1]:\n            R = r + gamma * R\n            Gt.insert(0, R)\n        Gt = torch.tensor(Gt, dtype=torch.float)\n        # print(""The agent is updateing...."")\n        for i in range(self.ppo_update_time):\n            for index in BatchSampler(SubsetRandomSampler(range(len(self.buffer))), self.batch_size, False):\n                if self.training_step % 1000 == 0:\n                    print(\'I_ep {} \xef\xbc\x8ctrain {} times\'.format(i_ep, self.training_step))\n                # with torch.no_grad():\n                Gt_index = Gt[index].view(-1, 1)\n                V = self.critic_net(state[index])\n                delta = Gt_index - V\n                advantage = delta.detach()\n                # epoch iteration, PPO core!!!\n                action_prob = self.actor_net(state[index]).gather(1, action[index])  # new policy\n\n                ratio = (action_prob / old_action_log_prob[index])\n                surr1 = ratio * advantage\n                surr2 = torch.clamp(ratio, 1 - self.clip_param, 1 + self.clip_param) * advantage\n\n                # update actor network\n                action_loss = -torch.min(surr1, surr2).mean()  # MAX->MIN desent\n                self.writer.add_scalar(\'loss/action_loss\', action_loss, global_step=self.training_step)\n                self.actor_optimizer.zero_grad()\n                action_loss.backward()\n                nn.utils.clip_grad_norm_(self.actor_net.parameters(), self.max_grad_norm)\n                self.actor_optimizer.step()\n\n                # update critic network\n                value_loss = F.mse_loss(Gt_index, V)\n                self.writer.add_scalar(\'loss/value_loss\', value_loss, global_step=self.training_step)\n                self.critic_net_optimizer.zero_grad()\n                value_loss.backward()\n                nn.utils.clip_grad_norm_(self.critic_net.parameters(), self.max_grad_norm)\n                self.critic_net_optimizer.step()\n                self.training_step += 1\n\n        del self.buffer[:]  # clear experience\n\n\ndef main():\n    agent = PPO()\n    for i_epoch in range(1000):\n        state = env.reset()\n        if render: env.render()\n\n        for t in count():\n            action, action_prob = agent.select_action(state)\n            next_state, reward, done, _ = env.step(action)\n            trans = Transition(state, action, action_prob, reward, next_state)\n            if render: env.render()\n            agent.store_transition(trans)\n            state = next_state\n\n            if done:\n                if len(agent.buffer) >= agent.batch_size: agent.update(i_epoch)\n                agent.writer.add_scalar(\'Steptime/steptime\', t, global_step=i_epoch)\n                break\n\n\nif __name__ == \'__main__\':\n    main()\n    print(""end"")\n'"
Char07 PPO/PPO_pendulum.py,22,"b'import argparse\nimport pickle\nfrom collections import namedtuple\n\nimport matplotlib.pyplot as plt\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n\nparser = argparse.ArgumentParser(description=\'Solve the Pendulum-v0 with PPO\')\nparser.add_argument(\n    \'--gamma\', type=float, default=0.9, metavar=\'G\', help=\'discount factor (default: 0.9)\')\nparser.add_argument(\'--seed\', type=int, default=0, metavar=\'N\', help=\'random seed (default: 0)\')\nparser.add_argument(\'--render\', action=\'store_true\', help=\'render the environment\')\nparser.add_argument(\n    \'--log-interval\',\n    type=int,\n    default=10,\n    metavar=\'N\',\n    help=\'interval between training status logs (default: 10)\')\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\n\nTrainingRecord = namedtuple(\'TrainingRecord\', [\'ep\', \'reward\'])\nTransition = namedtuple(\'Transition\', [\'s\', \'a\', \'a_log_p\', \'r\', \'s_\'])\n\n\nclass ActorNet(nn.Module):\n\n    def __init__(self):\n        super(ActorNet, self).__init__()\n        self.fc = nn.Linear(3, 100)\n        self.mu_head = nn.Linear(100, 1)\n        self.sigma_head = nn.Linear(100, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc(x))\n        mu = 2.0 * F.tanh(self.mu_head(x))\n        sigma = F.softplus(self.sigma_head(x))\n        return (mu, sigma)\n\n\nclass CriticNet(nn.Module):\n\n    def __init__(self):\n        super(CriticNet, self).__init__()\n        self.fc = nn.Linear(3, 100)\n        self.v_head = nn.Linear(100, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc(x))\n        state_value = self.v_head(x)\n        return state_value\n\n\nclass Agent():\n\n    clip_param = 0.2\n    max_grad_norm = 0.5\n    ppo_epoch = 10\n    buffer_capacity, batch_size = 1000, 32\n\n    def __init__(self):\n        self.training_step = 0\n        self.anet = ActorNet().float()\n        self.cnet = CriticNet().float()\n        self.buffer = []\n        self.counter = 0\n\n        self.optimizer_a = optim.Adam(self.anet.parameters(), lr=1e-4)\n        self.optimizer_c = optim.Adam(self.cnet.parameters(), lr=3e-4)\n\n    def select_action(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        with torch.no_grad():\n            (mu, sigma) = self.anet(state)\n        dist = Normal(mu, sigma)\n        action = dist.sample()\n        action_log_prob = dist.log_prob(action)\n        action = action.clamp(-2.0, 2.0)\n        return action.item(), action_log_prob.item()\n\n    def get_value(self, state):\n\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        with torch.no_grad():\n            state_value = self.cnet(state)\n        return state_value.item()\n\n    def save_param(self):\n        torch.save(self.anet.state_dict(), \'param/ppo_anet_params.pkl\')\n        torch.save(self.cnet.state_dict(), \'param/ppo_cnet_params.pkl\')\n\n    def store(self, transition):\n        self.buffer.append(transition)\n        self.counter += 1\n        return self.counter % self.buffer_capacity == 0\n\n    def update(self):\n        self.training_step += 1\n\n        s = torch.tensor([t.s for t in self.buffer], dtype=torch.float)\n        a = torch.tensor([t.a for t in self.buffer], dtype=torch.float).view(-1, 1)\n        r = torch.tensor([t.r for t in self.buffer], dtype=torch.float).view(-1, 1)\n        s_ = torch.tensor([t.s_ for t in self.buffer], dtype=torch.float)\n\n        old_action_log_probs = torch.tensor(\n            [t.a_log_p for t in self.buffer], dtype=torch.float).view(-1, 1)\n\n        r = (r - r.mean()) / (r.std() + 1e-5)\n        with torch.no_grad():\n            target_v = r + args.gamma * self.cnet(s_)\n\n        adv = (target_v - self.cnet(s)).detach()\n\n        for _ in range(self.ppo_epoch):\n            for index in BatchSampler(\n                    SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, False):\n\n                (mu, sigma) = self.anet(s[index])\n                dist = Normal(mu, sigma)\n                action_log_probs = dist.log_prob(a[index])\n                ratio = torch.exp(action_log_probs - old_action_log_probs[index])\n\n                surr1 = ratio * adv[index]\n                surr2 = torch.clamp(ratio, 1.0 - self.clip_param,\n                                    1.0 + self.clip_param) * adv[index]\n                action_loss = -torch.min(surr1, surr2).mean()\n\n                self.optimizer_a.zero_grad()\n                action_loss.backward()\n                nn.utils.clip_grad_norm_(self.anet.parameters(), self.max_grad_norm)\n                self.optimizer_a.step()\n\n                value_loss = F.smooth_l1_loss(self.cnet(s[index]), target_v[index])\n                self.optimizer_c.zero_grad()\n                value_loss.backward()\n                nn.utils.clip_grad_norm_(self.cnet.parameters(), self.max_grad_norm)\n                self.optimizer_c.step()\n\n        del self.buffer[:]\n\n\ndef main():\n    env = gym.make(\'Pendulum-v0\')\n    env.seed(args.seed)\n\n    agent = Agent()\n\n    training_records = []\n    running_reward = -1000\n    state = env.reset()\n    for i_ep in range(1000):\n        score = 0\n        state = env.reset()\n\n        for t in range(200):\n            action, action_log_prob = agent.select_action(state)\n            state_, reward, done, _ = env.step([action])\n            if args.render:\n                env.render()\n            if agent.store(Transition(state, action, action_log_prob, (reward + 8) / 8, state_)):\n                agent.update()\n            score += reward\n            state = state_\n\n        running_reward = running_reward * 0.9 + score * 0.1\n        training_records.append(TrainingRecord(i_ep, running_reward))\n\n        if i_ep % args.log_interval == 0:\n            print(\'Ep {}\\tMoving average score: {:.2f}\\t\'.format(i_ep, running_reward))\n        if running_reward > -200:\n            print(""Solved! Moving average score is now {}!"".format(running_reward))\n            env.close()\n            agent.save_param()\n            with open(\'log/ppo_training_records.pkl\', \'wb\') as f:\n                pickle.dump(training_records, f)\n            break\n\n    plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n    plt.title(\'PPO\')\n    plt.xlabel(\'Episode\')\n    plt.ylabel(\'Moving averaged episode reward\')\n    plt.savefig(""img/ppo.png"")\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Char09 SAC/SAC.py,28,"b'import argparse\nimport pickle\nfrom collections import namedtuple\nfrom itertools import count\n\nimport os\nimport numpy as np\n\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nfrom torch.autograd import grad\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\nfrom tensorboardX import SummaryWriter\n\n\n\'\'\'\nImplementation of soft actor critic\nOriginal paper: https://arxiv.org/abs/1801.01290\nNot the author\'s implementation !\n\'\'\'\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nparser = argparse.ArgumentParser()\n\n\nparser.add_argument(""--env_name"", default=""Pendulum-v0"")  # OpenAI gym environment name\nparser.add_argument(\'--tau\',  default=0.005, type=float) # target smoothing coefficient\nparser.add_argument(\'--target_update_interval\', default=1, type=int)\nparser.add_argument(\'--gradient_steps\', default=1, type=int)\n\n\nparser.add_argument(\'--learning_rate\', default=3e-4, type=int)\nparser.add_argument(\'--gamma\', default=0.99, type=int) # discount gamma\nparser.add_argument(\'--capacity\', default=10000, type=int) # replay buffer size\nparser.add_argument(\'--iteration\', default=100000, type=int) #  num of  games\nparser.add_argument(\'--batch_size\', default=128, type=int) # mini batch size\nparser.add_argument(\'--seed\', default=1, type=int)\n\n# optional parameters\nparser.add_argument(\'--num_hidden_layers\', default=2, type=int)\nparser.add_argument(\'--num_hidden_units_per_layer\', default=256, type=int)\nparser.add_argument(\'--sample_frequency\', default=256, type=int)\nparser.add_argument(\'--activation\', default=\'Relu\', type=str)\nparser.add_argument(\'--render\', default=False, type=bool) # show UI or not\nparser.add_argument(\'--log_interval\', default=2000, type=int) #\nparser.add_argument(\'--load\', default=False, type=bool) # load model\nargs = parser.parse_args()\n\nclass NormalizedActions(gym.ActionWrapper):\n    def _action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = low + (action + 1.0) * 0.5 * (high - low)\n        action = np.clip(action, low, high)\n\n        return action\n\n    def _reverse_action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = 2 * (action - low) / (high - low) - 1\n        action = np.clip(action, low, high)\n\n        return action\n\n\nenv = NormalizedActions(gym.make(args.env_name))\n\n# Set seeds\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.shape[0]\nmax_action = float(env.action_space.high[0])\nmin_Val = torch.tensor(1e-7).float()\nTransition = namedtuple(\'Transition\', [\'s\', \'a\', \'r\', \'s_\', \'d\'])\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, min_log_std=-20, max_log_std=2):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.mu_head = nn.Linear(256, 1)\n        self.log_std_head = nn.Linear(256, 1)\n        self.max_action = max_action\n\n        self.min_log_std = min_log_std\n        self.max_log_std = max_log_std\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        mu = self.mu_head(x)\n        log_std_head = F.relu(self.log_std_head(x))\n        log_std_head = torch.clamp(log_std_head, self.min_log_std, self.max_log_std)\n        return mu, log_std_head\n\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass Q(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(Q, self).__init__()\n        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, s, a):\n        s = s.reshape(-1, state_dim)\n        a = a.reshape(-1, action_dim)\n        x = torch.cat((s, a), -1) # combination s and a\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass SAC():\n    def __init__(self):\n        super(SAC, self).__init__()\n\n        self.policy_net = Actor(state_dim).to(device)\n        self.value_net = Critic(state_dim).to(device)\n        self.Q_net = Q(state_dim, action_dim).to(device)\n        self.Target_value_net = Critic(state_dim).to(device)\n\n        self.replay_buffer = [Transition] * args.capacity\n        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=args.learning_rate)\n        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=args.learning_rate)\n        self.Q_optimizer = optim.Adam(self.Q_net.parameters(), lr=args.learning_rate)\n        self.num_transition = 0 # pointer of replay buffer\n        self.num_training = 1\n        self.writer = SummaryWriter(\'./exp-SAC\')\n\n        self.value_criterion = nn.MSELoss()\n        self.Q_criterion = nn.MSELoss()\n\n        for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n            target_param.data.copy_(param.data)\n\n        os.makedirs(\'./SAC_model/\', exist_ok=True)\n\n    def select_action(self, state):\n        state = torch.FloatTensor(state).to(device)\n        mu, log_sigma = self.policy_net(state)\n        sigma = torch.exp(log_sigma)\n        dist = Normal(mu, sigma)\n        z = dist.sample()\n        action = torch.tanh(z).detach().cpu().numpy()\n        return action.item() # return a scalar, float32\n\n    def store(self, s, a, r, s_, d):\n        index = self.num_transition % args.capacity\n        transition = Transition(s, a, r, s_, d)\n        self.replay_buffer[index] = transition\n        self.num_transition += 1\n\n    def get_action_log_prob(self, state):\n\n        batch_mu, batch_log_sigma = self.policy_net(state)\n        batch_sigma = torch.exp(batch_log_sigma)\n        dist = Normal(batch_mu, batch_sigma)\n        z = dist.sample()\n        action = torch.tanh(z)\n        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + min_Val)\n        return action, log_prob, z, batch_mu, batch_log_sigma\n\n\n    def update(self):\n        if self.num_training % 500 == 0:\n            print(""Training ... {} "".format(self.num_training))\n        s = torch.tensor([t.s for t in self.replay_buffer]).float().to(device)\n        a = torch.tensor([t.a for t in self.replay_buffer]).to(device)\n        r = torch.tensor([t.r for t in self.replay_buffer]).to(device)\n        s_ = torch.tensor([t.s_ for t in self.replay_buffer]).float().to(device)\n        d = torch.tensor([t.d for t in self.replay_buffer]).float().to(device)\n\n        for _ in range(args.gradient_steps):\n            #for index in BatchSampler(SubsetRandomSampler(range(args.capacity)), args.batch_size, False):\n            index = np.random.choice(range(args.capacity), args.batch_size, replace=False)\n            bn_s = s[index]\n            bn_a = a[index].reshape(-1, 1)\n            bn_r = r[index].reshape(-1, 1)\n            bn_s_ = s_[index]\n            bn_d = d[index].reshape(-1, 1)\n\n\n            target_value = self.Target_value_net(bn_s_)\n            next_q_value = bn_r + (1 - bn_d) * args.gamma * target_value\n\n            excepted_value = self.value_net(bn_s)\n            excepted_Q = self.Q_net(bn_s, bn_a)\n\n            sample_action, log_prob, z, batch_mu, batch_log_sigma = self.get_action_log_prob(bn_s)\n            excepted_new_Q = self.Q_net(bn_s, sample_action)\n            next_value = excepted_new_Q - log_prob\n\n            # !!!Note that the actions are sampled according to the current policy,\n            # instead of replay buffer. (From original paper)\n\n            V_loss = self.value_criterion(excepted_value, next_value.detach())  # J_V\n            V_loss = V_loss.mean()\n\n            # Single Q_net this is different from original paper!!!\n            Q_loss = self.Q_criterion(excepted_Q, next_q_value.detach()) # J_Q\n            Q_loss = Q_loss.mean()\n\n            log_policy_target = excepted_new_Q - excepted_value\n\n            pi_loss = log_prob * (log_prob- log_policy_target).detach()\n            pi_loss = pi_loss.mean()\n\n            self.writer.add_scalar(\'Loss/V_loss\', V_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/Q_loss\', Q_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/pi_loss\', pi_loss, global_step=self.num_training)\n            # mini batch gradient descent\n            self.value_optimizer.zero_grad()\n            V_loss.backward(retain_graph=True)\n            nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)\n            self.value_optimizer.step()\n\n            self.Q_optimizer.zero_grad()\n            Q_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.Q_net.parameters(), 0.5)\n            self.Q_optimizer.step()\n\n            self.policy_optimizer.zero_grad()\n            pi_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)\n            self.policy_optimizer.step()\n\n            # soft update\n            for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n                target_param.data.copy_(target_param * (1 - args.tau) + param * args.tau)\n\n            self.num_training += 1\n\n    def save(self):\n        torch.save(self.policy_net.state_dict(), \'./SAC_model/policy_net.pth\')\n        torch.save(self.value_net.state_dict(), \'./SAC_model/value_net.pth\')\n        torch.save(self.Q_net.state_dict(), \'./SAC_model/Q_net.pth\')\n        print(""===================================="")\n        print(""Model has been saved..."")\n        print(""===================================="")\n\n    def load(self):\n        torch.load(self.policy_net.state_dict(), \'./SAC_model/policy_net.pth\')\n        torch.load(self.value_net.state_dict(), \'./SAC_model/value_net.pth\')\n        torch.load(self.Q_net.state_dict(), \'./SAC_model/Q_net.pth\')\n        print()\n\ndef main():\n\n    agent = SAC()\n    if args.load: agent.load()\n    if args.render: env.render()\n    print(""===================================="")\n    print(""Collection Experience..."")\n    print(""===================================="")\n\n    ep_r = 0\n    for i in range(args.iteration):\n        state = env.reset()\n        for t in range(200):\n            action = agent.select_action(state)\n            next_state, reward, done, info = env.step(np.float32(action))\n            ep_r += reward\n            if args.render: env.render()\n            agent.store(state, action, reward, next_state, done)\n\n            if agent.num_transition >= args.capacity:\n                agent.update()\n\n            state = next_state\n            if done or t == 199:\n                if i % 10 == 0:\n                    print(""Ep_i {}, the ep_r is {}, the t is {}"".format(i, ep_r, t))\n                break\n        if i % args.log_interval == 0:\n            agent.save()\n        agent.writer.add_scalar(\'ep_r\', ep_r, global_step=i)\n        ep_r = 0\n\n\nif __name__ == \'__main__\':\n    main()'"
Char09 SAC/SAC_BipedalWalker-v2.py,34,"b'import argparse\nfrom collections import namedtuple\nfrom itertools import count\nimport pickle\n\nimport os, random\nimport numpy as np\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nfrom tensorboardX import SummaryWriter\n\n\n\'\'\'\nImplementation of soft actor critic, dual Q network version \nOriginal paper: https://arxiv.org/abs/1801.01290\nNot the author\'s implementation !\n\'\'\'\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nparser = argparse.ArgumentParser()\n\nparser.add_argument(""--env_name"", default=""BipedalWalker-v2"")  # OpenAI gym environment name\nparser.add_argument(\'--tau\',  default=0.005, type=float) # target smoothing coefficient\nparser.add_argument(\'--target_update_interval\', default=1, type=int)\nparser.add_argument(\'--gradient_steps\', default=1, type=int)\n\nparser.add_argument(\'--learning_rate\', default=3e-4, type=float)\nparser.add_argument(\'--gamma\', default=0.99, type=int) # discount gamma\nparser.add_argument(\'--capacity\', default=1000000, type=int) # replay buffer size\nparser.add_argument(\'--iteration\', default=100000, type=int) #  num of  games\nparser.add_argument(\'--batch_size\', default=128, type=int) # mini batch size\nparser.add_argument(\'--seed\', default=1, type=int)\n\n# optional parameters\nparser.add_argument(\'--num_hidden_layers\', default=2, type=int)\nparser.add_argument(\'--num_hidden_units_per_layer\', default=256, type=int)\nparser.add_argument(\'--sample_frequency\', default=256, type=int)\nparser.add_argument(\'--activation\', default=\'Relu\', type=str)\nparser.add_argument(\'--render\', default=False, type=bool) # show UI or not\nparser.add_argument(\'--log_interval\', default=50, type=int) #\nparser.add_argument(\'--load\', default=False, type=bool) # load model\nparser.add_argument(\'--render_interval\', default=100, type=int) # after render_interval, the env.render() will work\nargs = parser.parse_args()\n\nclass NormalizedActions(gym.ActionWrapper):\n    def _action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = low + (action + 1.0) * 0.5 * (high - low)\n        action = np.clip(action, low, high)\n\n        return action\n\n    def _reverse_action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = 2 * (action - low) / (high - low) - 1\n        action = np.clip(action, low, high)\n\n        return action\n\n\nenv = NormalizedActions(gym.make(args.env_name))\n\n# Set seeds\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.shape[0]\nmax_action = float(env.action_space.high[0])\nmin_Val = torch.tensor(1e-7).float().to(device)\n\nclass Replay_buffer():\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.state_pool = torch.zeros(self.capacity, state_dim).float().to(device)\n        self.action_pool = torch.zeros(self.capacity, action_dim).float().to(device)\n        self.reward_pool = torch.zeros(self.capacity, 1).float().to(device)\n        self.next_state_pool = torch.zeros(self.capacity, state_dim).float().to(device)\n        self.done_pool = torch.zeros(self.capacity, 1).float().to(device)\n        self.num_transition = 0\n\n    def push(self, s, a, r, s_, d):\n        index = self.num_transition % self.capacity\n        s = torch.tensor(s).float().to(device)\n        a = torch.tensor(a).float().to(device)\n        r = torch.tensor(r).float().to(device)\n        s_ = torch.tensor(s_).float().to(device)\n        d = torch.tensor(d).float().to(device)\n        for pool, ele in zip([self.state_pool, self.action_pool, self.reward_pool, self.next_state_pool, self.done_pool],\n                           [s, a, r, s_, d]):\n            pool[index] = ele\n        self.num_transition += 1\n\n    def sample(self, batch_size):\n        index = np.random.choice(range(self.capacity), batch_size, replace=False)\n        bn_s, bn_a, bn_r, bn_s_, bn_d = self.state_pool[index], self.action_pool[index], self.reward_pool[index],\\\n                                        self.next_state_pool[index], self.done_pool[index]\n\n        return bn_s, bn_a, bn_r, bn_s_, bn_d\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim=action_dim, min_log_std=-10, max_log_std=2):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.mu_head = nn.Linear(512, action_dim)\n        self.log_std_head = nn.Linear(512, action_dim)\n        self.max_action = max_action\n\n        self.min_log_std = min_log_std\n        self.max_log_std = max_log_std\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        mu = self.mu_head(x)\n        log_std_head = F.relu(self.log_std_head(x))\n        log_std_head = torch.clamp(log_std_head, self.min_log_std, self.max_log_std)\n        return mu, log_std_head\n\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass Q(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(Q, self).__init__()\n        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, s, a):\n        s = s.reshape(-1, state_dim)\n        a = a.reshape(-1, action_dim)\n        x = torch.cat((s, a), -1) # combination s and a\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass SAC():\n    def __init__(self):\n        super(SAC, self).__init__()\n\n        self.policy_net = Actor(state_dim).to(device)\n        self.value_net = Critic(state_dim).to(device)\n        self.Target_value_net = Critic(state_dim).to(device)\n        self.Q_net1 = Q(state_dim, action_dim).to(device)\n        self.Q_net2 = Q(state_dim, action_dim).to(device)\n\n        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=args.learning_rate)\n        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=args.learning_rate)\n        self.Q1_optimizer = optim.Adam(self.Q_net1.parameters(), lr=args.learning_rate)\n        self.Q2_optimizer = optim.Adam(self.Q_net2.parameters(), lr=args.learning_rate)\n\n        self.replay_buffer = Replay_buffer(args.capacity)\n        self.num_transition = 0 # pointer of replay buffer\n        self.num_training = 0\n        self.writer = SummaryWriter(\'./exp-SAC_dual_Q_network\')\n\n        self.value_criterion = nn.MSELoss()\n        self.Q1_criterion = nn.MSELoss()\n        self.Q2_criterion = nn.MSELoss()\n\n        for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n            target_param.data.copy_(param.data)\n\n        os.makedirs(\'./SAC_model/\', exist_ok=True)\n\n    def select_action(self, state):\n        state = torch.FloatTensor(state).to(device)\n        mu, log_sigma = self.policy_net(state)\n        sigma = torch.exp(log_sigma)\n        dist = Normal(mu, sigma)\n        z = dist.sample()\n        action = torch.tanh(z).detach().cpu().numpy()\n        return action # return a scalar, float32\n\n    def evaluate(self, state):\n        batch_mu, batch_log_sigma = self.policy_net(state)\n        batch_sigma = torch.exp(batch_log_sigma)\n        dist = Normal(batch_mu, batch_sigma)\n        noise = Normal(0, 1)\n\n        z = noise.sample()\n        action = torch.tanh(batch_mu + batch_sigma*z.to(device))\n        log_prob = dist.log_prob(batch_mu + batch_sigma * z.to(device)) - torch.log(1 - action.pow(2) + min_Val)\n        return action, log_prob, z, batch_mu, batch_log_sigma\n\n    def update(self):\n        if self.num_training % 500 == 0:\n            print(""Training ... \\t{} times "".format(self.num_training))\n\n        for _ in range(args.gradient_steps):\n            bn_s, bn_a, bn_r, bn_s_, bn_d = self.replay_buffer.sample(args.batch_size)\n\n            target_value = self.Target_value_net(bn_s_)\n            next_q_value = bn_r + (1 - bn_d) * args.gamma * target_value\n\n            excepted_value = self.value_net(bn_s)\n            excepted_Q1 = self.Q_net1(bn_s, bn_a)\n            excepted_Q2 = self.Q_net2(bn_s, bn_a)\n            sample_action, log_prob, z, batch_mu, batch_log_sigma = self.evaluate(bn_s)\n            excepted_new_Q = torch.min(self.Q_net1(bn_s, sample_action), self.Q_net2(bn_s, sample_action))\n            next_value = excepted_new_Q - log_prob\n\n            # !!!Note that the actions are sampled according to the current policy,\n            # instead of replay buffer. (From original paper)\n            V_loss = self.value_criterion(excepted_value, next_value.detach()).mean()  # J_V\n\n            # Dual Q net\n            Q1_loss = self.Q1_criterion(excepted_Q1, next_q_value.detach()).mean() # J_Q\n            Q2_loss = self.Q2_criterion(excepted_Q2, next_q_value.detach()).mean()\n\n            pi_loss = (log_prob - excepted_new_Q).mean() # according to original paper\n\n            self.writer.add_scalar(\'Loss/V_loss\', V_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/Q1_loss\', Q1_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/Q2_loss\', Q2_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/policy_loss\', pi_loss, global_step=self.num_training)\n\n            # mini batch gradient descent\n            self.value_optimizer.zero_grad()\n            V_loss.backward(retain_graph=True)\n            nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)\n            self.value_optimizer.step()\n\n            self.Q1_optimizer.zero_grad()\n            Q1_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.Q_net1.parameters(), 0.5)\n            self.Q1_optimizer.step()\n\n            self.Q2_optimizer.zero_grad()\n            Q2_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.Q_net2.parameters(), 0.5)\n            self.Q2_optimizer.step()\n\n            self.policy_optimizer.zero_grad()\n            pi_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)\n            self.policy_optimizer.step()\n\n            # update target v net update\n            for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n                target_param.data.copy_(target_param * (1 - args.tau) + param * args.tau)\n\n            self.num_training += 1\n\n    def save(self):\n        torch.save(self.policy_net.state_dict(), \'./SAC_model/policy_net.pth\')\n        torch.save(self.value_net.state_dict(), \'./SAC_model/value_net.pth\')\n        torch.save(self.Q_net1.state_dict(), \'./SAC_model/Q_net1.pth\')\n        torch.save(self.Q_net2.state_dict(), \'./SAC_model/Q_net2.pth\')\n        print(""===================================="")\n        print(""Model has been saved..."")\n        print(""===================================="")\n\n    def load(self):\n        self.policy_net.load_state_dict(torch.load(\'./SAC_model/policy_net.pth\'))\n        self.value_net.load_state_dict(torch.load( \'./SAC_model/value_net.pth\'))\n        self.Q_net1.load_state_dict(torch.load(\'./SAC_model/Q_net1.pth\'))\n        self.Q_net2.load_state_dict(torch.load(\'./SAC_model/Q_net2.pth\'))\n        print(""===================================="")\n        print(""model has been loaded..."")\n        print(""===================================="")\n\n\ndef main():\n    agent = SAC()\n    if args.load: agent.load()\n    print(""===================================="")\n    print(""Collection Experience..."")\n    print(""===================================="")\n\n    ep_r = 0\n    for i in range(args.iteration):\n        state = env.reset()\n        for t in range(200):\n            action = agent.select_action(state)\n            next_state, reward, done, info = env.step(np.float32(action))\n            ep_r += reward\n            if args.render and i >= args.render_interval : env.render()\n            agent.replay_buffer.push(state, action, reward, next_state, done)\n\n            state = next_state\n            if done:\n                if agent.replay_buffer.num_transition >= args.capacity:\n                    agent.update()\n                if i > 100:\n                    print(""Ep_i \\t{}, the ep_r is \\t{}, the step is \\t{}"".format(i, ep_r, t))\n                break\n        if i % args.log_interval == 0:\n            agent.save()\n        agent.writer.add_scalar(\'ep_r\', ep_r, global_step=i)\n        ep_r = 0\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Char09 SAC/SAC_dual_Q_net.py,29,"b'import argparse\nfrom collections import namedtuple\nfrom itertools import count\n\nimport os\nimport numpy as np\n\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nfrom tensorboardX import SummaryWriter\n\n\n\'\'\'\nImplementation of soft actor critic, dual Q network version \nOriginal paper: https://arxiv.org/abs/1801.01290\nNot the author\'s implementation !\n\'\'\'\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nparser = argparse.ArgumentParser()\n\n\nparser.add_argument(""--env_name"", default=""Pendulum-v0"")  # OpenAI gym environment name\nparser.add_argument(\'--tau\',  default=0.005, type=float) # target smoothing coefficient\nparser.add_argument(\'--target_update_interval\', default=1, type=int)\nparser.add_argument(\'--gradient_steps\', default=1, type=int)\n\n\nparser.add_argument(\'--learning_rate\', default=3e-4, type=int)\nparser.add_argument(\'--gamma\', default=0.99, type=int) # discount gamma\nparser.add_argument(\'--capacity\', default=10000, type=int) # replay buffer size\nparser.add_argument(\'--iteration\', default=100000, type=int) #  num of  games\nparser.add_argument(\'--batch_size\', default=128, type=int) # mini batch size\nparser.add_argument(\'--seed\', default=1, type=int)\n\n# optional parameters\nparser.add_argument(\'--num_hidden_layers\', default=2, type=int)\nparser.add_argument(\'--num_hidden_units_per_layer\', default=256, type=int)\nparser.add_argument(\'--sample_frequency\', default=256, type=int)\nparser.add_argument(\'--activation\', default=\'Relu\', type=str)\nparser.add_argument(\'--render\', default=False, type=bool) # show UI or not\nparser.add_argument(\'--log_interval\', default=2000, type=int) #\nparser.add_argument(\'--load\', default=False, type=bool) # load model\nargs = parser.parse_args()\n\nclass NormalizedActions(gym.ActionWrapper):\n    def _action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = low + (action + 1.0) * 0.5 * (high - low)\n        action = np.clip(action, low, high)\n\n        return action\n\n    def _reverse_action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = 2 * (action - low) / (high - low) - 1\n        action = np.clip(action, low, high)\n\n        return action\n\n\nenv = NormalizedActions(gym.make(args.env_name))\n\n# Set seeds\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.shape[0]\nmax_action = float(env.action_space.high[0])\nmin_Val = torch.tensor(1e-7).float().to(device)\nTransition = namedtuple(\'Transition\', [\'s\', \'a\', \'r\', \'s_\', \'d\'])\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, min_log_std=-10, max_log_std=2):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.mu_head = nn.Linear(256, 1)\n        self.log_std_head = nn.Linear(256, 1)\n        self.max_action = max_action\n\n        self.min_log_std = min_log_std\n        self.max_log_std = max_log_std\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        mu = self.mu_head(x)\n        log_std_head = F.relu(self.log_std_head(x))\n        log_std_head = torch.clamp(log_std_head, self.min_log_std, self.max_log_std)\n        return mu, log_std_head\n\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass Q(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(Q, self).__init__()\n        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, s, a):\n        s = s.reshape(-1, state_dim)\n        a = a.reshape(-1, action_dim)\n        x = torch.cat((s, a), -1) # combination s and a\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass SAC():\n    def __init__(self):\n        super(SAC, self).__init__()\n\n        self.policy_net = Actor(state_dim).to(device)\n        self.value_net = Critic(state_dim).to(device)\n        self.Target_value_net = Critic(state_dim).to(device)\n        self.Q_net1 = Q(state_dim, action_dim).to(device)\n        self.Q_net2 = Q(state_dim, action_dim).to(device)\n\n        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=args.learning_rate)\n        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=args.learning_rate)\n        self.Q1_optimizer = optim.Adam(self.Q_net1.parameters(), lr=args.learning_rate)\n        self.Q2_optimizer = optim.Adam(self.Q_net2.parameters(), lr=args.learning_rate)\n\n        self.replay_buffer = [Transition] * args.capacity\n        self.num_transition = 0 # pointer of replay buffer\n        self.num_training = 1\n        self.writer = SummaryWriter(\'./exp-SAC_dual_Q_network\')\n\n        self.value_criterion = nn.MSELoss()\n        self.Q1_criterion = nn.MSELoss()\n        self.Q2_criterion = nn.MSELoss()\n\n        for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n            target_param.data.copy_(param.data)\n\n        os.makedirs(\'./SAC_model/\', exist_ok=True)\n\n    def select_action(self, state):\n        state = torch.FloatTensor(state).to(device)\n        mu, log_sigma = self.policy_net(state)\n        sigma = torch.exp(log_sigma)\n        dist = Normal(mu, sigma)\n        z = dist.sample()\n        action = torch.tanh(z).detach().cpu().numpy()\n        return action.item() # return a scalar, float32\n\n    def store(self, s, a, r, s_, d):\n        index = self.num_transition % args.capacity\n        transition = Transition(s, a, r, s_, d)\n        self.replay_buffer[index] = transition\n        self.num_transition += 1\n\n    def evaluate(self, state):\n        batch_mu, batch_log_sigma = self.policy_net(state)\n        batch_sigma = torch.exp(batch_log_sigma)\n        dist = Normal(batch_mu, batch_sigma)\n        noise = Normal(0, 1)\n\n        z = noise.sample()\n        action = torch.tanh(batch_mu + batch_sigma*z.to(device))\n        log_prob = dist.log_prob(batch_mu + batch_sigma * z.to(device)) - torch.log(1 - action.pow(2) + min_Val)\n        return action, log_prob, z, batch_mu, batch_log_sigma\n\n    def update(self):\n        if self.num_training % 500 == 0:\n            print(""Training ... {} times "".format(self.num_training))\n        s = torch.tensor([t.s for t in self.replay_buffer]).float().to(device)\n        a = torch.tensor([t.a for t in self.replay_buffer]).to(device)\n        r = torch.tensor([t.r for t in self.replay_buffer]).to(device)\n        s_ = torch.tensor([t.s_ for t in self.replay_buffer]).float().to(device)\n        d = torch.tensor([t.d for t in self.replay_buffer]).float().to(device)\n\n        for _ in range(args.gradient_steps):\n            #for index in BatchSampler(SubsetRandomSampler(range(args.capacity)), args.batch_size, False):\n            index = np.random.choice(range(args.capacity), args.batch_size, replace=False)\n            bn_s = s[index]\n            bn_a = a[index].reshape(-1, 1)\n            bn_r = r[index].reshape(-1, 1)\n            bn_s_ = s_[index]\n            bn_d = d[index].reshape(-1, 1)\n\n            target_value = self.Target_value_net(bn_s_)\n            next_q_value = bn_r + (1 - bn_d) * args.gamma * target_value\n\n            excepted_value = self.value_net(bn_s)\n            excepted_Q1 = self.Q_net1(bn_s, bn_a)\n            excepted_Q2 = self.Q_net2(bn_s, bn_a)\n            sample_action, log_prob, z, batch_mu, batch_log_sigma = self.evaluate(bn_s)\n            excepted_new_Q = torch.min(self.Q_net1(bn_s, sample_action), self.Q_net2(bn_s, sample_action))\n            next_value = excepted_new_Q - log_prob\n\n            # !!!Note that the actions are sampled according to the current policy,\n            # instead of replay buffer. (From original paper)\n            V_loss = self.value_criterion(excepted_value, next_value.detach()).mean()  # J_V\n\n            # Dual Q net\n            Q1_loss = self.Q1_criterion(excepted_Q1, next_q_value.detach()).mean() # J_Q\n            Q2_loss = self.Q2_criterion(excepted_Q2, next_q_value.detach()).mean()\n\n            pi_loss = (log_prob - excepted_new_Q).mean() # according to original paper\n\n            self.writer.add_scalar(\'Loss/V_loss\', V_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/Q1_loss\', Q1_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/Q2_loss\', Q2_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/policy_loss\', pi_loss, global_step=self.num_training)\n\n            # mini batch gradient descent\n            self.value_optimizer.zero_grad()\n            V_loss.backward(retain_graph=True)\n            nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)\n            self.value_optimizer.step()\n\n            self.Q1_optimizer.zero_grad()\n            Q1_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.Q_net1.parameters(), 0.5)\n            self.Q1_optimizer.step()\n\n            self.Q2_optimizer.zero_grad()\n            Q2_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.Q_net2.parameters(), 0.5)\n            self.Q2_optimizer.step()\n\n            self.policy_optimizer.zero_grad()\n            pi_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)\n            self.policy_optimizer.step()\n\n            # update target v net update\n            for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n                target_param.data.copy_(target_param * (1 - args.tau) + param * args.tau)\n\n            self.num_training += 1\n\n    def save(self):\n        torch.save(self.policy_net.state_dict(), \'./SAC_model/policy_net.pth\')\n        torch.save(self.value_net.state_dict(), \'./SAC_model/value_net.pth\')\n        torch.save(self.Q_net1.state_dict(), \'./SAC_model/Q_net1.pth\')\n        torch.save(self.Q_net2.state_dict(), \'./SAC_model/Q_net2.pth\')\n        print(""===================================="")\n        print(""Model has been saved..."")\n        print(""===================================="")\n\n    def load(self):\n        self.policy_net.load_state_dict(torch.load(\'./SAC_model/policy_net.pth\'))\n        self.value_net.load_state_dict(torch.load( \'./SAC_model/value_net.pth\'))\n        self.Q_net1.load_state_dict(torch.load(\'./SAC_model/Q_net1.pth\'))\n        self.Q_net2.load_state_dict(torch.load(\'./SAC_model/Q_net2.pth\'))\n        print(""model has been load"")\n\n\ndef main():\n\n    agent = SAC()\n    if args.load: agent.load()\n    if args.render: env.render()\n    print(""===================================="")\n    print(""Collection Experience..."")\n    print(""===================================="")\n\n    ep_r = 0\n    for i in range(args.iteration):\n        state = env.reset()\n        for t in range(200):\n            action = agent.select_action(state)\n            next_state, reward, done, info = env.step(np.float32(action))\n            ep_r += reward\n            if args.render: env.render()\n            agent.store(state, action, reward, next_state, done)\n\n            if agent.num_transition >= args.capacity:\n                agent.update()\n\n            state = next_state\n            if done or t == 199:\n                if i % 10 == 0:\n                    print(""Ep_i {}, the ep_r is {}, the t is {}"".format(i, ep_r, t))\n                break\n        if i % args.log_interval == 0:\n            agent.save()\n        agent.writer.add_scalar(\'ep_r\', ep_r, global_step=i)\n        ep_r = 0\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Char09 SAC/test_agent.py,29,"b'import argparse\nfrom collections import namedtuple\nfrom itertools import count\nimport pickle\nimport os\nimport numpy as np\n\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nfrom tensorboardX import SummaryWriter\n\n\n\'\'\'\nImplementation of soft actor critic, dual Q network version \nOriginal paper: https://arxiv.org/abs/1801.01290\nNot the author\'s implementation !\n\'\'\'\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nparser = argparse.ArgumentParser()\n\n\nparser.add_argument(""--env_name"", default=""BipedalWalker-v2"")  # OpenAI gym environment name\nparser.add_argument(\'--tau\',  default=0.005, type=float) # target smoothing coefficient\nparser.add_argument(\'--target_update_interval\', default=1, type=int)\nparser.add_argument(\'--gradient_steps\', default=1, type=int)\nparser.add_argument(\'--mode\', default=\'test\', type=str) # test or train\n\nparser.add_argument(\'--learning_rate\', default=3e-4, type=int)\nparser.add_argument(\'--gamma\', default=0.99, type=int) # discount gamma\nparser.add_argument(\'--capacity\', default=10000, type=int) # replay buffer size\nparser.add_argument(\'--iteration\', default=100000, type=int) #  num of  games\nparser.add_argument(\'--batch_size\', default=128, type=int) # mini batch size\nparser.add_argument(\'--seed\', default=1, type=int)\n\n# optional parameters\nparser.add_argument(\'--num_hidden_layers\', default=2, type=int)\nparser.add_argument(\'--num_hidden_units_per_layer\', default=256, type=int)\nparser.add_argument(\'--sample_frequency\', default=256, type=int)\nparser.add_argument(\'--activation\', default=\'Relu\', type=str)\nparser.add_argument(\'--render\', default=False, type=bool) # show UI or not\nparser.add_argument(\'--log_interval\', default=50, type=int) #\nparser.add_argument(\'--load\', default=False, type=bool) # load model\nparser.add_argument(\'--render_interval\', default=100, type=int) # after render_interval, the env.render() will work\nargs = parser.parse_args()\n\nclass NormalizedActions(gym.ActionWrapper):\n    def _action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = low + (action + 1.0) * 0.5 * (high - low)\n        action = np.clip(action, low, high)\n\n        return action\n\n    def _reverse_action(self, action):\n        low = self.action_space.low\n        high = self.action_space.high\n\n        action = 2 * (action - low) / (high - low) - 1\n        action = np.clip(action, low, high)\n\n        return action\n\n\nenv = NormalizedActions(gym.make(args.env_name))\n\n# Set seeds\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.shape[0]\nmax_action = float(env.action_space.high[0])\nmin_Val = torch.tensor(1e-7).float().to(device)\nTransition = namedtuple(\'Transition\', [\'s\', \'a\', \'r\', \'s_\', \'d\'])\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim=action_dim, min_log_std=-10, max_log_std=2):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 256)\n        self.fc2 = nn.Linear(256, 512)\n        self.mu_head = nn.Linear(512, action_dim)\n        self.log_std_head = nn.Linear(512, action_dim)\n        self.max_action = max_action\n\n        self.min_log_std = min_log_std\n        self.max_log_std = max_log_std\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        mu = self.mu_head(x)\n        log_std_head = F.relu(self.log_std_head(x))\n        log_std_head = torch.clamp(log_std_head, self.min_log_std, self.max_log_std)\n        return mu, log_std_head\n\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass Q(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(Q, self).__init__()\n        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, s, a):\n        s = s.reshape(-1, state_dim)\n        a = a.reshape(-1, action_dim)\n        x = torch.cat((s, a), -1) # combination s and a\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass SAC():\n    def __init__(self):\n        super(SAC, self).__init__()\n\n        self.policy_net = Actor(state_dim).to(device)\n        self.value_net = Critic(state_dim).to(device)\n        self.Target_value_net = Critic(state_dim).to(device)\n        self.Q_net1 = Q(state_dim, action_dim).to(device)\n        self.Q_net2 = Q(state_dim, action_dim).to(device)\n\n        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=args.learning_rate)\n        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=args.learning_rate)\n        self.Q1_optimizer = optim.Adam(self.Q_net1.parameters(), lr=args.learning_rate)\n        self.Q2_optimizer = optim.Adam(self.Q_net2.parameters(), lr=args.learning_rate)\n\n        self.replay_buffer = [Transition] * args.capacity\n        self.num_transition = 0 # pointer of replay buffer\n        self.num_training = 0\n        self.writer = SummaryWriter(\'./test_agent\')\n\n        self.value_criterion = nn.MSELoss()\n        self.Q1_criterion = nn.MSELoss()\n        self.Q2_criterion = nn.MSELoss()\n\n        for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n            target_param.data.copy_(param.data)\n\n        os.makedirs(\'./SAC_model/\', exist_ok=True)\n\n    def select_action(self, state):\n        state = torch.FloatTensor(state).to(device)\n        mu, log_sigma = self.policy_net(state)\n        sigma = torch.exp(log_sigma)\n        dist = Normal(mu, sigma)\n        z = dist.sample()\n        action = torch.tanh(z).detach().cpu().numpy()\n        return action # return a scalar, float32\n\n    def store(self, s, a, r, s_, d):\n        index = self.num_transition % args.capacity\n        transition = Transition(s, a, r, s_, d)\n        self.replay_buffer[index] = transition\n        self.num_transition += 1\n\n    def evaluate(self, state):\n        batch_mu, batch_log_sigma = self.policy_net(state)\n        batch_sigma = torch.exp(batch_log_sigma)\n        dist = Normal(batch_mu, batch_sigma)\n        noise = Normal(0, 1)\n\n        z = noise.sample()\n        action = torch.tanh(batch_mu + batch_sigma*z.to(device))\n        log_prob = dist.log_prob(batch_mu + batch_sigma * z.to(device)) - torch.log(1 - action.pow(2) + min_Val)\n        return action, log_prob, z, batch_mu, batch_log_sigma\n\n    def update(self):\n        if self.num_training % 500 == 0:\n            print(""Training ... \\t{} times "".format(self.num_training))\n        s = torch.tensor([t.s for t in self.replay_buffer]).float().to(device)\n        a = torch.tensor([t.a for t in self.replay_buffer]).to(device)\n        r = torch.tensor([t.r for t in self.replay_buffer]).to(device)\n        s_ = torch.tensor([t.s_ for t in self.replay_buffer]).float().to(device)\n        d = torch.tensor([t.d for t in self.replay_buffer]).float().to(device)\n\n        for _ in range(args.gradient_steps):\n            #for index in BatchSampler(SubsetRandomSampler(range(args.capacity)), args.batch_size, False):\n            index = np.random.choice(range(args.capacity), args.batch_size, replace=False)\n            bn_s = s[index].reshape(-1, state_dim)\n            bn_a = a[index].reshape(-1, action_dim)\n            bn_r = r[index].reshape(-1, 1)\n            bn_s_ = s_[index].reshape(-1, state_dim)\n            bn_d = d[index].reshape(-1, 1)\n\n            target_value = self.Target_value_net(bn_s_)\n            next_q_value = bn_r + (1 - bn_d) * args.gamma * target_value\n\n            excepted_value = self.value_net(bn_s)\n            excepted_Q1 = self.Q_net1(bn_s, bn_a)\n            excepted_Q2 = self.Q_net2(bn_s, bn_a)\n            sample_action, log_prob, z, batch_mu, batch_log_sigma = self.evaluate(bn_s)\n            excepted_new_Q = torch.min(self.Q_net1(bn_s, sample_action), self.Q_net2(bn_s, sample_action))\n            next_value = excepted_new_Q - log_prob\n\n            # !!!Note that the actions are sampled according to the current policy,\n            # instead of replay buffer. (From original paper)\n            V_loss = self.value_criterion(excepted_value, next_value.detach()).mean()  # J_V\n\n            # Dual Q net\n            Q1_loss = self.Q1_criterion(excepted_Q1, next_q_value.detach()).mean() # J_Q\n            Q2_loss = self.Q2_criterion(excepted_Q2, next_q_value.detach()).mean()\n\n            pi_loss = (log_prob - excepted_new_Q).mean() # according to original paper\n\n            self.writer.add_scalar(\'Loss/V_loss\', V_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/Q1_loss\', Q1_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/Q2_loss\', Q2_loss, global_step=self.num_training)\n            self.writer.add_scalar(\'Loss/policy_loss\', pi_loss, global_step=self.num_training)\n\n            # mini batch gradient descent\n            self.value_optimizer.zero_grad()\n            V_loss.backward(retain_graph=True)\n            nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)\n            self.value_optimizer.step()\n\n            self.Q1_optimizer.zero_grad()\n            Q1_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.Q_net1.parameters(), 0.5)\n            self.Q1_optimizer.step()\n\n            self.Q2_optimizer.zero_grad()\n            Q2_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.Q_net2.parameters(), 0.5)\n            self.Q2_optimizer.step()\n\n            self.policy_optimizer.zero_grad()\n            pi_loss.backward(retain_graph = True)\n            nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)\n            self.policy_optimizer.step()\n\n            # update target v net update\n            for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n                target_param.data.copy_(target_param * (1 - args.tau) + param * args.tau)\n\n            self.num_training += 1\n\n    def save(self):\n        torch.save(self.policy_net.state_dict(), \'./SAC_model/policy_net.pth\')\n        torch.save(self.value_net.state_dict(), \'./SAC_model/value_net.pth\')\n        torch.save(self.Q_net1.state_dict(), \'./SAC_model/Q_net1.pth\')\n        torch.save(self.Q_net2.state_dict(), \'./SAC_model/Q_net1.pth\')\n        print(""===================================="")\n        print(""Model has been saved..."")\n        print(""===================================="")\n\n    def load(self):\n        self.policy_net.load_state_dict(torch.load(\'./SAC_model/policy_net.pth\'))\n        self.value_net.load_state_dict(torch.load( \'./SAC_model/value_net.pth\'))\n        self.Q_net1.load_state_dict(torch.load(\'./SAC_model/Q_net1.pth\'))\n        self.Q_net2.load_state_dict(torch.load(\'./SAC_model/Q_net1.pth\'))\n        print(""model has been load"")\n\n\ndef main():\n    agent = SAC()\n    ep_r = 0\n    if args.mode == \'test\':\n        agent.load()\n        for i in range(args.iteration):\n            state = env.reset()\n            for t in count():\n                action = agent.select_action(state)\n                next_state, reward, done, info = env.step(np.float32(action))\n                ep_r += reward\n                env.render()\n                if done:\n                    break\n                state = next_state\n    else:\n        print(""===================================="")\n        print(""Collection Experience..."")\n        print(""===================================="")\n\n\n        for i in range(args.iteration):\n            state = env.reset()\n            for t in range(200):\n                action = agent.select_action(state)\n                next_state, reward, done, info = env.step(np.float32(action))\n                ep_r += reward\n                if args.render and i >= args.render_interval : env.render()\n                agent.store(state, action, reward, next_state, done)\n\n                if agent.num_transition >= args.capacity:\n                    agent.update()\n\n                state = next_state\n                if done:\n                    if i > 100:\n                        print(""Ep_i \\t{}, the ep_r is \\t{}, the step is \\t{}"".format(i, ep_r, t))\n                    break\n            if i % args.log_interval == 0:\n                agent.save()\n            agent.writer.add_scalar(\'ep_r\', ep_r, global_step=i)\n            ep_r = 0\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Char10 TD3/TD3.py,30,"b'import argparse\nfrom collections import namedtuple\nfrom itertools import count\n\nimport os, sys, random\nimport numpy as np\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nfrom tensorboardX import SummaryWriter\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'--mode\', default=\'train\', type=str) # mode = \'train\' or \'test\'\nparser.add_argument(""--env_name"", default=""Pendulum-v0"")  # OpenAI gym environment name\xef\xbc\x8c BipedalWalker-v2\nparser.add_argument(\'--tau\',  default=0.005, type=float) # target smoothing coefficient\nparser.add_argument(\'--target_update_interval\', default=1, type=int)\nparser.add_argument(\'--iteration\', default=5, type=int)\n\nparser.add_argument(\'--learning_rate\', default=3e-4, type=float)\nparser.add_argument(\'--gamma\', default=0.99, type=int) # discounted factor\nparser.add_argument(\'--capacity\', default=50000, type=int) # replay buffer size\nparser.add_argument(\'--num_iteration\', default=100000, type=int) #  num of  games\nparser.add_argument(\'--batch_size\', default=100, type=int) # mini batch size\nparser.add_argument(\'--seed\', default=1, type=int)\n\n# optional parameters\nparser.add_argument(\'--num_hidden_layers\', default=2, type=int)\nparser.add_argument(\'--sample_frequency\', default=256, type=int)\nparser.add_argument(\'--activation\', default=\'Relu\', type=str)\nparser.add_argument(\'--render\', default=False, type=bool) # show UI or not\nparser.add_argument(\'--log_interval\', default=50, type=int) #\nparser.add_argument(\'--load\', default=False, type=bool) # load model\nparser.add_argument(\'--render_interval\', default=100, type=int) # after render_interval, the env.render() will work\nparser.add_argument(\'--policy_noise\', default=0.2, type=float)\nparser.add_argument(\'--noise_clip\', default=0.5, type=float)\nparser.add_argument(\'--policy_delay\', default=2, type=int)\nparser.add_argument(\'--exploration_noise\', default=0.1, type=float)\nparser.add_argument(\'--max_episode\', default=2000, type=int)\nparser.add_argument(\'--print_log\', default=5, type=int)\nargs = parser.parse_args()\n\n\n\n# Set seeds\n# env.seed(args.seed)\n# torch.manual_seed(args.seed)\n# np.random.seed(args.seed)\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nscript_name = os.path.basename(__file__)\nenv = gym.make(args.env_name)\n\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.shape[0]\nmax_action = float(env.action_space.high[0])\nmin_Val = torch.tensor(1e-7).float().to(device) # min value\n\ndirectory = \'./exp\' + script_name + args.env_name +\'./\'\n\'\'\'\nImplementation of TD3 with pytorch \nOriginal paper: https://arxiv.org/abs/1802.09477\nNot the author\'s implementation !\n\'\'\'\n\nclass Replay_buffer():\n    \'\'\'\n    Code based on:\n    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n    Expects tuples of (state, next_state, action, reward, done)\n    \'\'\'\n    def __init__(self, max_size=args.capacity):\n        self.storage = []\n        self.max_size = max_size\n        self.ptr = 0\n\n    def push(self, data):\n        if len(self.storage) == self.max_size:\n            self.storage[int(self.ptr)] = data\n            self.ptr = (self.ptr + 1) % self.max_size\n        else:\n            self.storage.append(data)\n\n    def sample(self, batch_size):\n        ind = np.random.randint(0, len(self.storage), size=batch_size)\n        x, y, u, r, d = [], [], [], [], []\n\n        for i in ind:\n            X, Y, U, R, D = self.storage[i]\n            x.append(np.array(X, copy=False))\n            y.append(np.array(Y, copy=False))\n            u.append(np.array(U, copy=False))\n            r.append(np.array(R, copy=False))\n            d.append(np.array(D, copy=False))\n\n        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1)\n\n\nclass Actor(nn.Module):\n\n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n\n        self.fc1 = nn.Linear(state_dim, 400)\n        self.fc2 = nn.Linear(400, 300)\n        self.fc3 = nn.Linear(300, action_dim)\n\n        self.max_action = max_action\n\n    def forward(self, state):\n        a = F.relu(self.fc1(state))\n        a = F.relu(self.fc2(a))\n        a = torch.tanh(self.fc3(a)) * self.max_action\n        return a\n\n\nclass Critic(nn.Module):\n\n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n\n        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n        self.fc2 = nn.Linear(400, 300)\n        self.fc3 = nn.Linear(300, 1)\n\n    def forward(self, state, action):\n        state_action = torch.cat([state, action], 1)\n\n        q = F.relu(self.fc1(state_action))\n        q = F.relu(self.fc2(q))\n        q = self.fc3(q)\n        return q\n\n\nclass TD3():\n    def __init__(self, state_dim, action_dim, max_action):\n\n        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n        self.critic_1 = Critic(state_dim, action_dim).to(device)\n        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n        self.critic_2 = Critic(state_dim, action_dim).to(device)\n        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n\n        self.actor_optimizer = optim.Adam(self.actor.parameters())\n        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters())\n        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters())\n\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n\n        self.max_action = max_action\n        self.memory = Replay_buffer(args.capacity)\n        self.writer = SummaryWriter(directory)\n        self.num_critic_update_iteration = 0\n        self.num_actor_update_iteration = 0\n        self.num_training = 0\n\n    def select_action(self, state):\n        state = torch.tensor(state.reshape(1, -1)).float().to(device)\n        return self.actor(state).cpu().data.numpy().flatten()\n\n    def update(self, num_iteration):\n\n        if self.num_training % 500 == 0:\n            print(""===================================="")\n            print(""model has been trained for {} times..."".format(self.num_training))\n            print(""===================================="")\n        for i in range(num_iteration):\n            x, y, u, r, d = self.memory.sample(args.batch_size)\n            state = torch.FloatTensor(x).to(device)\n            action = torch.FloatTensor(u).to(device)\n            next_state = torch.FloatTensor(y).to(device)\n            done = torch.FloatTensor(d).to(device)\n            reward = torch.FloatTensor(r).to(device)\n\n            # Select next action according to target policy:\n            noise = torch.ones_like(action).data.normal_(0, args.policy_noise).to(device)\n            noise = noise.clamp(-args.noise_clip, args.noise_clip)\n            next_action = (self.actor_target(next_state) + noise)\n            next_action = next_action.clamp(-self.max_action, self.max_action)\n\n            # Compute target Q-value:\n            target_Q1 = self.critic_1_target(next_state, next_action)\n            target_Q2 = self.critic_2_target(next_state, next_action)\n            target_Q = torch.min(target_Q1, target_Q2)\n            target_Q = reward + ((1 - done) * args.gamma * target_Q).detach()\n\n            # Optimize Critic 1:\n            current_Q1 = self.critic_1(state, action)\n            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n            self.critic_1_optimizer.zero_grad()\n            loss_Q1.backward()\n            self.critic_1_optimizer.step()\n            self.writer.add_scalar(\'Loss/Q1_loss\', loss_Q1, global_step=self.num_critic_update_iteration)\n\n            # Optimize Critic 2:\n            current_Q2 = self.critic_2(state, action)\n            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n            self.critic_2_optimizer.zero_grad()\n            loss_Q2.backward()\n            self.critic_2_optimizer.step()\n            self.writer.add_scalar(\'Loss/Q2_loss\', loss_Q2, global_step=self.num_critic_update_iteration)\n            # Delayed policy updates:\n            if i % args.policy_delay == 0:\n                # Compute actor loss:\n                actor_loss = - self.critic_1(state, self.actor(state)).mean()\n\n                # Optimize the actor\n                self.actor_optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor_optimizer.step()\n                self.writer.add_scalar(\'Loss/actor_loss\', actor_loss, global_step=self.num_actor_update_iteration)\n                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n                    target_param.data.copy_(((1- args.tau) * target_param.data) + args.tau * param.data)\n\n                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n                    target_param.data.copy_(((1 - args.tau) * target_param.data) + args.tau * param.data)\n\n                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n                    target_param.data.copy_(((1 - args.tau) * target_param.data) + args.tau * param.data)\n\n                self.num_actor_update_iteration += 1\n        self.num_critic_update_iteration += 1\n        self.num_training += 1\n\n    def save(self):\n        torch.save(self.actor.state_dict(), directory+\'actor.pth\')\n        torch.save(self.actor_target.state_dict(), directory+\'actor_target.pth\')\n        torch.save(self.critic_1.state_dict(), directory+\'critic_1.pth\')\n        torch.save(self.critic_1_target.state_dict(), directory+\'critic_1_target.pth\')\n        torch.save(self.critic_2.state_dict(), directory+\'critic_2.pth\')\n        torch.save(self.critic_2_target.state_dict(), directory+\'critic_2_target.pth\')\n        print(""===================================="")\n        print(""Model has been saved..."")\n        print(""===================================="")\n\n    def load(self):\n        self.actor.load_state_dict(torch.load(directory + \'actor.pth\'))\n        self.actor_target.load_state_dict(torch.load(directory + \'actor_target.pth\'))\n        self.critic_1.load_state_dict(torch.load(directory + \'critic_1.pth\'))\n        self.critic_1_target.load_state_dict(torch.load(directory + \'critic_1_target.pth\'))\n        self.critic_2.load_state_dict(torch.load(directory + \'critic_2.pth\'))\n        self.critic_2_target.load_state_dict(torch.load(directory + \'critic_2_target.pth\'))\n        print(""===================================="")\n        print(""model has been loaded..."")\n        print(""===================================="")\n\n\ndef main():\n    agent = TD3(state_dim, action_dim, max_action)\n    ep_r = 0\n\n    if args.mode == \'test\':\n        agent.load()\n        for i in range(args.iteration):\n            state = env.reset()\n            for t in count():\n                action = agent.select_action(state)\n                next_state, reward, done, info = env.step(np.float32(action))\n                ep_r += reward\n                env.render()\n                if done or t ==2000 :\n                    print(""Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}"".format(i, ep_r, t))\n                    break\n                state = next_state\n\n    elif args.mode == \'train\':\n        print(""===================================="")\n        print(""Collection Experience..."")\n        print(""===================================="")\n        if args.load: agent.load()\n        for i in range(args.num_iteration):\n            state = env.reset()\n            for t in range(2000):\n\n                action = agent.select_action(state)\n                action = action + np.random.normal(0, args.exploration_noise, size=env.action_space.shape[0])\n                action = action.clip(env.action_space.low, env.action_space.high)\n                next_state, reward, done, info = env.step(action)\n                ep_r += reward\n                if args.render and i >= args.render_interval : env.render()\n                agent.memory.push((state, next_state, action, reward, np.float(done)))\n                if i+1 % 10 == 0:\n                    print(\'Episode {},  The memory size is {} \'.format(i, len(agent.memory.storage)))\n                if len(agent.memory.storage) >= args.capacity-1:\n                    agent.update(10)\n\n                state = next_state\n                if done or t == args.max_episode -1:\n                    agent.writer.add_scalar(\'ep_r\', ep_r, global_step=i)\n                    if i % args.print_log == 0:\n                        print(""Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}"".format(i, ep_r, t))\n                    ep_r = 0\n                    break\n\n            if i % args.log_interval == 0:\n                agent.save()\n\n    else:\n        raise NameError(""mode wrong!!!"")\n\nif __name__ == \'__main__\':\n    main()\n'"
Char10 TD3/TD3_BipedalWalker-v2.py,30,"b'import argparse\nfrom collections import namedtuple\nfrom itertools import count\n\nimport os, sys, random\nimport numpy as np\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nfrom tensorboardX import SummaryWriter\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'--mode\', default=\'train\', type=str) # mode = \'train\' or \'test\'\n# OpenAI gym environment name, # [\'BipedalWalker-v2\', \'Pendulum-v0\'] or any continuous environment\n# Note that if you want test in another game, you should fine-tuning.\nparser.add_argument(""--env_name"", default=""BipedalWalker-v2"")\nparser.add_argument(\'--tau\',  default=0.005, type=float) # target smoothing coefficient\nparser.add_argument(\'--target_update_interval\', default=1, type=int)\nparser.add_argument(\'--test_iteration\', default=10, type=int)\n\nparser.add_argument(\'--learning_rate\', default=3e-4, type=float)\nparser.add_argument(\'--gamma\', default=0.99, type=int) # discounted factor\nparser.add_argument(\'--capacity\', default=50000, type=int) # replay buffer size\nparser.add_argument(\'--num_iteration\', default=100000, type=int) #  num of  games\nparser.add_argument(\'--batch_size\', default=100, type=int) # mini batch size\nparser.add_argument(\'--seed\', default=False, type=bool)\nparser.add_argument(\'--random_seed\', default=9527, type=int)\n# optional parameters\nparser.add_argument(\'--num_hidden_layers\', default=2, type=int)\nparser.add_argument(\'--sample_frequency\', default=256, type=int)\nparser.add_argument(\'--render\', default=False, type=bool) # show UI or not\nparser.add_argument(\'--log_interval\', default=50, type=int) #\nparser.add_argument(\'--load\', default=False, type=bool) # load model\nparser.add_argument(\'--render_interval\', default=100, type=int) # after render_interval, the env.render() will work\nparser.add_argument(\'--policy_noise\', default=0.2, type=float)\nparser.add_argument(\'--noise_clip\', default=0.5, type=float)\nparser.add_argument(\'--policy_delay\', default=2, type=int)\nparser.add_argument(\'--exploration_noise\', default=0.1, type=float)\nparser.add_argument(\'--max_episode\', default=2000, type=int)\nparser.add_argument(\'--print_log\', default=5, type=int)\nargs = parser.parse_args()\n\n\n\n\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nscript_name = os.path.basename(__file__)\nenv = gym.make(args.env_name)\nif args.seed:\n    env.seed(args.random_seed)\n    torch.manual_seed(args.random_seed)\n    np.random.seed(args.random_seed)\n\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.shape[0]\nmax_action = float(env.action_space.high[0])\nmin_Val = torch.tensor(1e-7).float().to(device) # min value\n\ndirectory = \'./exp\' + script_name + args.env_name +\'./\'\n\'\'\'\nImplementation of TD3 with pytorch \nOriginal paper: https://arxiv.org/abs/1802.09477\nNot the author\'s implementation !\n\'\'\'\n\nclass Replay_buffer():\n    \'\'\'\n    Code based on:\n    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n    Expects tuples of (state, next_state, action, reward, done)\n    \'\'\'\n    def __init__(self, max_size=args.capacity):\n        self.storage = []\n        self.max_size = max_size\n        self.ptr = 0\n\n    def push(self, data):\n        if len(self.storage) == self.max_size:\n            self.storage[int(self.ptr)] = data\n            self.ptr = (self.ptr + 1) % self.max_size\n        else:\n            self.storage.append(data)\n\n    def sample(self, batch_size):\n        ind = np.random.randint(0, len(self.storage), size=batch_size)\n        x, y, u, r, d = [], [], [], [], []\n\n        for i in ind:\n            X, Y, U, R, D = self.storage[i]\n            x.append(np.array(X, copy=False))\n            y.append(np.array(Y, copy=False))\n            u.append(np.array(U, copy=False))\n            r.append(np.array(R, copy=False))\n            d.append(np.array(D, copy=False))\n\n        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1)\n\n\nclass Actor(nn.Module):\n\n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n\n        self.fc1 = nn.Linear(state_dim, 400)\n        self.fc2 = nn.Linear(400, 300)\n        self.fc3 = nn.Linear(300, action_dim)\n\n        self.max_action = max_action\n\n    def forward(self, state):\n        a = F.relu(self.fc1(state))\n        a = F.relu(self.fc2(a))\n        a = torch.tanh(self.fc3(a)) * self.max_action\n        return a\n\n\nclass Critic(nn.Module):\n\n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n\n        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n        self.fc2 = nn.Linear(400, 300)\n        self.fc3 = nn.Linear(300, 1)\n\n    def forward(self, state, action):\n        state_action = torch.cat([state, action], 1)\n\n        q = F.relu(self.fc1(state_action))\n        q = F.relu(self.fc2(q))\n        q = self.fc3(q)\n        return q\n\n\nclass TD3():\n    def __init__(self, state_dim, action_dim, max_action):\n\n        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n        self.critic_1 = Critic(state_dim, action_dim).to(device)\n        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n        self.critic_2 = Critic(state_dim, action_dim).to(device)\n        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n\n        self.actor_optimizer = optim.Adam(self.actor.parameters())\n        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters())\n        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters())\n\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n\n        self.max_action = max_action\n        self.memory = Replay_buffer(args.capacity)\n        self.writer = SummaryWriter(directory)\n        self.num_critic_update_iteration = 0\n        self.num_actor_update_iteration = 0\n        self.num_training = 0\n\n    def select_action(self, state):\n        state = torch.tensor(state.reshape(1, -1)).float().to(device)\n        return self.actor(state).cpu().data.numpy().flatten()\n\n    def update(self, num_iteration):\n\n        if self.num_training % 500 == 0:\n            print(""===================================="")\n            print(""model has been trained for {} times..."".format(self.num_training))\n            print(""===================================="")\n        for i in range(num_iteration):\n            x, y, u, r, d = self.memory.sample(args.batch_size)\n            state = torch.FloatTensor(x).to(device)\n            action = torch.FloatTensor(u).to(device)\n            next_state = torch.FloatTensor(y).to(device)\n            done = torch.FloatTensor(d).to(device)\n            reward = torch.FloatTensor(r).to(device)\n\n            # Select next action according to target policy:\n            noise = torch.ones_like(action).data.normal_(0, args.policy_noise).to(device)\n            noise = noise.clamp(-args.noise_clip, args.noise_clip)\n            next_action = (self.actor_target(next_state) + noise)\n            next_action = next_action.clamp(-self.max_action, self.max_action)\n\n            # Compute target Q-value:\n            target_Q1 = self.critic_1_target(next_state, next_action)\n            target_Q2 = self.critic_2_target(next_state, next_action)\n            target_Q = torch.min(target_Q1, target_Q2)\n            target_Q = reward + ((1 - done) * args.gamma * target_Q).detach()\n\n            # Optimize Critic 1:\n            current_Q1 = self.critic_1(state, action)\n            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n            self.critic_1_optimizer.zero_grad()\n            loss_Q1.backward()\n            self.critic_1_optimizer.step()\n            self.writer.add_scalar(\'Loss/Q1_loss\', loss_Q1, global_step=self.num_critic_update_iteration)\n\n            # Optimize Critic 2:\n            current_Q2 = self.critic_2(state, action)\n            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n            self.critic_2_optimizer.zero_grad()\n            loss_Q2.backward()\n            self.critic_2_optimizer.step()\n            self.writer.add_scalar(\'Loss/Q2_loss\', loss_Q2, global_step=self.num_critic_update_iteration)\n            # Delayed policy updates:\n            if i % args.policy_delay == 0:\n                # Compute actor loss:\n                actor_loss = - self.critic_1(state, self.actor(state)).mean()\n\n                # Optimize the actor\n                self.actor_optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor_optimizer.step()\n                self.writer.add_scalar(\'Loss/actor_loss\', actor_loss, global_step=self.num_actor_update_iteration)\n                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n                    target_param.data.copy_(((1- args.tau) * target_param.data) + args.tau * param.data)\n\n                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n                    target_param.data.copy_(((1 - args.tau) * target_param.data) + args.tau * param.data)\n\n                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n                    target_param.data.copy_(((1 - args.tau) * target_param.data) + args.tau * param.data)\n\n                self.num_actor_update_iteration += 1\n        self.num_critic_update_iteration += 1\n        self.num_training += 1\n\n    def save(self):\n        torch.save(self.actor.state_dict(), directory+\'actor.pth\')\n        torch.save(self.actor_target.state_dict(), directory+\'actor_target.pth\')\n        torch.save(self.critic_1.state_dict(), directory+\'critic_1.pth\')\n        torch.save(self.critic_1_target.state_dict(), directory+\'critic_1_target.pth\')\n        torch.save(self.critic_2.state_dict(), directory+\'critic_2.pth\')\n        torch.save(self.critic_2_target.state_dict(), directory+\'critic_2_target.pth\')\n        print(""===================================="")\n        print(""Model has been saved..."")\n        print(""===================================="")\n\n    def load(self):\n        self.actor.load_state_dict(torch.load(directory + \'actor.pth\'))\n        self.actor_target.load_state_dict(torch.load(directory + \'actor_target.pth\'))\n        self.critic_1.load_state_dict(torch.load(directory + \'critic_1.pth\'))\n        self.critic_1_target.load_state_dict(torch.load(directory + \'critic_1_target.pth\'))\n        self.critic_2.load_state_dict(torch.load(directory + \'critic_2.pth\'))\n        self.critic_2_target.load_state_dict(torch.load(directory + \'critic_2_target.pth\'))\n        print(""===================================="")\n        print(""model has been loaded..."")\n        print(""===================================="")\n\n\ndef main():\n    agent = TD3(state_dim, action_dim, max_action)\n    ep_r = 0\n\n    if args.mode == \'test\':\n        agent.load()\n        for i in range(args.test_iteration):\n            state = env.reset()\n            for t in count():\n                action = agent.select_action(state)\n                next_state, reward, done, info = env.step(np.float32(action))\n                ep_r += reward\n                env.render()\n                if done or t ==2000 :\n                    print(""Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}"".format(i, ep_r, t))\n                    ep_r = 0\n                    break\n                state = next_state\n\n\n    elif args.mode == \'train\':\n        print(""===================================="")\n        print(""Collection Experience..."")\n        print(""===================================="")\n        if args.load: agent.load()\n        for i in range(args.num_iteration):\n            state = env.reset()\n            for t in range(2000):\n\n                action = agent.select_action(state)\n                action = action + np.random.normal(0, args.exploration_noise, size=env.action_space.shape[0])\n                action = action.clip(env.action_space.low, env.action_space.high)\n                next_state, reward, done, info = env.step(action)\n                ep_r += reward\n                if args.render and i >= args.render_interval : env.render()\n                agent.memory.push((state, next_state, action, reward, np.float(done)))\n                if i+1 % 10 == 0:\n                    print(\'Episode {},  The memory size is {} \'.format(i, len(agent.memory.storage)))\n                if len(agent.memory.storage) >= args.capacity-1:\n                    agent.update(10)\n\n                state = next_state\n                if done or t == args.max_episode -1:\n                    agent.writer.add_scalar(\'ep_r\', ep_r, global_step=i)\n                    if i % args.print_log == 0:\n                        print(""Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}"".format(i, ep_r, t))\n                    ep_r = 0\n                    break\n\n            if i % args.log_interval == 0:\n                agent.save()\n\n    else:\n        raise NameError(""mode wrong!!!"")\n\nif __name__ == \'__main__\':\n    main()\n'"
More/plot.py,0,"b""import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport os\nsns.set(style='darkgrid')\n\ndef get_info(filename):\n    filename = filename.replace('.npy', '') # remove .npy\n    algo, env, seed = re.split('_', filename)\n    seed = int(seed)\n    return algo, env, seed\n\n\ndef get_file_name(path='./'):\n    file_names = []\n    for _, __, file_name in os.walk(path):\n        file_names += file_name\n    data_name = [f for f in file_names if '.npy' in f]\n    return data_name\n\ndef exact_data(file_name, steps):\n    '''\n    exact data from single .npy file\n    :param file_name:\n    :return: a Dataframe include time, seed, algo_name, avg_reward\n    '''\n    avg_reward = np.load(file_name).reshape(-1, 1)\n    algo, env_name, seed = get_info(file_name)\n    df = pd.DataFrame(avg_reward)\n    df.columns = ['Average Return']\n    df['Time Steps (1e6)'] = steps\n    df['Algorithm'] = algo\n    df['env'] = env_name\n    df['seed'] = seed\n    return df\n\n\nif __name__ == '__main__':\n    file_names = get_file_name('./')\n    _, env_name, __ = get_info(file_names[0])\n    df = pd.DataFrame([])\n    steps = np.linspace(0, 1, 201)\n    for file in file_names:\n        data = exact_data(file, steps)\n        df = pd.concat([df, data], axis=0)\n    sns.lineplot(x='Time Steps (1e6)', y='Average Return', data=df, hue='Algorithm',ci=90)\n    plt.title(env_name)\n    plt.savefig(env_name + '.svg')\n    plt.show()\n"""
