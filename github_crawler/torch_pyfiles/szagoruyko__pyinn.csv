file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\nimport os\nimport shutil\nimport sys\nfrom setuptools import setup, find_packages\n\nVERSION = \'0.0.1\'\n\nlong_description = ""Manually fused PyTorch NN ops""\n\nsetup_info = dict(\n    # Metadata\n    name=\'pyinn\',\n    version=VERSION,\n    author=\'Sergey Zagoruyko\',\n    author_email=\'sergey.zagoruyko@enpc.fr\',\n    url=\'https://github.com/szagoruyko/pyinn\',\n    description=\'Manually fused PyTorch NN ops\',\n    long_description=long_description,\n    license=\'BSD\',\n\n    # Package info\n    packages=find_packages(exclude=(\'test\',)),\n\n    zip_safe=True,\n\n    install_requires=[\n        \'torch\',\n        \'cupy\',\n        # \'scikit-cuda\',\n    ]\n)\n\nsetup(**setup_info)\n'"
pyinn/__init__.py,0,"b'from .ncrelu import ncrelu\nfrom .dgmm import dgmm\nfrom .cdgmm import cdgmm\nfrom .im2col import im2col, col2im\nfrom .conv2d_depthwise import conv2d_depthwise\nfrom .modules import Conv2dDepthwise\n'"
pyinn/cdgmm.py,8,"b'import torch\nfrom pyinn.utils import Stream, load_kernel\n\n\nkernel = """"""\nextern ""C""\n__global__ void swap(float2 *x, int total)\n{\n   int tx = blockIdx.x * blockDim.x + threadIdx.x;\n   if(tx >= total)\n      return;\n\n   float2 v = x[tx];\n   //x[tx] = make_float2(v.y, v.x);\n   x[tx] = make_float2(v.x, -v.y);\n}\n""""""\n\nCUDA_NUM_THREADS = 1024\n\n\ndef GET_BLOCKS(N, K=CUDA_NUM_THREADS):\n    return (N + K - 1) // K\n\n\ndef swap(x):\n    assert x.size(-1) == 2\n    total = x.numel() // 2\n    with torch.cuda.device_of(x):\n        f = load_kernel(\'swap\', kernel)\n        f(args=[x.data_ptr(), total],\n          block=(CUDA_NUM_THREADS,1,1),\n          grid=(GET_BLOCKS(total),1,1),\n          stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n\n\ndef cublas_cdgmm(A, x, out=None):\n    if out is not None:\n        assert out.is_contiguous() and out.size() == A.size()\n    else:\n        out = A.new(A.size())\n    assert x.dim() == 2 and x.size(-1) == 2 and A.size(-1) == 2\n    assert A.dim() == 3\n    assert x.size(0) == A.size(1) or x.size(0) == A.size(0)\n    assert A.type() == x.type() == out.type()\n    assert A.is_contiguous()\n\n    if not isinstance(A, (torch.cuda.FloatTensor, torch.cuda.DoubleTensor)):\n        raise NotImplementedError\n    else:\n        m, n = A.size(1), A.size(0)\n        if x.size(0) == A.size(1):\n            mode = \'l\'\n        elif x.size(0) == A.size(0):\n            mode = \'r\'\n        lda, ldc = m, m\n        incx = 1\n        handle = torch.cuda.current_blas_handle()\n        stream = torch.cuda.current_stream()._as_parameter_\n        from skcuda import cublas\n        cublas.cublasSetStream(handle, stream)\n        args = [handle, mode, m, n, A.data_ptr(), lda, x.data_ptr(), incx, out.data_ptr(), ldc]\n        if isinstance(A, torch.cuda.FloatTensor):\n            cublas.cublasCdgmm(*args)\n        elif isinstance(A, torch.cuda.DoubleTensor):\n            cublas.cublasZdgmm(*args)\n        return out\n\n\nclass CDGMM(torch.autograd.Function):\n    def forward(self, input, x):\n        self.save_for_backward(input, x)\n        return cublas_cdgmm(input, x)\n\n    def backward(self, grad_output):\n        input, x = self.saved_tensors\n        grad_input = grad_x = None\n        if self.needs_input_grad[0]:\n            grad_output = grad_output.contiguous()\n            swap(x)\n            grad_input = cublas_cdgmm(grad_output.contiguous(), x)\n            swap(x)\n            \n            assert grad_input.size() == input.size()\n        if self.needs_input_grad[1]:\n            raise NotImplementedError\n            # dim = 0 if x.size(0) == input.size(1) else 1\n            # grad_x = (grad_output * input).sum(dim).squeeze(dim)\n            # assert grad_x.size() == x.size()\n        return grad_input, grad_x\n\n\ndef cdgmm(input, x):\n    """"""Complex multiplication with a diagonal matrix.\n\n    Does `input.mm(x.diag())` where input and x are complex.\n\n    Args:\n        input: 3D tensor with last dimension of size 2\n        x: 2D tensor with last dimension of size 2\n    """"""\n    return CDGMM()(input, x)\n'"
pyinn/conv2d_depthwise.py,8,"b'from torch.autograd import Function\nimport torch\nfrom torch.nn.modules.utils import _pair\nfrom pyinn.utils import Dtype, Stream, load_kernel\nimport torch.nn.functional as F\n\nCUDA_NUM_THREADS = 1024\n\nkernel_loop = \'\'\'\n#define CUDA_KERNEL_LOOP(i, n)                        \\\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \\\n      i < (n);                                       \\\n      i += blockDim.x * gridDim.x)\n\'\'\'\n\n\ndef GET_BLOCKS(N):\n    return (N + CUDA_NUM_THREADS - 1) // CUDA_NUM_THREADS\n\n\n_conv2d_depthwise_kernel = kernel_loop + \'\'\'\nextern ""C""\n__global__ void conv2d_dw_forward_kernel(\nconst ${Dtype}* bottom_data, const ${Dtype}* weight_data, ${Dtype}* top_data) {\n  CUDA_KERNEL_LOOP(index, ${nthreads}) {\n    const int n = index / ${channels} / ${top_height} / ${top_width};\n    const int c = (index / ${top_height} / ${top_width}) % ${channels};\n    const int h = (index / ${top_width}) % ${top_height};\n    const int w = index % ${top_width};\n    const ${Dtype}* weight = weight_data + c * ${kernel_h} * ${kernel_w};\n    ${Dtype} value = 0;\n    for (int kh = 0; kh < ${kernel_h}; ++kh) {\n      for (int kw = 0; kw < ${kernel_w}; ++kw) {\n        const int h_in = -${pad_h} + h * ${stride_h} + kh * ${dilation_h};\n        const int w_in = -${pad_w} + w * ${stride_w} + kw * ${dilation_w};\n        if ((h_in >= 0) && (h_in < ${bottom_height})\n          && (w_in >= 0) && (w_in < ${bottom_width})) {\n          const int offset = ((n * ${channels} + c) * ${bottom_height} + h_in)\n            * ${bottom_width} + w_in;\n          value += (*weight) * bottom_data[offset];\n        }\n        ++weight;\n      }\n    }\n    top_data[index] = value;\n  }\n}\n\'\'\'\n\n\n_conv2d_depthwise_kernel_backward_grad_input = kernel_loop + \'\'\'\nextern ""C""\n__global__ void conv2d_dw_backward_grad_input_kernel(\n    const ${Dtype}* const top_diff, const ${Dtype}* const weight_data, ${Dtype}* const bottom_diff) {\n  CUDA_KERNEL_LOOP(index, ${nthreads}) {\n    const int n = index / ${channels} / ${bottom_height} / ${bottom_width};\n    const int c = (index / ${bottom_height} / ${bottom_width}) % ${channels};\n    const int h = (index / ${bottom_width}) % ${bottom_height};\n    const int w = index % ${bottom_width};\n    const ${Dtype}* weight = weight_data + c * ${kernel_h} * ${kernel_w};\n    ${Dtype} value = 0;\n    for (int kh = 0; kh < ${kernel_h}; ++kh) {\n      for (int kw = 0; kw < ${kernel_w}; ++kw) {\n        const int h_out_s = h + ${pad_h} - kh * ${dilation_h};\n        const int w_out_s = w + ${pad_w} - kw * ${dilation_w};\n        if (((h_out_s % ${stride_h}) == 0) && ((w_out_s % ${stride_w}) == 0)) {\n          const int h_out = h_out_s / ${stride_h};\n          const int w_out = w_out_s / ${stride_w};\n          if ((h_out >= 0) && (h_out < ${top_height})\n                && (w_out >= 0) && (w_out < ${top_width})) {\n            const int offset = ((n * ${channels} + c) * ${top_height} + h_out)\n                  * ${top_width} + w_out;\n            value += (*weight) * top_diff[offset];\n          }\n        }\n        ++weight;\n      }\n    }\n    bottom_diff[index] = value;\n  }\n}\n\'\'\'\n\n\n_conv2d_depthwise_kernel_backward_grad_weight = kernel_loop + \'\'\'\nextern ""C""\n__global__ void conv2d_dw_backward_grad_weight_kernel(\n    const ${Dtype}* const top_diff, const ${Dtype}* const bottom_data, ${Dtype}* const buffer_data) {\n  CUDA_KERNEL_LOOP(index, ${nthreads}) {\n    const int h = (index / ${top_width}) % ${top_height};\n    const int w = index % ${top_width};\n    const int kh = (index / ${kernel_w} / ${num} / ${top_height} / ${top_width})\n          % ${kernel_h};\n    const int kw = (index / ${num} / ${top_height} / ${top_width}) % ${kernel_w};\n    const int h_in = -${pad_h} + h * ${stride_h} + kh * ${dilation_h};\n    const int w_in = -${pad_w} + w * ${stride_w} + kw * ${dilation_w};\n    if ((h_in >= 0) && (h_in < ${bottom_height})\n          && (w_in >= 0) && (w_in < ${bottom_width})) {\n      const int c = index / ${kernel_h} / ${kernel_w} / ${num} / ${top_height} / ${top_width};\n      const int n = (index / ${top_height} / ${top_width}) % ${num};\n      const int top_offset = ((n * ${channels} + c) * ${top_height} + h)\n            * ${top_width} + w;\n      const int bottom_offset = ((n * ${channels} + c) * ${bottom_height} + h_in)\n            * ${bottom_width} + w_in;\n      buffer_data[index] = top_diff[top_offset] * bottom_data[bottom_offset];\n    } else {\n      buffer_data[index] = 0;\n    }\n  }\n}\n\'\'\'\n\n\nclass Conv2dDepthwise(Function):\n\n    def __init__(self, stride, padding, dilation):\n        super(Conv2dDepthwise, self).__init__()\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n\n    def forward(self, input, weight):\n        assert input.dim() == 4 and input.is_cuda and weight.is_cuda\n        batch_size, channels, height, width = input.size()\n        kernel_h, kernel_w = weight.size()[2:]\n        output_h = int((height + 2 * self.padding[0] - (self.dilation[0] * (kernel_h - 1) + 1)) / self.stride[0] + 1)\n        output_w = int((width + 2 * self.padding[1] - (self.dilation[1] * (kernel_w - 1) + 1)) / self.stride[1] + 1)\n\n        output = input.new(batch_size, channels, output_h, output_w)\n        n = output.numel()\n\n        with torch.cuda.device_of(input):\n            f = load_kernel(\'conv2d_dw_forward_kernel\', _conv2d_depthwise_kernel, Dtype=Dtype(input), nthreads=n,\n                            num=batch_size, channels=channels,\n                            bottom_height=height, bottom_width=width,\n                            top_height=output_h, top_width=output_w,\n                            kernel_h=kernel_h, kernel_w=kernel_w,\n                            stride_h=self.stride[0], stride_w=self.stride[1],\n                            dilation_h=self.dilation[0], dilation_w=self.dilation[1],\n                            pad_h=self.padding[0], pad_w=self.padding[1])\n            f(block=(CUDA_NUM_THREADS,1,1),\n              grid=(GET_BLOCKS(n),1,1),\n              args=[input.data_ptr(), weight.data_ptr(), output.data_ptr()],\n              stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n\n        self.save_for_backward(input, weight)\n        return output\n\n    def backward(self, grad_output):\n        assert grad_output.is_cuda and grad_output.is_contiguous()\n        input, weight = self.saved_tensors\n\n        batch_size, channels, height, width = input.size()\n        kernel_h, kernel_w = weight.size()[2:]\n        output_h, output_w = grad_output.size()[2:]\n\n        grad_input, grad_weight = None, None\n\n        opt = dict(Dtype=Dtype(grad_output),\n                   num=batch_size, channels=channels,\n                   bottom_height=height, bottom_width=width,\n                   top_height=output_h, top_width=output_w,\n                   kernel_h=kernel_h, kernel_w=kernel_w,\n                   stride_h=self.stride[0], stride_w=self.stride[1],\n                   dilation_h=self.dilation[0], dilation_w=self.dilation[1],\n                   pad_h=self.padding[0], pad_w=self.padding[1])\n\n        with torch.cuda.device_of(input):\n            if self.needs_input_grad[0]:\n                grad_input = input.new(input.size())\n\n                n = grad_input.numel()\n                opt[\'nthreads\'] = n\n\n                f = load_kernel(\'conv2d_dw_backward_grad_input_kernel\',\n                                _conv2d_depthwise_kernel_backward_grad_input, **opt)\n                f(block=(CUDA_NUM_THREADS,1,1),\n                  grid=(GET_BLOCKS(n),1,1),\n                  args=[grad_output.data_ptr(), weight.data_ptr(), grad_input.data_ptr()],\n                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n\n            if self.needs_input_grad[1]:\n                weight_buffer = weight.new(channels, kernel_h, kernel_w, batch_size, output_h, output_w)\n\n                n = weight_buffer.numel()\n                opt[\'nthreads\'] = n\n\n                f = load_kernel(\'conv2d_dw_backward_grad_weight_kernel\',\n                                _conv2d_depthwise_kernel_backward_grad_weight, **opt)\n                f(block=(CUDA_NUM_THREADS,1,1),\n                  grid=(GET_BLOCKS(n),1,1),\n                  args=[grad_output.data_ptr(), input.data_ptr(), weight_buffer.data_ptr()],\n                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n                grad_weight = weight_buffer.view(weight.size() + (-1,)).sum(-1)\n\n        return grad_input, grad_weight\n\n\ndef conv2d_depthwise(input, weight, bias=None, stride=1, padding=0, dilation=1):\n    """"""Depthwise 2D convolution.\n\n    Implements depthwise convolution as in https://arxiv.org/pdf/1704.04861v1.pdf\n    MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\n\n    CUDA kernels from https://github.com/BVLC/caffe/pull/5665\n    CPU side is done by F.conv2d\n\n    Equivalent to:\n        `F.conv2d(input, weight, groups=input.size(1))`\n    """"""\n    assert input.size(1) == weight.size(0)\n    if input.is_cuda:\n        out = Conv2dDepthwise(stride, padding, dilation)(input, weight)\n        if bias is not None:\n            out += bias.view(1,-1,1,1)\n    else:\n        groups = input.size(1)\n        out = F.conv2d(input, weight, bias, stride, padding, dilation, groups)\n    return out\n'"
pyinn/dgmm.py,8,"b'import torch\n\n\ndef cublas_dgmm(A, x, out=None):\n    if out is not None:\n        assert out.is_contiguous() and out.size() == A.size()\n    else:\n        out = A.new(A.size())\n    assert x.dim() == 1\n    assert x.numel() == A.size(-1) or x.numel() == A.size(0)\n    assert A.type() == x.type() == out.type()\n    assert A.is_contiguous()\n\n    if not isinstance(A, (torch.cuda.FloatTensor, torch.cuda.DoubleTensor)):\n        if x.numel() == A.size(-1):\n            return A.mm(torch.diag(x), out=out.view_as(A))\n        else:\n            return torch.diag(x).mm(A, out=out.view_as(A))\n    else:\n        if x.numel() == A.size(-1):\n            m, n =  A.size(-1), A.numel() // A.size(-1)\n            mode = \'l\'\n            # A.mm(x.diag(), out=out)\n            # return out\n        elif x.numel() == A.size(0):\n            n, m = A.size(0), A.numel() // A.size(0)\n            mode = \'r\'\n            # if A.stride(0) == 1:\n            #     mode = \'l\'\n            #     n, m = m, n\n            # x.diag().mm(A, out=out)\n            # return out\n        lda, ldc = m, m\n        incx = 1\n        handle = torch.cuda.current_blas_handle()\n        stream = torch.cuda.current_stream()._as_parameter_\n        from skcuda import cublas\n        cublas.cublasSetStream(handle, stream)\n        args = [handle, mode, m, n, A.data_ptr(), lda, x.data_ptr(), incx, out.data_ptr(), ldc]\n        if isinstance(A, torch.cuda.FloatTensor):\n            cublas.cublasSdgmm(*args)\n        elif isinstance(A, torch.cuda.DoubleTensor):\n            cublas.cublasDdgmm(*args)\n        return out\n\n\nclass DGMM(torch.autograd.Function):\n    def forward(self, input, x):\n        self.save_for_backward(input, x)\n        return cublas_dgmm(input, x)\n\n    def backward(self, grad_output):\n        input, x = self.saved_tensors\n        grad_input = grad_x = None\n        if self.needs_input_grad[0]:\n            grad_input = cublas_dgmm(grad_output.contiguous(), x)\n            assert grad_input.size() == input.size()\n        if self.needs_input_grad[1]:\n            dim = 0 if x.numel() == input.size(-1) else 1\n            grad_x = (grad_output * input).sum(dim).squeeze(dim)\n            # grad_x = grad_output.t().mm(input).diag()\n            assert grad_x.size() == x.size()\n        return grad_input, grad_x\n\n\ndef dgmm(input, x):\n    """"""Multiplication with a diagonal matrix.\n\n    Used CUDA dgmm function, sometimes is faster than expand.\n\n    In torch functions does `input.mm(x.diag())`. Both left and right\n    mutliplications are supported.\n\n    Args:\n        input: 2D tensor\n        x: 1D tensor\n    """"""\n    return DGMM()(input, x)\n'"
pyinn/im2col.py,7,"b'from torch.autograd import Function\nimport torch\nfrom torch.nn.modules.utils import _pair\nfrom pyinn.utils import Dtype, Stream, load_kernel\n\nCUDA_NUM_THREADS = 1024\n\n\ndef GET_BLOCKS(N):\n    return (N + CUDA_NUM_THREADS - 1) // CUDA_NUM_THREADS\n\n\n_im2col_kernel = \'\'\'\n#define CUDA_KERNEL_LOOP(i, n)                        \\\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \\\n      i < (n);                                       \\\n      i += blockDim.x * gridDim.x)\n\n// Kernel for fast unfold+copy\n// (borrowed from Caffe: https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu)\nextern ""C""\n__global__ void im2col_kernel(const ${Dtype}* data_im, ${Dtype}* data_col) {\n  CUDA_KERNEL_LOOP(index, ${n}) {\n    int w_out = index % ${width_col};\n    index /= ${width_col};\n    int h_out = index % ${height_col};\n    int channel_in = index / ${height_col};\n    int channel_out = channel_in * ${ksize_h} * ${ksize_w};\n    int h_in = h_out * ${stride_h} - ${pad_h};\n    int w_in = w_out * ${stride_w} - ${pad_w};\n    data_col += (channel_out * ${height_col} + h_out) * ${width_col} + w_out;\n    data_im += (channel_in * ${height} + h_in) * ${width} + w_in;\n    #pragma unroll\n    for (int i = 0; i < ${ksize_h}; ++i) {\n      for (int j = 0; j < ${ksize_w}; ++j) {\n        int h = h_in + i;\n        int w = w_in + j;\n        *data_col = (h >= 0 && w >= 0 && h < ${height} && w < ${width}) ?\n          data_im[i * ${width} + j] : 0;\n        data_col += ${height_col} * ${width_col};\n      }\n    }\n  }\n}\n\'\'\'\n\n\n_col2im_kernel = \'\'\'\n#define CUDA_KERNEL_LOOP(i, n)                        \\\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; \\\n      i < (n);                                       \\\n      i += blockDim.x * gridDim.x)\n\nextern ""C""\n__global__ void col2im_kernel(const ${Dtype}* data_col, ${Dtype}* data_im) {\n  CUDA_KERNEL_LOOP(index, ${n}) {\n    ${Dtype} val = 0;\n    int w = index % ${width} + ${pad_w};\n    int h = (index / ${width}) % ${height} + ${pad_h};\n    int c = index / (${width} * ${height});\n    // compute the start and end of the output\n    int w_col_start = (w < ${ksize_w}) ? 0 : (w - ${ksize_w}) / ${stride_w} + 1;\n    int w_col_end = min(w / ${stride_w} + 1, ${width_col});\n    int h_col_start = (h < ${ksize_h}) ? 0 : (h - ${ksize_h}) / ${stride_h} + 1;\n    int h_col_end = min(h / ${stride_h} + 1, ${height_col});\n\n    // equivalent implementation\n    int offset = (c * ${ksize_h} * ${ksize_w} + h * ${ksize_w} + w) * ${height_col} * ${width_col};\n    int coeff_h_col = (1 - ${stride_h} * ${ksize_w} * ${height_col}) * ${width_col};\n    int coeff_w_col = (1 - ${stride_w} * ${height_col} * ${width_col});\n    #pragma unroll\n    for (int h_col = h_col_start; h_col < h_col_end; ++h_col) {\n      for (int w_col = w_col_start; w_col < w_col_end; ++w_col) {\n        val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];\n      }\n    }\n    data_im[index] = val;\n  }\n}\n\'\'\'\n\n\ndef im2col_shape(size, kernel_size, stride, padding):\n    ksize_h, ksize_w = _pair(kernel_size)\n    stride_h, stride_w = _pair(stride)\n    pad_h, pad_w = _pair(padding)\n    n_input_plane, height, width = size\n    height_col = (height + 2 * pad_h - ksize_h) // stride_h + 1\n    width_col = (width + 2 * pad_w - ksize_w) // stride_w + 1\n    return n_input_plane, ksize_h, ksize_w, height_col, width_col\n\n\ndef _im2col(data, kernel_size, stride, padding, out=None):\n    assert data.dim() == 3 and data.is_cuda\n    ksize_h, ksize_w = _pair(kernel_size)\n    stride_h, stride_w = _pair(stride)\n    pad_h, pad_w = _pair(padding)\n    n_input_plane, height, width = data.size()\n    height_col = (height + 2 * pad_h - ksize_h) // stride_h + 1\n    width_col = (width + 2 * pad_w - ksize_w) // stride_w + 1\n    n = n_input_plane * height_col * width_col\n\n    shape = torch.Size((n_input_plane, ksize_h, ksize_w, height_col, width_col))\n    if out is not None:\n        assert out.size() == shape\n        data_col = out\n    else:\n        data_col = data.new(*shape)\n\n    with torch.cuda.device_of(data):\n        f = load_kernel(\'im2col_kernel\', _im2col_kernel, Dtype=Dtype(data), n=n,\n                        height_col=height_col,\n                        width_col=width_col,\n                        height=height, width=width,\n                        ksize_h=ksize_h, ksize_w=ksize_w,\n                        pad_h=pad_h, pad_w=pad_w,\n                        stride_h=stride_h, stride_w=stride_w,\n                        channels=n_input_plane)\n        f(block=(CUDA_NUM_THREADS,1,1),\n          grid=(GET_BLOCKS(n),1,1),\n          args=[data.data_ptr(), data_col.data_ptr()],\n          stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n    return data_col\n\n\ncol2im_modules = {}\n\n\ndef col2im_shape(size, kernel_size, stride, padding, input_size=None):\n    ksize_h, ksize_w = _pair(kernel_size)\n    stride_h, stride_w = _pair(stride)\n    pad_h, pad_w = _pair(padding)\n    n_input_plane, ksize_h, ksize_w, height_col, width_col = size\n    if input_size is not None:\n        height, width = input_size\n    else:\n        height = (height_col - 1) * stride_h - 2 * pad_h + ksize_h\n        width = (width_col - 1) * stride_w - 2 * pad_w + ksize_w\n    return n_input_plane, height, width\n\ndef _col2im(data_col, kernel_size, stride, padding, out=None, input_size=None):\n    assert data_col.dim() == 5\n    ksize_h, ksize_w = _pair(kernel_size)\n    stride_h, stride_w = _pair(stride)\n    pad_h, pad_w = _pair(padding)\n    n_input_plane, ksize_h, ksize_w, height_col, width_col = data_col.size()\n    if input_size is not None:\n        height, width = input_size\n    else:\n        height = (height_col - 1) * stride_h - 2 * pad_h + ksize_h\n        width = (width_col - 1) * stride_w - 2 * pad_w + ksize_w\n    n = n_input_plane * height * width\n\n    if out is not None:\n        assert tuple(out.size()) == (n_input_plane, height, width)\n        data = out\n    else:\n        data = data_col.new(n_input_plane, height, width)\n\n    with torch.cuda.device_of(data_col):\n        f = load_kernel(\'col2im_kernel\', _col2im_kernel, Dtype=Dtype(data), n=n,\n                        height_col=height_col,\n                        width_col=width_col,\n                        height=height, width=width,\n                        ksize_h=ksize_h, ksize_w=ksize_w,\n                        pad_h=pad_h, pad_w=pad_w,\n                        stride_h=stride_h, stride_w=stride_w,\n                        channels=n_input_plane)\n        f(block=(CUDA_NUM_THREADS,1,1),\n          grid=(GET_BLOCKS(n),1,1),\n          args=[data_col.data_ptr(), data.data_ptr()],\n          stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n    return data\n\n\ndef im2col_batch(input, kernel_size, stride, padding):\n    if input.dim() == 3:\n        return _im2col(input, kernel_size, stride, padding)\n    elif input.dim() == 4:\n        shape = (input.size(0),) + im2col_shape(input.size()[1:], kernel_size, stride, padding)\n        out = input.new(*shape)\n        for x, o in zip(input, out):\n            _im2col(x, kernel_size, stride, padding, out=o)\n        return out\n\n\ndef col2im_batch(grad_output, kernel_size, stride, padding, input_size=None):\n    if grad_output.dim() == 5:\n        return _col2im(grad_output, kernel_size, stride, padding, out=None, input_size=input_size)\n    elif grad_output.dim() == 6:\n        shape = (grad_output.size(0),) + col2im_shape(grad_output.size()[1:], kernel_size, stride, padding, input_size)\n        grad_input = grad_output.new(*shape)\n        for go, gx in zip(grad_output, grad_input):\n            _col2im(go, kernel_size, stride, padding, out=gx, input_size=input_size)\n        return grad_input\n\n\nclass Im2Col(Function):\n    def __init__(self, kernel_size, stride, padding):\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, input):\n        assert(input.is_contiguous())\n        self.input_size = input.size()[-2:]\n        return im2col_batch(input, self.kernel_size, self.stride, self.padding)\n\n    def backward(self, grad_output):\n        if not grad_output.is_contiguous():\n            grad_output = grad_output.contiguous()\n        assert(grad_output.is_contiguous())\n        return col2im_batch(grad_output, self.kernel_size, self.stride, self.padding, self.input_size)\n\n\nclass Col2Im(Function):\n    def __init__(self, kernel_size, stride, padding, input_size=None):\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.input_size = input_size\n\n    def forward(self, input):\n        assert(input.is_contiguous())\n        return col2im_batch(input, self.kernel_size, self.stride, self.padding, self.input_size)\n\n    def backward(self, grad_output):\n        if not grad_output.is_contiguous():\n            grad_output = grad_output.contiguous()\n        assert(grad_output.is_contiguous())        \n        return im2col_batch(grad_output, self.kernel_size, self.stride, self.padding)\n\n\ndef im2col(input, kernel_size, stride, padding):\n    """"""Rearrange image blocks into columns\n\n    The representation is used in GEMM-based convolution.\n    Output is 5D (or 6D in case of minibatch) tensor.\n\n    Minibatch implementation is inefficient, and could be done in a single CUDA kernel.\n\n    TODO: add CPU version (via numpy?)\n    """"""\n    return Im2Col(kernel_size, stride, padding)(input)\n\n\ndef col2im(input, kernel_size, stride, padding):\n    """"""Converts columns back to NCHW format.\n\n    This is used in backward wrt inputs in GEMM-based convolution.\n    """"""\n    return Col2Im(kernel_size, stride, padding)(input)\n'"
pyinn/modules.py,0,"b'from torch import nn\nimport pyinn as P\n\n\nclass Conv2dDepthwise(nn.Conv2d):\n    """"""Depthwise 2D convolution.\n\n    Implements depthwise convolution as in https://arxiv.org/pdf/1704.04861v1.pdf\n    MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\n\n    CUDA kernels from https://github.com/BVLC/caffe/pull/5665\n    CPU side is done by F.conv2d\n\n    Equivalent to:\n        `nn.Conv2d(channels, channels, kernel_size, groups=channels)`\n\n    Args:\n        channels (int): Number of channels in the input image\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int or tuple, optional): Stride of the convolution\n        padding (int or tuple, optional): Zero-padding added to both sides of the input\n        dilation (int or tuple, optional): Spacing between kernel elements\n        bias (bool, optional): If True, adds a learnable bias to the output\n\n    Shape:\n        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`\n        - Output: :math:`(N, C_{in}, H_{out}, W_{out})` where\n          :math:`H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\\_size[0] - 1) - 1) / stride[0] + 1)`\n          :math:`W_{out} = floor((W_{in}  + 2 * padding[1] - dilation[1] * (kernel\\_size[1] - 1) - 1) / stride[1] + 1)`\n\n    Attributes:\n        weight (Tensor): the learnable weights of the module of shape\n                         (channels, 1, kernel_size[0], kernel_size[1])\n        bias (Tensor):   the learnable bias of the module of shape (channels)\n    """"""\n\n    def __init__(self, channels, kernel_size, stride=1,\n                 padding=0, dilation=1, bias=True):\n        super(Conv2dDepthwise, self).__init__(channels, channels, kernel_size,\n                                              stride, padding, dilation, groups=channels, bias=bias)\n\n    def forward(self, input):\n        return P.conv2d_depthwise(input, self.weight, self.bias, self.stride,\n                                  self.padding, self.dilation)\n'"
pyinn/ncrelu.py,8,"b'import torch\nfrom pyinn.utils import Stream, Dtype, load_kernel\n\nCUDA_NUM_THREADS = 1024\n\n\ndef GET_BLOCKS(N, K=CUDA_NUM_THREADS):\n    return (N + K - 1) // K\n\n\n# k = i_n * 2CHW + i_c * HW + i_h * W + i_w\n# i_n * 2CHW + (i_c + C) * HW + i_h * W + i_w =\n# k + chw\n\n\nkernels = \'\'\'\nextern ""C""\n__global__ void ncrelu_forward(${Dtype} *dst, unsigned char* mask, const ${Dtype} *src, int chw, int total)\n{\n   int tx = blockIdx.x * blockDim.x + threadIdx.x;\n   if(tx >= total)\n      return;\n\n   ${Dtype} v = src[tx];\n   unsigned char flag = v >= 0;\n   mask[tx] = flag;\n   dst[tx + tx / chw * chw] = flag ? v : 0.f;\n   dst[tx + tx / chw * chw + chw] = flag ? 0.f : v;\n}\n\nextern ""C""\n__global__ void ncrelu_backward(${Dtype} *grad_input, const unsigned char *mask, const ${Dtype} *grad_output,\n                                int chw, int total)\n{\n   int tx = blockIdx.x * blockDim.x + threadIdx.x;\n   if(tx >= total)\n      return;\n\n   grad_output += tx + tx / chw * chw;\n   bool flag = mask[tx];\n   grad_input[tx] = flag ? grad_output[0] : grad_output[chw];\n}\n\'\'\'\n\n\ndef ncrelu_forward(input):\n    assert input.dim() == 4 and input.is_contiguous()\n    n, c, h, w = input.size()\n\n    with torch.cuda.device_of(input):\n        output = input.new(n, 2 * c, h, w)\n        mask = torch.cuda.ByteTensor(input.size())\n        f = load_kernel(\'ncrelu_forward\', kernels, Dtype=Dtype(input))\n        f(args=[output.data_ptr(), mask.data_ptr(), input.data_ptr(), c*h*w, input.numel()],\n          block=(CUDA_NUM_THREADS,1,1),\n          grid=(GET_BLOCKS(input.numel()),1,1),\n          stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n    return output, mask\n\n\ndef ncrelu_backward(grad_output, mask):\n    assert grad_output.get_device() == mask.get_device()\n    assert grad_output.is_contiguous()\n    n, c, h, w = mask.size()\n\n    with torch.cuda.device_of(grad_output):\n        grad_input = grad_output.new(mask.size())\n        f = load_kernel(\'ncrelu_backward\', kernels, Dtype=Dtype(grad_output))\n        f(args=[grad_input.data_ptr(), mask.data_ptr(), grad_output.data_ptr(), c*h*w, mask.numel()],\n          block=(CUDA_NUM_THREADS,1,1),\n          grid=(GET_BLOCKS(mask.numel()),1,1),\n          stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n    return grad_input\n\n\nclass NCRELU(torch.autograd.Function):\n\n    def forward(self, input):\n        output, self.mask = ncrelu_forward(input)\n        return output\n\n    def backward(self, grad_output):\n        return ncrelu_backward(grad_output, self.mask)\n\n\ndef ncrelu(input):\n    """""" Applies NCReLU (negative concatenated ReLU) nonlinearity.\n\n    Does `torch.cat([x.clamp(min=0), x.clamp(max=0)], dim=1)` in a single fused op.\n    See https://arxiv.org/abs/1706.00388\n    DiracNets: Training Very Deep Neural Networks Without Skip-Connections\n\n    Args:\n        input: 4D tensor\n    """"""\n    if not input.is_cuda:\n        return torch.cat([input.clamp(min=0), input.clamp(max=0)], dim=1)\n    else:\n        return NCRELU()(input)\n'"
pyinn/utils.py,2,"b""from collections import namedtuple\nimport cupy\nimport torch\nfrom string import Template\n\n\nStream = namedtuple('Stream', ['ptr'])\n\n\ndef Dtype(t):\n    if isinstance(t, torch.cuda.FloatTensor):\n        return 'float'\n    elif isinstance(t, torch.cuda.DoubleTensor):\n        return 'double'\n\n\n@cupy.util.memoize(for_each_device=True)\ndef load_kernel(kernel_name, code, **kwargs):\n    code = Template(code).substitute(**kwargs)\n    kernel_code = cupy.cuda.compile_with_cache(code)\n    return kernel_code.get_function(kernel_name)\n"""
test/benchmark.py,11,"b""import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.init import kaiming_normal\nfrom pyinn import conv2d_depthwise\nfrom torchnet.meter import TimeMeter\nfrom torch.backends import cudnn\ncudnn.benchmark = True\n\n\ndef mobilenet(depth, width, depthwise_function):\n    cfg = [64, (128, 2), 128, (256, 2), 256, (512, 2), 512, 512, 512, 512, 512, (1024, 2), 1024]\n\n    cast = lambda x: x.cuda()\n\n    ni = 32\n    params = {'conv0': cast(kaiming_normal(torch.Tensor(ni, 3, 3, 3)))}\n\n    for i, x in enumerate(cfg):\n        no = x if isinstance(x, int) else x[0]\n        params['block%d.conv0' % i] = cast(kaiming_normal(torch.Tensor(ni, 1, 3, 3)))\n        params['block%d.conv1' % i] = cast(kaiming_normal(torch.Tensor(no, ni, 1, 1)))\n        ni = no\n\n    params = {k: Variable(v, requires_grad=True) for k, v in params.items()}\n\n    def f(input, params):\n        o = F.conv2d(input, params['conv0'], padding=1, stride=2)\n        o = F.relu(o, inplace=True)\n        for i, x in enumerate(cfg):\n            stride = 1 if isinstance(x, int) else x[1]\n            o = depthwise_function(o, params['block%d.conv0' % i], stride=stride, padding=1)\n            o = F.conv2d(o, params['block%d.conv1' % i])\n            o = F.relu(o, inplace=True)\n        return o\n\n    return f, params\n\n\ndef fconv2d(x, w, stride, padding):\n    return F.conv2d(x, w, stride=stride, padding=padding, groups=x.size(1))\n\n\nx = torch.autograd.Variable(torch.randn(256,3,224,224).cuda())\n\nf_pyinn, params = mobilenet(18, 1, conv2d_depthwise)\nf_torch, params = mobilenet(18, 1, fconv2d)\n\n# warmup\nf_pyinn(x, params).sum().backward()\nf_torch(x, params).sum().backward()\n\ntorch.cuda.synchronize()\nmeter = TimeMeter('s')\n\nfor i in range(10):\n    f_torch(x, params).sum().backward()\n    torch.cuda.synchronize()\n\nprint(meter.value())\n\nmeter.reset()\n\nfor i in range(10):\n    f_pyinn(x, params).sum().backward()\n    torch.cuda.synchronize()\n\nprint(meter.value())\n"""
test/test.py,29,"b""import unittest\nfrom functools import partial\nimport torch\nfrom torch.autograd import gradcheck, Variable\nimport pyinn as P\nfrom pyinn.modules import Conv2dDepthwise\nimport torch.nn.functional as F\n\n\ndef ncrelu_ref(input):\n    return torch.cat([F.relu(input), -F.relu(-input)], 1)\n\n\ndef cdgmm_ref(A, B):\n    C = Variable(A.data.new(A.size()))\n\n    A_r = A[..., 0].contiguous().view(-1, A.size(-2))\n    A_i = A[..., 1].contiguous().view(-1, A.size(-2))\n\n    B_r = B[..., 0].contiguous().view(-1).unsqueeze(0).expand_as(A_i)\n    B_i = B[..., 1].contiguous().view(-1).unsqueeze(0).expand_as(A_r)\n\n    C[..., 0] = A_r * B_r - A_i * B_i\n    C[..., 1] = A_r * B_i + A_i * B_r\n    return C\n\n\nclass TestPYINN(unittest.TestCase):\n\n    def testNCReLU(self):\n        for dtype in [torch.cuda.FloatTensor, torch.cuda.DoubleTensor]:\n            x = Variable(torch.randn(2,5,3,1).type(dtype), requires_grad=True)\n            #go = Variable(torch.randn(2,10,3,1).cuda(), requires_grad=False)\n            go = torch.randn(2,10,3,1).type(dtype)\n\n            self.assertEqual((ncrelu_ref(x).data - P.ncrelu(x).data).abs().sum(), 0)\n\n            ncrelu_ref(x).backward(go)\n            gref = x.grad.data.clone()\n            x.grad.data.zero_()\n            P.ncrelu(x).backward(go)\n            g = x.grad.data.clone()\n            self.assertLess((g - gref).abs().sum(), 1e-8)\n\n    def testDGMM(self):\n        inputs = Variable(torch.randn(16, 8).cuda())\n        x = Variable(torch.randn(8).cuda())\n\n        c_ref = inputs.mm(torch.diag(x))\n        c_out = P.dgmm(inputs, x)\n        self.assertEqual((c_ref.data - c_out.data).abs().max(), 0, 'DGMM left')\n\n        # transposed\n        c_ref = torch.diag(x).mm(inputs.t())\n        c_out = P.dgmm(inputs.t().contiguous(), x)\n        self.assertEqual((c_ref.data - c_out.data).abs().max(), 0, 'DGMM right')\n\n        # grad wrt inputs\n        inputs.requires_grad, x.requires_grad = True, False\n        P.dgmm(inputs, x).sum().backward()\n        g_out = inputs.grad.data.clone()\n\n        inputs.grad.data.zero_()\n        inputs.mm(torch.diag(x)).sum().backward()\n        g_ref = inputs.grad.data.clone()\n\n        self.assertEqual((g_ref - g_out).abs().max(), 0)\n\n        # grad wrt x\n        inputs.requires_grad, x.requires_grad = False, True\n        P.dgmm(inputs, x).sum().backward()\n        g_out = x.grad.data.clone()\n\n        x.grad.data.zero_()\n        inputs.mm(torch.diag(x)).sum().backward()\n        g_ref = x.grad.data.clone()\n\n        self.assertLess((g_ref - g_out).abs().max(), 1e-6)\n        \n        # grad wrt inputs and x\n        inputs.requires_grad, x.requires_grad = True, True\n        x.grad.data.zero_()\n        inputs.grad.data.zero_()\n        P.dgmm(inputs, x).sum().backward()\n        g_x_out = x.grad.data.clone()\n        g_inputs_out = inputs.grad.data.clone()\n\n        x.grad.data.zero_()\n        inputs.grad.data.zero_()\n        inputs.mm(torch.diag(x)).sum().backward()\n        g_x_ref = x.grad.data.clone()\n        g_x_inputs_out = inputs.grad.data.clone()\n\n        self.assertLess((g_ref - g_out).abs().max(), 1e-6)\n        self.assertLess((g_x_ref - g_x_out).abs().max(), 1e-6)\n\n    def testCDGMM(self):\n\n        inputs = Variable(torch.randn(16, 8, 2).cuda())\n        x = Variable(torch.randn(8, 2).cuda())\n\n        c_ref = cdgmm_ref(inputs, x)\n        c_out = P.cdgmm(inputs, x)\n        self.assertLess((c_ref.data - c_out.data).abs().max(), 1e-6, 'CDGMM left')\n\n        # grad wrt inputs\n        inputs.requires_grad, x.requires_grad = True, False\n        P.cdgmm(inputs, x).sum().backward()\n        g_out = inputs.grad.data.clone()\n\n        inputs.grad.data.zero_()\n        cdgmm_ref(inputs, x).sum().backward()\n        g_ref = inputs.grad.data.clone()\n\n        self.assertLess((g_out - g_ref).abs().max(), 1e-6, 'CDGMM grad wrt A')\n\n        # grad wrt x\n        # inputs.requires_grad, x.requires_grad = False, True\n        # P.cdgmm(inputs, x).sum().backward()\n        # g_out = x.grad.data.clone()\n\n        # x.grad.data.zero_()\n        # cdgmm_ref(inputs, x).sum().backward()\n        # g_ref = x.grad.data.clone()\n\n        # self.assertEqual((g_ref - g_out).abs().max(), 0)\n\n    def testCDGMMscat(self):\n        shapes = [((1, 3, 40, 40, 2), (40, 40, 2)),\n                  ((1, 3, 20, 20, 2), (20, 20, 2))]\n\n        def cdgmm_ref(A, B):\n            C = Variable(A.data.new(A.size()))\n\n            A_r = A[..., 0].contiguous().view(-1, A.size(-2)*A.size(-3))\n            A_i = A[..., 1].contiguous().view(-1, A.size(-2)*A.size(-3))\n\n            B_r = B[...,0].contiguous().view(B.size(-2)*B.size(-3)).unsqueeze(0).expand_as(A_i)\n            B_i = B[..., 1].contiguous().view(B.size(-2)*B.size(-3)).unsqueeze(0).expand_as(A_r)\n\n            C[..., 0] = A_r * B_r - A_i * B_i\n            C[..., 1] = A_r * B_i + A_i * B_r\n            return C\n\n        def cdgmm_scat(A, B):\n            A_ = A.view(-1, A.size(-2)*A.size(-3), 2)\n            B_ = B.view(-1, 2)\n            return P.cdgmm(A_, B_).view_as(A)\n\n        for shape in shapes:\n            inputs = Variable(torch.randn(*shape[0]).cuda())\n            x = Variable(torch.randn(*shape[1]).cuda())\n\n            c_ref = cdgmm_ref(inputs, x)\n\n            c = cdgmm_scat(inputs, x)\n\n            self.assertLess((c_ref.data - c.data).abs().max(), 1e-6, 'CDGMM left')\n\n            inputs.requires_grad, x.requires_grad = True, False\n            cdgmm_scat(inputs, x).sum().backward()\n            g_out = inputs.grad.data.clone()\n\n            inputs.grad.data.zero_()\n            cdgmm_ref(inputs, x).sum().backward()\n            g_ref = inputs.grad.data.clone()\n\n            self.assertLess((g_out - g_ref).abs().max(), 1e-6, 'CDGMM grad wrt A')\n\n\n    def test_im2col(self):\n        src = Variable(torch.randn(8,7,7).cuda())\n        k = 1\n        pad = 0\n        s = (1,1)\n        dst = P.im2col(src, k, s, pad)\n        back = P.col2im(dst, k, s, pad)\n        self.assertEqual((src - back).data.abs().max(), 0)\n\n    def test_im2col_batch(self):\n        src = Variable(torch.randn(4,8,7,7).cuda())\n        k = 1\n        pad = 0\n        s = (1,1)\n        dst = P.im2col(src, k, s, pad)\n        back = P.col2im(dst, k, s, pad)\n        self.assertEqual((src - back).data.abs().max(), 0)\n\n    def test_conv2d_depthwise(self):\n        n = 6\n        x = Variable(torch.randn(1,n,5,5).double().cuda(), requires_grad=True)\n        w = Variable(torch.randn(n,1,3,3).double().cuda(), requires_grad=True)\n        y_fast = P.conv2d_depthwise(x, w, padding=1)\n        y_ref = F.conv2d(x, w, padding=1, groups=n)\n        go = torch.randn(y_fast.size()).double().cuda()\n\n        self.assertLess((y_fast - y_ref).data.abs().max(), 1e-9)\n\n        x.requires_grad = True\n        w.requires_grad = True\n        y_fast.backward(go)\n        gx_fast = x.grad.data.clone()\n        gw_fast = w.grad.data.clone()\n\n        x.grad.data.zero_()\n        w.grad.data.zero_()\n        y_ref.backward(go)\n        gx_ref = x.grad.data.clone()\n        gw_ref = w.grad.data.clone()\n\n        self.assertTrue(gradcheck(partial(P.conv2d_depthwise, padding=1), (x, w,)))\n\n    def test_conv2d_depthwise_multigpu(self):\n        n = 6\n        a0 = Variable(torch.randn(1,n,5,5).cuda(0), requires_grad=True)\n        a1 = Variable(torch.randn(1,n,5,5).cuda(1), requires_grad=True)\n        w0 = Variable(torch.randn(n,1,3,3).double().cuda(0), requires_grad=True)\n        w1 = Variable(torch.randn(n,1,3,3).double().cuda(1), requires_grad=True)\n        y0 = P.conv2d_depthwise(a0, w0, padding=1)\n        go = torch.randn(y0.size()).double().cuda()\n        y0.backward(go)\n        y1 = P.conv2d_depthwise(a1, w1, padding=1)\n        y1.backward(go.cuda(1))\n\n    def test_modules(self):\n        module = Conv2dDepthwise(channels=8, kernel_size=3)\n        x = Variable(torch.randn(1,8,5,5))\n        y = module(x)\n        y_cuda = module.cuda()(x.cuda())\n        self.assertLess((y - y_cuda.cpu()).data.abs().max(), 1e-6)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
