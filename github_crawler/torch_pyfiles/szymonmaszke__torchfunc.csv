file_path,api_count,code
setup.py,0,"b'import pathlib\n\nimport setuptools\n\n\ndef read(HERE: pathlib.Path, filename, variable):\n    namespace = {}\n\n    exec(open(HERE / ""torchfunc"" / filename).read(), namespace)  # get version\n    return namespace[variable]\n\n\nHERE = pathlib.Path(__file__).resolve().parent\n\nsetuptools.setup(\n    name=read(HERE, pathlib.Path(""_name.py""), ""_name""),\n    version=read(HERE, pathlib.Path(""_version.py""), ""__version__""),\n    license=""MIT"",\n    author=""Szymon Maszke"",\n    author_email=""szymon.maszke@protonmail.com"",\n    description=""PyTorch functions to improve performance, analyse models and make your life easier."",\n    long_description=open(""README.md"", ""r"").read(),\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/szymonmaszke/torchfunc"",\n    packages=setuptools.find_packages(),\n    python_requires="">=3.6"",\n    install_requires=open(""environments/requirements.txt"").read().splitlines(),\n    classifiers=[\n        ""Development Status :: 2 - Pre-Alpha"",\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Intended Audience :: Developers"",\n        ""Operating System :: OS Independent"",\n        ""Topic :: Scientific/Engineering"",\n        ""Topic :: Software Development :: Libraries"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n    ],\n    project_urls={\n        ""Website"": ""https://szymonmaszke.github.io/torchfunc"",\n        ""Documentation"": ""https://szymonmaszke.github.io/torchfunc/#torchfunc"",\n        ""Issues"": ""https://github.com/szymonmaszke/torchfunc/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc"",\n    },\n    keywords=""pytorch torch functions performance visualize utils utilities recording"",\n)\n'"
tests/__init__.py,0,"b'from . import performance, hooks\n'"
tests/cuda_test.py,7,"b'import torch\n\nimport torchfunc\n\n\ndef test_reset():\n    if torch.cuda.is_available():\n        tensor = torch.cuda.FloatTensor(100, 100)\n        torchfunc.sizeof(tensor)\n        cached = torch.cuda.max_memory_cached()\n        del tensor\n        torch.cuda.reset_max_memory_cached()\n        assert cached == torch.cuda.max_memory_cached()\n        torchfunc.cuda.reset()\n        torch.cuda.reset_max_memory_cached()\n        assert 0 == torch.cuda.max_memory_cached()\n'"
tests/module_test.py,5,"b'import itertools\nimport pathlib\nimport shutil\nimport tempfile\n\nimport torch\n\nimport pytest\nimport torchfunc\n\n\ndef _test_value(condition, parameter):\n    if condition:\n        return not parameter.requires_grad\n    return parameter.requires_grad\n\n\ndef _test_network(model: bool, bias: bool, weights: bool):\n    module = (\n        torch.nn.Sequential(\n            torch.nn.Linear(100, 50), torch.nn.Linear(50, 50), torch.nn.Linear(50, 10)\n        )\n        if model\n        else torch.nn.Linear(20, 40)\n    )\n\n    for parameter in module.parameters():\n        assert parameter.requires_grad\n    for name, parameter in torchfunc.module.freeze(\n        module, weights, bias\n    ).named_parameters():\n        assert _test_value(bias if ""bias"" in name else weights, parameter)\n\n\ndef test_freezing():\n    for model, bias, weights in itertools.product([True, False], repeat=3):\n        _test_network(model, bias, weights)\n\n\n@pytest.fixture\ndef snapshot():\n    modules = [\n        torch.nn.Sequential(\n            torch.nn.Linear(100, 50), torch.nn.Linear(50, 50), torch.nn.Linear(50, 10)\n        )\n        for _ in range(4)\n    ]\n\n    snapshot = torchfunc.module.Snapshot()\n    snapshot += modules[0]\n    snapshot += modules[1]\n    return snapshot\n\n\ndef test_snapshot_len(snapshot):\n    assert len(snapshot) == 2\n\n\ndef test_snapshot_save(snapshot):\n    folder = ""TORCHFUNC_SNAPSHOT""\n    temp_dir = pathlib.Path(tempfile.gettempdir()) / folder\n    temp_dir.mkdir()\n    snapshot.save(temp_dir)\n    assert len([file for file in temp_dir.iterdir()]) == 2\n    shutil.rmtree(temp_dir)\n'"
tests/record_test.py,0,b'import torchfunc\n\n\ndef test_forward_pre_recorder():\n    pass\n'
tests/torchfunc_test.py,9,"b'import sys\nimport time\n\nimport torch\n\nimport torchfunc\n\n\ndef test_timer_context_manager():\n    with torchfunc.Timer() as timer:\n        time.sleep(1)\n        last_in_block = timer.checkpoint()  # register checkpoint\n    last_time = timer.checkpoint()\n    time.sleep(1)\n    assert last_time == timer.checkpoint() == timer.time()\n    assert last_in_block != last_time\n\n\ndef test_timer_decorator():\n    @torchfunc.Timer()\n    def wrapped():\n        time.sleep(1)\n        result = 0\n        for value in range(11):\n            result += value\n        return int(result / 55)\n\n    value, passed_time = wrapped()\n    assert value == 1\n    assert passed_time > 1\n\n\ndef test_seed():\n    torchfunc.seed(0)\n    assert 0 == torch.initial_seed()\n\n\ndef test_seed_str():\n    assert str(torchfunc.seed(0)) == ""torchfunc.seed""\n\n\ndef test_seed_representation():\n    assert repr(torchfunc.seed(0)) == ""torchfunc.seed(value=0, cuda=False)""\n\n\ndef test_seed_context_manager():\n    first_seed = torch.initial_seed()\n    with torchfunc.seed(0):\n        assert 0 == torch.initial_seed()\n    assert torch.initial_seed() == first_seed\n\n\ndef test_seed_decorator():\n    first_seed = torch.initial_seed()\n\n    @torchfunc.seed(0)\n    def wrapped():\n        assert 0 == torch.initial_seed()\n\n    wrapped()\n    assert torch.initial_seed() == first_seed\n\n\ndef test_info():\n    assert isinstance(torchfunc.info(), str)\n\n\ndef test_sizeof_tensor():\n    assert torchfunc.sizeof(torch.FloatTensor(12, 12)) == 12 * 12 * 4\n\n\ndef test_sizeof_model():\n    model = torch.nn.Linear(20, 20)\n    bias = 20 * 4\n    weights = 20 * 20 * 4\n    assert torchfunc.sizeof(model) == bias + weights\n'"
torchfunc/__init__.py,13,"b'import contextlib\nimport functools\nimport itertools\nimport sys\nimport time\nimport typing\nfrom importlib.util import find_spec\n\nimport numpy as np\nimport torch\n\nfrom . import cuda, hooks, module, performance\nfrom ._base import Base\nfrom ._dev_utils._general import _cuda_info, _general_info\nfrom ._version import __version__\n\n\nclass Timer(Base, contextlib.AbstractContextManager):\n    r""""""**Measure execution time of function.**\n\n    Can be used as context manager or function decorator, perform checkpoints\n    or display absolute time from measurements beginning.\n\n    **Used as context manager**::\n\n        with Timer() as timer:\n            ... # your operations\n            print(timer) # __str__ calls timer.time() internally\n            timer.checkpoint() # register checkpoint\n            ... # more operations\n            print(timer.checkpoint()) # time since last timer.checkpoint() call\n\n        ... # even more operations\n        print(timer) # time taken for the block, will not be updated outside of it\n\n    When execution leaves the block, timer will be blocked. Last checkpoint and time taken\n    to execute whole block will be returned by `checkpoint()` and `time()` methods respectively.\n\n    **Used as function decorator**::\n\n        @Timer()\n        def foo():\n            return 42\n\n        value, time = foo()\n\n    Parameters\n    ----------\n    function : Callable, optional\n            No argument function used to measure time. Default: time.perf_counter\n\n    """"""\n\n    def __init__(self, function: typing.Callable = time.perf_counter):\n        self.function = function\n\n        self.start = self.function()\n        self.last = self.start\n        self.last_checkpoint = self.start\n\n        self.ended: bool = False\n\n    def time(self):\n        """"""**Time taken since the object creation (measurements beginning).**\n\n        Returns\n        -------\n        time-like\n                Whatever `self.function() - self.function()` returns,\n                usually fraction of seconds\n        """"""\n        if not self.ended:\n            return self.function() - self.start\n        return self.last - self.start\n\n    def checkpoint(self):\n        """"""**Time taken since last checkpoint call.**\n\n        If wasn\'t called before, it is the same as as Timer creation time (first call returns\n        the same thing as `time()`)\n\n        Returns\n        -------\n        time-like\n                Whatever `self.function() - self.function()` returns,\n                usually fraction of seconds\n        """"""\n        if not self.ended:\n            self.last_checkpoint = self.last\n            self.last = self.function()\n        return self.last - self.last_checkpoint\n\n    def __call__(self, function):\n        @functools.wraps(function)\n        def decorated(*args, **kwargs):\n            self.start = self.function()\n            values = function(*args, **kwargs)\n            self.__exit__()\n            return values, self.time()\n\n        return decorated\n\n    def __exit__(self, *_, **__) -> None:\n        self.last = self.function()\n        self.ended: bool = True\n        return False\n\n    def __str__(self) -> str:\n        return str(self.time())\n\n\nclass seed(Base):\n    r""""""**Seed PyTorch and numpy.**\n\n    This code is based on PyTorch\'s reproducibility guide: https://pytorch.org/docs/stable/notes/randomness.html\n    Can be used as standard seeding procedure, context manager (seed will be changed only within block) or function decorator.\n\n    **Standard seed**::\n\n            torchfunc.Seed(0) # no surprises I guess\n\n    **Used as context manager**::\n\n        with Seed(1):\n            ... # your operations\n\n        print(torch.initial_seed()) # Should be back to seed pre block\n\n    **Used as function decorator**::\n\n        @Seed(1) # Seed only within function\n        def foo():\n            return 42\n\n    **Important:** It\'s impossible to put original `numpy` seed after context manager\n    or decorator, hence it will be set to original PyTorch\'s seed.\n\n    Parameters\n    ----------\n    value: int\n            Seed value used in np.random_seed and torch.manual_seed. Usually int is provided\n    cuda: bool, optional\n            Whether to set PyTorch\'s cuda backend into deterministic mode (setting cudnn.benchmark to `False`\n            and cudnn.deterministic to `True`). If `False`, consecutive runs may be slightly different.\n            If `True`, automatic autotuning for convolutions layers with consistent input shape will be turned off.\n            Default: `False`\n\n    """"""\n\n    def __init__(self, value, cuda: bool = False):\n        self.value = value\n        self.cuda = cuda\n\n        self._last_seed = torch.initial_seed()\n        np.random.seed(self.value)\n        torch.manual_seed(self.value)\n\n        if self.cuda:\n            torch.backends.cudnn.deterministic = True\n            torch.backends.cudnn.benchmark = False\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *_, **__):\n        torch.manual_seed(self._last_seed)\n        np.random.seed(self._last_seed)\n        return False\n\n    def __call__(self, function):\n        @functools.wraps(function)\n        def decorated(*args, **kwargs):\n            value = function(*args, **kwargs)\n            self.__exit__()\n            return value\n\n        return decorated\n\n\ndef info(general: bool = True, cuda: bool = True) -> str:\n    r""""""**Return host related info as string.**\n\n    This function may help you tailor your module\'s architecture to specific environment\n    it will be run on.\n\n    For in-depth info regarding possible performance improvements see `torchfunc.performance` submodule.\n\n    **Information is divided into two sections:**\n\n    - general - related to OS, Python version etc.\n    - cuda - specific to CUDA hardware\n\n    **Example**::\n\n            print(torchfunc.info(general=False))\n\n    Parameters\n    ----------\n    general: bool, optional\n            Return general informations. Default: `True`\n    cuda: bool, optional\n            Return CUDA related information. Default: `True`\n\n    Returns\n    -------\n    str\n            Description of system and/or GPU.\n\n    """"""\n    info_string = """"\n    if general:\n        info_string += _general_info()\n        info_string += ""\\n""\n    if cuda:\n        info_string += _cuda_info()\n    return info_string\n\n\ndef sizeof(obj) -> int:\n    r""""""**Get size in bytes of Tensor, torch.nn.Module or standard object.**\n\n    Specific routines are defined for torch.tensor objects and torch.nn.Module\n    objects. They will calculate how much memory in bytes those object consume.\n\n    If another object is passed, `sys.getsizeof` will be called on it.\n\n    This function works similarly to C++\'s sizeof operator.\n\n    **Example**::\n\n        module = torch.nn.Linear(20, 20)\n        bias = 20 * 4 # in bytes\n        weights = 20 * 20 * 4 # in bytes\n        print(torchfunc.sizeof(model) == bias + weights) # True\n\n\n    Parameters\n    ----------\n    obj\n            Object whose size will be measured.\n\n    Returns\n    -------\n    int\n            Size in bytes of the object\n\n    """"""\n    if torch.is_tensor(obj):\n        return obj.element_size() * obj.numel()\n\n    elif isinstance(obj, torch.nn.Module):\n        return sum(\n            sizeof(tensor)\n            for tensor in itertools.chain(obj.buffers(), obj.parameters())\n        )\n    else:\n        return sys.getsizeof(obj)\n\n\ndef installed(module: str) -> bool:\n    """"""**Return True if module is installed.**\n\n    **Example**::\n\n        # Check whether mixed precision library available\n        print(torchfunc.installed(""apex""))\n\n    Parameters\n    ----------\n    module: str\n            Name of the module to be checked.\n\n    Returns\n    -------\n    bool\n            True if installed.\n\n    """"""\n    return find_spec(module) is not None\n'"
torchfunc/_base.py,0,"b'import abc\n\n\nclass Base(abc.ABC):\n    def __str__(self) -> str:\n        return ""{}.{}"".format(type(self).__module__, type(self).__name__)\n\n    def __repr__(self) -> str:\n        parameters = "", "".join(\n            ""{}={}"".format(key, value)\n            for key, value in self.__dict__.items()\n            if not key.startswith(""_"")\n        )\n        return ""{}({})"".format(self, parameters)\n'"
torchfunc/_name.py,0,"b'_name = ""torchfunc""\n'"
torchfunc/_version.py,0,"b'__version__ = ""0.2.0""\n'"
torchfunc/cuda.py,4,"b'r""""""\n**This module provides CUDA related functionalities (e.g. resetting it\'s state).**\n\n""""""\n\nimport torch\n\n\ndef reset() -> None:\n    r""""""**Reset cuda state by emptying cache and collecting IPC.**\n\n    Calls `torch.cuda.empty_cache()` and `torch.cuda.ipc_collect()` consecutively.\n\n    Example::\n\n        tensor = torch.cuda.FloatTensor(100, 100)\n        del tensor\n        torchfunc.cuda.reset() # Now memory is freed\n\n    """"""\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n'"
torchfunc/module.py,41,"b'r""""""\n**This module provides functionalities related to torch.nn.Module instances (e.g. freezing parameters).**\n\nFor performance analysis of `torch.nn.Module` please see subpackage `performance`.\n\n""""""\n\nimport contextlib\nimport copy\nimport datetime\nimport pathlib\n\nimport torch\n\nfrom ._base import Base\n\n\ndef _switch(module: torch.nn.Module, weight: bool, bias: bool, value: bool):\n    for name, param in module.named_parameters():\n        if bias and weight:\n            param.requires_grad_(value)\n        elif (""bias"" in name and bias) or (""weight"" in name and weight):\n            param.requires_grad_(value)\n\n    return module\n\n\ndef freeze(\n    module: torch.nn.Module, weight: bool = True, bias: bool = True\n) -> torch.nn.Module:\n    r""""""**Freeze module\'s parameters.**\n\n    Sets `requires_grad` to `False` for specified parameters in module.\n    If bias and weight are specified, ALL parameters will be frozen (even if their names\n    are not matched by `weight` and bias).\n\n    If you want to freeze only those whose names contain `bias` or `weight`,\n    call the function twice consecutively (once with `bias=True` and `weight=False` and vice versa).\n\n    Example::\n\n        logistic_regression = torch.nn.Sequential(\n            torch.nn.Linear(784, 10),\n            torch.nn.Sigmoid(),\n        )\n\n        # Freeze only bias in logistic regression\n        torchfunc.freeze(logistic_regression, weight = False)\n\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n            Module whose weights and biases will be frozen.\n    weight : bool, optional\n            Freeze weights. Default: True\n    bias : bool, optional\n            Freeze bias. Default: True\n\n    Returns\n    -------\n    module : torch.nn.Module\n        Module after parameters were frozen\n\n    """"""\n    return _switch(module, weight, bias, value=False)\n\n\ndef unfreeze(\n    module: torch.nn.Module, weight: bool = True, bias: bool = True\n) -> torch.nn.Module:\n    r""""""**Unfreeze module\'s parameters.**\n\n    Sets `requires_grad` to `True` for all parameters in module.\n    Works as complementary function to freeze, see it\'s documentation.\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n            Module whose weights and biases will be unfrozen.\n    weight : bool, optional\n            Freeze weights. Default: True\n    bias : bool, optional\n            Freeze bias. Default: True\n\n    Returns\n    -------\n    module : torch.nn.Module\n        Module after parameters were unfrozen\n\n    """"""\n    return _switch(module, weight, bias, value=False)\n\n\ndef named_parameters(\n    module: torch.nn.Module, name: str, prefix: str = """", recurse: bool = True\n):\n    r""""""**Iterate only over** `module`\'s **parameters having** `name` **as part of their name.**\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Module whose weights and biases will be unfrozen.\n    name : str\n        Name which parameter needs to be returned\n    prefix : str, optional\n        Prefix to prepend to all parameter names. Default: `\'\'` (no prefix)\n    recurse : bool, optional\n        If `True`, then yields parameters of this module and all submodules.\n        Otherwise, yields only parameters that are direct members of this module.\n        Default: `True`\n\n    Yields\n    ------\n    torch.nn.Parameter\n        Module\'s parameters satisfying `name` constraint\n\n    """"""\n    for param_name, param in module.named_parameters():\n        if name in param_name:\n            yield param\n\n\ndef weight_parameters(module: torch.nn.Module, prefix: str = """", recurse: bool = True):\n    r""""""**Iterate only over** `module`\'s **parameters considered weights**.\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Module whose weights and biases will be unfrozen.\n    prefix : str, optional\n        Prefix to prepend to all parameter names. Default: `\'\'` (no prefix)\n    recurse : bool, optional\n        If `True`, then yields parameters of this module and all submodules.\n        Otherwise, yields only parameters that are direct members of this module.\n        Default: `True`\n\n    Yields\n    ------\n    torch.nn.Parameter\n        Module\'s parameters being `weight` (e.g. their name consist `weight` string)\n\n    """"""\n    yield from named_parameters(module, ""weight"", prefix, recurse)\n\n\ndef bias_parameters(module: torch.nn.Module, prefix: str = """", recurse: bool = True):\n    r""""""**Iterate only over** `module`\'s **parameters considered biases**.\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Module whose weights and biases will be unfrozen.\n    prefix : str, optional\n        Prefix to prepend to all parameter names. Default: `\'\'` (no prefix)\n    recurse : bool, optional\n        If `True`, then yields parameters of this module and all submodules.\n        Otherwise, yields only parameters that are direct members of this module.\n        Default: `True`\n\n    Yields\n    ------\n    torch.nn.Parameter\n        Module\'s parameters being `bias` (e.g. their name consist `bias` string)\n\n    """"""\n    yield from named_parameters(module, ""bias"", prefix, recurse)\n\n\ndef device(obj):\n    r""""""**Return ** `device` **of** `torch.nn.module` **or other** `obj` **containing device field**.\n\n    Example::\n\n        module = torch.nn.Linear(100, 10)\n        print(torchfunc.module.device(module)) # ""cpu""\n\n    Parameters\n    ----------\n    obj : torch.nn.Module or torch.Tensor\n        Object containing `device` field or containing parameters with device, e.g. module\n\n    Returns\n    -------\n    Optional[torch.device]\n        Instance of device on which object is currently held. If object is contained\n        on multiple devices, `None` is returned\n\n    """"""\n    if isinstance(obj, torch.nn.Module):\n        device = next(obj.paramaters()).device\n        for parameter in enumerate(obj.parameters()):\n            if parameter.device != device:\n                return None\n\n        return device\n\n    return obj.device\n\n\n# Check for every device of parameter and optionally cast?\n@contextlib.contextmanager\ndef switch_device(obj, target):\n    r""""""**Context manager/decorator switching** `device` **of** `torch.nn.module` **or other** `obj` **to the specified target**.\n\n    After `with` block ends (or function) specified object is casted back to original\n    device.\n\n    Example::\n\n        module = torch.nn.Linear(100, 10)\n        with torchfunc.module.switch_device(module, torch.device(""cuda"")):\n            ... # module is on cuda now\n\n        torchfunc.module.device(module) # back on CPU\n\n    Parameters\n    ----------\n    obj : torch.nn.Module or torch.Tensor\n        Object containing `device` field or containing parameters with device, e.g. module\n    target : torch.device-like\n        PyTorch device or string, compatible with `to` cast.\n\n    """"""\n    current_device = device(obj)\n    obj.to(target)\n    try:\n        yield current_device\n    finally:\n        obj.to(current_device)\n\n\nclass Snapshot(Base):\n    r""""""**Save module snapshots in memory and/or disk.**\n\n    Next modules can be added with `+` or `+=` and their state or whole model saved\n    to disk with appropriate methods.\n\n    All added modules are saved unless removed with `pop()` method.\n\n    Additionally, self-explainable methods like `len`, `__iter__` or item access\n    are provided (although there is no `__setitem__` as it\'s discouraged\n    to mutate contained modules).\n\n    Example::\n\n        snapshot = torchfunc.module.Snapshot()\n        snapshot += torch.nn.Sequential(torch.nn.Linear(784, 10), torch.nn.Sigmoid())\n        snapshot.save(""models"") # Save all modules to models folder\n\n    Parameters\n    ----------\n    *modules : torch.nn.Module\n        Var args of PyTorch modules to be kept.\n\n    """"""\n\n    def __init__(self, *modules: torch.nn.Module):\n        if modules:\n            self.modules = list(modules)\n        else:\n            self.modules = []\n        self.timestamps = []\n\n    def __len__(self):\n        return len(self.modules)\n\n    def __iter__(self):\n        return iter(self.modules)\n\n    def __getitem__(self, index):\n        return self.modules[index]\n\n    def __iadd__(self, other):\n        self.modules.append(other)\n        self.timestamps.append(datetime.datetime.now())\n        return self\n\n    def __radd__(self, other):\n        return self + other\n\n    def __add__(self, other: torch.nn.Module):\n        new = Snapshot(copy.deepcopy(self.modules))\n        new += other\n        return new\n\n    def pop(self, index: int = -1):\n        r""""""**Remove module at** `index` **from memory.**\n\n        Parameters\n        ----------\n        index : int, optional\n            Index of module to be removed. Default: -1 (last module)\n\n        Returns\n        ----------\n        module : torch.nn.Module\n            Module removed by this operation\n\n        """"""\n        return self.modules.pop(index)\n\n    def _save(self, folder: pathlib.Path, remove: bool, state: bool, *indices) -> None:\n        if folder is None:\n            folder = pathlib.Path(""."")\n\n        def _single_save(index):\n            module = self.modules[index]\n            if state:\n                module = module.state_dict()\n            name = pathlib.Path(\n                ""module_"" + ""state_""\n                if state\n                else """" + ""{}_{}.pt"".format(len(self.modules), self.timestamps[index])\n            )\n            if remove:\n                self.pop(index)\n            torch.save(module, folder / name)\n\n        if not indices:\n            indices = range(len(self))\n\n        for index in indices:\n            _single_save(index)\n\n    def save(\n        self, folder: pathlib.Path = None, remove: bool = False, *indices: int\n    ) -> None:\n        r""""""**Save module to disk.**\n\n        Snapshot(s) will be saved using the following naming convention::\n\n            module_""index""_""timestamp"".pt\n\n        See `PyTorch\'s docs <https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model>`__\n        for more information.\n\n        Parameters\n        ----------\n        folder : pathlib.Path, optional\n            Name of the folder where model will be saved. It has to exist.\n            Defaults to current working directory.\n        remove : bool, optional\n            Whether module should be removed from memory after saving. Useful\n            for keeping only best/last model in memory. Default: `False`\n        *indices: int, optional\n            Possibly empty varargs containing indices of modules to be saved.\n            Negative indexing is supported.\n            If empty, save all models.\n\n        """"""\n        self._save(folder, remove, state=False, *indices)\n\n    def save_state(\n        self, folder: pathlib.Path = None, remove: bool = False, *indices: int\n    ) -> None:\n        r""""""**Save module\'s state to disk.**\n\n        Snapshot(s) will be saved with using the following naming convention::\n\n            module_""index""_""timestamp"".pt\n\n        See PyTorch\'s docs about `state_dict <https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended>`__ for more\n        information.\n\n        Parameters\n        ----------\n        folder : pathlib.Path, optional\n            Name of the folder where model will be saved. It has to exist.\n            Defaults to current working directory.\n        remove : bool, optional\n            Whether module should be removed from memory after saving. Useful\n            for keeping only best/last model in memory. Default: False\n        *indices: int, optional\n            Possibly empty varargs containing indices of modules to be saved.\n            Negative indexing is supported.\n            If empty, save all models.\n\n        """"""\n        self._save(folder, remove, state=True, *indices)\n'"
tests/hooks/recorders_test.py,7,"b'import pathlib\nimport shutil\nimport tempfile\n\nimport torch\n\nimport pytest\nimport torchfunc\n\n\n@pytest.fixture\ndef recorder():\n    model = torch.nn.Sequential(\n        torch.nn.Linear(784, 100),\n        torch.nn.ReLU(),\n        torch.nn.Linear(100, 50),\n        torch.nn.ReLU(),\n        torch.nn.Linear(50, 10),\n    )\n\n    _recorder = torchfunc.hooks.recorders.ForwardPre(reduction=lambda x, y: x + y)\n    _recorder.children(model, indices=(2, 3))\n\n    for _ in range(1000):\n        model(torch.randn(1, 784))\n\n    return _recorder\n\n\ndef test_len(recorder):\n    assert len(recorder) == 2\n\n\ndef test_shapes(recorder):\n    assert recorder[0].shape == (1, 100)\n    assert recorder[1].shape == (1, 50)\n\n\ndef test_samples(recorder):\n    assert recorder.samples(0) == 1000\n    for sample in recorder.iter_samples():\n        assert sample == 1000\n\n\ndef test_save(recorder):\n    folder = ""TORCHFUNC_RECORDER""\n    temp_dir = pathlib.Path(tempfile.gettempdir()) / folder\n    recorder.save(temp_dir, mkdir=True)\n    assert len([file for file in temp_dir.iterdir()]) == 2\n    shutil.rmtree(temp_dir)\n\n\ndef test_remove(recorder):\n    recorder.remove(0)\n    assert len(recorder) == 1\n'"
tests/hooks/registrators_test.py,8,"b'import pathlib\nimport shutil\nimport tempfile\n\nimport torch\n\nimport pytest\nimport torchfunc\n\n\n@pytest.fixture\ndef registrator():\n    class Hook:\n        def __init__(self):\n            self.counter = 0\n\n        def __call__(self, *_):\n            self.counter += 1\n\n        def __int__(self):\n            return self.counter\n\n    model = torch.nn.Sequential(\n        torch.nn.Linear(784, 100),\n        torch.nn.ReLU(),\n        torch.nn.Linear(100, 50),\n        torch.nn.ReLU(),\n        torch.nn.Linear(50, 10),\n    )\n\n    _registrator = torchfunc.hooks.registrators.Forward(Hook())\n    _registrator.modules(model, types=(torch.nn.Linear,))\n\n    for _ in range(1000):\n        model(torch.randn(1, 784))\n\n    return _registrator\n\n\ndef test_hook_calls(registrator):\n    assert int(registrator.hook) == 3 * 1000\n\n\ndef test_len(registrator):\n    assert len(registrator) == 3\n\n\ndef test_remove(registrator):\n    registrator.remove(0)\n    assert len(registrator) == 2\n'"
tests/performance/__init__.py,0,b''
tests/performance/layers_test.py,10,"b'import torch\n\nimport torchfunc\n\n\ndef test_depthwise():\n    model = torch.nn.Sequential(\n        torch.nn.Conv1d(64, 64, kernel_size=3, groups=64),\n        torch.nn.Conv2d(3, 32, kernel_size=3, groups=1),\n        torch.nn.Conv2d(32, 32, kernel_size=3, groups=32),\n    )\n    values = tuple(torchfunc.performance.layers.Depthwise().modules(model))\n    assert values == (1, 3)\n\n\ndef test_inplace():\n    model = torch.nn.Sequential(\n        torch.nn.ReLU(inplace=True),\n        torch.nn.ReLU6(inplace=True),\n        torch.nn.Conv1d(64, 64, kernel_size=3, groups=64),\n        torch.nn.Conv2d(3, 32, kernel_size=3, groups=1),\n        torch.nn.Conv2d(32, 32, kernel_size=3, groups=32),\n    )\n    values = torchfunc.performance.layers.Inplace().children(model)\n    for i, value in enumerate(values):\n        assert i == value\n'"
tests/performance/performance_test.py,15,"b'import torch\n\nimport torchfunc\n\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convolution = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 32, 3),\n            torch.nn.ReLU(inplace=True),  # Inplace may harm kernel fusion\n            torch.nn.Conv2d(32, 128, 3, groups=32),  # Depthwise is slower in PyTorch\n            torch.nn.ReLU(inplace=True),  # Same as before\n            torch.nn.Conv2d(128, 250, 3),  # Wrong output size for TensorCores\n        )\n\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(250, 64),  # Wrong input size for TensorCores\n            torch.nn.ReLU(),  # Should be fine\n            torch.nn.Linear(64, 10),  # Wrong output size for TensorCores\n        )\n\n    def forward(self, inputs):\n        convolved = torch.nn.AdaptiveAvgPool2d(1)(self.convolution(inputs)).flatten()\n        return self.classifier(convolved)\n\n\ndef test_report():\n    goal = r""""""\n===========================GENERAL TIPS===========================\n\n- Make sure you are running newest PyTorch version. See available releases: https://github.com/pytorch/pytorch/tags\n- Use GPU for larger batches, CPU might be suitable for smaller jobs.\n- Use mixed-precision training on GPU, preferably automated, e.g. NVIDIA Apex: https://github.com/NVIDIA/apex.\n\n===========================SPECIFIC TIPS===========================\n\n=======> Module should be an instance of torch.jit.ScriptModule.\nSee https://pytorch.org/docs/stable/jit.html for more information.\n=======> NVIDIA\'s Apex is not installed. It is the easiest way to use mixed precision training.\nSee https://github.com/NVIDIA/apex for more information and installation.\n=======> In-place operations might harm kernel fusion. Indices of those modules:\n[3, 5]\nYou may want to remove inplace flag (see this issue: https://github.com/pytorch/pytorch/issues/23655)\n=======> Depthwise convolutions are not currently using specialized kernel and might be slower.\nSee this issue: https://github.com/pytorch/pytorch/issues/18631 for more information.\nIndices of those modules:\n[4]\nYou may want to decrease number of groups (like it\'s done for ResNeXt) for possible speed & accuracy improvements.\n=======> TensorCores incompatible modules:\nModules where float type is not torch.half:\n[2, 4, 6, 8, 10]\nModules where inputs shape should be divisible by 8:\n[2, 8]\nModules where outputs shape should be divisible by 8:\n[6, 10]""""""\n    tips = torchfunc.performance.tips(Model())\n    print(tips)\n    assert tips == goal\n'"
tests/performance/technology_test.py,0,b'import torchfunc\n\n\ndef test_tensorcores():\n    # TBD\n    pass\n'
torchfunc/_dev_utils/__init__.py,0,"b'__all__ = [""_general""]\n\nfrom . import _general\n'"
torchfunc/_dev_utils/_general.py,6,"b'import multiprocessing\nimport platform\nimport typing\n\nimport torch\n\n\ndef _general_info():\n    return ""\\n"".join(\n        [\n            ""Python version: {}"".format(platform.python_version()),\n            ""Python implementation: {}"".format(platform.python_implementation()),\n            ""Python compiler: {}"".format(platform.python_compiler()),\n            ""PyTorch version: {}"".format(torch.__version__),\n            ""System: {}, version: {}"".format(\n                platform.system() or ""Unable to determine"",\n                platform.release() or ""Unable to determine"",\n            ),\n            ""Processor: {}"".format(platform.processor() or ""Unable to determine""),\n            ""Number of CPUs: {}"".format(multiprocessing.cpu_count()),\n        ]\n    )\n\n\ndef _cuda_info():\n    def _cuda_devices_formatting(\n        info_function: typing.Callable,\n        formatting_function: typing.Callable = None,\n        mapping_function: typing.Callable = None,\n    ):\n        def _setup_default(function):\n            return (lambda arg: arg) if function is None else function\n\n        formatting_function = _setup_default(formatting_function)\n        mapping_function = _setup_default(mapping_function)\n\n        return "" | "".join(\n            mapping_function(\n                [\n                    formatting_function(info_function(i))\n                    for i in range(torch.cuda.device_count())\n                ]\n            )\n        )\n\n    def _device_properties(attribute):\n        return _cuda_devices_formatting(\n            lambda i: getattr(torch.cuda.get_device_properties(i), attribute),\n            mapping_function=lambda in_bytes: map(str, in_bytes),\n        )\n\n    return ""\\n"".join(\n        [\n            ""Available CUDA devices count: {}"".format(torch.cuda.device_count()),\n            ""CUDA devices names: {}"".format(\n                _cuda_devices_formatting(torch.cuda.get_device_name)\n            ),\n            ""Major.Minor CUDA capabilities of devices: {}"".format(\n                _cuda_devices_formatting(\n                    torch.cuda.get_device_capability,\n                    formatting_function=lambda capabilities: ""."".join(\n                        map(str, capabilities)\n                    ),\n                )\n            ),\n            ""Device total memory (bytes): {}"".format(\n                _device_properties(""total_memory"")\n            ),\n            ""Device multiprocessor count: {}"".format(\n                _device_properties(""multi_processor_count"")\n            ),\n        ]\n    )\n'"
torchfunc/hooks/__init__.py,0,"b'r""""""\n**This package provides hook related functionalities (e.g. recording network state,\neasier hook registration).**\n\nTo record neural network states and how it interacts with data, see module `recorders`.\nTo register different `hook` for your neural network (e.g. concatenated modules),\nsee module `registrator`.\n\n""""""\n\nfrom . import recorders, registrators\n'"
torchfunc/hooks/_dev_utils.py,0,"b'import typing\n\nimport torch\n\nfrom .._base import Base\n\n\ndef register_condition(module, types, index, indices):\n    return not (\n        ((indices is None) and (types is not None) and (not isinstance(module, types)))\n        or ((types is None) and (indices is not None) and (index not in indices))\n    )\n'"
torchfunc/hooks/recorders.py,21,"b'r""""""\n**This module allows one to record neural network state (for example when data passes through it).**\n\n`recorders` are organized similarly to\n`torch.nn.Module`\'s hooks (e.g. `backward`, `forward` and `forward pre`).\nAdditionally, each can record input or output from specified modules, which\ngives us, for example, `ForwardInput` (record input to specified module(s) during forward pass).\n\nExample should make it more clear::\n\n    # MNIST classifier\n    model = torch.nn.Sequential(\n        torch.nn.Linear(784, 100),\n        torch.nn.ReLU(),\n        torch.nn.Linear(100, 50),\n        torch.nn.ReLU(),\n        torch.nn.Linear(50, 10),\n    )\n\n    # Recorder which sums layer inputs from consecutive forward calls\n    recorder = torchfunc.hooks.recorders.ForwardPre(reduction=lambda x, y: x+y)\n    # Record inputs going into Linear(100, 50) and Linear(50, 10)\n    recorder.children(model, indices=(2, 3))\n    # Train your network normally (pass data through it somehow)\n    ...\n    # Save tensors (of shape 100 and 50) in folder, each named 1.pt and 2.pt respectively\n    recorder.save(pathlib.Path(""./analysis""))\n\nYou could specify `types` instead of `indices` (for example all forward inputs to `torch.nn.Linear` will be registered),\niterate over modules recursively instead of shallow iteration with `children` method etc.\n\nEach `recorder` has one or more `subrecorders`; those usually correspond to specific layer\nfor which recording will be done. In the above case, there are two `subrecorders`,\nboth of `torch.nn.Linear` type.\n\nAdditionally one can post-process data contained within `recorder` using `apply`\nfunctionality.\n\nConcrete methods recording different data passing through network are specified below:\n\n""""""\n\nimport pathlib\nimport typing\n\nimport torch\n\nfrom .._base import Base\nfrom ._dev_utils import register_condition\n\n\nclass _Recorder(Base):\n    r""""""**{}**\n\n    You can record only some of the data based on external conditions if `condition`\n    `callable` is specified.\n\n    Data can be cumulated together via `reduction` parameter, which is advised\n    from the memory perspective.\n\n    Parameters\n    ----------\n    condition : Callable, optional\n        No argument callable. If True returned, record data.\n        Can be used to save data based on external environment (e.g. dataset\'s label).\n        If None, will record every data point. Default: `None`\n    reduction : Callable, optional\n        Operation to use on incoming data. Should take two arguments, and return one.\n        Acts similarly to reduction argument of Python\'s `functools.reduce <https://docs.python.org/3/library/functools.html#functools.reduce>`__.\n        If `None`, data will be added to list, which may be very memory intensive.\n        Default: `None`\n\n    Attributes\n    ----------\n    data : List\n        Keeps data passing through subrecorders, optionally reduced by `reduction`.\n        Each item represents data for specified `subrecorder`.\n    subrecorders: List[Hooks]\n        List containing registered subrecorders.\n    handles : List[torch.utils.subrecorders.RemovableHandle]\n        Handles for registered subrecorders, each corresponds to specific `subrecorder`.\n        Can be used to unregister certain subrecorders (though discouraged, please use `remove` method).\n\n    """"""\n\n    def __init__(self, register_method, method):\n        self._register_method: typing.Callable = register_method\n        self._method: typing.Callable = method\n        self.data = []\n        self.subrecorders = []\n        self.handles = []\n\n    def _register_hook(\n        self,\n        network,\n        iterating_function: str,\n        types: typing.Tuple[typing.Any] = None,\n        indices: typing.Tuple[int] = None,\n    ):\n        last_index = 0\n        for index, module in enumerate(getattr(network, iterating_function)()):\n            if register_condition(module, types, index, indices):\n                hook = self._method(last_index, self.data)\n                self.handles.append(getattr(module, self._register_method)(hook))\n                self.subrecorders.append(hook)\n                last_index += 1\n\n    def __setitem__(self, index, item):\n        self.data[index] = item\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __iter__(self):\n        return iter(self.data)\n\n    def __len__(self):\n        return len(self.subrecorders)\n\n    def remove(self, index):\n        r""""""**Remove subrecorder specified by** `index`.\n\n        Subrecorder will not record data passing through it and will be removed\n        from `subrecorders` attribute.\n\n        Parameters\n        ----------\n        index: int\n            Index of subrecorder (usually layer)\n\n        Returns\n        -------\n        torch.Tensor\n            Data contained in subrecorder.\n\n        """"""\n        self.handles[index].remove()\n        self.subrecorders.pop(index)\n        return self.data.pop(index)\n\n    def samples(self, index) -> int:\n        r""""""**Count of samples passed through subrecorder under** `index`.\n\n        Parameters\n        ----------\n        index: int\n            Index of `subrecorder` (usually layer)\n\n        Returns\n        -------\n        int\n            How many samples passed through specified `subrecorder`.\n\n        """"""\n        return self.subrecorders[index].samples\n\n    def iter_samples(self):\n        r""""""**Iterate over count of samples for each subrecorder**.\n\n        Parameters\n        ----------\n        index: int\n            Index of subrecorder (usually layer)\n\n        Returns\n        -------\n        int\n            How many samples passed through this subrecorder.\n\n        """"""\n        for hook in self.subrecorders:\n            yield hook.samples\n\n    def modules(\n        self,\n        module: torch.nn.Module,\n        types: typing.Tuple[typing.Any] = None,\n        indices: typing.List[int] = None,\n    ):\n        r""""""**Register** `subrecorders` **using types and/or indices via** `modules` **method**.\n\n        This function will use `modules` method of `torch.nn.Module` to iterate over available submodules. If you wish to iterate non-recursively, use `children`.\n\n        **Important:**\n\n        If `types` and `indices` are left with their default values, all modules\n        will have `subrecorders` registered.\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n            Module (usually neural network) for which inputs will be collected.\n        types : Tuple[typing.Any], optional\n            Module types for which data will be recorded. E.g. `(torch.nn.Conv2d, torch.nn.Linear)`\n            will register `subrecorders` on every module being instance of either `Conv2d` or `Linear`.\n            Default: `None`\n        indices : Iterable[int], optional\n            Indices of modules whose inputs will be registered.\n            Default: `None`\n\n        Returns\n        -------\n        self\n        """"""\n\n        self._register_hook(module, ""modules"", types, indices)\n        return self\n\n    def children(\n        self,\n        network,\n        types: typing.Tuple[typing.Any] = None,\n        indices: typing.List[int] = None,\n    ):\n        r""""""**Register** `subrecorders` **using types and/or indices via** `children` **method**.\n\n        This function will use `children` method of `torch.nn.Module` to iterate over available submodules. If you wish to iterate recursively, use `modules`.\n\n        **Important:**\n\n        If `types` and `indices` are left with their default values, all modules\n        will have `subrecorders` registered.\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n            Module (usually neural network) for which inputs will be collected.\n        types : Tuple[typing.Any], optional\n            Module types for which data will be recorded. E.g. `(torch.nn.Conv2d, torch.nn.Linear)`\n            will register `subrecorders` on every module being instance of either `Conv2d` or `Linear`.\n            Default: `None`\n        indices : Iterable[int], optional\n            Indices of modules whose inputs will be registered.\n            Default: `None`\n\n        Returns\n        -------\n        self\n        """"""\n\n        self._register_hook(network, ""children"", types, indices)\n        return self\n\n    def save(self, path: pathlib.Path, mkdir: bool = False, *args, **kwargs):\n        """"""**Save data tensors within specified path.**\n\n        Each data tensor will be indexed by integer `[0, N)`, where indices\n        represent consecutive `subrecorders`.\n\n        Parameters\n        ----------\n        path: pathlib.Path\n                Path where tensors will be saved.\n        mkdir: bool, optional\n                If True, create directory if doesn\'t exists. Default: False\n        *args:\n                Varargs passed to `pathlib.Path`\'s `mkdir` method if `mkdir` argument set to True.\n        *kwargs:\n                Kwarargs passed to `pathlib.Path`\'s `mkdir` method if `mkdir` argument set to True.\n\n        """"""\n        if mkdir:\n            path.mkdir(*args, **kwargs)\n        for index, subrecorder in enumerate(self):\n            torch.save(subrecorder, path / ""{}.pt"".format(index))\n\n    def apply(self, function: typing.Callable):\n        """"""**Apply function to data contained in each subrecorder.**\n\n        Data will be modified an saved inside data of each subrecorder.\n        This function may make `recorder` unusable, it\'s up to user\n        to ensure correct functioning after this functionality was used.\n\n        Parameters\n        ----------\n        function: Callable\n                Single argument (`torch.Tensor` data from `subrecorder`) callable\n                returning anything.\n\n        """"""\n        for subrecorder in self:\n            subrecorder = function(subrecorder)\n\n    def apply_sample(self, function: typing.Callable) -> None:\n        """"""**Apply function to data contained in each subrecorder.**\n\n        Works like `apply`, except `Callable` is passed number of samples passed\n        through `subrecorder` as second argument\n\n        Parameters\n        ----------\n        function: Callable\n                Two argument (`torch.Tensor` data from `subrecorder` and number of `samples` which passed through it)\n                `Callable` returning anything.\n\n        """"""\n        for subrecorder, sample in zip(self, self.iter_samples()):\n            subrecorder = function(subrecorder, sample)\n\n\nclass _Hook:\n    def __init__(self, index: int, data: typing.List, samples: int = 0):\n        self.index = index\n        self.data = data\n        self.samples = samples\n\n    def _call(self, to_record, condition, reduction):\n        if condition is None or condition():\n            self.samples += 1\n            if self.index >= len(self.data):\n                self.data.append(to_record[0])\n                if reduction is None:\n                    self.data[-1] = [self.data[-1]]\n            else:\n                if reduction is not None:\n                    self.data[self.index] = reduction(\n                        self.data[self.index], to_record[0]\n                    )\n                else:\n                    self.data[self.index].append(to_record[0])\n\n\nclass ForwardPre(_Recorder):\n    __doc__ = _Recorder.__doc__.format(\n        ""Record input values before forward of specified layer(s).""\n    )\n\n    def __init__(\n        self, condition: typing.Callable = None, reduction: typing.Callable = None\n    ):\n        self.condition = condition\n        self.reduction = reduction\n\n        class ForwardPreHook(_Hook):\n            def __call__(inner_self, module, inputs):\n                inner_self._call(inputs, self.condition, self.reduction)\n\n        super().__init__(""register_forward_pre_hook"", ForwardPreHook)\n\n\nclass ForwardInput(_Recorder):\n    __doc__ = _Recorder.__doc__.format(\n        ""Record input values after forward of specified layer(s).""\n    )\n\n    def __init__(\n        self, condition: typing.Callable = None, reduction: typing.Callable = None\n    ):\n        self.condition = condition\n        self.reduction = reduction\n\n        class ForwardInputHook(_Hook):\n            def __call__(inner_self, module, inputs, _):\n                inner_self._call(inputs, self.condition, self.reduction)\n\n        super().__init__(""register_forward_hook"", ForwardInputHook)\n\n\nclass ForwardOutput(_Recorder):\n    __doc__ = _Recorder.__doc__.format(\n        ""Record output values after forward of specified layer(s).""\n    )\n\n    def __init__(\n        self, condition: typing.Callable = None, reduction: typing.Callable = None\n    ):\n        self.condition = condition\n        self.reduction = reduction\n\n        class ForwardOutputHook(_Hook):\n            def __call__(inner_self, module, _, outputs):\n                inner_self._call(outputs, self.condition, self.reduction)\n\n        super().__init__(""register_forward_hook"", ForwardOutputHook)\n\n\nclass BackwardInput(_Recorder):\n    __doc__ = _Recorder.__doc__.format(\n        ""Record input gradients after those are calculated w.r.t. specified module.""\n    )\n\n    def __init__(\n        self, condition: typing.Callable = None, reduction: typing.Callable = None\n    ):\n        self.condition = condition\n        self.reduction = reduction\n\n        class BackwardInputHook(_Hook):\n            def __call__(inner_self, module, grad_inputs, _):\n                inner_self._call(grad_inputs, self.condition, self.reduction)\n\n        super().__init__(""register_backward_hook"", BackwardInputHook)\n\n\nclass BackwardOutput(_Recorder):\n    __doc__ = _Recorder.__doc__.format(\n        ""Record output gradients after those are calculated w.r.t. specified module.""\n    )\n\n    def __init__(\n        self, condition: typing.Callable = None, reduction: typing.Callable = None\n    ):\n        self.condition = condition\n        self.reduction = reduction\n\n        class BackwardOutputHook(_Hook):\n            def __call__(inner_self, module, _, outputs):\n                inner_self._call(outputs, self.condition, self.reduction)\n\n        super().__init__(""register_backward_hook"", BackwardOutputHook)\n'"
torchfunc/hooks/registrators.py,17,"b'r""""""\n**This module allows you for easier hook registration (e.g. based on** `type` **or** `index` **within network).**\n\nExample::\n\n    # Example forward pre hook\n    def example_forward_pre(module, inputs):\n        return inputs + 1\n\n    # MNIST classifier\n    model = torch.nn.Sequential(\n        torch.nn.Linear(784, 100),\n        torch.nn.ReLU(),\n        torch.nn.Linear(100, 50),\n        torch.nn.ReLU(),\n        torch.nn.Linear(50, 10),\n    )\n    registrator = torchfunc.hooks.registrators.ForwardPre()\n    # Register forwardPreHook for all torch.nn.Linear submodules\n    registrator.modules(model, example_forward_pre, types=(torch.nn.Linear))\n\nYou could specify indices instead of types (for example all inputs to `torch.nn.Linear` will be registered),\nand iterate over `children` instead of `modules`.\n""""""\n\nimport typing\n\nimport torch\n\nfrom .._base import Base\nfrom ._dev_utils import register_condition\n\n\nclass _Registrator(Base):\n    r""""""**{}**\n\n    Attributes\n    ----------\n    handles : List[torch.utils.hooks.RemovableHandle]\n        Handles for registered hooks, each corresponds to specific submodule.\n        Can be used to unregister certain hooks (though discouraged).\n\n    """"""\n\n    def __init__(self, register_method, hook):\n        self._register_method: typing.Callable = register_method\n        self.hook: typing.Callable = hook\n        self.handles = []\n\n    def _register_hook(\n        self,\n        network,\n        iterating_function: str,\n        types: typing.Tuple[typing.Any] = None,\n        indices: typing.List[int] = None,\n    ):\n        for index, module in enumerate(getattr(network, iterating_function)()):\n            if register_condition(module, types, index, indices):\n                self.handles.append(getattr(module, self._register_method)(self.hook))\n\n    def __iter__(self):\n        return iter(self.handles)\n\n    def __len__(self):\n        return len(self.handles)\n\n    def remove(self, index) -> None:\n        r""""""**Remove hook specified by** `index`.\n\n        Parameters\n        ----------\n        index: int\n            Index of subhook (usually registered for layer)\n\n        """"""\n        handle = self.handles.pop(index)\n        handle.remove()\n\n    def modules(\n        self,\n        module: torch.nn.Module,\n        types: typing.Tuple[typing.Any] = None,\n        indices: typing.List[int] = None,\n    ):\n        r""""""**Register** `hook` **using types and/or indices via** `modules` **hook**.\n\n        This function will use `modules` method of `torch.nn.Module` to iterate over available submodules. If you wish to iterate non-recursively, use `children`.\n\n        **Important:**\n\n        If `types` and `indices` are left with their default values, all modules\n        will have `subrecorders` registered.\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n            Module (usually neural network) for which inputs will be collected.\n        types : Tuple[typing.Any], optional\n            Module types for which data will be recorded. E.g. `(torch.nn.Conv2d, torch.nn.Linear)`\n            will register `subrecorders` on every module being instance of either `Conv2d` or `Linear`.\n            Default: `None`\n        indices : Iterable[int], optional\n            Indices of modules whose inputs will be registered.\n            Default: `None`\n\n        Returns\n        -------\n        self\n        """"""\n\n        self._register_hook(module, ""modules"", types, indices)\n        return self\n\n    def children(\n        self,\n        network,\n        types: typing.Tuple[typing.Any] = None,\n        indices: typing.List[int] = None,\n    ):\n        r""""""**Register** `subrecorders` **using types and/or indices via** `children` **hook**.\n\n        This function will use `children` method of `torch.nn.Module` to iterate over available submodules. If you wish to iterate recursively, use `modules`.\n\n        **Important:**\n\n        If `types` and `indices` are left with their default values, all modules\n        will have `subrecorders` registered.\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n            Module (usually neural network) for which inputs will be collected.\n        types : Tuple[typing.Any], optional\n            Module types for which data will be recorded. E.g. `(torch.nn.Conv2d, torch.nn.Linear)`\n            will register `subrecorders` on every module being instance of either `Conv2d` or `Linear`.\n            Default: `None`\n        indices : Iterable[int], optional\n            Indices of modules whose inputs will be registered.\n            Default: `None`\n\n        Returns\n        -------\n        self\n        """"""\n\n        self._register_hook(network, ""children"", types, indices)\n        return self\n\n\nclass ForwardPre(_Registrator):\n    __doc__ = _Registrator.__doc__.format(\n        ""Register forward pre hook based on module\'s type or indices.""\n    )\n\n    def __init__(self, hook: typing.Callable):\n        self.hook = hook\n        super().__init__(""register_forward_pre_hook"", self.hook)\n\n\nclass Forward(_Registrator):\n    __doc__ = _Registrator.__doc__.format(\n        ""Register forward hook based on module\'s type or indices.""\n    )\n\n    def __init__(self, hook: typing.Callable):\n        self.hook = hook\n        super().__init__(""register_forward_hook"", self.hook)\n\n\nclass Backward(_Registrator):\n    __doc__ = _Registrator.__doc__.format(\n        ""Register backward hook based on module\'s type or indices.""\n    )\n\n    def __init__(self, hook: typing.Callable):\n        self.hook = hook\n        super().__init__(""register_backward_hook"", self.hook)\n'"
torchfunc/performance/__init__.py,22,"b'r""""""\n**This package allows you to get info and tips about performance of your neural networks.**\n\nFollowing functions should be considered as general recommendations.\nFor specific/customized tips, use specific submodules.\n\n""""""\n\nimport typing\n\nimport torch\n\nimport torchfunc\n\nfrom . import layers, technology\nfrom .layers import Depthwise, Inplace\nfrom .technology import TensorCores\n\n\ndef report(module: torch.nn.Module) -> typing.Dict[str, typing.Any]:\n    r""""""**Run essential module\'s performance analysis with default settings.**\n\n    Following tests will be performed:\n\n    - Module being an instance of `torch.nn.ScriptModule`\n    - Apex (mixed precision training) availability\n    - Any inplace ops used\n    - Analysis of compliance with `TensorCores` technology\n    - Any depthwise convolution used\n\n    Report returns data in machine-ready type; if you wish to have easy to follow\n    guidelines use function `tips`.\n\n    **Example**::\n\n        model = torch.nn.Sequential(\n            torch.nn.Linear(784, 100),\n            torch.nn.ReLU(),\n            torch.nn.Linear(100, 50),\n            torch.nn.ReLU(),\n            torch.nn.Linear(50, 10),\n        )\n        report = torchfunc.performance.report(model)\n\n    Parameters\n    ----------\n    module: torch.nn.Module\n            Module to be tested against test suite.\n\n    Returns\n    -------\n    Dict[str, Any]\n            Dictionary with keys:\n\n            - torchscript: True if module is an instance of torch.jit.ScriptModule\n            - apex: True if apex installed (recommended mixed precision training library from NVidia for PyTorch)\n            - tensorcores: same as `torchscript.performance.technology.TensorCores`\n            - inplace: same as `torchscript.performance.layers.Inplace`\n            - depthwise: same as `torchscript.performance.layers.Depthwise`\n\n    """"""\n\n    return {\n        ""torchscript"": isinstance(module, torch.jit.ScriptModule),\n        ""apex"": torchfunc.installed(""apex""),\n        ""tensorcores"": TensorCores().modules(module),\n        ""inplace"": Inplace().modules(module),\n        ""depthwise"": Depthwise().modules(module),\n    }\n\n\n# Text parsing is not the prettiest thing ever :(\ndef tips(module: torch.nn.Module, general: bool = True, specific: bool = True):\n    r""""""**Return string describing possible performance improvements one can undertake.**\n\n    Internally report will be called and it\'s output parsed and described.\n    It is the easiest way to get information about your module/network\n    and to quickly check possible performance improvements you could use.\n\n    **Example**::\n\n        model = torch.nn.Sequential(\n            torch.nn.Linear(784, 100),\n            torch.nn.ReLU(),\n            torch.nn.Linear(100, 50),\n            torch.nn.ReLU(),\n            torch.nn.Linear(50, 10),\n        )\n        print(torchfunc.performance.tips(model)) # Display found vulnerabilities\n\n    Parameters\n    ----------\n    module: torch.nn.Module\n            Module to be tested against test suite.\n    general: bool, optional\n            Return general (not specific to your module) tips. Default: True\n    specific: bool, optional\n            Return specific tips for your module. Default: True\n\n    Returns\n    -------\n    str\n            Human readable version of report, highlighting steps\n            one can take to improve module\'s performance.\n\n    """"""\n\n    def general_tips():\n        return (\n            ""\\n===========================GENERAL TIPS===========================\\n""\n            + ""\\n- Make sure you are running newest PyTorch version. ""\n            + ""See available releases: https://github.com/pytorch/pytorch/tags\\n""\n            + ""- Use GPU for larger batches, CPU might be suitable for smaller jobs.\\n""\n            + ""- Use mixed-precision training on GPU, preferably automated, e.g. NVIDIA Apex: https://github.com/NVIDIA/apex.\\n""\n        )\n\n    def specific_tips():\n        def parse_string(text: str) -> str:\n            if text != """":\n                return ""\\n=======> "" + text\n            return text\n\n        def torchscript():\n            if not isinstance(module, torch.jit.ScriptModule):\n                return (\n                    ""Module should be an instance of torch.jit.ScriptModule.\\n""\n                    + ""See https://pytorch.org/docs/stable/jit.html for more information.""\n                )\n            return """"\n\n        def apex():\n            if not torchfunc.installed(""apex""):\n                return (\n                    ""NVIDIA\'s Apex is not installed. It is the easiest way to use mixed precision training.\\n""\n                    + ""See https://github.com/NVIDIA/apex for more information and installation.""\n                )\n            return """"\n\n        specific_tips = """"\n        specific_tips += parse_string(torchscript())\n        specific_tips += parse_string(apex())\n        specific_tips += parse_string(Inplace().tips(module))\n        specific_tips += parse_string(Depthwise().tips(module))\n        specific_tips += parse_string(TensorCores().tips(module))\n        if specific_tips != """":\n            specific_tips = (\n                ""\\n===========================SPECIFIC TIPS===========================\\n""\n                + specific_tips\n            )\n        return specific_tips\n\n    ###########################################################################\n    #\n    #                               MAIN LOGIC\n    #\n    ###########################################################################\n\n    results = """"\n    if general:\n        results += general_tips()\n    if specific:\n        results += specific_tips()\n\n    return results\n'"
torchfunc/performance/layers.py,24,"b'r""""""\n**Check any performance caveats related to PyTorch and it\'s layers.**\n\nUsing functionalities below you can check whether your architecture follows\ncurrent good practices related to performance of `torch.nn.Module` concrete layers.\n\n""""""\n\nimport abc\nimport collections\nimport sys\nimport typing\n\nimport torch\n\nfrom .._base import Base\n\n\nclass Depthwise(Base):\n    r""""""**Check whether any convolution layer is a so-called depthwise convolution.**\n\n    Depthwise convolution is faster for images with input data in format\n    (batch, height, width, channel) as specialized kernels are available.\n\n    Currently PyTorch does not support this functionality, so using those may actually\n    slow down your neural network.\n\n    Depthwise convolution might still be useful in order to save memory, not so\n    performance-wise.\n\n    For easy to follow guidelines, use `tips` method of this class.\n\n    Example::\n\n        model = torch.nn.Sequential(\n            torch.nn.Conv1d(64, 64, kernel_size=3, groups=64),\n            torch.nn.Conv2d(3, 32, kernel_size=3, groups=1),\n            torch.nn.Conv2d(32, 32, kernel_size=3, groups=32),\n        )\n        for index in torchfunc.performance.layers.Depthwise().children(model):\n            print(index) # Should print 0 and 2\n\n    Attributes\n    ----------\n    checkers : Tuple[Callable], optional\n            Functions checking whether given module is depthwise convolution.\n            Should return True in such case, False otherwise.\n            Default: `Depthwise.default_checker`; if module\'s groups count is equal\n            to module\'s `in_channels` True is returned. Works for PyTorch\'s `ConvNd` layers.\n\n    """"""\n\n    def __init__(\n        self, checkers: typing.Tuple[typing.Callable[[torch.nn.Module], bool]] = None\n    ):\n        self.checkers: typing.Tuple[typing.Callable] = (\n            Depthwise.default_checker,\n        ) if checkers is None else checkers\n\n    @classmethod\n    def default_checker(cls, module):\n        r""""""**Default checking method suitable for PyTorch\'s built-in convolution layers.**\n\n        Checks whether count of groups is equal to count of in_channels.\n\n        **Important:**\n\n        If you want to provide custom checker, you should return `True`\n        (module being depthwise convolution) or `False` for any\n        module that is passed to this function.\n\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n                Module (or submodule) for which True means it\'s depthwise.\n\n        Returns\n        ----------\n        List[int]\n                Submodule\'s indices where depthwise convolution was located.\n        """"""\n        if hasattr(module, ""groups"") and hasattr(module, ""in_channels""):\n            return module.groups == module.in_channels and module.in_channels != 1\n        return False\n\n    def _analyse(self, module, function):\n        for index, submodule in enumerate(getattr(module, function)()):\n            for checker in self.checkers:\n                if checker(submodule):\n                    yield index\n\n    def modules(self, module: torch.nn.Module):\n        r""""""**Look for Depthwise convolution using** `modules()` **method (recursive scanning).**\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n                Module to be scanned\n\n        Yields\n        ------\n        int\n                Indices where module is considered depthwise convolution.\n        """"""\n\n        yield from self._analyse(module, ""modules"")\n\n    def children(self, module: torch.nn.Module):\n        r""""""**Look for Depthwise convolution using module\'s** `children()` **method (shallow scanning).**\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n                Module to be scanned\n\n        Yields\n        ------\n        int\n                Indices where module is considered depthwise convolution.\n        """"""\n        yield from self._analyse(module, ""children"")\n\n    def tips(self, module: torch.nn.Module) -> str:\n        r""""""**Return** `str` **representation of** `modules()` **method.**\n\n        It is advised to use this function to get tips in order to easily fix\n        performance issues related to depthwise convolution.\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n                Module to be scanned\n\n        Returns\n        -------\n        str\n                String representing tips related to depthwise convolution.\n        """"""\n        depthwise = self.modules(module)\n        if depthwise:\n            return (\n                ""Depthwise convolutions are not currently using specialized kernel and might be slower.\\n""\n                + ""See this issue: https://github.com/pytorch/pytorch/issues/18631 for more information.\\n""\n                + ""Indices of those modules:\\n""\n                + str(list(depthwise))\n                + ""\\nYou may want to decrease number of groups (like it\'s done for ResNeXt) for possible speed & accuracy improvements.""\n            )\n        return """"\n\n\nclass Inplace(Base):\n    r""""""**Check whether any submodule/child of module is set to inplace mode.**\n\n    Inplace operations may interfere with traced module (kernel fusion) and cause slowdowns.\n    See `this issue <https://github.com/pytorch/pytorch/issues/23655>`__ for more information.\n\n    **Example**::\n\n        model = torch.nn.Sequential(\n            torch.nn.Conv2d(3, 64, kernel_size=3, groups=64),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Conv2d(64, 64, kernel_size=3),\n            torch.nn.ReLU6(inplace=True),\n            torch.nn.Conv2d(64, 128, kernel_size=3, groups=32),\n        )\n\n        for index in torchfunc.performance.layers.Inplace().children(model):\n            print(index) # Should print 1 and 3\n\n    For easy to follow guidelines, use `tips` method of this class.\n\n    Attributes\n    ----------\n    attribute: Tuple[str], optional\n            Attributes names indicating whether current op is inplace. Do not specify if you are not using\n            custom modules not following pytorch\'s conventions. Default: `(""inplace"",)`.\n            Existence of all those attributes will be checked in module. If any of them exists\n            and is `True`, it will be considered as inplace operation.\n\n    """"""\n\n    def __init__(self, inplace: typing.Tuple[str] = (""inplace"",)):\n        self.inplace = inplace\n\n    def _analyse(self, module: torch.nn.Module, method: str):\n        for index, submodule in enumerate(getattr(module, method)()):\n            for attribute in self.inplace:\n                if hasattr(submodule, attribute):\n                    if getattr(submodule, attribute):\n                        yield index\n\n    def modules(self, module: torch.nn.Module):\n        r""""""**Look for inplace operation using** `modules()` **method (recursive scanning).**\n\n        Yields\n        ------\n        int\n                Indices where module is probably `inplace`.\n        """"""\n        yield from self._analyse(module, ""modules"")\n\n    def children(self, module: torch.nn.Module):\n        r""""""**Look for inplace operation using** `children()` **method (shallow scanning).**\n\n        Yields\n        ------\n        int\n                Indices where module is probably `inplace`.\n        """"""\n        yield from self._analyse(module, ""children"")\n\n    def tips(self, module: torch.nn.Module) -> str:\n        r""""""**Return** `str` **representation of** `modules()` **method.**\n\n        It is advised to use this function to get tips in order to easily fix\n        performance issues related to inplace operations.\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n                Module to be scanned\n\n        Returns\n        -------\n        str\n                String representing tips related to inplace operations.\n        """"""\n        inplace = self.modules(module)\n        if inplace:\n            return (\n                ""In-place operations might harm kernel fusion. Indices of those modules:\\n""\n                + str(list(inplace))\n                + ""\\nYou may want to remove inplace flag (see this issue: https://github.com/pytorch/pytorch/issues/23655)""\n            )\n        return """"\n'"
torchfunc/performance/technology.py,38,"b'r""""""\n**Analyse technological aspects (e.g. compatibility with Tensor Cores) of your module.**\n\nUsing functionalities below you can check whether your architecture can use\ntechnology dependent speed improvements.\n\n""""""\nimport collections\nimport typing\n\nimport torch\n\nfrom .._base import Base\n\n# TO-DO\n# https://stackoverflow.com/questions/47913943/is-it-possible-to-see-that-kernel-execution-happened-on-tensor-cores-or-not-via (?)\n# Arithmetic Intensity: https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html#math-mem\n\n\nclass TensorCores(Base):\n    r""""""**Perform Tensor Cores compatibility tests for given module and it\'s submodules/children.**\n\n    Interpretation of data returned from this function may pose some problems to users\n    unfamiliar with ideas standing behind Tensor Cores.\n\n    Is is advised to use method `tips` to get user friendly information your\n    `torch.nn.Module`\'s compatitilibty with Tensor Cores.\n\n    Example::\n\n        model = torch.nn.Sequential(\n            torch.nn.Linear(128, 100).half(), # Half precision is compatible\n            torch.nn.ReLU(),\n            torch.nn.Linear(100, 50),\n            torch.nn.ReLU(),\n            torch.nn.Linear(50, 10).half(),\n        )\n\n        analysis = torchscripts.peformance.technology.TensorCores().children(model)\n        # Should return dictionary indicating problems with second Linear (wrong shape and type)\n        # And last Linear (wrong shape)\n\n    Attributes\n    ----------\n    linear_types: Tuple[torch.nn.Module], optional\n            Tuple of types to be considered linear and which should run with tensor\n            cores kernels.\n\n            **Default:** `(torch.nn.Linear, torch.nn.Bilinear)`\n    convolution_types: Tuple[torch.nn.Module], optional\n            Tuple of types to be considered convolutional and which should run with tensor\n            cores kernels.\n\n            **Default:** `(torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d)`\n    linear_inputs: Dict[torch.nn.Module, Tuple[str]], optional\n            Dict-like where key is the type of module (e.g. `torch.nn.Linear`) and values\n            are tuples of attribute names specifying names of input attributes of this type of layer.\n            You could use `collections.defaultdict` for easier specification of prevailing attribute names\n            like `in_features` for torch.nn.Linear.\n            More than one input can be specified, as is the case for `torch.nn.Bilinear`.\n\n            **Default:** `{default_type: (""in_features"",), torch.nn.Bilinear: (""in_features1"", ""in_features2"")}`\n    linear_outputs: Dict[torch.nn.Module, Tuple[str]], optional\n            Dict-like where key is the type of module (e.g. `torch.nn.Linear`) and values\n            are tuples of attribute names specifying names of output attributes of this type of layer.\n            You could use `collections.defaultdict` for easier specification of prevailing attribute names\n            like `out_features` for `torch.nn.Linear`.\n            More than one output can be specified, same as `linear_inputs`.\n\n            **Default:** `{default_type: (""out_features"",)}`\n    convolution_inputs: Dict[torch.nn.Module, Tuple[str]], optional\n            Dict-like where key is the type of module (e.g. `torch.nn.Conv2d`) and values\n            are tuples of attribute names specifying names of input channels attributes of this type of layer.\n            You could use `collections.defaultdict` for easier specification of prevailing attribute names\n            like `in_channels` for all torch\'s convolutions.\n            More than one output can be specified, same as `linear_inputs`.\n\n            **Default:** `{default_type: (""in_channels"",)}`\n    convolution_outputs: Dict[torch.nn.Module, Tuple[str]], optional\n            Dict-like where key is the type of module (e.g. torch.nn.Conv2d) and values\n            are tuples of attribute names specifying names of output channels attributes of this type of layer.\n            You could use collections.defaultdict for easier specification of prevailing attribute names\n            like out_channels for all torch\'s convolutions.\n            More than one output can be specified, same as linear_inputs.\n\n            **Default:** `{default_type: (""out_channels"",)}`\n    float_types: typing.Tuple[types], optional\n            Floating point types compatible with TensorCores.\n\n            **Default:** `(torch.half, )`\n    integer_types: typing.Tuple[types], optional\n            Interger types compatible with TensorCores.\n\n            **Default:** `(torch.short, )`\n\n    """"""\n\n    def __init__(\n        self,\n        linear_types=(torch.nn.Linear, torch.nn.Bilinear,),\n        convolution_types=(torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d,),\n        linear_inputs=None,\n        linear_outputs=None,\n        convolution_inputs=None,\n        convolution_outputs=None,\n        float_types=(torch.half,),\n        integer_types=(torch.short,),\n    ):\n\n        self.linear_types = linear_types\n        self.convolution_types = convolution_types\n        if linear_inputs is None:\n            self.linear_inputs = collections.defaultdict(lambda: (""in_features"",))\n            self.linear_inputs[torch.nn.Bilinear] = (""in_features1"", ""in_features2"")\n        else:\n            self.linear_inputs = linear_inputs\n        if linear_outputs is None:\n            self.linear_outputs = collections.defaultdict(lambda: (""out_features"",))\n        else:\n            self.linear_outputs = linear_outputs\n        if convolution_inputs is None:\n            self.convolution_inputs = collections.defaultdict(lambda: (""in_channels"",))\n        else:\n            self.convolution_inputs = convolution_inputs\n        if convolution_outputs is None:\n            self.convolution_outputs = collections.defaultdict(\n                lambda: (""out_channels"",)\n            )\n        else:\n            self.convolution_outputs = convolution_outputs\n        self.float_types = float_types\n        self.integer_types = integer_types\n\n    def _analyse(self, module, function: str):\n        def _correct_types(data, submodule, index, is_float: bool):\n            correct_types = self.float_types if is_float else self.integer_types\n            if not any(\n                correct_type == submodule.weight.dtype for correct_type in correct_types\n            ):\n                data[""type""][""float"" if is_float else ""integer""].append(index)\n\n        def _correct_shapes(\n            data, submodule, index, attributes, attribute_name, is_float: bool\n        ):\n            for attribute in attributes[type(submodule)]:\n                if hasattr(submodule, attribute):\n                    shape = getattr(submodule, attribute)\n                    correct = shape % (8 if is_float else 16) == 0\n                    if not correct:\n                        data[""shape""][""float"" if is_float else ""integer""][\n                            attribute_name\n                        ].append(index)\n\n        def _find_problems(data, submodule, index, is_float: bool):\n            def _operation_problems(operation: str):\n                for entry in (""inputs"", ""outputs""):\n                    _correct_shapes(\n                        data,\n                        submodule,\n                        index,\n                        getattr(self, operation + ""_"" + entry),\n                        entry,\n                        is_float,\n                    )\n\n            _correct_types(data, submodule, index, is_float)\n\n            if isinstance(submodule, self.linear_types):\n                _operation_problems(""linear"")\n            elif isinstance(submodule, self.convolution_types):\n                _operation_problems(""convolution"")\n\n        #######################################################################\n        #\n        #                           MAIN FUNCTION\n        #\n        #######################################################################\n\n        data = {\n            ""type"": {""float"": [], ""integer"": []},\n            ""shape"": {\n                ""float"": {""inputs"": [], ""outputs"": []},\n                ""integer"": {""inputs"": [], ""outputs"": []},\n            },\n        }\n\n        for index, submodule in enumerate(getattr(module, function)()):\n            if hasattr(submodule, ""weight""):\n                if torch.is_floating_point(submodule.weight):\n                    _find_problems(data, submodule, index, is_float=True)\n                else:\n                    _find_problems(data, submodule, index, is_float=False)\n\n        return data\n\n    def modules(self, module: torch.nn.Module):\n        r""""""**Check Tensor Cores compatibility using** `modules()` **method (recursive scanning).**\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n                Module to be scanned for Tensor Cores compatibility\n\n        Returns\n        -------\n        Nested dictionary\n                Multilevel dictionary describing modules incompatible with tensor cores.\n                First level consists of two fields:\n\n                - `type`: incompatible type with TensorCores\n                - `shape`: incompatible types with TensorCores\n\n                Second level for type:\n\n                - `float`: module is floating point type but it\'s type is incompatible.\n                Contains list of submodule\'s indices posing this problem.\n                - `integer`: module is integer type but it\'s type is incompatible\n                Contains list of submodule\'s indices posing this problem.\n\n                Second level for shape:\n\n                - `float`: module is floating point type and has incorrect shape\n                - `integer`: module is integer type and has incorrect shape\n\n                Third level for shape\'s `float` and `integer`:\n\n                - `input`: module\'s input shape is incompatible with Tensor Cores\n                Contains list of submodule\'s indices posing this problem.\n                - `output`: module\'s output shape is incompatible with Tensor Cores\n                Contains list of submodule\'s indices posing this problem.\n\n                As it\'s hard to parse, it is suggested to use tips for readable output.\n        """"""\n\n        return self._analyse(module, ""modules"")\n\n    def children(self, module: torch.nn.Module):\n        r""""""**Check Tensor Cores compatibility using** `children()` **method (shallow scanning).**\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n                Module to be scanned for Tensor Cores compatibility\n\n        Returns\n        -------\n        Nested dictionary\n                Multilevel dictionary describing modules incompatible with tensor cores.\n                First level consists of two fields:\n\n                - `type`: incompatible type with TensorCores\n                - `shape`: incompatible types with TensorCores\n\n                Second level for type:\n\n                - `float`: module is floating point type but it\'s type is incompatible.\n                Contains list of submodule\'s indices posing this problem.\n                - `integer`: module is integer type but it\'s type is incompatible\n                Contains list of submodule\'s indices posing this problem.\n\n                Second level for shape:\n\n                - `float`: module is floating point type and has incorrect shape\n                - `integer`: module is integer type and has incorrect shape\n\n                Third level for shape\'s `float` and `integer`:\n\n                - `input`: module\'s input shape is incompatible with Tensor Cores\n                Contains list of submodule\'s indices posing this problem.\n                - `output`: module\'s output shape is incompatible with Tensor Cores\n                Contains list of submodule\'s indices posing this problem.\n\n                As it\'s hard to parse, it is suggested to use tips for readable output.\n        """"""\n\n        return self._analyse(module, ""children"")\n\n    def tips(self, module: torch.nn.Module) -> str:\n        r""""""**Return** `str` **representation of** `modules()` **method.**\n\n        It is advised to use this function to get tips in order to easily fix\n        possible performance issues related to Tensor Cores.\n\n        Parameters\n        ----------\n        module : torch.nn.Module\n                Module to be scanned\n\n        Returns\n        -------\n        str\n                String representing tips related to Tensor Cores.\n        """"""\n        data = self.modules(module)\n\n        def types():\n            _types = data[""type""]\n\n            def parse_type(is_float: bool, goal):\n                key = ""float"" if is_float else ""integer""\n                if _types[key]:\n                    return ""\\nModules where {} type is not {}:\\n"".format(\n                        key, goal\n                    ) + str(_types[key])\n                return """"\n\n            return parse_type(True, ""torch.half"") + parse_type(False, ""torch.short"")\n\n        def shape():\n            def parse_shape(dictionary, is_input: bool, goal):\n                key = ""inputs"" if is_input else ""outputs""\n                if dictionary[key]:\n                    return ""\\nModules where {} shape should be divisible by {}:\\n"".format(\n                        key, goal\n                    ) + str(\n                        dictionary[key]\n                    )\n                return """"\n\n            _shapes = data[""shape""]\n\n            def floating():\n                _floats = _shapes[""float""]\n                return parse_shape(_floats, True, 8) + parse_shape(_floats, False, 8)\n\n            def integer():\n                _integers = _shapes[""integer""]\n                return parse_shape(_integers, True, 16) + parse_shape(\n                    _integers, False, 16\n                )\n\n            return floating() + integer()\n\n        output = types() + shape()\n        if output != """":\n            output = ""TensorCores incompatible modules:"" + output\n        return output\n'"
docs/code/source/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nimport pytorch_sphinx_theme\n\n\ndef get_version():\n    namespace = {}\n\n    exec(open(""../../../torchfunc/_version.py"").read(), namespace)  # get version\n    return namespace[""__version__""]\n\n\nsys.path.insert(0, os.path.abspath(""../../..""))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = ""torchfunc""\ncopyright = ""2019, Szymon Maszke""\nauthor = ""Szymon Maszke""\nversion = get_version()\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.autosummary"",\n    ""sphinx.ext.doctest"",\n    ""sphinx.ext.intersphinx"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.napoleon"",\n    ""sphinx.ext.viewcode"",\n    ""sphinxcontrib.katex"",\n]\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""pytorch_sphinx_theme""\nhtml_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]\n\nhtml_theme_options = {\n    ""related"": ""https://szymonmaszke.github.io/torchfunc/related.html"",\n    ""roadmap"": ""https://github.com/szymonmaszke/torchfunc/blob/master/ROADMAP.md"",\n    ""github_issues"": ""https://github.com/szymonmaszke/torchfunc/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc"",\n    ""home"": ""https://szymonmaszke.github.io/torchfunc"",\n    ""installation"": ""https://szymonmaszke.github.io/torchfunc/#installation"",\n    ""github"": ""https://github.com/szymonmaszke/torchfunc"",\n    ""docs"": ""https://szymonmaszke.github.io/torchfunc/#torchfunc"",\n    ""collapse_navigation"": False,\n    ""display_version"": True,\n    ""logo_only"": False,\n    ""canonical_url"": ""https://szymonmaszke.github.io/torchfunc/"",\n}\n\n# Other settings\n\ndefault_role = ""py:obj""  # Reference to Python by default\n'"
