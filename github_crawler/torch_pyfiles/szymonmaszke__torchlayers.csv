file_path,api_count,code
setup.py,0,"b'import pathlib\n\nimport setuptools\n\n\ndef read(HERE: pathlib.Path, filename, variable):\n    namespace = {}\n\n    exec(open(HERE / ""torchlayers"" / filename).read(), namespace)  # get version\n    return namespace[variable]\n\n\nHERE = pathlib.Path(__file__).resolve().parent\n\nsetuptools.setup(\n    name=read(HERE, pathlib.Path(""_name.py""), ""_name""),\n    version=read(HERE, pathlib.Path(""_version.py""), ""__version__""),\n    license=""MIT"",\n    author=""Szymon Maszke"",\n    author_email=""szymon.maszke@protonmail.com"",\n    description=""Input shape inference and SOTA custom layers for PyTorch."",\n    long_description=open(""README.md"", ""r"").read(),\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/pypa/torchlayers"",\n    packages=setuptools.find_packages(),\n    install_requires=[""torch>=1.3.0"",],\n    python_requires="">=3.7"",\n    classifiers=[\n        ""Development Status :: 2 - Pre-Alpha"",\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Intended Audience :: Developers"",\n        ""Operating System :: OS Independent"",\n        ""Topic :: Scientific/Engineering"",\n        ""Topic :: Software Development :: Libraries"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n    ],\n    project_urls={\n        ""Website"": ""https://szymonmaszke.github.io/torchlayers"",\n        ""Documentation"": ""https://szymonmaszke.github.io/torchlayers/#torchlayers"",\n        ""Issues"": ""https://github.com/szymonmaszke/torchlayers/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc"",\n    },\n    keywords=""pytorch keras input inference automatic shape layers sota custom imagenet resnet efficientnet"",\n)\n'"
tests/__init__.py,0,b''
tests/activations_test.py,2,"b'import torch\n\nimport pytest\nimport torchlayers as tl\n\n\n@pytest.mark.parametrize(""klass"", (""Swish"", ""HardSwish"", ""HardSigmoid""))\ndef test_object(klass):\n    getattr(tl, klass)()(torch.randn(4, 5, 6))\n\n\n@pytest.mark.parametrize(""function"", (""swish"", ""hard_swish"", ""hard_sigmoid""))\ndef test_functional(function):\n    getattr(tl, function)(torch.randn(4, 5, 6))\n'"
tests/attributes_test.py,0,"b'import torch\n\nimport torchlayers as tl\n\n\ndef test_module():\n    layer = tl.Conv2d(64, kernel_size=3)\n    assert layer.__module__ == ""torchlayers""\n'"
tests/convolution_test.py,20,"b'import itertools\n\nimport torch\n\nimport pytest\nimport torchlayers as tl\n\n\nclass AutoEncoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = tl.Sequential(\n            tl.Conv(64, kernel_size=7),\n            tl.activations.Swish(),  # Direct access to module .activations\n            tl.InvertedResidualBottleneck(squeeze_excitation=False),\n            tl.AvgPool(),  # shape 64 x 128 x 128, kernel_size=2 by default\n            tl.HardSwish(),  # Access simply through tl\n            tl.SeparableConv(128),  # Up number of channels to 128\n            tl.InvertedResidualBottleneck(),  # Default with squeeze excitation\n            torch.nn.ReLU(),\n            tl.AvgPool(),  # shape 128 x 64 x 64, kernel_size=2 by default\n            tl.DepthwiseConv(256),  # DepthwiseConv easier to use\n            # Pass input thrice through the same weights like in PolyNet\n            tl.Poly(tl.InvertedResidualBottleneck(), order=3),\n            tl.ReLU(),  # all torch.nn can be accessed via tl\n            tl.MaxPool(),  # shape 256 x 32 x 32\n            tl.Fire(out_channels=512),  # shape 512 x 32 x 32\n            tl.SqueezeExcitation(hidden=64),\n            tl.InvertedResidualBottleneck(),\n            tl.MaxPool(),  # shape 512 x 16 x 16\n            tl.InvertedResidualBottleneck(squeeze_excitation=False),\n            tl.Dropout(),  # Default 0.5 and Dropout2d for images\n            # Randomly Switch the last two layers with 0.5 probability\n            tl.StochasticDepth(\n                torch.nn.Sequential(\n                    tl.InvertedResidualBottleneck(squeeze_excitation=False),\n                    tl.InvertedResidualBottleneck(squeeze_excitation=False),\n                ),\n                p=0.5,\n            ),\n            tl.AvgPool(),  # shape 512 x 8 x 8\n        )\n\n        # Will make this one easier and repetitive\n        self.decoder = tl.Sequential(\n            tl.Poly(tl.InvertedResidualBottleneck(), order=2),\n            # Has ICNR initialization by default as well\n            tl.ConvPixelShuffle(out_channels=512, upscale_factor=2),\n            # Shape 512 x 16 x 16 after PixelShuffle\n            tl.Poly(tl.InvertedResidualBottleneck(), order=3),\n            tl.ConvPixelShuffle(out_channels=256, upscale_factor=2),\n            tl.StandardNormalNoise(),  # add Gaussian Noise\n            # Shape 256 x 32 x 32\n            tl.Poly(tl.InvertedResidualBottleneck(), order=3),\n            tl.ConvPixelShuffle(out_channels=128, upscale_factor=2),\n            # Shape 128 x 64 x 64\n            tl.Poly(tl.InvertedResidualBottleneck(), order=4),\n            tl.ConvPixelShuffle(out_channels=64, upscale_factor=2),\n            # Shape 64 x 128 x 128\n            tl.InvertedResidualBottleneck(),\n            tl.Conv(256),\n            tl.Swish(),\n            tl.BatchNorm(),\n            tl.ConvPixelShuffle(out_channels=32, upscale_factor=2),\n            # Shape 32 x 256 x 256\n            tl.Conv(16),\n            tl.Swish(),\n            tl.Conv(3),\n            # Shape 3 x 256 x 256\n        )\n\n    def forward(self, inputs):\n        return self.decoder(self.encoder(inputs))\n\n\n@pytest.fixture\ndef classification_model():\n    return tl.Sequential(\n        tl.Conv(64),\n        tl.ReLU(),\n        tl.MaxPool(),\n        tl.Residual(\n            tl.Sequential(\n                tl.Conv(64, groups=16),\n                tl.ReLU(),\n                tl.GroupNorm(num_groups=4),\n                tl.Conv(64, groups=16),\n                tl.ChannelShuffle(groups=16),\n                tl.ReLU(),\n            )\n        ),\n        tl.SqueezeExcitation(),\n        tl.Sequential(tl.Dropout(), tl.Conv(128), tl.ReLU(), tl.InstanceNorm(),),\n        tl.Poly(\n            tl.WayPoly(tl.Fire(128), tl.Fire(128), tl.Fire(128), tl.Fire(128),),\n            order=2,\n        ),\n        tl.AvgPool(),\n        tl.StochasticDepth(tl.Fire(128, hidden_channels=64)),\n        tl.ReLU(),\n        tl.GlobalAvgPool(),\n        tl.Linear(64),\n    )\n\n\n@pytest.fixture\ndef autoencoder_model():\n    return AutoEncoder()\n\n\ndef test_text_cnn():\n    model = torch.nn.Sequential(\n        tl.Conv(64),  # specify ONLY out_channels\n        torch.nn.ReLU(),  # use torch.nn wherever you wish\n        tl.BatchNorm(),  # BatchNormNd inferred from input\n        tl.Conv(128),  # Default kernel_size equal to 3\n        tl.ReLU(),\n        tl.Conv(256, kernel_size=11),  # ""same"" padding as default\n        tl.GlobalMaxPool(),  # Known from Keras\n        tl.Linear(10),  # Output for 10 classes\n    )\n\n    tl.build(model, torch.randn(2, 300, 1))\n\n\ndef test_classification(classification_model):\n    classification_model = tl.build(classification_model, torch.randn(16, 3, 28, 28))\n    optimizer = torch.optim.Adam(classification_model.parameters())\n    criterion = torch.nn.CrossEntropyLoss()\n\n    for _ in range(16):\n        output = classification_model(torch.randn(16, 3, 28, 28))\n        loss = criterion(output, torch.randint(10, (16,)))\n        loss.backward()\n\n        optimizer.zero_grad()\n\n\ndef test_conv_pixel_shuffle():\n    model = tl.build(\n        tl.ConvPixelShuffle(out_channels=512, upscale_factor=2),\n        torch.randn(1, 256, 128, 128),\n    )\n\n\ndef test_residual_bottleneck():\n    model = tl.build(tl.InvertedResidualBottleneck(), torch.randn(1, 128, 128, 128))\n\n\ndef test_autoencoder(autoencoder_model):\n    autoencoder_model = tl.build(autoencoder_model, torch.randn(1, 3, 256, 256))\n\n    optimizer = torch.optim.Adam(autoencoder_model.parameters())\n    criterion = torch.nn.MSELoss()\n\n    for _ in range(16):\n        inputs = torch.randn(1, 3, 256, 256)\n        output = autoencoder_model(inputs)\n        loss = criterion(output, inputs)\n        loss.backward()\n\n        optimizer.zero_grad()\n\n\ndef test_same_padding_1d():\n    inputs = torch.randn(1, 5, 125)\n    for kernel_size, stride, dilation in itertools.product(\n        range(1, 20, 2), *[range(1, 20, 2) for _ in range(2)]\n    ):\n        classification_model = tl.build(\n            tl.Conv(\n                5,\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                padding=""same"",\n            ),\n            inputs,\n        )\n        output = classification_model(inputs)\n        assert output.shape == inputs.shape\n\n\ndef test_same_padding_2d():\n    inputs = torch.randn(1, 5, 100, 100)\n    for kernel_size, stride, dilation in itertools.product(\n        range(1, 15, 2), *[range(1, 8, 2) for _ in range(2)]\n    ):\n        for kernel_size2, stride2, dilation2 in itertools.product(\n            range(1, 8, 2), *[range(1, 8, 2) for _ in range(2)]\n        ):\n            classification_model = tl.build(\n                tl.Conv(\n                    5,\n                    kernel_size=(kernel_size, kernel_size2),\n                    stride=(stride, stride2),\n                    dilation=(dilation, dilation2),\n                    padding=""same"",\n                ),\n                inputs,\n            )\n            output = classification_model(inputs)\n            assert output.shape == inputs.shape\n'"
tests/general_test.py,12,"b'import torch\n\nimport pytest\nimport torchlayers as tl\n\n\nclass ConcatenateProxy(torch.nn.Module):\n    # Return same tensor three times\n    # You could explicitly return a list or tuple as well\n    def forward(self, tensor):\n        return tensor, tensor, tensor\n\n\nclass _CustomLinearImpl(torch.nn.Linear):\n    def __init__(self, in_features, out_features, bias: bool = True):\n        super().__init__(in_features, out_features, bias)\n        self.some_params = torch.nn.Parameter(torch.randn(2, out_features))\n\n\nCustomLinear = tl.infer(_CustomLinearImpl)\n\n\n@pytest.fixture\ndef model():\n    return tl.Sequential(\n        tl.Conv(64),\n        tl.ReLU(),\n        tl.MaxPool(),\n        tl.BatchNorm(),\n        tl.Conv(128),\n        tl.ReLU(),\n        tl.MaxPool(),\n        tl.Conv(256),\n        tl.ReLU(),\n        tl.Reshape(-1),\n    )\n\n\ndef test_reshape(model):\n    assert model(torch.randn(16, 1, 32, 32)).shape == (16, 256 * 8 * 8)\n\n\ndef test_lambda():\n    layer = tl.Lambda(lambda inputs: inputs * 3)\n    output = layer(torch.ones(16))\n    assert torch.sum(output) == 16 * 3\n\n\ndef test_concatenate():\n    model = torch.nn.Sequential(ConcatenateProxy(), tl.Concatenate(dim=-1))\n    assert model(torch.randn(64, 20)).shape == torch.randn(64, 60).shape\n\n\ndef test_custom_inferable_output():\n    layer = CustomLinear(out_features=32)\n    assert layer(torch.rand(16, 64)).shape == torch.randn(16, 32).shape\n\n\ndef test_custom_inferable_parameters():\n    layer = CustomLinear(32)\n    layer(torch.rand(16, 64))\n    assert layer.some_params.shape == (2, 32)\n\n\ndef test_custom_inferable_build():\n    layer = CustomLinear(32)\n    layer = tl.build(layer, torch.rand(16, 64))\n    assert layer.some_params.shape == (2, 32)\n\n\ndef test_smoke_summary(model):\n    summary = tl.summary(model, torch.randn(1, 3, 32, 32)).string(\n        inputs=False, buffers=False\n    )\n    assert (len(summary)) == 1459\n'"
tests/jit_test.py,8,"b'import pathlib\nimport tempfile\n\nimport torch\n\nimport torchlayers as tl\n\n\ndef test_basic_jit():\n    inputs = torch.randn(16, 3, 32, 32)\n\n    layer = tl.build(tl.Conv(64), inputs)\n    output = layer(inputs)\n    new_model = torch.jit.script(layer)\n\n    new_output = new_model(inputs)\n    assert torch.allclose(output, new_output)\n\n\ndef test_basic_jit_save():\n    inputs = torch.randn(16, 3, 32, 32)\n    temp = pathlib.Path(tempfile.gettempdir())\n\n    layer = tl.build(tl.Conv(64), inputs)\n    output = layer(inputs)\n    new_model = torch.jit.script(layer)\n\n    torch.jit.save(new_model, str(temp / ""jit.pt""))\n\n    loaded_model = torch.jit.load(str(temp / ""jit.pt""))\n    new_output = loaded_model(inputs)\n    assert torch.allclose(output, new_output)\n\n\n# Test with JIT all custom modules\n'"
tests/linear_test.py,2,"b'import torch\n\nimport torchlayers as tl\n\n\ndef test_last_dimension_linear():\n    module = tl.Linear(64)\n    module = tl.build(module, torch.randn(1, 3, 32, 32))\n    assert module(torch.randn(2, 6, 24, 32)).shape == (2, 6, 24, 64)\n'"
tests/normalization_test.py,1,"b'import torch\n\nimport pytest\nimport torchlayers as tl\n\n\ndef test_2dbatchnorm():\n    layer = tl.BatchNorm()\n    layer(torch.randn(20, 100))\n'"
tests/padding_test.py,1,"b'import torch\n\nimport torchlayers as tl\n\n\ndef test_odd_padding():\n    for kernel_size in range(1, 14, 2):\n        layer = tl.Conv(64, kernel_size=kernel_size)\n        assert layer(torch.rand(1, 3, 28, 28)).shape == (1, 64, 28, 28)\n'"
tests/pickle_test.py,12,"b'import pathlib\nimport tempfile\n\nimport torch\n\nimport torchlayers as tl\n\n\ndef test_save():\n    inputs = torch.randn(16, 32)\n    temp = pathlib.Path(tempfile.gettempdir())\n\n    layer = tl.build(tl.Linear(64), inputs)\n    output = layer(inputs)\n    torch.save(layer, temp / ""linear_model.pt"")\n\n    new_layer = torch.load(temp / ""linear_model.pt"")\n    new_output = new_layer(inputs)\n    assert torch.allclose(output, new_output)\n\n\ndef test_convolution_save():\n    inputs = torch.randn(16, 3, 32, 32)\n    temp = pathlib.Path(tempfile.gettempdir())\n\n    layer = tl.build(tl.Conv2d(64, kernel_size=3), inputs)\n    output = layer(inputs)\n    torch.save(layer, temp / ""conv_model.pt"")\n\n    new_layer = torch.load(temp / ""conv_model.pt"")\n    new_output = new_layer(inputs)\n    assert torch.allclose(output, new_output)\n\n\ndef test_dimension_save():\n    inputs = torch.randn(16, 3, 32, 32)\n    temp = pathlib.Path(tempfile.gettempdir())\n\n    layer = tl.build(tl.Conv(64), inputs)\n    output = layer(inputs)\n    torch.save(layer, temp / ""conv_model.pt"")\n\n    new_layer = torch.load(temp / ""conv_model.pt"")\n    new_output = new_layer(inputs)\n    assert torch.allclose(output, new_output)\n'"
tests/preprocessing_test.py,4,"b'import torch\nimport torchvision\n\nimport pytest\nimport torchfunc\nimport torchlayers as tl\n\n# torchvision.transforms.RandomHorizontalFlip(p=1.0) vs tl.RandomHorizontalFlip\n# torchvision.transforms.RandomVerticalFlip(p=1.0) vs tl.RandomVerticalFlip\n# torchvision.transforms.Normalize vs tl.Normalize with same mean and std\n# torchvision.transforms.functional.rotate via Lambdas with all Rotate90\'s\n# RandomHorizontalVerticalFlip with both from torchvision one after another\n\n# RandomApply withh the same probability and same outputs\n# RandomChoice via same seed?\n# RandomOrder via same seed?\n\n\nclass TorchvisionRotate:\n    def __init__(self, angle):\n        self.angle = angle\n\n    def __call__(self, x):\n        return torchvision.transforms.functional.to_tensor(\n            torchvision.transforms.functional.rotate(x, self.angle)\n        )\n\n\ndef generate_inputs():\n    operations = [\n        # Flipping\n        (\n            torchvision.transforms.Compose(\n                [\n                    torchvision.transforms.RandomHorizontalFlip(p=1.0),\n                    torchvision.transforms.ToTensor(),\n                ]\n            ),\n            tl.RandomHorizontalFlip(p=1.0),\n        ),\n        (\n            torchvision.transforms.Compose(\n                [\n                    torchvision.transforms.RandomVerticalFlip(p=1.0),\n                    torchvision.transforms.ToTensor(),\n                ]\n            ),\n            tl.RandomVerticalFlip(p=1.0),\n        ),\n        (\n            torchvision.transforms.Compose(\n                [\n                    torchvision.transforms.RandomVerticalFlip(p=1.0),\n                    torchvision.transforms.RandomHorizontalFlip(p=1.0),\n                    torchvision.transforms.ToTensor(),\n                ]\n            ),\n            tl.RandomVerticalHorizontalFlip(p=1.0),\n        ),\n        # Normalization\n        (\n            torchvision.transforms.Compose(\n                [\n                    torchvision.transforms.ToTensor(),\n                    torchvision.transforms.Normalize(\n                        [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n                    ),\n                ]\n            ),\n            tl.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ),\n        # Random 90 rotations\n        (TorchvisionRotate(90), tl.AntiClockwiseRandomRotate90(p=1.0),),\n        (TorchvisionRotate(-90), tl.ClockwiseRandomRotate90(p=1.0),),\n        (TorchvisionRotate(180), tl.ClockwiseRandomRotate90(p=1.0, k=2),),\n    ]\n\n    dataset = lambda transform: torchvision.datasets.CIFAR10(\n        root=""."", download=True, transform=transform\n    )\n    for torchvision_transform, torchlayers_module in operations:\n        yield torch.utils.data.DataLoader(\n            dataset(torchvision_transform), batch_size=2, shuffle=False\n        ), torch.utils.data.DataLoader(\n            dataset(torchvision.transforms.ToTensor()), batch_size=2, shuffle=False\n        ), torchlayers_module\n\n\n@pytest.mark.parametrize(\n    ""torchvision_dataloader,dataloader,module"", list(generate_inputs())\n)\ndef test_functionality(torchvision_dataloader, dataloader, module):\n    for i, ((torchvision_images, _), (images, _)) in enumerate(\n        zip(torchvision_dataloader, dataloader)\n    ):\n        if i == 10:\n            break\n        transformed_images = module(images)\n        assert torch.allclose(transformed_images, torchvision_images)\n\n\n# @pytest.mark.parametrize(\n#     ""torchvision_dataloader,dataloader,module"",\n#     list(generate_inputs(different_batches=True)),\n# )\n# def test_time(torchvision_dataloader, dataloader, module):\n#     THRESHOLD = 1\n#     LIMIT = 50\n\n#     @torchfunc.Timer()\n#     def time_torchvision(limit):\n#         total = 0\n#         for i, (torchvision_images, _) in enumerate(torchvision_dataloader):\n#             if i == limit:\n#                 break\n#             total += torchvision_images.mean()\n#         return total\n\n#     @torchfunc.Timer()\n#     def time_torchlayers(limit):\n#         total = 0\n#         for i, (images, _) in enumerate(dataloader):\n#             if i == limit:\n#                 break\n#             total += module(images).mean()\n#         return total\n\n#     torchvision_value, torchvision_time = time_torchvision(LIMIT)\n#     torchlayers_value, torchlayers_time = time_torchlayers(LIMIT)\n\n#     assert torch.allclose(torchvision_value, torchlayers_value)\n#     assert THRESHOLD * torchlayers_time < torchvision_time\n'"
tests/recurrent_test.py,4,"b'import torch\n\nimport pytest\nimport torchlayers as tl\n\n\ndef test_gru():\n    model = tl.build(\n        tl.GRU(hidden_size=40, dropout=0.5, batch_first=True, num_layers=3),\n        torch.randn(5, 3, 10),\n    )\n\n    output, _ = model(torch.randn(5, 3, 10))\n    assert output.shape == (5, 3, 40)\n\n\ndef test_lstm():\n    model = tl.build(tl.LSTM(hidden_size=20), torch.randn(5, 3, 10))\n\n    output, (_, _) = model(torch.randn(5, 3, 10))\n    assert output.shape == (5, 3, 20)\n'"
tests/regularization_test.py,7,"b'import itertools\n\nimport torch\n\nimport pytest\nimport torchlayers as tl\n\n\ndef single_module(regularization: str, name: str):\n    return getattr(tl, regularization)(tl.Linear(10), weight_decay=1000000, name=name)\n\n\ndef multiple_modules(regularization: str, name: str):\n    return getattr(tl, regularization)(\n        torch.nn.Sequential(\n            tl.Linear(40), tl.ReLU(), tl.Linear(20), tl.ReLU(), tl.Linear(10)\n        ),\n        weight_decay=1000000,\n        name=name,\n    )\n\n\ndef generate_inputs():\n    modules = (single_module, multiple_modules)\n    names = (""bias"", ""weight"", None)\n    regularizations = (""L1"", ""L2"")\n    steps = (1, 3)\n    for module, regularization, name, steps in itertools.product(\n        modules, regularizations, names, steps\n    ):\n        yield module(regularization, name), name, steps\n\n\ndef check_gradient_correctness(module, name):\n    for param_name, param in module.named_parameters():\n        if name is not None:\n            if name in param_name:\n                assert torch.abs(param.grad).sum() > 10000.0\n            else:\n                assert torch.abs(param.grad).sum() < 10000.0\n        else:\n            assert torch.abs(param.grad).sum() > 10000.0\n\n\n@pytest.mark.parametrize(""module,name,steps"", list(generate_inputs()))\ndef test_weight_decay(module, name, steps: bool):\n    tl.build(module, torch.randn(1, 20))\n    for _ in range(steps):\n        output = module(torch.randn(5, 20))\n        output.sum().backward()\n    check_gradient_correctness(module, name)\n    module.zero_grad()\n    output = module(torch.rand(5, 20))\n    output.sum().backward()\n    check_gradient_correctness(module, name)\n'"
tests/torchlayers_test.py,6,"b'import pathlib\nimport tempfile\n\nimport torch\n\nimport pytest\nimport torchlayers as tl\n\n\n@pytest.fixture\ndef model():\n    return tl.Sequential(\n        tl.Conv(64),\n        tl.BatchNorm(),\n        tl.ReLU(),\n        tl.Conv(128),\n        tl.BatchNorm(),\n        tl.ReLU(),\n        tl.Conv(256),\n        tl.GlobalMaxPool(),\n        tl.Linear(64),\n        tl.BatchNorm(),\n        tl.Linear(10),\n    )\n\n\ndef test_functionality(model):\n    # Initialize\n    model = tl.build(model, torch.randn(16, 3, 28, 28))\n\n    optimizer = torch.optim.Adam(model.parameters())\n    criterion = torch.nn.CrossEntropyLoss()\n\n    for _ in range(16):\n        output = model(torch.randn(16, 3, 28, 28))\n        loss = criterion(output, torch.randint(2, (16,)))\n        loss.backward()\n\n        optimizer.zero_grad()\n\n\ndef test_print_pre_init(model):\n    target = r""""""Sequential(\n  (0): Conv(in_channels=?, out_channels=64, kernel_size=3, stride=1, padding=same, dilation=1, groups=1, bias=True, padding_mode=zeros)\n  (1): BatchNorm(num_features=?, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU()\n  (3): Conv(in_channels=?, out_channels=128, kernel_size=3, stride=1, padding=same, dilation=1, groups=1, bias=True, padding_mode=zeros)\n  (4): BatchNorm(num_features=?, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): ReLU()\n  (6): Conv(in_channels=?, out_channels=256, kernel_size=3, stride=1, padding=same, dilation=1, groups=1, bias=True, padding_mode=zeros)\n  (7): GlobalMaxPool()\n  (8): Linear(in_features=?, out_features=64, bias=True)\n  (9): BatchNorm(num_features=?, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (10): Linear(in_features=?, out_features=10, bias=True)\n)""""""\n\n    assert target == str(model)\n\n\ndef test_attribute_access_existing():\n    layer = tl.Conv(64)\n    assert layer.kernel_size == 3\n    assert layer.padding == ""same""\n\n\ndef test_attribute_access_notinstantiated():\n    layer = tl.Conv(64)\n    with pytest.raises(AttributeError):\n        non_instantiated_channels = layer.in_channels\n\n    layer(torch.randn(1, 8, 28, 28))\n    assert layer.in_channels == 8\n'"
torchlayers/__init__.py,69,"b'import collections\nimport inspect\nimport io\nimport types\nimport typing\nimport warnings\n\nimport torch\n\nfrom . import (_dev_utils, activations, convolution, normalization, pooling,\n               preprocessing, regularization, upsample)\nfrom ._version import __version__\nfrom .module import InferDimension\n\n__all__ = [""summary"", ""build"", ""infer"", ""Lambda"", ""Reshape"", ""Concatenate""]\n\n\ndef summary(module, *args, **kwargs):\n    """"""Create summary of PyTorch module.\n\n    Works similarly to `summary` functionality provided by `keras`.\n\n    Example::\n\n        import torchlayers as tl\n\n\n        class Classifier(tl.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv1 = tl.Conv2d(64, kernel_size=6)\n                self.conv2 = tl.Conv2d(128, kernel_size=3)\n                self.conv3 = tl.Conv2d(256, kernel_size=3, padding=1)\n                self.pooling = tl.GlobalMaxPool()\n                self.dense = tl.Linear(10)\n\n            def forward(self, x):\n                x = torch.relu(self.conv1(x))\n                x = torch.relu(self.conv2(x))\n                x = torch.relu(self.conv3(x))\n                return self.dense(self.pooling(x))\n\n        clf = Classifier()\n        tl.build(clf, torch.randn(1, 3, 32, 32))\n        # You have to print the summary explicitly\n        print(tl.summary(clf, torch.randn(1, 3, 32, 32)))\n\n    Above would print (please notice explicit usage of `print` statement)::\n\n\n                Layer (type)       |      Inputs      |     Outputs      | Params (size in MB)  | Buffers (size in MB)\n        ===============================================================================================================\n        ---------------------------------------------------------------------------------------------------------------\n               conv1 (Conv2d)      |  (1, 3, 32, 32)  | (1, 64, 27, 27)  |  6976 (0.027904 MB)  |   0 (0.000000 MB)\n        ---------------------------------------------------------------------------------------------------------------\n               conv2 (Conv2d)      | (1, 64, 27, 27)  | (1, 128, 25, 25) | 73856 (0.295424 MB)  |   0 (0.000000 MB)\n        ---------------------------------------------------------------------------------------------------------------\n               conv3 (Conv2d)      | (1, 128, 25, 25) | (1, 256, 25, 25) | 295168 (1.180672 MB) |   0 (0.000000 MB)\n        ---------------------------------------------------------------------------------------------------------------\n         pooling (GlobalMaxPool2d) | (1, 256, 25, 25) |     (1, 256)     |   0 (0.000000 MB)    |   0 (0.000000 MB)\n        ---------------------------------------------------------------------------------------------------------------\n               dense (Linear)      |     (1, 256)     |     (1, 10)      |  2570 (0.010280 MB)  |   0 (0.000000 MB)\n        ---------------------------------------------------------------------------------------------------------------\n        ===============================================================================================================\n        Total params: 378570 (1.514280 Mb)\n        Total buffers: 0 (0.000000 Mb)\n\n\n    Custom `Summarizer` object is returned from the function. Users can specify which columns to return\n    (if any) using `string` function.\n    Calling `summary` on the above model like this::\n\n        print(tl.summary(clf, torch.randn(1, 3, 32, 32)).string(buffers=False, inputs=False))\n\n    Would give the following output to `stdout`:\n\n\n                Layer (type)       |     Outputs      | Params (size in MB)\n        =====================================================================\n        ---------------------------------------------------------------------\n               conv1 (Conv2d)      | (1, 64, 27, 27)  |  6976 (0.027904 MB)\n        ---------------------------------------------------------------------\n               conv2 (Conv2d)      | (1, 128, 25, 25) | 73856 (0.295424 MB)\n        ---------------------------------------------------------------------\n               conv3 (Conv2d)      | (1, 256, 25, 25) | 295168 (1.180672 MB)\n        ---------------------------------------------------------------------\n         pooling (GlobalMaxPool2d) |     (1, 256)     |   0 (0.000000 MB)\n        ---------------------------------------------------------------------\n               dense (Linear)      |     (1, 10)      |  2570 (0.010280 MB)\n        ---------------------------------------------------------------------\n        =====================================================================\n        Total params: 378570 (1.514280 Mb)\n        Total buffers: 0 (0.000000 Mb)\n\n    `string` method has following flags (if `True` the column is returned, `True` by default)\n    specifiable by user:\n\n        - layers\n        - inputs\n        - outputs\n        - params\n        - buffers\n\n    Returned `Summarizer` object has following fields which can be accessed and\n    read/modified by user:\n\n        - names - `list` containing names of consecutive modules\n        - modules - `list` containing names of **classes** of consecutive modules\n        - inputs - `list` containing inputs (possibly multiple of them) to layers.\n        Each `input` is described by `shape` (if it\'s a `torch.Tensor` instance)\n        or by class name otherwise\n        - outputs - `list` containing outputs (possibly multiple of them) of layers.\n        Each `output` is described by `shape` (if it\'s a `torch.Tensor` instance)\n        or by class name otherwise\n        - params - `list` containing number of parameters for each layer\n        - params_sizes - `list` containing sizes of parameters for layers (in megabytes)\n        - buffers - `list` containing number of buffer parameters for each layer\n        - buffers_sizes - `list` containing sizes of buffers for layers (in megabytes)\n\n    Finally, `Summarizer` can also be called with other inputs so it gathers\n    representation based on the last provided input. For example::\n\n        summarizer = tl.summary(module, torch.randn(1, 3, 32, 32))\n        with summarizer:\n            summarizer(torch.randn(5, 3, 64, 64))\n\n        # Inputs and outputs summary for input of shape (5, 3, 64, 64)\n        print(summarizer)\n\n    .. note::\n            This function can be called before or after `torchlayers.build`\n            but the results may vary (names and types of layers before and after\n            inference).\n\n    .. note::\n            This function runs input through the network hence the input shapes\n            are inferred after this call. Network will not be built into\n            PyTorch counterparts though (e.g. `tl.Conv` will not become `torch.nn.Conv2d` or a-like).\n\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Instance of module to create summary for\n    *args\n        Arguments required by module\'s `forward`\n    **kwargs\n        Keyword arguments required by module\'s `forward`\n    """"""\n\n    class Summarizer:\n        def __init__(self, module):\n            self.module = module\n\n            self.names = []\n            self.modules = []\n\n            self.inputs = []\n            self.outputs = []\n\n            self.params = []\n            self.params_sizes = []\n\n            self.buffers = []\n            self.buffers_sizes = []\n\n            self._handles = []\n            self._last_module_state = None\n\n        def __enter__(self):\n            self._last_module_state = self.module.training\n            self.module.eval()\n            for name, submodule in list(self.module.named_modules())[1:]:\n                if ""."" not in name:\n                    self.names.append(name)\n                    self.modules.append(type(submodule).__name__)\n                    self._handles.append(submodule.register_forward_hook(self.hook))\n            return self\n\n        def __exit__(self, *_, **__):\n            for handle in self._handles:\n                handle.remove()\n            self.module.train(self._last_module_state)\n            return False\n\n        @staticmethod\n        def _parse(item):\n            if torch.is_tensor(item):\n                return tuple(item.shape)\n\n            if isinstance(item, collections.abc.Iterable):\n                return Summarizer._unwrap(\n                    tuple(Summarizer._parse(element) for element in item)\n                )\n            else:\n                return type(item).__name__\n\n        @staticmethod\n        def _unwrap(item):\n            if len(item) == 1:\n                return item[0]\n            return item\n\n        def hook(self, module, inputs, outputs):\n            def _elements_count(iterator):\n                return sum(element.numel() for element in iterator)\n\n            def _elements_size_in_mb(iterator):\n                return (\n                    sum(\n                        element.numel() * element.element_size() for element in iterator\n                    )\n                    / 1_000_000\n                )\n\n            # Inputs and outputs\n            self.inputs.append(Summarizer._parse(inputs))\n            self.outputs.append(Summarizer._parse(outputs))\n\n            # Parameters and buffers\n            self.params.append(_elements_count(module.parameters()))\n            self.params_sizes.append(_elements_size_in_mb(module.parameters()))\n            self.buffers.append(_elements_count(module.buffers()))\n            self.buffers_sizes.append(_elements_size_in_mb(module.buffers()))\n\n        def __call__(self, *args, **kwargs):\n            with torch.no_grad():\n                return self.module(*args, **kwargs)\n            return self\n\n        def string(\n            self,\n            layers: bool = True,\n            inputs: bool = True,\n            outputs: bool = True,\n            params: bool = True,\n            buffers: bool = True,\n        ):\n            def create_text():\n                def conditional_return(conditions, values):\n                    for condition, value in zip(conditions, values):\n                        if condition:\n                            yield value\n\n                def footer(\n                    total_params, total_params_sizes, total_buffers, total_buffers_sizes\n                ):\n                    return ""Total params: {} ({:.6f} MB)\\nTotal buffers: {} ({:.6f} MB)"".format(\n                        total_params,\n                        total_params_sizes,\n                        total_buffers,\n                        total_buffers_sizes,\n                    )\n\n                layers_text = [""Layer (type)""]\n                inputs_text = [""Inputs""]\n                outputs_text = [""Outputs""]\n                params_text = [""Params (size in MB)""]\n                buffers_text = [""Buffers (size in MB)""]\n\n                total_params, total_params_sizes, total_buffers, total_buffers_sizes = (\n                    0,\n                    0,\n                    0,\n                    0,\n                )\n                for (\n                    name,\n                    module,\n                    inp,\n                    out,\n                    param,\n                    param_size,\n                    buffer,\n                    buffer_size,\n                ) in zip(\n                    self.names,\n                    self.modules,\n                    self.inputs,\n                    self.outputs,\n                    self.params,\n                    self.params_sizes,\n                    self.buffers,\n                    self.buffers_sizes,\n                ):\n\n                    layers_text.append(""{} ({})"".format(name, module))\n                    inputs_text.append(""{}"".format(inp))\n                    outputs_text.append(""{}"".format(out))\n                    params_text.append(""{} ({:.6f} MB)"".format(param, param_size))\n                    buffers_text.append(""{} ({:.6f} MB)"".format(buffer, buffer_size))\n                    total_params += param\n                    total_params_sizes += param_size\n                    total_buffers += buffer\n                    total_buffers_sizes += buffer_size\n\n                return (\n                    footer(\n                        total_params,\n                        total_params_sizes,\n                        total_buffers,\n                        total_buffers_sizes,\n                    ),\n                    conditional_return(\n                        (layers, inputs, outputs, params, buffers),\n                        (\n                            layers_text,\n                            inputs_text,\n                            outputs_text,\n                            params_text,\n                            buffers_text,\n                        ),\n                    ),\n                )\n\n            def center(columns):\n                # + 2 to leave some space for representation on both sides\n                for column in columns:\n                    longest = max(map(len, column)) + 2\n                    yield tuple(map(lambda string: string.center(longest), column))\n\n            def join(columns):\n                return tuple(""|"".join(row) for row in zip(*columns))\n\n            footer, columns = create_text()\n            rows = join(center(columns))\n            representation = rows[0]\n            representation += ""\\n"" + ""="" * len(rows[0]) + ""\\n""\n            representation += ""-"" * len(rows[0]) + ""\\n""\n            for row in rows[1:]:\n                representation += row\n                representation += ""\\n"" + ""-"" * len(row) + ""\\n""\n\n            representation += ""="" * len(rows[0]) + ""\\n""\n            representation += footer\n            return representation\n\n        def __str__(self):\n            return self.string()\n\n    with Summarizer(module) as summarizer:\n        summarizer(*args, **kwargs)\n        return summarizer\n\n\ndef build(module, *args, **kwargs):\n    """"""Build PyTorch layer or module by providing example input.\n\n    This method should be used **always** after creating module using `torchlayers`\n    and shape inference especially.\n\n    Works similarly to `build` functionality provided by `keras`.\n\n    Provided module will be ""compiled"" to PyTorch primitives to remove any\n    overhead.\n\n    `torchlayers` also supports `post_build` function to perform some action after\n    shape was inferred (weight initialization example below)::\n\n\n        import torch\n        import torchlayers as tl\n\n        class _MyModuleImpl(torch.nn.Linear):\n            def post_build(self):\n                # You can do anything here really\n                torch.nn.init.eye_(self.weights)\n\n        MyModule = tl.infer(_MyModuleImpl)\n\n    `post_build` should have no arguments other than `self` so all necessary\n    data should be saved in `module` beforehand.\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Instance of module to build\n    *args\n        Arguments required by module\'s `forward`\n    **kwargs\n        Keyword arguments required by module\'s `forward`\n    """"""\n\n    def torch_compile(module):\n        with io.BytesIO() as buffer:\n            torch.save(module, buffer)\n            return torch.load(io.BytesIO(buffer.getvalue()))\n\n    def run_post(module):\n        for submodule in module.modules():\n            function = getattr(submodule, ""post_build"", None)\n            if function is not None:\n                post_build = getattr(submodule, ""post_build"")\n                if not callable(post_build):\n                    raise ValueError(\n                        ""{}\'s post_build is required to be a method."".format(submodule)\n                    )\n                submodule.post_build()\n\n    with torch.no_grad():\n        module.eval()\n        module(*args, **kwargs)\n    module.train()\n    module = torch_compile(module)\n    run_post(module)\n    return module\n\n\ndef infer(module_class, index: str = 1):\n    """"""Allows custom user modules to infer input shape.\n\n    Input shape should be the first argument after `self`.\n\n    Usually used as class decorator, e.g.::\n\n        import torch\n        import torchlayers as tl\n\n        class _StrangeLinearImpl(torch.nn.Linear):\n            def __init__(self, in_features, out_features, bias: bool = True):\n                super().__init__(in_features, out_features, bias)\n                self.params = torch.nn.Parameter(torch.randn(out_features))\n\n            def forward(self, inputs):\n                super().forward(inputs) + self.params\n\n        # Now you can use shape inference of in_features\n        StrangeLinear = tl.infer(_StrangeLinearImpl)\n\n        # in_features can be inferred\n        layer = StrangeLinear(out_features=64)\n\n\n    Parameters\n    ----------\n    module_class: torch.nn.Module\n        Class of module to be updated with shape inference capabilities.\n\n    index: int, optional\n        Index into `tensor.shape` input which should be inferred, e.g. tensor.shape[1].\n        Default: `1` (`0` being batch dimension)\n\n    """"""\n\n    init_arguments = [\n        str(argument)\n        for argument in inspect.signature(module_class.__init__).parameters.values()\n    ]\n\n    # Other argument than self\n    if len(init_arguments) > 1:\n        name = module_class.__name__\n        infered_module = type(\n            name, (torch.nn.Module,), {_dev_utils.infer.MODULE_CLASS: module_class},\n        )\n        parsed_arguments, uninferable_arguments = _dev_utils.infer.parse_arguments(\n            init_arguments, infered_module\n        )\n\n        setattr(\n            infered_module, ""__init__"", _dev_utils.infer.create_init(parsed_arguments),\n        )\n\n        setattr(\n            infered_module,\n            ""forward"",\n            _dev_utils.infer.create_forward(\n                _dev_utils.infer.MODULE,\n                _dev_utils.infer.MODULE_CLASS,\n                parsed_arguments,\n                index,\n            ),\n        )\n        setattr(\n            infered_module,\n            ""__repr__"",\n            _dev_utils.infer.create_repr(\n                _dev_utils.infer.MODULE, **uninferable_arguments\n            ),\n        )\n        setattr(\n            infered_module,\n            ""__getattr__"",\n            _dev_utils.infer.create_getattr(_dev_utils.infer.MODULE),\n        )\n\n        setattr(\n            infered_module,\n            ""__reduce__"",\n            _dev_utils.infer.create_reduce(_dev_utils.infer.MODULE, parsed_arguments),\n        )\n\n        return infered_module\n\n    return module_class\n\n\nclass Lambda(torch.nn.Module):\n    """"""Use any function as `torch.nn.Module`\n\n    Simple proxy which allows you to use your own custom in\n    `torch.nn.Sequential` and other requiring `torch.nn.Module` as input::\n\n        import torch\n        import torchlayers as tl\n\n        model = torch.nn.Sequential(tl.Lambda(lambda tensor: tensor ** 2))\n        model(torch.randn(64 , 20))\n\n    Parameters\n    ----------\n    function : Callable\n        Any user specified function\n\n    Returns\n    -------\n    Any\n        Anything `function` returns\n\n    """"""\n\n    def __init__(self, function: typing.Callable):\n        super().__init__()\n        self.function: typing.Callable = function\n\n    def forward(self, *args, **kwargs) -> typing.Any:\n        return self.function(*args, **kwargs)\n\n\nclass Concatenate(torch.nn.Module):\n    """"""Concatenate list of tensors.\n\n    Mainly useful in `torch.nn.Sequential` when previous layer returns multiple\n    tensors, e.g.::\n\n        import torch\n        import torchlayers as tl\n\n        class Foo(torch.nn.Module):\n            # Return same tensor three times\n            # You could explicitly return a list or tuple as well\n            def forward(tensor):\n                return tensor, tensor, tensor\n\n\n        model = torch.nn.Sequential(Foo(), tl.Concatenate())\n        model(torch.randn(64 , 20))\n\n    All tensors must have the same shape (except in the concatenating dimension).\n\n    Parameters\n    ----------\n    dim : int\n        Dimension along which tensors will be concatenated\n\n    Returns\n    -------\n    torch.Tensor\n        Concatenated tensor along specified `dim`.\n\n    """"""\n\n    def __init__(self, dim: int):\n        super().__init__()\n        self.dim: int = dim\n\n    def forward(self, inputs):\n        return torch.cat(inputs, dim=self.dim)\n\n\nclass Reshape(torch.nn.Module):\n    """"""Reshape tensor excluding `batch` dimension\n\n    Reshapes input `torch.Tensor` features while preserving batch dimension.\n    Standard `torch.reshape` values (e.g. `-1`) are supported, e.g.::\n\n        import torch\n        import torchlayers as tl\n\n        layer = tl.Reshape(20, -1)\n        layer(torch.randn(64, 80)) # shape (64, 20, 4)\n\n    All tensors must have the same shape (except in the concatenating dimension).\n    If possible, no copy of `tensor` will be performed.\n\n    Parameters\n    ----------\n    shapes: *int\n        Variable length list of shapes used in view function\n\n    Returns\n    -------\n    torch.Tensor\n        Concatenated tensor\n\n    """"""\n\n    def __init__(self, *shapes: int):\n        super().__init__()\n        self.shapes: typing.Tuple[int] = shapes\n\n    def forward(self, tensor):\n        return torch.reshape(tensor, (tensor.shape[0], *self.shapes))\n\n\n###############################################################################\n#\n#                       MODULE AND SHAPE INFERENCE\n#\n###############################################################################\n\n\nmodules_map = {\n    # PyTorch specific tensor index\n    ""Linear"": (torch.nn, -1),\n    ""RNN"": (torch.nn, 2),\n    ""LSTM"": (torch.nn, 2),\n    ""GRU"": (torch.nn, 2),\n    ""MultiheadAttention"": (torch.nn, 2),\n    ""Transformer"": (torch.nn, 2),\n    ""TransformerEncoderLayer"": (torch.nn, 2),\n    ""TransformerDecoderLayer"": (torch.nn, 2),\n    # PyTorch default (1) tensor index\n    ""RNNCell"": torch.nn,\n    ""LSTMCell"": torch.nn,\n    ""GRUCell"": torch.nn,\n    ""Conv1d"": torch.nn,\n    ""Conv2d"": torch.nn,\n    ""Conv3d"": torch.nn,\n    ""ConvTranspose1d"": torch.nn,\n    ""ConvTranspose2d"": torch.nn,\n    ""ConvTranspose3d"": torch.nn,\n    ""BatchNorm1d"": torch.nn,\n    ""BatchNorm2d"": torch.nn,\n    ""BatchNorm3d"": torch.nn,\n    ""SyncBatchNorm"": torch.nn,\n    ""InstanceNorm1d"": torch.nn,\n    ""InstanceNorm2d"": torch.nn,\n    ""InstanceNorm3d"": torch.nn,\n    # Torchlayers convolution\n    ""SqueezeExcitation"": convolution,\n    ""Fire"": convolution,\n    ""Conv"": convolution,\n    ""ConvTranspose"": convolution,\n    ""DepthwiseConv"": convolution,\n    ""SeparableConv"": convolution,\n    ""InvertedResidualBottleneck"": convolution,\n    # Torchlayers normalization\n    ""BatchNorm"": normalization,\n    ""InstanceNorm"": normalization,\n    ""GroupNorm"": normalization,\n    # Torchlayers upsample\n    ""ConvPixelShuffle"": upsample,\n}\n\n\ndef __dir__():\n    return [key for key in modules_map if not key.startswith(""*"")] + dir(torch.nn)\n\n\ndef __getattr__(name: str):\n    def infer_module(module, tensor_index):\n        klass = getattr(module, name, None)\n        if klass is not None:\n            return infer(klass, tensor_index)\n        return None\n\n    def process_entry(data) -> typing.Optional:\n        if isinstance(data, typing.Iterable):\n            if len(data) != 2:\n                raise AttributeError(\n                    f""Value of to-be-inferred module: {name} second argument has ""\n                    f""to be of length 2 but got {len(data)}. Check torchlayers documentation.""\n                )\n            return infer_module(*data)\n\n        if isinstance(data, types.ModuleType):\n            return infer_module(data, 1)\n\n        raise AttributeError(\n            ""Torchlayers recognized entry to infer but it\'s entry is of incorrect type. ""\n            ""Check documentation about registration of user modules for more info.""\n        )\n\n    def registered_module():\n        for key, data in modules_map.items():\n            if key.startswith(""*""):\n                klass = process_entry(data)\n                if klass is not None:\n                    return klass\n\n        return None\n\n    def noninferable_torchlayers():\n        for module in (\n            activations,\n            convolution,\n            normalization,\n            pooling,\n            regularization,\n            preprocessing,\n            upsample,\n        ):\n            klass = getattr(module, name, None)\n            if klass is not None:\n                return klass\n\n        return None\n\n    data = modules_map.get(name)\n    if data is None:\n        klass = registered_module()\n    else:\n        klass = process_entry(data)\n\n    # Try to return from torchlayers without inference\n    if klass is None:\n        klass = noninferable_torchlayers()\n\n    # Try to return from torch.nn without inference\n    if klass is None:\n        klass = getattr(torch.nn, name, None)\n\n    # As far as we know there is no such module\n    if klass is None:\n        raise AttributeError(f""module {__name__} has no attribute {name}"")\n    return klass\n'"
torchlayers/_name.py,0,"b'_name = ""torchlayers""\n'"
torchlayers/_version.py,0,"b'__version__ = ""0.1.1""\n'"
torchlayers/activations.py,21,"b'import torch\n\n\ndef hard_sigmoid(tensor: torch.Tensor, inplace: bool = False) -> torch.Tensor:\n    """"""\n    Applies HardSigmoid function element-wise.\n\n    See :class:`torchlayers.activations.HardSigmoid` for more details.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        Tensor activated element-wise\n    inplace : bool, optional\n        Whether operation should be performed `in-place`. Default: `False`\n\n    Returns\n    -------\n    torch.Tensor\n    """"""\n    return torch.nn.functional.hardtanh(tensor, min_val=0, inplace=inplace)\n\n\nclass HardSigmoid(torch.nn.Module):\n    """"""\n    Applies HardSigmoid function element-wise.\n\n    Uses `torch.nn.functional.hardtanh` internally with `0` and `1` ranges.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        Tensor activated element-wise\n\n    """"""\n\n    def forward(self, tensor: torch.Tensor):\n        return hard_sigmoid(tensor)\n\n\ndef swish(tensor: torch.Tensor, beta: float = 1.0) -> torch.Tensor:\n    """"""\n    Applies Swish function element-wise.\n\n    See :class:`torchlayers.activations.Swish` for more details.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        Tensor activated element-wise\n    beta : float, optional\n        Multiplier used for sigmoid. Default: 1.0 (no multiplier)\n\n    Returns\n    -------\n    torch.Tensor\n    """"""\n    return torch.sigmoid(beta * tensor) * tensor\n\n\nclass Swish(torch.nn.Module):\n    r""""""\n    Applies Swish function element-wise.\n\n    .. math::\n\n        Swish(x) = x / (1 + \\exp(-beta * x))\n\n    This form was originally proposed by Prajit Ramachandran et al. in\n    `Searching for Activation Functions <https://arxiv.org/pdf/1710.05941.pdf>`__\n\n    Parameters\n    ----------\n    beta : float, optional\n        Multiplier used for sigmoid. Default: 1.0 (no multiplier)\n\n    """"""\n\n    def __init__(self, beta: float = 1.0):\n        super().__init__()\n        self.beta = beta\n\n    def forward(self, tensor: torch.Tensor):\n        return swish(tensor, self.beta)\n\n\ndef hard_swish(tensor: torch.Tensor) -> torch.Tensor:\n    """"""\n    Applies HardSwish function element-wise.\n\n    See :class:`torchlayers.activations.HardSwish` for more details.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        Tensor activated element-wise\n\n    Returns\n    -------\n    torch.Tensor\n    """"""\n    return tensor * torch.nn.functional.relu6(tensor + 3) / 6\n\n\nclass HardSwish(torch.nn.Module):\n    r""""""\n    Applies HardSwish function element-wise.\n\n    .. math::\n\n        HardSwish(x) = x * \\min(\\max(0,x + 3), 6) / 6\n\n\n    While similar in effect to `Swish` should be more CPU-efficient.\n    Above formula proposed by in Andrew Howard et al. in `Searching for MobileNetV3 <https://arxiv.org/pdf/1905.02244.pdf>`__.\n\n    Parameters\n    ----------\n    tensor : torch.Tensor\n        Tensor activated element-wise\n\n    """"""\n\n    def forward(self, tensor: torch.Tensor):\n        return hard_swish(tensor)\n'"
torchlayers/convolution.py,74,"b'import collections\nimport itertools\nimport math\nimport typing\n\nimport torch\n\nfrom . import _dev_utils, module, normalization, pooling\n\n# TO-DO: Ensure torch.jit compatibility of layers below\n\n\nclass _Conv(module.InferDimension):\n    """"""Base layer for convolution-like modules.\n\n    Passes all arguments to `_dev_utils.modules.module.InferDimension` and sets up\n    `same` padding.\n\n    See concrete classes (e.g. `Conv` or `DepthwiseConv`) for specific use cases.\n\n    Parameters\n    ----------\n    dispatcher: typing.Dict[int, typing.Any]\n        Dispatching dictionary for dimensionality reduction\n    **kwargs\n        Any arguments needed for class creation.\n\n    """"""\n\n    def __init__(\n        self, dispatcher: typing.Dict[int, typing.Any], **kwargs,\n    ):\n        super().__init__(dispatcher=dispatcher, initializer=self._pad, **kwargs)\n\n    def _pad(self, inner_class, inputs, **kwargs):\n        def _dimension_pad(dimension, kernel_size, stride, dilation):\n            if kernel_size % 2 == 0:\n                raise ValueError(\n                    \'Only asymmetric ""same"" padding is currently supported. `kernel_size` size has to be odd, but got `kernel_size={}`\'.format(\n                        kernel_size\n                    )\n                )\n            if stride % 2 == 0:\n                raise ValueError(\n                    \'Only asymmetric ""same"" padding is currently supported. `stride` size has to be odd, but got `stride={}`\'.format(\n                        kernel_size\n                    )\n                )\n\n            return math.ceil(\n                (dimension * stride - dimension + dilation * (kernel_size - 1)) / 2\n            )\n\n        def _expand_if_needed(dimensions, argument):\n            if isinstance(argument, collections.abc.Iterable):\n                return argument\n            return tuple(itertools.repeat(argument, len(dimensions)))\n\n        if isinstance(kwargs[""padding""], str) and kwargs[""padding""].lower() == ""same"":\n            dimensions = inputs.shape[2:]\n            paddings = tuple(\n                _dimension_pad(dimension, kernel_size, stride, dilation)\n                for dimension, kernel_size, stride, dilation in zip(\n                    dimensions,\n                    *[\n                        _expand_if_needed(dimensions, kwargs[name])\n                        for name in (""kernel_size"", ""stride"", ""dilation"")\n                    ],\n                )\n            )\n            kwargs[""padding""] = paddings\n\n        return inner_class(**kwargs)\n\n\nclass Conv(_Conv):\n    """"""Standard convolution layer.\n\n    Based on input shape it either creates 1D, 2D or 3D convolution for inputs of shape\n    3D, 4D, 5D respectively (including batch as first dimension).\n\n    Additional `same` `padding` mode was added and set as default.\n    This mode preserves all dimensions excepts channels.\n\n\n    `kernel_size` got a default value of `3`.\n\n    Otherwise acts exactly like PyTorch\'s Convolution, see\n    `documentation <https://pytorch.org/docs/stable/nn.html#convolution-layers>`__.\n\n    Example::\n\n        import torchlayers as tl\n\n\n        class Classifier(tl.Module):\n            def __init__(self, out_shape):\n                super().__init__()\n                self.conv1 = tl.Conv(64, 6)\n                self.conv2 = tl.Conv(128)\n                self.conv3 = tl.Conv(256)\n                self.pooling = tl.GlobalMaxPool()\n                self.dense = tl.Linear(out_shape)\n\n            def forward(self, x):\n                x = torch.relu(self.conv1(x))\n                x = torch.relu(self.conv2(x))\n                x = torch.relu(self.conv3(x))\n                return self.dense(self.pooling(x))\n\n\n    .. note::\n                **IMPORTANT**: `same` currently works only for odd values of `kernel_size`,\n                `dilation` and `stride`. If any of those is even you should explicitly pad\n                your input asymmetrically with `torch.functional.pad` or a-like.\n\n    Parameters\n    ----------\n    in_channels : int\n        Number of channels in the input image\n    out_channels : int\n        Number of channels produced by the convolution\n    kernel_size : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Size of the convolving kernel. User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `3`\n    stride : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Stride of the convolution. User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `3`\n    padding : Union[str, int, Tuple[int, int], Tuple[int, int, int]], optional\n        Padding added to both sides of the input. String ""same"" can be used with odd\n        `kernel_size`, `stride` and `dilation`\n        User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `same`\n    dilation : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Spacing between kernel elements. String ""same"" can be used with odd\n        `kernel_size`, `stride` and `dilation`\n        User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `1`\n    groups : int, optional\n        Number of blocked connections from input channels to output channels. Default: 1\n    bias : bool, optional\n        If ``True``, adds a learnable bias to the output. Default: ``True``\n    padding_mode : string, optional\n        Accepted values `zeros` and `circular` Default: `zeros`\n\n    """"""\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size=3,\n        stride=1,\n        padding=""same"",\n        dilation=1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = ""zeros"",\n    ):\n        super().__init__(\n            dispatcher={5: torch.nn.Conv3d, 4: torch.nn.Conv2d, 3: torch.nn.Conv1d},\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n            padding_mode=padding_mode,\n        )\n\n\n# Fix ConvTranspose and add same padding?\nclass ConvTranspose(_Conv):\n    """"""Standard transposed convolution layer.\n\n    Based on input shape it either creates 1D, 2D or 3D convolution (for inputs of shape\n    3D, 4D, 5D including batch as first dimension).\n\n    Otherwise acts exactly like PyTorch\'s Convolution, see\n    `documentation <https://pytorch.org/docs/stable/nn.html#convolution-layers>`__.\n\n    Default argument for `kernel_size` was added equal to `3`.\n\n    .. note::\n                **IMPORTANT**: `same` currently works only for odd values of `kernel_size`,\n                `dilation` and `stride`. If any of those is even you should explicitly pad\n                your input asymmetrically with `torch.functional.pad` or a-like.\n\n    Parameters\n    ----------\n    in_channels : int\n        Number of channels in the input image\n    out_channels : int\n        Number of channels produced by the convolution\n    kernel_size : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Size of the convolving kernel. User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `3`\n    stride : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Stride of the convolution. User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `3`\n    padding : Union[str, int, Tuple[int, int], Tuple[int, int, int]], optional\n        Padding added to both sides of the input. String ""same"" can be used with odd\n        `kernel_size`, `stride` and `dilation`\n        User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `same`\n    output_padding : int or tuple, optional\n        Additional size added to one side of the output shape. Default: 0\n    dilation : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Spacing between kernel elements. String ""same"" can be used with odd\n        `kernel_size`, `stride` and `dilation`\n        User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `1`\n    groups : int, optional\n        Number of blocked connections from input channels to output channels. Default: 1\n    bias : bool, optional\n        If ``True``, adds a learnable bias to the output. Default: ``True``\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n    padding_mode : string, optional\n        Accepted values `zeros` and `circular` Default: `zeros`\n\n    """"""\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size=3,\n        stride=1,\n        padding=""same"",\n        output_padding=0,\n        dilation=1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode=""zeros"",\n    ):\n        super().__init__(\n            dispatcher={\n                5: torch.nn.ConvTranspose3d,\n                4: torch.nn.ConvTranspose2d,\n                3: torch.nn.ConvTranspose1d,\n            },\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            output_padding=output_padding,\n            groups=groups,\n            bias=bias,\n            dilation=dilation,\n            padding_mode=padding_mode,\n        )\n\n\nclass DepthwiseConv(_Conv):\n    """"""Depthwise convolution layer.\n\n    Based on input shape it either creates 1D, 2D or 3D depthwise convolution\n    for inputs of shape 3D, 4D, 5D respectively (including batch as first dimension).\n\n    Additional `same` `padding` mode was added and set as default.\n    This mode preserves all dimensions excepts channels.\n\n    `kernel_size` got a default value of `3`.\n\n    .. note::\n                **IMPORTANT**: `same` currently works only for odd values of `kernel_size`,\n                `dilation` and `stride`. If any of those is even you should explicitly pad\n                your input asymmetrically with `torch.functional.pad` or a-like.\n\n    .. note::\n                **IMPORTANT**: `out_channels` has to be divisible by `in_channels` without remainder\n                (e.g. `out_channels=64` and `in_channels=32`), otherwise error is thrown.\n\n\n    Parameters\n    ----------\n    in_channels : int\n        Number of channels in the input image\n    out_channels : int\n        Number of channels produced by the convolution\n    kernel_size : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Size of the convolving kernel. User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `3`\n    stride : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Stride of the convolution. User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `3`\n    padding : Union[str, int, Tuple[int, int], Tuple[int, int, int]], optional\n        Padding added to both sides of the input. String ""same"" can be used with odd\n        `kernel_size`, `stride` and `dilation`\n        User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `same`\n    dilation : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Spacing between kernel elements. String ""same"" can be used with odd\n        `kernel_size`, `stride` and `dilation`\n        User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `1`\n    bias : bool, optional\n        If ``True``, adds a learnable bias to the output. Default: ``True``\n    padding_mode : string, optional\n        Accepted values `zeros` and `circular` Default: `zeros`\n\n    """"""\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size=3,\n        stride=1,\n        padding=""same"",\n        dilation=1,\n        bias: bool = True,\n        padding_mode: str = ""zeros"",\n    ):\n        if out_channels % in_channels != 0:\n            raise ValueError(\n                ""Depthwise separable convolution needs out_channels divisible by in_channels without remainder.""\n            )\n\n        super().__init__(\n            dispatcher={5: torch.nn.Conv3d, 4: torch.nn.Conv2d, 3: torch.nn.Conv1d},\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=in_channels,\n            bias=bias,\n            padding_mode=padding_mode,\n        )\n\n\nclass SeparableConv(torch.nn.Module):\n    """"""Separable convolution layer (a.k.a. depthwise separable convolution).\n\n    Based on input shape it either creates 1D, 2D or 3D separable convolution\n    for inputs of shape 3D, 4D, 5D respectively (including batch as first dimension).\n\n    Additional `same` `padding` mode was added and set as default.\n    This mode preserves all dimensions excepts channels.\n\n    `kernel_size` got a default value of `3`.\n\n    .. note::\n                **IMPORTANT**: `same` currently works only for odd values of `kernel_size`,\n                `dilation` and `stride`. If any of those is even you should explicitly pad\n                your input asymmetrically with `torch.functional.pad` or a-like.\n\n    Parameters\n    ----------\n    in_channels : int\n        Number of channels in the input image\n    out_channels : int\n        Number of channels produced by the convolution\n    kernel_size : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Size of the convolving kernel. User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `3`\n    stride : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Stride of the convolution. User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `3`\n    padding : Union[str, int, Tuple[int, int], Tuple[int, int, int]], optional\n        Padding added to both sides of the input. String ""same"" can be used with odd\n        `kernel_size`, `stride` and `dilation`\n        User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `same`\n    dilation : Union[int, Tuple[int, int], Tuple[int, int, int]], optional\n        Spacing between kernel elements. String ""same"" can be used with odd\n        `kernel_size`, `stride` and `dilation`\n        User can specify `int` or 2-tuple (for `Conv2d`)\n        or 3-tuple (for `Conv3d`). Default: `1`\n    bias : bool, optional\n        If ``True``, adds a learnable bias to the output. Default: ``True``\n    padding_mode : string, optional\n        Accepted values `zeros` and `circular` Default: `zeros`\n\n    """"""\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size=3,\n        stride=1,\n        padding=""same"",\n        dilation=1,\n        bias: bool = True,\n        padding_mode: str = ""zeros"",\n    ):\n        super().__init__()\n\n        self.in_channels: int = in_channels\n        self.out_channels: int = out_channels\n        self.kernel_size: typing.Union[\n            int, typing.Tuple[int, int], typing.Tuple[int, int, int]\n        ] = kernel_size\n        self.stride: typing.Union[\n            int, typing.Tuple[int, int], typing.Tuple[int, int, int]\n        ] = stride\n        self.padding: typing.Union[\n            str, int, typing.Tuple[int, int], typing.Tuple[int, int, int]\n        ] = padding\n        self.dilation: typing.Union[\n            int, typing.Tuple[int, int], typing.Tuple[int, int, int]\n        ] = dilation\n        self.bias: bool = bias\n        self.padding_mode: str = padding_mode\n\n        self.depthwise = Conv(\n            in_channels=in_channels,\n            out_channels=in_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=in_channels,\n            bias=bias,\n            padding_mode=padding_mode,\n        )\n\n        self.pointwise = Conv(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            dilation=1,\n            groups=1,\n            bias=False,\n            padding_mode=padding_mode,\n        )\n\n    def forward(self, inputs):\n        return self.pointwise(self.depthwise(inputs))\n\n\nclass ChannelShuffle(torch.nn.Module):\n    """"""Shuffle output channels.\n\n    When using group convolution knowledge transfer between next layers is reduced\n    (as the same input channels are convolved with the same output channels).\n\n    This layer reshuffles output channels via simple `reshape` in order to mix the representation\n    from separate groups and improve knowledge transfer.\n\n    Originally proposed by Xiangyu Zhang et al. in:\n    `ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices <https://arxiv.org/abs/1707.01083>`__\n\n    Example::\n\n\n        import torchlayers as tl\n\n        model = tl.Sequential(\n            tl.Conv(64),\n            tl.Swish(),\n            tl.Conv(128, groups=16),\n            tl.ChannelShuffle(groups=16),\n            tl.Conv(256),\n            tl.GlobalMaxPool(),\n            tl.Linear(10),\n        )\n\n    Parameters\n    ----------\n    groups : int\n        Number of groups used in the previous convolutional layer.\n\n    """"""\n\n    def __init__(self, groups: int):\n        super().__init__()\n        self.groups: int = groups\n\n    def forward(self, inputs):\n        return (\n            inputs.reshape(inputs.shape[0], self.groups, -1, *inputs.shape[2:])\n            .transpose(1, 2)\n            .reshape(*inputs.shape)\n        )\n\n\nclass ChannelSplit(torch.nn.Module):\n    """"""Convenience layer splitting tensor using `p`.\n\n    Returns two outputs, splitted accordingly to parameters.\n\n    Example::\n\n        import torchlayers as tl\n\n\n        class Net(tl.Module):\n            def __init__(self):\n                super().__init__()\n                self.layer = tl.Conv(256, groups=16)\n                self.splitter = tl.ChannelSplit(0.5)\n\n            def forward(x):\n                outputs = self.layer(x)\n                half, rest = self.splitter(outputs)\n                return half # for some reason\n\n\n    Parameters\n    ----------\n    p : float\n        Percentage of channels to go into first group\n    dim : int, optional\n        Dimension along which input will be splitted. Default: `1` (channel dimension)\n\n    """"""\n\n    def __init__(self, p: float, dim: int = 1):\n        super().__init__()\n        if not 0.0 < p < 1.0:\n            raise ValueError(\n                ""Ratio of small expand fire module has to be between 0 and 1.""\n            )\n\n        self.p: float = p\n        self.dim: int = dim\n\n    def forward(self, inputs):\n        return torch.split(inputs, int(inputs.shape[1] * self.p), dim=self.dim)\n\n\nclass Residual(torch.nn.Module):\n    """"""Residual connection adding input to output of provided module.\n\n    Originally proposed by He et al. in `ResNet <www.arxiv.org/abs/1512.03385>`__\n\n    For correct usage it is advised to keep input line (skip connection) without\n    any layer or activation and implement transformations only in module arguments\n    (as per `Identity Mappings in Deep Residual Networks <https://arxiv.org/pdf/1603.05027.pdf>`__).\n\n    Example::\n\n\n        import torch\n        import torchlayers as tl\n\n        # ResNet-like block\n        class _BlockImpl(tl.Module):\n            def __init__(self, in_channels: int):\n                self.block = tl.Residual(\n                    tl.Sequential(\n                        tl.Conv(in_channels),\n                        tl.ReLU(),\n                        tl.Conv(4 * in_channels),\n                        tl.ReLU(),\n                        tl.Conv(in_channels),\n                    )\n                )\n\n            def forward(self, x):\n                return self.block(x)\n\n\n        Block = tl.infer(_BlockImpl)\n\n\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Convolutional PyTorch module (or other compatible module).\n        Shape of module\'s `inputs` has to be equal to it\'s `outputs`, both\n        should be addable `torch.Tensor` instances.\n    projection : torch.nn.Module, optional\n        If shapes of `inputs` and `module` results are different, it\'s user\n        responsibility to add custom `projection` module (usually `1x1` convolution).\n        Default: `None`\n\n    """"""\n\n    def __init__(self, module: torch.nn.Module, projection: torch.nn.Module = None):\n        super().__init__()\n        self.module: torch.nn.Module = module\n        self.projection: torch.nn.Module = projection\n\n    def forward(self, inputs):\n        output = self.module(inputs)\n        if self.projection is not None:\n            inputs = self.projections(inputs)\n        return output + inputs\n\n\nclass Dense(torch.nn.Module):\n    """"""Dense residual connection concatenating input channels and output channels of provided module.\n\n    Originally proposed by Gao Huang et al. in `Densely Connected Convolutional Networks <https://arxiv.org/abs/1608.06993>`__\n\n    Can be used just like `torchlayers.convolution.Residual` but concatenates\n    channels (dimension can be specified) instead of adding.\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Convolutional PyTorch module (or other compatible module).\n        Shape of module\'s `inputs` has to be equal to it\'s `outputs`, both\n        should be addable `torch.Tensor` instances.\n    dim : int, optional\n        Dimension along which `input` and module\'s `output` will be concatenated.\n        Default: `1` (channel-wise)\n\n    """"""\n\n    def __init__(self, module: torch.nn.Module, dim: int = 1):\n        super().__init__()\n        self.module: torch.nn.Module = module\n        self.dim: int = dim\n\n    def forward(self, inputs):\n        return torch.cat(self.module(inputs), inputs, dim=self.dim)\n\n\nclass Poly(torch.nn.Module):\n    """"""Apply one module to input multiple times and sum.\n\n    It\'s equation for `order` equal to :math:`N` could be expressed as\n\n    .. math::\n\n        I + F + F^2 + ... + F^N\n\n    where :math:`I` is identity mapping and :math:`F` is output of `module` applied :math:`^N` times.\n\n    Originally proposed by Xingcheng Zhang et al. in\n    `PolyNet: A Pursuit of Structural Diversity in Very Deep Networks <https://arxiv.org/abs/1608.06993>`__\n\n    Example::\n\n        import torchlayers as tl\n\n        # Any input will be passed 3 times\n        # Through the same convolutional layer (weights and biases shared)\n        layer = tl.Sequential(tl.Conv(64), tl.Poly(tl.Conv(64), order=3))\n        layer(torch.randn(1, 3, 32, 32))\n\n    Above can be rewritten by the following::\n\n        x = torch.randn(1, 3, 32, 32)\n\n        first_convolution = tl.Conv(64)\n        output = first_convolution(x)\n\n        shared_convolution = tl.Conv(64)\n        first_level = shared_convolution(output)\n        second_level = shared_convolution(first_level)\n        third_level = shared_convolution(second_level)\n\n        # That\'s what tl.Poly would return\n        final = output + first_level + second_level + third_level\n\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Convolutional PyTorch module (or other compatible module).\n        `inputs` shape has to be equal to it\'s `output` shape\n        (for 2D convolution it would be :math:`(C, H, W)`)\n    order : int, optional\n        Order of PolyInception module. For order equal to `1` acts just like\n        ResNet, order of `2` was used in original paper. Default: `2`\n    """"""\n\n    def __init__(self, module: torch.nn.Module, order: int = 2):\n        super().__init__()\n        if order < 1:\n            raise ValueError(""Order of Poly cannot be less than 1."")\n\n        self.module: torch.nn.Module = module\n        self.order: int = order\n\n    def extra_repr(self):\n        return f""order={self.order},""\n\n    def forward(self, inputs):\n        outputs = [self.module(inputs)]\n        for _ in range(1, self.order):\n            outputs.append(self.module(outputs[-1]))\n        return torch.stack([inputs] + outputs, dim=0).sum(dim=0)\n\n\nclass MPoly(torch.nn.Module):\n    """"""Apply multiple modules to input multiple times and sum.\n\n    It\'s equation for `poly_modules` length equal to :math:`N` could be expressed by\n\n    .. math::\n\n        I + F_1 + F_1(F_0) + ... + F_N(F_{N-1}...F_0)\n\n    where :math:`I` is identity and consecutive :math:`F_N` are consecutive modules\n    applied to output of previous ones.\n\n    Originally proposed by Xingcheng Zhang et al. in\n    `PolyNet: A Pursuit of Structural Diversity in Very Deep Networks <https://arxiv.org/abs/1608.06993>`__\n\n    Parameters\n    ----------\n    *poly_modules : torch.nn.Module\n        Variable arg of modules to use. If empty, acts as an identity.\n        For single module acts like `ResNet`. `2` was used in original paper.\n        All modules need `inputs` and `outputs` of equal `shape`.\n\n    """"""\n\n    def __init__(self, *poly_modules: torch.nn.Module):\n        super().__init__()\n        self.poly_modules: torch.nn.Module = torch.nn.ModuleList(poly_modules)\n\n    def forward(self, inputs):\n        outputs = [self.poly_modules[0](inputs)]\n        for module in self.poly_modules[1:]:\n            outputs.append(module(outputs[-1]))\n        return torch.stack([inputs] + outputs, dim=0).sum(dim=0)\n\n\nclass WayPoly(torch.nn.Module):\n    """"""Apply multiple modules to input and sum.\n\n    It\'s equation for `poly_modules` length equal to :math:`N` could be expressed by\n\n    .. math::\n\n        I + F_1(I) + F_2(I) + ... + F_N\n\n    where :math:`I` is identity and consecutive :math:`F_N` are consecutive `poly_modules`\n    applied to input.\n\n    Could be considered as an extension of standard `ResNet` to many parallel modules.\n\n    Originally proposed by Xingcheng Zhang et al. in\n    `PolyNet: A Pursuit of Structural Diversity in Very Deep Networks <https://arxiv.org/abs/1608.06993>`__\n\n    Parameters\n    ----------\n    *poly_modules : torch.nn.Module\n        Variable arg of modules to use. If empty, acts as an identity.\n        For single module acts like `ResNet`. `2` was used in original paper.\n        All modules need `inputs` and `outputs` of equal `shape`.\n    """"""\n\n    def __init__(self, *poly_modules: torch.nn.Module):\n        super().__init__()\n        self.poly_modules: torch.nn.Module = torch.nn.ModuleList(poly_modules)\n\n    def forward(self, inputs):\n        outputs = []\n        for module in self.poly_modules:\n            outputs.append(module(inputs))\n        return torch.stack([inputs] + outputs, dim=0).sum(dim=0)\n\n\nclass SqueezeExcitation(torch.nn.Module):\n    """"""Learn channel-wise excitation maps for `inputs`.\n\n    Provided `inputs` will be squeezed into `in_channels` via average pooling,\n    passed through two non-linear layers, rescaled to :math:`[0, 1]` via `sigmoid`-like function\n    and multiplied with original input channel-wise.\n\n    Originally proposed by Xingcheng Zhang et al. in\n    `Squeeze-and-Excitation Networks <https://arxiv.org/abs/1709.01507>`__\n\n    Example::\n\n\n        import torchlayers as tl\n\n        # Assume only 128 channels can be an input in this case\n        block = tl.Residual(tl.Conv(128), tl.SqueezeExcitation(), tl.Conv(128))\n\n\n    Parameters\n    ----------\n    in_channels : int\n        Number of channels in the input\n    hidden : int, optional\n        Size of the hidden `torch.nn.Linear` layer. Usually smaller than `in_channels`\n        (at least in original research paper). Default: `1/16` of `in_channels` as\n        suggested by original paper.\n    activation : Callable[[Tensor], Tensor], optional\n        One argument callable performing activation after hidden layer.\n        Default: `torch.nn.ReLU()`\n    sigmoid : Callable[[Tensor], Tensor], optional\n        One argument callable squashing values after excitation.\n        Default: `torch.nn.Sigmoid`\n\n    """"""\n\n    def __init__(\n        self, in_channels: int, hidden: int = None, activation=None, sigmoid=None,\n    ):\n        super().__init__()\n        self.in_channels: int = in_channels\n        self.hidden: int = hidden if hidden is not None else in_channels // 16\n        self.activation: typing.Callable[\n            [torch.Tensor], torch.Tensor\n        ] = activation if activation is not None else torch.nn.ReLU()\n        self.sigmoid: typing.Callable[\n            [torch.Tensor], torch.Tensor\n        ] = sigmoid if sigmoid is not None else torch.nn.Sigmoid()\n\n        self._pooling = pooling.GlobalAvgPool()\n        self._first = torch.nn.Linear(in_channels, self.hidden)\n        self._second = torch.nn.Linear(self.hidden, in_channels)\n\n    def forward(self, inputs):\n        excitation = self.sigmoid(\n            self._second(self.activation(self._first(self._pooling(inputs))))\n        )\n\n        return inputs * excitation.view(\n            *excitation.shape, *([1] * (len(inputs.shape) - 2))\n        )\n\n\nclass Fire(torch.nn.Module):\n    """"""Squeeze and Expand number of channels efficiently operation-wise.\n\n    First input channels will be squeezed to `hidden` channels and :math:`1 x 1` convolution.\n    After that those will be expanded to `out_channels` partially done by :math:`3 x 3` convolution\n    and partially by :math:`1 x 1` convolution (as specified by `p` parameter).\n\n    Originally proposed by Forrest N. Iandola et al. in\n    `SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size <https://arxiv.org/abs/1602.07360>`__\n\n    Parameters\n    ----------\n    in_channels : int\n        Number of channels in the input\n    out_channels : int\n        Number of channels produced by Fire module\n    hidden_channels : int, optional\n        Number of hidden channels (squeeze convolution layer).\n        Default: `None` (half of `in_channels`)\n    p : float, optional\n        Ratio of :math:`1 x 1` convolution taken from total `out_channels`.\n        The more, the more :math:`1 x 1` convolution will be used during expanding.\n        Default: `0.5` (half of `out_channels`)\n\n    """"""\n\n    def __init__(\n        self, in_channels: int, out_channels: int, hidden_channels=None, p: float = 0.5,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        if hidden_channels is None:\n            if in_channels >= 16:\n                self.hidden_channels = in_channels // 2\n            else:\n                self.hidden_channels = 8\n        else:\n            self.hidden_channels = hidden_channels\n\n        if not 0.0 < p < 1.0:\n            raise ValueError(""Fire\'s p has to be between 0 and 1, got {}"".format(p))\n\n        self.p: float = p\n\n        self.squeeze = Conv(in_channels, self.hidden_channels, kernel_size=1)\n\n        small_out_channels = int(out_channels * self.p)\n        self.expand_small = Conv(\n            self.hidden_channels, small_out_channels, kernel_size=1\n        )\n        self.expand_large = Conv(\n            self.hidden_channels,\n            out_channels - small_out_channels,\n            kernel_size=3,\n            padding=1,\n        )\n\n    def forward(self, inputs):\n        squeeze = self.squeeze(inputs)\n        return torch.cat(\n            (self.expand_small(squeeze), self.expand_large(squeeze)), dim=1\n        )\n\n\nclass InvertedResidualBottleneck(torch.nn.Module):\n    """"""Inverted residual block used in MobileNetV2, MNasNet, Efficient Net and other architectures.\n\n    Originally proposed by Mark Sandler et al. in\n    `MobileNetV2: Inverted Residuals and Linear Bottlenecks <0.5MB model size <https://arxiv.org/abs/1801.04381>`__\n\n    Expanded with `SqueezeExcitation` after depthwise convolution by Mingxing Tan et al. in\n    `MnasNet: Platform-Aware Neural Architecture Search for Mobile <https://arxiv.org/abs/1807.11626>`__\n\n    Due to it\'s customizable nature blocks from other research papers could be easily produced, e.g.\n    `Searching for MobileNetV3 <https://arxiv.org/pdf/1905.02244.pdf>`__ by providing\n    `torchlayers.HardSwish()` as `activation`, `torchlayers.HardSigmoid()` as `squeeze_excitation_activation`\n    and `squeeze_excitation_hidden` equal to `hidden_channels // 4`.\n\n    Parameters\n    ----------\n    in_channels: int\n        Number of channels in the input\n    hidden_channels: int, optional\n        Number of hidden channels (expanded). Should be greater than `in_channels`, usually\n        by factor of `4`. Default: `in_channels * 4`\n    activation: typing.Callable, optional\n        One argument callable performing activation after hidden layer.\n        Default: `torch.nn.ReLU6()`\n    batchnorm: bool, optional\n        Whether to apply Batch Normalization layer after initial convolution,\n        depthwise expanding part (before squeeze excitation) and final squeeze.\n        Default: `True`\n    squeeze_excitation: bool, optional\n        Whether to use standard `SqueezeExcitation` (see `SqueezeExcitation` module)\n        after depthwise convolution.\n        Default: `True`\n    squeeze_excitation_hidden: int, optional\n        Size of the hidden `torch.nn.Linear` layer. Usually smaller than `in_channels`\n        (at least in original research paper). Default: `1/16` of `in_channels` as\n        suggested by original paper.\n    squeeze_excitation_activation: typing.Callable, optional\n        One argument callable performing activation after hidden layer.\n        Default: `torch.nn.ReLU()`\n    squeeze_excitation_sigmoid: typing.Callable, optional\n        One argument callable squashing values after excitation.\n        Default: `torch.nn.Sigmoid`\n\n    """"""\n\n    def __init__(\n        self,\n        in_channels: int,\n        hidden_channels: int = None,\n        activation=None,\n        batchnorm: bool = True,\n        squeeze_excitation: bool = True,\n        squeeze_excitation_hidden: int = None,\n        squeeze_excitation_activation=None,\n        squeeze_excitation_sigmoid=None,\n    ):\n        def _add_batchnorm(block, channels):\n            if batchnorm:\n                block.append(normalization.BatchNorm(channels))\n            return block\n\n        super().__init__()\n\n        # Argument assignments\n        self.in_channels: int = in_channels\n        self.hidden_channels: int = hidden_channels if hidden_channels is not None else in_channels * 4\n        self.activation: typing.Callable[\n            [torch.Tensor], torch.Tensor\n        ] = torch.nn.ReLU6() if activation is None else activation\n        self.batchnorm: bool = batchnorm\n        self.squeeze_excitation: bool = squeeze_excitation\n        self.squeeze_excitation_hidden: int = squeeze_excitation_hidden\n        self.squeeze_excitation_activation: typing.Callable[\n            [torch.Tensor], torch.Tensor\n        ] = squeeze_excitation_activation\n        self.squeeze_excitation_sigmoid: typing.Callable[\n            [torch.Tensor], torch.Tensor\n        ] = squeeze_excitation_sigmoid\n\n        # Initial expanding block\n        initial = torch.nn.Sequential(\n            *_add_batchnorm(\n                [\n                    Conv(self.in_channels, self.hidden_channels, kernel_size=1),\n                    self.activation,\n                ],\n                self.hidden_channels,\n            )\n        )\n\n        # Depthwise block\n        depthwise_modules = _add_batchnorm(\n            [\n                Conv(\n                    self.hidden_channels,\n                    self.hidden_channels,\n                    kernel_size=3,\n                    groups=self.hidden_channels,\n                ),\n                self.activation,\n            ],\n            self.hidden_channels,\n        )\n\n        if squeeze_excitation:\n            depthwise_modules.append(\n                SqueezeExcitation(\n                    self.hidden_channels,\n                    squeeze_excitation_hidden,\n                    squeeze_excitation_activation,\n                    squeeze_excitation_sigmoid,\n                )\n            )\n\n        depthwise = torch.nn.Sequential(*depthwise_modules)\n\n        # Squeeze to in channels\n        squeeze = torch.nn.Sequential(\n            *_add_batchnorm(\n                [Conv(self.hidden_channels, self.in_channels, kernel_size=1,),],\n                self.in_channels,\n            )\n        )\n\n        self.block = Residual(torch.nn.Sequential(initial, depthwise, squeeze))\n\n    def forward(self, inputs):\n        return self.block(inputs)\n'"
torchlayers/module.py,11,"b'import typing\n\nimport torch\n\nfrom . import _dev_utils\n\n\nclass InferDimension(torch.nn.Module):\n    """"""Infer dimensionality of module from input using dispatcher.\n\n    Users can pass provide their own modules to infer dimensionality\n    from input tensor by inheriting from this module and providing\n    `super().__init__()` with `dispatcher` method::\n\n\n        import torchlayers as tl\n\n        class BatchNorm(tl.InferDimension):\n            def __init__(\n                self,\n                num_features: int,\n                eps: float = 1e-05,\n                momentum: float = 0.1,\n                affine: bool = True,\n                track_running_stats: bool = True,\n            ):\n                super().__init__(\n                    dispatcher={\n                        # 5 dimensional tensor -> create torch.nn.BatchNorm3d\n                        5: torch.nn.BatchNorm3d,\n                        # 4 dimensional tensor -> create torch.nn.BatchNorm2d\n                        4: torch.nn.BatchNorm2d,\n                        3: torch.nn.BatchNorm1d,\n                        2: torch.nn.BatchNorm1d,\n                    },\n                    num_features=num_features,\n                    eps=eps,\n                    momentum=momentum,\n                    affine=affine,\n                    track_running_stats=track_running_stats,\n                )\n\n    All dimension-agnostic modules in `torchlayers` are created this way.\n    This class can also be mixed with `torchlayers.infer` for dimensionality\n    and shape inference in one.\n\n    This class works correctly with `torchlayers.build` and other provided\n    functionalities.\n\n    Parameters\n    ----------\n    dispatcher: Dict[int, torch.nn.Module]\n        Key should be length of input\'s tensor shape. Value should be a `torch.nn.Module`\n        to be used for the dimensionality.\n    initializer: Callable[[torch.nn.Module, torch.Tensor, **kwargs], torch.nn.Module], optional\n        How to initialize dispatched module. Can be used to modify it\'s creation.\n        First argument - dispatched module class, second - input tensor, **kwargs are\n        arguments to use for module initialization. Should return module\'s instance.\n        By default dispatched module initialized with **kwargs is returned.\n    **kwargs:\n        Arguments used to initialize dispatched module\n\n    """"""\n\n    def __init__(\n        self,\n        dispatcher: typing.Dict[int, torch.nn.Module],\n        initializer: typing.Callable = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self._dispatcher = dispatcher\n        self._inner_module_name = ""_inner_module""\n\n        if initializer is None:\n            self._initializer = lambda dispatched_class, _, **kwargs: dispatched_class(\n                **kwargs\n            )\n        else:\n            self._initializer = initializer\n\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\n        self._noninferable_attributes = [key for key in kwargs]\n        self._repr = _dev_utils.infer.create_repr(self._inner_module_name, **kwargs)\n        self._reduce = _dev_utils.infer.create_reduce(\n            self._inner_module_name, self._noninferable_attributes\n        )\n\n    def __repr__(self):\n        return self._repr(self)\n\n    def __reduce__(self):\n        return self._reduce(self)\n\n    def forward(self, inputs):\n        module = getattr(self, self._inner_module_name, None)\n        if module is None:\n            dimensionality = len(inputs.shape)\n            dispatched_class = self._dispatcher.get(dimensionality)\n            if dispatched_class is None:\n                dispatched_class = self._dispatcher.get(""*"")\n                if dispatched_class is None:\n                    raise ValueError(\n                        ""{} could not be inferred from shape. Got tensor of dimensionality: {} but only {} are allowed"".format(\n                            self._module_name,\n                            dimensionality,\n                            list(self._dispatcher.keys()),\n                        )\n                    )\n\n            self.add_module(\n                self._inner_module_name,\n                self._initializer(\n                    dispatched_class,\n                    inputs,\n                    **{\n                        key: getattr(self, key) for key in self._noninferable_attributes\n                    },\n                ),\n            )\n\n        return getattr(self, self._inner_module_name)(inputs)\n'"
torchlayers/normalization.py,10,"b'import collections\n\nimport torch\n\nfrom . import module\n\n\nclass InstanceNorm(module.InferDimension):\n    """"""Apply Instance Normalization over inferred dimension (3D up to 5D).\n\n    Based on input shape it either creates 1D, 2D or 3D instance normalization for inputs of shape\n    3D, 4D, 5D respectively (including batch as first dimension).\n\n    Otherwise works like standard PyTorch\'s `InstanceNorm <https://pytorch.org/docs/stable/nn.html#torch.nn.InstanceNorm1d>`__\n\n    Parameters\n    ----------\n    num_features : int\n        :math:`C` (number of channels in input) from an expected input.\n        Can be number of outputs of previous linear layer as well\n    eps : float, optional\n        Value added to the denominator for numerical stability.\n        Default: `1e-5`\n    momentum : float, optional\n        Value used for the `running_mean` and `running_var`\n        computation. Default: `0.1`\n    affine : bool, optional\n        If ``True``, this module has learnable affine parameters, initialized just like in batch normalization.\n        Default: ``False``\n    track_running_stats : bool, optional\n        If ``True``, this module tracks the running mean and variance,\n        and when set to ``False``, this module does not track such statistics and always uses batch\n        statistics in both training and eval modes.\n        Default: ``False``\n\n    """"""\n\n    def __init__(\n        self,\n        num_features: int,\n        eps: float = 1e-05,\n        momentum: float = 0.1,\n        affine: bool = False,\n        track_running_stats: bool = False,\n    ):\n        super().__init__(\n            dispatcher={\n                5: torch.nn.InstanceNorm3d,\n                4: torch.nn.InstanceNorm2d,\n                3: torch.nn.InstanceNorm1d,\n            },\n            num_features=num_features,\n            eps=eps,\n            momentum=momentum,\n            affine=affine,\n            track_running_stats=track_running_stats,\n        )\n\n\nclass BatchNorm(module.InferDimension):\n    """"""Apply Batch Normalization over inferred dimension (2D up to 5D).\n\n    Based on input shape it either creates `1D`, `2D` or `3D` batch normalization for inputs of shape\n    `2D/3D`, `4D`, `5D` respectively (including batch as first dimension).\n\n    Otherwise works like standard PyTorch\'s `BatchNorm <https://pytorch.org/docs/stable/nn.html#batchnorm1d>`__.\n\n    Parameters\n    ----------\n    num_features : int\n        :math:`C` (number of channels in input) from an expected input.\n        Can be number of outputs of previous linear layer as well\n    eps : float, optional\n        Value added to the denominator for numerical stability.\n        Default: `1e-5`\n    momentum : float, optional\n        Value used for the `running_mean` and `running_var`\n        computation. Can be set to ``None`` for cumulative moving average\n        (i.e. simple average). Default: `0.1`\n    affine : bool, optional\n        If ``True``, this module has learnable affine parameters.\n        Default: ``True``\n    track_running_stats : bool, optional\n        If ``True``, this module tracks the running mean and variance,\n        and when set to ``False``, this module does not track such statistics and always uses batch\n        statistics in both training and eval modes.\n        Default: ``True``\n\n    """"""\n\n    def __init__(\n        self,\n        num_features: int,\n        eps: float = 1e-05,\n        momentum: float = 0.1,\n        affine: bool = True,\n        track_running_stats: bool = True,\n    ):\n        super().__init__(\n            dispatcher={\n                5: torch.nn.BatchNorm3d,\n                4: torch.nn.BatchNorm2d,\n                3: torch.nn.BatchNorm1d,\n                2: torch.nn.BatchNorm1d,\n            },\n            num_features=num_features,\n            eps=eps,\n            momentum=momentum,\n            affine=affine,\n            track_running_stats=track_running_stats,\n        )\n\n\n# Arguments had the wrong order unfortunately\nclass GroupNorm(torch.nn.GroupNorm):\n    """"""Apply Group Normalization over a mini-batch of inputs.\n\n    Works exactly like PyTorch\'s counterpart, but `num_channels` is used as first argument\n    so it can be inferred during first forward pass.\n\n    Parameters\n    ----------\n    num_channels : int\n        Number of channels expected in input\n    num_groups : int\n        Number of groups to separate the channels into\n    eps : float, optional\n        Value added to the denominator for numerical stability.\n        Default: `1e-5`\n    affine : bool, optional\n        If ``True``, this module has learnable affine parameters.\n        Default: ``True``\n\n    """"""\n\n    def __init__(\n        self,\n        num_channels: int,\n        num_groups: int,\n        eps: float = 1e-05,\n        affine: bool = True,\n    ):\n        super().__init__(\n            num_groups=num_groups, num_channels=num_channels, eps=eps, affine=affine,\n        )\n'"
torchlayers/pooling.py,38,"b'import typing\n\nimport torch\n\nfrom . import module\n\n\nclass _GlobalPool(torch.nn.Module):\n    def forward(self, inputs):\n        return self._pooling(inputs).reshape(inputs.shape[0], -1)\n\n\nclass GlobalMaxPool1d(_GlobalPool):\n    """"""Applies a 1D global max pooling over the last dimension.\n\n    Usually used after last `Conv1d` layer to get maximum feature values\n    for each timestep.\n\n    Internally operates as `torch.nn.AdaptiveMaxPool1d` with redundant `1` dimensions\n    flattened.\n\n    Returns\n    -------\n    `torch.Tensor`\n        `2D` tensor `(batch, features)`\n\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self._pooling = torch.nn.AdaptiveMaxPool1d(1)\n\n\nclass GlobalMaxPool2d(_GlobalPool):\n    """"""Applies a 2D global max pooling over the last dimension(s).\n\n    Usually used after last `Conv2d` layer to get maximum value feature values\n    for each channel. Can be used on `3D` or `4D` input (though the latter is more common).\n\n    Internally operates as `torch.nn.AdaptiveMaxPool2d` with redundant `1` dimensions\n    flattened.\n\n    Returns\n    -------\n    `torch.Tensor`\n        `2D` tensor `(batch, features)`\n\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self._pooling = torch.nn.AdaptiveMaxPool2d(1)\n\n\nclass GlobalMaxPool3d(_GlobalPool):\n    """"""Applies a 3D global max pooling over the last dimension(s).\n\n    Usually used after last `Conv3d` layer to get maximum value feature values\n    for each channel. Can be used on `4D` or `5D` input (though the latter is more common).\n\n    Internally operates as `torch.nn.AdaptiveMaxPool3d` with redundant `1` dimensions\n    flattened.\n\n    Returns\n    -------\n    `torch.Tensor`\n        `2D` tensor `(batch, features)`\n\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self._pooling = torch.nn.AdaptiveMaxPool3d(1)\n\n\nclass GlobalAvgPool1d(_GlobalPool):\n    """"""Applies a 1D global average pooling over the last dimension.\n\n    Usually used after last `Conv1d` layer to get mean of features values\n    for each timestep.\n\n    Internally operates as `torch.nn.AdaptiveAvgPool1d` with redundant `1` dimensions\n    flattened.\n\n    Returns\n    -------\n    `torch.Tensor`\n        `2D` tensor `(batch, features)`\n\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self._pooling = torch.nn.AdaptiveAvgPool1d(1)\n\n\nclass GlobalAvgPool2d(_GlobalPool):\n    """"""Applies a 2D global average pooling over the last dimension(s).\n\n    Usually used after last `Conv2d` layer to get mean value of features values\n    for each channel. Can be used on `3D` or `4D` input (though the latter is more common).\n\n    Internally operates as `torch.nn.AdaptiveAvgPool3d` with redundant `1` dimensions\n    flattened.\n\n    Returns\n    -------\n    `torch.Tensor`\n        `2D` tensor `(batch, features)`\n\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self._pooling = torch.nn.AdaptiveAvgPool2d(1)\n\n\nclass GlobalAvgPool3d(_GlobalPool):\n    """"""Applies a 3D global average pooling over the last dimension(s).\n\n    Usually used after last `Conv3d` layer to get mean value of features values\n    for each channel. Can be used on `4D` or `5D` input (though the latter is more common).\n\n    Internally operates as `torch.nn.AdaptiveAvgPool3d` with redundant `1` dimensions\n    flattened.\n\n    Returns\n    -------\n    `torch.Tensor`\n        `2D` tensor `(batch, features)`\n\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self._pooling = torch.nn.AdaptiveAvgPool3d(1)\n\n\nclass GlobalMaxPool(module.InferDimension):\n    """"""Perform `max` pooling operation leaving maximum values from channels.\n\n    Usually used after last convolution layer (`torchlayers.Conv`)\n    to get pixels of maximum value from each channel.\n\n    Depending on shape of passed `torch.Tensor` either `1D`, `2D` or `3D` `GlobalMaxPool`\n    will be used for `3D`, `4D` and `5D` shape respectively (batch included).\n\n    Internally operates as `torchlayers.pooling.GlobalMaxPoolNd`.\n\n    Returns\n    -------\n    `torch.Tensor`\n        `2D` tensor `(batch, features)`\n\n    """"""\n\n    def __init__(self):\n        super().__init__(\n            dispatcher={5: GlobalMaxPool3d, 4: GlobalMaxPool2d, 3: GlobalMaxPool1d}\n        )\n\n\nclass GlobalAvgPool(module.InferDimension):\n    """"""Perform `mean` pooling operation leaving average values from channels.\n\n    Usually used after last convolution layer (`torchlayers.Conv`) to get mean\n    of pixels from each channel.\n\n    Depending on shape of passed `torch.Tensor` either `1D`, `2D` or `3D`\n    pooling will be used for `3D`, `4D` and `5D`\n    shape respectively (batch included).\n\n    Internally operates as `torchlayers.pooling.GlobalAvgPoolNd`.\n\n    Returns\n    -------\n    `torch.Tensor`\n        `2D` tensor `(batch, features)`\n\n    """"""\n\n    def __init__(self):\n        super().__init__(\n            dispatcher={5: GlobalAvgPool3d, 4: GlobalAvgPool2d, 3: GlobalAvgPool1d}\n        )\n\n\nclass MaxPool(module.InferDimension):\n    """"""Perform `max` operation across first `torch.Tensor` dimension.\n\n    Depending on shape of passed `torch.Tensor` either `torch.nn.MaxPool1D`,\n    `torch.nn.MaxPool2D` or `torch.nn.MaxPool3D` pooling will be used\n    for `3D`, `4D` and `5D` shape respectively (batch included).\n\n    Default value for `kernel_size` (`2`) was added.\n\n    Parameters\n    ----------\n    kernel_size: int, optional\n        The size of the window to take a max over. Default: `2`\n    stride: int, optional\n        The stride of the window. Default value is :attr:`kernel_size`\n    padding: int, optional\n        Implicit zero padding to be added on both sides. Default: `0`\n    dilation: int\n        Parameter controlling the stride of elements in the window. Default: `1`\n    return_indices: bool, optional\n        If ``True``, will return the max indices along with the outputs.\n        Useful for :class:`torch.nn.MaxUnpool` later. Default: `False`\n    ceil_mode: bool, optional\n        When True, will use `ceil` instead of `floor` to compute the output shape.\n        Default: `False`\n\n    Returns\n    -------\n    `torch.Tensor`\n        Same shape as `input` with values pooled.\n\n    """"""\n\n    def __init__(\n        self,\n        kernel_size: int = 2,\n        stride: int = None,\n        padding: int = 0,\n        dilation: int = 1,\n        return_indices: bool = False,\n        ceil_mode: bool = False,\n    ):\n        super().__init__(\n            dispatcher={\n                5: torch.nn.MaxPool3d,\n                4: torch.nn.MaxPool2d,\n                3: torch.nn.MaxPool1d,\n            },\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            return_indices=return_indices,\n            ceil_mode=ceil_mode,\n        )\n\n\nclass AvgPool(module.InferDimension):\n    """"""Perform `avg` operation across first `torch.Tensor` dimension.\n\n    Depending on shape of passed `torch.Tensor` either `torch.nn.AvgPool1D`,\n    `torch.nn.AvgPool2D` or `torch.nn.AvgPool3D` pooling will be used\n    for `3D`, `4D` and `5D` shape respectively (batch included).\n\n    Default value for `kernel_size` (`2`) was added.\n\n    Parameters\n    ----------\n    kernel_size: int, optional\n        The size of the window. Default: `2`\n    stride: int, optional\n        The stride of the window. Default value is :attr:`kernel_size`\n    padding: int, oprtional\n        Implicit zero padding to be added on both sides. Default: `0`\n    ceil_mode: bool, opriontal\n        When True, will use `ceil` instead of `floor` to compute the output shape.\n        Default: `True`\n    count_include_pad: bool, optional\n        When True, will include the zero-padding in the averaging. Default: `True`\n\n    Returns\n    -------\n    `torch.Tensor`\n        Same shape as `input` with values pooled.\n\n    """"""\n\n    def __init__(\n        self,\n        kernel_size: int = 2,\n        stride: int = None,\n        padding: int = 0,\n        ceil_mode: bool = False,\n        count_include_pad: bool = True,\n    ):\n        super().__init__(\n            dispatcher={\n                5: torch.nn.AvgPool3d,\n                4: torch.nn.AvgPool2d,\n                3: torch.nn.AvgPool1d,\n            },\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            ceil_mode=ceil_mode,\n            count_include_pad=count_include_pad,\n        )\n'"
torchlayers/preprocessing.py,24,"b'import abc\nimport numbers\nimport random\n\nimport torch\n\n\nclass _GetInputs(torch.nn.Module):\n    def _get_inputs(self, inputs):\n        if self.inplace:\n            return inputs\n        return inputs.clone()\n\n\nclass RandomApply(_GetInputs):\n    """"""Apply randomly a list of transformations with a given probability.\n\n\n    .. note::\n            **IMPORTANT**: This function is a `no-op` during inference phase\n            (module in `eval` mode).\n\n    Parameters\n    ----------\n    transforms : List | Tuple\n        List of transformations\n    p : float, optional\n        Probability to apply list of transformations. Default: `0.5`\n    inplace : bool, optional\n        Whether to run this operation in-place. Default: `False`\n\n    """"""\n\n    def __init__(self, transforms, p: float = 0.5, inplace: bool = False):\n        super().__init__()\n        self.transforms = torch.nn.ModuleList(transforms)\n        self.p: float = p\n        self.inplace: bool = inplace\n\n    def forward(self, inputs):\n        if not self.training:\n            return inputs\n\n        x = self._get_inputs(inputs)\n        if random.random() > self.p:\n            return inputs\n        for transform in self.transforms:\n            x = transform(x)\n        return x\n\n\nclass RandomChoice(torch.nn.Module):\n    """"""Apply single transformation randomly picked from a list.\n\n    .. note::\n            **IMPORTANT**: This function is a `no-op` during inference phase\n            (module in `eval` mode).\n\n    Parameters\n    ----------\n    transforms : List | Tuple\n        List of transformations\n    """"""\n\n    def __init__(self, transforms):\n        super().__init__()\n        self.transforms = torch.nn.ModuleList(transforms)\n\n    def forward(self, inputs):\n        if not self.training:\n            return inputs\n\n        transform = random.choice(self.transforms)\n        return transform(inputs)\n\n\nclass RandomOrder(_GetInputs):\n    """"""Apply single transformation randomly picked from a list.\n\n    .. note::\n            **IMPORTANT**: This function is a `no-op` during inference phase\n            (module in `eval` mode).\n\n    Parameters\n    ----------\n    transforms : List | Tuple\n        List of transformations\n    inplace : bool, optional\n        Whether to run this operation in-place. Default: `False`\n    """"""\n\n    def __init__(self, transforms, inplace: bool = False):\n        super().__init__()\n        self.transforms = torch.nn.ModuleList(transforms)\n        self.inplace: bool = inplace\n\n    def forward(self, inputs):\n        if not self.training:\n            return inputs\n\n        x = self._get_inputs(inputs)\n\n        order = list(range(len(self.transforms)))\n        random.shuffle(order)\n        for i in order:\n            x = self.transforms[i](x)\n        return x\n\n\nclass Normalize(torch.nn.Module):\n    """"""Normalize batch of tensor images with mean and standard deviation.\n\n    Given mean values: `(M1,...,Mn)` and std values: `(S1,..,Sn)` for `n` channels\n    (or other broadcastable to `n` values),\n    this transform will normalize each channel of tensors in batch via formula:\n    `output[channel] = (input[channel] - mean[channel]) / std[channel]`\n\n    Parameters\n    ----------\n    mean : Tuple | List | torch.tensor\n        Sequence of means for each channel\n    std : Tuple | List | torch.tensor\n        Sequence of means for each channel\n    inplace : bool, optional\n        Whether to run this operation in-place. Default: `False`\n\n    """"""\n\n    @classmethod\n    def _transform_to_tensor(cls, tensor, name: str):\n        if not torch.is_tensor(tensor):\n            if isinstance(tensor, (tuple, list)):\n                return torch.tensor(tensor)\n            else:\n                raise ValueError(\n                    ""{} is not an instance of either list, tuple or torch.tensor."".format(\n                        name\n                    )\n                )\n        return tensor\n\n    @classmethod\n    def _check_shape(cls, tensor, name):\n        if len(tensor.shape) > 1:\n            raise ValueError(\n                ""{} should be 0 or 1 dimensional tensor. Got {} dimensional tensor."".format(\n                    name, len(tensor.shape)\n                )\n            )\n\n    def __init__(self, mean: torch.Tensor, std: torch.Tensor, inplace: bool = False):\n        tensor_mean = Normalize._transform_to_tensor(mean, ""mean"")\n        tensor_std = Normalize._transform_to_tensor(std, ""std"")\n        Normalize._check_shape(tensor_mean, ""mean"")\n        Normalize._check_shape(tensor_std, ""std"")\n\n        if torch.any(tensor_std == 0):\n            raise ValueError(\n                ""One or more std values are zero which would lead to division by zero.""\n            )\n\n        super().__init__()\n\n        self.register_buffer(""mean"", tensor_mean)\n        self.register_buffer(""std"", tensor_std)\n        self.inplace: bool = inplace\n\n    def forward(self, inputs):\n        inputs_length = len(inputs.shape) - 2\n        mean = self.mean.view(1, -1, *([1] * inputs_length))\n        std = self.std.view(1, -1, *([1] * inputs_length))\n        if self.inplace:\n            inputs.sub_(mean).div_(std)\n            return inputs\n        return (inputs - mean) / std\n\n\nclass Transform(_GetInputs):\n    """"""{header}\n\n    {body}\n\n    .. note::\n            **IMPORTANT**: This function is a `no-op` during inference phase\n            (module in `eval` mode).\n\n    Parameters\n    ----------\n    p : float, optional\n        Probability of applying transformation. Default: `0.5`\n    batch : bool, optional\n        Whether this operation should be applied on whole batch.\n        If `True` the same transformation is applied on whole batch (or to\n        no image in batch at all). If `False` apply transformation to `p` percent\n        of images contained in batch at random. Default: `False`\n    inplace : bool, optional\n        Whether to run this operation in-place. Default: `False`\n\n    """"""\n\n    def __init__(self, p: float = 0.5, batch: bool = False, inplace: bool = False):\n        if p < 0 or p > 1:\n            raise ValueError(""Probability of rotation should be between 0 and 1"")\n        super().__init__()\n        self.p = p\n        self.batch: bool = batch\n        self.inplace: bool = inplace\n\n    def forward(self, inputs):\n        if self.training:\n            x = self._get_inputs(inputs)\n            if self.batch:\n                if random.random() < self.p:\n                    return self.transform(x)\n            else:\n                indices = torch.randperm(x.shape[0])[: int(x.shape[0] * self.p)]\n                x[indices] = self.transform(x[indices])\n                return x\n\n        return inputs\n\n    @abc.abstractmethod\n    def transform(self, x):\n        pass\n\n\nclass _RandomRotate90(Transform):\n    """"""Randomly rotate image {} by `90` degrees `k` times.\n\n    Rotation will be done in a clockwise manner.\n\n    .. note::\n            **IMPORTANT**: This function is a `no-op` during inference phase.\n\n    Parameters\n    ----------\n    p : float, optional\n        Probability of applying transformation. Default: `0.5`\n    k : int, optional\n        Number of times to rotate. Default: `1`\n    batch : bool, optional\n        Whether this operation should be applied on whole batch.\n        If `True` the same transformation is applied on whole batch (or to\n        no image in batch at all). If `False` apply transformation to `p` percent\n        of images contained in batch at random. Default: `False`\n    inplace : bool, optional\n        Whether to run this operation in-place. Default: `False`\n\n    """"""\n\n    def __init__(\n        self, p: float = 0.5, k: int = 1, batch: bool = False, inplace: bool = False\n    ):\n        super().__init__(p, batch, inplace)\n        self.k: int = k\n\n\nclass ClockwiseRandomRotate90(_RandomRotate90):\n    __doc__ = _RandomRotate90.__doc__.format(""clockwise"")\n\n    def transform(self, x):\n        return torch.rot90(x, k=self.k, dims=(-1, -2))\n\n\nclass AntiClockwiseRandomRotate90(_RandomRotate90):\n    __doc__ = _RandomRotate90.__doc__.format(""anticlockwise"")\n\n    def transform(self, x):\n        return torch.rot90(x, k=self.k, dims=(-2, -1))\n\n\nclass RandomHorizontalFlip(Transform):\n    __doc__ = Transform.__doc__.format(\n        header=""Randomly perform horizontal flip on batch of images."", body=""""\n    )\n\n    def transform(self, x):\n        return torch.flip(x, dims=(-1,))\n\n\nclass RandomVerticalFlip(Transform):\n    __doc__ = Transform.__doc__.format(\n        header=""Randomly perform vertical flip on batch of images."", body=""""\n    )\n\n    def transform(self, x):\n        return torch.flip(x, dims=(-2,))\n\n\nclass RandomVerticalHorizontalFlip(Transform):\n    __doc__ = Transform.__doc__.format(\n        header=""Randomly perform vertical and horizontal flip on batch of images."",\n        body="""",\n    )\n\n    def transform(self, x):\n        return torch.flip(x, dims=(-2, -1))\n\n\nclass RandomErasing(Transform):\n    """"""Randomly select rectangle regions in a batch of image and erase their pixels.\n\n    Originally proposed by Zhong et al. in `Random Erasing Data Augmentation <https://arxiv.org/pdf/1708.04896.pdf>`__\n\n    .. note::\n            **IMPORTANT**: This function is a `no-op` during inference phase.\n    .. note::\n            Each image in batch will have the same rectangle region cut out\n            due to efficiency reasons. It probably doesn\'t alter the idea\n            drastically but exact effects weren\'t tested.\n\n\n    Parameters\n    ----------\n    max_rectangles : int\n        Maximum number of rectangles to create.\n    max_height : int\n        Maximum height of the rectangle.\n    max_width : int, optional\n        Maximum width of the rectangle. Default: same as `max_height`\n    min_rectangles : int, optional\n        Minimum number of rectangles to create. Default: same as `max_rectangles`\n    min_height : int, optional\n        Minimum height of the rectangle. Default: same as `max_height`\n    min_width : int, optional\n        Minimum width of the rectangle. Default: same as `min_width`\n    fill : Callable, optional\n        Callable used to fill the rectangle. It will be passed three arguments:\n        `size` (as a `tuple`), `dtype`, `layout` and `device` of original tensor.\n        If you want to specify `random uniform` filling you can use this:\n        `lambda` function: `random = lambda size, dtype, layout, device: torch.randn(*size, dtype=dtype, layout=layout, device=device)`.\n        If non-default is used, users are responsible for ensuring correct tensor format based\n        on callable passed arguments.\n        Default: fill with `0.0`.\n    p : float, optional\n        Probability of applying transformation. Default: `0.5`\n    batch : bool, optional\n        Whether this operation should be applied on whole batch.\n        If `True` the same transformation is applied on whole batch (or to\n        no image in batch at all). If `False` apply transformation to `p` percent\n        of images contained in batch at random. Default: `False`\n    inplace : bool, optional\n        Whether to run this operation in-place. Default: `False`\n\n    """"""\n\n    @classmethod\n    def _check_min_max(cls, minimum, maximum, name: str):\n        if minimum > maximum:\n            raise ValueError(\n                ""{} minimum is greater than maximum. Got minimum: {} and maximum: {}."".format(\n                    name.capitalize(), minimum, maximum\n                )\n            )\n\n    @classmethod\n    def _check_greater_than_zero(cls, value, name: str):\n        if value <= 0:\n            raise ValueError(\n                ""Minimal {} should be greater than 0. Got {}"".format(name, value)\n            )\n\n    @classmethod\n    def _conditional_default(cls, value, default):\n        if value is None:\n            return default\n        return value\n\n    def __init__(\n        self,\n        max_rectangles: int,\n        max_height: int,\n        max_width: int = None,\n        min_rectangles: int = None,\n        min_height: int = None,\n        min_width: int = None,\n        fill=None,\n        p: float = 0.5,\n        batch: bool = False,\n        inplace: bool = False,\n    ):\n        RandomErasing._check_greater_than_zero(min_rectangles, ""holes"")\n        RandomErasing._check_greater_than_zero(min_height, ""height"")\n        RandomErasing._check_greater_than_zero(min_width, ""width"")\n\n        RandomErasing._check_min_max(min_rectangles, max_rectangles, ""holes"")\n        RandomErasing._check_min_max(min_width, max_width, ""width"")\n        RandomErasing._check_min_max(min_height, max_height, ""height"")\n\n        self.max_rectangles: int = max_rectangles\n        self.max_height: int = max_height\n\n        self.max_width = RandomErasing._conditional_default(max_width, max_height)\n        self.min_rectangles = RandomErasing._conditional_default(\n            min_rectangles, max_rectangles\n        )\n        self.min_height = RandomErasing._conditional_default(min_height, max_height)\n        self.min_width = RandomErasing._conditional_default(min_width, self.max_width)\n\n        self.fill = RandomErasing._conditional_default(fill, lambda *_: 0.0)\n\n        super().__init__(p, batch, inplace)\n\n    def transform(self, x):\n        holes = random.randint(self.min_rectangles, self.max_rectangles)\n\n        start_hs = torch.randint(0, x.shape[-2] - self.max_height, (holes,))\n        start_ws = torch.randint(0, x.shape[-1] - self.max_width, (holes,))\n        heights = torch.randint(self.min_height, self.max_height, (holes,))\n        widths = torch.randint(self.min_width, self.max_width, (holes,))\n\n        for start_h, start_w, height, width in zip(start_hs, start_ws, heights, widths):\n            x[..., start_h : start_h + height, start_w : start_w + width] = self.fill(\n                (*(x.shape[:-2]), start_h + height, start_w + width),\n                x.dtype,\n                x.layout,\n                x.device,\n            )\n\n        return x\n'"
torchlayers/regularization.py,26,"b'import abc\n\nimport torch\n\nfrom . import module\n\n\nclass StochasticDepth(torch.nn.Module):\n    """"""Randomly skip module during training with specified `p`, leaving inference untouched.\n\n    Originally proposed by Gao Huang et al. in\n    `Deep Networks with Stochastic Depth <www.arxiv.org/abs/1512.03385>`__.\n\n    Originally devised as regularization, though `other research <https://web.stanford.edu/class/cs331b/2016/projects/kaplan_smith_jiang.pdf>`__  suggests:\n\n    - ""[...] StochasticDepth Nets are less tuned for low-level feature extraction but more tuned for higher level feature differentiation.""\n    - ""[...] Stochasticity does not help with the \xe2\x80\x9ddead neurons\xe2\x80\x9d problem; in fact the problem is actually more pronounced in the early layers. Nonetheless, the Stochastic Depth Network has relatively fewer dead neurons in later layers.""\n\n    It might be useful to employ this technique to layers closer to the bottleneck.\n\n    Example::\n\n\n        import torchlayers as tl\n\n        # Assume only 128 channels can be an input in this case\n        block = tl.StochasticDepth(tl.Conv(128), p=0.3)\n        # May skip tl.Conv with 0.3 probability\n        block(torch.randn(1, 3, 32, 32))\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Any module whose output might be skipped\n        (output shape of it has to be equal to the shape of inputs).\n    p : float, optional\n        Probability of survival (e.g. the layer will be kept). Default: ``0.5``\n\n    """"""\n\n    def __init__(self, module: torch.nn.Module, p: float = 0.5):\n        super().__init__()\n        if not 0 < p < 1:\n            raise ValueError(\n                ""Stochastic Depth p has to be between 0 and 1 but got {}"".format(p)\n            )\n        self.module: torch.nn.Module = module\n        self.p: float = p\n        self._sampler = torch.Tensor(1)\n\n    def forward(self, inputs):\n        if self.training and self._sampler.uniform_():\n            return inputs\n        return self.p * self.module(inputs)\n\n\nclass Dropout(module.InferDimension):\n    """"""Randomly zero out some of the tensor elements.\n\n    .. note::\n            Changes input only if `module` is in `train` mode.\n\n    Based on input shape it either creates `2D` or `3D` version of dropout for inputs of shape\n    `4D`, `5D` respectively (including batch as first dimension).\n    For every other dimension, standard `torch.nn.Dropout` will be used.\n\n    Parameters\n    ----------\n    p : float, optional\n        Probability of an element to be zeroed. Default: ``0.5``\n    inplace : bool, optional\n        If ``True``, will do this operation in-place. Default: ``False``\n\n    """"""\n\n    def __init__(self, p=0.5, inplace=False):\n        super().__init__(\n            dispatcher={\n                5: torch.nn.Dropout3d,\n                4: torch.nn.Dropout2d,\n                ""*"": torch.nn.Dropout,\n            },\n            p=p,\n            inplace=inplace,\n        )\n\n\nclass StandardNormalNoise(torch.nn.Module):\n    """"""Add noise from standard normal distribution during forward pass.\n\n    .. note::\n            Changes input only if `module` is in `train` mode.\n\n    Example::\n\n\n        import torchlayers as tl\n\n        model = tl.Sequential(\n            tl.StandardNormalNoise(), tl.Linear(10), tl.ReLU(), tl.tl.Linear(1)\n        )\n        tl.build(model, torch.randn(3, 30))\n\n        # Noise from Standard Normal distribution will be added\n        model(torch.randn(3, 30))\n\n        model.eval()\n        # Eval mode, no noise added\n        model(torch.randn(3, 30))\n\n    """"""\n\n    def forward(self, inputs):\n        if self.training:\n            return inputs + torch.randn_like(inputs)\n        return inputs\n\n\nclass UniformNoise(torch.nn.Module):\n    """"""Add noise from uniform `[0, 1)` distribution during forward pass.\n\n    .. note::\n            Changes input only if `module` is in `train` mode.\n\n    Example::\n\n\n        import torchlayers as tl\n\n        noisy_linear_regression = tl.Sequential(\n            tl.UniformNoise(), tl.Linear(1)\n        )\n        tl.build(model, torch.randn(1, 10))\n\n        # Noise from Uniform distribution will be added\n        model(torch.randn(64, 10))\n\n        model.eval()\n        # Eval mode, no noise added\n        model(torch.randn(64, 10))\n\n    """"""\n\n    def forward(self, inputs):\n        if self.training:\n            return inputs + torch.rand_like(inputs)\n        return inputs\n\n\nclass WeightDecay(torch.nn.Module):\n    def __init__(self, module, weight_decay, name: str = None):\n        if weight_decay <= 0.0:\n            raise ValueError(\n                ""Regularization\'s weight_decay should be greater than 0.0, got {}"".format(\n                    weight_decay\n                )\n            )\n\n        super().__init__()\n        self.module = module\n        self.weight_decay = weight_decay\n        self.name = name\n\n        self.hook = self.module.register_backward_hook(self._weight_decay_hook)\n\n    def remove(self):\n        self.hook.remove()\n\n    def _weight_decay_hook(self, *_):\n        if self.name is None:\n            for param in self.module.parameters():\n                if param.grad is None or torch.all(param.grad == 0.0):\n                    param.grad = self.regularize(param)\n        else:\n            for name, param in self.module.named_parameters():\n                if self.name in name and (\n                    param.grad is None or torch.all(param.grad == 0.0)\n                ):\n                    param.grad = self.regularize(param)\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n    def extra_repr(self) -> str:\n        representation = ""weight_decay={}"".format(self.weight_decay)\n        if self.name is not None:\n            representation += "", name={}"".format(self.name)\n        return representation\n\n    @abc.abstractmethod\n    def regularize(self, parameter):\n        pass\n\n\nclass L2(WeightDecay):\n    r""""""Regularize module\'s parameters using L2 weight decay.\n\n    Example::\n\n        import torchlayers as tl\n\n        # Regularize only weights of Linear module\n        regularized_layer = tl.L2(tl.Linear(30), weight_decay=1e-5, name=""weight"")\n\n    .. note::\n            Backward hook will be registered on `module`. If you wish\n            to remove `L2` regularization use `remove()` method.\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Module whose parameters will be regularized.\n    weight_decay : float\n        Strength of regularization (has to be greater than `0.0`).\n    name : str, optional\n        Name of parameter to be regularized (if any).\n        Default: all parameters will be regularized (including ""bias"").\n\n    """"""\n\n    def regularize(self, parameter):\n        return self.weight_decay * parameter.data\n\n\nclass L1(WeightDecay):\n    """"""Regularize module\'s parameters using L1 weight decay.\n\n    Example::\n\n        import torchlayers as tl\n\n        # Regularize all parameters of Linear module\n        regularized_layer = tl.L1(tl.Linear(30), weight_decay=1e-5)\n\n    .. note::\n            Backward hook will be registered on `module`. If you wish\n            to remove `L1` regularization use `remove()` method.\n\n    Parameters\n    ----------\n    module : torch.nn.Module\n        Module whose parameters will be regularized.\n    weight_decay : float\n        Strength of regularization (has to be greater than `0.0`).\n    name : str, optional\n        Name of parameter to be regularized (if any).\n        Default: all parameters will be regularized (including ""bias"").\n\n    """"""\n\n    def regularize(self, parameter):\n        return self.weight_decay * torch.sign(parameter.data)\n'"
torchlayers/upsample.py,13,"b'import typing\n\nimport torch\n\nfrom . import convolution\n\n\nclass ConvPixelShuffle(torch.nn.Module):\n    """"""Two dimensional convolution with ICNR initialization followed by PixelShuffle.\n\n    Increases `height` and `width` of `input` tensor by scale, acts like\n    learnable upsampling. Due to `ICNR weight initialization <https://arxiv.org/abs/1707.02937>`__\n    of `convolution` it has similar starting point to nearest neighbour upsampling.\n\n    `kernel_size` got a default value of `3`, `upscale_factor` got a default\n    value of `2`.\n\n    Example::\n\n        import torchlayers as tl\n\n\n        class MiniAutoEncoder(tl.Module):\n            def __init__(self, out_channels):\n                super().__init__()\n                self.conv1 = tl.Conv(64)\n                # Twice smaller image by default\n                self.pooling = tl.MaxPool()\n                # Twice larger (upscale_factor=2) by default\n                self.upsample = tl.ConvPixelShuffle(out_channels)\n\n            def forward(self, x):\n                x = self.conv1(x)\n                pooled = self.pooling(x)\n                return self.upsample(pooled)\n\n\n        out_channels = 3\n        network = MiniAutoEncoder(out_channels)\n        tl.build(network, torch.randn(1, out_channels, 64, 64))\n        assert network(torch.randn(5, out_channels, 64, 64)).shape == [5, out_channels, 64, 64]\n\n    .. note::\n\n        Currently only `4D` input is allowed (`[batch, channels, height, width]`),\n        due to `torch.nn.PixelShuffle` not supporting `3D` or `5D` inputs.\n        See [this PyTorch PR](https://github.com/pytorch/pytorch/pull/6340/files)\n        for example of dimension-agnostic implementation.\n\n    Parameters\n    ----------\n    in_channels : int\n        Number of channels in the input image\n    out_channels : int\n        Number of channels produced after PixelShuffle\n    upscale_factor : int, optional\n        Factor to increase spatial resolution by. Default: `2`\n    kernel_size : int or tuple, optional\n        Size of the convolving kernel. Default: `3`\n    stride : int or tuple, optional\n        Stride of the convolution. Default: 1\n    padding: int or tuple, optional\n        Zero-padding added to both sides of the input. Default: 0\n    padding_mode: string, optional\n        Accepted values `zeros` and `circular` Default: `zeros`\n    dilation: int or tuple, optional\n        Spacing between kernel elements. Default: 1\n    groups: int, optional\n        Number of blocked connections from input channels to output channels. Default: 1\n    bias: bool, optional\n        If ``True``, adds a learnable bias to the output. Default: ``True``\n    initializer: typing.Callable[[torch.Tensor,], torch.Tensor], optional\n        Initializer for ICNR initialization, can be a function from `torch.nn.init`.\n        Receive tensor as argument and returns tensor after initialization.\n        Default: `torch.nn.init.kaiming_normal_`\n\n    """"""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        upscale_factor: int = 2,\n        kernel_size: int = 3,\n        stride: int = 1,\n        padding=""same"",\n        dilation: int = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = ""zeros"",\n        initializer=None,\n    ):\n        super().__init__()\n        self.convolution = convolution.Conv(\n            in_channels,\n            out_channels * upscale_factor * upscale_factor,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode,\n        )\n\n        self.upsample = torch.nn.PixelShuffle(upscale_factor)\n        if initializer is None:\n            self.initializer = torch.nn.init.kaiming_normal_\n        else:\n            self.initializer = initializer\n\n    def post_build(self):\n        """"""Initialize weights after layer was built.""""""\n        self.icnr_initialization(self.convolution.weight.data)\n\n    def icnr_initialization(self, tensor):\n        """"""ICNR initializer for checkerboard artifact free sub pixel convolution.\n\n        Originally presented in\n        `Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize <https://arxiv.org/abs/1707.02937>`__\n        Initializes convolutional layer prior to `torch.nn.PixelShuffle`.\n        Weights are initialized according to `initializer` passed to to `__init__`.\n\n        Parameters\n        ----------\n        tensor: torch.Tensor\n                Tensor to be initialized using ICNR init.\n\n        Returns\n        -------\n        torch.Tensor\n                Tensor initialized using ICNR.\n\n        """"""\n\n        if self.upsample.upscale_factor == 1:\n            return self.initializer(tensor)\n\n        new_shape = [int(tensor.shape[0] / (self.upsample.upscale_factor ** 2))] + list(\n            tensor.shape[1:]\n        )\n\n        subkernel = self.initializer(torch.zeros(new_shape)).transpose(0, 1)\n\n        kernel = subkernel.reshape(subkernel.shape[0], subkernel.shape[1], -1).repeat(\n            1, 1, self.upsample.upscale_factor ** 2\n        )\n\n        return kernel.reshape([-1, tensor.shape[0]] + list(tensor.shape[2:])).transpose(\n            0, 1\n        )\n\n    def forward(self, inputs):\n        return self.upsample(self.convolution(inputs))\n'"
torchlayers/_dev_utils/__init__.py,0,b'from . import infer\n'
torchlayers/_dev_utils/helpers.py,0,"b'import collections\nimport itertools\nimport typing\n\n\ndef is_kwarg(argument: str) -> bool:\n    """"""`True` if argument name is `**kwargs`""""""\n    return ""**"" in argument\n\n\ndef is_vararg(argument: str) -> bool:\n    """"""`True` if argument name is `*args`""""""\n    return ""*"" in argument and ""**"" not in argument\n\n\ndef remove_vararg(argument: str) -> str:\n    """"""`True` if argument name is `*args`""""""\n    return argument.split(""*"")[1]\n\n\ndef remove_kwarg(argument: str) -> str:\n    """"""Removes `**` in kwargs so those can be assigned in dynamic `__init__` creation""""""\n    return argument.split(""**"")[1]\n\n\ndef remove_right_side(argument: str) -> str:\n    """"""Removes default arguments so their names can be used as values or names of variables.""""""\n    return argument.split(""="")[0]\n\n\ndef remove_type_hint(argument: str) -> str:\n    """"""Removes any Python type hints.\n\n    Those are incompatible with `exec` during dynamic `__init__` creation\n    (at least not without major workarounds).\n\n    Default values (right hand side) is preserved if exists.\n\n    """"""\n    splitted_on_type_hint = argument.split("":"")\n    no_type_hint = splitted_on_type_hint[0]\n    if len(splitted_on_type_hint) > 1:\n        splitted_on_default = splitted_on_type_hint[1].split(""="")\n        if len(splitted_on_default) > 1:\n            no_type_hint += ""={}"".format(splitted_on_default[1])\n    return no_type_hint\n\n\ndef create_vars(\n    self,\n    non_inferable_names: typing.Dict[str, typing.Any],\n    varargs_variable: str,\n    kwargs_variable: str,\n) -> typing.List[str]:\n    """"""\n    Create list of arguments for uninstantiated `__repr__` module.\n\n    Parameters\n    ----------\n    **non_inferable_names : Dict[str, Any]\n        Non-inferable names and their respective values of the module\n    varargs_variable : str\n        Name of variable possibly holding varargs for module\'s __init__.\n    kwargs_variable : str\n        Name of variable possibly holding kwargs for module\'s __init__.\n\n    Returns\n    -------\n    typing.List[str]:\n        List of strings formatted in the manner argument=value to display\n        for uninstantiated module.\n    """"""\n    dictionary = {**non_inferable_names, **collections.OrderedDict(vars(self))}\n\n    args = [\n        ""{}={}"".format(key, value)\n        for key, value in dictionary.items()\n        if not key.startswith(""_"") and key != ""training""\n    ]\n    varargs = getattr(self, varargs_variable, None)\n    if varargs is not None:\n        args += [str(var) for var in varargs]\n\n    kwargs = getattr(self, kwargs_variable, None)\n    if kwargs is not None:\n        args += [""{}={}"".format(key, value) for key, value in kwargs.items()]\n\n    return args\n'"
torchlayers/_dev_utils/infer.py,4,"b'import inspect\nimport pickle\nimport typing\n\nfrom . import helpers\n\nVARARGS_VARIABLE = ""_torchlayers_varargs_variable""\nKWARGS_VARIABLE = ""_torchlayers_kwargs_variable""\n\nMODULE = ""_torchlayers_infered_module""\nMODULE_CLASS = ""_torchlayers_infered_class_module""\n\n\ndef parse_arguments(\n    init_arguments: typing.List[str], module\n) -> typing.Tuple[typing.List[str], typing.Dict[str, typing.Any]]:\n    """"""Parse init arguments.\n\n    This function will:\n\n        - create list of names which cannot be determined from non-instantiated module\n        (e.g. `input_shape`) and assign them values (later used for non-instantiated __repr__)\n        - remove type hints from argument to be inferred (the first one after `self`)\n        and default value if provided\n        - remove type hints from all `__init__` arguments and preserve their default\n        arguments (custom exec won\'t be able to parse type hints without explicit imports)\n        - remove `self` so it won\'t be later assigned to dynamically created non-instantiated\n        module\n        - WORKAROUND: add arguments related to recurrent neural networks to the uninferable group as those\n        only have *args and **kwargs and are otherwise unparsable.\n\n\n    Parameters\n    ----------\n    init_arguments : List[str]\n        __init__ arguments gathered by `inspect.signature(cls).parameters`\n    module : type\n        `type` metaclass inheriting from `torch.nn.Module` and named like original\n        class (without shape inference)\n\n    Returns\n    -------\n    Tuple[List[str], Dict[str, Any]]\n        First item are arguments used for dynamic __init__ creation of inferable\n        module. Second item is dictionary where key is name of argument and value can be anything.\n        Those are uninferable arguments (not present in inferable __init__)\n        and used solely for inferable module\'s __repr__ creation.\n\n    """"""\n\n    def _add_rnn_uninferable(uninferable: typing.Dict, module) -> typing.Dict:\n        if module.__name__ in [""RNN"", ""LSTM"", ""GRU""]:\n            uninferable.update(\n                {\n                    ""input_size"": ""?"",\n                    ""num_layers"": 1,\n                    ""bias"": False,\n                    ""batch_first"": False,\n                    ""dropout"": 0.0,\n                    ""bidirectional"": False,\n                }\n            )\n        return uninferable\n\n    def _add_infered_shape_variable_name(init_arguments: str) -> typing.Dict:\n        first_argument = helpers.remove_right_side(\n            helpers.remove_type_hint(init_arguments[1])\n        )\n        if not helpers.is_vararg(first_argument) and not helpers.is_kwarg(\n            first_argument\n        ):\n            # Remove the argument to be inferred\n            init_arguments.pop(1)\n            return {first_argument: ""?""}\n\n        return {}\n\n    uninferable_arguments = _add_rnn_uninferable(\n        _add_infered_shape_variable_name(init_arguments), module\n    )\n\n    # Pop self from arguments always so it won\'t be class-assigned later\n    init_arguments.pop(0)\n    return (\n        [helpers.remove_type_hint(argument) for argument in init_arguments],\n        uninferable_arguments,\n    )\n\n\ndef create_init(parsed_init_arguments) -> typing.Callable:\n    """"""\n    Dynamically create inferable module\'s `__init__`.\n\n    Function has to be executed in `namespace` dictionary in order\n    to build it from parsed strings.\n\n    Function is string representation of constructor with proper inheritance\n    of `torch.nn.Module`.\n\n    If `*args` or `**kwargs` are encountered in original `__init__` they will\n    be saved for later  in `VARARGS_VARIABLE` and `KWARGS_VARIABLE` respectively.\n\n    They will be unpacked after all arguments during first `forward` call when\n    module is instantiated.\n\n    Parameters\n    ----------\n    parsed_init_arguments : List[str]\n        __init__ arguments parsed by `parse_arguments` function.\n\n    Returns\n    -------\n    Callable\n        Function being __init__ of uninstantiated module.\n\n    """"""\n    namespace = {}\n\n    joined_arguments = "", "".join(parsed_init_arguments)\n    function = """"""def __init__(self, {}):"""""".format(joined_arguments)\n    function += ""\\n super(type(self), self).__init__()\\n""\n\n    for argument in parsed_init_arguments:\n        no_defaults = helpers.remove_right_side(argument)\n\n        # Is vararg save for forward use\n        if helpers.is_vararg(no_defaults):\n            function += "" self.{} = {}\\n"".format(\n                VARARGS_VARIABLE, helpers.remove_vararg(no_defaults)\n            )\n        # Same for kwawrg\n        elif helpers.is_kwarg(no_defaults):\n            function += "" self.{} = {}\\n"".format(\n                KWARGS_VARIABLE, helpers.remove_kwarg(no_defaults)\n            )\n        # ""Normal"" variable, assign to self\n        else:\n            function += "" self.{0} = {0}\\n"".format(no_defaults)\n\n    exec(function, namespace)\n    return namespace[""__init__""]\n\n\ndef create_forward(\n    module, module_class, parsed_init_arguments, inference_index: int\n) -> typing.Callable:\n    """"""\n    Return forward function which instantiates module after first pass.\n\n    Based on module type either `input.shape[1]` or `input.shape[2]` (for RNNs)\n    will be gathered from input and used as first argument during `__init__`\n    call of instantiated module.\n    After that arguments saved in non-instantiated module will be passed, followed\n    by `varargs` and `kwargs` (if any).\n\n    Module will be fully usable after the first pass (has parameters, can be trained)\n    but it is highly advised to use `torchlayers.build` to remove any non-instantiated\n    module overlay.\n\n\n    Parameters\n    ----------\n    module : str\n        Name of variable where instantiated module will be saved. Usually equal to\n        global variable `MODULE`\n    module_class : torch.nn.Module class\n        Name of variable where `__class__` of module to be instantiated is kept.\n        Used to instantiate module only.\n    parsed_init_arguments : List[str]\n        __init__ arguments parsed by `parse_arguments` function. Used to get names\n        of variables saved in non-instantiated module to be passed to `__init__`\n        of to be instantiated module.\n\n    Returns\n    -------\n    Callable\n        Function being `forward` of uninstantiated module.\n\n    """"""\n\n    def _non_vararg_kwarg(arguments):\n        return (\n            helpers.remove_right_side(argument)\n            for argument in arguments\n            if not helpers.is_vararg(argument) and not helpers.is_kwarg(argument)\n        )\n\n    def forward(self, inputs, *args, **kwargs):\n        infered_module = getattr(self, module, None)\n        if infered_module is None:\n            module_cls = getattr(self, module_class)\n            # All arguments from non-instantiated module\n            init_args = [\n                getattr(self, argument)\n                for argument in _non_vararg_kwarg(parsed_init_arguments)\n                # List of varargs (if any)\n            ] + list(getattr(self, VARARGS_VARIABLE, ()))\n            # Kwargs to be unpacked (if any)\n            init_kwargs = getattr(self, KWARGS_VARIABLE, {})\n\n            self.add_module(\n                module,\n                module_cls(\n                    # Shape to be inferred. Either 1 for all modules or 2 for RNNs\n                    inputs.shape[inference_index],\n                    *init_args,\n                    **init_kwargs\n                ),\n            )\n\n            infered_module = getattr(self, module)\n\n        return infered_module(inputs, *args, **kwargs)\n\n    return forward\n\n\ndef create_repr(module, **non_inferable_names) -> typing.Callable:\n    """"""\n    Uninstantiated module representation.\n\n    Representation is gathered from saved variables names and values of module\n    to be instantiated, saved `*args` variables (if any) and saved `**kwargs` variables.\n\n    Additionally names not saved in module (e.g. `input_shape` to be inferred)\n    are added at the beginning. Usually their value is ""?"", RNNs being and edge\n    case with all their arguments in `non_inferable_names` group\n    (and except the ones provided explicitly by user).\n\n\n    Parameters\n    ----------\n    module : str\n        Name of variable where instantiated module will be saved. Usually equal to\n        global variable `MODULE`\n    **non_inferable_names : Dict[str, Any]\n        Non-inferable names and their respective values of the module\n\n    Returns\n    -------\n    Callable\n        Function being `__repr__` of uninstantiated module.\n\n    """"""\n\n    def __repr__(self) -> str:\n        infered_module = getattr(self, module, None)\n        if infered_module is None:\n            parameters = "", "".join(\n                argument_representation\n                for argument_representation in helpers.create_vars(\n                    self, non_inferable_names, VARARGS_VARIABLE, KWARGS_VARIABLE\n                )\n            )\n            return ""{}({})"".format(type(self).__name__, parameters)\n\n        return repr(infered_module)\n\n    return __repr__\n\n\ndef create_getattr(module) -> typing.Callable:\n    """"""\n    Create __getattr__ of uninstantiated module.\n\n    Will return values from `uninstantiated` network if exist, if not\n    will check it\'s `instantiated` network (if it exists), otherwise\n    return `NoAttributeError`.\n\n    Can be considered proxy passing user calls `module` after instantiation.\n\n    Parameters\n    ----------\n    module : str\n        Name of variable where instantiated module will be saved. Usually equal to\n        global variable `MODULE`\n\n    Returns\n    -------\n    Callable\n        __getattr__ function\n    """"""\n\n    def __getattr__(self, name) -> str:\n        if name == module:\n            return super(type(self), self).__getattr__(name)\n        return getattr(getattr(self, module), name)\n\n    return __getattr__\n\n\n# FIXED FOR PyTorch 1.4.0, 1.2.0 should work fine as well although it may throw warnings\n\n# For warning regarding inability to find source code\n# https://github.com/pytorch/pytorch/blob/master/torch/_utils_internal.py#L44\n\n# getsourcefile\n# https://github.com/python/cpython/blob/master/Lib/inspect.py#L692\n# getfile\n# https://github.com/python/cpython/blob/master/Lib/inspect.py#L654\n# Simulate self.__module__.__file__ variable appropriately\n\n# getsourcelines\n# https://github.com/python/cpython/blob/master/Lib/inspect.py#L958\n\n\ndef create_reduce(module, parsed_init_arguments):\n    """"""\n    Create __reduce__ of uninstantiated module.\n\n    This is one of core functionalities. Using custom `__reduce__` modules\n    are reduced to their instantiated (after shape inference) versions.\n\n    Due to this operation and `torchlayers.build` zero-overhead of shape\n    inference is provided and support of `torchscript`.\n\n    If the module was not instantiated `ValueError` error is thrown.\n\n    Parameters\n    ----------\n    module : str\n        Name of variable where instantiated module will be saved. Usually equal to\n        global variable `MODULE`\n\n    parsed_init_arguments : List[str]\n        __init__ arguments parsed by `parse_arguments` function. Used to get names\n        of variables saved in non-instantiated module to be passed to `__init__`\n        of to be instantiated module.\n\n    Returns\n    -------\n    Callable\n        __getattr__ function\n    """"""\n\n    def __reduce__(self):\n        infered_module = getattr(self, module, None)\n        if infered_module is None:\n            raise ValueError(\n                ""Model cannot be pickled as it wasn\'t instantiated. Pass example input through this module.""\n            )\n\n        custom_reduce = getattr(infered_module, ""__reduce__"", None)\n        if custom_reduce is None:\n            raise ValueError(\n                ""Infered module does not have a __reduce__ method. Does it inherit from torch.nn.Module?""\n            )\n        return custom_reduce()\n\n    return __reduce__\n'"
docs/code/source/conf.py,1,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nimport pytorch_sphinx_theme\n\n\ndef get_version():\n    namespace = {}\n\n    exec(open(""../../../torchlayers/_version.py"").read(), namespace)  # get version\n    return namespace[""__version__""]\n\n\nsys.path.insert(0, os.path.abspath(""../../..""))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = ""torchlayers""\ncopyright = ""2019, Szymon Maszke""\nauthor = ""Szymon Maszke""\nversion = get_version()\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.autosummary"",\n    ""sphinx.ext.doctest"",\n    ""sphinx.ext.intersphinx"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.napoleon"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.autosectionlabel"",\n    ""sphinxcontrib.katex"",\n    ""javasphinx"",\n]\n\nautosectionlabel_prefix_document = True\n\nsource_suffix = "".rst""\n\n# The master toctree document.\nmaster_doc = ""index""\n\nintersphinx_mapping = {\n    ""python"": (""https://docs.python.org/"", None),\n    ""numpy"": (""https://docs.scipy.org/doc/numpy/"", None),\n    ""pytorch"": (""https://pytorch.org/docs/stable/"", None),\n}\n\nkatex_prerender = True\n\nnapoleon_use_ivar = True\n\n# # Called automatically by Sphinx, making this `conf.py` an ""extension"".\ndef setup(app):\n    # NOTE: in Sphinx 1.8+ `html_css_files` is an official configuration value\n    # and can be moved outside of this function (and the setup(app) function\n    # can be deleted).\n    html_css_files = [\n        ""https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css""\n    ]\n\n    # In Sphinx 1.8 it was renamed to `add_css_file`, 1.7 and prior it is\n    # `add_stylesheet` (deprecated in 1.8).\n    add_css = getattr(app, ""add_css_file"", app.add_stylesheet)\n    for css_file in html_css_files:\n        add_css(css_file)\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""pytorch_sphinx_theme""\nhtml_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]\n\nhtml_theme_options = {\n    ""related"": ""https://szymonmaszke.github.io/torchlayers/related.html"",\n    ""roadmap"": ""https://github.com/szymonmaszke/torchlayers/blob/master/ROADMAP.md"",\n    ""github_issues"": ""https://github.com/szymonmaszke/torchlayers/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc"",\n    ""home"": ""https://szymonmaszke.github.io/torchlayers"",\n    ""installation"": ""https://szymonmaszke.github.io/torchlayers/#installation"",\n    ""github"": ""https://github.com/szymonmaszke/torchlayers"",\n    ""docs"": ""https://szymonmaszke.github.io/torchlayers/#torchlayers"",\n    ""collapse_navigation"": False,\n    ""display_version"": True,\n    ""logo_only"": False,\n    ""canonical_url"": ""https://szymonmaszke.github.io/torchlayers/"",\n}\n\n# Other settings\n\ndefault_role = ""py:obj""  # Reference to Python by default\n'"
