file_path,api_count,code
new_prune.py,20,"b'import torch\nimport argparse\nfrom yolomodel import *\nimport torch.nn.functional as F\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=""YOLO v3 Prune"")\n    parser.add_argument(""--cfg"",dest=""cfgfile"",help=""\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b"",\n                        default=r""D:/yolotest/cfg/yolov3.cfg"",type=str)\n    parser.add_argument(""--weights"",dest=""weightsfile"",help=""\xe6\x9d\x83\xe9\x87\x8d\xe6\x96\x87\xe4\xbb\xb6"",\n                        default=r""D:/yolotest/yolov3.weights"",type=str)\n    parser.add_argument(\'--percent\', type=float, default=0.5,help=\'\xe5\x89\xaa\xe6\x9e\x9d\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\')\n    return parser.parse_args()\n\n\n#alpha = 0.1\nargs = arg_parse()\nstart = 0\nCUDA = torch.cuda.is_available()\nprint(""load network"")\nmodel = Darknet(args.cfgfile)\nprint(""done!"")\nprint(""load weightsfile"")\nmodel.load_weights(args.weightsfile)\nif CUDA:\n    model.cuda()\n# \xe6\xa0\xb9\xe6\x8d\xaeshortcut\xe6\x89\xbe\xe5\x88\xb0\xe4\xb8\x8d\xe5\xba\x94\xe8\xaf\xa5\xe8\xa2\xab\xe8\xa3\x81\xe7\x9a\x84\xef\xbc\x8c\xe5\xb9\xb6\xe8\xae\xb0\xe5\xbd\x95\xe5\x85\xb6\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x8c\xe6\x94\xbe\xe5\x9c\xa8donntprune\xe4\xb8\xad\nnnlist = model.module_list\ndonntprune = dontprune(model)\ntotal = 0\nfor i in range(len(nnlist)):\n    if \'conv_with_bn\' in list(nnlist[i].named_children())[0][0]:\n        if i not in donntprune:\n            total += list(nnlist[i].named_children())[1][1].weight.data.shape[0]\nbn = torch.zeros(total)\nindex = 0\nfor i in range(len(nnlist)):\n    if \'conv_with_bn\' in list(nnlist[i].named_children())[0][0]:\n        if i not in donntprune:\n            size = list(nnlist[i].named_children())[1][1].weight.data.shape[0]\n            bn[index:(index+size)] = list(nnlist[i].named_children())[1][1].weight.data.abs().clone()\n            index += size\ny, i = torch.sort(bn)\nthre_index = int(total * args.percent)\nthre = y[thre_index].cuda()\nprint(y)\npruned = 0\ncfg = []\ncfg_mask = []\nprint(thre)\nprint(\'--\'*30)\nprint(""Pre-processing..."")\n# \xe5\xa4\x84\xe7\x90\x86bias\xe5\x80\xbc\nremain_bias_list = dict()\nfor i in range(len(nnlist)):\n    if i not in donntprune:\n        for name in nnlist[i].named_children():\n            if ""_"".join(name[0].split(""_"")[0:-1]) == \'batch_norm\':\n                weight_copy = name[1].weight.data.abs().clone()\n                mask = weight_copy.gt(thre).float().cuda()  # \xe6\x8e\xa9\xe6\xa8\xa1\n                if int(torch.sum(mask)) == 0: # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xaf\xa5\xe5\xb1\x82\xe6\x89\x80\xe6\x9c\x89\xe9\x83\xbd\xe8\xa2\xab\xe5\x89\xaa\xe6\x8e\x89\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\n                    mask[int(torch.argmax(weight_copy))]=1.\n                pruned = pruned + mask.shape[0] - torch.sum(mask)\n                name[1].weight.data.mul_(mask)  # \xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbf\xae\xe6\x94\xb9\xce\xb3\xef\xbc\x8c\n                cfg.append(int(torch.sum(mask)))\n                cfg_mask.append(mask.clone())\n                print(\'layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}\'.\n                          format(i, mask.shape[0], int(torch.sum(mask))))\n                bias_mask = torch.ones_like(mask) - mask\n                remain_bias = bias_mask*name[1].bias.data\n                remain_bias_list[i] = remain_bias\n                for next_name in nnlist[i+1].named_children():\n                    if ""_"".join(next_name[0].split(""_"")[0:-1]) == \'conv_with_bn\':\n                        activations = torch.mm(F.relu(remain_bias).view(1,-1),next_name[1].weight.data.sum(dim = [2,3]).transpose(1,0).contiguous())\n                        mean = nnlist[i+1][1].running_mean-activations\n                        mean = mean.view(-1)\n                        nnlist[i + 1][1].running_mean = mean\n                    elif ""_"".join(next_name[0].split(""_"")[0:-1]) == \'conv_without_bn\':\n                        activations = torch.mm(F.relu(remain_bias).view(1,-1),next_name[1].weight.data.sum(dim = [2,3]).transpose(1,0).contiguous())\n                        bias = next_name[1].bias.data + activations\n                        bias = bias.view(-1)\n                        next_name[1].bias.data = bias\n                    elif next_name[0].split(""_"")[0] == \'maxpool\':\n                        activations = torch.mm(F.relu(remain_bias).view(1, -1) , nnlist[i + 2][0].weight.sum(dim=[2, 3]).transpose(1,0).contiguous())\n                        mean = nnlist[i + 2][1].running_mean - activations\n                        mean = mean.view(-1)\n                        nnlist[i + 2][1].running_mean = mean\n                    elif next_name[0].split(""_"")[0] == \'reorg\':\n                        stride = next_name[1].stride\n                        remain_bias_list[i+1] = torch.squeeze(remain_bias.expand(int(stride*stride),int(remain_bias.size(0))).transpose(1,0).contiguous().view(1,-1))\n            elif name[0].split(""_"")[0] == \'route\':\n                prev_1 = name[1].layers[0]+i\n                have_prev_2 = False\n                if name[1].layers[1] !=0:\n                    prev_2 = name[1].layers[1] + i\n                    have_prev_2 = True\n                if isinstance(nnlist[prev_1][0],nn.Conv2d):\n                    if not have_prev_2:\n                        remain_bias = remain_bias_list[prev_1]\n                    else:\n                        remain_bias = torch.cat((remain_bias_list[prev_1],remain_bias_list[prev_2]),0)\n                    activations = torch.mm(F.relu(remain_bias).view(1, -1),nnlist[i + 1][0].weight.sum(dim=[2, 3]).transpose(1, 0).contiguous())\n                    mean = nnlist[i + 1][1].running_mean - activations\n                    mean = mean.view(-1)\n                    nnlist[i + 1][1].running_mean = mean\n    else:\n        for name in nnlist[i].named_children():\n            if ""_"".join(name[0].split(""_"")[0:-1]) == \'batch_norm\':\n                dontp = name[1].weight.data.numel()\n                mask = torch.ones(name[1].weight.data.shape)\n                print(\'layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}\'.\n                            format(i, dontp, int(dontp)))\n                cfg.append(int(dontp))\n                cfg_mask.append(mask.clone())\n\npruned_ratio = pruned/total\nprint(\'Pre-processing Successful!\')\nprint(\'--\'*30)\n#print(cfg)\n# \xe5\x86\x99\xe5\x87\xba\xe8\xa2\xab\xe5\x87\x8f\xe6\x9e\x9d\xe7\x9a\x84cfg\xe6\x96\x87\xe4\xbb\xb6\nprunecfg = write_cfg(args.cfgfile,cfg)\nnewmodel = Darknet(prunecfg)\nnewmodel.header_info = model.header_info\nif CUDA:\n    newmodel.cuda()\nold_modules = list(model.modules())\nnew_modules = list(newmodel.modules())\nlayer_id_in_cfg = 0\nstart_mask = torch.ones(3)\nend_mask = cfg_mask[layer_id_in_cfg]\nprint(""pruning..."")\nv=0\nfor layer_id in range(len(old_modules)):\n    m0 = old_modules[layer_id]\n    m1 = new_modules[layer_id]\n    if isinstance(m0, nn.BatchNorm2d):# \xe5\x90\x91\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe5\x86\x99\xe5\x85\xa5\n        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n        if idx1.size == 1:\n            idx1 = np.resize(idx1,(1,))\n        m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n        m1.bias.data = m0.bias.data[idx1.tolist()].clone() #\xe5\x8e\xbb\xe6\x8e\x89\xe7\x9a\x84bias\xe5\xaf\xbc\xe8\x87\xb4\xe7\xb2\xbe\xe5\xba\xa6\xe5\xa4\xa7\xe9\x87\x8f\xe4\xb8\x8b\xe9\x99\x8d\n        m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n        m1.running_var = m0.running_var[idx1.tolist()].clone()\n        layer_id_in_cfg += 1\n        start_mask = end_mask.clone()\n        if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n            end_mask = cfg_mask[layer_id_in_cfg]\n    elif isinstance(m0, nn.Sequential):\n        for name in m0.named_children():\n            if name[0].split(""_"")[0] == \'route\':\n                ind = v+old_modules[layer_id + 1].layers[0]\n                cfg_mask1 = cfg_mask[route_problem(model, ind)]\n                if old_modules[layer_id + 1].layers[1]!=0:\n                    ind =v + old_modules[layer_id + 1].layers[1]\n                    cfg_mask1 = cfg_mask1.unsqueeze(0)\n                    cfg_mask2 = cfg_mask[route_problem(model, ind)].unsqueeze(0).cuda()\n                    cfg_mask3 = torch.cat((cfg_mask1,cfg_mask2),1)\n                    cfg_mask1 = cfg_mask3.squeeze(0)\n                start_mask = cfg_mask1.clone()\n            elif name[0].split(""_"")[0] == \'reorg\':\n                stride = name[1].stride\n                cfg_mask[layer_id_in_cfg - 1] = torch.squeeze(\n                    start_mask.expand(int(stride * stride), int(start_mask.size(0))).transpose(1, 0).contiguous().view(\n                        1, -1))\n            elif ""_"".join(name[0].split(""_"")[0:-1]) == \'conv_with_bn\':\n                idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n                idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n                print(\'Conv In shape: {:d}, Out shape {:d}.\'.format(idx0.size, idx1.size))\n                if idx0.size == 1:\n                    idx0 = np.resize(idx0, (1,))\n                if idx1.size == 1:\n                    idx1 = np.resize(idx1, (1,))\n                w1 = old_modules[layer_id + 1].weight.data[:, idx0.tolist(), :, :].clone()\n                w1 = w1[idx1.tolist(), :, :, :].clone()\n                new_modules[layer_id + 1].weight.data = w1.clone()\n            elif ""_"".join(name[0].split(""_"")[0:-1]) == \'conv_without_bn\':\n                idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n                w1 = old_modules[layer_id + 1].weight.data[:, idx0.tolist(), :, :].clone()\n                new_modules[layer_id + 1].weight.data = w1.clone()\n                new_modules[layer_id + 1].bias.data = old_modules[layer_id + 1].bias.data.clone()\n                print(\'Detect: In shape: {:d}, Out shape {:d}.\'.format(new_modules[layer_id + 1].weight.data.size(1),\n                      new_modules[layer_id + 1].weight.data.size(0)))\n        v=v+1\n    elif isinstance(m0, nn.Linear):\n            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n            if idx0.size == 1:\n                idx0 = np.resize(idx0, (1,))\n            m1.weight.data = m0.weight.data[:, idx0].clone()\n            m1.bias.data = m0.bias.data.clone()\nprint(\'--\'*30)\nprint(\'prune done!\')\nprint(\'pruned ratio %.3f\'%pruned_ratio)\nprunedweights = os.path.join(\'\\\\\'.join(args.weightsfile.split(""/"")[0:-1]),""prune_""+args.weightsfile.split(""/"")[-1])\nprint(\'save weights file in %s\'%prunedweights)\nnewmodel.save_weights(prunedweights)\nprint(\'done!\')\n\n\n'"
parse_config.py,4,"b'import glob\nimport random\nimport os\nimport numpy as np\n\nimport torch\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom skimage.transform import resize\n\nimport sys\n\ndef parse_data_config(path):\n    """"""Parses the data configuration file""""""\n    options = dict()\n    options[\'gpus\'] = \'0,1,2,3\'\n    options[\'num_workers\'] = \'10\'\n    with open(path, \'r\') as fp:\n        lines = fp.readlines()\n    for line in lines:\n        line = line.strip()\n        if line == \'\' or line.startswith(\'#\'):\n            continue\n        key, value = line.split(\'=\')\n        options[key.strip()] = value.strip()\n    return options\n\n\n\nclass ListDataset(Dataset):\n    def __init__(self, list_path, img_size=416):\n        with open(list_path, \'r\') as file:\n            self.img_files = file.readlines()\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x9b\xbe\xe7\x89\x87\xe5\xad\x98\xe6\x94\xbelabel\xe7\x9a\x84txt\n        self.label_files = [path.replace(\'images\', \'labels\').replace(\'.png\', \'.txt\').replace(\'.jpg\', \'.txt\') for path in self.img_files]\n        self.img_shape = (img_size, img_size)\n        self.max_objects = 50\n\n    def __getitem__(self, index):\n\n        #---------\n        #  Image\n        #---------\n\n        img_path = self.img_files[index % len(self.img_files)].rstrip()\n        img = np.array(Image.open(img_path))\n\n        # Handles images with less than three channels\n        while len(img.shape) != 3:\n            index += 1\n            img_path = self.img_files[index % len(self.img_files)].rstrip()\n            img = np.array(Image.open(img_path))\n\n        h, w, _ = img.shape\n        dim_diff = np.abs(h - w)\n        # Upper (left) and lower (right) padding\n        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n        # Determine padding\n        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n        # Add padding\n        input_img = np.pad(img, pad, \'constant\', constant_values=128) / 255.\n        padded_h, padded_w, _ = input_img.shape\n        # Resize and normalize\n        input_img = resize(input_img, (*self.img_shape, 3), mode=\'reflect\')\n        # Channels-first\n        input_img = np.transpose(input_img, (2, 0, 1))\n        # As pytorch tensor\n        input_img = torch.from_numpy(input_img).float()\n\n        #---------\n        #  Label  #\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xaf\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe3\x80\x81\xe4\xb8\x89\xe4\xb8\xaa\xe6\x98\xaf\xe6\xa1\x86\xe4\xb8\xad\xe5\xbf\x83xy\xef\xbc\x8c\xe7\xac\xac\xe5\x9b\x9b\xe4\xba\x94\xe4\xb8\xaa\xe6\x98\xaf\xe9\x95\xbf\xe5\xae\xbd\n        #---------\n\n        label_path = self.label_files[index % len(self.img_files)].rstrip()\n\n        labels = None\n        if os.path.exists(label_path):\n            labels = np.loadtxt(label_path).reshape(-1, 5)\n            # Extract coordinates for unpadded + unscaled image\n            x1 = w * (labels[:, 1] - labels[:, 3]/2)\n            y1 = h * (labels[:, 2] - labels[:, 4]/2)\n            x2 = w * (labels[:, 1] + labels[:, 3]/2)\n            y2 = h * (labels[:, 2] + labels[:, 4]/2)\n            # Adjust for added padding\n            x1 += pad[1][0]\n            y1 += pad[0][0]\n            x2 += pad[1][0]\n            y2 += pad[0][0]\n            # Calculate ratios from coordinates\n            labels[:, 1] = ((x1 + x2) / 2) / padded_w  #\n            labels[:, 2] = ((y1 + y2) / 2) / padded_h\n            labels[:, 3] *= w / padded_w\n            labels[:, 4] *= h / padded_h\n        # Fill matrix\n        filled_labels = np.zeros((self.max_objects, 5))\n        if labels is not None:\n            filled_labels[range(len(labels))[:self.max_objects]] = labels[:self.max_objects]\n        filled_labels = torch.from_numpy(filled_labels)\n\n        return img_path, input_img, filled_labels\n\n    def __len__(self):\n        return len(self.img_files)\n\n\n\nclass ImageFolder(Dataset):\n    def __init__(self, folder_path, img_size=416):\n        self.files = sorted(glob.glob(\'%s/*.*\' % folder_path))\n        self.img_shape = (img_size, img_size)\n\n    def __getitem__(self, index):\n        img_path = self.files[index % len(self.files)]\n        # Extract image\n        img = np.array(Image.open(img_path))\n        h, w, _ = img.shape\n        dim_diff = np.abs(h - w)\n        # Upper (left) and lower (right) padding\n        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n        # Determine padding\n        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n        # Add padding\n        input_img = np.pad(img, pad, \'constant\', constant_values=127.5) / 255.\n        # Resize and normalize\n        input_img = resize(input_img, (*self.img_shape, 3), mode=\'reflect\')\n        # Channels-first\n        input_img = np.transpose(input_img, (2, 0, 1))\n        # As pytorch tensor\n        input_img = torch.from_numpy(input_img).float()\n\n        return img_path, input_img\n\n    def __len__(self):\n        return len(self.files)\n'"
prune.py,12,"b'from __future__ import division\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom util import *\nimport argparse\nimport os\nfrom yolomodel import Darknet\nfrom yolomodel import shortcutLayer\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=""YOLO v3 Prune"")\n    parser.add_argument(""--cfg"",dest=""cfgfile"",help=""\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b"",\n                        default=r""D:/yolotest/cfg/yolov3.cfg"",type=str)\n    parser.add_argument(""--weights"",dest=""weightsfile"",help=""\xe6\x9d\x83\xe9\x87\x8d\xe6\x96\x87\xe4\xbb\xb6"",\n                        default=r""D:/yolotest/cfg/yolov3.weights"",type=str)\n    parser.add_argument(\'--percent\', type=float, default=0.3,help=\'\xe5\x89\xaa\xe6\x9e\x9d\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\')\n    return parser.parse_args()\n\n\nargs = arg_parse()\nstart = 0\nCUDA = torch.cuda.is_available()\nprint(""load network"")\nmodel = Darknet(args.cfgfile)\nprint(""done!"")\nprint(""load weightsfile"")\nmodel.load_weights(args.weightsfile)\nif CUDA:\n    model.cuda()\n# \xe6\xa0\xb9\xe6\x8d\xaeshortcut\xe6\x89\xbe\xe5\x88\xb0\xe4\xb8\x8d\xe5\xba\x94\xe8\xaf\xa5\xe8\xa2\xab\xe8\xa3\x81\xe7\x9a\x84\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe5\xb9\xb6\xe8\xae\xb0\xe5\xbd\x95\xe5\x85\xb6\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x8c\xe6\x94\xbe\xe5\x9c\xa8donntprune\xe4\xb8\xad\ndonntprune = []\nfor k,m in enumerate(model.modules()):\n    if isinstance(m, shortcutLayer):\n        x= k+m.froms-8\n        donntprune.append(x)\n        x = k-3\n        donntprune.append(x)\n#print(donntprune)\n#\xe7\xbb\x9f\xe8\xae\xa1\xe6\x89\x80\xe6\x9c\x89\xe5\xba\x94\xe8\xaf\xa5\xe8\xa2\xab\xe8\xa3\x81\xe7\x9a\x84\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe6\x80\xbb\xe5\xa4\xa7\xe5\xb0\x8f\ntotal = 0\nfor k,m in enumerate(model.modules()):\n     if isinstance(m, nn.BatchNorm2d):\n         if k not in donntprune:\n            total += m.weight.data.shape[0]\n#print(total)\nbn = torch.zeros(total)\n#print(bn)\nindex = 0\nfor k,m in enumerate(model.modules()):\n    if isinstance(m, nn.BatchNorm2d):\n        if k not in donntprune:\n            size = m.weight.data.shape[0]\n            bn[index:(index+size)] = m.weight.data.abs().clone()\n            index += size\ny, i = torch.sort(bn)# y,i\xe6\x98\xaf\xe4\xbb\x8e\xe5\xb0\x8f\xe5\x88\xb0\xe5\xa4\xa7\xe6\x8e\x92\xe5\x88\x97\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84bn\xef\xbc\x8cy\xe6\x98\xafweight\xef\xbc\x8ci\xe6\x98\xaf\xe5\xba\x8f\xe5\x8f\xb7\nthre_index = int(total * args.percent)\nthre = y[thre_index].cuda()\npruned = 0\ncfg = []\ncfg_mask = []\nprint(\'--\'*30)\nprint(""Pre-processing..."")\nfor k, m in enumerate(model.modules()):\n    #isinstance()\xe5\x87\xbd\xe6\x95\xb0\xe6\x9d\xa5\xe5\x88\xa4\xe6\x96\xad\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb7\xb2\xe7\x9f\xa5\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\n    if isinstance(m, nn.BatchNorm2d):\n        if k not in donntprune:\n            weight_copy = m.weight.data.abs().clone()\n            mask = weight_copy.gt(thre).float().cuda()  # \xe6\x8e\xa9\xe6\xa8\xa1\n            pruned = pruned + mask.shape[0] - torch.sum(mask)\n            m.weight.data.mul_(mask)# \xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbf\xae\xe6\x94\xb9m\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe6\x94\xb9\xe4\xba\x86model\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe5\xb9\xb6\xe6\x94\xbe\xe5\x9c\xa8\xe4\xba\x86model\xe4\xb8\xad\n            m.bias.data.mul_(mask)\n            cfg.append(int(torch.sum(mask)))\n            cfg_mask.append(mask.clone())\n            print(\'layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}\'.\n                    format(k, mask.shape[0], int(torch.sum(mask))))\n        else:\n            dontp = m.weight.data.numel()\n            mask = torch.ones(m.weight.data.shape)\n            print(\'layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}\'.\n                    format(k, dontp, int(dontp)))\n            cfg.append(int(dontp))\n            cfg_mask.append(mask.clone())\npruned_ratio = pruned/total\nprint(\'Pre-processing Successful!\')\nprint(\'--\'*30)\n#print(cfg)\n# \xe5\x86\x99\xe5\x87\xba\xe8\xa2\xab\xe5\x87\x8f\xe6\x9e\x9d\xe7\x9a\x84cfg\xe6\x96\x87\xe4\xbb\xb6\nprunecfg = write_cfg(args.cfgfile,cfg)\nnewmodel = Darknet(prunecfg)\nnewmodel.header_info = model.header_info\nif CUDA:\n    newmodel.cuda()\nold_modules = list(model.modules())\nnew_modules = list(newmodel.modules())\nlayer_id_in_cfg = 0\nstart_mask = torch.ones(3)\nend_mask = cfg_mask[layer_id_in_cfg]\nprint(""pruning..."")\nv=0\nfor layer_id in range(len(old_modules)):\n    m0 = old_modules[layer_id]\n    m1 = new_modules[layer_id]\n    #print(m1)\n    if isinstance(m0, nn.BatchNorm2d):# \xe5\x90\x91\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe5\x86\x99\xe5\x85\xa5\n        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n        if idx1.size == 1:\n            idx1 = np.resize(idx1,(1,))\n        #print(idx1.size)\n        m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n        m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n        m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n        m1.running_var = m0.running_var[idx1.tolist()].clone()\n        layer_id_in_cfg += 1\n        start_mask = end_mask.clone()\n        if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n            end_mask = cfg_mask[layer_id_in_cfg]\n    elif isinstance(m0, nn.Sequential):\n        for name in m0.named_children():\n            if name[0].split(""_"")[0] == \'route\':\n                #print(old_modules[layer_id + 1].layers)\n                #print(m0)\n                ind = v+old_modules[layer_id + 1].layers[0]\n                #print(ind)\n                cfg_mask1 = cfg_mask[route_problem(model, ind)]\n                #print(cfg_mask1.shape)\n                if old_modules[layer_id + 1].layers[1]!=0:\n                    ind =v + old_modules[layer_id + 1].layers[1]\n                    #print(ind)\n                    cfg_mask1 = cfg_mask1.unsqueeze(0)\n                    #print(cfg_mask1.shape)\n                    cfg_mask2 = cfg_mask[route_problem(model, ind)].unsqueeze(0).cuda()\n                    #print(cfg_mask2.shape)\n                    cfg_mask3 = torch.cat((cfg_mask1,cfg_mask2),1)\n                    #print(cfg_mask3.shape)\n                    cfg_mask1 = cfg_mask3.squeeze(0)\n                    #print(cfg_mask1.shape)\n                start_mask = cfg_mask1.clone()\n            elif name[0].split(""_"")[0] == \'reorg\':\n                stride = name[1].stride\n                cfg_mask[layer_id_in_cfg-1] = torch.squeeze(start_mask.expand(int(stride*stride),int(start_mask.size(0))).transpose(1,0).contiguous().view(1,-1))\n            elif ""_"".join(name[0].split(""_"")[0:-1]) == \'conv_with_bn\':\n                idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n                idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n                print(\'Conv In shape: {:d}, Out shape {:d}.\'.format(idx0.size, idx1.size))\n                if idx0.size == 1:\n                    idx0 = np.resize(idx0, (1,))\n                if idx1.size == 1:\n                    idx1 = np.resize(idx1, (1,))\n                w1 = old_modules[layer_id + 1].weight.data[:, idx0.tolist(), :, :].clone()\n                w1 = w1[idx1.tolist(), :, :, :].clone()\n                new_modules[layer_id + 1].weight.data = w1.clone()\n            elif ""_"".join(name[0].split(""_"")[0:-1]) == \'conv_without_bn\':\n                idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n                w1 = old_modules[layer_id + 1].weight.data[:, idx0.tolist(), :, :].clone()\n                new_modules[layer_id + 1].weight.data = w1.clone()\n                new_modules[layer_id + 1].bias.data = old_modules[layer_id + 1].bias.data.clone()\n                #print(new_modules[layer_id + 1].weight.data.size())\n                print(\'Detect: In shape: {:d}, Out shape {:d}.\'.format(new_modules[layer_id + 1].weight.data.size(1),\n                      new_modules[layer_id + 1].weight.data.size(0)))\n        v=v+1\n    elif isinstance(m0, nn.Linear):\n            idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n            if idx0.size == 1:\n                idx0 = np.resize(idx0, (1,))\n            m1.weight.data = m0.weight.data[:, idx0].clone()\n            m1.bias.data = m0.bias.data.clone()\nprint(\'--\'*30)\nprint(\'prune done!\')\nprint(\'pruned ratio %.3f\'%pruned_ratio)\nprunedweights = os.path.join(\'\\\\\'.join(args.weightsfile.split(""/"")[0:-1]),""prune_""+args.weightsfile.split(""/"")[-1])\nprint(\'save weights file in %s\'%prunedweights)\n#\xe4\xbf\x9d\xe5\xad\x98\xe6\x96\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9d\x83\xe9\x87\x8d\nnewmodel.save_weights(prunedweights)\nprint(\'done!\')\n'"
sparsity_train.py,11,"b'from __future__ import division\n\nfrom yolomodel import *\nfrom util  import *\nfrom parse_config import *\nimport os\nimport argparse\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\n\n\ndef arg_parse():\n    parser = argparse.ArgumentParser(description=""YOLO v3 Train"")\n    parser.add_argument(""--image_folder"", type=str, default=r""D:\\yolotest\\data\\coco.data"", help=""path to dataset"")\n    parser.add_argument(""--epochs"",dest=""epochs"",help=""epochs"",default=2000)\n    parser.add_argument(""--cfg"",dest=""cfgfile"",help=""\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b"",\n                        default=r""D:/yolotest/cfg/yolov3.cfg"",type=str)\n    parser.add_argument(""--weights"",dest=""weightsfile"",help=""\xe6\x9d\x83\xe9\x87\x8d\xe6\x96\x87\xe4\xbb\xb6"",\n                        default=r""D:/yolotest/cfg/yolov3.weights"",type=str)\n    parser.add_argument(""--reso"", dest=\'reso\', help=""resize\xe5\x9b\xbe\xe7\x89\x87\xe5\xa4\xa7\xe5\xb0\x8f"",\n                        default=""416"", type=str)\n    parser.add_argument(""--n_cpu"",dest=\'n_cpu\',type=int,default=2,help=""torch\xe5\xa4\x9a\xe7\xba\xbf\xe7\xa8\x8b\xe6\xa0\xb8\xe6\x95\xb0"")\n    parser.add_argument(""--use_cuda"", type=bool, default=True, help=""whether to use cuda if available"")\n    parser.add_argument(""-sr"", dest=\'sr\', action=\'store_true\',\n                    help=\'train with channel sparsity regularization\')\n    parser.add_argument(\'--s\', type=float, default=0.0001,\n                        help=\'\xe7\xa8\x80\xe7\x96\x8f\xe5\x8c\x96\xe6\xaf\x94\xe7\x8e\x87\')\n    parser.add_argument(""--checkpoint_interval"", type=int, default=1, help=""interval between saving model weights"")\n    parser.add_argument(\n        ""--checkpoint_dir"", type=str, default=""checkpoints"", help=""directory where model checkpoints are saved""\n    )\n    parser.add_argument(""--alpha"",type=float,default=1.,help=""bn\xe5\xb1\x82\xe6\x94\xbe\xe7\xbc\xa9\xe7\xb3\xbb\xe6\x95\xb0"")\n    return parser.parse_args()\n\n# \xe5\x8f\xaa\xe7\xa8\x80\xe7\x96\x8f\xe5\x8c\x96\xe9\x9d\x9eshortcut\xe7\x9a\x84\xe5\xb1\x82\ndef updateBN(model,s,donntprune):\n    for k,m in enumerate(model.modules()):\n        if isinstance(m, nn.BatchNorm2d):\n            if k not in donntprune:\n                m.weight.grad.data.add_(s*torch.sign(m.weight.data))\n\n\ndef train():\n    args = arg_parse()\n    cuda = torch.cuda.is_available() and args.use_cuda\n    data_config = parse_data_config(args.image_folder)\n    train_path = data_config[""train""]\n    classes_path = data_config[""names""]\n    classes = load_classes(classes_path)\n    num_classes = len(classes)\n    alpha = args.alpha\n    os.makedirs(args.checkpoint_dir, exist_ok=True)\n\n    # Initiate model\n    print(""load network"")\n    model = Darknet(args.cfgfile)\n    print(""done!"")\n    print(""load weightsfile"")\n    model.load_weights(args.weightsfile)\n    # Get hyper parameters\n    hyperparams = model.blocks[0]\n    learning_rate = float(hyperparams[""learning_rate""])\n    momentum = float(hyperparams[""momentum""])\n    decay = float(hyperparams[""decay""])\n    burn_in = int(hyperparams[""burn_in""])\n    inp_dim = int(model.net_info[""height""])\n    batch_size = int(hyperparams[""batch""])\n    if cuda:\n        model = model.cuda()\n    model.train()\n    model = scale_gama(alpha,model,scale_down=True)\n    # Get dataloader\n    dataloader = torch.utils.data.DataLoader(\n        ListDataset(train_path,img_size=inp_dim), batch_size=batch_size, shuffle=False, num_workers=args.n_cpu\n    )\n    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n    #optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n    optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=momentum,weight_decay=decay)\n    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n    #\xe8\xae\xb0\xe5\xbd\x95\xe5\x93\xaa\xe4\xba\x9b\xe6\x98\xafshortcut\xe5\xb1\x82\n    donntprune = []\n    for k, m in enumerate(model.modules()):\n        if isinstance(m, shortcutLayer):\n            x = k + m.froms - 8\n            donntprune.append(x)\n            x = k - 3\n            donntprune.append(x)\n    # print(donntprune)\n\n    for epoch in range(args.epochs):\n        exp_lr_scheduler.step(epoch)\n        for batch_i, (_, imgs, targets) in enumerate(dataloader):\n            imgs = Variable(imgs.type(Tensor))\n            targets = Variable(targets.type(Tensor), requires_grad=False)\n            optimizer.zero_grad()\n            loss = model(imgs, targets)\n            loss.backward()\n            if args.sr:\n                updateBN(model,args.s,donntprune)\n            optimizer.step()\n            print(\n                ""[Epoch %d/%d, Batch %d/%d] [Losses: x %f, y %f, w %f, h %f, conf %f, cls %f, total %f, recall: %.5f, precision: %.5f]""\n                % (\n                    epoch,\n                    args.epochs,\n                    batch_i,\n                    len(dataloader),\n                    model.losses[""x""],\n                    model.losses[""y""],\n                    model.losses[""w""],\n                    model.losses[""h""],\n                    model.losses[""conf""],\n                    model.losses[""cls""],\n                    loss.item(),\n                    model.losses[""recall""],\n                    model.losses[""precision""],\n                )\n            )\n            model.seen += imgs.size(0)\n\n        if epoch % args.checkpoint_interval == 0:\n            if args.sr:\n                model.train(False)\n                total = 0\n                for k, m in enumerate(model.modules()):\n                    if isinstance(m, nn.BatchNorm2d):\n                        if k not in donntprune:\n                            total += m.weight.data.shape[0]\n                bn = torch.zeros(total)\n                index = 0\n                for k, m in enumerate(model.modules()):\n                    if isinstance(m, nn.BatchNorm2d):\n                        if k not in donntprune:\n                            size = m.weight.data.shape[0]\n                            bn[index:(index + size)] = m.weight.data.abs().clone()\n                            index += size\n                y, i = torch.sort(bn)  # y,i\xe6\x98\xaf\xe4\xbb\x8e\xe5\xb0\x8f\xe5\x88\xb0\xe5\xa4\xa7\xe6\x8e\x92\xe5\x88\x97\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84bn\xef\xbc\x8cy\xe6\x98\xafweight\xef\xbc\x8ci\xe6\x98\xaf\xe5\xba\x8f\xe5\x8f\xb7\n                number = int(len(y)/5)  # \xe5\xb0\x86\xe6\x80\xbb\xe7\xb1\xbb\xe5\x88\x86\xe4\xb8\xba5\xe7\xbb\x84\n                # \xe8\xbe\x93\xe5\x87\xba\xe7\xa8\x80\xe7\x96\x8f\xe5\x8c\x96\xe6\xb0\xb4\xe5\xb9\xb3\n                print(""0~20%%:%f,20~40%%:%f,40~60%%:%f,60~80%%:%f,80~100%%:%f""%(y[number],y[2*number],y[3*number],y[4*number],y[-1]))\n                model.train()\n            model = scale_gama(alpha, model, scale_down=False)\n            model.save_weights(""%s/yolov3_sparsity_%d.weights"" % (args.checkpoint_dir, epoch))\n            model = scale_gama(alpha, model, scale_down=True)\n            print(""save weights in %s/yolov3_sparsity_%d.weights"" % (args.checkpoint_dir, epoch))\n  \n\n\nif __name__ ==\'__main__\':\n    train()\n'"
util.py,27,"b'from __future__ import  division\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2\nimport os\nimport math\n\ndef unique(tensor):\n    tensor_np = tensor.cpu().numpy()\n    #\xe5\x8e\xbb\xe9\x99\xa4\xe6\x95\xb0\xe7\xbb\x84\xe4\xb8\xad\xe7\x9a\x84\xe9\x87\x8d\xe5\xa4\x8d\xe6\x95\xb0\xe5\xad\x97\xef\xbc\x8c\xe5\xb9\xb6\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8e\x92\xe5\xba\x8f\xe4\xb9\x8b\xe5\x90\x8e\xe8\xbe\x93\xe5\x87\xba\xe3\x80\x82\n    unique_np = np.unique(tensor_np)\n    unique_tensor = torch.from_numpy(unique_np)\n\n    tensor_res = tensor.new(unique_tensor.shape)\n    tensor_res.copy_(unique_tensor)\n    return tensor_res\n\n\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    """"""\n    Returns the IoU of two bounding boxes\n    """"""\n    if not x1y1x2y2:\n        # Transform from center and width to exact coordinates\n        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n    else:\n        # Get the coordinates of bounding boxes\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n\n    # get the corrdinates of the intersection rectangle\n    inter_rect_x1 = torch.max(b1_x1, b2_x1)  # 1*3\n    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n    # Intersection area  clamp\xe5\xb0\x86input\xe4\xb8\xad\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xe9\x99\x90\xe5\x88\xb6\xe5\x9c\xa8[min,max]\xe8\x8c\x83\xe5\x9b\xb4\xe5\x86\x85\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaaTensor,\xe9\x87\x8d\xe5\x90\x88\xe5\x8c\xba\xe5\x9f\x9f\n    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n        inter_rect_y2 - inter_rect_y1 + 1, min=0\n    )\n    # Union Area\n    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n\n    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n\n    return iou\n\n\ndef non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):\n    """"""\n    Removes detections with lower object confidence score than \'conf_thres\' and performs\n    Non-Maximum Suppression to further filter detections.\n    Returns detections with shape:\n        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n    """"""\n\n    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n    box_corner = prediction.new(prediction.shape)\n    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n    prediction[:, :, :4] = box_corner[:, :, :4]\n\n    output = [None for _ in range(len(prediction))]\n    for image_i, image_pred in enumerate(prediction):\n        # Filter out confidence scores below threshold\n        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()\n        image_pred = image_pred[conf_mask]\n        # If none are remaining => process next image\n        if not image_pred.size(0):\n            continue\n        # Get score and class with highest confidence\n        class_conf, class_pred = torch.max(image_pred[:, 5 : 5 + num_classes], 1, keepdim=True)\n        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n        # Iterate through all predicted classes\n        unique_labels = detections[:, -1].cpu().unique()\n        if prediction.is_cuda:\n            unique_labels = unique_labels.cuda()\n        for c in unique_labels:\n            # Get the detections with the particular class\n            detections_class = detections[detections[:, -1] == c]\n            # Sort the detections by maximum objectness confidence\n            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n            detections_class = detections_class[conf_sort_index]\n            # Perform non-maximum suppression\n            max_detections = []\n            while detections_class.size(0):\n                # Get detection with highest confidence and save as max detection\n                max_detections.append(detections_class[0].unsqueeze(0))\n                # Stop if we\'re at the last detection\n                if len(detections_class) == 1:\n                    break\n                # Get the IOUs for all boxes with lower confidence\n                ious = bbox_iou(max_detections[-1], detections_class[1:])\n                # Remove detections with IoU >= NMS threshold\n                detections_class = detections_class[1:][ious < nms_thres]\n\n            max_detections = torch.cat(max_detections).data\n            # Add max detections to outputs\n            output[image_i] = (\n                max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))\n            )\n\n    return output\n\n\ndef letterbox_image(img, inp_dim):\n    \'\'\'resize image with unchanged aspect ratio using padding\'\'\'\n    img_w, img_h = img.shape[1], img.shape[0]\n    w, h = inp_dim\n    new_w = int(img_w * min(w / img_w, h / img_h))\n    new_h = int(img_h * min(w / img_w, h / img_h))\n    resized_image = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n    canvas[(h - new_h) // 2:(h - new_h) // 2 + new_h, (w - new_w) // 2:(w - new_w) // 2 + new_w, :] = resized_image\n    return canvas\n\n\ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network.\n\n    Returns a Variable\n    """"""\n    img = (letterbox_image(img, (inp_dim, inp_dim)))\n    img = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n    img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)\n    return img\n\n\ndef load_classes(namesfile):\n    fp = open(namesfile, ""r"")\n    names = fp.read().split(""\\n"")[:-1]\n    return names\n\n\ndef build_targets(\n    pred_boxes, pred_conf, pred_cls, target, anchors, num_anchors, num_classes, grid_size, ignore_thres, img_dim\n):\n    nB = target.size(0)  # B*50*5\n    nA = num_anchors  # 3\n    nC = num_classes  # 80\n    nG = grid_size  # 13\n    mask = torch.zeros(nB, nA, nG, nG)  # B*3*13*13\n    conf_mask = torch.ones(nB, nA, nG, nG)  # B*3*13*13\n    tx = torch.zeros(nB, nA, nG, nG)\n    ty = torch.zeros(nB, nA, nG, nG)\n    tw = torch.zeros(nB, nA, nG, nG)\n    th = torch.zeros(nB, nA, nG, nG)\n    tconf = torch.ByteTensor(nB, nA, nG, nG).fill_(0)\n    tcls = torch.ByteTensor(nB, nA, nG, nG, nC).fill_(0)\n\n    nGT = 0\n    nCorrect = 0\n    for b in range(nB):\n        for t in range(target.shape[1]):\n            # \xe5\xa6\x82\xe6\x9e\x9ctarget\xe6\xb2\xa1\xe6\x9c\x89\xe6\xa0\x87\xe8\xae\xb0\xe7\x9a\x84\xe7\x9b\xae\xe6\xa0\x87\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe9\x83\xbd\xe6\x98\xaf\xe6\x9c\x89\xe7\x9b\xae\xe6\xa0\x87\xe7\x9a\x84\n            if target[b, t].sum() == 0:\n                continue\n            nGT += 1\n            # Convert to position relative to box\xef\xbc\x8c\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86\xe5\x9c\xa8\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe4\xb8\x8a\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n            gx = target[b, t, 1] * nG\n            gy = target[b, t, 2] * nG\n            gw = target[b, t, 3] * nG\n            gh = target[b, t, 4] * nG\n            # Get grid box indices\n            gi = int(gx)\n            gj = int(gy)\n            # Get shape of gt box 1*4\n            gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0)\n            # Get shape of anchor box 3*4   [0., 0., scaled_anchors.w, scaled_anchors.h]\n            anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((len(anchors), 2)), np.array(anchors)), 1))\n            # \xe8\xae\xa1\xe7\xae\x97\xe7\x9b\xae\xe6\xa0\x87\xe5\x92\x8canchors\xe6\xa1\x86\xe7\x9a\x84iou 1*3\n            anch_ious = bbox_iou(gt_box, anchor_shapes)\n            # \xe9\x87\x8d\xe5\x8f\xa0\xe5\xa4\xa7\xe4\xba\x8e\xe9\x98\x88\xe5\x80\xbc\xe8\xae\xbe\xe7\xbd\xae\xe6\x8e\xa9\xe7\xa0\x81\xe4\xb8\xba\xe9\x9b\xb6\n            #\xe5\xa6\x82\xe6\x9e\x9c\xe5\x85\x88\xe9\xaa\x8c\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe4\xb8\x8e\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa1\x86\xe7\x9a\x84\xe9\x87\x8d\xe5\x8f\xa0\xe5\xba\xa6\xe6\xaf\x94\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe4\xbb\xbb\xe4\xbd\x95\xe5\x85\xb6\xe4\xbb\x96\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe9\x83\xbd\xe8\xa6\x81\xe5\xa5\xbd\xef\xbc\x8c\n            # \xe5\x88\x99\xe8\xaf\xa5\xe5\x80\xbc\xe5\xba\x94\xe8\xaf\xa5\xe4\xb8\xba1\xe3\x80\x82 \xe5\xa6\x82\xe6\x9e\x9c\xe5\x85\x88\xe9\xaa\x8c\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe4\xb8\x8d\xe6\x98\xaf\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84\xef\xbc\x8c\xe4\xbd\x86\xe7\xa1\xae\xe5\xae\x9e\xe4\xb8\x8e\xe7\x9c\x9f\xe5\xae\x9e\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84\xe9\x87\x8d\xe5\x8f\xa0\xe8\xb6\x85\xe8\xbf\x87\xe6\x9f\x90\xe4\xb8\xaa\xe9\x98\x88\xe5\x80\xbc(\xe8\xbf\x99\xe9\x87\x8c\xe6\x98\xaf0.5)\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\xb0\xb1\xe5\xbf\xbd\xe7\x95\xa5\xe8\xbf\x99\xe6\xac\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe3\x80\x82\n            conf_mask[b, anch_ious > ignore_thres, gj, gi] = 0\n            # Find the best matching anchor box\n            best_n = np.argmax(anch_ious)\n            # Get ground truth box\n            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gh])).unsqueeze(0)\n            # Get the best prediction \xe6\x89\xbe\xe5\x88\xb0\xe6\x9c\x80\xe4\xbd\xb3\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86 (B*3*13*13*4)\n            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)\n            # Masks\n            mask[b, best_n, gj, gi] = 1\n            # \xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba1\n            conf_mask[b, best_n, gj, gi] = 1\n            # Coordinates\n            tx[b, best_n, gj, gi] = gx - gi  # \xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\xb0\x8f\xe6\xa1\x86\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\n            ty[b, best_n, gj, gi] = gy - gj\n            # Width and height\n            tw[b, best_n, gj, gi] = math.log(gw / anchors[best_n][0] + 1e-16)\n            th[b, best_n, gj, gi] = math.log(gh / anchors[best_n][1] + 1e-16)\n            # One-hot encoding of label\n            target_label = int(target[b, t, 0])\n            tcls[b, best_n, gj, gi, target_label] = 1  #  b*3*13*13*80\n            tconf[b, best_n, gj, gi] = 1\n\n            # \xe8\xae\xa1\xe7\xae\x97\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86\xe5\x92\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe7\x9a\x84iou\n            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n            pred_label = torch.argmax(pred_cls[b, best_n, gj, gi]) # pred_cls\xe6\x98\xaf\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\n            score = pred_conf[b, best_n, gj, gi]\n            if iou > 0.5 and pred_label == target_label and score > 0.5:\n                nCorrect += 1\n\n    return nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls\n\n\ndef write_cfg(cfgfile,cfg):\n    with open(cfgfile,\'r\') as f:\n        lines = f.read().split(\'\\n\')  # store the lines in a list\n        lines = [x for x in lines if len(x) > 0]  # get read of the empty lines\n        lines = [x for x in lines if x[0] != \'#\']  # get rid of comments\n        #lines = [x.rstrip().lstrip() for x in lines]  # get rid of fringe whitespaces\\\n\n    block = {}\n    blocks = []\n    #D:/yolotest/cfg/yolov3.cfg\n    prunedcfg = os.path.join(\'\\\\\'.join(cfgfile.split(""/"")[0:-1]),""prune_""+cfgfile.split(""/"")[-1])\n    for line in lines:\n        if line[0] == ""["":  # This marks the start of a new block\n            if len(block) != 0:  # If block is not empty, implies it is storing values of previous block.\n                blocks.append(block)  # add it the blocks list\n                block = {}  # re-init the block\n            block[""type""] = line[1:-1].rstrip()\n        else:\n            key, value = line.split(""="")\n            block[key.rstrip()] = value.lstrip()\n    blocks.append(block)\n    x=0\n    #print(blocks[1])\n    for block in blocks:\n        if \'batch_normalize\' in block:\n            block[\'filters\']=cfg[x]\n            x= x+1\n    ##\n    with open(prunedcfg,\'w\') as f:\n        for block in blocks:\n            for i in block:\n                if i==""type"":\n                    f.write(\'\\n\')\n                    f.write(""[""+block[i]+""]\\n"")\n                    for j in block:\n                        if j != ""type"":\n                            f.write(j+""=""+str(block[j])+\'\\n\')\n    print(\'save pruned cfg file in %s\'%prunedcfg)\n    return prunedcfg\n\n\ndef route_problem(model,ind):\n    ds = list(model.children())\n    dsas = list(ds[0].children())\n    # print(dsas[90])\n    sum1 = 0\n    for k in range(ind+1):\n        for i in dsas[k].named_children():\n            if ""_"".join(i[0].split(""_"")[0:-1]) == \'conv_with_bn\':\n                sum1 = sum1 + 1\n    #print(sum1)\n    return sum1-1\n\ndef scale_gama(alpha,model,scale_down = False):\n    \'\'\'\xe6\x94\xbe\xe7\xbc\xa9bn\xe5\xb1\x82\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c\xe5\x8a\xa0\xe5\xbf\xab\xe7\xa8\x80\xe7\x96\x8f\'\'\'\n    alpha_ = 1 / alpha\n\n    if not scale_down:\n        # after training we want to scale back up so need to invert alpha\n        alpha_  = alpha\n        alpha   = 1 / alpha\n    nnlist = model.module_list\n    for i in range(len(nnlist)):\n        for name in nnlist[i].named_children():\n            if ""_"".join(name[0].split(""_"")[0:-1]) == \'conv_with_bn\':\n                name[1].weight.data =  name[1].weight.data * alpha_\n                #print(name[0])\n            elif ""_"".join(name[0].split(""_"")[0:-1]) == \'batch_norm\':\n                name[1].weight.data =  name[1].weight.data * alpha\n                #print(name[0])\n    return model\n\n\ndef dontprune(model):\n\n    dontprune=[]\n    nnlist = model.module_list\n    for i in range(len(nnlist)):\n        for name in nnlist[i].named_children():\n            if name[0].split(""_"")[0] == \'shortcut\':\n                if \'conv\' in list(nnlist[name[1].froms+i].named_children())[0][0]:\n                    dontprune.append(name[1].froms+i)\n                else:\n                    dontprune.append(name[1].froms + i-1)\n                dontprune.append(i-1)\n    return dontprune\n\n\n\n\n\n\n\n\n\n'"
yolomodel.py,28,"b'from __future__ import division\nfrom util import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nfrom collections import defaultdict\n\n\nclass shortcutLayer(nn.Module):\n    def __init__(self,froms):\n        super(shortcutLayer, self).__init__()\n        self.froms = froms\n\n\nclass Reorg(nn.Module):\n    def __init__(self,stride):\n        super(Reorg,self).__init__()\n        self.stride = stride\n\n\nclass Route(nn.Module):\n    def __init__(self,layers):\n        super(Route, self).__init__()\n        self.layers =layers\n\n\nclass DetectionLayer(nn.Module):\n    def __init__(self, anchors, num_classes, img_dim,ignore_thresh ):\n        super(DetectionLayer, self).__init__()\n        self.anchors = anchors\n        self.num_anchors = len(anchors)\n        self.num_classes = num_classes\n        self.bbox_attrs = 5 + num_classes\n        self.image_dim = img_dim\n        self.ignore_thres = ignore_thresh\n        self.lambda_coord = 1\n\n        self.mse_loss = nn.MSELoss(reduction=\'elementwise_mean\')  # Coordinate loss \xe5\x9d\x87\xe6\x96\xb9\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n        self.bce_loss = nn.BCELoss(reduction=\'elementwise_mean\')  # Confidence loss \xe9\x80\x82\xe7\x94\xa8\xe4\xba\x8e\xe5\xa4\x9a\xe7\x9b\xae\xe6\xa0\x87\xe5\x88\x86\xe7\xb1\xbb\n        self.ce_loss = nn.CrossEntropyLoss()  # Class loss\n\n    def forward(self, x, targets=None):\n        nA = self.num_anchors  #x.size=1*255*13*13\n        nB = x.size(0)\n        nG = x.size(2)\n        stride = self.image_dim / nG #    stride=416/13=32\n\n        # Tensors for cuda support\n        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n                            #(B*3*85*13*13)------>(B*3*13*13*85)\n        prediction = x.view(nB, nA, self.bbox_attrs, nG, nG).permute(0, 1, 3, 4, 2).contiguous()\n\n        # Get outputs (1*3*13*13)\n        x = torch.sigmoid(prediction[..., 0])  # Center x\n        y = torch.sigmoid(prediction[..., 1])  # Center y\n        w = prediction[..., 2]  # Width\n        h = prediction[..., 3]  # Height\n        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n\n        # Calculate offsets for each grid          (1*1*13*13) [0...12],[0...12]\n        grid_x = torch.arange(nG).repeat(nG, 1).view([1, 1, nG, nG]).type(FloatTensor)\n        # [0...0],[1....1]....[12...12]\n        grid_y = torch.arange(nG).repeat(nG, 1).t().view([1, 1, nG, nG]).type(FloatTensor)\n        #anchors\xe5\x9c\xa8\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n        scaled_anchors = FloatTensor([(a_w / stride, a_h / stride) for a_w, a_h in self.anchors])\n        # anchors   1*3*1*1\n        anchor_w = scaled_anchors[:, 0:1].view((1, nA, 1, 1))\n        anchor_h = scaled_anchors[:, 1:2].view((1, nA, 1, 1))\n\n        # Add offset and scale with anchors  (B*3*13*13*4)\n        pred_boxes = FloatTensor(prediction[..., :4].shape)\n        #B*3*13*13 +1*1*13*13,\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe4\xb8\x8a\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\n        pred_boxes[..., 0] = x.data + grid_x\n        pred_boxes[..., 1] = y.data + grid_y\n        #B*3*13*13 * 1*3*1*1\xef\xbc\x8c,\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe4\xb8\x8a\xe7\x9a\x84\xe9\x95\xbf\xe5\xae\xbd\xe5\xa4\xa7\xe5\xb0\x8f\n        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n\n        # Training\n        if targets is not None:\n            # target\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaaB*50*5\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\n            if x.is_cuda:\n                self.mse_loss = self.mse_loss.cuda()\n                self.bce_loss = self.bce_loss.cuda()\n                self.ce_loss = self.ce_loss.cuda()\n\n            nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls = build_targets(\n                pred_boxes=pred_boxes.cpu().data,\n                pred_conf=pred_conf.cpu().data,\n                pred_cls=pred_cls.cpu().data,\n                target=targets.cpu().data,\n                anchors=scaled_anchors.cpu().data,\n                num_anchors=nA,\n                num_classes=self.num_classes,\n                grid_size=nG,\n                ignore_thres=self.ignore_thres,\n                img_dim=self.image_dim,\n            )\n\n            nProposals = int((pred_conf > 0.5).sum().item())\n            recall = float(nCorrect / nGT) if nGT else 1\n            precision = float(nCorrect / nProposals)\n\n            # Handle masks\n            mask = Variable(mask.type(ByteTensor))\n            conf_mask = Variable(conf_mask.type(ByteTensor))\n\n            # Handle target variables\n            tx = Variable(tx.type(FloatTensor), requires_grad=False)\n            ty = Variable(ty.type(FloatTensor), requires_grad=False)\n            tw = Variable(tw.type(FloatTensor), requires_grad=False)\n            th = Variable(th.type(FloatTensor), requires_grad=False)\n            tconf = Variable(tconf.type(FloatTensor), requires_grad=False)\n            tcls = Variable(tcls.type(LongTensor), requires_grad=False)\n            # Get conf mask where gt and where there is no gt\n            conf_mask_true = mask\n            conf_mask_false = conf_mask - mask\n            # Mask outputs to ignore non-existing objects\n            loss_x = self.mse_loss(x[mask], tx[mask])  # x\xe6\x98\xaf\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe7\x9b\xb8\xe5\xaf\xb9\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\n            loss_y = self.mse_loss(y[mask], ty[mask])\n            loss_w = self.mse_loss(w[mask], tw[mask])\n            loss_h = self.mse_loss(h[mask], th[mask])\n            loss_conf = self.bce_loss(pred_conf[conf_mask_false], tconf[conf_mask_false]) + \\\n                        self.bce_loss(pred_conf[conf_mask_true], tconf[conf_mask_true])\n            loss_cls = (1 / nB) * self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))\n            loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n\n            return (\n                loss,\n                loss_x.item(),\n                loss_y.item(),\n                loss_w.item(),\n                loss_h.item(),\n                loss_conf.item(),\n                loss_cls.item(),\n                recall,\n                precision,\n            )\n\n        else:\n            # If not in training phase return predictions\n            output = torch.cat(\n                (   #(1*(3*13*13)*4)\n                    pred_boxes.view(nB, -1, 4) * stride,\n                    #1*(3*13*13)*1\n                    pred_conf.view(nB, -1, 1),\n                    #1*(3*13*13)*80\n                    pred_cls.view(nB, -1, self.num_classes),\n                ),\n                -1,\n            )\n            #print(output.shape)\n            return output\n\n\ndef parse_cfg(cfgfile):\n    file = open(cfgfile, \'r\')\n    lines = file.read().split(\'\\n\')  # store the lines in a list\n    lines = [x for x in lines if len(x) > 0]  # get read of the empty lines\n    lines = [x for x in lines if x[0] != \'#\']  # get rid of comments\n    lines = [x.rstrip().lstrip() for x in lines]  # get rid of fringe whitespaces\n    block ={}\n    blocks =[]\n    for line in lines:\n        if line[0] == \'[\':\n            if len(block)!= 0:\n                blocks.append(block)\n                block ={}\n            block[\'type\'] = line[1:-1].rstrip()\n        else:\n            key,value = line.split(\'=\')\n            block[key.rstrip()] = value.lstrip()\n    blocks.append(block)\n    return blocks\n\n\ndef create_modules(blocks):\n    net_info = blocks[0]\n    module_list = nn.ModuleList()\n    prev_filters = 3\n    output_filters = []\n    for index, x in enumerate(blocks[1:]):\n        module = nn.Sequential()\n        # check the type of block\n        # create a new module for the block\n        # append to module_list\n        if (x[""type""] == ""convolutional""):\n        # Get the info about the layer\n            activation = x[""activation""]\n            filters = int(x[""filters""])\n            padding = int(x[""pad""])\n            kernel_size = int(x[""size""])\n            stride = int(x[""stride""])\n            try:\n                batch_normalize = int(x[""batch_normalize""])\n                bias = False\n            except:\n                batch_normalize = 0\n                bias = True\n\n            if padding:\n                pad = (kernel_size - 1) // 2\n            else:\n                pad = 0\n\n            if batch_normalize:\n                conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=bias)\n                module.add_module(""conv_with_bn_{0}"".format(index), conv)\n                bn = nn.BatchNorm2d(filters)\n                module.add_module(""batch_norm_{0}"".format(index), bn)\n            else:\n                conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=bias)\n                module.add_module(""conv_without_bn_{0}"".format(index), conv)\n\n            # Check the activation.\n            # It is either Linear or a Leaky ReLU for YOLO\n            if activation == ""leaky"":\n                activn = nn.LeakyReLU(0.1, inplace=True)\n                module.add_module(""leaky_{0}"".format(index), activn)\n\n            # If it\'s an upsampling layer\n            # We use Bilinear2dUpsampling\n        elif (x[""type""] == ""upsample""):\n            stride = int(x[""stride""])\n            upsample = nn.Upsample(scale_factor=2, mode=""nearest"")\n            module.add_module(""upsample_{}"".format(index), upsample)\n            # If it is a route layer\n        elif (x[""type""] == ""route""):\n            x[""layers""] = x[""layers""].split(\',\')\n            # Start of a route\n            start = int(x[""layers""][0])\n            # end, if there exists one.\n            try:\n                end = int(x[""layers""][1])\n            except:\n                end = 0\n            # Positive anotation\n            if start > 0:\n                start = start - index\n            if end > 0:\n                end = end - index\n            route = Route([start,end])\n            module.add_module(""route_{0}"".format(index), route)\n            if end < 0:\n                filters = output_filters[index + start] + output_filters[index + end]\n            else:\n                filters = output_filters[index + start]\n\n            # shortcut corresponds to skip connection\n        elif x[""type""] == ""shortcut"":\n            froms = int(x[\'from\'])\n            shortcut = shortcutLayer(froms)\n            module.add_module(""shortcut_{}"".format(index), shortcut)\n            # Yolo is the detection layer\n        elif x[""type""] == ""yolo"" or x[""type""] == ""region"":\n            try:\n                mask = x[""mask""].split("","")\n                mask = [int(x) for x in mask]\n                anchors = x[""anchors""].split("","")\n                anchors = [int(a) for a in anchors]\n            except:\n                mask = [int(x) for x in range(int(x[\'num\']))]\n                anchors = x[""anchors""].split("","")\n                anchors = [32*float(a) for a in anchors]\n            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n            anchors = [anchors[i] for i in mask]\n            num_classes = int(x[""classes""])\n            img_height = int(net_info[""height""])\n            try:\n                ignore_thresh = float(x[""ignore_thresh""])\n            except:\n                ignore_thresh = float(x[""thresh""])\n            detection = DetectionLayer(anchors,num_classes,img_height,ignore_thresh)\n            module.add_module(""Detection_{}"".format(index), detection)\n\n        elif x[""type""] == ""maxpool"":\n            kernel_size = int(x[""size""])\n            stride = int(x[""stride""])\n            pool = nn.MaxPool2d(stride=stride,kernel_size=kernel_size)\n            module.add_module(""maxpool_{0}"".format(index), pool)\n\n        elif x[""type""] == ""reorg"":\n            stride = int(x[""stride""])\n            reorg = Reorg(stride=stride)\n            module.add_module(""reorg_{0}"".format(index),reorg)\n            filters = filters*4\n\n        module_list.append(module)\n        prev_filters = filters\n        output_filters.append(filters)\n\n\n    return (net_info, module_list)\n\nclass Darknet(nn.Module):\n    def __init__(self, cfgfile):\n        super(Darknet, self).__init__()\n        self.blocks = parse_cfg(cfgfile)\n        self.net_info, self.module_list = create_modules(self.blocks)\n        self.img_size = self.net_info[""height""]\n        self.seen = 0\n        self.header_info = np.array([0, 0, 0, self.seen, 0])\n        self.loss_names = [""x"", ""y"", ""w"", ""h"", ""conf"", ""cls"", ""recall"", ""precision""]\n\n    def forward(self, x, targets=None):\n        is_training = targets is not None\n        modules = self.blocks[1:]\n        outputs = [] #We cache the outputs for the route layer\n        layer_outputs = []\n        self.losses = defaultdict(float)\n        for i, module in enumerate(modules):\n            module_type = (module[""type""])\n            if module_type == ""convolutional"" or module_type == ""upsample"":\n                x = self.module_list[i](x)\n            elif module_type == ""route"":\n                layers = module[""layers""]\n                layers = [int(a) for a in layers]\n                if (layers[0]) > 0:\n                    layers[0] = layers[0] - i\n                if len(layers) == 1:\n                    x = outputs[i + (layers[0])]\n                else:\n                    if (layers[1]) > 0:\n                        layers[1] = layers[1] - i\n                    map1 = outputs[i + layers[0]]\n                    map2 = outputs[i + layers[1]]\n                    x = torch.cat((map1, map2), 1)\n            elif module_type == ""shortcut"": # \xe6\xae\x8b\xe5\xb7\xae\n                from_ = int(module[""from""])\n                x = outputs[i + from_]+outputs[i - 1]\n            elif module_type ==""maxpool"":\n                x = self.module_list[i](x)\n            elif module_type == ""reorg"":\n                stride = int(module[""stride""])\n                B,C,H,W = x.size()\n                x = x.view(B, C, int(H / stride), stride, int(W / stride), stride).transpose(3, 4).contiguous()\n                x = x.view(B, C, int(H / stride * W / stride), int(stride * stride)).transpose(2, 3).contiguous()\n                x = x.view(B, C, int(stride * stride), int(H / stride), int(W / stride)).transpose(1, 2).contiguous()\n                x = x.view(B, int(stride * stride * C), int(H / stride), int(W / stride))\n            elif module_type == ""yolo"" or module_type == ""region"":\n                if is_training:\n                    x, *losses = self.module_list[i][0](x, targets)\n                    for name, loss in zip(self.loss_names, losses):\n                        self.losses[name] += loss\n                # Test phase: Get detections\n                else:\n                    x = self.module_list[i](x)\n                layer_outputs.append(x)\n            outputs.append(x)\n\n        self.losses[""recall""] /= 3\n        self.losses[""precision""] /= 3\n        return sum(layer_outputs) if is_training else torch.cat(layer_outputs, 1)\n\n\n    def load_weights(self, weightfile):\n        # Open the weights file\n        fp = open(weightfile, ""rb"")\n        # The first 5 values are header information\n        # 1. Major version number\n        # 2. Minor Version Number\n        # 3. Subversion number\n        # 4,5. Images seen by the network (during training)\n        header = np.fromfile(fp, dtype=np.int32, count=5)\n        # Needed to write header when saving weights\n        self.header_info = header\n        self.seen = header[3]\n        weights = np.fromfile(fp, dtype=np.float32)\n        fp.close()\n        ptr = 0\n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i + 1][""type""]\n            # If module_type is convolutional load weights\n            # Otherwise ignore.\n            if module_type == ""convolutional"":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i + 1][""batch_normalize""])\n                except:\n                    batch_normalize = 0\n                conv = model[0]\n\n                if (batch_normalize):\n                    bn = model[1]\n                    # Get the number of weights of Batch Norm Layer\n                    num_bn_biases = bn.bias.numel()\n                    # Load the weights\n                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n                    ptr += num_bn_biases\n                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr += num_bn_biases\n                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr += num_bn_biases\n                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr += num_bn_biases\n                    # Cast the loaded weights into dims of model weights.\n                    bn_biases = bn_biases.view_as(bn.bias.data)\n                    bn_weights = bn_weights.view_as(bn.weight.data)\n                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n                    bn_running_var = bn_running_var.view_as(bn.running_var)\n                    # Copy the data to model\n                    bn.bias.data.copy_(bn_biases)\n                    bn.weight.data.copy_(bn_weights)\n                    bn.running_mean.copy_(bn_running_mean)\n                    bn.running_var.copy_(bn_running_var)\n\n                else:\n                    # Number of biases\n                    num_biases = conv.bias.numel()\n                    # Load the weights\n                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n                    ptr = ptr + num_biases\n                    # reshape the loaded weights according to the dims of the model weights\n                    conv_biases = conv_biases.view_as(conv.bias.data)\n                    # Finally copy the data\n                    conv.bias.data.copy_(conv_biases)\n                # Let us load the weights for the Convolutional layers\n                num_weights = conv.weight.numel()\n                # Do the same as above for weights\n                conv_weights = torch.from_numpy(weights[ptr:ptr + num_weights])\n                ptr = ptr + num_weights\n                conv_weights = conv_weights.view_as(conv.weight.data)\n                conv.weight.data.copy_(conv_weights)\n        print(""done!"")\n\n\n    def save_weights(self,path,cutoff=-1):\n        """"""save layers between 0 and cutoff (cutoff = -1 -> all are saved)""""""\n        fp = open(path,\'wb\')\n        self.header_info[3]=self.seen\n        self.header_info.tofile(fp)\n        for i in range(len(self.module_list[:cutoff])):\n            module_type = self.blocks[i + 1][""type""]\n            # If module_type is convolutional load weights\n            # Otherwise ignore.\n            if module_type == ""convolutional"":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i + 1][""batch_normalize""])\n                except:\n                    batch_normalize = 0\n                conv = model[0]\n                if (batch_normalize):\n                    bn = model[1]\n                    bn.bias.data.cpu().numpy().tofile(fp)\n                    bn.weight.data.cpu().numpy().tofile(fp)\n                    bn.running_mean.data.cpu().numpy().tofile(fp)\n                    bn.running_var.data.cpu().numpy().tofile(fp)\n                else:\n                    conv.bias.data.cpu().numpy().tofile(fp)\n                conv.weight.data.cpu().numpy().tofile(fp)\n        fp.close()\n\n\n    def model_init(self):\n        """"""init""""""\n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i + 1][""type""]\n            if module_type == ""convolutional"":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i + 1][""batch_normalize""])\n                except:\n                    batch_normalize = 0\n                conv = model[0]\n\n                if (batch_normalize):\n                    bn = model[1]\n                    torch.nn.init.constant_(bn.weight.data,0.5)\n                    torch.nn.init.constant_(bn.bias.data,0.0)\n                else:\n                    torch.nn.init.constant_(conv.bias.data,0.0)\n                torch.nn.init.xavier_uniform_(conv.weight.data, gain=1)\n\n'"
