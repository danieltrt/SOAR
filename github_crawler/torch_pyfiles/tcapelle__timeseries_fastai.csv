file_path,api_count,code
setup.py,0,"b'from pkg_resources import parse_version\nfrom configparser import ConfigParser\nimport setuptools\nassert parse_version(setuptools.__version__)>=parse_version(\'36.2\')\n\n# note: all settings are in settings.ini; edit there, not here\nconfig = ConfigParser(delimiters=[\'=\'])\nconfig.read(\'settings.ini\')\ncfg = config[\'DEFAULT\']\n\ncfg_keys = \'version description keywords author author_email\'.split()\nexpected = cfg_keys + ""lib_name user branch license status min_python audience language"".split()\nfor o in expected: assert o in cfg, ""missing expected setting: {}"".format(o)\nsetup_cfg = {o:cfg[o] for o in cfg_keys}\n\nlicenses = {\n    \'apache2\': (\'Apache Software License 2.0\',\'OSI Approved :: Apache Software License\'),\n}\nstatuses = [ \'1 - Planning\', \'2 - Pre-Alpha\', \'3 - Alpha\',\n    \'4 - Beta\', \'5 - Production/Stable\', \'6 - Mature\', \'7 - Inactive\' ]\npy_versions = \'2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8\'.split()\n\nrequirements = cfg.get(\'requirements\',\'\').split()\nlic = licenses[cfg[\'license\']]\nmin_python = cfg[\'min_python\']\n\nsetuptools.setup(\n    name = cfg[\'lib_name\'],\n    license = lic[0],\n    classifiers = [\n        \'Development Status :: \' + statuses[int(cfg[\'status\'])],\n        \'Intended Audience :: \' + cfg[\'audience\'].title(),\n        \'License :: \' + lic[1],\n        \'Natural Language :: \' + cfg[\'language\'].title(),\n    ] + [\'Programming Language :: Python :: \'+o for o in py_versions[py_versions.index(min_python):]],\n    url = \'https://github.com/{}/{}\'.format(cfg[\'user\'],cfg[\'lib_name\']),\n    packages = setuptools.find_packages(),\n    include_package_data = True,\n    install_requires = requirements,\n    python_requires  = \'>=\' + cfg[\'min_python\'],\n    long_description = open(\'README.md\').read(),\n    long_description_content_type = \'text/markdown\',\n    zip_safe = False,\n    entry_points = { \'console_scripts\': cfg.get(\'console_scripts\',\'\').split() },\n    **setup_cfg)\n\n'"
ucr.py,1,"b'from fastscript import *\n\nfrom timeseries_fastai.imports import *\nfrom timeseries_fastai.data import *\nfrom timeseries_fastai.core import *\nfrom timeseries_fastai.models import *\nfrom timeseries_fastai.tabular import *\n\nPATH = get_ucr()\nNON_NAN_TASKS = \'\'\'ACSF1 Adiac ArrowHead BME Beef BeetleFly BirdChicken CBF Car Chinatown \nChlorineConcentration CinCECGTorso Coffee Computers CricketX CricketY CricketZ Crop \nDiatomSizeReduction DistalPhalanxOutlineAgeGroup DistalPhalanxOutlineCorrect \nDistalPhalanxTW ECG200 ECG5000 ECGFiveDays EOGHorizontalSignal EOGVerticalSignal \nEarthquakes ElectricDevices EthanolLevel FaceAll FaceFour FacesUCR FiftyWords Fish \nFordA FordB FreezerRegularTrain FreezerSmallTrain Fungi GunPoint GunPointAgeSpan \nGunPointMaleVersusFemale GunPointOldVersusYoung Ham HandOutlines Haptics Herring \nHouseTwenty InlineSkate InsectEPGRegularTrain InsectEPGSmallTrain InsectWingbeatSound \nItalyPowerDemand LargeKitchenAppliances Lightning2 Lightning7 Mallat Meat MedicalImages \nMelbournePedestrian MiddlePhalanxOutlineAgeGroup MiddlePhalanxOutlineCorrect MiddlePhalanxTW \nMixedShapesRegularTrain MixedShapesSmallTrain MoteStrain NonInvasiveFetalECGThorax1 \nNonInvasiveFetalECGThorax2 OSULeaf OliveOil PhalangesOutlinesCorrect Phoneme PigAirwayPressure \nPigArtPressure PigCVP Plane PowerCons ProximalPhalanxOutlineAgeGroup ProximalPhalanxOutlineCorrect \nProximalPhalanxTW RefrigerationDevices Rock ScreenType SemgHandGenderCh2 SemgHandMovementCh2 \nSemgHandSubjectCh2 ShapeletSim ShapesAll SmallKitchenAppliances SmoothSubspace \nSonyAIBORobotSurface1 SonyAIBORobotSurface2 StarLightCurves Strawberry SwedishLeaf Symbols \nSyntheticControl ToeSegmentation1 ToeSegmentation2 Trace TwoLeadECG TwoPatterns UMD \nUWaveGestureLibraryAll UWaveGestureLibraryX UWaveGestureLibraryY UWaveGestureLibraryZ Wafer \nWine WordSynonyms Worms WormsTwoClass Yoga\'\'\'.split()\n\ndef compute_metrics(learn):\n    ""compute oguiza Metrics on UCR""\n    results = np.array(learn.recorder.values)\n    acc_ = results[-1,-1]\n    accmax_ = results[:, -1].max()\n    loss_ = results[-1,0]\n    val_loss_ = results[-1,1]\n    return acc_, accmax_, loss_, val_loss_ \n\ndef max_bs(N):\n    N = N//6\n    k=1\n    while (N//2**k)>1: k+=1\n    return min(2**k, 32)\n\ndef get_dls(path, task, bs=None, workers=None):\n    df_train, df_test = load_df_ucr(path, task)\n    bs = ifnone(bs, max_bs(len(df_train)))\n    x_cols = df_train.columns[0:-1].to_list()\n    \n    df_main = stack_train_valid(df_train, df_test)\n    splits=[range_of(df_train), list(range(len(df_train), len(df_main)))]\n    to = TSPandas(df_main, [Normalize], x_names=x_cols, y_names=\'target\', splits=splits)\n    \n    return to.dataloaders(bs, 2*bs)\n\ndef get_model(dls, arch):\n    num_classes = dls.c\n    arch = arch.lower()\n    if arch==\'resnet\':     model = create_resnet(1, num_classes, conv_sizes=[64, 128, 256])\n    elif arch==\'fcn\':      model = create_fcn(1, num_classes, ks=9, conv_sizes=[128, 256, 128])\n    elif arch==\'mlp\':      model = create_mlp(dls.train.one_batch()[0].shape[-1], num_classes)\n    elif arch==\'inception\':model = create_inception(1, num_classes)\n    else: \n        print(\'Please chosse a model in [resnet, FCN, MLP, inception]\')\n        return None\n    return model\n\ndef train_task(path, task=\'Adiac\', arch=\'resnet\', epochs=40, lr=5e-4):\n    ""trains arch over task with params""\n    dls = get_dls(path, task)\n    model = get_model(dls, arch)\n    learn = Learner(dls, model, wd=1e-2, metrics=[accuracy])\n    learn.fit_one_cycle(epochs, lr)                          \n    return learn\n\n    \ndef run_tasks(tasks, arch=\'resnet\', lr=1e-3, epochs=1, mixup=0.2, fp16=True):\n    results = [compute_metrics(train_task(PATH, task, arch, epochs, lr, mixup, fp16)) for task in tasks]\n    return pd.DataFrame(data=results, columns=[\'acc\', \'acc_max\', \'train_loss\', \'val_loss\'], index=tasks)\n\ndef list2csv(a, sep=\', \'):\n    return sep.join([f\'{i:.2f}\' for i in a])+\'\\n\'\n\n@call_parse\ndef main(\n    arch:    Param(""Network arch. [resnet, FCN, MLP, inception, All]. (default: \\\'resnet\\\')"", str)=\'resnet\',\n    tasks:   Param(""Which tasks from UCR to run, [task, All]. (default: \\\'All\\\')"", str)=\'Adiac\',\n    epochs:  Param(""Number of epochs.(default: 40)"", int)=40,\n    lr:      Param(""Learning rate.(default: 1e-3)"", float)=1e-3, \n    filename:Param(""output filename"", str)=\'results.csv\',\n    gpu:     Param(""GPU to run on"", int)=None,\n    #opt params:\n    opt:     Param(""Optimizer (adam,rms,sgd,ranger)"", str)=\'ranger\',\n    sched:   Param(""Scheduler (flat_cos, one_cyle, flat)"", str)=\'flat_cos\',\n    sqrmom:  Param(""sqr_mom"", float)=0.99,\n    mom:     Param(""Momentum"", float)=0.9,\n    eps:     Param(""epsilon"", float)=1e-6,\n    beta:    Param(""SAdam softplus beta"", float)=0.,\n    mixup: Param(""Mixup"", float)=0.2,\n    fp16:  Param(""Use mixed precision training"", int)=1,\n    ):\n\n    ""Training of UCR.""\n\n        #gpu = setup_distrib(gpu)\n    if gpu is not None: torch.cuda.set_device(gpu)\n    if   opt==\'adam\'  : opt_func = partial(Adam, mom=mom, sqr_mom=sqrmom, eps=eps)\n    elif opt==\'rms\'   : opt_func = partial(RMSprop, sqr_mom=sqrmom)\n    elif opt==\'sgd\'   : opt_func = partial(SGD, mom=mom)\n    elif opt==\'ranger\': opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)\n\n    if tasks.lower()==\'all\':tasks=NON_NAN_TASKS\n    elif tasks.lower()==\'bench\':\n        tasks =  [ \'Wine\', \'BeetleFly\', \'InlineSkate\', \'MiddlePhalanxTW\', \'OliveOil\', \'SmallKitchenAppliances\', \'WordSynonyms\', \n                \'MiddlePhalanxOutlineAgeGroup\', \'MoteStrain\', \'Phoneme\', \'Herring\', \'ScreenType\', \'ChlorineConcentration\'] \n    else: tasks = [tasks]\n    with open(filename, \'w\') as f:\n        f.write(\'task, acc, acc_max, train_loss, val_loss\\n\')\n        for task in tasks:\n            dls = get_dls(PATH, task)\n            print(f\'Training for {epochs} epochs with lr = {lr} with bs={dls.train.bs, dls.valid.bs}\')\n            learn = Learner(dls, model=get_model(dls, arch), opt_func=opt_func, \\\n                    metrics=[accuracy])\n            if fp16: learn = learn.to_fp16()\n            cbs = MixUp(mixup) if mixup else []\n            if sched == \'flat_cos\':    learn.fit_flat_cos(epochs, lr, wd=1e-2, cbs=cbs)\n            elif sched == \'one_cycle\': learn.fit_one_cycle(epochs, lr, wd=1e-2, cbs=cbs)\n            else:                      learn.fit(epochs, lr, wd=1e-2, cbs=cbs)\n            f.write(task +\', \'+ list2csv(compute_metrics(learn)))\n'"
timeseries_fastai/__init__.py,0,"b'__version__ = ""2.0.1""\n'"
timeseries_fastai/_nbdev.py,0,"b'# AUTOGENERATED BY NBDEV! DO NOT EDIT!\n\n__all__ = [""index"", ""modules"", ""custom_doc_links"", ""git_url""]\n\nindex = {""maybe_unsqueeze"": ""00_core.ipynb"",\n         ""show_array"": ""00_core.ipynb"",\n         ""TSeries"": ""00_core.ipynb"",\n         ""URLs.UCR"": ""00_core.ipynb"",\n         ""get_ucr"": ""00_core.ipynb"",\n         ""load_df_ucr"": ""00_core.ipynb"",\n         ""TSBlock"": ""01_data.ipynb"",\n         ""stack_train_valid"": ""03_tabular.ipynb"",\n         ""TSDataLoaders"": ""01_data.ipynb"",\n         ""act_fn"": ""02_models.ipynb"",\n         ""AdaptiveConcatPool1d"": ""02_models.ipynb"",\n         ""create_mlp"": ""02_models.ipynb"",\n         ""create_fcn"": ""02_models.ipynb"",\n         ""res_block_1d"": ""02_models.ipynb"",\n         ""create_resnet"": ""02_models.ipynb"",\n         ""Shortcut"": ""02_models.ipynb"",\n         ""conv"": ""02_models.ipynb"",\n         ""InceptionModule"": ""02_models.ipynb"",\n         ""create_inception"": ""02_models.ipynb"",\n         ""TabularTS"": ""03_tabular.ipynb"",\n         ""TSPandas"": ""03_tabular.ipynb"",\n         ""setups"": ""03_tabular.ipynb"",\n         ""encodes"": ""03_tabular.ipynb"",\n         ""decodes"": ""03_tabular.ipynb"",\n         ""NormalizeTS"": ""03_tabular.ipynb"",\n         ""ReadTSBatch"": ""03_tabular.ipynb"",\n         ""TabularTSDataloader"": ""03_tabular.ipynb""}\n\nmodules = [""core.py"",\n           ""data.py"",\n           ""models.py"",\n           ""tabular.py""]\n\ndoc_url = ""https://tcapelle.github.io/timeseries_fastai/""\n\ngit_url = ""https://github.com/tcapelle/timeseries_fastai/tree/master/""\n\ndef custom_doc_links(name): return None\n'"
timeseries_fastai/core.py,0,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_core.ipynb (unless otherwise specified).\n\n__all__ = [\'maybe_unsqueeze\', \'show_array\', \'TSeries\', \'get_ucr\', \'load_df_ucr\']\n\n# Cell\nfrom fastcore.test import *\nfrom .imports import *\n\n# Cell\nimport pandas as pd\nfrom fastcore.all import *\nfrom scipy.io import arff\n\n# Cell\ndef maybe_unsqueeze(x):\n    ""Add empty dimension if it is a rank 1 tensor/array""\n    if isinstance(x, np.ndarray): return x[None,:] if len(x.shape)==1 else x\n    if isinstance(x, Tensor): return x.unsqueeze(0) if len(x.shape)==1 else x\n    else: return None\n\n# Cell\ndef show_array(array, ax=None, figsize=None, title=None, ctx=None, tx=None, **kwargs):\n    ""Show an array on `ax`.""\n    # Handle pytorch axis order\n    if hasattrs(array, (\'data\',\'cpu\',\'permute\')):\n        array = array.data.cpu()\n    elif not isinstance(array,np.ndarray):\n        array=array(array)\n    arrays = maybe_unsqueeze(array)\n    ax = ifnone(ax,ctx)\n    if figsize is None: figsize = (5,5)\n    if ax is None: _,ax = plt.subplots(figsize=figsize)\n    tx = ifnone(tx,np.arange(arrays[0].shape[0]))\n    label = kwargs.pop(\'label\', \'x\')\n    for a, c in zip(arrays, [\'b\', \'c\', \'m\', \'y\', \'k\',]):\n        ax.plot(tx, a, \'-\'+c,label=label, **kwargs)\n    if title is not None: ax.set_title(title)\n    ax.legend()\n    return ax\n\n# Cell\nclass TSeries(TensorBase):\n    ""Basic Timeseries wrapper""\n    @classmethod\n    def create(cls, x):\n        return cls(maybe_unsqueeze(as_tensor(x)))\n\n    @property\n    def channels(self): return self.shape[0]\n\n    @property\n    def len(self): return self.shape[-1]\n\n    def __repr__(self):\n        return f\'TSeries(ch={self.channels}, len={self.len})\'\n\n    def show(self, ctx=None, **kwargs):\n        return show_array(self, ctx=ctx, **kwargs)\n\n# Cell\nURLs.UCR = \'http://www.timeseriesclassification.com/Downloads/Archives/Univariate2018_arff.zip\'\n\n# Cell\n@delegates(untar_data)\ndef get_ucr(**kwargs):\n    ""zipped file has different name as .zip""\n    ucr_path = untar_data(URLs.UCR, **kwargs)\n    if not ucr_path.exists():\n        zf = zipfile.ZipFile(URLs.path(URLs.UCR))\n        actual_folder = ucr_path.parent/zf.namelist()[0]\n        actual_folder.rename(ucr_path)\n        print(f\'Renaming {actual_folder} to {ucr_path}\')\n    return ucr_path\n\n# Cell\n# ""this functions are based on https://github.com/mb4310/Time-Series""\ndef load_df_ucr(path, task):\n    ""Loads arff files from UCR""\n    try:\n        print(f\'Loading files from: {path}/{task}\')\n        dfs = []\n        for file in [\'TRAIN\', \'TEST\']:\n            filename = f\'{task}/{task}_{file}.arff\'\n            data = arff.loadarff(str(path/filename))\n            dfs.append(pd.DataFrame(data[0]))\n        return dfs\n    except:\n        print(f\'Error loading files: {path}/{task}\')'"
timeseries_fastai/data.py,0,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_data.ipynb (unless otherwise specified).\n\n__all__ = [\'TSBlock\', \'stack_train_valid\', \'TSDataLoaders\']\n\n# Cell\nfrom .imports import *\nfrom .core import *\nfrom fastai2.basics import *\nfrom fastai2.torch_core import *\nfrom fastai2.vision.data import get_grid\n\n# Cell\ndef TSBlock(cls=TSeries):\n    ""A TimeSeries Block to process one timeseries""\n    return TransformBlock(type_tfms=cls.create)\n\n# Cell\ndef stack_train_valid(df_train, df_valid):\n    ""Stack df_train and df_valid, adds `valid_col`=True/False for df_valid/df_train""\n    return pd.concat([df_train.assign(valid_col=False), df_valid.assign(valid_col=True)]).reset_index(drop=True)\n\n# Cell\nclass TSDataLoaders(DataLoaders):\n    ""A TimeSeries DataLoader""\n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_df(cls, df, path=\'.\', valid_pct=0.2, seed=None, x_cols=None, label_col=None,\n                y_block=None, valid_col=None, item_tfms=None, batch_tfms=None, **kwargs):\n        ""Create a DataLoader from a pandas DataFrame""\n        y_block = ifnone(y_block, CategoryBlock)\n        splitter = RandomSplitter(valid_pct, seed=seed) if valid_col is None else ColSplitter(valid_col)\n        dblock = DataBlock(blocks=(TSBlock, y_block),\n                           get_x=lambda o: o[x_cols].astype(np.float32),\n                           get_y=ColReader(label_col),\n                           splitter=splitter,\n                           item_tfms=item_tfms,\n                           batch_tfms=batch_tfms)\n        return cls.from_dblock(dblock, df, path=path, **kwargs)\n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_dfs(cls, df_train, df_valid, path=\'.\', x_cols=None, label_col=None,\n                y_block=None, item_tfms=None, batch_tfms=None, **kwargs):\n        ""Create a DataLoader from a df_train and df_valid""\n        df = stack_train_valid(df_train, df_valid)\n        return cls.from_df(df, path, x_cols=x_cols, valid_col=\'valid_col\', label_col=label_col,\n                y_block=y_block, item_tfms=item_tfms, batch_tfms=batch_tfms,**kwargs)\n\n# Cell\n@typedispatch\ndef show_batch(x: TSeries, y, samples, ctxs=None, max_n=10,rows=None, cols=None, figsize=None, **kwargs):\n    ""Show batch for TSeries objects""\n    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), rows=rows, cols=cols, add_vert=1, figsize=figsize)\n    ctxs = show_batch[object](x, y, samples=samples, ctxs=ctxs, max_n=max_n, **kwargs)\n    return ctxs'"
timeseries_fastai/imports.py,0,b'from fastai2.basics import *\nfrom fastai2.torch_basics import *\nfrom fastai2.torch_core import *\nfrom fastai2.callback.all import *\nfrom fastai2.data.all import *\nfrom fastai2.vision.data import get_grid'
timeseries_fastai/models.py,3,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_models.ipynb (unless otherwise specified).\n\n__all__ = [\'act_fn\', \'AdaptiveConcatPool1d\', \'create_mlp\', \'create_fcn\', \'res_block_1d\', \'create_resnet\', \'Shortcut\',\n           \'conv\', \'InceptionModule\', \'create_inception\']\n\n# Cell\nfrom .core import *\nimport torch\nimport torch.nn as nn\nfrom fastcore.all import *\nfrom fastai2.basics import *\nfrom fastai2.torch_core import *\nfrom fastai2.layers import *\nfrom fastai2.vision import *\n\n# Cell\nact_fn = nn.ReLU(inplace=True)\n\n# Cell\nclass AdaptiveConcatPool1d(nn.Module):\n    ""Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`""\n    def __init__(self, size=None):\n        super().__init__()\n        self.size = size or 1\n        self.ap = nn.AdaptiveAvgPool1d(self.size)\n        self.mp = nn.AdaptiveMaxPool1d(self.size)\n    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n\n# Cell\ndef create_mlp(ni, nout, linear_sizes=[500, 500, 500]):\n    layers = []\n    sizes = zip([ni]+linear_sizes, linear_sizes+[nout])\n    for n1, n2 in sizes:\n            layers += LinBnDrop(n1, n2, p=0.2, act=act_fn if n2!=nout else None)\n    return nn.Sequential(Flatten(),\n                         *layers)\n\n# Cell\ndef create_fcn(ni, nout, ks=9, conv_sizes=[128, 256, 128], stride=1):\n    layers = []\n    sizes = zip([ni]+conv_sizes, conv_sizes)\n    for n1, n2 in sizes:\n            layers += [ConvLayer(n1, n2, ks=ks, ndim=1, stride=stride)]\n    return nn.Sequential(*layers,\n                         AdaptiveConcatPool1d(),\n                         Flatten(),\n                         *LinBnDrop(2*n2, nout)\n                         )\n\n# Cell\ndef res_block_1d(nf, ks=[5,3]):\n    ""Resnet block as described in the paper.""\n    return SequentialEx(ConvLayer(nf, nf, ks=ks[0], ndim=1, ),\n                        ConvLayer(nf, nf, ks=ks[1], ndim=1, act_cls=None),\n                        MergeLayer())\n\n# Cell\ndef create_resnet(ni, nout, kss=[9,5,3], conv_sizes=[64, 128, 128], stride=1):\n    ""Basic 11 Layer - 1D resnet builder""\n    layers = []\n    sizes = zip([ni]+conv_sizes, conv_sizes)\n    for n1, n2 in sizes:\n            layers += [ConvLayer(n1, n2, ks=kss[0], stride=stride, ndim=1),\n                       res_block_1d(n2, kss[1:3])]\n    return nn.Sequential(*layers,\n                         AdaptiveConcatPool1d(),\n                         Flatten(),\n                        *LinBnDrop(2*n2, nout, p=0.1)\n                        )\n\n# Cell\nclass Shortcut(Module):\n    ""Merge a shortcut with the result of the module by adding them. Adds Conv, BN and ReLU""\n    def __init__(self, ni, nf, act_fn=act_fn):\n        self.act_fn=act_fn\n        self.conv=ConvLayer(ni, nf, ks=1, ndim=1)\n        self.bn=nn.BatchNorm1d(nf)\n    def forward(self, x): return act_fn(x + self.bn(self.conv(x.orig)))\n\n# Cell\ndef conv(ni, nf, ks=3, stride=1, bias=False):\n    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n\n# Cell\nclass InceptionModule(Module):\n    ""The inception Module from `ni` inputs to len(\'kss\')*`nb_filters`+`bottleneck_size`""\n    def __init__(self, ni, nb_filters=32, kss=[39, 19, 9], bottleneck_size=32, stride=1):\n        if (bottleneck_size>0 and ni>1): self.bottleneck = conv(ni, bottleneck_size, 1, stride)\n        else: self.bottleneck = noop\n        self.convs = nn.ModuleList([conv(bottleneck_size if (bottleneck_size>1 and ni>1) else ni, nb_filters, ks) for ks in kss])\n        self.conv_bottle = nn.Sequential(nn.MaxPool1d(3, stride, padding=1), conv(ni, nb_filters, 1))\n        self.bn_relu = nn.Sequential(nn.BatchNorm1d((len(kss)+1)*nb_filters), nn.ReLU())\n    def forward(self, x):\n        bottled = self.bottleneck(x)\n        return self.bn_relu(torch.cat([c(bottled) for c in self.convs]+[self.conv_bottle(x)], dim=1))\n\n# Cell\ndef create_inception(ni, nout, kss=[39, 19, 9], depth=6, bottleneck_size=32, nb_filters=32, head=True):\n    ""Creates an InceptionTime arch from `ni` channels to `nout` outputs.""\n    layers = []\n    n_ks = len(kss) + 1\n    for d in range(depth):\n        im = SequentialEx(InceptionModule(ni if d==0 else n_ks*nb_filters, kss=kss, bottleneck_size=bottleneck_size))\n        if d%3==2: im.append(Shortcut(n_ks*nb_filters, n_ks*nb_filters))\n        layers.append(im)\n    head = [AdaptiveConcatPool1d(), Flatten(), nn.Linear(2*n_ks*nb_filters, nout)] if head else []\n    return  nn.Sequential(*layers, *head)'"
timeseries_fastai/tabular.py,0,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_tabular.ipynb (unless otherwise specified).\n\n__all__ = [\'TabularTS\', \'TSPandas\', \'setups\', \'encodes\', \'decodes\', \'NormalizeTS\', \'setups\', \'encodes\', \'decodes\',\n           \'ReadTSBatch\', \'TabularTSDataloader\', \'stack_train_valid\']\n\n# Cell\nfrom .imports import *\nfrom .core import *\nfrom fastai2.basics import *\nfrom fastai2.torch_core import *\nfrom fastai2.vision.data import get_grid\nfrom fastai2.tabular.core import TabularProc, _TabIloc\n\n# Cell\nclass TabularTS(CollBase, GetAttr, FilteredBase):\n    ""A `DataFrame` wrapper that knows which cols are x/y, and returns rows in `__getitem__`""\n    _default, with_cont=\'procs\',True\n    def __init__(self, df, procs=None, x_names=None, y_names=None, block_y=None, splits=None,\n                 do_setup=True, device=None, inplace=False):\n        if inplace and splits is not None:\n            warn(""Using inplace with splits will trigger a pandas error. Set `pd.options.mode.chained_assignment=None` to avoid it."")\n        if not inplace: df = df.copy()\n        if splits is not None: df = df.iloc[sum(splits, [])]\n        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n        super().__init__(df)\n\n        self.x_names,self.y_names,self.device = L(x_names),L(y_names),device\n        if block_y is None and self.y_names:\n            # Make ys categorical if they\'re not numeric\n            ys = df[self.y_names]\n            if len(ys.select_dtypes(include=\'number\').columns)!=len(ys.columns): block_y = CategoryBlock()\n            else: block_y = RegressionBlock()\n        if block_y is not None and do_setup:\n            if callable(block_y): block_y = block_y()\n            procs = L(procs) + block_y.type_tfms\n        self.procs = Pipeline(procs)\n        self.split = len(df) if splits is None else len(splits[0])\n        if do_setup: self.setup()\n\n    def new(self, df):\n        return type(self)(df, do_setup=False, block_y=TransformBlock(),\n                          **attrdict(self, \'procs\',\'x_names\',\'y_names\', \'device\'))\n\n    def subset(self, i): return self.new(self.items[slice(0,self.split) if i==0 else slice(self.split,len(self))])\n    def copy(self): self.items = self.items.copy(); return self\n    def decode(self): return self.procs.decode(self)\n    def decode_row(self, row): return self.new(pd.DataFrame(row).T).decode().items.iloc[0]\n    def show(self, max_n=10, **kwargs): display_df(self.new(self.all_cols[:max_n]).decode().items)\n    def setup(self): self.procs.setup(self)\n    def process(self): self.procs(self)\n    def loc(self): return self.items.loc\n    def iloc(self): return _TabIloc(self)\n    def targ(self): return self.items[self.y_names]\n    def x_names (self): return self.x_names\n    def n_subsets(self): return 2\n    def y(self): return self[self.y_names[0]]\n    def new_empty(self): return self.new(pd.DataFrame({}, columns=self.items.columns))\n    def to_device(self, d=None):\n        self.device = d\n        return self\n\n    def all_col_names (self):\n        ys = [n for n in self.y_names if n in self.items.columns]\n        return self.x_names + self.y_names if len(ys) == len(self.y_names) else self.x_names\n\nproperties(TabularTS,\'loc\',\'iloc\',\'targ\',\'all_col_names\',\'n_subsets\',\'y\')\n\n# Cell\nclass TSPandas(TabularTS):\n    def transform(self, cols, f, all_col=True):\n        if not all_col: cols = [c for c in cols if c in self.items.columns]\n        if len(cols) > 0: self[cols] = self[cols].transform(f)\n\n# Cell\ndef _add_prop(cls, nm):\n    @property\n    def f(o): return o[list(getattr(o,nm+\'_names\'))]\n    @f.setter\n    def fset(o, v): o[getattr(o,nm+\'_names\')] = v\n    setattr(cls, nm+\'s\', f)\n    setattr(cls, nm+\'s\', fset)\n\n_add_prop(TabularTS, \'y\')\n_add_prop(TabularTS, \'x\')\n_add_prop(TabularTS, \'all_col\')\n\n# Cell\ndef _apply_cats (voc, add, c):\n    if not is_categorical_dtype(c):\n        return pd.Categorical(c, categories=voc[c.name][add:]).codes+add\n    return c.cat.codes+add #if is_categorical_dtype(c) else c.map(voc[c.name].o2i)\ndef _decode_cats(voc, c): return c.map(dict(enumerate(voc[c.name].items)))\n\n# Cell\n@Categorize\ndef setups(self, to:TabularTS):\n    if len(to.y_names) > 0:\n        self.vocab = CategoryMap(getattr(to, \'train\', to).iloc[:,to.y_names[0]].items)\n        self.c = len(self.vocab)\n    return self(to)\n\n@Categorize\ndef encodes(self, to:TabularTS):\n    to.transform(to.y_names, partial(_apply_cats, {n: self.vocab for n in to.y_names}, 0), all_col=False)\n    return to\n\n@Categorize\ndef decodes(self, to:TabularTS):\n    to.transform(to.y_names, partial(_decode_cats, {n: self.vocab for n in to.y_names}), all_col=False)\n    return to\n\n# Cell\nclass NormalizeTS(TabularProc):\n    ""Normalize the x variables.""\n    order = 2\n    def setups(self, dsets): self.means,self.stds = dsets.xs.mean(),dsets.xs.std(ddof=0)+1e-7\n    def encodes(self, to): to.conts = (to.xs-self.means) / self.stds\n    def decodes(self, to): to.conts = (to.xs*self.stds ) + self.means\n\n# Cell\n@Normalize\ndef setups(self, to:TabularTS):\n    self.means,self.stds = getattr(to, \'train\', to).xs.mean(),getattr(to, \'train\', to).xs.std(ddof=0)+1e-7\n    return self(to)\n\n@Normalize\ndef encodes(self, to:TabularTS):\n    to.xs = (to.xs-self.means) / self.stds\n    return to\n\n@Normalize\ndef decodes(self, to:TabularTS):\n    to.xs = (to.xs*self.stds ) + self.means\n    return to\n\n# Cell\ndef _maybe_expand(o): return o[:,None] if o.ndim==1 else o\n\n# Cell\nclass ReadTSBatch(ItemTransform):\n    def __init__(self, to): self.to = to\n\n    def encodes(self, to):\n        res = (tensor(to.xs).float().unsqueeze(1), )\n        ys = [n for n in to.y_names if n in to.items.columns]\n        if len(ys) == len(to.y_names): res = res + (tensor(to.targ),)\n        if to.device is not None: res = to_device(res, to.device)\n        return res\n\n    def decodes(self, o):\n        o = [_maybe_expand(o_) for o_ in to_np(o) if o_.size != 0]\n        vals = np.concatenate(o, axis=1)\n        try: df = pd.DataFrame(vals, columns=self.to.all_col_names)\n        except: df = pd.DataFrame(vals, columns=self.to.x_names)\n        to = self.to.new(df)\n        return to\n\n# Cell\n@typedispatch\ndef show_batch(x: TabularTS, y, its, max_n=10, ctxs=None):\n    x.show()\n\n# Cell\n@delegates()\nclass TabularTSDataloader(TfmdDL):\n    do_item = noops\n    def __init__(self, dataset, bs=16, shuffle=False, after_batch=None, num_workers=0, **kwargs):\n        if after_batch is None: after_batch = L(TransformBlock().batch_tfms)+ReadTSBatch(dataset)\n        super().__init__(dataset, bs=bs, shuffle=shuffle, after_batch=after_batch, num_workers=num_workers, **kwargs)\n\n    def create_batch(self, b): return self.dataset.iloc[b]\n\nTSPandas._dl_type = TabularTSDataloader\n\n# Cell\ndef stack_train_valid(df_train, df_valid):\n    ""Stack df_train and df_valid, adds `valid_col`=True/False for df_valid/df_train""\n    return pd.concat([df_train.assign(valid_col=False), df_valid.assign(valid_col=True)]).reset_index(drop=True)'"
v1/download.py,0,"b'from pathlib import Path\nfrom typing import Union\nimport os\nimport shutil\nimport requests\nfrom fastprogress.fastprogress import progress_bar\nimport zipfile\n\n\nPathOrStr = Union[Path,str]\nLOCAL_PATH = Path.cwd()\nDATA_PATH = Path.home()/\'tmp\'\nUCR_LINK = \'http://www.timeseriesclassification.com/Downloads/Archives/Univariate2018_arff.zip\'\n\n## From fastai\n\ndef ifnone(a, b): return b if a is None else a\ndef url2name(url): return url.split(\'/\')[-1].split(\'.\')[0]\n\ndef url2path(url, ext=\'.zip\'):\n    ""Change `url` to a path.""\n    name = url2name(url)\n    return DATA_PATH/(name+ext)\n\ndef datapath4file(filename, ext:str=\'.zip\'):\n    local_path = LOCAL_PATH/\'data\'/filename\n    return local_path\n\ndef download_data(url:str, fname:PathOrStr, data:bool=True, ext:str=\'.tgz\') -> Path:\n    ""Download `url` to destination `fname`.""\n    fname = Path(fname)\n    os.makedirs(fname.parent, exist_ok=True)\n    if not fname.exists():\n        print(f\'Downloading {url}\')\n        download_url(url, fname)\n    return fname\n\ndef unzip_data(url:str=UCR_LINK, fname:PathOrStr=None, dest:PathOrStr=None, force_download=False) -> Path:\n    ""Download `url` to `fname` if `dest` doesn\'t exist, and un-zip to folder `dest`.""\n    fname = Path(ifnone(fname, url2path(url)))\n    dest = LOCAL_PATH/\'Univariate_arff\' if dest is None else Path(dest)\n    fname = Path(ifnone(fname, url2path(url)))\n    if force_download:\n        print(f""A new version of the dataset is available."")\n        if fname.exists(): os.remove(fname)\n        if dest.exists(): shutil.rmtree(dest)\n    if not dest.exists():\n        fname = download_data(url, fname=fname)\n        with zipfile.ZipFile(fname, \'r\') as zip_ref:\n            zip_ref.extractall(dest.parent)\n    else: print(f\'Files present in : {dest}\')\n    return dest\n\ndef download_url(url:str, dest:str, overwrite:bool=False,\n                 show_progress=True, chunk_size=1024*1024, timeout=4, retries=5)->None:\n    ""Download `url` to `dest` unless it exists and not `overwrite`.""\n    if os.path.exists(dest) and not overwrite: return\n\n    s = requests.Session()\n    s.mount(\'http://\',requests.adapters.HTTPAdapter(max_retries=retries))\n    u = s.get(url, stream=True, timeout=timeout)\n    try: file_size = int(u.headers[""Content-Length""])\n    except: show_progress = False\n\n    with open(dest, \'wb\') as f:\n        nbytes = 0\n        if show_progress: pbar = progress_bar(range(file_size), auto_update=False, leave=False)\n        try:\n            for chunk in u.iter_content(chunk_size=chunk_size):\n                nbytes += len(chunk)\n                if show_progress: pbar.update(nbytes)\n                f.write(chunk)\n        except requests.exceptions.ConnectionError as e:\n            print(f\'Try downloading your file manually from {url}\')\n            import sys;sys.exit(1)'"
v1/inception.py,2,"b'\nimport torch\nimport torch.nn as nn\nimport fastai\nfrom fastai.vision import *\nfrom models import *\n\nact_fn = nn.ReLU(inplace=True)\ndef conv(ni, nf, ks=3, stride=1, bias=False):\n    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n\nclass Shortcut(Module):\n    ""Merge a shortcut with the result of the module by adding them. Adds Conv, BN and ReLU""\n    def __init__(self, ni, nf, act_fn=act_fn): \n        self.act_fn=act_fn\n        self.conv=conv(ni, nf, 1)\n        self.bn=nn.BatchNorm1d(nf)\n    def forward(self, x): return act_fn(x + self.bn(self.conv(x.orig)))\n\nclass InceptionModule(Module):\n    ""An inception module for TimeSeries, based on https://arxiv.org/pdf/1611.06455.pdf""\n    def __init__(self, ni, nb_filters=32, kss=[39, 19, 9], bottleneck_size=32, stride=1):\n        if (bottleneck_size>0 and ni>1): self.bottleneck = conv(ni, bottleneck_size, 1, stride)\n        else: self.bottleneck = noop\n        self.convs = nn.ModuleList([conv(bottleneck_size if (bottleneck_size>1 and ni>1) else ni, nb_filters, ks) for ks in listify(kss)])\n        self.conv_bottle = nn.Sequential(nn.MaxPool1d(3, stride, padding=1), conv(ni, nb_filters, 1))\n        self.bn_relu = nn.Sequential(nn.BatchNorm1d((len(kss)+1)*nb_filters), nn.ReLU())\n    def forward(self, x):\n        bottled = self.bottleneck(x)\n        return self.bn_relu(torch.cat([c(bottled) for c in self.convs]+[self.conv_bottle(x)], dim=1))\n\ndef create_inception(ni, nout, kss=[39, 19, 9], depth=6, bottleneck_size=32, nb_filters=32, head=True):\n    ""Inception time architecture""\n    layers = []\n    n_ks = len(kss) + 1\n    for d in range(depth):\n        im = SequentialEx(InceptionModule(ni if d==0 else n_ks*nb_filters, kss=kss, bottleneck_size=bottleneck_size))\n        if d%3==2: im.append(Shortcut(n_ks*nb_filters, n_ks*nb_filters))      \n        layers.append(im)\n    head = [AdaptiveConcatPool1d(), Flatten(), nn.Linear(2*n_ks*nb_filters, nout)] if head else []\n    return  nn.Sequential(*layers, *head)\n\n\n##################\n##Inception Resnet\n##################\n\ndef res_block_1d(nf, ks=[5,3]):\n    ""Resnet block as described in the paper.""\n    return SequentialEx(conv_layer(nf, nf, ks=ks[0]),\n                        conv_layer(nf, nf, ks=ks[1], zero_bn=True, act=False),\n                        MergeLayer())\n\ndef create_inception_resnet(ni, nout, kss=[3,5,7], conv_sizes=[64, 128, 256], stride=1, head=True): \n    ""A resnet with only 1 inception layer""\n    layers = []\n    sizes = zip([ni]+conv_sizes, conv_sizes)\n    for n1, n2 in sizes:\n        layers += [InceptionModule(n1, n2//(len(kss)+1), kss=kss) if n1==1 else conv_layer(n1, n2, ks=kss[0], stride=stride), res_block_1d(n2, kss[1:3])]\n    head = [AdaptiveConcatPool1d(), Flatten(),  nn.Linear(2*n2, nout)] if head else []\n    return nn.Sequential(*layers, *head)\n\n'"
v1/models.py,3,"b'import torch\nimport torch.nn as nn\nimport fastai\nfrom fastai.vision import *\n\n"""""" Helper functions to create a improved version of the 1D resnet Architecture proposed in Wang et Al 2017:\nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7966039&tag=1\nMostly based on the crappy implementation for keras found here:\nhttps://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline/blob/master/ResNet.py\n\nChanges:\nks of 1st conv is 9 instead of 8.\nAdaptiveConcatPool instead of Max pool (better)\nInversed ReLU and BatchNorm Layers\n""""""\n\n# or: ELU+init (a=0.54; gain=1.55)\nact_fn = nn.ReLU(inplace=True)\n\ndef conv(ni, nf, ks=3, stride=1, bias=False):\n    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n\ndef noop(x): return x\n\ndef init_cnn(m):\n    if getattr(m, \'bias\', None) is not None: nn.init.constant_(m.bias, 0)\n    if isinstance(m, (nn.Conv1d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n    for l in m.children(): init_cnn(l)\n\ndef conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n    bn = nn.BatchNorm1d(nf)\n    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n    layers = [conv(ni, nf, ks, stride=stride), bn]\n    if act: layers.append(act_fn)\n    return nn.Sequential(*layers)\n\nclass AdaptiveConcatPool1d(nn.Module):\n    ""Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`.""\n    def __init__(self, sz:Optional[int]=None):\n        ""Output will be 2*sz or 2 if sz is None""\n        super().__init__()\n        self.output_size = sz or 1\n        self.ap = nn.AdaptiveAvgPool1d(self.output_size)\n        self.mp = nn.AdaptiveMaxPool1d(self.output_size)\n    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n\ndef res_block_1d(nf, ks=[5,3]):\n    ""Resnet block as described in the paper.""\n    return SequentialEx(conv_layer(nf, nf, ks=ks[0]),\n                        conv_layer(nf, nf, ks=ks[1], zero_bn=True, act=False),\n                        MergeLayer())\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, expansion, ni, nh, stride=1):\n        super().__init__()\n        nf,ni = nh*expansion,ni*expansion\n        layers  = [conv_layer(ni, nh, 5, stride=stride),\n                   conv_layer(nh, nf, 3, zero_bn=True, act=False)\n        ] if expansion == 1 else [\n                   conv_layer(ni, nh, 5),\n                   conv_layer(nh, nh, 3, stride=stride),\n                   conv_layer(nh, nf, 1, zero_bn=True, act=False)\n        ]\n        self.convs = nn.Sequential(*layers)\n        # TODO: check whether act=True works better\n        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False)\n        self.pool = noop if stride==1 else nn.AvgPool1d(2, ceil_mode=True)\n\n    def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x)))\n\n\nclass XResNet(nn.Sequential):\n    def __init__(self, expansion, layers, c_in=1, c_out=3):\n        stem = []\n        sizes = [c_in,32,32,64]\n        # for i in range(3):\n        #     stem.append(conv_layer(sizes[i], sizes[i+1], ks=9 if i==0 else 5, stride=2 if i==0 else 1))\n        stem.append(conv_layer(c_in, 64, ks=9, stride=1))\n        block_szs = [64//expansion,64,128,256]\n        blocks = [self._make_layer(expansion, block_szs[i], block_szs[i+1], l, 1)\n                  for i,l in enumerate(layers)]\n        super().__init__(\n            *stem,\n            *blocks,\n            AdaptiveConcatPool1d(1), Flatten(),\n            nn.Linear(2*block_szs[-2]*expansion, c_out),\n        )\n        init_cnn(self)\n\n    def _make_layer(self, expansion, ni, nf, blocks, stride):\n        return nn.Sequential(\n            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)\n              for i in range(blocks)])\n\n\ndef create_resnet(ni, nout, kss=[9,5,3], conv_sizes=[64, 128, 128], stride=1): \n    ""Basic 11 Layer - 1D resnet builder""\n    layers = []\n    sizes = zip([ni]+conv_sizes, conv_sizes)\n    for n1, n2 in sizes:\n            layers += [conv_layer(n1, n2, ks=kss[0], stride=stride),\n                       res_block_1d(n2, kss[1:3])]\n    return nn.Sequential(*layers, \n                         AdaptiveConcatPool1d(),\n                         Flatten(),\n                        *bn_drop_lin(2*n2, nout, p=0.1)\n                        )\n\ndef create_xresnet(ni, nout, ks=9, conv_sizes=[1,2], expansion=1): \n    return XResNet(expansion=expansion, layers=conv_sizes, c_in=ni, c_out=nout)\n\n\ndef create_fcn(ni, nout, ks=9, conv_sizes=[128, 256, 128], stride=1):\n    layers = []\n    sizes = zip([ni]+conv_sizes, conv_sizes)\n    for n1, n2 in sizes:\n            layers += [conv_layer(n1, n2, ks=ks, stride=stride)]\n    return nn.Sequential(*layers, \n                         AdaptiveConcatPool1d(),\n                         Flatten(),\n                         *bn_drop_lin(2*n2, nout)\n                         )\n\ndef create_mlp(ni, nout, linear_sizes=[500, 500, 500]):\n    layers = []\n    sizes = zip([ni]+linear_sizes, linear_sizes+[nout])\n    for n1, n2 in sizes:\n            layers += bn_drop_lin(n1, n2, p=0.2, actn=act_fn if n2!=nout else None)\n    return nn.Sequential(Flatten(),\n                         *layers)\n\nclass Cat(Module):\n    ""Concatenate layers outputs over a given dim""\n    def __init__(self, *layers, dim=1): \n        self.layers = nn.ModuleList(layers)\n        self.dim=dim\n    def forward(self, x):\n        return torch.cat([l(x) for l in self.layers], dim=self.dim)\n\nclass Noop(Module):\n    def forward(self, x): return x'"
v1/res2net.py,13,"b'#!usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\nImplementation of Res2Net with extended modifications (Res2Net-Plus):\nImprovements:  3x3 stem instead of 7x7, BN before activation, Mish activation instead of ReLU\nthis file: https://github.com/lessw2020/res2net-plus\nall based on original paper and impl:\nhttps://arxiv.org/abs/1904.01169v2\nthen based on https://github.com/gasvn/Res2Net\nthen based on:\nhttps://github.com/frgfm/Holocron/blob/master/holocron/models/res2net.py\nand finally:\nhttps://github.com/lessw2020/res2net-plus\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models.resnet import conv1x1, conv3x3\nfrom torchvision.models.utils import load_state_dict_from_url\n\nfrom fastai.torch_core import *\nimport torch.nn as nn\nimport torch,math,sys\nimport torch.utils.model_zoo as model_zoo\nfrom functools import partial\n#from ...torch_core import Module\nfrom fastai.torch_core import Module\n\nimport torch.nn.functional as F  #(uncomment if needed,but you likely already have it)\n\n\n\n\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):  \n        return x *( torch.tanh(F.softplus(x))) \n        \nact_fn = Mish()\n\ndef conv(ni, nf, ks=3, stride=1, bias=False):\n    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n\n\nclass Res2Block(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=4, dilation=1, scale=4, first_block=False, norm_layer=None):\n        """"""Implements a residual block\n        Args:\n            inplanes (int): input channel dimensionality\n            planes (int): output channel dimensionality\n            stride (int): stride used for conv3x3\n            downsample (torch.nn.Module): module used for downsampling\n            groups: num of convolution groups\n            base_width: base width\n            dilation (int): dilation rate of conv3x3            \n            scale (int): scaling ratio for cascade convs\n            first_block (bool): whether the block is the first to be placed in the conv layer\n            norm_layer (torch.nn.Module): norm layer to be used in blocks\n        """"""\n        super(Res2Block, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm1d\n\n        width = int(planes * (base_width / 64.)) * groups\n\n        self.conv1 = conv(inplanes, width * scale, 1)\n        self.bn1 = norm_layer(width * scale)\n\n        # If scale == 1, single conv else identity & (scale - 1) convs\n        nb_branches = max(scale, 2) - 1\n        if first_block:\n            self.pool = nn.AvgPool1d(kernel_size=3, stride=stride, padding=1)\n        self.convs = nn.ModuleList([conv(width, width, 3, stride)\n                                    for _ in range(nb_branches)])\n        self.bns = nn.ModuleList([norm_layer(width) for _ in range(nb_branches)])\n        self.first_block = first_block\n        self.scale = scale\n\n        self.conv3 = conv(width * scale, planes * self.expansion, 1)\n        \n        self.relu = Mish() #nn.ReLU(inplace=False)\n        self.bn3 = norm_layer(planes * self.expansion)  #bn reverse\n\n        self.downsample = downsample\n\n    def forward(self, x):\n\n        residual = x\n\n        out = self.conv1(x)\n        \n        out = self.relu(out)\n        out = self.bn1(out) #bn reverse\n\n        # Chunk the feature map\n        xs = torch.chunk(out, self.scale, dim=1)\n        # Initialize output as empty tensor for proper concatenation\n        y = 0\n        for idx, conv in enumerate(self.convs):\n            # Add previous y-value\n            if self.first_block:\n                y = xs[idx]\n            else:\n                y += xs[idx]\n            y = conv(y)\n            y = self.relu(self.bns[idx](y))\n            # Concatenate with previously computed values\n            out = torch.cat((out, y), 1) if idx > 0 else y\n        # Use last chunk as x1\n        if self.scale > 1:\n            if self.first_block:\n                out = torch.cat((out, self.pool(xs[len(self.convs)])), 1)\n            else:\n                out = torch.cat((out, xs[len(self.convs)]), 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\ndef conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n    bn = nn.BatchNorm1d(nf)\n    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n    if act:\n        layers = [conv(ni, nf, ks, stride=stride), act_fn, bn]\n    else:\n        layers = [conv(ni, nf, ks, stride=stride), bn]\n        \n    \n    #if act: layers.append(act_fn)\n    return nn.Sequential(*layers)\n\n\nclass Res2Net(nn.Module):\n    """"""Implements a Res2Net model as described in https://arxiv.org/pdf/1904.01169.pdf\n    Args:\n        block (torch.nn.Module): class constructor to be used for residual blocks\n        layers (list<int>): layout of layers\n        num_classes (int): number of output classes\n        zero_init_residual (bool): whether the residual connections should be initialized at zero\n        groups (int): number of convolution groups\n        width_per_group (int): number of channels per group\n        scale (int): scaling ratio within blocks\n        replace_stride_with_dilation (list<bool>): whether stride should be traded for dilation\n        norm_layer (torch.nn.Module): norm layer to be used\n    """"""\n\n    def __init__(self, block, layers, c_in=3,num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=26, scale=4, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(Res2Net, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm1d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        \n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(""replace_stride_with_dilation should be None ""\n                             ""or a 3-element tuple, got {}"".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.scale = scale\n        #self.conv1 = nn.Conv1d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n        #                       bias=False)\n        #modify stem\n        #stem = []\n        sizes = [c_in,32,64,64]  #modified per Grankin\n        #for i in range(3):\n        #    stem.append(conv_layer(sizes[i], sizes[i+1], stride=2 if i==0 else 1))\n        \n        #stem (initial entry layers)\n        self.conv1 = conv_layer(c_in, sizes[1], stride=2)\n        self.conv2 = conv_layer(sizes[1],sizes[2])\n        self.conv3 = conv_layer(sizes[2],sizes[3])\n        \n        \n        \n        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n        \n        #nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottle2neck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv(self.inplanes, planes * block.expansion, 1, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, self.scale, first_block=True, norm_layer=norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                scale=self.scale, first_block=False, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \n        #stem layers\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        \n        x = self.maxpool(x)\n        \n        #res2 block layers\n        x = self.layer1(x)\n        # print(\'1: \', x.shape)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef create_res2net(ni, nout, layers=[3, 4, 6, 3], scale=4, width=26):\n    return Res2Net(Res2Block, layers, c_in=ni, num_classes=nout, scale=scale, width_per_group=width)'"
v1/ucr.py,2,"b'import fastai\nimport torch\nimport torch.nn as nn\nfrom models import *\nfrom inception import *\nfrom res2net import create_res2net\nfrom utils import *\nfrom download import unzip_data\nfrom fastai.script import *\nfrom fastai.vision import *\nfrom tabulate import tabulate\nimport time\n\n""runs a bucnh of archs over UCR dataset""\n\n\ndef get_lists(df_train, df_test):\n    ""get item lists to create databunch""\n    x_train, y_train, x_test, y_test = process_dfs(df_train, df_test)\n    train_list = ItemList(x_train[:, None, :])\n    test_list = ItemList(x_test[:, None, :])\n    return ItemLists(\'.\', train_list, test_list).label_from_lists(y_train, y_test, label_cls=CategoryList)\n\ndef to_TDS(x,y):\n    return TensorDataset(torch.Tensor(x).unsqueeze(dim=1),  torch.Tensor(y).long())\n\ndef process_dfs(df_train, df_test, unsqueeze=False):\n    df_train.dropna(inplace=True)\n    df_test.dropna(inplace=True)\n    num_classes = df_train.target.nunique()\n    x_train, y_train = df_train.values[:,:-1].astype(\'float32\'), df_train.values[:,-1].astype(\'int\')\n    x_test, y_test = df_test.values[:,:-1].astype(\'float32\'), df_test.values[:,-1].astype(\'int\')\n\n    x_train_mean = x_train.mean()\n    x_train_std = x_train.std()\n    #scale\n    x_train = (x_train - x_train_mean)/(x_train_std)\n    x_test = (x_test - x_train_mean)/(x_train_std)\n\n    if not unsqueeze: return x_train.astype(\'float32\'), y_train.astype(\'int\'), x_test.astype(\'float32\'), y_test.astype(\'int\')\n    else: return (x_train[:,None, :].astype(\'float32\'), y_train.astype(\'int\'), \n                  x_test[:, None, :].astype(\'float32\'),  y_test.astype(\'int\'))\n\ndef max_bs(N):\n    N = N//6\n    k=1\n    while (N//2**k)>1: k+=1\n    return min(2**k, 32)\n\ndef create_databunch(src, bs=64):\n    tr_ds, val_ds = src.train, src.valid\n    drop_last = True if (len(tr_ds)%bs==1 or len(val_ds)%bs==1) else False #pytorch batchnorm fails with bs=1\n    train_dl = DataLoader(tr_ds, batch_size=bs, shuffle=True, drop_last=drop_last)\n    valid_dl = DataLoader(val_ds, batch_size=2*bs, shuffle=True, drop_last=drop_last)\n    return DataBunch(train_dl, valid_dl)\n\ndef train_task(path, task=\'Adiac\', arch=\'resnet\', epochs=40, lr=5e-4, mixup=False, one_cycle=True):\n    ""trains arch over task with params""\n    df_train, df_test = load_df(path, task)\n    num_classes = df_train.target.nunique()\n    src = get_lists(df_train, df_test)\n    #compute bs\n    bs = max_bs(len(src.train))\n    print(f\'Training for {epochs} epochs with lr = {lr}, bs={bs}\')\n    db = create_databunch(src, bs)\n    if arch.lower() == \'resnet\':\n        model = create_resnet(1, num_classes, conv_sizes=[64, 128, 256])\n    elif arch.lower() == \'fcn\':\n        model = create_fcn(1, num_classes, ks=9, conv_sizes=[128, 256, 128])\n    elif arch.lower() == \'mlp\':\n        model = create_mlp(src.train[0][0].shape[1], num_classes)\n    elif arch.lower() == \'iresnet\':\n        model = create_inception_resnet(1, num_classes, kss=[39, 19, 9], conv_sizes=[128, 128, 256], stride=1)\n    elif arch.lower() == \'inception\':\n        model = create_inception(1, num_classes)\n    elif arch.lower() == \'res2net\':\n        model = create_res2net(1, num_classes)\n    else: \n        print(\'Please chosse a model in [resnet, FCN, MLP, inception, iresnet]\')\n        return None\n    learn = fastai.basic_train.Learner(db, \n                                       model, \n                                       loss_func = CrossEntropyFlat(), \n                                       metrics=[accuracy],\n                                       wd=1e-2)\n    if mixup: learn = learn.mixup()\n    if one_cycle: learn.fit_one_cycle(epochs, lr)   \n    else: learn.fit_fc(epochs, lr)                             \n    return learn\n\ndef compute_metrics(learn):\n    ""compute oguiza Metrics on UCR""\n    early_stop = math.ceil(np.argmin(learn.recorder.losses) / len(learn.data.train_dl))\n    acc_ = learn.recorder.metrics[-1][0].item()\n    acces_ = learn.recorder.metrics[early_stop - 1][0].item()\n    accmax_ = np.max(learn.recorder.metrics)\n    loss_ = learn.recorder.losses[-1].item()\n    val_loss_ = learn.recorder.val_losses[-1].item()\n    return acc_, acces_, accmax_, loss_, val_loss_ \n\n\n@call_parse\ndef main(arch:Param(""Network arch. [resnet, FCN, MLP, inception, iresnet, res2net, All]. (default: \\\'resnet\\\')"", str)=\'resnet\',\n         tasks:Param(""Which tasks from UCR to run, [task, All]. (default: \\\'All\\\')"", str)=\'Adiac\',\n         epochs:Param(""Number of epochs.(default: 40)"", int)=40,\n         lr:Param(""Learning rate.(default: 1e-3)"", float)=1e-3, \n         mixup:Param(""Use Mixup"", bool)=False, \n         one_cycle:Param(""Use once_cycle policy"", bool)=True,\n         filename:Param(""output filename"", str)=None,\n         ):\n    ""Training UCR script""\n    path = unzip_data()\n    summary = pd.read_csv(path/\'SummaryData.csv\', index_col=0)\n    flist = summary.index\n    archs = [\'MLP\', \'FCN\', \'resnet\', \'iresnet\', \'inception\'] if arch.lower()==\'all\' else [arch]\n    if tasks.lower()==\'all\':tasks=flist\n    elif tasks.lower()==\'bench\':\n        tasks =  [ \'Wine\', \'BeetleFly\', \'InlineSkate\', \'MiddlePhalanxTW\', \'OliveOil\', \'SmallKitchenAppliances\', \'WordSynonyms\', \n                \'MiddlePhalanxOutlineAgeGroup\', \'MoteStrain\', \'Phoneme\', \'Herring\', \'ScreenType\', \'ChlorineConcentration\'] \n    else: tasks = [tasks]\n    print(f\'Training UCR with {archs} tasks: {tasks}\')\n    columns = [\'epochs\', \'loss\', \'val_loss\', \'accuracy\', \'accuracy_ts\', \'max_accuracy\', \'time (s)\']\n    results = pd.DataFrame(index=tasks, columns=pd.MultiIndex.from_product([archs, columns]))\n    for task in tasks:\n        for model in archs:\n            try:\n                print(f\'\\n>>Training {model} over {task}\')\n                start_time = time.time()\n                learner = train_task(path, task, model, epochs, lr, mixup, one_cycle)\n                acc_, acces_, accmax_, loss_, val_loss_  = compute_metrics(learner)\n                duration = \'{:.0f}\'.format(time.time() - start_time)\n                results.loc[task, (model, slice(None))] = epochs, loss_, val_loss_ ,acc_, acces_, accmax_, duration\n            except Exception as e: \n                print(\'>>Error ocurred:\', e)\n                print(task,model)\n                pass\n    \n    fname = \'-\'.join(archs)\n    tnames = \'-\'.join(tasks)\n    filename = ifnone(filename, f\'results_{tnames}_{fname}\')\n    \n    try:\n        print(results.head())\n        results.to_hdf(filename + \'.hdf\', key=\'df\')\n    except:\n        print(""problem saving to HDF, saving to csv"")\n        results.to_csv(filename + \'.csv\', header=True)\n    table = results.loc[slice(None), (slice(None), \'accuracy\')]\n    print(tabulate(table,  tablefmt=""pipe"", headers=table.columns.levels[0]))'"
v1/utils.py,1,"b'import numpy as np\nimport pandas as pd\nfrom scipy.io import arff\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport torch.nn as nn\nfrom fastai.callbacks.hooks import params_size\n\n""this functions are based on https://github.com/mb4310/Time-Series""\n\ndef load_df(path, task):\n    ""Loads arff files from UCR""\n    try:\n        print(f\'Loading files from: {path}/{task}\')\n        dfs = []\n        for file in [\'TRAIN\', \'TEST\']:\n            filename = f\'{task}/{task}_{file}.arff\'\n            data = arff.loadarff(path/filename)\n            dfs.append(pd.DataFrame(data[0]))\n        return dfs\n    except:\n        print(\'Error loading files\')'"
