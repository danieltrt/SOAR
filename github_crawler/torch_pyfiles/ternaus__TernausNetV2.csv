file_path,api_count,code
models/__init__.py,0,b''
models/ternausnet2.py,10,"b'""""""The network definition that was used for a second place solution at the DeepGlobe Building Detection challenge.""""""\n\nimport torch\nfrom torch import nn\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nfrom torch.nn import Sequential\nfrom collections import OrderedDict\nfrom modules.bn import ABN\n\nfrom modules.wider_resnet import WiderResNet\n\n\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super(ConvRelu, self).__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    """"""Paramaters for Deconvolution were chosen to avoid artifacts, following\n    link https://distill.pub/2016/deconv-checkerboard/\n    """"""\n\n    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=False):\n        super(DecoderBlock, self).__init__()\n        self.in_channels = in_channels\n\n        if is_deconv:\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n                                   padding=1),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.block = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\'nearest\'),\n                ConvRelu(in_channels, middle_channels),\n                ConvRelu(middle_channels, out_channels)\n            )\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass TernausNetV2(nn.Module):\n    """"""Variation of the UNet architecture with InplaceABN encoder.""""""\n\n    def __init__(self, num_classes=1, num_filters=32, is_deconv=False, num_input_channels=11, **kwargs):\n        """"""\n\n        Args:\n            num_classes: Number of output classes.\n            num_filters:\n            is_deconv:\n                True: Deconvolution layer is used in the Decoder block.\n                False: Upsampling layer is used in the Decoder block.\n            num_input_channels: Number of channels in the input images.\n        """"""\n        super(TernausNetV2, self).__init__()\n\n        if \'norm_act\' not in kwargs:\n            norm_act = ABN\n        else:\n            norm_act = kwargs[\'norm_act\']\n\n        self.pool = nn.MaxPool2d(2, 2)\n\n        encoder = WiderResNet(structure=[3, 3, 6, 3, 1, 1], classes=1000, norm_act=norm_act)\n\n        self.conv1 = Sequential(\n            OrderedDict([(\'conv1\', nn.Conv2d(num_input_channels, 64, 3, padding=1, bias=False))]))\n        self.conv2 = encoder.mod2\n        self.conv3 = encoder.mod3\n        self.conv4 = encoder.mod4\n        self.conv5 = encoder.mod5\n\n        self.center = DecoderBlock(1024, num_filters * 8, num_filters * 8, is_deconv=is_deconv)\n        self.dec5 = DecoderBlock(1024 + num_filters * 8, num_filters * 8, num_filters * 8, is_deconv=is_deconv)\n        self.dec4 = DecoderBlock(512 + num_filters * 8, num_filters * 8, num_filters * 8, is_deconv=is_deconv)\n        self.dec3 = DecoderBlock(256 + num_filters * 8, num_filters * 2, num_filters * 2, is_deconv=is_deconv)\n        self.dec2 = DecoderBlock(128 + num_filters * 2, num_filters * 2, num_filters, is_deconv=is_deconv)\n        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(self.pool(conv1))\n        conv3 = self.conv3(self.pool(conv2))\n        conv4 = self.conv4(self.pool(conv3))\n        conv5 = self.conv5(self.pool(conv4))\n\n        center = self.center(self.pool(conv5))\n\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n        return self.final(dec1)\n'"
modules/__init__.py,0,b''
modules/bn.py,6,"b'""""""Adapted from https://github.com/mapillary/inplace_abn .""""""\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional\n\nfrom queue import Queue\n\nfrom .functions import (ACT_ELU, ACT_RELU, ACT_LEAKY_RELU, inplace_abn, inplace_abn_sync)\n\n\nclass ABN(nn.Module):\n    """"""Activated Batch Normalization.\n\n    This gathers a `BatchNorm2d` and an activation function in a single module\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(ABN, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.ones(num_features))\n            self.bias = nn.Parameter(torch.zeros(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.constant_(self.running_mean, 0)\n        nn.init.constant_(self.running_var, 1)\n        if self.affine:\n            nn.init.constant_(self.weight, 1)\n            nn.init.constant_(self.bias, 0)\n\n    def forward(self, x):\n        x = functional.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias,\n                                  self.training, self.momentum, self.eps)\n\n        if self.activation == ACT_RELU:\n            return functional.relu(x, inplace=True)\n        elif self.activation == ACT_LEAKY_RELU:\n            return functional.leaky_relu(x, negative_slope=self.slope, inplace=True)\n        elif self.activation == ACT_ELU:\n            return functional.elu(x, inplace=True)\n        else:\n            return x\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABN(ABN):\n    """"""InPlace Activated Batch Normalization.""""""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABN, self).__init__(num_features, eps, momentum, affine, activation, slope)\n\n    def forward(self, x):\n        return inplace_abn(x, self.weight, self.bias, self.running_mean, self.running_var,\n                           self.training, self.momentum, self.eps, self.activation, self.slope)\n\n\nclass InPlaceABNSync(ABN):\n    """"""InPlace Activated Batch Normalization with cross-GPU synchronization.\n\n    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DataParallel`.\n    """"""\n\n    def __init__(self, num_features, devices=None, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"",\n                 slope=0.01):\n        """"""Creates a synchronized, InPlace Activated Batch Normalization module.\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        devices : list of int or None\n            IDs of the GPUs that will run the replicas of this module.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABNSync, self).__init__(num_features, eps, momentum, affine, activation, slope)\n        self.devices = devices if devices else list(range(torch.cuda.device_count()))\n\n        # Initialize queues\n        self.worker_ids = self.devices[1:]\n        self.master_queue = Queue(len(self.worker_ids))\n        self.worker_queues = [Queue(1) for _ in self.worker_ids]\n\n    def forward(self, x):\n        if x.get_device() == self.devices[0]:\n            # Master mode\n            extra = {\n                ""is_master"": True,\n                ""master_queue"": self.master_queue,\n                ""worker_queues"": self.worker_queues,\n                ""worker_ids"": self.worker_ids\n            }\n        else:\n            # Worker mode\n            extra = {\n                ""is_master"": False,\n                ""master_queue"": self.master_queue,\n                ""worker_queue"": self.worker_queues[self.worker_ids.index(x.get_device())]\n            }\n\n        return inplace_abn_sync(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                extra, self.training, self.momentum, self.eps, self.activation, self.slope)\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, devices={devices}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n'"
modules/functions.py,3,"b'""""""Adaptation from https://github.com/mapillary/inplace_abn .""""""\n\nfrom os import path\n\nfrom torch import autograd\nfrom torch.cuda import comm\nfrom torch.autograd.function import once_differentiable\nfrom torch.utils.cpp_extension import load\n\n_src_path = path.join(path.dirname(path.abspath(__file__)), ""src"")\n_backend = load(name=""inplace_abn"",\n                extra_cflags=[""-O3""],\n                sources=[path.join(_src_path, f) for f in [\n                    ""inplace_abn.cpp"",\n                    ""inplace_abn_cpu.cpp"",\n                    ""inplace_abn_cuda.cu""\n                ]],\n                extra_cuda_cflags=[""--expt-extended-lambda""])\n\n# Activation names\nACT_RELU = ""relu""\nACT_LEAKY_RELU = ""leaky_relu""\nACT_ELU = ""elu""\nACT_NONE = ""none""\n\n\ndef _check(fn, *args, **kwargs):\n    success = fn(*args, **kwargs)\n    if not success:\n        raise RuntimeError(""CUDA Error encountered in {}"".format(fn))\n\n\ndef _broadcast_shape(x):\n    out_size = []\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            out_size.append(1)\n        else:\n            out_size.append(s)\n    return out_size\n\n\ndef _reduce(x):\n    if len(x.size()) == 2:\n        return x.sum(dim=0)\n    else:\n        n, c = x.size()[0:2]\n        return x.contiguous().view((n, c, -1)).sum(2).sum(0)\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_forward(x, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_forward(x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_backward(x, dx, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_backward(x, dx)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\nclass InPlaceABN(autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        count = _count_samples(x)\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n        else:\n            # TODO: implement simplified CUDA backward for inference mode\n            edz = dz.new_zeros(dz.size(1))\n            eydz = dz.new_zeros(dz.size(1))\n\n        dx, dweight, dbias = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = dweight if ctx.affine else None\n        dbias = dbias if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\n\nclass InPlaceABNSync(autograd.Function):\n    @classmethod\n    def forward(cls, ctx, x, weight, bias, running_mean, running_var,\n                extra, training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        cls._parse_extra(ctx, extra)\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        count = _count_samples(x) * (ctx.master_queue.maxsize + 1)\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n\n            if ctx.is_master:\n                means, vars = [mean.unsqueeze(0)], [var.unsqueeze(0)]\n                for _ in range(ctx.master_queue.maxsize):\n                    mean_w, var_w = ctx.master_queue.get()\n                    ctx.master_queue.task_done()\n                    means.append(mean_w.unsqueeze(0))\n                    vars.append(var_w.unsqueeze(0))\n\n                means = comm.gather(means)\n                vars = comm.gather(vars)\n\n                mean = means.mean(0)\n                var = (vars + (mean - means) ** 2).mean(0)\n\n                tensors = comm.broadcast_coalesced((mean, var), [mean.get_device()] + ctx.worker_ids)\n                for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                    queue.put(ts)\n            else:\n                ctx.master_queue.put((mean, var))\n                mean, var = ctx.worker_queue.get()\n                ctx.worker_queue.task_done()\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n\n            if ctx.is_master:\n                edzs, eydzs = [edz], [eydz]\n                for _ in range(len(ctx.worker_queues)):\n                    edz_w, eydz_w = ctx.master_queue.get()\n                    ctx.master_queue.task_done()\n                    edzs.append(edz_w)\n                    eydzs.append(eydz_w)\n\n                edz = comm.reduce_add(edzs) / (ctx.master_queue.maxsize + 1)\n                eydz = comm.reduce_add(eydzs) / (ctx.master_queue.maxsize + 1)\n\n                tensors = comm.broadcast_coalesced((edz, eydz), [edz.get_device()] + ctx.worker_ids)\n                for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                    queue.put(ts)\n            else:\n                ctx.master_queue.put((edz, eydz))\n                edz, eydz = ctx.worker_queue.get()\n                ctx.worker_queue.task_done()\n        else:\n            edz = dz.new_zeros(dz.size(1))\n            eydz = dz.new_zeros(dz.size(1))\n\n        dx, dweight, dbias = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = dweight if ctx.affine else None\n        dbias = dbias if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def _parse_extra(ctx, extra):\n        ctx.is_master = extra[""is_master""]\n        if ctx.is_master:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queues = extra[""worker_queues""]\n            ctx.worker_ids = extra[""worker_ids""]\n        else:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queue = extra[""worker_queue""]\n\n\ninplace_abn = InPlaceABN.apply\ninplace_abn_sync = InPlaceABNSync.apply\n\n__all__ = [""inplace_abn"", ""inplace_abn_sync"", ""ACT_RELU"", ""ACT_LEAKY_RELU"", ""ACT_ELU"", ""ACT_NONE""]\n'"
modules/misc.py,0,"b'from torch import nn\n\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        return inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n'"
modules/residual.py,0,"b'from __future__ import absolute_import\nfrom collections import OrderedDict\n\nfrom torch import nn\n\nfrom .bn import ABN\n\n\nclass IdentityResidualBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 channels,\n                 stride=1,\n                 dilation=1,\n                 groups=1,\n                 norm_act=ABN,\n                 dropout=None):\n        """"""Configurable identity-mapping residual block\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps. Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        """"""\n        super(IdentityResidualBlock, self).__init__()\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(""channels must contain either two or three values"")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,\n                                    dilation=dilation)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    dilation=dilation))\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]\n        else:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    groups=groups, dilation=dilation)),\n                (""bn3"", norm_act(channels[1])),\n                (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))\n            ]\n            if dropout is not None:\n                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        if hasattr(self, ""proj_conv""):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n\n        return out\n'"
modules/wider_resnet.py,1,"b'""""""Adaptation from https://github.com/mapillary/inplace_abn .""""""\n\nfrom collections import OrderedDict\nimport torch.nn as nn\n\nfrom modules.bn import ABN\nfrom modules.misc import GlobalAvgPool2d\nfrom modules.residual import IdentityResidualBlock\n\n\nclass WiderResNet(nn.Module):\n    def __init__(self,\n                 structure,\n                 norm_act=ABN,\n                 classes=0):\n        """"""Wider ResNet with pre-activation (identity mapping) blocks\n\n        Parameters\n        ----------\n        structure : list of int\n            Number of residual blocks in each of the six modules of the network.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        classes : int\n            If not `0` also include global average pooling and a fully-connected layer with `classes` outputs at the end\n            of the network.\n        """"""\n        super(WiderResNet, self).__init__()\n        self.structure = structure\n\n        if len(structure) != 6:\n            raise ValueError(""Expected a structure with six values"")\n\n        # Initial layers\n        self.mod1 = nn.Sequential(OrderedDict([\n            (""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))\n        ]))\n\n        # Groups of residual blocks\n        in_channels = 64\n        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048), (1024, 2048, 4096)]\n        for mod_id, num in enumerate(structure):\n            # Create blocks for module\n            blocks = []\n            for block_id in range(num):\n                blocks.append((\n                    ""block%d"" % (block_id + 1),\n                    IdentityResidualBlock(in_channels, channels[mod_id], norm_act=norm_act)\n                ))\n\n                # Update channels and p_keep\n                in_channels = channels[mod_id][-1]\n\n            # Create module\n            if mod_id <= 4:\n                self.add_module(""pool%d"" % (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n\n        # Pooling and predictor\n        self.bn_out = norm_act(in_channels)\n        if classes != 0:\n            self.classifier = nn.Sequential(OrderedDict([\n                (""avg_pool"", GlobalAvgPool2d()),\n                (""fc"", nn.Linear(in_channels, classes))\n            ]))\n\n    def forward(self, img):\n        out = self.mod1(img)\n        out = self.mod2(self.pool2(out))\n        out = self.mod3(self.pool3(out))\n        out = self.mod4(self.pool4(out))\n        out = self.mod5(self.pool5(out))\n        out = self.mod6(self.pool6(out))\n        out = self.mod7(out)\n        out = self.bn_out(out)\n\n        if hasattr(self, ""classifier""):\n            out = self.classifier(out)\n\n        return out\n'"
