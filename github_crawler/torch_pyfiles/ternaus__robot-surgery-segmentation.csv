file_path,api_count,code
dataset.py,4,"b'import torch\nimport numpy as np\nimport cv2\nfrom torch.utils.data import Dataset\nimport prepare_data\nfrom albumentations.torch.functional import img_to_tensor\n\n\nclass RoboticsDataset(Dataset):\n    def __init__(self, file_names, to_augment=False, transform=None, mode=\'train\', problem_type=None):\n        self.file_names = file_names\n        self.to_augment = to_augment\n        self.transform = transform\n        self.mode = mode\n        self.problem_type = problem_type\n\n    def __len__(self):\n        return len(self.file_names)\n\n    def __getitem__(self, idx):\n        img_file_name = self.file_names[idx]\n        image = load_image(img_file_name)\n        mask = load_mask(img_file_name, self.problem_type)\n\n        data = {""image"": image, ""mask"": mask}\n        augmented = self.transform(**data)\n        image, mask = augmented[""image""], augmented[""mask""]\n\n        if self.mode == \'train\':\n            if self.problem_type == \'binary\':\n                return img_to_tensor(image), torch.from_numpy(np.expand_dims(mask, 0)).float()\n            else:\n                return img_to_tensor(image), torch.from_numpy(mask).long()\n        else:\n            return img_to_tensor(image), str(img_file_name)\n\n\ndef load_image(path):\n    img = cv2.imread(str(path))\n    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n\ndef load_mask(path, problem_type):\n    if problem_type == \'binary\':\n        mask_folder = \'binary_masks\'\n        factor = prepare_data.binary_factor\n    elif problem_type == \'parts\':\n        mask_folder = \'parts_masks\'\n        factor = prepare_data.parts_factor\n    elif problem_type == \'instruments\':\n        factor = prepare_data.instrument_factor\n        mask_folder = \'instruments_masks\'\n\n    mask = cv2.imread(str(path).replace(\'images\', mask_folder).replace(\'jpg\', \'png\'), 0)\n\n    return (mask / factor).astype(np.uint8)\n'"
evaluate.py,0,"b""from pathlib import Path\nimport argparse\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom prepare_data import height, width, h_start, w_start\n\n\ndef general_dice(y_true, y_pred):\n    result = []\n\n    if y_true.sum() == 0:\n        if y_pred.sum() == 0:\n            return 1\n        else:\n            return 0\n\n    for instrument_id in set(y_true.flatten()):\n        if instrument_id == 0:\n            continue\n        result += [dice(y_true == instrument_id, y_pred == instrument_id)]\n\n    return np.mean(result)\n\n\ndef general_jaccard(y_true, y_pred):\n    result = []\n\n    if y_true.sum() == 0:\n        if y_pred.sum() == 0:\n            return 1\n        else:\n            return 0\n\n    for instrument_id in set(y_true.flatten()):\n        if instrument_id == 0:\n            continue\n        result += [jaccard(y_true == instrument_id, y_pred == instrument_id)]\n\n    return np.mean(result)\n\n\ndef jaccard(y_true, y_pred):\n    intersection = (y_true * y_pred).sum()\n    union = y_true.sum() + y_pred.sum() - intersection\n    return (intersection + 1e-15) / (union + 1e-15)\n\n\ndef dice(y_true, y_pred):\n    return (2 * (y_true * y_pred).sum() + 1e-15) / (y_true.sum() + y_pred.sum() + 1e-15)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    arg = parser.add_argument\n\n    arg('--train_path', type=str, default='data/cropped_train',\n        help='path where train images with ground truth are located')\n    arg('--target_path', type=str, default='predictions/unet11', help='path with predictions')\n    arg('--problem_type', type=str, default='parts', choices=['binary', 'parts', 'instruments'])\n    args = parser.parse_args()\n\n    result_dice = []\n    result_jaccard = []\n\n    if args.problem_type == 'binary':\n        for instrument_id in tqdm(range(1, 9)):\n            instrument_dataset_name = 'instrument_dataset_' + str(instrument_id)\n\n            for file_name in (\n                    Path(args.train_path) / instrument_dataset_name / 'binary_masks').glob('*'):\n                y_true = (cv2.imread(str(file_name), 0) > 0).astype(np.uint8)\n\n                pred_file_name = (Path(args.target_path) / 'binary' / instrument_dataset_name / file_name.name)\n\n                pred_image = (cv2.imread(str(pred_file_name), 0) > 255 * 0.5).astype(np.uint8)\n                y_pred = pred_image[h_start:h_start + height, w_start:w_start + width]\n\n                result_dice += [dice(y_true, y_pred)]\n                result_jaccard += [jaccard(y_true, y_pred)]\n\n    elif args.problem_type == 'parts':\n        for instrument_id in tqdm(range(1, 9)):\n            instrument_dataset_name = 'instrument_dataset_' + str(instrument_id)\n            for file_name in (\n                    Path(args.train_path) / instrument_dataset_name / 'parts_masks').glob('*'):\n                y_true = cv2.imread(str(file_name), 0)\n\n                pred_file_name = Path(args.target_path) / 'parts' / instrument_dataset_name / file_name.name\n\n                y_pred = cv2.imread(str(pred_file_name), 0)[h_start:h_start + height, w_start:w_start + width]\n\n                result_dice += [general_dice(y_true, y_pred)]\n                result_jaccard += [general_jaccard(y_true, y_pred)]\n\n    elif args.problem_type == 'instruments':\n        for instrument_id in tqdm(range(1, 9)):\n            instrument_dataset_name = 'instrument_dataset_' + str(instrument_id)\n            for file_name in (\n                    Path(args.train_path) / instrument_dataset_name / 'instruments_masks').glob('*'):\n                y_true = cv2.imread(str(file_name), 0)\n\n                pred_file_name = Path(args.target_path) / 'instruments' / instrument_dataset_name / file_name.name\n\n                y_pred = cv2.imread(str(pred_file_name), 0)[h_start:h_start + height, w_start:w_start + width]\n\n                result_dice += [general_dice(y_true, y_pred)]\n                result_jaccard += [general_jaccard(y_true, y_pred)]\n\n    print('Dice = ', np.mean(result_dice), np.std(result_dice))\n    print('Jaccard = ', np.mean(result_jaccard), np.std(result_jaccard))\n"""
generate_masks.py,6,"b'""""""\nScript generates predictions, splitting original images into tiles, and assembling prediction back together\n""""""\nimport argparse\nfrom prepare_train_val import get_split\nfrom dataset import RoboticsDataset\nimport cv2\nfrom models import UNet16, LinkNet34, UNet11, UNet, AlbuNet\nimport torch\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nimport utils\nimport prepare_data\nfrom torch.utils.data import DataLoader\nfrom torch.nn import functional as F\nfrom prepare_data import (original_height,\n                          original_width,\n                          h_start, w_start\n                          )\nfrom albumentations import Compose, Normalize\n\n\ndef img_transform(p=1):\n    return Compose([\n        Normalize(p=1)\n    ], p=p)\n\n\ndef get_model(model_path, model_type=\'UNet11\', problem_type=\'binary\'):\n    """"""\n\n    :param model_path:\n    :param model_type: \'UNet\', \'UNet16\', \'UNet11\', \'LinkNet34\', \'AlbuNet\'\n    :param problem_type: \'binary\', \'parts\', \'instruments\'\n    :return:\n    """"""\n    if problem_type == \'binary\':\n        num_classes = 1\n    elif problem_type == \'parts\':\n        num_classes = 4\n    elif problem_type == \'instruments\':\n        num_classes = 8\n\n    if model_type == \'UNet16\':\n        model = UNet16(num_classes=num_classes)\n    elif model_type == \'UNet11\':\n        model = UNet11(num_classes=num_classes)\n    elif model_type == \'LinkNet34\':\n        model = LinkNet34(num_classes=num_classes)\n    elif model_type == \'AlbuNet\':\n        model = AlbuNet(num_classes=num_classes)\n    elif model_type == \'UNet\':\n        model = UNet(num_classes=num_classes)\n\n    state = torch.load(str(model_path))\n    state = {key.replace(\'module.\', \'\'): value for key, value in state[\'model\'].items()}\n    model.load_state_dict(state)\n\n    if torch.cuda.is_available():\n        return model.cuda()\n\n    model.eval()\n\n    return model\n\n\ndef predict(model, from_file_names, batch_size, to_path, problem_type, img_transform):\n    loader = DataLoader(\n        dataset=RoboticsDataset(from_file_names, transform=img_transform, mode=\'predict\', problem_type=problem_type),\n        shuffle=False,\n        batch_size=batch_size,\n        num_workers=args.workers,\n        pin_memory=torch.cuda.is_available()\n    )\n\n    with torch.no_grad():\n        for batch_num, (inputs, paths) in enumerate(tqdm(loader, desc=\'Predict\')):\n            inputs = utils.cuda(inputs)\n\n            outputs = model(inputs)\n\n            for i, image_name in enumerate(paths):\n                if problem_type == \'binary\':\n                    factor = prepare_data.binary_factor\n                    t_mask = (F.sigmoid(outputs[i, 0]).data.cpu().numpy() * factor).astype(np.uint8)\n                elif problem_type == \'parts\':\n                    factor = prepare_data.parts_factor\n                    t_mask = (outputs[i].data.cpu().numpy().argmax(axis=0) * factor).astype(np.uint8)\n                elif problem_type == \'instruments\':\n                    factor = prepare_data.instrument_factor\n                    t_mask = (outputs[i].data.cpu().numpy().argmax(axis=0) * factor).astype(np.uint8)\n\n                h, w = t_mask.shape\n\n                full_mask = np.zeros((original_height, original_width))\n                full_mask[h_start:h_start + h, w_start:w_start + w] = t_mask\n\n                instrument_folder = Path(paths[i]).parent.parent.name\n\n                (to_path / instrument_folder).mkdir(exist_ok=True, parents=True)\n\n                cv2.imwrite(str(to_path / instrument_folder / (Path(paths[i]).stem + \'.png\')), full_mask)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    arg = parser.add_argument\n    arg(\'--model_path\', type=str, default=\'data/models/UNet\', help=\'path to model folder\')\n    arg(\'--model_type\', type=str, default=\'UNet\', help=\'network architecture\',\n        choices=[\'UNet\', \'UNet11\', \'UNet16\', \'LinkNet34\', \'AlbuNet\'])\n    arg(\'--output_path\', type=str, help=\'path to save images\', default=\'1\')\n    arg(\'--batch-size\', type=int, default=4)\n    arg(\'--fold\', type=int, default=-1, choices=[0, 1, 2, 3, -1], help=\'-1: all folds\')\n    arg(\'--problem_type\', type=str, default=\'binary\', choices=[\'binary\', \'parts\', \'instruments\'])\n    arg(\'--workers\', type=int, default=12)\n\n    args = parser.parse_args()\n\n    if args.fold == -1:\n        for fold in [0, 1, 2, 3]:\n            _, file_names = get_split(fold)\n            model = get_model(str(Path(args.model_path).joinpath(\'model_{fold}.pt\'.format(fold=fold))),\n                              model_type=args.model_type, problem_type=args.problem_type)\n\n            print(\'num file_names = {}\'.format(len(file_names)))\n\n            output_path = Path(args.output_path)\n            output_path.mkdir(exist_ok=True, parents=True)\n\n            predict(model, file_names, args.batch_size, output_path, problem_type=args.problem_type,\n                    img_transform=img_transform(p=1))\n    else:\n        _, file_names = get_split(args.fold)\n        model = get_model(str(Path(args.model_path).joinpath(\'model_{fold}.pt\'.format(fold=args.fold))),\n                          model_type=args.model_type, problem_type=args.problem_type)\n\n        print(\'num file_names = {}\'.format(len(file_names)))\n\n        output_path = Path(args.output_path)\n        output_path.mkdir(exist_ok=True, parents=True)\n\n        predict(model, file_names, args.batch_size, output_path, problem_type=args.problem_type,\n                img_transform=img_transform(p=1))\n'"
loss.py,4,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport utils\nimport numpy as np\n\n\nclass LossBinary:\n    """"""\n    Loss defined as \\alpha BCE - (1 - \\alpha) SoftJaccard\n    """"""\n\n    def __init__(self, jaccard_weight=0):\n        self.nll_loss = nn.BCEWithLogitsLoss()\n        self.jaccard_weight = jaccard_weight\n\n    def __call__(self, outputs, targets):\n        loss = (1 - self.jaccard_weight) * self.nll_loss(outputs, targets)\n\n        if self.jaccard_weight:\n            eps = 1e-15\n            jaccard_target = (targets == 1).float()\n            jaccard_output = F.sigmoid(outputs)\n\n            intersection = (jaccard_output * jaccard_target).sum()\n            union = jaccard_output.sum() + jaccard_target.sum()\n\n            loss -= self.jaccard_weight * torch.log((intersection + eps) / (union - intersection + eps))\n        return loss\n\n\nclass LossMulti:\n    def __init__(self, jaccard_weight=0, class_weights=None, num_classes=1):\n        if class_weights is not None:\n            nll_weight = utils.cuda(\n                torch.from_numpy(class_weights.astype(np.float32)))\n        else:\n            nll_weight = None\n        self.nll_loss = nn.NLLLoss2d(weight=nll_weight)\n        self.jaccard_weight = jaccard_weight\n        self.num_classes = num_classes\n\n    def __call__(self, outputs, targets):\n        loss = (1 - self.jaccard_weight) * self.nll_loss(outputs, targets)\n\n        if self.jaccard_weight:\n            eps = 1e-15\n            for cls in range(self.num_classes):\n                jaccard_target = (targets == cls).float()\n                jaccard_output = outputs[:, cls].exp()\n                intersection = (jaccard_output * jaccard_target).sum()\n\n                union = jaccard_output.sum() + jaccard_target.sum()\n                loss -= torch.log((intersection + eps) / (union - intersection + eps)) * self.jaccard_weight\n        return loss\n'"
models.py,16,"b'from torch import nn\nimport torch\nfrom torchvision import models\nimport torchvision\nfrom torch.nn import functional as F\n\n\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_: int, out: int):\n        super(ConvRelu, self).__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    """"""\n    Paramaters for Deconvolution were chosen to avoid artifacts, following\n    link https://distill.pub/2016/deconv-checkerboard/\n    """"""\n\n    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n        super(DecoderBlock, self).__init__()\n        self.in_channels = in_channels\n\n        if is_deconv:\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n                                   padding=1),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.block = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\'bilinear\'),\n                ConvRelu(in_channels, middle_channels),\n                ConvRelu(middle_channels, out_channels),\n            )\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass UNet11(nn.Module):\n    def __init__(self, num_classes=1, num_filters=32, pretrained=False):\n        """"""\n        :param num_classes:\n        :param num_filters:\n        :param pretrained:\n            False - no pre-trained network used\n            True - encoder pre-trained with VGG11\n        """"""\n        super().__init__()\n        self.pool = nn.MaxPool2d(2, 2)\n\n        self.num_classes = num_classes\n\n        self.encoder = models.vgg11(pretrained=pretrained).features\n\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Sequential(self.encoder[0],\n                                   self.relu)\n\n        self.conv2 = nn.Sequential(self.encoder[3],\n                                   self.relu)\n\n        self.conv3 = nn.Sequential(\n            self.encoder[6],\n            self.relu,\n            self.encoder[8],\n            self.relu,\n        )\n        self.conv4 = nn.Sequential(\n            self.encoder[11],\n            self.relu,\n            self.encoder[13],\n            self.relu,\n        )\n\n        self.conv5 = nn.Sequential(\n            self.encoder[16],\n            self.relu,\n            self.encoder[18],\n            self.relu,\n        )\n\n        self.center = DecoderBlock(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv=True)\n        self.dec5 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv=True)\n        self.dec4 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 4, is_deconv=True)\n        self.dec3 = DecoderBlock(256 + num_filters * 4, num_filters * 4 * 2, num_filters * 2, is_deconv=True)\n        self.dec2 = DecoderBlock(128 + num_filters * 2, num_filters * 2 * 2, num_filters, is_deconv=True)\n        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n\n        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(self.pool(conv1))\n        conv3 = self.conv3(self.pool(conv2))\n        conv4 = self.conv4(self.pool(conv3))\n        conv5 = self.conv5(self.pool(conv4))\n        center = self.center(self.pool(conv5))\n\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n\n        if self.num_classes > 1:\n            x_out = F.log_softmax(self.final(dec1), dim=1)\n        else:\n            x_out = self.final(dec1)\n\n        return x_out\n\n\nclass UNet16(nn.Module):\n    def __init__(self, num_classes=1, num_filters=32, pretrained=False):\n        """"""\n        :param num_classes:\n        :param num_filters:\n        :param pretrained:\n            False - no pre-trained network used\n            True - encoder pre-trained with VGG11\n        """"""\n        super().__init__()\n        self.num_classes = num_classes\n\n        self.pool = nn.MaxPool2d(2, 2)\n\n        self.encoder = torchvision.models.vgg16(pretrained=pretrained).features\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv1 = nn.Sequential(self.encoder[0],\n                                   self.relu,\n                                   self.encoder[2],\n                                   self.relu)\n\n        self.conv2 = nn.Sequential(self.encoder[5],\n                                   self.relu,\n                                   self.encoder[7],\n                                   self.relu)\n\n        self.conv3 = nn.Sequential(self.encoder[10],\n                                   self.relu,\n                                   self.encoder[12],\n                                   self.relu,\n                                   self.encoder[14],\n                                   self.relu)\n\n        self.conv4 = nn.Sequential(self.encoder[17],\n                                   self.relu,\n                                   self.encoder[19],\n                                   self.relu,\n                                   self.encoder[21],\n                                   self.relu)\n\n        self.conv5 = nn.Sequential(self.encoder[24],\n                                   self.relu,\n                                   self.encoder[26],\n                                   self.relu,\n                                   self.encoder[28],\n                                   self.relu)\n\n        self.center = DecoderBlock(512, num_filters * 8 * 2, num_filters * 8)\n\n        self.dec5 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8)\n        self.dec4 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8)\n        self.dec3 = DecoderBlock(256 + num_filters * 8, num_filters * 4 * 2, num_filters * 2)\n        self.dec2 = DecoderBlock(128 + num_filters * 2, num_filters * 2 * 2, num_filters)\n        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(self.pool(conv1))\n        conv3 = self.conv3(self.pool(conv2))\n        conv4 = self.conv4(self.pool(conv3))\n        conv5 = self.conv5(self.pool(conv4))\n\n        center = self.center(self.pool(conv5))\n\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n\n        if self.num_classes > 1:\n            x_out = F.log_softmax(self.final(dec1), dim=1)\n        else:\n            x_out = self.final(dec1)\n\n        return x_out\n\n\nclass DecoderBlockLinkNet(nn.Module):\n    def __init__(self, in_channels, n_filters):\n        super().__init__()\n\n        self.relu = nn.ReLU(inplace=True)\n\n        # B, C, H, W -> B, C/4, H, W\n        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n\n        # B, C/4, H, W -> B, C/4, 2 * H, 2 * W\n        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=4,\n                                          stride=2, padding=1, output_padding=0)\n        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n\n        # B, C/4, H, W -> B, C, H, W\n        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n        self.norm3 = nn.BatchNorm2d(n_filters)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.deconv2(x)\n        x = self.norm2(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.norm3(x)\n        x = self.relu(x)\n        return x\n\n\nclass LinkNet34(nn.Module):\n    def __init__(self, num_classes=1, num_channels=3, pretrained=True):\n        super().__init__()\n        assert num_channels == 3\n        self.num_classes = num_classes\n        filters = [64, 128, 256, 512]\n        resnet = models.resnet34(pretrained=pretrained)\n\n        self.firstconv = resnet.conv1\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n\n        # Decoder\n        self.decoder4 = DecoderBlockLinkNet(filters[3], filters[2])\n        self.decoder3 = DecoderBlockLinkNet(filters[2], filters[1])\n        self.decoder2 = DecoderBlockLinkNet(filters[1], filters[0])\n        self.decoder1 = DecoderBlockLinkNet(filters[0], filters[0])\n\n        # Final Classifier\n        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 3, stride=2)\n        self.finalrelu1 = nn.ReLU(inplace=True)\n        self.finalconv2 = nn.Conv2d(32, 32, 3)\n        self.finalrelu2 = nn.ReLU(inplace=True)\n        self.finalconv3 = nn.Conv2d(32, num_classes, 2, padding=1)\n\n    # noinspection PyCallingNonCallable\n    def forward(self, x):\n        # Encoder\n        x = self.firstconv(x)\n        x = self.firstbn(x)\n        x = self.firstrelu(x)\n        x = self.firstmaxpool(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder with Skip Connections\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n\n        # Final Classification\n        f1 = self.finaldeconv1(d1)\n        f2 = self.finalrelu1(f1)\n        f3 = self.finalconv2(f2)\n        f4 = self.finalrelu2(f3)\n        f5 = self.finalconv3(f4)\n\n        if self.num_classes > 1:\n            x_out = F.log_softmax(f5, dim=1)\n        else:\n            x_out = f5\n        return x_out\n\n\nclass Conv3BN(nn.Module):\n    def __init__(self, in_: int, out: int, bn=False):\n        super().__init__()\n        self.conv = conv3x3(in_, out)\n        self.bn = nn.BatchNorm2d(out) if bn else None\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        x = self.activation(x)\n        return x\n\n\nclass UNetModule(nn.Module):\n    def __init__(self, in_: int, out: int):\n        super().__init__()\n        self.l1 = Conv3BN(in_, out)\n        self.l2 = Conv3BN(out, out)\n\n    def forward(self, x):\n        x = self.l1(x)\n        x = self.l2(x)\n        return x\n\n\nclass UNet(nn.Module):\n    """"""\n    Vanilla UNet.\n\n    Implementation from https://github.com/lopuhin/mapillary-vistas-2017/blob/master/unet_models.py\n    """"""\n    output_downscaled = 1\n    module = UNetModule\n\n    def __init__(self,\n                 input_channels: int = 3,\n                 filters_base: int = 32,\n                 down_filter_factors=(1, 2, 4, 8, 16),\n                 up_filter_factors=(1, 2, 4, 8, 16),\n                 bottom_s=4,\n                 num_classes=1,\n                 add_output=True):\n        super().__init__()\n        self.num_classes = num_classes\n        assert len(down_filter_factors) == len(up_filter_factors)\n        assert down_filter_factors[-1] == up_filter_factors[-1]\n        down_filter_sizes = [filters_base * s for s in down_filter_factors]\n        up_filter_sizes = [filters_base * s for s in up_filter_factors]\n        self.down, self.up = nn.ModuleList(), nn.ModuleList()\n        self.down.append(self.module(input_channels, down_filter_sizes[0]))\n        for prev_i, nf in enumerate(down_filter_sizes[1:]):\n            self.down.append(self.module(down_filter_sizes[prev_i], nf))\n        for prev_i, nf in enumerate(up_filter_sizes[1:]):\n            self.up.append(self.module(\n                down_filter_sizes[prev_i] + nf, up_filter_sizes[prev_i]))\n        pool = nn.MaxPool2d(2, 2)\n        pool_bottom = nn.MaxPool2d(bottom_s, bottom_s)\n        upsample = nn.Upsample(scale_factor=2)\n        upsample_bottom = nn.Upsample(scale_factor=bottom_s)\n        self.downsamplers = [None] + [pool] * (len(self.down) - 1)\n        self.downsamplers[-1] = pool_bottom\n        self.upsamplers = [upsample] * len(self.up)\n        self.upsamplers[-1] = upsample_bottom\n        self.add_output = add_output\n        if add_output:\n            self.conv_final = nn.Conv2d(up_filter_sizes[0], num_classes, 1)\n\n    def forward(self, x):\n        xs = []\n        for downsample, down in zip(self.downsamplers, self.down):\n            x_in = x if downsample is None else downsample(xs[-1])\n            x_out = down(x_in)\n            xs.append(x_out)\n\n        x_out = xs[-1]\n        for x_skip, upsample, up in reversed(\n                list(zip(xs[:-1], self.upsamplers, self.up))):\n            x_out = upsample(x_out)\n            x_out = up(torch.cat([x_out, x_skip], 1))\n\n        if self.add_output:\n            x_out = self.conv_final(x_out)\n            if self.num_classes > 1:\n                x_out = F.log_softmax(x_out, dim=1)\n        return x_out\n\n\nclass AlbuNet(nn.Module):\n    """"""\n        UNet (https://arxiv.org/abs/1505.04597) with Resnet34(https://arxiv.org/abs/1512.03385) encoder\n        Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n        """"""\n\n    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n        """"""\n        :param num_classes:\n        :param num_filters:\n        :param pretrained:\n            False - no pre-trained network is used\n            True  - encoder is pre-trained with resnet34\n        :is_deconv:\n            False: bilinear interpolation is used in decoder\n            True: deconvolution is used in decoder\n        """"""\n        super().__init__()\n        self.num_classes = num_classes\n\n        self.pool = nn.MaxPool2d(2, 2)\n\n        self.encoder = torchvision.models.resnet34(pretrained=pretrained)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv1 = nn.Sequential(self.encoder.conv1,\n                                   self.encoder.bn1,\n                                   self.encoder.relu,\n                                   self.pool)\n\n        self.conv2 = self.encoder.layer1\n\n        self.conv3 = self.encoder.layer2\n\n        self.conv4 = self.encoder.layer3\n\n        self.conv5 = self.encoder.layer4\n\n        self.center = DecoderBlock(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n\n        self.dec5 = DecoderBlock(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n        self.dec4 = DecoderBlock(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n        self.dec3 = DecoderBlock(128 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n        self.dec2 = DecoderBlock(64 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2, is_deconv)\n        self.dec1 = DecoderBlock(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n        self.dec0 = ConvRelu(num_filters, num_filters)\n        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n        conv4 = self.conv4(conv3)\n        conv5 = self.conv5(conv4)\n\n        center = self.center(self.pool(conv5))\n\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(dec2)\n        dec0 = self.dec0(dec1)\n\n        if self.num_classes > 1:\n            x_out = F.log_softmax(self.final(dec0), dim=1)\n        else:\n            x_out = self.final(dec0)\n\n        return x_out\n'"
prepare_data.py,0,"b'""""""\n[1] Merge masks with different instruments into one binary mask\n[2] Crop black borders from images and masks\n""""""\nfrom pathlib import Path\n\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\n\ndata_path = Path(\'data\')\n\ntrain_path = data_path / \'train\'\n\ncropped_train_path = data_path / \'cropped_train\'\n\noriginal_height, original_width = 1080, 1920\nheight, width = 1024, 1280\nh_start, w_start = 28, 320\n\nbinary_factor = 255\nparts_factor = 85\ninstrument_factor = 32\n\n\nif __name__ == \'__main__\':\n    for instrument_index in range(1, 9):\n        instrument_folder = \'instrument_dataset_\' + str(instrument_index)\n\n        (cropped_train_path / instrument_folder / \'images\').mkdir(exist_ok=True, parents=True)\n\n        binary_mask_folder = (cropped_train_path / instrument_folder / \'binary_masks\')\n        binary_mask_folder.mkdir(exist_ok=True, parents=True)\n\n        parts_mask_folder = (cropped_train_path / instrument_folder / \'parts_masks\')\n        parts_mask_folder.mkdir(exist_ok=True, parents=True)\n\n        instrument_mask_folder = (cropped_train_path / instrument_folder / \'instruments_masks\')\n        instrument_mask_folder.mkdir(exist_ok=True, parents=True)\n\n        mask_folders = list((train_path / instrument_folder / \'ground_truth\').glob(\'*\'))\n        # mask_folders = [x for x in mask_folders if \'Other\' not in str(mask_folders)]\n\n        for file_name in tqdm(list((train_path / instrument_folder / \'left_frames\').glob(\'*\'))):\n            img = cv2.imread(str(file_name))\n            old_h, old_w, _ = img.shape\n\n            img = img[h_start: h_start + height, w_start: w_start + width]\n            cv2.imwrite(str(cropped_train_path / instrument_folder / \'images\' / (file_name.stem + \'.jpg\')), img,\n                        [cv2.IMWRITE_JPEG_QUALITY, 100])\n\n            mask_binary = np.zeros((old_h, old_w))\n            mask_parts = np.zeros((old_h, old_w))\n            mask_instruments = np.zeros((old_h, old_w))\n\n            for mask_folder in mask_folders:\n                mask = cv2.imread(str(mask_folder / file_name.name), 0)\n\n                if \'Bipolar_Forceps\' in str(mask_folder):\n                    mask_instruments[mask > 0] = 1\n                elif \'Prograsp_Forceps\' in str(mask_folder):\n                    mask_instruments[mask > 0] = 2\n                elif \'Large_Needle_Driver\' in str(mask_folder):\n                    mask_instruments[mask > 0] = 3\n                elif \'Vessel_Sealer\' in str(mask_folder):\n                    mask_instruments[mask > 0] = 4\n                elif \'Grasping_Retractor\' in str(mask_folder):\n                    mask_instruments[mask > 0] = 5\n                elif \'Monopolar_Curved_Scissors\' in str(mask_folder):\n                    mask_instruments[mask > 0] = 6\n                elif \'Other\' in str(mask_folder):\n                    mask_instruments[mask > 0] = 7\n\n                if \'Other\' not in str(mask_folder):\n                    mask_binary += mask\n\n                    mask_parts[mask == 10] = 1  # Shaft\n                    mask_parts[mask == 20] = 2  # Wrist\n                    mask_parts[mask == 30] = 3  # Claspers\n\n            mask_binary = (mask_binary[h_start: h_start + height, w_start: w_start + width] > 0).astype(\n                np.uint8) * binary_factor\n            mask_parts = (mask_parts[h_start: h_start + height, w_start: w_start + width]).astype(\n                np.uint8) * parts_factor\n            mask_instruments = (mask_instruments[h_start: h_start + height, w_start: w_start + width]).astype(\n                np.uint8) * instrument_factor\n\n            cv2.imwrite(str(binary_mask_folder / file_name.name), mask_binary)\n            cv2.imwrite(str(parts_mask_folder / file_name.name), mask_parts)\n            cv2.imwrite(str(instrument_mask_folder / file_name.name), mask_instruments)\n'"
prepare_train_val.py,0,"b""from prepare_data import data_path\n\n\ndef get_split(fold):\n    folds = {0: [1, 3],\n             1: [2, 5],\n             2: [4, 8],\n             3: [6, 7]}\n\n    train_path = data_path / 'cropped_train'\n\n    train_file_names = []\n    val_file_names = []\n\n    for instrument_id in range(1, 9):\n        if instrument_id in folds[fold]:\n            val_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\n        else:\n            train_file_names += list((train_path / ('instrument_dataset_' + str(instrument_id)) / 'images').glob('*'))\n\n    return train_file_names, val_file_names\n"""
train.py,6,"b""import argparse\nimport json\nfrom pathlib import Path\nfrom validation import validation_binary, validation_multi\n\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nimport torch.backends.cudnn as cudnn\nimport torch.backends.cudnn\n\nfrom models import UNet11, LinkNet34, UNet, UNet16, AlbuNet\nfrom loss import LossBinary, LossMulti\nfrom dataset import RoboticsDataset\nimport utils\nimport sys\nfrom prepare_train_val import get_split\n\nfrom albumentations import (\n    HorizontalFlip,\n    VerticalFlip,\n    Normalize,\n    Compose,\n    PadIfNeeded,\n    RandomCrop,\n    CenterCrop\n)\n\nmoddel_list = {'UNet11': UNet11,\n               'UNet16': UNet16,\n               'UNet': UNet,\n               'AlbuNet': AlbuNet,\n               'LinkNet34': LinkNet34}\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    arg = parser.add_argument\n    arg('--jaccard-weight', default=0.5, type=float)\n    arg('--device-ids', type=str, default='0', help='For example 0,1 to run on two GPUs')\n    arg('--fold', type=int, help='fold', default=0)\n    arg('--root', default='runs/debug', help='checkpoint root')\n    arg('--batch-size', type=int, default=1)\n    arg('--n-epochs', type=int, default=100)\n    arg('--lr', type=float, default=0.0001)\n    arg('--workers', type=int, default=12)\n    arg('--train_crop_height', type=int, default=1024)\n    arg('--train_crop_width', type=int, default=1280)\n    arg('--val_crop_height', type=int, default=1024)\n    arg('--val_crop_width', type=int, default=1280)\n    arg('--type', type=str, default='binary', choices=['binary', 'parts', 'instruments'])\n    arg('--model', type=str, default='UNet', choices=moddel_list.keys())\n\n    args = parser.parse_args()\n\n    root = Path(args.root)\n    root.mkdir(exist_ok=True, parents=True)\n\n    if not utils.check_crop_size(args.train_crop_height, args.train_crop_width):\n        print('Input image sizes should be divisible by 32, but train '\n              'crop sizes ({train_crop_height} and {train_crop_width}) '\n              'are not.'.format(train_crop_height=args.train_crop_height, train_crop_width=args.train_crop_width))\n        sys.exit(0)\n\n    if not utils.check_crop_size(args.val_crop_height, args.val_crop_width):\n        print('Input image sizes should be divisible by 32, but validation '\n              'crop sizes ({val_crop_height} and {val_crop_width}) '\n              'are not.'.format(val_crop_height=args.val_crop_height, val_crop_width=args.val_crop_width))\n        sys.exit(0)\n\n    if args.type == 'parts':\n        num_classes = 4\n    elif args.type == 'instruments':\n        num_classes = 8\n    else:\n        num_classes = 1\n\n    if args.model == 'UNet':\n        model = UNet(num_classes=num_classes)\n    else:\n        model_name = moddel_list[args.model]\n        model = model_name(num_classes=num_classes, pretrained=True)\n\n    if torch.cuda.is_available():\n        if args.device_ids:\n            device_ids = list(map(int, args.device_ids.split(',')))\n        else:\n            device_ids = None\n        model = nn.DataParallel(model, device_ids=device_ids).cuda()\n    else:\n        raise SystemError('GPU device not found')\n\n    if args.type == 'binary':\n        loss = LossBinary(jaccard_weight=args.jaccard_weight)\n    else:\n        loss = LossMulti(num_classes=num_classes, jaccard_weight=args.jaccard_weight)\n\n    cudnn.benchmark = True\n\n    def make_loader(file_names, shuffle=False, transform=None, problem_type='binary', batch_size=1):\n        return DataLoader(\n            dataset=RoboticsDataset(file_names, transform=transform, problem_type=problem_type),\n            shuffle=shuffle,\n            num_workers=args.workers,\n            batch_size=batch_size,\n            pin_memory=torch.cuda.is_available()\n        )\n\n    train_file_names, val_file_names = get_split(args.fold)\n\n    print('num train = {}, num_val = {}'.format(len(train_file_names), len(val_file_names)))\n\n    def train_transform(p=1):\n        return Compose([\n            PadIfNeeded(min_height=args.train_crop_height, min_width=args.train_crop_width, p=1),\n            RandomCrop(height=args.train_crop_height, width=args.train_crop_width, p=1),\n            VerticalFlip(p=0.5),\n            HorizontalFlip(p=0.5),\n            Normalize(p=1)\n        ], p=p)\n\n    def val_transform(p=1):\n        return Compose([\n            PadIfNeeded(min_height=args.val_crop_height, min_width=args.val_crop_width, p=1),\n            CenterCrop(height=args.val_crop_height, width=args.val_crop_width, p=1),\n            Normalize(p=1)\n        ], p=p)\n\n    train_loader = make_loader(train_file_names, shuffle=True, transform=train_transform(p=1), problem_type=args.type,\n                               batch_size=args.batch_size)\n    valid_loader = make_loader(val_file_names, transform=val_transform(p=1), problem_type=args.type,\n                               batch_size=len(device_ids))\n\n    root.joinpath('params.json').write_text(\n        json.dumps(vars(args), indent=True, sort_keys=True))\n\n    if args.type == 'binary':\n        valid = validation_binary\n    else:\n        valid = validation_multi\n\n    utils.train(\n        init_optimizer=lambda lr: Adam(model.parameters(), lr=lr),\n        args=args,\n        model=model,\n        criterion=loss,\n        train_loader=train_loader,\n        valid_loader=valid_loader,\n        validation=valid,\n        fold=args.fold,\n        num_classes=num_classes\n    )\n\n\nif __name__ == '__main__':\n    main()\n"""
utils.py,4,"b'import json\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport random\nimport numpy as np\n\nimport torch\nimport tqdm\n\n\ndef cuda(x):\n    return x.cuda(async=True) if torch.cuda.is_available() else x\n\n\ndef write_event(log, step, **data):\n    data[\'step\'] = step\n    data[\'dt\'] = datetime.now().isoformat()\n    log.write(json.dumps(data, sort_keys=True))\n    log.write(\'\\n\')\n    log.flush()\n\n\ndef check_crop_size(image_height, image_width):\n    """"""Checks if image size divisible by 32.\n\n    Args:\n        image_height:\n        image_width:\n\n    Returns:\n        True if both height and width divisible by 32 and False otherwise.\n\n    """"""\n    return image_height % 32 == 0 and image_width % 32 == 0\n\n\ndef train(args, model, criterion, train_loader, valid_loader, validation, init_optimizer, n_epochs=None, fold=None,\n          num_classes=None):\n    lr = args.lr\n    n_epochs = n_epochs or args.n_epochs\n    optimizer = init_optimizer(lr)\n\n    root = Path(args.root)\n    model_path = root / \'model_{fold}.pt\'.format(fold=fold)\n    if model_path.exists():\n        state = torch.load(str(model_path))\n        epoch = state[\'epoch\']\n        step = state[\'step\']\n        model.load_state_dict(state[\'model\'])\n        print(\'Restored model, epoch {}, step {:,}\'.format(epoch, step))\n    else:\n        epoch = 1\n        step = 0\n\n    save = lambda ep: torch.save({\n        \'model\': model.state_dict(),\n        \'epoch\': ep,\n        \'step\': step,\n    }, str(model_path))\n\n    report_each = 10\n    log = root.joinpath(\'train_{fold}.log\'.format(fold=fold)).open(\'at\', encoding=\'utf8\')\n    valid_losses = []\n    for epoch in range(epoch, n_epochs + 1):\n        model.train()\n        random.seed()\n        tq = tqdm.tqdm(total=(len(train_loader) * args.batch_size))\n        tq.set_description(\'Epoch {}, lr {}\'.format(epoch, lr))\n        losses = []\n        tl = train_loader\n        try:\n            mean_loss = 0\n            for i, (inputs, targets) in enumerate(tl):\n                inputs = cuda(inputs)\n\n                with torch.no_grad():\n                    targets = cuda(targets)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                optimizer.zero_grad()\n                batch_size = inputs.size(0)\n                loss.backward()\n                optimizer.step()\n                step += 1\n                tq.update(batch_size)\n                losses.append(loss.item())\n                mean_loss = np.mean(losses[-report_each:])\n                tq.set_postfix(loss=\'{:.5f}\'.format(mean_loss))\n                if i and i % report_each == 0:\n                    write_event(log, step, loss=mean_loss)\n            write_event(log, step, loss=mean_loss)\n            tq.close()\n            save(epoch + 1)\n            valid_metrics = validation(model, criterion, valid_loader, num_classes)\n            write_event(log, step, **valid_metrics)\n            valid_loss = valid_metrics[\'valid_loss\']\n            valid_losses.append(valid_loss)\n        except KeyboardInterrupt:\n            tq.close()\n            print(\'Ctrl+C, saving snapshot\')\n            save(epoch)\n            print(\'done.\')\n            return\n'"
validation.py,2,"b""import numpy as np\nimport utils\nfrom torch import nn\nimport torch\n\n\ndef validation_binary(model, criterion, valid_loader, num_classes=None):\n    with torch.no_grad():\n        model.eval()\n        losses = []\n\n        jaccard = []\n\n        for inputs, targets in valid_loader:\n            inputs = utils.cuda(inputs)\n            targets = utils.cuda(targets)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            losses.append(loss.item())\n            jaccard += get_jaccard(targets, (outputs > 0).float())\n\n        valid_loss = np.mean(losses)  # type: float\n\n        valid_jaccard = np.mean(jaccard).astype(np.float64)\n\n        print('Valid loss: {:.5f}, jaccard: {:.5f}'.format(valid_loss, valid_jaccard))\n        metrics = {'valid_loss': valid_loss, 'jaccard_loss': valid_jaccard}\n        return metrics\n\n\ndef get_jaccard(y_true, y_pred):\n    epsilon = 1e-15\n    intersection = (y_pred * y_true).sum(dim=-2).sum(dim=-1)\n    union = y_true.sum(dim=-2).sum(dim=-1) + y_pred.sum(dim=-2).sum(dim=-1)\n\n    return list(((intersection + epsilon) / (union - intersection + epsilon)).data.cpu().numpy())\n\n\ndef validation_multi(model: nn.Module, criterion, valid_loader, num_classes):\n    with torch.no_grad():\n        model.eval()\n        losses = []\n        confusion_matrix = np.zeros(\n            (num_classes, num_classes), dtype=np.uint32)\n        for inputs, targets in valid_loader:\n            inputs = utils.cuda(inputs)\n            targets = utils.cuda(targets)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            losses.append(loss.item())\n            output_classes = outputs.data.cpu().numpy().argmax(axis=1)\n            target_classes = targets.data.cpu().numpy()\n            confusion_matrix += calculate_confusion_matrix_from_arrays(\n                output_classes, target_classes, num_classes)\n\n        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n        valid_loss = np.mean(losses)  # type: float\n        ious = {'iou_{}'.format(cls + 1): iou\n                for cls, iou in enumerate(calculate_iou(confusion_matrix))}\n\n        dices = {'dice_{}'.format(cls + 1): dice\n                 for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n\n        average_iou = np.mean(list(ious.values()))\n        average_dices = np.mean(list(dices.values()))\n\n        print(\n            'Valid loss: {:.4f}, average IoU: {:.4f}, average Dice: {:.4f}'.format(valid_loss,\n                                                                                   average_iou,\n                                                                                   average_dices))\n        metrics = {'valid_loss': valid_loss, 'iou': average_iou}\n        metrics.update(ious)\n        metrics.update(dices)\n        return metrics\n\n\ndef calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n    replace_indices = np.vstack((\n        ground_truth.flatten(),\n        prediction.flatten())\n    ).T\n    confusion_matrix, _ = np.histogramdd(\n        replace_indices,\n        bins=(nr_labels, nr_labels),\n        range=[(0, nr_labels), (0, nr_labels)]\n    )\n    confusion_matrix = confusion_matrix.astype(np.uint32)\n    return confusion_matrix\n\n\ndef calculate_iou(confusion_matrix):\n    ious = []\n    for index in range(confusion_matrix.shape[0]):\n        true_positives = confusion_matrix[index, index]\n        false_positives = confusion_matrix[:, index].sum() - true_positives\n        false_negatives = confusion_matrix[index, :].sum() - true_positives\n        denom = true_positives + false_positives + false_negatives\n        if denom == 0:\n            iou = 0\n        else:\n            iou = float(true_positives) / denom\n        ious.append(iou)\n    return ious\n\n\ndef calculate_dice(confusion_matrix):\n    dices = []\n    for index in range(confusion_matrix.shape[0]):\n        true_positives = confusion_matrix[index, index]\n        false_positives = confusion_matrix[:, index].sum() - true_positives\n        false_negatives = confusion_matrix[index, :].sum() - true_positives\n        denom = 2 * true_positives + false_positives + false_negatives\n        if denom == 0:\n            dice = 0\n        else:\n            dice = 2 * float(true_positives) / denom\n        dices.append(dice)\n    return dices\n"""
