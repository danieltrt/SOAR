file_path,api_count,code
model.py,1,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport torch as t\nimport torch.nn as nn\n\nfrom torch import LongTensor as LT\nfrom torch import FloatTensor as FT\n\n\nclass Bundler(nn.Module):\n\n    def forward(self, data):\n        raise NotImplementedError\n\n    def forward_i(self, data):\n        raise NotImplementedError\n\n    def forward_o(self, data):\n        raise NotImplementedError\n\n\nclass Word2Vec(Bundler):\n\n    def __init__(self, vocab_size=20000, embedding_size=300, padding_idx=0):\n        super(Word2Vec, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.ivectors = nn.Embedding(self.vocab_size, self.embedding_size, padding_idx=padding_idx)\n        self.ovectors = nn.Embedding(self.vocab_size, self.embedding_size, padding_idx=padding_idx)\n        self.ivectors.weight = nn.Parameter(t.cat([t.zeros(1, self.embedding_size), FT(self.vocab_size - 1, self.embedding_size).uniform_(-0.5 / self.embedding_size, 0.5 / self.embedding_size)]))\n        self.ovectors.weight = nn.Parameter(t.cat([t.zeros(1, self.embedding_size), FT(self.vocab_size - 1, self.embedding_size).uniform_(-0.5 / self.embedding_size, 0.5 / self.embedding_size)]))\n        self.ivectors.weight.requires_grad = True\n        self.ovectors.weight.requires_grad = True\n\n    def forward(self, data):\n        return self.forward_i(data)\n\n    def forward_i(self, data):\n        v = LT(data)\n        v = v.cuda() if self.ivectors.weight.is_cuda else v\n        return self.ivectors(v)\n\n    def forward_o(self, data):\n        v = LT(data)\n        v = v.cuda() if self.ovectors.weight.is_cuda else v\n        return self.ovectors(v)\n\n\nclass SGNS(nn.Module):\n\n    def __init__(self, embedding, vocab_size=20000, n_negs=20, weights=None):\n        super(SGNS, self).__init__()\n        self.embedding = embedding\n        self.vocab_size = vocab_size\n        self.n_negs = n_negs\n        self.weights = None\n        if weights is not None:\n            wf = np.power(weights, 0.75)\n            wf = wf / wf.sum()\n            self.weights = FT(wf)\n\n    def forward(self, iword, owords):\n        batch_size = iword.size()[0]\n        context_size = owords.size()[1]\n        if self.weights is not None:\n            nwords = t.multinomial(self.weights, batch_size * context_size * self.n_negs, replacement=True).view(batch_size, -1)\n        else:\n            nwords = FT(batch_size, context_size * self.n_negs).uniform_(0, self.vocab_size - 1).long()\n        ivectors = self.embedding.forward_i(iword).unsqueeze(2)\n        ovectors = self.embedding.forward_o(owords)\n        nvectors = self.embedding.forward_o(nwords).neg()\n        oloss = t.bmm(ovectors, ivectors).squeeze().sigmoid().log().mean(1)\n        nloss = t.bmm(nvectors, ivectors).squeeze().sigmoid().log().view(-1, context_size, self.n_negs).sum(2).mean(1)\n        return -(oloss + nloss).mean()\n'"
plot.py,0,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport pickle\nimport argparse\nimport matplotlib\nimport numpy as np\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom matplotlib import pyplot as plt\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'./data/\', help=""data directory path"")\n    parser.add_argument(\'--result_dir\', type=str, default=\'./result/\', help=""result directory path"")\n    parser.add_argument(\'--model\', type=str, default=\'tsne\', choices=[\'pca\', \'tsne\'], help=""model for visualization"")\n    parser.add_argument(\'--top_k\', type=int, default=1000, help=""scatter top-k words"")\n    return parser.parse_args()\n\n\ndef plot(args):\n    wc = pickle.load(open(os.path.join(args.data_dir, \'wc.dat\'), \'rb\'))\n    words = sorted(wc, key=wc.get, reverse=True)[:args.top_k]\n    if args.model == \'pca\':\n        model = PCA(n_components=2)\n    elif args.model == \'tsne\':\n        model = TSNE(n_components=2, perplexity=30, init=\'pca\', method=\'exact\', n_iter=5000)\n    word2idx = pickle.load(open(\'data/word2idx.dat\', \'rb\'))\n    idx2vec = pickle.load(open(\'data/idx2vec.dat\', \'rb\'))\n    X = [idx2vec[word2idx[word]] for word in words]\n    X = model.fit_transform(X)\n    plt.figure(figsize=(18, 18))\n    for i in range(len(X)):\n        plt.text(X[i, 0], X[i, 1], words[i], bbox=dict(facecolor=\'blue\', alpha=0.1))\n    plt.xlim((np.min(X[:, 0]), np.max(X[:, 0])))\n    plt.ylim((np.min(X[:, 1]), np.max(X[:, 1])))\n    if not os.path.isdir(args.result_dir):\n        os.mkdir(args.result_dir)\n    plt.savefig(os.path.join(args.result_dir, args.model) + \'.png\')\n\n\nif __name__ == \'__main__\':\n    matplotlib.rc(\'font\', family=\'AppleGothic\')\n    plot(parse_args())\n'"
preprocess.py,0,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport codecs\nimport pickle\nimport argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'./data/\', help=""data directory path"")\n    parser.add_argument(\'--vocab\', type=str, default=\'./data/corpus.txt\', help=""corpus path for building vocab"")\n    parser.add_argument(\'--corpus\', type=str, default=\'./data/corpus.txt\', help=""corpus path"")\n    parser.add_argument(\'--unk\', type=str, default=\'<UNK>\', help=""UNK token"")\n    parser.add_argument(\'--window\', type=int, default=5, help=""window size"")\n    parser.add_argument(\'--max_vocab\', type=int, default=20000, help=""maximum number of vocab"")\n    return parser.parse_args()\n\n\nclass Preprocess(object):\n\n    def __init__(self, window=5, unk=\'<UNK>\', data_dir=\'./data/\'):\n        self.window = window\n        self.unk = unk\n        self.data_dir = data_dir\n\n    def skipgram(self, sentence, i):\n        iword = sentence[i]\n        left = sentence[max(i - self.window, 0): i]\n        right = sentence[i + 1: i + 1 + self.window]\n        return iword, [self.unk for _ in range(self.window - len(left))] + left + right + [self.unk for _ in range(self.window - len(right))]\n\n    def build(self, filepath, max_vocab=20000):\n        print(""building vocab..."")\n        step = 0\n        self.wc = {self.unk: 1}\n        with codecs.open(filepath, \'r\', encoding=\'utf-8\') as file:\n            for line in file:\n                step += 1\n                if not step % 1000:\n                    print(""working on {}kth line"".format(step // 1000), end=\'\\r\')\n                line = line.strip()\n                if not line:\n                    continue\n                sent = line.split()\n                for word in sent:\n                    self.wc[word] = self.wc.get(word, 0) + 1\n        print("""")\n        self.idx2word = [self.unk] + sorted(self.wc, key=self.wc.get, reverse=True)[:max_vocab - 1]\n        self.word2idx = {self.idx2word[idx]: idx for idx, _ in enumerate(self.idx2word)}\n        self.vocab = set([word for word in self.word2idx])\n        pickle.dump(self.wc, open(os.path.join(self.data_dir, \'wc.dat\'), \'wb\'))\n        pickle.dump(self.vocab, open(os.path.join(self.data_dir, \'vocab.dat\'), \'wb\'))\n        pickle.dump(self.idx2word, open(os.path.join(self.data_dir, \'idx2word.dat\'), \'wb\'))\n        pickle.dump(self.word2idx, open(os.path.join(self.data_dir, \'word2idx.dat\'), \'wb\'))\n        print(""build done"")\n\n    def convert(self, filepath):\n        print(""converting corpus..."")\n        step = 0\n        data = []\n        with codecs.open(filepath, \'r\', encoding=\'utf-8\') as file:\n            for line in file:\n                step += 1\n                if not step % 1000:\n                    print(""working on {}kth line"".format(step // 1000), end=\'\\r\')\n                line = line.strip()\n                if not line:\n                    continue\n                sent = []\n                for word in line.split():\n                    if word in self.vocab:\n                        sent.append(word)\n                    else:\n                        sent.append(self.unk)\n                for i in range(len(sent)):\n                    iword, owords = self.skipgram(sent, i)\n                    data.append((self.word2idx[iword], [self.word2idx[oword] for oword in owords]))\n        print("""")\n        pickle.dump(data, open(os.path.join(self.data_dir, \'train.dat\'), \'wb\'))\n        print(""conversion done"")\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    preprocess = Preprocess(window=args.window, unk=args.unk, data_dir=args.data_dir)\n    preprocess.build(args.vocab, max_vocab=args.max_vocab)\n    preprocess.convert(args.corpus)\n'"
train.py,2,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport pickle\nimport random\nimport argparse\nimport torch as t\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\nfrom model import Word2Vec, SGNS\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--name\', type=str, default=\'sgns\', help=""model name"")\n    parser.add_argument(\'--data_dir\', type=str, default=\'./data/\', help=""data directory path"")\n    parser.add_argument(\'--save_dir\', type=str, default=\'./pts/\', help=""model directory path"")\n    parser.add_argument(\'--e_dim\', type=int, default=300, help=""embedding dimension"")\n    parser.add_argument(\'--n_negs\', type=int, default=20, help=""number of negative samples"")\n    parser.add_argument(\'--epoch\', type=int, default=100, help=""number of epochs"")\n    parser.add_argument(\'--mb\', type=int, default=4096, help=""mini-batch size"")\n    parser.add_argument(\'--ss_t\', type=float, default=1e-5, help=""subsample threshold"")\n    parser.add_argument(\'--conti\', action=\'store_true\', help=""continue learning"")\n    parser.add_argument(\'--weights\', action=\'store_true\', help=""use weights for negative sampling"")\n    parser.add_argument(\'--cuda\', action=\'store_true\', help=""use CUDA"")\n    return parser.parse_args()\n\n\nclass PermutedSubsampledCorpus(Dataset):\n\n    def __init__(self, datapath, ws=None):\n        data = pickle.load(open(datapath, \'rb\'))\n        if ws is not None:\n            self.data = []\n            for iword, owords in data:\n                if random.random() > ws[iword]:\n                    self.data.append((iword, owords))\n        else:\n            self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        iword, owords = self.data[idx]\n        return iword, np.array(owords)\n\n\ndef train(args):\n    idx2word = pickle.load(open(os.path.join(args.data_dir, \'idx2word.dat\'), \'rb\'))\n    wc = pickle.load(open(os.path.join(args.data_dir, \'wc.dat\'), \'rb\'))\n    wf = np.array([wc[word] for word in idx2word])\n    wf = wf / wf.sum()\n    ws = 1 - np.sqrt(args.ss_t / wf)\n    ws = np.clip(ws, 0, 1)\n    vocab_size = len(idx2word)\n    weights = wf if args.weights else None\n    if not os.path.isdir(args.save_dir):\n        os.mkdir(args.save_dir)\n    model = Word2Vec(vocab_size=vocab_size, embedding_size=args.e_dim)\n    modelpath = os.path.join(args.save_dir, \'{}.pt\'.format(args.name))\n    sgns = SGNS(embedding=model, vocab_size=vocab_size, n_negs=args.n_negs, weights=weights)\n    if os.path.isfile(modelpath) and args.conti:\n        sgns.load_state_dict(t.load(modelpath))\n    if args.cuda:\n        sgns = sgns.cuda()\n    optim = Adam(sgns.parameters())\n    optimpath = os.path.join(args.save_dir, \'{}.optim.pt\'.format(args.name))\n    if os.path.isfile(optimpath) and args.conti:\n        optim.load_state_dict(t.load(optimpath))\n    for epoch in range(1, args.epoch + 1):\n        dataset = PermutedSubsampledCorpus(os.path.join(args.data_dir, \'train.dat\'))\n        dataloader = DataLoader(dataset, batch_size=args.mb, shuffle=True)\n        total_batches = int(np.ceil(len(dataset) / args.mb))\n        pbar = tqdm(dataloader)\n        pbar.set_description(""[Epoch {}]"".format(epoch))\n        for iword, owords in pbar:\n            loss = sgns(iword, owords)\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n            pbar.set_postfix(loss=loss.item())\n    idx2vec = model.ivectors.weight.data.cpu().numpy()\n    pickle.dump(idx2vec, open(os.path.join(args.data_dir, \'idx2vec.dat\'), \'wb\'))\n    t.save(sgns.state_dict(), os.path.join(args.save_dir, \'{}.pt\'.format(args.name)))\n    t.save(optim.state_dict(), os.path.join(args.save_dir, \'{}.optim.pt\'.format(args.name)))\n\n\nif __name__ == \'__main__\':\n    train(parse_args())\n'"
