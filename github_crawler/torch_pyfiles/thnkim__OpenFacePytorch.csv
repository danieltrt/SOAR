file_path,api_count,code
SpatialCrossMapLRN_temp.py,7,"b""# This is a simple modification of https://github.com/pytorch/pytorch/blob/master/torch/legacy/nn/SpatialCrossMapLRN.py.\n\nimport torch\nfrom torch.legacy.nn.Module import Module\nfrom torch.legacy.nn.utils import clear\n\n\nclass SpatialCrossMapLRN_temp(Module):\n\n\tdef __init__(self, size, alpha=1e-4, beta=0.75, k=1, gpuDevice=0):\n\t\tsuper(SpatialCrossMapLRN_temp, self).__init__()\n\n\t\tself.size = size\n\t\tself.alpha = alpha\n\t\tself.beta = beta\n\t\tself.k = k\n\t\tself.scale = None\n\t\tself.paddedRatio = None\n\t\tself.accumRatio = None\n\t\tself.gpuDevice = gpuDevice\n\n\tdef updateOutput(self, input):\n\t\tassert input.dim() == 4\n\n\t\tif self.scale is None:\n\t\t\tself.scale = input.new()\n\t\t\t\n\t\tif self.output is None:\n\t\t\tself.output = input.new()\n\n\t\tbatchSize = input.size(0)\n\t\tchannels = input.size(1)\n\t\tinputHeight = input.size(2)\n\t\tinputWidth = input.size(3)\n\n\t\tif input.is_cuda:\t\n\t\t\tself.output = self.output.cuda(self.gpuDevice)\n\t\t\tself.scale = self.scale.cuda(self.gpuDevice)\n\n\t\tself.output.resize_as_(input)\n\t\tself.scale.resize_as_(input)\n\n\t\t# use output storage as temporary buffer\n\t\tinputSquare = self.output\n\t\ttorch.pow(input, 2, out=inputSquare)\n\n\t\tprePad = int((self.size - 1) / 2 + 1)\n\t\tprePadCrop = channels if prePad > channels else prePad\n\n\t\tscaleFirst = self.scale.select(1, 0)\n\t\tscaleFirst.zero_()\n\t\t# compute first feature map normalization\n\t\tfor c in range(prePadCrop):\n\t\t\tscaleFirst.add_(inputSquare.select(1, c))\n\n\t\t# reuse computations for next feature maps normalization\n\t\t# by adding the next feature map and removing the previous\n\t\tfor c in range(1, channels):\n\t\t\tscalePrevious = self.scale.select(1, c - 1)\n\t\t\tscaleCurrent = self.scale.select(1, c)\n\t\t\tscaleCurrent.copy_(scalePrevious)\n\t\t\tif c < channels - prePad + 1:\n\t\t\t\tsquareNext = inputSquare.select(1, c + prePad - 1)\n\t\t\t\tscaleCurrent.add_(1, squareNext)\n\n\t\t\tif c > prePad:\n\t\t\t\tsquarePrevious = inputSquare.select(1, c - prePad)\n\t\t\t\tscaleCurrent.add_(-1, squarePrevious)\n\n\t\tself.scale.mul_(self.alpha / self.size).add_(self.k)\n\n\t\ttorch.pow(self.scale, -self.beta, out=self.output)\n\t\tself.output.mul_(input)\n\n\t\treturn self.output\n\n\tdef updateGradInput(self, input, gradOutput):\n\t\tassert input.dim() == 4\n\n\t\tbatchSize = input.size(0)\n\t\tchannels = input.size(1)\n\t\tinputHeight = input.size(2)\n\t\tinputWidth = input.size(3)\n\n\t\tif self.paddedRatio is None:\n\t\t\tself.paddedRatio = input.new()\n\t\tif self.accumRatio is None:\n\t\t\tself.accumRatio = input.new()\n\t\tself.paddedRatio.resize_(channels + self.size - 1, inputHeight, inputWidth)\n\t\tself.accumRatio.resize_(inputHeight, inputWidth)\n\n\t\tcacheRatioValue = 2 * self.alpha * self.beta / self.size\n\t\tinversePrePad = int(self.size - (self.size - 1) / 2)\n\n\t\tself.gradInput.resize_as_(input)\n\t\ttorch.pow(self.scale, -self.beta, out=self.gradInput).mul_(gradOutput)\n\n\t\tself.paddedRatio.zero_()\n\t\tpaddedRatioCenter = self.paddedRatio.narrow(0, inversePrePad, channels)\n\t\tfor n in range(batchSize):\n\t\t\ttorch.mul(gradOutput[n], self.output[n], out=paddedRatioCenter)\n\t\t\tpaddedRatioCenter.div_(self.scale[n])\n\t\t\ttorch.sum(self.paddedRatio.narrow(0, 0, self.size - 1), 0, out=self.accumRatio)\n\t\t\tfor c in range(channels):\n\t\t\t\tself.accumRatio.add_(self.paddedRatio[c + self.size - 1])\n\t\t\t\tself.gradInput[n][c].addcmul_(-cacheRatioValue, input[n][c], self.accumRatio)\n\t\t\t\tself.accumRatio.add_(-1, self.paddedRatio[c])\n\n\t\treturn self.gradInput\n\n\tdef clearState(self):\n\t\tclear(self, 'scale', 'paddedRatio', 'accumRatio')\n\t\treturn super(SpatialCrossMapLRN_temp, self).clearState()\n\t\n"""
loadOpenFace.py,16,"b'import sys\nimport numpy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nfrom collections import OrderedDict\ntry:\n    from . SpatialCrossMapLRN_temp import SpatialCrossMapLRN_temp\nexcept:\n    try:\n        from SpatialCrossMapLRN_temp import SpatialCrossMapLRN_temp\n    except:\n        SpatialCrossMapLRN_temp = None\nimport os\nimport time\n\nimport pathlib\ncontaining_dir = str(pathlib.Path(__file__).resolve().parent)\n\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\n\n#\ndef Conv2d(in_dim, out_dim, kernel, stride, padding):\n    l = torch.nn.Conv2d(in_dim, out_dim, kernel, stride=stride, padding=padding)\n    return l\n\ndef BatchNorm(dim):\n    l = torch.nn.BatchNorm2d(dim)\n    return l\n\ndef CrossMapLRN(size, alpha, beta, k=1.0, gpuDevice=0):\n    if SpatialCrossMapLRN_temp is not None:\n        lrn = SpatialCrossMapLRN_temp(size, alpha, beta, k, gpuDevice=gpuDevice)\n        n = Lambda( lambda x,lrn=lrn: Variable(lrn.forward(x.data).cuda(gpuDevice)) if x.data.is_cuda else Variable(lrn.forward(x.data)) )\n    else:\n        n = nn.LocalResponseNorm(size, alpha, beta, k).cuda(gpuDevice)\n    return n\n\ndef Linear(in_dim, out_dim):\n    l = torch.nn.Linear(in_dim, out_dim)\n    return l\n\n\nclass Inception(nn.Module):\n    def __init__(self, inputSize, kernelSize, kernelStride, outputSize, reduceSize, pool, useBatchNorm, reduceStride=None, padding=True):\n        super(Inception, self).__init__()\n        #\n        self.seq_list = []\n        self.outputSize = outputSize\n\n        #\n        # 1x1 conv (reduce) -> 3x3 conv\n        # 1x1 conv (reduce) -> 5x5 conv\n        # ...\n        for i in range(len(kernelSize)):\n            od = OrderedDict()\n            # 1x1 conv\n            od[\'1_conv\'] = Conv2d(inputSize, reduceSize[i], (1, 1), reduceStride[i] if reduceStride is not None else 1, (0,0))\n            if useBatchNorm:\n                od[\'2_bn\'] = BatchNorm(reduceSize[i])\n            od[\'3_relu\'] = nn.ReLU()\n            # nxn conv\n            pad = int(numpy.floor(kernelSize[i] / 2)) if padding else 0\n            od[\'4_conv\'] = Conv2d(reduceSize[i], outputSize[i], kernelSize[i], kernelStride[i], pad)\n            if useBatchNorm:\n                od[\'5_bn\'] = BatchNorm(outputSize[i])\n            od[\'6_relu\'] = nn.ReLU()\n            #\n            self.seq_list.append(nn.Sequential(od))\n\n        ii = len(kernelSize)\n        # pool -> 1x1 conv\n        od = OrderedDict()\n        od[\'1_pool\'] = pool\n        if ii < len(reduceSize) and reduceSize[ii] is not None:\n            i = ii\n            od[\'2_conv\'] = Conv2d(inputSize, reduceSize[i], (1,1), reduceStride[i] if reduceStride is not None else 1, (0,0))\n            if useBatchNorm:\n                od[\'3_bn\'] = BatchNorm(reduceSize[i])\n            od[\'4_relu\'] = nn.ReLU()\n        #\n        self.seq_list.append(nn.Sequential(od))\n        ii += 1\n\n        # reduce: 1x1 conv (channel-wise pooling)\n        if ii < len(reduceSize) and reduceSize[ii] is not None:\n            i = ii\n            od = OrderedDict()\n            od[\'1_conv\'] = Conv2d(inputSize, reduceSize[i], (1,1), reduceStride[i] if reduceStride is not None else 1, (0,0))\n            if useBatchNorm:\n                od[\'2_bn\'] = BatchNorm(reduceSize[i])\n            od[\'3_relu\'] = nn.ReLU()\n            self.seq_list.append(nn.Sequential(od))\n\n        self.seq_list = nn.ModuleList(self.seq_list)\n\n\n    def forward(self, input):\n        x = input\n\n        ys = []\n        target_size = None\n        depth_dim = 0\n        for seq in self.seq_list:\n            #print(seq)\n            #print(self.outputSize)\n            #print(\'x_size:\', x.size())\n            y = seq(x)\n            y_size = y.size()\n            #print(\'y_size:\', y_size)\n            ys.append(y)\n            #\n            if target_size is None:\n                target_size = [0] * len(y_size)\n            #\n            for i in range(len(target_size)):\n                target_size[i] = max(target_size[i], y_size[i])\n            depth_dim += y_size[1]\n\n        target_size[1] = depth_dim\n        #print(\'target_size:\', target_size)\n\n        for i in range(len(ys)):\n            y_size = ys[i].size()\n            pad_l = int((target_size[3] - y_size[3]) // 2)\n            pad_t = int((target_size[2] - y_size[2]) // 2)\n            pad_r = target_size[3] - y_size[3] - pad_l\n            pad_b = target_size[2] - y_size[2] - pad_t\n            ys[i] = F.pad(ys[i], (pad_l, pad_r, pad_t, pad_b))\n\n        output = torch.cat(ys, 1)\n\n        return output\n\n\nclass netOpenFace(nn.Module):\n    def __init__(self, useCuda, gpuDevice=0):\n        super(netOpenFace, self).__init__()\n\n        self.gpuDevice = gpuDevice\n\n        self.layer1 = Conv2d(3, 64, (7,7), (2,2), (3,3))\n        self.layer2 = BatchNorm(64)\n        self.layer3 = nn.ReLU()\n        self.layer4 = nn.MaxPool2d((3,3), stride=(2,2), padding=(1,1))\n        self.layer5 = CrossMapLRN(5, 0.0001, 0.75, gpuDevice=gpuDevice)\n        self.layer6 = Conv2d(64, 64, (1,1), (1,1), (0,0))\n        self.layer7 = BatchNorm(64)\n        self.layer8 = nn.ReLU()\n        self.layer9 = Conv2d(64, 192, (3,3), (1,1), (1,1))\n        self.layer10 = BatchNorm(192)\n        self.layer11 = nn.ReLU()\n        self.layer12 = CrossMapLRN(5, 0.0001, 0.75, gpuDevice=gpuDevice)\n        self.layer13 = nn.MaxPool2d((3,3), stride=(2,2), padding=(1,1))\n        self.layer14 = Inception(192, (3,5), (1,1), (128,32), (96,16,32,64), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)\n        self.layer15 = Inception(256, (3,5), (1,1), (128,64), (96,32,64,64), nn.LPPool2d(2, (3,3), stride=(3,3)), True)\n        self.layer16 = Inception(320, (3,5), (2,2), (256,64), (128,32,None,None), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)\n        self.layer17 = Inception(640, (3,5), (1,1), (192,64), (96,32,128,256), nn.LPPool2d(2, (3,3), stride=(3,3)), True)\n        self.layer18 = Inception(640, (3,5), (2,2), (256,128), (160,64,None,None), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)\n        self.layer19 = Inception(1024, (3,), (1,), (384,), (96,96,256), nn.LPPool2d(2, (3,3), stride=(3,3)), True)\n        self.layer21 = Inception(736, (3,), (1,), (384,), (96,96,256), nn.MaxPool2d((3,3), stride=(2,2), padding=(0,0)), True)\n        self.layer22 = nn.AvgPool2d((3,3), stride=(1,1), padding=(0,0))\n        self.layer25 = Linear(736, 128)\n\n        #\n        self.resize1 = nn.UpsamplingNearest2d(scale_factor=3)\n        self.resize2 = nn.AvgPool2d(4)\n\n        #\n        # self.eval()\n\n        if useCuda:\n            self.cuda(gpuDevice)\n\n\n    def forward(self, input):\n        x = input\n\n        if x.data.is_cuda and self.gpuDevice != 0:\n            x = x.cuda(self.gpuDevice)\n\n        #\n        if x.size()[-1] == 128:\n            x = self.resize2(self.resize1(x))\n\n        x = self.layer8(self.layer7(self.layer6(self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x))))))))\n        x = self.layer13(self.layer12(self.layer11(self.layer10(self.layer9(x)))))\n        x = self.layer14(x)\n        x = self.layer15(x)\n        x = self.layer16(x)\n        x = self.layer17(x)\n        x = self.layer18(x)\n        x = self.layer19(x)\n        x = self.layer21(x)\n        x = self.layer22(x)\n        x = x.view((-1, 736))\n\n        x_736 = x\n\n        x = self.layer25(x)\n        x_norm = torch.sqrt(torch.sum(x**2, 1) + 1e-6)\n        x = torch.div(x, x_norm.view(-1, 1).expand_as(x))\n\n        return (x, x_736)\n\n\ndef prepareOpenFace(useCuda=True, gpuDevice=0, useMultiGPU=False):\n    model = netOpenFace(useCuda, gpuDevice)\n    model.load_state_dict(torch.load(os.path.join(containing_dir, \'openface.pth\')))\n\n    if useMultiGPU:\n        model = nn.DataParallel(model)\n\n    return model\n\n#\nif __name__ == \'__main__\':\n    #\n    useCuda = True\n    if useCuda:\n        assert torch.cuda.is_available()\n    else:\n        assert False, \'Sorry, .pth file contains CUDA version of the network only.\'\n\n    nof = prepareOpenFace()\n    nof = nof.eval()\n\n\n    # test\n    #\n    I = numpy.reshape(numpy.array(range(96 * 96), dtype=numpy.float32) * 0.01, (1,96,96))\n    I = numpy.concatenate([I, I, I], axis=0)\n    I_ = torch.from_numpy(I).unsqueeze(0)\n\n    if useCuda:\n        I_ = I_.cuda()\n\n    print(nof)\n    I_ = Variable(I_)\n    print(nof(I_))\n\n\n\n    #\n    import cv2\n\n    def ReadImage(pathname):\n        img = cv2.imread(pathname)\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = cv2.resize(img, (96, 96), interpolation=cv2.INTER_LINEAR)\n        img = numpy.transpose(img, (2, 0, 1))\n        img = img.astype(numpy.float32) / 255.0\n        print(numpy.min(img), numpy.max(img))\n        print(numpy.sum(img[0]), numpy.sum(img[1]), numpy.sum(img[2]))\n        I_ = torch.from_numpy(img).unsqueeze(0)\n        if useCuda:\n            I_ = I_.cuda()\n        return I_\n\n    img_paths = [\t\\\n        \'/home/polphit/Downloads/face_images/lennon-1.jpg_aligned.png\',\t\\\n        \'/home/polphit/Downloads/face_images/lennon-2.jpg_aligned.png\',\t\\\n        \'/home/polphit/Downloads/face_images/clapton-1.jpg_aligned.png\',\t\\\n        \'/home/polphit/Downloads/face_images/clapton-2.jpg_aligned.png\',\t\\\n    ]\n    imgs = []\n    for img_path in img_paths:\n        imgs.append(ReadImage(img_path))\n\n    I_ = torch.cat(imgs, 0)\n    I_ = Variable(I_, requires_grad=False)\n    start = time.time()\n    f, f_736 = nof(I_)\n    print(""  + Forward pass took {} seconds."".format(time.time() - start))\n    print(f)\n    for i in range(f_736.size(0) - 1):\n        for j in range(i + 1, f_736.size(0)):\n            df = f_736[i] - f_736[j]\n            print(img_paths[i].split(\'/\')[-1], img_paths[j].split(\'/\')[-1], torch.dot(df, df))\n\n    # in OpenFace\'s sample code, cosine distance is usually used for f (128d).\n    \n'"
