file_path,api_count,code
dataloader.py,0,"b'from utils import *\n\nclass data():\n    def __init__(self):\n        self.idx = None # input index\n        self.x0 = [[]] # string input, raw sentence\n        self.x1 = [[]] # string input, tokenized\n        self.xc = [[]] # indexed input, character-level\n        self.xw = [[]] # indexed input, word-level\n        self.y0 = [[]] # actual output\n        self.y1 = None # predicted output\n        self.prob = None # probability\n        self.attn = None # attention heatmap\n\n    def sort(self):\n        self.idx = list(range(len(self.xw)))\n        self.idx.sort(key = lambda x: -len(self.xw[x]))\n        xc = [self.xc[i] for i in self.idx]\n        xw = [self.xw[i] for i in self.idx]\n        lens = [len(x) for x in (self.xw if HRE else xw)]\n        return xc, xw, lens\n\n    def unsort(self):\n        self.idx = sorted(range(len(self.x0)), key = lambda x: self.idx[x])\n        self.y1 = [self.y1[i] for i in self.idx]\n        if self.prob: self.prob = [self.prob[i] for i in self.idx]\n        if self.attn: self.attn = [self.attn[i] for i in self.idx]\n\nclass dataloader():\n    def __init__(self):\n        for a, b in data().__dict__.items():\n            setattr(self, a, b)\n\n    def append_item(self, x0 = None, x1 = None, xc = None, xw = None, y0 = None):\n        if x0: self.x0[-1].append(x0)\n        if x1: self.x1[-1].append(x1)\n        if xc: self.xc[-1].append(xc)\n        if xw: self.xw[-1].append(xw)\n        if y0: self.y0[-1].extend(y0)\n\n    def append_row(self):\n        self.x0.append([])\n        self.x1.append([])\n        self.xc.append([])\n        self.xw.append([])\n        self.y0.append([])\n\n    def strip(self):\n        if len(self.xw[-1]):\n            return\n        self.x0.pop()\n        self.x1.pop()\n        self.xc.pop()\n        self.xw.pop()\n        self.y0.pop()\n\n    @staticmethod\n    def flatten(ls):\n        if HRE:\n            return [list(x) for x in ls for x in x]\n        return [list(*x) for x in ls]\n\n    def split(self): # split into batches\n        for i in range(0, len(self.y0), BATCH_SIZE):\n            batch = data()\n            j = i + min(BATCH_SIZE, len(self.x0) - i)\n            batch.x0 = self.x0[i:j]\n            batch.x1 = self.flatten(self.x1[i:j])\n            batch.xc = self.flatten(self.xc[i:j])\n            batch.xw = self.flatten(self.xw[i:j])\n            batch.y0 = self.y0[i:j]\n            yield batch\n\n    def tensor(self, bc, bw, lens = None, sos = False, eos = False):\n        _p, _s, _e = [PAD_IDX], [SOS_IDX], [EOS_IDX]\n        if HRE and lens:\n            d_len = max(lens) # document length (Ld)\n            i, _bc, _bw = 0, [], []\n            for j in lens:\n                if sos:\n                    _bc.append([[]])\n                    _bw.append([])\n                _bc.extend(bc[i:i + j] + [[[]] for _ in range(d_len - j)])\n                _bw.extend(bw[i:i + j] + [[] for _ in range(d_len - j)])\n                if eos:\n                    _bc.append([[]])\n                    _bw.append([])\n                i += j\n            bc, bw = _bc, _bw\n        if bw:\n            s_len = max(map(len, bw)) # sentence length (Ls)\n            bw = [_s * sos + x + _e * eos + _p * (s_len - len(x)) for x in bw]\n            bw = LongTensor(bw) # [B * Ld, Ls]\n        if bc:\n            w_len = max(max(map(len, x)) for x in bc) # word length (Lw)\n            w_pad = [_p * (w_len + 2)]\n            bc = [[_s + w + _e + _p * (w_len - len(w)) for w in x] for x in bc]\n            bc = [w_pad * sos + x + w_pad * (s_len - len(x) + eos) for x in bc]\n            bc = LongTensor(bc) # [B * Ld, Ls, Lw]\n        return bc, bw\n'"
embedding.py,9,"b'from utils import *\n\nclass embed(nn.Module):\n    def __init__(self, cti_size, wti_size, hre = False):\n        super().__init__()\n        self.hre = hre # hierarchical recurrent encoding\n\n        # architecture\n        for model, dim in EMBED.items():\n            if model == ""char-cnn"":\n                self.char_embed = self.cnn(cti_size, dim)\n            elif model == ""char-rnn"":\n                self.char_embed = self.rnn(cti_size, dim)\n            if model == ""lookup"":\n                self.word_embed = nn.Embedding(wti_size, dim, padding_idx = PAD_IDX)\n            elif model == ""sae"":\n                self.word_embed = self.sae(wti_size, dim)\n        if self.hre:\n            self.sent_embed = self.rnn(EMBED_SIZE, EMBED_SIZE, True)\n        self = self.cuda() if CUDA else self\n\n    def forward(self, xc, xw):\n        hc, hw = None, None\n        if ""char-cnn"" in EMBED or ""char-rnn"" in EMBED:\n            hc = self.char_embed(xc)\n        if ""lookup"" in EMBED or ""sae"" in EMBED:\n            hw = self.word_embed(xw)\n        h = torch.cat([h for h in [hc, hw] if type(h) == torch.Tensor], 2)\n        if self.hre:\n            h = self.sent_embed(h) if self.hre else h\n        return h\n\n    class cnn(nn.Module):\n        def __init__(self, vocab_size, embed_size):\n            super().__init__()\n            dim = 50\n            num_featmaps = 50 # feature maps generated by each kernel\n            kernel_sizes = [3]\n\n            # architecture\n            self.embed = nn.Embedding(vocab_size, dim, padding_idx = PAD_IDX)\n            self.conv = nn.ModuleList([nn.Conv2d(\n                in_channels = 1, # Ci\n                out_channels = num_featmaps, # Co\n                kernel_size = (i, dim) # height, width\n            ) for i in kernel_sizes]) # num_kernels (K)\n            self.dropout = nn.Dropout(DROPOUT)\n            self.fc = nn.Linear(len(kernel_sizes) * num_featmaps, embed_size)\n\n        def forward(self, x):\n            b = x.size(0) # batch_size (B)\n            x = x.view(-1, x.size(2)) # [B * word_seq_len (Lw), char_seq_len (Lc)]\n            x = self.embed(x) # [B * Lw, Lc, dim]\n            x = x.unsqueeze(1) # [B * Lw, Ci, Lc, W]\n            h = [conv(x) for conv in self.conv] # [B * Lw, Co, Lc, 1] * K\n            h = [F.relu(k).squeeze(3) for k in h] # [B * Lw, Co, Lc] * K\n            h = [F.max_pool1d(k, k.size(2)).squeeze(2) for k in h] # [B * Lw, Co] * K\n            h = torch.cat(h, 1) # [B * Lw, Co * K]\n            h = self.dropout(h)\n            h = self.fc(h) # fully connected layer [B * Lw, embed_size]\n            h = h.view(b, -1, h.size(1)) # [B, Lw, embed_size]\n            return h\n\n    class rnn(nn.Module):\n        def __init__(self, vocab_size, embed_size, embedded = False):\n            super().__init__()\n            self.dim = embed_size\n            self.rnn_type = ""GRU"" # LSTM, GRU\n            self.num_dirs = 2 # unidirectional: 1, bidirectional: 2\n            self.num_layers = 2\n            self.embedded = embedded # True: sent_embed, False: word_embed\n\n            # architecture\n            self.embed = nn.Embedding(vocab_size, embed_size, padding_idx = PAD_IDX)\n            self.rnn = getattr(nn, self.rnn_type)(\n                input_size = self.dim,\n                hidden_size = self.dim // self.num_dirs,\n                num_layers = self.num_layers,\n                bias = True,\n                batch_first = True,\n                dropout = DROPOUT,\n                bidirectional = (self.num_dirs == 2)\n            )\n\n        def init_state(self, b): # initialize RNN states\n            n = self.num_layers * self.num_dirs\n            h = self.dim // self.num_dirs\n            hs = zeros(n, b, h) # hidden state\n            if self.rnn_type == ""LSTM"":\n                cs = zeros(n, b, h) # LSTM cell state\n                return (hs, cs)\n            return hs\n\n        def forward(self, x):\n            b = x.size(0) # batch_size (B)\n            s = self.init_state(b * (1 if self.embedded else x.size(1)))\n            if not self.embedded: # word_embed\n                x = x.view(-1, x.size(2)) # [B * word_seq_len (Lw), char_seq_len (Lc)]\n                x = self.embed(x) # [B * Lw, Lc, embed_size (H)]\n            h, s = self.rnn(x, s)\n            h = s if self.rnn_type == ""GRU"" else s[-1]\n            h = torch.cat([x for x in h[-self.num_dirs:]], 1) # final hidden state [B * Lw, H]\n            h = h.view(b, -1, h.size(1)) # [B, Lw, H]\n            return h\n\n    class sae(nn.Module): # self-attentive encoder\n        def __init__(self, vocab_size, embed_size = 512):\n            super().__init__()\n            dim = embed_size\n            num_layers = 1\n\n            # architecture\n            self.embed = nn.Embedding(vocab_size, dim, padding_idx = PAD_IDX)\n            self.pe = self.pos_encoding(dim)\n            self.layers = nn.ModuleList([self.layer(dim) for _ in range(num_layers)])\n\n        def forward(self, x):\n            mask = x.eq(PAD_IDX).view(x.size(0), 1, 1, -1)\n            x = self.embed(x)\n            h = x + self.pe[:x.size(1)]\n            for layer in self.layers:\n                h = layer(h, mask)\n            return h\n\n        @staticmethod\n        def pos_encoding(dim, maxlen = 1000): # positional encoding\n            pe = Tensor(maxlen, dim)\n            pos = torch.arange(0, maxlen, 1.).unsqueeze(1)\n            k = torch.exp(-np.log(10000) * torch.arange(0, dim, 2.) / dim)\n            pe[:, 0::2] = torch.sin(pos * k)\n            pe[:, 1::2] = torch.cos(pos * k)\n            return pe\n\n        class layer(nn.Module): # encoder layer\n            def __init__(self, dim):\n                super().__init__()\n\n                # architecture\n                self.attn = embed.sae.attn_mh(dim)\n                self.ffn = embed.sae.ffn(dim)\n\n            def forward(self, x, mask):\n                z = self.attn(x, x, x, mask)\n                z = self.ffn(z)\n                return z\n\n        class attn_mh(nn.Module): # multi-head attention\n            def __init__(self, dim):\n                super().__init__()\n                self.D = dim # dimension of model\n                self.H = 8 # number of heads\n                self.Dk = self.D // self.H # dimension of key\n                self.Dv = self.D // self.H # dimension of value\n\n                # architecture\n                self.Wq = nn.Linear(self.D, self.H * self.Dk) # query\n                self.Wk = nn.Linear(self.D, self.H * self.Dk) # key for attention distribution\n                self.Wv = nn.Linear(self.D, self.H * self.Dv) # value for context representation\n                self.Wo = nn.Linear(self.H * self.Dv, self.D)\n                self.dropout = nn.Dropout(DROPOUT)\n                self.norm = nn.LayerNorm(self.D)\n\n            def attn_sdp(self, q, k, v, mask): # scaled dot-product attention\n                c = np.sqrt(self.Dk) # scale factor\n                a = torch.matmul(q, k.transpose(2, 3)) / c # compatibility function\n                a = a.masked_fill(mask, -10000) # masking in log space\n                a = F.softmax(a, -1)\n                a = torch.matmul(a, v)\n                return a # attention weights\n\n            def forward(self, q, k, v, mask):\n                b = q.size(0) # batch_size (B)\n                x = q # identity\n                q = self.Wq(q).view(b, -1, self.H, self.Dk).transpose(1, 2)\n                k = self.Wk(k).view(b, -1, self.H, self.Dk).transpose(1, 2)\n                v = self.Wv(v).view(b, -1, self.H, self.Dv).transpose(1, 2)\n                z = self.attn_sdp(q, k, v, mask)\n                z = z.transpose(1, 2).contiguous().view(b, -1, self.H * self.Dv)\n                z = self.Wo(z)\n                z = self.norm(x + self.dropout(z)) # residual connection and dropout\n                return z\n\n        class ffn(nn.Module): # position-wise feed-forward networks\n            def __init__(self, dim):\n                super().__init__()\n                dim_ffn = 2048\n\n                # architecture\n                self.layers = nn.Sequential(\n                    nn.Linear(dim, dim_ffn),\n                    nn.ReLU(),\n                    nn.Dropout(DROPOUT),\n                    nn.Linear(dim_ffn, dim)\n                )\n                self.norm = nn.LayerNorm(dim)\n\n            def forward(self, x):\n                z = x + self.layers(x) # residual connection\n                z = self.norm(z) # layer normalization\n                return z\n'"
evaluate.py,0,"b'from predict import *\n\ndef evaluate(result, summary = False):\n    avg = defaultdict(float) # average\n    tp = defaultdict(int) # true positives\n    tpfn = defaultdict(int) # true positives + false negatives\n    tpfp = defaultdict(int) # true positives + false positives\n    for _, y0, y1 in result: # actual value, prediction\n        if HRE:\n            tp[y0] += (y0 == y1)\n            tpfn[y0] += 1\n            tpfp[y1] += 1\n            continue\n        for y0, y1 in zip(y0, y1):\n            tp[y0] += (y0 == y1)\n            tpfn[y0] += 1\n            tpfp[y1] += 1\n    print()\n    for y in sorted(tpfn.keys()):\n        pr = (tp[y] / tpfp[y]) if tpfp[y] else 0\n        rc = (tp[y] / tpfn[y]) if tpfn[y] else 0\n        avg[""macro_pr""] += pr\n        avg[""macro_rc""] += rc\n        if not summary:\n            print(""label = %s"" % y)\n            print(""precision = %f (%d/%d)"" % (pr, tp[y], tpfp[y]))\n            print(""recall = %f (%d/%d)"" % (rc, tp[y], tpfn[y]))\n            print(""f1 = %f\\n"" % f1(pr, rc))\n    avg[""macro_pr""] /= len(tpfn)\n    avg[""macro_rc""] /= len(tpfn)\n    avg[""micro_f1""] = sum(tp.values()) / sum(tpfn.values())\n    print(""macro precision = %f"" % avg[""macro_pr""])\n    print(""macro recall = %f"" % avg[""macro_rc""])\n    print(""macro f1 = %f"" % f1(avg[""macro_pr""], avg[""macro_rc""]))\n    print(""micro f1 = %f"" % avg[""micro_f1""])\n\nif __name__ == ""__main__"":\n    if len(sys.argv) != 6:\n        sys.exit(""Usage: %s model char_to_idx word_to_idx tag_to_idx test_data"" % sys.argv[0])\n    evaluate(predict(sys.argv[5], *load_model()))\n'"
model.py,5,"b'from utils import *\nfrom embedding import embed\n\nclass rnn_crf(nn.Module):\n    def __init__(self, cti_size, wti_size, num_tags):\n        super().__init__()\n        self.rnn = rnn(cti_size, wti_size, num_tags)\n        self.crf = crf(num_tags)\n        self = self.cuda() if CUDA else self\n\n    def forward(self, xc, xw, y0): # for training\n        self.zero_grad()\n        self.rnn.batch_size = y0.size(0)\n        self.crf.batch_size = y0.size(0)\n        mask = y0[:, 1:].gt(PAD_IDX).float()\n        h = self.rnn(xc, xw, mask)\n        Z = self.crf.forward(h, mask)\n        score = self.crf.score(h, y0, mask)\n        return torch.mean(Z - score) # NLL loss\n\n    def decode(self, xc, xw, lens): # for inference\n        self.rnn.batch_size = len(lens)\n        self.crf.batch_size = len(lens)\n        if HRE:\n            mask = Tensor([[1] * x + [PAD_IDX] * (lens[0] - x) for x in lens])\n        else:\n            mask = xw.gt(PAD_IDX).float()\n        h = self.rnn(xc, xw, mask)\n        return self.crf.decode(h, mask)\n\nclass rnn(nn.Module):\n    def __init__(self, cti_size, wti_size, num_tags):\n        super().__init__()\n        self.batch_size = 0\n\n        # architecture\n        self.embed = embed(cti_size, wti_size, HRE)\n        self.rnn = getattr(nn, RNN_TYPE)(\n            input_size = EMBED_SIZE,\n            hidden_size = HIDDEN_SIZE // NUM_DIRS,\n            num_layers = NUM_LAYERS,\n            bias = True,\n            batch_first = True,\n            dropout = DROPOUT,\n            bidirectional = (NUM_DIRS == 2)\n        )\n        self.out = nn.Linear(HIDDEN_SIZE, num_tags) # RNN output to tag\n\n    def init_state(self, b): # initialize RNN states\n        n = NUM_LAYERS * NUM_DIRS\n        h = HIDDEN_SIZE // NUM_DIRS\n        hs = zeros(n, b, h) # hidden state\n        if RNN_TYPE == ""LSTM"":\n            cs = zeros(n, b, h) # LSTM cell state\n            return (hs, cs)\n        return hs\n\n    def forward(self, xc, xw, mask):\n        hs = self.init_state(self.batch_size)\n        x = self.embed(xc, xw)\n        if HRE: # [B * doc_len, 1, H] -> [B, doc_len, H]\n            x = x.view(self.batch_size, -1, EMBED_SIZE)\n        x = nn.utils.rnn.pack_padded_sequence(x, mask.sum(1).int(), batch_first = True)\n        h, _ = self.rnn(x, hs)\n        h, _ = nn.utils.rnn.pad_packed_sequence(h, batch_first = True)\n        h = self.out(h)\n        h *= mask.unsqueeze(2)\n        return h\n\nclass crf(nn.Module):\n    def __init__(self, num_tags):\n        super().__init__()\n        self.batch_size = 0\n        self.num_tags = num_tags\n\n        # matrix of transition scores from j to i\n        self.trans = nn.Parameter(randn(num_tags, num_tags))\n        self.trans.data[SOS_IDX, :] = -10000 # no transition to SOS\n        self.trans.data[:, EOS_IDX] = -10000 # no transition from EOS except to PAD\n        self.trans.data[:, PAD_IDX] = -10000 # no transition from PAD except to PAD\n        self.trans.data[PAD_IDX, :] = -10000 # no transition to PAD except from EOS\n        self.trans.data[PAD_IDX, EOS_IDX] = 0\n        self.trans.data[PAD_IDX, PAD_IDX] = 0\n\n    def forward(self, h, mask): # forward algorithm\n        # initialize forward variables in log space\n        score = Tensor(self.batch_size, self.num_tags).fill_(-10000) # [B, C]\n        score[:, SOS_IDX] = 0.\n        trans = self.trans.unsqueeze(0) # [1, C, C]\n        for t in range(h.size(1)): # recursion through the sequence\n            mask_t = mask[:, t].unsqueeze(1)\n            emit_t = h[:, t].unsqueeze(2) # [B, C, 1]\n            score_t = score.unsqueeze(1) + emit_t + trans # [B, 1, C] -> [B, C, C]\n            score_t = log_sum_exp(score_t) # [B, C, C] -> [B, C]\n            score = score_t * mask_t + score * (1 - mask_t)\n        score = log_sum_exp(score + self.trans[EOS_IDX])\n        return score # partition function\n\n    def score(self, h, y0, mask): # calculate the score of a given sequence\n        score = Tensor(self.batch_size).fill_(0.)\n        h = h.unsqueeze(3)\n        trans = self.trans.unsqueeze(2)\n        for t in range(h.size(1)): # recursion through the sequence\n            mask_t = mask[:, t]\n            emit_t = torch.cat([h[t, y0[t + 1]] for h, y0 in zip(h, y0)])\n            trans_t = torch.cat([trans[y0[t + 1], y0[t]] for y0 in y0])\n            score += (emit_t + trans_t) * mask_t\n        last_tag = y0.gather(1, mask.sum(1).long().unsqueeze(1)).squeeze(1)\n        score += self.trans[EOS_IDX, last_tag]\n        return score\n\n    def decode(self, h, mask): # Viterbi decoding\n        # initialize backpointers and viterbi variables in log space\n        bptr = LongTensor()\n        score = Tensor(self.batch_size, self.num_tags).fill_(-10000)\n        score[:, SOS_IDX] = 0.\n\n        for t in range(h.size(1)): # recursion through the sequence\n            mask_t = mask[:, t].unsqueeze(1)\n            score_t = score.unsqueeze(1) + self.trans # [B, 1, C] -> [B, C, C]\n            score_t, bptr_t = score_t.max(2) # best previous scores and tags\n            score_t += h[:, t] # plus emission scores\n            bptr = torch.cat((bptr, bptr_t.unsqueeze(1)), 1)\n            score = score_t * mask_t + score * (1 - mask_t)\n        score += self.trans[EOS_IDX]\n        best_score, best_tag = torch.max(score, 1)\n\n        # back-tracking\n        bptr = bptr.tolist()\n        best_path = [[i] for i in best_tag.tolist()]\n        for b in range(self.batch_size):\n            i = best_tag[b] # best tag\n            j = int(mask[b].sum().item())\n            for bptr_t in reversed(bptr[b][:j]):\n                i = bptr_t[i]\n                best_path[b].append(i)\n            best_path[b].pop()\n            best_path[b].reverse()\n\n        return best_path\n'"
parameters.py,9,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nUNIT = ""word"" # unit of tokenization (char, word, sent)\nTASK = None # task (None, word-segmentation, sentence-segmentation)\nRNN_TYPE = ""LSTM"" # LSTM or GRU\nNUM_DIRS = 2 # unidirectional: 1, bidirectional: 2\nNUM_LAYERS = 2\nBATCH_SIZE = 128\nHRE = (UNIT == ""sent"") # hierarchical recurrent encoding\nEMBED = {""lookup"": 300} # embeddings (char-cnn, char-rnn, lookup, sae)\nEMBED_SIZE = sum(EMBED.values())\nHIDDEN_SIZE = 1000\nDROPOUT = 0.5\nLEARNING_RATE = 2e-4\nEVAL_EVERY = 10\nSAVE_EVERY = 10\n\nPAD = ""<PAD>"" # padding\nSOS = ""<SOS>"" # start of sequence\nEOS = ""<EOS>"" # end of sequence\nUNK = ""<UNK>"" # unknown token\n\nPAD_IDX = 0\nSOS_IDX = 1\nEOS_IDX = 2\nUNK_IDX = 3\n\nCUDA = torch.cuda.is_available()\ntorch.manual_seed(0) # for reproducibility\n# torch.cuda.set_device(0)\n\nTensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\nLongTensor = torch.cuda.LongTensor if CUDA else torch.LongTensor\nrandn = lambda *x: torch.randn(*x).cuda() if CUDA else torch.randn\nzeros = lambda *x: torch.zeros(*x).cuda() if CUDA else torch.zeros\n\nKEEP_IDX = False # use the existing indices when adding more training data\nNUM_DIGITS = 4 # number of decimal places to print\n'"
predict.py,1,"b'from model import *\nfrom utils import *\nfrom dataloader import *\n\ndef load_model():\n    cti = load_tkn_to_idx(sys.argv[2]) # char_to_idx\n    wti = load_tkn_to_idx(sys.argv[3]) # word_to_idx\n    itt = load_idx_to_tkn(sys.argv[4]) # idx_to_tag\n    model = rnn_crf(len(cti), len(wti), len(itt))\n    print(model)\n    load_checkpoint(sys.argv[1], model)\n    return model, cti, wti, itt\n\ndef run_model(model, data, itt):\n    with torch.no_grad():\n        model.eval()\n        for batch in data.split():\n            xc, xw, lens = batch.sort()\n            xc, xw = data.tensor(xc, xw, lens)\n            y1 = model.decode(xc, xw, lens)\n            batch.y1 = [[itt[i] for i in x] for x in y1]\n            batch.unsort()\n            for x0, y0, y1 in zip(batch.x0, batch.y0, batch.y1):\n                if not HRE:\n                    y0, y1 = [y0], [y1]\n                for x0, y0, y1 in zip(x0, y0, y1):\n                    yield x0, y0, y1\n\ndef predict(filename, model, cti, wti, itt):\n    data = dataloader()\n    with open(filename) as fo:\n        text = fo.read().strip().split(""\\n"" * (HRE + 1))\n    for block in text:\n        for x0 in block.split(""\\n""):\n            if re.match(""\\S+/\\S+( \\S+/\\S+)*$"", x0): # word/tag\n                x0, y0 = zip(*[re.split(""/(?=[^/]+$)"", x) for x in x0.split("" "")])\n                x0 = "" "".join(x0)\n            elif re.match(""[^\\t]+\\t\\S+$"", x0): # sentence \\t label\n                x0, *y0 = x0.split(""\\t"")\n            else: # no ground truth provided\n                y0 = []\n            x1 = tokenize(x0)\n            xc = [[cti[c] if c in cti else UNK_IDX for c in w] for w in x1]\n            xw = [wti[w] if w in wti else UNK_IDX for w in x1]\n            data.append_item(x0, x1, xc, xw, y0)\n        data.append_row()\n    data.strip()\n    return run_model(model, data, itt)\n\nif __name__ == ""__main__"":\n    if len(sys.argv) != 6:\n        sys.exit(""Usage: %s model char_to_idx word_to_idx tag_to_idx test_data"" % sys.argv[0])\n    result = predict(sys.argv[5], *load_model())\n    for x0, y0, y1 in result:\n        if not TASK:\n            print((x0, y0, y1) if y0 else (x0, y1))\n        else: # word/sentence segmentation\n            if y0:\n                print(iob_to_txt(x0, y0))\n            print(iob_to_txt(x0, y1))\n            print()\n'"
prepare.py,0,"b'from utils import *\n\ndef load_data():\n    data = []\n    if KEEP_IDX:\n        cti = load_tkn_to_idx(sys.argv[1] + "".char_to_idx"")\n        wti = load_tkn_to_idx(sys.argv[1] + "".word_to_idx"")\n        tti = load_tkn_to_idx(sys.argv[1] + "".tag_to_idx"")\n    else:\n        cti = {PAD: PAD_IDX, SOS: SOS_IDX, EOS: EOS_IDX, UNK: UNK_IDX}\n        wti = {PAD: PAD_IDX, SOS: SOS_IDX, EOS: EOS_IDX, UNK: UNK_IDX}\n        tti = {PAD: PAD_IDX, SOS: SOS_IDX, EOS: EOS_IDX}\n    fo = open(sys.argv[1])\n    if HRE:\n        tmp = []\n        txt = fo.read().strip().split(""\\n\\n"")\n        for doc in txt:\n            data.append([])\n            for line in doc.split(""\\n""):\n                x, y = load_line(line, cti, wti, tti)\n                data[-1].append((x, y))\n        for doc in sorted(data, key = lambda x: -len(x)):\n            tmp.extend(doc)\n            tmp.append(None)\n        data = tmp[:-1]\n    else:\n        for line in fo:\n            line = line.strip()\n            x, y = load_line(line, cti, wti, tti)\n            data.append((x, y))\n        data.sort(key = lambda x: -len(x[0])) # sort by source sequence length\n    fo.close()\n    return data, cti, wti, tti\n\ndef load_line(line, cti, wti, tti):\n    x, y = [], []\n    if HRE:\n        line, y = line.split(""\\t"")\n        if y not in tti:\n            tti[y] = len(tti)\n        y = [str(tti[y])]\n    for w in line.split("" ""):\n        w, tag = (w, None) if HRE else re.split(""/(?=[^/]+$)"", w)\n        w0 = normalize(w) # for character embedding\n        w1 = w0.lower() # for word embedding\n        if not KEEP_IDX:\n            for c in w0:\n                if c not in cti:\n                    cti[c] = len(cti)\n            if w1 not in wti:\n                wti[w1] = len(wti)\n            if tag and tag not in tti:\n                tti[tag] = len(tti)\n        x.append(""+"".join(str(cti[c]) for c in w0) + "":%d"" % wti[w1])\n        if tag:\n            y.append(str(tti[tag]))\n    return x, y\n\nif __name__ == ""__main__"":\n    if len(sys.argv) != 2:\n        sys.exit(""Usage: %s training_data"" % sys.argv[0])\n    data, cti, wti, tti = load_data()\n    save_data(sys.argv[1] + "".csv"", data)\n    if not KEEP_IDX:\n        save_tkn_to_idx(sys.argv[1] + "".char_to_idx"", cti)\n        save_tkn_to_idx(sys.argv[1] + "".word_to_idx"", wti)\n        save_tkn_to_idx(sys.argv[1] + "".tag_to_idx"", tti)\n'"
train.py,1,"b'from model import *\nfrom utils import *\nfrom evaluate import *\nfrom dataloader import *\n\ndef load_data():\n    data = dataloader()\n    batch = []\n    cti = load_tkn_to_idx(sys.argv[2]) # char_to_idx\n    wti = load_tkn_to_idx(sys.argv[3]) # word_to_idx\n    itt = load_idx_to_tkn(sys.argv[4]) # idx_to_tkn\n    print(""loading %s..."" % sys.argv[5])\n    with open(sys.argv[5]) as fo:\n        text = fo.read().strip().split(""\\n"" * (HRE + 1))\n    for block in text:\n        for line in block.split(""\\n""):\n            x, y = line.split(""\\t"")\n            x = [x.split("":"") for x in x.split("" "")]\n            y = [int(y)] if HRE else [int(x) for x in y.split("" "")]\n            xc, xw = zip(*[(list(map(int, xc.split(""+""))), int(xw)) for xc, xw in x])\n            data.append_item(xc = xc, xw = xw, y0 = y)\n        data.append_row()\n    data.strip()\n    for _batch in data.split():\n        xc, xw = data.tensor(*_batch.sort())\n        _, y0 = data.tensor(None, _batch.y0, sos = True)\n        batch.append((xc, xw, y0))\n    print(""data size: %d"" % len(data.y0))\n    print(""batch size: %d"" % BATCH_SIZE)\n    return batch, cti, wti, itt\n\ndef train():\n    num_epochs = int(sys.argv[-1])\n    batch, cti, wti, itt = load_data()\n    model = rnn_crf(len(cti), len(wti), len(itt))\n    optim = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n    print(model)\n    epoch = load_checkpoint(sys.argv[1], model) if isfile(sys.argv[1]) else 0\n    filename = re.sub(""\\.epoch[0-9]+$"", """", sys.argv[1])\n    print(""training model..."")\n    for ei in range(epoch + 1, epoch + num_epochs + 1):\n        loss_sum = 0\n        timer = time()\n        for xc, xw, y0 in batch:\n            loss = model(xc, xw, y0) # forward pass and compute loss\n            loss.backward() # compute gradients\n            optim.step() # update parameters\n            loss_sum += loss.item()\n        timer = time() - timer\n        loss_sum /= len(batch)\n        if ei % SAVE_EVERY and ei != epoch + num_epochs:\n            save_checkpoint("""", None, ei, loss_sum, timer)\n        else:\n            save_checkpoint(filename, model, ei, loss_sum, timer)\n        if EVAL_EVERY and (ei % EVAL_EVERY == 0 or ei == epoch + num_epochs):\n            args = [model, cti, wti, itt]\n            evaluate(predict(sys.argv[6], *args), True)\n            model.train()\n            print()\n\nif __name__ == ""__main__"":\n    if len(sys.argv) not in [7, 8]:\n        sys.exit(""Usage: %s model char_to_idx word_to_idx tag_to_idx training_data (validation_data) num_epoch"" % sys.argv[0])\n    if len(sys.argv) == 7:\n        EVAL_EVERY = False\n    train()\n'"
utils.py,4,"b'import sys\nimport re\nfrom time import time\nfrom os.path import isfile\nfrom parameters import *\nfrom collections import defaultdict\n\ndef normalize(x):\n    # x = re.sub(""[\\uAC00-\\uD7A3]+"", ""\\uAC00"", x) \xc2\xa3 convert Hangeul to \xea\xb0\x80\n    # x = re.sub(""[\\u3040-\\u30FF]+"", ""\\u3042"", x) # convert Hiragana and Katakana to \xe3\x81\x82\n    # x = re.sub(""[\\u4E00-\\u9FFF]+"", ""\\u6F22"", x) # convert CJK unified ideographs to \xe6\xbc\xa2\n    x = re.sub(""\\s+"", "" "", x)\n    x = re.sub(""^ | $"", """", x)\n    x = x.lower()\n    return x\n\ndef tokenize(x, norm = True):\n    if norm:\n        x = normalize(x)\n    if UNIT == ""char"":\n        return re.sub("" "", """", x)\n    if UNIT in (""word"", ""sent""):\n        return x.split("" "")\n\ndef save_data(filename, data):\n    fo = open(filename, ""w"")\n    for seq in data:\n        fo.write(("" "".join(seq[0]) + ""\\t"" + "" "".join(seq[1]) if seq else """") + ""\\n"")\n    fo.close()\n\ndef load_tkn_to_idx(filename):\n    print(""loading %s"" % filename)\n    tkn_to_idx = {}\n    fo = open(filename)\n    for line in fo:\n        line = line[:-1]\n        tkn_to_idx[line] = len(tkn_to_idx)\n    fo.close()\n    return tkn_to_idx\n\ndef load_idx_to_tkn(filename):\n    print(""loading %s"" % filename)\n    idx_to_tkn = []\n    fo = open(filename)\n    for line in fo:\n        line = line[:-1]\n        idx_to_tkn.append(line)\n    fo.close()\n    return idx_to_tkn\n\ndef save_tkn_to_idx(filename, tkn_to_idx):\n    fo = open(filename, ""w"")\n    for tkn, _ in sorted(tkn_to_idx.items(), key = lambda x: x[1]):\n        fo.write(""%s\\n"" % tkn)\n    fo.close()\n\ndef load_checkpoint(filename, model = None):\n    print(""loading %s"" % filename)\n    checkpoint = torch.load(filename)\n    if model:\n        model.load_state_dict(checkpoint[""state_dict""])\n    epoch = checkpoint[""epoch""]\n    loss = checkpoint[""loss""]\n    print(""saved model: epoch = %d, loss = %f"" % (checkpoint[""epoch""], checkpoint[""loss""]))\n    return epoch\n\ndef save_checkpoint(filename, model, epoch, loss, time):\n    print(""epoch = %d, loss = %f, time = %f"" % (epoch, loss, time))\n    if filename and model:\n        print(""saving %s"" % filename)\n        checkpoint = {}\n        checkpoint[""state_dict""] = model.state_dict()\n        checkpoint[""epoch""] = epoch\n        checkpoint[""loss""] = loss\n        torch.save(checkpoint, filename + "".epoch%d"" % epoch)\n        print(""saved model at epoch %d"" % epoch)\n\ndef log_sum_exp(x):\n    m = torch.max(x, -1)[0]\n    return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(-1)), -1))\n\ndef iob_to_txt(x, y): # for word/sentence segmentation\n    out = [[]]\n    if re.match(""(\\S+/\\S+( |$))+"", x): # token/tag\n        x = re.sub(r""/[^ /]+\\b"", """", x) # remove tags\n    for i, (j, k) in enumerate(zip(tokenize(x, False), y)):\n        if i and k[0] == ""B"":\n            out.append([])\n        out[-1].append(j)\n    if TASK == ""word-segmentation"":\n        d1, d2 = """", "" ""\n    if TASK == ""sentence-segmentation"":\n        d1, d2 = "" "", ""\\n""\n    return d2.join(d1.join(x) for x in out)\n\ndef f1(p, r):\n    return 2 * p * r / (p + r) if p + r else 0\n'"
pos-tagging/brown2ptb.py,0,"b'import sys\nimport re\n\ndef convert(tkn):\n    out = []\n    for word, tag in tkn:\n        tags = []\n        for x in tag.split(""+""):\n            neg = False\n            pos = False\n            if x[:3] == ""FW-"": # foreign word\n                tags = [""FW""]\n                break\n            if x[-3:] == ""-NC"": x = x[:-3]\n            if x[-3:] == ""-HL"": x = x[:-3]\n            if x[-3:] == ""-TL"": x = x[:-3]\n            if len(x) and x[-1] == ""*"":\n                x = x[:-1]\n                neg = True\n            if len(x) and x[-1] == ""$"":\n                if x == ""PP$"": tags.append(""DT"") # possessive pronoun\n                elif x == ""PP$$"": tags.append(""PRO"") # possessive pronoun\n                elif x == ""WP$"": tags.append(""WH"") # possessive wh-pronoun\n                else:\n                    x = x[:-1]\n                    pos = True\n            if re.match(""[^A-Z]+$"", x): tags.append(x) # other special characters\n            if x == ""ABL"": tags.append(""RB"")\n            if re.match(""A(B[NX]|[PT])$"", x): tags.append(""DT"")\n            if x == ""BE"": tags.append(""VB"")\n            if re.match(""BEDZ?$"", x): tags.append(""VB"")\n            if re.match(""BE[GN]$"", x): tags.append(""VB"")\n            if re.match(""BE[MRZ]$"", x): tags.append(""VB"")\n            if x == ""CC"": tags.append(""CC"")\n            if x == ""CS"": tags.append(""CC"")\n            if x == ""CD"": tags.append(""CD"") # cardinal numeral\n            if x == ""OD"": tags.append(""JJ"") # ordinal numeral\n            if re.match(""DO[DZ]?$"", x): tags.append(""VB"")\n            if re.match(""DT[ISX]?$"", x): tags.append(""DT"")\n            if x == ""EX"": tags.append(""RB"")\n            if x == ""HV"": tags.append(""VB"")\n            if re.match(""HV[DGNZ]$"", x): tags.append(""VB"")\n            if x == ""IN"": tags.append(""IN"")\n            if re.match(""JJ[RST]?$"", x): tags.append(""JJ"")\n            if x == ""MD"": tags.append(""AUX"")\n            if x == ""NIL"": tags.append(""UNK"")\n            if re.match(""NNS?$"", x): tags.append(""NN"") # noun\n            if re.match(""NPS?$"", x): tags.append(""NN"") # proper noun\n            if re.match(""NRS?$"", x): tags.append(""NN"") # adverbial noun\n            if re.match(""P(N|P[LOS]S?)$"", x): tags.append(""PRO"")\n            if re.match(""QLP?$"", x): tags.append(""RB"")\n            if re.match(""RB[RT]?$"", x): tags.append(""RB"")\n            if x == ""RN"": tags.append(""RB"") # nominal adverb\n            if x == ""RP"": tags.append(""RP"") # particle\n            if x == ""TO"": tags.append(""RP"")\n            if x == ""UH"": tags.append(""UH"") # interjection\n            if re.match(""VB[DGNZ]?$"", x): tags.append(""VB"")\n            if x == ""WDT"": tags.append(""WH"")\n            if re.match(""WP[OS]$"", x): tags.append(""WH"")\n            if re.match(""W(QL|RB)$"", x): tags.append(""WH"")\n            if neg:\n                tags.append(""NEG"")\n            if pos:\n                tags.append(""POS"")\n        out.append(word + ""/"" + ""+"".join(tags))\n    return out\n\nif __name__ == ""__main__"":\n    fin = open(""brown.tagged.merged.uniq"", ""r"")\n    fout = open(""brown.tagged.merged.uniq.ptb"", ""w"")\n    for line in fin:\n        line = line.strip()\n        tkn = [re.split(""/(?=[^/]+$)"", x) for x in line.split()]\n        tkn = convert(tkn)\n        fout.write("" "".join(tkn) + ""\\n"")\n    fin.close()\n    fout.close()\n'"
pos-tagging/char+tag+iob.py,0,"b'import sys\nimport re\n\nif __name__ == ""__main__"": # tag every character in tag-IOB2 format for POS tagging\n    if len(sys.argv) != 2:\n        sys.exit(""Usage: %s training_data"" % sys.argv[0])\n    fi = open(sys.argv[1])\n    fo = open(sys.argv[1] + "".char+tag+iob"", ""w"")\n    for line in fi:\n        line = line.strip().split("" "")\n        out = []\n        for token in line:\n            word, tag = re.split(""/(?=[^/]+$)"", token)\n            out.extend([""%s/%s-%s"" % (x, tag, ""I"" if i else ""B"") for i, x in enumerate(word)])\n        fo.write("" "".join(out) + ""\\n"")\n    fi.close()\n    fo.close()\n'"
pos-tagging/filter.py,0,"b'import sys\nimport re\nfrom collections import defaultdict\n\nif len(sys.argv) not in [2, 3]:\n    sys.exit(""Usage: %s brown|ptb word|tag|word/tag"" % sys.argv[0])\n\npl = defaultdict(int)\n\nif sys.argv[1] == ""brown"":\n    fo = open(""brown.tagged.merged.uniq"")\nif sys.argv[1] == ""ptb"":\n    fo = open(""brown.tagged.merged.uniq.ptb"")\n\nfor line in fo:\n    line = line.strip()\n    tkn = [re.split(""/(?=[^/]+$)"", x) for x in line.split()]\n    for i, (word, tag) in enumerate(tkn):\n        word = word.lower()\n        tag = tag.upper()\n        if len(sys.argv) == 2:\n            pl[tag] += 1\n        elif word == sys.argv[2] or tag == sys.argv[2]:\n            pl[word + "" "" + tag] += 1\n        elif sys.argv[2] == word + ""/"" + tag:\n            out = tkn[max(0, i - 2):i]\n            out += [tkn[i]]\n            out += tkn[i + 1:min(len(tkn), i + 3)]\n            print("" "".join([""/"".join(x) for x in out]))\nfo.close()\n\nfor k, v in sorted(pl.items(), key = lambda x: -x[1]):\n    print(k, v)\n\nprint(""%d in total"" % len(pl))\n'"
sentence-classification/block-tokenize.py,0,"b'import sys\nimport random\n\nif __name__ == ""__main__"": # tokenize documents into blocks\n    if len(sys.argv) != 3:\n        sys.exit(""Usage: %s sizes data"" % sys.argv[0])\n    fi = open(sys.argv[2])\n    fo = open(sys.argv[2] + "".block"", ""w"")\n    data = fi.read().strip().split(""\\n\\n"")\n    sizes = list(map(int, sys.argv[1].split("","")))\n    blocks = dict()\n    for doc in data:\n        doc = doc.split(""\\n"")\n        for i in range(len(doc)):\n            for z in sizes:\n                blocks[""\\n"".join(doc[i:i + z])] = True\n                # duplicate blocks are removed\n    blocks = list(blocks.keys())\n    random.shuffle(blocks)\n    fo.write(""\\n\\n"".join(blocks) + ""\\n"")\n    fi.close()\n    fo.close()\n'"
sentence-segmentation/word+iob.py,0,"b'import sys\n\nif __name__ == ""__main__"": # tag words in IOB2 format for sentence segmentation\n    if len(sys.argv) != 2:\n        sys.exit(""Usage: %s data"" % sys.argv[0])\n    fi = open(sys.argv[1])\n    fo = open(sys.argv[1] + "".word+iob"", ""w"")\n    data = fi.read()\n    data = data.strip()\n    data = data.split(""\\n\\n"")\n    for line in data:\n        line = line.split(""\\n"")\n        line = [x.split("" "") for x in line]\n        line = [[x[0] + ""/B""] + [w + ""/I"" for w in x[1:]] for x in line]\n        fo.write("" "".join("" "".join(x) for x in line) + ""\\n"")\n    fi.close()\n    fo.close()\n'"
word-segmentation/char+iob.py,0,"b'import sys\n\nif __name__ == ""__main__"": # tag every character in IOB2 format for word segmentation\n    if len(sys.argv) != 2:\n        sys.exit(""Usage: %s training_data"" % sys.argv[0])\n    fi = open(sys.argv[1])\n    fo = open(sys.argv[1] + "".char+iob"", ""w"")\n    for line in fi:\n        line = line.strip()\n        line = line.split("" "")\n        line = [[w[0] + ""/B""] + [c + ""/I"" for c in w[1:]] for w in line]\n        fo.write("" "".join("" "".join(w) for w in line) + ""\\n"")\n    fi.close()\n    fo.close()\n'"
