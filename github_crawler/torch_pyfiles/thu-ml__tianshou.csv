file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nfrom setuptools import setup, find_packages\n\nimport re\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the version string\nwith open(path.join(here, \'tianshou\', \'__init__.py\')) as f:\n    version = re.search(r\'__version__ = \\\'(.*?)\\\'\', f.read()).group(1)\n\nsetup(\n    name=\'tianshou\',\n    version=version,\n    description=\'A Library for Deep Reinforcement Learning\',\n    long_description=open(\'README.md\', encoding=\'utf8\').read(),\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/thu-ml/tianshou\',\n    author=\'TSAIL\',\n    author_email=\'trinkle23897@gmail.com\',\n    license=\'MIT\',\n    python_requires=\'>=3.6\',\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 3 - Alpha\',\n        # Indicate who your project is intended for\n        \'Intended Audience :: Science/Research\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n        # Pick your license as you wish (should match ""license"" above)\n        \'License :: OSI Approved :: MIT License\',\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n    ],\n    keywords=\'reinforcement learning platform pytorch\',\n    packages=find_packages(exclude=[\'test\', \'test.*\',\n                                    \'examples\', \'examples.*\',\n                                    \'docs\', \'docs.*\']),\n    install_requires=[\n        \'gym>=0.15.0\',\n        \'tqdm\',\n        \'numpy\',\n        \'tensorboard\',\n        \'torch>=1.4.0\',\n    ],\n    extras_require={\n        \'dev\': [\n            \'Sphinx\',\n            \'sphinx_rtd_theme\',\n            \'sphinxcontrib-bibtex\',\n            \'flake8\',\n            \'pytest\',\n            \'pytest-cov\',\n        ],\n        \'atari\': [\n            \'atari_py\',\n            \'cv2\',\n        ],\n        \'mujoco\': [\n            \'mujoco_py\',\n        ],\n        \'pybullet\': [\n            \'pybullet\',\n        ],\n    },\n)\n'"
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\nimport tianshou\nimport sphinx_rtd_theme\n\n# Get the version string\nversion = tianshou.__version__\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Tianshou\'\ncopyright = \'2020, Tianshou contributors.\'\nauthor = \'Tianshou contributors\'\n\n# The full version, including alpha/beta/rc tags\nrelease = version\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.imgmath\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinxcontrib.bibtex\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\nsource_suffix = [\'.rst\', \'.md\']\nmaster_doc = \'index\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\nautodoc_default_options = {\'special-members\': \'__call__, __getitem__, __len__\'}\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_logo = \'_static/images/tianshou-logo.png\'\n\n\ndef setup(app):\n    app.add_stylesheet(""css/style.css"")\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\n# intersphinx_mapping = {\'https://docs.python.org/3/\': None}\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\n# todo_include_todos = False\n'"
examples/ant_v2_ddpg.py,5,"b'import gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.policy import DDPGPolicy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\nfrom tianshou.env import VectorEnv, SubprocVectorEnv\n\nfrom continuous_net import Actor, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Ant-v2\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--actor-lr\', type=float, default=1e-4)\n    parser.add_argument(\'--critic-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--tau\', type=float, default=0.005)\n    parser.add_argument(\'--exploration-noise\', type=float, default=0.1)\n    parser.add_argument(\'--epoch\', type=int, default=100)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=2400)\n    parser.add_argument(\'--collect-per-step\', type=int, default=4)\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_ddpg(args=get_args()):\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    args.max_action = env.action_space.high[0]\n    # train_envs = gym.make(args.task)\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = SubprocVectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    actor = Actor(\n        args.layer_num, args.state_shape, args.action_shape,\n        args.max_action, args.device\n    ).to(args.device)\n    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n    critic = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic_optim = torch.optim.Adam(critic.parameters(), lr=args.critic_lr)\n    policy = DDPGPolicy(\n        actor, actor_optim, critic, critic_optim,\n        args.tau, args.gamma, args.exploration_noise,\n        [env.action_space.low[0], env.action_space.high[0]],\n        reward_normalization=True, ignore_done=True)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # log\n    writer = SummaryWriter(args.logdir + \'/\' + \'ddpg\')\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn, writer=writer, task=args.task)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_ddpg()\n'"
examples/ant_v2_sac.py,7,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.policy import SACPolicy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\nfrom tianshou.env import VectorEnv, SubprocVectorEnv\n\nfrom continuous_net import ActorProb, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Ant-v2\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--actor-lr\', type=float, default=3e-4)\n    parser.add_argument(\'--critic-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--tau\', type=float, default=0.005)\n    parser.add_argument(\'--alpha\', type=float, default=0.2)\n    parser.add_argument(\'--epoch\', type=int, default=100)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=2400)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\'--rew-norm\', type=bool, default=True)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_sac(args=get_args()):\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    args.max_action = env.action_space.high[0]\n    # train_envs = gym.make(args.task)\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = SubprocVectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    actor = ActorProb(\n        args.layer_num, args.state_shape, args.action_shape,\n        args.max_action, args.device\n    ).to(args.device)\n    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n    critic1 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n    critic2 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n    policy = SACPolicy(\n        actor, actor_optim, critic1, critic1_optim, critic2, critic2_optim,\n        args.tau, args.gamma, args.alpha,\n        [env.action_space.low[0], env.action_space.high[0]],\n        reward_normalization=args.rew_norm, ignore_done=True)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # train_collector.collect(n_step=args.buffer_size)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'sac\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_sac()\n'"
examples/ant_v2_td3.py,6,"b'import gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.policy import TD3Policy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\nfrom tianshou.env import VectorEnv, SubprocVectorEnv\n\nfrom continuous_net import Actor, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Ant-v2\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--actor-lr\', type=float, default=3e-4)\n    parser.add_argument(\'--critic-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--tau\', type=float, default=0.005)\n    parser.add_argument(\'--exploration-noise\', type=float, default=0.1)\n    parser.add_argument(\'--policy-noise\', type=float, default=0.2)\n    parser.add_argument(\'--noise-clip\', type=float, default=0.5)\n    parser.add_argument(\'--update-actor-freq\', type=int, default=2)\n    parser.add_argument(\'--epoch\', type=int, default=100)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=2400)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_td3(args=get_args()):\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    args.max_action = env.action_space.high[0]\n    # train_envs = gym.make(args.task)\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = SubprocVectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    actor = Actor(\n        args.layer_num, args.state_shape, args.action_shape,\n        args.max_action, args.device\n    ).to(args.device)\n    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n    critic1 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n    critic2 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n    policy = TD3Policy(\n        actor, actor_optim, critic1, critic1_optim, critic2, critic2_optim,\n        args.tau, args.gamma, args.exploration_noise, args.policy_noise,\n        args.update_actor_freq, args.noise_clip,\n        [env.action_space.low[0], env.action_space.high[0]],\n        reward_normalization=True, ignore_done=True)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # train_collector.collect(n_step=args.buffer_size)\n    # log\n    writer = SummaryWriter(args.logdir + \'/\' + \'td3\')\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn, writer=writer, task=args.task)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_td3()\n'"
examples/continuous_net.py,11,"b""import torch\nimport numpy as np\nfrom torch import nn\n\n\nclass Actor(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape,\n                 max_action, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.model = [\n            nn.Linear(np.prod(state_shape), 128),\n            nn.ReLU(inplace=True)]\n        for i in range(layer_num):\n            self.model += [nn.Linear(128, 128), nn.ReLU(inplace=True)]\n        self.model += [nn.Linear(128, np.prod(action_shape))]\n        self.model = nn.Sequential(*self.model)\n        self._max = max_action\n\n    def forward(self, s, **kwargs):\n        s = torch.tensor(s, device=self.device, dtype=torch.float)\n        batch = s.shape[0]\n        s = s.view(batch, -1)\n        logits = self.model(s)\n        logits = self._max * torch.tanh(logits)\n        return logits, None\n\n\nclass ActorProb(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape,\n                 max_action, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.model = [\n            nn.Linear(np.prod(state_shape), 128),\n            nn.ReLU(inplace=True)]\n        for i in range(layer_num):\n            self.model += [nn.Linear(128, 128), nn.ReLU(inplace=True)]\n        self.model = nn.Sequential(*self.model)\n        self.mu = nn.Linear(128, np.prod(action_shape))\n        self.sigma = nn.Linear(128, np.prod(action_shape))\n        self._max = max_action\n\n    def forward(self, s, **kwargs):\n        if not isinstance(s, torch.Tensor):\n            s = torch.tensor(s, device=self.device, dtype=torch.float)\n        batch = s.shape[0]\n        s = s.view(batch, -1)\n        logits = self.model(s)\n        mu = self._max * torch.tanh(self.mu(logits))\n        sigma = torch.exp(self.sigma(logits))\n        return (mu, sigma), None\n\n\nclass Critic(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape=0, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.model = [\n            nn.Linear(np.prod(state_shape) + np.prod(action_shape), 128),\n            nn.ReLU(inplace=True)]\n        for i in range(layer_num):\n            self.model += [nn.Linear(128, 128), nn.ReLU(inplace=True)]\n        self.model += [nn.Linear(128, 1)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, s, a=None):\n        if not isinstance(s, torch.Tensor):\n            s = torch.tensor(s, device=self.device, dtype=torch.float)\n        if a is not None and not isinstance(a, torch.Tensor):\n            a = torch.tensor(a, device=self.device, dtype=torch.float)\n        batch = s.shape[0]\n        s = s.view(batch, -1)\n        if a is None:\n            logits = self.model(s)\n        else:\n            a = a.view(batch, -1)\n            logits = self.model(torch.cat([s, a], dim=1))\n        return logits\n"""
examples/discrete_net.py,5,"b""import torch\nimport numpy as np\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape=0, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.model = [\n            nn.Linear(np.prod(state_shape), 128),\n            nn.ReLU(inplace=True)]\n        for i in range(layer_num):\n            self.model += [nn.Linear(128, 128), nn.ReLU(inplace=True)]\n        if action_shape:\n            self.model += [nn.Linear(128, np.prod(action_shape))]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, s, state=None, info={}):\n        if not isinstance(s, torch.Tensor):\n            s = torch.tensor(s, device=self.device, dtype=torch.float)\n        batch = s.shape[0]\n        s = s.view(batch, -1)\n        logits = self.model(s)\n        return logits, state\n\n\nclass Actor(nn.Module):\n    def __init__(self, preprocess_net, action_shape):\n        super().__init__()\n        self.preprocess = preprocess_net\n        self.last = nn.Linear(128, np.prod(action_shape))\n\n    def forward(self, s, state=None, info={}):\n        logits, h = self.preprocess(s, state)\n        logits = F.softmax(self.last(logits), dim=-1)\n        return logits, h\n\n\nclass Critic(nn.Module):\n    def __init__(self, preprocess_net):\n        super().__init__()\n        self.preprocess = preprocess_net\n        self.last = nn.Linear(128, 1)\n\n    def forward(self, s):\n        logits, h = self.preprocess(s, None)\n        logits = self.last(logits)\n        return logits\n\n\nclass DQN(nn.Module):\n\n    def __init__(self, h, w, action_shape, device='cpu'):\n        super(DQN, self).__init__()\n        self.device = device\n\n        self.conv1 = nn.Conv2d(4, 16, kernel_size=5, stride=2)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n        self.bn3 = nn.BatchNorm2d(32)\n\n        def conv2d_size_out(size, kernel_size=5, stride=2):\n            return (size - (kernel_size - 1) - 1) // stride + 1\n\n        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n        linear_input_size = convw * convh * 32\n        self.fc = nn.Linear(linear_input_size, 512)\n        self.head = nn.Linear(512, action_shape)\n\n    def forward(self, x, state=None, info={}):\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, device=self.device, dtype=torch.float)\n        x = x.permute(0, 3, 1, 2)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.fc(x.reshape(x.size(0), -1))\n        return self.head(x), state\n"""
examples/halfcheetahBullet_v0_sac.py,7,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import SubprocVectorEnv\nfrom tianshou.policy import SACPolicy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\n\ntry:\n    import pybullet_envs\nexcept ImportError:\n    pass\n\nfrom continuous_net import ActorProb, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'HalfCheetahBulletEnv-v0\')\n    parser.add_argument(\'--run-id\', type=str, default=\'test\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--actor-lr\', type=float, default=3e-4)\n    parser.add_argument(\'--critic-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--tau\', type=float, default=0.005)\n    parser.add_argument(\'--alpha\', type=float, default=0.2)\n    parser.add_argument(\'--epoch\', type=int, default=200)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=4)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--log-interval\', type=int, default=100)\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_sac(args=get_args()):\n    torch.set_num_threads(1)\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    args.max_action = env.action_space.high[0]\n    # you can also use tianshou.env.SubprocVectorEnv\n    # train_envs = gym.make(args.task)\n    train_envs = SubprocVectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = SubprocVectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    actor = ActorProb(\n        args.layer_num, args.state_shape, args.action_shape,\n        args.max_action, args.device\n    ).to(args.device)\n    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n    critic1 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n    critic2 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n    policy = SACPolicy(\n        actor, actor_optim, critic1, critic1_optim, critic2, critic2_optim,\n        args.tau, args.gamma, args.alpha,\n        [env.action_space.low[0], env.action_space.high[0]],\n        reward_normalization=True, ignore_done=True)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # train_collector.collect(n_step=args.buffer_size)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'sac\', args.run_id)\n    writer = SummaryWriter(log_path)\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn,\n        writer=writer, log_interval=args.log_interval)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    __all__ = (\'pybullet_envs\',)  # Avoid F401 error :)\n    test_sac()\n'"
examples/point_maze_td3.py,6,"b'import gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.policy import TD3Policy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\nfrom tianshou.env import VectorEnv, SubprocVectorEnv\nfrom continuous_net import Actor, Critic\nfrom mujoco.register import reg\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'PointMaze-v1\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--actor-lr\', type=float, default=3e-5)\n    parser.add_argument(\'--critic-lr\', type=float, default=1e-4)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--tau\', type=float, default=0.005)\n    parser.add_argument(\'--exploration-noise\', type=float, default=0.1)\n    parser.add_argument(\'--policy-noise\', type=float, default=0.2)\n    parser.add_argument(\'--noise-clip\', type=float, default=0.5)\n    parser.add_argument(\'--update-actor-freq\', type=int, default=2)\n    parser.add_argument(\'--epoch\', type=int, default=100)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=2400)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    parser.add_argument(\'--max_episode_steps\', type=int, default=2000)\n\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_td3(args=get_args()):\n    reg()\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    args.max_action = env.action_space.high[0]\n    # train_envs = gym.make(args.task)\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = SubprocVectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    actor = Actor(\n        args.layer_num, args.state_shape, args.action_shape,\n        args.max_action, args.device\n    ).to(args.device)\n    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n    critic1 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n    critic2 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n    policy = TD3Policy(\n        actor, actor_optim, critic1, critic1_optim, critic2, critic2_optim,\n        args.tau, args.gamma, args.exploration_noise, args.policy_noise,\n        args.update_actor_freq, args.noise_clip,\n        [env.action_space.low[0], env.action_space.high[0]],\n        reward_normalization=True, ignore_done=True)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # train_collector.collect(n_step=args.buffer_size)\n    # log\n    writer = SummaryWriter(args.logdir + \'/\' + \'td3\')\n\n    def stop_fn(x):\n        if env.spec.reward_threshold:\n            return x >= env.spec.reward_threshold\n        else:\n            return False\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn, writer=writer, task=args.task)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_step=1000, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_td3()\n'"
examples/pong_a2c.py,5,"b'import torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.policy import A2CPolicy\nfrom tianshou.env import SubprocVectorEnv\nfrom tianshou.trainer import onpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\nfrom tianshou.env.atari import create_atari_environment\n\nfrom discrete_net import Net, Actor, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Pong\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--lr\', type=float, default=3e-4)\n    parser.add_argument(\'--gamma\', type=float, default=0.9)\n    parser.add_argument(\'--epoch\', type=int, default=100)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=100)\n    parser.add_argument(\'--repeat-per-collect\', type=int, default=1)\n    parser.add_argument(\'--batch-size\', type=int, default=64)\n    parser.add_argument(\'--layer-num\', type=int, default=2)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=8)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    # a2c special\n    parser.add_argument(\'--vf-coef\', type=float, default=0.5)\n    parser.add_argument(\'--ent-coef\', type=float, default=0.001)\n    parser.add_argument(\'--max-grad-norm\', type=float, default=None)\n    parser.add_argument(\'--max_episode_steps\', type=int, default=2000)\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_a2c(args=get_args()):\n    env = create_atari_environment(\n        args.task, max_episode_steps=args.max_episode_steps)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.env.action_space.shape or env.env.action_space.n\n    # train_envs = gym.make(args.task)\n    train_envs = SubprocVectorEnv(\n        [lambda: create_atari_environment(\n            args.task, max_episode_steps=args.max_episode_steps)\n            for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = SubprocVectorEnv(\n        [lambda: create_atari_environment(\n            args.task, max_episode_steps=args.max_episode_steps)\n            for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    net = Net(args.layer_num, args.state_shape, device=args.device)\n    actor = Actor(net, args.action_shape).to(args.device)\n    critic = Critic(net).to(args.device)\n    optim = torch.optim.Adam(list(\n        actor.parameters()) + list(critic.parameters()), lr=args.lr)\n    dist = torch.distributions.Categorical\n    policy = A2CPolicy(\n        actor, critic, optim, dist, args.gamma, vf_coef=args.vf_coef,\n        ent_coef=args.ent_coef, max_grad_norm=args.max_grad_norm)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # log\n    writer = SummaryWriter(args.logdir + \'/\' + \'a2c\')\n\n    def stop_fn(x):\n        if env.env.spec.reward_threshold:\n            return x >= env.spec.reward_threshold\n        else:\n            return False\n\n    # trainer\n    result = onpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.repeat_per_collect,\n        args.test_num, args.batch_size, stop_fn=stop_fn, writer=writer,\n        task=args.task)\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = create_atari_environment(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_a2c()\n'"
examples/pong_dqn.py,4,"b'import torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.policy import DQNPolicy\nfrom tianshou.env import SubprocVectorEnv\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\nfrom tianshou.env.atari import create_atari_environment\n\nfrom discrete_net import DQN\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Pong\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--eps-test\', type=float, default=0.05)\n    parser.add_argument(\'--eps-train\', type=float, default=0.1)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.9)\n    parser.add_argument(\'--n-step\', type=int, default=1)\n    parser.add_argument(\'--target-update-freq\', type=int, default=320)\n    parser.add_argument(\'--epoch\', type=int, default=100)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=64)\n    parser.add_argument(\'--layer-num\', type=int, default=3)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=8)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_dqn(args=get_args()):\n    env = create_atari_environment(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.env.action_space.shape or env.env.action_space.n\n    # train_envs = gym.make(args.task)\n    train_envs = SubprocVectorEnv([\n        lambda: create_atari_environment(args.task)\n        for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = SubprocVectorEnv([\n        lambda: create_atari_environment(\n            args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    net = DQN(\n        args.state_shape[0], args.state_shape[1],\n        args.action_shape, args.device)\n    net = net.to(args.device)\n    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n    policy = DQNPolicy(\n        net, optim, args.gamma, args.n_step,\n        use_target_network=args.target_update_freq > 0,\n        target_update_freq=args.target_update_freq)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # policy.set_eps(1)\n    train_collector.collect(n_step=args.batch_size * 4)\n    print(len(train_collector.buffer))\n    # log\n    writer = SummaryWriter(args.logdir + \'/\' + \'dqn\')\n\n    def stop_fn(x):\n        if env.env.spec.reward_threshold:\n            return x >= env.spec.reward_threshold\n        else:\n            return False\n\n    def train_fn(x):\n        policy.set_eps(args.eps_train)\n\n    def test_fn(x):\n        policy.set_eps(args.eps_test)\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, train_fn=train_fn, test_fn=test_fn,\n        stop_fn=stop_fn, writer=writer, task=args.task)\n\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = create_atari_environment(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_dqn(get_args())\n'"
examples/pong_ppo.py,5,"b'import torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.policy import PPOPolicy\nfrom tianshou.env import SubprocVectorEnv\nfrom tianshou.trainer import onpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\nfrom tianshou.env.atari import create_atari_environment\n\nfrom discrete_net import Net, Actor, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Pong\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--epoch\', type=int, default=100)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=100)\n    parser.add_argument(\'--repeat-per-collect\', type=int, default=2)\n    parser.add_argument(\'--batch-size\', type=int, default=64)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=8)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    # ppo special\n    parser.add_argument(\'--vf-coef\', type=float, default=0.5)\n    parser.add_argument(\'--ent-coef\', type=float, default=0.0)\n    parser.add_argument(\'--eps-clip\', type=float, default=0.2)\n    parser.add_argument(\'--max-grad-norm\', type=float, default=0.5)\n    parser.add_argument(\'--max_episode_steps\', type=int, default=2000)\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_ppo(args=get_args()):\n    env = create_atari_environment(\n        args.task, max_episode_steps=args.max_episode_steps)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space().shape or env.action_space().n\n    # train_envs = gym.make(args.task)\n    train_envs = SubprocVectorEnv([lambda: create_atari_environment(\n        args.task, max_episode_steps=args.max_episode_steps)\n        for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = SubprocVectorEnv([lambda: create_atari_environment(\n        args.task, max_episode_steps=args.max_episode_steps)\n        for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    net = Net(args.layer_num, args.state_shape, device=args.device)\n    actor = Actor(net, args.action_shape).to(args.device)\n    critic = Critic(net).to(args.device)\n    optim = torch.optim.Adam(list(\n        actor.parameters()) + list(critic.parameters()), lr=args.lr)\n    dist = torch.distributions.Categorical\n    policy = PPOPolicy(\n        actor, critic, optim, dist, args.gamma,\n        max_grad_norm=args.max_grad_norm,\n        eps_clip=args.eps_clip,\n        vf_coef=args.vf_coef,\n        ent_coef=args.ent_coef,\n        action_range=None)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # log\n    writer = SummaryWriter(args.logdir + \'/\' + \'ppo\')\n\n    def stop_fn(x):\n        if env.env.spec.reward_threshold:\n            return x >= env.spec.reward_threshold\n        else:\n            return False\n\n    # trainer\n    result = onpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.repeat_per_collect,\n        args.test_num, args.batch_size, stop_fn=stop_fn, writer=writer,\n        task=args.task)\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = create_atari_environment(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_step=2000, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_ppo()\n'"
test/__init__.py,0,b''
tianshou/__init__.py,0,"b""from tianshou import data, env, utils, policy, trainer, \\\n    exploration\n\n__version__ = '0.2.3'\n__all__ = [\n    'env',\n    'data',\n    'utils',\n    'policy',\n    'trainer',\n    'exploration',\n]\n"""
examples/mujoco/__init__.py,0,b''
examples/mujoco/maze_env_utils.py,0,"b'""""""Adapted from rllab maze_env_utils.py.""""""\nimport math\n\n\nclass Move(object):\n    X = 11\n    Y = 12\n    Z = 13\n    XY = 14\n    XZ = 15\n    YZ = 16\n    XYZ = 17\n    SpinXY = 18\n\n\ndef can_move_x(movable):\n    return movable in [Move.X, Move.XY, Move.XZ, Move.XYZ,\n                       Move.SpinXY]\n\n\ndef can_move_y(movable):\n    return movable in [Move.Y, Move.XY, Move.YZ, Move.XYZ,\n                       Move.SpinXY]\n\n\ndef can_move_z(movable):\n    return movable in [Move.Z, Move.XZ, Move.YZ, Move.XYZ]\n\n\ndef can_spin(movable):\n    return movable in [Move.SpinXY]\n\n\ndef can_move(movable):\n    return can_move_x(movable) or can_move_y(movable) or can_move_z(movable)\n\n\ndef construct_maze(maze_id=\'Maze\'):\n    if maze_id == \'Maze\':\n        structure = [\n            [1, 1, 1, 1, 1],\n            [1, \'r\', 0, 0, 1],\n            [1, 1, 1, 0, 1],\n            [1, \'g\', 0, 0, 1],\n            [1, 1, 1, 1, 1],\n        ]\n    elif maze_id == \'Maze1\':\n        structure = [\n            [1, 1, 1, 1, 1, 1, 1, 1],\n            [1, \'r\', 1, 0, 0, 0, 0, 1],\n            [1, 0, 0, 0, 1, 1, 0, 1],\n            [1, 1, 1, 1, 1, 0, 0, 1],\n            [1, 0, 0, 0, 1, 0, 1, 1],\n            [1, 0, 0, 0, 1, 0, 1, 1],\n            [1, 0, 1, 0, 0, 0, 0, 1],\n            [1, 1, 1, 1, 1, 1, 1, 1],\n        ]\n    elif maze_id == \'Maze2\':\n        structure = [\n            [0, 0, 0, 0, 0],\n            [0, \'r\', 0, 0, 0],\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0],\n        ]\n    # transfer maze\n    elif maze_id == \'Maze3\':\n        structure = [\n            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n            [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n            [1, 0, \'r\', 0, 0, 1, 0, 0, 0, 0, 1],\n            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n            [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n            [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n            [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1],\n            [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1],\n            [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n            [1, 0, 0, 0, 0, 1, 0, 0, 0, \'g\', 1],\n            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        ]\n    elif maze_id == \'Maze4\':\n        structure = [\n            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n            [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n            [1, 0, \'r\', 0, 0, 1, 0, 0, 0, 0, 1],\n            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n            [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n            [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n            [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n            [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n            [1, 0, 0, 0, 0, 1, 0, 0, 0, \'g\', 1],\n            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        ]\n    elif maze_id == \'Push\':\n        structure = [\n            [1, 1, 1, 1, 1],\n            [1, 0, \'r\', 1, 1],\n            [1, 0, Move.XY, 0, 1],\n            [1, 1, 0, 1, 1],\n            [1, 1, 1, 1, 1],\n        ]\n    elif maze_id == \'Fall\':\n        structure = [\n            [1, 1, 1, 1],\n            [1, \'r\', 0, 1],\n            [1, 0, Move.YZ, 1],\n            [1, -1, -1, 1],\n            [1, 0, 0, 1],\n            [1, 1, 1, 1],\n        ]\n    elif maze_id == \'Block\':\n        structure = [\n            [1, 1, 1, 1, 1],\n            [1, \'r\', 0, 0, 1],\n            [1, 0, 0, 0, 1],\n            [1, 0, 0, 0, 1],\n            [1, 1, 1, 1, 1],\n        ]\n    elif maze_id == \'BlockMaze\':\n        structure = [\n            [1, 1, 1, 1],\n            [1, \'r\', 0, 1],\n            [1, 1, 0, 1],\n            [1, 0, 0, 1],\n            [1, 1, 1, 1],\n        ]\n    else:\n        raise NotImplementedError(\n            \'The provided MazeId %s is not recognized\' % maze_id)\n\n    return structure\n\n\ndef line_intersect(pt1, pt2, ptA, ptB):\n    """"""\n    Taken from https://www.cs.hmc.edu/ACM/lectures/intersections.html\n    this returns the intersection of Line(pt1,pt2) and Line(ptA,ptB)\n    """"""\n\n    DET_TOLERANCE = 0.00000001\n\n    # the first line is pt1 + r*(pt2-pt1)\n    # in component form:\n    x1, y1 = pt1\n    x2, y2 = pt2\n    dx1 = x2 - x1\n    dy1 = y2 - y1\n\n    # the second line is ptA + s*(ptB-ptA)\n    x, y = ptA\n    xB, yB = ptB\n    dx = xB - x\n    dy = yB - y\n\n    DET = (-dx1 * dy + dy1 * dx)\n\n    if math.fabs(DET) < DET_TOLERANCE:\n        return (0, 0, 0, 0, 0)\n\n    # now, the determinant should be OK\n    DETinv = 1.0 / DET\n\n    # find the scalar amount along the ""self"" segment\n    r = DETinv * (-dy * (x - x1) + dx * (y - y1))\n\n    # find the scalar amount along the input line\n    s = DETinv * (-dy1 * (x - x1) + dx1 * (y - y1))\n\n    # return the average of the two descriptions\n    xi = (x1 + r * dx1 + x + s * dx) / 2.0\n    yi = (y1 + r * dy1 + y + s * dy) / 2.0\n    return (xi, yi, 1, r, s)\n\n\ndef ray_segment_intersect(ray, segment):\n    """"""\n    Check if the ray originated from (x, y) with direction theta\n    intersects the line segment (x1, y1) -- (x2, y2), and return\n    the intersection point if there is one\n    """"""\n    (x, y), theta = ray\n    # (x1, y1), (x2, y2) = segment\n    pt1 = (x, y)\n    len = 1\n    pt2 = (x + len * math.cos(theta), y + len * math.sin(theta))\n    xo, yo, valid, r, s = line_intersect(pt1, pt2, *segment)\n    if valid and r >= 0 and 0 <= s <= 1:\n        return (xo, yo)\n    return None\n\n\ndef point_distance(p1, p2):\n    x1, y1 = p1\n    x2, y2 = p2\n    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n'"
examples/mujoco/point.py,0,"b'""""""Wrapper for creating the ant environment in gym_mujoco.""""""\n\nimport math\nimport numpy as np\nfrom gym import utils\nfrom gym.envs.mujoco import mujoco_env\n\n\nclass PointEnv(mujoco_env.MujocoEnv, utils.EzPickle):\n    FILE = ""point.xml""\n    ORI_IND = 2\n\n    def __init__(self, file_path=None, expose_all_qpos=True, noisy_init=False):\n        self._expose_all_qpos = expose_all_qpos\n        self.noisy_init = noisy_init\n        mujoco_env.MujocoEnv.__init__(self, file_path, 1)\n        utils.EzPickle.__init__(self)\n\n    @property\n    def physics(self):\n        return self.model\n\n    def _step(self, a):\n        return self.step(a)\n\n    def step(self, action):\n        # action[0] is velocity, action[1] is direction\n        action[0] = 0.2 * action[0]\n        qpos = np.copy(self.data.qpos)\n        qpos[2] += action[1]\n        ori = qpos[2]\n        # compute increment in each direction\n        dx = math.cos(ori) * action[0]\n        dy = math.sin(ori) * action[0]\n        # ensure that the robot is within reasonable range\n        qpos[0] = np.clip(qpos[0] + dx, -100, 100)\n        qpos[1] = np.clip(qpos[1] + dy, -100, 100)\n        qvel = np.squeeze(self.data.qvel)\n        self.set_state(qpos, qvel)\n        for _ in range(0, self.frame_skip):\n            # self.physics.step()\n            self.sim.step()\n        next_obs = self._get_obs()\n        reward = 0\n        done = False\n        info = {}\n        return next_obs, reward, done, info\n\n    def _get_obs(self):\n        if self._expose_all_qpos:\n            return np.concatenate([\n                self.data.qpos.flat[:3],  # Only point-relevant coords.\n                self.data.qvel.flat[:3]])\n        return np.concatenate([\n            self.data.qpos.flat[2:3],\n            self.data.qvel.flat[:3]])\n\n    def reset_model(self):\n        if self.noisy_init:\n            qpos = self.init_qpos + self.np_random.uniform(\n                size=self.model.nq, low=-.1, high=.1)\n            qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1\n\n        else:\n            qpos = self.init_qpos\n            qvel = self.init_qvel\n\n        # Set everything other than point to original position and 0 velocity.\n        qpos[3:] = self.init_qpos[3:]\n        qvel[3:] = 0.\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    def get_ori(self):\n        return self.data.qpos[self.__class__.ORI_IND]\n\n    def set_xy(self, xy):\n        qpos = np.copy(self.data.qpos)\n        qpos[0] = xy[0]\n        qpos[1] = xy[1]\n\n        qvel = self.data.qvel\n        self.set_state(qpos, qvel)\n\n    def get_xy(self):\n        qpos = np.copy(self.data.qpos)\n        return qpos[:2]\n\n    def viewer_setup(self):\n\n        self.viewer.cam.trackbodyid = -1\n        self.viewer.cam.distance = 80\n        self.viewer.cam.elevation = -90\n'"
examples/mujoco/point_maze_env.py,0,"b'""""""Adapted from rllab maze_env.py.""""""\n\nimport os\nimport tempfile\nimport xml.etree.ElementTree as ET\nimport math\nimport numpy as np\nimport gym\nfrom . import maze_env_utils\nfrom .point import PointEnv\nfrom gym.utils import seeding\n\n# Directory that contains mujoco xml files.\nMODEL_DIR = os.path.join(os.path.dirname(__file__), \'assets\')\n\n\nclass PointMazeEnv(gym.Env):\n    MODEL_CLASS = PointEnv\n\n    MAZE_HEIGHT = None\n    MAZE_SIZE_SCALING = None\n\n    def __init__(\n            self,\n            maze_id=None,\n            maze_height=0.5,\n            maze_size_scaling=8,\n            n_bins=0,\n            sensor_range=3.,\n            sensor_span=2 * math.pi,\n            observe_blocks=False,\n            put_spin_near_agent=False,\n            top_down_view=False,\n            manual_collision=False,\n            goal=None,\n            EPS=0.25,\n            max_episode_steps=2000,\n            *args,\n            **kwargs):\n        self._maze_id = maze_id\n\n        model_cls = self.__class__.MODEL_CLASS\n        if model_cls is None:\n            raise ""MODEL_CLASS unspecified!""\n        xml_path = os.path.join(MODEL_DIR, model_cls.FILE)\n        self.tree = tree = ET.parse(xml_path)\n        self.worldbody = worldbody = tree.find("".//worldbody"")\n        self.visualize_goal = False\n        self.max_episode_steps = max_episode_steps\n        self.t = 0\n        self.MAZE_HEIGHT = height = maze_height\n        self.MAZE_SIZE_SCALING = size_scaling = maze_size_scaling\n        self._n_bins = n_bins\n        self._sensor_range = sensor_range * size_scaling\n        self._sensor_span = sensor_span\n        self._observe_blocks = observe_blocks\n        self._put_spin_near_agent = put_spin_near_agent\n        self._top_down_view = top_down_view\n        self._manual_collision = manual_collision\n\n        self.MAZE_STRUCTURE = structure = maze_env_utils.construct_maze(\n            maze_id=self._maze_id)\n        # Elevate the maze to allow for falling.\n        self.elevated = any(-1 in row for row in structure)\n        self.blocks = any(\n            any(maze_env_utils.can_move(r) for r in row)\n            for row in structure)  # Are there any movable blocks?\n\n        torso_x, torso_y = self._find_robot()  # x, y coordinates\n        self._init_torso_x = torso_x\n        self._init_torso_y = torso_y\n        self._init_positions = [\n            (x - torso_x, y - torso_y)\n            for x, y in self._find_all_robots()]\n\n        self._view = np.zeros([5, 5, 3])\n\n        height_offset = 0.\n        if self.elevated:\n            height_offset = height * size_scaling\n            torso = tree.find("".//body[@name=\'torso\']"")\n            torso.set(\'pos\', \'0 0 %.2f\' % (0.75 + height_offset))\n        if self.blocks:\n            default = tree.find("".//default"")\n            default.find(\'.//geom\').set(\'solimp\', \'.995 .995 .01\')\n\n        self.movable_blocks = []\n        for i in range(len(structure)):\n            for j in range(len(structure[0])):\n                struct = structure[i][j]\n                if struct == \'r\' and self._put_spin_near_agent:\n                    struct = maze_env_utils.Move.SpinXY\n                if self.elevated and struct not in [-1]:\n                    # Create elevated platform.\n                    ET.SubElement(\n                        worldbody, ""geom"",\n                        name=""elevated_%d_%d"" % (i, j),\n                        pos=""%f %f %f"" % (j * size_scaling - torso_x,\n                                          i * size_scaling - torso_y,\n                                          height / 2 * size_scaling),\n                        size=""%f %f %f"" % (0.5 * size_scaling,\n                                           0.5 * size_scaling,\n                                           height / 2 * size_scaling),\n                        type=""box"",\n                        material="""",\n                        contype=""1"",\n                        conaffinity=""1"",\n                        rgba=""0.9 0.9 0.9 1"",\n                    )\n                if struct == 1:  # Unmovable block.\n                    # Offset all coordinates so that robot starts at the origin\n                    ET.SubElement(\n                        worldbody, ""geom"",\n                        name=""block_%d_%d"" % (i, j),\n                        pos=""%f %f %f"" % (j * size_scaling - torso_x,\n                                          i * size_scaling - torso_y,\n                                          height_offset +\n                                          height / 2 * size_scaling),\n                        size=""%f %f %f"" % (0.5 * size_scaling,\n                                           0.5 * size_scaling,\n                                           height / 2 * size_scaling),\n                        type=""box"",\n                        material="""",\n                        contype=""1"",\n                        conaffinity=""1"",\n                        rgba=""0.4 0.4 0.4 1"",\n                    )\n                elif maze_env_utils.can_move(struct):\n                    name = ""movable_%d_%d"" % (i, j)\n                    self.movable_blocks.append((name, struct))\n                    falling = maze_env_utils.can_move_z(struct)\n                    spinning = maze_env_utils.can_spin(struct)\n                    x_offset = 0.25 * size_scaling if spinning else 0.0\n                    y_offset = 0.0\n                    shrink = 0.1 if spinning else 0.99 if falling else 1.0\n                    height_shrink = 0.1 if spinning else 1.0\n                    _x = j * size_scaling - torso_x + x_offset\n                    _y = i * size_scaling - torso_y + y_offset\n                    _z = height / 2 * size_scaling * height_shrink\n                    movable_body = ET.SubElement(\n                        worldbody, ""body"",\n                        name=name,\n                        pos=""%f %f %f"" % (_x, _y, height_offset + _z),\n                    )\n                    ET.SubElement(\n                        movable_body, ""geom"",\n                        name=""block_%d_%d"" % (i, j),\n                        pos=""0 0 0"",\n                        size=""%f %f %f"" % (0.5 * size_scaling * shrink,\n                                           0.5 * size_scaling * shrink,\n                                           _z),\n                        type=""box"",\n                        material="""",\n                        mass=""0.001"" if falling else ""0.0002"",\n                        contype=""1"",\n                        conaffinity=""1"",\n                        rgba=""0.9 0.1 0.1 1""\n                    )\n                    if maze_env_utils.can_move_x(struct):\n                        ET.SubElement(\n                            movable_body, ""joint"",\n                            armature=""0"",\n                            axis=""1 0 0"",\n                            damping=""0.0"",\n                            limited=""true"" if falling else ""false"",\n                            range=""%f %f"" % (-size_scaling, size_scaling),\n                            margin=""0.01"",\n                            name=""movable_x_%d_%d"" % (i, j),\n                            pos=""0 0 0"",\n                            type=""slide""\n                        )\n                    if maze_env_utils.can_move_y(struct):\n                        ET.SubElement(\n                            movable_body, ""joint"",\n                            armature=""0"",\n                            axis=""0 1 0"",\n                            damping=""0.0"",\n                            limited=""true"" if falling else ""false"",\n                            range=""%f %f"" % (-size_scaling, size_scaling),\n                            margin=""0.01"",\n                            name=""movable_y_%d_%d"" % (i, j),\n                            pos=""0 0 0"",\n                            type=""slide""\n                        )\n                    if maze_env_utils.can_move_z(struct):\n                        ET.SubElement(\n                            movable_body, ""joint"",\n                            armature=""0"",\n                            axis=""0 0 1"",\n                            damping=""0.0"",\n                            limited=""true"",\n                            range=""%f 0"" % (-height_offset),\n                            margin=""0.01"",\n                            name=""movable_z_%d_%d"" % (i, j),\n                            pos=""0 0 0"",\n                            type=""slide""\n                        )\n                    if maze_env_utils.can_spin(struct):\n                        ET.SubElement(\n                            movable_body, ""joint"",\n                            armature=""0"",\n                            axis=""0 0 1"",\n                            damping=""0.0"",\n                            limited=""false"",\n                            name=""spinable_%d_%d"" % (i, j),\n                            pos=""0 0 0"",\n                            type=""ball""\n                        )\n\n        torso = tree.find("".//body[@name=\'torso\']"")\n        geoms = torso.findall("".//geom"")\n        for geom in geoms:\n            if \'name\' not in geom.attrib:\n                raise Exception(""Every geom of the torso must have a name ""\n                                ""defined"")\n\n        _, file_path = tempfile.mkstemp(text=True, suffix=\'.xml\')\n        tree.write(file_path)\n\n        self.wrapped_env = model_cls(*args, file_path=file_path, **kwargs)\n        self.args = args\n        self.kwargs = kwargs\n        self.GOAL = goal\n        if self.GOAL is not None:\n            self.GOAL = self.unwrapped._rowcol_to_xy(*self.GOAL)\n        self.EPS = EPS\n\n    def get_ori(self):\n        return self.wrapped_env.get_ori()\n\n    def get_top_down_view(self):\n        self._view = np.zeros_like(self._view)\n\n        def valid(row, col):\n            return self._view.shape[0] > row >= 0 \\\n                   and self._view.shape[1] > col >= 0\n\n        def update_view(x, y, d, row=None, col=None):\n            if row is None or col is None:\n                x = x - self._robot_x\n                y = y - self._robot_y\n\n                row, col = self._xy_to_rowcol(x, y)\n                update_view(x, y, d, row=row, col=col)\n                return\n\n            row, row_frac, col, col_frac = int(row), row % 1, int(col), col % 1\n            if row_frac < 0:\n                row_frac += 1\n            if col_frac < 0:\n                col_frac += 1\n\n            if valid(row, col):\n                self._view[row, col, d] += (\n                        (min(1., row_frac + 0.5) - max(0., row_frac - 0.5)) *\n                        (min(1., col_frac + 0.5) - max(0., col_frac - 0.5)))\n            if valid(row - 1, col):\n                self._view[row - 1, col, d] += (\n                        (max(0., 0.5 - row_frac)) *\n                        (min(1., col_frac + 0.5) - max(0., col_frac - 0.5)))\n            if valid(row + 1, col):\n                self._view[row + 1, col, d] += (\n                        (max(0., row_frac - 0.5)) *\n                        (min(1., col_frac + 0.5) - max(0., col_frac - 0.5)))\n            if valid(row, col - 1):\n                self._view[row, col - 1, d] += (\n                        (min(1., row_frac + 0.5) - max(0., row_frac - 0.5)) *\n                        (max(0., 0.5 - col_frac)))\n            if valid(row, col + 1):\n                self._view[row, col + 1, d] += (\n                        (min(1., row_frac + 0.5) - max(0., row_frac - 0.5)) *\n                        (max(0., col_frac - 0.5)))\n            if valid(row - 1, col - 1):\n                self._view[row - 1, col - 1, d] += (\n                        (max(0., 0.5 - row_frac)) * max(0., 0.5 - col_frac))\n            if valid(row - 1, col + 1):\n                self._view[row - 1, col + 1, d] += (\n                        (max(0., 0.5 - row_frac)) * max(0., col_frac - 0.5))\n            if valid(row + 1, col + 1):\n                self._view[row + 1, col + 1, d] += (\n                        (max(0., row_frac - 0.5)) * max(0., col_frac - 0.5))\n            if valid(row + 1, col - 1):\n                self._view[row + 1, col - 1, d] += (\n                        (max(0., row_frac - 0.5)) * max(0., 0.5 - col_frac))\n\n        # Draw ant.\n        robot_x, robot_y = self.wrapped_env.get_body_com(""torso"")[:2]\n        self._robot_x = robot_x\n        self._robot_y = robot_y\n        self._robot_ori = self.get_ori()\n\n        structure = self.MAZE_STRUCTURE\n        size_scaling = self.MAZE_SIZE_SCALING\n\n        # Draw immovable blocks and chasms.\n        for i in range(len(structure)):\n            for j in range(len(structure[0])):\n                if structure[i][j] == 1:  # Wall.\n                    update_view(j * size_scaling - self._init_torso_x,\n                                i * size_scaling - self._init_torso_y,\n                                0)\n                if structure[i][j] == -1:  # Chasm.\n                    update_view(j * size_scaling - self._init_torso_x,\n                                i * size_scaling - self._init_torso_y,\n                                1)\n\n        # Draw movable blocks.\n        for block_name, block_type in self.movable_blocks:\n            block_x, block_y = self.wrapped_env.get_body_com(block_name)[:2]\n            update_view(block_x, block_y, 2)\n\n        import cv2\n        cv2.imshow(\'x.jpg\', cv2.resize(\n            np.uint8(self._view * 255), (512, 512),\n            interpolation=cv2.INTER_CUBIC))\n        cv2.waitKey(0)\n\n        return self._view\n\n    def get_range_sensor_obs(self):\n        """"""Returns egocentric range sensor observations of maze.""""""\n        robot_x, robot_y, robot_z = self.wrapped_env.get_body_com(""torso"")[:3]\n        ori = self.get_ori()\n\n        structure = self.MAZE_STRUCTURE\n        size_scaling = self.MAZE_SIZE_SCALING\n        height = self.MAZE_HEIGHT\n\n        segments = []\n        # Get line segments (corresponding to outer boundary) of each immovable\n        # block or drop-off.\n        for i in range(len(structure)):\n            for j in range(len(structure[0])):\n                if structure[i][j] in [1, -1]:  # There\'s a wall or drop-off.\n                    cx = j * size_scaling - self._init_torso_x\n                    cy = i * size_scaling - self._init_torso_y\n                    x1 = cx - 0.5 * size_scaling\n                    x2 = cx + 0.5 * size_scaling\n                    y1 = cy - 0.5 * size_scaling\n                    y2 = cy + 0.5 * size_scaling\n                    struct_segments = [\n                        ((x1, y1), (x2, y1)),\n                        ((x2, y1), (x2, y2)),\n                        ((x2, y2), (x1, y2)),\n                        ((x1, y2), (x1, y1)),\n                    ]\n                    for seg in struct_segments:\n                        segments.append(dict(\n                            segment=seg,\n                            type=structure[i][j],\n                        ))\n\n        for block_name, block_type in self.movable_blocks:\n            block_x, block_y, block_z = \\\n                self.wrapped_env.get_body_com(block_name)[:3]\n            if (block_z + height * size_scaling / 2 >= robot_z and\n                    robot_z >= block_z - height * size_scaling / 2):\n                # Block in view.\n                x1 = block_x - 0.5 * size_scaling\n                x2 = block_x + 0.5 * size_scaling\n                y1 = block_y - 0.5 * size_scaling\n                y2 = block_y + 0.5 * size_scaling\n                struct_segments = [\n                    ((x1, y1), (x2, y1)),\n                    ((x2, y1), (x2, y2)),\n                    ((x2, y2), (x1, y2)),\n                    ((x1, y2), (x1, y1)),\n                ]\n                for seg in struct_segments:\n                    segments.append(dict(\n                        segment=seg,\n                        type=block_type,\n                    ))\n\n        # 3 for wall, drop-off, block\n        sensor_readings = np.zeros((self._n_bins, 3))\n        for ray_idx in range(self._n_bins):\n            ray_ori = (ori - self._sensor_span * 0.5 + (\n                    2 * ray_idx + 1.0) /\n                       (2 * self._n_bins) * self._sensor_span)\n            ray_segments = []\n            # Get all segments that intersect with ray.\n            for seg in segments:\n                p = maze_env_utils.ray_segment_intersect(\n                    ray=((robot_x, robot_y), ray_ori),\n                    segment=seg[""segment""])\n                if p is not None:\n                    ray_segments.append(dict(\n                        segment=seg[""segment""],\n                        type=seg[""type""],\n                        ray_ori=ray_ori,\n                        distance=maze_env_utils.point_distance(\n                            p, (robot_x, robot_y)),\n                    ))\n            if len(ray_segments) > 0:\n                # Find out which segment is intersected first.\n                first_seg = sorted(\n                    ray_segments, key=lambda x: x[""distance""])[0]\n                seg_type = first_seg[""type""]\n                idx = (0 if seg_type == 1 else  # Wall.\n                       1 if seg_type == -1 else  # Drop-off.\n                       2 if maze_env_utils.can_move(seg_type) else  # Block.\n                       None)\n                if first_seg[""distance""] <= self._sensor_range:\n                    sensor_readings[ray_idx][idx] = \\\n                        (self._sensor_range - first_seg[\n                            ""distance""]) / self._sensor_range\n        return sensor_readings\n\n    def _get_obs(self):\n        wrapped_obs = self.wrapped_env._get_obs()\n        if self._top_down_view:\n            self.get_top_down_view()\n\n        if self._observe_blocks:\n            additional_obs = []\n            for block_name, block_type in self.movable_blocks:\n                additional_obs.append(\n                    self.wrapped_env.get_body_com(block_name))\n            wrapped_obs = np.concatenate([wrapped_obs[:3]] + additional_obs +\n                                         [wrapped_obs[3:]])\n\n        self.get_range_sensor_obs()\n        return wrapped_obs\n\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def reset(self, goal=None):\n        self.goal = goal\n\n        if self.visualize_goal:  # remove the prev goal and add a new goal\n            goal_x, goal_y = goal[0], goal[1]\n            size_scaling = self.MAZE_SIZE_SCALING\n            # remove the original goal\n            try:\n                self.worldbody.remove(self.goal_element)\n            except AttributeError:\n                pass\n            # offset all coordinates so that robot starts at the origin\n            self.goal_element = \\\n                ET.SubElement(\n                    self.worldbody, ""geom"",\n                    name=""goal_%d_%d"" % (goal_x, goal_y),\n                    pos=""%f %f %f"" % (goal_x,\n                                      goal_y,\n                                      self.MAZE_HEIGHT / 2 * size_scaling),\n                    # smaller than the block to prevent collision\n                    size=""%f %f %f"" % (0.1 * size_scaling,\n                                       0.1 * size_scaling,\n                                       self.MAZE_HEIGHT / 2 * size_scaling),\n                    type=""box"",\n                    material="""",\n                    contype=""1"",\n                    conaffinity=""1"",\n                    rgba=""1.0 0.0 0.0 0.5""\n                )\n            # Note: running the lines below will make the robot position wrong!\n            # (because the graph is rebuilt)\n            torso = self.tree.find("".//body[@name=\'torso\']"")\n            geoms = torso.findall("".//geom"")\n            for geom in geoms:\n                if \'name\' not in geom.attrib:\n                    raise Exception(""Every geom of the torso must have a name ""\n                                    ""defined"")\n            _, file_path = tempfile.mkstemp(text=True, suffix=\'.xml\')\n            self.tree.write(file_path)\n            # here we write a temporal file with the robot specifications.\n            # Why not the original one??\n\n            model_cls = self.__class__.MODEL_CLASS\n            # file to the robot specifications; model_cls is AntEnv\n            self.wrapped_env = model_cls(\n                *self.args, file_path=file_path, **self.kwargs)\n\n        self.t = 0\n        self.trajectory = []\n        self.wrapped_env.reset()\n        if len(self._init_positions) > 1:\n            xy = self._init_positions[self.np_random.randint(\n                len(self._init_positions))]\n            self.wrapped_env.set_xy(xy)\n        return self._get_obs()\n\n    @property\n    def viewer(self):\n        return self.wrapped_env.viewer\n\n    def render(self, *args, **kwargs):\n        return self.wrapped_env.render(*args, **kwargs)\n\n    @property\n    def observation_space(self):\n        shape = self._get_obs().shape\n        high = np.inf * np.ones(shape)\n        low = -high\n        return gym.spaces.Box(low, high)\n\n    @property\n    def action_space(self):\n        return self.wrapped_env.action_space\n\n    def _find_robot(self):\n        structure = self.MAZE_STRUCTURE\n        size_scaling = self.MAZE_SIZE_SCALING\n        for i in range(len(structure)):\n            for j in range(len(structure[0])):\n                if structure[i][j] == \'r\':\n                    return j * size_scaling, i * size_scaling\n        assert False, \'No robot in maze specification.\'\n\n    def _find_all_robots(self):\n        structure = self.MAZE_STRUCTURE\n        size_scaling = self.MAZE_SIZE_SCALING\n        coords = []\n        for i in range(len(structure)):\n            for j in range(len(structure[0])):\n                if structure[i][j] == \'r\':\n                    coords.append((j * size_scaling, i * size_scaling))\n        return coords\n\n    def _is_in_collision(self, pos):\n        x, y = pos\n        structure = self.MAZE_STRUCTURE\n        scale = self.MAZE_SIZE_SCALING\n        for i in range(len(structure)):\n            for j in range(len(structure[0])):\n                if structure[i][j] == 1:\n                    minx = j * scale - scale * 0.5 - self._init_torso_x\n                    maxx = j * scale + scale * 0.5 - self._init_torso_x\n                    miny = i * scale - scale * 0.5 - self._init_torso_y\n                    maxy = i * scale + scale * 0.5 - self._init_torso_y\n                    if minx <= x <= maxx and miny <= y <= maxy:\n                        return True\n        return False\n\n    def _rowcol_to_xy(self, j, i):\n        scale = self.MAZE_SIZE_SCALING\n        minx = j * scale - scale * 0.5 - self._init_torso_x\n        maxx = j * scale + scale * 0.5 - self._init_torso_x\n        miny = i * scale - scale * 0.5 - self._init_torso_y\n        maxy = i * scale + scale * 0.5 - self._init_torso_y\n        return (minx + maxx) / 2, (miny + maxy) / 2\n\n    def step(self, action):\n        self.t += 1\n        if self._manual_collision:\n            old_pos = self.wrapped_env.get_xy()\n            inner_next_obs, inner_reward, inner_done, info = \\\n                self.wrapped_env.step(action)\n            new_pos = self.wrapped_env.get_xy()\n            if self._is_in_collision(new_pos):\n                self.wrapped_env.set_xy(old_pos)\n        else:\n            inner_next_obs, inner_reward, inner_done, info = \\\n                self.wrapped_env.step(action)\n        next_obs = self._get_obs()\n        done = False\n        if self.goal is not None:\n            done = bool(((next_obs[:2] - self.goal[:2]) ** 2).sum() < self.EPS)\n\n        new_pos = self.wrapped_env.get_xy()\n        if self._is_in_collision(new_pos) or inner_done:\n            done = True\n        if self.t >= self.max_episode_steps:\n            done = True\n        return next_obs, inner_reward, done, info\n'"
examples/mujoco/register.py,0,"b'from gym.envs.registration import register\n\n\ndef reg():\n    register(\n        id=\'PointMaze-v0\',\n        entry_point=\'mujoco.point_maze_env:PointMazeEnv\',\n        kwargs={\n            ""maze_size_scaling"": 4,\n            ""maze_id"": ""Maze2"",\n            ""maze_height"": 0.5,\n            ""manual_collision"": True,\n            ""goal"": (1, 3),\n        }\n    )\n\n    register(\n        id=\'PointMaze-v1\',\n        entry_point=\'mujoco.point_maze_env:PointMazeEnv\',\n        kwargs={\n            ""maze_size_scaling"": 2,\n            ""maze_id"": ""Maze2"",\n            ""maze_height"": 0.5,\n            ""manual_collision"": True,\n            ""goal"": (1, 3),\n        }\n    )\n'"
test/base/__init__.py,0,b''
test/base/env.py,0,"b""import time\nimport gym\nfrom gym.spaces.discrete import Discrete\n\n\nclass MyTestEnv(gym.Env):\n    def __init__(self, size, sleep=0, dict_state=False):\n        self.size = size\n        self.sleep = sleep\n        self.dict_state = dict_state\n        self.action_space = Discrete(1)\n        self.reset()\n\n    def reset(self, state=0):\n        self.done = False\n        self.index = state\n        return {'index': self.index} if self.dict_state else self.index\n\n    def step(self, action):\n        if self.done:\n            raise ValueError('step after done !!!')\n        if self.sleep > 0:\n            time.sleep(self.sleep)\n        if self.index == self.size:\n            self.done = True\n            if self.dict_state:\n                return {'index': self.index}, 0, True, {}\n            else:\n                return self.index, 0, True, {}\n        if action == 0:\n            self.index = max(self.index - 1, 0)\n            if self.dict_state:\n                return {'index': self.index}, 0, False, {'key': 1, 'env': self}\n            else:\n                return self.index, 0, False, {}\n        elif action == 1:\n            self.index += 1\n            self.done = self.index == self.size\n            if self.dict_state:\n                return {'index': self.index}, int(self.done), self.done, \\\n                    {'key': 1, 'env': self}\n            else:\n                return self.index, int(self.done), self.done, \\\n                    {'key': 1, 'env': self}\n"""
test/base/test_batch.py,22,"b'import torch\nimport pickle\nimport pytest\nimport numpy as np\n\nfrom tianshou.data import Batch, to_torch\n\n\ndef test_batch():\n    batch = Batch(obs=[0], np=np.zeros([3, 4]))\n    assert batch.obs == batch[""obs""]\n    batch.obs = [1]\n    assert batch.obs == [1]\n    batch.append(batch)\n    assert batch.obs == [1, 1]\n    assert batch.np.shape == (6, 4)\n    assert batch[0].obs == batch[1].obs\n    batch.obs = np.arange(5)\n    for i, b in enumerate(batch.split(1, shuffle=False)):\n        if i != 5:\n            assert b.obs == batch[i].obs\n        else:\n            with pytest.raises(AttributeError):\n                batch[i].obs\n            with pytest.raises(AttributeError):\n                b.obs\n    print(batch)\n\n\ndef test_batch_over_batch():\n    batch = Batch(a=[3, 4, 5], b=[4, 5, 6])\n    batch2 = Batch(c=[6, 7, 8], b=batch)\n    batch2.b.b[-1] = 0\n    print(batch2)\n    assert batch2.values()[-1] == batch2.c\n    assert batch2[-1].b.b == 0\n\n\ndef test_batch_over_batch_to_torch():\n    batch = Batch(\n        a=np.ones((1,), dtype=np.float64),\n        b=Batch(\n            c=np.ones((1,), dtype=np.float64),\n            d=torch.ones((1,), dtype=torch.float64)\n        )\n    )\n    batch.to_torch()\n    assert isinstance(batch.a, torch.Tensor)\n    assert isinstance(batch.b.c, torch.Tensor)\n    assert isinstance(batch.b.d, torch.Tensor)\n    assert batch.a.dtype == torch.float64\n    assert batch.b.c.dtype == torch.float64\n    assert batch.b.d.dtype == torch.float64\n    batch.to_torch(dtype=torch.float32)\n    assert batch.a.dtype == torch.float32\n    assert batch.b.c.dtype == torch.float32\n    assert batch.b.d.dtype == torch.float32\n\n\ndef test_utils_to_torch():\n    batch = Batch(\n        a=np.ones((1,), dtype=np.float64),\n        b=Batch(\n            c=np.ones((1,), dtype=np.float64),\n            d=torch.ones((1,), dtype=torch.float64)\n        )\n    )\n    a_torch_float = to_torch(batch.a, dtype=torch.float32)\n    assert a_torch_float.dtype == torch.float32\n    a_torch_double = to_torch(batch.a, dtype=torch.float64)\n    assert a_torch_double.dtype == torch.float64\n    batch_torch_float = to_torch(batch, dtype=torch.float32)\n    assert batch_torch_float.a.dtype == torch.float32\n    assert batch_torch_float.b.c.dtype == torch.float32\n    assert batch_torch_float.b.d.dtype == torch.float32\n\n\ndef test_batch_pickle():\n    batch = Batch(obs=Batch(a=0.0, c=torch.Tensor([1.0, 2.0])),\n                  np=np.zeros([3, 4]))\n    batch_pk = pickle.loads(pickle.dumps(batch))\n    assert batch.obs.a == batch_pk.obs.a\n    assert torch.all(batch.obs.c == batch_pk.obs.c)\n    assert np.all(batch.np == batch_pk.np)\n\n\ndef test_batch_from_to_numpy_without_copy():\n    batch = Batch(a=np.ones((1,)), b=Batch(c=np.ones((1,))))\n    a_mem_addr_orig = batch.a.__array_interface__[\'data\'][0]\n    c_mem_addr_orig = batch.b.c.__array_interface__[\'data\'][0]\n    batch.to_torch()\n    batch.to_numpy()\n    a_mem_addr_new = batch.a.__array_interface__[\'data\'][0]\n    c_mem_addr_new = batch.b.c.__array_interface__[\'data\'][0]\n    assert a_mem_addr_new == a_mem_addr_orig\n    assert c_mem_addr_new == c_mem_addr_orig\n\n\nif __name__ == \'__main__\':\n    test_batch()\n    test_batch_over_batch()\n    test_batch_over_batch_to_torch()\n    test_utils_to_torch()\n    test_batch_pickle()\n    test_batch_from_to_numpy_without_copy()\n'"
test/base/test_buffer.py,0,"b""import numpy as np\nfrom tianshou.data import ReplayBuffer, PrioritizedReplayBuffer\n\nif __name__ == '__main__':\n    from env import MyTestEnv\nelse:  # pytest\n    from test.base.env import MyTestEnv\n\n\ndef test_replaybuffer(size=10, bufsize=20):\n    env = MyTestEnv(size)\n    buf = ReplayBuffer(bufsize)\n    buf2 = ReplayBuffer(bufsize)\n    obs = env.reset()\n    action_list = [1] * 5 + [0] * 10 + [1] * 10\n    for i, a in enumerate(action_list):\n        obs_next, rew, done, info = env.step(a)\n        buf.add(obs, a, rew, done, obs_next, info)\n        obs = obs_next\n        assert len(buf) == min(bufsize, i + 1)\n    data, indice = buf.sample(bufsize * 2)\n    assert (indice < len(buf)).all()\n    assert (data.obs < size).all()\n    assert (0 <= data.done).all() and (data.done <= 1).all()\n    assert len(buf) > len(buf2)\n    buf2.update(buf)\n    assert len(buf) == len(buf2)\n    assert buf2[0].obs == buf[5].obs\n    assert buf2[-1].obs == buf[4].obs\n\n\ndef test_stack(size=5, bufsize=9, stack_num=4):\n    env = MyTestEnv(size)\n    buf = ReplayBuffer(bufsize, stack_num)\n    obs = env.reset(1)\n    for i in range(15):\n        obs_next, rew, done, info = env.step(1)\n        buf.add(obs, 1, rew, done, None, info)\n        obs = obs_next\n        if done:\n            obs = env.reset(1)\n    indice = np.arange(len(buf))\n    assert np.allclose(buf.get(indice, 'obs'), np.array([\n        [1, 1, 1, 2], [1, 1, 2, 3], [1, 2, 3, 4],\n        [1, 1, 1, 1], [1, 1, 1, 2], [1, 1, 2, 3],\n        [3, 3, 3, 3], [3, 3, 3, 4], [1, 1, 1, 1]]))\n    print(buf)\n\n\ndef test_priortized_replaybuffer(size=32, bufsize=15):\n    env = MyTestEnv(size)\n    buf = PrioritizedReplayBuffer(bufsize, 0.5, 0.5)\n    obs = env.reset()\n    action_list = [1] * 5 + [0] * 10 + [1] * 10\n    for i, a in enumerate(action_list):\n        obs_next, rew, done, info = env.step(a)\n        buf.add(obs, a, rew, done, obs_next, info, np.random.randn() - 0.5)\n        obs = obs_next\n        assert np.isclose(np.sum((buf.weight / buf._weight_sum)[:buf._size]),\n                          1, rtol=1e-12)\n        data, indice = buf.sample(len(buf) // 2)\n        if len(buf) // 2 == 0:\n            assert len(data) == len(buf)\n        else:\n            assert len(data) == len(buf) // 2\n        assert len(buf) == min(bufsize, i + 1)\n        assert np.isclose(buf._weight_sum, (buf.weight).sum())\n    data, indice = buf.sample(len(buf) // 2)\n    buf.update_weight(indice, -data.weight / 2)\n    assert np.isclose(buf.weight[indice], np.power(\n        np.abs(-data.weight / 2), buf._alpha)).all()\n    assert np.isclose(buf._weight_sum, (buf.weight).sum())\n\n\nif __name__ == '__main__':\n    test_replaybuffer()\n    test_stack()\n    test_priortized_replaybuffer(233333, 200000)\n"""
test/base/test_collector.py,1,"b""import numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.policy import BasePolicy\nfrom tianshou.env import VectorEnv, SubprocVectorEnv\nfrom tianshou.data import Collector, Batch, ReplayBuffer\n\nif __name__ == '__main__':\n    from env import MyTestEnv\nelse:  # pytest\n    from test.base.env import MyTestEnv\n\n\nclass MyPolicy(BasePolicy):\n    def __init__(self, dict_state=False):\n        super().__init__()\n        self.dict_state = dict_state\n\n    def forward(self, batch, state=None):\n        if self.dict_state:\n            return Batch(act=np.ones(batch.obs['index'].shape[0]))\n        return Batch(act=np.ones(batch.obs.shape[0]))\n\n    def learn(self):\n        pass\n\n\ndef preprocess_fn(**kwargs):\n    # modify info before adding into the buffer\n    if kwargs.get('info', None) is not None:\n        n = len(kwargs['obs'])\n        info = kwargs['info']\n        for i in range(n):\n            info[i].update(rew=kwargs['rew'][i])\n        return {'info': info}\n        # or\n        # return Batch(info=info)\n    else:\n        return {}\n\n\nclass Logger(object):\n    def __init__(self, writer):\n        self.cnt = 0\n        self.writer = writer\n\n    def log(self, info):\n        self.writer.add_scalar(\n            'key', np.mean(info['key']), global_step=self.cnt)\n        self.cnt += 1\n\n\ndef test_collector():\n    writer = SummaryWriter('log/collector')\n    logger = Logger(writer)\n    env_fns = [lambda x=i: MyTestEnv(size=x, sleep=0) for i in [2, 3, 4, 5]]\n\n    venv = SubprocVectorEnv(env_fns)\n    policy = MyPolicy()\n    env = env_fns[0]()\n    c0 = Collector(policy, env, ReplayBuffer(size=100, ignore_obs_next=False),\n                   preprocess_fn)\n    c0.collect(n_step=3, log_fn=logger.log)\n    assert np.allclose(c0.buffer.obs[:3], [0, 1, 0])\n    assert np.allclose(c0.buffer[:3].obs_next, [1, 2, 1])\n    c0.collect(n_episode=3, log_fn=logger.log)\n    assert np.allclose(c0.buffer.obs[:8], [0, 1, 0, 1, 0, 1, 0, 1])\n    assert np.allclose(c0.buffer[:8].obs_next, [1, 2, 1, 2, 1, 2, 1, 2])\n    c1 = Collector(policy, venv, ReplayBuffer(size=100, ignore_obs_next=False),\n                   preprocess_fn)\n    c1.collect(n_step=6)\n    assert np.allclose(c1.buffer.obs[:11], [0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 3])\n    assert np.allclose(c1.buffer[:11].obs_next,\n                       [1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 4])\n    c1.collect(n_episode=2)\n    assert np.allclose(c1.buffer.obs[11:21], [0, 1, 2, 3, 4, 0, 1, 0, 1, 2])\n    assert np.allclose(c1.buffer[11:21].obs_next,\n                       [1, 2, 3, 4, 5, 1, 2, 1, 2, 3])\n    c2 = Collector(policy, venv, ReplayBuffer(size=100, ignore_obs_next=False),\n                   preprocess_fn)\n    c2.collect(n_episode=[1, 2, 2, 2])\n    assert np.allclose(c2.buffer.obs_next[:26], [\n        1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5,\n        1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5])\n    c2.reset_env()\n    c2.collect(n_episode=[2, 2, 2, 2])\n    assert np.allclose(c2.buffer.obs_next[26:54], [\n        1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5,\n        1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5])\n\n\ndef test_collector_with_dict_state():\n    env = MyTestEnv(size=5, sleep=0, dict_state=True)\n    policy = MyPolicy(dict_state=True)\n    c0 = Collector(policy, env, ReplayBuffer(size=100), preprocess_fn)\n    c0.collect(n_step=3)\n    c0.collect(n_episode=3)\n    env_fns = [lambda x=i: MyTestEnv(size=x, sleep=0, dict_state=True)\n               for i in [2, 3, 4, 5]]\n    envs = VectorEnv(env_fns)\n    c1 = Collector(policy, envs, ReplayBuffer(size=100), preprocess_fn)\n    c1.collect(n_step=10)\n    c1.collect(n_episode=[2, 1, 1, 2])\n    batch = c1.sample(10)\n    print(batch)\n    c0.buffer.update(c1.buffer)\n    assert np.allclose(c0.buffer[:len(c0.buffer)].obs.index, [\n        0., 1., 2., 3., 4., 0., 1., 2., 3., 4., 0., 1., 2., 3., 4., 0., 1.,\n        0., 1., 2., 0., 1., 0., 1., 2., 3., 0., 1., 2., 3., 4., 0., 1., 0.,\n        1., 2., 0., 1., 0., 1., 2., 3., 0., 1., 2., 3., 4.])\n    c2 = Collector(policy, envs, ReplayBuffer(size=100, stack_num=4),\n                   preprocess_fn)\n    c2.collect(n_episode=[0, 0, 0, 10])\n    batch = c2.sample(10)\n    print(batch['obs_next']['index'])\n\n\nif __name__ == '__main__':\n    test_collector()\n    test_collector_with_dict_state()\n"""
test/base/test_env.py,0,"b""import time\nimport numpy as np\nfrom gym.spaces.discrete import Discrete\nfrom tianshou.env import VectorEnv, SubprocVectorEnv, RayVectorEnv\n\nif __name__ == '__main__':\n    from env import MyTestEnv\nelse:  # pytest\n    from test.base.env import MyTestEnv\n\n\ndef test_vecenv(size=10, num=8, sleep=0.001):\n    verbose = __name__ == '__main__'\n    env_fns = [\n        lambda i=i: MyTestEnv(size=i, sleep=sleep)\n        for i in range(size, size + num)\n    ]\n    venv = [\n        VectorEnv(env_fns),\n        SubprocVectorEnv(env_fns),\n    ]\n    if verbose:\n        venv.append(RayVectorEnv(env_fns))\n    for v in venv:\n        v.seed()\n    action_list = [1] * 5 + [0] * 10 + [1] * 20\n    if not verbose:\n        o = [v.reset() for v in venv]\n        for i, a in enumerate(action_list):\n            o = []\n            for v in venv:\n                A, B, C, D = v.step([a] * num)\n                if sum(C):\n                    A = v.reset(np.where(C)[0])\n                o.append([A, B, C, D])\n            for i in zip(*o):\n                for j in range(1, len(i) - 1):\n                    assert (i[0] == i[j]).all()\n    else:\n        t = [0, 0, 0]\n        for i, e in enumerate(venv):\n            t[i] = time.time()\n            e.reset()\n            for a in action_list:\n                done = e.step([a] * num)[2]\n                if sum(done) > 0:\n                    e.reset(np.where(done)[0])\n            t[i] = time.time() - t[i]\n        print(f'VectorEnv: {t[0]:.6f}s')\n        print(f'SubprocVectorEnv: {t[1]:.6f}s')\n        print(f'RayVectorEnv: {t[2]:.6f}s')\n    for v in venv:\n        assert v.size == list(range(size, size + num))\n        assert v.env_num == num\n        assert v.action_space == [Discrete(1)] * num\n\n    for v in venv:\n        v.close()\n\n\nif __name__ == '__main__':\n    test_vecenv()\n"""
test/continuous/__init__.py,0,b''
test/continuous/net.py,18,"b""import torch\nimport numpy as np\nfrom torch import nn\n\nfrom tianshou.data import to_torch\n\n\nclass Actor(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape,\n                 max_action, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.model = [\n            nn.Linear(np.prod(state_shape), 128),\n            nn.ReLU(inplace=True)]\n        for i in range(layer_num):\n            self.model += [nn.Linear(128, 128), nn.ReLU(inplace=True)]\n        self.model += [nn.Linear(128, np.prod(action_shape))]\n        self.model = nn.Sequential(*self.model)\n        self._max = max_action\n\n    def forward(self, s, **kwargs):\n        s = to_torch(s, device=self.device, dtype=torch.float)\n        batch = s.shape[0]\n        s = s.view(batch, -1)\n        logits = self.model(s)\n        logits = self._max * torch.tanh(logits)\n        return logits, None\n\n\nclass ActorProb(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape,\n                 max_action, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.model = [\n            nn.Linear(np.prod(state_shape), 128),\n            nn.ReLU(inplace=True)]\n        for i in range(layer_num):\n            self.model += [nn.Linear(128, 128), nn.ReLU(inplace=True)]\n        self.model = nn.Sequential(*self.model)\n        self.mu = nn.Linear(128, np.prod(action_shape))\n        self.sigma = nn.Parameter(torch.zeros(np.prod(action_shape), 1))\n        # self.sigma = nn.Linear(128, np.prod(action_shape))\n        self._max = max_action\n\n    def forward(self, s, **kwargs):\n        s = to_torch(s, device=self.device, dtype=torch.float)\n        batch = s.shape[0]\n        s = s.view(batch, -1)\n        logits = self.model(s)\n        mu = self.mu(logits)\n        shape = [1] * len(mu.shape)\n        shape[1] = -1\n        sigma = (self.sigma.view(shape) + torch.zeros_like(mu)).exp()\n        # assert sigma.shape == mu.shape\n        # mu = self._max * torch.tanh(self.mu(logits))\n        # sigma = torch.exp(self.sigma(logits))\n        return (mu, sigma), None\n\n\nclass Critic(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape=0, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.model = [\n            nn.Linear(np.prod(state_shape) + np.prod(action_shape), 128),\n            nn.ReLU(inplace=True)]\n        for i in range(layer_num):\n            self.model += [nn.Linear(128, 128), nn.ReLU(inplace=True)]\n        self.model += [nn.Linear(128, 1)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, s, a=None, **kwargs):\n        s = to_torch(s, device=self.device, dtype=torch.float)\n        batch = s.shape[0]\n        s = s.view(batch, -1)\n        if a is not None:\n            if not isinstance(a, torch.Tensor):\n                a = torch.tensor(a, device=self.device, dtype=torch.float)\n            a = a.view(batch, -1)\n            s = torch.cat([s, a], dim=1)\n        logits = self.model(s)\n        return logits\n\n\nclass RecurrentActorProb(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape,\n                 max_action, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.nn = nn.LSTM(input_size=np.prod(state_shape), hidden_size=128,\n                          num_layers=layer_num, batch_first=True)\n        self.mu = nn.Linear(128, np.prod(action_shape))\n        self.sigma = nn.Parameter(torch.zeros(np.prod(action_shape), 1))\n\n    def forward(self, s, **kwargs):\n        s = to_torch(s, device=self.device, dtype=torch.float)\n        # s [bsz, len, dim] (training) or [bsz, dim] (evaluation)\n        # In short, the tensor's shape in training phase is longer than which\n        # in evaluation phase.\n        if len(s.shape) == 2:\n            bsz, dim = s.shape\n            length = 1\n        else:\n            bsz, length, dim = s.shape\n        s = s.view(bsz, length, -1)\n        logits, _ = self.nn(s)\n        logits = logits[:, -1]\n        mu = self.mu(logits)\n        shape = [1] * len(mu.shape)\n        shape[1] = -1\n        sigma = (self.sigma.view(shape) + torch.zeros_like(mu)).exp()\n        return (mu, sigma), None\n\n\nclass RecurrentCritic(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape=0, device='cpu'):\n        super().__init__()\n        self.state_shape = state_shape\n        self.action_shape = action_shape\n        self.device = device\n        self.nn = nn.LSTM(input_size=np.prod(state_shape), hidden_size=128,\n                          num_layers=layer_num, batch_first=True)\n        self.fc2 = nn.Linear(128 + np.prod(action_shape), 1)\n\n    def forward(self, s, a=None):\n        s = to_torch(s, device=self.device, dtype=torch.float)\n        # s [bsz, len, dim] (training) or [bsz, dim] (evaluation)\n        # In short, the tensor's shape in training phase is longer than which\n        # in evaluation phase.\n        assert len(s.shape) == 3\n        self.nn.flatten_parameters()\n        s, (h, c) = self.nn(s)\n        s = s[:, -1]\n        if a is not None:\n            if not isinstance(a, torch.Tensor):\n                a = torch.tensor(a, device=self.device, dtype=torch.float)\n            s = torch.cat([s, a], dim=1)\n        s = self.fc2(s)\n        return s\n"""
test/continuous/test_ddpg.py,7,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.policy import DDPGPolicy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\n\nif __name__ == \'__main__\':\n    from net import Actor, Critic\nelse:  # pytest\n    from test.continuous.net import Actor, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Pendulum-v0\')\n    parser.add_argument(\'--seed\', type=int, default=0)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--actor-lr\', type=float, default=1e-4)\n    parser.add_argument(\'--critic-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--tau\', type=float, default=0.005)\n    parser.add_argument(\'--exploration-noise\', type=float, default=0.1)\n    parser.add_argument(\'--epoch\', type=int, default=20)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=2400)\n    parser.add_argument(\'--collect-per-step\', type=int, default=4)\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\'--rew-norm\', type=int, default=1)\n    parser.add_argument(\'--ignore-done\', type=int, default=1)\n    parser.add_argument(\'--n-step\', type=int, default=1)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_ddpg(args=get_args()):\n    torch.set_num_threads(1)  # we just need only one thread for NN\n    env = gym.make(args.task)\n    if args.task == \'Pendulum-v0\':\n        env.spec.reward_threshold = -250\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    args.max_action = env.action_space.high[0]\n    # you can also use tianshou.env.SubprocVectorEnv\n    # train_envs = gym.make(args.task)\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    actor = Actor(\n        args.layer_num, args.state_shape, args.action_shape,\n        args.max_action, args.device\n    ).to(args.device)\n    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n    critic = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic_optim = torch.optim.Adam(critic.parameters(), lr=args.critic_lr)\n    policy = DDPGPolicy(\n        actor, actor_optim, critic, critic_optim,\n        args.tau, args.gamma, args.exploration_noise,\n        [env.action_space.low[0], env.action_space.high[0]],\n        reward_normalization=args.rew_norm,\n        ignore_done=args.ignore_done,\n        estimation_step=args.n_step)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'ddpg\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_ddpg()\n'"
test/continuous/test_ppo.py,9,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.policy import PPOPolicy\nfrom tianshou.policy.dist import DiagGaussian\nfrom tianshou.trainer import onpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\n\nif __name__ == \'__main__\':\n    from net import ActorProb, Critic\nelse:  # pytest\n    from test.continuous.net import ActorProb, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Pendulum-v0\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--epoch\', type=int, default=20)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=1)\n    parser.add_argument(\'--repeat-per-collect\', type=int, default=2)\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=16)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    # ppo special\n    parser.add_argument(\'--vf-coef\', type=float, default=0.5)\n    parser.add_argument(\'--ent-coef\', type=float, default=0.01)\n    parser.add_argument(\'--eps-clip\', type=float, default=0.2)\n    parser.add_argument(\'--max-grad-norm\', type=float, default=0.5)\n    parser.add_argument(\'--gae-lambda\', type=float, default=0.95)\n    parser.add_argument(\'--rew-norm\', type=int, default=1)\n    parser.add_argument(\'--dual-clip\', type=float, default=None)\n    parser.add_argument(\'--value-clip\', type=int, default=1)\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_ppo(args=get_args()):\n    torch.set_num_threads(1)  # we just need only one thread for NN\n    env = gym.make(args.task)\n    if args.task == \'Pendulum-v0\':\n        env.spec.reward_threshold = -250\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    args.max_action = env.action_space.high[0]\n    # you can also use tianshou.env.SubprocVectorEnv\n    # train_envs = gym.make(args.task)\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    actor = ActorProb(\n        args.layer_num, args.state_shape, args.action_shape,\n        args.max_action, args.device\n    ).to(args.device)\n    critic = Critic(\n        args.layer_num, args.state_shape, device=args.device\n    ).to(args.device)\n    # orthogonal initialization\n    for m in list(actor.modules()) + list(critic.modules()):\n        if isinstance(m, torch.nn.Linear):\n            torch.nn.init.orthogonal_(m.weight)\n            torch.nn.init.zeros_(m.bias)\n    optim = torch.optim.Adam(list(\n        actor.parameters()) + list(critic.parameters()), lr=args.lr)\n    dist = DiagGaussian\n    policy = PPOPolicy(\n        actor, critic, optim, dist, args.gamma,\n        max_grad_norm=args.max_grad_norm,\n        eps_clip=args.eps_clip,\n        vf_coef=args.vf_coef,\n        ent_coef=args.ent_coef,\n        reward_normalization=args.rew_norm,\n        # dual_clip=args.dual_clip,\n        # dual clip cause monotonically increasing log_std :)\n        value_clip=args.value_clip,\n        # action_range=[env.action_space.low[0], env.action_space.high[0]],)\n        # if clip the action, ppo would not converge :)\n        gae_lambda=args.gae_lambda)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'ppo\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = onpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.repeat_per_collect,\n        args.test_num, args.batch_size, stop_fn=stop_fn, save_fn=save_fn,\n        writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_ppo()\n'"
test/continuous/test_sac_with_il.py,9,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\nfrom tianshou.policy import SACPolicy, ImitationPolicy\n\nif __name__ == \'__main__\':\n    from net import Actor, ActorProb, Critic\nelse:  # pytest\n    from test.continuous.net import Actor, ActorProb, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Pendulum-v0\')\n    parser.add_argument(\'--seed\', type=int, default=0)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--actor-lr\', type=float, default=3e-4)\n    parser.add_argument(\'--critic-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--il-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--tau\', type=float, default=0.005)\n    parser.add_argument(\'--alpha\', type=float, default=0.2)\n    parser.add_argument(\'--epoch\', type=int, default=20)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=2400)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\'--rew-norm\', type=int, default=1)\n    parser.add_argument(\'--ignore-done\', type=int, default=1)\n    parser.add_argument(\'--n-step\', type=int, default=4)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_sac_with_il(args=get_args()):\n    torch.set_num_threads(1)  # we just need only one thread for NN\n    env = gym.make(args.task)\n    if args.task == \'Pendulum-v0\':\n        env.spec.reward_threshold = -250\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    args.max_action = env.action_space.high[0]\n    # you can also use tianshou.env.SubprocVectorEnv\n    # train_envs = gym.make(args.task)\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    actor = ActorProb(\n        args.layer_num, args.state_shape, args.action_shape,\n        args.max_action, args.device\n    ).to(args.device)\n    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n    critic1 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n    critic2 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n    policy = SACPolicy(\n        actor, actor_optim, critic1, critic1_optim, critic2, critic2_optim,\n        args.tau, args.gamma, args.alpha,\n        [env.action_space.low[0], env.action_space.high[0]],\n        reward_normalization=args.rew_norm,\n        ignore_done=args.ignore_done,\n        estimation_step=args.n_step)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # train_collector.collect(n_step=args.buffer_size)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'sac\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n    # here we define an imitation collector with a trivial policy\n    if args.task == \'Pendulum-v0\':\n        env.spec.reward_threshold = -300  # lower the goal\n    net = Actor(1, args.state_shape, args.action_shape,\n                args.max_action, args.device).to(args.device)\n    optim = torch.optim.Adam(net.parameters(), lr=args.il_lr)\n    il_policy = ImitationPolicy(net, optim, mode=\'continuous\')\n    il_test_collector = Collector(il_policy, test_envs)\n    train_collector.reset()\n    result = offpolicy_trainer(\n        il_policy, train_collector, il_test_collector, args.epoch,\n        args.step_per_epoch // 5, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    il_test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(il_policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_sac_with_il()\n'"
test/continuous/test_td3.py,8,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.policy import TD3Policy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\n\nif __name__ == \'__main__\':\n    from net import Actor, Critic\nelse:  # pytest\n    from test.continuous.net import Actor, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'Pendulum-v0\')\n    parser.add_argument(\'--seed\', type=int, default=0)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--actor-lr\', type=float, default=3e-4)\n    parser.add_argument(\'--critic-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--tau\', type=float, default=0.005)\n    parser.add_argument(\'--exploration-noise\', type=float, default=0.1)\n    parser.add_argument(\'--policy-noise\', type=float, default=0.2)\n    parser.add_argument(\'--noise-clip\', type=float, default=0.5)\n    parser.add_argument(\'--update-actor-freq\', type=int, default=2)\n    parser.add_argument(\'--epoch\', type=int, default=20)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=2400)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=128)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\'--rew-norm\', type=int, default=1)\n    parser.add_argument(\'--ignore-done\', type=int, default=1)\n    parser.add_argument(\'--n-step\', type=int, default=1)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_td3(args=get_args()):\n    torch.set_num_threads(1)  # we just need only one thread for NN\n    env = gym.make(args.task)\n    if args.task == \'Pendulum-v0\':\n        env.spec.reward_threshold = -250\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    args.max_action = env.action_space.high[0]\n    # you can also use tianshou.env.SubprocVectorEnv\n    # train_envs = gym.make(args.task)\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    actor = Actor(\n        args.layer_num, args.state_shape, args.action_shape,\n        args.max_action, args.device\n    ).to(args.device)\n    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n    critic1 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n    critic2 = Critic(\n        args.layer_num, args.state_shape, args.action_shape, args.device\n    ).to(args.device)\n    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n    policy = TD3Policy(\n        actor, actor_optim, critic1, critic1_optim, critic2, critic2_optim,\n        args.tau, args.gamma, args.exploration_noise, args.policy_noise,\n        args.update_actor_freq, args.noise_clip,\n        [env.action_space.low[0], env.action_space.high[0]],\n        reward_normalization=args.rew_norm,\n        ignore_done=args.ignore_done,\n        estimation_step=args.n_step)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # train_collector.collect(n_step=args.buffer_size)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'td3\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_td3()\n'"
test/discrete/__init__.py,0,b''
test/discrete/net.py,3,"b""import torch\nimport numpy as np\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom tianshou.data import to_torch\n\n\nclass Net(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape=0, device='cpu',\n                 softmax=False):\n        super().__init__()\n        self.device = device\n        self.model = [\n            nn.Linear(np.prod(state_shape), 128),\n            nn.ReLU(inplace=True)]\n        for i in range(layer_num):\n            self.model += [nn.Linear(128, 128), nn.ReLU(inplace=True)]\n        if action_shape:\n            self.model += [nn.Linear(128, np.prod(action_shape))]\n        if softmax:\n            self.model += [nn.Softmax(dim=-1)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, s, state=None, info={}):\n        s = to_torch(s, device=self.device, dtype=torch.float)\n        batch = s.shape[0]\n        s = s.view(batch, -1)\n        logits = self.model(s)\n        return logits, state\n\n\nclass Actor(nn.Module):\n    def __init__(self, preprocess_net, action_shape):\n        super().__init__()\n        self.preprocess = preprocess_net\n        self.last = nn.Linear(128, np.prod(action_shape))\n\n    def forward(self, s, state=None, info={}):\n        logits, h = self.preprocess(s, state)\n        logits = F.softmax(self.last(logits), dim=-1)\n        return logits, h\n\n\nclass Critic(nn.Module):\n    def __init__(self, preprocess_net):\n        super().__init__()\n        self.preprocess = preprocess_net\n        self.last = nn.Linear(128, 1)\n\n    def forward(self, s, **kwargs):\n        logits, h = self.preprocess(s, state=kwargs.get('state', None))\n        logits = self.last(logits)\n        return logits\n\n\nclass Recurrent(nn.Module):\n    def __init__(self, layer_num, state_shape, action_shape, device='cpu'):\n        super().__init__()\n        self.state_shape = state_shape\n        self.action_shape = action_shape\n        self.device = device\n        self.fc1 = nn.Linear(np.prod(state_shape), 128)\n        self.nn = nn.LSTM(input_size=128, hidden_size=128,\n                          num_layers=layer_num, batch_first=True)\n        self.fc2 = nn.Linear(128, np.prod(action_shape))\n\n    def forward(self, s, state=None, info={}):\n        s = to_torch(s, device=self.device, dtype=torch.float)\n        # s [bsz, len, dim] (training) or [bsz, dim] (evaluation)\n        # In short, the tensor's shape in training phase is longer than which\n        # in evaluation phase.\n        if len(s.shape) == 2:\n            bsz, dim = s.shape\n            length = 1\n        else:\n            bsz, length, dim = s.shape\n        s = self.fc1(s.view([bsz * length, dim]))\n        s = s.view(bsz, length, -1)\n        self.nn.flatten_parameters()\n        if state is None:\n            s, (h, c) = self.nn(s)\n        else:\n            # we store the stack data in [bsz, len, ...] format\n            # but pytorch rnn needs [len, bsz, ...]\n            s, (h, c) = self.nn(s, (state['h'].transpose(0, 1).contiguous(),\n                                    state['c'].transpose(0, 1).contiguous()))\n        s = self.fc2(s[:, -1])\n        # please ensure the first dim is batch size: [bsz, len, ...]\n        return s, {'h': h.transpose(0, 1).detach(),\n                   'c': c.transpose(0, 1).detach()}\n"""
test/discrete/test_a2c_with_il.py,8,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.data import Collector, ReplayBuffer\nfrom tianshou.policy import A2CPolicy, ImitationPolicy\nfrom tianshou.trainer import onpolicy_trainer, offpolicy_trainer\n\nif __name__ == \'__main__\':\n    from net import Net, Actor, Critic\nelse:  # pytest\n    from test.discrete.net import Net, Actor, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'CartPole-v0\')\n    parser.add_argument(\'--seed\', type=int, default=1)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--lr\', type=float, default=3e-4)\n    parser.add_argument(\'--il-lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.9)\n    parser.add_argument(\'--epoch\', type=int, default=10)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--repeat-per-collect\', type=int, default=1)\n    parser.add_argument(\'--batch-size\', type=int, default=64)\n    parser.add_argument(\'--layer-num\', type=int, default=2)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    # a2c special\n    parser.add_argument(\'--vf-coef\', type=float, default=0.5)\n    parser.add_argument(\'--ent-coef\', type=float, default=0.0)\n    parser.add_argument(\'--max-grad-norm\', type=float, default=None)\n    parser.add_argument(\'--gae-lambda\', type=float, default=1.)\n    parser.add_argument(\'--rew-norm\', type=bool, default=False)\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_a2c_with_il(args=get_args()):\n    torch.set_num_threads(1)  # for poor CPU\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    # you can also use tianshou.env.SubprocVectorEnv\n    # train_envs = gym.make(args.task)\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    net = Net(args.layer_num, args.state_shape, device=args.device)\n    actor = Actor(net, args.action_shape).to(args.device)\n    critic = Critic(net).to(args.device)\n    optim = torch.optim.Adam(list(\n        actor.parameters()) + list(critic.parameters()), lr=args.lr)\n    dist = torch.distributions.Categorical\n    policy = A2CPolicy(\n        actor, critic, optim, dist, args.gamma, gae_lambda=args.gae_lambda,\n        vf_coef=args.vf_coef, ent_coef=args.ent_coef,\n        max_grad_norm=args.max_grad_norm, reward_normalization=args.rew_norm)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'a2c\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = onpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.repeat_per_collect,\n        args.test_num, args.batch_size, stop_fn=stop_fn, save_fn=save_fn,\n        writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n    # here we define an imitation collector with a trivial policy\n    if args.task == \'CartPole-v0\':\n        env.spec.reward_threshold = 190  # lower the goal\n    net = Net(1, args.state_shape, device=args.device)\n    net = Actor(net, args.action_shape).to(args.device)\n    optim = torch.optim.Adam(net.parameters(), lr=args.il_lr)\n    il_policy = ImitationPolicy(net, optim, mode=\'discrete\')\n    il_test_collector = Collector(il_policy, test_envs)\n    train_collector.reset()\n    result = offpolicy_trainer(\n        il_policy, train_collector, il_test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    il_test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(il_policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_a2c_with_il()\n'"
test/discrete/test_dqn.py,5,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.policy import DQNPolicy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\n\nif __name__ == \'__main__\':\n    from net import Net\nelse:  # pytest\n    from test.discrete.net import Net\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'CartPole-v0\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--eps-test\', type=float, default=0.05)\n    parser.add_argument(\'--eps-train\', type=float, default=0.1)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.9)\n    parser.add_argument(\'--n-step\', type=int, default=3)\n    parser.add_argument(\'--target-update-freq\', type=int, default=320)\n    parser.add_argument(\'--epoch\', type=int, default=10)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=64)\n    parser.add_argument(\'--layer-num\', type=int, default=3)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_dqn(args=get_args()):\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    # train_envs = gym.make(args.task)\n    # you can also use tianshou.env.SubprocVectorEnv\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    net = Net(args.layer_num, args.state_shape, args.action_shape, args.device)\n    net = net.to(args.device)\n    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n    policy = DQNPolicy(\n        net, optim, args.gamma, args.n_step,\n        use_target_network=args.target_update_freq > 0,\n        target_update_freq=args.target_update_freq)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # policy.set_eps(1)\n    train_collector.collect(n_step=args.batch_size)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'dqn\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    def train_fn(x):\n        policy.set_eps(args.eps_train)\n\n    def test_fn(x):\n        policy.set_eps(args.eps_test)\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, train_fn=train_fn, test_fn=test_fn,\n        stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_dqn(get_args())\n'"
test/discrete/test_drqn.py,5,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.policy import DQNPolicy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\n\nif __name__ == \'__main__\':\n    from net import Recurrent\nelse:  # pytest\n    from test.discrete.net import Recurrent\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'CartPole-v0\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--eps-test\', type=float, default=0.05)\n    parser.add_argument(\'--eps-train\', type=float, default=0.1)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--stack-num\', type=int, default=4)\n    parser.add_argument(\'--lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.9)\n    parser.add_argument(\'--n-step\', type=int, default=4)\n    parser.add_argument(\'--target-update-freq\', type=int, default=320)\n    parser.add_argument(\'--epoch\', type=int, default=10)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=64)\n    parser.add_argument(\'--layer-num\', type=int, default=3)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_drqn(args=get_args()):\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    # train_envs = gym.make(args.task)\n    # you can also use tianshou.env.SubprocVectorEnv\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task)for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    net = Recurrent(args.layer_num, args.state_shape,\n                    args.action_shape, args.device)\n    net = net.to(args.device)\n    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n    policy = DQNPolicy(\n        net, optim, args.gamma, args.n_step,\n        use_target_network=args.target_update_freq > 0,\n        target_update_freq=args.target_update_freq)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(\n            args.buffer_size, stack_num=args.stack_num, ignore_obs_next=True))\n    # the stack_num is for RNN training: sample framestack obs\n    test_collector = Collector(policy, test_envs)\n    # policy.set_eps(1)\n    train_collector.collect(n_step=args.batch_size)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'drqn\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    def train_fn(x):\n        policy.set_eps(args.eps_train)\n\n    def test_fn(x):\n        policy.set_eps(args.eps_test)\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, train_fn=train_fn, test_fn=test_fn,\n        stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_drqn(get_args())\n'"
test/discrete/test_pdqn.py,5,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.policy import DQNPolicy\nfrom tianshou.trainer import offpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer, PrioritizedReplayBuffer\n\nif __name__ == \'__main__\':\n    from net import Net\nelse:  # pytest\n    from test.discrete.net import Net\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'CartPole-v0\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--eps-test\', type=float, default=0.05)\n    parser.add_argument(\'--eps-train\', type=float, default=0.1)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.9)\n    parser.add_argument(\'--n-step\', type=int, default=3)\n    parser.add_argument(\'--target-update-freq\', type=int, default=320)\n    parser.add_argument(\'--epoch\', type=int, default=10)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--batch-size\', type=int, default=64)\n    parser.add_argument(\'--layer-num\', type=int, default=3)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\'--prioritized-replay\', type=int, default=1)\n    parser.add_argument(\'--alpha\', type=float, default=0.5)\n    parser.add_argument(\'--beta\', type=float, default=0.5)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_pdqn(args=get_args()):\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    # train_envs = gym.make(args.task)\n    # you can also use tianshou.env.SubprocVectorEnv\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    net = Net(args.layer_num, args.state_shape, args.action_shape, args.device)\n    net = net.to(args.device)\n    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n    policy = DQNPolicy(\n        net, optim, args.gamma, args.n_step,\n        use_target_network=args.target_update_freq > 0,\n        target_update_freq=args.target_update_freq)\n    # collector\n    if args.prioritized_replay > 0:\n        buf = PrioritizedReplayBuffer(\n            args.buffer_size, alpha=args.alpha, beta=args.alpha)\n    else:\n        buf = ReplayBuffer(args.buffer_size)\n    train_collector = Collector(\n        policy, train_envs, buf)\n    test_collector = Collector(policy, test_envs)\n    # policy.set_eps(1)\n    train_collector.collect(n_step=args.batch_size)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'dqn\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    def train_fn(x):\n        policy.set_eps(args.eps_train)\n\n    def test_fn(x):\n        policy.set_eps(args.eps_test)\n\n    # trainer\n    result = offpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.test_num,\n        args.batch_size, train_fn=train_fn, test_fn=test_fn,\n        stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_pdqn(get_args())\n'"
test/discrete/test_pg.py,6,"b'import os\nimport gym\nimport time\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.policy import PGPolicy\nfrom tianshou.trainer import onpolicy_trainer\nfrom tianshou.data import Batch, Collector, ReplayBuffer\n\nif __name__ == \'__main__\':\n    from net import Net\nelse:  # pytest\n    from test.discrete.net import Net\n\n\ndef compute_return_base(batch, aa=None, bb=None, gamma=0.1):\n    returns = np.zeros_like(batch.rew)\n    last = 0\n    for i in reversed(range(len(batch.rew))):\n        returns[i] = batch.rew[i]\n        if not batch.done[i]:\n            returns[i] += last * gamma\n        last = returns[i]\n    batch.returns = returns\n    return batch\n\n\ndef test_fn(size=2560):\n    policy = PGPolicy(None, None, None, discount_factor=0.1)\n    buf = ReplayBuffer(100)\n    buf.add(1, 1, 1, 1, 1)\n    fn = policy.process_fn\n    # fn = compute_return_base\n    batch = Batch(\n        done=np.array([1, 0, 0, 1, 0, 1, 0, 1.]),\n        rew=np.array([0, 1, 2, 3, 4, 5, 6, 7.]),\n    )\n    batch = fn(batch, buf, 0)\n    ans = np.array([0, 1.23, 2.3, 3, 4.5, 5, 6.7, 7])\n    assert abs(batch.returns - ans).sum() <= 1e-5\n    batch = Batch(\n        done=np.array([0, 1, 0, 1, 0, 1, 0.]),\n        rew=np.array([7, 6, 1, 2, 3, 4, 5.]),\n    )\n    batch = fn(batch, buf, 0)\n    ans = np.array([7.6, 6, 1.2, 2, 3.4, 4, 5])\n    assert abs(batch.returns - ans).sum() <= 1e-5\n    batch = Batch(\n        done=np.array([0, 1, 0, 1, 0, 0, 1.]),\n        rew=np.array([7, 6, 1, 2, 3, 4, 5.]),\n    )\n    batch = fn(batch, buf, 0)\n    ans = np.array([7.6, 6, 1.2, 2, 3.45, 4.5, 5])\n    assert abs(batch.returns - ans).sum() <= 1e-5\n    batch = Batch(\n        done=np.array([0, 0, 0, 1., 0, 0, 0, 1, 0, 0, 0, 1]),\n        rew=np.array([\n            101, 102, 103., 200, 104, 105, 106, 201, 107, 108, 109, 202])\n    )\n    v = np.array([2., 3., 4, -1, 5., 6., 7, -2, 8., 9., 10, -3])\n    ret = policy.compute_episodic_return(batch, v, gamma=0.99, gae_lambda=0.95)\n    returns = np.array([\n        454.8344, 376.1143, 291.298, 200.,\n        464.5610, 383.1085, 295.387, 201.,\n        474.2876, 390.1027, 299.476, 202.])\n    assert abs(ret.returns - returns).sum() <= 1e-3\n    if __name__ == \'__main__\':\n        batch = Batch(\n            done=np.random.randint(100, size=size) == 0,\n            rew=np.random.random(size),\n        )\n        cnt = 3000\n        t = time.time()\n        for _ in range(cnt):\n            compute_return_base(batch)\n        print(f\'vanilla: {(time.time() - t) / cnt}\')\n        t = time.time()\n        for _ in range(cnt):\n            policy.process_fn(batch, buf, 0)\n        print(f\'policy: {(time.time() - t) / cnt}\')\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'CartPole-v0\')\n    parser.add_argument(\'--seed\', type=int, default=1626)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--lr\', type=float, default=3e-4)\n    parser.add_argument(\'--gamma\', type=float, default=0.9)\n    parser.add_argument(\'--epoch\', type=int, default=10)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=1000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=10)\n    parser.add_argument(\'--repeat-per-collect\', type=int, default=2)\n    parser.add_argument(\'--batch-size\', type=int, default=64)\n    parser.add_argument(\'--layer-num\', type=int, default=3)\n    parser.add_argument(\'--training-num\', type=int, default=8)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\'--rew-norm\', type=int, default=1)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_pg(args=get_args()):\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    # train_envs = gym.make(args.task)\n    # you can also use tianshou.env.SubprocVectorEnv\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    net = Net(\n        args.layer_num, args.state_shape, args.action_shape,\n        device=args.device, softmax=True)\n    net = net.to(args.device)\n    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n    dist = torch.distributions.Categorical\n    policy = PGPolicy(net, optim, dist, args.gamma,\n                      reward_normalization=args.rew_norm)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'pg\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = onpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.repeat_per_collect,\n        args.test_num, args.batch_size, stop_fn=stop_fn, save_fn=save_fn,\n        writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    # test_fn()\n    test_pg()\n'"
test/discrete/test_ppo.py,10,"b'import os\nimport gym\nimport torch\nimport pprint\nimport argparse\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tianshou.env import VectorEnv\nfrom tianshou.policy import PPOPolicy\nfrom tianshou.trainer import onpolicy_trainer\nfrom tianshou.data import Collector, ReplayBuffer\n\nif __name__ == \'__main__\':\n    from net import Net, Actor, Critic\nelse:  # pytest\n    from test.discrete.net import Net, Actor, Critic\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'CartPole-v0\')\n    parser.add_argument(\'--seed\', type=int, default=0)\n    parser.add_argument(\'--buffer-size\', type=int, default=20000)\n    parser.add_argument(\'--lr\', type=float, default=1e-3)\n    parser.add_argument(\'--gamma\', type=float, default=0.99)\n    parser.add_argument(\'--epoch\', type=int, default=10)\n    parser.add_argument(\'--step-per-epoch\', type=int, default=2000)\n    parser.add_argument(\'--collect-per-step\', type=int, default=20)\n    parser.add_argument(\'--repeat-per-collect\', type=int, default=2)\n    parser.add_argument(\'--batch-size\', type=int, default=64)\n    parser.add_argument(\'--layer-num\', type=int, default=1)\n    parser.add_argument(\'--training-num\', type=int, default=20)\n    parser.add_argument(\'--test-num\', type=int, default=100)\n    parser.add_argument(\'--logdir\', type=str, default=\'log\')\n    parser.add_argument(\'--render\', type=float, default=0.)\n    parser.add_argument(\n        \'--device\', type=str,\n        default=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    # ppo special\n    parser.add_argument(\'--vf-coef\', type=float, default=0.5)\n    parser.add_argument(\'--ent-coef\', type=float, default=0.0)\n    parser.add_argument(\'--eps-clip\', type=float, default=0.2)\n    parser.add_argument(\'--max-grad-norm\', type=float, default=0.5)\n    parser.add_argument(\'--gae-lambda\', type=float, default=0.8)\n    parser.add_argument(\'--rew-norm\', type=int, default=1)\n    parser.add_argument(\'--dual-clip\', type=float, default=None)\n    parser.add_argument(\'--value-clip\', type=int, default=1)\n    args = parser.parse_known_args()[0]\n    return args\n\n\ndef test_ppo(args=get_args()):\n    torch.set_num_threads(1)  # for poor CPU\n    env = gym.make(args.task)\n    args.state_shape = env.observation_space.shape or env.observation_space.n\n    args.action_shape = env.action_space.shape or env.action_space.n\n    # train_envs = gym.make(args.task)\n    # you can also use tianshou.env.SubprocVectorEnv\n    train_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.training_num)])\n    # test_envs = gym.make(args.task)\n    test_envs = VectorEnv(\n        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n    # seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    train_envs.seed(args.seed)\n    test_envs.seed(args.seed)\n    # model\n    net = Net(args.layer_num, args.state_shape, device=args.device)\n    actor = Actor(net, args.action_shape).to(args.device)\n    critic = Critic(net).to(args.device)\n    # orthogonal initialization\n    for m in list(actor.modules()) + list(critic.modules()):\n        if isinstance(m, torch.nn.Linear):\n            torch.nn.init.orthogonal_(m.weight)\n            torch.nn.init.zeros_(m.bias)\n    optim = torch.optim.Adam(list(\n        actor.parameters()) + list(critic.parameters()), lr=args.lr)\n    dist = torch.distributions.Categorical\n    policy = PPOPolicy(\n        actor, critic, optim, dist, args.gamma,\n        max_grad_norm=args.max_grad_norm,\n        eps_clip=args.eps_clip,\n        vf_coef=args.vf_coef,\n        ent_coef=args.ent_coef,\n        action_range=None,\n        gae_lambda=args.gae_lambda,\n        reward_normalization=args.rew_norm,\n        dual_clip=args.dual_clip,\n        value_clip=args.value_clip)\n    # collector\n    train_collector = Collector(\n        policy, train_envs, ReplayBuffer(args.buffer_size))\n    test_collector = Collector(policy, test_envs)\n    # log\n    log_path = os.path.join(args.logdir, args.task, \'ppo\')\n    writer = SummaryWriter(log_path)\n\n    def save_fn(policy):\n        torch.save(policy.state_dict(), os.path.join(log_path, \'policy.pth\'))\n\n    def stop_fn(x):\n        return x >= env.spec.reward_threshold\n\n    # trainer\n    result = onpolicy_trainer(\n        policy, train_collector, test_collector, args.epoch,\n        args.step_per_epoch, args.collect_per_step, args.repeat_per_collect,\n        args.test_num, args.batch_size, stop_fn=stop_fn, save_fn=save_fn,\n        writer=writer)\n    assert stop_fn(result[\'best_reward\'])\n    train_collector.close()\n    test_collector.close()\n    if __name__ == \'__main__\':\n        pprint.pprint(result)\n        # Let\'s watch its performance!\n        env = gym.make(args.task)\n        collector = Collector(policy, env)\n        result = collector.collect(n_episode=1, render=args.render)\n        print(f\'Final reward: {result[""rew""]}, length: {result[""len""]}\')\n        collector.close()\n\n\nif __name__ == \'__main__\':\n    test_ppo()\n'"
tianshou/data/__init__.py,0,"b""from tianshou.data.batch import Batch\nfrom tianshou.data.utils import to_numpy, to_torch, \\\n    to_torch_as\nfrom tianshou.data.buffer import ReplayBuffer, \\\n    ListReplayBuffer, PrioritizedReplayBuffer\nfrom tianshou.data.collector import Collector\n\n__all__ = [\n    'Batch',\n    'to_numpy',\n    'to_torch',\n    'to_torch_as',\n    'ReplayBuffer',\n    'ListReplayBuffer',\n    'PrioritizedReplayBuffer',\n    'Collector'\n]\n"""
tianshou/data/batch.py,11,"b'import torch\nimport warnings\nimport pprint\nimport numpy as np\nfrom typing import Any, List, Union, Iterator, Optional\n\n# Disable pickle warning related to torch, since it has been removed\n# on torch master branch. See Pull Request #39003 for details:\n# https://github.com/pytorch/pytorch/pull/39003\nwarnings.filterwarnings(\n    ""ignore"", message=""pickle support for Storage will be removed in 1.5."")\n\n\nclass Batch:\n    """"""Tianshou provides :class:`~tianshou.data.Batch` as the internal data\n    structure to pass any kind of data to other methods, for example, a\n    collector gives a :class:`~tianshou.data.Batch` to policy for learning.\n    Here is the usage:\n    ::\n\n        >>> import numpy as np\n        >>> from tianshou.data import Batch\n        >>> data = Batch(a=4, b=[5, 5], c=\'2312312\')\n        >>> data.b\n        [5, 5]\n        >>> data.b = np.array([3, 4, 5])\n        >>> print(data)\n        Batch(\n            a: 4,\n            b: array([3, 4, 5]),\n            c: \'2312312\',\n        )\n\n    In short, you can define a :class:`Batch` with any key-value pair. The\n    current implementation of Tianshou typically use 7 reserved keys in\n    :class:`~tianshou.data.Batch`:\n\n    * ``obs`` the observation of step :math:`t` ;\n    * ``act`` the action of step :math:`t` ;\n    * ``rew`` the reward of step :math:`t` ;\n    * ``done`` the done flag of step :math:`t` ;\n    * ``obs_next`` the observation of step :math:`t+1` ;\n    * ``info`` the info of step :math:`t` (in ``gym.Env``, the ``env.step()``\\\n        function return 4 arguments, and the last one is ``info``);\n    * ``policy`` the data computed by policy in step :math:`t`;\n\n    :class:`~tianshou.data.Batch` has other methods, including\n    :meth:`~tianshou.data.Batch.__getitem__`,\n    :meth:`~tianshou.data.Batch.__len__`,\n    :meth:`~tianshou.data.Batch.append`,\n    and :meth:`~tianshou.data.Batch.split`:\n    ::\n\n        >>> data = Batch(obs=np.array([0, 11, 22]), rew=np.array([6, 6, 6]))\n        >>> # here we test __getitem__\n        >>> index = [2, 1]\n        >>> data[index].obs\n        array([22, 11])\n\n        >>> # here we test __len__\n        >>> len(data)\n        3\n\n        >>> data.append(data)  # similar to list.append\n        >>> data.obs\n        array([0, 11, 22, 0, 11, 22])\n\n        >>> # split whole data into multiple small batch\n        >>> for d in data.split(size=2, shuffle=False):\n        ...     print(d.obs, d.rew)\n        [ 0 11] [6 6]\n        [22  0] [6 6]\n        [11 22] [6 6]\n    """"""\n\n    def __new__(cls, **kwargs) -> None:\n        self = super().__new__(cls)\n        self._meta = {}\n        return self\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__()\n        for k, v in kwargs.items():\n            if isinstance(v, (list, np.ndarray)) \\\n                    and len(v) > 0 and isinstance(v[0], dict) and k != \'info\':\n                self._meta[k] = list(v[0].keys())\n                for k_ in v[0].keys():\n                    k__ = \'_\' + k + \'@\' + k_\n                    self.__dict__[k__] = np.array([\n                        v[i][k_] for i in range(len(v))\n                    ])\n            elif isinstance(v, dict):\n                self._meta[k] = list(v.keys())\n                for k_, v_ in v.items():\n                    k__ = \'_\' + k + \'@\' + k_\n                    self.__dict__[k__] = v_\n            else:\n                self.__dict__[k] = v\n\n    def __getstate__(self):\n        """"""Pickling interface. Only the actual data are serialized\n        for both efficiency and simplicity.\n        """"""\n        state = {}\n        for k in self.keys():\n            v = self[k]\n            if isinstance(v, Batch):\n                v = v.__getstate__()\n            state[k] = v\n        return state\n\n    def __setstate__(self, state):\n        """"""Unpickling interface. At this point, self is an empty Batch\n        instance that has not been initialized, so it can safely be\n        initialized by the pickle state.\n        """"""\n        self.__init__(**state)\n\n    def __getitem__(self, index: Union[str, slice]) -> Union[\'Batch\', dict]:\n        """"""Return self[index].""""""\n        if isinstance(index, str):\n            return self.__getattr__(index)\n        b = Batch()\n        for k, v in self.__dict__.items():\n            if k != \'_meta\' and hasattr(v, \'__len__\'):\n                try:\n                    b.__dict__.update(**{k: v[index]})\n                except IndexError:\n                    continue\n        b._meta = self._meta\n        return b\n\n    def __getattr__(self, key: str) -> Union[\'Batch\', Any]:\n        """"""Return self.key""""""\n        if key not in self._meta.keys():\n            if key not in self.__dict__:\n                raise AttributeError(key)\n            return self.__dict__[key]\n        d = {}\n        for k_ in self._meta[key]:\n            k__ = \'_\' + key + \'@\' + k_\n            d[k_] = self.__dict__[k__]\n        return Batch(**d)\n\n    def __repr__(self) -> str:\n        """"""Return str(self).""""""\n        s = self.__class__.__name__ + \'(\\n\'\n        flag = False\n        for k in sorted(list(self.__dict__) + list(self._meta)):\n            if k[0] != \'_\' and (self.__dict__.get(k, None) is not None or\n                                k in self._meta):\n                rpl = \'\\n\' + \' \' * (6 + len(k))\n                obj = pprint.pformat(self.__getattr__(k)).replace(\'\\n\', rpl)\n                s += f\'    {k}: {obj},\\n\'\n                flag = True\n        if flag:\n            s += \')\'\n        else:\n            s = self.__class__.__name__ + \'()\'\n        return s\n\n    def keys(self) -> List[str]:\n        """"""Return self.keys().""""""\n        return sorted(list(self._meta.keys()) +\n                      [k for k in self.__dict__.keys() if k[0] != \'_\'])\n\n    def values(self) -> List[Any]:\n        """"""Return self.values().""""""\n        return [self[k] for k in self.keys()]\n\n    def get(self, k: str, d: Optional[Any] = None) -> Union[\'Batch\', Any]:\n        """"""Return self[k] if k in self else d. d defaults to None.""""""\n        if k in self.__dict__ or k in self._meta:\n            return self.__getattr__(k)\n        return d\n\n    def to_numpy(self) -> None:\n        """"""Change all torch.Tensor to numpy.ndarray. This is an inplace\n        operation.\n        """"""\n        for k, v in self.__dict__.items():\n            if isinstance(v, torch.Tensor):\n                self.__dict__[k] = v.detach().cpu().numpy()\n            elif isinstance(v, Batch):\n                v.to_numpy()\n\n    def to_torch(self,\n                 dtype: Optional[torch.dtype] = None,\n                 device: Union[str, int, torch.device] = \'cpu\'\n                 ) -> None:\n        """"""Change all numpy.ndarray to torch.Tensor. This is an inplace\n        operation.\n        """"""\n        if not isinstance(device, torch.device):\n            device = torch.device(device)\n\n        for k, v in self.__dict__.items():\n            if isinstance(v, np.ndarray):\n                v = torch.from_numpy(v).to(device)\n                if dtype is not None:\n                    v = v.type(dtype)\n                self.__dict__[k] = v\n            if isinstance(v, torch.Tensor):\n                if dtype is not None and v.dtype != dtype:\n                    must_update_tensor = True\n                elif v.device.type != device.type:\n                    must_update_tensor = True\n                elif device.index is not None and \\\n                        device.index != v.device.index:\n                    must_update_tensor = True\n                else:\n                    must_update_tensor = False\n                if must_update_tensor:\n                    if dtype is not None:\n                        v = v.type(dtype)\n                    self.__dict__[k] = v.to(device)\n            elif isinstance(v, Batch):\n                v.to_torch(dtype, device)\n\n    def append(self, batch: \'Batch\') -> None:\n        """"""Append a :class:`~tianshou.data.Batch` object to current batch.""""""\n        assert isinstance(batch, Batch), \'Only append Batch is allowed!\'\n        for k, v in batch.__dict__.items():\n            if k == \'_meta\':\n                self._meta.update(batch._meta)\n                continue\n            if v is None:\n                continue\n            if not hasattr(self, k) or self.__dict__[k] is None:\n                self.__dict__[k] = v\n            elif isinstance(v, np.ndarray):\n                self.__dict__[k] = np.concatenate([self.__dict__[k], v])\n            elif isinstance(v, torch.Tensor):\n                self.__dict__[k] = torch.cat([self.__dict__[k], v])\n            elif isinstance(v, list):\n                self.__dict__[k] += v\n            else:\n                s = f\'No support for append with type \\\n                      {type(v)} in class Batch.\'\n                raise TypeError(s)\n\n    def __len__(self) -> int:\n        """"""Return len(self).""""""\n        r = [len(v) for k, v in self.__dict__.items() if hasattr(v, \'__len__\')]\n        return max(r) if len(r) > 0 else 0\n\n    def split(self, size: Optional[int] = None,\n              shuffle: bool = True) -> Iterator[\'Batch\']:\n        """"""Split whole data into multiple small batch.\n\n        :param int size: if it is ``None``, it does not split the data batch;\n            otherwise it will divide the data batch with the given size.\n            Default to ``None``.\n        :param bool shuffle: randomly shuffle the entire data batch if it is\n            ``True``, otherwise remain in the same. Default to ``True``.\n        """"""\n        length = len(self)\n        if size is None:\n            size = length\n        if shuffle:\n            indices = np.random.permutation(length)\n        else:\n            indices = np.arange(length)\n        for idx in np.arange(0, length, size):\n            yield self[indices[idx:(idx + size)]]\n'"
tianshou/data/buffer.py,0,"b'import pprint\nimport numpy as np\nfrom typing import Any, Tuple, Union, Optional\n\nfrom tianshou.data.batch import Batch\n\n\nclass ReplayBuffer(object):\n    """""":class:`~tianshou.data.ReplayBuffer` stores data generated from\n    interaction between the policy and environment. It stores basically 7 types\n    of data, as mentioned in :class:`~tianshou.data.Batch`, based on\n    ``numpy.ndarray``. Here is the usage:\n    ::\n\n        >>> import numpy as np\n        >>> from tianshou.data import ReplayBuffer\n        >>> buf = ReplayBuffer(size=20)\n        >>> for i in range(3):\n        ...     buf.add(obs=i, act=i, rew=i, done=i, obs_next=i + 1, info={})\n        >>> len(buf)\n        3\n        >>> buf.obs\n        # since we set size = 20, len(buf.obs) == 20.\n        array([0., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n               0., 0., 0., 0.])\n\n        >>> buf2 = ReplayBuffer(size=10)\n        >>> for i in range(15):\n        ...     buf2.add(obs=i, act=i, rew=i, done=i, obs_next=i + 1, info={})\n        >>> len(buf2)\n        10\n        >>> buf2.obs\n        # since its size = 10, it only stores the last 10 steps\' result.\n        array([10., 11., 12., 13., 14.,  5.,  6.,  7.,  8.,  9.])\n\n        >>> # move buf2\'s result into buf (meanwhile keep it chronologically)\n        >>> buf.update(buf2)\n        array([ 0.,  1.,  2.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n                0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\n        >>> # get a random sample from buffer\n        >>> # the batch_data is equal to buf[incide].\n        >>> batch_data, indice = buf.sample(batch_size=4)\n        >>> batch_data.obs == buf[indice].obs\n        array([ True,  True,  True,  True])\n\n    :class:`~tianshou.data.ReplayBuffer` also supports frame_stack sampling\n    (typically for RNN usage, see issue#19), ignoring storing the next\n    observation (save memory in atari tasks), and multi-modal observation (see\n    issue#38):\n    ::\n\n        >>> buf = ReplayBuffer(size=9, stack_num=4, ignore_obs_next=True)\n        >>> for i in range(16):\n        ...     done = i % 5 == 0\n        ...     buf.add(obs={\'id\': i}, act=i, rew=i, done=done,\n        ...             obs_next={\'id\': i + 1})\n        >>> print(buf)  # you can see obs_next is not saved in buf\n        ReplayBuffer(\n            act: array([ 9., 10., 11., 12., 13., 14., 15.,  7.,  8.]),\n            done: array([0., 1., 0., 0., 0., 0., 1., 0., 0.]),\n            info: Batch(),\n            obs: Batch(\n                     id: array([ 9., 10., 11., 12., 13., 14., 15.,  7.,  8.]),\n                 ),\n            policy: Batch(),\n            rew: array([ 9., 10., 11., 12., 13., 14., 15.,  7.,  8.]),\n        )\n        >>> index = np.arange(len(buf))\n        >>> print(buf.get(index, \'obs\').id)\n        [[ 7.  7.  8.  9.]\n         [ 7.  8.  9. 10.]\n         [11. 11. 11. 11.]\n         [11. 11. 11. 12.]\n         [11. 11. 12. 13.]\n         [11. 12. 13. 14.]\n         [12. 13. 14. 15.]\n         [ 7.  7.  7.  7.]\n         [ 7.  7.  7.  8.]]\n        >>> # here is another way to get the stacked data\n        >>> # (stack only for obs and obs_next)\n        >>> abs(buf.get(index, \'obs\')[\'id\'] - buf[index].obs.id).sum().sum()\n        0.0\n        >>> # we can get obs_next through __getitem__, even if it doesn\'t exist\n        >>> print(buf[:].obs_next.id)\n        [[ 7.  8.  9. 10.]\n         [ 7.  8.  9. 10.]\n         [11. 11. 11. 12.]\n         [11. 11. 12. 13.]\n         [11. 12. 13. 14.]\n         [12. 13. 14. 15.]\n         [12. 13. 14. 15.]\n         [ 7.  7.  7.  8.]\n         [ 7.  7.  8.  9.]]\n    """"""\n\n    def __init__(self, size: int, stack_num: Optional[int] = 0,\n                 ignore_obs_next: bool = False, **kwargs) -> None:\n        super().__init__()\n        self._maxsize = size\n        self._stack = stack_num\n        self._save_s_ = not ignore_obs_next\n        self._meta = {}\n        self.reset()\n\n    def __len__(self) -> int:\n        """"""Return len(self).""""""\n        return self._size\n\n    def __repr__(self) -> str:\n        """"""Return str(self).""""""\n        s = self.__class__.__name__ + \'(\\n\'\n        flag = False\n        for k in sorted(list(self.__dict__) + list(self._meta)):\n            if k[0] != \'_\' and (self.__dict__.get(k, None) is not None or\n                                k in self._meta):\n                rpl = \'\\n\' + \' \' * (6 + len(k))\n                obj = pprint.pformat(self.__getattr__(k)).replace(\'\\n\', rpl)\n                s += f\'    {k}: {obj},\\n\'\n                flag = True\n        if flag:\n            s += \')\'\n        else:\n            s = self.__class__.__name__ + \'()\'\n        return s\n\n    def __getattr__(self, key: str) -> Union[Batch, np.ndarray]:\n        """"""Return self.key""""""\n        if key not in self._meta:\n            if key not in self.__dict__:\n                raise AttributeError(key)\n            return self.__dict__[key]\n        d = {}\n        for k_ in self._meta[key]:\n            k__ = \'_\' + key + \'@\' + k_\n            if k__ in self.__dict__:\n                d[k_] = self.__dict__[k__]\n            else:\n                d[k_] = self.__getattr__(k__)\n        return Batch(**d)\n\n    def _add_to_buffer(self, name: str, inst: Any) -> None:\n        if inst is None:\n            if getattr(self, name, None) is None:\n                self.__dict__[name] = None\n            return\n        if name in self._meta:\n            for k in inst.keys():\n                self._add_to_buffer(\'_\' + name + \'@\' + k, inst[k])\n            return\n        if self.__dict__.get(name, None) is None:\n            if isinstance(inst, np.ndarray):\n                self.__dict__[name] = np.zeros(\n                    (self._maxsize, *inst.shape), dtype=inst.dtype)\n            elif isinstance(inst, (dict, Batch)):\n                if self._meta.get(name, None) is None:\n                    self._meta[name] = list(inst.keys())\n                for k in inst.keys():\n                    k_ = \'_\' + name + \'@\' + k\n                    self._add_to_buffer(k_, inst[k])\n            elif np.isscalar(inst):\n                self.__dict__[name] = np.zeros(\n                    (self._maxsize,), dtype=np.asarray(inst).dtype)\n            else:  # fall back to np.object\n                self.__dict__[name] = np.array(\n                    [None for _ in range(self._maxsize)])\n        if isinstance(inst, np.ndarray) and \\\n                self.__dict__[name].shape[1:] != inst.shape:\n            raise ValueError(\n                ""Cannot add data to a buffer with different shape, ""\n                f""key: {name}, expect shape: {self.__dict__[name].shape[1:]}, ""\n                f""given shape: {inst.shape}."")\n        if name not in self._meta:\n            self.__dict__[name][self._index] = inst\n\n    def update(self, buffer: \'ReplayBuffer\') -> None:\n        """"""Move the data from the given buffer to self.""""""\n        i = begin = buffer._index % len(buffer)\n        while True:\n            self.add(\n                buffer.obs[i], buffer.act[i], buffer.rew[i], buffer.done[i],\n                buffer.obs_next[i] if self._save_s_ else None,\n                buffer.info[i], buffer.policy[i])\n            i = (i + 1) % len(buffer)\n            if i == begin:\n                break\n\n    def add(self,\n            obs: Union[dict, np.ndarray],\n            act: Union[np.ndarray, float],\n            rew: float,\n            done: bool,\n            obs_next: Optional[Union[dict, np.ndarray]] = None,\n            info: dict = {},\n            policy: Optional[Union[dict, Batch]] = {},\n            **kwargs) -> None:\n        """"""Add a batch of data into replay buffer.""""""\n        assert isinstance(info, (dict, Batch)), \\\n            \'You should return a dict in the last argument of env.step().\'\n        self._add_to_buffer(\'obs\', obs)\n        self._add_to_buffer(\'act\', act)\n        self._add_to_buffer(\'rew\', rew)\n        self._add_to_buffer(\'done\', done)\n        if self._save_s_:\n            self._add_to_buffer(\'obs_next\', obs_next)\n        self._add_to_buffer(\'info\', info)\n        self._add_to_buffer(\'policy\', policy)\n        if self._maxsize > 0:\n            self._size = min(self._size + 1, self._maxsize)\n            self._index = (self._index + 1) % self._maxsize\n        else:\n            self._size = self._index = self._index + 1\n\n    def reset(self) -> None:\n        """"""Clear all the data in replay buffer.""""""\n        self._index = self._size = 0\n\n    def sample(self, batch_size: int) -> Tuple[Batch, np.ndarray]:\n        """"""Get a random sample from buffer with size equal to batch_size. \\\n        Return all the data in the buffer if batch_size is ``0``.\n\n        :return: Sample data and its corresponding index inside the buffer.\n        """"""\n        if batch_size > 0:\n            indice = np.random.choice(self._size, batch_size)\n        else:\n            indice = np.concatenate([\n                np.arange(self._index, self._size),\n                np.arange(0, self._index),\n            ])\n        return self[indice], indice\n\n    def get(self, indice: Union[slice, np.ndarray], key: str,\n            stack_num: Optional[int] = None) -> Union[Batch, np.ndarray]:\n        """"""Return the stacked result, e.g. [s_{t-3}, s_{t-2}, s_{t-1}, s_t],\n        where s is self.key, t is indice. The stack_num (here equals to 4) is\n        given from buffer initialization procedure.\n        """"""\n        if stack_num is None:\n            stack_num = self._stack\n        if not isinstance(indice, np.ndarray):\n            if np.isscalar(indice):\n                indice = np.array(indice)\n            elif isinstance(indice, slice):\n                indice = np.arange(\n                    0 if indice.start is None\n                    else self._size - indice.start if indice.start < 0\n                    else indice.start,\n                    self._size if indice.stop is None\n                    else self._size - indice.stop if indice.stop < 0\n                    else indice.stop,\n                    1 if indice.step is None else indice.step)\n        # set last frame done to True\n        last_index = (self._index - 1 + self._size) % self._size\n        last_done, self.done[last_index] = self.done[last_index], True\n        if key == \'obs_next\' and (not self._save_s_ or self.obs_next is None):\n            indice += 1 - self.done[indice].astype(np.int)\n            indice[indice == self._size] = 0\n            key = \'obs\'\n        if stack_num == 0:\n            self.done[last_index] = last_done\n            if key in self._meta:\n                return {k: self.__dict__[\'_\' + key + \'@\' + k][indice]\n                        for k in self._meta[key]}\n            else:\n                return self.__dict__[key][indice]\n        if key in self._meta:\n            many_keys = self._meta[key]\n            stack = {k: [] for k in self._meta[key]}\n        else:\n            stack = []\n            many_keys = None\n        for i in range(stack_num):\n            if many_keys is not None:\n                for k_ in many_keys:\n                    k__ = \'_\' + key + \'@\' + k_\n                    stack[k_] = [self.__dict__[k__][indice]] + stack[k_]\n            else:\n                stack = [self.__dict__[key][indice]] + stack\n            pre_indice = indice - 1\n            pre_indice[pre_indice == -1] = self._size - 1\n            indice = pre_indice + self.done[pre_indice].astype(np.int)\n            indice[indice == self._size] = 0\n        self.done[last_index] = last_done\n        if many_keys is not None:\n            for k in stack:\n                stack[k] = np.stack(stack[k], axis=1)\n            stack = Batch(**stack)\n        else:\n            stack = np.stack(stack, axis=1)\n        return stack\n\n    def __getitem__(self, index: Union[slice, np.ndarray]) -> Batch:\n        """"""Return a data batch: self[index]. If stack_num is set to be > 0,\n        return the stacked obs and obs_next with shape [batch, len, ...].\n        """"""\n        return Batch(\n            obs=self.get(index, \'obs\'),\n            act=self.act[index],\n            # act_=self.get(index, \'act\'),  # stacked action, for RNN\n            rew=self.rew[index],\n            done=self.done[index],\n            obs_next=self.get(index, \'obs_next\'),\n            info=self.info[index],\n            policy=self.get(index, \'policy\'),\n        )\n\n\nclass ListReplayBuffer(ReplayBuffer):\n    """"""The function of :class:`~tianshou.data.ListReplayBuffer` is almost the\n    same as :class:`~tianshou.data.ReplayBuffer`. The only difference is that\n    :class:`~tianshou.data.ListReplayBuffer` is based on ``list``.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.data.ReplayBuffer` for more\n        detailed explanation.\n    """"""\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__(size=0, ignore_obs_next=False, **kwargs)\n\n    def _add_to_buffer(\n            self, name: str,\n            inst: Union[dict, Batch, np.ndarray, float, int, bool]) -> None:\n        if inst is None:\n            return\n        if self.__dict__.get(name, None) is None:\n            self.__dict__[name] = []\n        self.__dict__[name].append(inst)\n\n    def reset(self) -> None:\n        self._index = self._size = 0\n        for k in list(self.__dict__):\n            if isinstance(self.__dict__[k], list):\n                self.__dict__[k] = []\n\n\nclass PrioritizedReplayBuffer(ReplayBuffer):\n    """"""Prioritized replay buffer implementation.\n\n    :param float alpha: the prioritization exponent.\n    :param float beta: the importance sample soft coefficient.\n    :param str mode: defaults to ``weight``.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.data.ReplayBuffer` for more\n        detailed explanation.\n    """"""\n\n    def __init__(self, size: int, alpha: float, beta: float,\n                 mode: str = \'weight\', **kwargs) -> None:\n        if mode != \'weight\':\n            raise NotImplementedError\n        super().__init__(size, **kwargs)\n        self._alpha = alpha\n        self._beta = beta\n        self._weight_sum = 0.0\n        self.weight = np.zeros(size, dtype=np.float64)\n        self._amortization_freq = 50\n        self._amortization_counter = 0\n\n    def add(self,\n            obs: Union[dict, np.ndarray],\n            act: Union[np.ndarray, float],\n            rew: float,\n            done: bool,\n            obs_next: Optional[Union[dict, np.ndarray]] = None,\n            info: dict = {},\n            policy: Optional[Union[dict, Batch]] = {},\n            weight: float = 1.0,\n            **kwargs) -> None:\n        """"""Add a batch of data into replay buffer.""""""\n        self._weight_sum += np.abs(weight) ** self._alpha - \\\n            self.weight[self._index]\n        # we have to sacrifice some convenience for speed :(\n        self._add_to_buffer(\'weight\', np.abs(weight) ** self._alpha)\n        super().add(obs, act, rew, done, obs_next, info, policy)\n        self._check_weight_sum()\n\n    def sample(self, batch_size: int,\n               importance_sample: bool = True\n               ) -> Tuple[Batch, np.ndarray]:\n        """"""Get a random sample from buffer with priority probability. \\\n        Return all the data in the buffer if batch_size is ``0``.\n\n        :return: Sample data and its corresponding index inside the buffer.\n        """"""\n        if batch_size > 0 and batch_size <= self._size:\n            # Multiple sampling of the same sample\n            # will cause weight update conflict\n            indice = np.random.choice(\n                self._size, batch_size,\n                p=(self.weight / self.weight.sum())[:self._size],\n                replace=False)\n            # self._weight_sum is not work for the accuracy issue\n            # p=(self.weight/self._weight_sum)[:self._size], replace=False)\n        elif batch_size == 0:\n            indice = np.concatenate([\n                np.arange(self._index, self._size),\n                np.arange(0, self._index),\n            ])\n        else:\n            # if batch_size larger than len(self),\n            # it will lead to a bug in update weight\n            raise ValueError(""batch_size should be less than len(self)"")\n        batch = self[indice]\n        if importance_sample:\n            impt_weight = Batch(\n                impt_weight=1 / np.power(\n                    self._size * (batch.weight / self._weight_sum),\n                    self._beta))\n            batch.append(impt_weight)\n        self._check_weight_sum()\n        return batch, indice\n\n    def reset(self) -> None:\n        self._amortization_counter = 0\n        super().reset()\n\n    def update_weight(self, indice: Union[slice, np.ndarray],\n                      new_weight: np.ndarray) -> None:\n        """"""Update priority weight by indice in this buffer.\n\n        :param np.ndarray indice: indice you want to update weight\n        :param np.ndarray new_weight: new priority weight you wangt to update\n        """"""\n        self._weight_sum += np.power(np.abs(new_weight), self._alpha).sum() \\\n            - self.weight[indice].sum()\n        self.weight[indice] = np.power(np.abs(new_weight), self._alpha)\n\n    def __getitem__(self, index: Union[slice, np.ndarray]) -> Batch:\n        return Batch(\n            obs=self.get(index, \'obs\'),\n            act=self.act[index],\n            # act_=self.get(index, \'act\'),  # stacked action, for RNN\n            rew=self.rew[index],\n            done=self.done[index],\n            obs_next=self.get(index, \'obs_next\'),\n            info=self.info[index],\n            weight=self.weight[index],\n            policy=self.get(index, \'policy\'),\n        )\n\n    def _check_weight_sum(self) -> None:\n        # keep an accurate _weight_sum\n        self._amortization_counter += 1\n        if self._amortization_counter % self._amortization_freq == 0:\n            self._weight_sum = np.sum(self.weight)\n            self._amortization_counter = 0\n'"
tianshou/data/collector.py,4,"b'import gym\nimport time\nimport torch\nimport warnings\nimport numpy as np\nfrom typing import Any, Dict, List, Union, Optional, Callable\n\nfrom tianshou.utils import MovAvg\nfrom tianshou.env import BaseVectorEnv\nfrom tianshou.policy import BasePolicy\nfrom tianshou.data import Batch, ReplayBuffer, ListReplayBuffer, to_numpy\n\n\nclass Collector(object):\n    """"""The :class:`~tianshou.data.Collector` enables the policy to interact\n    with different types of environments conveniently.\n\n    :param policy: an instance of the :class:`~tianshou.policy.BasePolicy`\n        class.\n    :param env: a ``gym.Env`` environment or an instance of the\n        :class:`~tianshou.env.BaseVectorEnv` class.\n    :param buffer: an instance of the :class:`~tianshou.data.ReplayBuffer`\n        class, or a list of :class:`~tianshou.data.ReplayBuffer`. If set to\n        ``None``, it will automatically assign a small-size\n        :class:`~tianshou.data.ReplayBuffer`.\n    :param function preprocess_fn: a function called before the data has been\n        added to the buffer, see issue #42, defaults to ``None``.\n    :param int stat_size: for the moving average of recording speed, defaults\n        to 100.\n\n    The ``preprocess_fn`` is a function called before the data has been added\n    to the buffer with batch format, which receives up to 7 keys as listed in\n    :class:`~tianshou.data.Batch`. It will receive with only ``obs`` when the\n    collector resets the environment. It returns either a dict or a\n    :class:`~tianshou.data.Batch` with the modified keys and values. Examples\n    are in ""test/base/test_collector.py"".\n\n    Example:\n    ::\n\n        policy = PGPolicy(...)  # or other policies if you wish\n        env = gym.make(\'CartPole-v0\')\n        replay_buffer = ReplayBuffer(size=10000)\n        # here we set up a collector with a single environment\n        collector = Collector(policy, env, buffer=replay_buffer)\n\n        # the collector supports vectorized environments as well\n        envs = VectorEnv([lambda: gym.make(\'CartPole-v0\') for _ in range(3)])\n        buffers = [ReplayBuffer(size=5000) for _ in range(3)]\n        # you can also pass a list of replay buffer to collector, for multi-env\n        # collector = Collector(policy, envs, buffer=buffers)\n        collector = Collector(policy, envs, buffer=replay_buffer)\n\n        # collect at least 3 episodes\n        collector.collect(n_episode=3)\n        # collect 1 episode for the first env, 3 for the third env\n        collector.collect(n_episode=[1, 0, 3])\n        # collect at least 2 steps\n        collector.collect(n_step=2)\n        # collect episodes with visual rendering (the render argument is the\n        #   sleep time between rendering consecutive frames)\n        collector.collect(n_episode=1, render=0.03)\n\n        # sample data with a given number of batch-size:\n        batch_data = collector.sample(batch_size=64)\n        # policy.learn(batch_data)  # btw, vanilla policy gradient only\n        #   supports on-policy training, so here we pick all data in the buffer\n        batch_data = collector.sample(batch_size=0)\n        policy.learn(batch_data)\n        # on-policy algorithms use the collected data only once, so here we\n        #   clear the buffer\n        collector.reset_buffer()\n\n    For the scenario of collecting data from multiple environments to a single\n    buffer, the cache buffers will turn on automatically. It may return the\n    data more than the given limitation.\n\n    .. note::\n\n        Please make sure the given environment has a time limitation.\n    """"""\n\n    def __init__(self,\n                 policy: BasePolicy,\n                 env: Union[gym.Env, BaseVectorEnv],\n                 buffer: Optional[Union[ReplayBuffer, List[ReplayBuffer]]]\n                 = None,\n                 preprocess_fn: Callable[[Any], Union[dict, Batch]] = None,\n                 stat_size: Optional[int] = 100,\n                 **kwargs) -> None:\n        super().__init__()\n        self.env = env\n        self.env_num = 1\n        self.collect_time = 0\n        self.collect_step = 0\n        self.collect_episode = 0\n        self.buffer = buffer\n        self.policy = policy\n        self.preprocess_fn = preprocess_fn\n        # if preprocess_fn is None:\n        #     def _prep(**kwargs):\n        #         return kwargs\n        #     self.preprocess_fn = _prep\n        self.process_fn = policy.process_fn\n        self._multi_env = isinstance(env, BaseVectorEnv)\n        self._multi_buf = False  # True if buf is a list\n        # need multiple cache buffers only if storing in one buffer\n        self._cached_buf = []\n        if self._multi_env:\n            self.env_num = len(env)\n            if isinstance(self.buffer, list):\n                assert len(self.buffer) == self.env_num, \\\n                    \'The number of data buffer does not match the number of \' \\\n                    \'input env.\'\n                self._multi_buf = True\n            elif isinstance(self.buffer, ReplayBuffer) or self.buffer is None:\n                self._cached_buf = [\n                    ListReplayBuffer() for _ in range(self.env_num)]\n            else:\n                raise TypeError(\'The buffer in data collector is invalid!\')\n        self.stat_size = stat_size\n        self.reset()\n\n    def reset(self) -> None:\n        """"""Reset all related variables in the collector.""""""\n        self.reset_env()\n        self.reset_buffer()\n        # state over batch is either a list, an np.ndarray, or a torch.Tensor\n        self.state = None\n        self.step_speed = MovAvg(self.stat_size)\n        self.episode_speed = MovAvg(self.stat_size)\n        self.collect_step = 0\n        self.collect_episode = 0\n        self.collect_time = 0\n\n    def reset_buffer(self) -> None:\n        """"""Reset the main data buffer.""""""\n        if self._multi_buf:\n            for b in self.buffer:\n                b.reset()\n        else:\n            if self.buffer is not None:\n                self.buffer.reset()\n\n    def get_env_num(self) -> int:\n        """"""Return the number of environments the collector have.""""""\n        return self.env_num\n\n    def reset_env(self) -> None:\n        """"""Reset all of the environment(s)\' states and reset all of the cache\n        buffers (if need).\n        """"""\n        self._obs = self.env.reset()\n        if not self._multi_env:\n            self._obs = self._make_batch(self._obs)\n        if self.preprocess_fn:\n            self._obs = self.preprocess_fn(obs=self._obs).get(\'obs\', self._obs)\n        self._act = self._rew = self._done = self._info = None\n        if self._multi_env:\n            self.reward = np.zeros(self.env_num)\n            self.length = np.zeros(self.env_num)\n        else:\n            self.reward, self.length = 0, 0\n        for b in self._cached_buf:\n            b.reset()\n\n    def seed(self, seed: Optional[Union[int, List[int]]] = None) -> None:\n        """"""Reset all the seed(s) of the given environment(s).""""""\n        if hasattr(self.env, \'seed\'):\n            return self.env.seed(seed)\n\n    def render(self, **kwargs) -> None:\n        """"""Render all the environment(s).""""""\n        if hasattr(self.env, \'render\'):\n            return self.env.render(**kwargs)\n\n    def close(self) -> None:\n        """"""Close the environment(s).""""""\n        if hasattr(self.env, \'close\'):\n            self.env.close()\n\n    def _make_batch(self, data: Any) -> np.ndarray:\n        """"""Return [data].""""""\n        if isinstance(data, np.ndarray):\n            return data[None]\n        else:\n            return np.array([data])\n\n    def _reset_state(self, id: Union[int, List[int]]) -> None:\n        """"""Reset self.state[id].""""""\n        if self.state is None:\n            return\n        if isinstance(self.state, list):\n            self.state[id] = None\n        elif isinstance(self.state, (dict, Batch)):\n            for k in self.state.keys():\n                if isinstance(self.state[k], list):\n                    self.state[k][id] = None\n                elif isinstance(self.state[k], (torch.Tensor, np.ndarray)):\n                    self.state[k][id] = 0\n        elif isinstance(self.state, (torch.Tensor, np.ndarray)):\n            self.state[id] = 0\n\n    def collect(self,\n                n_step: int = 0,\n                n_episode: Union[int, List[int]] = 0,\n                render: Optional[float] = None,\n                log_fn: Optional[Callable[[dict], None]] = None\n                ) -> Dict[str, float]:\n        """"""Collect a specified number of step or episode.\n\n        :param int n_step: how many steps you want to collect.\n        :param n_episode: how many episodes you want to collect (in each\n            environment).\n        :type n_episode: int or list\n        :param float render: the sleep time between rendering consecutive\n            frames, defaults to ``None`` (no rendering).\n        :param function log_fn: a function which receives env info, typically\n            for tensorboard logging.\n\n        .. note::\n\n            One and only one collection number specification is permitted,\n            either ``n_step`` or ``n_episode``.\n\n        :return: A dict including the following keys\n\n            * ``n/ep`` the collected number of episodes.\n            * ``n/st`` the collected number of steps.\n            * ``v/st`` the speed of steps per second.\n            * ``v/ep`` the speed of episode per second.\n            * ``rew`` the mean reward over collected episodes.\n            * ``len`` the mean length over collected episodes.\n        """"""\n        warning_count = 0\n        if not self._multi_env:\n            n_episode = np.sum(n_episode)\n        start_time = time.time()\n        assert sum([(n_step != 0), (n_episode != 0)]) == 1, \\\n            ""One and only one collection number specification is permitted!""\n        cur_step = 0\n        cur_episode = np.zeros(self.env_num) if self._multi_env else 0\n        reward_sum = 0\n        length_sum = 0\n        while True:\n            if warning_count >= 100000:\n                warnings.warn(\n                    \'There are already many steps in an episode. \'\n                    \'You should add a time limitation to your environment!\',\n                    Warning)\n            batch = Batch(\n                obs=self._obs, act=self._act, rew=self._rew,\n                done=self._done, obs_next=None, info=self._info,\n                policy=None)\n            with torch.no_grad():\n                result = self.policy(batch, self.state)\n            self.state = result.get(\'state\', None)\n            self._policy = to_numpy(result.policy) \\\n                if hasattr(result, \'policy\') else [{}] * self.env_num\n            self._act = to_numpy(result.act)\n            obs_next, self._rew, self._done, self._info = self.env.step(\n                self._act if self._multi_env else self._act[0])\n            if not self._multi_env:\n                obs_next = self._make_batch(obs_next)\n                self._rew = self._make_batch(self._rew)\n                self._done = self._make_batch(self._done)\n                self._info = self._make_batch(self._info)\n            if log_fn:\n                log_fn(self._info if self._multi_env else self._info[0])\n            if render:\n                self.env.render()\n                if render > 0:\n                    time.sleep(render)\n            self.length += 1\n            self.reward += self._rew\n            if self.preprocess_fn:\n                result = self.preprocess_fn(\n                    obs=self._obs, act=self._act, rew=self._rew,\n                    done=self._done, obs_next=obs_next, info=self._info,\n                    policy=self._policy)\n                self._obs = result.get(\'obs\', self._obs)\n                self._act = result.get(\'act\', self._act)\n                self._rew = result.get(\'rew\', self._rew)\n                self._done = result.get(\'done\', self._done)\n                obs_next = result.get(\'obs_next\', obs_next)\n                self._info = result.get(\'info\', self._info)\n                self._policy = result.get(\'policy\', self._policy)\n            if self._multi_env:\n                for i in range(self.env_num):\n                    data = {\n                        \'obs\': self._obs[i], \'act\': self._act[i],\n                        \'rew\': self._rew[i], \'done\': self._done[i],\n                        \'obs_next\': obs_next[i], \'info\': self._info[i],\n                        \'policy\': self._policy[i]}\n                    if self._cached_buf:\n                        warning_count += 1\n                        self._cached_buf[i].add(**data)\n                    elif self._multi_buf:\n                        warning_count += 1\n                        self.buffer[i].add(**data)\n                        cur_step += 1\n                    else:\n                        warning_count += 1\n                        if self.buffer is not None:\n                            self.buffer.add(**data)\n                        cur_step += 1\n                    if self._done[i]:\n                        if n_step != 0 or np.isscalar(n_episode) or \\\n                                cur_episode[i] < n_episode[i]:\n                            cur_episode[i] += 1\n                            reward_sum += self.reward[i]\n                            length_sum += self.length[i]\n                            if self._cached_buf:\n                                cur_step += len(self._cached_buf[i])\n                                if self.buffer is not None:\n                                    self.buffer.update(self._cached_buf[i])\n                        self.reward[i], self.length[i] = 0, 0\n                        if self._cached_buf:\n                            self._cached_buf[i].reset()\n                        self._reset_state(i)\n                if sum(self._done):\n                    obs_next = self.env.reset(np.where(self._done)[0])\n                    if self.preprocess_fn:\n                        obs_next = self.preprocess_fn(obs=obs_next).get(\n                            \'obs\', obs_next)\n                if n_episode != 0:\n                    if isinstance(n_episode, list) and \\\n                            (cur_episode >= np.array(n_episode)).all() or \\\n                            np.isscalar(n_episode) and \\\n                            cur_episode.sum() >= n_episode:\n                        break\n            else:\n                if self.buffer is not None:\n                    self.buffer.add(\n                        self._obs[0], self._act[0], self._rew[0],\n                        self._done[0], obs_next[0], self._info[0],\n                        self._policy[0])\n                cur_step += 1\n                if self._done:\n                    cur_episode += 1\n                    reward_sum += self.reward[0]\n                    length_sum += self.length\n                    self.reward, self.length = 0, 0\n                    self.state = None\n                    obs_next = self._make_batch(self.env.reset())\n                    if self.preprocess_fn:\n                        obs_next = self.preprocess_fn(obs=obs_next).get(\n                            \'obs\', obs_next)\n                if n_episode != 0 and cur_episode >= n_episode:\n                    break\n            if n_step != 0 and cur_step >= n_step:\n                break\n            self._obs = obs_next\n        self._obs = obs_next\n        if self._multi_env:\n            cur_episode = sum(cur_episode)\n        duration = max(time.time() - start_time, 1e-9)\n        self.step_speed.add(cur_step / duration)\n        self.episode_speed.add(cur_episode / duration)\n        self.collect_step += cur_step\n        self.collect_episode += cur_episode\n        self.collect_time += duration\n        if isinstance(n_episode, list):\n            n_episode = np.sum(n_episode)\n        else:\n            n_episode = max(cur_episode, 1)\n        return {\n            \'n/ep\': cur_episode,\n            \'n/st\': cur_step,\n            \'v/st\': self.step_speed.get(),\n            \'v/ep\': self.episode_speed.get(),\n            \'rew\': reward_sum / n_episode,\n            \'len\': length_sum / n_episode,\n        }\n\n    def sample(self, batch_size: int) -> Batch:\n        """"""Sample a data batch from the internal replay buffer. It will call\n        :meth:`~tianshou.policy.BasePolicy.process_fn` before returning\n        the final batch data.\n\n        :param int batch_size: ``0`` means it will extract all the data from\n            the buffer, otherwise it will extract the data with the given\n            batch_size.\n        """"""\n        if self._multi_buf:\n            if batch_size > 0:\n                lens = [len(b) for b in self.buffer]\n                total = sum(lens)\n                batch_index = np.random.choice(\n                    len(self.buffer), batch_size, p=np.array(lens) / total)\n            else:\n                batch_index = np.array([])\n            batch_data = Batch()\n            for i, b in enumerate(self.buffer):\n                cur_batch = (batch_index == i).sum()\n                if batch_size and cur_batch or batch_size <= 0:\n                    batch, indice = b.sample(cur_batch)\n                    batch = self.process_fn(batch, b, indice)\n                    batch_data.append(batch)\n        else:\n            batch_data, indice = self.buffer.sample(batch_size)\n            batch_data = self.process_fn(batch_data, self.buffer, indice)\n        return batch_data\n'"
tianshou/data/utils.py,12,"b'import torch\nimport numpy as np\nfrom typing import Union, Optional\n\nfrom tianshou.data import Batch\n\n\ndef to_numpy(x: Union[\n    torch.Tensor, dict, Batch, np.ndarray]) -> Union[\n        dict, Batch, np.ndarray]:\n    """"""Return an object without torch.Tensor.""""""\n    if isinstance(x, torch.Tensor):\n        x = x.detach().cpu().numpy()\n    elif isinstance(x, dict):\n        for k, v in x.items():\n            x[k] = to_numpy(v)\n    elif isinstance(x, Batch):\n        x.to_numpy()\n    return x\n\n\ndef to_torch(x: Union[torch.Tensor, dict, Batch, np.ndarray],\n             dtype: Optional[torch.dtype] = None,\n             device: Union[str, int] = \'cpu\'\n             ) -> Union[dict, Batch, torch.Tensor]:\n    """"""Return an object without np.ndarray.""""""\n    if isinstance(x, np.ndarray):\n        x = torch.from_numpy(x).to(device)\n        if dtype is not None:\n            x = x.type(dtype)\n    if isinstance(x, torch.Tensor):\n        if dtype is not None:\n            x = x.type(dtype)\n        x = x.to(device)\n    elif isinstance(x, dict):\n        for k, v in x.items():\n            x[k] = to_torch(v, dtype, device)\n    elif isinstance(x, Batch):\n        x.to_torch(dtype, device)\n    return x\n\n\ndef to_torch_as(x: Union[torch.Tensor, dict, Batch, np.ndarray],\n                y: torch.Tensor\n                ) -> Union[dict, Batch, torch.Tensor]:\n    """"""Return an object without np.ndarray. Same as\n    ``to_torch(x, dtype=y.dtype, device=y.device)``.\n    """"""\n    assert isinstance(y, torch.Tensor)\n    return to_torch(x, dtype=y.dtype, device=y.device)\n'"
tianshou/env/__init__.py,0,"b""from tianshou.env.vecenv import BaseVectorEnv, VectorEnv, \\\n    SubprocVectorEnv, RayVectorEnv\n\n__all__ = [\n    'BaseVectorEnv',\n    'VectorEnv',\n    'SubprocVectorEnv',\n    'RayVectorEnv',\n]\n"""
tianshou/env/atari.py,0,"b""import cv2\nimport gym\nimport numpy as np\nfrom gym.spaces.box import Box\n\n\ndef create_atari_environment(name=None, sticky_actions=True,\n                             max_episode_steps=2000):\n    game_version = 'v0' if sticky_actions else 'v4'\n    name = '{}NoFrameskip-{}'.format(name, game_version)\n    env = gym.make(name)\n    env = env.env\n    env = preprocessing(env, max_episode_steps=max_episode_steps)\n    return env\n\n\nclass preprocessing(object):\n    def __init__(self, env, frame_skip=4, terminal_on_life_loss=False,\n                 size=84, max_episode_steps=2000):\n        self.max_episode_steps = max_episode_steps\n        self.env = env\n        self.terminal_on_life_loss = terminal_on_life_loss\n        self.frame_skip = frame_skip\n        self.size = size\n        self.count = 0\n        obs_dims = self.env.observation_space\n\n        self.screen_buffer = [\n            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),\n            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8)\n        ]\n\n        self.game_over = False\n        self.lives = 0\n\n    @property\n    def observation_space(self):\n        return Box(low=0, high=255, shape=(self.size, self.size, 4),\n                   dtype=np.uint8)\n\n    def action_space(self):\n        return self.env.action_space\n\n    def reward_range(self):\n        return self.env.reward_range\n\n    def metadata(self):\n        return self.env.metadata\n\n    def close(self):\n        return self.env.close()\n\n    def reset(self):\n        self.count = 0\n        self.env.reset()\n        self.lives = self.env.ale.lives()\n        self._grayscale_obs(self.screen_buffer[0])\n        self.screen_buffer[1].fill(0)\n\n        return np.stack([\n            self._pool_and_resize() for _ in range(self.frame_skip)], axis=-1)\n\n    def render(self, mode='human'):\n        return self.env.render(mode)\n\n    def step(self, action):\n        total_reward = 0.\n        observation = []\n        for t in range(self.frame_skip):\n            self.count += 1\n            _, reward, terminal, info = self.env.step(action)\n            total_reward += reward\n\n            if self.terminal_on_life_loss:\n                lives = self.env.ale.lives()\n                is_terminal = terminal or lives < self.lives\n                self.lives = lives\n            else:\n                is_terminal = terminal\n\n            if is_terminal:\n                break\n            elif t >= self.frame_skip - 2:\n                t_ = t - (self.frame_skip - 2)\n                self._grayscale_obs(self.screen_buffer[t_])\n\n            observation.append(self._pool_and_resize())\n        while len(observation) > 0 and len(observation) < self.frame_skip:\n            observation.append(observation[-1])\n        if len(observation) > 0:\n            observation = np.stack(observation, axis=-1)\n        else:\n            observation = np.stack([\n                self._pool_and_resize() for _ in range(self.frame_skip)],\n                axis=-1)\n        if self.count >= self.max_episode_steps:\n            terminal = True\n        else:\n            terminal = False\n        return observation, total_reward, (terminal or is_terminal), info\n\n    def _grayscale_obs(self, output):\n        self.env.ale.getScreenGrayscale(output)\n        return output\n\n    def _pool_and_resize(self):\n        if self.frame_skip > 1:\n            np.maximum(self.screen_buffer[0], self.screen_buffer[1],\n                       out=self.screen_buffer[0])\n\n        transformed_image = cv2.resize(self.screen_buffer[0],\n                                       (self.size, self.size),\n                                       interpolation=cv2.INTER_AREA)\n        int_image = np.asarray(transformed_image, dtype=np.uint8)\n        # return np.expand_dims(int_image, axis=2)\n        return int_image\n"""
tianshou/env/utils.py,0,"b'import cloudpickle\n\n\nclass CloudpickleWrapper(object):\n    """"""A cloudpickle wrapper used in :class:`~tianshou.env.SubprocVectorEnv`""""""\n\n    def __init__(self, data):\n        self.data = data\n\n    def __getstate__(self):\n        return cloudpickle.dumps(self.data)\n\n    def __setstate__(self, data):\n        self.data = cloudpickle.loads(data)\n'"
tianshou/env/vecenv.py,0,"b'import gym\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom multiprocessing import Process, Pipe\nfrom typing import List, Tuple, Union, Optional, Callable\n\ntry:\n    import ray\nexcept ImportError:\n    pass\n\nfrom tianshou.env.utils import CloudpickleWrapper\n\n\nclass BaseVectorEnv(ABC, gym.Env):\n    """"""Base class for vectorized environments wrapper. Usage:\n    ::\n\n        env_num = 8\n        envs = VectorEnv([lambda: gym.make(task) for _ in range(env_num)])\n        assert len(envs) == env_num\n\n    It accepts a list of environment generators. In other words, an environment\n    generator ``efn`` of a specific task means that ``efn()`` returns the\n    environment of the given task, for example, ``gym.make(task)``.\n\n    All of the VectorEnv must inherit :class:`~tianshou.env.BaseVectorEnv`.\n    Here are some other usages:\n    ::\n\n        envs.seed(2)  # which is equal to the next line\n        envs.seed([2, 3, 4, 5, 6, 7, 8, 9])  # set specific seed for each env\n        obs = envs.reset()  # reset all environments\n        obs = envs.reset([0, 5, 7])  # reset 3 specific environments\n        obs, rew, done, info = envs.step([1] * 8)  # step synchronously\n        envs.render()  # render all environments\n        envs.close()  # close all environments\n    """"""\n\n    def __init__(self, env_fns: List[Callable[[], gym.Env]]) -> None:\n        self._env_fns = env_fns\n        self.env_num = len(env_fns)\n        self._obs = None\n        self._rew = None\n        self._done = None\n        self._info = None\n\n    def __len__(self) -> int:\n        """"""Return len(self), which is the number of environments.""""""\n        return self.env_num\n\n    def __getattribute__(self, key):\n        """"""Switch between the default attribute getter or one\n           looking at wrapped environment level depending on the key.""""""\n        if key not in (\'observation_space\', \'action_space\'):\n            return super().__getattribute__(key)\n        else:\n            return self.__getattr__(key)\n\n    @abstractmethod\n    def __getattr__(self, key):\n        """"""Try to retrieve an attribute from each individual wrapped\n           environment, if it does not belong to the wrapping vector\n           environment class.""""""\n        pass\n\n    @abstractmethod\n    def reset(self, id: Optional[Union[int, List[int]]] = None):\n        """"""Reset the state of all the environments and return initial\n        observations if id is ``None``, otherwise reset the specific\n        environments with given id, either an int or a list.\n        """"""\n        pass\n\n    @abstractmethod\n    def step(self, action: np.ndarray\n             ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        """"""Run one timestep of all the environments\xe2\x80\x99 dynamics. When the end of\n        episode is reached, you are responsible for calling reset(id) to reset\n        this environment\xe2\x80\x99s state.\n\n        Accept a batch of action and return a tuple (obs, rew, done, info).\n\n        :param numpy.ndarray action: a batch of action provided by the agent.\n\n        :return: A tuple including four items:\n\n            * ``obs`` a numpy.ndarray, the agent\'s observation of current \\\n                environments\n            * ``rew`` a numpy.ndarray, the amount of rewards returned after \\\n                previous actions\n            * ``done`` a numpy.ndarray, whether these episodes have ended, in \\\n                which case further step() calls will return undefined results\n            * ``info`` a numpy.ndarray, contains auxiliary diagnostic \\\n                information (helpful for debugging, and sometimes learning)\n        """"""\n        pass\n\n    @abstractmethod\n    def seed(self, seed: Optional[Union[int, List[int]]] = None) -> List[int]:\n        """"""Set the seed for all environments.\n\n        Accept ``None``, an int (which will extend ``i`` to\n        ``[i, i + 1, i + 2, ...]``) or a list.\n\n        :return: The list of seeds used in this env\'s random number \\\n        generators. The first value in the list should be the ""main"" seed, or \\\n        the value which a reproducer pass to ""seed"".\n        """"""\n        pass\n\n    @abstractmethod\n    def render(self, **kwargs) -> None:\n        """"""Render all of the environments.""""""\n        pass\n\n    @abstractmethod\n    def close(self) -> None:\n        """"""Close all of the environments.\n\n        Environments will automatically close() themselves when garbage\n        collected or when the program exits.\n        """"""\n        pass\n\n\nclass VectorEnv(BaseVectorEnv):\n    """"""Dummy vectorized environment wrapper, implemented in for-loop.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.env.BaseVectorEnv` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self, env_fns: List[Callable[[], gym.Env]]) -> None:\n        super().__init__(env_fns)\n        self.envs = [_() for _ in env_fns]\n\n    def __getattr__(self, key):\n        return [getattr(env, key) if hasattr(env, key) else None\n                for env in self.envs]\n\n    def reset(self, id: Optional[Union[int, List[int]]] = None) -> None:\n        if id is None:\n            self._obs = np.stack([e.reset() for e in self.envs])\n        else:\n            if np.isscalar(id):\n                id = [id]\n            for i in id:\n                self._obs[i] = self.envs[i].reset()\n        return self._obs\n\n    def step(self, action: np.ndarray\n             ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        assert len(action) == self.env_num\n        result = [e.step(a) for e, a in zip(self.envs, action)]\n        self._obs, self._rew, self._done, self._info = zip(*result)\n        self._obs = np.stack(self._obs)\n        self._rew = np.stack(self._rew)\n        self._done = np.stack(self._done)\n        self._info = np.stack(self._info)\n        return self._obs, self._rew, self._done, self._info\n\n    def seed(self, seed: Optional[Union[int, List[int]]] = None) -> List[int]:\n        if np.isscalar(seed):\n            seed = [seed + _ for _ in range(self.env_num)]\n        elif seed is None:\n            seed = [seed] * self.env_num\n        result = []\n        for e, s in zip(self.envs, seed):\n            if hasattr(e, \'seed\'):\n                result.append(e.seed(s))\n        return result\n\n    def render(self, **kwargs) -> None:\n        result = []\n        for e in self.envs:\n            if hasattr(e, \'render\'):\n                result.append(e.render(**kwargs))\n        return result\n\n    def close(self) -> None:\n        return [e.close() for e in self.envs]\n\n\ndef worker(parent, p, env_fn_wrapper):\n    parent.close()\n    env = env_fn_wrapper.data()\n    try:\n        while True:\n            cmd, data = p.recv()\n            if cmd == \'step\':\n                p.send(env.step(data))\n            elif cmd == \'reset\':\n                p.send(env.reset())\n            elif cmd == \'close\':\n                p.send(env.close())\n                p.close()\n                break\n            elif cmd == \'render\':\n                p.send(env.render(**data) if hasattr(env, \'render\') else None)\n            elif cmd == \'seed\':\n                p.send(env.seed(data) if hasattr(env, \'seed\') else None)\n            elif cmd == \'getattr\':\n                p.send(getattr(env, data) if hasattr(env, data) else None)\n            else:\n                p.close()\n                raise NotImplementedError\n    except KeyboardInterrupt:\n        p.close()\n\n\nclass SubprocVectorEnv(BaseVectorEnv):\n    """"""Vectorized environment wrapper based on subprocess.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.env.BaseVectorEnv` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self, env_fns: List[Callable[[], gym.Env]]) -> None:\n        super().__init__(env_fns)\n        self.closed = False\n        self.parent_remote, self.child_remote = \\\n            zip(*[Pipe() for _ in range(self.env_num)])\n        self.processes = [\n            Process(target=worker, args=(\n                parent, child, CloudpickleWrapper(env_fn)), daemon=True)\n            for (parent, child, env_fn) in zip(\n                self.parent_remote, self.child_remote, env_fns)\n        ]\n        for p in self.processes:\n            p.start()\n        for c in self.child_remote:\n            c.close()\n\n    def __getattr__(self, key):\n        for p in self.parent_remote:\n            p.send([\'getattr\', key])\n        return [p.recv() for p in self.parent_remote]\n\n    def step(self, action: np.ndarray\n             ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        assert len(action) == self.env_num\n        for p, a in zip(self.parent_remote, action):\n            p.send([\'step\', a])\n        result = [p.recv() for p in self.parent_remote]\n        self._obs, self._rew, self._done, self._info = zip(*result)\n        self._obs = np.stack(self._obs)\n        self._rew = np.stack(self._rew)\n        self._done = np.stack(self._done)\n        self._info = np.stack(self._info)\n        return self._obs, self._rew, self._done, self._info\n\n    def reset(self, id: Optional[Union[int, List[int]]] = None) -> None:\n        if id is None:\n            for p in self.parent_remote:\n                p.send([\'reset\', None])\n            self._obs = np.stack([p.recv() for p in self.parent_remote])\n            return self._obs\n        else:\n            if np.isscalar(id):\n                id = [id]\n            for i in id:\n                self.parent_remote[i].send([\'reset\', None])\n            for i in id:\n                self._obs[i] = self.parent_remote[i].recv()\n            return self._obs\n\n    def seed(self, seed: Optional[Union[int, List[int]]] = None) -> List[int]:\n        if np.isscalar(seed):\n            seed = [seed + _ for _ in range(self.env_num)]\n        elif seed is None:\n            seed = [seed] * self.env_num\n        for p, s in zip(self.parent_remote, seed):\n            p.send([\'seed\', s])\n        return [p.recv() for p in self.parent_remote]\n\n    def render(self, **kwargs) -> None:\n        for p in self.parent_remote:\n            p.send([\'render\', kwargs])\n        return [p.recv() for p in self.parent_remote]\n\n    def close(self) -> None:\n        if self.closed:\n            return\n        for p in self.parent_remote:\n            p.send([\'close\', None])\n        result = [p.recv() for p in self.parent_remote]\n        self.closed = True\n        for p in self.processes:\n            p.join()\n        return result\n\n\nclass RayVectorEnv(BaseVectorEnv):\n    """"""Vectorized environment wrapper based on\n    `ray <https://github.com/ray-project/ray>`_. However, according to our\n    test, it is about two times slower than\n    :class:`~tianshou.env.SubprocVectorEnv`.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.env.BaseVectorEnv` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self, env_fns: List[Callable[[], gym.Env]]) -> None:\n        super().__init__(env_fns)\n        try:\n            if not ray.is_initialized():\n                ray.init()\n        except NameError:\n            raise ImportError(\n                \'Please install ray to support RayVectorEnv: pip3 install ray\')\n        self.envs = [\n            ray.remote(gym.Wrapper).options(num_cpus=0).remote(e())\n            for e in env_fns]\n\n    def __getattr__(self, key):\n        return ray.get([e.getattr.remote(key) for e in self.envs])\n\n    def step(self, action: np.ndarray\n             ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        assert len(action) == self.env_num\n        result = ray.get([e.step.remote(a) for e, a in zip(self.envs, action)])\n        self._obs, self._rew, self._done, self._info = zip(*result)\n        self._obs = np.stack(self._obs)\n        self._rew = np.stack(self._rew)\n        self._done = np.stack(self._done)\n        self._info = np.stack(self._info)\n        return self._obs, self._rew, self._done, self._info\n\n    def reset(self, id: Optional[Union[int, List[int]]] = None) -> None:\n        if id is None:\n            result_obj = [e.reset.remote() for e in self.envs]\n            self._obs = np.stack(ray.get(result_obj))\n        else:\n            result_obj = []\n            if np.isscalar(id):\n                id = [id]\n            for i in id:\n                result_obj.append(self.envs[i].reset.remote())\n            for _, i in enumerate(id):\n                self._obs[i] = ray.get(result_obj[_])\n        return self._obs\n\n    def seed(self, seed: Optional[Union[int, List[int]]] = None) -> List[int]:\n        if not hasattr(self.envs[0], \'seed\'):\n            return\n        if np.isscalar(seed):\n            seed = [seed + _ for _ in range(self.env_num)]\n        elif seed is None:\n            seed = [seed] * self.env_num\n        return ray.get([e.seed.remote(s) for e, s in zip(self.envs, seed)])\n\n    def render(self, **kwargs) -> None:\n        if not hasattr(self.envs[0], \'render\'):\n            return\n        return ray.get([e.render.remote(**kwargs) for e in self.envs])\n\n    def close(self) -> None:\n        return ray.get([e.close.remote() for e in self.envs])\n'"
tianshou/exploration/__init__.py,0,"b""from tianshou.exploration.random import OUNoise\n\n__all__ = [\n    'OUNoise',\n]\n"""
tianshou/exploration/random.py,0,"b'import numpy as np\nfrom typing import Union, Optional\n\n\nclass OUNoise(object):\n    """"""Class for Ornstein-Uhlenbeck process, as used for exploration in DDPG.\n    Usage:\n    ::\n\n        # init\n        self.noise = OUNoise()\n        # generate noise\n        noise = self.noise(logits.shape, eps)\n\n    For required parameters, you can refer to the stackoverflow page. However,\n    our experiment result shows that (similar to OpenAI SpinningUp) using\n    vanilla gaussian process has little difference from using the\n    Ornstein-Uhlenbeck process.\n    """"""\n\n    def __init__(self,\n                 sigma: float = 0.3,\n                 theta: float = 0.15,\n                 dt: float = 1e-2,\n                 x0: Optional[Union[float, np.ndarray]] = None\n                 ) -> None:\n        self.alpha = theta * dt\n        self.beta = sigma * np.sqrt(dt)\n        self.x0 = x0\n        self.reset()\n\n    def __call__(self, size: tuple, mu: float = .1) -> np.ndarray:\n        """"""Generate new noise. Return a ``numpy.ndarray`` which size is equal\n        to ``size``.\n        """"""\n        if self.x is None or self.x.shape != size:\n            self.x = 0\n        r = self.beta * np.random.normal(size=size)\n        self.x = self.x + self.alpha * (mu - self.x) + r\n        return self.x\n\n    def reset(self) -> None:\n        """"""Reset to the initial state.""""""\n        self.x = None\n'"
tianshou/policy/__init__.py,0,"b""from tianshou.policy.base import BasePolicy\nfrom tianshou.policy.imitation.base import ImitationPolicy\nfrom tianshou.policy.modelfree.dqn import DQNPolicy\nfrom tianshou.policy.modelfree.pg import PGPolicy\nfrom tianshou.policy.modelfree.a2c import A2CPolicy\nfrom tianshou.policy.modelfree.ddpg import DDPGPolicy\nfrom tianshou.policy.modelfree.ppo import PPOPolicy\nfrom tianshou.policy.modelfree.td3 import TD3Policy\nfrom tianshou.policy.modelfree.sac import SACPolicy\n\n__all__ = [\n    'BasePolicy',\n    'ImitationPolicy',\n    'DQNPolicy',\n    'PGPolicy',\n    'A2CPolicy',\n    'DDPGPolicy',\n    'PPOPolicy',\n    'TD3Policy',\n    'SACPolicy',\n]\n"""
tianshou/policy/base.py,12,"b'import torch\nimport numpy as np\nfrom torch import nn\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Union, Optional, Callable\n\nfrom tianshou.data import Batch, ReplayBuffer, to_torch_as\n\n\nclass BasePolicy(ABC, nn.Module):\n    """"""Tianshou aims to modularizing RL algorithms. It comes into several\n    classes of policies in Tianshou. All of the policy classes must inherit\n    :class:`~tianshou.policy.BasePolicy`.\n\n    A policy class typically has four parts:\n\n    * :meth:`~tianshou.policy.BasePolicy.__init__`: initialize the policy, \\\n        including coping the target network and so on;\n    * :meth:`~tianshou.policy.BasePolicy.forward`: compute action with given \\\n        observation;\n    * :meth:`~tianshou.policy.BasePolicy.process_fn`: pre-process data from \\\n        the replay buffer (this function can interact with replay buffer);\n    * :meth:`~tianshou.policy.BasePolicy.learn`: update policy with a given \\\n        batch of data.\n\n    Most of the policy needs a neural network to predict the action and an\n    optimizer to optimize the policy. The rules of self-defined networks are:\n\n    1. Input: observation ``obs`` (may be a ``numpy.ndarray``, a \\\n        ``torch.Tensor``, a dict or any others), hidden state ``state`` (for \\\n        RNN usage), and other information ``info`` provided by the \\\n        environment.\n    2. Output: some ``logits``, the next hidden state ``state``, and the \\\n        intermediate result during policy forwarding procedure ``policy``. The\\\n        ``logits`` could be a tuple instead of a ``torch.Tensor``. It depends \\\n        on how the policy process the network output. For example, in PPO, the\\\n        return of the network might be ``(mu, sigma), state`` for Gaussian \\\n        policy. The ``policy`` can be a Batch of torch.Tensor or other things,\\\n        which will be stored in the replay buffer, and can be accessed in the \\\n        policy update process (e.g. in ``policy.learn()``, the \\\n        ``batch.policy`` is what you need).\n\n    Since :class:`~tianshou.policy.BasePolicy` inherits ``torch.nn.Module``,\n    you can use :class:`~tianshou.policy.BasePolicy` almost the same as\n    ``torch.nn.Module``, for instance, loading and saving the model:\n    ::\n\n        torch.save(policy.state_dict(), \'policy.pth\')\n        policy.load_state_dict(torch.load(\'policy.pth\'))\n    """"""\n\n    def __init__(self, **kwargs) -> None:\n        super().__init__()\n        self.observation_space = kwargs.get(\'observation_space\')\n        self.action_space = kwargs.get(\'action_space\')\n\n    def process_fn(self, batch: Batch, buffer: ReplayBuffer,\n                   indice: np.ndarray) -> Batch:\n        """"""Pre-process the data from the provided replay buffer. Check out\n        :ref:`policy_concept` for more information.\n        """"""\n        return batch\n\n    @abstractmethod\n    def forward(self, batch: Batch,\n                state: Optional[Union[dict, Batch, np.ndarray]] = None,\n                **kwargs) -> Batch:\n        """"""Compute action over the given batch data.\n\n        :return: A :class:`~tianshou.data.Batch` which MUST have the following\\\n        keys:\n\n            * ``act`` an numpy.ndarray or a torch.Tensor, the action over \\\n                given batch data.\n            * ``state`` a dict, an numpy.ndarray or a torch.Tensor, the \\\n                internal state of the policy, ``None`` as default.\n\n        Other keys are user-defined. It depends on the algorithm. For example,\n        ::\n\n            # some code\n            return Batch(logits=..., act=..., state=None, dist=...)\n\n        After version >= 0.2.3, the keyword ""policy"" is reserverd and the\n        corresponding data will be stored into the replay buffer in numpy. For\n        instance,\n        ::\n\n            # some code\n            return Batch(..., policy=Batch(log_prob=dist.log_prob(act)))\n            # and in the sampled data batch, you can directly call\n            # batch.policy.log_prob to get your data, although it is stored in\n            # np.ndarray.\n        """"""\n        pass\n\n    @abstractmethod\n    def learn(self, batch: Batch, **kwargs\n              ) -> Dict[str, Union[float, List[float]]]:\n        """"""Update policy with a given batch of data.\n\n        :return: A dict which includes loss and its corresponding label.\n        """"""\n        pass\n\n    @staticmethod\n    def compute_episodic_return(\n            batch: Batch,\n            v_s_: Optional[Union[np.ndarray, torch.Tensor]] = None,\n            gamma: float = 0.99,\n            gae_lambda: float = 0.95) -> Batch:\n        """"""Compute returns over given full-length episodes, including the\n        implementation of Generalized Advantage Estimator (arXiv:1506.02438).\n\n        :param batch: a data batch which contains several full-episode data\n            chronologically.\n        :type batch: :class:`~tianshou.data.Batch`\n        :param v_s_: the value function of all next states :math:`V(s\')`.\n        :type v_s_: numpy.ndarray\n        :param float gamma: the discount factor, should be in [0, 1], defaults\n            to 0.99.\n        :param float gae_lambda: the parameter for Generalized Advantage\n            Estimation, should be in [0, 1], defaults to 0.95.\n\n        :return: a Batch. The result will be stored in batch.returns.\n        """"""\n        if v_s_ is None:\n            v_s_ = np.zeros_like(batch.rew)\n        else:\n            if not isinstance(v_s_, np.ndarray):\n                v_s_ = np.array(v_s_, np.float)\n            v_s_ = v_s_.reshape(batch.rew.shape)\n        returns = np.roll(v_s_, 1, axis=0)\n        m = (1. - batch.done) * gamma\n        delta = batch.rew + v_s_ * m - returns\n        m *= gae_lambda\n        gae = 0.\n        for i in range(len(batch.rew) - 1, -1, -1):\n            gae = delta[i] + m[i] * gae\n            returns[i] += gae\n        batch.returns = returns\n        return batch\n\n    @staticmethod\n    def compute_nstep_return(\n        batch: Batch,\n        buffer: ReplayBuffer,\n        indice: np.ndarray,\n        target_q_fn: Callable[[ReplayBuffer, np.ndarray], torch.Tensor],\n        gamma: float = 0.99,\n        n_step: int = 1,\n        rew_norm: bool = False\n    ) -> np.ndarray:\n        r""""""Compute n-step return for Q-learning targets:\n\n        .. math::\n            G_t = \\sum_{i = t}^{t + n - 1} \\gamma^{i - t}(1 - d_i)r_i +\n            \\gamma^n (1 - d_{t + n}) Q_{\\mathrm{target}}(s_{t + n})\n\n        , where :math:`\\gamma` is the discount factor,\n        :math:`\\gamma \\in [0, 1]`, :math:`d_t` is the done flag of step\n        :math:`t`.\n\n        :param batch: a data batch, which is equal to buffer[indice].\n        :type batch: :class:`~tianshou.data.Batch`\n        :param buffer: a data buffer which contains several full-episode data\n            chronologically.\n        :type buffer: :class:`~tianshou.data.ReplayBuffer`\n        :param indice: sampled timestep.\n        :type indice: numpy.ndarray\n        :param function target_q_fn: a function receives :math:`t+n-1` step\'s\n            data and compute target Q value.\n        :param float gamma: the discount factor, should be in [0, 1], defaults\n            to 0.99.\n        :param int n_step: the number of estimation step, should be an int\n            greater than 0, defaults to 1.\n        :param bool rew_norm: normalize the reward to Normal(0, 1), defaults\n            to ``False``.\n\n        :return: a Batch. The result will be stored in batch.returns as a\n            torch.Tensor with shape (bsz, ).\n        """"""\n        if rew_norm:\n            bfr = buffer.rew[:min(len(buffer), 1000)]  # avoid large buffer\n            mean, std = bfr.mean(), bfr.std()\n            if np.isclose(std, 0):\n                mean, std = 0, 1\n        else:\n            mean, std = 0, 1\n        returns = np.zeros_like(indice)\n        gammas = np.zeros_like(indice) + n_step\n        done, rew, buf_len = buffer.done, buffer.rew, len(buffer)\n        for n in range(n_step - 1, -1, -1):\n            now = (indice + n) % buf_len\n            gammas[done[now] > 0] = n\n            returns[done[now] > 0] = 0\n            returns = (rew[now] - mean) / std + gamma * returns\n        terminal = (indice + n_step - 1) % buf_len\n        target_q = target_q_fn(buffer, terminal).squeeze()\n        target_q[gammas != n_step] = 0\n        returns = to_torch_as(returns, target_q)\n        gammas = to_torch_as(gamma ** gammas, target_q)\n        batch.returns = target_q * gammas + returns\n        return batch\n'"
tianshou/policy/dist.py,1,"b'import torch\n\n\nclass DiagGaussian(torch.distributions.Normal):\n    """"""Diagonal Gaussian distribution.""""""\n\n    def log_prob(self, actions):\n        return super().log_prob(actions).sum(-1, keepdim=True)\n\n    def entropy(self):\n        return super().entropy().sum(-1)\n'"
tianshou/trainer/__init__.py,0,"b""from tianshou.trainer.utils import test_episode, gather_info\nfrom tianshou.trainer.onpolicy import onpolicy_trainer\nfrom tianshou.trainer.offpolicy import offpolicy_trainer\n\n__all__ = [\n    'gather_info',\n    'test_episode',\n    'onpolicy_trainer',\n    'offpolicy_trainer',\n]\n"""
tianshou/trainer/offpolicy.py,2,"b'import time\nimport tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nfrom typing import Dict, List, Union, Callable, Optional\n\nfrom tianshou.data import Collector\nfrom tianshou.policy import BasePolicy\nfrom tianshou.utils import tqdm_config, MovAvg\nfrom tianshou.trainer import test_episode, gather_info\n\n\ndef offpolicy_trainer(\n        policy: BasePolicy,\n        train_collector: Collector,\n        test_collector: Collector,\n        max_epoch: int,\n        step_per_epoch: int,\n        collect_per_step: int,\n        episode_per_test: Union[int, List[int]],\n        batch_size: int,\n        train_fn: Optional[Callable[[int], None]] = None,\n        test_fn: Optional[Callable[[int], None]] = None,\n        stop_fn: Optional[Callable[[float], bool]] = None,\n        save_fn: Optional[Callable[[BasePolicy], None]] = None,\n        log_fn: Optional[Callable[[dict], None]] = None,\n        writer: Optional[SummaryWriter] = None,\n        log_interval: int = 1,\n        verbose: bool = True,\n        **kwargs\n) -> Dict[str, Union[float, str]]:\n    """"""A wrapper for off-policy trainer procedure.\n\n    :param policy: an instance of the :class:`~tianshou.policy.BasePolicy`\n        class.\n    :param train_collector: the collector used for training.\n    :type train_collector: :class:`~tianshou.data.Collector`\n    :param test_collector: the collector used for testing.\n    :type test_collector: :class:`~tianshou.data.Collector`\n    :param int max_epoch: the maximum of epochs for training. The training\n        process might be finished before reaching the ``max_epoch``.\n    :param int step_per_epoch: the number of step for updating policy network\n        in one epoch.\n    :param int collect_per_step: the number of frames the collector would\n        collect before the network update. In other words, collect some frames\n        and do one policy network update.\n    :param episode_per_test: the number of episodes for one policy evaluation.\n    :param int batch_size: the batch size of sample data, which is going to\n        feed in the policy network.\n    :param function train_fn: a function receives the current number of epoch\n        index and performs some operations at the beginning of training in this\n        epoch.\n    :param function test_fn: a function receives the current number of epoch\n        index and performs some operations at the beginning of testing in this\n        epoch.\n    :param function save_fn: a function for saving policy when the undiscounted\n        average mean reward in evaluation phase gets better.\n    :param function stop_fn: a function receives the average undiscounted\n        returns of the testing result, return a boolean which indicates whether\n        reaching the goal.\n    :param function log_fn: a function receives env info for logging.\n    :param torch.utils.tensorboard.SummaryWriter writer: a TensorBoard\n        SummaryWriter.\n    :param int log_interval: the log interval of the writer.\n    :param bool verbose: whether to print the information.\n\n    :return: See :func:`~tianshou.trainer.gather_info`.\n    """"""\n    global_step = 0\n    best_epoch, best_reward = -1, -1\n    stat = {}\n    start_time = time.time()\n    test_in_train = train_collector.policy == policy\n    for epoch in range(1, 1 + max_epoch):\n        # train\n        policy.train()\n        if train_fn:\n            train_fn(epoch)\n        with tqdm.tqdm(total=step_per_epoch, desc=f\'Epoch #{epoch}\',\n                       **tqdm_config) as t:\n            while t.n < t.total:\n                result = train_collector.collect(n_step=collect_per_step,\n                                                 log_fn=log_fn)\n                data = {}\n                if test_in_train and stop_fn and stop_fn(result[\'rew\']):\n                    test_result = test_episode(\n                        policy, test_collector, test_fn,\n                        epoch, episode_per_test)\n                    if stop_fn and stop_fn(test_result[\'rew\']):\n                        if save_fn:\n                            save_fn(policy)\n                        for k in result.keys():\n                            data[k] = f\'{result[k]:.2f}\'\n                        t.set_postfix(**data)\n                        return gather_info(\n                            start_time, train_collector, test_collector,\n                            test_result[\'rew\'])\n                    else:\n                        policy.train()\n                        if train_fn:\n                            train_fn(epoch)\n                for i in range(min(\n                        result[\'n/st\'] // collect_per_step, t.total - t.n)):\n                    global_step += 1\n                    losses = policy.learn(train_collector.sample(batch_size))\n                    for k in result.keys():\n                        data[k] = f\'{result[k]:.2f}\'\n                        if writer and global_step % log_interval == 0:\n                            writer.add_scalar(\n                                k, result[k], global_step=global_step)\n                    for k in losses.keys():\n                        if stat.get(k) is None:\n                            stat[k] = MovAvg()\n                        stat[k].add(losses[k])\n                        data[k] = f\'{stat[k].get():.6f}\'\n                        if writer and global_step % log_interval == 0:\n                            writer.add_scalar(\n                                k, stat[k].get(), global_step=global_step)\n                    t.update(1)\n                    t.set_postfix(**data)\n            if t.n <= t.total:\n                t.update()\n        # test\n        result = test_episode(\n            policy, test_collector, test_fn, epoch, episode_per_test)\n        if best_epoch == -1 or best_reward < result[\'rew\']:\n            best_reward = result[\'rew\']\n            best_epoch = epoch\n            if save_fn:\n                save_fn(policy)\n        if verbose:\n            print(f\'Epoch #{epoch}: test_reward: {result[""rew""]:.6f}, \'\n                  f\'best_reward: {best_reward:.6f} in #{best_epoch}\')\n        if stop_fn and stop_fn(best_reward):\n            break\n    return gather_info(\n        start_time, train_collector, test_collector, best_reward)\n'"
tianshou/trainer/onpolicy.py,2,"b'import time\nimport tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nfrom typing import Dict, List, Union, Callable, Optional\n\nfrom tianshou.data import Collector\nfrom tianshou.policy import BasePolicy\nfrom tianshou.utils import tqdm_config, MovAvg\nfrom tianshou.trainer import test_episode, gather_info\n\n\ndef onpolicy_trainer(\n        policy: BasePolicy,\n        train_collector: Collector,\n        test_collector: Collector,\n        max_epoch: int,\n        step_per_epoch: int,\n        collect_per_step: int,\n        repeat_per_collect: int,\n        episode_per_test: Union[int, List[int]],\n        batch_size: int,\n        train_fn: Optional[Callable[[int], None]] = None,\n        test_fn: Optional[Callable[[int], None]] = None,\n        stop_fn: Optional[Callable[[float], bool]] = None,\n        save_fn: Optional[Callable[[BasePolicy], None]] = None,\n        log_fn: Optional[Callable[[dict], None]] = None,\n        writer: Optional[SummaryWriter] = None,\n        log_interval: int = 1,\n        verbose: bool = True,\n        **kwargs\n) -> Dict[str, Union[float, str]]:\n    """"""A wrapper for on-policy trainer procedure.\n\n    :param policy: an instance of the :class:`~tianshou.policy.BasePolicy`\n        class.\n    :param train_collector: the collector used for training.\n    :type train_collector: :class:`~tianshou.data.Collector`\n    :param test_collector: the collector used for testing.\n    :type test_collector: :class:`~tianshou.data.Collector`\n    :param int max_epoch: the maximum of epochs for training. The training\n        process might be finished before reaching the ``max_epoch``.\n    :param int step_per_epoch: the number of step for updating policy network\n        in one epoch.\n    :param int collect_per_step: the number of frames the collector would\n        collect before the network update. In other words, collect some frames\n        and do one policy network update.\n    :param int repeat_per_collect: the number of repeat time for policy\n        learning, for example, set it to 2 means the policy needs to learn each\n        given batch data twice.\n    :param episode_per_test: the number of episodes for one policy evaluation.\n    :type episode_per_test: int or list of ints\n    :param int batch_size: the batch size of sample data, which is going to\n        feed in the policy network.\n    :param function train_fn: a function receives the current number of epoch\n        index and performs some operations at the beginning of training in this\n        epoch.\n    :param function test_fn: a function receives the current number of epoch\n        index and performs some operations at the beginning of testing in this\n        epoch.\n    :param function save_fn: a function for saving policy when the undiscounted\n        average mean reward in evaluation phase gets better.\n    :param function stop_fn: a function receives the average undiscounted\n        returns of the testing result, return a boolean which indicates whether\n        reaching the goal.\n    :param function log_fn: a function receives env info for logging.\n    :param torch.utils.tensorboard.SummaryWriter writer: a TensorBoard\n        SummaryWriter.\n    :param int log_interval: the log interval of the writer.\n    :param bool verbose: whether to print the information.\n\n    :return: See :func:`~tianshou.trainer.gather_info`.\n    """"""\n    global_step = 0\n    best_epoch, best_reward = -1, -1\n    stat = {}\n    start_time = time.time()\n    test_in_train = train_collector.policy == policy\n    for epoch in range(1, 1 + max_epoch):\n        # train\n        policy.train()\n        if train_fn:\n            train_fn(epoch)\n        with tqdm.tqdm(total=step_per_epoch, desc=f\'Epoch #{epoch}\',\n                       **tqdm_config) as t:\n            while t.n < t.total:\n                result = train_collector.collect(n_episode=collect_per_step,\n                                                 log_fn=log_fn)\n                data = {}\n                if test_in_train and stop_fn and stop_fn(result[\'rew\']):\n                    test_result = test_episode(\n                        policy, test_collector, test_fn,\n                        epoch, episode_per_test)\n                    if stop_fn and stop_fn(test_result[\'rew\']):\n                        if save_fn:\n                            save_fn(policy)\n                        for k in result.keys():\n                            data[k] = f\'{result[k]:.2f}\'\n                        t.set_postfix(**data)\n                        return gather_info(\n                            start_time, train_collector, test_collector,\n                            test_result[\'rew\'])\n                    else:\n                        policy.train()\n                        if train_fn:\n                            train_fn(epoch)\n                losses = policy.learn(\n                    train_collector.sample(0), batch_size, repeat_per_collect)\n                train_collector.reset_buffer()\n                step = 1\n                for k in losses.keys():\n                    if isinstance(losses[k], list):\n                        step = max(step, len(losses[k]))\n                global_step += step\n                for k in result.keys():\n                    data[k] = f\'{result[k]:.2f}\'\n                    if writer and global_step % log_interval == 0:\n                        writer.add_scalar(\n                            k, result[k], global_step=global_step)\n                for k in losses.keys():\n                    if stat.get(k) is None:\n                        stat[k] = MovAvg()\n                    stat[k].add(losses[k])\n                    data[k] = f\'{stat[k].get():.6f}\'\n                    if writer and global_step % log_interval == 0:\n                        writer.add_scalar(\n                            k, stat[k].get(), global_step=global_step)\n                t.update(step)\n                t.set_postfix(**data)\n            if t.n <= t.total:\n                t.update()\n        # test\n        result = test_episode(\n            policy, test_collector, test_fn, epoch, episode_per_test)\n        if best_epoch == -1 or best_reward < result[\'rew\']:\n            best_reward = result[\'rew\']\n            best_epoch = epoch\n            if save_fn:\n                save_fn(policy)\n        if verbose:\n            print(f\'Epoch #{epoch}: test_reward: {result[""rew""]:.6f}, \'\n                  f\'best_reward: {best_reward:.6f} in #{best_epoch}\')\n        if stop_fn and stop_fn(best_reward):\n            break\n    return gather_info(\n        start_time, train_collector, test_collector, best_reward)\n'"
tianshou/trainer/utils.py,0,"b'import time\nimport numpy as np\nfrom typing import Dict, List, Union, Callable\n\nfrom tianshou.data import Collector\nfrom tianshou.policy import BasePolicy\n\n\ndef test_episode(\n        policy: BasePolicy,\n        collector: Collector,\n        test_fn: Callable[[int], None],\n        epoch: int,\n        n_episode: Union[int, List[int]]) -> Dict[str, float]:\n    """"""A simple wrapper of testing policy in collector.""""""\n    collector.reset_env()\n    collector.reset_buffer()\n    policy.eval()\n    if test_fn:\n        test_fn(epoch)\n    if collector.get_env_num() > 1 and np.isscalar(n_episode):\n        n = collector.get_env_num()\n        n_ = np.zeros(n) + n_episode // n\n        n_[:n_episode % n] += 1\n        n_episode = list(n_)\n    return collector.collect(n_episode=n_episode)\n\n\ndef gather_info(start_time: float,\n                train_c: Collector,\n                test_c: Collector,\n                best_reward: float\n                ) -> Dict[str, Union[float, str]]:\n    """"""A simple wrapper of gathering information from collectors.\n\n    :return: A dictionary with the following keys:\n\n        * ``train_step`` the total collected step of training collector;\n        * ``train_episode`` the total collected episode of training collector;\n        * ``train_time/collector`` the time for collecting frames in the \\\n            training collector;\n        * ``train_time/model`` the time for training models;\n        * ``train_speed`` the speed of training (frames per second);\n        * ``test_step`` the total collected step of test collector;\n        * ``test_episode`` the total collected episode of test collector;\n        * ``test_time`` the time for testing;\n        * ``test_speed`` the speed of testing (frames per second);\n        * ``best_reward`` the best reward over the test results;\n        * ``duration`` the total elapsed time.\n    """"""\n    duration = time.time() - start_time\n    model_time = duration - train_c.collect_time - test_c.collect_time\n    train_speed = train_c.collect_step / (duration - test_c.collect_time)\n    test_speed = test_c.collect_step / test_c.collect_time\n    return {\n        \'train_step\': train_c.collect_step,\n        \'train_episode\': train_c.collect_episode,\n        \'train_time/collector\': f\'{train_c.collect_time:.2f}s\',\n        \'train_time/model\': f\'{model_time:.2f}s\',\n        \'train_speed\': f\'{train_speed:.2f} step/s\',\n        \'test_step\': test_c.collect_step,\n        \'test_episode\': test_c.collect_episode,\n        \'test_time\': f\'{test_c.collect_time:.2f}s\',\n        \'test_speed\': f\'{test_speed:.2f} step/s\',\n        \'best_reward\': best_reward,\n        \'duration\': f\'{duration:.2f}s\',\n    }\n'"
tianshou/utils/__init__.py,0,"b""from tianshou.utils.config import tqdm_config\nfrom tianshou.utils.moving_average import MovAvg\n\n__all__ = [\n    'MovAvg',\n    'tqdm_config',\n]\n"""
tianshou/utils/config.py,0,"b""tqdm_config = {\n    'dynamic_ncols': True,\n    'ascii': True,\n}\n"""
tianshou/utils/moving_average.py,4,"b'import torch\nimport numpy as np\nfrom typing import Union\n\nfrom tianshou.data import to_numpy\n\n\nclass MovAvg(object):\n    """"""Class for moving average. It will automatically exclude the infinity and\n    NaN. Usage:\n    ::\n\n        >>> stat = MovAvg(size=66)\n        >>> stat.add(torch.tensor(5))\n        5.0\n        >>> stat.add(float(\'inf\'))  # which will not add to stat\n        5.0\n        >>> stat.add([6, 7, 8])\n        6.5\n        >>> stat.get()\n        6.5\n        >>> print(f\'{stat.mean():.2f}\xc2\xb1{stat.std():.2f}\')\n        6.50\xc2\xb11.12\n    """"""\n\n    def __init__(self, size: int = 100) -> None:\n        super().__init__()\n        self.size = size\n        self.cache = []\n        self.banned = [np.inf, np.nan, -np.inf]\n\n    def add(self, x: Union[float, list, np.ndarray, torch.Tensor]) -> float:\n        """"""Add a scalar into :class:`MovAvg`. You can add ``torch.Tensor`` with\n        only one element, a python scalar, or a list of python scalar.\n        """"""\n        if isinstance(x, torch.Tensor):\n            x = to_numpy(x.flatten())\n        if isinstance(x, list) or isinstance(x, np.ndarray):\n            for _ in x:\n                if _ not in self.banned:\n                    self.cache.append(_)\n        elif x not in self.banned:\n            self.cache.append(x)\n        if self.size > 0 and len(self.cache) > self.size:\n            self.cache = self.cache[-self.size:]\n        return self.get()\n\n    def get(self) -> float:\n        """"""Get the average.""""""\n        if len(self.cache) == 0:\n            return 0\n        return np.mean(self.cache)\n\n    def mean(self) -> float:\n        """"""Get the average. Same as :meth:`get`.""""""\n        return self.get()\n\n    def std(self) -> float:\n        """"""Get the standard deviation.""""""\n        if len(self.cache) == 0:\n            return 0\n        return np.std(self.cache)\n'"
tianshou/policy/imitation/__init__.py,0,b''
tianshou/policy/imitation/base.py,6,"b'import torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom typing import Dict, Union, Optional\n\nfrom tianshou.data import Batch, to_torch\nfrom tianshou.policy import BasePolicy\n\n\nclass ImitationPolicy(BasePolicy):\n    """"""Implementation of vanilla imitation learning (for continuous action space).\n\n    :param torch.nn.Module model: a model following the rules in\n        :class:`~tianshou.policy.BasePolicy`. (s -> a)\n    :param torch.optim.Optimizer optim: for optimizing the model.\n    :param str mode: indicate the imitation type (""continuous"" or ""discrete""\n        action space), defaults to ""continuous"".\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.policy.BasePolicy` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self, model: torch.nn.Module, optim: torch.optim.Optimizer,\n                 mode: str = \'continuous\', **kwargs) -> None:\n        super().__init__()\n        self.model = model\n        self.optim = optim\n        assert mode in [\'continuous\', \'discrete\'], \\\n            f\'Mode {mode} is not in [""continuous"", ""discrete""]\'\n        self.mode = mode\n\n    def forward(self,\n                batch: Batch,\n                state: Optional[Union[dict, Batch, np.ndarray]] = None,\n                **kwargs) -> Batch:\n        logits, h = self.model(batch.obs, state=state, info=batch.info)\n        if self.mode == \'discrete\':\n            a = logits.max(dim=1)[1]\n        else:\n            a = logits\n        return Batch(logits=logits, act=a, state=h)\n\n    def learn(self, batch: Batch, **kwargs) -> Dict[str, float]:\n        self.optim.zero_grad()\n        if self.mode == \'continuous\':\n            a = self(batch).act\n            a_ = to_torch(batch.act, dtype=torch.float, device=a.device)\n            loss = F.mse_loss(a, a_)\n        elif self.mode == \'discrete\':  # classification\n            a = self(batch).logits\n            a_ = to_torch(batch.act, dtype=torch.long, device=a.device)\n            loss = F.nll_loss(a, a_)\n        loss.backward()\n        self.optim.step()\n        return {\'loss\': loss.item()}\n'"
tianshou/policy/modelfree/__init__.py,0,b''
tianshou/policy/modelfree/a2c.py,12,"b'import torch\nimport numpy as np\nfrom torch import nn\nimport torch.nn.functional as F\nfrom typing import Dict, List, Union, Optional\n\nfrom tianshou.policy import PGPolicy\nfrom tianshou.data import Batch, ReplayBuffer, to_torch_as, to_numpy\n\n\nclass A2CPolicy(PGPolicy):\n    """"""Implementation of Synchronous Advantage Actor-Critic. arXiv:1602.01783\n\n    :param torch.nn.Module actor: the actor network following the rules in\n        :class:`~tianshou.policy.BasePolicy`. (s -> logits)\n    :param torch.nn.Module critic: the critic network. (s -> V(s))\n    :param torch.optim.Optimizer optim: the optimizer for actor and critic\n        network.\n    :param torch.distributions.Distribution dist_fn: for computing the action,\n        defaults to ``torch.distributions.Categorical``.\n    :param float discount_factor: in [0, 1], defaults to 0.99.\n    :param float vf_coef: weight for value loss, defaults to 0.5.\n    :param float ent_coef: weight for entropy loss, defaults to 0.01.\n    :param float max_grad_norm: clipping gradients in back propagation,\n        defaults to ``None``.\n    :param float gae_lambda: in [0, 1], param for Generalized Advantage\n        Estimation, defaults to 0.95.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.policy.BasePolicy` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self,\n                 actor: torch.nn.Module,\n                 critic: torch.nn.Module,\n                 optim: torch.optim.Optimizer,\n                 dist_fn: torch.distributions.Distribution\n                 = torch.distributions.Categorical,\n                 discount_factor: float = 0.99,\n                 vf_coef: float = .5,\n                 ent_coef: float = .01,\n                 max_grad_norm: Optional[float] = None,\n                 gae_lambda: float = 0.95,\n                 reward_normalization: bool = False,\n                 **kwargs) -> None:\n        super().__init__(None, optim, dist_fn, discount_factor, **kwargs)\n        self.actor = actor\n        self.critic = critic\n        assert 0 <= gae_lambda <= 1, \'GAE lambda should be in [0, 1].\'\n        self._lambda = gae_lambda\n        self._w_vf = vf_coef\n        self._w_ent = ent_coef\n        self._grad_norm = max_grad_norm\n        self._batch = 64\n        self._rew_norm = reward_normalization\n\n    def process_fn(self, batch: Batch, buffer: ReplayBuffer,\n                   indice: np.ndarray) -> Batch:\n        if self._lambda in [0, 1]:\n            return self.compute_episodic_return(\n                batch, None, gamma=self._gamma, gae_lambda=self._lambda)\n        v_ = []\n        with torch.no_grad():\n            for b in batch.split(self._batch, shuffle=False):\n                v_.append(to_numpy(self.critic(b.obs_next)))\n        v_ = np.concatenate(v_, axis=0)\n        return self.compute_episodic_return(\n            batch, v_, gamma=self._gamma, gae_lambda=self._lambda)\n\n    def forward(self, batch: Batch,\n                state: Optional[Union[dict, Batch, np.ndarray]] = None,\n                **kwargs) -> Batch:\n        """"""Compute action over the given batch data.\n\n        :return: A :class:`~tianshou.data.Batch` which has 4 keys:\n\n            * ``act`` the action.\n            * ``logits`` the network\'s raw output.\n            * ``dist`` the action distribution.\n            * ``state`` the hidden state.\n\n        .. seealso::\n\n            Please refer to :meth:`~tianshou.policy.BasePolicy.forward` for\n            more detailed explanation.\n        """"""\n        logits, h = self.actor(batch.obs, state=state, info=batch.info)\n        if isinstance(logits, tuple):\n            dist = self.dist_fn(*logits)\n        else:\n            dist = self.dist_fn(logits)\n        act = dist.sample()\n        return Batch(logits=logits, act=act, state=h, dist=dist)\n\n    def learn(self, batch: Batch, batch_size: int, repeat: int,\n              **kwargs) -> Dict[str, List[float]]:\n        self._batch = batch_size\n        r = batch.returns\n        if self._rew_norm and not np.isclose(r.std(), 0):\n            batch.returns = (r - r.mean()) / r.std()\n        losses, actor_losses, vf_losses, ent_losses = [], [], [], []\n        for _ in range(repeat):\n            for b in batch.split(batch_size):\n                self.optim.zero_grad()\n                dist = self(b).dist\n                v = self.critic(b.obs)\n                a = to_torch_as(b.act, v)\n                r = to_torch_as(b.returns, v)\n                a_loss = -(dist.log_prob(a) * (r - v).detach()).mean()\n                vf_loss = F.mse_loss(r[:, None], v)\n                ent_loss = dist.entropy().mean()\n                loss = a_loss + self._w_vf * vf_loss - self._w_ent * ent_loss\n                loss.backward()\n                if self._grad_norm is not None:\n                    nn.utils.clip_grad_norm_(\n                        list(self.actor.parameters()) +\n                        list(self.critic.parameters()),\n                        max_norm=self._grad_norm)\n                self.optim.step()\n                actor_losses.append(a_loss.item())\n                vf_losses.append(vf_loss.item())\n                ent_losses.append(ent_loss.item())\n                losses.append(loss.item())\n        return {\n            \'loss\': losses,\n            \'loss/actor\': actor_losses,\n            \'loss/vf\': vf_losses,\n            \'loss/ent\': ent_losses,\n        }\n'"
tianshou/policy/modelfree/ddpg.py,12,"b'import torch\nimport numpy as np\nfrom copy import deepcopy\nimport torch.nn.functional as F\nfrom typing import Dict, Tuple, Union, Optional\n\nfrom tianshou.policy import BasePolicy\n# from tianshou.exploration import OUNoise\nfrom tianshou.data import Batch, ReplayBuffer, to_torch_as\n\n\nclass DDPGPolicy(BasePolicy):\n    """"""Implementation of Deep Deterministic Policy Gradient. arXiv:1509.02971\n\n    :param torch.nn.Module actor: the actor network following the rules in\n        :class:`~tianshou.policy.BasePolicy`. (s -> logits)\n    :param torch.optim.Optimizer actor_optim: the optimizer for actor network.\n    :param torch.nn.Module critic: the critic network. (s, a -> Q(s, a))\n    :param torch.optim.Optimizer critic_optim: the optimizer for critic\n        network.\n    :param float tau: param for soft update of the target network, defaults to\n        0.005.\n    :param float gamma: discount factor, in [0, 1], defaults to 0.99.\n    :param float exploration_noise: the noise intensity, add to the action,\n        defaults to 0.1.\n    :param action_range: the action range (minimum, maximum).\n    :type action_range: (float, float)\n    :param bool reward_normalization: normalize the reward to Normal(0, 1),\n        defaults to ``False``.\n    :param bool ignore_done: ignore the done flag while training the policy,\n        defaults to ``False``.\n    :param int estimation_step: greater than 1, the number of steps to look\n        ahead.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.policy.BasePolicy` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self,\n                 actor: torch.nn.Module,\n                 actor_optim: torch.optim.Optimizer,\n                 critic: torch.nn.Module,\n                 critic_optim: torch.optim.Optimizer,\n                 tau: float = 0.005,\n                 gamma: float = 0.99,\n                 exploration_noise: float = 0.1,\n                 action_range: Optional[Tuple[float, float]] = None,\n                 reward_normalization: bool = False,\n                 ignore_done: bool = False,\n                 estimation_step: int = 1,\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n        if actor is not None:\n            self.actor, self.actor_old = actor, deepcopy(actor)\n            self.actor_old.eval()\n            self.actor_optim = actor_optim\n        if critic is not None:\n            self.critic, self.critic_old = critic, deepcopy(critic)\n            self.critic_old.eval()\n            self.critic_optim = critic_optim\n        assert 0 <= tau <= 1, \'tau should in [0, 1]\'\n        self._tau = tau\n        assert 0 <= gamma <= 1, \'gamma should in [0, 1]\'\n        self._gamma = gamma\n        assert 0 <= exploration_noise, \'noise should not be negative\'\n        self._eps = exploration_noise\n        assert action_range is not None\n        self._range = action_range\n        self._action_bias = (action_range[0] + action_range[1]) / 2\n        self._action_scale = (action_range[1] - action_range[0]) / 2\n        # it is only a little difference to use rand_normal\n        # self.noise = OUNoise()\n        self._rm_done = ignore_done\n        self._rew_norm = reward_normalization\n        assert estimation_step > 0, \'estimation_step should greater than 0\'\n        self._n_step = estimation_step\n\n    def set_eps(self, eps: float) -> None:\n        """"""Set the eps for exploration.""""""\n        self._eps = eps\n\n    def train(self) -> None:\n        """"""Set the module in training mode, except for the target network.""""""\n        self.training = True\n        self.actor.train()\n        self.critic.train()\n\n    def eval(self) -> None:\n        """"""Set the module in evaluation mode, except for the target network.""""""\n        self.training = False\n        self.actor.eval()\n        self.critic.eval()\n\n    def sync_weight(self) -> None:\n        """"""Soft-update the weight for the target network.""""""\n        for o, n in zip(self.actor_old.parameters(), self.actor.parameters()):\n            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n        for o, n in zip(\n                self.critic_old.parameters(), self.critic.parameters()):\n            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n\n    def _target_q(self, buffer: ReplayBuffer,\n                  indice: np.ndarray) -> torch.Tensor:\n        batch = buffer[indice]  # batch.obs_next: s_{t+n}\n        with torch.no_grad():\n            target_q = self.critic_old(batch.obs_next, self(\n                batch, model=\'actor_old\', input=\'obs_next\', eps=0).act)\n        return target_q\n\n    def process_fn(self, batch: Batch, buffer: ReplayBuffer,\n                   indice: np.ndarray) -> Batch:\n        if self._rm_done:\n            batch.done = batch.done * 0.\n        batch = self.compute_nstep_return(\n            batch, buffer, indice, self._target_q,\n            self._gamma, self._n_step, self._rew_norm)\n        return batch\n\n    def forward(self, batch: Batch,\n                state: Optional[Union[dict, Batch, np.ndarray]] = None,\n                model: str = \'actor\',\n                input: str = \'obs\',\n                eps: Optional[float] = None,\n                **kwargs) -> Batch:\n        """"""Compute action over the given batch data.\n\n        :param float eps: in [0, 1], for exploration use.\n\n        :return: A :class:`~tianshou.data.Batch` which has 2 keys:\n\n            * ``act`` the action.\n            * ``state`` the hidden state.\n\n        .. seealso::\n\n            Please refer to :meth:`~tianshou.policy.BasePolicy.forward` for\n            more detailed explanation.\n        """"""\n        model = getattr(self, model)\n        obs = getattr(batch, input)\n        logits, h = model(obs, state=state, info=batch.info)\n        logits += self._action_bias\n        if eps is None:\n            eps = self._eps\n        if eps > 0:\n            # noise = np.random.normal(0, eps, size=logits.shape)\n            # logits += to_torch(noise, device=logits.device)\n            # noise = self.noise(logits.shape, eps)\n            logits += torch.randn(\n                size=logits.shape, device=logits.device) * eps\n        logits = logits.clamp(self._range[0], self._range[1])\n        return Batch(act=logits, state=h)\n\n    def learn(self, batch: Batch, **kwargs) -> Dict[str, float]:\n        current_q = self.critic(batch.obs, batch.act)\n        target_q = to_torch_as(batch.returns, current_q)\n        target_q = target_q[:, None]\n        critic_loss = F.mse_loss(current_q, target_q)\n        self.critic_optim.zero_grad()\n        critic_loss.backward()\n        self.critic_optim.step()\n        actor_loss = -self.critic(batch.obs, self(batch, eps=0).act).mean()\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        self.actor_optim.step()\n        self.sync_weight()\n        return {\n            \'loss/actor\': actor_loss.item(),\n            \'loss/critic\': critic_loss.item(),\n        }\n'"
tianshou/policy/modelfree/dqn.py,8,"b'import torch\nimport numpy as np\nfrom copy import deepcopy\nimport torch.nn.functional as F\nfrom typing import Dict, Union, Optional\n\nfrom tianshou.policy import BasePolicy\nfrom tianshou.data import Batch, ReplayBuffer, PrioritizedReplayBuffer, \\\n    to_torch_as, to_numpy\n\n\nclass DQNPolicy(BasePolicy):\n    """"""Implementation of Deep Q Network. arXiv:1312.5602\n    Implementation of Double Q-Learning. arXiv:1509.06461\n\n    :param torch.nn.Module model: a model following the rules in\n        :class:`~tianshou.policy.BasePolicy`. (s -> logits)\n    :param torch.optim.Optimizer optim: a torch.optim for optimizing the model.\n    :param float discount_factor: in [0, 1].\n    :param int estimation_step: greater than 1, the number of steps to look\n        ahead.\n    :param int target_update_freq: the target network update frequency (``0``\n        if you do not use the target network).\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.policy.BasePolicy` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self,\n                 model: torch.nn.Module,\n                 optim: torch.optim.Optimizer,\n                 discount_factor: float = 0.99,\n                 estimation_step: int = 1,\n                 target_update_freq: Optional[int] = 0,\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.model = model\n        self.optim = optim\n        self.eps = 0\n        assert 0 <= discount_factor <= 1, \'discount_factor should in [0, 1]\'\n        self._gamma = discount_factor\n        assert estimation_step > 0, \'estimation_step should greater than 0\'\n        self._n_step = estimation_step\n        self._target = target_update_freq > 0\n        self._freq = target_update_freq\n        self._cnt = 0\n        if self._target:\n            self.model_old = deepcopy(self.model)\n            self.model_old.eval()\n\n    def set_eps(self, eps: float) -> None:\n        """"""Set the eps for epsilon-greedy exploration.""""""\n        self.eps = eps\n\n    def train(self) -> None:\n        """"""Set the module in training mode, except for the target network.""""""\n        self.training = True\n        self.model.train()\n\n    def eval(self) -> None:\n        """"""Set the module in evaluation mode, except for the target network.""""""\n        self.training = False\n        self.model.eval()\n\n    def sync_weight(self) -> None:\n        """"""Synchronize the weight for the target network.""""""\n        self.model_old.load_state_dict(self.model.state_dict())\n\n    def _target_q(self, buffer: ReplayBuffer,\n                  indice: np.ndarray) -> torch.Tensor:\n        batch = buffer[indice]  # batch.obs_next: s_{t+n}\n        if self._target:\n            # target_Q = Q_old(s_, argmax(Q_new(s_, *)))\n            a = self(batch, input=\'obs_next\', eps=0).act\n            with torch.no_grad():\n                target_q = self(\n                    batch, model=\'model_old\', input=\'obs_next\').logits\n            target_q = target_q[np.arange(len(a)), a]\n        else:\n            with torch.no_grad():\n                target_q = self(batch, input=\'obs_next\').logits.max(dim=1)[0]\n        return target_q\n\n    def process_fn(self, batch: Batch, buffer: ReplayBuffer,\n                   indice: np.ndarray) -> Batch:\n        r""""""Compute the n-step return for Q-learning targets:\n\n        .. math::\n            G_t = \\sum_{i = t}^{t + n - 1} \\gamma^{i - t}(1 - d_i)r_i +\n            \\gamma^n (1 - d_{t + n}) \\max_a Q_{old}(s_{t + n}, \\arg\\max_a\n            (Q_{new}(s_{t + n}, a)))\n\n        , where :math:`\\gamma` is the discount factor,\n        :math:`\\gamma \\in [0, 1]`, :math:`d_t` is the done flag of step\n        :math:`t`. If there is no target network, the :math:`Q_{old}` is equal\n        to :math:`Q_{new}`.\n        """"""\n        batch = self.compute_nstep_return(\n            batch, buffer, indice, self._target_q, self._gamma, self._n_step)\n        if isinstance(buffer, PrioritizedReplayBuffer):\n            batch.update_weight = buffer.update_weight\n            batch.indice = indice\n        return batch\n\n    def forward(self, batch: Batch,\n                state: Optional[Union[dict, Batch, np.ndarray]] = None,\n                model: str = \'model\',\n                input: str = \'obs\',\n                eps: Optional[float] = None,\n                **kwargs) -> Batch:\n        """"""Compute action over the given batch data.\n\n        :param float eps: in [0, 1], for epsilon-greedy exploration method.\n\n        :return: A :class:`~tianshou.data.Batch` which has 3 keys:\n\n            * ``act`` the action.\n            * ``logits`` the network\'s raw output.\n            * ``state`` the hidden state.\n\n        .. seealso::\n\n            Please refer to :meth:`~tianshou.policy.BasePolicy.forward` for\n            more detailed explanation.\n        """"""\n        model = getattr(self, model)\n        obs = getattr(batch, input)\n        q, h = model(obs, state=state, info=batch.info)\n        act = to_numpy(q.max(dim=1)[1])\n        # add eps to act\n        if eps is None:\n            eps = self.eps\n        if not np.isclose(eps, 0):\n            for i in range(len(q)):\n                if np.random.rand() < eps:\n                    act[i] = np.random.randint(q.shape[1])\n        return Batch(logits=q, act=act, state=h)\n\n    def learn(self, batch: Batch, **kwargs) -> Dict[str, float]:\n        if self._target and self._cnt % self._freq == 0:\n            self.sync_weight()\n        self.optim.zero_grad()\n        q = self(batch).logits\n        q = q[np.arange(len(q)), batch.act]\n        r = to_torch_as(batch.returns, q)\n        if hasattr(batch, \'update_weight\'):\n            td = r - q\n            batch.update_weight(batch.indice, to_numpy(td))\n            impt_weight = to_torch_as(batch.impt_weight, q)\n            loss = (td.pow(2) * impt_weight).mean()\n        else:\n            loss = F.mse_loss(q, r)\n        loss.backward()\n        self.optim.step()\n        self._cnt += 1\n        return {\'loss\': loss.item()}\n'"
tianshou/policy/modelfree/pg.py,7,"b'import torch\nimport numpy as np\nfrom typing import Dict, List, Union, Optional\n\nfrom tianshou.policy import BasePolicy\nfrom tianshou.data import Batch, ReplayBuffer, to_torch\n\n\nclass PGPolicy(BasePolicy):\n    """"""Implementation of Vanilla Policy Gradient.\n\n    :param torch.nn.Module model: a model following the rules in\n        :class:`~tianshou.policy.BasePolicy`. (s -> logits)\n    :param torch.optim.Optimizer optim: a torch.optim for optimizing the model.\n    :param torch.distributions.Distribution dist_fn: for computing the action.\n    :param float discount_factor: in [0, 1].\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.policy.BasePolicy` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self,\n                 model: torch.nn.Module,\n                 optim: torch.optim.Optimizer,\n                 dist_fn: torch.distributions.Distribution\n                 = torch.distributions.Categorical,\n                 discount_factor: float = 0.99,\n                 reward_normalization: bool = False,\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.model = model\n        self.optim = optim\n        self.dist_fn = dist_fn\n        assert 0 <= discount_factor <= 1, \'discount factor should in [0, 1]\'\n        self._gamma = discount_factor\n        self._rew_norm = reward_normalization\n\n    def process_fn(self, batch: Batch, buffer: ReplayBuffer,\n                   indice: np.ndarray) -> Batch:\n        r""""""Compute the discounted returns for each frame:\n\n        .. math::\n            G_t = \\sum_{i=t}^T \\gamma^{i-t}r_i\n\n        , where :math:`T` is the terminal time step, :math:`\\gamma` is the\n        discount factor, :math:`\\gamma \\in [0, 1]`.\n        """"""\n        # batch.returns = self._vanilla_returns(batch)\n        # batch.returns = self._vectorized_returns(batch)\n        # return batch\n        return self.compute_episodic_return(\n            batch, gamma=self._gamma, gae_lambda=1.)\n\n    def forward(self, batch: Batch,\n                state: Optional[Union[dict, Batch, np.ndarray]] = None,\n                **kwargs) -> Batch:\n        """"""Compute action over the given batch data.\n\n        :return: A :class:`~tianshou.data.Batch` which has 4 keys:\n\n            * ``act`` the action.\n            * ``logits`` the network\'s raw output.\n            * ``dist`` the action distribution.\n            * ``state`` the hidden state.\n\n        .. seealso::\n\n            Please refer to :meth:`~tianshou.policy.BasePolicy.forward` for\n            more detailed explanation.\n        """"""\n        logits, h = self.model(batch.obs, state=state, info=batch.info)\n        if isinstance(logits, tuple):\n            dist = self.dist_fn(*logits)\n        else:\n            dist = self.dist_fn(logits)\n        act = dist.sample()\n        return Batch(logits=logits, act=act, state=h, dist=dist)\n\n    def learn(self, batch: Batch, batch_size: int, repeat: int,\n              **kwargs) -> Dict[str, List[float]]:\n        losses = []\n        r = batch.returns\n        if self._rew_norm and not np.isclose(r.std(), 0):\n            batch.returns = (r - r.mean()) / r.std()\n        for _ in range(repeat):\n            for b in batch.split(batch_size):\n                self.optim.zero_grad()\n                dist = self(b).dist\n                a = to_torch(b.act, device=dist.logits.device)\n                r = to_torch(b.returns, device=dist.logits.device)\n                loss = -(dist.log_prob(a) * r).sum()\n                loss.backward()\n                self.optim.step()\n                losses.append(loss.item())\n        return {\'loss\': losses}\n\n    # def _vanilla_returns(self, batch):\n    #     returns = batch.rew[:]\n    #     last = 0\n    #     for i in range(len(returns) - 1, -1, -1):\n    #         if not batch.done[i]:\n    #             returns[i] += self._gamma * last\n    #         last = returns[i]\n    #     return returns\n\n    # def _vectorized_returns(self, batch):\n    #     # according to my tests, it is slower than _vanilla_returns\n    #     # import scipy.signal\n    #     convolve = np.convolve\n    #     # convolve = scipy.signal.convolve\n    #     rew = batch.rew[::-1]\n    #     batch_size = len(rew)\n    #     gammas = self._gamma ** np.arange(batch_size)\n    #     c = convolve(rew, gammas)[:batch_size]\n    #     T = np.where(batch.done[::-1])[0]\n    #     d = np.zeros_like(rew)\n    #     d[T] += c[T] - rew[T]\n    #     d[T[1:]] -= d[T[:-1]] * self._gamma ** np.diff(T)\n    #     return (c - convolve(d, gammas)[:batch_size])[::-1]\n'"
tianshou/policy/modelfree/ppo.py,16,"b'import torch\nimport numpy as np\nfrom torch import nn\nfrom typing import Dict, List, Tuple, Union, Optional\n\nfrom tianshou.policy import PGPolicy\nfrom tianshou.data import Batch, ReplayBuffer, to_numpy, to_torch_as\n\n\nclass PPOPolicy(PGPolicy):\n    r""""""Implementation of Proximal Policy Optimization. arXiv:1707.06347\n\n    :param torch.nn.Module actor: the actor network following the rules in\n        :class:`~tianshou.policy.BasePolicy`. (s -> logits)\n    :param torch.nn.Module critic: the critic network. (s -> V(s))\n    :param torch.optim.Optimizer optim: the optimizer for actor and critic\n        network.\n    :param torch.distributions.Distribution dist_fn: for computing the action.\n    :param float discount_factor: in [0, 1], defaults to 0.99.\n    :param float max_grad_norm: clipping gradients in back propagation,\n        defaults to ``None``.\n    :param float eps_clip: :math:`\\epsilon` in :math:`L_{CLIP}` in the original\n        paper, defaults to 0.2.\n    :param float vf_coef: weight for value loss, defaults to 0.5.\n    :param float ent_coef: weight for entropy loss, defaults to 0.01.\n    :param action_range: the action range (minimum, maximum).\n    :type action_range: (float, float)\n    :param float gae_lambda: in [0, 1], param for Generalized Advantage\n        Estimation, defaults to 0.95.\n    :param float dual_clip: a parameter c mentioned in arXiv:1912.09729 Equ. 5,\n        where c > 1 is a constant indicating the lower bound,\n        defaults to 5.0 (set ``None`` if you do not want to use it).\n    :param bool value_clip: a parameter mentioned in arXiv:1811.02553 Sec. 4.1,\n        defaults to ``True``.\n    :param bool reward_normalization: normalize the returns to Normal(0, 1),\n        defaults to ``True``.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.policy.BasePolicy` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self,\n                 actor: torch.nn.Module,\n                 critic: torch.nn.Module,\n                 optim: torch.optim.Optimizer,\n                 dist_fn: torch.distributions.Distribution,\n                 discount_factor: float = 0.99,\n                 max_grad_norm: Optional[float] = None,\n                 eps_clip: float = .2,\n                 vf_coef: float = .5,\n                 ent_coef: float = .01,\n                 action_range: Optional[Tuple[float, float]] = None,\n                 gae_lambda: float = 0.95,\n                 dual_clip: Optional[float] = None,\n                 value_clip: bool = True,\n                 reward_normalization: bool = True,\n                 **kwargs) -> None:\n        super().__init__(None, None, dist_fn, discount_factor, **kwargs)\n        self._max_grad_norm = max_grad_norm\n        self._eps_clip = eps_clip\n        self._w_vf = vf_coef\n        self._w_ent = ent_coef\n        self._range = action_range\n        self.actor = actor\n        self.critic = critic\n        self.optim = optim\n        self._batch = 64\n        assert 0 <= gae_lambda <= 1, \'GAE lambda should be in [0, 1].\'\n        self._lambda = gae_lambda\n        assert dual_clip is None or dual_clip > 1, \\\n            \'Dual-clip PPO parameter should greater than 1.\'\n        self._dual_clip = dual_clip\n        self._value_clip = value_clip\n        self._rew_norm = reward_normalization\n\n    def process_fn(self, batch: Batch, buffer: ReplayBuffer,\n                   indice: np.ndarray) -> Batch:\n        if self._rew_norm:\n            mean, std = batch.rew.mean(), batch.rew.std()\n            if not np.isclose(std, 0):\n                batch.rew = (batch.rew - mean) / std\n        if self._lambda in [0, 1]:\n            return self.compute_episodic_return(\n                batch, None, gamma=self._gamma, gae_lambda=self._lambda)\n        v_ = []\n        with torch.no_grad():\n            for b in batch.split(self._batch, shuffle=False):\n                v_.append(self.critic(b.obs_next))\n        v_ = to_numpy(torch.cat(v_, dim=0))\n        return self.compute_episodic_return(\n            batch, v_, gamma=self._gamma, gae_lambda=self._lambda)\n\n    def forward(self, batch: Batch,\n                state: Optional[Union[dict, Batch, np.ndarray]] = None,\n                **kwargs) -> Batch:\n        """"""Compute action over the given batch data.\n\n        :return: A :class:`~tianshou.data.Batch` which has 4 keys:\n\n            * ``act`` the action.\n            * ``logits`` the network\'s raw output.\n            * ``dist`` the action distribution.\n            * ``state`` the hidden state.\n\n        .. seealso::\n\n            Please refer to :meth:`~tianshou.policy.BasePolicy.forward` for\n            more detailed explanation.\n        """"""\n        logits, h = self.actor(batch.obs, state=state, info=batch.info)\n        if isinstance(logits, tuple):\n            dist = self.dist_fn(*logits)\n        else:\n            dist = self.dist_fn(logits)\n        act = dist.sample()\n        if self._range:\n            act = act.clamp(self._range[0], self._range[1])\n        return Batch(logits=logits, act=act, state=h, dist=dist)\n\n    def learn(self, batch: Batch, batch_size: int, repeat: int,\n              **kwargs) -> Dict[str, List[float]]:\n        self._batch = batch_size\n        losses, clip_losses, vf_losses, ent_losses = [], [], [], []\n        v = []\n        old_log_prob = []\n        with torch.no_grad():\n            for b in batch.split(batch_size, shuffle=False):\n                v.append(self.critic(b.obs))\n                old_log_prob.append(self(b).dist.log_prob(\n                    to_torch_as(b.act, v[0])))\n        batch.v = torch.cat(v, dim=0)  # old value\n        batch.act = to_torch_as(batch.act, v[0])\n        batch.logp_old = torch.cat(old_log_prob, dim=0)\n        batch.returns = to_torch_as(\n            batch.returns, v[0]).reshape(batch.v.shape)\n        if self._rew_norm:\n            mean, std = batch.returns.mean(), batch.returns.std()\n            if not np.isclose(std.item(), 0):\n                batch.returns = (batch.returns - mean) / std\n        batch.adv = batch.returns - batch.v\n        if self._rew_norm:\n            mean, std = batch.adv.mean(), batch.adv.std()\n            if not np.isclose(std.item(), 0):\n                batch.adv = (batch.adv - mean) / std\n        for _ in range(repeat):\n            for b in batch.split(batch_size):\n                dist = self(b).dist\n                value = self.critic(b.obs)\n                ratio = (dist.log_prob(b.act) - b.logp_old).exp().float()\n                surr1 = ratio * b.adv\n                surr2 = ratio.clamp(\n                    1. - self._eps_clip, 1. + self._eps_clip) * b.adv\n                if self._dual_clip:\n                    clip_loss = -torch.max(torch.min(surr1, surr2),\n                                           self._dual_clip * b.adv).mean()\n                else:\n                    clip_loss = -torch.min(surr1, surr2).mean()\n                clip_losses.append(clip_loss.item())\n                if self._value_clip:\n                    v_clip = b.v + (value - b.v).clamp(\n                        -self._eps_clip, self._eps_clip)\n                    vf1 = (b.returns - value).pow(2)\n                    vf2 = (b.returns - v_clip).pow(2)\n                    vf_loss = .5 * torch.max(vf1, vf2).mean()\n                else:\n                    vf_loss = .5 * (b.returns - value).pow(2).mean()\n                vf_losses.append(vf_loss.item())\n                e_loss = dist.entropy().mean()\n                ent_losses.append(e_loss.item())\n                loss = clip_loss + self._w_vf * vf_loss - self._w_ent * e_loss\n                losses.append(loss.item())\n                self.optim.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(list(\n                    self.actor.parameters()) + list(self.critic.parameters()),\n                    self._max_grad_norm)\n                self.optim.step()\n        return {\n            \'loss\': losses,\n            \'loss/clip\': clip_losses,\n            \'loss/vf\': vf_losses,\n            \'loss/ent\': ent_losses,\n        }\n'"
tianshou/policy/modelfree/sac.py,19,"b'import torch\nimport numpy as np\nfrom copy import deepcopy\nimport torch.nn.functional as F\nfrom typing import Dict, Tuple, Union, Optional\n\nfrom tianshou.policy import DDPGPolicy\nfrom tianshou.policy.dist import DiagGaussian\nfrom tianshou.data import Batch, to_torch_as, ReplayBuffer\n\n\nclass SACPolicy(DDPGPolicy):\n    """"""Implementation of Soft Actor-Critic. arXiv:1812.05905\n\n    :param torch.nn.Module actor: the actor network following the rules in\n        :class:`~tianshou.policy.BasePolicy`. (s -> logits)\n    :param torch.optim.Optimizer actor_optim: the optimizer for actor network.\n    :param torch.nn.Module critic1: the first critic network. (s, a -> Q(s,\n        a))\n    :param torch.optim.Optimizer critic1_optim: the optimizer for the first\n        critic network.\n    :param torch.nn.Module critic2: the second critic network. (s, a -> Q(s,\n        a))\n    :param torch.optim.Optimizer critic2_optim: the optimizer for the second\n        critic network.\n    :param float tau: param for soft update of the target network, defaults to\n        0.005.\n    :param float gamma: discount factor, in [0, 1], defaults to 0.99.\n    :param float exploration_noise: the noise intensity, add to the action,\n        defaults to 0.1.\n    :param float alpha: entropy regularization coefficient, default to 0.2.\n    :param action_range: the action range (minimum, maximum).\n    :type action_range: (float, float)\n    :param bool reward_normalization: normalize the reward to Normal(0, 1),\n        defaults to ``False``.\n    :param bool ignore_done: ignore the done flag while training the policy,\n        defaults to ``False``.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.policy.BasePolicy` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self,\n                 actor: torch.nn.Module,\n                 actor_optim: torch.optim.Optimizer,\n                 critic1: torch.nn.Module,\n                 critic1_optim: torch.optim.Optimizer,\n                 critic2: torch.nn.Module,\n                 critic2_optim: torch.optim.Optimizer,\n                 tau: float = 0.005,\n                 gamma: float = 0.99,\n                 alpha: float = 0.2,\n                 action_range: Optional[Tuple[float, float]] = None,\n                 reward_normalization: bool = False,\n                 ignore_done: bool = False,\n                 estimation_step: int = 1,\n                 **kwargs) -> None:\n        super().__init__(None, None, None, None, tau, gamma, 0,\n                         action_range, reward_normalization, ignore_done,\n                         estimation_step, **kwargs)\n        self.actor, self.actor_optim = actor, actor_optim\n        self.critic1, self.critic1_old = critic1, deepcopy(critic1)\n        self.critic1_old.eval()\n        self.critic1_optim = critic1_optim\n        self.critic2, self.critic2_old = critic2, deepcopy(critic2)\n        self.critic2_old.eval()\n        self.critic2_optim = critic2_optim\n        self._alpha = alpha\n        self.__eps = np.finfo(np.float32).eps.item()\n\n    def train(self) -> None:\n        self.training = True\n        self.actor.train()\n        self.critic1.train()\n        self.critic2.train()\n\n    def eval(self) -> None:\n        self.training = False\n        self.actor.eval()\n        self.critic1.eval()\n        self.critic2.eval()\n\n    def sync_weight(self) -> None:\n        for o, n in zip(\n                self.critic1_old.parameters(), self.critic1.parameters()):\n            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n        for o, n in zip(\n                self.critic2_old.parameters(), self.critic2.parameters()):\n            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n\n    def forward(self, batch: Batch,\n                state: Optional[Union[dict, Batch, np.ndarray]] = None,\n                input: str = \'obs\', **kwargs) -> Batch:\n        obs = getattr(batch, input)\n        logits, h = self.actor(obs, state=state, info=batch.info)\n        assert isinstance(logits, tuple)\n        dist = DiagGaussian(*logits)\n        x = dist.rsample()\n        y = torch.tanh(x)\n        act = y * self._action_scale + self._action_bias\n        log_prob = dist.log_prob(x) - torch.log(\n            self._action_scale * (1 - y.pow(2)) + self.__eps)\n        act = act.clamp(self._range[0], self._range[1])\n        return Batch(\n            logits=logits, act=act, state=h, dist=dist, log_prob=log_prob)\n\n    def _target_q(self, buffer: ReplayBuffer,\n                  indice: np.ndarray) -> torch.Tensor:\n        batch = buffer[indice]  # batch.obs: s_{t+n}\n        with torch.no_grad():\n            obs_next_result = self(batch, input=\'obs_next\')\n            a_ = obs_next_result.act\n            batch.act = to_torch_as(batch.act, a_)\n            target_q = torch.min(\n                self.critic1_old(batch.obs_next, a_),\n                self.critic2_old(batch.obs_next, a_),\n            ) - self._alpha * obs_next_result.log_prob\n        return target_q\n\n    def learn(self, batch: Batch, **kwargs) -> Dict[str, float]:\n        # critic 1\n        current_q1 = self.critic1(batch.obs, batch.act)\n        target_q = to_torch_as(batch.returns, current_q1)[:, None]\n        critic1_loss = F.mse_loss(current_q1, target_q)\n        self.critic1_optim.zero_grad()\n        critic1_loss.backward()\n        self.critic1_optim.step()\n        # critic 2\n        current_q2 = self.critic2(batch.obs, batch.act)\n        critic2_loss = F.mse_loss(current_q2, target_q)\n        self.critic2_optim.zero_grad()\n        critic2_loss.backward()\n        self.critic2_optim.step()\n        # actor\n        obs_result = self(batch)\n        a = obs_result.act\n        current_q1a = self.critic1(batch.obs, a)\n        current_q2a = self.critic2(batch.obs, a)\n        actor_loss = (self._alpha * obs_result.log_prob - torch.min(\n            current_q1a, current_q2a)).mean()\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        self.actor_optim.step()\n        self.sync_weight()\n        return {\n            \'loss/actor\': actor_loss.item(),\n            \'loss/critic1\': critic1_loss.item(),\n            \'loss/critic2\': critic2_loss.item(),\n        }\n'"
tianshou/policy/modelfree/td3.py,17,"b'import torch\nimport numpy as np\nfrom copy import deepcopy\nimport torch.nn.functional as F\nfrom typing import Dict, Tuple, Optional\n\nfrom tianshou.policy import DDPGPolicy\nfrom tianshou.data import Batch, ReplayBuffer\n\n\nclass TD3Policy(DDPGPolicy):\n    """"""Implementation of Twin Delayed Deep Deterministic Policy Gradient,\n    arXiv:1802.09477\n\n    :param torch.nn.Module actor: the actor network following the rules in\n        :class:`~tianshou.policy.BasePolicy`. (s -> logits)\n    :param torch.optim.Optimizer actor_optim: the optimizer for actor network.\n    :param torch.nn.Module critic1: the first critic network. (s, a -> Q(s,\n        a))\n    :param torch.optim.Optimizer critic1_optim: the optimizer for the first\n        critic network.\n    :param torch.nn.Module critic2: the second critic network. (s, a -> Q(s,\n        a))\n    :param torch.optim.Optimizer critic2_optim: the optimizer for the second\n        critic network.\n    :param float tau: param for soft update of the target network, defaults to\n        0.005.\n    :param float gamma: discount factor, in [0, 1], defaults to 0.99.\n    :param float exploration_noise: the noise intensity, add to the action,\n        defaults to 0.1.\n    :param float policy_noise: the noise used in updating policy network,\n        default to 0.2.\n    :param int update_actor_freq: the update frequency of actor network,\n        default to 2.\n    :param float noise_clip: the clipping range used in updating policy\n        network, default to 0.5.\n    :param action_range: the action range (minimum, maximum).\n    :type action_range: (float, float)\n    :param bool reward_normalization: normalize the reward to Normal(0, 1),\n        defaults to ``False``.\n    :param bool ignore_done: ignore the done flag while training the policy,\n        defaults to ``False``.\n\n    .. seealso::\n\n        Please refer to :class:`~tianshou.policy.BasePolicy` for more detailed\n        explanation.\n    """"""\n\n    def __init__(self,\n                 actor: torch.nn.Module,\n                 actor_optim: torch.optim.Optimizer,\n                 critic1: torch.nn.Module,\n                 critic1_optim: torch.optim.Optimizer,\n                 critic2: torch.nn.Module,\n                 critic2_optim: torch.optim.Optimizer,\n                 tau: float = 0.005,\n                 gamma: float = 0.99,\n                 exploration_noise: float = 0.1,\n                 policy_noise: float = 0.2,\n                 update_actor_freq: int = 2,\n                 noise_clip: float = 0.5,\n                 action_range: Optional[Tuple[float, float]] = None,\n                 reward_normalization: bool = False,\n                 ignore_done: bool = False,\n                 estimation_step: int = 1,\n                 **kwargs) -> None:\n        super().__init__(actor, actor_optim, None, None, tau, gamma,\n                         exploration_noise, action_range, reward_normalization,\n                         ignore_done, estimation_step, **kwargs)\n        self.critic1, self.critic1_old = critic1, deepcopy(critic1)\n        self.critic1_old.eval()\n        self.critic1_optim = critic1_optim\n        self.critic2, self.critic2_old = critic2, deepcopy(critic2)\n        self.critic2_old.eval()\n        self.critic2_optim = critic2_optim\n        self._policy_noise = policy_noise\n        self._freq = update_actor_freq\n        self._noise_clip = noise_clip\n        self._cnt = 0\n        self._last = 0\n\n    def train(self) -> None:\n        self.training = True\n        self.actor.train()\n        self.critic1.train()\n        self.critic2.train()\n\n    def eval(self) -> None:\n        self.training = False\n        self.actor.eval()\n        self.critic1.eval()\n        self.critic2.eval()\n\n    def sync_weight(self) -> None:\n        for o, n in zip(self.actor_old.parameters(), self.actor.parameters()):\n            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n        for o, n in zip(\n                self.critic1_old.parameters(), self.critic1.parameters()):\n            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n        for o, n in zip(\n                self.critic2_old.parameters(), self.critic2.parameters()):\n            o.data.copy_(o.data * (1 - self._tau) + n.data * self._tau)\n\n    def _target_q(self, buffer: ReplayBuffer,\n                  indice: np.ndarray) -> torch.Tensor:\n        batch = buffer[indice]  # batch.obs: s_{t+n}\n        with torch.no_grad():\n            a_ = self(batch, model=\'actor_old\', input=\'obs_next\').act\n            dev = a_.device\n            noise = torch.randn(size=a_.shape, device=dev) * self._policy_noise\n            if self._noise_clip > 0:\n                noise = noise.clamp(-self._noise_clip, self._noise_clip)\n            a_ += noise\n            a_ = a_.clamp(self._range[0], self._range[1])\n            target_q = torch.min(\n                self.critic1_old(batch.obs_next, a_),\n                self.critic2_old(batch.obs_next, a_))\n        return target_q\n\n    def learn(self, batch: Batch, **kwargs) -> Dict[str, float]:\n        # critic 1\n        current_q1 = self.critic1(batch.obs, batch.act)\n        target_q = batch.returns[:, None]\n        critic1_loss = F.mse_loss(current_q1, target_q)\n        self.critic1_optim.zero_grad()\n        critic1_loss.backward()\n        self.critic1_optim.step()\n        # critic 2\n        current_q2 = self.critic2(batch.obs, batch.act)\n        critic2_loss = F.mse_loss(current_q2, target_q)\n        self.critic2_optim.zero_grad()\n        critic2_loss.backward()\n        self.critic2_optim.step()\n        if self._cnt % self._freq == 0:\n            actor_loss = -self.critic1(\n                batch.obs, self(batch, eps=0).act).mean()\n            self.actor_optim.zero_grad()\n            actor_loss.backward()\n            self._last = actor_loss.item()\n            self.actor_optim.step()\n            self.sync_weight()\n        self._cnt += 1\n        return {\n            \'loss/actor\': self._last,\n            \'loss/critic1\': critic1_loss.item(),\n            \'loss/critic2\': critic2_loss.item(),\n        }\n'"
