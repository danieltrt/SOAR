file_path,api_count,code
inference_video.py,3,"b'#------------------------------------------------------------------------------\n#\tLibraries\n#------------------------------------------------------------------------------\nimport cv2, torch, argparse\nfrom time import time\nimport numpy as np\nfrom torch.nn import functional as F\n\nfrom models import UNet\nfrom dataloaders import transforms\nfrom utils import utils\n\n\n#------------------------------------------------------------------------------\n#   Argument parsing\n#------------------------------------------------------------------------------\nparser = argparse.ArgumentParser(description=""Arguments for the script"")\n\nparser.add_argument(\'--use_cuda\', action=\'store_true\', default=False,\n                    help=\'Use GPU acceleration\')\n\nparser.add_argument(\'--bg\', type=str, default=None,\n                    help=\'Path to the background image file\')\n\nparser.add_argument(\'--watch\', action=\'store_true\', default=False,\n                    help=\'Indicate show result live\')\n\nparser.add_argument(\'--input_sz\', type=int, default=320,\n                    help=\'Input size\')\n\nparser.add_argument(\'--checkpoint\', type=str, default=""/media/antiaegis/storing/FORGERY/segmentation/checkpoints/HumanSeg/UNet_MobileNetV2/model_best.pth"",\n                    help=\'Path to the trained model file\')\n\nparser.add_argument(\'--video\', type=str, default=""/media/antiaegis/storing/FORGERY/segmentation/videos/Directions.54138969.mp4"",\n                    help=\'Path to the input video\')\n\nparser.add_argument(\'--output\', type=str, default=""/media/antiaegis/storing/FORGERY/segmentation/videos/Directions.54138969.output.mp4"",\n                    help=\'Path to the output video\')\n\nargs = parser.parse_args()\n\n\n#------------------------------------------------------------------------------\n#\tParameters\n#------------------------------------------------------------------------------\n# Video input\ncap = cv2.VideoCapture(args.video)\n_, frame = cap.read()\nH, W = frame.shape[:2]\n\n# Video output\nfourcc = cv2.VideoWriter_fourcc(*\'DIVX\')\nout = cv2.VideoWriter(args.output, fourcc, 30, (W,H))\nfont = cv2.FONT_HERSHEY_SIMPLEX\n\n# Background\nif args.bg is not None:\n\tBACKGROUND = cv2.imread(args.bg)[...,::-1]\n\tBACKGROUND = cv2.resize(BACKGROUND, (W,H), interpolation=cv2.INTER_LINEAR)\n\tKERNEL_SZ = 25\n\tSIGMA = 0\n\n# Alpha transperency\nelse:\n\tCOLOR1 = [255, 0, 0]\n\tCOLOR2 = [0, 0, 255]\n\n\n#------------------------------------------------------------------------------\n#\tCreate model and load weights\n#------------------------------------------------------------------------------\nmodel = UNet(\n    backbone=""mobilenetv2"",\n    num_classes=2,\n\tpretrained_backbone=None\n)\nif args.use_cuda:\n\tmodel = model.cuda()\ntrained_dict = torch.load(args.checkpoint, map_location=""cpu"")[\'state_dict\']\nmodel.load_state_dict(trained_dict, strict=False)\nmodel.eval()\n\n\n#------------------------------------------------------------------------------\n#   Predict frames\n#------------------------------------------------------------------------------\ni = 0\nwhile(cap.isOpened()):\n\t# Read frame from camera\n\tstart_time = time()\n\t_, frame = cap.read()\n\t# image = cv2.transpose(frame[...,::-1])\n\timage = frame[...,::-1]\n\th, w = image.shape[:2]\n\tread_cam_time = time()\n\n\t# Predict mask\n\tX, pad_up, pad_left, h_new, w_new = utils.preprocessing(image, expected_size=args.input_sz, pad_value=0)\n\tpreproc_time = time()\n\twith torch.no_grad():\n\t\tif args.use_cuda:\n\t\t\tmask = model(X.cuda())\n\t\t\tmask = mask[..., pad_up: pad_up+h_new, pad_left: pad_left+w_new]\n\t\t\tmask = F.interpolate(mask, size=(h,w), mode=\'bilinear\', align_corners=True)\n\t\t\tmask = F.softmax(mask, dim=1)\n\t\t\tmask = mask[0,1,...].cpu().numpy()\n\t\telse:\n\t\t\tmask = model(X)\n\t\t\tmask = mask[..., pad_up: pad_up+h_new, pad_left: pad_left+w_new]\n\t\t\tmask = F.interpolate(mask, size=(h,w), mode=\'bilinear\', align_corners=True)\n\t\t\tmask = F.softmax(mask, dim=1)\n\t\t\tmask = mask[0,1,...].numpy()\n\tpredict_time = time()\n\n\t# Draw result\n\tif args.bg is None:\n\t\timage_alpha = utils.draw_matting(image, mask)\n\t\t# image_alpha = utils.draw_transperency(image, mask, COLOR1, COLOR2)\n\telse:\n\t\timage_alpha = utils.draw_fore_to_back(image, mask, BACKGROUND, kernel_sz=KERNEL_SZ, sigma=SIGMA)\n\tdraw_time = time()\n\n\t# Print runtime\n\tread = read_cam_time-start_time\n\tpreproc = preproc_time-read_cam_time\n\tpred = predict_time-preproc_time\n\tdraw = draw_time-predict_time\n\ttotal = read + preproc + pred + draw\n\tfps = 1 / total\n\tprint(""read: %.3f [s]; preproc: %.3f [s]; pred: %.3f [s]; draw: %.3f [s]; total: %.3f [s]; fps: %.2f [Hz]"" % \n\t\t(read, preproc, pred, draw, total, fps))\n\n\t# Wait for interupt\n\tcv2.putText(image_alpha, ""%.2f [fps]"" % (fps), (10, 50), font, 1.5, (0, 255, 0), 2, cv2.LINE_AA)\n\tout.write(image_alpha[..., ::-1])\n\tif args.watch:\n\t\tcv2.imshow(\'webcam\', image_alpha[..., ::-1])\n\tif cv2.waitKey(1) & 0xFF == ord(\'q\'):\n\t\tbreak\n\ncap.release()\ncv2.destroyAllWindows()\n'"
inference_webcam.py,1,"b'#------------------------------------------------------------------------------\n#\tLibraries\n#------------------------------------------------------------------------------\nimport torch, argparse\nfrom models import UNet\nfrom base import VideoInference\n\n\n#------------------------------------------------------------------------------\n#   Argument parsing\n#------------------------------------------------------------------------------\nparser = argparse.ArgumentParser(description=""Arguments for the script"")\n\nparser.add_argument(\'--use_cuda\', action=\'store_true\', default=True,\n                    help=\'Use GPU acceleration\')\n\nparser.add_argument(\'--input_size\', type=int, default=320,\n                    help=\'Input size\')\n\nparser.add_argument(\'--checkpoint\', type=str, default=""model_best.pth"",\n                    help=\'Path to the trained model file\')\n\nargs = parser.parse_args()\n\n\n#------------------------------------------------------------------------------\n#\tMain execution\n#------------------------------------------------------------------------------\n# Build model\nmodel = UNet(backbone=""resnet18"", num_classes=2)\ntrained_dict = torch.load(args.checkpoint, map_location=""cpu"")[\'state_dict\']\nmodel.load_state_dict(trained_dict, strict=False)\nif args.use_cuda:\n\tmodel.cuda()\nmodel.eval()\n\n\n# Inference\ninference = VideoInference(\n    model=model,\n    video_path=0,\n    input_size=args.input_size,\n    use_cuda=args.use_cuda,\n    draw_mode=\'matting\',\n)\ninference.run()'"
measure_model.py,1,"b'#------------------------------------------------------------------------------\n#\tLibraries\n#------------------------------------------------------------------------------\nimport os\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\'\n\nimport models\n\nimport argparse\nfrom time import time\n\nimport torch\nfrom torchsummary import summary\n\n\n#------------------------------------------------------------------------------\n#   Argument parsing\n#------------------------------------------------------------------------------\nparser = argparse.ArgumentParser(description=""Arguments for the script"")\n\nparser.add_argument(\'--use_cuda\', action=\'store_true\', default=False,\n\t\t\t\t\thelp=\'Use GPU acceleration\')\n\nparser.add_argument(\'--input_sz\', type=int, default=224,\n\t\t\t\t\thelp=\'Size of the input\')\n\nparser.add_argument(\'--n_measures\', type=int, default=1,\n\t\t\t\t\thelp=\'Number of time measurements\')\n\nargs = parser.parse_args()\n\n\n#------------------------------------------------------------------------------\n#\tCreate model\n#------------------------------------------------------------------------------\n# UNet\nmodel = models.UNet(\n\tbackbone=""mobilenetv2"",\n\tnum_classes=2,\n)\n\n# # DeepLabV3+\n# model = DeepLabV3Plus(\n#     backbone=\'resnet18\',\n#     output_stride=16,\n#     num_classes=2,\n#     pretrained_backbone=None,\n# )\n\n# # BiSeNet\n# model = BiSeNet(\n#     backbone=\'resnet18\',\n#     num_classes=2,\n#     pretrained_backbone=None,\n# )\n\n# # PSPNet\n# model = PSPNet(\n# \tbackbone=\'resnet18\',\n# \tnum_classes=2,\n# \tpretrained_backbone=None,\n# )\n\n# # ICNet\n# model = ICNet(\n#     backbone=\'resnet18\',\n#     num_classes=2,\n#     pretrained_backbone=None,\n# )\n\n\n# #------------------------------------------------------------------------------\n# #   Summary network\n# #------------------------------------------------------------------------------\nmodel.train()\nmodel.summary(input_shape=(3, args.input_sz, args.input_sz), device=\'cpu\')\n\n\n# #------------------------------------------------------------------------------\n# #   Measure time\n# #------------------------------------------------------------------------------\n# input = torch.randn([1, 3, args.input_sz, args.input_sz], dtype=torch.float)\n# if args.use_cuda:\n# \tmodel.cuda()\n# \tinput = input.cuda()\n\n# for _ in range(10):\n# \tmodel(input)\n\n# start_time = time()\n# for _ in range(args.n_measures):\n# \tmodel(input)\n# finish_time = time()\n\n# if args.use_cuda:\n# \tprint(""Inference time on cuda: %.2f [ms]"" % ((finish_time-start_time)*1000/args.n_measures))\n# \tprint(""Inference fps on cuda: %.2f [fps]"" % (1 / ((finish_time-start_time)/args.n_measures)))\n# else:\n# \tprint(""Inference time on cpu: %.2f [ms]"" % ((finish_time-start_time)*1000/args.n_measures))\n# \tprint(""Inference fps on cpu: %.2f [fps]"" % (1 / ((finish_time-start_time)/args.n_measures)))\n'"
train.py,3,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport os, json, argparse, torch, warnings\nwarnings.filterwarnings(""ignore"")\n\nimport models as module_arch\nimport evaluation.losses as module_loss\nimport evaluation.metrics as module_metric\nimport dataloaders.dataloader as module_data\n\nfrom utils.logger import Logger\nfrom trainer.trainer import Trainer\n\n\n#------------------------------------------------------------------------------\n#   Get instance\n#------------------------------------------------------------------------------\ndef get_instance(module, name, config, *args):\n\treturn getattr(module, config[name][\'type\'])(*args, **config[name][\'args\'])\n\n\n#------------------------------------------------------------------------------\n#   Main function\n#------------------------------------------------------------------------------\ndef main(config, resume):\n\ttrain_logger = Logger()\n\n\t# Build model architecture\n\tmodel = get_instance(module_arch, \'arch\', config)\n\timg_sz = config[""train_loader""][""args""][""resize""]\n\tmodel.summary(input_shape=(3, img_sz, img_sz))\n\n\t# Setup data_loader instances\n\ttrain_loader = get_instance(module_data, \'train_loader\', config).loader\n\tvalid_loader = get_instance(module_data, \'valid_loader\', config).loader\n\n\t# Get function handles of loss and metrics\n\tloss = getattr(module_loss, config[\'loss\'])\n\tmetrics = [getattr(module_metric, met) for met in config[\'metrics\']]\n\n\t# Build optimizer, learning rate scheduler.\n\ttrainable_params = filter(lambda p: p.requires_grad, model.parameters())\n\toptimizer = get_instance(torch.optim, \'optimizer\', config, trainable_params)\n\tlr_scheduler = get_instance(torch.optim.lr_scheduler, \'lr_scheduler\', config, optimizer)\n\n\t# Create trainer and start training\n\ttrainer = Trainer(model, loss, metrics, optimizer, \n\t\t\t\t\t  resume=resume,\n\t\t\t\t\t  config=config,\n\t\t\t\t\t  data_loader=train_loader,\n\t\t\t\t\t  valid_data_loader=valid_loader,\n\t\t\t\t\t  lr_scheduler=lr_scheduler,\n\t\t\t\t\t  train_logger=train_logger)\n\ttrainer.train()\n\n\n#------------------------------------------------------------------------------\n#   Main execution\n#------------------------------------------------------------------------------\nif __name__ == \'__main__\':\n\n\t# Argument parsing\n\tparser = argparse.ArgumentParser(description=\'Train model\')\n\n\tparser.add_argument(\'-c\', \'--config\', default=None, type=str,\n\t\t\t\t\t\t   help=\'config file path (default: None)\')\n\n\tparser.add_argument(\'-r\', \'--resume\', default=None, type=str,\n\t\t\t\t\t\t   help=\'path to latest checkpoint (default: None)\')\n\n\tparser.add_argument(\'-d\', \'--device\', default=None, type=str,\n\t\t\t\t\t\t   help=\'indices of GPUs to enable (default: all)\')\n \n\targs = parser.parse_args()\n\n\n\t# Load config file\n\tif args.config:\n\t\tconfig = json.load(open(args.config))\n\t\tpath = os.path.join(config[\'trainer\'][\'save_dir\'], config[\'name\'])\n\n\n\t# Load config file from checkpoint, in case new config file is not given.\n\t# Use \'--config\' and \'--resume\' arguments together to load trained model and train more with changed config.\n\telif args.resume:\n\t\tconfig = torch.load(args.resume)[\'config\']\n\n\n\t# AssertionError\n\telse:\n\t\traise AssertionError(""Configuration file need to be specified. Add \'-c config.json\', for example."")\n\t\n\n\t# Set visible devices\n\tif args.device:\n\t\tos.environ[""CUDA_VISIBLE_DEVICES""]=args.device\n\n\n\t# Run the main function\n\tmain(config, args.resume)\n'"
base/__init__.py,0,"b'from .base_data_loader import *\nfrom .base_model import *\nfrom .base_trainer import *\n\n\n#------------------------------------------------------------------------------\n#   Bag of Inferences\n#------------------------------------------------------------------------------\nfrom base.base_inference import (\n    BaseInference,\n    VideoInference\n)'"
base/base_data_loader.py,3,"b""#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n\n#------------------------------------------------------------------------------\n#   BaseDataLoader\n#------------------------------------------------------------------------------\nclass BaseDataLoader(DataLoader):\n    def __init__(self, dataset, batch_size, shuffle, validation_split, num_workers, collate_fn=default_collate):\n        self.validation_split = validation_split\n        self.shuffle = shuffle\n        \n        self.batch_idx = 0\n        self.n_samples = len(dataset)\n\n        self.sampler, self.valid_sampler = self._split_sampler(self.validation_split)\n\n        self.init_kwargs = {\n            'dataset': dataset,\n            'batch_size': batch_size,\n            'shuffle': self.shuffle,\n            'collate_fn': collate_fn,\n            'num_workers': num_workers\n            }\n        super(BaseDataLoader, self).__init__(sampler=self.sampler, **self.init_kwargs)\n\n\n    def _split_sampler(self, split):\n        if split == 0.0:\n            return None, None\n\n        idx_full = np.arange(self.n_samples)\n\n        np.random.seed(0) \n        np.random.shuffle(idx_full)\n\n        len_valid = int(self.n_samples * split)\n\n        valid_idx = idx_full[0:len_valid]\n        train_idx = np.delete(idx_full, np.arange(0, len_valid))\n        \n        train_sampler = SubsetRandomSampler(train_idx)\n        valid_sampler = SubsetRandomSampler(valid_idx)\n        \n        # turn off shuffle option which is mutually exclusive with sampler\n        self.shuffle = False\n        self.n_samples = len(train_idx)\n\n        return train_sampler, valid_sampler\n\n\n    def split_validation(self):\n        if self.valid_sampler is None:\n            return None\n        else:\n            return DataLoader(sampler=self.valid_sampler, **self.init_kwargs)\n"""
base/base_inference.py,3,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport cv2, torch\nimport numpy as np\nfrom time import time\nfrom torch.nn import functional as F\n\n\n#------------------------------------------------------------------------------\n#   BaseInference\n#------------------------------------------------------------------------------\nclass BaseInference(object):\n\tdef __init__(self, model, color_f=[255,0,0], color_b=[0,0,255], kernel_sz=25, sigma=0, background_path=None):\n\t\tself.model = model\n\t\tself.color_f = color_f\n\t\tself.color_b = color_b\n\t\tself.kernel_sz = kernel_sz\n\t\tself.sigma = sigma\n\t\tself.background_path = background_path\n\t\tif background_path is not None:\n\t\t\tself.background = cv2.imread(background_path)[...,::-1]\n\t\t\tself.background = self.background.astype(np.float32)\n\n\n\tdef load_image(self):\n\t\traise NotImplementedError\n\n\n\tdef preprocess(self, image, *args):\n\t\traise NotImplementedError\n\n\n\tdef predict(self, X):\n\t\traise NotImplementedError\n\n\n\tdef draw_matting(self, image, mask):\n\t\t""""""\n\t\timage (np.uint8) shape (H,W,3)\n\t\tmask  (np.float32) range from 0 to 1, shape (H,W)\n\t\t""""""\n\t\tmask = 255*(1.0-mask)\n\t\tmask = np.expand_dims(mask, axis=2)\n\t\tmask = np.tile(mask, (1,1,3))\n\t\tmask = mask.astype(np.uint8)\n\t\timage_alpha = cv2.add(image, mask)\n\t\treturn image_alpha\n\n\n\tdef draw_transperency(self, image, mask):\n\t\t""""""\n\t\timage (np.uint8) shape (H,W,3)\n\t\tmask  (np.float32) range from 0 to 1, shape (H,W)\n\t\t""""""\n\t\tmask = mask.round()\n\t\talpha = np.zeros_like(image, dtype=np.uint8)\n\t\talpha[mask==1, :] = self.color_f\n\t\talpha[mask==0, :] = self.color_b\n\t\timage_alpha = cv2.add(image, alpha)\n\t\treturn image_alpha\n\n\n\tdef draw_background(self, image, mask):\n\t\t""""""\n\t\timage (np.uint8) shape (H,W,3)\n\t\tmask  (np.float32) range from 0 to 1, shape (H,W)\n\t\t""""""\n\t\timage = image.astype(np.float32)\n\t\tmask_filtered = cv2.GaussianBlur(mask, (self.kernel_sz, self.kernel_sz), self.sigma)\n\t\tmask_filtered = np.expand_dims(mask_filtered, axis=2)\n\t\tmask_filtered = np.tile(mask_filtered, (1,1,3))\n\n\t\timage_alpha = image*mask_filtered + self.background*(1-mask_filtered)\n\t\treturn image_alpha.astype(np.uint8)\n\n\n#------------------------------------------------------------------------------\n#   VideoInference\n#------------------------------------------------------------------------------\nclass VideoInference(BaseInference):\n\tdef __init__(self, model, video_path, input_size, use_cuda=True, draw_mode=\'matting\',\n\t\t\t\tcolor_f=[255,0,0], color_b=[0,0,255], kernel_sz=25, sigma=0, background_path=None):\n\n\t\t# Initialize\n\t\tsuper(VideoInference, self).__init__(model, color_f, color_b, kernel_sz, sigma, background_path)\n\t\tself.input_size = input_size\n\t\tself.use_cuda = use_cuda\n\t\tself.draw_mode = draw_mode\n\t\tif draw_mode==\'matting\':\n\t\t\tself.draw_func = self.draw_matting\n\t\telif draw_mode==\'transperency\':\n\t\t\tself.draw_func = self.draw_transperency\n\t\telif draw_mode==\'background\':\n\t\t\tself.draw_func = self.draw_background\n\t\telse:\n\t\t\traise NotImplementedError\n\n\t\t# Preprocess\n\t\tself.mean = np.array([0.485,0.456,0.406])[None,None,:]\n\t\tself.std = np.array([0.229,0.224,0.225])[None,None,:]\n\n\t\t# Read video\n\t\tself.video_path = video_path\n\t\tself.cap = cv2.VideoCapture(video_path)\n\t\t_, frame = self.cap.read()\n\t\tself.H, self.W = frame.shape[:2]\n\n\n\tdef load_image(self):\n\t\t_, frame = self.cap.read()\n\t\timage = frame[...,::-1]\n\t\treturn image\n\n\n\tdef preprocess(self, image):\n\t\timage = cv2.resize(image, (self.input_size,self.input_size), interpolation=cv2.INTER_LINEAR)\n\t\timage = image.astype(np.float32) / 255.0\n\t\timage = (image - self.mean) / self.std\n\t\tX = np.transpose(image, axes=(2, 0, 1))\n\t\tX = np.expand_dims(X, axis=0)\n\t\tX = torch.tensor(X, dtype=torch.float32)\n\t\treturn X\n\n\n\tdef predict(self, X):\n\t\twith torch.no_grad():\n\t\t\tif self.use_cuda:\n\t\t\t\tmask = self.model(X.cuda())\n\t\t\t\tmask = F.interpolate(mask, size=(self.H, self.W), mode=\'bilinear\', align_corners=True)\n\t\t\t\tmask = F.softmax(mask, dim=1)\n\t\t\t\tmask = mask[0,1,...].cpu().numpy()\n\t\t\telse:\n\t\t\t\tmask = self.model(X)\n\t\t\t\tmask = F.interpolate(mask, size=(self.H, self.W), mode=\'bilinear\', align_corners=True)\n\t\t\t\tmask = F.softmax(mask, dim=1)\n\t\t\t\tmask = mask[0,1,...].numpy()\n\t\t\treturn mask\n\n\n\tdef run(self):\n\t\twhile(True):\n\t\t\t# Read frame from camera\n\t\t\tstart_time = time()\n\t\t\timage = self.load_image()\n\t\t\tread_cam_time = time()\n\n\t\t\t# Preprocess\n\t\t\tX = self.preprocess(image)\n\t\t\tpreproc_time = time()\n\n\t\t\t# Predict\n\t\t\tmask = self.predict(X)\n\t\t\tpredict_time = time()\n\n\t\t\t# Draw result\n\t\t\timage_alpha = self.draw_func(image, mask)\n\t\t\tdraw_time = time()\n\n\t\t\t# Wait for interupt\n\t\t\tcv2.imshow(\'webcam\', image_alpha[..., ::-1])\n\t\t\tif cv2.waitKey(1) & 0xFF == ord(\'q\'):\n\t\t\t\tbreak\n\n\t\t\t# Print runtime\n\t\t\tread = read_cam_time-start_time\n\t\t\tpreproc = preproc_time-read_cam_time\n\t\t\tpred = predict_time-preproc_time\n\t\t\tdraw = draw_time-predict_time\n\t\t\ttotal = read + preproc + pred + draw\n\t\t\tfps = 1 / total\n\t\t\tprint(""read: %.3f [s]; preproc: %.3f [s]; pred: %.3f [s]; draw: %.3f [s]; total: %.3f [s]; fps: %.2f [Hz]"" % \n\t\t\t\t(read, preproc, pred, draw, total, fps))'"
base/base_model.py,4,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\n\nimport torchsummary\nimport os, warnings, sys\nfrom utils import add_flops_counting_methods, flops_to_string\n\n\n#------------------------------------------------------------------------------\n#   BaseModel\n#------------------------------------------------------------------------------\nclass BaseModel(nn.Module):\n\tdef __init__(self):\n\t\tsuper(BaseModel, self).__init__()\n\n\tdef summary(self, input_shape, batch_size=1, device=\'cpu\', print_flops=False):\n\t\tprint(""[%s] Network summary..."" % (self.__class__.__name__))\n\t\ttorchsummary.summary(self, input_size=input_shape, batch_size=batch_size, device=device)\n\t\tif print_flops:\n\t\t\tinput = torch.randn([1, *input_shape], dtype=torch.float)\n\t\t\tcounter = add_flops_counting_methods(self)\n\t\t\tcounter.eval().start_flops_count()\n\t\t\tcounter(input)\n\t\t\tprint(\'Flops:  {}\'.format(flops_to_string(counter.compute_average_flops_cost())))\n\t\t\tprint(\'----------------------------------------------------------------\')\n\n\tdef init_weights(self):\n\t\tprint(""[%s] Initialize weights..."" % (self.__class__.__name__))\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n\t\t\t\tnn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n\t\t\t\tif m.bias is not None:\n\t\t\t\t\tm.bias.data.zero_()\n\t\t\telif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n\t\t\t\tnn.init.constant_(m.weight, 1)\n\t\t\t\tnn.init.constant_(m.bias, 0)\n\t\t\telif isinstance(m, nn.Linear):\n\t\t\t\tm.weight.data.normal_(0, 0.01)\n\t\t\t\tm.bias.data.zero_()\n\n\tdef load_pretrained_model(self, pretrained):\n\t\tif isinstance(pretrained, str):\n\t\t\tprint(""[%s] Load pretrained model from %s"" % (self.__class__.__name__, pretrained))\n\t\t\tpretrain_dict = torch.load(pretrained, map_location=\'cpu\')\n\t\t\tif \'state_dict\' in pretrain_dict:\n\t\t\t\tpretrain_dict = pretrain_dict[\'state_dict\']\n\t\telif isinstance(pretrained, dict):\n\t\t\tprint(""[%s] Load pretrained model"" % (self.__class__.__name__))\n\t\t\tpretrain_dict = pretrained\n\n\t\tmodel_dict = {}\n\t\tstate_dict = self.state_dict()\n\t\tfor k, v in pretrain_dict.items():\n\t\t\tif k in state_dict:\n\t\t\t\tif state_dict[k].shape==v.shape:\n\t\t\t\t\tmodel_dict[k] = v\n\t\t\t\telse:\n\t\t\t\t\tprint(""[%s]""%(self.__class__.__name__), k, ""is ignored due to not matching shape"")\n\t\t\telse:\n\t\t\t\tprint(""[%s]""%(self.__class__.__name__), k, ""is ignored due to not matching key"")\n\t\tstate_dict.update(model_dict)\n\t\tself.load_state_dict(state_dict)\n\n\n#------------------------------------------------------------------------------\n#   BaseBackbone\n#------------------------------------------------------------------------------\nclass BaseBackbone(BaseModel):\n\tdef __init__(self):\n\t\tsuper(BaseBackbone, self).__init__()\n\n\tdef load_pretrained_model_extended(self, pretrained):\n\t\t""""""\n\t\tThis function is specifically designed for loading pretrain with different in_channels\n\t\t""""""\n\t\tif isinstance(pretrained, str):\n\t\t\tprint(""[%s] Load pretrained model from %s"" % (self.__class__.__name__, pretrained))\n\t\t\tpretrain_dict = torch.load(pretrained, map_location=\'cpu\')\n\t\t\tif \'state_dict\' in pretrain_dict:\n\t\t\t\tpretrain_dict = pretrain_dict[\'state_dict\']\n\t\telif isinstance(pretrained, dict):\n\t\t\tprint(""[%s] Load pretrained model"" % (self.__class__.__name__))\n\t\t\tpretrain_dict = pretrained\n\n\t\tmodel_dict = {}\n\t\tstate_dict = self.state_dict()\n\t\tfor k, v in pretrain_dict.items():\n\t\t\tif k in state_dict:\n\t\t\t\tif state_dict[k].shape!=v.shape:\n\t\t\t\t\tmodel_dict[k] = state_dict[k]\n\t\t\t\t\tmodel_dict[k][:,:3,...] = v\n\t\t\t\telse:\n\t\t\t\t\tmodel_dict[k] = v\n\t\t\telse:\n\t\t\t\tprint(""[%s]""%(self.__class__.__name__), k, ""is ignored"")\n\t\tstate_dict.update(model_dict)\n\t\tself.load_state_dict(state_dict)\n\n\n#------------------------------------------------------------------------------\n#  BaseBackboneWrapper\n#------------------------------------------------------------------------------\nclass BaseBackboneWrapper(BaseBackbone):\n\tdef __init__(self):\n\t\tsuper(BaseBackboneWrapper, self).__init__()\n\n\tdef train(self, mode=True):\n\t\tif mode:\n\t\t\tprint(""[%s] Switch to train mode"" % (self.__class__.__name__))\n\t\telse:\n\t\t\tprint(""[%s] Switch to eval mode"" % (self.__class__.__name__))\n\n\t\tsuper(BaseBackboneWrapper, self).train(mode)\n\t\tself._freeze_stages()\n\t\tif mode and self.norm_eval:\n\t\t\tfor module in self.modules():\n\t\t\t\t# trick: eval have effect on BatchNorm only\n\t\t\t\tif isinstance(module, nn.BatchNorm2d):\n\t\t\t\t\tmodule.eval()\n\t\t\t\telif isinstance(module, nn.Sequential):\n\t\t\t\t\tfor m in module:\n\t\t\t\t\t\tif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n\t\t\t\t\t\t\tm.eval()\n\n\tdef init_from_imagenet(self, archname):\n\t\tpass\n\n\tdef _freeze_stages(self):\n\t\tpass\n'"
base/base_trainer.py,6,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nfrom time import time\nimport os, math, json, logging, datetime, torch\nfrom utils.visualization import WriterTensorboardX\n\n\n#------------------------------------------------------------------------------\n#   Class of BaseTrainer\n#------------------------------------------------------------------------------\nclass BaseTrainer:\n\t""""""\n\tBase class for all trainers\n\t""""""\n\tdef __init__(self, model, loss, metrics, optimizer, resume, config, train_logger=None):\n\t\tself.config = config\n\n\t\t# Setup directory for checkpoint saving\n\t\tstart_time = datetime.datetime.now().strftime(\'%m%d_%H%M%S\')\n\t\tself.checkpoint_dir = os.path.join(config[\'trainer\'][\'save_dir\'], config[\'name\'], start_time)\n\t\tos.makedirs(self.checkpoint_dir, exist_ok=True)\n\n\t\t# Setup logger\n\t\tlogging.basicConfig(\n\t\t\tlevel=logging.INFO,\n\t\t\tformat=""%(asctime)s %(message)s"",\n\t\t\thandlers=[\n\t\t\t\tlogging.FileHandler(os.path.join(self.checkpoint_dir, ""train.log"")),\n\t\t\t\tlogging.StreamHandler(),\n\t\t])\n\t\tself.logger = logging.getLogger(self.__class__.__name__)\n\n\t\t# Setup GPU device if available, move model into configured device\n\t\tself.device, device_ids = self._prepare_device(config[\'n_gpu\'])\n\t\tself.model = model.to(self.device)\n\t\tif len(device_ids) > 1:\n\t\t\tself.model = torch.nn.DataParallel(model, device_ids=device_ids)\n\n\t\tself.loss = loss\n\t\tself.metrics = metrics\n\t\tself.optimizer = optimizer\n\n\t\tself.epochs = config[\'trainer\'][\'epochs\']\n\t\tself.save_freq = config[\'trainer\'][\'save_freq\']\n\t\tself.verbosity = config[\'trainer\'][\'verbosity\']\n\n\t\tself.train_logger = train_logger\n\n\t\t# configuration to monitor model performance and save best\n\t\tself.monitor = config[\'trainer\'][\'monitor\']\n\t\tself.monitor_mode = config[\'trainer\'][\'monitor_mode\']\n\t\tassert self.monitor_mode in [\'min\', \'max\', \'off\']\n\t\tself.monitor_best = math.inf if self.monitor_mode == \'min\' else -math.inf\n\t\tself.start_epoch = 1\n\n\t\t# setup visualization writer instance\n\t\twriter_train_dir = os.path.join(config[\'visualization\'][\'log_dir\'], config[\'name\'], start_time, ""train"")\n\t\twriter_valid_dir = os.path.join(config[\'visualization\'][\'log_dir\'], config[\'name\'], start_time, ""valid"")\n\t\tself.writer_train = WriterTensorboardX(writer_train_dir, self.logger, config[\'visualization\'][\'tensorboardX\'])\n\t\tself.writer_valid = WriterTensorboardX(writer_valid_dir, self.logger, config[\'visualization\'][\'tensorboardX\'])\n\n\t\t# Save configuration file into checkpoint directory\n\t\tconfig_save_path = os.path.join(self.checkpoint_dir, \'config.json\')\n\t\twith open(config_save_path, \'w\') as handle:\n\t\t\tjson.dump(config, handle, indent=4, sort_keys=False)\n\n\t\t# Resume\n\t\tif resume:\n\t\t\tself._resume_checkpoint(resume)\n\t\n\n\tdef _prepare_device(self, n_gpu_use):\n\t\t"""""" \n\t\tsetup GPU device if available, move model into configured device\n\t\t"""""" \n\t\tn_gpu = torch.cuda.device_count()\n\t\tif n_gpu_use > 0 and n_gpu == 0:\n\t\t\tself.logger.warning(""Warning: There\\\'s no GPU available on this machine, training will be performed on CPU."")\n\t\t\tn_gpu_use = 0\n\t\tif n_gpu_use > n_gpu:\n\t\t\tmsg = ""Warning: The number of GPU\\\'s configured to use is {}, but only {} are available on this machine."".format(n_gpu_use, n_gpu)\n\t\t\tself.logger.warning(msg)\n\t\t\tn_gpu_use = n_gpu\n\t\tdevice = torch.device(\'cuda:0\' if n_gpu_use > 0 else \'cpu\')\n\t\tlist_ids = list(range(n_gpu_use))\n\t\treturn device, list_ids\n\n\n\tdef train(self):\n\t\tfor epoch in range(self.start_epoch, self.epochs + 1):\n\t\t\tself.logger.info(""\\n----------------------------------------------------------------"")\n\t\t\tself.logger.info(""[EPOCH %d]"" % (epoch))\n\t\t\tstart_time = time()\n\t\t\tresult = self._train_epoch(epoch)\n\t\t\tfinish_time = time()\n\t\t\tself.logger.info(""Finish at {}, Runtime: {:.3f} [s]"".format(datetime.datetime.now(), finish_time-start_time))\n\t\t\t\n\t\t\t# save logged informations into log dict\n\t\t\tlog = {}\n\t\t\tfor key, value in result.items():\n\t\t\t\tif key == \'train_metrics\':\n\t\t\t\t\tlog.update({\'train_\' + mtr.__name__ : value[i] for i, mtr in enumerate(self.metrics)})\n\t\t\t\telif key == \'valid_metrics\':\n\t\t\t\t\tlog.update({\'valid_\' + mtr.__name__ : value[i] for i, mtr in enumerate(self.metrics)})\n\t\t\t\telse:\n\t\t\t\t\tlog[key] = value\n\n\t\t\t# print logged informations to the screen\n\t\t\tif self.train_logger is not None:\n\t\t\t\tself.train_logger.add_entry(log)\n\t\t\t\tif self.verbosity >= 1:\n\t\t\t\t\tfor key, value in sorted(list(log.items())):\n\t\t\t\t\t\tself.logger.info(\'{:25s}: {}\'.format(str(key), value))\n\n\t\t\t# evaluate model performance according to configured metric, save best checkpoint as model_best\n\t\t\tbest = False\n\t\t\tif self.monitor_mode != \'off\':\n\t\t\t\ttry:\n\t\t\t\t\tif  (self.monitor_mode == \'min\' and log[self.monitor] < self.monitor_best) or\\\n\t\t\t\t\t\t(self.monitor_mode == \'max\' and log[self.monitor] > self.monitor_best):\n\t\t\t\t\t\tself.logger.info(""Monitor improved from %f to %f"" % (self.monitor_best, log[self.monitor]))\n\t\t\t\t\t\tself.monitor_best = log[self.monitor]\n\t\t\t\t\t\tbest = True\n\t\t\t\texcept KeyError:\n\t\t\t\t\tif epoch == 1:\n\t\t\t\t\t\tmsg = ""Warning: Can\\\'t recognize metric named \'{}\' "".format(self.monitor)\\\n\t\t\t\t\t\t\t+ ""for performance monitoring. model_best checkpoint won\\\'t be updated.""\n\t\t\t\t\t\tself.logger.warning(msg)\n\n\t\t\t# Save checkpoint\n\t\t\tself._save_checkpoint(epoch, save_best=best)\n\n\n\tdef _train_epoch(self, epoch):\n\t\t""""""\n\t\tTraining logic for an epoch\n\n\t\t:param epoch: Current epoch number\n\t\t""""""\n\t\traise NotImplementedError\n\n\n\tdef _save_checkpoint(self, epoch, save_best=False):\n\t\t""""""\n\t\tSaving checkpoints\n\n\t\t:param epoch: current epoch number\n\t\t:param log: logging information of the epoch\n\t\t:param save_best: if True, rename the saved checkpoint to \'model_best.pth\'\n\t\t""""""\n\t\t# Construct savedict\n\t\tarch = type(self.model).__name__\n\t\tstate = {\n\t\t\t\'arch\': arch,\n\t\t\t\'epoch\': epoch,\n\t\t\t\'logger\': self.train_logger,\n\t\t\t\'state_dict\': self.model.state_dict(),\n\t\t\t\'optimizer\': self.optimizer.state_dict(),\n\t\t\t\'monitor_best\': self.monitor_best,\n\t\t\t\'config\': self.config\n\t\t}\n\n\t\t# Save checkpoint for each epoch\n\t\tif self.save_freq is not None:\t# Use None mode to avoid over disk space with large models\n\t\t\tif epoch % self.save_freq == 0:\n\t\t\t\tfilename = os.path.join(self.checkpoint_dir, \'epoch{}.pth\'.format(epoch))\n\t\t\t\ttorch.save(state, filename)\n\t\t\t\tself.logger.info(""Saving checkpoint at {}"".format(filename))\n\n\t\t# Save the best checkpoint\n\t\tif save_best:\n\t\t\tbest_path = os.path.join(self.checkpoint_dir, \'model_best.pth\')\n\t\t\ttorch.save(state, best_path)\n\t\t\tself.logger.info(""Saving current best at {}"".format(best_path))\n\t\telse:\n\t\t\tself.logger.info(""Monitor is not improved from %f"" % (self.monitor_best))\n\n\n\tdef _resume_checkpoint(self, resume_path):\n\t\t""""""\n\t\tResume from saved checkpoints\n\n\t\t:param resume_path: Checkpoint path to be resumed\n\t\t""""""\n\t\tself.logger.info(""Loading checkpoint: {}"".format(resume_path))\n\t\tcheckpoint = torch.load(resume_path)\n\t\tself.start_epoch = checkpoint[\'epoch\'] + 1\n\t\tself.monitor_best = checkpoint[\'monitor_best\']\n\n\t\t# load architecture params from checkpoint.\n\t\tif checkpoint[\'config\'][\'arch\'] != self.config[\'arch\']:\n\t\t\tself.logger.warning(\'Warning: Architecture configuration given in config file is different from that of checkpoint. \' + \\\n\t\t\t\t\t\t\t\t\'This may yield an exception while state_dict is being loaded.\')\n\t\tself.model.load_state_dict(checkpoint[\'state_dict\'], strict=True)\n\n\t\t# # load optimizer state from checkpoint only when optimizer type is not changed. \n\t\t# if checkpoint[\'config\'][\'optimizer\'][\'type\'] != self.config[\'optimizer\'][\'type\']:\n\t\t# \tself.logger.warning(\'Warning: Optimizer type given in config file is different from that of checkpoint. \' + \\\n\t\t# \t\t\t\t\t\t\'Optimizer parameters not being resumed.\')\n\t\t# else:\n\t\t# \tself.optimizer.load_state_dict(checkpoint[\'optimizer\'])\n\t\n\t\tself.train_logger = checkpoint[\'logger\']\n\t\tself.logger.info(""Checkpoint \'{}\' (epoch {}) loaded"".format(resume_path, self.start_epoch-1))'"
dataloaders/dataloader.py,3,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\nimport cv2, os\nimport numpy as np\nfrom random import shuffle\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom dataloaders import transforms\n# import transforms\n\n\n#------------------------------------------------------------------------------\n#\tDataLoader for Semantic Segmentation\n#------------------------------------------------------------------------------\nclass SegmentationDataLoader(object):\n\tdef __init__(self, pairs_file, color_channel=""RGB"", resize=224, padding_value=0,\n\t\t\t\tcrop_range=[0.75, 1.0], flip_hor=0.5, rotate=0.3, angle=10, noise_std=5,\n\t\t\t\tnormalize=True, one_hot=False, is_training=True,\n\t\t\t\tshuffle=True, batch_size=1, n_workers=1, pin_memory=True):\n\n\t\t# Storage parameters\n\t\tsuper(SegmentationDataLoader, self).__init__()\n\t\tself.pairs_file = pairs_file\n\t\tself.color_channel = color_channel\n\t\tself.resize = resize\n\t\tself.padding_value = padding_value\n\t\tself.crop_range = crop_range\n\t\tself.flip_hor = flip_hor\n\t\tself.rotate = rotate\n\t\tself.angle = angle\n\t\tself.noise_std = noise_std\n\t\tself.normalize = normalize\n\t\tself.one_hot = one_hot\n\t\tself.is_training = is_training\n\t\tself.shuffle = shuffle\n\t\tself.batch_size = batch_size\n\t\tself.n_workers = n_workers\n\t\tself.pin_memory = pin_memory\n\n\t\t# Dataset\n\t\tself.dataset = SegmentationDataset(\n\t\t\tpairs_file=self.pairs_file,\n\t\t\tcolor_channel=self.color_channel,\n\t\t\tresize=self.resize,\n\t\t\tpadding_value=self.padding_value,\n\t\t\tcrop_range=self.crop_range,\n\t\t\tflip_hor=self.flip_hor,\n\t\t\trotate=self.rotate,\n\t\t\tangle=self.angle,\n\t\t\tnoise_std=self.noise_std,\n\t\t\tnormalize=self.normalize,\n\t\t\tone_hot=self.one_hot,\n\t\t\tis_training=self.is_training,\n\t\t)\n\n\t@property\n\tdef loader(self):\n\t\treturn DataLoader(\n\t\t\tself.dataset,\n\t\t\tbatch_size=self.batch_size,\n\t\t\tshuffle=self.shuffle,\n\t\t\tnum_workers=self.n_workers,\n\t\t\tpin_memory=self.pin_memory,\n\t\t)\n\n\n#------------------------------------------------------------------------------\n#\tDataset for Semantic Segmentation\n#------------------------------------------------------------------------------\nclass SegmentationDataset(Dataset):\n\t""""""\n\tThe dataset requires label is a grayscale image with value {0,1,...,C-1},\n\twhere C is the number of classes.\n\t""""""\n\tdef __init__(self, pairs_file, color_channel=""RGB"", resize=512, padding_value=0,\n\t\tis_training=True, noise_std=5, crop_range=[0.75, 1.0], flip_hor=0.5, rotate=0.3, angle=10,\n\t\tone_hot=False, normalize=True, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]):\n\n\t\t# Get list of image and label files\n\t\tself.image_files, self.label_files = [], []\n\t\tfp = open(pairs_file, ""r"")\n\t\tlines = fp.read().split(""\\n"")\n\t\tlines = [line.strip() for line in lines if len(line)]\n\t\tlines = [line.split("", "") for line in lines]\n\n\t\tprint(""[Dataset] Checking file paths..."")\n\t\terror_flg = False\n\t\tfor line in lines:\n\t\t\timage_file, label_file = line\n\t\t\tif not os.path.exists(image_file):\n\t\t\t\tprint(""%s does not exist!"" % (image_file))\n\t\t\t\terror_flg = True\n\t\t\tif not os.path.exists(label_file):\n\t\t\t\tprint(""%s does not exist!"" % (label_file))\n\t\t\t\terror_flg = True\n\t\t\tself.image_files.append(image_file)\n\t\t\tself.label_files.append(label_file)\n\t\tif error_flg:\n\t\t\traise ValueError(""Some file paths are corrupted! Please re-check your file paths!"")\n\t\tprint(""[Dataset] Number of sample pairs:"", len(self.image_files))\n\n\t\t# Parameters\n\t\tself.color_channel = color_channel\n\t\tself.resize = resize\n\t\tself.padding_value = padding_value\n\t\tself.is_training = is_training\n\t\tself.noise_std = noise_std\n\t\tself.crop_range = crop_range\n\t\tself.flip_hor = flip_hor\n\t\tself.rotate = rotate\n\t\tself.angle = angle\n\t\tself.one_hot = one_hot\n\t\tself.normalize = normalize\n\t\tself.mean = np.array(mean)[None,None,:]\n\t\tself.std = np.array(std)[None,None,:]\n\n\tdef __len__(self):\n\t\treturn len(self.image_files)\n\n\tdef __getitem__(self, idx):\n\t\t# Read image and label\n\t\timg_file, label_file = self.image_files[idx], self.label_files[idx]\n\t\timage = cv2.imread(img_file)[...,::-1]\n\t\tlabel = cv2.imread(label_file, 0)\n\n\t\t# Augmentation if in training phase\n\t\tif self.is_training:\n\t\t\timage = transforms.random_noise(image, std=self.noise_std)\n\t\t\timage, label = transforms.flip_horizon(image, label, self.flip_hor)\n\t\t\timage, label = transforms.rotate_90(image, label, self.rotate)\n\t\t\timage, label = transforms.rotate_angle(image, label, self.angle)\n\t\t\timage, label = transforms.random_crop(image, label, self.crop_range)\n\n\t\t# Resize: the greater side is refered, the rest is padded\n\t\timage = transforms.resize_image(image, expected_size=self.resize, pad_value=self.padding_value, mode=cv2.INTER_LINEAR)\n\t\tlabel = transforms.resize_image(label, expected_size=self.resize, pad_value=self.padding_value, mode=cv2.INTER_NEAREST)\n\n\t\t# Preprocess image\n\t\tif self.normalize:\n\t\t\timage = image.astype(np.float32) / 255.0\n\t\t\timage = (image - self.mean) / self.std\n\t\timage = np.transpose(image, axes=(2, 0, 1))\n\n\t\t# Preprocess label\n\t\tlabel[label>0] = 1\n\t\tif self.one_hot:\n\t\t\tlabel = (np.arange(label.max()+1) == label[...,None]).astype(int)\n\n\t\t# Convert to tensor and return\n\t\timage = torch.tensor(image.copy(), dtype=torch.float32)\n\t\tlabel = torch.tensor(label.copy(), dtype=torch.float32)\n\t\treturn image, label'"
dataloaders/transforms.py,0,"b'#------------------------------------------------------------------------------\n#   Library\n#------------------------------------------------------------------------------\nimport cv2\nimport numpy as np\n\n\n#------------------------------------------------------------------------------\n#   Random crop\n#------------------------------------------------------------------------------\ndef random_crop(image, label, crop_range):\n\t""""""\n\tcropped image is a square.\n\n\timage (ndarray) with shape [H,W,3]\n\tlabel (ndarray) with shape [H,W]\n\tcrop_ratio (list) contains 2 bounds\n\t""""""\n\t##### Exception #####\n\tif crop_range[0]==crop_range[1] and crop_range[0]==1.0:\n\t\treturn image, label\n\n\t# Get random crop_ratio\n\tcrop_ratio = np.random.choice(np.linspace(crop_range[0], crop_range[1], num=10), size=())\n\t\n\t# Get random coordinates\n\tH, W = label.shape\n\tsize = H if H<W else W\n\tsize = int(size*crop_ratio)\n\tmax_i, max_j = H-size, W-size\n\ti = np.random.choice(np.arange(0, max_i+1), size=())\n\tj = np.random.choice(np.arange(0, max_j+1), size=())\n\n\t# Crop\n\timage_cropped = image[i:i+size, j:j+size, :]\n\tlabel_cropped = label[i:i+size, j:j+size]\n\treturn image_cropped, label_cropped\n\n\n#------------------------------------------------------------------------------\n#   Horizontal flip\n#------------------------------------------------------------------------------\ndef flip_horizon(image, label, prob):\n\tif prob:\n\t\tif np.random.choice([False, True], size=(), p=[1-prob, prob]):\n\t\t\timage = np.flip(image, axis=1)\n\t\t\tlabel = np.flip(label, axis=1)\n\treturn image, label\n\n\n#------------------------------------------------------------------------------\n#   Rotate 90\n#------------------------------------------------------------------------------\ndef rotate_90(image, label, prob):\n\tif prob:\n\t\tk = np.random.choice([-1, 0, 1], size=(), p=[prob/2, 1-prob, prob/2])\n\t\tif k:\n\t\t\timage = np.rot90(image, k=k, axes=(0,1))\n\t\t\tlabel = np.rot90(label, k=k, axes=(0,1))\n\treturn image, label\n\n\n#------------------------------------------------------------------------------\n#   Rotate angle\n#------------------------------------------------------------------------------\ndef rotate_angle(image, label, angle_max):\n\tif angle_max:\n\t\t# Random angle in range [-angle_max, angle_max]\n\t\tangle = np.random.choice(np.linspace(-angle_max, angle_max, num=21), size=())\n\n\t\t# Get parameters for affine transform\n\t\t(h, w) = image.shape[:2]\n\t\t(cX, cY) = (w // 2, h // 2)\n\n\t\tM = cv2.getRotationMatrix2D((cX, cY), angle, 1.0)\n\t\tcos = np.abs(M[0, 0])\n\t\tsin = np.abs(M[0, 1])\n\n\t\tnW = int((h * sin) + (w * cos))\n\t\tnH = int((h * cos) + (w * sin))\n\n\t\tM[0, 2] += (nW / 2) - cX\n\t\tM[1, 2] += (nH / 2) - cY\n\n\t\t# Perform transform\n\t\timage = cv2.warpAffine(image, M, (nW, nH))\n\t\tlabel = cv2.warpAffine(label, M, (nW, nH))\n\treturn image, label\n\n\n#------------------------------------------------------------------------------\n#  Gaussian noise\n#------------------------------------------------------------------------------\ndef random_noise(image, std):\n\tif std:\n\t\tnoise = np.random.normal(0, std, size=image.shape)\n\t\timage = image + noise\n\t\timage[image<0] = 0\n\t\timage[image>255] = 255\n\t\timage = image.astype(np.uint8)\n\treturn image\n\n\n#------------------------------------------------------------------------------\n#  Resize image\n#------------------------------------------------------------------------------\ndef resize_image(image, expected_size, pad_value, ret_params=False, mode=cv2.INTER_LINEAR):\n\t""""""\n\timage (ndarray) with either shape of [H,W,3] for RGB or [H,W] for grayscale.\n\tPadding is added so that the content of image is in the center.\n\t""""""\n\th, w = image.shape[:2]\n\tif w>h:\n\t\tw_new = int(expected_size)\n\t\th_new = int(h * w_new / w)\n\t\timage = cv2.resize(image, (w_new, h_new), interpolation=mode)\n\n\t\tpad_up = (w_new - h_new) // 2\n\t\tpad_down = w_new - h_new - pad_up\n\t\tif len(image.shape)==3:\n\t\t\tpad_width = ((pad_up, pad_down), (0,0), (0,0))\n\t\t\tconstant_values=((pad_value, pad_value), (0,0), (0,0))\n\t\telif len(image.shape)==2:\n\t\t\tpad_width = ((pad_up, pad_down), (0,0))\n\t\t\tconstant_values=((pad_value, pad_value), (0,0))\n\n\t\timage = np.pad(\n\t\t\timage,\n\t\t\tpad_width=pad_width,\n\t\t\tmode=""constant"",\n\t\t\tconstant_values=constant_values,\n\t\t)\n\t\tif ret_params:\n\t\t\treturn image, pad_up, 0, h_new, w_new\n\t\telse:\n\t\t\treturn image\n\n\telif w<h:\n\t\th_new = int(expected_size)\n\t\tw_new = int(w * h_new / h)\n\t\timage = cv2.resize(image, (w_new, h_new), interpolation=mode)\n\n\t\tpad_left = (h_new - w_new) // 2\n\t\tpad_right = h_new - w_new - pad_left\n\t\tif len(image.shape)==3:\n\t\t\tpad_width = ((0,0), (pad_left, pad_right), (0,0))\n\t\t\tconstant_values=((0,0), (pad_value, pad_value), (0,0))\n\t\telif len(image.shape)==2:\n\t\t\tpad_width = ((0,0), (pad_left, pad_right))\n\t\t\tconstant_values=((0,0), (pad_value, pad_value))\n\n\t\timage = np.pad(\n\t\t\timage,\n\t\t\tpad_width=pad_width,\n\t\t\tmode=""constant"",\n\t\t\tconstant_values=constant_values,\n\t\t)\n\t\tif ret_params:\n\t\t\treturn image, 0, pad_left, h_new, w_new\n\t\telse:\n\t\t\treturn image\n\n\telse:\n\t\timage = cv2.resize(image, (expected_size, expected_size), interpolation=mode)\n\t\tif ret_params:\n\t\t\treturn image, 0, 0, expected_size, expected_size\n\t\telse:\n\t\t\treturn image\n'"
dataset/create_pairs.py,0,"b'#------------------------------------------------------------------------------\n#\tLibraries\n#------------------------------------------------------------------------------\nimport os, cv2\nimport numpy as np\nfrom glob import glob\nfrom tqdm import tqdm\nfrom random import shuffle, seed\nfrom multiprocessing import Pool, Manager\n\n\n#------------------------------------------------------------------------------\n#\tMain execution\n#------------------------------------------------------------------------------\n# Get files\nimage_files  = sorted(glob(""/media/antiaegis/storing/datasets/HumanSeg/EG/data_for_run/images/*.*""))\nimage_files += sorted(glob(""/media/antiaegis/storing/datasets/HumanSeg/Supervisely/data_for_run/images/*.*""))\n\nlabel_files  = sorted(glob(""/media/antiaegis/storing/datasets/HumanSeg/EG/data_for_run/labels/*.*""))\nlabel_files += sorted(glob(""/media/antiaegis/storing/datasets/HumanSeg/Supervisely/data_for_run/labels/*.*""))\n\nassert len(image_files)==len(label_files)\nn_files = len(image_files)\n\n# Shuffle\nseed(0)\nshuffle(image_files)\nseed(0)\nshuffle(label_files)\n\n# Count number of pixels belong to categories\nmanager = Manager()\nforegrounds = manager.list([])\nbackgrounds = manager.list([])\n\ndef pool_func(args):\n    label_file = args\n    img = cv2.imread(label_file, 0)\n    foreground = np.sum((img>0).astype(np.uint8)) / img.size\n    background = np.sum((img==0).astype(np.uint8)) / img.size\n    foregrounds.append(foreground)\n    backgrounds.append(background)\n\npools = Pool(processes=8)\nargs = label_files\nfor _ in tqdm(pools.imap_unordered(pool_func, args), total=len(label_files)):\n    pass\n\nforegrounds = [element for element in foregrounds]\nbackgrounds = [element for element in backgrounds]\nprint(""foregrounds:"", sum(foregrounds)/n_files)\nprint(""backgrounds:"", sum(backgrounds)/n_files)\nprint(""ratio:"", sum(foregrounds) / sum(backgrounds))\n\n# Divide into 3 groups: small, averg, and large\nRATIO = [0.2, 0.8]\naverg_ind = []\nfor idx, foreground in enumerate(foregrounds):\n    if RATIO[0] <= foreground <= RATIO[1]:\n        averg_ind.append(idx)\nprint(""Number of averg indices:"", len(averg_ind))\n\n# Split train/valid\nRATIO = 0.9\nTRAIN_FILE = ""dataset/antiaegis_train_mask.txt""\nVALID_FILE = ""dataset/antiaegis_valid_mask.txt""\n\nshuffle(averg_ind)\nind_train = averg_ind[:int(RATIO*len(averg_ind))]\nind_valid = averg_ind[int(RATIO*len(averg_ind)):]\nprint(""Number of training samples:"", len(ind_train))\nprint(""Number of validating samples:"", len(ind_valid))\n\nfp = open(TRAIN_FILE, ""w"")\nfor idx in ind_train:\n    image_file, label_file = image_files[idx], label_files[idx]\n    line = ""%s, %s"" % (image_file, label_file)\n    fp.writelines(line + ""\\n"")\n\nfp = open(VALID_FILE, ""w"")\nfor idx in ind_valid:\n    image_file, label_file = image_files[idx], label_files[idx]\n    line = ""%s, %s"" % (image_file, label_file)\n    fp.writelines(line + ""\\n"")\n\n\n#------------------------------------------------------------------------------\n#   Check training dataset\n#------------------------------------------------------------------------------\nfp = open(TRAIN_FILE, \'r\')\nlines = fp.read().split(""\\n"")\nlines = [line.strip() for line in lines if len(line)]\nlines = [line.split("", "") for line in lines]\n\nprint(""Checking %d training samples..."" % (len(lines)))\nfor line in lines:\n    image_file, label_file = line\n    if not os.path.exists(image_file):\n        print(""%s does not exist!"" % (image_file))\n    if not os.path.exists(label_file):\n        print(""%s does not exist!"" % (label_file))\n\n\n#------------------------------------------------------------------------------\n#   Check validating dataset\n#------------------------------------------------------------------------------\nfp = open(VALID_FILE, \'r\')\nlines = fp.read().split(""\\n"")\nlines = [line.strip() for line in lines if len(line)]\nlines = [line.split("", "") for line in lines]\n\nprint(""Checking %d validating samples..."" % (len(lines)))\nfor line in lines:\n    image_file, label_file = line\n    if not os.path.exists(image_file):\n        print(""%s does not exist!"" % (image_file))\n    if not os.path.exists(label_file):\n        print(""%s does not exist!"" % (label_file))'"
evaluation/losses.py,21,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn.functional as F\n\n\n#------------------------------------------------------------------------------\n#   Fundamental losses\n#------------------------------------------------------------------------------\ndef dice_loss(logits, targets, smooth=1.0):\n\t""""""\n\tlogits: (torch.float32)  shape (N, C, H, W)\n\ttargets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\n\t""""""\n\toutputs = F.softmax(logits, dim=1)\n\ttargets = torch.unsqueeze(targets, dim=1)\n\ttargets = torch.zeros_like(logits).scatter_(dim=1, index=targets.type(torch.int64), src=torch.tensor(1.0))\n\n\tinter = outputs * targets\n\tdice = 1 - ((2*inter.sum(dim=(2,3)) + smooth) / (outputs.sum(dim=(2,3))+targets.sum(dim=(2,3)) + smooth))\n\treturn dice.mean()\n\n\ndef dice_loss_with_sigmoid(sigmoid, targets, smooth=1.0):\n\t""""""\n\tsigmoid: (torch.float32)  shape (N, 1, H, W)\n\ttargets: (torch.float32) shape (N, H, W), value {0,1}\n\t""""""\n\toutputs = torch.squeeze(sigmoid, dim=1)\n\n\tinter = outputs * targets\n\tdice = 1 - ((2*inter.sum(dim=(1,2)) + smooth) / (outputs.sum(dim=(1,2))+targets.sum(dim=(1,2)) + smooth))\n\tdice = dice.mean()\n\treturn dice\n\n\ndef ce_loss(logits, targets):\n\t""""""\n\tlogits: (torch.float32)  shape (N, C, H, W)\n\ttargets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\n\t""""""\n\ttargets = targets.type(torch.int64)\n\tce_loss = F.cross_entropy(logits, targets)\n\treturn ce_loss\n\n\n#------------------------------------------------------------------------------\n#   Custom loss for BiSeNet\n#------------------------------------------------------------------------------\ndef custom_bisenet_loss(logits, targets):\n\t""""""\n\tlogits: (torch.float32) (main_out, feat_os16_sup, feat_os32_sup) of shape (N, C, H, W)\n\ttargets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\n\t""""""\n\tif type(logits)==tuple:\n\t\tmain_loss = ce_loss(logits[0], targets)\n\t\tos16_loss = ce_loss(logits[1], targets)\n\t\tos32_loss = ce_loss(logits[2], targets)\n\t\treturn main_loss + os16_loss + os32_loss\n\telse:\n\t\treturn ce_loss(logits, targets)\n\n\n#------------------------------------------------------------------------------\n#   Custom loss for PSPNet\n#------------------------------------------------------------------------------\ndef custom_pspnet_loss(logits, targets, alpha=0.4):\n\t""""""\n\tlogits: (torch.float32) (main_out, aux_out) of shape (N, C, H, W), (N, C, H/8, W/8)\n\ttargets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\n\t""""""\n\tif type(logits)==tuple:\n\t\twith torch.no_grad():\n\t\t\t_targets = torch.unsqueeze(targets, dim=1)\n\t\t\taux_targets = F.interpolate(_targets, size=logits[1].shape[-2:], mode=\'bilinear\', align_corners=True)[:,0,...]\n\n\t\tmain_loss = ce_loss(logits[0], targets)\n\t\taux_loss = ce_loss(logits[1], aux_targets)\n\t\treturn main_loss + alpha*aux_loss\n\telse:\n\t\treturn ce_loss(logits, targets)\n\n\n#------------------------------------------------------------------------------\n#   Custom loss for ICNet\n#------------------------------------------------------------------------------\ndef custom_icnet_loss(logits, targets, alpha=[0.4, 0.16]):\n\t""""""\n\tlogits: (torch.float32)\n\t\t[train_mode] (x_124_cls, x_12_cls, x_24_cls) of shape\n\t\t\t\t\t\t(N, C, H/4, W/4), (N, C, H/8, W/8), (N, C, H/16, W/16)\n\n\t\t[valid_mode] x_124_cls of shape (N, C, H, W)\n\n\ttargets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\n\t""""""\n\tif type(logits)==tuple:\n\t\twith torch.no_grad():\n\t\t\ttargets = torch.unsqueeze(targets, dim=1)\n\t\t\ttarget1 = F.interpolate(targets, size=logits[0].shape[-2:], mode=\'bilinear\', align_corners=True)[:,0,...]\n\t\t\ttarget2 = F.interpolate(targets, size=logits[1].shape[-2:], mode=\'bilinear\', align_corners=True)[:,0,...]\n\t\t\ttarget3 = F.interpolate(targets, size=logits[2].shape[-2:], mode=\'bilinear\', align_corners=True)[:,0,...]\n\n\t\tloss1 = ce_loss(logits[0], target1)\n\t\tloss2 = ce_loss(logits[1], target2)\n\t\tloss3 = ce_loss(logits[2], target3)\n\t\treturn loss1 + alpha[0]*loss2 + alpha[1]*loss3\n\n\telse:\n\t\treturn ce_loss(logits, targets)'"
evaluation/metrics.py,22,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport torch\nfrom torch.nn import functional as F\n\n\n#------------------------------------------------------------------------------\n#   Fundamental metrics\n#------------------------------------------------------------------------------\ndef miou(logits, targets, eps=1e-6):\n\t""""""\n\tlogits: (torch.float32)  shape (N, C, H, W)\n\ttargets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\n\t""""""\n\toutputs = torch.argmax(logits, dim=1, keepdim=True).type(torch.int64)\n\ttargets = torch.unsqueeze(targets, dim=1).type(torch.int64)\n\toutputs = torch.zeros_like(logits).scatter_(dim=1, index=outputs, src=torch.tensor(1.0)).type(torch.int8)\n\ttargets = torch.zeros_like(logits).scatter_(dim=1, index=targets, src=torch.tensor(1.0)).type(torch.int8)\n\n\tinter = (outputs & targets).type(torch.float32).sum(dim=(2,3))\n\tunion = (outputs | targets).type(torch.float32).sum(dim=(2,3))\n\tiou = inter / (union + eps)\n\treturn iou.mean()\n\n\ndef iou_with_sigmoid(sigmoid, targets, eps=1e-6):\n\t""""""\n\tsigmoid: (torch.float32) shape (N, 1, H, W)\n\ttargets: (torch.float32) shape (N, H, W), value {0,1}\n\t""""""\n\toutputs = torch.squeeze(sigmoid, dim=1).type(torch.int8)\n\ttargets = targets.type(torch.int8)\n\n\tinter = (outputs & targets).type(torch.float32).sum(dim=(1,2))\n\tunion = (outputs | targets).type(torch.float32).sum(dim=(1,2))\n\tiou = inter / (union + eps)\n\treturn iou.mean()\n\n\n#------------------------------------------------------------------------------\n#   Custom IoU for BiSeNet\n#------------------------------------------------------------------------------\ndef custom_bisenet_miou(logits, targets):\n\t""""""\n\tlogits: (torch.float32) (main_out, feat_os16_sup, feat_os32_sup) of shape (N, C, H, W)\n\ttargets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\n\t""""""\n\tif type(logits)==tuple:\n\t\treturn miou(logits[0], targets)\n\telse:\n\t\treturn miou(logits, targets)\n\n\n#------------------------------------------------------------------------------\n#   Custom IoU for PSPNet\n#------------------------------------------------------------------------------\ndef custom_pspnet_miou(logits, targets):\n\t""""""\n\tlogits: (torch.float32) (main_out, aux_out) of shape (N, C, H, W), (N, C, H/8, W/8)\n\ttargets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\n\t""""""\n\tif type(logits)==tuple:\n\t\treturn miou(logits[0], targets)\n\telse:\n\t\treturn miou(logits, targets)\n\n\n#------------------------------------------------------------------------------\n#   Custom IoU for BiSeNet\n#------------------------------------------------------------------------------\ndef custom_icnet_miou(logits, targets):\n\t""""""\n\tlogits: (torch.float32)\n\t\t[train_mode] (x_124_cls, x_12_cls, x_24_cls) of shape\n\t\t\t\t\t\t(N, C, H/4, W/4), (N, C, H/8, W/8), (N, C, H/16, W/16)\n\n\t\t[valid_mode] x_124_cls of shape (N, C, H, W)\n\n\ttargets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\n\t""""""\n\tif type(logits)==tuple:\n\t\ttargets = torch.unsqueeze(targets, dim=1)\n\t\ttargets = F.interpolate(targets, size=logits[0].shape[-2:], mode=\'bilinear\', align_corners=True)[:,0,...]\n\t\treturn miou(logits[0], targets)\n\telse:\n\t\treturn miou(logits, targets)'"
models/BiSeNet.py,10,"b""#------------------------------------------------------------------------------\n#  Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom base.base_model import BaseModel\nfrom models.backbonds import ResNet\n\n\n#------------------------------------------------------------------------------\n#  Convolutional block\n#------------------------------------------------------------------------------\nclass ConvBlock(nn.Module):\n\tdef __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n\t\tsuper(ConvBlock, self).__init__()\n\t\tself.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n\t\tself.bn = nn.BatchNorm2d(out_channels)\n\n\tdef forward(self, input):\n\t\tx = self.conv(input)\n\t\tx = self.bn(x)\n\t\tx = F.relu(x, inplace=True)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  Spatial Path\n#------------------------------------------------------------------------------\nclass SpatialPath(nn.Module):\n\tdef __init__(self, basic_channels=64):\n\t\tsuper(SpatialPath, self).__init__()\n\t\tself.conv1 = ConvBlock(in_channels=3, out_channels=basic_channels , kernel_size=3, stride=2, padding=1)\n\t\tself.conv2 = ConvBlock(in_channels=basic_channels, out_channels=2*basic_channels, kernel_size=3, stride=2, padding=1)\n\t\tself.conv3 = ConvBlock(in_channels=2*basic_channels, out_channels=4*basic_channels, kernel_size=3, stride=2, padding=1)\n\n\tdef forward(self, input):\n\t\tx = self.conv1(input)\n\t\tx = self.conv2(x)\n\t\tx = self.conv3(x)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  Attention Refinement Module\n#------------------------------------------------------------------------------\nclass Attention(nn.Module):\n\tdef __init__(self, in_channels):\n\t\tsuper(Attention, self).__init__()\n\t\tself.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n\n\tdef forward(self, input):\n\t\tx = F.adaptive_avg_pool2d(input, (1,1))\n\t\tx = self.conv(x)\n\t\tx = torch.sigmoid(x)\n\t\tx = torch.mul(input, x)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  Feature Fusion Module\n#------------------------------------------------------------------------------\nclass Fusion(nn.Module):\n\tdef __init__(self, in_channels1, in_channels2, num_classes, kernel_size=3):\n\t\tsuper(Fusion, self).__init__()\n\t\tin_channels = in_channels1 + in_channels2\n\t\tself.convblock = ConvBlock(in_channels, num_classes, kernel_size, padding=1)\n\t\tself.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1, bias=False)\n\t\tself.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1, bias=False)\n\n\tdef forward(self, input1, input2):\n\t\tinput = torch.cat([input1, input2], dim=1)\n\t\tinput = self.convblock(input)\n\t\tx = F.adaptive_avg_pool2d(input, (1,1))\n\t\tx = self.conv1(x)\n\t\tx = F.relu(x, inplace=True)\n\t\tx = self.conv2(x)\n\t\tx = torch.sigmoid(x)\n\t\tx = torch.mul(input, x)\n\t\tx = torch.add(input, x)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  BiSeNet\n#------------------------------------------------------------------------------\nclass BiSeNet(BaseModel):\n\tdef __init__(self, backbone='resnet18', num_classes=2, pretrained_backbone=None):\n\t\tsuper(BiSeNet, self).__init__()\n\t\tif backbone=='resnet18':\n\t\t\tself.spatial_path = SpatialPath(basic_channels=64)\n\t\t\tself.context_path = ResNet.resnet18(num_classes=None)\n\t\t\tself.arm_os16 = Attention(in_channels=256)\n\t\t\tself.arm_os32 = Attention(in_channels=512)\n\t\t\tself.ffm = Fusion(in_channels1=256, in_channels2=768, num_classes=num_classes, kernel_size=3)\n\t\t\tself.conv_final = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n\t\t\tself.sup_os16 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n\t\t\tself.sup_os32 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n\t\telse:\n\t\t\traise NotImplementedError\n\n\t\tself._init_weights()\n\t\tif pretrained_backbone is not None:\n\t\t\tself.context_path._load_pretrained_model(pretrained_backbone)\n\n\n\tdef forward(self, input):\n\t\t# Spatial path\n\t\tfeat_spatial = self.spatial_path(input)\n\n\t\t# Context path\n\t\tfeat_os32, feat_os16 = self._run_context_path(input)\n\t\tfeat_gap = F.adaptive_avg_pool2d(feat_os32, (1,1))\n\n\t\tfeat_os16 = self.arm_os16(feat_os16)\n\t\tfeat_os32 = self.arm_os32(feat_os32)\n\t\tfeat_os32 = torch.mul(feat_os32, feat_gap)\n\n\t\tfeat_os16 = F.interpolate(feat_os16, scale_factor=2, mode='bilinear', align_corners=True)\n\t\tfeat_os32 = F.interpolate(feat_os32, scale_factor=4, mode='bilinear', align_corners=True)\n\t\tfeat_context = torch.cat([feat_os16, feat_os32], dim=1)\n\n\t\t# Supervision\n\t\tif self.training:\n\t\t\tfeat_os16_sup = self.sup_os16(feat_os16)\n\t\t\tfeat_os32_sup = self.sup_os32(feat_os32)\n\t\t\tfeat_os16_sup = F.interpolate(feat_os16_sup, scale_factor=8, mode='bilinear', align_corners=True)\n\t\t\tfeat_os32_sup = F.interpolate(feat_os32_sup, scale_factor=8, mode='bilinear', align_corners=True)\n\n\t\t# Fusion\n\t\tx = self.ffm(feat_spatial, feat_context)\n\t\tx = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)\n\t\tx = self.conv_final(x)\n\n\t\t# Output\n\t\tif self.training:\n\t\t\treturn x, feat_os16_sup, feat_os32_sup\n\t\telse:\n\t\t\treturn x\n\n\n\tdef _run_context_path(self, input):\n\t\t# Stage1\n\t\tx1 = self.context_path.conv1(input)\n\t\tx1 = self.context_path.bn1(x1)\n\t\tx1 = self.context_path.relu(x1)\n\t\t# Stage2\n\t\tx2 = self.context_path.maxpool(x1)\n\t\tx2 = self.context_path.layer1(x2)\n\t\t# Stage3\n\t\tx3 = self.context_path.layer2(x2)\n\t\t# Stage4\n\t\tx4 = self.context_path.layer3(x3)\n\t\t# Stage5\n\t\tx5 = self.context_path.layer4(x4)\n\t\t# Output\n\t\treturn x5, x4\n\n\n\tdef _init_weights(self):\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.Conv2d):\n\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\t\t\t\tif m.bias is not None:\n\t\t\t\t\tm.bias.data.zero_()\n\t\t\telif isinstance(m, nn.BatchNorm2d):\n\t\t\t\tnn.init.constant_(m.weight, 1)\n\t\t\t\tnn.init.constant_(m.bias, 0)"""
models/DeepLab.py,4,"b""#------------------------------------------------------------------------------\n#  Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom base.base_model import BaseModel\nfrom models.backbonds import ResNet, VGG\n\n\n#------------------------------------------------------------------------------\n#  ASSP\n#------------------------------------------------------------------------------\nclass _ASPPModule(nn.Module):\n\tdef __init__(self, inplanes, planes, kernel_size, padding, dilation):\n\t\tsuper(_ASPPModule, self).__init__()\n\t\tself.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size, stride=1, padding=padding, dilation=dilation, bias=False)\n\t\tself.bn = nn.BatchNorm2d(planes)\n\t\tself.relu = nn.ReLU(inplace=True)\n\n\tdef forward(self, x):\n\t\tx = self.atrous_conv(x)\n\t\tx = self.bn(x)\n\t\treturn self.relu(x)\n\n\nclass ASPP(nn.Module):\n\tdef __init__(self, output_stride, inplanes):\n\t\tsuper(ASPP, self).__init__()\n\n\t\tif output_stride == 16:\n\t\t\tdilations = [1, 6, 12, 18]\n\t\telif output_stride == 8:\n\t\t\tdilations = [1, 12, 24, 36]\n\n\t\tself.aspp1 = _ASPPModule(inplanes, 256, 1, padding=0, dilation=dilations[0])\n\t\tself.aspp2 = _ASPPModule(inplanes, 256, 3, padding=dilations[1], dilation=dilations[1])\n\t\tself.aspp3 = _ASPPModule(inplanes, 256, 3, padding=dilations[2], dilation=dilations[2])\n\t\tself.aspp4 = _ASPPModule(inplanes, 256, 3, padding=dilations[3], dilation=dilations[3])\n\n\t\tself.global_avg_pool = nn.Sequential(\n\t\t\tnn.AdaptiveAvgPool2d((1, 1)),\n\t\t\tnn.Conv2d(inplanes, 256, 1, stride=1, bias=False),\n\t\t\tnn.BatchNorm2d(256),\n\t\t\tnn.ReLU(inplace=True),\n\t\t)\n\t\tself.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n\t\tself.bn1 = nn.BatchNorm2d(256)\n\t\tself.relu = nn.ReLU(inplace=True)\n\t\tself.dropout = nn.Dropout(0.5)\n\n\tdef forward(self, x):\n\t\tx1 = self.aspp1(x)\n\t\tx2 = self.aspp2(x)\n\t\tx3 = self.aspp3(x)\n\t\tx4 = self.aspp4(x)\n\t\tx5 = self.global_avg_pool(x)\n\t\tx5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n\t\tx = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n\t\tx = self.conv1(x)\n\t\tx = self.bn1(x)\n\t\tx = self.relu(x)\n\t\treturn self.dropout(x)\n\n\n#------------------------------------------------------------------------------\n#  Decoder\n#------------------------------------------------------------------------------\nclass Decoder(nn.Module):\n\tdef __init__(self, num_classes, low_level_inplanes):\n\t\tsuper(Decoder, self).__init__()\n\n\t\tself.conv1 = nn.Conv2d(low_level_inplanes, 48, 1, bias=False)\n\t\tself.bn1 = nn.BatchNorm2d(48)\n\t\tself.relu = nn.ReLU(inplace=True)\n\t\tself.last_conv = nn.Sequential(\n\t\t\tnn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n\t\t\tnn.BatchNorm2d(256),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Dropout(0.5),\n\t\t\tnn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n\t\t\tnn.BatchNorm2d(256),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Dropout(0.1),\n\t\t\tnn.Conv2d(256, num_classes, kernel_size=1, stride=1),\n\t\t)\n\n\tdef forward(self, x, low_level_feat):\n\t\tlow_level_feat = self.conv1(low_level_feat)\n\t\tlow_level_feat = self.bn1(low_level_feat)\n\t\tlow_level_feat = self.relu(low_level_feat)\n\n\t\tx = F.interpolate(x, size=low_level_feat.size()[2:], mode='bilinear', align_corners=True)\n\t\tx = torch.cat((x, low_level_feat), dim=1)\n\t\tx = self.last_conv(x)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  DeepLabV3Plus\n#------------------------------------------------------------------------------\nclass DeepLabV3Plus(BaseModel):\n\tdef __init__(self, backbone='resnet50', output_stride=16, num_classes=2, freeze_bn=False, pretrained_backbone=None):\n\t\tsuper(DeepLabV3Plus, self).__init__()\n\t\tif 'resnet' in backbone:\n\t\t\tif backbone=='resnet18':\n\t\t\t\tnum_layers = 18\n\t\t\t\tinplanes = 512\n\t\t\t\tlow_level_inplanes = 64\n\t\t\telif backbone=='resnet34':\n\t\t\t\tnum_layers = 34\n\t\t\t\tinplanes = 512\n\t\t\t\tlow_level_inplanes = 64\n\t\t\telif backbone=='resnet50':\n\t\t\t\tnum_layers = 50\n\t\t\t\tinplanes = 2048\n\t\t\t\tlow_level_inplanes = 256\n\t\t\telif backbone=='resnet101':\n\t\t\t\tnum_layers = 101\n\t\t\t\tinplanes = 2048\n\t\t\t\tlow_level_inplanes = 256\n\n\t\t\tself.backbone = ResNet.get_resnet(num_layers=num_layers, num_classes=None)\n\t\t\tself._run_backbone = self._run_backbone_resnet\n\t\t\tself.aspp = ASPP(output_stride, inplanes=inplanes)\n\t\t\tself.decoder = Decoder(num_classes, low_level_inplanes=low_level_inplanes)\n\n\t\telif backbone=='vgg16':\n\t\t\tself.backbone = VGG.vgg16_bn(output_stride=output_stride)\n\t\t\tself.aspp = ASPP(output_stride, inplanes=512)\n\t\t\tself.decoder = Decoder(num_classes, low_level_inplanes=256)\n\n\t\telse:\n\t\t\traise NotImplementedError\n\n\t\tself._init_weights()\n\t\tif pretrained_backbone is not None:\n\t\t\tself.backbone._load_pretrained_model(pretrained_backbone)\n\t\tif freeze_bn:\n\t\t\tself._freeze_bn()\n\n\n\tdef forward(self, input):\n\t\tx, low_feat = self._run_backbone(input)\n\t\tx = self.aspp(x)\n\t\tx = self.decoder(x, low_feat)\n\t\tx = F.interpolate(x, size=input.shape[-2:], mode='bilinear', align_corners=True)\n\t\treturn x\n\n\n\tdef _run_backbone_resnet(self, input):\n\t\t# Stage1\n\t\tx1 = self.backbone.conv1(input)\n\t\tx1 = self.backbone.bn1(x1)\n\t\tx1 = self.backbone.relu(x1)\n\t\t# Stage2\n\t\tx2 = self.backbone.maxpool(x1)\n\t\tx2 = self.backbone.layer1(x2)\n\t\t# Stage3\n\t\tx3 = self.backbone.layer2(x2)\n\t\t# Stage4\n\t\tx4 = self.backbone.layer3(x3)\n\t\t# Stage5\n\t\tx5 = self.backbone.layer4(x4)\n\t\t# Output\n\t\treturn x5, x2\n\n\n\tdef _freeze_bn(self):\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.BatchNorm2d):\n\t\t\t\tm.eval()\n\n\n\tdef _init_weights(self):\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.Conv2d):\n\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\t\t\t\tif m.bias is not None:\n\t\t\t\t\tm.bias.data.zero_()\n\t\t\telif isinstance(m, nn.BatchNorm2d):\n\t\t\t\tnn.init.constant_(m.weight, 1)\n\t\t\t\tnn.init.constant_(m.bias, 0)"""
models/ICNet.py,2,"b""#------------------------------------------------------------------------------\n#  Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nfrom base.base_model import BaseModel\nfrom models.backbonds import ResNet\n\n\n#------------------------------------------------------------------------------\n#  Convolutional block\n#------------------------------------------------------------------------------\nclass ConvBlock(nn.Module):\n\tdef __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n\t\tsuper(ConvBlock, self).__init__()\n\t\tself.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n\t\tself.bn = nn.BatchNorm2d(out_channels)\n\n\tdef forward(self, input):\n\t\tx = self.conv(input)\n\t\tx = self.bn(x)\n\t\tx = F.relu(x, inplace=True)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  Pyramid Pooling Module\n#------------------------------------------------------------------------------\nclass PyramidPoolingModule(nn.Module):\n\tdef __init__(self, pyramids=[1,2,3,6]):\n\t\tsuper(PyramidPoolingModule, self).__init__()\n\t\tself.pyramids = pyramids\n\n\tdef forward(self, input):\n\t\tfeat = input\n\t\theight, width = input.shape[2:]\n\t\tfor bin_size in self.pyramids:\n\t\t\tx = F.adaptive_avg_pool2d(input, output_size=bin_size)\n\t\t\tx = F.interpolate(x, size=(height, width), mode='bilinear', align_corners=True)\n\t\t\tfeat  = feat + x\n\t\treturn feat\n\n\n#------------------------------------------------------------------------------\n#  Cascade Feature Fusion\n#------------------------------------------------------------------------------\nclass CascadeFeatFusion(nn.Module):\n\tdef __init__(self, low_channels, high_channels, out_channels, num_classes):\n\t\tsuper(CascadeFeatFusion, self).__init__()\n\t\tself.conv_low = nn.Sequential(OrderedDict([\n\t\t\t('conv', nn.Conv2d(low_channels, out_channels, kernel_size=3, dilation=2, padding=2, bias=False)),\n\t\t\t('bn', nn.BatchNorm2d(out_channels))\n\t\t]))\n\t\tself.conv_high = nn.Sequential(OrderedDict([\n\t\t\t('conv', nn.Conv2d(high_channels, out_channels, kernel_size=1, bias=False)),\n\t\t\t('bn', nn.BatchNorm2d(out_channels))\n\t\t]))\n\t\tself.conv_low_cls = nn.Conv2d(out_channels, num_classes, kernel_size=1, bias=False)\n\n\tdef forward(self, input_low, input_high):\n\t\tinput_low = F.interpolate(input_low, size=input_high.shape[2:], mode='bilinear', align_corners=True)\n\t\tx_low = self.conv_low(input_low)\n\t\tx_high = self.conv_high(input_high)\n\t\tx = x_low + x_high\n\t\tx = F.relu(x, inplace=True)\n\n\t\tif self.training:\n\t\t\tx_low_cls = self.conv_low_cls(input_low)\n\t\t\treturn x, x_low_cls\n\t\telse:\n\t\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  ICNet\n#------------------------------------------------------------------------------\nclass ICNet(BaseModel):\n\tpyramids = [1, 2, 3, 6]\n\tbackbone_os = 8\n\n\tdef __init__(self, backbone='resnet18', num_classes=2, pretrained_backbone=None):\n\t\tsuper(ICNet, self).__init__()\n\t\tif 'resnet' in backbone:\n\t\t\tif backbone=='resnet18':\n\t\t\t\tn_layers = 18\n\t\t\t\tstage5_channels = 512\n\t\t\telif backbone=='resnet34':\n\t\t\t\tn_layers = 34\n\t\t\t\tstage5_channels = 512\n\t\t\telif backbone=='resnet50':\n\t\t\t\tn_layers = 50\n\t\t\t\tstage5_channels = 2048\n\t\t\telif backbone=='resnet101':\n\t\t\t\tn_layers = 101\n\t\t\t\tstage5_channels = 2048\n\t\t\telse:\n\t\t\t\traise NotImplementedError\n\n\t\t\t# Sub1\n\t\t\tself.conv_sub1 = nn.Sequential(OrderedDict([\n\t\t\t\t('conv1', ConvBlock(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False)),\n\t\t\t\t('conv2', ConvBlock(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False)),\n\t\t\t\t('conv3', ConvBlock(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1, bias=False))\n\t\t\t]))\n\n\t\t\t# Sub2 and Sub4\n\t\t\tself.backbone = ResNet.get_resnet(n_layers, output_stride=self.backbone_os, num_classes=None)\n\t\t\tself.ppm = PyramidPoolingModule(pyramids=self.pyramids)\n\t\t\tself.conv_sub4_reduce = ConvBlock(stage5_channels, stage5_channels//4, kernel_size=1, bias=False)\n\n\t\t\t# Cascade Feature Fusion\n\t\t\tself.cff_24 = CascadeFeatFusion(low_channels=stage5_channels//4, high_channels=128, out_channels=128, num_classes=num_classes)\n\t\t\tself.cff_12 = CascadeFeatFusion(low_channels=128, high_channels=64, out_channels=128, num_classes=num_classes)\n\n\t\t\t# Classification\n\t\t\tself.conv_cls = nn.Conv2d(in_channels=128, out_channels=num_classes, kernel_size=1, bias=False)\n\n\t\telse:\n\t\t\traise NotImplementedError\n\n\t\tself._init_weights()\n\t\tif pretrained_backbone is not None:\n\t\t\tself.backbone._load_pretrained_model(pretrained_backbone)\n\n\n\tdef forward(self, input):\n\t\t# Sub1\n\t\tx_sub1 = self.conv_sub1(input)\n\n\t\t# Sub2\n\t\tx_sub2 = F.interpolate(input, scale_factor=0.5, mode='bilinear', align_corners=True)\n\t\tx_sub2 = self._run_backbone_sub2(x_sub2)\n\n\t\t# Sub4\n\t\tx_sub4 = F.interpolate(x_sub2, scale_factor=0.5, mode='bilinear', align_corners=True)\n\t\tx_sub4 = self._run_backbone_sub4(x_sub4)\n\t\tx_sub4 = self.ppm(x_sub4)\n\t\tx_sub4 = self.conv_sub4_reduce(x_sub4)\n\n\t\t# Output\n\t\tif self.training:\n\t\t\t# Cascade Feature Fusion\n\t\t\tx_cff_24, x_24_cls = self.cff_24(x_sub4, x_sub2)\n\t\t\tx_cff_12, x_12_cls = self.cff_12(x_cff_24, x_sub1)\n\n\t\t\t# Classification\n\t\t\tx_cff_12 = F.interpolate(x_cff_12, scale_factor=2, mode='bilinear', align_corners=True)\n\t\t\tx_124_cls = self.conv_cls(x_cff_12)\n\t\t\treturn x_124_cls, x_12_cls, x_24_cls\n\n\t\telse:\n\t\t\t# Cascade Feature Fusion\n\t\t\tx_cff_24 = self.cff_24(x_sub4, x_sub2)\n\t\t\tx_cff_12 = self.cff_12(x_cff_24, x_sub1)\n\n\t\t\t# Classification\n\t\t\tx_cff_12 = F.interpolate(x_cff_12, scale_factor=2, mode='bilinear', align_corners=True)\n\t\t\tx_124_cls = self.conv_cls(x_cff_12)\n\t\t\tx_124_cls = F.interpolate(x_124_cls, scale_factor=4, mode='bilinear', align_corners=True)\n\t\t\treturn x_124_cls\n\n\n\tdef _run_backbone_sub2(self, input):\n\t\t# Stage1\n\t\tx = self.backbone.conv1(input)\n\t\tx = self.backbone.bn1(x)\n\t\tx = self.backbone.relu(x)\n\t\t# Stage2\n\t\tx = self.backbone.maxpool(x)\n\t\tx = self.backbone.layer1(x)\n\t\t# Stage3\n\t\tx = self.backbone.layer2(x)\n\t\treturn x\n\n\n\tdef _run_backbone_sub4(self, input):\n\t\t# Stage4\n\t\tx = self.backbone.layer3(input)\n\t\t# Stage5\n\t\tx = self.backbone.layer4(x)\n\t\treturn x\n\n\n\tdef _init_weights(self):\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.Conv2d):\n\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\t\t\t\tif m.bias is not None:\n\t\t\t\t\tm.bias.data.zero_()\n\t\t\telif isinstance(m, nn.BatchNorm2d):\n\t\t\t\tnn.init.constant_(m.weight, 1)\n\t\t\t\tnn.init.constant_(m.bias, 0)"""
models/PSPNet.py,3,"b'#------------------------------------------------------------------------------\n#  Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nfrom base.base_model import BaseModel\nfrom models.backbonds import ResNet\n\n\n#------------------------------------------------------------------------------\n#  Convolutional block\n#------------------------------------------------------------------------------\nclass ConvBlock(nn.Module):\n\tdef __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n\t\tsuper(ConvBlock, self).__init__()\n\t\tself.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n\t\tself.bn = nn.BatchNorm2d(out_channels)\n\n\tdef forward(self, input):\n\t\tx = self.conv(input)\n\t\tx = self.bn(x)\n\t\tx = F.relu(x, inplace=True)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  Pyramid Pooling Module\n#------------------------------------------------------------------------------\nclass PyramidPoolingModule(nn.Module):\n\tdef __init__(self, in_channels, pyramids=[1,2,3,6]):\n\t\tsuper(PyramidPoolingModule, self).__init__()\n\t\tself.pyramids = pyramids\n\t\tout_channels = in_channels // len(pyramids)\n\n\t\tself.convs = nn.ModuleList()\n\t\tfor _ in pyramids:\n\t\t\tconv = ConvBlock(in_channels, out_channels, kernel_size=1, bias=False)\n\t\t\tself.convs.append(conv)\n\n\tdef forward(self, input):\n\t\tfeat = [input]\n\t\theight, width = input.shape[2:]\n\t\tfor i, bin_size in enumerate(self.pyramids):\n\t\t\tx = F.adaptive_avg_pool2d(input, output_size=bin_size)\n\t\t\tx = self.convs[i](x)\n\t\t\tx = F.interpolate(x, size=(height, width), mode=\'bilinear\', align_corners=True)\n\t\t\tfeat.append(x)\n\t\tx = torch.cat(feat, dim=1)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  PSPNet\n#------------------------------------------------------------------------------\nclass PSPNet(BaseModel):\n\tdropout = 0.1\n\tpyramids = [1, 2, 3, 6]\n\tbackbone_os = 8\n\n\tdef __init__(self, backbone=\'resnet18\', num_classes=2, pretrained_backbone=None):\n\t\tsuper(PSPNet, self).__init__()\n\t\tif \'resnet\' in backbone:\n\t\t\tif backbone==\'resnet18\':\n\t\t\t\tn_layers = 18\n\t\t\t\tstage5_channels = 512\n\t\t\telif backbone==\'resnet34\':\n\t\t\t\tn_layers = 34\n\t\t\t\tstage5_channels = 512\n\t\t\telif backbone==\'resnet50\':\n\t\t\t\tn_layers = 50\n\t\t\t\tstage5_channels = 2048\n\t\t\telif backbone==\'resnet101\':\n\t\t\t\tn_layers = 101\n\t\t\t\tstage5_channels = 2048\n\t\t\telse:\n\t\t\t\traise NotImplementedError\n\n\t\t\tself.run_backbone = self._run_backbone_resnet\n\t\t\tself.backbone = ResNet.get_resnet(n_layers, output_stride=self.backbone_os, num_classes=None)\n\t\t\tself.pyramid = PyramidPoolingModule(in_channels=stage5_channels, pyramids=self.pyramids)\n\t\t\tself.main_output = nn.Sequential(OrderedDict([\n\t\t\t\t(""conv1"", ConvBlock(2*stage5_channels, stage5_channels//4, kernel_size=3, padding=1, bias=False)),\n\t\t\t\t(""dropout"", nn.Dropout2d(p=self.dropout)),\n\t\t\t\t(""conv2"", nn.Conv2d(stage5_channels//4, num_classes, kernel_size=1, bias=False)),\n\t\t\t]))\n\t\t\tself.aux_output = nn.Sequential(OrderedDict([\n\t\t\t\t(""conv1"", ConvBlock(stage5_channels//2, stage5_channels//8, kernel_size=3, padding=1, bias=False)),\n\t\t\t\t(""dropout"", nn.Dropout2d(p=self.dropout)),\n\t\t\t\t(""conv2"", nn.Conv2d(stage5_channels//8, num_classes, kernel_size=1, bias=False)),\n\t\t\t]))\n\t\telse:\n\t\t\traise NotImplementedError\n\n\t\tself.init_weights()\n\t\tif pretrained_backbone is not None:\n\t\t\tself.backbone.load_pretrained_model(pretrained_backbone)\n\n\n\tdef forward(self, input):\n\t\tinp_shape = input.shape[2:]\n\t\tif self.training:\n\t\t\tfeat_stage5, feat_stage4 = self.run_backbone(input)\n\t\t\tfeat_pyramid = self.pyramid(feat_stage5)\n\t\t\tmain_out = self.main_output(feat_pyramid)\n\t\t\tmain_out = F.interpolate(main_out, size=inp_shape, mode=\'bilinear\', align_corners=True)\n\t\t\taux_out = self.aux_output(feat_stage4)\n\t\t\treturn main_out, aux_out\n\t\telse:\n\t\t\tfeat_stage5 = self.run_backbone(input)\n\t\t\tfeat_pyramid = self.pyramid(feat_stage5)\n\t\t\tmain_out = self.main_output(feat_pyramid)\n\t\t\tmain_out = F.interpolate(main_out, size=inp_shape, mode=\'bilinear\', align_corners=True)\n\t\t\treturn main_out\n\n\n\tdef _run_backbone_resnet(self, input):\n\t\t# Stage1\n\t\tx1 = self.backbone.conv1(input)\n\t\tx1 = self.backbone.bn1(x1)\n\t\tx1 = self.backbone.relu(x1)\n\t\t# Stage2\n\t\tx2 = self.backbone.maxpool(x1)\n\t\tx2 = self.backbone.layer1(x2)\n\t\t# Stage3\n\t\tx3 = self.backbone.layer2(x2)\n\t\t# Stage4\n\t\tx4 = self.backbone.layer3(x3)\n\t\t# Stage5\n\t\tx5 = self.backbone.layer4(x4)\n\t\t# Output\n\t\tif self.training:\n\t\t\treturn x5, x4\n\t\telse:\n\t\t\treturn x5'"
models/UNet.py,3,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import reduce\n\nfrom base import BaseModel\nfrom models.backbonds import MobileNetV2, ResNet\n\n\n#------------------------------------------------------------------------------\n#   Decoder block\n#------------------------------------------------------------------------------\nclass DecoderBlock(nn.Module):\n\tdef __init__(self, in_channels, out_channels, block_unit):\n\t\tsuper(DecoderBlock, self).__init__()\n\t\tself.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, padding=1, stride=2)\n\t\tself.block_unit = block_unit\n\n\tdef forward(self, input, shortcut):\n\t\tx = self.deconv(input)\n\t\tx = torch.cat([x, shortcut], dim=1)\n\t\tx = self.block_unit(x)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#   Class of UNet\n#------------------------------------------------------------------------------\nclass UNet(BaseModel):\n\tdef __init__(self, backbone=""mobilenetv2"", num_classes=2, pretrained_backbone=None):\n\t\tsuper(UNet, self).__init__()\n\t\tif backbone==\'mobilenetv2\':\n\t\t\talpha = 1.0\n\t\t\texpansion = 6\n\t\t\tself.backbone = MobileNetV2.MobileNetV2(alpha=alpha, expansion=expansion, num_classes=None)\n\t\t\tself._run_backbone = self._run_backbone_mobilenetv2\n\t\t\t# Stage 1\n\t\t\tchannel1 = MobileNetV2._make_divisible(int(96*alpha), 8)\n\t\t\tblock_unit = MobileNetV2.InvertedResidual(2*channel1, channel1, 1, expansion)\n\t\t\tself.decoder1 = DecoderBlock(self.backbone.last_channel, channel1, block_unit)\n\t\t\t# Stage 2\n\t\t\tchannel2 = MobileNetV2._make_divisible(int(32*alpha), 8)\n\t\t\tblock_unit = MobileNetV2.InvertedResidual(2*channel2, channel2, 1, expansion)\n\t\t\tself.decoder2 = DecoderBlock(channel1, channel2, block_unit)\n\t\t\t# Stage 3\n\t\t\tchannel3 = MobileNetV2._make_divisible(int(24*alpha), 8)\n\t\t\tblock_unit = MobileNetV2.InvertedResidual(2*channel3, channel3, 1, expansion)\n\t\t\tself.decoder3 = DecoderBlock(channel2, channel3, block_unit)\n\t\t\t# Stage 4\n\t\t\tchannel4 = MobileNetV2._make_divisible(int(16*alpha), 8)\n\t\t\tblock_unit = MobileNetV2.InvertedResidual(2*channel4, channel4, 1, expansion)\n\t\t\tself.decoder4 = DecoderBlock(channel3, channel4, block_unit)\n\n\t\telif \'resnet\' in backbone:\n\t\t\tif backbone==\'resnet18\':\n\t\t\t\tn_layers = 18\n\t\t\telif backbone==\'resnet34\':\n\t\t\t\tn_layers = 34\n\t\t\telif backbone==\'resnet50\':\n\t\t\t\tn_layers = 50\n\t\t\telif backbone==\'resnet101\':\n\t\t\t\tn_layers = 101\n\t\t\telse:\n\t\t\t\traise NotImplementedError\n\t\t\tfilters = 64\n\t\t\tself.backbone = ResNet.get_resnet(n_layers, num_classes=None)\n\t\t\tself._run_backbone = self._run_backbone_resnet\n\t\t\tblock = ResNet.BasicBlock if (n_layers==18 or n_layers==34) else ResNet.Bottleneck\n\t\t\t# Stage 1\n\t\t\tlast_channel = 8*filters if (n_layers==18 or n_layers==34) else 32*filters\n\t\t\tchannel1 = 4*filters if (n_layers==18 or n_layers==34) else 16*filters\n\t\t\tdownsample = nn.Sequential(ResNet.conv1x1(2*channel1, channel1), nn.BatchNorm2d(channel1))\n\t\t\tblock_unit = block(2*channel1, int(channel1/block.expansion), 1, downsample)\n\t\t\tself.decoder1 = DecoderBlock(last_channel, channel1, block_unit)\n\t\t\t# Stage 2\n\t\t\tchannel2 = 2*filters if (n_layers==18 or n_layers==34) else 8*filters\n\t\t\tdownsample = nn.Sequential(ResNet.conv1x1(2*channel2, channel2), nn.BatchNorm2d(channel2))\n\t\t\tblock_unit = block(2*channel2, int(channel2/block.expansion), 1, downsample)\n\t\t\tself.decoder2 = DecoderBlock(channel1, channel2, block_unit)\n\t\t\t# Stage 3\n\t\t\tchannel3 = filters if (n_layers==18 or n_layers==34) else 4*filters\n\t\t\tdownsample = nn.Sequential(ResNet.conv1x1(2*channel3, channel3), nn.BatchNorm2d(channel3))\n\t\t\tblock_unit = block(2*channel3, int(channel3/block.expansion), 1, downsample)\n\t\t\tself.decoder3 = DecoderBlock(channel2, channel3, block_unit)\n\t\t\t# Stage 4\n\t\t\tchannel4 = filters\n\t\t\tdownsample = nn.Sequential(ResNet.conv1x1(2*channel4, channel4), nn.BatchNorm2d(channel4))\n\t\t\tblock_unit = block(2*channel4, int(channel4/block.expansion), 1, downsample)\n\t\t\tself.decoder4 = DecoderBlock(channel3, channel4, block_unit)\n\n\t\telse:\n\t\t\traise NotImplementedError\n\n\t\tself.conv_last = nn.Sequential(\n\t\t\tnn.Conv2d(channel4, 3, kernel_size=3, padding=1),\n\t\t\tnn.Conv2d(3, num_classes, kernel_size=3, padding=1),\n\t\t)\n\n\t\t# Initialize\n\t\tself._init_weights()\n\t\tif pretrained_backbone is not None:\n\t\t\tself.backbone._load_pretrained_model(pretrained_backbone)\n\n\n\tdef forward(self, input):\n\t\tx1, x2, x3, x4, x5 = self._run_backbone(input)\n\t\tx = self.decoder1(x5, x4)\n\t\tx = self.decoder2(x, x3)\n\t\tx = self.decoder3(x, x2)\n\t\tx = self.decoder4(x, x1)\n\t\tx = self.conv_last(x)\n\t\tx = F.interpolate(x, size=input.shape[-2:], mode=\'bilinear\', align_corners=True)\n\t\treturn x\n\n\n\tdef _run_backbone_mobilenetv2(self, input):\n\t\tx = input\n\t\t# Stage1\n\t\tx = reduce(lambda x, n: self.backbone.features[n](x), list(range(0,2)), x)\n\t\tx1 = x\n\t\t# Stage2\n\t\tx = reduce(lambda x, n: self.backbone.features[n](x), list(range(2,4)), x)\n\t\tx2 = x\n\t\t# Stage3\n\t\tx = reduce(lambda x, n: self.backbone.features[n](x), list(range(4,7)), x)\n\t\tx3 = x\n\t\t# Stage4\n\t\tx = reduce(lambda x, n: self.backbone.features[n](x), list(range(7,14)), x)\n\t\tx4 = x\n\t\t# Stage5\n\t\tx5 = reduce(lambda x, n: self.backbone.features[n](x), list(range(14,19)), x)\n\t\treturn x1, x2, x3, x4, x5\n\n\n\tdef _run_backbone_resnet(self, input):\n\t\t# Stage1\n\t\tx1 = self.backbone.conv1(input)\n\t\tx1 = self.backbone.bn1(x1)\n\t\tx1 = self.backbone.relu(x1)\n\t\t# Stage2\n\t\tx2 = self.backbone.maxpool(x1)\n\t\tx2 = self.backbone.layer1(x2)\n\t\t# Stage3\n\t\tx3 = self.backbone.layer2(x2)\n\t\t# Stage4\n\t\tx4 = self.backbone.layer3(x3)\n\t\t# Stage5\n\t\tx5 = self.backbone.layer4(x4)\n\t\treturn x1, x2, x3, x4, x5\n\n\n\tdef _init_weights(self):\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.Conv2d):\n\t\t\t\tnn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n\t\t\t\tif m.bias is not None:\n\t\t\t\t\tm.bias.data.zero_()\n\t\t\telif isinstance(m, nn.BatchNorm2d):\n\t\t\t\tnn.init.constant_(m.weight, 1)\n\t\t\t\tnn.init.constant_(m.bias, 0)\n'"
models/UNetPlus.py,3,"b""#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nfrom base import BaseModel\nfrom models import backbones\n\n\n#------------------------------------------------------------------------------\n#   DecoderBlock\n#------------------------------------------------------------------------------\nclass DecoderBlock(nn.Module):\n\tdef __init__(self, in_channels, out_channels, block, use_deconv=True, squeeze=1, dropout=0.2):\n\t\tsuper(DecoderBlock, self).__init__()\n\t\tself.use_deconv = use_deconv\n\n\t\t# Deconvolution\n\t\tif self.use_deconv:\n\t\t\tif squeeze==1:\n\t\t\t\tself.upsampler = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n\t\t\telse:\n\t\t\t\thid_channels = int(in_channels/squeeze)\n\t\t\t\tself.upsampler = nn.Sequential(OrderedDict([\n\t\t\t\t\t('conv1', nn.Conv2d(in_channels, hid_channels, kernel_size=1, bias=False)),\n\t\t\t\t\t('bn1', nn.BatchNorm2d(hid_channels)),\n\t\t\t\t\t('relu1', nn.ReLU(inplace=True)),\n\t\t\t\t\t('dropout1', nn.Dropout2d(p=dropout)),\n\n\t\t\t\t\t('conv2', nn.ConvTranspose2d(hid_channels, hid_channels, kernel_size=4, stride=2, padding=1, bias=False)),\n\t\t\t\t\t('bn2', nn.BatchNorm2d(hid_channels)),\n\t\t\t\t\t('relu2', nn.ReLU(inplace=True)),\n\t\t\t\t\t('dropout2', nn.Dropout2d(p=dropout)),\n\n\t\t\t\t\t('conv3', nn.Conv2d(hid_channels, out_channels, kernel_size=1, bias=False)),\n\t\t\t\t\t('bn3', nn.BatchNorm2d(out_channels)),\n\t\t\t\t\t('relu3', nn.ReLU(inplace=True)),\n\t\t\t\t\t('dropout3', nn.Dropout2d(p=dropout)),\n\t\t\t\t]))\n\t\telse:\n\t\t\tself.upsampler = nn.Sequential(OrderedDict([\n\t\t\t\t('conv', nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)),\n\t\t\t\t('bn', nn.BatchNorm2d(out_channels)),\n\t\t\t\t('relu', nn.ReLU(inplace=True)),\n\t\t\t]))\n\n\t\t# Block\n\t\tself.block = nn.Sequential(OrderedDict([\n\t\t\t('bottleneck', block(2*out_channels, out_channels)),\n\t\t\t('dropout', nn.Dropout(p=dropout))\n\t\t]))\n\n\n\tdef forward(self, x, shortcut):\n\t\tif not self.use_deconv:\n\t\t\tx = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n\t\tx = self.upsampler(x)\n\t\tx = torch.cat([x, shortcut], dim=1)\n\t\tx = self.block(x)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#   UNetPlus\n#------------------------------------------------------------------------------\nclass UNetPlus(BaseModel):\n\tdef __init__(self, backbone='resnet18', num_classes=2, in_channels=3,\n\t\t\t\tuse_deconv=False, squeeze=4, dropout=0.2, frozen_stages=-1,\n\t\t\t\tnorm_eval=True, init_backbone_from_imagenet=False):\n\n\t\t# Instantiate\n\t\tsuper(UNetPlus, self).__init__()\n\t\tself.in_channels = in_channels\n\n\t\t# Build Backbone and Decoder\n\t\tif ('resnet' in backbone) or ('resnext' in backbone) or ('wide_resnet' in backbone):\n\t\t\tself.backbone = getattr(backbones, backbone)(in_chans=in_channels, frozen_stages=frozen_stages, norm_eval=norm_eval)\n\t\t\tinplanes = 64\n\t\t\tblock = backbones.ResNetBasicBlock if '18' in backbone or '34' in backbone else backbones.ResNetBottleneckBlock\n\t\t\texpansion = block.expansion\n\n\t\t\tself.decoder = nn.Module()\n\t\t\tself.decoder.layer1 = DecoderBlock(8*inplanes*expansion, 4*inplanes*expansion, block, squeeze=squeeze, dropout=dropout, use_deconv=use_deconv)\n\t\t\tself.decoder.layer2 = DecoderBlock(4*inplanes*expansion, 2*inplanes*expansion, block, squeeze=squeeze, dropout=dropout, use_deconv=use_deconv)\n\t\t\tself.decoder.layer3 = DecoderBlock(2*inplanes*expansion, inplanes*expansion, block, squeeze=squeeze, dropout=dropout, use_deconv=use_deconv)\n\t\t\tself.decoder.layer4 = DecoderBlock(inplanes*expansion, inplanes, block, squeeze=squeeze, dropout=dropout, use_deconv=use_deconv)\n\t\t\tout_channels = inplanes\n\n\t\telif 'efficientnet' in backbone:\n\t\t\tself.backbone = getattr(backbones, backbone)(in_chans=in_channels, frozen_stages=frozen_stages, norm_eval=norm_eval)\n\t\t\tblock = backbones.EfficientNetBlock\n\t\t\tnum_channels = self.backbone.stage_features[self.backbone.model_name]\n\n\t\t\tself.decoder = nn.Module()\n\t\t\tself.decoder.layer1 = DecoderBlock(num_channels[4], num_channels[3], block, squeeze=squeeze, dropout=dropout, use_deconv=use_deconv)\n\t\t\tself.decoder.layer2 = DecoderBlock(num_channels[3], num_channels[2], block, squeeze=squeeze, dropout=dropout, use_deconv=use_deconv)\n\t\t\tself.decoder.layer3 = DecoderBlock(num_channels[2], num_channels[1], block, squeeze=squeeze, dropout=dropout, use_deconv=use_deconv)\n\t\t\tself.decoder.layer4 = DecoderBlock(num_channels[1], num_channels[0], block, squeeze=squeeze, dropout=dropout, use_deconv=use_deconv)\n\t\t\tout_channels = num_channels[0]\n\n\t\telse:\n\t\t\traise NotImplementedError\n\n\t\t# Build Head\n\t\tself.mask = nn.Sequential(OrderedDict([\n\t\t\t('conv1', nn.Conv2d(out_channels, 32, kernel_size=3, padding=1, bias=False)),\n\t\t\t('bn1', nn.BatchNorm2d(num_features=32)),\n\t\t\t('relu1', nn.ReLU(inplace=True)),\n\t\t\t('dropout1', nn.Dropout2d(p=dropout)),\n\t\t\t('conv2', nn.Conv2d(32, 16, kernel_size=3, padding=1, bias=False)),\n\t\t\t('bn2', nn.BatchNorm2d(num_features=16)),\n\t\t\t('relu2', nn.ReLU(inplace=True)),\n\t\t\t('dropout2', nn.Dropout2d(p=dropout)),\n\t\t\t('conv3', nn.Conv2d(16, num_classes, kernel_size=3, padding=1)),\n\t\t]))\n\n\t\t# Initialize weights\n\t\tself.init_weights()\n\t\tif init_backbone_from_imagenet:\n\t\t\tself.backbone.init_from_imagenet(archname=backbone)\n\n\tdef forward(self, images, **kargs):\n\t\t# Encoder\n\t\tx1, x2, x3, x4, x5 = self.backbone(images)\n\n\t\t# Decoder\n\t\ty = self.decoder.layer1(x5, x4)\n\t\ty = self.decoder.layer2(y, x3)\n\t\ty = self.decoder.layer3(y, x2)\n\t\ty = self.decoder.layer4(y, x1)\n\t\ty = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n\n\t\t# Output\n\t\treturn self.mask(y)\n"""
models/__init__.py,0,"b""from models.UNetPlus import UNetPlus\n\nfrom models.UNet import UNet\nfrom models.DeepLab import DeepLabV3Plus\nfrom models.BiSeNet import BiSeNet\nfrom models.PSPNet import PSPNet\nfrom models.ICNet import ICNet\n\n__all__ = [\n\t'UNetPlus',\n\t'UNet', 'DeepLabV3Plus', 'BiSeNet', 'PSPNet', 'ICNet',\n]\n"""
trainer/trainer.py,2,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid\nfrom base.base_trainer import BaseTrainer\n\n\n#------------------------------------------------------------------------------\n#  Poly learning-rate Scheduler\n#------------------------------------------------------------------------------\ndef poly_lr_scheduler(optimizer, init_lr, curr_iter, max_iter, power=0.9):\n\tfor g in optimizer.param_groups:\n\t\tg[\'lr\'] = init_lr * (1 - curr_iter/max_iter)**power\n\n\n#------------------------------------------------------------------------------\n#   Class of Trainer\n#------------------------------------------------------------------------------\nclass Trainer(BaseTrainer):\n\t""""""\n\tTrainer class\n\n\tNote:\n\t\tInherited from BaseTrainer.\n\t""""""\n\tdef __init__(self, model, loss, metrics, optimizer, resume, config,\n\t\t\t\t data_loader, valid_data_loader=None, lr_scheduler=None, train_logger=None):\n\t\tsuper(Trainer, self).__init__(model, loss, metrics, optimizer, resume, config, train_logger)\n\t\tself.config = config\n\t\tself.data_loader = data_loader\n\t\tself.valid_data_loader = valid_data_loader\n\t\tself.do_validation = self.valid_data_loader is not None\n\t\tself.lr_scheduler = lr_scheduler\n\t\tself.max_iter = len(self.data_loader) * self.epochs\n\t\tself.init_lr = optimizer.param_groups[0][\'lr\']\n\n\n\tdef _eval_metrics(self, output, target):\n\t\tacc_metrics = np.zeros(len(self.metrics))\n\t\tfor i, metric in enumerate(self.metrics):\n\t\t\tacc_metrics[i] += metric(output, target)\n\t\treturn acc_metrics\n\n\n\tdef _train_epoch(self, epoch):\n\t\t""""""\n\t\tTraining logic for an epoch\n\n\t\t:param epoch: Current training epoch.\n\t\t:return: A log that contains all information you want to save.\n\n\t\tNote:\n\t\t\tIf you have additional information to record, for example:\n\t\t\t\t> additional_log = {""x"": x, ""y"": y}\n\t\t\tmerge it with log before return. i.e.\n\t\t\t\t> log = {**log, **additional_log}\n\t\t\t\t> return log\n\n\t\t\tThe metrics in log must have the key \'metrics\'.\n\t\t""""""\n\t\tprint(""Train on epoch..."")\n\t\tself.model.train()\n\t\tself.writer_train.set_step(epoch)\n\t\n\t\t# Perform training\n\t\ttotal_loss = 0\n\t\ttotal_metrics = np.zeros(len(self.metrics))\n\t\tn_iter = len(self.data_loader)\n\t\tfor batch_idx, (data, target) in tqdm(enumerate(self.data_loader), total=n_iter):\n\t\t\tcurr_iter = batch_idx + (epoch-1)*n_iter\n\t\t\tdata, target = data.to(self.device), target.to(self.device)\n\t\t\tself.optimizer.zero_grad()\n\t\t\toutput = self.model(data)\n\t\t\tloss = self.loss(output, target)\n\t\t\tloss.backward()\n\t\t\tself.optimizer.step()\n\n\t\t\ttotal_loss += loss.item()\n\t\t\ttotal_metrics += self._eval_metrics(output, target)\n\n\t\t\tif (batch_idx==n_iter-2) and (self.verbosity>=2):\n\t\t\t\tself.writer_train.add_image(\'train/input\', make_grid(data[:,:3,:,:].cpu(), nrow=4, normalize=True))\n\t\t\t\tself.writer_train.add_image(\'train/label\', make_grid(target.unsqueeze(1).cpu(), nrow=4, normalize=True))\n\t\t\t\tif type(output)==tuple or type(output)==list:\n\t\t\t\t\tself.writer_train.add_image(\'train/output\', make_grid(F.softmax(output[0], dim=1)[:,1:2,:,:].cpu(), nrow=4, normalize=True))\n\t\t\t\telse:\n\t\t\t\t\t# self.writer_train.add_image(\'train/output\', make_grid(output.cpu(), nrow=4, normalize=True))\n\t\t\t\t\tself.writer_train.add_image(\'train/output\', make_grid(F.softmax(output, dim=1)[:,1:2,:,:].cpu(), nrow=4, normalize=True))\n\n\t\t\tpoly_lr_scheduler(self.optimizer, self.init_lr, curr_iter, self.max_iter, power=0.9)\n\n\t\t# Record log\n\t\ttotal_loss /= len(self.data_loader)\n\t\ttotal_metrics /= len(self.data_loader)\n\t\tlog = {\n\t\t\t\'train_loss\': total_loss,\n\t\t\t\'train_metrics\': total_metrics.tolist(),\n\t\t}\n\n\t\t# Write training result to TensorboardX\n\t\tself.writer_train.add_scalar(\'loss\', total_loss)\n\t\tfor i, metric in enumerate(self.metrics):\n\t\t\tself.writer_train.add_scalar(\'metrics/%s\'%(metric.__name__), total_metrics[i])\n\n\t\tif self.verbosity>=2:\n\t\t\tfor i in range(len(self.optimizer.param_groups)):\n\t\t\t\tself.writer_train.add_scalar(\'lr/group%d\'%(i), self.optimizer.param_groups[i][\'lr\'])\n\n\t\t# Perform validating\n\t\tif self.do_validation:\n\t\t\tprint(""Validate on epoch..."")\n\t\t\tval_log = self._valid_epoch(epoch)\n\t\t\tlog = {**log, **val_log}\n\n\t\t# Learning rate scheduler\n\t\tif self.lr_scheduler is not None:\n\t\t\tself.lr_scheduler.step()\n\n\t\treturn log\n\n\n\tdef _valid_epoch(self, epoch):\n\t\t""""""\n\t\tValidate after training an epoch\n\n\t\t:return: A log that contains information about validation\n\n\t\tNote:\n\t\t\tThe validation metrics in log must have the key \'valid_metrics\'.\n\t\t""""""\n\t\tself.model.eval()\n\t\ttotal_val_loss = 0\n\t\ttotal_val_metrics = np.zeros(len(self.metrics))\n\t\tn_iter = len(self.valid_data_loader)\n\t\tself.writer_valid.set_step(epoch)\n\n\t\twith torch.no_grad():\n\t\t\t# Validate\n\t\t\tfor batch_idx, (data, target) in tqdm(enumerate(self.valid_data_loader), total=n_iter):\n\t\t\t\tdata, target = data.to(self.device), target.to(self.device)\n\t\t\t\toutput = self.model(data)\n\t\t\t\tloss = self.loss(output, target)\n\n\t\t\t\ttotal_val_loss += loss.item()\n\t\t\t\ttotal_val_metrics += self._eval_metrics(output, target)\n\n\t\t\t\tif (batch_idx==n_iter-2) and(self.verbosity>=2):\n\t\t\t\t\tself.writer_valid.add_image(\'valid/input\', make_grid(data[:,:3,:,:].cpu(), nrow=4, normalize=True))\n\t\t\t\t\tself.writer_valid.add_image(\'valid/label\', make_grid(target.unsqueeze(1).cpu(), nrow=4, normalize=True))\n\t\t\t\t\tif type(output)==tuple or type(output)==list:\n\t\t\t\t\t\tself.writer_valid.add_image(\'valid/output\', make_grid(F.softmax(output[0], dim=1)[:,1:2,:,:].cpu(), nrow=4, normalize=True))\n\t\t\t\t\telse:\n\t\t\t\t\t\t# self.writer_valid.add_image(\'valid/output\', make_grid(output.cpu(), nrow=4, normalize=True))\n\t\t\t\t\t\tself.writer_valid.add_image(\'valid/output\', make_grid(F.softmax(output, dim=1)[:,1:2,:,:].cpu(), nrow=4, normalize=True))\n\n\t\t\t# Record log\n\t\t\ttotal_val_loss /= len(self.valid_data_loader)\n\t\t\ttotal_val_metrics /= len(self.valid_data_loader)\n\t\t\tval_log = {\n\t\t\t\t\'valid_loss\': total_val_loss,\n\t\t\t\t\'valid_metrics\': total_val_metrics.tolist(),\n\t\t\t}\n\n\t\t\t# Write validating result to TensorboardX\n\t\t\tself.writer_valid.add_scalar(\'loss\', total_val_loss)\n\t\t\tfor i, metric in enumerate(self.metrics):\n\t\t\t\tself.writer_valid.add_scalar(\'metrics/%s\'%(metric.__name__), total_val_metrics[i])\n\n\t\treturn val_log'"
utils/__init__.py,0,"b'from .flops_counter import add_flops_counting_methods, flops_to_string\n'"
utils/flops_counter.py,16,"b'import torch.nn as nn\nimport torch\nimport numpy as np\n\ndef flops_to_string(flops):\n    if flops // 10**9 > 0:\n        return str(round(flops / 10.**9, 2)) + \'GMac\'\n    elif flops // 10**6 > 0:\n        return str(round(flops / 10.**6, 2)) + \'MMac\'\n    elif flops // 10**3 > 0:\n        return str(round(flops / 10.**3, 2)) + \'KMac\'\n    return str(flops) + \'Mac\'\n\ndef get_model_parameters_number(model, as_string=True):\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    if not as_string:\n        return params_num\n\n    if params_num // 10 ** 6 > 0:\n        return str(round(params_num / 10 ** 6, 2)) + \'M\'\n    elif params_num // 10 ** 3:\n        return str(round(params_num / 10 ** 3, 2)) + \'k\'\n\n    return str(params_num)\n\ndef add_flops_counting_methods(net_main_module):\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n\n    net_main_module.reset_flops_count()\n\n    # Adding variables necessary for masked flops computation\n    net_main_module.apply(add_flops_mask_variable_or_reset)\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Returns current mean flops consumption per image.\n\n    """"""\n\n    batches_count = self.__batch_counter__\n    flops_sum = 0\n    for module in self.modules():\n        if is_supported_instance(module):\n            flops_sum += module.__flops__\n\n    return flops_sum / batches_count\n\n\ndef start_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n\n    """"""\n    add_batch_counter_hook_function(self)\n    self.apply(add_flops_counter_hook_function)\n\n\ndef stop_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n\n    """"""\n    remove_batch_counter_hook_function(self)\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Resets statistics computed so far.\n\n    """"""\n    add_batch_counter_variables_or_reset(self)\n    self.apply(add_flops_counter_variable_or_reset)\n\n\ndef add_flops_mask(module, mask):\n    def add_flops_mask_func(module):\n        if isinstance(module, torch.nn.Conv2d):\n            module.__mask__ = mask\n    module.apply(add_flops_mask_func)\n\n\ndef remove_flops_mask(module):\n    module.apply(add_flops_mask_variable_or_reset)\n\n\n# ---- Internal functions\ndef is_supported_instance(module):\n    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.ReLU) \\\n       or isinstance(module, torch.nn.PReLU) or isinstance(module, torch.nn.ELU) \\\n       or isinstance(module, torch.nn.LeakyReLU) or isinstance(module, torch.nn.ReLU6) \\\n       or isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.MaxPool2d) \\\n       or isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.BatchNorm2d) \\\n       or isinstance(module, torch.nn.Upsample):\n        return True\n\n    return False\n\n\ndef empty_flops_counter_hook(module, input, output):\n    module.__flops__ += 0\n\n\ndef upsample_flops_counter_hook(module, input, output):\n    output_size = output[0]\n    batch_size = output_size.shape[0]\n    output_elements_count = batch_size\n    for val in output_size.shape[1:]:\n        output_elements_count *= val\n    module.__flops__ += output_elements_count\n\n\ndef relu_flops_counter_hook(module, input, output):\n    input = input[0]\n    batch_size = input.shape[0]\n    active_elements_count = batch_size\n    for val in input.shape[1:]:\n        active_elements_count *= val\n\n    module.__flops__ += active_elements_count\n\n\ndef linear_flops_counter_hook(module, input, output):\n    input = input[0]\n    batch_size = input.shape[0]\n    module.__flops__ += batch_size * input.shape[1] * output.shape[1]\n\n\ndef pool_flops_counter_hook(module, input, output):\n    input = input[0]\n    module.__flops__ += np.prod(input.shape)\n\ndef bn_flops_counter_hook(module, input, output):\n    module.affine\n    input = input[0]\n\n    batch_flops = np.prod(input.shape)\n    if module.affine:\n        batch_flops *= 2\n    module.__flops__ += batch_flops\n\ndef conv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    output_height, output_width = output.shape[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = kernel_height * kernel_width * in_channels * filters_per_channel\n\n    active_elements_count = batch_size * output_height * output_width\n\n    if conv_module.__mask__ is not None:\n        # (b, 1, h, w)\n        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n        active_elements_count = flops_mask.sum()\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += overall_flops\n\n\ndef batch_counter_hook(module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n    batch_size = input.shape[0]\n    module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_variables_or_reset(module):\n\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        module.__batch_counter_handle__.remove()\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            return\n\n        if isinstance(module, torch.nn.Conv2d):\n            handle = module.register_forward_hook(conv_flops_counter_hook)\n        elif isinstance(module, torch.nn.ReLU) or isinstance(module, torch.nn.PReLU) \\\n             or isinstance(module, torch.nn.ELU) or isinstance(module, torch.nn.LeakyReLU) \\\n             or isinstance(module, torch.nn.ReLU6):\n            handle = module.register_forward_hook(relu_flops_counter_hook)\n        elif isinstance(module, torch.nn.Linear):\n            handle = module.register_forward_hook(linear_flops_counter_hook)\n        elif isinstance(module, torch.nn.AvgPool2d) or isinstance(module, torch.nn.MaxPool2d):\n            handle = module.register_forward_hook(pool_flops_counter_hook)\n        elif isinstance(module, torch.nn.BatchNorm2d):\n            handle = module.register_forward_hook(bn_flops_counter_hook)\n        elif isinstance(module, torch.nn.Upsample):\n            handle = module.register_forward_hook(upsample_flops_counter_hook)\n        else:\n            handle = module.register_forward_hook(empty_flops_counter_hook)\n        module.__flops_handle__ = handle\n\n\ndef remove_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            module.__flops_handle__.remove()\n            del module.__flops_handle__\n# --- Masked flops counting\n\n\n# Also being run in the initialization\ndef add_flops_mask_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__mask__ = None\n'"
utils/logger.py,0,"b'import json\nimport logging\n\nlogging.basicConfig(level=logging.INFO, format=\'\')\n\nclass Logger:\n    """"""\n    Training process logger\n\n    Note:\n        Used by BaseTrainer to save training history.\n    """"""\n    def __init__(self):\n        self.entries = {}\n\n    def add_entry(self, entry):\n        self.entries[len(self.entries) + 1] = entry\n\n    def __str__(self):\n        return json.dumps(self.entries, sort_keys=True, indent=4)\n'"
utils/utils.py,1,"b'#------------------------------------------------------------------------------\n#\tLibraries\n#------------------------------------------------------------------------------\nimport os, cv2, torch\nimport numpy as np\nfrom dataloaders import transforms\n\n\n#------------------------------------------------------------------------------\n#  Preprocessing\n#------------------------------------------------------------------------------\nmean = np.array([0.485, 0.456, 0.406])[None,None,:]\nstd = np.array([0.229, 0.224, 0.225])[None,None,:]\n\ndef preprocessing(image, expected_size=224, pad_value=0):\n\timage, pad_up, pad_left, h_new, w_new = transforms.resize_image(image, expected_size, pad_value, ret_params=True)\n\timage = image.astype(np.float32) / 255.0\n\timage = (image - mean) / std\n\tX = np.transpose(image, axes=(2, 0, 1))\n\tX = np.expand_dims(X, axis=0)\n\tX = torch.tensor(X, dtype=torch.float32)\n\treturn X, pad_up, pad_left, h_new, w_new\n\n\n#------------------------------------------------------------------------------\n#  Draw image with transperency\n#------------------------------------------------------------------------------\ndef draw_transperency(image, mask, color_f, color_b):\n\t""""""\n\timage (np.uint8)\n\tmask  (np.float32) range from 0 to 1 \n\t""""""\n\tmask = mask.round()\n\talpha = np.zeros_like(image, dtype=np.uint8)\n\talpha[mask==1, :] = color_f\n\talpha[mask==0, :] = color_b\n\timage_alpha = cv2.add(image, alpha)\n\treturn image_alpha\n\n\n#------------------------------------------------------------------------------\n#   Draw matting\n#------------------------------------------------------------------------------\ndef draw_matting(image, mask):\n\t""""""\n\timage (np.uint8)\n\tmask  (np.float32) range from 0 to 1 \n\t""""""\n\tmask = 255*(1.0-mask)\n\tmask = np.expand_dims(mask, axis=2)\n\tmask = np.tile(mask, (1,1,3))\n\tmask = mask.astype(np.uint8)\n\timage_matting = cv2.add(image, mask)\n\treturn image_matting\n\n\n#------------------------------------------------------------------------------\n#  Draw foreground pasted into background\n#------------------------------------------------------------------------------\ndef draw_fore_to_back(image, mask, background, kernel_sz=13, sigma=0):\n\t""""""\n\timage (np.uint8)\n\tmask  (np.float32) range from 0 to 1 \n\t""""""\n\tmask_filtered = cv2.GaussianBlur(mask, (kernel_sz, kernel_sz), sigma)\n\tmask_filtered = np.expand_dims(mask_filtered, axis=2)\n\tmask_filtered = np.tile(mask_filtered, (1,1,3))\n\timage_alpha = image*mask_filtered + background*(1-mask_filtered)\n\treturn image_alpha.astype(np.uint8)'"
utils/visualization.py,0,"b'#------------------------------------------------------------------------------\n#    Libraries\n#------------------------------------------------------------------------------\nimport importlib\nimport warnings\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\nimport matplotlib.pyplot as plt\n\n\n#------------------------------------------------------------------------------\n#    Tensorboard Writer\n#------------------------------------------------------------------------------\nclass WriterTensorboardX():\n    def __init__(self, writer_dir, logger, enable):\n        self.writer = None\n        if enable:\n            log_path = writer_dir\n            try:\n                self.writer = importlib.import_module(\'tensorboardX\').SummaryWriter(log_path)\n            except ModuleNotFoundError:\n                message = """"""TensorboardX visualization is configured to use, but currently not installed on this machine. Please install the package by \'pip install tensorboardx\' command or turn off the option in the \'config.json\' file.""""""\n                warnings.warn(message, UserWarning)\n                logger.warn(message)\n        self.step = 0\n        self.tensorboard_writer_ftns = [\'add_scalar\', \'add_scalars\', \'add_image\', \'add_audio\', \'add_text\', \'add_histogram\', \'add_pr_curve\', \'add_embedding\']\n\n    def set_step(self, step):\n        self.step = step\n\n    def __getattr__(self, name):\n        """"""\n        If visualization is configured to use:\n            return add_data() methods of tensorboard with additional information (step, tag) added.\n        Otherwise:\n            return blank function handle that does nothing\n        """"""\n        if name in self.tensorboard_writer_ftns:\n            add_data = getattr(self.writer, name, None)\n            def wrapper(tag, data, *args, **kwargs):\n                if add_data is not None:\n                    add_data(\'{}\'.format(tag), data, self.step, *args, **kwargs)\n            return wrapper\n        else:\n            # default action for returning methods defined in this class, set_step() for instance.\n            try:\n                attr = object.__getattr__(name)\n            except AttributeError:\n                raise AttributeError(""type object \'WriterTensorboardX\' has no attribute \'{}\'"".format(name))\n            return attr\n\n\n#------------------------------------------------------------------------------\n#   Function to plot Tensorboard\n#------------------------------------------------------------------------------\ndef plot_tensorboard(train_file, valid_file, scalar_names, set_grid=False):\n    # Read Tensorboard files\n    train_event_acc = EventAccumulator(train_file)\n    valid_event_acc = EventAccumulator(valid_file)\n    train_event_acc.Reload()\n    valid_event_acc.Reload()\n\n    # Get scalar values\n    train_scalars, valid_scalars = {}, {}\n    for scalar_name in scalar_names:\n        train_scalars[scalar_name] = train_event_acc.Scalars(scalar_name)\n        valid_scalars[scalar_name] = valid_event_acc.Scalars(scalar_name)\n\n    # Convert to list\n    n_epochs = len(train_scalars[""loss""])\n    epochs = [train_scalars[""loss""][i][1] for i in range(n_epochs)]\n\n    train_lists, valid_lists = {}, {}\n    for scalar_name in scalar_names:\n        train_lists[scalar_name] = [train_scalars[scalar_name][i][2] for i in range(n_epochs)]\n        valid_lists[scalar_name] = [valid_scalars[scalar_name][i][2] for i in range(n_epochs)]\n\n    # Plot\n    for scalar_name in scalar_names:\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n        if set_grid:\n            ax.set_xticks(epochs)\n\n        ax.plot(epochs, train_lists[scalar_name], label=\'train\')\n        ax.plot(epochs, valid_lists[scalar_name], label=\'valid\')\n\n        plt.xlabel(""epochs"")\n        plt.ylabel(scalar_name)\n        plt.legend(frameon=True)\n        plt.grid(True)\n        plt.show()\n\n\n#------------------------------------------------------------------------------\n#   Test bench\n#------------------------------------------------------------------------------\nif __name__ == \'__main__\':\n    train_file = ""checkpoints/runs/Mnist_LeNet/1125_110943/train/events.out.tfevents.1543118983.antiaegis""\n    valid_file = ""checkpoints/runs/Mnist_LeNet/1125_110943/valid/events.out.tfevents.1543118983.antiaegis""\n    plot_tensorboard(train_file, valid_file, [""loss"", ""my_metric"", ""my_metric2""])'"
models/backbonds/MobileNetV2.py,2,"b'#------------------------------------------------------------------------------\n#  Libraries\n#------------------------------------------------------------------------------\nimport math, torch, json\nimport torch.nn as nn\nfrom functools import reduce\n\n\n#------------------------------------------------------------------------------\n#  Useful functions\n#------------------------------------------------------------------------------\ndef _make_divisible(v, divisor, min_value=None):\n\tif min_value is None:\n\t\tmin_value = divisor\n\tnew_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n\t# Make sure that round down does not go down by more than 10%.\n\tif new_v < 0.9 * v:\n\t\tnew_v += divisor\n\treturn new_v\n\n\ndef conv_bn(inp, oup, stride):\n\treturn nn.Sequential(\n\t\tnn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n\t\tnn.BatchNorm2d(oup),\n\t\tnn.ReLU6(inplace=True)\n\t)\n\n\ndef conv_1x1_bn(inp, oup):\n\treturn nn.Sequential(\n\t\tnn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n\t\tnn.BatchNorm2d(oup),\n\t\tnn.ReLU6(inplace=True)\n\t)\n\n\n#------------------------------------------------------------------------------\n#  Class of Inverted Residual block\n#------------------------------------------------------------------------------\nclass InvertedResidual(nn.Module):\n\tdef __init__(self, inp, oup, stride, expansion, dilation=1):\n\t\tsuper(InvertedResidual, self).__init__()\n\t\tself.stride = stride\n\t\tassert stride in [1, 2]\n\n\t\thidden_dim = round(inp * expansion)\n\t\tself.use_res_connect = self.stride == 1 and inp == oup\n\n\t\tif expansion == 1:\n\t\t\tself.conv = nn.Sequential(\n\t\t\t\t# dw\n\t\t\t\tnn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, dilation=dilation, bias=False),\n\t\t\t\tnn.BatchNorm2d(hidden_dim),\n\t\t\t\tnn.ReLU6(inplace=True),\n\t\t\t\t# pw-linear\n\t\t\t\tnn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n\t\t\t\tnn.BatchNorm2d(oup),\n\t\t\t)\n\t\telse:\n\t\t\tself.conv = nn.Sequential(\n\t\t\t\t# pw\n\t\t\t\tnn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n\t\t\t\tnn.BatchNorm2d(hidden_dim),\n\t\t\t\tnn.ReLU6(inplace=True),\n\t\t\t\t# dw\n\t\t\t\tnn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, dilation=dilation, bias=False),\n\t\t\t\tnn.BatchNorm2d(hidden_dim),\n\t\t\t\tnn.ReLU6(inplace=True),\n\t\t\t\t# pw-linear\n\t\t\t\tnn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n\t\t\t\tnn.BatchNorm2d(oup),\n\t\t\t)\n\n\tdef forward(self, x):\n\t\tif self.use_res_connect:\n\t\t\treturn x + self.conv(x)\n\t\telse:\n\t\t\treturn self.conv(x)\n\n\n#------------------------------------------------------------------------------\n#  Class of MobileNetV2\n#------------------------------------------------------------------------------\nclass MobileNetV2(nn.Module):\n\tdef __init__(self, alpha=1.0, expansion=6, num_classes=1000):\n\t\tsuper(MobileNetV2, self).__init__()\n\t\tself.num_classes = num_classes\n\t\tinput_channel = 32\n\t\tlast_channel = 1280\n\t\tinterverted_residual_setting = [\n\t\t\t# t, c, n, s\n\t\t\t[1        , 16, 1, 1],\n\t\t\t[expansion, 24, 2, 2],\n\t\t\t[expansion, 32, 3, 2],\n\t\t\t[expansion, 64, 4, 2],\n\t\t\t[expansion, 96, 3, 1],\n\t\t\t[expansion, 160, 3, 2],\n\t\t\t[expansion, 320, 1, 1],\n\t\t]\n\n\t\t# building first layer\n\t\tinput_channel = _make_divisible(input_channel*alpha, 8)\n\t\tself.last_channel = _make_divisible(last_channel*alpha, 8) if alpha > 1.0 else last_channel\n\t\tself.features = [conv_bn(3, input_channel, 2)]\n\n\t\t# building inverted residual blocks\n\t\tfor t, c, n, s in interverted_residual_setting:\n\t\t\toutput_channel = _make_divisible(int(c*alpha), 8)\n\t\t\tfor i in range(n):\n\t\t\t\tif i == 0:\n\t\t\t\t\tself.features.append(InvertedResidual(input_channel, output_channel, s, expansion=t))\n\t\t\t\telse:\n\t\t\t\t\tself.features.append(InvertedResidual(input_channel, output_channel, 1, expansion=t))\n\t\t\t\tinput_channel = output_channel\n\n\t\t# building last several layers\n\t\tself.features.append(conv_1x1_bn(input_channel, self.last_channel))\n\n\t\t# make it nn.Sequential\n\t\tself.features = nn.Sequential(*self.features)\n\n\t\t# building classifier\n\t\tif self.num_classes is not None:\n\t\t\tself.classifier = nn.Sequential(\n\t\t\t\tnn.Dropout(0.2),\n\t\t\t\tnn.Linear(self.last_channel, num_classes),\n\t\t\t)\n\n\t\t# Initialize weights\n\t\tself._init_weights()\n\n\n\tdef forward(self, x, feature_names=None):\n\t\t# Stage1\n\t\tx = reduce(lambda x, n: self.features[n](x), list(range(0,2)), x)\n\n\t\t# Stage2\n\t\tx = reduce(lambda x, n: self.features[n](x), list(range(2,4)), x)\n\n\t\t# Stage3\n\t\tx = reduce(lambda x, n: self.features[n](x), list(range(4,7)), x)\n\n\t\t# Stage4\n\t\tx = reduce(lambda x, n: self.features[n](x), list(range(7,14)), x)\n\n\t\t# Stage5\n\t\tx = reduce(lambda x, n: self.features[n](x), list(range(14,19)), x)\n\n\t\t# Classification\n\t\tif self.num_classes is not None:\n\t\t\tx = x.mean(dim=(2,3))\n\t\t\tx = self.classifier(x)\n\t\t\t\n\t\t# Output\n\t\treturn x\n\n\n\tdef _load_pretrained_model(self, pretrained_file):\n\t\tpretrain_dict = torch.load(pretrained_file, map_location=\'cpu\')\n\t\tmodel_dict = {}\n\t\tstate_dict = self.state_dict()\n\t\tprint(""[MobileNetV2] Loading pretrained model..."")\n\t\tfor k, v in pretrain_dict.items():\n\t\t\tif k in state_dict:\n\t\t\t\tmodel_dict[k] = v\n\t\t\telse:\n\t\t\t\tprint(k, ""is ignored"")\n\t\tstate_dict.update(model_dict)\n\t\tself.load_state_dict(state_dict)\n\n\n\tdef _init_weights(self):\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.Conv2d):\n\t\t\t\tn = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n\t\t\t\tm.weight.data.normal_(0, math.sqrt(2. / n))\n\t\t\t\tif m.bias is not None:\n\t\t\t\t\tm.bias.data.zero_()\n\t\t\telif isinstance(m, nn.BatchNorm2d):\n\t\t\t\tm.weight.data.fill_(1)\n\t\t\t\tm.bias.data.zero_()\n\t\t\telif isinstance(m, nn.Linear):\n\t\t\t\tn = m.weight.size(1)\n\t\t\t\tm.weight.data.normal_(0, 0.01)\n\t\t\t\tm.bias.data.zero_()'"
models/backbonds/ResNet.py,1,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\nfrom base import BaseBackbone\n\n\n#------------------------------------------------------------------------------\n#   Util functions\n#------------------------------------------------------------------------------\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n\treturn nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n\treturn nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\n#------------------------------------------------------------------------------\n#   Class of Basic block\n#------------------------------------------------------------------------------\nclass BasicBlock(nn.Module):\n\texpansion = 1\n\n\tdef __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n\t\tsuper(BasicBlock, self).__init__()\n\t\tself.conv1 = conv3x3(inplanes, planes, stride, dilation=dilation)\n\t\tself.bn1 = nn.BatchNorm2d(planes)\n\t\tself.relu = nn.ReLU(inplace=True)\n\n\t\tself.conv2 = conv3x3(planes, planes, dilation=dilation)\n\t\tself.bn2 = nn.BatchNorm2d(planes)\n\n\t\tself.downsample = downsample\n\t\tself.stride = stride\n\n\n\tdef forward(self, x):\n\t\tresidual = x\n\n\t\tout = self.conv1(x)\n\t\tout = self.bn1(out)\n\t\tout = self.relu(out)\n\n\t\tout = self.conv2(out)\n\t\tout = self.bn2(out)\n\n\t\tif self.downsample is not None:\n\t\t\tresidual = self.downsample(x)\n\n\t\tout += residual\n\t\tout = self.relu(out)\n\n\t\treturn out\n\n\n#------------------------------------------------------------------------------\n#   Class of Residual bottleneck\n#------------------------------------------------------------------------------\nclass Bottleneck(nn.Module):\n\texpansion = 4\n\n\tdef __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n\t\tsuper(Bottleneck, self).__init__()\n\t\tself.conv1 = conv1x1(inplanes, planes)\n\t\tself.bn1 = nn.BatchNorm2d(planes)\n\n\t\tself.conv2 = conv3x3(planes, planes, stride, dilation=dilation)\n\t\tself.bn2 = nn.BatchNorm2d(planes)\n\n\t\tself.conv3 = conv1x1(planes, planes * self.expansion)\n\t\tself.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\t\t\n\t\tself.relu = nn.ReLU(inplace=True)\n\t\tself.downsample = downsample\n\t\tself.stride = stride\n\n\n\tdef forward(self, x):\n\t\tresidual = x\n\n\t\tout = self.conv1(x)\n\t\tout = self.bn1(out)\n\t\tout = self.relu(out)\n\n\t\tout = self.conv2(out)\n\t\tout = self.bn2(out)\n\t\tout = self.relu(out)\n\n\t\tout = self.conv3(out)\n\t\tout = self.bn3(out)\n\n\t\tif self.downsample is not None:\n\t\t\tresidual = self.downsample(x)\n\n\t\tout += residual\n\t\tout = self.relu(out)\n\n\t\treturn out\n\n\n#------------------------------------------------------------------------------\n#   Class of ResNet\n#------------------------------------------------------------------------------\nclass ResNet(BaseBackbone):\n\tbasic_inplanes = 64\n\n\tdef __init__(self, block, layers, output_stride=32, num_classes=1000):\n\t\tsuper(ResNet, self).__init__()\n\t\tself.inplanes = self.basic_inplanes\n\t\tself.output_stride = output_stride\n\t\tself.num_classes = num_classes\n\n\t\tif output_stride==8:\n\t\t\tstrides   = [1, 2, 1, 1]\n\t\t\tdilations = [1, 1, 2, 4]\n\t\telif output_stride==16:\n\t\t\tstrides   = [1, 2, 2, 1]\n\t\t\tdilations = [1, 1, 1, 2]\n\t\telif output_stride==32:\n\t\t\tstrides   = [1, 2, 2, 2]\n\t\t\tdilations = [1, 1, 1, 1]\n\t\telse:\n\t\t\traise NotImplementedError\n\n\t\tself.conv1 = nn.Conv2d(3, self.basic_inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n\t\tself.bn1 = nn.BatchNorm2d(self.basic_inplanes)\n\t\tself.relu = nn.ReLU(inplace=True)\n\t\tself.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n\t\tself.layer1 = self._make_layer(block, 1*self.basic_inplanes, num_layers=layers[0], stride=strides[0], dilation=dilations[0])\n\t\tself.layer2 = self._make_layer(block, 2*self.basic_inplanes, num_layers=layers[1], stride=strides[1], dilation=dilations[1])\n\t\tself.layer3 = self._make_layer(block, 4*self.basic_inplanes, num_layers=layers[2], stride=strides[2], dilation=dilations[2])\n\t\tself.layer4 = self._make_layer(block, 8*self.basic_inplanes, num_layers=layers[3], stride=strides[3], dilation=dilations[3])\n\n\t\tif self.num_classes is not None:\n\t\t\tself.fc = nn.Linear(8*self.basic_inplanes * block.expansion, num_classes)\n\n\t\tself.init_weights()\n\n\n\tdef forward(self, x):\n\t\t# Stage1\n\t\tx = self.conv1(x)\n\t\tx = self.bn1(x)\n\t\tx = self.relu(x)\n\t\t# Stage2\n\t\tx = self.maxpool(x)\n\t\tx = self.layer1(x)\n\t\t# Stage3\n\t\tx = self.layer2(x)\n\t\t# Stage4\n\t\tx = self.layer3(x)\n\t\t# Stage5\n\t\tx = self.layer4(x)\n\t\t# Classification\n\t\tif self.num_classes is not None:\n\t\t\tx = x.mean(dim=(2,3))\n\t\t\tx = self.fc(x)\n\t\t# Output\n\t\treturn x\n\n\n\tdef _make_layer(self, block, planes, num_layers, stride=1, dilation=1, grids=None):\n\t\t# Downsampler\n\t\tdownsample = None\n\t\tif (stride != 1) or (self.inplanes != planes * block.expansion):\n\t\t\tdownsample = nn.Sequential(\n\t\t\t\tconv1x1(self.inplanes, planes * block.expansion, stride),\n\t\t\t\tnn.BatchNorm2d(planes * block.expansion))\n\t\t# Multi-grids\n\t\tif dilation!=1:\n\t\t\tdilations = [dilation*(2**layer_idx) for layer_idx in range(num_layers)]\n\t\telse:\n\t\t\tdilations = num_layers*[dilation]\n\t\t# Construct layers\n\t\tlayers = []\n\t\tlayers.append(block(self.inplanes, planes, stride, downsample, dilations[0]))\n\t\tself.inplanes = planes * block.expansion\n\t\tfor i in range(1, num_layers):\n\t\t\tlayers.append(block(self.inplanes, planes, dilation=dilations[i]))\n\t\treturn nn.Sequential(*layers)\n\n\n#------------------------------------------------------------------------------\n#   Instances of ResNet\n#------------------------------------------------------------------------------\ndef resnet18(pretrained=None, **kwargs):\n\tmodel = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n\tif pretrained is not None:\n\t\tmodel._load_pretrained_model(pretrained)\n\treturn model\n\n\ndef resnet34(pretrained=None, **kwargs):\n\tmodel = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n\tif pretrained is not None:\n\t\tmodel._load_pretrained_model(pretrained)\n\treturn model\n\n\ndef resnet50(pretrained=None, **kwargs):\n\tmodel = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n\tif pretrained is not None:\n\t\tmodel._load_pretrained_model(pretrained)\n\treturn model\n\n\ndef resnet101(pretrained=None, **kwargs):\n\tmodel = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n\tif pretrained is not None:\n\t\tmodel._load_pretrained_model(pretrained)\n\treturn model\n\n\ndef resnet152(pretrained=None, **kwargs):\n\tmodel = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n\tif pretrained is not None:\n\t\tmodel._load_pretrained_model(pretrained)\n\treturn model\n\n\ndef get_resnet(num_layers, **kwargs):\n\tif num_layers==18:\n\t\treturn resnet18(**kwargs)\n\telif num_layers==34:\n\t\treturn resnet34(**kwargs)\n\telif num_layers==50:\n\t\treturn resnet50(**kwargs)\n\telif num_layers==101:\n\t\treturn resnet101(**kwargs)\n\telif num_layers==152:\n\t\treturn resnet152(**kwargs)\n\telse:\n\t\traise NotImplementedError'"
models/backbonds/VGG.py,2,"b'#------------------------------------------------------------------------------\n#   Libraries\n#------------------------------------------------------------------------------\nimport torch\nimport torch.nn as nn\n\n\n#------------------------------------------------------------------------------\n#   Class of VGG\n#------------------------------------------------------------------------------\nclass VGG(nn.Module):\n\tdef __init__(self, blocks, input_sz=224, num_classes=1000, output_stride=32):\n\t\tsuper(VGG, self).__init__()\n\t\tself.output_stride = output_stride\n\t\tif output_stride==8:\n\t\t\tstrides   = [2, 2, 2, 1, 1]\n\t\t\tdilations = [1, 1, 1, 2, 4]\n\t\telif output_stride==16:\n\t\t\tstrides   = [2, 2, 2, 2, 1]\n\t\t\tdilations = [1, 1, 1, 1, 2]\n\t\telif output_stride==32:\n\t\t\tstrides   = [2, 2, 2, 2, 2]\n\t\t\tdilations = [1, 1, 1, 1, 1]\n\t\telse:\n\t\t\traise NotImplementedError\n\n\t\tself.layer1 = self._build_block(in_channels=3, block=blocks[0], dilation=dilations[0])\n\t\tself.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) if strides[0]==2 else nn.Sequential()\n\n\t\tself.layer2 = self._build_block(in_channels=blocks[0][-1], block=blocks[1], dilation=dilations[1])\n\t\tself.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2) if strides[1]==2 else nn.Sequential()\n\n\t\tself.layer3 = self._build_block(in_channels=blocks[1][-1], block=blocks[2], dilation=dilations[2])\n\t\tself.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2) if strides[2]==2 else nn.Sequential()\n\n\t\tself.layer4 = self._build_block(in_channels=blocks[2][-1], block=blocks[3], dilation=dilations[3])\n\t\tself.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2) if strides[3]==2 else nn.Sequential()\n\n\t\tself.layer5 = self._build_block(in_channels=blocks[3][-1], block=blocks[4], dilation=dilations[4])\n\t\tself.maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2) if strides[4]==2 else nn.Sequential()\n\n\t\tif output_stride==32:\n\t\t\tlinear_out_channels = 512 * int(input_sz/32)**2\n\t\t\tself.classifier = nn.Sequential(\n\t\t\t\tnn.Linear(linear_out_channels, 4096),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.Dropout(),\n\t\t\t\tnn.Linear(4096, 4096),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.Dropout(),\n\t\t\t\tnn.Linear(4096, num_classes),\n\t\t\t)\n\n\t\tself._init_weights()\n\n\n\tdef forward(self, x, feature_names=None):\n\t\tlow_features = {}\n\n\t\tx = self.layer1(x)\n\t\tx = self.maxpool1(x)\n\n\t\tx = self.layer2(x)\n\t\tx = self.maxpool2(x)\n\n\t\tx = self.layer3(x)\n\t\tlow_features[\'layer3\'] = x\n\t\tx = self.maxpool3(x)\n\n\t\tx = self.layer4(x)\n\t\tx = self.maxpool4(x)\n\n\t\tx = self.layer5(x)\n\t\tx = self.maxpool5(x)\n\n\t\tif self.output_stride==32:\n\t\t\tx = x.view(x.size(0), -1)\n\t\t\tx = self.classifier(x)\n\n\t\tif feature_names is not None:\n\t\t\tif type(feature_names)==str:\n\t\t\t\treturn x, low_features[feature_names]\n\t\t\telif type(feature_names)==list:\n\t\t\t\treturn tuple([x] + [low_features[name] for name in feature_names])\n\t\telse:\n\t\t\treturn x\n\n\n\tdef _build_block(self, in_channels, block, dilation):\n\t\tlayers = []\n\t\tfor layer_idx, out_channels in enumerate(block):\n\t\t\tif dilation!=1:\n\t\t\t\tgrid = 2**layer_idx\n\t\t\t\tdilation *= grid\n\t\t\tconv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation)\n\t\t\tlayers += [conv2d, nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)]\n\t\t\tin_channels = out_channels\n\t\treturn nn.Sequential(*layers)\n\n\n\tdef _load_pretrained_model(self, pretrained_file):\n\t\tpretrain_dict = torch.load(pretrained_file, map_location=\'cpu\')\n\t\tmodel_dict = {}\n\t\tstate_dict = self.state_dict()\n\t\tprint(""[VGG] Loading pretrained model..."")\n\t\tfor k, v in pretrain_dict.items():\n\t\t\tif k in state_dict:\n\t\t\t\tmodel_dict[k] = v\n\t\t\telse:\n\t\t\t\tprint(k, ""is ignored"")\n\t\tstate_dict.update(model_dict)\n\t\tself.load_state_dict(state_dict)\n\t\n\n\tdef _init_weights(self):\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.Conv2d):\n\t\t\t\tnn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n\t\t\telif isinstance(m, nn.BatchNorm2d):\n\t\t\t\tnn.init.constant_(m.weight, 1)\n\t\t\t\tnn.init.constant_(m.bias, 0)\n\t\t\telif isinstance(m, nn.Linear):\n\t\t\t\tm.weight.data.normal_(0, 0.01)\n\t\t\t\tm.bias.data.zero_()\n\n\n#------------------------------------------------------------------------------\n#   Instances of VGG\n#------------------------------------------------------------------------------\nblocks = {\n\t\'A\': [1*[64], 1*[128], 2*[256], 2*[512], 2*[512]],\n\t\'B\': [2*[64], 2*[128], 2*[256], 2*[512], 2*[512]],\n\t\'D\': [2*[64], 2*[128], 3*[256], 3*[512], 3*[512]],\n\t\'E\': [2*[64], 2*[128], 4*[256], 4*[512], 4*[512]],\n}\n\ndef vgg11_bn(pretrained=None, **kwargs):\n\tmodel = VGG(blocks[\'A\'], **kwargs)\n\tif pretrained:\n\t\tmodel._load_pretrained_model(pretrained)\n\treturn model\n\ndef vgg13_bn(pretrained=None, **kwargs):\n\tmodel = VGG(blocks[\'B\'], **kwargs)\n\tif pretrained:\n\t\tmodel._load_pretrained_model(pretrained)\n\treturn model\n\ndef vgg16_bn(pretrained=None, **kwargs):\n\tmodel = VGG(blocks[\'D\'], **kwargs)\n\tif pretrained:\n\t\tmodel._load_pretrained_model(pretrained)\n\treturn model\n\ndef vgg19_bn(pretrained=None, **kwargs):\n\tmodel = VGG(blocks[\'E\'], **kwargs)\n\tif pretrained:\n\t\tmodel._load_pretrained_model(pretrained)\n\treturn model\n\ndef get_vgg(n_layers, **kwargs):\n\tif n_layers==11:\n\t\treturn vgg11_bn(**kwargs)\n\telif n_layers==13:\n\t\treturn vgg13_bn(**kwargs)\n\telif n_layers==16:\n\t\treturn vgg16_bn(**kwargs)\n\telif n_layers==19:\n\t\treturn vgg19_bn(**kwargs)\n\telse:\n\t\traise NotImplementedError'"
models/backbonds/Xception.py,4,"b'"""""" \nCreates an Xception Model as defined in:\n\nFrancois Chollet\nXception: Deep Learning with Depthwise Separable Convolutions\nhttps://arxiv.org/pdf/1610.02357.pdf\n\nThis weights ported from the Keras implementation. Achieves the following performance on the validation set:\n\nLoss:0.9173 Prec@1:78.892 Prec@5:94.292\n\nREMEMBER to set your image size to 3x299x299 for both test and validation\n\nnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                  std=[0.5, 0.5, 0.5])\n\nThe resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n""""""\n#------------------------------------------------------------------------------\n#  Libraries\n#------------------------------------------------------------------------------\nimport torch, math\nimport torch.nn as nn\nfrom torch.nn import init\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\nmodel_urls = {\n    \'xception\':\'https://www.dropbox.com/s/1hplpzet9d7dv29/xception-c0a72b38.pth.tar?dl=1\'\n}\n\n\n#------------------------------------------------------------------------------\n#  Depthwise Separable Convolution\n#------------------------------------------------------------------------------\nclass DWSConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n        super(DWSConv2d, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\n    \n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\n#------------------------------------------------------------------------------\n#  Xception block\n#------------------------------------------------------------------------------\nclass Block(nn.Module):\n    def __init__(self, in_filters, out_filters, reps, strides=1, start_with_relu=True, grow_first=True):\n        super(Block, self).__init__()\n\n        if out_filters != in_filters or strides!=1:\n            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip=None\n        \n        self.relu = nn.ReLU(inplace=True)\n        rep=[]\n\n        filters=in_filters\n        if grow_first:\n            rep.append(self.relu)\n            rep.append(DWSConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n            filters = out_filters\n\n        for i in range(reps-1):\n            rep.append(self.relu)\n            rep.append(DWSConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(filters))\n        \n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(DWSConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n\n        if not start_with_relu:\n            rep = rep[1:]\n        else:\n            rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3,strides,1))\n        self.rep = nn.Sequential(*rep)\n\n\n    def forward(self,inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x+=skip\n        return x\n\n\n\n#------------------------------------------------------------------------------\n#  Xception\n#------------------------------------------------------------------------------\nclass Xception(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(Xception, self).__init__()\n        self.num_classes = num_classes\n\n        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        #do relu here\n\n        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n\n        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n\n        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n\n        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n\n        self.conv3 = DWSConv2d(1024,1536,3,1,1)\n        self.bn3 = nn.BatchNorm2d(1536)\n\n        #do relu here\n        self.conv4 = DWSConv2d(1536,2048,3,1,1)\n        self.bn4 = nn.BatchNorm2d(2048)\n\n        self.fc = nn.Linear(2048, num_classes)\n\n        # Init weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        \n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n        \n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n        \n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n#------------------------------------------------------------------------------\n#  Instance\n#------------------------------------------------------------------------------\ndef xception(pretrained=False,**kwargs):\n    model = Xception(**kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'xception\']))\n    return model'"
models/backbones/__init__.py,0,"b'from .resnet import (\n    ResNet, BasicBlock, Bottleneck, ResNetBasicBlock, ResNetBottleneckBlock,\n    resnet18, resnet26, resnet26d, resnet34,\n    resnet50, resnet101, resnet152,\n    tv_resnet34, tv_resnet50, tv_resnext50_32x4d,\n    wide_resnet50_2, wide_resnet101_2,\n    resnext50_32x4d, resnext50d_32x4d, resnext101_32x4d, resnext101_32x8d, resnext101_64x4d,\n    ig_resnext101_32x8d, ig_resnext101_32x16d, ig_resnext101_32x32d, ig_resnext101_32x48d,\n)\n\nfrom .efficientnet import (\n    EfficientNet, InvertedResidual, EfficientNetBlock,\n    efficientnet_b0, efficientnet_b1, efficientnet_b2, efficientnet_b3,\n    efficientnet_b4, efficientnet_b5, efficientnet_b6, efficientnet_b7,\n)\n'"
models/backbones/efficientnet.py,2,"b'#------------------------------------------------------------------------------\n#  Libraries\n#------------------------------------------------------------------------------\nfrom base import BaseBackbone\nfrom timm.models.gen_efficientnet import (\n\tInvertedResidual, default_cfgs, load_pretrained, _round_channels,\n\t_decode_arch_def, _resolve_bn_args, swish,\n)\nfrom timm.models.gen_efficientnet import GenEfficientNet as BaseEfficientNet\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom collections import OrderedDict\n\n\n#------------------------------------------------------------------------------\n#   EfficientNetBlock\n#------------------------------------------------------------------------------\nclass EfficientNetBlock(nn.Module):\n\tdef __init__(self, in_channels, out_channels, num_blocks=1, kernel_size=3):\n\t\tsuper(EfficientNetBlock, self).__init__()\n\t\tif num_blocks==2:\n\t\t\tself.block = nn.Sequential(\n\t\t\t\tInvertedResidual(\n\t\t\t\t\tin_channels, in_channels, dw_kernel_size=kernel_size,\n\t\t\t\t\tact_fn=swish, exp_ratio=6.0, se_ratio=0.25),\n\t\t\t\tInvertedResidual(\n\t\t\t\t\tin_channels, out_channels, dw_kernel_size=kernel_size,\n\t\t\t\t\tact_fn=swish, exp_ratio=6.0, se_ratio=0.25))\n\t\telse:\n\t\t\tself.block = nn.Sequential(\n\t\t\t\tInvertedResidual(\n\t\t\t\t\tin_channels, out_channels, dw_kernel_size=kernel_size,\n\t\t\t\t\tact_fn=swish, exp_ratio=6.0, se_ratio=0.25))\n\n\tdef forward(self, x):\n\t\tx = self.block(x)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  EfficientNet\n#------------------------------------------------------------------------------\nclass EfficientNet(BaseEfficientNet, BaseBackbone):\n\tstage_indices = {\n\t\t""tf_efficientnet_b0"": [0, 1, 2, 4, 6],\n\t\t""tf_efficientnet_b1"": [0, 1, 2, 4, 6],\n\t\t""tf_efficientnet_b2"": [0, 1, 2, 4, 6],\n\t\t""tf_efficientnet_b3"": [0, 1, 2, 4, 6],\n\t\t""tf_efficientnet_b4"": [0, 1, 2, 4, 6],\n\t\t""tf_efficientnet_b5"": [0, 1, 2, 4, 6],\n\t\t""tf_efficientnet_b6"": [0, 1, 2, 4, 6],\n\t\t""tf_efficientnet_b7"": [0, 1, 2, 4, 6],\n\t}\n\tstage_features = {\n\t\t""tf_efficientnet_b0"": [16, 24, 40, 112, 320],\n\t\t""tf_efficientnet_b1"": [16, 24, 40, 112, 320],\n\t\t""tf_efficientnet_b2"": [16, 24, 48, 120, 352],\n\t\t""tf_efficientnet_b3"": [24, 32, 48, 136, 384],\n\t\t""tf_efficientnet_b4"": [24, 32, 56, 160, 448],\n\t\t""tf_efficientnet_b5"": [24, 40, 64, 176, 512],\n\t\t""tf_efficientnet_b6"": [32, 40, 72, 200, 576],\n\t\t""tf_efficientnet_b7"": [32, 48, 80, 224, 640],\n\t}\n\tdef __init__(self, block_args, model_name, frozen_stages=-1, norm_eval=False, **kargs):\n\t\tsuper(EfficientNet, self).__init__(block_args=block_args, **kargs)\n\t\tself.frozen_stages = frozen_stages\n\t\tself.norm_eval = norm_eval\n\t\tself.model_name = model_name\n\n\tdef forward(self, input):\n\t\t# Stem\n\t\tx = self.conv_stem(input)\n\t\tx = self.bn1(x)\n\t\tx = self.act_fn(x, inplace=True)\n\n\t\t# Blocks\n\t\touts = []\n\t\tfor idx, block in enumerate(self.blocks):\n\t\t\tx = block(x)\n\t\t\tif idx in self.stage_indices[self.model_name]:\n\t\t\t\touts.append(x)\n\t\treturn tuple(outs)\n\n\tdef init_from_imagenet(self, archname):\n\t\tprint(""[%s] Load pretrained weights from ImageNet"" % (self.__class__.__name__))\n\t\tload_pretrained(self, default_cfgs[""tf_%s""%(archname)], self.num_classes)\n\n\tdef _freeze_stages(self):\n\t\t# Freeze stem\n\t\tif self.frozen_stages>=0:\n\t\t\tself.bn1.eval()\n\t\t\tfor module in [self.conv_stem, self.bn1]:\n\t\t\t\tfor param in module.parameters():\n\t\t\t\t\tparam.requires_grad = False\n\n\t\t# Chosen subsequent blocks are also frozen gradient\n\t\tfrozen_stages = list(range(1, self.frozen_stages+1))\n\t\tfor idx, block in enumerate(self.blocks):\n\t\t\tif idx <= self.stage_indices[self.model_name][0]:\n\t\t\t\tstage = 1\n\t\t\telif self.stage_indices[self.model_name][0] < idx <= self.stage_indices[self.model_name][1]:\n\t\t\t\tstage = 2\n\t\t\telif self.stage_indices[self.model_name][1] < idx <= self.stage_indices[self.model_name][2]:\n\t\t\t\tstage = 3\n\t\t\telif self.stage_indices[self.model_name][2] < idx <= self.stage_indices[self.model_name][3]:\n\t\t\t\tstage = 4\n\t\t\tif stage in frozen_stages:\n\t\t\t\tblock.eval()\n\t\t\t\tfor param in block.parameters():\n\t\t\t\t\tparam.requires_grad = False\n\t\t\telse:\n\t\t\t\tbreak\n\n\n#------------------------------------------------------------------------------\n#  Versions of EfficientNet\n#------------------------------------------------------------------------------\ndef _gen_efficientnet(model_name, channel_multiplier=1.0, depth_multiplier=1.0, num_classes=1000, **kwargs):\n\t""""""\n\tEfficientNet params\n\tname: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n\t\'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n\t\'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n\t\'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n\t\'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n\t\'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n\t\'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n\t\'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n\t\'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n\tArgs:\n\t  channel_multiplier: multiplier to number of channels per layer\n\t  depth_multiplier: multiplier to number of repeats per stage\n\t""""""\n\tarch_def = [\n\t\t[\'ds_r1_k3_s1_e1_c16_se0.25\'],\n\t\t[\'ir_r2_k3_s2_e6_c24_se0.25\'],\n\t\t[\'ir_r2_k5_s2_e6_c40_se0.25\'],\n\t\t[\'ir_r3_k3_s2_e6_c80_se0.25\'],\n\t\t[\'ir_r3_k5_s1_e6_c112_se0.25\'],\n\t\t[\'ir_r4_k5_s2_e6_c192_se0.25\'],\n\t\t[\'ir_r1_k3_s1_e6_c320_se0.25\'],\n\t]\n\t# NOTE: other models in the family didn\'t scale the feature count\n\tnum_features = _round_channels(1280, channel_multiplier, 8, None)\n\tmodel = EfficientNet(\n\t\t_decode_arch_def(arch_def, depth_multiplier),\n\t\tmodel_name=model_name,\n\t\tnum_classes=num_classes,\n\t\tstem_size=32,\n\t\tchannel_multiplier=channel_multiplier,\n\t\tnum_features=num_features,\n\t\tbn_args=_resolve_bn_args(kwargs),\n\t\tact_fn=swish,\n\t\t**kwargs\n\t)\n\treturn model\n\ndef efficientnet_b0(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t"""""" EfficientNet-B0 """"""\n\tmodel_name = ""tf_efficientnet_b0""\n\tdefault_cfg = default_cfgs[model_name]\n\t# NOTE for train, drop_rate should be 0.2\n\t#kwargs[\'drop_connect_rate\'] = 0.2  # set when training, TODO add as cmd arg\n\tmodel = _gen_efficientnet(\n\t\tmodel_name=model_name, channel_multiplier=1.0, depth_multiplier=1.0,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfgs[model_name], num_classes)\n\treturn model\n\ndef efficientnet_b1(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t"""""" EfficientNet-B1 """"""\n\tmodel_name = ""tf_efficientnet_b1""\n\tdefault_cfg = default_cfgs[model_name]\n\t# NOTE for train, drop_rate should be 0.2\n\t#kwargs[\'drop_connect_rate\'] = 0.2  # set when training, TODO add as cmd arg\n\tmodel = _gen_efficientnet(\n\t\tmodel_name=model_name, channel_multiplier=1.0, depth_multiplier=1.1,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfgs[model_name], num_classes)\n\treturn model\n\ndef efficientnet_b2(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t"""""" EfficientNet-B2 """"""\n\tmodel_name = ""tf_efficientnet_b2""\n\tdefault_cfg = default_cfgs[model_name]\n\t# NOTE for train, drop_rate should be 0.3\n\t#kwargs[\'drop_connect_rate\'] = 0.2  # set when training, TODO add as cmd arg\n\tmodel = _gen_efficientnet(\n\t\tmodel_name=model_name, channel_multiplier=1.1, depth_multiplier=1.2,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfgs[model_name], num_classes)\n\treturn model\n\ndef efficientnet_b3(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t"""""" EfficientNet-B3 """"""\n\tmodel_name = ""tf_efficientnet_b3""\n\tdefault_cfg = default_cfgs[model_name]\n\t# NOTE for train, drop_rate should be 0.3\n\t#kwargs[\'drop_connect_rate\'] = 0.2  # set when training, TODO add as cmd arg\n\tmodel = _gen_efficientnet(\n\t\tmodel_name=model_name, channel_multiplier=1.2, depth_multiplier=1.4,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfgs[model_name], num_classes)\n\treturn model\n\ndef efficientnet_b4(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t"""""" EfficientNet-B4 """"""\n\tmodel_name = ""tf_efficientnet_b4""\n\tdefault_cfg = default_cfgs[model_name]\n\t# NOTE for train, drop_rate should be 0.4\n\t#kwargs[\'drop_connect_rate\'] = 0.2  #  set when training, TODO add as cmd arg\n\tmodel = _gen_efficientnet(\n\t\tmodel_name=model_name, channel_multiplier=1.4, depth_multiplier=1.8,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfgs[model_name], num_classes)\n\treturn model\n\ndef efficientnet_b5(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t"""""" EfficientNet-B5 """"""\n\t# NOTE for train, drop_rate should be 0.4\n\t#kwargs[\'drop_connect_rate\'] = 0.2  # set when training, TODO add as cmd arg\n\tmodel_name = ""tf_efficientnet_b5""\n\tdefault_cfg = default_cfgs[model_name]\n\tmodel = _gen_efficientnet(\n\t\tmodel_name=model_name, channel_multiplier=1.6, depth_multiplier=2.2,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfgs[model_name], num_classes)\n\treturn model\n\ndef efficientnet_b6(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t"""""" EfficientNet-B6 """"""\n\t# NOTE for train, drop_rate should be 0.5\n\t#kwargs[\'drop_connect_rate\'] = 0.2  # set when training, TODO add as cmd arg\n\tmodel_name = ""tf_efficientnet_b6""\n\tdefault_cfg = default_cfgs[model_name]\n\tmodel = _gen_efficientnet(\n\t\tmodel_name=model_name, channel_multiplier=1.8, depth_multiplier=2.6,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfgs[model_name], num_classes)\n\treturn model\n\ndef efficientnet_b7(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t"""""" EfficientNet-B7 """"""\n\t# NOTE for train, drop_rate should be 0.5\n\t#kwargs[\'drop_connect_rate\'] = 0.2  # set when training, TODO add as cmd arg\n\tmodel_name = ""tf_efficientnet_b7""\n\tdefault_cfg = default_cfgs[model_name]\n\tmodel = _gen_efficientnet(\n\t\tmodel_name=model_name, channel_multiplier=2.0, depth_multiplier=3.1,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfgs[model_name], num_classes)\n\treturn model\n'"
models/backbones/resnet.py,6,"b'#------------------------------------------------------------------------------\n#  Libraries\n#------------------------------------------------------------------------------\nfrom base import BaseBackboneWrapper\nfrom timm.models.resnet import ResNet as BaseResNet\nfrom timm.models.resnet import default_cfgs, load_pretrained, BasicBlock, Bottleneck\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\n\n#------------------------------------------------------------------------------\n#   ResNetBlock\n#------------------------------------------------------------------------------\nclass ResNetBasicBlock(nn.Module):\n\texpansion = 1\n\n\tdef __init__(self, in_channels, out_channels):\n\t\tsuper(ResNetBasicBlock, self).__init__()\n\t\tdownsample = nn.Sequential(OrderedDict([\n\t\t\t(""conv"", nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)),\n\t\t\t(""bn"", nn.BatchNorm2d(out_channels))\n\t\t]))\n\t\tself.block = BasicBlock(\n\t\t\tin_channels,\n\t\t\tint(out_channels/BasicBlock.expansion),\n\t\t\tdownsample=downsample,\n\t\t)\n\n\tdef forward(self, x):\n\t\tx = self.block(x)\n\t\treturn x\n\n\nclass ResNetBottleneckBlock(nn.Module):\n\texpansion = 4\n\t\n\tdef __init__(self, in_channels, out_channels):\n\t\tsuper(ResNetBottleneckBlock, self).__init__()\n\t\tdownsample = nn.Sequential(OrderedDict([\n\t\t\t(""conv"", nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)),\n\t\t\t(""bn"", nn.BatchNorm2d(out_channels))\n\t\t]))\n\t\tself.block = Bottleneck(\n\t\t\tin_channels,\n\t\t\tint(out_channels/Bottleneck.expansion),\n\t\t\tdownsample=downsample,\n\t\t)\n\n\tdef forward(self, x):\n\t\tx = self.block(x)\n\t\treturn x\n\n\n#------------------------------------------------------------------------------\n#  ResNet\n#------------------------------------------------------------------------------\nclass ResNet(BaseResNet, BaseBackboneWrapper):\n\tdef __init__(self, block, layers, frozen_stages=-1, norm_eval=False, **kargs):\n\t\tsuper(ResNet, self).__init__(block=block, layers=layers, **kargs)\n\t\tself.frozen_stages = frozen_stages\n\t\tself.norm_eval = norm_eval\n\n\tdef forward(self, input):\n\t\t# Stem\n\t\tx1 = self.conv1(input)\n\t\tx1 = self.bn1(x1)\n\t\tx1 = self.relu(x1)\n\t\t# Stage1\n\t\tx2 = self.maxpool(x1)\n\t\tx2 = self.layer1(x2)\n\t\t# Stage2\n\t\tx3 = self.layer2(x2)\n\t\t# Stage3\n\t\tx4 = self.layer3(x3)\n\t\t# Stage4\n\t\tx5 = self.layer4(x4)\n\t\t# Output\n\t\treturn x1, x2, x3, x4, x5\n\n\tdef init_from_imagenet(self, archname):\n\t\tload_pretrained(self, default_cfgs[archname], self.num_classes)\n\n\tdef _freeze_stages(self):\n\t\t# Freeze stem\n\t\tif self.frozen_stages>=0:\n\t\t\tself.bn1.eval()\n\t\t\tfor module in [self.conv1, self.bn1]:\n\t\t\t\tfor param in module.parameters():\n\t\t\t\t\tparam.requires_grad = False\n\n\t\t# Chosen subsequent blocks are also frozen\n\t\tfor stage_idx in range(1, self.frozen_stages+1):\n\t\t\tfor module in getattr(self, ""layer%d""%(stage_idx)):\n\t\t\t\tmodule.eval()\n\t\t\t\tfor param in module.parameters():\n\t\t\t\t\tparam.requires_grad = False\n\n\n#------------------------------------------------------------------------------\n#  Versions of ResNet\n#------------------------------------------------------------------------------\ndef resnet18(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNet-18 model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnet18\']\n\tmodel = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnet34(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNet-34 model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnet34\']\n\tmodel = ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnet26(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNet-26 model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnet26\']\n\tmodel = ResNet(Bottleneck, [2, 2, 2, 2], num_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnet26d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNet-26 v1d model.\n\tThis is technically a 28 layer ResNet, sticking with \'d\' modifier from Gluon for now.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnet26d\']\n\tmodel = ResNet(\n\t\tBottleneck, [2, 2, 2, 2], stem_width=32, deep_stem=True, avg_down=True,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnet50(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNet-50 model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnet50\']\n\tmodel = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnet101(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNet-101 model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnet101\']\n\tmodel = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnet152(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNet-152 model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnet152\']\n\tmodel = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef tv_resnet34(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNet-34 model with original Torchvision weights.\n\t""""""\n\tmodel = ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfgs[\'tv_resnet34\']\n\tif pretrained:\n\t\tload_pretrained(model, model.default_cfg, num_classes, in_chans)\n\treturn model\n\ndef tv_resnet50(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNet-50 model with original Torchvision weights.\n\t""""""\n\tmodel = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfgs[\'tv_resnet50\']\n\tif pretrained:\n\t\tload_pretrained(model, model.default_cfg, num_classes, in_chans)\n\treturn model\n\ndef wide_resnet50_2(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a Wide ResNet-50-2 model.\n\tThe model is the same as ResNet except for the bottleneck number of channels\n\twhich is twice larger in every block. The number of channels in outer 1x1\n\tconvolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n\tchannels, and in Wide ResNet-50-2 has 2048-1024-2048.\n\t""""""\n\tmodel = ResNet(\n\t\tBottleneck, [3, 4, 6, 3], base_width=128,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfgs[\'wide_resnet50_2\']\n\tif pretrained:\n\t\tload_pretrained(model, model.default_cfg, num_classes, in_chans)\n\treturn model\n\ndef wide_resnet101_2(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a Wide ResNet-101-2 model.\n\tThe model is the same as ResNet except for the bottleneck number of channels\n\twhich is twice larger in every block. The number of channels in outer 1x1\n\tconvolutions is the same.\n\t""""""\n\tmodel = ResNet(\n\t\tBottleneck, [3, 4, 23, 3], base_width=128,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfgs[\'wide_resnet101_2\']\n\tif pretrained:\n\t\tload_pretrained(model, model.default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnext50_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt50-32x4d model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnext50_32x4d\']\n\tmodel = ResNet(\n\t\tBottleneck, [3, 4, 6, 3], cardinality=32, base_width=4,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnext50d_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt50d-32x4d model. ResNext50 w/ deep stem & avg pool downsample\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnext50d_32x4d\']\n\tmodel = ResNet(\n\t\tBottleneck, [3, 4, 6, 3], cardinality=32, base_width=4,\n\t\tstem_width=32, deep_stem=True, avg_down=True,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnext101_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt-101 32x4d model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnext101_32x4d\']\n\tmodel = ResNet(\n\t\tBottleneck, [3, 4, 23, 3], cardinality=32, base_width=4,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnext101_32x8d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt-101 32x8d model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnext101_32x8d\']\n\tmodel = ResNet(\n\t\tBottleneck, [3, 4, 23, 3], cardinality=32, base_width=8,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef resnext101_64x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt101-64x4d model.\n\t""""""\n\tdefault_cfg = default_cfgs[\'resnext101_32x4d\']\n\tmodel = ResNet(\n\t\tBottleneck, [3, 4, 23, 3], cardinality=64, base_width=4,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef tv_resnext50_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt50-32x4d model with original Torchvision weights.\n\t""""""\n\tdefault_cfg = default_cfgs[\'tv_resnext50_32x4d\']\n\tmodel = ResNet(\n\t\tBottleneck, [3, 4, 6, 3], cardinality=32, base_width=4,\n\t\tnum_classes=num_classes, in_chans=in_chans, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef ig_resnext101_32x8d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data\n\tand finetuned on ImageNet from Figure 5 in\n\t`""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_\n\tWeights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n\tArgs:\n\t\tpretrained (bool): load pretrained weights\n\t\tnum_classes (int): number of classes for classifier (default: 1000 for pretrained)\n\t\tin_chans (int): number of input planes (default: 3 for pretrained / color)\n\t""""""\n\tdefault_cfg = default_cfgs[\'ig_resnext101_32x8d\']\n\tmodel = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=8, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef ig_resnext101_32x16d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt-101 32x16 model pre-trained on weakly-supervised data\n\tand finetuned on ImageNet from Figure 5 in\n\t`""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_\n\tWeights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n\tArgs:\n\t\tpretrained (bool): load pretrained weights\n\t\tnum_classes (int): number of classes for classifier (default: 1000 for pretrained)\n\t\tin_chans (int): number of input planes (default: 3 for pretrained / color)\n\t""""""\n\tdefault_cfg = default_cfgs[\'ig_resnext101_32x16d\']\n\tmodel = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=16, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef ig_resnext101_32x32d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt-101 32x32 model pre-trained on weakly-supervised data\n\tand finetuned on ImageNet from Figure 5 in\n\t`""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_\n\tWeights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n\tArgs:\n\t\tpretrained (bool): load pretrained weights\n\t\tnum_classes (int): number of classes for classifier (default: 1000 for pretrained)\n\t\tin_chans (int): number of input planes (default: 3 for pretrained / color)\n\t""""""\n\tdefault_cfg = default_cfgs[\'ig_resnext101_32x32d\']\n\tmodel = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=32, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n\ndef ig_resnext101_32x48d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n\t""""""Constructs a ResNeXt-101 32x48 model pre-trained on weakly-supervised data\n\tand finetuned on ImageNet from Figure 5 in\n\t`""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_\n\tWeights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n\tArgs:\n\t\tpretrained (bool): load pretrained weights\n\t\tnum_classes (int): number of classes for classifier (default: 1000 for pretrained)\n\t\tin_chans (int): number of input planes (default: 3 for pretrained / color)\n\t""""""\n\tdefault_cfg = default_cfgs[\'ig_resnext101_32x48d\']\n\tmodel = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=48, **kwargs)\n\tmodel.default_cfg = default_cfg\n\tif pretrained:\n\t\tload_pretrained(model, default_cfg, num_classes, in_chans)\n\treturn model\n'"
