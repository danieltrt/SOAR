file_path,api_count,code
inference.py,5,"b'import os\nimport json\nimport torch\nimport argparse\n\nfrom model import SentenceVAE\nfrom utils import to_var, idx2word, interpolate\n\n\ndef main(args):\n\n    with open(args.data_dir+\'/ptb.vocab.json\', \'r\') as file:\n        vocab = json.load(file)\n\n    w2i, i2w = vocab[\'w2i\'], vocab[\'i2w\']\n\n    model = SentenceVAE(\n        vocab_size=len(w2i),\n        sos_idx=w2i[\'<sos>\'],\n        eos_idx=w2i[\'<eos>\'],\n        pad_idx=w2i[\'<pad>\'],\n        unk_idx=w2i[\'<unk>\'],\n        max_sequence_length=args.max_sequence_length,\n        embedding_size=args.embedding_size,\n        rnn_type=args.rnn_type,\n        hidden_size=args.hidden_size,\n        word_dropout=args.word_dropout,\n        embedding_dropout=args.embedding_dropout,\n        latent_size=args.latent_size,\n        num_layers=args.num_layers,\n        bidirectional=args.bidirectional\n        )\n\n    if not os.path.exists(args.load_checkpoint):\n        raise FileNotFoundError(args.load_checkpoint)\n\n    model.load_state_dict(torch.load(args.load_checkpoint))\n    print(""Model loaded from %s""%(args.load_checkpoint))\n\n    if torch.cuda.is_available():\n        model = model.cuda()\n    \n    model.eval()\n\n    samples, z = model.inference(n=args.num_samples)\n    print(\'----------SAMPLES----------\')\n    print(*idx2word(samples, i2w=i2w, pad_idx=w2i[\'<pad>\']), sep=\'\\n\')\n\n    z1 = torch.randn([args.latent_size]).numpy()\n    z2 = torch.randn([args.latent_size]).numpy()\n    z = to_var(torch.from_numpy(interpolate(start=z1, end=z2, steps=8)).float())\n    samples, _ = model.inference(z=z)\n    print(\'-------INTERPOLATION-------\')\n    print(*idx2word(samples, i2w=i2w, pad_idx=w2i[\'<pad>\']), sep=\'\\n\')\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'-c\', \'--load_checkpoint\', type=str)\n    parser.add_argument(\'-n\', \'--num_samples\', type=int, default=10)\n\n    parser.add_argument(\'-dd\', \'--data_dir\', type=str, default=\'data\')\n    parser.add_argument(\'-ms\', \'--max_sequence_length\', type=int, default=50)\n    parser.add_argument(\'-eb\', \'--embedding_size\', type=int, default=300)\n    parser.add_argument(\'-rnn\', \'--rnn_type\', type=str, default=\'gru\')\n    parser.add_argument(\'-hs\', \'--hidden_size\', type=int, default=256)\n    parser.add_argument(\'-wd\', \'--word_dropout\', type=float, default=0)\n    parser.add_argument(\'-ed\', \'--embedding_dropout\', type=float, default=0.5)\n    parser.add_argument(\'-ls\', \'--latent_size\', type=int, default=16)\n    parser.add_argument(\'-nl\', \'--num_layers\', type=int, default=1)\n    parser.add_argument(\'-bi\', \'--bidirectional\', action=\'store_true\')\n\n    args = parser.parse_args()\n\n    args.rnn_type = args.rnn_type.lower()\n\n    assert args.rnn_type in [\'rnn\', \'lstm\', \'gru\']\n    assert 0 <= args.word_dropout <= 1\n\n    main(args)\n'"
model.py,17,"b""import torch\nimport torch.nn as nn\nimport torch.nn.utils.rnn as rnn_utils\nfrom utils import to_var\n\nclass SentenceVAE(nn.Module):\n\n    def __init__(self, vocab_size, embedding_size, rnn_type, hidden_size, word_dropout, embedding_dropout, latent_size,\n                sos_idx, eos_idx, pad_idx, unk_idx, max_sequence_length, num_layers=1, bidirectional=False):\n\n        super().__init__()\n        self.tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n\n        self.max_sequence_length = max_sequence_length\n        self.sos_idx = sos_idx\n        self.eos_idx = eos_idx\n        self.pad_idx = pad_idx\n        self.unk_idx = unk_idx\n\n        self.latent_size = latent_size\n\n        self.rnn_type = rnn_type\n        self.bidirectional = bidirectional\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.word_dropout_rate = word_dropout\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n\n        if rnn_type == 'rnn':\n            rnn = nn.RNN\n        elif rnn_type == 'gru':\n            rnn = nn.GRU\n        # elif rnn_type == 'lstm':\n        #     rnn = nn.LSTM\n        else:\n            raise ValueError()\n\n        self.encoder_rnn = rnn(embedding_size, hidden_size, num_layers=num_layers, bidirectional=self.bidirectional, batch_first=True)\n        self.decoder_rnn = rnn(embedding_size, hidden_size, num_layers=num_layers, bidirectional=self.bidirectional, batch_first=True)\n\n        self.hidden_factor = (2 if bidirectional else 1) * num_layers\n\n        self.hidden2mean = nn.Linear(hidden_size * self.hidden_factor, latent_size)\n        self.hidden2logv = nn.Linear(hidden_size * self.hidden_factor, latent_size)\n        self.latent2hidden = nn.Linear(latent_size, hidden_size * self.hidden_factor)\n        self.outputs2vocab = nn.Linear(hidden_size * (2 if bidirectional else 1), vocab_size)\n\n    def forward(self, input_sequence, length):\n\n        batch_size = input_sequence.size(0)\n        sorted_lengths, sorted_idx = torch.sort(length, descending=True)\n        input_sequence = input_sequence[sorted_idx]\n\n        # ENCODER\n        input_embedding = self.embedding(input_sequence)\n\n        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n\n        _, hidden = self.encoder_rnn(packed_input)\n\n        if self.bidirectional or self.num_layers > 1:\n            # flatten hidden state\n            hidden = hidden.view(batch_size, self.hidden_size*self.hidden_factor)\n        else:\n            hidden = hidden.squeeze()\n\n        # REPARAMETERIZATION\n        mean = self.hidden2mean(hidden)\n        logv = self.hidden2logv(hidden)\n        std = torch.exp(0.5 * logv)\n\n        z = to_var(torch.randn([batch_size, self.latent_size]))\n        z = z * std + mean\n\n        # DECODER\n        hidden = self.latent2hidden(z)\n\n        if self.bidirectional or self.num_layers > 1:\n            # unflatten hidden state\n            hidden = hidden.view(self.hidden_factor, batch_size, self.hidden_size)\n        else:\n            hidden = hidden.unsqueeze(0)\n\n        # decoder input\n        if self.word_dropout_rate > 0:\n            # randomly replace decoder input with <unk>\n            prob = torch.rand(input_sequence.size())\n            if torch.cuda.is_available():\n                prob=prob.cuda()\n            prob[(input_sequence.data - self.sos_idx) * (input_sequence.data - self.pad_idx) == 0] = 1\n            decoder_input_sequence = input_sequence.clone()\n            decoder_input_sequence[prob < self.word_dropout_rate] = self.unk_idx\n            input_embedding = self.embedding(decoder_input_sequence)\n        input_embedding = self.embedding_dropout(input_embedding)\n        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n\n        # decoder forward pass\n        outputs, _ = self.decoder_rnn(packed_input, hidden)\n\n        # process outputs\n        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True)[0]\n        padded_outputs = padded_outputs.contiguous()\n        _,reversed_idx = torch.sort(sorted_idx)\n        padded_outputs = padded_outputs[reversed_idx]\n        b,s,_ = padded_outputs.size()\n\n        # project outputs to vocab\n        logp = nn.functional.log_softmax(self.outputs2vocab(padded_outputs.view(-1, padded_outputs.size(2))), dim=-1)\n        logp = logp.view(b, s, self.embedding.num_embeddings)\n\n\n        return logp, mean, logv, z\n\n\n    def inference(self, n=4, z=None):\n\n        if z is None:\n            batch_size = n\n            z = to_var(torch.randn([batch_size, self.latent_size]))\n        else:\n            batch_size = z.size(0)\n\n        hidden = self.latent2hidden(z)\n\n        if self.bidirectional or self.num_layers > 1:\n            # unflatten hidden state\n            hidden = hidden.view(self.hidden_factor, batch_size, self.hidden_size)\n\n        hidden = hidden.unsqueeze(0)\n\n        # required for dynamic stopping of sentence generation\n        sequence_idx = torch.arange(0, batch_size, out=self.tensor()).long() # all idx of batch\n        sequence_running = torch.arange(0, batch_size, out=self.tensor()).long() # all idx of batch which are still generating\n        sequence_mask = torch.ones(batch_size, out=self.tensor()).byte()\n\n        running_seqs = torch.arange(0, batch_size, out=self.tensor()).long() # idx of still generating sequences with respect to current loop\n\n        generations = self.tensor(batch_size, self.max_sequence_length).fill_(self.pad_idx).long()\n\n        t=0\n        while(t<self.max_sequence_length and len(running_seqs)>0):\n\n            if t == 0:\n                input_sequence = to_var(torch.Tensor(batch_size).fill_(self.sos_idx).long())\n\n            input_sequence = input_sequence.unsqueeze(1)\n\n            input_embedding = self.embedding(input_sequence)\n\n            output, hidden = self.decoder_rnn(input_embedding, hidden)\n\n            logits = self.outputs2vocab(output)\n\n            input_sequence = self._sample(logits)\n\n            # save next input\n            generations = self._save_sample(generations, input_sequence, sequence_running, t)\n\n            # update gloabl running sequence\n            sequence_mask[sequence_running] = (input_sequence != self.eos_idx).data\n            sequence_running = sequence_idx.masked_select(sequence_mask)\n\n            # update local running sequences\n            running_mask = (input_sequence != self.eos_idx).data\n            running_seqs = running_seqs.masked_select(running_mask)\n\n            # prune input and hidden state according to local update\n            if len(running_seqs) > 0:\n                input_sequence = input_sequence[running_seqs]\n                hidden = hidden[:, running_seqs]\n\n                running_seqs = torch.arange(0, len(running_seqs), out=self.tensor()).long()\n\n            t += 1\n\n        return generations, z\n\n    def _sample(self, dist, mode='greedy'):\n\n        if mode == 'greedy':\n            _, sample = torch.topk(dist, 1, dim=-1)\n        sample = sample.squeeze()\n\n        return sample\n\n    def _save_sample(self, save_to, sample, running_seqs, t):\n        # select only still running\n        running_latest = save_to[running_seqs]\n        # update token at position t\n        running_latest[:,t] = sample.data\n        # save back\n        save_to[running_seqs] = running_latest\n\n        return save_to\n"""
ptb.py,1,"b'import os\nimport io\nimport json\nimport torch\nimport numpy as np\nfrom collections import defaultdict\nfrom torch.utils.data import Dataset\nfrom nltk.tokenize import TweetTokenizer\n\nfrom utils import OrderedCounter\n\nclass PTB(Dataset):\n\n    def __init__(self, data_dir, split, create_data, **kwargs):\n\n        super().__init__()\n        self.data_dir = data_dir\n        self.split = split\n        self.max_sequence_length = kwargs.get(\'max_sequence_length\', 50)\n        self.min_occ = kwargs.get(\'min_occ\', 3)\n\n        self.raw_data_path = os.path.join(data_dir, \'ptb.\'+split+\'.txt\')\n        self.data_file = \'ptb.\'+split+\'.json\'\n        self.vocab_file = \'ptb.vocab.json\'\n\n        if create_data:\n            print(""Creating new %s ptb data.""%split.upper())\n            self._create_data()\n\n        elif not os.path.exists(os.path.join(self.data_dir, self.data_file)):\n            print(""%s preprocessed file not found at %s. Creating new.""%(split.upper(), os.path.join(self.data_dir, self.data_file)))\n            self._create_data()\n\n        else:\n            self._load_data()\n\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        idx = str(idx)\n\n        return {\n            \'input\': np.asarray(self.data[idx][\'input\']),\n            \'target\': np.asarray(self.data[idx][\'target\']),\n            \'length\': self.data[idx][\'length\']\n        }\n\n    @property\n    def vocab_size(self):\n        return len(self.w2i)\n\n    @property\n    def pad_idx(self):\n        return self.w2i[\'<pad>\']\n\n    @property\n    def sos_idx(self):\n        return self.w2i[\'<sos>\']\n\n    @property\n    def eos_idx(self):\n        return self.w2i[\'<eos>\']\n\n    @property\n    def unk_idx(self):\n        return self.w2i[\'<unk>\']\n\n    def get_w2i(self):\n        return self.w2i\n\n    def get_i2w(self):\n        return self.i2w\n\n\n    def _load_data(self, vocab=True):\n\n        with open(os.path.join(self.data_dir, self.data_file), \'r\') as file:\n            self.data = json.load(file)\n        if vocab:\n            with open(os.path.join(self.data_dir, self.vocab_file), \'r\') as file:\n                vocab = json.load(file)\n            self.w2i, self.i2w = vocab[\'w2i\'], vocab[\'i2w\']\n\n    def _load_vocab(self):\n        with open(os.path.join(self.data_dir, self.vocab_file), \'r\') as vocab_file:\n            vocab = json.load(vocab_file)\n\n        self.w2i, self.i2w = vocab[\'w2i\'], vocab[\'i2w\']\n\n    def _create_data(self):\n\n        if self.split == \'train\':\n            self._create_vocab()\n        else:\n            self._load_vocab()\n\n        tokenizer = TweetTokenizer(preserve_case=False)\n\n        data = defaultdict(dict)\n        with open(self.raw_data_path, \'r\') as file:\n\n            for i, line in enumerate(file):\n\n                words = tokenizer.tokenize(line)\n\n                input = [\'<sos>\'] + words\n                input = input[:self.max_sequence_length]\n\n                target = words[:self.max_sequence_length-1]\n                target = target + [\'<eos>\']\n\n                assert len(input) == len(target), ""%i, %i""%(len(input), len(target))\n                length = len(input)\n\n                input.extend([\'<pad>\'] * (self.max_sequence_length-length))\n                target.extend([\'<pad>\'] * (self.max_sequence_length-length))\n\n                input = [self.w2i.get(w, self.w2i[\'<unk>\']) for w in input]\n                target = [self.w2i.get(w, self.w2i[\'<unk>\']) for w in target]\n\n                id = len(data)\n                data[id][\'input\'] = input\n                data[id][\'target\'] = target\n                data[id][\'length\'] = length\n\n        with io.open(os.path.join(self.data_dir, self.data_file), \'wb\') as data_file:\n            data = json.dumps(data, ensure_ascii=False)\n            data_file.write(data.encode(\'utf8\', \'replace\'))\n\n        self._load_data(vocab=False)\n\n    def _create_vocab(self):\n\n        assert self.split == \'train\', ""Vocablurary can only be created for training file.""\n\n        tokenizer = TweetTokenizer(preserve_case=False)\n\n        w2c = OrderedCounter()\n        w2i = dict()\n        i2w = dict()\n\n        special_tokens = [\'<pad>\', \'<unk>\', \'<sos>\', \'<eos>\']\n        for st in special_tokens:\n            i2w[len(w2i)] = st\n            w2i[st] = len(w2i)\n\n        with open(self.raw_data_path, \'r\') as file:\n\n            for i, line in enumerate(file):\n                words = tokenizer.tokenize(line)\n                w2c.update(words)\n\n            for w, c in w2c.items():\n                if c > self.min_occ and w not in special_tokens:\n                    i2w[len(w2i)] = w\n                    w2i[w] = len(w2i)\n\n        assert len(w2i) == len(i2w)\n\n        print(""Vocablurary of %i keys created."" %len(w2i))\n\n        vocab = dict(w2i=w2i, i2w=i2w)\n        with io.open(os.path.join(self.data_dir, self.vocab_file), \'wb\') as vocab_file:\n            data = json.dumps(vocab, ensure_ascii=False)\n            vocab_file.write(data.encode(\'utf8\', \'replace\'))\n\n        self._load_vocab()\n'"
train.py,14,"b'import os\nimport json\nimport time\nimport torch\nimport argparse\nimport numpy as np\nfrom multiprocessing import cpu_count\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom collections import OrderedDict, defaultdict\n\nfrom ptb import PTB\nfrom utils import to_var, idx2word, expierment_name\nfrom model import SentenceVAE\n\ndef main(args):\n\n    ts = time.strftime(\'%Y-%b-%d-%H:%M:%S\', time.gmtime())\n\n    splits = [\'train\', \'valid\'] + ([\'test\'] if args.test else [])\n\n    datasets = OrderedDict()\n    for split in splits:\n        datasets[split] = PTB(\n            data_dir=args.data_dir,\n            split=split,\n            create_data=args.create_data,\n            max_sequence_length=args.max_sequence_length,\n            min_occ=args.min_occ\n        )\n\n    model = SentenceVAE(\n        vocab_size=datasets[\'train\'].vocab_size,\n        sos_idx=datasets[\'train\'].sos_idx,\n        eos_idx=datasets[\'train\'].eos_idx,\n        pad_idx=datasets[\'train\'].pad_idx,\n        unk_idx=datasets[\'train\'].unk_idx,\n        max_sequence_length=args.max_sequence_length,\n        embedding_size=args.embedding_size,\n        rnn_type=args.rnn_type,\n        hidden_size=args.hidden_size,\n        word_dropout=args.word_dropout,\n        embedding_dropout=args.embedding_dropout,\n        latent_size=args.latent_size,\n        num_layers=args.num_layers,\n        bidirectional=args.bidirectional\n        )\n\n    if torch.cuda.is_available():\n        model = model.cuda()\n\n    print(model)\n\n    if args.tensorboard_logging:\n        writer = SummaryWriter(os.path.join(args.logdir, expierment_name(args,ts)))\n        writer.add_text(""model"", str(model))\n        writer.add_text(""args"", str(args))\n        writer.add_text(""ts"", ts)\n\n    save_model_path = os.path.join(args.save_model_path, ts)\n    os.makedirs(save_model_path)\n\n    def kl_anneal_function(anneal_function, step, k, x0):\n        if anneal_function == \'logistic\':\n            return float(1/(1+np.exp(-k*(step-x0))))\n        elif anneal_function == \'linear\':\n            return min(1, step/x0)\n\n    NLL = torch.nn.NLLLoss(size_average=False, ignore_index=datasets[\'train\'].pad_idx)\n    def loss_fn(logp, target, length, mean, logv, anneal_function, step, k, x0):\n\n        # cut-off unnecessary padding from target, and flatten\n        target = target[:, :torch.max(length).data[0]].contiguous().view(-1)\n        logp = logp.view(-1, logp.size(2))\n        \n        # Negative Log Likelihood\n        NLL_loss = NLL(logp, target)\n\n        # KL Divergence\n        KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n        KL_weight = kl_anneal_function(anneal_function, step, k, x0)\n\n        return NLL_loss, KL_loss, KL_weight\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n\n    tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n    step = 0\n    for epoch in range(args.epochs):\n\n        for split in splits:\n\n            data_loader = DataLoader(\n                dataset=datasets[split],\n                batch_size=args.batch_size,\n                shuffle=split==\'train\',\n                num_workers=cpu_count(),\n                pin_memory=torch.cuda.is_available()\n            )\n\n            tracker = defaultdict(tensor)\n\n            # Enable/Disable Dropout\n            if split == \'train\':\n                model.train()\n            else:\n                model.eval()\n\n            for iteration, batch in enumerate(data_loader):\n\n                batch_size = batch[\'input\'].size(0)\n\n                for k, v in batch.items():\n                    if torch.is_tensor(v):\n                        batch[k] = to_var(v)\n\n                # Forward pass\n                logp, mean, logv, z = model(batch[\'input\'], batch[\'length\'])\n\n                # loss calculation\n                NLL_loss, KL_loss, KL_weight = loss_fn(logp, batch[\'target\'],\n                    batch[\'length\'], mean, logv, args.anneal_function, step, args.k, args.x0)\n\n                loss = (NLL_loss + KL_weight * KL_loss)/batch_size\n\n                # backward + optimization\n                if split == \'train\':\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    step += 1\n\n\n                # bookkeepeing\n                tracker[\'ELBO\'] = torch.cat((tracker[\'ELBO\'], loss.data))\n\n                if args.tensorboard_logging:\n                    writer.add_scalar(""%s/ELBO""%split.upper(), loss.data[0], epoch*len(data_loader) + iteration)\n                    writer.add_scalar(""%s/NLL Loss""%split.upper(), NLL_loss.data[0]/batch_size, epoch*len(data_loader) + iteration)\n                    writer.add_scalar(""%s/KL Loss""%split.upper(), KL_loss.data[0]/batch_size, epoch*len(data_loader) + iteration)\n                    writer.add_scalar(""%s/KL Weight""%split.upper(), KL_weight, epoch*len(data_loader) + iteration)\n\n                if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n                    print(""%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f""\n                        %(split.upper(), iteration, len(data_loader)-1, loss.data[0], NLL_loss.data[0]/batch_size, KL_loss.data[0]/batch_size, KL_weight))\n\n                if split == \'valid\':\n                    if \'target_sents\' not in tracker:\n                        tracker[\'target_sents\'] = list()\n                    tracker[\'target_sents\'] += idx2word(batch[\'target\'].data, i2w=datasets[\'train\'].get_i2w(), pad_idx=datasets[\'train\'].pad_idx)\n                    tracker[\'z\'] = torch.cat((tracker[\'z\'], z.data), dim=0)\n\n            print(""%s Epoch %02d/%i, Mean ELBO %9.4f""%(split.upper(), epoch, args.epochs, torch.mean(tracker[\'ELBO\'])))\n\n            if args.tensorboard_logging:\n                writer.add_scalar(""%s-Epoch/ELBO""%split.upper(), torch.mean(tracker[\'ELBO\']), epoch)\n\n            # save a dump of all sentences and the encoded latent space\n            if split == \'valid\':\n                dump = {\'target_sents\':tracker[\'target_sents\'], \'z\':tracker[\'z\'].tolist()}\n                if not os.path.exists(os.path.join(\'dumps\', ts)):\n                    os.makedirs(\'dumps/\'+ts)\n                with open(os.path.join(\'dumps/\'+ts+\'/valid_E%i.json\'%epoch), \'w\') as dump_file:\n                    json.dump(dump,dump_file)\n\n            # save checkpoint\n            if split == \'train\':\n                checkpoint_path = os.path.join(save_model_path, ""E%i.pytorch""%(epoch))\n                torch.save(model.state_dict(), checkpoint_path)\n                print(""Model saved at %s""%checkpoint_path)\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--data_dir\', type=str, default=\'data\')\n    parser.add_argument(\'--create_data\', action=\'store_true\')\n    parser.add_argument(\'--max_sequence_length\', type=int, default=60)\n    parser.add_argument(\'--min_occ\', type=int, default=1)\n    parser.add_argument(\'--test\', action=\'store_true\')\n\n    parser.add_argument(\'-ep\', \'--epochs\', type=int, default=10)\n    parser.add_argument(\'-bs\', \'--batch_size\', type=int, default=32)\n    parser.add_argument(\'-lr\', \'--learning_rate\', type=float, default=0.001)\n\n    parser.add_argument(\'-eb\', \'--embedding_size\', type=int, default=300)\n    parser.add_argument(\'-rnn\', \'--rnn_type\', type=str, default=\'gru\')\n    parser.add_argument(\'-hs\', \'--hidden_size\', type=int, default=256)\n    parser.add_argument(\'-nl\', \'--num_layers\', type=int, default=1)\n    parser.add_argument(\'-bi\', \'--bidirectional\', action=\'store_true\')\n    parser.add_argument(\'-ls\', \'--latent_size\', type=int, default=16)\n    parser.add_argument(\'-wd\', \'--word_dropout\', type=float, default=0)\n    parser.add_argument(\'-ed\', \'--embedding_dropout\', type=float, default=0.5)\n\n    parser.add_argument(\'-af\', \'--anneal_function\', type=str, default=\'logistic\')\n    parser.add_argument(\'-k\', \'--k\', type=float, default=0.0025)\n    parser.add_argument(\'-x0\', \'--x0\', type=int, default=2500)\n\n    parser.add_argument(\'-v\',\'--print_every\', type=int, default=50)\n    parser.add_argument(\'-tb\',\'--tensorboard_logging\', action=\'store_true\')\n    parser.add_argument(\'-log\',\'--logdir\', type=str, default=\'logs\')\n    parser.add_argument(\'-bin\',\'--save_model_path\', type=str, default=\'bin\')\n\n    args = parser.parse_args()\n\n    args.rnn_type = args.rnn_type.lower()\n    args.anneal_function = args.anneal_function.lower()\n\n    assert args.rnn_type in [\'rnn\', \'lstm\', \'gru\']\n    assert args.anneal_function in [\'logistic\', \'linear\']\n    assert 0 <= args.word_dropout <= 1\n\n    main(args)\n'"
utils.py,2,"b'import torch\nimport numpy as np\nfrom torch.autograd import Variable\nfrom collections import defaultdict, Counter, OrderedDict\n\nclass OrderedCounter(Counter, OrderedDict):\n    \'Counter that remembers the order elements are first encountered\'\n\n    def __repr__(self):\n        return \'%s(%r)\' % (self.__class__.__name__, OrderedDict(self))\n\n    def __reduce__(self):\n        return self.__class__, (OrderedDict(self),)\n\ndef to_var(x, volatile=False):\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x, volatile=volatile)\n\n\ndef idx2word(idx, i2w, pad_idx):\n\n    sent_str = [str()]*len(idx)\n\n    for i, sent in enumerate(idx):\n\n        for word_id in sent:\n\n            if word_id == pad_idx:\n                break\n            sent_str[i] += i2w[str(word_id)] + "" ""\n\n        sent_str[i] = sent_str[i].strip()\n\n\n    return sent_str\n\n\ndef interpolate(start, end, steps):\n\n    interpolation = np.zeros((start.shape[0], steps + 2))\n\n    for dim, (s,e) in enumerate(zip(start,end)):\n        interpolation[dim] = np.linspace(s,e,steps+2)\n\n    return interpolation.T\n\ndef expierment_name(args, ts):\n\n    exp_name = str()\n    exp_name += ""BS=%i_""%args.batch_size\n    exp_name += ""LR={}_"".format(args.learning_rate)\n    exp_name += ""EB=%i_""%args.embedding_size\n    exp_name += ""%s_""%args.rnn_type.upper()\n    exp_name += ""HS=%i_""%args.hidden_size\n    exp_name += ""L=%i_""%args.num_layers\n    exp_name += ""BI=%i_""%args.bidirectional\n    exp_name += ""LS=%i_""%args.latent_size\n    exp_name += ""WD={}_"".format(args.word_dropout)\n    exp_name += ""ANN=%s_""%args.anneal_function.upper()\n    exp_name += ""K={}_"".format(args.k)\n    exp_name += ""X0=%i_""%args.x0\n    exp_name += ""TS=%s""%ts\n\n    return exp_name\n'"
