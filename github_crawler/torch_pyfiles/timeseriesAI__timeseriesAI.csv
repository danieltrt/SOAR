file_path,api_count,code
setup.py,0,"b'from pkg_resources import parse_version\nfrom configparser import ConfigParser\nimport setuptools\nassert parse_version(setuptools.__version__)>=parse_version(\'36.2\')\n\n# note: all settings are in settings.ini; edit there, not here\nconfig = ConfigParser(delimiters=[\'=\'])\nconfig.read(\'settings.ini\')\ncfg = config[\'DEFAULT\']\n\ncfg_keys = \'version description keywords author author_email\'.split()\nexpected = cfg_keys + ""lib_name user branch license status min_python audience language"".split()\nfor o in expected: assert o in cfg, ""missing expected setting: {}"".format(o)\nsetup_cfg = {o:cfg[o] for o in cfg_keys}\n\nlicenses = {\n    \'apache2\': (\'Apache Software License 2.0\',\'OSI Approved :: Apache Software License\'),\n}\nstatuses = [ \'1 - Planning\', \'2 - Pre-Alpha\', \'3 - Alpha\',\n    \'4 - Beta\', \'5 - Production/Stable\', \'6 - Mature\', \'7 - Inactive\' ]\npy_versions = \'2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8\'.split()\n\nrequirements = cfg.get(\'requirements\',\'\').split()\nlic = licenses[cfg[\'license\']]\nmin_python = cfg[\'min_python\']\n\nsetuptools.setup(\n    name = cfg[\'lib_name\'],\n    license = lic[0],\n    classifiers = [\n        \'Development Status :: \' + statuses[int(cfg[\'status\'])],\n        \'Intended Audience :: \' + cfg[\'audience\'].title(),\n        \'License :: \' + lic[1],\n        \'Natural Language :: \' + cfg[\'language\'].title(),\n    ] + [\'Programming Language :: Python :: \'+o for o in py_versions[py_versions.index(min_python):]],\n    url = \'https://github.com/{}/{}\'.format(cfg[\'user\'],cfg[\'lib_name\']),\n    packages = setuptools.find_packages(),\n    include_package_data = True,\n    install_requires = requirements,\n    python_requires  = \'>=\' + cfg[\'min_python\'],\n    long_description = open(\'README.md\').read(),\n    long_description_content_type = \'text/markdown\',\n    zip_safe = False,\n    entry_points = { \'console_scripts\': cfg.get(\'console_scripts\',\'\').split() },\n    **setup_cfg)\n\n'"
nbs/__init__.py,0,b''
nbs/save_nb.py,0,"b'__all__ = [\'save_nb\', \'last_saved\']\n\nfrom IPython.display import display, Javascript\nimport os\nfrom pathlib import Path\nimport time\nfrom time import gmtime, strftime\nfrom fastai2.data.transforms import get_files\n\n# Cell\ndef save_nb():\n    display(Javascript(\'IPython.notebook.save_checkpoint()\'))\n    time.sleep(1)\n    print(\'\\nCurrent notebook saved.\\n\')\n\n# Cell\ndef last_saved(max_elapsed=10):\n    print()\n    lib_path = Path(os.getcwd()).parent\n    folder = lib_path/str(lib_path).split(\'/\')[-1]\n    current_time = time.time()\n    elapsed = 0\n    for fp in get_files(folder):\n        fp = str(fp)\n        fn = fp.split(\'/\')[-1]\n        if not fn.endswith("".py"") or fn.startswith(""_"") or fn.startswith(""."") or fn in [\'imports.py\', \'all.py\']: continue\n        elapsed_time = current_time - os.path.getmtime(fp)\n        print(f""{fn:30} saved {elapsed_time:10.0f} s ago"")\n        elapsed += elapsed_time\n    if elapsed < max_elapsed: print(\'Correct conversion!\')\n    print(f\'Total elapsed time {elapsed:.0f} s\')\n    print(strftime(""%d-%m-%Y %H:%M:%S"", gmtime()))'"
tsai/__init__.py,0,"b'__version__ = ""0.1.0""'"
tsai/_nbdev.py,2,"b'# AUTOGENERATED BY NBDEV! DO NOT EDIT!\n\n__all__ = [""index"", ""modules"", ""custom_doc_links"", ""git_url""]\n\nindex = {""totensor"": ""000_utils.ipynb"",\n         ""toarray"": ""000_utils.ipynb"",\n         ""to3dtensor"": ""000_utils.ipynb"",\n         ""to2dtensor"": ""000_utils.ipynb"",\n         ""to1dtensor"": ""000_utils.ipynb"",\n         ""to3darray"": ""000_utils.ipynb"",\n         ""to2darray"": ""000_utils.ipynb"",\n         ""to1darray"": ""000_utils.ipynb"",\n         ""to3d"": ""000_utils.ipynb"",\n         ""to2d"": ""000_utils.ipynb"",\n         ""to1d"": ""000_utils.ipynb"",\n         ""to2dPlus"": ""000_utils.ipynb"",\n         ""to3dPlus"": ""000_utils.ipynb"",\n         ""to2dPlusTensor"": ""000_utils.ipynb"",\n         ""to2dPlusArray"": ""000_utils.ipynb"",\n         ""to3dPlusTensor"": ""000_utils.ipynb"",\n         ""to3dPlusArray"": ""000_utils.ipynb"",\n         ""Todtype"": ""000_utils.ipynb"",\n         ""bytes2size"": ""000_utils.ipynb"",\n         ""bytes2GB"": ""000_utils.ipynb"",\n         ""delete_all_in_dir"": ""000_utils.ipynb"",\n         ""reverse_dict"": ""000_utils.ipynb"",\n         ""is_tuple"": ""000_utils.ipynb"",\n         ""itemify"": ""000_utils.ipynb"",\n         ""is_none"": ""000_utils.ipynb"",\n         ""ifisnone"": ""000_utils.ipynb"",\n         ""ifnoneelse"": ""000_utils.ipynb"",\n         ""ifisnoneelse"": ""000_utils.ipynb"",\n         ""ifelse"": ""000_utils.ipynb"",\n         ""is_not_close"": ""000_utils.ipynb"",\n         ""test_not_close"": ""000_utils.ipynb"",\n         ""test_type"": ""000_utils.ipynb"",\n         ""stack"": ""000_utils.ipynb"",\n         ""cat2int"": ""000_utils.ipynb"",\n         ""cycle_dl"": ""000_utils.ipynb"",\n         ""memmap2cache"": ""000_utils.ipynb"",\n         ""package_versions"": ""000_utils.ipynb"",\n         ""hardware_details"": ""000_utils.ipynb"",\n         ""TrainValTestSplitter"": ""000b_data.validation.ipynb"",\n         ""get_splits"": ""000b_data.validation.ipynb"",\n         ""check_overlap"": ""000b_data.validation.ipynb"",\n         ""leakage_finder"": ""000b_data.validation.ipynb"",\n         ""get_predefined_splits"": ""000b_data.validation.ipynb"",\n         ""combine_split_data"": ""000b_data.validation.ipynb"",\n         ""decompress_from_url"": ""001_data.external.ipynb"",\n         ""get_UCR_univariate_list"": ""001_data.external.ipynb"",\n         ""get_UCR_multivariate_list"": ""001_data.external.ipynb"",\n         ""stack_padding"": ""001_data.external.ipynb"",\n         ""get_UCR_data"": ""001_data.external.ipynb"",\n         ""NumpyTensor"": ""002_data.core.ipynb"",\n         ""ToNumpyTensor"": ""002_data.core.ipynb"",\n         ""TSTensor"": ""002_data.core.ipynb"",\n         ""ToTSTensor"": ""002_data.core.ipynb"",\n         ""NumpyTensorBlock"": ""002_data.core.ipynb"",\n         ""TSTensorBlock"": ""002_data.core.ipynb"",\n         ""TorchDataset"": ""002_data.core.ipynb"",\n         ""NumpyDataset"": ""002_data.core.ipynb"",\n         ""TSDataset"": ""002_data.core.ipynb"",\n         ""NumpyDatasets"": ""002_data.core.ipynb"",\n         ""TSDatasets"": ""002_data.core.ipynb"",\n         ""add_ds"": ""002_data.core.ipynb"",\n         ""NumpyDatasets.add_test"": ""002_data.core.ipynb"",\n         ""NumpyDatasets.add_unlabeled"": ""002_data.core.ipynb"",\n         ""NumpyDataLoader"": ""002_data.core.ipynb"",\n         ""show_tuple"": ""002_data.core.ipynb"",\n         ""TSDataLoader"": ""002_data.core.ipynb"",\n         ""NumpyDataLoaders"": ""002_data.core.ipynb"",\n         ""TSDataLoaders"": ""002_data.core.ipynb"",\n         ""DataLoader.cws"": ""002_data.core.ipynb"",\n         ""TSStandardize"": ""003_data.transforms.ipynb"",\n         ""torch.Tensor.mul_min"": ""003_data.transforms.ipynb"",\n         ""TSTensor.mul_min"": ""003_data.transforms.ipynb"",\n         ""NumpyTensor.mul_min"": ""003_data.transforms.ipynb"",\n         ""torch.Tensor.mul_max"": ""003_data.transforms.ipynb"",\n         ""TSTensor.mul_max"": ""003_data.transforms.ipynb"",\n         ""NumpyTensor.mul_max"": ""003_data.transforms.ipynb"",\n         ""TSNormalize"": ""003_data.transforms.ipynb"",\n         ""TSIdentity"": ""003_data.transforms.ipynb"",\n         ""TSShuffle_HLs"": ""003_data.transforms.ipynb"",\n         ""TSMagNoise"": ""003_data.transforms.ipynb"",\n         ""TSMagMulNoise"": ""003_data.transforms.ipynb"",\n         ""random_curve_generator"": ""003_data.transforms.ipynb"",\n         ""random_cum_curve_generator"": ""003_data.transforms.ipynb"",\n         ""random_cum_noise_generator"": ""003_data.transforms.ipynb"",\n         ""TSTimeNoise"": ""003_data.transforms.ipynb"",\n         ""TSMagWarp"": ""003_data.transforms.ipynb"",\n         ""TSTimeWarp"": ""003_data.transforms.ipynb"",\n         ""TSMagScale"": ""003_data.transforms.ipynb"",\n         ""TSMagScaleVar"": ""003_data.transforms.ipynb"",\n         ""TSZoomIn"": ""003_data.transforms.ipynb"",\n         ""TSZoomOut"": ""003_data.transforms.ipynb"",\n         ""TSScale"": ""003_data.transforms.ipynb"",\n         ""TSRandomTimeStep"": ""003_data.transforms.ipynb"",\n         ""TSBlur"": ""003_data.transforms.ipynb"",\n         ""TSSmooth"": ""003_data.transforms.ipynb"",\n         ""maddest"": ""003_data.transforms.ipynb"",\n         ""TSDenoise"": ""003_data.transforms.ipynb"",\n         ""TSRandomNoise"": ""003_data.transforms.ipynb"",\n         ""TSLookBack"": ""003_data.transforms.ipynb"",\n         ""TSVarOut"": ""003_data.transforms.ipynb"",\n         ""TSCutOut"": ""003_data.transforms.ipynb"",\n         ""TSTimeStepOut"": ""003_data.transforms.ipynb"",\n         ""TSCrop"": ""003_data.transforms.ipynb"",\n         ""TSRandomCrop"": ""003_data.transforms.ipynb"",\n         ""TSRandomResizedCrop"": ""003_data.transforms.ipynb"",\n         ""TSCenterCrop"": ""003_data.transforms.ipynb"",\n         ""TSMaskOut"": ""003_data.transforms.ipynb"",\n         ""TSTranslateX"": ""003_data.transforms.ipynb"",\n         ""TSFlip"": ""003_data.transforms.ipynb"",\n         ""TSRandomFlip"": ""003_data.transforms.ipynb"",\n         ""TSShift"": ""003_data.transforms.ipynb"",\n         ""TSRandomRotate"": ""003_data.transforms.ipynb"",\n         ""TSNeg"": ""003_data.transforms.ipynb"",\n         ""TSRandomNeg"": ""003_data.transforms.ipynb"",\n         ""TSFreqNoise"": ""003_data.transforms.ipynb"",\n         ""TSFreqWarp"": ""003_data.transforms.ipynb"",\n         ""TSFreqScale"": ""003_data.transforms.ipynb"",\n         ""TabularDataset"": ""005_data.tabular.ipynb"",\n         ""TabularDataLoader"": ""005_data.tabular.ipynb"",\n         ""MatthewsCorrCoef"": ""007_metrics.ipynb"",\n         ""Learner.save_all"": ""008_learner.ipynb"",\n         ""load_learner_all"": ""008_learner.ipynb"",\n         ""Recorder.plot_metrics"": ""008_learner.ipynb"",\n         ""LookAhead"": ""009_optimizer.ipynb"",\n         ""Ralamb"": ""009_optimizer.ipynb"",\n         ""ralamb"": ""009_optimizer.ipynb"",\n         ""RangerLars"": ""009_optimizer.ipynb"",\n         ""rangerlars"": ""009_optimizer.ipynb"",\n         ""Ranger"": ""009_optimizer.ipynb"",\n         ""rangergc"": ""009_optimizer.ipynb"",\n         ""generate_kernels"": ""010_rocket_functions.ipynb"",\n         ""apply_kernel"": ""010_rocket_functions.ipynb"",\n         ""apply_kernels"": ""010_rocket_functions.ipynb"",\n         ""ROCKET"": ""010_rocket_functions.ipynb"",\n         ""noop"": ""100_layers.ipynb"",\n         ""mish"": ""100_layers.ipynb"",\n         ""Mish"": ""100_layers.ipynb"",\n         ""get_act_layer"": ""100_layers.ipynb"",\n         ""same_padding1d"": ""100_layers.ipynb"",\n         ""Pad1d"": ""100_layers.ipynb"",\n         ""Conv1dSame"": ""100_layers.ipynb"",\n         ""Chomp1d"": ""100_layers.ipynb"",\n         ""Conv1dCausal"": ""100_layers.ipynb"",\n         ""Conv1d"": ""100_layers.ipynb"",\n         ""CoordConv1D"": ""100_layers.ipynb"",\n         ""LambdaPlus"": ""100_layers.ipynb"",\n         ""Flatten"": ""100_layers.ipynb"",\n         ""Squeeze"": ""100_layers.ipynb"",\n         ""Unsqueeze"": ""100_layers.ipynb"",\n         ""YRange"": ""100_layers.ipynb"",\n         ""Temp"": ""100_layers.ipynb"",\n         ""get_layers"": ""100b_models_utils.ipynb"",\n         ""count_params"": ""100b_models_utils.ipynb"",\n         ""ResBlock"": ""101_ResNet.ipynb"",\n         ""ResNet"": ""101_ResNet.ipynb"",\n         ""shortcut"": ""102_InceptionTime.ipynb"",\n         ""Inception"": ""102_InceptionTime.ipynb"",\n         ""InceptionBlock"": ""102_InceptionTime.ipynb"",\n         ""InceptionTime"": ""102_InceptionTime.ipynb"",\n         ""FCN"": ""103_FCN.ipynb"",\n         ""Block"": ""104_ResCNN.ipynb"",\n         ""ResCNN"": ""104_ResCNN.ipynb""}\n\nmodules = [""utils.py"",\n           ""data/validation.py"",\n           ""data/external.py"",\n           ""data/core.py"",\n           ""data/transforms.py"",\n           ""data/tabular.py"",\n           ""metrics.py"",\n           ""learner.py"",\n           ""optimizer.py"",\n           ""rocket_functions.py"",\n           ""models/layers.py"",\n           ""models/utils.py"",\n           ""models/ResNet.py"",\n           ""models/InceptionTime.py"",\n           ""models/FCN.py"",\n           ""models/ResCNN.py""]\n\ndoc_url = ""https://timeseriesai.github.io/timeseriesAI//""\n\ngit_url = ""https://github.com/timeseriesAI/timeseriesAI/tree/master/""\n\ndef custom_doc_links(name): return None\n'"
tsai/all.py,0,b'from .imports import *\nfrom .utils import *\nfrom .data.all import *\nfrom .metrics import *\nfrom .learner import *\nfrom .optimizer import *\nfrom .rocket_functions import *\nfrom .models.all import *'
tsai/imports.py,2,"b'import fastai2\nfrom fastai2.imports import *\nfrom fastai2.data.all import *\nfrom fastai2.torch_core import *\nfrom fastai2.learner import *\nfrom fastai2.metrics import *\nfrom fastai2.callback.all import *\nfrom fastai2.vision.data import *\nfrom fastai2.interpret import *\nfrom fastai2.optimizer import *\nfrom fastai2.torch_core import Module\nfrom fastai2.data.transforms import get_files\nimport fastcore\nfrom fastcore.test import *\nfrom fastcore.utils import *\nimport torch\nimport torch.nn as nn\nimport psutil\nimport scipy as sp\nimport sklearn.metrics as skm\nimport gc\nimport os\nfrom numbers import Integral\nfrom pathlib import Path\nimport time\nfrom time import gmtime, strftime\nfrom IPython.display import Audio, display, HTML, Javascript\nimport tsai\n\nPATH = Path(os.getcwd())\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\ncpus = defaults.cpus\n\ndef save_nb(verbose=False):\n    display(Javascript(\'IPython.notebook.save_checkpoint()\'))\n    time.sleep(1)\n    pv(\'\\nCurrent notebook saved.\\n\', verbose)\n\ndef last_saved(max_elapsed=10):\n    print(\'\\n\')\n    lib_path = Path(os.getcwd()).parent\n    folder = lib_path/\'tsai\'\n    print(\'Checking folder:\', folder)\n    counter = 0\n    elapsed = 0\n    current_time = time.time()\n    for fp in get_files(folder):\n        fp = str(fp)\n        fn = fp.split(\'/\')[-1]\n        if not fn.endswith("".py"") or fn.startswith(""_"") or fn.startswith(""."") or fn in [\'imports.py\', \'all.py\']: continue\n        elapsed_time = current_time - os.path.getmtime(fp)\n        if elapsed_time > max_elapsed: \n            print(f""{fn:30} saved {elapsed_time:10.0f} s ago ***"")\n            counter += 1\n        elapsed += elapsed_time\n    if counter == 0: \n        print(\'Correct conversion! \xf0\x9f\x98\x83\')\n        output = 1\n    else: \n        print(\'Incorrect conversion! \xf0\x9f\x98\x94\')\n        output = 0\n    print(f\'Total elapsed time {elapsed:.0f} s\')\n    print(strftime(""%a, %d %b %Y %H:%M:%S %Z\\n""),""\\n"")\n    return output\n    \ndef beep(inp=1):\n    mult = 1.6*inp if inp else .08\n    wave = np.sin(mult*np.arange(1000))\n    return Audio(wave, rate=10000, autoplay=True)\n\ndef create_scripts(max_elapsed=10):\n    from nbdev.export import notebook2script\n    save_nb()\n    notebook2script()\n    return last_saved(max_elapsed)\n    '"
tsai/learner.py,2,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/008_learner.ipynb (unless otherwise specified).\n\n__all__ = [\'load_learner_all\']\n\n# Cell\nfrom fastai2.learner import *\nfrom .imports import *\n\n# Cell\n@patch\ndef save_all(self:Learner, path=\'export\', dls_fname=\'dls\', model_fname=\'model\', learner_fname=\'learner\'):\n\n    path = Path(path)\n    if not os.path.exists(path): os.makedirs(path)\n\n    # Save the dls\n    torch.save(self.dls, path/f\'{dls_fname}.pth\')\n\n    # Saves the model along with optimizer\n    self.model_dir = path\n    self.save(model_fname)\n\n    # Export learn without the items and the optimizer state for inference\n    self.export(path/f\'{learner_fname}.pkl\')\n\n    print(f\'Learner saved:\')\n    print(f""path          = \'{path}\'"")\n    print(f""dls_fname     = \'{dls_fname}\'"")\n    print(f""model_fname   = \'{model_fname}.pth\'"")\n    print(f""learner_fname = \'{learner_fname}.pkl\'"")\n\n\ndef load_learner_all(path=\'export\', dls_fname=\'dls\', model_fname=\'model\', learner_fname=\'learner\', cpu=True):\n    path = Path(path)\n    learn = load_learner(path/f\'{learner_fname}.pkl\', cpu=cpu)\n    learn.load(f\'{model_fname}\')\n    dls = torch.load(path/f\'{dls_fname}.pth\')\n    learn.dls = dls\n    return learn\n\n# Cell\n@patch\n@delegates(subplots)\ndef plot_metrics(self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs):\n    metrics = np.stack(self.values)\n    names = self.metric_names[1:-1]\n    n = len(names) - 1\n    if nrows is None and ncols is None:\n        nrows = int(math.sqrt(n))\n        ncols = int(np.ceil(n / nrows))\n    elif nrows is None: nrows = int(np.ceil(n / ncols))\n    elif ncols is None: ncols = int(np.ceil(n / nrows))\n    figsize = figsize or (ncols * 6, nrows * 4)\n    fig, axs = subplots(nrows, ncols, figsize=figsize, **kwargs)\n    axs = [ax if i < n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n]\n    for i, (name, ax) in enumerate(zip(names, [axs[0]] + axs)):\n        ax.plot(metrics[:, i], color=\'#1f77b4\' if i == 0 else \'#ff7f0e\', label=\'valid\' if i > 0 else \'train\')\n        ax.set_title(name if i > 1 else \'losses\')\n        ax.legend(loc=\'best\')\n    plt.show()'"
tsai/metrics.py,0,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/007_metrics.ipynb (unless otherwise specified).\n\n__all__ = [\'MatthewsCorrCoef\']\n\n# Cell\nfrom .imports import *\nfrom fastai2.metrics import *\n\n# Cell\ndef MatthewsCorrCoef(axis=-1, sample_weight=None):\n    ""Matthews correlation coefficient for binary classification problems""\n    return skm_to_fastai(skm.matthews_corrcoef, axis=axis, sample_weight=sample_weight)'"
tsai/optimizer.py,11,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/009_optimizer.ipynb (unless otherwise specified).\n\n__all__ = [\'LookAhead\', \'Ralamb\', \'ralamb\', \'RangerLars\', \'rangerlars\', \'Ranger\', \'rangergc\']\n\n# Cell\nfrom .imports import *\n\n# Cell\nfrom collections import defaultdict\n\n# Cell\n"""""" Lookahead Optimizer Wrapper.\nImplemented by Mikhail Grankin (Github: mgrankin) in his excellent collection of Pytorch optimizers https://github.com/mgrankin/over9000\nImplementation modified from: https://github.com/alphadl/lookahead.pytorch\nPaper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n""""""\nclass LookAhead(torch.optim.Optimizer):\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0: raise ValueError(f\'Invalid slow update rate: {alpha}\')\n        if not 1 <= k: raise ValueError(f\'Invalid lookahead steps: {k}\')\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults = base_optimizer.defaults\n        self.defaults.update(defaults)\n        self.state = defaultdict(dict)\n        # manually add our defaults to the param groups\n        for name, default in defaults.items():\n            for group in self.param_groups: group.setdefault(name, default)\n\n    def update_slow(self, group):\n        for fast_p in group[""params""]:\n            if fast_p.grad is None: continue\n            param_state = self.state[fast_p]\n            if \'slow_buffer\' not in param_state:\n                param_state[\'slow_buffer\'] = torch.empty_like(fast_p.data)\n                param_state[\'slow_buffer\'].copy_(fast_p.data)\n            slow = param_state[\'slow_buffer\']\n            slow.add_(group[\'lookahead_alpha\'], fast_p.data - slow)\n            fast_p.data.copy_(slow)\n\n    def sync_lookahead(self):\n        for group in self.param_groups:\n            self.update_slow(group)\n\n    def step(self, closure=None):\n        loss = self.base_optimizer.step(closure)\n        for group in self.param_groups:\n            group[\'lookahead_step\'] += 1\n            if group[\'lookahead_step\'] % group[\'lookahead_k\'] == 0: self.update_slow(group)\n        return loss\n\n    def state_dict(self):\n        fast_state_dict = self.base_optimizer.state_dict()\n        slow_state = {(id(k) if isinstance(k, torch.Tensor) else k): v for k, v in self.state.items()}\n        fast_state = fast_state_dict[\'state\']\n        param_groups = fast_state_dict[\'param_groups\']\n        return {\'state\': fast_state, \'slow_state\': slow_state, \'param_groups\': param_groups}\n\n    def load_state_dict(self, state_dict):\n        fast_state_dict = {\'state\': state_dict[\'state\'], \'param_groups\': state_dict[\'param_groups\']}\n        self.base_optimizer.load_state_dict(fast_state_dict)\n\n        # We want to restore the slow state, but share param_groups reference\n        # with base_optimizer. This is a bit redundant but least code\n        slow_state_new = False\n        if \'slow_state\' not in state_dict:\n            print(\'Loading state_dict from optimizer without Lookahead applied.\')\n            state_dict[\'slow_state\'] = defaultdict(dict)\n            slow_state_new = True\n        slow_state_dict = {\n            \'state\': state_dict[\'slow_state\'],\n            \'param_groups\': state_dict[\'param_groups\'],  # this is pointless but saves code\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n        if slow_state_new:\n            # reapply defaults to catch missing lookahead specific ones\n            for name, default in self.defaults.items():\n                for group in self.param_groups:\n                    group.setdefault(name, default)\n\n# Cell\n\'\'\'Implemented by Mikhail Grankin (Github: mgrankin) in his excellent collection of Pytorch optimizers https://github.com/mgrankin/over9000\'\'\'\n\n# RAdam + LARS\nclass Ralamb(torch.optim.Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(Ralamb, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(Ralamb, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None: loss = closure()\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None: continue\n                grad = p.grad.data.float()\n                if grad.is_sparse: raise RuntimeError(\'Ralamb does not support sparse gradients\')\n                p_data_fp32 = p.data.float()\n                state = self.state[p]\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                state[\'step\'] += 1\n                buffered = self.buffer[int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]: N_sma, radam_step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2**state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    # more conservative since it\'s an approximated value\n                    if N_sma >= 5:\n                        radam_step_size = math.sqrt(\n                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) *\n                            (N_sma - 2) / N_sma * N_sma_max /\n                            (N_sma_max - 2)) / (1 - beta1**state[\'step\'])\n                    else:\n                        radam_step_size = 1.0 / (1 - beta1**state[\'step\'])\n                    buffered[2] = radam_step_size\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n                # more conservative since it\'s an approximated value\n                radam_step = p_data_fp32.clone()\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    radam_step.addcdiv_(-radam_step_size * group[\'lr\'], exp_avg, denom)\n                else:\n                    radam_step.add_(-radam_step_size * group[\'lr\'], exp_avg)\n                radam_norm = radam_step.pow(2).sum().sqrt()\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n                if weight_norm == 0 or radam_norm == 0: trust_ratio = 1\n                else: trust_ratio = weight_norm / radam_norm\n                state[\'weight_norm\'] = weight_norm\n                state[\'adam_norm\'] = radam_norm\n                state[\'trust_ratio\'] = trust_ratio\n                if N_sma >= 5:\n                    p_data_fp32.addcdiv_(-radam_step_size * group[\'lr\'] * trust_ratio, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-radam_step_size * group[\'lr\'] * trust_ratio, exp_avg)\n                p.data.copy_(p_data_fp32)\n        return loss\n\n# Cell\ndef ralamb(*args, **kwargs):\n    return OptimWrapper(Ralamb(*args, **kwargs))\n\ndef RangerLars(params, alpha=0.5, k=6, *args, **kwargs):\n    ralamb = Ralamb(params, *args, **kwargs)\n    return LookAhead(ralamb, alpha, k)\n\ndef rangerlars(*args, **kwargs):\n    return OptimWrapper(RangerLars(*args, **kwargs))\n\n# Cell\n\'\'\'Ranger was developed by Less Wright (Github: lessw2020) and shared in his excellent repo https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\'\'\'\n\n# Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\n\n# https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n# and/or\n# https://github.com/lessw2020/Best-Deep-Learning-Optimizers\n\n# Ranger has now been used to capture 12 records on the FastAI leaderboard.\n\n# This version = 20.4.11\n\n# Credits:\n# Gradient Centralization --> https://arxiv.org/abs/2004.01461v2 (a new optimization technique for DNNs), github:  https://github.com/Yonghongwei/Gradient-Centralization\n# RAdam -->  https://github.com/LiyuanLucasLiu/RAdam\n# Lookahead --> rewritten by lessw2020, but big thanks to Github @LonePatient and @RWightman for ideas from their code.\n# Lookahead paper --> MZhang,G Hinton  https://arxiv.org/abs/1907.08610\n\n# summary of changes:\n# 4/11/20 - add gradient centralization option.  Set new testing benchmark for accuracy with it, toggle with use_gc flag at init.\n# full code integration with all updates at param level instead of group, moves slow weights into state dict (from generic weights),\n# supports group learning rates (thanks @SHolderbach), fixes sporadic load from saved model issues.\n# changes 8/31/19 - fix references to *self*.N_sma_threshold;\n# changed eps to 1e-5 as better default than 1e-8.\n\nclass Ranger(torch.optim.Optimizer):\n    def __init__(self, params, lr=1e-3,                       # lr\n                alpha=0.5, k=6, N_sma_threshhold=5,           # Ranger options\n                betas=(.95,0.999), eps=1e-5, weight_decay=0,  # Adam options\n                use_gc=True, gc_conv_only=False               # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n                ):\n\n        #parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\'Invalid slow update rate: {alpha}\')\n        if not 1 <= k:\n            raise ValueError(f\'Invalid lookahead steps: {k}\')\n        if not lr > 0:\n            raise ValueError(f\'Invalid Learning Rate: {lr}\')\n        if not eps > 0:\n            raise ValueError(f\'Invalid eps: {eps}\')\n\n        #parameter comments:\n        # beta1 (momentum) of .95 seems to work better than .90...\n        #N_sma_threshold of 5 seems better in testing than 4.\n        #In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n\n        #prep defaults and init torch.optim base\n        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas, N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n        super().__init__(params,defaults)\n\n        #adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n\n        #look ahead params\n\n        self.alpha = alpha\n        self.k = k\n\n        #radam buffer for state\n        self.radam_buffer = [[None,None,None] for ind in range(10)]\n\n        #gc on or off\n        self.use_gc=use_gc\n\n        #level of gradient centralization\n        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n\n\n        print(f""Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}"")\n        if (self.use_gc and self.gc_gradient_threshold==1):\n            print(f""GC applied to both conv and fc layers"")\n        elif (self.use_gc and self.gc_gradient_threshold==3):\n            print(f""GC applied to conv layers only"")\n\n    def __setstate__(self, state):\n        print(""set state called"")\n        super(Ranger, self).__setstate__(state)\n\n\n    def step(self, closure=None):\n        loss = None\n        #note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.\n        #Uncomment if you need to use the actual closure...\n\n        #if closure is not None:\n            #loss = closure()\n\n        #Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n\n                if grad.is_sparse:\n                    raise RuntimeError(\'Ranger optimizer does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]  #get state dict for this param\n\n                if len(state) == 0:   #if first time to run...init dictionary with our desired entries\n                    #if self.first_run_check==0:\n                        #self.first_run_check=1\n                        #print(""Initializing slow buffer...should not see this at load from saved model!"")\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n\n                    #look ahead weight storage now in state dict\n                    state[\'slow_buffer\'] = torch.empty_like(p.data)\n                    state[\'slow_buffer\'].copy_(p.data)\n\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                #begin computations\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                #GC operation for Conv layers and FC layers\n                if grad.dim() > self.gc_gradient_threshold: grad.add_(-grad.mean(dim = tuple(range(1,grad.dim())), keepdim = True))\n\n                state[\'step\'] += 1\n\n                #compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                #compute mean moving avg\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                buffered = self.radam_buffer[int(state[\'step\'] % 10)]\n\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state[\'step\'])\n                    buffered[2] = step_size\n\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n\n                # apply lr\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(-step_size * group[\'lr\'], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group[\'lr\'], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n                #integrated look ahead...\n                #we do it at the param level instead of group level\n                if state[\'step\'] % group[\'k\'] == 0:\n                    slow_p = state[\'slow_buffer\'] #get access to slow param tensor\n                    slow_p.add_(self.alpha, p.data - slow_p)  #(fast weights - slow weights) * alpha\n                    p.data.copy_(slow_p)  #copy interpolated weights to RAdam param tensor\n\n        return loss\n\n# Cell\ndef rangergc(*args, **kwargs):\n    return OptimWrapper(Ranger(*args, **kwargs))'"
tsai/rocket_functions.py,8,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/010_rocket_functions.ipynb (unless otherwise specified).\n\n__all__ = [\'generate_kernels\', \'apply_kernel\', \'apply_kernels\', \'ROCKET\']\n\n# Cell\nfrom .imports import *\nfrom .data.external import *\n\n# Cell\nfrom sklearn.linear_model import RidgeClassifierCV\nfrom numba import njit, prange\n\n# Cell\n# Angus Dempster, Francois Petitjean, Geoff Webb\n\n# Dempster A, Petitjean F, Webb GI (2019) ROCKET: Exceptionally fast and\n# accurate time series classification using random convolutional kernels.\n# arXiv:1910.13051\n\n# changes:\n# - added kss parameter to generate_kernels\n# - convert X to np.float64\n\ndef generate_kernels(input_length, num_kernels, kss=[7, 9, 11], pad=True, dilate=True):\n    candidate_lengths = np.array((kss))\n    # initialise kernel parameters\n    weights = np.zeros((num_kernels, candidate_lengths.max())) # see note\n    lengths = np.zeros(num_kernels, dtype = np.int32) # see note\n    biases = np.zeros(num_kernels)\n    dilations = np.zeros(num_kernels, dtype = np.int32)\n    paddings = np.zeros(num_kernels, dtype = np.int32)\n    # note: only the first *lengths[i]* values of *weights[i]* are used\n    for i in range(num_kernels):\n        length = np.random.choice(candidate_lengths)\n        _weights = np.random.normal(0, 1, length)\n        bias = np.random.uniform(-1, 1)\n        if dilate: dilation = 2 ** np.random.uniform(0, np.log2((input_length - 1) // (length - 1)))\n        else: dilation = 1\n        if pad: padding = ((length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n        else: padding = 0\n        weights[i, :length] = _weights - _weights.mean()\n        lengths[i], biases[i], dilations[i], paddings[i] = length, bias, dilation, padding\n    return weights, lengths, biases, dilations, paddings\n\n@njit(fastmath = True)\ndef apply_kernel(X, weights, length, bias, dilation, padding):\n    # zero padding\n    if padding > 0:\n        _input_length = len(X)\n        _X = np.zeros(_input_length + (2 * padding))\n        _X[padding:(padding + _input_length)] = X\n        X = _X\n    input_length = len(X)\n    output_length = input_length - ((length - 1) * dilation)\n    _ppv = 0 # ""proportion of positive values""\n    _max = np.NINF\n    for i in range(output_length):\n        _sum = bias\n        for j in range(length):\n            _sum += weights[j] * X[i + (j * dilation)]\n        if _sum > 0:\n            _ppv += 1\n        if _sum > _max:\n            _max = _sum\n    return _ppv / output_length, _max\n\n@njit(parallel = True, fastmath = True)\ndef apply_kernels(X, kernels):\n    X = X.astype(np.float64)\n    weights, lengths, biases, dilations, paddings = kernels\n    num_examples = len(X)\n    num_kernels = len(weights)\n    # initialise output\n    _X = np.zeros((num_examples, num_kernels * 2)) # 2 features per kernel\n    for i in prange(num_examples):\n        for j in range(num_kernels):\n            _X[i, (j * 2):((j * 2) + 2)] = \\\n            apply_kernel(X[i], weights[j][:lengths[j]], lengths[j], biases[j], dilations[j], paddings[j])\n    return _X\n\n# Cell\nclass ROCKET(nn.Module):\n    def __init__(self, c_in, seq_len, n_kernels=10000, kss=[7, 9, 11]):\n\n        \'\'\'\n        ROCKET is a GPU Pytorch implementation of the ROCKET methods generate_kernels\n        and apply_kernels that can be used  with univariate and multivariate time series.\n        Input: is a 3d torch tensor of type torch.float32. When used with univariate TS,\n        make sure you transform the 2d to 3d by adding unsqueeze(1).\n        c_in: number of channels or features. For univariate c_in is 1.\n        seq_len: sequence length\n        \'\'\'\n        super().__init__()\n        kss = [ks for ks in kss if ks < seq_len]\n        convs = nn.ModuleList()\n        for i in range(n_kernels):\n            ks = np.random.choice(kss)\n            dilation = 2**np.random.uniform(0, np.log2((seq_len - 1) // (ks - 1)))\n            padding = int((ks - 1) * dilation // 2) if np.random.randint(2) == 1 else 0\n            weight = torch.randn(1, c_in, ks)\n            weight -= weight.mean()\n            bias = 2 * (torch.rand(1) - .5)\n            layer = nn.Conv1d(c_in, 1, ks, padding=2 * padding, dilation=int(dilation), bias=True)\n            layer.weight = torch.nn.Parameter(weight, requires_grad=False)\n            layer.bias = torch.nn.Parameter(bias, requires_grad=False)\n            convs.append(layer)\n        self.convs = convs\n        self.n_kernels = n_kernels\n        self.kss = kss\n\n    def forward(self, x):\n        for i in range(self.n_kernels):\n            out = self.convs[i](x)\n            _max = out.max(dim=-1).values\n            _ppv = torch.gt(out, 0).sum(dim=-1).float() / out.shape[-1]\n            cat = torch.cat((_max, _ppv), dim=-1)\n            output = cat if i == 0 else torch.cat((output, cat), dim=-1)\n        return output'"
tsai/utils.py,14,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/000_utils.ipynb (unless otherwise specified).\n\n__all__ = [\'totensor\', \'toarray\', \'to3dtensor\', \'to2dtensor\', \'to1dtensor\', \'to3darray\', \'to2darray\', \'to1darray\',\n           \'to3d\', \'to2d\', \'to1d\', \'to2dPlus\', \'to3dPlus\', \'to2dPlusTensor\', \'to2dPlusArray\', \'to3dPlusTensor\',\n           \'to3dPlusArray\', \'Todtype\', \'bytes2size\', \'bytes2GB\', \'delete_all_in_dir\', \'reverse_dict\', \'is_tuple\',\n           \'itemify\', \'is_none\', \'ifisnone\', \'ifnoneelse\', \'ifisnoneelse\', \'ifelse\', \'is_not_close\', \'test_not_close\',\n           \'test_type\', \'stack\', \'cat2int\', \'cycle_dl\', \'memmap2cache\', \'package_versions\', \'hardware_details\']\n\n# Cell\nfrom .imports import *\n\n# Cell\ndef totensor(o):\n    if isinstance(o, torch.Tensor): return o\n    elif isinstance(o, np.ndarray):  return torch.from_numpy(o)\n    assert False, f""Can\'t convert {type(o)} to torch.Tensor""\n\n\ndef toarray(o):\n    if isinstance(o, np.ndarray): return o\n    elif isinstance(o, torch.Tensor): return o.cpu().numpy()\n    assert False, f""Can\'t convert {type(o)} to np.array""\n\n\ndef to3dtensor(o):\n    o = totensor(o)\n    if o.ndim == 3: return o\n    elif o.ndim == 1: return o[None, None]\n    elif o.ndim == 2: return o[:, None]\n    assert False, f\'Please, review input dimensions {o.ndim}\'\n\n\ndef to2dtensor(o):\n    o = totensor(o)\n    if o.ndim == 2: return o\n    elif o.ndim == 1: return o[None]\n    elif o.ndim == 3: return o[0]\n    assert False, f\'Please, review input dimensions {o.ndim}\'\n\n\ndef to1dtensor(o):\n    o = totensor(o)\n    if o.ndim == 1: return o\n    elif o.ndim == 3: return o[0,0]\n    if o.ndim == 2: return o[0]\n    assert False, f\'Please, review input dimensions {o.ndim}\'\n\n\ndef to3darray(o):\n    o = toarray(o)\n    if o.ndim == 3: return o\n    elif o.ndim == 1: return o[None, None]\n    elif o.ndim == 2: return o[:, None]\n    assert False, f\'Please, review input dimensions {o.ndim}\'\n\n\ndef to2darray(o):\n    o = toarray(o)\n    if o.ndim == 2: return o\n    elif o.ndim == 1: return o[None]\n    elif o.ndim == 3: return o[0]\n    assert False, f\'Please, review input dimensions {o.ndim}\'\n\n\ndef to1darray(o):\n    o = toarray(o)\n    if o.ndim == 1: return o\n    elif o.ndim == 3: o = o[0,0]\n    elif o.ndim == 2: o = o[0]\n    assert False, f\'Please, review input dimensions {o.ndim}\'\n\n\ndef to3d(o):\n    if o.ndim == 3: return o\n    if isinstance(o, np.ndarray): return to3darray(o)\n    if isinstance(o, torch.Tensor): return to3dtensor(o)\n\n\ndef to2d(o):\n    if o.ndim == 2: return o\n    if isinstance(o, np.ndarray): return to2darray(o)\n    if isinstance(o, torch.Tensor): return to2dtensor(o)\n\n\ndef to1d(o):\n    if o.ndim == 1: return o\n    if isinstance(o, np.ndarray): return to1darray(o)\n    if isinstance(o, torch.Tensor): return to1dtensor(o)\n\n\ndef to2dPlus(o):\n    if o.ndim >= 2: return o\n    if isinstance(o, np.ndarray): return to2darray(o)\n    elif isinstance(o, torch.Tensor): return to2dtensor(o)\n\n\ndef to3dPlus(o):\n    if o.ndim >= 3: return o\n    if isinstance(o, np.ndarray): return to3darray(o)\n    elif isinstance(o, torch.Tensor): return to3dtensor(o)\n\n\ndef to2dPlusTensor(o):\n    return to2dPlus(totensor(o))\n\n\ndef to2dPlusArray(o):\n    return to2dPlus(toarray(o))\n\n\ndef to3dPlusTensor(o):\n    return to3dPlus(totensor(o))\n\n\ndef to3dPlusArray(o):\n    return to3dPlus(toarray(o))\n\n\ndef Todtype(dtype):\n    def _to_type(o, dtype=dtype):\n        if o.dtype == dtype: return o\n        elif isinstance(o, torch.Tensor): o = o.to(dtype=dtype)\n        elif isinstance(o, np.ndarray): o = o.astype(dtype)\n        return o\n    return _to_type\n\n# Cell\ndef bytes2size(size_bytes):\n    if size_bytes == 0: return ""0B""\n    size_name = (""B"", ""KB"", ""MB"", ""GB"", ""TB"", ""PB"", ""EB"", ""ZB"", ""YB"")\n    i = int(math.floor(math.log(size_bytes, 1024)))\n    p = math.pow(1024, i)\n    s = round(size_bytes / p, 2)\n    return ""%s %s"" % (s, size_name[i])\n\ndef bytes2GB(byts):\n    return round(byts / math.pow(1024, 3), 2)\n\n# Cell\ndef delete_all_in_dir(tgt_dir, exception=None):\n    if exception is not None and len(L(exception)) > 1: exception = tuple(exception)\n    for file in os.listdir(tgt_dir):\n        if exception is not None and file.endswith(exception): continue\n        file_path = os.path.join(tgt_dir, file)\n        if os.path.isfile(file_path) or os.path.islink(file_path): os.unlink(file_path)\n        elif os.path.isdir(file_path): shutil.rmtree(file_path)\n\n# Cell\ndef reverse_dict(dictionary):\n    return {v: k for k, v in dictionary.items()}\n\n# Cell\ndef is_tuple(o): return isinstance(o, tuple)\n\n# Cell\ndef itemify(*o, tup_id=None):\n    items = L(*o).zip()\n    if tup_id is not None: return L([item[tup_id] for item in items])\n    else: return items\n\n# Cell\ndef is_none(o):\n    return o in [[], [None], None]\n\ndef ifisnone(a, b):\n    ""`a` if `a` is None else `b`""\n    return None if is_none(a) else b\n\ndef ifnoneelse(a, b, c=None):\n    ""`b` if `a` is None else `c`""\n    return b if a is None else ifnone(c, a)\n\ndef ifisnoneelse(a, b, c=None):\n    ""`b` if `a` is None else `c`""\n    return b if is_none(a) else ifnone(c, a)\n\ndef ifelse(a, b, c):\n    ""`b` if `a` is True else `c`""\n    return b if a else c\n\n# Cell\ndef is_not_close(a, b, eps=1e-5):\n    ""Is `a` within `eps` of `b`""\n    if hasattr(a, \'__array__\') or hasattr(b, \'__array__\'):\n        return (abs(a - b) > eps).all()\n    if isinstance(a, (Iterable, Generator)) or isinstance(b, (Iterable, Generator)):\n        return is_not_close(np.array(a), np.array(b), eps=eps)\n    return abs(a - b) > eps\n\n\ndef test_not_close(a, b, eps=1e-5):\n    ""`test` that `a` is within `eps` of `b`""\n    test(a, b, partial(is_not_close, eps=eps), \'not_close\')\n\ndef test_type(a, b):\n    return test_eq(type(a),type(b))\n\n# Cell\ndef stack(o, axis=0):\n    if isinstance(o[0], torch.Tensor): return torch.stack(tuple(o), dim=axis)\n    else: return np.stack(o, axis)\n\n# Cell\ndef cat2int(o):\n    cat = Categorize()\n    cat.setup(o)\n    return stack(TfmdLists(o, cat)[:])\n\n# Cell\ndef cycle_dl(dl):\n    for _ in dl: _\n\n# Cell\ndef memmap2cache(o, bs=64, verbose=True):\n    print(\'Writing to buffer cache...\\n\')\n    start = partial = time.time()\n    n_batches = len(o) // bs\n    for i in range(n_batches):\n        np.array(o[slice(bs*i, bs*(1+i))])\n        if verbose and i > 0 and i%10==0:\n            print(f\'{i:4} {1000*(time.time() - partial)/10:5.0f} ms\')\n            partial = time.time()\n    print(\'\\n...complete\')\n    print(f\'\\nTotal time : {1000*(time.time() - start):.1f} ms ({1000 * (time.time() - start)/n_batches:.1f} ms / batch)\')\n\n# Cell\ndef package_versions():\n    print(\'tsai       :\', tsai.__version__)\n    print(\'fastai2    :\', fastai2.__version__)\n    print(\'fastcore   :\', fastcore.__version__)\n    print(\'torch      :\', torch.__version__)\n    print(\'scipy      :\', sp.__version__)\n    print(\'numpy      :\', np.__version__)\n    print(\'pandas     :\', pd.__version__)\n\ndef hardware_details():\n    print(f\'Total RAM  : {bytes2GB(psutil.virtual_memory().total):6.2f} GB\')\n    print(f\'Used RAM   : {bytes2GB(psutil.virtual_memory().used):6.2f} GB\')\n    print(\'n_cpus     :\', cpus)\n    iscuda = torch.cuda.is_available()\n    if iscuda: print(\'device     : {} ({})\'.format(device, torch.cuda.get_device_name(0)))\n    else: print(\'device     :\', device)'"
tsai/data/__init__.py,0,b''
tsai/data/all.py,0,b'from .external import *\nfrom .core import *\nfrom .transforms import *\nfrom .tabular import *\nfrom .validation import *'
tsai/data/core.py,5,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/002_data.core.ipynb (unless otherwise specified).\n\n__all__ = [\'NumpyTensor\', \'ToNumpyTensor\', \'TSTensor\', \'ToTSTensor\', \'NumpyTensorBlock\', \'TSTensorBlock\',\n           \'TorchDataset\', \'NumpyDataset\', \'TSDataset\', \'NumpyDatasets\', \'TSDatasets\', \'add_ds\', \'NumpyDataLoader\',\n           \'show_tuple\', \'TSDataLoader\', \'NumpyDataLoaders\', \'TSDataLoaders\']\n\n# Cell\nfrom ..imports import *\nfrom ..utils import *\nfrom .external import *\nfrom .validation import *\n\n# Cell\nclass NumpyTensor(TensorBase):\n    ""Returns a `tensor` with subclass `NumpyTensor` that has a show method""\n    def __new__(cls, o, **kwargs):\n        if isinstance(o, (list, L)): o = stack(o)\n        res = cast(tensor(o), cls)\n        res._meta = kwargs\n        return res\n    def __getitem__(self, idx):\n        res = super().__getitem__(idx)\n        return res.as_subclass(type(self))\n    def __repr__(self):\n        if self.numel() == 1: return f\'{self}\'\n        else: return f\'NumpyTensor(shape:{list(self.shape)})\'\n    def show(self, ax=None, ctx=None, title=None, title_color=\'black\', **kwargs):\n        if self.ndim != 2: self = type(self)(to2dtensor(self))\n        ax = ifnone(ax,ctx)\n        if ax is None: fig, ax = plt.subplots(**kwargs)\n        ax.plot(self.T)\n        ax.axis(xmin=0, xmax=self.shape[-1] - 1)\n        ax.set_title(title, weight=\'bold\', color=title_color)\n        plt.tight_layout()\n        return ax\n\nclass ToNumpyTensor(Transform):\n    ""Transforms np.ndarray to NumpyTensor""\n    def encodes(self, o:np.ndarray): return NumpyTensor(o)\n\n# Cell\nclass TSTensor(NumpyTensor):\n    \'\'\'Returns a `tensor` with subclass `TSTensor` that has a show method\'\'\'\n    @property\n    def vars(self): return self.shape[-2]\n    @property\n    def len(self): return self.shape[-1]\n    def __repr__(self):\n        if self.numel() == 1: return f\'{self}\'\n        elif self.ndim >= 3:   return f\'TSTensor(samples:{self.shape[-3]}, vars:{self.shape[-2]}, len:{self.shape[-1]})\'\n        elif self.ndim == 2: return f\'TSTensor(vars:{self.shape[-2]}, len:{self.shape[-1]})\'\n        elif self.ndim == 1: return f\'TSTensor(len:{self.shape[-1]})\'\n\nclass ToTSTensor(Transform):\n    def encodes(self, o:np.ndarray): return TSTensor(o)\n\n# Cell\nclass NumpyTensorBlock():\n    def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs=None):\n        self.type_tfms  =                 L(type_tfms)\n        self.item_tfms  = ToNumpyTensor + L(item_tfms)\n        self.batch_tfms =                 L(batch_tfms)\n        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)\n\nclass TSTensorBlock():\n    def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs=None):\n        self.type_tfms  =              L(type_tfms)\n        self.item_tfms  = ToTSTensor + L(item_tfms)\n        self.batch_tfms =              L(batch_tfms)\n        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)\n\n# Cell\nclass TorchDataset():\n    def __init__(self, X, y=None): self.X, self.y = X, y\n    def __getitem__(self, idx): return (self.X[idx],) if self.y is None else (self.X[idx], self.y[idx])\n    def __len__(self): return len(self.X)\n\nclass NumpyDataset():\n    def __init__(self, X, y=None, types=None): self.X, self.y, self.types = X, y, types\n    def __getitem__(self, idx):\n        if self.types is None: return (self.X[idx], self.y[idx]) if self.y is not None else (self.X[idx])\n        else: return (self.types[0](self.X[idx]), self.types[1](self.y[idx])) if self.y is not None else (self.types[0](self.X[idx]))\n    def __len__(self): return len(self.X)\n    @property\n    def c(self): return 0 if self.y is None else 1 if isinstance(self.y[0], float) else len(np.unique(self.y))\n\nclass TSDataset():\n    def __init__(self, X, y=None, types=None, sel_vars=None, sel_steps=None):\n        self.X, self.y, self.types = to3darray(X), y, types\n        self.sel_vars = ifnone(sel_vars, slice(None))\n        self.sel_steps = ifnone(sel_steps,slice(None))\n    def __getitem__(self, idx):\n        if self.types is None: return (self.X[idx, self.sel_vars, self.sel_steps], self.y[idx]) if self.y is not None else (self.X[idx])\n        else:\n            return (self.types[0](self.X[idx, self.sel_vars, self.sel_steps]), self.types[1](self.y[idx])) if self.y is not None \\\n            else (self.types[0](self.X[idx]))\n    def __len__(self): return len(self.X)\n    @property\n    def c(self): return 0 if self.y is None else 1 if isinstance(self.y[0], float) else len(np.unique(self.y))\n    @property\n    def vars(self): return self[0][0].shape[-2]\n    @property\n    def len(self): return self[0][0].shape[-1]\n\n# Cell\nclass NumpyDatasets(Datasets):\n    ""A dataset that creates tuples from X (and y) and applies `tfms` of type item_tfms""\n    _xtype, _ytype = NumpyTensor, None # Expected X and y output types (must have a show method)\n    def __init__(self, X=None, y=None, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, inplace=True, **kwargs):\n        self.inplace = inplace\n        if tls is None:\n            X = itemify(X, tup_id=0)\n            y = itemify(y, tup_id=0) if y is not None else y\n            items = tuple((X,)) if y is None else tuple((X,y))\n            self.tfms = L(ifnone(tfms,[None]*len(ifnone(tls,items))))\n        self.tls = L(tls if tls else [TfmdLists(item, t, **kwargs) for item,t in zip(items,self.tfms)])\n        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n        if len(self.tls[0]) > 0:\n            self.types = L([ifnone(_typ, type(tl[0]) if isinstance(tl[0], torch.Tensor) else tensor) for tl,_typ in zip(self.tls, [self._xtype, self._ytype])])\n            self.ptls = L([tl if not self.inplace else tl[:] if type(tl[0]).__name__ == \'memmap\' else tensor(stack(tl[:])) for tl in self.tls])\n\n    def __getitem__(self, it):\n        return tuple([typ(ptl[it]) for i,(ptl,typ) in enumerate(zip(self.ptls,self.types))])\n\n    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp, inplace=self.inplace, tfms=self.tfms)\n\n    def _new(self, X, *args, y=None, **kwargs):\n        items = ifnoneelse(y,tuple((X,)),tuple((X, y)))\n        return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n\n    def show_at(self, idx, **kwargs):\n        self.show(self[idx], **kwargs)\n        plt.show()\n\n    @property\n    def items(self): return tuple([tl.items for tl in self.tls])\n    @items.setter\n    def items(self, vs):\n        for tl,c in zip(self.tls, vs): tl.items = v\n\n\nclass TSDatasets(NumpyDatasets):\n    ""A dataset that creates tuples from X (and y) and applies `item_tfms`""\n    _xtype, _ytype = TSTensor, None # Expected X and y output types (torch.Tensor - default - or subclass)\n    def __init__(self, X=None, y=None, items=None, sel_vars=None, sel_steps=None, tfms=None, tls=None, n_inp=None, dl_type=None,\n                 inplace=True, **kwargs):\n        self.inplace = inplace\n        if tls is None:\n            X = itemify(to3darray(X), tup_id=0)\n            y = itemify(y, tup_id=0) if y is not None else y\n            items = tuple((X,)) if y is None else tuple((X,y))\n            self.tfms = L(ifnone(tfms,[None]*len(ifnone(tls,items))))\n        self.sel_vars = ifnone(sel_vars, slice(None))\n        self.sel_steps = ifnone(sel_steps,slice(None))\n        self.tls = L(tls if tls else [TfmdLists(item, t, **kwargs) for item,t in zip(items,self.tfms)])\n        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n        if len(self.tls[0]) > 0:\n            self.types = L([ifnone(_typ, type(tl[0]) if isinstance(tl[0], torch.Tensor) else tensor) for tl,_typ in zip(self.tls, [self._xtype, self._ytype])])\n            self.ptls = L([tl if not self.inplace else tl[:] if type(tl[0]).__name__ == \'memmap\' else tensor(stack(tl[:])) for tl in self.tls])\n\n    def __getitem__(self, it):\n        return tuple([typ(ptl[it])[...,self.sel_vars, self.sel_steps] if i==0 else typ(ptl[it]) for i,(ptl,typ) in enumerate(zip(self.ptls,self.types))])\n\n    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp, inplace=self.inplace, tfms=self.tfms,\n                                           sel_vars=self.sel_vars, sel_steps=self.sel_steps)\n    @property\n    def vars(self): return self[0][0].shape[-2]\n    @property\n    def len(self): return self[0][0].shape[-1]\n\n# Cell\ndef add_ds(dsets, X, y=None, test_items=None, rm_tfms=None, with_labels=False, inplace=True):\n    ""Create test datasets from X (and y) using validation transforms of `dsets`""\n    items = ifnoneelse(y,tuple((X,)),tuple((X, y)))\n    with_labels = ifnoneelse(y,False,True)\n    if isinstance(dsets, (Datasets, NumpyDatasets, TSDatasets)):\n        tls = dsets.tls if with_labels else dsets.tls[:dsets.n_inp]\n        new_tls = L([tl._new(item, split_idx=1) for tl,item in zip(tls, items)])\n        if rm_tfms is None: rm_tfms = [tl.infer_idx(get_first(item)) for tl,item in zip(new_tls, items)]\n        else:               rm_tfms = tuplify(rm_tfms, match=new_tls)\n        for i,j in enumerate(rm_tfms): new_tls[i].tfms.fs = new_tls[i].tfms.fs[j:]\n        if isinstance(dsets, TSDatasets):\n            return TSDatasets(tls=new_tls, n_inp=dsets.n_inp, inplace=inplace, tfms=dsets.tfms,\n                              sel_vars=dsets.sel_vars, sel_steps=dsets.sel_steps)\n        elif isinstance(dsets, NumpyDatasets):\n            return NumpyDatasets(tls=new_tls, n_inp=dsets.n_inp, inplace=inplace, tfms=dsets.tfms)\n        elif isinstance(dsets, Datasets): return Datasets(tls=new_tls)\n    elif isinstance(dsets, TfmdLists):\n        new_tl = dsets._new(items, split_idx=1)\n        if rm_tfms is None: rm_tfms = dsets.infer_idx(get_first(items))\n        new_tl.tfms.fs = new_tl.tfms.fs[rm_tfms:]\n        return new_tl\n    else: raise Exception(f""This method requires using the fastai library to assemble your data.Expected a `Datasets` or a `TfmdLists` but got {dsets.__class__.__name__}"")\n\n@patch\ndef add_test(self:NumpyDatasets, X, y=None, test_items=None, rm_tfms=None, with_labels=False, inplace=False):\n    return add_ds(self, X, y=y, test_items=test_items, rm_tfms=rm_tfms, with_labels=with_labels, inplace=inplace)\n\n@patch\ndef add_unlabeled(self:NumpyDatasets, X, test_items=None, rm_tfms=None, with_labels=False, inplace=False):\n    return add_ds(self, X, y=None, test_items=test_items, rm_tfms=rm_tfms, with_labels=with_labels, inplace=inplace)\n\n# Cell\n_batch_tfms = (\'after_item\',\'before_batch\',\'after_batch\')\n\nclass NumpyDataLoader(TfmdDL):\n    idxs = None\n    do_item = noops # create batch returns indices\n    def __init__(self, dataset, bs=64, shuffle=False, num_workers=None, verbose=False, do_setup=True, batch_tfms=None, **kwargs):\n        \'\'\'batch_tfms == after_batch (either can be used)\'\'\'\n        if num_workers is None: num_workers = min(16, defaults.cpus)\n        for nm in _batch_tfms:\n            if nm == \'after_batch\' and batch_tfms is not None: kwargs[nm] = Pipeline(batch_tfms)\n            else: kwargs[nm] = Pipeline(kwargs.get(nm,None))\n        bs = min(bs, len(dataset))\n        super().__init__(dataset, bs=bs, shuffle=shuffle, num_workers=num_workers, **kwargs)\n        if do_setup:\n            for nm in _batch_tfms:\n                pv(f""Setting up {nm}: {kwargs[nm]}"", verbose)\n                kwargs[nm].setup(self)\n\n    def create_batch(self, b):\n        it = b if self.shuffle else slice(b[0], b[0] + self.bs)\n        self.idxs = b\n        return self.dataset[it]\n\n    def create_item(self, s): return s\n\n    def get_idxs(self):\n        idxs = Inf.count if self.indexed else Inf.nones\n        if self.n is not None: idxs = list(range(len(self.dataset)))\n        if self.shuffle: idxs = self.shuffle_fn(idxs)\n        return idxs\n\n    @delegates(plt.subplots)\n    def show_batch(self, b=None, ctxs=None, max_n=9, nrows=3, ncols=3, figsize=(16, 10), **kwargs):\n        b = self.one_batch()\n        db = self.decode_batch(b, max_n=max_n)\n        if figsize is None: figsize = (ncols*6, max_n//ncols*4)\n        if ctxs is None: ctxs = get_grid(min(len(db), nrows*ncols), nrows=None, ncols=ncols, figsize=figsize, **kwargs)\n        for i,ctx in enumerate(ctxs): show_tuple(db[i], ctx=ctx)\n\n    @delegates(plt.subplots)\n    def show_results(self, b, preds, ctxs=None, max_n=9, nrows=3, ncols=3, figsize=(16, 10), **kwargs):\n        t = self.decode_batch(b, max_n=max_n)\n        p = self.decode_batch((b[0],preds), max_n=max_n)\n        if figsize is None: figsize = (ncols*6, max_n//ncols*4)\n        if ctxs is None: ctxs = get_grid(min(len(t), nrows*ncols), nrows=None, ncols=ncols, figsize=figsize, **kwargs)\n        for i,ctx in enumerate(ctxs):\n            title = f\'True: {t[i][1]}\\nPred: {p[i][1]}\'\n            color = \'green\' if t[i][1] == p[i][1] else \'red\'\n            t[i][0].show(ctx=ctx, title=title, title_color=color)\n\n@delegates(plt.subplots)\ndef show_tuple(tup, **kwargs):\n    ""Display a timeseries plot from a decoded tuple""\n    tup[0].show(title=\'unlabeled\' if len(tup) == 1 else tup[1], **kwargs)\n\nclass TSDataLoader(NumpyDataLoader):\n    @property\n    def vars(self): return self.dataset[0][0].shape[-2]\n    @property\n    def len(self): return self.dataset[0][0].shape[-1]\n\n# Cell\n_batch_tfms = (\'after_item\',\'before_batch\',\'after_batch\')\n\nclass NumpyDataLoaders(DataLoaders):\n    _xblock = NumpyTensorBlock\n    _dl_type = NumpyDataLoader\n    def __init__(self, *loaders, path=\'.\', device=default_device()):\n        self.loaders,self.path = list(loaders),Path(path)\n        self.device = device\n\n    @classmethod\n    @delegates(DataLoaders.from_dblock)\n    def from_numpy(cls, X, y=None, splitter=None, valid_pct=0.2, seed=0, item_tfms=None, batch_tfms=None, **kwargs):\n        ""Create timeseries dataloaders from arrays (X and y, unless unlabeled)""\n        if splitter is None: splitter = RandomSplitter(valid_pct=valid_pct, seed=seed)\n        getters = [ItemGetter(0), ItemGetter(1)] if y is not None else [ItemGetter(0)]\n        dblock = DataBlock(blocks=(cls._xblock, CategoryBlock),\n                           getters=getters,\n                           splitter=splitter,\n                           item_tfms=item_tfms,\n                           batch_tfms=batch_tfms)\n\n        source = itemify(X) if y is None else itemify(X,y)\n        return cls.from_dblock(dblock, source, **kwargs)\n\n    @classmethod\n    def from_dsets(cls, *ds, path=\'.\', bs=64, num_workers=0, batch_tfms=None, device=None, shuffle_train=True, **kwargs):\n        default = (shuffle_train,) + (False,) * (len(ds)-1)\n        defaults = {\'shuffle\': default, \'drop_last\': default}\n        kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items()})\n        kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)]\n        if not is_listy(bs): bs = [bs]\n        if len(bs) != len(ds): bs = bs * len(ds)\n        device = ifnone(device,default_device())\n        return cls(*[cls._dl_type(d, bs=b, num_workers=num_workers, batch_tfms=batch_tfms, **k) \\\n                     for d,k,b in zip(ds, kwargs, bs)], path=path, device=device)\n\nclass TSDataLoaders(NumpyDataLoaders):\n    _xblock = TSTensorBlock\n    _dl_type = TSDataLoader\n\n# Cell\n@patch\ndef cws(self:DataLoader):\n    if isinstance(tensor(self.dataset[0][-1]).item(),Integral):\n        target = torch.Tensor(self.dataset.items[-1]).to(dtype=torch.int64)\n        # Compute samples weight (each sample should get its own weight)\n        class_sample_count = torch.tensor([(target == t).sum() for t in torch.unique(target, sorted=True)])\n        weights = 1. / class_sample_count.float()\n        return (weights / weights.sum()).to(default_device())\n    else: return None'"
tsai/data/external.py,0,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/001_data.external.ipynb (unless otherwise specified).\n\n__all__ = [\'decompress_from_url\', \'get_UCR_univariate_list\', \'get_UCR_multivariate_list\', \'stack_padding\',\n           \'get_UCR_data\']\n\n# Cell\nfrom ..imports import *\nfrom ..utils import *\nfrom .validation import *\n\n# Cell\nimport tempfile\ntry: from urllib import urlretrieve\nexcept ImportError: from urllib.request import urlretrieve\nimport shutil\nfrom pyunpack import Archive\nfrom scipy.io import arff\nfrom sktime.utils.load_data import load_from_tsfile_to_dataframe\n\n# Cell\ndef decompress_from_url(url, target_dir=None, verbose=False):\n    #Download\n    try:\n        fname = os.path.basename(url)\n        tmpdir = tempfile.mkdtemp()\n        local_comp_fname = os.path.join(tmpdir, fname)\n        urlretrieve(url, local_comp_fname)\n    except:\n        shutil.rmtree(tmpdir)\n        if verbose: sys.stderr.write(""Could not download url. Please, check url.\\n"")\n\n    #Decompress\n    try:\n        if not os.path.exists(target_dir): os.makedirs(target_dir)\n        Archive(local_comp_fname).extractall(target_dir)\n        shutil.rmtree(tmpdir)\n        return target_dir\n    except:\n        shutil.rmtree(tmpdir)\n        if verbose: sys.stderr.write(""Could not uncompress file, aborting.\\n"")\n        return None\n\n# Cell\ndef get_UCR_univariate_list():\n    return [\n        \'ACSF1\', \'Adiac\', \'AllGestureWiimoteX\', \'AllGestureWiimoteY\',\n        \'AllGestureWiimoteZ\', \'ArrowHead\', \'Beef\', \'BeetleFly\', \'BirdChicken\',\n        \'BME\', \'Car\', \'CBF\', \'Chinatown\', \'ChlorineConcentration\',\n        \'CinCECGTorso\', \'Coffee\', \'Computers\', \'CricketX\', \'CricketY\',\n        \'CricketZ\', \'Crop\', \'DiatomSizeReduction\',\n        \'DistalPhalanxOutlineAgeGroup\', \'DistalPhalanxOutlineCorrect\',\n        \'DistalPhalanxTW\', \'DodgerLoopDay\', \'DodgerLoopGame\',\n        \'DodgerLoopWeekend\', \'Earthquakes\', \'ECG200\', \'ECG5000\', \'ECGFiveDays\',\n        \'ElectricDevices\', \'EOGHorizontalSignal\', \'EOGVerticalSignal\',\n        \'EthanolLevel\', \'FaceAll\', \'FaceFour\', \'FacesUCR\', \'FiftyWords\',\n        \'Fish\', \'FordA\', \'FordB\', \'FreezerRegularTrain\', \'FreezerSmallTrain\',\n        \'Fungi\', \'GestureMidAirD1\', \'GestureMidAirD2\', \'GestureMidAirD3\',\n        \'GesturePebbleZ1\', \'GesturePebbleZ2\', \'GunPoint\', \'GunPointAgeSpan\',\n        \'GunPointMaleVersusFemale\', \'GunPointOldVersusYoung\', \'Ham\',\n        \'HandOutlines\', \'Haptics\', \'Herring\', \'HouseTwenty\', \'InlineSkate\',\n        \'InsectEPGRegularTrain\', \'InsectEPGSmallTrain\', \'InsectWingbeatSound\',\n        \'ItalyPowerDemand\', \'LargeKitchenAppliances\', \'Lightning2\',\n        \'Lightning7\', \'Mallat\', \'Meat\', \'MedicalImages\', \'MelbournePedestrian\',\n        \'MiddlePhalanxOutlineAgeGroup\', \'MiddlePhalanxOutlineCorrect\',\n        \'MiddlePhalanxTW\', \'MixedShapesRegularTrain\', \'MixedShapesSmallTrain\',\n        \'MoteStrain\', \'NonInvasiveFetalECGThorax1\',\n        \'NonInvasiveFetalECGThorax2\', \'OliveOil\', \'OSULeaf\',\n        \'PhalangesOutlinesCorrect\', \'Phoneme\', \'PickupGestureWiimoteZ\',\n        \'PigAirwayPressure\', \'PigArtPressure\', \'PigCVP\', \'PLAID\', \'Plane\',\n        \'PowerCons\', \'ProximalPhalanxOutlineAgeGroup\',\n        \'ProximalPhalanxOutlineCorrect\', \'ProximalPhalanxTW\',\n        \'RefrigerationDevices\', \'Rock\', \'ScreenType\', \'SemgHandGenderCh2\',\n        \'SemgHandMovementCh2\', \'SemgHandSubjectCh2\', \'ShakeGestureWiimoteZ\',\n        \'ShapeletSim\', \'ShapesAll\', \'SmallKitchenAppliances\', \'SmoothSubspace\',\n        \'SonyAIBORobotSurface1\', \'SonyAIBORobotSurface2\', \'StarLightCurves\',\n        \'Strawberry\', \'SwedishLeaf\', \'Symbols\', \'SyntheticControl\',\n        \'ToeSegmentation1\', \'ToeSegmentation2\', \'Trace\', \'TwoLeadECG\',\n        \'TwoPatterns\', \'UMD\', \'UWaveGestureLibraryAll\', \'UWaveGestureLibraryX\',\n        \'UWaveGestureLibraryY\', \'UWaveGestureLibraryZ\', \'Wafer\', \'Wine\',\n        \'WordSynonyms\', \'Worms\', \'WormsTwoClass\', \'Yoga\'\n    ]\n\ntest_eq(len(get_UCR_univariate_list()), 128)\n\n# Cell\ndef get_UCR_multivariate_list():\n    return [\n        \'ArticularyWordRecognition\', \'AtrialFibrillation\', \'BasicMotions\',\n        \'CharacterTrajectories\', \'Cricket\', \'DuckDuckGeese\', \'EigenWorms\',\n        \'Epilepsy\', \'ERing\', \'EthanolConcentration\', \'FaceDetection\',\n        \'FingerMovements\', \'HandMovementDirection\', \'Handwriting\', \'Heartbeat\',\n        \'InsectWingbeat\', \'JapaneseVowels\', \'Libras\', \'LSST\', \'MotorImagery\',\n        \'NATOPS\', \'PEMS-SF\', \'PenDigits\', \'PhonemeSpectra\', \'RacketSports\',\n        \'SelfRegulationSCP1\', \'SelfRegulationSCP2\', \'SpokenArabicDigits\',\n        \'StandWalkJump\', \'UWaveGestureLibrary\'\n    ]\n\ntest_eq(len(get_UCR_multivariate_list()), 30)\n\n# Cell\ndef stack_padding(arr):\n    def resize(row, size):\n        new = np.array(row)\n        new.resize(size)\n        return new\n    row_length = max(arr, key=len).__len__()\n    mat = np.array( [resize(row, row_length) for row in arr] )\n    return mat\n\ndef get_UCR_data(dsid, path=\'.\', parent_dir=\'data/UCR\', verbose=False, drop_na=False, on_disk=True, return_split=True):\n    if verbose: print(\'Dataset:\', dsid)\n    assert dsid in get_UCR_univariate_list() + get_UCR_multivariate_list(), f\'{dsid} is not a UCR dataset\'\n    full_parent_dir = Path(path)/parent_dir\n    full_tgt_dir = full_parent_dir/dsid\n    if not all([os.path.isfile(f\'{full_parent_dir}/{dsid}/{fn}.npy\') for fn in [\'X_train\', \'X_valid\', \'y_train\', \'y_valid\', \'X\', \'y\']]):\n        if dsid in [\'InsectWingbeat\', \'DuckDuckGeese\']:\n            if verbose: print(\'There are problems with the original zip file and data cannot correctly downloaded\')\n            return None, None, None, None\n        src_website = \'http://www.timeseriesclassification.com/Downloads\'\n        if verbose: print(f\'Downloading and decompressing data to {full_tgt_dir}...\')\n        decompress_from_url(f\'{src_website}/{dsid}.zip\', target_dir=full_tgt_dir, verbose=verbose)\n        if verbose: print(\'...data downloaded and decompressed\')\n        X_train_df, y_train = load_from_tsfile_to_dataframe(full_tgt_dir/f\'{dsid}_TRAIN.ts\')\n        X_valid_df, y_valid = load_from_tsfile_to_dataframe(full_tgt_dir/f\'{dsid}_TEST.ts\')\n        X_train_ = []\n        X_valid_ = []\n        for i in range(X_train_df.shape[-1]):\n            X_train_.append(stack_padding(X_train_df[f\'dim_{i}\'])) # stack arrays even if they have different lengths\n            X_valid_.append(stack_padding(X_valid_df[f\'dim_{i}\']))\n        X_train = np.transpose(np.stack(X_train_, axis=-1), (0, 2, 1)).astype(np.float32)\n        X_valid = np.transpose(np.stack(X_valid_, axis=-1), (0, 2, 1)).astype(np.float32)\n        np.save(f\'{full_tgt_dir}/X_train.npy\', X_train)\n        np.save(f\'{full_tgt_dir}/y_train.npy\', y_train)\n        np.save(f\'{full_tgt_dir}/X_valid.npy\', X_valid)\n        np.save(f\'{full_tgt_dir}/y_valid.npy\', y_valid)\n        np.save(f\'{full_tgt_dir}/X.npy\', concat(X_train, X_valid))\n        np.save(f\'{full_tgt_dir}/y.npy\', concat(y_train, y_valid))\n        del X_train, X_valid, y_train, y_valid\n        delete_all_in_dir(full_tgt_dir, exception=\'.npy\')\n\n    mmap_mode=\'r+\' if on_disk else None\n    X_train = np.load(f\'{full_tgt_dir}/X_train.npy\', mmap_mode=mmap_mode)\n    y_train = np.load(f\'{full_tgt_dir}/y_train.npy\', mmap_mode=mmap_mode)\n    X_valid = np.load(f\'{full_tgt_dir}/X_valid.npy\', mmap_mode=mmap_mode)\n    y_valid = np.load(f\'{full_tgt_dir}/y_valid.npy\', mmap_mode=mmap_mode)\n\n    if return_split:\n        if verbose:\n            print(\'X_train:\', X_train.shape)\n            print(\'y_train:\', y_train.shape)\n            print(\'X_valid:\', X_valid.shape)\n            print(\'y_valid:\', y_valid.shape, \'\\n\')\n        return X_train, y_train, X_valid, y_valid\n    else:\n        X = np.load(f\'{full_tgt_dir}/X.npy\', mmap_mode=mmap_mode)\n        y = np.load(f\'{full_tgt_dir}/y.npy\', mmap_mode=mmap_mode)\n        splits = get_predefined_splits(*[X_train, X_valid])\n        if verbose:\n            print(\'X      :\', X .shape)\n            print(\'y      :\', y .shape)\n            print(\'splits :\', splits, \'\\n\')\n        return X, y, splits'"
tsai/data/tabular.py,0,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/005_data.tabular.ipynb (unless otherwise specified).\n\n__all__ = [\'TabularDataset\', \'TabularDataLoader\']\n\n# Cell\nfrom ..imports import *\nfrom fastai2.tabular.all import *\n\n# Cell\nclass TabularDataset():\n    ""A `Numpy` dataset from a `TabularPandas` object""\n    def __init__(self, to):\n        self.cats = to.cats.to_numpy().astype(np.long)\n        self.conts = to.conts.to_numpy().astype(np.float32)\n        self.ys = to.ys.to_numpy()\n    def __getitem__(self, idx): return self.cats[idx], self.conts[idx], self.ys[idx]\n    def __len__(self): return len(self.cats)\n    @property\n    def c(self): return 0 if self.ys is None else 1 if isinstance(self.ys[0], float) else len(np.unique(self.ys))\n\nclass TabularDataLoader(DataLoader):\n    def __init__(self, dataset, bs=1, num_workers=0, device=None, train=False, **kwargs):\n        device = ifnone(device, default_device())\n        super().__init__(dataset, bs=min(bs, len(dataset)), num_workers=num_workers, shuffle=train, device=device, drop_last=train, **kwargs)\n        self.device, self.shuffle = device, train\n    def create_item(self, s): return s\n    def get_idxs(self):\n        idxs = Inf.count if self.indexed else Inf.nones\n        if self.n is not None: idxs = list(range(len(self.dataset)))\n        if self.shuffle: self.shuffle_fn()\n        return idxs\n    def create_batch(self, b):\n        return self.dataset[b[0]:b[0]+self.bs]\n    def shuffle_fn(self):\n        ""Shuffle dataset after each epoch""\n        rng = np.random.permutation(len(self.dataset))\n        self.dataset.cats = self.dataset.cats[rng]\n        self.dataset.conts = self.dataset.conts[rng]\n        self.dataset.ys = self.dataset.ys[rng]\n    def to(self, device):\n        self.device = device\n#     def ds_to(self, device=None):\n        self.dataset.cats = tensor(self.dataset.cats).to(device=self.device)\n        self.dataset.conts = tensor(self.dataset.conts).to(device=self.device)\n        self.dataset.ys = tensor(self.dataset.ys).to(device=self.device)'"
tsai/data/transforms.py,16,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/003_data.transforms.ipynb (unless otherwise specified).\n\n__all__ = [\'TSStandardize\', \'TSNormalize\', \'TSIdentity\', \'TSShuffle_HLs\', \'TSMagNoise\', \'TSMagMulNoise\',\n           \'random_curve_generator\', \'random_cum_curve_generator\', \'random_cum_noise_generator\', \'TSTimeNoise\',\n           \'TSMagWarp\', \'TSTimeWarp\', \'TSMagScale\', \'TSMagScaleVar\', \'TSZoomIn\', \'TSZoomOut\', \'TSScale\',\n           \'TSRandomTimeStep\', \'TSBlur\', \'TSSmooth\', \'maddest\', \'TSDenoise\', \'TSRandomNoise\', \'TSLookBack\', \'TSVarOut\',\n           \'TSCutOut\', \'TSTimeStepOut\', \'TSCrop\', \'TSRandomCrop\', \'TSRandomResizedCrop\', \'TSCenterCrop\', \'TSMaskOut\',\n           \'TSTranslateX\', \'TSFlip\', \'TSRandomFlip\', \'TSShift\', \'TSRandomRotate\', \'TSNeg\', \'TSRandomNeg\', \'TSFreqNoise\',\n           \'TSFreqWarp\', \'TSFreqScale\']\n\n# Cell\nfrom ..imports import *\nfrom ..utils import *\nfrom .external import *\nfrom .core import *\n\n# Cell\nfrom scipy.interpolate import CubicSpline\nfrom scipy.ndimage import convolve1d\nimport pywt\n\n# Cell\nclass TSStandardize(Transform):\n    ""Standardize/destd batch of `NumpyTensor` or `TSTensor`""\n    parameters, order = L(\'mean\', \'std\'), 99\n    def __init__(self, mean=None, std=None, by_sample=False, by_var=False, verbose=False):\n        self.mean = tensor(mean) if mean is not None else None\n        self.std = tensor(std) if std is not None else None\n        self.by_sample, self.by_var = by_sample, by_var\n        if by_sample and by_var: self.axes = (2)\n        elif by_sample: self.axes = (1, 2)\n        elif by_var: self.axes = (0, 2)\n        else: self.axes = ()\n        self.verbose = verbose\n\n    @classmethod\n    def from_stats(cls, mean, std): return cls(mean, std)\n\n    def setups(self, dl: DataLoader):\n        if self.mean is None or self.std is None:\n            pv(f\'{self.__class__.__name__} setup mean={self.mean}, std={self.std}, by_sample={self.by_sample}, by_var={self.by_var}\', self.verbose)\n            x, *_ = dl.one_batch()\n            self.mean, self.std = x.mean(self.axes, keepdim=self.axes!=()), x.std(self.axes, keepdim=self.axes!=()) + 1e-7\n            pv(f\'mean: {self.mean}  std: {self.std}\\n\', self.verbose)\n\n    def encodes(self, x:(NumpyTensor, TSTensor)):\n        if self.by_sample: self.mean, self.std = x.mean(self.axes, keepdim=self.axes!=()), x.std(self.axes, keepdim=self.axes!=()) + 1e-7\n        return (x - self.mean) / self.std\n\n# Cell\n@patch\ndef mul_min(x:(torch.Tensor, TSTensor, NumpyTensor), axes=(), keepdim=False):\n    if axes == (): return retain_type(x.min(), x)\n    axes = reversed(sorted(axes if is_listy(axes) else [axes]))\n    min_x = x\n    for ax in axes: min_x, _ = min_x.min(ax, keepdim)\n    return retain_type(min_x, x)\n\n@patch\ndef mul_max(x:(torch.Tensor, TSTensor, NumpyTensor), axes=(), keepdim=False):\n    if axes == (): return retain_type(x.max(), x)\n    axes = reversed(sorted(axes if is_listy(axes) else [axes]))\n    max_x = x\n    for ax in axes: max_x, _ = max_x.max(ax, keepdim)\n    return retain_type(max_x, x)\n\nclass TSNormalize(Transform):\n    ""Normalize/denorm batch of `NumpyTensor` or `TSTensor`""\n    parameters, order = L(\'min\', \'max\'), 99\n\n    def __init__(self, min=None, max=None, range_min=-1, range_max=1, by_sample=True, by_var=False, verbose=False):\n        self.min = tensor(min) if min is not None else None\n        self.max = tensor(max) if max is not None else None\n        self.range_min, self.range_max = range_min, range_max\n        self.by_sample, self.by_var = by_sample, by_var\n        if by_sample and by_var: self.axes = (2)\n        elif by_sample: self.axes = (1, 2)\n        elif by_var: self.axes = (0, 2)\n        else: self.axes = ()\n        self.verbose = verbose\n\n    @classmethod\n    def from_stats(cls, min, max, range_min=0, range_max=1): return cls(min, max, self.range_min, self.range_max)\n\n    def setups(self, dl: DataLoader):\n        if self.min is None or self.max is None:\n            pv(f\'{self.__class__.__name__} setup min={self.min}, max={self.max}, range_min={self.range_min}, range_max={self.range_max}, by_sample={self.by_sample}, by_var={self.by_var}\',  self.verbose)\n            x, *_ = dl.one_batch()\n            self.min, self.max = x.mul_min(self.axes, keepdim=self.axes!=()), x.mul_max(self.axes, keepdim=self.axes!=())\n            pv(f\'min: {self.min}  max: {self.max}\\n\', self.verbose)\n\n    def encodes(self, x:(NumpyTensor, TSTensor)):\n        if self.by_sample: self.min, self.max = x.mul_min(self.axes, keepdim=self.axes!=()), x.mul_max(self.axes, keepdim=self.axes!=())\n        return ((x - self.min) / (self.max - self.min)) * (self.range_max - self.range_min) + self.range_min\n\n# Cell\nclass TSIdentity(Transform):\n    ""Applies the identity tfm to a `TSTensor` batch""\n    order = 90\n    def __init__(self, magnitude=0., **kwargs): self.magnitude = magnitude\n    def encodes(self, o: TSTensor): return o\n\n# Cell\nclass TSShuffle_HLs(Transform):\n    ""Randomly shuffles His/Lows of an OHLC `TSTensor` batch""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        timesteps = o.shape[-1] // 4\n        pos_rand_list = np.random.choice(np.arange(timesteps),size=random.randint(0, timesteps),replace=False)\n        rand_list = pos_rand_list * 4\n        highs = rand_list + 1\n        lows = highs + 1\n        a = np.vstack([highs, lows]).flatten(\'F\')\n        b = np.vstack([lows, highs]).flatten(\'F\')\n        output = o.clone()\n        output[...,a] = output[...,b]\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSMagNoise(Transform):\n    ""Applies additive noise on the y-axis for each step of a `TSTensor` batch""\n    order = 90\n    def __init__(self, magnitude=.02, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        noise = torch.normal(0, self.magnitude, (1, seq_len), dtype=o.dtype, device=o.device)\n        output = o + noise\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSMagMulNoise(Transform):\n    ""Applies multiplicative noise on the y-axis for each step of a `TSTensor` batch""\n    order = 90\n    def __init__(self, magnitude=.01, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        seq_len = o.shape[-1]\n        noise = torch.normal(1, self.magnitude, (1, seq_len), dtype=o.dtype, device=o.device)\n        output = o * noise\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\ndef random_curve_generator(o, magnitude=.1, order=4, noise=None):\n    seq_len = o.shape[-1]\n    f = CubicSpline(np.linspace(-seq_len, 2 * seq_len - 1, 3 * (order - 1) + 1, dtype=int),\n                    np.random.normal(loc=1.0, scale=magnitude, size=3 * (order - 1) + 1), axis=-1)\n    return f(np.arange(seq_len))\n\ndef random_cum_curve_generator(o, magnitude=.1, order=4, noise=None):\n    x = random_curve_generator(o, magnitude=magnitude, order=order, noise=noise).cumsum()\n    x -= x[0]\n    x /= x[-1]\n    x = np.clip(x, 0, 1)\n    return x * (o.shape[-1] - 1)\n\ndef random_cum_noise_generator(o, magnitude=.1, noise=None):\n    seq_len = o.shape[-1]\n    x = np.clip(np.ones(seq_len) + np.random.normal(loc=0, scale=magnitude, size=seq_len), 0, 1000).cumsum()\n    x -= x[0]\n    x /= x[-1]\n    return x * (o.shape[-1] - 1)\n\n# Cell\nclass TSTimeNoise(Transform):\n    ""Applies noise to each step in the x-axis of a `TSTensor` batch based on smooth random curve""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        f = CubicSpline(np.arange(o.shape[-1]), o.cpu(), axis=-1)\n        output = o.new(f(random_cum_noise_generator(o, magnitude=self.magnitude)))\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSMagWarp(Transform):\n    ""Applies warping to the y-axis of a `TSTensor` batch based on a smooth random curve""\n    order = 90\n    def __init__(self, magnitude=.02, ord=4, ex=None, **kwargs): self.magnitude, self.ord, self.ex = magnitude, ord, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        y_mult = random_curve_generator(o, magnitude=self.magnitude, order=self.ord)\n        output = o * o.new(y_mult)\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSTimeWarp(Transform):\n    ""Applies time warping to the x-axis of a `TSTensor` batch based on a smooth random curve""\n    order = 90\n    def __init__(self, magnitude=.02, ord=4, ex=None, **kwargs): self.magnitude, self.ord, self.ex = magnitude, ord, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        f = CubicSpline(np.arange(seq_len), o.cpu(), axis=-1)\n        output = o.new(f(random_cum_curve_generator(o, magnitude=self.magnitude, order=self.ord)))\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSMagScale(Transform):\n    ""Applies scaling to each step in the y-axis of a `TSTensor` batch based on a smooth random curve""\n    order = 90\n    def __init__(self, magnitude=.02, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        scale = 1 + 2 * (torch.rand(1, device=o.device) - .5) * self.magnitude\n        output = o * scale\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSMagScaleVar(Transform):\n    ""Applies scaling to each variable and step in the y-axis of a `TSTensor` batch based on smooth random curves""\n    order = 90\n    def __init__(self, magnitude=.02, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        scale = 1 + 2 * (torch.rand((o.shape[-2], 1), device=o.device) - .5) * self.magnitude\n        output = o * scale\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSZoomIn(Transform):\n    ""Amplifies a sequence focusing on a random section of the steps""\n    order = 90\n    def __init__(self, magnitude=.02, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        lambd = np.random.beta(self.magnitude, self.magnitude)\n        lambd = max(lambd, 1 - lambd)\n        win_len = int(seq_len * lambd)\n        start = 0 if win_len == seq_len else np.random.randint(0, seq_len - win_len)\n        f = CubicSpline(np.arange(win_len), o[..., start : start + win_len].cpu(), axis=-1)\n        output = o.new(f(np.linspace(0, win_len - 1, num=seq_len)))\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSZoomOut(Transform):\n    ""Compresses a sequence on the x-axis""\n    order = 90\n    def __init__(self, magnitude=.02, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        lambd = np.random.beta(self.magnitude, self.magnitude)\n        lambd = max(lambd, 1 - lambd)\n        win_len = int(seq_len * lambd)\n        if win_len == seq_len: start = 0\n        else: start = np.random.randint(0, seq_len - win_len)\n        f = CubicSpline(np.arange(o.shape[-1]), o.cpu(), axis=-1)\n        output = torch.zeros_like(o, dtype=o.dtype, device=o.device)\n        output[..., start:start + win_len] = o.new(f(np.linspace(0, seq_len - 1, num=win_len)))\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSScale(Transform):\n    ""Randomly amplifies/ compresses a sequence on the x-axis""\n    order = 90\n    def __init__(self, magnitude=.02, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        if np.random.rand() <= .5: return TSZoomIn(magnitude=self.magnitude, ex=self.ex)(o)\n        else: return TSZoomOut(magnitude=self.magnitude, ex=self.ex)(o)\n\n# Cell\nclass TSRandomTimeStep(Transform):\n    ""Compresses a sequence on the x-axis by randomly selecting sequence steps""\n    order = 90\n    def __init__(self, magnitude=.02, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        new_seq_len = int(seq_len * max(.5, (1 - np.random.rand() * self.magnitude)))\n        timesteps = np.sort(np.random.choice(np.arange(seq_len),new_seq_len, replace=False))\n        f = CubicSpline(np.arange(len(timesteps)), o[..., timesteps].cpu(), axis=-1)\n        output = o.new(f(np.linspace(0, new_seq_len - 1, num=seq_len)))\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSBlur(Transform):\n    ""Blurs a sequence applying a filter of type [1, 0..., 1]""\n    order = 90\n    def __init__(self, magnitude=.05, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        if self.magnitude == 3:  filterargs = np.array([1, 0, 1])\n        else:\n            magnitude = tuple((3, 3 + int(self.magnitude * 4)))\n            n_zeros = int(np.random.choice(np.arange(magnitude[0], magnitude[1] + 1, 2))) - 2\n            filterargs = np.array([1] + [0] * n_zeros + [1])\n        w = filterargs * np.random.rand(len(filterargs))\n        w = w / w.sum()\n        output = o.new(convolve1d(o.cpu(), w, mode=\'nearest\'))\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSSmooth(Transform):\n    ""Smoothens a sequence applying a filter of type [1, 5..., 1]""\n    order = 90\n    def __init__(self, magnitude=.05, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        if self.magnitude == 3:  filterargs = np.array([1, 5, 1])\n        else:\n            magnitude = tuple((3, 3 + int(self.magnitude * 4)))\n            n_ones = int(np.random.choice(np.arange(magnitude[0], magnitude[1] + 1, 2))) // 2\n            filterargs = np.array([1] * n_ones + [5] + [1] * n_ones)\n        w = filterargs * np.random.rand(len(filterargs))\n        w = w / w.sum()\n        output = o.new(convolve1d(o.cpu(), w, mode=\'nearest\'))\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\ndef maddest(d, axis=None): #Mean Absolute Deviation\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\nclass TSDenoise(Transform):\n    ""Denoises a sequence applying a wavelet decomposition method""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, wavelet=\'db4\', level=2, thr=None, thr_mode=\'hard\', pad_mode=\'per\', **kwargs):\n        self.magnitude, self.ex = magnitude, ex\n        self.wavelet, self.level, self.thr, self.thr_mode, self.pad_mode = wavelet, level, thr, thr_mode, pad_mode\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        """"""\n        1. Adapted from waveletSmooth function found here:\n        http://connor-johnson.com/2016/01/24/using-pywavelets-to-remove-high-frequency-noise/\n        2. Threshold equation and using hard mode in threshold as mentioned\n        in section \'3.2 denoising based on optimized singular values\' from paper by Tomas Vantuch:\n        http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n        """"""\n        seq_len = o.shape[-1]\n        # Decompose to get the wavelet coefficients\n        coeff = pywt.wavedec(o.cpu(), self.wavelet, mode=self.pad_mode)\n        if self.thr is None:\n            # Calculate sigma for threshold as defined in http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n            # As noted by @harshit92 MAD referred to in the paper is Mean Absolute Deviation not Median Absolute Deviation\n            sigma = (1/0.6745) * maddest(coeff[-self.level])\n\n            # Calculate the univeral threshold\n            uthr = sigma * np.sqrt(2*np.log(seq_len))\n            coeff[1:] = (pywt.threshold(c, value=uthr, mode=self.thr_mode) for c in coeff[1:])\n        elif self.thr == \'random\': coeff[1:] = (pywt.threshold(c, value=np.random.rand(), mode=self.thr_mode) for c in coeff[1:])\n        else: coeff[1:] = (pywt.threshold(c, value=self.thr, mode=self.thr_mode) for c in coeff[1:])\n\n        # Reconstruct the signal using the thresholded coefficients\n        output = o.new(pywt.waverec(coeff, self.wavelet, mode=self.pad_mode)[..., :seq_len])\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSRandomNoise(Transform):\n    ""Applys random noise using a wavelet decomposition method""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, wavelet=\'db4\', level=2, mode=\'constant\', **kwargs):\n        self.magnitude, self.ex = magnitude, ex\n        self.wavelet, self.level, self.mode = wavelet, level, mode\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        self.level = 1 if self.level is None else self.level\n        coeff = pywt.wavedec(o.cpu(), self.wavelet, mode=self.mode, level=self.level)\n        coeff[1:] = [c * (1 + 2 * (np.random.rand() - .5) * self.magnitude) for c in coeff[1:]]\n        output = o.new(pywt.waverec(coeff, self.wavelet, mode=self.mode)[..., :o.shape[-1]])\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSLookBack(Transform):\n    ""Selects a random number of sequence steps starting from the end""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        lambd = np.random.beta(self.magnitude, self.magnitude)\n        lambd = min(lambd, 1 - lambd)\n        lookback_per = int(lambd * seq_len)\n        output = o.clone()\n        output[..., :lookback_per] = 0\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSVarOut(Transform):\n    ""Set the value of a random number of variables to zero""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        in_vars = o.shape[-2]\n        if in_vars == 1: return o\n        lambd = np.random.beta(self.magnitude, self.magnitude)\n        lambd = min(lambd, 1 - lambd)\n        p = np.arange(in_vars).cumsum()\n        p = p/p[-1]\n        p = p / p.sum()\n        p = p[::-1]\n        out_vars = np.random.choice(np.arange(in_vars), int(lambd * in_vars), p=p, replace=False)\n        if len(out_vars) == 0:  return o\n        output = o.clone()\n        output[...,out_vars,:] = 0\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSCutOut(Transform):\n    ""Sets a random section of the sequence to zero""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        lambd = np.random.beta(self.magnitude, self.magnitude)\n        lambd = min(lambd, 1 - lambd)\n        win_len = int(seq_len * lambd)\n        start = np.random.randint(-win_len + 1, seq_len)\n        end = start + win_len\n        start = max(0, start)\n        end = min(end, seq_len)\n        output = o.clone()\n        output[..., start:end] = 0\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSTimeStepOut(Transform):\n    ""Sets random sequence steps to zero""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        magnitude = min(.5, self.magnitude)\n        seq_len = o.shape[-1]\n        timesteps = np.sort(np.random.choice(np.arange(seq_len), int(seq_len * magnitude), replace=False))\n        output = o.clone()\n        output[..., timesteps] = 0\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSCrop(Transform):\n    ""Crops a section of the sequence of a predefined length""\n    order = 90\n    def __init__(self, magnitude=.50, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        magnitude = min(.5, self.magnitude)\n        seq_len = o.shape[-1]\n        win_len = int(seq_len * (1 - magnitude))\n        start = np.random.randint(0, seq_len - win_len)\n        end = start + win_len\n        output = torch.zeros_like(o, dtype=o.dtype, device=o.device)\n        output[..., start - end :] = o[..., start : end]\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSRandomCrop(Transform):\n    ""Crops a section of the sequence of a random length""\n    order = 90\n    def __init__(self, magnitude=.05, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        lambd = np.random.beta(self.magnitude, self.magnitude)\n        lambd = max(lambd, 1 - lambd)\n        win_len = int(seq_len * lambd)\n        if win_len == seq_len: return o\n        start = np.random.randint(0, seq_len - win_len)\n        output = torch.zeros_like(o, dtype=o.dtype, device=o.device)\n        output[..., start : start + win_len] = o[..., start : start + win_len]\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSRandomResizedCrop(Transform):\n    ""Crops a section of the sequence of a random length""\n    order = 90\n    def __init__(self, magnitude=.01, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        lambd = np.random.beta(self.magnitude, self.magnitude)\n        lambd = max(lambd, 1 - lambd)\n        win_len = int(seq_len * lambd)\n        if win_len == seq_len: return o\n        start = np.random.randint(0, seq_len - win_len)\n        f = CubicSpline(np.arange(win_len), o[..., start : start + win_len].cpu(), axis=-1)\n        return o.new(f(np.linspace(0, win_len, num=seq_len)))\n\n# Cell\nclass TSCenterCrop(Transform):\n    ""Crops a section of the sequence of a random length from the center""\n    order = 90\n    def __init__(self, magnitude=.5, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        lambd = np.random.beta(self.magnitude, self.magnitude)\n        lambd = max(lambd, 1 - lambd)\n        win_len = int(seq_len * lambd)\n        start = seq_len // 2 - win_len // 2\n        end = start + win_len\n        start = max(0, start)\n        end = min(end, seq_len)\n        output = torch.zeros_like(o, dtype=o.dtype, device=o.device)\n        output[..., start : end] = o[..., start : end]\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSMaskOut(Transform):\n    ""Set a random number of steps to zero""\n    order = 90\n    def __init__(self, magnitude=.05, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        mask = torch.rand_like(o) <= self.magnitude\n        output = o.clone()\n        output[mask] = 0\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSTranslateX(Transform):\n    ""Moves a selected sequence window a random number of steps""\n    order = 90\n    def __init__(self, magnitude=.05, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        lambd = np.random.beta(self.magnitude, self.magnitude)\n        lambd = min(lambd, 1 - lambd)\n        shift = int(seq_len * lambd * self.magnitude)\n        if shift == 0: return o\n        if np.random.rand() < .5: shift = -shift\n        new_start = max(0, shift)\n        new_end = min(seq_len + shift, seq_len)\n        start = max(0, -shift)\n        end = min(seq_len - shift, seq_len)\n        output = torch.zeros_like(o, dtype=o.dtype, device=o.device)\n        output[..., new_start : new_end] = o[..., start : end]\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSFlip(Transform):\n    ""Flips the sequence along the x-axis""\n    order = 90\n    def __init__(self, magnitude=None, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        output = torch.flip(o, [-1])\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSRandomFlip(Transform):\n    ""Flips the sequence along the x-axis""\n    order = 90\n    def __init__(self, magnitude=None, ex=None, p=0.5, **kwargs):\n        self.magnitude, self.ex, self.p = magnitude, ex, p\n    def encodes(self, o:TSTensor):\n        if random.random() < self.p: return o\n        output = torch.flip(o, [-1])\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSShift(Transform):\n    ""Shifts and splits a sequence""\n    order = 90\n    def __init__(self, magnitude=None, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        pos = np.random.randint(0, o.shape[-1])\n        output = torch.cat((o[..., pos:], o[..., :pos]), dim=-1)\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSRandomRotate(Transform):\n    ""Randomly rotates the sequence along the z-axis""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        flat_x = o.view(o.shape[0], -1)\n        ran = flat_x.max(dim=-1, keepdim=True).values - flat_x.min(dim=-1, keepdim=True).values\n        trend = torch.linspace(0, 1, o.shape[-1], device=o.device) * ran\n        t = (1 + self.magnitude * 2 * (np.random.rand() - .5) * trend)\n        t -= t.mean(-1, keepdim=True)\n        if o.ndim == 3: t = t.unsqueeze(1)\n        output = o + t\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSNeg(Transform):\n    ""Applies a negative value to the time sequence""\n    order = 90\n    def __init__(self, magnitude=None, ex=None, **kwargs): self.magnitude, self.ex = magnitude, ex\n    def encodes(self, o: TSTensor): return - o\n\n# Cell\nclass TSRandomNeg(Transform):\n    ""Randomly applies a negative value to the time sequence""\n    order = 90\n    def __init__(self, magnitude=None, ex=None, p=.5, **kwargs): self.magnitude, self.ex, self.p = magnitude, ex, p\n    def encodes(self, o: TSTensor):\n        if self.p < random.random(): return o\n        return - o\n\n# Cell\nclass TSFreqNoise(Transform):\n    ""Applies noise based on a wavelet decomposition""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, wavelet=\'db4\', level=2, mode=\'constant\', **kwargs):\n        self.magnitude, self.ex = magnitude, ex\n        self.wavelet, self.level, self.mode = wavelet, level, mode\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        self.level = 1 if self.level is None else self.level\n        coeff = pywt.wavedec(o.cpu(), self.wavelet, mode=self.mode, level=self.level)\n        coeff[1:] = [c + 2 * (np.random.rand() - .5) * self.magnitude for c in coeff[1:]]\n        output = o.new(pywt.waverec(coeff, self.wavelet, mode=self.mode)[..., :seq_len])\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSFreqWarp(Transform):\n    ""Applies warp based on a wavelet decomposition""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, wavelet=\'db4\', level=2, mode=\'constant\', **kwargs):\n        self.magnitude, self.ex = magnitude, ex\n        self.wavelet, self.level, self.mode = wavelet, level, mode\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        self.level = 1 if self.level is None else self.level\n        new_x = random_cum_noise_generator(o[:o.shape[-1] // 2], magnitude=self.magnitude)\n        coeff = pywt.wavedec(o.cpu(), self.wavelet, mode=self.mode, level=self.level)\n        coeff[1:] = [CubicSpline(np.arange(c.shape[-1]), c, axis=-1)(new_x[:c.shape[-1]]) for c in coeff[1:]]\n        output = o.new(pywt.waverec(coeff, self.wavelet, mode=self.mode)[..., :seq_len])\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output\n\n# Cell\nclass TSFreqScale(Transform):\n    ""Modifies the scale based on a wavelet decomposition""\n    order = 90\n    def __init__(self, magnitude=.1, ex=None, wavelet=\'db4\', level=2, mode=\'constant\', **kwargs):\n        self.magnitude, self.ex = magnitude, ex\n        self.wavelet, self.level, self.mode = wavelet, level, mode\n    def encodes(self, o: TSTensor):\n        if self.magnitude <= 0: return o\n        seq_len = o.shape[-1]\n        self.level = 1 if self.level is None else self.level\n        coeff = pywt.wavedec(o.cpu(), self.wavelet, mode=self.mode, level=self.level)\n        coeff[1:] = [c * (1 + 2 * (np.random.rand() - .5) * self.magnitude) for c in coeff[1:]]\n        output = o.new(pywt.waverec(coeff, self.wavelet, mode=self.mode)[..., :seq_len])\n        if self.ex is not None: output[...,self.ex,:] = o[...,self.ex,:]\n        return output'"
tsai/data/validation.py,0,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/000b_data.validation.ipynb (unless otherwise specified).\n\n__all__ = [\'TrainValTestSplitter\', \'get_splits\', \'check_overlap\', \'leakage_finder\', \'get_predefined_splits\',\n           \'combine_split_data\']\n\n# Cell\nfrom ..imports import *\nfrom ..utils import *\n\n# Cell\nfrom sklearn.model_selection import train_test_split\n\n# Cell\ndef TrainValTestSplitter(valid_size=0.2, test_size=0.2, random_state=None, stratify=None, **kwargs):\n    ""Split `items` into random train, valid and test subsets using sklearn train_test_split utility.""\n    valid_size = valid_size / (1 - test_size)\n    stratify1 = None if stratify is None else stratify\n    def _inner(o, **kwargs):\n        train_valid, test = train_test_split(range(len(o)), test_size=test_size, random_state=random_state, stratify=stratify1, **kwargs)\n        stratify2 = None if stratify1 is None else stratify1[train_valid]\n        train, valid = train_test_split(range(len(train_valid)), test_size=valid_size, random_state=random_state, stratify=stratify2, **kwargs)\n        return L(L(train_valid)[train]), L(L(train_valid)[valid]), L(test)\n    return _inner\n\ndef get_splits(y, valid_size=0.2, test_size=0.2, stratify=True, merge_train_valid=False, merge_train_test=False, train_perc=None,\n               random_state=None, **kwargs):\n    if test_size == 0: splits = L(TrainTestSplitter(test_size=valid_size, random_state=random_state, stratify=y if stratify else None)(y))\n    else: splits = L(TrainValTestSplitter(valid_size=valid_size, test_size=test_size, random_state=random_state, stratify=y if stratify else None)(y))\n    if merge_train_valid: splits[0] = concat(splits[0], splits[1])\n    if merge_train_test: splits[0] = concat(splits[0], splits[2])\n    if train_perc: splits[0] = L(np.random.choice(len(train), int(len(train) * train_perc), False))\n    leakage_finder(*splits)\n    return splits\n\ndef check_overlap(a, b):\n    overlap = L([i for i in a if i in b])\n    if overlap == []: return\n    return overlap\n\ndef leakage_finder(train, val, test=None):\n    if check_overlap(train, val) is not None:\n        print(\'train-val leakage!\')\n        print(check_overlap(train, val), \'\\n\')\n    if test is not None:\n        if check_overlap(train, test) is not None:\n            print(\'train-test leakage!\')\n            print(check_overlap(train, test), \'\\n\')\n        if check_overlap(val, test) is not None:\n            print(\'val-test leakage!\')\n            print(check_overlap(val, test), \'\\n\')\n\n# Cell\ndef get_predefined_splits(*xs):\n    \'\'\'xs is a list with X_train, X_valid, ...\'\'\'\n    splits_ = []\n    start = 0\n    for x in xs:\n        splits_.append(L(list(np.arange(start, start + len(x)))))\n        start += len(x)\n    return tuple(splits_)\n\ndef combine_split_data(xs, ys=None):\n    \'\'\'xs is a list with X_train, X_valid, .... ys is None or a list with y_train, y_valid, .... \'\'\'\n    xs = [to3d(x) for x in xs]\n    splits = get_predefined_splits(*xs)\n    if ys is None: return concat(*xs), None, splits\n    else: return concat(*xs), concat(*ys), splits'"
tsai/models/FCN.py,0,"b""# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/103_FCN.ipynb (unless otherwise specified).\n\n__all__ = ['FCN']\n\n# Cell\nfrom ..imports import *\nfrom .layers import *\n\n# Cell\nclass FCN(Module):\n    def __init__(self, c_in, c_out, layers=[128, 256, 128], kss=[7, 5, 3]):\n        self.conv1 = Conv1d(c_in, layers[0], kss[0], padding='same', act_fn='relu')\n        self.conv2 = Conv1d(layers[0], layers[1], kss[1], padding='same', act_fn='relu')\n        self.conv3 = Conv1d(layers[1], layers[2], kss[2], padding='same', act_fn='relu')\n        self.gap = nn.AdaptiveAvgPool1d(1)\n        self.squeeze = Squeeze(-1)\n        self.fc = nn.Linear(layers[-1], c_out)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.squeeze(self.gap(x))\n        return self.fc(x)"""
tsai/models/InceptionTime.py,2,"b""# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/102_InceptionTime.ipynb (unless otherwise specified).\n\n__all__ = ['shortcut', 'Inception', 'InceptionBlock', 'InceptionTime']\n\n# Cell\nfrom ..imports import *\nfrom .layers import *\n\n# Cell\n# This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com based on:\n\n# Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2019).\n# InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\n# Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n\n\ndef shortcut(c_in, c_out):\n    return nn.Sequential(*[nn.Conv1d(c_in, c_out, kernel_size=1), nn.BatchNorm1d(c_out)])\n\n\nclass Inception(Module):\n    def __init__(self, c_in, bottleneck=32, ks=40, nb_filters=32):\n        self.bottleneck = nn.Conv1d(c_in, bottleneck, 1) if bottleneck and c_in > 1 else noop\n        mts_feat = bottleneck or c_in\n        conv_layers = []\n        kss = [ks // (2**i) for i in range(3)]\n        # ensure odd kss until nn.Conv1d with padding='same' is available in pytorch\n        kss = [ksi if ksi % 2 != 0 else ksi - 1 for ksi in kss]\n        for i in range(len(kss)): conv_layers.append(nn.Conv1d(mts_feat, nb_filters, kernel_size=kss[i], padding=kss[i]//2))\n        self.conv_layers = nn.ModuleList(conv_layers)\n        self.maxpool = nn.MaxPool1d(3, stride=1, padding=1)\n        self.conv = nn.Conv1d(c_in, nb_filters, kernel_size=1)\n        self.bn = nn.BatchNorm1d(nb_filters * 4)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        input_tensor = x\n        x = self.bottleneck(input_tensor)\n        for i in range(3):\n            out_ = self.conv_layers[i](x)\n            if i == 0: out = out_\n            else: out = torch.cat((out, out_), 1)\n        mp = self.conv(self.maxpool(input_tensor))\n        inc_out = torch.cat((out, mp), 1)\n        return self.act(self.bn(inc_out))\n\n\nclass InceptionBlock(Module):\n    def __init__(self,c_in,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n        self.residual = residual\n        self.depth = depth\n\n        #inception & residual layers\n        inc_mods = []\n        res_layers = []\n        res = 0\n        for d in range(depth):\n            inc_mods.append(Inception(c_in if d == 0 else nb_filters * 4, bottleneck=bottleneck if d > 0 else 0,ks=ks, nb_filters=nb_filters))\n            if self.residual and d % 3 == 2:\n                res_layers.append(shortcut(c_in if res == 0 else nb_filters * 4, nb_filters * 4))\n                res += 1\n            else: res_layer = res_layers.append(None)\n        self.inc_mods = nn.ModuleList(inc_mods)\n        self.res_layers = nn.ModuleList(res_layers)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        res = x\n        for d, l in enumerate(range(self.depth)):\n            x = self.inc_mods[d](x)\n            if self.residual and d % 3 == 2:\n                res = self.res_layers[d](res)\n                x += res\n                res = x\n                x = self.act(x)\n        return x\n\nclass InceptionTime(Module):\n    def __init__(self,c_in,c_out,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n        self.block = InceptionBlock(c_in, bottleneck=bottleneck, ks=ks, nb_filters=nb_filters, residual=residual, depth=depth)\n        self.gap = nn.AdaptiveAvgPool1d(1)\n        self.squeeze = Squeeze(-1)\n        self.fc = nn.Linear(nb_filters * 4, c_out)\n\n    def forward(self, x):\n        x = self.block(x)\n        x = self.squeeze(self.gap(x))\n        x = self.fc(x)\n        return x"""
tsai/models/ResCNN.py,0,"b""# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/104_ResCNN.ipynb (unless otherwise specified).\n\n__all__ = ['Block', 'ResCNN']\n\n# Cell\nfrom ..imports import *\nfrom .layers import *\n\n# Cell\nclass Block(Module):\n    def __init__(self, ni, nf, ks=[7, 5, 3], act_fn='relu'):\n        self.conv1 = Conv1d(ni, nf, ks[0], padding='same', act_fn=act_fn)\n        self.conv2 = Conv1d(nf, nf, ks[1], padding='same', act_fn=act_fn)\n        self.conv3 = Conv1d(nf, nf, ks[2], padding='same', act_fn=False)\n\n        # expand channels for the sum if necessary\n        self.shortcut = noop if ni == nf else Conv1d(ni, nf, ks=1, act_fn=False)\n        self.act_fn = get_act_layer(act_fn)\n\n    def forward(self, x):\n        res = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        sc = self.shortcut(res)\n        x += sc\n        x = self.act_fn(x)\n        return x\n\n\nclass ResCNN(Module):\n    def __init__(self, c_in, c_out):\n        nf = 64\n        self.block = Block(c_in, nf, ks=[7, 5, 3], act_fn='relu')\n        self.conv1 = Conv1d(nf, nf * 2, ks=3, padding='same', act_fn='leakyrelu', act_kwargs={'negative_slope':.2})\n        self.conv2 = Conv1d(nf * 2, nf * 4, ks=3, padding='same', act_fn='prelu')\n        self.conv3 = Conv1d(nf * 4, nf * 2, ks=3, padding='same', act_fn='elu', act_kwargs={'alpha':.3})\n        self.gap = nn.AdaptiveAvgPool1d(1)\n        self.lin = nn.Linear(nf * 2, c_out)\n\n    def forward(self, x):\n        x = self.block(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.gap(x).squeeze(-1)\n        return self.lin(x)"""
tsai/models/ResNet.py,0,"b""# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/101_ResNet.ipynb (unless otherwise specified).\n\n__all__ = ['ResBlock', 'ResNet']\n\n# Cell\nfrom ..imports import *\nfrom .layers import *\n\n# Cell\nclass ResBlock(Module):\n    def __init__(self, ni, nf, ks=[7, 5, 3]):\n        self.conv1 = Conv1d(ni, nf, ks[0], padding='same', act_fn='relu')\n        self.conv2 = Conv1d(nf, nf, ks[1], padding='same', act_fn='relu')\n        self.conv3 = Conv1d(nf, nf, ks[2], padding='same', act_fn='relu')\n\n        # expand channels for the sum if necessary\n        self.shortcut = noop if ni == nf else Conv1d(ni, nf, ks=1, act_fn=False)\n        self.act_fn = nn.ReLU()\n\n    def forward(self, x):\n        res = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        sc = self.shortcut(res)\n        x += sc\n        x = self.act_fn(x)\n        return x\n\nclass ResNet(Module):\n    def __init__(self,c_in, c_out):\n        nf = 64\n        self.block1 = ResBlock(c_in, nf, ks=[7, 5, 3])\n        self.block2 = ResBlock(nf, nf * 2, ks=[7, 5, 3])\n        self.block3 = ResBlock(nf * 2, nf * 2, ks=[7, 5, 3])\n        self.gap = nn.AdaptiveAvgPool1d(1)\n        self.squeeze = Squeeze(-1)\n        self.fc = nn.Linear(nf * 2, c_out)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.squeeze(self.gap(x))\n        return self.fc(x)"""
tsai/models/__init__.py,0,b''
tsai/models/all.py,0,b'from .layers import *\nfrom .utils import *\nfrom .ResNet import *\nfrom .InceptionTime import *\nfrom .FCN import *\nfrom .ResCNN import *'
tsai/models/layers.py,6,"b'# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/100_layers.ipynb (unless otherwise specified).\n\n__all__ = [\'noop\', \'mish\', \'Mish\', \'get_act_layer\', \'same_padding1d\', \'Pad1d\', \'Conv1dSame\', \'Chomp1d\', \'Conv1dCausal\',\n           \'Conv1d\', \'CoordConv1D\', \'LambdaPlus\', \'Flatten\', \'Squeeze\', \'Unsqueeze\', \'YRange\', \'Temp\']\n\n# Cell\nfrom fastai2.torch_core import Module\nfrom ..imports import *\n\n# Cell\ndef noop(x): return x\n\n# Cell\n# Misra, D. (2019). Mish: A Self Regularized Non-Monotonic Neural Activation Function. arXiv preprint arXiv:1908.08681.\n# https://arxiv.org/abs/1908.08681\n# GitHub: https://github.com/digantamisra98/Mish\n@torch.jit.script\ndef mish(input):\n    \'\'\'Applies the mish function element-wise: mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\'\'\'\n    return input * torch.tanh(F.softplus(input))\n\nclass Mish(Module):\n    def forward(self, input):\n        return mish(input)\n\n# Cell\ndef get_act_layer(act_fn, act_kwargs={}):\n    act_fn = act_fn.lower()\n    assert act_fn in [\'relu\', \'leakyrelu\', \'prelu\', \'elu\', \'mish\', \'swish\'], \'incorrect act_fn\'\n    if act_fn == \'relu\': return nn.ReLU()\n    elif act_fn == \'leakyrelu\': return nn.LeakyReLU(**act_kwargs)\n    elif act_fn == \'prelu\': return nn.PReLU(**act_kwargs)\n    elif act_fn == \'elu\': return nn.ELU(**act_kwargs)\n    elif act_fn == \'mish\': return Mish()\n    elif act_fn == \'swish\': return Swish()\n\n# Cell\ndef same_padding1d(seq_len, ks, stride=1, dilation=1):\n    effective_ks = (ks - 1) * dilation + 1\n    out_dim = (seq_len + stride - 1) // stride\n    p = max(0, (out_dim - 1) * stride + effective_ks - seq_len)\n    padding_before = p // 2\n    padding_after = p - padding_before\n    return padding_before, padding_after\n\n\nclass Pad1d(nn.ConstantPad1d):\n    def __init__(self, padding, value=0.):\n        super().__init__(padding, value)\n\n\nclass Conv1dSame(Module):\n    ""Conv1d with padding=\'same\'""\n\n    def __init__(self, c_in, c_out, ks=3, stride=1, dilation=1, **kwargs):\n        self.ks, self.stride, self.dilation = ks, stride, dilation\n        self.conv1d_same = nn.Conv1d(c_in, c_out, ks, stride=stride, dilation=dilation, **kwargs)\n        self.pad = Pad1d\n\n\n    def forward(self, x):\n        self.padding = same_padding1d(x.shape[-1],self.ks,stride=self.stride,dilation=self.dilation)\n        return self.conv1d_same(self.pad(self.padding)(x))\n\n# Cell\n# https://github.com/locuslab/TCN/blob/master/TCN/tcn.py\nclass Chomp1d(Module):\n    def __init__(self, chomp_size):\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass Conv1dCausal(Module):\n    def __init__(self, c_in, c_out, ks, stride=1, dilation=1, **kwargs):\n        padding = (ks - 1) * dilation\n        self.conv = nn.Conv1d(c_in, c_out, ks, stride=stride, padding=padding, dilation=dilation, **kwargs)\n        self.chomp = Chomp1d(math.ceil(padding / stride))\n\n    def forward(self, x):\n        return self.chomp(self.conv(x))\n\n# Cell\ndef Conv1d(c_in, c_out, ks=3, stride=1, padding=\'same\', dilation=1, bias=True, act_fn=\'relu\', act_kwargs={},\n           bn_before_conv=False, bn_before_act=True, bn_after_act=False, zero_bn=False, **kwargs):\n    \'\'\'conv1d with default padding=\'same\', bn and act_fn (default = \'relu\')\'\'\'\n    layers = []\n    if bn_before_conv: layers.append(nn.BatchNorm1d(c_in))\n    if padding == \'same\': layers.append(Conv1dSame(c_in, c_out, ks, stride=stride, dilation=dilation, bias=bias, **kwargs))\n    elif padding == \'causal\': layers.append(Conv1dCausal(c_in, c_out, ks, stride=stride, dilation=dilation, bias=bias, **kwargs))\n    else:\n        if padding == \'valid\': padding = 0\n        layers.append(nn.Conv1d(c_in, c_out, ks, stride=stride, padding=padding, dilation=dilation, bias=bias, **kwargs))\n    if bn_before_act: layers.append(nn.BatchNorm1d(c_out))\n    if act_fn: layers.append(get_act_layer(act_fn, act_kwargs))\n    if bn_after_act:\n        bn = nn.BatchNorm1d(c_out)\n        nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n        layers.append(bn)\n    return nn.Sequential(*layers)\n\n# Cell\nclass CoordConv1D(Module):\n    def forward(self, x):\n        bs, _, seq_len = x.size()\n        cc = torch.arange(seq_len, device=device, dtype=torch.float) / (seq_len - 1)\n        cc = cc * 2 - 1\n        cc = cc.repeat(bs, 1, 1)\n        x = torch.cat([x, cc], dim=1)\n        return x\n\n# Cell\nclass LambdaPlus(Module):\n    def __init__(self, func, *args, **kwargs): self.func,self.args,self.kwargs=func,args,kwargs\n    def forward(self, x): return self.func(x, *self.args, **self.kwargs)\n\n# Cell\nclass Flatten(Module):\n    def forward(self, x): return x.view(x.size(0), -1)\n\nclass Squeeze(Module):\n    def __init__(self, dim=-1):\n        self.dim = dim\n    def forward(self, x): return x.squeeze(dim=self.dim)\n\nclass Unsqueeze(Module):\n    def __init__(self, dim=-1):\n        self.dim = dim\n    def forward(self, x): return x.unsqueeze(dim=self.dim)\n\nclass YRange(Module):\n    def __init__(self, y_range:tuple):\n        self.y_range = y_range\n        self.sigmoid = torch.sigmoid\n    def forward(self, x):\n        x = self.sigmoid(x)\n        return x * (self.y_range[1] - self.y_range[0]) + self.y_range[0]\n\nclass Temp(Module):\n    def __init__(self, temp):\n        self.temp = float(temp)\n        self.temp = nn.Parameter(torch.Tensor(1).fill_(self.temp).to(device))\n    def forward(self, x):\n        return x.div_(self.temp)'"
tsai/models/utils.py,0,"b""# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/100b_models_utils.ipynb (unless otherwise specified).\n\n__all__ = ['get_layers', 'count_params']\n\n# Cell\nfrom ..imports import *\n\n# Cell\ndef get_layers(model, cond=noop):\n    if isinstance(model, Learner): model=model.model\n    return [m for m in flatten_model(model) if any([c(m) for c in L(cond)])]\n\ndef count_params(model):\n    if isinstance(model, Learner): model = model.model\n    count = 0\n    for l in get_layers(model):\n        for i in range(len(list(l.parameters()))):\n            count += len(list(l.parameters())[i].data.flatten())\n    return count"""
