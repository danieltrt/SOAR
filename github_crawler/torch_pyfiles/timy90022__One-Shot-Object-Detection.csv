file_path,api_count,code
_init_paths.py,0,"b""import os.path as osp\nimport sys\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nthis_dir = osp.dirname(__file__)\n\n# Add lib to PYTHONPATH\nlib_path = osp.join(this_dir, 'lib')\nadd_path(lib_path)\n\ncoco_path = osp.join(this_dir, 'data', 'coco', 'PythonAPI')\nadd_path(coco_path)\n"""
test_compare.py,19,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport pandas as pd\n\nimport _init_paths\nimport os\nimport sys\nimport numpy as np\nimport argparse\nimport pprint\nimport pdb\nimport time\nimport cv2\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nimport pickle\nfrom roi_data_layer.roidb import combined_roidb\nfrom roi_data_layer.roibatchLoader import roibatchLoader\nfrom model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\nfrom model.rpn.bbox_transform import clip_boxes\n# from model.nms.nms_wrapper import nms\nfrom model.roi_layers import nms\nfrom model.rpn.bbox_transform import bbox_transform_inv\nfrom model.utils.net_utils import save_net, load_net, vis_detections\nfrom model.faster_rcnn.vgg16 import vgg16\nfrom model.faster_rcnn.resnet import resnet\n\nimport pdb\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\ndef save_weight(weight, time, seen):\n  time = np.where(time==0, 1, time)\n  weight = weight/time[:,np.newaxis]\n  result_map = np.zeros((len(weight), len(weight)))\n  for i in range(len(weight)):\n    for j in range(len(weight)):\n      v1 = weight[i]\n      v2 = weight[j]\n      # v1_ = np.linalg.norm(v1)\n      # v2_ = np.linalg.norm(v2)\n      # v12 = np.sum(v1*v2)\n      # print(v12)\n      # print(v1_)\n      # print(v2_)\n      distance = np.linalg.norm(v1-v2)\n      if np.sum(v1*v2)== 0 :\n        result_map[i][j] = 0\n      else:\n        result_map[i][j] = distance\n      \n\n  df = pd.DataFrame (result_map)\n\n  ## save to xlsx file\n\n  filepath = \'similarity_%d.xlsx\'%(seen)\n\n  df.to_excel(filepath, index=False)\n\n  weight = weight*255\n\n\n  cv2.imwrite(\'./weight_%d.png\'%(seen), weight)\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Train a Fast R-CNN network\')\n  parser.add_argument(\'--dataset\', dest=\'dataset\',\n                      help=\'training dataset\',\n                      default=\'coco\', type=str)\n  parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                      help=\'optional config file\',\n                      default=\'cfgs/res101.yml\', type=str)\n  parser.add_argument(\'--net\', dest=\'net\',\n                      help=\'vgg16, res50, res101, res152\',\n                      default=\'res50\', type=str)\n  parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                      help=\'set config keys\', default=None,\n                      nargs=argparse.REMAINDER)\n  parser.add_argument(\'--load_dir\', dest=\'load_dir\',\n                      help=\'directory to load models\', default=""models"",\n                      type=str)\n  parser.add_argument(\'--cuda\', dest=\'cuda\',\n                      help=\'whether use CUDA\',\n                      default=True)\n  parser.add_argument(\'--ls\', dest=\'large_scale\',\n                      help=\'whether use large imag scale\',\n                      action=\'store_true\')\n  parser.add_argument(\'--mGPUs\', dest=\'mGPUs\',\n                      help=\'whether use multiple GPUs\',\n                      default=True)\n  parser.add_argument(\'--cag\', dest=\'class_agnostic\',\n                      help=\'whether perform class_agnostic bbox regression\',\n                      default=True)\n  parser.add_argument(\'--parallel_type\', dest=\'parallel_type\',\n                      help=\'which part of model to parallel, 0: all, 1: model before roi pooling\',\n                      default=0, type=int)\n  parser.add_argument(\'--s\', dest=\'checksession\',\n                      help=\'checksession to load model\',\n                      default=1, type=int)\n  parser.add_argument(\'--checkepoch\', dest=\'checkepoch\',\n                      help=\'checkepoch to load network\',\n                      default=10, type=int)\n  parser.add_argument(\'--p\', dest=\'checkpoint\',\n                      help=\'checkpoint to load network\',\n                      default=1663, type=int)\n  parser.add_argument(\'--vis\', dest=\'vis\',\n                      help=\'visualization mode\',\n                      action=\'store_true\')\n  parser.add_argument(\'--seen\', dest=\'seen\',default=2, type=int)\n  parser.add_argument(\'--a\', dest=\'average\',default=1, type=int)\n  parser.add_argument(\'--g\', dest=\'group\',\n                      help=\'which group\',\n                      default=0) \n  args = parser.parse_args()\n  return args\n\nlr = cfg.TRAIN.LEARNING_RATE\nmomentum = cfg.TRAIN.MOMENTUM\nweight_decay = cfg.TRAIN.WEIGHT_DECAY\n\nif __name__ == \'__main__\':\n\n  args = parse_args()\n\n  print(\'Called with args:\')\n  print(args)\n\n  if torch.cuda.is_available() and not args.cuda:\n    print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\n  np.random.seed(cfg.RNG_SEED)\n  if args.dataset == ""pascal_voc"":\n      args.imdb_name = ""voc_2007_trainval""\n      args.imdbval_name = ""voc_2007_test""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n  elif args.dataset == ""pascal_voc_0712"":\n      args.imdb_name = ""voc_2007_trainval+voc_2012_trainval""\n      args.imdbval_name = ""voc_2007_test""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n  elif args.dataset == ""coco"":\n      args.imdb_name = ""coco_2017_train""\n      args.imdbval_name = ""coco_2017_val""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[4, 8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n  elif args.dataset == ""imagenet"":\n      args.imdb_name = ""imagenet_train""\n      args.imdbval_name = ""imagenet_val""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n  elif args.dataset == ""vg"":\n      args.imdb_name = ""vg_150-50-50_minitrain""\n      args.imdbval_name = ""vg_150-50-50_minival""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[4, 8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n\n  # args.cfg_file = ""cfgs/{}_ls.yml"".format(args.net) if args.large_scale else ""cfgs/{}.yml"".format(args.net)\n  args.cfg_file = ""cfgs/{}_{}.yml"".format(args.net, args.group) if args.group != 0 else ""cfgs/{}.yml"".format(args.net)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n\n  cfg.TRAIN.USE_FLIPPED = False\n  # imdb_vs, roidb_vs, ratio_list_vs, ratio_index_vs, query_vs = combined_roidb(\'coco_2014_valminusminival\', False)\n  imdb_vu, roidb_vu, ratio_list_vu, ratio_index_vu, query_vu = combined_roidb(args.imdbval_name, False, seen=args.seen)\n  # imdb_vs.competition_mode(on=True)\n  imdb_vu.competition_mode(on=True)\n\n\n\n\n  input_dir = args.load_dir + ""/"" + args.net + ""/"" + args.dataset\n  if not os.path.exists(input_dir):\n    raise Exception(\'There is no input directory for loading network from \' + input_dir)\n  load_name = os.path.join(input_dir,\n    \'faster_rcnn_{}_{}_{}.pth\'.format(args.checksession, args.checkepoch, args.checkpoint))\n\n  # initilize the network here.\n  if args.net == \'vgg16\':\n    fasterRCNN = vgg16(imdb_vu.classes, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == \'res101\':\n    fasterRCNN = resnet(imdb_vu.classes, 101, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == \'res50\':\n    fasterRCNN = resnet(imdb_vu.classes, 50, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == \'res152\':\n    fasterRCNN = resnet(imdb_vu.classes, 152, pretrained=False, class_agnostic=args.class_agnostic)\n  else:\n    print(""network is not defined"")\n    pdb.set_trace()\n\n  fasterRCNN.create_architecture()\n\n  print(""load checkpoint %s"" % (load_name))\n  checkpoint = torch.load(load_name)\n  fasterRCNN.load_state_dict(checkpoint[\'model\'])\n  if \'pooling_mode\' in checkpoint.keys():\n    cfg.POOLING_MODE = checkpoint[\'pooling_mode\']\n\n\n  print(\'load model successfully!\')\n  # initilize the tensor holder here.\n  im_data = torch.FloatTensor(1)\n  query   = torch.FloatTensor(1)\n  im_info = torch.FloatTensor(1)\n  catgory = torch.LongTensor(1)\n  gt_boxes = torch.FloatTensor(1)\n\n  # ship to cuda\n  if args.cuda:\n    im_data = im_data.cuda()\n    query = query.cuda()\n    im_info = im_info.cuda()\n    catgory = catgory.cuda()\n    gt_boxes = gt_boxes.cuda()\n\n  # make variable\n  im_data = Variable(im_data)\n  query = Variable(query)\n  im_info = Variable(im_info)\n  catgory = Variable(catgory)\n  gt_boxes = Variable(gt_boxes)\n\n  if args.cuda:\n    cfg.CUDA = True\n\n  if args.cuda:\n    fasterRCNN.cuda()\n\n  start = time.time()\n  max_per_image = 100\n\n  vis = args.vis\n\n  if vis:\n    thresh = 0.05\n  else:\n    thresh = 0.0\n\n  save_name = \'faster_rcnn_10\'\n\n\n  # output_dir_vs = get_output_dir(imdb_vs, \'faster_rcnn_seen\')\n  output_dir_vu = get_output_dir(imdb_vu, \'faster_rcnn_unseen\')\n  all_weight = np.zeros((len(ratio_index_vu[0]),1024))\n  all_times = np.zeros((imdb_vu.num_classes))\n\n\n\n  dataset_vu = roibatchLoader(roidb_vu, ratio_list_vu, ratio_index_vu, query_vu, 1, imdb_vu.num_classes, training=False, seen=args.seen)\n  fasterRCNN.eval()\n  all_ap = []\n  for avg in range(args.average):\n    dataset_vu.query_position = avg\n    dataloader_vu = torch.utils.data.DataLoader(dataset_vu, batch_size=1,shuffle=False, num_workers=0,pin_memory=True)\n\n    data_iter_vu = iter(dataloader_vu)\n    num_images_vu = len(imdb_vu.image_index)\n    num_detect = len(ratio_index_vu[0])\n\n    all_boxes = [[[] for _ in xrange(num_images_vu)]\n                for _ in xrange(imdb_vu.num_classes)]\n    \n    _t = {\'im_detect\': time.time(), \'misc\': time.time()}\n    det_file = os.path.join(output_dir_vu, \'detections_%d_%d.pkl\'%(args.seen, avg))\n    print(det_file)\n    \n    \n    for i,index in enumerate(ratio_index_vu[0]):\n      \n      data = next(data_iter_vu)\n      im_data.data.resize_(data[0].size()).copy_(data[0])\n      query.data.resize_(data[1].size()).copy_(data[1])\n      im_info.data.resize_(data[2].size()).copy_(data[2])\n      gt_boxes.data.resize_(data[3].size()).copy_(data[3])\n      catgory.data.resize_(data[4].size()).copy_(data[4])\n\n      det_tic = time.time()\n      rois, cls_prob, bbox_pred, \\\n      rpn_loss_cls, rpn_loss_box, \\\n      RCNN_loss_cls, _, RCNN_loss_bbox, \\\n      rois_label, weight = fasterRCNN(im_data, query, im_info, gt_boxes, catgory)\n\n      # all_weight[data[4],:] = all_weight[data[4],:] + weight.view(-1).detach().cpu().numpy()\n      all_weight[i,:] = weight.view(-1).detach().cpu().numpy()\n      all_times[data[4]] = all_times[data[4]] + 1\n\n      \n\n      scores = cls_prob.data\n      boxes = rois.data[:, :, 1:5]\n\n      if cfg.TEST.BBOX_REG:\n          # Apply bounding-box regression deltas\n          box_deltas = bbox_pred.data\n          if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n          # Optionally normalize targets by a precomputed mean and stdev\n            if args.class_agnostic:\n                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                box_deltas = box_deltas.view(1, -1, 4)\n            else:\n                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n\n          pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n          pred_boxes = clip_boxes(pred_boxes, im_info.data, 1)\n      else:\n          # Simply repeat the boxes, once for each class\n          pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\n      pred_boxes /= data[2][0][2].item()\n\n      scores = scores.squeeze()\n      pred_boxes = pred_boxes.squeeze()\n      det_toc = time.time()\n      detect_time = det_toc - det_tic\n      misc_tic = time.time()\n      if vis and i%1==0:\n        print(i)\n        im = cv2.imread(dataset_vu._roidb[dataset_vu.ratio_index[i]][\'image\'])\n        im2show = np.copy(im)\n\n      inds = torch.nonzero(scores>thresh).view(-1)\n      # if there is det\n      if inds.numel() > 0:\n        cls_scores = scores[inds]\n        _, order = torch.sort(cls_scores, 0, True)\n        cls_boxes = pred_boxes[inds, :]\n        \n        cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n        # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n        cls_dets = cls_dets[order]\n        keep = nms(cls_boxes[order, :], cls_scores[order], cfg.TEST.NMS)\n        cls_dets = cls_dets[keep.view(-1).long()]\n        all_boxes[data[4]][index] = cls_dets.cpu().numpy()\n        if vis:\n          im2show = vis_detections(im2show, \'shot\', cls_dets.cpu().numpy(), 0.8)\n\n      # Limit to max_per_image detections *over all classes*\n      if max_per_image > 0:\n        try:\n          image_scores = all_boxes[data[4]][index][:,-1]\n          if len(image_scores) > max_per_image:\n              image_thresh = np.sort(image_scores)[-max_per_image]\n\n              keep = np.where(all_boxes[data[4]][index][:,-1] >= image_thresh)[0]\n              all_boxes[data[4]][index] = all_boxes[data[4]][index][keep, :]\n        except:\n          pass\n\n      misc_toc = time.time()\n      nms_time = misc_toc - misc_tic\n\n      sys.stdout.write(\'im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r\' \\\n          .format(i + 1, num_detect, detect_time, nms_time))\n      sys.stdout.flush()\n\n      if vis and i%1==0:\n        o_query = data[1][0].permute(1, 2,0).contiguous().cpu().numpy()\n        o_query *= [0.229, 0.224, 0.225]\n        o_query += [0.485, 0.456, 0.406]\n        o_query *= 255\n        o_query = o_query[:,:,::-1]\n\n        (h,w,c) = im2show.shape\n        o_query = cv2.resize(o_query, (h, h),interpolation=cv2.INTER_LINEAR)\n        # im2show = np.concatenate((im2show, o_query), axis=1)\n\n        cv2.imwrite(\'./test_img/%d.png\'%(i), im2show)\n    \n  \n    with open(det_file, \'wb\') as f:\n        print(\'hi\')\n        pickle.dump([all_boxes, all_weight, all_times], f, pickle.HIGHEST_PROTOCOL)\n\n    with open(det_file, \'rb\') as fid:\n        [all_boxes, all_weight, all_times] = pickle.load(fid)\n    save_weight(all_weight, all_times, args.seen)\n\n    print(\'Evaluating detections\')\n    aps = imdb_vu.evaluate_detections(all_boxes, output_dir_vu) \n    all_ap.append(aps)\n    print(aps)\n    print(all_ap)\n\n    end = time.time()\n    print(""test time: %0.4fs"" % (end - start))\n  \n  all_ap = np.array(all_ap)\n  print(np.mean(all_ap, axis=0))'"
test_net.py,18,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport pandas as pd\n\nimport _init_paths\nimport os\nimport sys\nimport numpy as np\nimport argparse\nimport pprint\nimport pdb\nimport time\nimport cv2\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nimport pickle\nfrom roi_data_layer.roidb import combined_roidb\nfrom roi_data_layer.roibatchLoader import roibatchLoader\nfrom model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\nfrom model.rpn.bbox_transform import clip_boxes\n# from model.nms.nms_wrapper import nms\nfrom model.roi_layers import nms\nfrom model.rpn.bbox_transform import bbox_transform_inv\nfrom model.utils.net_utils import save_net, load_net, vis_detections\nfrom model.faster_rcnn.vgg16 import vgg16\nfrom model.faster_rcnn.resnet import resnet\n\nimport pdb\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\ndef save_weight(weight, time, seen):\n  time = np.where(time==0, 1, time)\n  weight = weight/time[:,np.newaxis]\n  result_map = np.zeros((len(weight), len(weight)))\n  for i in range(len(weight)):\n    for j in range(len(weight)):\n      v1 = weight[i]\n      v2 = weight[j]\n      # v1_ = np.linalg.norm(v1)\n      # v2_ = np.linalg.norm(v2)\n      # v12 = np.sum(v1*v2)\n      # print(v12)\n      # print(v1_)\n      # print(v2_)\n      distance = np.linalg.norm(v1-v2)\n      if np.sum(v1*v2)== 0 :\n        result_map[i][j] = 0\n      else:\n        result_map[i][j] = distance\n      \n\n  df = pd.DataFrame (result_map)\n\n  ## save to xlsx file\n\n  filepath = \'similarity_%d.xlsx\'%(seen)\n\n  df.to_excel(filepath, index=False)\n\n  weight = weight*255\n\n\n  cv2.imwrite(\'./weight_%d.png\'%(seen), weight)\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Train a Fast R-CNN network\')\n  parser.add_argument(\'--dataset\', dest=\'dataset\',\n                      help=\'training dataset\',\n                      default=\'coco\', type=str)\n  parser.add_argument(\'--net\', dest=\'net\',\n                      help=\'vgg16, res50, res101, res152\',\n                      default=\'res50\', type=str)\n  parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                      help=\'set config keys\', default=None,\n                      nargs=argparse.REMAINDER)\n  parser.add_argument(\'--load_dir\', dest=\'load_dir\',\n                      help=\'directory to load models\', default=""models"",\n                      type=str)\n  parser.add_argument(\'--cuda\', dest=\'cuda\',\n                      help=\'whether use CUDA\',\n                      action=\'store_true\')\n  parser.add_argument(\'--ls\', dest=\'large_scale\',\n                      help=\'whether use large imag scale\',\n                      action=\'store_true\')\n  parser.add_argument(\'--mGPUs\', dest=\'mGPUs\',\n                      help=\'whether use multiple GPUs\',\n                      default=True)\n  parser.add_argument(\'--cag\', dest=\'class_agnostic\',\n                      help=\'whether perform class_agnostic bbox regression\',\n                      default=True)\n  parser.add_argument(\'--parallel_type\', dest=\'parallel_type\',\n                      help=\'which part of model to parallel, 0: all, 1: model before roi pooling\',\n                      default=0, type=int)\n  parser.add_argument(\'--s\', dest=\'checksession\',\n                      help=\'checksession to load model\',\n                      default=1, type=int)\n  parser.add_argument(\'--checkepoch\', dest=\'checkepoch\',\n                      help=\'checkepoch to load network\',\n                      default=10, type=int)\n  parser.add_argument(\'--p\', dest=\'checkpoint\',\n                      help=\'checkpoint to load network\',\n                      default=1663, type=int)\n  parser.add_argument(\'--vis\', dest=\'vis\',\n                      help=\'visualization mode\',\n                      action=\'store_true\')\n  parser.add_argument(\'--seen\', dest=\'seen\',\n                       help=\'Reserved: 1 training, 2 testing, 3 both\', default=2, type=int)\n  parser.add_argument(\'--a\', dest=\'average\', help=\'average the top_k candidate samples\', default=1, type=int)\n  parser.add_argument(\'--g\', dest=\'group\',\n                      help=\'which group want to training/testing\',\n                      default=0, type=int) \n  args = parser.parse_args()\n  return args\n\nlr = cfg.TRAIN.LEARNING_RATE\nmomentum = cfg.TRAIN.MOMENTUM\nweight_decay = cfg.TRAIN.WEIGHT_DECAY\n\nif __name__ == \'__main__\':\n\n  args = parse_args()\n\n  print(\'Called with args:\')\n  print(args)\n\n  if torch.cuda.is_available() and not args.cuda:\n    print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\n  np.random.seed(cfg.RNG_SEED)\n  if args.dataset == ""pascal_voc"":\n      args.imdb_name = ""voc_2007_trainval""\n      args.imdbval_name = ""voc_2007_test""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n  elif args.dataset == ""pascal_voc_0712"":\n      args.imdb_name = ""voc_2007_trainval+voc_2012_trainval""\n      args.imdbval_name = ""voc_2007_test""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n  elif args.dataset == ""coco"":\n      args.imdb_name = ""coco_2017_train""\n      args.imdbval_name = ""coco_2017_val""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[4, 8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n  elif args.dataset == ""imagenet"":\n      args.imdb_name = ""imagenet_train""\n      args.imdbval_name = ""imagenet_val""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n  elif args.dataset == ""vg"":\n      args.imdb_name = ""vg_150-50-50_minitrain""\n      args.imdbval_name = ""vg_150-50-50_minival""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[4, 8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\']\n\n  args.cfg_file = ""cfgs/{}_{}.yml"".format(args.net, args.group) if args.group != 0 else ""cfgs/{}.yml"".format(args.net)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n\n\n  # Load dataset\n  cfg.TRAIN.USE_FLIPPED = False\n  imdb_vu, roidb_vu, ratio_list_vu, ratio_index_vu, query_vu = combined_roidb(args.imdbval_name, False, seen=args.seen)\n  imdb_vu.competition_mode(on=True)\n  dataset_vu = roibatchLoader(roidb_vu, ratio_list_vu, ratio_index_vu, query_vu, 1, imdb_vu.num_classes, training=False, seen=args.seen)\n\n\n  \n  # initilize the network here.\n  if args.net == \'vgg16\':\n    fasterRCNN = vgg16(imdb_vu.classes, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == \'res101\':\n    fasterRCNN = resnet(imdb_vu.classes, 101, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == \'res50\':\n    fasterRCNN = resnet(imdb_vu.classes, 50, pretrained=False, class_agnostic=args.class_agnostic)\n  elif args.net == \'res152\':\n    fasterRCNN = resnet(imdb_vu.classes, 152, pretrained=False, class_agnostic=args.class_agnostic)\n  else:\n    print(""network is not defined"")\n    pdb.set_trace()\n  fasterRCNN.create_architecture()\n\n  # Load checkpoint\n  input_dir = args.load_dir + ""/"" + args.net + ""/"" + args.dataset\n  if not os.path.exists(input_dir):\n    raise Exception(\'There is no input directory for loading network from \' + input_dir)\n\n  load_name = os.path.join(input_dir,\n    \'faster_rcnn_{}_{}_{}.pth\'.format(args.checksession, args.checkepoch, args.checkpoint))\n  print(""load checkpoint %s"" % (load_name))\n  checkpoint = torch.load(load_name)\n  fasterRCNN.load_state_dict(checkpoint[\'model\'])\n  if \'pooling_mode\' in checkpoint.keys():\n    cfg.POOLING_MODE = checkpoint[\'pooling_mode\']\n\n  # initilize the tensor holder here.\n  print(\'load model successfully!\')\n  im_data = torch.FloatTensor(1)\n  query   = torch.FloatTensor(1)\n  im_info = torch.FloatTensor(1)\n  catgory = torch.LongTensor(1)\n  gt_boxes = torch.FloatTensor(1)\n\n  # ship to cuda\n  if args.cuda:\n    cfg.CUDA = True\n    fasterRCNN.cuda()\n    im_data = im_data.cuda()\n    query = query.cuda()\n    im_info = im_info.cuda()\n    catgory = catgory.cuda()\n    gt_boxes = gt_boxes.cuda()\n\n  # make variable\n  im_data = Variable(im_data)\n  query = Variable(query)\n  im_info = Variable(im_info)\n  catgory = Variable(catgory)\n  gt_boxes = Variable(gt_boxes)\n    \n  # record time\n  start = time.time()\n\n  # visiualization\n  vis = args.vis\n  if vis:\n    thresh = 0.05\n  else:\n    thresh = 0.0\n  max_per_image = 100\n\n  # create output Directory\n  output_dir_vu = get_output_dir(imdb_vu, \'faster_rcnn_unseen\')\n\n  fasterRCNN.eval()\n  for avg in range(args.average):\n    dataset_vu.query_position = avg\n    dataloader_vu = torch.utils.data.DataLoader(dataset_vu, batch_size=1,shuffle=False, num_workers=0,pin_memory=True)\n\n    data_iter_vu = iter(dataloader_vu)\n\n    # total quantity of testing images, each images include multiple detect class\n    num_images_vu = len(imdb_vu.image_index)\n    num_detect = len(ratio_index_vu[0])\n\n    all_boxes = [[[] for _ in xrange(num_images_vu)]\n                for _ in xrange(imdb_vu.num_classes)]\n\n    \n    _t = {\'im_detect\': time.time(), \'misc\': time.time()}\n    if args.group != 0:\n      det_file = os.path.join(output_dir_vu, \'sess%d_g%d_seen%d_%d.pkl\'%(args.checksession, args.group, args.seen, avg))\n    else:\n      det_file = os.path.join(output_dir_vu, \'sess%d_seen%d_%d.pkl\'%(args.checksession, args.seen, avg))\n    print(det_file)\n\n    if os.path.exists(det_file):\n      with open(det_file, \'rb\') as fid:\n        all_boxes = pickle.load(fid)\n    else:\n      for i,index in enumerate(ratio_index_vu[0]):\n        data = next(data_iter_vu)\n        im_data.data.resize_(data[0].size()).copy_(data[0])\n        query.data.resize_(data[1].size()).copy_(data[1])\n        im_info.data.resize_(data[2].size()).copy_(data[2])\n        gt_boxes.data.resize_(data[3].size()).copy_(data[3])\n        catgory.data.resize_(data[4].size()).copy_(data[4])\n\n\n        # Run Testing\n        det_tic = time.time()\n        rois, cls_prob, bbox_pred, \\\n        rpn_loss_cls, rpn_loss_box, \\\n        RCNN_loss_cls, _, RCNN_loss_bbox, \\\n        rois_label, weight = fasterRCNN(im_data, query, im_info, gt_boxes, catgory)\n\n\n        scores = cls_prob.data\n        boxes = rois.data[:, :, 1:5]\n\n        \n        # Apply bounding-box regression \n        if cfg.TEST.BBOX_REG:\n            # Apply bounding-box regression deltas\n            box_deltas = bbox_pred.data\n            if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n            # Optionally normalize targets by a precomputed mean and stdev\n              if args.class_agnostic:\n                  box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                            + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                  box_deltas = box_deltas.view(1, -1, 4)\n              else:\n                  box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n                            + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n                  box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n\n            pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n            pred_boxes = clip_boxes(pred_boxes, im_info.data, 1)\n        else:\n            # Simply repeat the boxes, once for each class\n            pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\n\n        # Resize to original ratio\n        pred_boxes /= data[2][0][2].item()\n\n        # Remove batch_size dimension\n        scores = scores.squeeze()\n        pred_boxes = pred_boxes.squeeze()\n\n        # Record time\n        det_toc = time.time()\n        detect_time = det_toc - det_tic\n        misc_tic = time.time()\n\n        # Post processing\n        inds = torch.nonzero(scores>thresh).view(-1)\n        if inds.numel() > 0:\n          # remove useless indices\n          cls_scores = scores[inds]\n          cls_boxes = pred_boxes[inds, :]\n          cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n\n          # rearrange order\n          _, order = torch.sort(cls_scores, 0, True)\n          cls_dets = cls_dets[order]\n\n          # NMS\n          keep = nms(cls_boxes[order, :], cls_scores[order], cfg.TEST.NMS)\n          cls_dets = cls_dets[keep.view(-1).long()]\n          all_boxes[catgory][index] = cls_dets.cpu().numpy()\n\n        # Limit to max_per_image detections *over all classes*\n        if max_per_image > 0:\n          try:\n            image_scores = all_boxes[catgory][index][:,-1]\n            if len(image_scores) > max_per_image:\n                image_thresh = np.sort(image_scores)[-max_per_image]\n\n                keep = np.where(all_boxes[catgory][index][:,-1] >= image_thresh)[0]\n                all_boxes[catgory][index] = all_boxes[catgory][index][keep, :]\n          except:\n            pass\n\n        misc_toc = time.time()\n        nms_time = misc_toc - misc_tic\n\n        sys.stdout.write(\'im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r\' \\\n            .format(i + 1, num_detect, detect_time, nms_time))\n        sys.stdout.flush()\n\n        # save test image\n        if vis and i%1==0:\n          im2show = cv2.imread(dataset_vu._roidb[dataset_vu.ratio_index[i]][\'image\'])\n          im2show = vis_detections(im2show, \'shot\', cls_dets.cpu().numpy(), 0.8)\n\n          o_query = data[1][0].permute(1, 2,0).contiguous().cpu().numpy()\n          o_query *= [0.229, 0.224, 0.225]\n          o_query += [0.485, 0.456, 0.406]\n          o_query *= 255\n          o_query = o_query[:,:,::-1]\n\n          (h,w,c) = im2show.shape\n          o_query = cv2.resize(o_query, (h, h),interpolation=cv2.INTER_LINEAR)\n          im2show = np.concatenate((im2show, o_query), axis=1)\n\n          cv2.imwrite(\'./test_img/%d_d.png\'%(i), im2show)\n      \n    \n      with open(det_file, \'wb\') as f:\n          pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n      \n    print(\'Evaluating detections\')\n    imdb_vu.evaluate_detections(all_boxes, output_dir_vu) \n\n\n    end = time.time()\n    print(""test time: %0.4fs"" % (end - start))'"
trainval_net.py,21,"b'# --------------------------------------------------------\n# Pytorch multi-GPU Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nimport os\nimport sys\nimport numpy as np\nimport argparse\nimport pprint\nimport pdb\nimport time\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import Sampler\n\nfrom roi_data_layer.roidb import combined_roidb\nfrom roi_data_layer.roibatchLoader import roibatchLoader\nfrom model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\nfrom model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n      adjust_learning_rate, save_checkpoint, clip_gradient\n\nfrom model.faster_rcnn.vgg16 import vgg16\nfrom model.faster_rcnn.resnet import resnet\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Train a Fast R-CNN network\')\n  parser.add_argument(\'--dataset\', dest=\'dataset\',\n                      help=\'training dataset\',\n                      default=\'coco\', type=str)\n  parser.add_argument(\'--net\', dest=\'net\',\n                    help=\'vgg16, res101\',\n                    default=\'res50\', type=str)\n  parser.add_argument(\'--start_epoch\', dest=\'start_epoch\',\n                      help=\'starting epoch\',\n                      default=1, type=int)\n  parser.add_argument(\'--epochs\', dest=\'max_epochs\',\n                      help=\'number of epochs to train\',\n                      default=20, type=int)\n  parser.add_argument(\'--disp_interval\', dest=\'disp_interval\',\n                      help=\'number of iterations to display\',\n                      default=10, type=int)\n  parser.add_argument(\'--checkpoint_interval\', dest=\'checkpoint_interval\',\n                      help=\'number of iterations to display\',\n                      default=10000, type=int)\n\n  parser.add_argument(\'--save_dir\', dest=\'save_dir\',\n                      help=\'directory to save models\', default=""models"",\n                      type=str)\n  parser.add_argument(\'--nw\', dest=\'num_workers\',\n                      help=\'number of worker to load data\',\n                      default=8, type=int)\n  parser.add_argument(\'--cuda\', dest=\'cuda\',\n                      help=\'whether use CUDA\',\n                      action=\'store_true\')\n  parser.add_argument(\'--g\', dest=\'group\',\n                      help=\'which group to train, split coco to four group\',\n                      default=0)\n  parser.add_argument(\'--seen\', dest=\'seen\',default=1, type=int)\n\n  parser.add_argument(\'--mGPUs\', dest=\'mGPUs\',\n                      help=\'whether use multiple GPUs\',\n                      action=\'store_true\')\n  parser.add_argument(\'--bs\', dest=\'batch_size\',\n                      help=\'batch_size\',\n                      default=128, type=int)\n  parser.add_argument(\'--bs_v\', dest=\'batch_size_val\',\n                      help=\'batch_size\',\n                      default=16, type=int)\n  parser.add_argument(\'--cag\', dest=\'class_agnostic\',\n                      help=\'whether perform class_agnostic bbox regression\',\n                      default=True)\n\n# config optimization\n  parser.add_argument(\'--o\', dest=\'optimizer\',\n                      help=\'training optimizer\',\n                      default=""sgd"", type=str)\n  parser.add_argument(\'--lr\', dest=\'lr\',\n                      help=\'starting learning rate\',\n                      default=0.01, type=float)\n  parser.add_argument(\'--lr_decay_step\', dest=\'lr_decay_step\',\n                      help=\'step to do learning rate decay, unit is epoch\',\n                      default=4, type=int)\n  parser.add_argument(\'--lr_decay_gamma\', dest=\'lr_decay_gamma\',\n                      help=\'learning rate decay ratio\',\n                      default=0.1, type=float)\n\n# set training session\n  parser.add_argument(\'--s\', dest=\'session\',\n                      help=\'training session\',\n                      default=1, type=int)\n\n# resume trained model\n  parser.add_argument(\'--r\', dest=\'resume\',\n                      help=\'resume checkpoint or not\',\n                      default=False, type=bool)\n  parser.add_argument(\'--checksession\', dest=\'checksession\',\n                      help=\'checksession to load model\',\n                      default=1, type=int)\n  parser.add_argument(\'--checkepoch\', dest=\'checkepoch\',\n                      help=\'checkepoch to load model\',\n                      default=1, type=int)\n  parser.add_argument(\'--checkpoint\', dest=\'checkpoint\',\n                      help=\'checkpoint to load model\',\n                      default=0, type=int)\n# log and diaplay\n  parser.add_argument(\'--use_tfb\', dest=\'use_tfboard\',\n                      help=\'whether use tensorboard\',\n                      default=True)\n\n  args = parser.parse_args()\n  return args\n\n\nclass sampler(Sampler):\n  def __init__(self, train_size, batch_size):\n    self.num_data = train_size\n    self.num_per_batch = int(train_size / batch_size)\n    self.batch_size = batch_size\n    self.range = torch.arange(0,batch_size).view(1, batch_size).long()\n    self.leftover_flag = False\n    if train_size % batch_size:\n      self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n      self.leftover_flag = True\n\n  def __iter__(self):\n    rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size\n    self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range\n\n    self.rand_num_view = self.rand_num.view(-1)\n\n    if self.leftover_flag:\n      self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)\n\n    return iter(self.rand_num_view)\n\n  def __len__(self):\n    return self.num_data\n\nif __name__ == \'__main__\':\n\n  args = parse_args()\n  val = False\n\n  print(\'Called with args:\')\n  print(args)\n\n  if args.dataset == ""pascal_voc"":\n      args.imdb_name = ""voc_2007_trainval""\n      args.imdbval_name = ""voc_2007_test""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\', \'MAX_NUM_GT_BOXES\', \'20\']\n  elif args.dataset == ""pascal_voc_0712"":\n      args.imdb_name = ""voc_2007_trainval+voc_2012_trainval""\n      args.imdbval_name = ""voc_2007_test""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\', \'MAX_NUM_GT_BOXES\', \'20\']\n  elif args.dataset == ""coco"":\n      args.imdb_name = ""coco_2017_train""\n      args.imdbval_name = ""coco_2017_minival""\n      args.set_cfgs = [\'ANCHOR_SCALES\', \'[4, 8, 16, 32]\', \'ANCHOR_RATIOS\', \'[0.5,1,2]\', \'MAX_NUM_GT_BOXES\', \'50\']\n\n\n  args.cfg_file = ""cfgs/{}_{}.yml"".format(args.net, args.group) if args.group != 0 else ""cfgs/{}.yml"".format(args.net)\n  \n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n  np.random.seed(cfg.RNG_SEED)\n\n  #torch.backends.cudnn.benchmark = True\n  if torch.cuda.is_available() and not args.cuda:\n    print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\n  # train set\n  # -- Note: Use validation set and disable the flipped to enable faster loading.\n  cfg.TRAIN.USE_FLIPPED = True\n  cfg.USE_GPU_NMS = args.cuda\n  \n  # create dataloader\n  imdb, roidb, ratio_list, ratio_index, query = combined_roidb(args.imdb_name, True, seen=args.seen)\n  train_size = len(roidb)\n  print(\'{:d} roidb entries\'.format(len(roidb)))\n  sampler_batch = sampler(train_size, args.batch_size)\n  dataset = roibatchLoader(roidb, ratio_list, ratio_index, query, args.batch_size, imdb.num_classes, training=True)\n  dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size,\n                            sampler=sampler_batch, num_workers=args.num_workers)\n\n  # create output directory\n  output_dir = args.save_dir + ""/"" + args.net + ""/"" + args.dataset\n  if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n  # initilize the tensor holder here.\n  im_data = torch.FloatTensor(1)\n  query   = torch.FloatTensor(1)\n  im_info = torch.FloatTensor(1)\n  num_boxes = torch.LongTensor(1)\n  gt_boxes = torch.FloatTensor(1)\n\n  # ship to cuda\n  if args.cuda:\n    im_data = im_data.cuda()\n    query   = query.cuda()\n    im_info = im_info.cuda()\n    num_boxes = num_boxes.cuda()\n    gt_boxes = gt_boxes.cuda()\n    cfg.CUDA = True\n\n  # make variable\n  im_data = Variable(im_data)\n  query   = Variable(query)\n  im_info = Variable(im_info)\n  num_boxes = Variable(num_boxes)\n  gt_boxes = Variable(gt_boxes)\n\n\n  # initilize the network here.\n  if args.net == \'vgg16\':\n    fasterRCNN = vgg16(imdb.classes, pretrained=True, class_agnostic=args.class_agnostic)\n  elif args.net == \'res101\':\n    fasterRCNN = resnet(imdb.classes, 101, pretrained=True, class_agnostic=args.class_agnostic)\n  elif args.net == \'res50\':\n    fasterRCNN = resnet(imdb.classes, 50, pretrained=True, class_agnostic=args.class_agnostic)\n  elif args.net == \'res152\':\n    fasterRCNN = resnet(imdb.classes, 152, pretrained=True, class_agnostic=args.class_agnostic)\n  else:\n    print(""network is not defined"")\n    pdb.set_trace()\n\n  fasterRCNN.create_architecture()\n  lr = args.lr\n\n  params = []\n  for key, value in dict(fasterRCNN.named_parameters()).items():\n    if value.requires_grad:\n      if \'bias\' in key:\n        params += [{\'params\':[value],\'lr\':lr*(cfg.TRAIN.DOUBLE_BIAS + 1), \\\n                \'weight_decay\': cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY or 0}]\n      else:\n        params += [{\'params\':[value],\'lr\':lr, \'weight_decay\': cfg.TRAIN.WEIGHT_DECAY}]\n\n  if args.cuda:\n    fasterRCNN.cuda()\n      \n  if args.optimizer == ""adam"":\n    lr = lr * 0.1\n    optimizer = torch.optim.Adam(params)\n  elif args.optimizer == ""sgd"":\n    optimizer = torch.optim.SGD(params, momentum=cfg.TRAIN.MOMENTUM)\n\n  if args.resume:\n    load_name = os.path.join(output_dir,\n      \'faster_rcnn_{}_{}_{}.pth\'.format(args.checksession, args.checkepoch, args.checkpoint))\n    print(""loading checkpoint %s"" % (load_name))\n    checkpoint = torch.load(load_name)\n    args.session = checkpoint[\'session\']\n    args.start_epoch = checkpoint[\'epoch\']\n    fasterRCNN.load_state_dict(checkpoint[\'model\'])\n    optimizer.load_state_dict(checkpoint[\'optimizer\'])\n    lr = optimizer.param_groups[0][\'lr\']\n    if \'pooling_mode\' in checkpoint.keys():\n      cfg.POOLING_MODE = checkpoint[\'pooling_mode\']\n    print(""loaded checkpoint %s"" % (load_name))\n\n  if args.mGPUs:\n    fasterRCNN = nn.DataParallel(fasterRCNN)\n\n  iters_per_epoch = int(train_size / args.batch_size)\n  \n  if args.use_tfboard:\n    from tensorboardX import SummaryWriter\n    logger = SummaryWriter(""logs"")\n\n  for epoch in range(args.start_epoch, args.max_epochs + 1):\n    # setting to train mode\n    fasterRCNN.train()\n    loss_temp = 0\n    start = time.time()\n\n    if epoch % (args.lr_decay_step + 1) == 0:\n        adjust_learning_rate(optimizer, args.lr_decay_gamma)\n        lr *= args.lr_decay_gamma\n\n    data_iter = iter(dataloader)\n    \n    for step in range(iters_per_epoch):\n      data = next(data_iter)\n      im_data.resize_(data[0].size()).copy_(data[0])\n      query.resize_(data[1].size()).copy_(data[1])\n      im_info.resize_(data[2].size()).copy_(data[2])\n      gt_boxes.resize_(data[3].size()).copy_(data[3])\n      num_boxes.resize_(data[4].size()).copy_(data[4])\n\n      fasterRCNN.zero_grad()\n      rois, cls_prob, bbox_pred, \\\n      rpn_loss_cls, rpn_loss_box, \\\n      RCNN_loss_cls, margin_loss, RCNN_loss_bbox, \\\n      rois_label, _ = fasterRCNN(im_data, query, im_info, gt_boxes, num_boxes)\n\n      loss = rpn_loss_cls.mean() + rpn_loss_box.mean() \\\n           + RCNN_loss_cls.mean() + margin_loss.mean() + RCNN_loss_bbox.mean()\n      loss_temp += loss.item()\n\n      # backward\n      optimizer.zero_grad()\n      loss.backward()\n      if args.net == ""vgg16"":\n          clip_gradient(fasterRCNN, 10.)\n      optimizer.step()\n\n      if step % args.disp_interval == 0:\n        end = time.time()\n        if step > 0:\n          loss_temp /= (args.disp_interval + 1)\n\n        if args.mGPUs:\n          loss_rpn_cls  = rpn_loss_cls.mean().item()\n          loss_rpn_box  = rpn_loss_box.mean().item()\n          loss_rcnn_cls = RCNN_loss_cls.mean().item()\n          loss_margin   = margin_loss.mean().item()\n          loss_rcnn_box = RCNN_loss_bbox.mean().item()\n          fg_cnt = torch.sum(rois_label.data.ne(0))\n          bg_cnt = rois_label.data.numel() - fg_cnt\n        else:\n          loss_rpn_cls = rpn_loss_cls.item()\n          loss_rpn_box = rpn_loss_box.item()\n          loss_rcnn_cls = RCNN_loss_cls.item()\n          loss_margin = margin_loss.item()\n          loss_rcnn_box = RCNN_loss_bbox.item()\n          fg_cnt = torch.sum(rois_label.data.ne(0))\n          bg_cnt = rois_label.data.numel() - fg_cnt\n\n        print(""[session %d][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e"" \\\n                                % (args.session, epoch, step, iters_per_epoch, loss_temp, lr))\n        print(""\\t\\t\\tfg/bg=(%d/%d), time cost: %f"" % (fg_cnt, bg_cnt, end-start))\n        print(""\\t\\t\\trpn_cls: %.3f, rpn_box: %.3f, rcnn_cls: %.3f, margin: %.3f, rcnn_box %.3f"" \\\n                      % (loss_rpn_cls, loss_rpn_box, loss_rcnn_cls, loss_margin, loss_rcnn_box))\n        if args.use_tfboard:\n          info = {\n            \'loss\': loss_temp,\n            \'loss_rpn_cls\': loss_rpn_cls,\n            \'loss_rpn_box\': loss_rpn_box,\n            \'loss_rcnn_cls\': loss_rcnn_cls,\n            \'loss_margin\': loss_margin,\n            \'loss_rcnn_box\': loss_rcnn_box\n          }\n          logger.add_scalars(""logs_s_{}/losses"".format(args.session), info, (epoch - 1) * iters_per_epoch + step)\n\n        loss_temp = 0\n        start = time.time()\n\n    \n    save_name = os.path.join(output_dir, \'faster_rcnn_{}_{}_{}.pth\'.format(args.session, epoch, step))\n    save_checkpoint({\n      \'session\': args.session,\n      \'epoch\': epoch + 1,\n      \'model\': fasterRCNN.module.state_dict() if args.mGPUs else fasterRCNN.state_dict(),\n      \'optimizer\': optimizer.state_dict(),\n      \'pooling_mode\': cfg.POOLING_MODE,\n      \'class_agnostic\': args.class_agnostic,\n    }, save_name)\n    print(\'save model: {}\'.format(save_name))\n\n  if args.use_tfboard:\n    logger.close()\n'"
lib/setup.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n#!/usr/bin/env python\n\nimport glob\nimport os\nimport numpy as np\n\nimport torch\nfrom setuptools import find_packages\nfrom setuptools import setup, Extension\nfrom torch.utils.cpp_extension import CUDA_HOME\nfrom torch.utils.cpp_extension import CppExtension\nfrom torch.utils.cpp_extension import CUDAExtension\n\nrequirements = [""torch"", ""torchvision""]\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, ""model"", ""csrc"")\n\n    main_file = glob.glob(os.path.join(extensions_dir, ""*.cpp""))\n    source_cpu = glob.glob(os.path.join(extensions_dir, ""cpu"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(extensions_dir, ""cuda"", ""*.cu""))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n\n    extra_compile_args = {""cxx"": []}\n    define_macros = []\n\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(""WITH_CUDA"", None)]\n        extra_compile_args[""nvcc""] = [\n            ""-DCUDA_HAS_FP16=1"",\n            ""-D__CUDA_NO_HALF_OPERATORS__"",\n            ""-D__CUDA_NO_HALF_CONVERSIONS__"",\n            ""-D__CUDA_NO_HALF2_OPERATORS__"",\n        ]\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            ""model._C"",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        ),\n        Extension(\n        \'pycocotools._mask\',\n        sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs = [numpy_include, \'pycocotools\'],\n        extra_compile_args={\n            \'cxx\': [],\n            \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    ),\n    ]\n\n    return ext_modules\n\n\nsetup(\n    name=""faster_rcnn"",\n    version=""0.1"",\n    description=""object detection in pytorch"",\n    packages=find_packages(exclude=(""configs"", ""tests"",)),\n    # install_requires=requirements,\n    ext_modules=get_extensions(),\n    cmdclass={""build_ext"": torch.utils.cpp_extension.BuildExtension},\n)\n'"
lib/datasets/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
lib/datasets/coco.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nfrom model.utils.config import cfg\nimport os.path as osp\nimport sys\nimport os\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport pickle\nimport json\nimport uuid\n# COCO API\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools import mask as COCOmask\n\nclass coco(imdb):\n  def __init__(self, image_set, year):\n    imdb.__init__(self, \'coco_\' + year + \'_\' + image_set)\n    # COCO specific config options\n    self.config = {\'use_salt\': True,\n                   \'cleanup\': True}\n    # name, paths\n    self._year = year\n    self._image_set = image_set\n    self._data_path = osp.join(cfg.DATA_DIR, \'coco\')\n\n    # load COCO API, classes, class <-> id mappings\n    self._COCO = COCO(self._get_ann_file())\n    cats = self._COCO.loadCats(self._COCO.getCatIds())\n    # class name\n    self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n    # class name to ind    (0~80) 0= __background__\n    self._class_to_ind = dict(list(zip(self.classes, list(range(self.num_classes)))))\n    # class name to cat_id (1~90) 1= person\n    self._class_to_coco_cat_id = dict(list(zip([c[\'name\'] for c in cats],\n                                               self._COCO.getCatIds())))\n    # Lookup table to map from COCO category ids to our internal class\n    # indices\n    # 1~90 : 1~80\n    self.coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                      self._class_to_ind[cls])\n                                     for cls in self._classes[1:]])\n    # 1~80 : 1~90\n    self.coco_class_ind_to_cat_id = dict([(self._class_to_ind[cls],\n                                      self._class_to_coco_cat_id[cls])\n                                     for cls in self._classes[1:]])\n\n    self._image_index = self._load_image_set_index()\n\n\n    \n\n    # Default to roidb handler\n    self.set_proposal_method(\'gt\')\n    self.competition_mode(False)\n\n    # Some image sets are ""views"" (i.e. subsets) into others.\n    # For example, minival2014 is a random 5000 image subset of val2014.\n    # This mapping tells us where the view\'s images and proposals come from.\n    self._view_map = {\n      \'minival2014\': \'val2014\',  # 5k val2014 subset\n      \'valminusminival2014\': \'val2014\',  # val2014 \\setminus minival2014\n      \'test-dev2015\': \'test2015\',\n      \'valminuscapval2014\': \'val2014\',\n      \'capval2014\': \'val2014\',\n      \'captest2014\': \'val2014\'\n    }\n    coco_name = image_set + year  # e.g., ""val2014""\n    self._data_name = (self._view_map[coco_name]\n                       if coco_name in self._view_map\n                       else coco_name)\n    # Dataset splits that have ground-truth annotations (test splits\n    # do not have gt annotations)\n    self._gt_splits = (\'train\', \'val\', \'minival\')\n\n    # set reference file\n    self._reference_dir  = os.path.join(cfg.DATA_DIR, ""coco_reference_image"")\n    self._reference_file = os.path.join(self._reference_dir, ""coco_{}_e2e_mask_rcnn_R_101_FPN_1x_caffe2.pkl"".format(self._data_name))\n    if not os.path.exists(self._reference_file):\n        print(\'No reference file.\')\n        assert False\n    else:\n        with open(self._reference_file, ""rb"") as f:\n            self.reference_image = pickle.load(f)\n\n    self.cat_data = {}\n\n    for i in self._class_to_ind.values():\n      # i = 1~80\n      self.cat_data[i] = []\n\n  def _get_ann_file(self):\n    prefix = \'instances\' if self._image_set.find(\'test\') == -1 \\\n      else \'image_info\'\n    return osp.join(self._data_path, \'annotations\',\n                    prefix + \'_\' + self._image_set + self._year + \'.json\')\n\n  def _load_image_set_index(self):\n    """"""\n    Load image ids.\n    """"""\n    image_ids = self._COCO.getImgIds()\n    return image_ids\n\n  def _get_widths(self):\n    anns = self._COCO.loadImgs(self._image_index)\n    widths = [ann[\'width\'] for ann in anns]\n    return widths\n\n  def image_path_at(self, i):\n    """"""\n    Return the absolute path to image i in the image sequence.\n    """"""\n    return self.image_path_from_index(self._image_index[i])\n\n  def image_id_at(self, i):\n    """"""\n    Return the absolute path to image i in the image sequence.\n    """"""\n    return self._image_index[i]\n\n  def image_path_from_index(self, index):\n    """"""\n    Construct an image path from the image\'s ""index"" identifier.\n    """"""\n    # Example image path for index=119993:\n    #   images/train2014/COCO_train2014_000000119993.jpg\n    if self._data_name==\'train2014\':\n      file_name = (\'COCO_\' + self._data_name + \'_\' +\n                  str(index).zfill(12) + \'.jpg\')\n    else:\n      file_name = (str(index).zfill(12) + \'.jpg\')\n      \n    image_path = osp.join(self._data_path, \'images\',\n                          self._data_name, file_name)\n    assert osp.exists(image_path), \\\n      \'Path does not exist: {}\'.format(image_path)\n    return image_path\n\n  def gt_roidb(self):\n    """"""\n    Return the database of ground-truth regions of interest.\n    This function loads/saves from/to a cache file to speed up future calls.\n    """"""\n    cache_file = osp.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n    \n    if osp.exists(cache_file):\n      with open(cache_file, \'rb\') as fid:\n        [roidb, self.cat_data] = pickle.load(fid)\n      print(\'{} gt roidb loaded from {}\'.format(self.name, cache_file))\n      return roidb\n    \n\n    gt_roidb = [self._load_coco_annotation(index)\n                for index in self._image_index]\n\n    with open(cache_file, \'wb\') as fid:\n      pickle.dump([gt_roidb,self.cat_data], fid, pickle.HIGHEST_PROTOCOL)\n    print(\'wrote gt roidb to {}\'.format(cache_file))\n    return gt_roidb\n\n  def _load_coco_annotation(self, index):\n    """"""\n    Loads COCO bounding-box instance annotations. Crowd instances are\n    handled by marking their overlaps (with all categories) to -1. This\n    overlap value means that crowd ""instances"" are excluded from training.\n    """"""\n    im_ann = self._COCO.loadImgs(index)[0]\n    im_path = self.image_path_from_index(index)\n    width = im_ann[\'width\']\n    height = im_ann[\'height\']\n\n    # Get the useful information\n    reference  = self.reference_image[index]\n    save_seq = reference.keys()\n\n    annIds = self._COCO.getAnnIds(imgIds=index, iscrowd=None)\n    objs = self._COCO.loadAnns(annIds)\n    # Sanitize bboxes -- some are invalid\n    valid_objs = []\n    for i, obj in enumerate(objs):\n      x1 = np.max((0, obj[\'bbox\'][0]))\n      y1 = np.max((0, obj[\'bbox\'][1]))\n      x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n      y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n      if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n        obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n        valid_objs.append(obj)\n      \n        if i in save_seq:\n          entry = {\n               \'boxes\': obj[\'clean_bbox\'],\n               \'image_path\': im_path\n               }\n          \n          self.cat_data[self.coco_cat_id_to_class_ind[obj[\'category_id\']]].append(entry)\n      \n    objs = valid_objs\n    num_objs = len(objs)\n\n    boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n    gt_classes = np.zeros((num_objs), dtype=np.int32)\n    overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n    seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n    \n\n    for ix, obj in enumerate(objs):\n      cls = self.coco_cat_id_to_class_ind[obj[\'category_id\']]\n      boxes[ix, :] = obj[\'clean_bbox\']\n      gt_classes[ix] = cls\n      seg_areas[ix] = obj[\'area\']\n      if obj[\'iscrowd\']:\n        # Set overlap to -1 for all classes for crowd objects\n        # so they will be excluded during training\n        overlaps[ix, :] = -1.0\n      else:\n        overlaps[ix, cls] = 1.0\n\n    ds_utils.validate_boxes(boxes, width=width, height=height)\n    overlaps = scipy.sparse.csr_matrix(overlaps)\n    return {\'width\': width,\n            \'height\': height,\n            \'boxes\': boxes,\n            \'gt_classes\': gt_classes,\n            \'gt_overlaps\': overlaps,\n            \'flipped\': False,\n            \'seg_areas\': seg_areas}\n\n  def _get_widths(self):\n    return [r[\'width\'] for r in self.roidb]\n\n  def append_flipped_images(self):\n    num_images = self.num_images\n    widths = self._get_widths()\n\n    for i in range(num_images):\n      boxes = self.roidb[i][\'boxes\'].copy()\n      oldx1 = boxes[:, 0].copy()\n      oldx2 = boxes[:, 2].copy()\n      boxes[:, 0] = widths[i] - oldx2 - 1\n      boxes[:, 2] = widths[i] - oldx1 - 1\n      assert (boxes[:, 2] >= boxes[:, 0]).all()\n      entry = {\'width\': widths[i],\n               \'height\': self.roidb[i][\'height\'],\n               \'boxes\': boxes,\n               \'gt_classes\': self.roidb[i][\'gt_classes\'],\n               \'gt_overlaps\': self.roidb[i][\'gt_overlaps\'],\n               \'flipped\': True,\n               \'seg_areas\': self.roidb[i][\'seg_areas\']}\n\n      self.roidb.append(entry)\n    self._image_index = self._image_index * 2\n\n  def _get_box_file(self, index):\n    # first 14 chars / first 22 chars / all chars + .mat\n    # COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n    file_name = (\'COCO_\' + self._data_name +\n                 \'_\' + str(index).zfill(12) + \'.mat\')\n    return osp.join(file_name[:14], file_name[:22], file_name)\n\n  def _print_detection_eval_metrics(self, coco_eval):\n    IoU_lo_thresh = 0.5\n    IoU_hi_thresh = 0.95\n\n    def _get_thr_ind(coco_eval, thr):\n      ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                     (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n      iou_thr = coco_eval.params.iouThrs[ind]\n      assert np.isclose(iou_thr, thr)\n      return ind\n\n    ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n    ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n    # precision has dims (iou, recall, cls, area range, max dets)\n    # area range index 0: all area ranges\n    # max dets index 2: 100 per image\n    precision = \\\n      coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n    ap_default = np.mean(precision[precision > -1])\n    print((\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n           \'~~~~\').format(IoU_lo_thresh, IoU_hi_thresh))\n    print(\'{:.1f}\'.format(100 * ap_default))\n    for cls_ind, cls in enumerate(self.classes):\n      if cls == \'__background__\':\n        continue\n      # minus 1 because of __background__\n      precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n      ap = np.mean(precision[precision > -1])\n      print(\'{:.1f}\'.format(100 * ap))\n\n    print(\'~~~~ Summary metrics ~~~~\')\n    coco_eval.summarize()\n\n  def _do_detection_eval(self, res_file, output_dir):\n    ann_type = \'bbox\'\n\n    tmp = [self.coco_cat_id_to_class_ind[i]-1 for i in self.list]\n\n    coco_dt = self._COCO.loadRes(res_file)\n\n    cocoEval = customCOCOeval(self._COCO, coco_dt, ""bbox"")\n    cocoEval.params.imgIds = self._image_index\n    cocoEval.evaluate()\n    # print(cocoEval.ious)\n    cocoEval.accumulate()\n    cocoEval.summarize(class_index=tmp)\n\n\n    eval_file = osp.join(output_dir, \'detection_results.pkl\')\n    with open(eval_file, \'wb\') as fid:\n      pickle.dump(cocoEval, fid, pickle.HIGHEST_PROTOCOL)\n    print(\'Wrote COCO eval results to: {}\'.format(eval_file))\n\n  def _coco_results_one_category(self, boxes, cat_id):\n    results = []\n    for im_ind, index in enumerate(self.image_index):\n      dets = boxes[im_ind]\n      if dets == []:\n        continue\n      dets = np.array(dets).astype(np.float)\n      scores = dets[:, -1]\n      xs = dets[:, 0]\n      ys = dets[:, 1]\n      ws = dets[:, 2] - xs + 1\n      hs = dets[:, 3] - ys + 1\n      for k in range(len(dets)):\n        results.extend(\n          [{\'image_id\': index,\n            \'category_id\': cat_id,\n            \'bbox\': [xs[k], ys[k], ws[k], hs[k]],\n            \'score\': scores[k]} ])\n    return results\n\n  def _write_coco_results_file(self, all_boxes, res_file):\n    # [{""image_id"": 42,\n    #   ""category_id"": 18,\n    #   ""bbox"": [258.15,41.29,348.26,243.78],\n    #   ""score"": 0.236}, ...]\n    results = []\n    for cls_ind, cls in enumerate(self.classes):\n      if cls == \'__background__\':\n        continue\n      print(\'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                       self.num_classes - 1))\n      coco_cat_id = self._class_to_coco_cat_id[cls]\n      results.extend(self._coco_results_one_category(all_boxes[cls_ind], coco_cat_id))\n    print(\'Writing results json to {}\'.format(res_file))\n    with open(res_file, \'w\') as fid:\n      json.dump(results, fid)\n\n  def evaluate_detections(self, all_boxes, output_dir):\n\n\n    res_file = osp.join(output_dir, (\'detections_\' +\n                                     self._image_set +\n                                     self._year +\n                                     \'_results\'))\n    if self.config[\'use_salt\']:\n      res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n    self._write_coco_results_file(all_boxes, res_file)\n    # Only do evaluation on non-test sets\n    if self._image_set.find(\'test\') == -1:\n      self._do_detection_eval(res_file, output_dir)\n    # Optionally cleanup results json file\n    if self.config[\'cleanup\']:\n      os.remove(res_file)\n\n  def competition_mode(self, on):\n    if on:\n      self.config[\'use_salt\'] = False\n      self.config[\'cleanup\'] = False\n    else:\n      self.config[\'use_salt\'] = True\n      self.config[\'cleanup\'] = True\n\n  def filter(self, seen=1):\n\n    # if want to use train_categories, seen = 1 \n    # if want to use test_categories , seen = 2\n    # if want to use both            , seen = 3\n\n    if seen==1:\n      self.list = cfg.train_categories\n      # Group number to class\n      if len(self.list)==1:\n        self.list = [self.coco_class_ind_to_cat_id[cat] for cat in range(1,81) if cat%4 != self.list[0]]\n\n    elif seen==2:\n      self.list = cfg.test_categories\n      # Group number to class\n      if len(self.list)==1:\n        self.list = [self.coco_class_ind_to_cat_id[cat] for cat in range(1,81) if cat%4 == self.list[0]]\n    \n    elif seen==3:\n      self.list = cfg.train_categories + cfg.test_categories\n      # Group number to class\n      if len(self.list)==2:\n        self.list = [self.coco_class_ind_to_cat_id[cat] for cat in range(1,81)]\n\n    # Transfer categories id to class indices\n    self.inverse_list = [self.coco_cat_id_to_class_ind[i] for i in self.list ]\n\n    # Which index need to be remove\n    all_index = list(range(len(self._image_index)))\n\n    for index, info in enumerate(self.roidb):\n      for cat in info[\'gt_classes\']:\n        if self.coco_class_ind_to_cat_id[cat] in self.list:\n            all_index.remove(index)\n            break\n\n    # Remove index from the end to start\n    all_index.reverse()\n    for index in all_index:\n      self._image_index.pop(index)\n      self.roidb.pop(index)\n  \n\nclass customCOCOeval(COCOeval):\n    \n    def summarize(self, class_index=None, verbose=1):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                if not class_index is None:\n                    s = s[:,:,class_index,aind,mind]\n                else:\n                    s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                if not class_index is None:\n                    s = s[:,class_index,aind,mind]\n                else:\n                    s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            if verbose > 0:\n                print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self, cass_index=None):\n        self.summarize(class_index)'"
lib/datasets/ds_utils.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef unique_boxes(boxes, scale=1.0):\n  """"""Return indices of unique boxes.""""""\n  v = np.array([1, 1e3, 1e6, 1e9])\n  hashes = np.round(boxes * scale).dot(v)\n  _, index = np.unique(hashes, return_index=True)\n  return np.sort(index)\n\n\ndef xywh_to_xyxy(boxes):\n  """"""Convert [x y w h] box format to [x1 y1 x2 y2] format.""""""\n  return np.hstack((boxes[:, 0:2], boxes[:, 0:2] + boxes[:, 2:4] - 1))\n\n\ndef xyxy_to_xywh(boxes):\n  """"""Convert [x1 y1 x2 y2] box format to [x y w h] format.""""""\n  return np.hstack((boxes[:, 0:2], boxes[:, 2:4] - boxes[:, 0:2] + 1))\n\n\ndef validate_boxes(boxes, width=0, height=0):\n  """"""Check that a set of boxes are valid.""""""\n  x1 = boxes[:, 0]\n  y1 = boxes[:, 1]\n  x2 = boxes[:, 2]\n  y2 = boxes[:, 3]\n  assert (x1 >= 0).all()\n  assert (y1 >= 0).all()\n  assert (x2 >= x1).all()\n  assert (y2 >= y1).all()\n  assert (x2 < width).all()\n  assert (y2 < height).all()\n\n\ndef filter_small_boxes(boxes, min_size):\n  w = boxes[:, 2] - boxes[:, 0]\n  h = boxes[:, 3] - boxes[:, 1]\n  keep = np.where((w >= min_size) & (h > min_size))[0]\n  return keep\n'"
lib/datasets/factory.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Factory method for easily getting imdbs by name.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n__sets = {}\nfrom datasets.pascal_voc import pascal_voc\nfrom datasets.coco import coco\nfrom datasets.imagenet import imagenet\nfrom datasets.vg import vg\n\nimport numpy as np\n\n# Set up voc_<year>_<split>\nfor year in [\'2007\', \'2012\']:\n  for split in [\'train\', \'val\', \'trainval\', \'test\']:\n    name = \'voc_{}_{}\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: pascal_voc(split, year))\n\n# Set up coco_2014_<split>\nfor year in [\'2014\']:\n  for split in [\'train\', \'val\', \'minival\', \'valminusminival\', \'trainval\']:\n    name = \'coco_{}_{}\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up coco_2014_cap_<split>\nfor year in [\'2014\', \'2017\']:\n  for split in [\'train\', \'val\', \'capval\', \'valminuscapval\', \'trainval\']:\n    name = \'coco_{}_{}\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up coco_2015_<split>\nfor year in [\'2015\']:\n  for split in [\'test\', \'test-dev\']:\n    name = \'coco_{}_{}\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up vg_<split>\n# for version in [\'1600-400-20\']:\n#     for split in [\'minitrain\', \'train\', \'minival\', \'val\', \'test\']:\n#         name = \'vg_{}_{}\'.format(version,split)\n#         __sets[name] = (lambda split=split, version=version: vg(version, split))\nfor version in [\'150-50-20\', \'150-50-50\', \'500-150-80\', \'750-250-150\', \'1750-700-450\', \'1600-400-20\']:\n    for split in [\'minitrain\', \'smalltrain\', \'train\', \'minival\', \'smallval\', \'val\', \'test\']:\n        name = \'vg_{}_{}\'.format(version,split)\n        __sets[name] = (lambda split=split, version=version: vg(version, split))\n        \n# set up image net.\nfor split in [\'train\', \'val\', \'val1\', \'val2\', \'test\']:\n    name = \'imagenet_{}\'.format(split)\n    devkit_path = \'data/imagenet/ILSVRC/devkit\'\n    data_path = \'data/imagenet/ILSVRC\'\n    __sets[name] = (lambda split=split, devkit_path=devkit_path, data_path=data_path: imagenet(split,devkit_path,data_path))\n\ndef get_imdb(name):\n  """"""Get an imdb (image database) by name.""""""\n\n  if name not in __sets:\n    raise KeyError(\'Unknown dataset: {}\'.format(name))\n  return __sets[name]()\n\n\ndef list_imdbs():\n  """"""List all registered imdbs.""""""\n  return list(__sets.keys())\n'"
lib/datasets/imagenet.py,0,"b'from __future__ import print_function\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport datasets\nimport datasets.imagenet\nimport os, sys\nfrom datasets.imdb import imdb\nimport xml.dom.minidom as minidom\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport subprocess\nimport pdb\nimport pickle\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n\nclass imagenet(imdb):\n    def __init__(self, image_set, devkit_path, data_path):\n        imdb.__init__(self, image_set)\n        self._image_set = image_set\n        self._devkit_path = devkit_path\n        self._data_path = data_path\n        synsets_image = sio.loadmat(os.path.join(self._devkit_path, \'data\', \'meta_det.mat\'))\n        synsets_video = sio.loadmat(os.path.join(self._devkit_path, \'data\', \'meta_vid.mat\'))\n        self._classes_image = (\'__background__\',)\n        self._wnid_image = (0,)\n\n        self._classes = (\'__background__\',)\n        self._wnid = (0,)\n\n        for i in xrange(200):\n            self._classes_image = self._classes_image + (synsets_image[\'synsets\'][0][i][2][0],)\n            self._wnid_image = self._wnid_image + (synsets_image[\'synsets\'][0][i][1][0],)\n\n        for i in xrange(30):\n            self._classes = self._classes + (synsets_video[\'synsets\'][0][i][2][0],)\n            self._wnid = self._wnid + (synsets_video[\'synsets\'][0][i][1][0],)\n\n        self._wnid_to_ind_image = dict(zip(self._wnid_image, xrange(201)))\n        self._class_to_ind_image = dict(zip(self._classes_image, xrange(201)))\n\n        self._wnid_to_ind = dict(zip(self._wnid, xrange(31)))\n        self._class_to_ind = dict(zip(self._classes, xrange(31)))\n\n        #check for valid intersection between video and image classes\n        self._valid_image_flag = [0]*201\n\n        for i in range(1,201):\n            if self._wnid_image[i] in self._wnid_to_ind:\n                self._valid_image_flag[i] = 1\n\n        self._image_ext = [\'.JPEG\']\n\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        self._roidb_handler = self.gt_roidb\n\n        # Specific config options\n        self.config = {\'cleanup\'  : True,\n                       \'use_salt\' : True,\n                       \'top_k\'    : 2000}\n\n        assert os.path.exists(self._devkit_path), \'Devkit path does not exist: {}\'.format(self._devkit_path)\n        assert os.path.exists(self._data_path), \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'Data\', self._image_set, index + self._image_ext[0])\n        assert os.path.exists(image_path), \'path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._data_path + /ImageSets/val.txt\n\n        if self._image_set == \'train\':\n            image_set_file = os.path.join(self._data_path, \'ImageSets\', \'trainr.txt\')\n            image_index = []\n            if os.path.exists(image_set_file):\n                f = open(image_set_file, \'r\')\n                data = f.read().split()\n                for lines in data:\n                    if lines != \'\':\n                        image_index.append(lines)\n                f.close()\n                return image_index\n\n            for i in range(1,200):\n                print(i)\n                image_set_file = os.path.join(self._data_path, \'ImageSets\', \'DET\', \'train_\' + str(i) + \'.txt\')\n                with open(image_set_file) as f:\n                    tmp_index = [x.strip() for x in f.readlines()]\n                    vtmp_index = []\n                    for line in tmp_index:\n                        line = line.split(\' \')\n                        image_list = os.popen(\'ls \' + self._data_path + \'/Data/DET/train/\' + line[0] + \'/*.JPEG\').read().split()\n                        tmp_list = []\n                        for imgs in image_list:\n                            tmp_list.append(imgs[:-5])\n                        vtmp_index = vtmp_index + tmp_list\n\n                num_lines = len(vtmp_index)\n                ids = np.random.permutation(num_lines)\n                count = 0\n                while count < 2000:\n                    image_index.append(vtmp_index[ids[count % num_lines]])\n                    count = count + 1\n\n            for i in range(1,201):\n                if self._valid_image_flag[i] == 1:\n                    image_set_file = os.path.join(self._data_path, \'ImageSets\', \'train_pos_\' + str(i) + \'.txt\')\n                    with open(image_set_file) as f:\n                        tmp_index = [x.strip() for x in f.readlines()]\n                    num_lines = len(tmp_index)\n                    ids = np.random.permutation(num_lines)\n                    count = 0\n                    while count < 2000:\n                        image_index.append(tmp_index[ids[count % num_lines]])\n                        count = count + 1\n            image_set_file = os.path.join(self._data_path, \'ImageSets\', \'trainr.txt\')\n            f = open(image_set_file, \'w\')\n            for lines in image_index:\n                f.write(lines + \'\\n\')\n            f.close()\n        else:\n            image_set_file = os.path.join(self._data_path, \'ImageSets\', \'val.txt\')\n            with open(image_set_file) as f:\n                image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = pickle.load(fid)\n            print(\'{} gt roidb loaded from {}\'.format(self.name, cache_file))\n            return roidb\n\n        gt_roidb = [self._load_imagenet_annotation(index)\n                    for index in self.image_index]\n        with open(cache_file, \'wb\') as fid:\n            pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n        print(\'wrote gt roidb to {}\'.format(cache_file))\n\n        return gt_roidb\n\n\n    def _load_imagenet_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt files of imagenet.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', self._image_set, index + \'.xml\')\n\n        # print \'Loading: {}\'.format(filename)\n        def get_data_from_tag(node, tag):\n            return node.getElementsByTagName(tag)[0].childNodes[0].data\n\n        with open(filename) as f:\n            data = minidom.parseString(f.read())\n\n        objs = data.getElementsByTagName(\'object\')\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            x1 = float(get_data_from_tag(obj, \'xmin\'))\n            y1 = float(get_data_from_tag(obj, \'ymin\'))\n            x2 = float(get_data_from_tag(obj, \'xmax\'))\n            y2 = float(get_data_from_tag(obj, \'ymax\'))\n            cls = self._wnid_to_ind[\n                    str(get_data_from_tag(obj, ""name"")).lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False}\n\nif __name__ == \'__main__\':\n    d = datasets.imagenet(\'val\', \'\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/imdb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\nimport PIL\n# from model.utils.cython_bbox import bbox_overlaps\nimport numpy as np\nimport scipy.sparse\nfrom model.utils.config import cfg\nimport pdb\n\nROOT_DIR = osp.join(osp.dirname(__file__), \'..\', \'..\')\n\nclass imdb(object):\n  """"""Image database.""""""\n\n  def __init__(self, name, classes=None):\n    self._name = name\n    self._num_classes = 0\n    if not classes:\n      self._classes = []\n    else:\n      self._classes = classes\n    self._image_index = []\n    self._obj_proposer = \'gt\'\n    self._roidb = None\n    self._roidb_handler = self.default_roidb\n    # Use this dict for storing dataset specific config options\n    self.config = {}\n\n  @property\n  def name(self):\n    return self._name\n\n  @property\n  def num_classes(self):\n    return len(self._classes)\n\n  @property\n  def classes(self):\n    return self._classes\n\n  @property\n  def image_index(self):\n    return self._image_index\n\n  @property\n  def roidb_handler(self):\n    return self._roidb_handler\n\n  @roidb_handler.setter\n  def roidb_handler(self, val):\n    self._roidb_handler = val\n\n  def set_proposal_method(self, method):\n    method = eval(\'self.\' + method + \'_roidb\')\n    self.roidb_handler = method\n\n  @property\n  def roidb(self):\n    # A roidb is a list of dictionaries, each with the following keys:\n    #   boxes\n    #   gt_overlaps\n    #   gt_classes\n    #   flipped\n    if self._roidb is not None:\n      return self._roidb\n    self._roidb = self.roidb_handler()\n    return self._roidb\n\n  @property\n  def cache_path(self):\n    cache_path = osp.abspath(osp.join(cfg.DATA_DIR, \'cache\'))\n    if not os.path.exists(cache_path):\n      os.makedirs(cache_path)\n    return cache_path\n\n  @property\n  def num_images(self):\n    return len(self.image_index)\n\n  def image_path_at(self, i):\n    raise NotImplementedError\n\n  def image_id_at(self, i):\n    raise NotImplementedError\n\n  def default_roidb(self):\n    raise NotImplementedError\n\n  def evaluate_detections(self, all_boxes, output_dir=None):\n    """"""\n    all_boxes is a list of length number-of-classes.\n    Each list element is a list of length number-of-images.\n    Each of those list elements is either an empty list []\n    or a numpy array of detection.\n\n    all_boxes[class][image] = [] or np.array of shape #dets x 5\n    """"""\n    raise NotImplementedError\n\n  def _get_widths(self):\n    return [PIL.Image.open(self.image_path_at(i)).size[0]\n            for i in range(self.num_images)]\n\n  def append_flipped_images(self):\n    num_images = self.num_images\n    widths = self._get_widths()\n    for i in range(num_images):\n      boxes = self.roidb[i][\'boxes\'].copy()\n      oldx1 = boxes[:, 0].copy()\n      oldx2 = boxes[:, 2].copy()\n      boxes[:, 0] = widths[i] - oldx2 - 1\n      boxes[:, 2] = widths[i] - oldx1 - 1\n      assert (boxes[:, 2] >= boxes[:, 0]).all()\n      entry = {\'boxes\': boxes,\n               \'gt_overlaps\': self.roidb[i][\'gt_overlaps\'],\n               \'gt_classes\': self.roidb[i][\'gt_classes\'],\n               \'flipped\': True}\n      self.roidb.append(entry)\n    self._image_index = self._image_index * 2\n\n  # def evaluate_recall(self, candidate_boxes=None, thresholds=None,\n  #                     area=\'all\', limit=None):\n  #   """"""Evaluate detection proposal recall metrics.\n  #\n  #   Returns:\n  #       results: dictionary of results with keys\n  #           \'ar\': average recall\n  #           \'recalls\': vector recalls at each IoU overlap threshold\n  #           \'thresholds\': vector of IoU overlap thresholds\n  #           \'gt_overlaps\': vector of all ground-truth overlaps\n  #   """"""\n  #   # Record max overlap value for each gt box\n  #   # Return vector of overlap values\n  #   areas = {\'all\': 0, \'small\': 1, \'medium\': 2, \'large\': 3,\n  #            \'96-128\': 4, \'128-256\': 5, \'256-512\': 6, \'512-inf\': 7}\n  #   area_ranges = [[0 ** 2, 1e5 ** 2],  # all\n  #                  [0 ** 2, 32 ** 2],  # small\n  #                  [32 ** 2, 96 ** 2],  # medium\n  #                  [96 ** 2, 1e5 ** 2],  # large\n  #                  [96 ** 2, 128 ** 2],  # 96-128\n  #                  [128 ** 2, 256 ** 2],  # 128-256\n  #                  [256 ** 2, 512 ** 2],  # 256-512\n  #                  [512 ** 2, 1e5 ** 2],  # 512-inf\n  #                  ]\n  #   assert area in areas, \'unknown area range: {}\'.format(area)\n  #   area_range = area_ranges[areas[area]]\n  #   gt_overlaps = np.zeros(0)\n  #   num_pos = 0\n  #   for i in range(self.num_images):\n  #     # Checking for max_overlaps == 1 avoids including crowd annotations\n  #     # (...pretty hacking :/)\n  #     max_gt_overlaps = self.roidb[i][\'gt_overlaps\'].toarray().max(axis=1)\n  #     gt_inds = np.where((self.roidb[i][\'gt_classes\'] > 0) &\n  #                        (max_gt_overlaps == 1))[0]\n  #     gt_boxes = self.roidb[i][\'boxes\'][gt_inds, :]\n  #     gt_areas = self.roidb[i][\'seg_areas\'][gt_inds]\n  #     valid_gt_inds = np.where((gt_areas >= area_range[0]) &\n  #                              (gt_areas <= area_range[1]))[0]\n  #     gt_boxes = gt_boxes[valid_gt_inds, :]\n  #     num_pos += len(valid_gt_inds)\n  #\n  #     if candidate_boxes is None:\n  #       # If candidate_boxes is not supplied, the default is to use the\n  #       # non-ground-truth boxes from this roidb\n  #       non_gt_inds = np.where(self.roidb[i][\'gt_classes\'] == 0)[0]\n  #       boxes = self.roidb[i][\'boxes\'][non_gt_inds, :]\n  #     else:\n  #       boxes = candidate_boxes[i]\n  #     if boxes.shape[0] == 0:\n  #       continue\n  #     if limit is not None and boxes.shape[0] > limit:\n  #       boxes = boxes[:limit, :]\n  #\n  #     overlaps = bbox_overlaps(boxes.astype(np.float),\n  #                              gt_boxes.astype(np.float))\n  #\n  #     _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n  #     for j in range(gt_boxes.shape[0]):\n  #       # find which proposal box maximally covers each gt box\n  #       argmax_overlaps = overlaps.argmax(axis=0)\n  #       # and get the iou amount of coverage for each gt box\n  #       max_overlaps = overlaps.max(axis=0)\n  #       # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n  #       gt_ind = max_overlaps.argmax()\n  #       gt_ovr = max_overlaps.max()\n  #       assert (gt_ovr >= 0)\n  #       # find the proposal box that covers the best covered gt box\n  #       box_ind = argmax_overlaps[gt_ind]\n  #       # record the iou coverage of this gt box\n  #       _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n  #       assert (_gt_overlaps[j] == gt_ovr)\n  #       # mark the proposal box and the gt box as used\n  #       overlaps[box_ind, :] = -1\n  #       overlaps[:, gt_ind] = -1\n  #     # append recorded iou coverage level\n  #     gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n  #\n  #   gt_overlaps = np.sort(gt_overlaps)\n  #   if thresholds is None:\n  #     step = 0.05\n  #     thresholds = np.arange(0.5, 0.95 + 1e-5, step)\n  #   recalls = np.zeros_like(thresholds)\n  #   # compute recall for each iou threshold\n  #   for i, t in enumerate(thresholds):\n  #     recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n  #   # ar = 2 * np.trapz(recalls, thresholds)\n  #   ar = recalls.mean()\n  #   return {\'ar\': ar, \'recalls\': recalls, \'thresholds\': thresholds,\n  #           \'gt_overlaps\': gt_overlaps}\n\n  def create_roidb_from_box_list(self, box_list, gt_roidb):\n    assert len(box_list) == self.num_images, \\\n      \'Number of boxes must match number of ground-truth images\'\n    roidb = []\n    for i in range(self.num_images):\n      boxes = box_list[i]\n      num_boxes = boxes.shape[0]\n      overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n\n      if gt_roidb is not None and gt_roidb[i][\'boxes\'].size > 0:\n        gt_boxes = gt_roidb[i][\'boxes\']\n        gt_classes = gt_roidb[i][\'gt_classes\']\n        gt_overlaps = bbox_overlaps(boxes.astype(np.float),\n                                    gt_boxes.astype(np.float))\n        argmaxes = gt_overlaps.argmax(axis=1)\n        maxes = gt_overlaps.max(axis=1)\n        I = np.where(maxes > 0)[0]\n        overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n\n      overlaps = scipy.sparse.csr_matrix(overlaps)\n      roidb.append({\n        \'boxes\': boxes,\n        \'gt_classes\': np.zeros((num_boxes,), dtype=np.int32),\n        \'gt_overlaps\': overlaps,\n        \'flipped\': False,\n        \'seg_areas\': np.zeros((num_boxes,), dtype=np.float32),\n      })\n    return roidb\n\n  @staticmethod\n  def merge_roidbs(a, b):\n    assert len(a) == len(b)\n    for i in range(len(a)):\n      a[i][\'boxes\'] = np.vstack((a[i][\'boxes\'], b[i][\'boxes\']))\n      a[i][\'gt_classes\'] = np.hstack((a[i][\'gt_classes\'],\n                                      b[i][\'gt_classes\']))\n      a[i][\'gt_overlaps\'] = scipy.sparse.vstack([a[i][\'gt_overlaps\'],\n                                                 b[i][\'gt_overlaps\']])\n      a[i][\'seg_areas\'] = np.hstack((a[i][\'seg_areas\'],\n                                     b[i][\'seg_areas\']))\n    return a\n\n  def competition_mode(self, on):\n    """"""Turn competition mode on or off.""""""\n    pass\n'"
lib/datasets/pascal_voc.py,0,"b'from __future__ import print_function\nfrom __future__ import absolute_import\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport xml.dom.minidom as minidom\n\nimport os\n# import PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport math\nimport glob\nimport uuid\nimport scipy.io as sio\nimport xml.etree.ElementTree as ET\nimport pickle\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\nfrom . import ds_utils\nfrom .voc_eval import voc_eval\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom model.utils.config import cfg\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n# <<<< obsolete\n\n\nclass pascal_voc(imdb):\n    def __init__(self, image_set, year, devkit_path=None):\n        imdb.__init__(self, \'voc_\' + year + \'_\' + image_set)\n        self._year = year\n        self._image_set = image_set\n        self._devkit_path = self._get_default_path() if devkit_path is None \\\n            else devkit_path\n        self._data_path = os.path.join(self._devkit_path, \'VOC\' + self._year)\n        self._classes = (\'__background__\',  # always index 0\n                         \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        # self._roidb_handler = self.selective_search_roidb\n        self._roidb_handler = self.gt_roidb\n        self._salt = str(uuid.uuid4())\n        self._comp_id = \'comp4\'\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\': True,\n                       \'use_salt\': True,\n                       \'use_diff\': False,\n                       \'matlab_eval\': False,\n                       \'rpn_file\': None,\n                       \'min_size\': 2}\n\n        assert os.path.exists(self._devkit_path), \\\n            \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n        assert os.path.exists(self._data_path), \\\n            \'Path does not exist: {}\'.format(self._data_path)\n        self.cat_data = {}\n\n        for i in self._class_to_ind.values():\n            # i = 0~20\n            self.cat_data[i] = []\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_id_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return i\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n            \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n            \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(cfg.DATA_DIR, \'VOCdevkit\' + self._year)\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                [roidb, self.cat_data] = pickle.load(fid)\n            print(\'{} gt roidb loaded from {}\'.format(self.name, cache_file))\n            return roidb\n\n        gt_roidb = [self._load_pascal_annotation(index)\n                    for index in self.image_index]\n        with open(cache_file, \'wb\') as fid:\n            pickle.dump([gt_roidb,self.cat_data], fid, pickle.HIGHEST_PROTOCOL)\n        print(\'wrote gt roidb to {}\'.format(cache_file))\n\n        return gt_roidb\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = pickle.load(fid)\n            print(\'{} ss roidb loaded from {}\'.format(self.name, cache_file))\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            pickle.dump(roidb, fid, pickle.HIGHEST_PROTOCOL)\n        print(\'wrote ss roidb to {}\'.format(cache_file))\n\n        return roidb\n\n    def rpn_roidb(self):\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            rpn_roidb = self._load_rpn_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, rpn_roidb)\n        else:\n            roidb = self._load_rpn_roidb(None)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb):\n        filename = self.config[\'rpn_file\']\n        print(\'loading {}\'.format(filename))\n        assert os.path.exists(filename), \\\n            \'rpn data not found at: {}\'.format(filename)\n        with open(filename, \'rb\') as f:\n            box_list = pickle.load(f)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(cfg.DATA_DIR,\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n            \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            boxes = raw_data[i][:, (1, 0, 3, 2)] - 1\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            box_list.append(boxes)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        tree = ET.parse(filename)\n        objs = tree.findall(\'object\')\n        # if not self.config[\'use_diff\']:\n        #     # Exclude the samples labeled as difficult\n        #     non_diff_objs = [\n        #         obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n        #     # if len(non_diff_objs) != len(objs):\n        #     #     print \'Removed {} difficult objects\'.format(\n        #     #         len(objs) - len(non_diff_objs))\n        #     objs = non_diff_objs\n        num_objs = len(objs)\n        im_path = self.image_path_from_index(index)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # ""Seg"" area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n        ishards = np.zeros((num_objs), dtype=np.int32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(\'bndbox\')\n            # Make pixel indexes 0-based\n            x1 = float(bbox.find(\'xmin\').text) \n            y1 = float(bbox.find(\'ymin\').text) \n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n\n\n            diffc = obj.find(\'difficult\')\n            difficult = 0 if diffc == None else int(diffc.text)\n            ishards[ix] = difficult\n\n            cls = self._class_to_ind[obj.find(\'name\').text.lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n            entry = {\n               \'boxes\': [x1, y1, x2, y2],\n               \'image_path\': im_path\n               }\n            self.cat_data[cls].append(entry)\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {\'boxes\': boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_ishard\': ishards,\n                \'gt_overlaps\': overlaps,\n                \'flipped\': False,\n                \'seg_areas\': seg_areas}\n\n    def _get_comp_id(self):\n        comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n                   else self._comp_id)\n        return comp_id\n\n    def _get_voc_results_file_template(self):\n        # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n        filename = self._get_comp_id() + \'_det_\' + self._image_set + \'_{:s}.txt\'\n        filedir = os.path.join(self._devkit_path, \'results\', \'VOC\' + self._year, \'Main\')\n        if not os.path.exists(filedir):\n            os.makedirs(filedir)\n        path = os.path.join(filedir, filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print(\'Writing {} VOC results file\'.format(cls))\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, -1],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir=\'output\'):\n        annopath = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'Annotations\',\n            \'{:s}.xml\')\n        imagesetfile = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'ImageSets\',\n            \'Main\',\n            self._image_set + \'.txt\')\n        cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = True if int(self._year) < 2010 else False\n        print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n        if not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(\n                filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n                use_07_metric=use_07_metric)\n            aps += [ap]\n            print(\'AP for {} = {:.4f}\'.format(cls, ap))\n            with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n                pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print(\'{:.3f}\'.format(ap))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n        return aps \n\n    def _do_matlab_eval(self, output_dir=\'output\'):\n        print(\'-----------------------------------------------------\')\n        print(\'Computing results with the official MATLAB eval code.\')\n        print(\'-----------------------------------------------------\')\n        path = os.path.join(cfg.ROOT_DIR, \'lib\', \'datasets\',\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n            .format(self._devkit_path, self._get_comp_id(),\n                    self._image_set, output_dir)\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        self._write_voc_results_file(all_boxes)\n        aps = self._do_python_eval(output_dir)\n        if self.config[\'matlab_eval\']:\n            self._do_matlab_eval(output_dir)\n        if self.config[\'cleanup\']:\n            for cls in self._classes:\n                if cls == \'__background__\':\n                    continue\n                filename = self._get_voc_results_file_template().format(cls)\n                os.remove(filename)\n        return aps\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\n    def filter(self, seen=1):\n        if seen==1:\n            self.list = [2,3,4,5,6,7,9,11,12,13,14,15,16,18,19,20]\n        elif seen==2:\n            self.list = [1,8,10,17]\n        elif seen==3:\n            self.list = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n        \n        \n\n        self.inverse_list = self.list \n        out = list(range(len(self._image_index)))\n\n        for index,tmp in enumerate(self.roidb):\n            for j in tmp[\'gt_classes\']:\n                if j in self.list:\n                    out.remove(index)\n                    break\n        out.reverse()\n\n        for tmp in out:\n            self._image_index.pop(tmp)\n            self.roidb.pop(tmp)\n\nif __name__ == \'__main__\':\n    d = pascal_voc(\'trainval\', \'2007\')\n    res = d.roidb\n    from IPython import embed;\n\n    embed()\n'"
lib/datasets/pascal_voc_rbg.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\n# import model.utils.cython_bbox\nimport pickle\nimport subprocess\nimport uuid\nfrom .voc_eval import voc_eval\nfrom model.utils.config import cfg\nimport pdb\n\n\nclass pascal_voc(imdb):\n  def __init__(self, image_set, year, devkit_path=None):\n    imdb.__init__(self, \'voc_\' + year + \'_\' + image_set)\n    self._year = year\n    self._image_set = image_set\n    self._devkit_path = self._get_default_path() if devkit_path is None \\\n      else devkit_path\n\n\n    self._data_path = os.path.join(self._devkit_path, \'VOC\' + self._year)\n    self._classes = (\'__background__\',  # always index 0\n                     \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                     \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                     \'cow\', \'diningtable\', \'dog\', \'horse\',\n                     \'motorbike\', \'person\', \'pottedplant\',\n                     \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n    self._class_to_ind = dict(list(zip(self.classes, list(range(self.num_classes)))))\n    self._image_ext = \'.jpg\'\n    self._image_index = self._load_image_set_index()\n    # Default to roidb handler\n    self._roidb_handler = self.gt_roidb\n    self._salt = str(uuid.uuid4())\n    self._comp_id = \'comp4\'\n\n    # PASCAL specific config options\n    self.config = {\'cleanup\': True,\n                   \'use_salt\': True,\n                   \'use_diff\': False,\n                   \'matlab_eval\': False,\n                   \'rpn_file\': None}\n\n    assert os.path.exists(self._devkit_path), \\\n      \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n    assert os.path.exists(self._data_path), \\\n      \'Path does not exist: {}\'.format(self._data_path)\n\n  def image_path_at(self, i):\n    """"""\n    Return the absolute path to image i in the image sequence.\n    """"""\n    return self.image_path_from_index(self._image_index[i])\n\n  def image_path_from_index(self, index):\n    """"""\n    Construct an image path from the image\'s ""index"" identifier.\n    """"""\n    image_path = os.path.join(self._data_path, \'JPEGImages\',\n                              index + self._image_ext)\n    assert os.path.exists(image_path), \\\n      \'Path does not exist: {}\'.format(image_path)\n    return image_path\n\n  def _load_image_set_index(self):\n    """"""\n    Load the indexes listed in this dataset\'s image set file.\n    """"""\n    # Example path to image set file:\n    # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n    image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                  self._image_set + \'.txt\')\n\n    assert os.path.exists(image_set_file), \\\n      \'Path does not exist: {}\'.format(image_set_file)\n    with open(image_set_file) as f:\n      image_index = [x.strip() for x in f.readlines()]\n    return image_index\n\n  def _get_default_path(self):\n    """"""\n    Return the default path where PASCAL VOC is expected to be installed.\n    """"""\n    return os.path.join(cfg.DATA_DIR, \'VOCdevkit\' + self._year)\n\n  def gt_roidb(self):\n    """"""\n    Return the database of ground-truth regions of interest.\n\n    This function loads/saves from/to a cache file to speed up future calls.\n    """"""\n    cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n    if os.path.exists(cache_file):\n      with open(cache_file, \'rb\') as fid:\n        try:\n          roidb = pickle.load(fid)\n        except:\n          roidb = pickle.load(fid, encoding=\'bytes\')\n      print(\'{} gt roidb loaded from {}\'.format(self.name, cache_file))\n      return roidb\n\n    gt_roidb = [self._load_pascal_annotation(index)\n                for index in self.image_index]\n    with open(cache_file, \'wb\') as fid:\n      pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n    print(\'wrote gt roidb to {}\'.format(cache_file))\n\n    return gt_roidb\n\n  def rpn_roidb(self):\n    if int(self._year) == 2007 or self._image_set != \'test\':\n      gt_roidb = self.gt_roidb()\n      rpn_roidb = self._load_rpn_roidb(gt_roidb)\n      roidb = imdb.merge_roidbs(gt_roidb, rpn_roidb)\n    else:\n      roidb = self._load_rpn_roidb(None)\n\n    return roidb\n\n  def _load_rpn_roidb(self, gt_roidb):\n    filename = self.config[\'rpn_file\']\n    print(\'loading {}\'.format(filename))\n    assert os.path.exists(filename), \\\n      \'rpn data not found at: {}\'.format(filename)\n    with open(filename, \'rb\') as f:\n      box_list = pickle.load(f)\n    return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n  def _load_pascal_annotation(self, index):\n    """"""\n    Load image and bounding boxes info from XML file in the PASCAL VOC\n    format.\n    """"""\n    filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n    tree = ET.parse(filename)\n    objs = tree.findall(\'object\')\n    if not self.config[\'use_diff\']:\n      # Exclude the samples labeled as difficult\n      non_diff_objs = [\n        obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n      # if len(non_diff_objs) != len(objs):\n      #     print \'Removed {} difficult objects\'.format(\n      #         len(objs) - len(non_diff_objs))\n      objs = non_diff_objs\n    num_objs = len(objs)\n\n    boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n    gt_classes = np.zeros((num_objs), dtype=np.int32)\n    overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n    # ""Seg"" area for pascal is just the box area\n    seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n    # Load object bounding boxes into a data frame.\n    for ix, obj in enumerate(objs):\n      bbox = obj.find(\'bndbox\')\n      # Make pixel indexes 0-based\n      x1 = float(bbox.find(\'xmin\').text) - 1\n      y1 = float(bbox.find(\'ymin\').text) - 1\n      x2 = float(bbox.find(\'xmax\').text) - 1\n      y2 = float(bbox.find(\'ymax\').text) - 1\n      cls = self._class_to_ind[obj.find(\'name\').text.lower().strip()]\n      boxes[ix, :] = [x1, y1, x2, y2]\n      gt_classes[ix] = cls\n      overlaps[ix, cls] = 1.0\n      seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n    overlaps = scipy.sparse.csr_matrix(overlaps)\n\n    return {\'boxes\': boxes,\n            \'gt_classes\': gt_classes,\n            \'gt_overlaps\': overlaps,\n            \'flipped\': False,\n            \'seg_areas\': seg_areas}\n\n  def _get_comp_id(self):\n    comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n               else self._comp_id)\n    return comp_id\n\n  def _get_voc_results_file_template(self):\n    # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n    filename = self._get_comp_id() + \'_det_\' + self._image_set + \'_{:s}.txt\'\n    path = os.path.join(\n      self._devkit_path,\n      \'results\',\n      \'VOC\' + self._year,\n      \'Main\',\n      filename)\n    return path\n\n  def _write_voc_results_file(self, all_boxes):\n    for cls_ind, cls in enumerate(self.classes):\n      if cls == \'__background__\':\n        continue\n      print(\'Writing {} VOC results file\'.format(cls))\n      filename = self._get_voc_results_file_template().format(cls)\n      with open(filename, \'wt\') as f:\n        for im_ind, index in enumerate(self.image_index):\n          dets = all_boxes[cls_ind][im_ind]\n          if dets == []:\n            continue\n          # the VOCdevkit expects 1-based indices\n          for k in range(dets.shape[0]):\n            f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                    format(index, dets[k, -1],\n                           dets[k, 0] + 1, dets[k, 1] + 1,\n                           dets[k, 2] + 1, dets[k, 3] + 1))\n\n  def _do_python_eval(self, output_dir=\'output\'):\n    annopath = os.path.join(\n      self._devkit_path,\n      \'VOC\' + self._year,\n      \'Annotations\',\n      \'{:s}.xml\')\n    imagesetfile = os.path.join(\n      self._devkit_path,\n      \'VOC\' + self._year,\n      \'ImageSets\',\n      \'Main\',\n      self._image_set + \'.txt\')\n    cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = True if int(self._year) < 2010 else False\n    print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n    if not os.path.isdir(output_dir):\n      os.mkdir(output_dir)\n    for i, cls in enumerate(self._classes):\n      if cls == \'__background__\':\n        continue\n      filename = self._get_voc_results_file_template().format(cls)\n      rec, prec, ap = voc_eval(\n        filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n        use_07_metric=use_07_metric)\n      aps += [ap]\n      print((\'AP for {} = {:.4f}\'.format(cls, ap)))\n      with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n        pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n    print((\'Mean AP = {:.4f}\'.format(np.mean(aps))))\n    print(\'~~~~~~~~\')\n    print(\'Results:\')\n    for ap in aps:\n      print((\'{:.3f}\'.format(ap)))\n    print((\'{:.3f}\'.format(np.mean(aps))))\n    print(\'~~~~~~~~\')\n    print(\'\')\n    print(\'--------------------------------------------------------------\')\n    print(\'Results computed with the **unofficial** Python eval code.\')\n    print(\'Results should be very close to the official MATLAB eval code.\')\n    print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n    print(\'-- Thanks, The Management\')\n    print(\'--------------------------------------------------------------\')\n\n  def _do_matlab_eval(self, output_dir=\'output\'):\n    print(\'-----------------------------------------------------\')\n    print(\'Computing results with the official MATLAB eval code.\')\n    print(\'-----------------------------------------------------\')\n    path = os.path.join(cfg.ROOT_DIR, \'lib\', \'datasets\',\n                        \'VOCdevkit-matlab-wrapper\')\n    cmd = \'cd {} && \'.format(path)\n    cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n    cmd += \'-r ""dbstop if error; \'\n    cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n      .format(self._devkit_path, self._get_comp_id(),\n              self._image_set, output_dir)\n    print((\'Running:\\n{}\'.format(cmd)))\n    status = subprocess.call(cmd, shell=True)\n\n  def evaluate_detections(self, all_boxes, output_dir):\n    pdb.set_trace()\n    self._write_voc_results_file(all_boxes)\n    self._do_python_eval(output_dir)\n    if self.config[\'matlab_eval\']:\n      self._do_matlab_eval(output_dir)\n    if self.config[\'cleanup\']:\n      for cls in self._classes:\n        if cls == \'__background__\':\n          continue\n        filename = self._get_voc_results_file_template().format(cls)\n        os.remove(filename)\n\n  def competition_mode(self, on):\n    if on:\n      self.config[\'use_salt\'] = False\n      self.config[\'cleanup\'] = False\n    else:\n      self.config[\'use_salt\'] = True\n      self.config[\'cleanup\'] = True\n\n\nif __name__ == \'__main__\':\n  from datasets.pascal_voc import pascal_voc\n\n  d = pascal_voc(\'trainval\', \'2007\')\n  res = d.roidb\n  from IPython import embed;\n\n  embed()\n'"
lib/datasets/vg.py,0,"b'from __future__ import print_function\nfrom __future__ import absolute_import\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport scipy.sparse\nimport gzip\nimport PIL\nimport json\nfrom .vg_eval import vg_eval\nfrom model.utils.config import cfg\nimport pickle\nimport pdb\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n\nclass vg(imdb):\n    def __init__(self, version, image_set, ):\n        imdb.__init__(self, \'vg_\' + version + \'_\' + image_set)\n        self._version = version\n        self._image_set = image_set\n        self._data_path = os.path.join(cfg.DATA_DIR, \'genome\')\n        self._img_path = os.path.join(cfg.DATA_DIR, \'vg\')\n        # VG specific config options\n        self.config = {\'cleanup\' : False}\n\n        # Load classes\n        self._classes = [\'__background__\']\n        self._class_to_ind = {}\n        self._class_to_ind[self._classes[0]] = 0\n        with open(os.path.join(self._data_path, self._version, \'objects_vocab.txt\')) as f:\n          count = 1\n          for object in f.readlines():\n            names = [n.lower().strip() for n in object.split(\',\')]\n            self._classes.append(names[0])\n            for n in names:\n              self._class_to_ind[n] = count\n            count += 1\n\n        # Load attributes\n        self._attributes = [\'__no_attribute__\']\n        self._attribute_to_ind = {}\n        self._attribute_to_ind[self._attributes[0]] = 0\n        with open(os.path.join(self._data_path, self._version, \'attributes_vocab.txt\')) as f:\n          count = 1\n          for att in f.readlines():\n            names = [n.lower().strip() for n in att.split(\',\')]\n            self._attributes.append(names[0])\n            for n in names:\n              self._attribute_to_ind[n] = count\n            count += 1\n\n        # Load relations\n        self._relations = [\'__no_relation__\']\n        self._relation_to_ind = {}\n        self._relation_to_ind[self._relations[0]] = 0\n        with open(os.path.join(self._data_path, self._version, \'relations_vocab.txt\')) as f:\n          count = 1\n          for rel in f.readlines():\n            names = [n.lower().strip() for n in rel.split(\',\')]\n            self._relations.append(names[0])\n            for n in names:\n              self._relation_to_ind[n] = count\n            count += 1\n\n\n        self._image_ext = \'.jpg\'\n        load_index_from_file = False\n        if os.path.exists(os.path.join(self._data_path, ""vg_image_index_{}.p"".format(self._image_set))):\n            with open(os.path.join(self._data_path, ""vg_image_index_{}.p"".format(self._image_set)), \'rb\') as fp:\n                self._image_index = pickle.load(fp)\n            load_index_from_file = True\n\n        load_id_from_file = False\n        if os.path.exists(os.path.join(self._data_path, ""vg_id_to_dir_{}.p"".format(self._image_set))):\n            with open(os.path.join(self._data_path, ""vg_id_to_dir_{}.p"".format(self._image_set)), \'rb\') as fp:\n                self._id_to_dir = pickle.load(fp)\n            load_id_from_file = True\n\n        if not load_index_from_file or not load_id_from_file:\n            self._image_index, self._id_to_dir = self._load_image_set_index()\n            with open(os.path.join(self._data_path, ""vg_image_index_{}.p"".format(self._image_set)), \'wb\') as fp:\n                pickle.dump(self._image_index, fp)\n            with open(os.path.join(self._data_path, ""vg_id_to_dir_{}.p"".format(self._image_set)), \'wb\') as fp:\n                pickle.dump(self._id_to_dir, fp)\n\n        self._roidb_handler = self.gt_roidb\n\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_id_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return i\n        # return self._image_index[i]\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        folder = self._id_to_dir[index]\n        image_path = os.path.join(self._img_path, folder,\n                                  str(index) + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _image_split_path(self):\n        if self._image_set == ""minitrain"":\n          return os.path.join(self._data_path, \'train.txt\')\n        if self._image_set == ""smalltrain"":\n          return os.path.join(self._data_path, \'train.txt\')\n        if self._image_set == ""minival"":\n          return os.path.join(self._data_path, \'val.txt\')\n        if self._image_set == ""smallval"":\n          return os.path.join(self._data_path, \'val.txt\')\n        else:\n          return os.path.join(self._data_path, self._image_set+\'.txt\')\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        training_split_file = self._image_split_path()\n        assert os.path.exists(training_split_file), \\\n                \'Path does not exist: {}\'.format(training_split_file)\n        with open(training_split_file) as f:\n          metadata = f.readlines()\n          if self._image_set == ""minitrain"":\n            metadata = metadata[:1000]\n          elif self._image_set == ""smalltrain"":\n            metadata = metadata[:20000]\n          elif self._image_set == ""minival"":\n            metadata = metadata[:100]\n          elif self._image_set == ""smallval"":\n            metadata = metadata[:2000]\n\n        image_index = []\n        id_to_dir = {}\n        for line in metadata:\n          im_file,ann_file = line.split()\n          image_id = int(ann_file.split(\'/\')[-1].split(\'.\')[0])\n          filename = self._annotation_path(image_id)\n          if os.path.exists(filename):\n              # Some images have no bboxes after object filtering, so there\n              # is no xml annotation for these.\n              tree = ET.parse(filename)\n              for obj in tree.findall(\'object\'):\n                  obj_name = obj.find(\'name\').text.lower().strip()\n                  if obj_name in self._class_to_ind:\n                      # We have to actually load and check these to make sure they have\n                      # at least one object actually in vocab\n                      image_index.append(image_id)\n                      id_to_dir[image_id] = im_file.split(\'/\')[0]\n                      break\n        return image_index, id_to_dir\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            fid = gzip.open(cache_file,\'rb\')\n            roidb = pickle.load(fid)\n            fid.close()\n            print(\'{} gt roidb loaded from {}\'.format(self.name, cache_file))\n            return roidb\n\n        gt_roidb = [self._load_vg_annotation(index)\n                    for index in self.image_index]\n        fid = gzip.open(cache_file,\'wb\')\n        pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n        fid.close()\n        print(\'wrote gt roidb to {}\'.format(cache_file))\n        return gt_roidb\n\n    def _get_size(self, index):\n      return PIL.Image.open(self.image_path_from_index(index)).size\n\n    def _annotation_path(self, index):\n        return os.path.join(self._data_path, \'xml\', str(index) + \'.xml\')\n\n    def _load_vg_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        width, height = self._get_size(index)\n        filename = self._annotation_path(index)\n        tree = ET.parse(filename)\n        objs = tree.findall(\'object\')\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        # Max of 16 attributes are observed in the data\n        gt_attributes = np.zeros((num_objs, 16), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # ""Seg"" area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        obj_dict = {}\n        ix = 0\n        for obj in objs:\n            obj_name = obj.find(\'name\').text.lower().strip()\n            if obj_name in self._class_to_ind:\n                bbox = obj.find(\'bndbox\')\n                x1 = max(0,float(bbox.find(\'xmin\').text))\n                y1 = max(0,float(bbox.find(\'ymin\').text))\n                x2 = min(width-1,float(bbox.find(\'xmax\').text))\n                y2 = min(height-1,float(bbox.find(\'ymax\').text))\n                # If bboxes are not positive, just give whole image coords (there are a few examples)\n                if x2 < x1 or y2 < y1:\n                    print(\'Failed bbox in %s, object %s\' % (filename, obj_name))\n                    x1 = 0\n                    y1 = 0\n                    x2 = width-1\n                    y2 = width-1\n                cls = self._class_to_ind[obj_name]\n                obj_dict[obj.find(\'object_id\').text] = ix\n                atts = obj.findall(\'attribute\')\n                n = 0\n                for att in atts:\n                    att = att.text.lower().strip()\n                    if att in self._attribute_to_ind:\n                        gt_attributes[ix, n] = self._attribute_to_ind[att]\n                        n += 1\n                    if n >= 16:\n                        break\n                boxes[ix, :] = [x1, y1, x2, y2]\n                gt_classes[ix] = cls\n                overlaps[ix, cls] = 1.0\n                seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n                ix += 1\n        # clip gt_classes and gt_relations\n        gt_classes = gt_classes[:ix]\n        gt_attributes = gt_attributes[:ix, :]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_attributes = scipy.sparse.csr_matrix(gt_attributes)\n\n        rels = tree.findall(\'relation\')\n        num_rels = len(rels)\n        gt_relations = set() # Avoid duplicates\n        for rel in rels:\n            pred = rel.find(\'predicate\').text\n            if pred: # One is empty\n                pred = pred.lower().strip()\n                if pred in self._relation_to_ind:\n                    try:\n                        triple = []\n                        triple.append(obj_dict[rel.find(\'subject_id\').text])\n                        triple.append(self._relation_to_ind[pred])\n                        triple.append(obj_dict[rel.find(\'object_id\').text])\n                        gt_relations.add(tuple(triple))\n                    except:\n                        pass # Object not in dictionary\n        gt_relations = np.array(list(gt_relations), dtype=np.int32)\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_attributes\' : gt_attributes,\n                \'gt_relations\' : gt_relations,\n                \'gt_overlaps\' : overlaps,\n                \'width\' : width,\n                \'height\': height,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        self._write_voc_results_file(self.classes, all_boxes, output_dir)\n        self._do_python_eval(output_dir)\n        if self.config[\'cleanup\']:\n            for cls in self._classes:\n                if cls == \'__background__\':\n                    continue\n                filename = self._get_vg_results_file_template(output_dir).format(cls)\n                os.remove(filename)\n\n    def evaluate_attributes(self, all_boxes, output_dir):\n        self._write_voc_results_file(self.attributes, all_boxes, output_dir)\n        self._do_python_eval(output_dir, eval_attributes = True)\n        if self.config[\'cleanup\']:\n            for cls in self._attributes:\n                if cls == \'__no_attribute__\':\n                    continue\n                filename = self._get_vg_results_file_template(output_dir).format(cls)\n                os.remove(filename)\n\n    def _get_vg_results_file_template(self, output_dir):\n        filename = \'detections_\' + self._image_set + \'_{:s}.txt\'\n        path = os.path.join(output_dir, filename)\n        return path\n\n    def _write_voc_results_file(self, classes, all_boxes, output_dir):\n        for cls_ind, cls in enumerate(classes):\n            if cls == \'__background__\':\n                continue\n            print(\'Writing ""{}"" vg results file\'.format(cls))\n            filename = self._get_vg_results_file_template(output_dir).format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(str(index), dets[k, -1],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n\n    def _do_python_eval(self, output_dir, pickle=True, eval_attributes = False):\n        # We re-use parts of the pascal voc python code for visual genome\n        aps = []\n        nposs = []\n        thresh = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = False\n        print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n        if not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        # Load ground truth\n        gt_roidb = self.gt_roidb()\n        if eval_attributes:\n            classes = self._attributes\n        else:\n            classes = self._classes\n        for i, cls in enumerate(classes):\n            if cls == \'__background__\' or cls == \'__no_attribute__\':\n                continue\n            filename = self._get_vg_results_file_template(output_dir).format(cls)\n            rec, prec, ap, scores, npos = vg_eval(\n                filename, gt_roidb, self.image_index, i, ovthresh=0.5,\n                use_07_metric=use_07_metric, eval_attributes=eval_attributes)\n\n            # Determine per class detection thresholds that maximise f score\n            if npos > 1:\n                f = np.nan_to_num((prec*rec)/(prec+rec))\n                thresh += [scores[np.argmax(f)]]\n            else:\n                thresh += [0]\n            aps += [ap]\n            nposs += [float(npos)]\n            print(\'AP for {} = {:.4f} (npos={:,})\'.format(cls, ap, npos))\n            if pickle:\n                with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n                    pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap,\n                        \'scores\': scores, \'npos\':npos}, f)\n\n        # Set thresh to mean for classes with poor results\n        thresh = np.array(thresh)\n        avg_thresh = np.mean(thresh[thresh!=0])\n        thresh[thresh==0] = avg_thresh\n        if eval_attributes:\n            filename = \'attribute_thresholds_\' + self._image_set + \'.txt\'\n        else:\n            filename = \'object_thresholds_\' + self._image_set + \'.txt\'\n        path = os.path.join(output_dir, filename)\n        with open(path, \'wt\') as f:\n            for i, cls in enumerate(classes[1:]):\n                f.write(\'{:s} {:.3f}\\n\'.format(cls, thresh[i]))\n\n        weights = np.array(nposs)\n        weights /= weights.sum()\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'Weighted Mean AP = {:.4f}\'.format(np.average(aps, weights=weights)))\n        print(\'Mean Detection Threshold = {:.3f}\'.format(avg_thresh))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap,npos in zip(aps,nposs):\n            print(\'{:.3f}\\t{:.3f}\'.format(ap,npos))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** PASCAL VOC Python eval code.\')\n        print(\'--------------------------------------------------------------\')\n\n\nif __name__ == \'__main__\':\n    d = vg(\'val\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
lib/datasets/vg_eval.py,0,"b'from __future__ import absolute_import\n# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os\nimport numpy as np\nfrom .voc_eval import voc_ap\n\ndef vg_eval( detpath,\n             gt_roidb,\n             image_index,\n             classindex,\n             ovthresh=0.5,\n             use_07_metric=False,\n             eval_attributes=False):\n    """"""rec, prec, ap, sorted_scores, npos = voc_eval(\n                                detpath, \n                                gt_roidb,\n                                image_index,\n                                classindex,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the Visual Genome evaluation.\n\n    detpath: Path to detections\n    gt_roidb: List of ground truth structs.\n    image_index: List of image ids.\n    classindex: Category index\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for item,imagename in zip(gt_roidb,image_index):\n        if eval_attributes:\n            bbox = item[\'boxes\'][np.where(np.any(item[\'gt_attributes\'].toarray() == classindex, axis=1))[0], :]\n        else:\n            bbox = item[\'boxes\'][np.where(item[\'gt_classes\'] == classindex)[0], :]\n        difficult = np.zeros((bbox.shape[0],)).astype(np.bool)\n        det = [False] * bbox.shape[0]\n        npos = npos + sum(~difficult)        \n        class_recs[str(imagename)] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n    if npos == 0:\n        # No ground truth examples\n        return 0,0,0,0,npos\n\n    # read dets\n    with open(detpath, \'r\') as f:\n        lines = f.readlines()\n    if len(lines) == 0:\n        # No detection examples\n        return 0,0,0,0,npos\n\n    splitlines = [x.strip().split(\' \') for x in lines]\n    image_ids = [x[0] for x in splitlines]\n    confidence = np.array([float(x[1]) for x in splitlines])\n    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = -np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        bb = BB[d, :].astype(float)\n        ovmax = -np.inf\n        BBGT = R[\'bbox\'].astype(float)\n\n        if BBGT.size > 0:\n            # compute overlaps\n            # intersection\n            ixmin = np.maximum(BBGT[:, 0], bb[0])\n            iymin = np.maximum(BBGT[:, 1], bb[1])\n            ixmax = np.minimum(BBGT[:, 2], bb[2])\n            iymax = np.minimum(BBGT[:, 3], bb[3])\n            iw = np.maximum(ixmax - ixmin + 1., 0.)\n            ih = np.maximum(iymax - iymin + 1., 0.)\n            inters = iw * ih\n\n            # union\n            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n            overlaps = inters / uni\n            ovmax = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n\n        if ovmax > ovthresh:\n            if not R[\'difficult\'][jmax]:\n                if not R[\'det\'][jmax]:\n                    tp[d] = 1.\n                    R[\'det\'][jmax] = 1\n                else:\n                    fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n    # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(npos)\n    # avoid divide by zero in case the first detection matches a difficult\n    # ground truth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = voc_ap(rec, prec, use_07_metric)\n    \n    return rec, prec, ap, sorted_scores, npos\n'"
lib/datasets/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\ndef parse_rec(filename):\n  """""" Parse a PASCAL VOC xml file """"""\n  tree = ET.parse(filename)\n  objects = []\n  for obj in tree.findall(\'object\'):\n    obj_struct = {}\n    obj_struct[\'name\'] = obj.find(\'name\').text\n    obj_struct[\'pose\'] = obj.find(\'pose\').text\n    obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n    obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n    bbox = obj.find(\'bndbox\')\n    obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                          int(bbox.find(\'ymin\').text),\n                          int(bbox.find(\'xmax\').text),\n                          int(bbox.find(\'ymax\').text)]\n    objects.append(obj_struct)\n\n  return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  """""" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  """"""\n  if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap\n\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False):\n  """"""rec, prec, ap = voc_eval(detpath,\n                              annopath,\n                              imagesetfile,\n                              classname,\n                              [ovthresh],\n                              [use_07_metric])\n\n  Top level function that does the PASCAL VOC evaluation.\n\n  detpath: Path to detections\n      detpath.format(classname) should produce the detection results file.\n  annopath: Path to annotations\n      annopath.format(imagename) should be the xml annotations file.\n  imagesetfile: Text file containing the list of images, one image per line.\n  classname: Category name (duh)\n  cachedir: Directory for caching the annotations\n  [ovthresh]: Overlap threshold (default = 0.5)\n  [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n      (default False)\n  """"""\n  # assumes detections are in detpath.format(classname)\n  # assumes annotations are in annopath.format(imagename)\n  # assumes imagesetfile is a text file with each line an image name\n  # cachedir caches the annotations in a pickle file\n\n  # first load gt\n  if not os.path.isdir(cachedir):\n    os.mkdir(cachedir)\n  cachefile = os.path.join(cachedir, \'%s_annots.pkl\' % imagesetfile)\n  # read list of images\n  with open(imagesetfile, \'r\') as f:\n    lines = f.readlines()\n  imagenames = [x.strip() for x in lines]\n\n  if not os.path.isfile(cachefile):\n    # load annotations\n    recs = {}\n    for i, imagename in enumerate(imagenames):\n      recs[imagename] = parse_rec(annopath.format(imagename))\n      if i % 100 == 0:\n        print(\'Reading annotation for {:d}/{:d}\'.format(\n          i + 1, len(imagenames)))\n    # save\n    print(\'Saving cached annotations to {:s}\'.format(cachefile))\n    with open(cachefile, \'wb\') as f:\n      pickle.dump(recs, f)\n  else:\n    # load\n    with open(cachefile, \'rb\') as f:\n      try:\n        recs = pickle.load(f)\n      except:\n        recs = pickle.load(f, encoding=\'bytes\')\n\n  # extract gt objects for this class\n  class_recs = {}\n  npos = 0\n  for imagename in imagenames:\n    R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n    bbox = np.array([x[\'bbox\'] for x in R])\n    difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n    det = [False] * len(R)\n    npos = npos + sum(~difficult)\n    class_recs[imagename] = {\'bbox\': bbox,\n                             \'difficult\': difficult,\n                             \'det\': det}\n\n  # read dets\n  detfile = detpath.format(classname)\n  with open(detfile, \'r\') as f:\n    lines = f.readlines()\n\n  splitlines = [x.strip().split(\' \') for x in lines]\n  image_ids = [x[0] for x in splitlines]\n  confidence = np.array([float(x[1]) for x in splitlines])\n  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n  nd = len(image_ids)\n  tp = np.zeros(nd)\n  fp = np.zeros(nd)\n\n  if BB.shape[0] > 0:\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    for d in range(nd):\n      R = class_recs[image_ids[d]]\n      bb = BB[d, :].astype(float)\n      ovmax = -np.inf\n      BBGT = R[\'bbox\'].astype(float)\n\n      if BBGT.size > 0:\n        # compute overlaps\n        # intersection\n        ixmin = np.maximum(BBGT[:, 0], bb[0])\n        iymin = np.maximum(BBGT[:, 1], bb[1])\n        ixmax = np.minimum(BBGT[:, 2], bb[2])\n        iymax = np.minimum(BBGT[:, 3], bb[3])\n        iw = np.maximum(ixmax - ixmin + 1., 0.)\n        ih = np.maximum(iymax - iymin + 1., 0.)\n        inters = iw * ih\n\n        # union\n        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n               (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n        overlaps = inters / uni\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n\n      if ovmax > ovthresh:\n        if not R[\'difficult\'][jmax]:\n          if not R[\'det\'][jmax]:\n            tp[d] = 1.\n            R[\'det\'][jmax] = 1\n          else:\n            fp[d] = 1.\n      else:\n        fp[d] = 1.\n\n  # compute precision recall\n  fp = np.cumsum(fp)\n  tp = np.cumsum(tp)\n  rec = tp / float(npos)\n  # avoid divide by zero in case the first detection matches a difficult\n  # ground truth\n  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n  ap = voc_ap(rec, prec, use_07_metric)\n\n  return rec, prec, ap\n'"
lib/model/__init__.py,0,b''
lib/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
lib/pycocotools/coco.py,0,"b'from __future__ import print_function\nfrom __future__ import absolute_import\n__author__ = \'tylin\'\n__version__ = \'1.0.1\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  segToMask  - Convert polygon segmentation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>segToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport datetime\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\n# from skimage.draw import polygon\nimport urllib\nimport copy\nimport itertools\nfrom . import mask\nimport os\ntry:\n    unicode        # Python 2\nexcept NameError:\n    unicode = str  # Python 3\n    \nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset = {}\n        self.anns = []\n        self.imgToAnns = {}\n        self.catToImgs = {}\n        self.imgs = {}\n        self.cats = {}\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            print(\'Done (t=%0.2fs)\'%(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns = {}\n        imgToAnns = {}\n        catToImgs = {}\n        cats = {}\n        imgs = {}\n        if \'annotations\' in self.dataset:\n            imgToAnns = {ann[\'image_id\']: [] for ann in self.dataset[\'annotations\']}\n            anns =      {ann[\'id\']:       [] for ann in self.dataset[\'annotations\']}\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']] += [ann]\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            imgs      = {im[\'id\']: {} for im in self.dataset[\'images\']}\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            cats = {cat[\'id\']: [] for cat in self.dataset[\'categories\']}\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n            catToImgs = {cat[\'id\']: [] for cat in self.dataset[\'categories\']}\n            if \'annotations\' in self.dataset:\n                for ann in self.dataset[\'annotations\']:\n                    catToImgs[ann[\'category_id\']] += [ann[\'image_id\']]\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'%s: %s\'%(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                # this can be changed by defaultdict\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            polygons = []\n            color = []\n            for ann in anns:\n                c = np.random.random((1, 3)).tolist()[0]\n                if type(ann[\'segmentation\']) == list:\n                    # polygon\n                    for seg in ann[\'segmentation\']:\n                        poly = np.array(seg).reshape((len(seg)/2, 2))\n                        polygons.append(Polygon(poly, True,alpha=0.4))\n                        color.append(c)\n                else:\n                    # mask\n                    t = self.imgs[ann[\'image_id\']]\n                    if type(ann[\'segmentation\'][\'counts\']) == list:\n                        rle = mask.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                    else:\n                        rle = [ann[\'segmentation\']]\n                    m = mask.decode(rle)\n                    img = np.ones( (m.shape[0], m.shape[1], 3) )\n                    if ann[\'iscrowd\'] == 1:\n                        color_mask = np.array([2.0,166.0,101.0])/255\n                    if ann[\'iscrowd\'] == 0:\n                        color_mask = np.random.random((1, 3)).tolist()[0]\n                    for i in range(3):\n                        img[:,:,i] = color_mask[i]\n                    ax.imshow(np.dstack( (img, m*0.5) ))\n            p = PatchCollection(polygons, facecolors=color, edgecolors=(0,0,0,1), linewidths=3, alpha=0.4)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n        # res.dataset[\'info\'] = copy.deepcopy(self.dataset[\'info\'])\n        # res.dataset[\'licenses\'] = copy.deepcopy(self.dataset[\'licenses\'])\n\n        print(\'Loading and preparing results...     \')\n        tic = time.time()\n        anns    = json.load(open(resFile))\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = mask.area([ann[\'segmentation\']])[0]\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = mask.toBbox([ann[\'segmentation\']])[0]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        print(\'DONE (t=%0.2fs)\'%(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download( self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urllib.urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded %d/%d images (t=%.1fs)\'%(i, N, time.time()- tic))\n'"
lib/pycocotools/cocoeval.py,0,"b'from __future__ import print_function\nfrom __future__ import absolute_import\n__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask\nimport copy\n\ntry:\n    string_types = (str, unicode)  # Python 2\nexcept NameError:\n    string_types = (str, )         # Python 3\n\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  useSegm    - [1] if true evaluate against ground-truth segments\n    #  useCats    - [1] if true use category labels for evaluation    # Note: if useSegm=0 the evaluation is run on bounding boxes.\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params()              # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        #\n        def _toMask(objs, coco):\n            # modify segmentation by reference\n            for obj in objs:\n                t = coco.imgs[obj[\'image_id\']]\n                if type(obj[\'segmentation\']) == list:\n                    if type(obj[\'segmentation\'][0]) == dict:\n                        print(\'debug\')\n                    obj[\'segmentation\'] = mask.frPyObjects(obj[\'segmentation\'],t[\'height\'],t[\'width\'])\n                    if len(obj[\'segmentation\']) == 1:\n                        obj[\'segmentation\'] = obj[\'segmentation\'][0]\n                    else:\n                        # an object can have multiple polygon regions\n                        # merge them into one RLE mask\n                        obj[\'segmentation\'] = mask.merge(obj[\'segmentation\'])\n                elif type(obj[\'segmentation\']) == dict and type(obj[\'segmentation\'][\'counts\']) == list:\n                    obj[\'segmentation\'] = mask.frPyObjects([obj[\'segmentation\']],t[\'height\'],t[\'width\'])[0]\n                elif type(obj[\'segmentation\']) == dict and \\\n                     type(obj[\'segmentation\'][\'counts\']) in string_types:\n                    pass\n                else:\n                    raise Exception(\'segmentation format not supported.\')\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        if p.useSegm:\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...      \')\n        p = self.params\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        computeIoU = self.computeIoU\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t=%0.2fs).\'%(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        dt = sorted(dt, key=lambda x: -x[\'score\'])\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.useSegm:\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        else:\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = mask.iou(d,g,iscrowd)\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        #\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if \'ignore\' not in g:\n                g[\'ignore\'] = 0\n            if g[\'iscrowd\'] == 1 or g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        # gt = sorted(gt, key=lambda x: x[\'_ignore\'])\n        gtind = [ind for (ind, g) in sorted(enumerate(gt), key=lambda ind_g: ind_g[1][\'_ignore\']) ]\n\n        gt = [gt[ind] for ind in gtind]\n        dt = sorted(dt, key=lambda x: -x[\'score\'])[0:maxDet]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        N_iou = len(self.ious[imgId, catId])\n        ious = self.ious[imgId, catId][0:maxDet, np.array(gtind)] if N_iou >0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...   \')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        # K0 = len(_pe.catIds)\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk+Na+i] for i in i_list]\n                    E = filter(None, E)\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\']  for e in E])\n                    npig = len([ig for ig in gtIg if ig == 0])\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs)\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S""),\n            \'precision\': precision,\n            \'recall\':   recall,\n        }\n        toc = time.time()\n        print(\'DONE (t=%0.2fs).\'%( toc-tic ))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr        = \' {:<18} {} @[ IoU={:<9} | area={:>6} | maxDets={:>3} ] = {}\'\n            titleStr    = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr     = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr      = \'%0.2f:%0.2f\'%(p.iouThrs[0], p.iouThrs[-1]) if iouThr is None else \'%0.2f\'%(iouThr)\n            areaStr     = areaRng\n            maxDetsStr  = \'%d\'%(maxDets)\n\n            aind = [i for i, aRng in enumerate([\'all\', \'small\', \'medium\', \'large\']) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate([1, 10, 100]) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                # areaRng\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaStr, maxDetsStr, \'%.3f\'%(float(mean_s))))\n            return mean_s\n\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        self.stats = np.zeros((12,))\n        self.stats[0] = _summarize(1)\n        self.stats[1] = _summarize(1,iouThr=.5)\n        self.stats[2] = _summarize(1,iouThr=.75)\n        self.stats[3] = _summarize(1,areaRng=\'small\')\n        self.stats[4] = _summarize(1,areaRng=\'medium\')\n        self.stats[5] = _summarize(1,areaRng=\'large\')\n        self.stats[6] = _summarize(0,maxDets=1)\n        self.stats[7] = _summarize(0,maxDets=10)\n        self.stats[8] = _summarize(0,maxDets=100)\n        self.stats[9]  = _summarize(0,areaRng=\'small\')\n        self.stats[10] = _summarize(0,areaRng=\'medium\')\n        self.stats[11] = _summarize(0,areaRng=\'large\')\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def __init__(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95-.5)/.05)+1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00-.0)/.01)+1, endpoint=True)\n        self.maxDets = [1,10,100]\n        self.areaRng = [ [0**2,1e5**2], [0**2, 32**2], [32**2, 96**2], [96**2, 1e5**2] ]\n        self.useSegm = 0\n        self.useCats = 1\n'"
lib/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nfrom . import _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\nencode      = _mask.encode\ndecode      = _mask.decode\niou         = _mask.iou\nmerge       = _mask.merge\narea        = _mask.area\ntoBbox      = _mask.toBbox\nfrPyObjects = _mask.frPyObjects'"
lib/roi_data_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
lib/roi_data_layer/minibatch.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\nimport numpy.random as npr\nfrom scipy.misc import imread\nfrom model.utils.config import cfg\nfrom model.utils.blob import prep_im_for_blob, im_list_to_blob\nimport pdb\ndef get_minibatch(roidb, num_classes):\n  """"""Given a roidb, construct a minibatch sampled from it.""""""\n  num_images = len(roidb)\n  # Sample random scales to use for each image in this batch\n  random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES),\n                  size=num_images)\n  assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n    \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n    format(num_images, cfg.TRAIN.BATCH_SIZE)\n\n  # Get the input image blob, formatted for caffe\n  im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n  blobs = {\'data\': im_blob}\n\n  assert len(im_scales) == 1, ""Single batch only""\n  assert len(roidb) == 1, ""Single batch only""\n  \n  # gt boxes: (x1, y1, x2, y2, cls)\n  if cfg.TRAIN.USE_ALL_GT:\n    # Include all ground truth boxes\n    gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n  else:\n    # For the COCO ground truth boxes, exclude the ones that are \'\'iscrowd\'\' \n    gt_inds = np.where((roidb[0][\'gt_classes\'] != 0) & np.all(roidb[0][\'gt_overlaps\'].toarray() > -1.0, axis=1))[0]\n  gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n  gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n  gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n  blobs[\'gt_boxes\'] = gt_boxes\n  blobs[\'im_info\'] = np.array(\n    [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n    dtype=np.float32)\n\n  blobs[\'img_id\'] = roidb[0][\'img_id\']\n\n  return blobs\n\ndef _get_image_blob(roidb, scale_inds):\n  """"""Builds an input blob from the images in the roidb at the specified\n  scales.\n  """"""\n  num_images = len(roidb)\n\n  processed_ims = []\n  im_scales = []\n  for i in range(num_images):\n    #im = cv2.imread(roidb[i][\'image\'])\n    im = imread(roidb[i][\'image\'])\n\n    if len(im.shape) == 2:\n      im = im[:,:,np.newaxis]\n      im = np.concatenate((im,im,im), axis=2)\n    # flip the channel, since the original one using cv2\n    # rgb -> bgr\n    # im = im[:,:,::-1]\n\n    if roidb[i][\'flipped\']:\n      im = im[:, ::-1, :]\n    target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n    im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size,\n                    cfg.TRAIN.MAX_SIZE)\n    im_scales.append(im_scale)\n    processed_ims.append(im)\n\n  # Create a blob to hold the input images\n  blob = im_list_to_blob(processed_ims)\n\n  return blob, im_scales\n'"
lib/roi_data_layer/roibatchLoader.py,18,"b'\n""""""The data layer used during training to train a Fast R-CNN network.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport torch\nfrom collections import Counter\n\nfrom scipy.misc import imread\nfrom model.utils.config import cfg\nfrom roi_data_layer.minibatch import get_minibatch, get_minibatch\nfrom model.utils.blob import prep_im_for_blob, im_list_to_blob, crop\nfrom model.rpn.bbox_transform import bbox_transform_inv, clip_boxes\n\nimport numpy as np\nimport cv2\nimport random\nimport time\nimport pdb\n\nclass roibatchLoader(data.Dataset):\n  def __init__(self, roidb, ratio_list, ratio_index, query, batch_size, num_classes, training=True, normalize=None, seen=True):\n    self._roidb = roidb\n    self._query = query\n    self._num_classes = num_classes\n    # we make the height of image consistent to trim_height, trim_width\n    self.trim_height = cfg.TRAIN.TRIM_HEIGHT\n    self.trim_width = cfg.TRAIN.TRIM_WIDTH\n    self.max_num_box = cfg.MAX_NUM_GT_BOXES\n    self.training = training\n    self.normalize = normalize\n    self.ratio_list = ratio_list\n    self.query_position = 0\n\n    if training:\n        self.ratio_index = ratio_index\n    else:\n        self.cat_list = ratio_index[1]\n        self.ratio_index = ratio_index[0]\n\n    self.batch_size = batch_size\n    self.data_size = len(self.ratio_list)\n\n    # given the ratio_list, we want to make the ratio same for each batch.\n    self.ratio_list_batch = torch.Tensor(self.data_size).zero_()\n    num_batch = int(np.ceil(len(ratio_index) / batch_size))\n    if self.training:\n        for i in range(num_batch):\n            left_idx = i*batch_size\n            right_idx = min((i+1)*batch_size-1, self.data_size-1)\n\n            if ratio_list[right_idx] < 1:\n                # for ratio < 1, we preserve the leftmost in each batch.\n                target_ratio = ratio_list[left_idx]\n            elif ratio_list[left_idx] > 1:\n                # for ratio > 1, we preserve the rightmost in each batch.\n                target_ratio = ratio_list[right_idx]\n            else:\n                # for ratio cross 1, we make it to be 1.\n                target_ratio = 1\n\n            self.ratio_list_batch[left_idx:(right_idx+1)] = target_ratio\n\n    self._cat_ids = [\n            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, \n            14, 15, 16, 17, 18, 19, 20, 21, 22, 23, \n            24, 25, 27, 28, 31, 32, 33, 34, 35, 36, \n            37, 38, 39, 40, 41, 42, 43, 44, 46, 47, \n            48, 49, 50, 51, 52, 53, 54, 55, 56, 57, \n            58, 59, 60, 61, 62, 63, 64, 65, 67, 70, \n            72, 73, 74, 75, 76, 77, 78, 79, 80, 81, \n            82, 84, 85, 86, 87, 88, 89, 90\n        ]\n    self._classes = {\n            ind + 1: cat_id for ind, cat_id in enumerate(self._cat_ids)\n        }\n    self._classes_inv = {\n            value: key for key, value in self._classes.items()\n        }\n    \n    self.filter(seen)\n    self.probability()\n\n\n  def __getitem__(self, index):\n    index_ratio = int(self.ratio_index[index])\n\n    # get the anchor index for current sample index\n    # here we set the anchor index to the last one\n    # sample in this group\n    minibatch_db = [self._roidb[index_ratio]]\n\n    blobs = get_minibatch(minibatch_db , self._num_classes)\n\n    blobs[\'gt_boxes\'] = [x for x in blobs[\'gt_boxes\'] if x[-1] in self.list_ind]\n    blobs[\'gt_boxes\'] = np.array(blobs[\'gt_boxes\'])\n\n\n    if self.training:\n        # Random choice query catgory\n        catgory = blobs[\'gt_boxes\'][:,-1]\n        cand = np.unique(catgory)\n        if len(cand)==1:\n            choice = cand[0]\n        else:\n            p = []\n            for i in cand:\n                p.append(self.show_time[i])\n            p = np.array(p)\n            p /= p.sum()\n            choice  = np.random.choice(cand,1,p=p)[0]\n\n        # Delete useless gt_boxes\n        blobs[\'gt_boxes\'][:,-1] = np.where(blobs[\'gt_boxes\'][:,-1]==choice,1,0)\n        # Get query image\n        query = self.load_query(choice)\n    else:\n        query = self.load_query(index, minibatch_db[0][\'img_id\'])\n\n    data = torch.from_numpy(blobs[\'data\'])\n    query = torch.from_numpy(query)\n    query = query.permute(0, 3, 1, 2).contiguous().squeeze(0)\n    im_info = torch.from_numpy(blobs[\'im_info\'])\n    \n    # we need to random shuffle the bounding box.\n    data_height, data_width = data.size(1), data.size(2)\n    if self.training:\n        np.random.shuffle(blobs[\'gt_boxes\'])\n        gt_boxes = torch.from_numpy(blobs[\'gt_boxes\'])\n\n        ########################################################\n        # padding the input image to fixed size for each group #\n        ########################################################\n\n        # NOTE1: need to cope with the case where a group cover both conditions. (done)\n        # NOTE2: need to consider the situation for the tail samples. (no worry)\n        # NOTE3: need to implement a parallel data loader. (no worry)\n        # get the index range\n\n        # if the image need to crop, crop to the target size.\n        ratio = self.ratio_list_batch[index]\n\n        if self._roidb[index_ratio][\'need_crop\']:\n            if ratio < 1:\n                # this means that data_width << data_height, we need to crop the\n                # data_height\n                min_y = int(torch.min(gt_boxes[:,1]))\n                max_y = int(torch.max(gt_boxes[:,3]))\n                trim_size = int(np.floor(data_width / ratio))\n                if trim_size > data_height:\n                    trim_size = data_height                \n                box_region = max_y - min_y + 1\n                if min_y == 0:\n                    y_s = 0\n                else:\n                    if (box_region-trim_size) < 0:\n                        y_s_min = max(max_y-trim_size, 0)\n                        y_s_max = min(min_y, data_height-trim_size)\n                        if y_s_min == y_s_max:\n                            y_s = y_s_min\n                        else:\n                            y_s = np.random.choice(range(y_s_min, y_s_max))\n                    else:\n                        y_s_add = int((box_region-trim_size)/2)\n                        if y_s_add == 0:\n                            y_s = min_y\n                        else:\n                            y_s = np.random.choice(range(min_y, min_y+y_s_add))\n                # crop the image\n                data = data[:, y_s:(y_s + trim_size), :, :]\n\n                # shift y coordiante of gt_boxes\n                gt_boxes[:, 1] = gt_boxes[:, 1] - float(y_s)\n                gt_boxes[:, 3] = gt_boxes[:, 3] - float(y_s)\n\n                # update gt bounding box according the trip\n                gt_boxes[:, 1].clamp_(0, trim_size - 1)\n                gt_boxes[:, 3].clamp_(0, trim_size - 1)\n\n            else:\n                # this means that data_width >> data_height, we need to crop the\n                # data_width\n                min_x = int(torch.min(gt_boxes[:,0]))\n                max_x = int(torch.max(gt_boxes[:,2]))\n                trim_size = int(np.ceil(data_height * ratio))\n                if trim_size > data_width:\n                    trim_size = data_width                \n                box_region = max_x - min_x + 1\n                if min_x == 0:\n                    x_s = 0\n                else:\n                    if (box_region-trim_size) < 0:\n                        x_s_min = max(max_x-trim_size, 0)\n                        x_s_max = min(min_x, data_width-trim_size)\n                        if x_s_min == x_s_max:\n                            x_s = x_s_min\n                        else:\n                            x_s = np.random.choice(range(x_s_min, x_s_max))\n                    else:\n                        x_s_add = int((box_region-trim_size)/2)\n                        if x_s_add == 0:\n                            x_s = min_x\n                        else:\n                            x_s = np.random.choice(range(min_x, min_x+x_s_add))\n                # crop the image\n                data = data[:, :, x_s:(x_s + trim_size), :]\n\n                # shift x coordiante of gt_boxes\n                gt_boxes[:, 0] = gt_boxes[:, 0] - float(x_s)\n                gt_boxes[:, 2] = gt_boxes[:, 2] - float(x_s)\n                # update gt bounding box according the trip\n                gt_boxes[:, 0].clamp_(0, trim_size - 1)\n                gt_boxes[:, 2].clamp_(0, trim_size - 1)\n\n        # based on the ratio, padding the image.\n        if ratio < 1:\n            # this means that data_width < data_height\n            trim_size = int(np.floor(data_width / ratio))\n\n            padding_data = torch.FloatTensor(int(np.ceil(data_width / ratio)), \\\n                                             data_width, 3).zero_()\n\n            padding_data[:data_height, :, :] = data[0]\n            # update im_info\n            im_info[0, 0] = padding_data.size(0)\n            # print(""height %d %d \\n"" %(index, anchor_idx))\n        elif ratio > 1:\n            # this means that data_width > data_height\n            # if the image need to crop.\n            padding_data = torch.FloatTensor(data_height, \\\n                                             int(np.ceil(data_height * ratio)), 3).zero_()\n            padding_data[:, :data_width, :] = data[0]\n            im_info[0, 1] = padding_data.size(1)\n        else:\n            trim_size = min(data_height, data_width)\n            padding_data = torch.FloatTensor(trim_size, trim_size, 3).zero_()\n            padding_data = data[0][:trim_size, :trim_size, :]\n            # gt_boxes.clamp_(0, trim_size)\n            gt_boxes[:, :4].clamp_(0, trim_size)\n            im_info[0, 0] = trim_size\n            im_info[0, 1] = trim_size\n\n\n        # check the bounding box:\n        not_keep = (gt_boxes[:,0] == gt_boxes[:,2]) | (gt_boxes[:,1] == gt_boxes[:,3])\n        # not_keep = (gt_boxes[:,2] - gt_boxes[:,0]) < 10 \n        # print(not_keep)\n        # not_keep = (gt_boxes[:,2] - gt_boxes[:,0]) < torch.FloatTensor([10]) | (gt_boxes[:,3] - gt_boxes[:,1]) < torch.FloatTensor([10])\n\n        keep = torch.nonzero(not_keep == 0).view(-1)\n\n        gt_boxes_padding = torch.FloatTensor(self.max_num_box, gt_boxes.size(1)).zero_()\n        if keep.numel() != 0 :\n            gt_boxes = gt_boxes[keep]\n            num_boxes = min(gt_boxes.size(0), self.max_num_box)\n            gt_boxes_padding[:num_boxes,:] = gt_boxes[:num_boxes]\n        else:\n            num_boxes = 0\n\n            # permute trim_data to adapt to downstream processing\n        padding_data = padding_data.permute(2, 0, 1).contiguous()\n        im_info = im_info.view(3)\n\n        return padding_data, query, im_info, gt_boxes_padding, num_boxes\n    else:\n        data = data.permute(0, 3, 1, 2).contiguous().view(3, data_height, data_width)\n        im_info = im_info.view(3)\n\n        # gt_boxes = torch.FloatTensor([1,1,1,1,1])\n        gt_boxes = torch.from_numpy(blobs[\'gt_boxes\'])\n        choice = self.cat_list[index]\n\n        return data, query, im_info, gt_boxes, choice\n\n  def load_query(self, choice, id=0):\n    \n    if self.training:\n        # Random choice query catgory image\n        all_data = self._query[choice]\n        data     = random.choice(all_data)\n    else:\n        # Take out the purpose category for testing\n        catgory = self.cat_list[choice]\n        # list all the candidate image \n        all_data = self._query[catgory]\n\n        # Use image_id to determine the random seed\n        # The list l is candidate sequence, which random by image_id\n        random.seed(id)\n        l = list(range(len(all_data)))\n        random.shuffle(l)\n\n        # choose the candidate sequence and take out the data information\n        position=l[self.query_position%len(l)]\n        data     = all_data[position]\n\n    # Get image\n    path       = data[\'image_path\']\n    im = imread(path)\n    \n\n    if len(im.shape) == 2:\n      im = im[:,:,np.newaxis]\n      im = np.concatenate((im,im,im), axis=2)\n\n    im = crop(im, data[\'boxes\'], cfg.TRAIN.query_size)\n    # flip the channel, since the original one using cv2\n    # rgb -> bgr\n    # im = im[:,:,::-1]\n    if random.randint(0,99)/100 > 0.5 and self.training:\n      im = im[:, ::-1, :]\n\n\n    im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, cfg.TRAIN.query_size,\n                    cfg.TRAIN.MAX_SIZE)\n    \n    query = im_list_to_blob([im])\n\n    return query\n\n  def __len__(self):\n    return len(self.ratio_index)\n  \n  def filter(self, seen):\n    if seen==1:\n      self.list = cfg.train_categories\n      # Group number to class\n      if len(self.list)==1:\n        self.list = [self._classes[cat] for cat in range(1,81) if cat%4 != self.list[0]]\n\n    elif seen==2:\n      self.list = cfg.test_categories\n      # Group number to class\n      if len(self.list)==1:\n        self.list = [self._classes[cat] for cat in range(1,81) if cat%4 == self.list[0]]\n    \n    elif seen==3:\n      self.list = cfg.train_categories + cfg.test_categories\n      # Group number to class\n      if len(self.list)==2:\n        self.list = [self._classes[cat] for cat in range(1,81)]\n\n    self.list_ind = [self._classes_inv[x] for x in self.list]\n  \n  def probability(self):\n    show_time = {}\n    for i in self.list_ind:\n        show_time[i] = 0\n    for roi in self._roidb:\n        result = Counter(roi[\'gt_classes\'])\n        for t in result:\n            if t in self.list_ind:\n                show_time[t] += result[t]\n\n    for i in self.list_ind:\n        show_time[i] = 1/show_time[i]\n\n    sum_prob = sum(show_time.values())\n\n    for i in self.list_ind:\n        show_time[i] = show_time[i]/sum_prob\n    \n    self.show_time = show_time\n'"
lib/roi_data_layer/roidb.py,0,"b'""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datasets\nimport numpy as np\nfrom model.utils.config import cfg\nfrom datasets.factory import get_imdb\nimport PIL\nimport pdb\n\ndef prepare_roidb(imdb):\n  """"""Enrich the imdb\'s roidb by adding some derived quantities that\n  are useful for training. This function precomputes the maximum\n  overlap, taken over ground-truth boxes, between each ROI and\n  each ground-truth box. The class with maximum overlap is also\n  recorded.\n  """"""\n\n  roidb = imdb.roidb\n  if not (imdb.name.startswith(\'coco\')):\n    sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n         for i in range(imdb.num_images)]\n         \n  for i in range(len(imdb.image_index)):\n    roidb[i][\'img_id\'] = imdb.image_id_at(i)\n    roidb[i][\'image\'] = imdb.image_path_at(i)\n    if not (imdb.name.startswith(\'coco\')):\n      roidb[i][\'width\'] = sizes[i][0]\n      roidb[i][\'height\'] = sizes[i][1]\n    # need gt_overlaps as a dense array for argmax\n    gt_overlaps = roidb[i][\'gt_overlaps\'].toarray()\n    # max overlap with gt over classes (columns)\n    max_overlaps = gt_overlaps.max(axis=1)\n    # gt class that had the max overlap\n    max_classes = gt_overlaps.argmax(axis=1)\n    roidb[i][\'max_classes\'] = max_classes\n    roidb[i][\'max_overlaps\'] = max_overlaps\n    # sanity checks\n    # max overlap of 0 => class should be zero (background)\n    zero_inds = np.where(max_overlaps == 0)[0]\n    assert all(max_classes[zero_inds] == 0)\n    # max overlap > 0 => class should not be zero (must be a fg class)\n    nonzero_inds = np.where(max_overlaps > 0)[0]\n    assert all(max_classes[nonzero_inds] != 0)\n\ndef rank_roidb_ratio(roidb):\n    # rank roidb based on the ratio between width and height.\n    ratio_large = 2 # largest ratio to preserve.\n    ratio_small = 0.5 # smallest ratio to preserve.    \n    \n    ratio_list = []\n    for i in range(len(roidb)):\n      width = roidb[i][\'width\']\n      height = roidb[i][\'height\']\n      ratio = width / float(height)\n\n      if ratio > ratio_large:\n        roidb[i][\'need_crop\'] = 1\n        ratio = ratio_large\n      elif ratio < ratio_small:\n        roidb[i][\'need_crop\'] = 1\n        ratio = ratio_small        \n      else:\n        roidb[i][\'need_crop\'] = 0\n\n      ratio_list.append(ratio)\n\n    ratio_list = np.array(ratio_list)\n    ratio_index = np.argsort(ratio_list)\n    return ratio_list[ratio_index], ratio_index\n\ndef filter_roidb(roidb):\n    # filter the image without bounding box.\n    print(\'before filtering, there are %d images...\' % (len(roidb)))\n    i = 0\n    while i < len(roidb):\n      if len(roidb[i][\'boxes\']) == 0:\n        del roidb[i]\n        i -= 1\n      i += 1\n\n    print(\'after filtering, there are %d images...\' % (len(roidb)))\n    return roidb\n\ndef test_rank_roidb_ratio(roidb, reserved):\n    # rank roidb based on the ratio between width and height.\n    ratio_large = 2 # largest ratio to preserve.\n    ratio_small = 0.5 # smallest ratio to preserve.  \n    \n\n    # Image can show more than one time for test different category \n    ratio_list = []\n    ratio_index = [] # image index reserved\n    cat_list = [] # category list reserved\n    for i in range(len(roidb)):\n      width = roidb[i][\'width\']\n      height = roidb[i][\'height\']\n      ratio = width / float(height)\n\n      if ratio > ratio_large:\n        roidb[i][\'need_crop\'] = 1\n        ratio = ratio_large\n      elif ratio < ratio_small:\n        roidb[i][\'need_crop\'] = 1\n        ratio = ratio_small        \n      else:\n        roidb[i][\'need_crop\'] = 0\n\n\n      for j in np.unique(roidb[i][\'max_classes\']):\n        if j in reserved:\n          ratio_list.append(ratio)\n          ratio_index.append(i)\n          cat_list.append(j)\n\n\n    ratio_list = np.array(ratio_list)\n    ratio_index = np.array(ratio_index)\n    cat_list = np.array(cat_list)\n    ratio_index = np.vstack((ratio_index,cat_list))\n\n    return ratio_list, ratio_index\n\ndef combined_roidb(imdb_names, training=True, seen=1):\n  """"""\n  Combine multiple roidbs\n  """"""\n\n  def get_training_roidb(imdb, training):\n    """"""Returns a roidb (Region of Interest database) for use in training.""""""\n    if cfg.TRAIN.USE_FLIPPED and training:\n      print(\'Appending horizontally-flipped training examples...\')\n      imdb.append_flipped_images()\n      print(\'done\')\n\n\n    print(\'Preparing training data...\')\n    prepare_roidb(imdb)\n    #ratio_index = rank_roidb_ratio(imdb)\n    print(\'done\')\n\n    return imdb.roidb\n  \n  def get_roidb(imdb_name, training):\n    imdb = get_imdb(imdb_name)\n    \n    print(\'Loaded dataset `{:s}` for training\'.format(imdb.name))\n    imdb.set_proposal_method(cfg.TRAIN.PROPOSAL_METHOD)\n    print(\'Set proposal method: {:s}\'.format(cfg.TRAIN.PROPOSAL_METHOD))\n\n    imdb.filter(seen)\n\n    roidb = get_training_roidb(imdb, training)\n\n\n\n    return imdb, roidb, imdb.cat_data, imdb.inverse_list\n\n  imdbs = []\n  roidbs = []\n  querys = []\n  for s in imdb_names.split(\'+\'):\n    imdb, roidb, query, reserved = get_roidb(s, training)\n    imdbs.append(imdb)\n    roidbs.append(roidb)\n    querys.append(query)\n  imdb = imdbs[0]\n  roidb = roidbs[0]\n  query = querys[0]\n\n\n  if len(roidbs) > 1 and training:\n    for r in roidbs[1:]:\n      roidb.extend(r)\n    for r in range(len(querys[0])):\n      query[r].extend(querys[1][r])\n    tmp = get_imdb(imdb_names.split(\'+\')[1])\n    imdb = datasets.imdb.imdb(imdb_names, tmp.classes)\n\n  if training:\n    roidb = filter_roidb(roidb)\n    ratio_list, ratio_index = rank_roidb_ratio(roidb)\n  else:\n    # Generate testing image, an image testing frequency(time) according to the reserved category\n    ratio_list, ratio_index = test_rank_roidb_ratio(roidb, reserved)\n\n  return imdb, roidb, ratio_list, ratio_index, query'"
lib/datasets/tools/mcg_munge.py,0,"b'from __future__ import print_function\nimport os\nimport sys\n\n""""""Hacky tool to convert file system layout of MCG boxes downloaded from\nhttp://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/\nso that it\'s consistent with those computed by Jan Hosang (see:\nhttp://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-\n  computing/research/object-recognition-and-scene-understanding/how-\n  good-are-detection-proposals-really/)\n\nNB: Boxes from the MCG website are in (y1, x1, y2, x2) order.\nBoxes from Hosang et al. are in (x1, y1, x2, y2) order.\n""""""\n\ndef munge(src_dir):\n    # stored as: ./MCG-COCO-val2014-boxes/COCO_val2014_000000193401.mat\n    # want:      ./MCG/mat/COCO_val2014_0/COCO_val2014_000000141/COCO_val2014_000000141334.mat\n\n    files = os.listdir(src_dir)\n    for fn in files:\n        base, ext = os.path.splitext(fn)\n        # first 14 chars / first 22 chars / all chars + .mat\n        # COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n        first = base[:14]\n        second = base[:22]\n        dst_dir = os.path.join(\'MCG\', \'mat\', first, second)\n        if not os.path.exists(dst_dir):\n            os.makedirs(dst_dir)\n        src = os.path.join(src_dir, fn)\n        dst = os.path.join(dst_dir, fn)\n        print(\'MV: {} -> {}\'.format(src, dst))\n        os.rename(src, dst)\n\nif __name__ == \'__main__\':\n    # src_dir should look something like:\n    #  src_dir = \'MCG-COCO-val2014-boxes\'\n    src_dir = sys.argv[1]\n    munge(src_dir)\n'"
lib/model/faster_rcnn/__init__.py,0,b''
lib/model/faster_rcnn/faster_rcnn.py,11,"b'import random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.models as models\nfrom torch.autograd import Variable\nimport numpy as np\nfrom model.utils.config import cfg\nfrom model.rpn.rpn import _RPN\n\nfrom model.roi_layers import ROIAlign, ROIPool\n\n# from model.roi_pooling.modules.roi_pool import _RoIPooling\n# from model.roi_align.modules.roi_align import RoIAlignAvg\n\nfrom model.rpn.proposal_target_layer_cascade import _ProposalTargetLayer\nimport time\nimport pdb\nfrom model.utils.net_utils import _smooth_l1_loss, _crop_pool_layer, _affine_grid_gen, _affine_theta\nfrom model.utils.net_utils import *\n\nclass match_block(nn.Module):\n    def __init__(self, inplanes):\n        super(match_block, self).__init__()\n\n        self.sub_sample = False\n\n        self.in_channels = inplanes\n        self.inter_channels = None\n\n        if self.inter_channels is None:\n            self.inter_channels = self.in_channels // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        conv_nd = nn.Conv2d\n        max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n        bn = nn.BatchNorm2d\n\n        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                         kernel_size=1, stride=1, padding=0)\n\n        self.W = nn.Sequential(\n            conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                    kernel_size=1, stride=1, padding=0),\n            bn(self.in_channels)\n        )\n        nn.init.constant_(self.W[1].weight, 0)\n        nn.init.constant_(self.W[1].bias, 0)\n\n        self.Q = nn.Sequential(\n            conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n                    kernel_size=1, stride=1, padding=0),\n            bn(self.in_channels)\n        )\n        nn.init.constant_(self.Q[1].weight, 0)\n        nn.init.constant_(self.Q[1].bias, 0)\n\n        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                             kernel_size=1, stride=1, padding=0)\n        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n                           kernel_size=1, stride=1, padding=0)\n\n        self.concat_project = nn.Sequential(\n            nn.Conv2d(self.inter_channels * 2, 1, 1, 1, 0, bias=False),\n            nn.ReLU()\n        )\n        \n        self.ChannelGate = ChannelGate(self.in_channels)\n        self.globalAvgPool = nn.AdaptiveAvgPool2d(1)\n\n\n        \n    def forward(self, detect, aim):\n\n        \n\n        batch_size, channels, height_a, width_a = aim.shape\n        batch_size, channels, height_d, width_d = detect.shape\n\n\n        #####################################find aim image similar object ####################################################\n\n        d_x = self.g(detect).view(batch_size, self.inter_channels, -1)\n        d_x = d_x.permute(0, 2, 1).contiguous()\n\n        a_x = self.g(aim).view(batch_size, self.inter_channels, -1)\n        a_x = a_x.permute(0, 2, 1).contiguous()\n\n        theta_x = self.theta(aim).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n\n        phi_x = self.phi(detect).view(batch_size, self.inter_channels, -1)\n\n        \n\n        f = torch.matmul(theta_x, phi_x)\n\n        N = f.size(-1)\n        f_div_C = f / N\n\n        f = f.permute(0, 2, 1).contiguous()\n        N = f.size(-1)\n        fi_div_C = f / N\n\n        non_aim = torch.matmul(f_div_C, d_x)\n        non_aim = non_aim.permute(0, 2, 1).contiguous()\n        non_aim = non_aim.view(batch_size, self.inter_channels, height_a, width_a)\n        non_aim = self.W(non_aim)\n        non_aim = non_aim + aim\n\n        non_det = torch.matmul(fi_div_C, a_x)\n        non_det = non_det.permute(0, 2, 1).contiguous()\n        non_det = non_det.view(batch_size, self.inter_channels, height_d, width_d)\n        non_det = self.Q(non_det)\n        non_det = non_det + detect\n\n        ##################################### Response in chaneel weight ####################################################\n\n        c_weight = self.ChannelGate(non_aim)\n        act_aim = non_aim * c_weight\n        act_det = non_det * c_weight\n\n        return non_det, act_det, act_aim, c_weight\n\nclass _fasterRCNN(nn.Module):\n    """""" faster RCNN """"""\n    def __init__(self, classes, class_agnostic):\n        super(_fasterRCNN, self).__init__()\n        self.classes = classes\n        self.n_classes = len(classes)\n        self.class_agnostic = class_agnostic\n\n        \n        self.match_net = match_block(self.dout_base_model)\n\n\n        # loss\n        self.RCNN_loss_cls = 0\n        self.RCNN_loss_bbox = 0\n\n        # define rpn\n        self.RCNN_rpn = _RPN(self.dout_base_model)\n        self.RCNN_proposal_target = _ProposalTargetLayer(self.n_classes)\n\n        # self.RCNN_roi_pool = _RoIPooling(cfg.POOLING_SIZE, cfg.POOLING_SIZE, 1.0/16.0)\n        # self.RCNN_roi_align = RoIAlignAvg(cfg.POOLING_SIZE, cfg.POOLING_SIZE, 1.0/16.0)\n\n        self.RCNN_roi_pool = ROIPool((cfg.POOLING_SIZE, cfg.POOLING_SIZE), 1.0/16.0)\n        self.RCNN_roi_align = ROIAlign((cfg.POOLING_SIZE, cfg.POOLING_SIZE), 1.0/16.0, 0)\n        self.triplet_loss = torch.nn.MarginRankingLoss(margin = cfg.TRAIN.MARGIN)\n\n    def forward(self, im_data, query, im_info, gt_boxes, num_boxes):\n        batch_size = im_data.size(0)\n\n        im_info = im_info.data\n        gt_boxes = gt_boxes.data\n        num_boxes = num_boxes.data\n\n        # feed image data to base model to obtain base feature map\n        detect_feat = self.RCNN_base(im_data)\n        query_feat = self.RCNN_base(query)\n\n        rpn_feat, act_feat, act_aim, c_weight = self.match_net(detect_feat, query_feat)\n\n\n        # feed base feature map tp RPN to obtain rois\n        rois, rpn_loss_cls, rpn_loss_bbox = self.RCNN_rpn(rpn_feat, im_info, gt_boxes, num_boxes)\n\n\n        # if it is training phrase, then use ground trubut bboxes for refining\n        if self.training:\n            roi_data = self.RCNN_proposal_target(rois, gt_boxes, num_boxes)\n            rois, rois_label, rois_target, rois_inside_ws, rois_outside_ws = roi_data\n\n            rois_label = Variable(rois_label.view(-1).long())\n            rois_target = Variable(rois_target.view(-1, rois_target.size(2)))\n            rois_inside_ws = Variable(rois_inside_ws.view(-1, rois_inside_ws.size(2)))\n            rois_outside_ws = Variable(rois_outside_ws.view(-1, rois_outside_ws.size(2)))\n        else:\n            rois_label = None\n            rois_target = None\n            rois_inside_ws = None\n            rois_outside_ws = None\n            rpn_loss_cls = 0\n            margin_loss = 0\n            rpn_loss_bbox = 0\n            score_label = None\n\n        rois = Variable(rois)\n        # do roi pooling based on predicted rois\n        if cfg.POOLING_MODE == \'align\':\n            pooled_feat = self.RCNN_roi_align(act_feat, rois.view(-1, 5))\n        elif cfg.POOLING_MODE == \'pool\':\n            pooled_feat = self.RCNN_roi_pool(act_feat, rois.view(-1,5))\n\n\n        # feed pooled features to top model\n        pooled_feat = self._head_to_tail(pooled_feat)\n        query_feat  = self._head_to_tail(act_aim)\n\n        # compute bbox offset\n        bbox_pred = self.RCNN_bbox_pred(pooled_feat)\n\n\n        pooled_feat = pooled_feat.view(batch_size, rois.size(1), -1)\n        query_feat = query_feat.unsqueeze(1).repeat(1,rois.size(1),1)\n\n\n        pooled_feat = torch.cat((pooled_feat,query_feat), dim=2).view(-1, 4096)\n\n\n        # compute object classification probability\n        score = self.RCNN_cls_score(pooled_feat)\n\n        score_prob = F.softmax(score, 1)[:,1]\n\n\n        RCNN_loss_cls = 0\n        RCNN_loss_bbox = 0\n        \n\n        if self.training:\n            # classification loss\n            \n            score_label = rois_label.view(batch_size, -1).float()\n            gt_map = torch.abs(score_label.unsqueeze(1)-score_label.unsqueeze(-1))\n\n            score_prob = score_prob.view(batch_size, -1)\n            pr_map = torch.abs(score_prob.unsqueeze(1)-score_prob.unsqueeze(-1))\n            target = -((gt_map-1)**2) + gt_map\n            \n            RCNN_loss_cls = F.cross_entropy(score, rois_label)\n\n            margin_loss = 3 * self.triplet_loss(pr_map, gt_map, target)\n\n            # RCNN_loss_cls = similarity + margin_loss\n    \n            # bounding box regression L1 loss\n            RCNN_loss_bbox = _smooth_l1_loss(bbox_pred, rois_target, rois_inside_ws, rois_outside_ws)\n\n\n        cls_prob = score_prob.view(batch_size, rois.size(1), -1)\n        bbox_pred = bbox_pred.view(batch_size, rois.size(1), -1)\n\n        return rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_bbox, RCNN_loss_cls, margin_loss, RCNN_loss_bbox, rois_label, c_weight\n\n    def _init_weights(self):\n        def normal_init(m, mean, stddev, truncated=False):\n            """"""\n            weight initalizer: truncated normal and random normal.\n            """"""\n            # x is a parameter\n            if truncated:\n                m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean) # not a perfect approximation\n            else:\n                m.weight.data.normal_(mean, stddev)\n                m.bias.data.zero_()\n\n        normal_init(self.RCNN_rpn.RPN_Conv, 0, 0.01, cfg.TRAIN.TRUNCATED)\n        normal_init(self.RCNN_rpn.RPN_cls_score, 0, 0.01, cfg.TRAIN.TRUNCATED)\n        normal_init(self.RCNN_rpn.RPN_bbox_pred, 0, 0.01, cfg.TRAIN.TRUNCATED)\n        normal_init(self.RCNN_cls_score[0], 0, 0.01, cfg.TRAIN.TRUNCATED)\n        normal_init(self.RCNN_cls_score[1], 0, 0.01, cfg.TRAIN.TRUNCATED)\n        normal_init(self.RCNN_bbox_pred, 0, 0.001, cfg.TRAIN.TRUNCATED)\n        \n\n    def create_architecture(self):\n        self._init_modules()\n        self._init_weights()\n'"
lib/model/faster_rcnn/resnet.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom model.utils.config import cfg\nfrom model.faster_rcnn.faster_rcnn import _fasterRCNN\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport pdb\nimport copy\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n       \'resnet152\']\n\n\nmodel_urls = {\n  \'resnet18\': \'https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth\',\n  \'resnet34\': \'https://s3.amazonaws.com/pytorch/models/resnet34-333f7ec4.pth\',\n  \'resnet50\': \'https://s3.amazonaws.com/pytorch/models/resnet50-19c8e357.pth\',\n  \'resnet101\': \'https://s3.amazonaws.com/pytorch/models/resnet101-5d3b4d8f.pth\',\n  \'resnet152\': \'https://s3.amazonaws.com/pytorch/models/resnet152-b121ed2d.pth\',\n}\n\ndef conv3x3(in_planes, out_planes, stride=1):\n  ""3x3 convolution with padding""\n  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n           padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n  expansion = 1\n\n  def __init__(self, inplanes, planes, stride=1, downsample=None):\n    super(BasicBlock, self).__init__()\n    self.conv1 = conv3x3(inplanes, planes, stride)\n    self.bn1 = nn.BatchNorm2d(planes)\n    self.relu = nn.ReLU(inplace=True)\n    self.conv2 = conv3x3(planes, planes)\n    self.bn2 = nn.BatchNorm2d(planes)\n    self.downsample = downsample\n    self.stride = stride\n\n  def forward(self, x):\n    residual = x\n\n    out = self.conv1(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n\n    out = self.conv2(out)\n    out = self.bn2(out)\n\n    if self.downsample is not None:\n      residual = self.downsample(x)\n\n    out += residual\n    out = self.relu(out)\n\n    return out\n\n\nclass Bottleneck(nn.Module):\n  expansion = 4\n\n  def __init__(self, inplanes, planes, stride=1, downsample=None):\n    super(Bottleneck, self).__init__()\n    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n    self.bn1 = nn.BatchNorm2d(planes)\n    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n                 padding=1, bias=False)\n    self.bn2 = nn.BatchNorm2d(planes)\n    self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n    self.bn3 = nn.BatchNorm2d(planes * 4)\n    self.relu = nn.ReLU(inplace=True)\n    self.downsample = downsample\n    self.stride = stride\n\n  def forward(self, x):\n    residual = x\n\n    out = self.conv1(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n\n    out = self.conv2(out)\n    out = self.bn2(out)\n    out = self.relu(out)\n\n    out = self.conv3(out)\n    out = self.bn3(out)\n\n    if self.downsample is not None:\n      residual = self.downsample(x)\n\n    out += residual\n    out = self.relu(out)\n\n    return out\n\n\nclass ResNet(nn.Module):\n  def __init__(self, block, layers, num_classes=1000):\n    self.inplanes = 64\n    super(ResNet, self).__init__()\n    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                 bias=False)\n    self.bn1 = nn.BatchNorm2d(64)\n    self.relu = nn.ReLU(inplace=True)\n    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True) # change\n    self.layer1 = self._make_layer(block, 64, layers[0])\n    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n    # it is slightly better whereas slower to set stride = 1\n    # self.layer4 = self._make_layer(block, 512, layers[3], stride=1)\n    self.avgpool = nn.AvgPool2d(7)\n    self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n\n  def _make_layer(self, block, planes, blocks, stride=1):\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n      downsample = nn.Sequential(\n        nn.Conv2d(self.inplanes, planes * block.expansion,\n              kernel_size=1, stride=stride, bias=False),\n        nn.BatchNorm2d(planes * block.expansion),\n      )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n      layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n  def forward(self, x):\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.maxpool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n\n    x = self.avgpool(x)\n    x = x.view(x.size(0), -1)\n    x = self.fc(x)\n\n    return x\n\n\ndef resnet18(pretrained=False):\n  """"""Constructs a ResNet-18 model.\n  Args:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n  """"""\n  model = ResNet(BasicBlock, [2, 2, 2, 2])\n  if pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n  return model\n\n\ndef resnet34(pretrained=False):\n  """"""Constructs a ResNet-34 model.\n  Args:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n  """"""\n  model = ResNet(BasicBlock, [3, 4, 6, 3])\n  if pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n  return model\n\n\ndef resnet50(pretrained=False):\n  """"""Constructs a ResNet-50 model.\n  Args:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n  """"""\n  model = ResNet(Bottleneck, [3, 4, 6, 3])\n  if pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n  return model\n\n\ndef resnet101(pretrained=False):\n  """"""Constructs a ResNet-101 model.\n  Args:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n  """"""\n  model = ResNet(Bottleneck, [3, 4, 23, 3])\n  if pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n  return model\n\n\ndef resnet152(pretrained=False):\n  """"""Constructs a ResNet-152 model.\n  Args:\n    pretrained (bool): If True, returns a model pre-trained on ImageNet\n  """"""\n  model = ResNet(Bottleneck, [3, 8, 36, 3])\n  if pretrained:\n    model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n  return model\n\nclass resnet(_fasterRCNN):\n  def __init__(self, classes, num_layers=101, pretrained=False, class_agnostic=False):\n    if num_layers==50:\n      self.model_path = \'../data/pretrain_imagenet_resnet50/model_best.pth.tar\'\n    elif num_layers==101:\n      self.model_path = \'../data/pretrain_imagenet_resnet101/model_best.pth.tar\'\n    self.dout_base_model = 1024\n    self.pretrained = pretrained\n    self.class_agnostic = class_agnostic\n    self.num_layers = num_layers\n\n    _fasterRCNN.__init__(self, classes, class_agnostic)\n\n  def _init_modules(self):\n    if self.num_layers==50:\n      resnet = resnet50()\n    else:\n      resnet = resnet101()\n\n    if self.pretrained == True:\n      print(""Loading pretrained weights from %s"" %(self.model_path))\n      state_dict = torch.load(self.model_path)\n      state_dict = state_dict[\'state_dict\']\n\n      state_dict_v2 = copy.deepcopy(state_dict)\n\n      for key in state_dict:\n        pre, post = key.split(\'module.\')\n        state_dict_v2[post] = state_dict_v2.pop(key)\n\n      resnet.load_state_dict(state_dict_v2)\n\n    # Build resnet.\n    self.RCNN_base = nn.Sequential(resnet.conv1, resnet.bn1,resnet.relu,\n      resnet.maxpool,resnet.layer1,resnet.layer2,resnet.layer3)\n\n    self.RCNN_top = nn.Sequential(resnet.layer4)\n\n\n    self.RCNN_cls_score = nn.Sequential(\n                          nn.Linear(2048*2, 8),\n                          nn.Linear(8, 2)\n                          )\n\n    if self.class_agnostic:\n      self.RCNN_bbox_pred = nn.Linear(2048, 4)\n    else:\n      self.RCNN_bbox_pred = nn.Linear(2048, 4 * self.n_classes)\n\n    # Fix blocks\n    for p in self.RCNN_base[0].parameters(): p.requires_grad=False\n    for p in self.RCNN_base[1].parameters(): p.requires_grad=False\n\n    assert (0 <= cfg.RESNET.FIXED_BLOCKS < 4)\n    if cfg.RESNET.FIXED_BLOCKS >= 3:\n      for p in self.RCNN_base[6].parameters(): p.requires_grad=False\n    if cfg.RESNET.FIXED_BLOCKS >= 2:\n      for p in self.RCNN_base[5].parameters(): p.requires_grad=False\n    if cfg.RESNET.FIXED_BLOCKS >= 1:\n      for p in self.RCNN_base[4].parameters(): p.requires_grad=False\n\n    def set_bn_fix(m):\n      classname = m.__class__.__name__\n      if classname.find(\'BatchNorm\') != -1:\n        for p in m.parameters(): p.requires_grad=False\n\n    self.RCNN_base.apply(set_bn_fix)\n    self.RCNN_top.apply(set_bn_fix)\n\n    # for num_layer,child in enumerate(self.RCNN_base.children()):\n    #   for param in child.parameters():\n    #     param.requires_grad = False\n\n  def train(self, mode=True):\n    # Override train so that the training mode is set as we want\n    nn.Module.train(self, mode)\n    if mode:\n      # Set fixed blocks to be in eval mode\n      self.RCNN_base.eval()\n      self.RCNN_base[5].train()\n      self.RCNN_base[6].train()\n\n      def set_bn_eval(m):\n        classname = m.__class__.__name__\n        if classname.find(\'BatchNorm\') != -1:\n          m.eval()\n\n      self.RCNN_base.apply(set_bn_eval)\n      self.RCNN_top.apply(set_bn_eval)\n\n  def _head_to_tail(self, pool5):\n    fc7 = self.RCNN_top(pool5).mean(3).mean(2)\n    return fc7\n'"
lib/model/faster_rcnn/vgg16.py,4,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nimport torchvision.models as models\nfrom model.faster_rcnn.faster_rcnn import _fasterRCNN\nimport pdb\n\nclass vgg16(_fasterRCNN):\n  def __init__(self, classes, pretrained=False, class_agnostic=False):\n    self.model_path = \'data/pretrained_model/vgg16_caffe.pth\'\n    self.dout_base_model = 512\n    self.pretrained = pretrained\n    self.class_agnostic = class_agnostic\n\n    _fasterRCNN.__init__(self, classes, class_agnostic)\n\n  def _init_modules(self):\n    vgg = models.vgg16()\n    if self.pretrained:\n        print(""Loading pretrained weights from %s"" %(self.model_path))\n        state_dict = torch.load(self.model_path)\n        vgg.load_state_dict({k:v for k,v in state_dict.items() if k in vgg.state_dict()})\n\n    vgg.classifier = nn.Sequential(*list(vgg.classifier._modules.values())[:-1])\n\n    # not using the last maxpool layer\n    self.RCNN_base = nn.Sequential(*list(vgg.features._modules.values())[:-1])\n\n    # Fix the layers before conv3:\n    for layer in range(10):\n      for p in self.RCNN_base[layer].parameters(): p.requires_grad = False\n\n    # self.RCNN_base = _RCNN_base(vgg.features, self.classes, self.dout_base_model)\n\n    self.RCNN_top = vgg.classifier\n\n    # not using the last maxpool layer\n    self.RCNN_cls_score = nn.Linear(4096, self.n_classes)\n\n    if self.class_agnostic:\n      self.RCNN_bbox_pred = nn.Linear(4096, 4)\n    else:\n      self.RCNN_bbox_pred = nn.Linear(4096, 4 * self.n_classes)      \n\n  def _head_to_tail(self, pool5):\n    \n    pool5_flat = pool5.view(pool5.size(0), -1)\n    fc7 = self.RCNN_top(pool5_flat)\n\n    return fc7\n\n'"
lib/model/nms/__init__.py,0,b''
lib/model/nms/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n#this_file = os.path.dirname(__file__)\n\nsources = []\nheaders = []\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/nms_cuda.c']\n    headers += ['src/nms_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/nms_cuda_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\nprint(extra_objects)\n\nffi = create_extension(\n    '_ext.nms',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/model/nms/nms_cpu.py,1,"b'from __future__ import absolute_import\n\nimport numpy as np\nimport torch\n\ndef nms_cpu(dets, thresh):\n    dets = dets.numpy()\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order.item(0)\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.maximum(x2[i], x2[order[1:]])\n        yy2 = np.maximum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return torch.IntTensor(keep)\n\n\n'"
lib/model/nms/nms_gpu.py,0,"b'from __future__ import absolute_import\nimport torch\nimport numpy as np\nfrom ._ext import nms\nimport pdb\n\ndef nms_gpu(dets, thresh):\n\tkeep = dets.new(dets.size(0), 1).zero_().int()\n\tnum_out = dets.new(1).zero_().int()\n\tnms.nms_cuda(keep, dets, num_out, thresh)\n\tkeep = keep[:num_out[0]]\n\treturn keep\n'"
lib/model/nms/nms_wrapper.py,1,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nimport torch\nfrom model.utils.config import cfg\nif torch.cuda.is_available():\n    from model.nms.nms_gpu import nms_gpu\nfrom model.nms.nms_cpu import nms_cpu\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n    if dets.shape[0] == 0:\n        return []\n    # ---numpy version---\n    # original: return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n    # ---pytorch version---\n\n    return nms_gpu(dets, thresh) if force_cpu == False else nms_cpu(dets, thresh)\n'"
lib/model/roi_align/__init__.py,0,b''
lib/model/roi_align/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\nsources = ['src/roi_align.c']\nheaders = ['src/roi_align.h']\nextra_objects = []\n#sources = []\n#headers = []\ndefines = []\nwith_cuda = False\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_align_cuda.c']\n    headers += ['src/roi_align_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n    \n    extra_objects = ['src/roi_align_kernel.cu.o']\n    extra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_align',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/model/roi_crop/__init__.py,0,b''
lib/model/roi_crop/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n#this_file = os.path.dirname(__file__)\n\nsources = ['src/roi_crop.c']\nheaders = ['src/roi_crop.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_crop_cuda.c']\n    headers += ['src/roi_crop_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/roi_crop_cuda_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_crop',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/model/roi_layers/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom .nms import nms\nfrom .roi_align import ROIAlign\nfrom .roi_align import roi_align\nfrom .roi_pool import ROIPool\nfrom .roi_pool import roi_pool\n\n__all__ = [""nms"", ""roi_align"", ""ROIAlign"", ""roi_pool"", ""ROIPool""]\n'"
lib/model/roi_layers/nms.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# from ._utils import _C\nfrom model import _C\n\nnms = _C.nms\n# nms.__doc__ = """"""\n# This function performs Non-maximum suppresion""""""\n'"
lib/model/roi_layers/roi_align.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom model import _C\n\nimport pdb\n\nclass _ROIAlign(Function):\n    @staticmethod\n    def forward(ctx, input, roi, output_size, spatial_scale, sampling_ratio):\n        ctx.save_for_backward(roi)\n        ctx.output_size = _pair(output_size)\n        ctx.spatial_scale = spatial_scale\n        ctx.sampling_ratio = sampling_ratio\n        ctx.input_shape = input.size()\n        output = _C.roi_align_forward(input, roi, spatial_scale, output_size[0], output_size[1], sampling_ratio)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        rois, = ctx.saved_tensors\n        output_size = ctx.output_size\n        spatial_scale = ctx.spatial_scale\n        sampling_ratio = ctx.sampling_ratio\n        bs, ch, h, w = ctx.input_shape\n        grad_input = _C.roi_align_backward(\n            grad_output,\n            rois,\n            spatial_scale,\n            output_size[0],\n            output_size[1],\n            bs,\n            ch,\n            h,\n            w,\n            sampling_ratio,\n        )\n        return grad_input, None, None, None, None\n\n\nroi_align = _ROIAlign.apply\n\n\nclass ROIAlign(nn.Module):\n    def __init__(self, output_size, spatial_scale, sampling_ratio):\n        super(ROIAlign, self).__init__()\n        self.output_size = output_size\n        self.spatial_scale = spatial_scale\n        self.sampling_ratio = sampling_ratio\n\n    def forward(self, input, rois):\n        return roi_align(\n            input, rois, self.output_size, self.spatial_scale, self.sampling_ratio\n        )\n\n    def __repr__(self):\n        tmpstr = self.__class__.__name__ + ""(""\n        tmpstr += ""output_size="" + str(self.output_size)\n        tmpstr += "", spatial_scale="" + str(self.spatial_scale)\n        tmpstr += "", sampling_ratio="" + str(self.sampling_ratio)\n        tmpstr += "")""\n        return tmpstr\n'"
lib/model/roi_layers/roi_pool.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom model import _C\n\n\nclass _ROIPool(Function):\n    @staticmethod\n    def forward(ctx, input, roi, output_size, spatial_scale):\n        ctx.output_size = _pair(output_size)\n        ctx.spatial_scale = spatial_scale\n        ctx.input_shape = input.size()\n        output, argmax = _C.roi_pool_forward(\n            input, roi, spatial_scale, output_size[0], output_size[1]\n        )\n        ctx.save_for_backward(input, roi, argmax)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, rois, argmax = ctx.saved_tensors\n        output_size = ctx.output_size\n        spatial_scale = ctx.spatial_scale\n        bs, ch, h, w = ctx.input_shape\n        grad_input = _C.roi_pool_backward(\n            grad_output,\n            input,\n            rois,\n            argmax,\n            spatial_scale,\n            output_size[0],\n            output_size[1],\n            bs,\n            ch,\n            h,\n            w,\n        )\n        return grad_input, None, None, None\n\n\nroi_pool = _ROIPool.apply\n\n\nclass ROIPool(nn.Module):\n    def __init__(self, output_size, spatial_scale):\n        super(ROIPool, self).__init__()\n        self.output_size = output_size\n        self.spatial_scale = spatial_scale\n\n    def forward(self, input, rois):\n        return roi_pool(input, rois, self.output_size, self.spatial_scale)\n\n    def __repr__(self):\n        tmpstr = self.__class__.__name__ + ""(""\n        tmpstr += ""output_size="" + str(self.output_size)\n        tmpstr += "", spatial_scale="" + str(self.spatial_scale)\n        tmpstr += "")""\n        return tmpstr\n'"
lib/model/roi_pooling/__init__.py,0,b''
lib/model/roi_pooling/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/roi_pooling.c']\nheaders = ['src/roi_pooling.h']\nextra_objects = []\ndefines = []\nwith_cuda = False\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_pooling_cuda.c']\n    headers += ['src/roi_pooling_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n    extra_objects = ['src/roi_pooling.cu.o']\n    extra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_pooling',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/model/rpn/__init__.py,0,b''
lib/model/rpn/anchor_target_layer.py,22,"b'from __future__ import absolute_import\n# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n# --------------------------------------------------------\n# Reorganized and modified by Jianwei Yang and Jiasen Lu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport numpy.random as npr\n\nfrom model.utils.config import cfg\nfrom .generate_anchors import generate_anchors\nfrom .bbox_transform import clip_boxes, bbox_overlaps_batch, bbox_transform_batch\n\nimport pdb\n\nDEBUG = False\n\ntry:\n    long        # Python 2\nexcept NameError:\n    long = int  # Python 3\n\n\nclass _AnchorTargetLayer(nn.Module):\n    """"""\n        Assign anchors to ground-truth targets. Produces anchor classification\n        labels and bounding-box regression targets.\n    """"""\n    def __init__(self, feat_stride, scales, ratios):\n        super(_AnchorTargetLayer, self).__init__()\n\n        self._feat_stride = feat_stride\n        self._scales = scales\n        anchor_scales = scales\n        self._anchors = torch.from_numpy(generate_anchors(scales=np.array(anchor_scales), ratios=np.array(ratios))).float()\n        self._num_anchors = self._anchors.size(0)\n\n        # allow boxes to sit over the edge by a small amount\n        self._allowed_border = 0  # default is 0\n\n    def forward(self, input):\n        # Algorithm:\n        #\n        # for each (H, W) location i\n        #   generate 9 anchor boxes centered on cell i\n        #   apply predicted bbox deltas at cell i to each of the 9 anchors\n        # filter out-of-image anchors\n\n        rpn_cls_score = input[0]\n        gt_boxes = input[1]\n        im_info = input[2]\n        num_boxes = input[3]\n\n        # map of shape (..., H, W)\n        height, width = rpn_cls_score.size(2), rpn_cls_score.size(3)\n\n        batch_size = gt_boxes.size(0)\n\n        feat_height, feat_width = rpn_cls_score.size(2), rpn_cls_score.size(3)\n        shift_x = np.arange(0, feat_width) * self._feat_stride\n        shift_y = np.arange(0, feat_height) * self._feat_stride\n        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n        shifts = torch.from_numpy(np.vstack((shift_x.ravel(), shift_y.ravel(),\n                                  shift_x.ravel(), shift_y.ravel())).transpose())\n        shifts = shifts.contiguous().type_as(rpn_cls_score).float()\n\n        A = self._num_anchors\n        K = shifts.size(0)\n\n        self._anchors = self._anchors.type_as(gt_boxes) # move to specific gpu.\n        all_anchors = self._anchors.view(1, A, 4) + shifts.view(K, 1, 4)\n        all_anchors = all_anchors.view(K * A, 4)\n\n        total_anchors = int(K * A)\n\n        keep = ((all_anchors[:, 0] >= -self._allowed_border) &\n                (all_anchors[:, 1] >= -self._allowed_border) &\n                (all_anchors[:, 2] < long(im_info[0][1]) + self._allowed_border) &\n                (all_anchors[:, 3] < long(im_info[0][0]) + self._allowed_border))\n\n        inds_inside = torch.nonzero(keep).view(-1)\n\n        # keep only inside anchors\n        anchors = all_anchors[inds_inside, :]\n\n        # label: 1 is positive, 0 is negative, -1 is dont care\n        labels = gt_boxes.new(batch_size, inds_inside.size(0)).fill_(-1)\n        bbox_inside_weights = gt_boxes.new(batch_size, inds_inside.size(0)).zero_()\n        bbox_outside_weights = gt_boxes.new(batch_size, inds_inside.size(0)).zero_()\n\n        overlaps = bbox_overlaps_batch(anchors, gt_boxes)\n\n        max_overlaps, argmax_overlaps = torch.max(overlaps, 2)\n        gt_max_overlaps, _ = torch.max(overlaps, 1)\n\n        if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n            labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n        gt_max_overlaps[gt_max_overlaps==0] = 1e-5\n        keep = torch.sum(overlaps.eq(gt_max_overlaps.view(batch_size,1,-1).expand_as(overlaps)), 2)\n\n        if torch.sum(keep) > 0:\n            labels[keep>0] = 1\n\n        # fg label: above threshold IOU\n        labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n\n        if cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n            labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n        num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)\n\n        sum_fg = torch.sum((labels == 1).int(), 1)\n        sum_bg = torch.sum((labels == 0).int(), 1)\n\n        for i in range(batch_size):\n            # subsample positive labels if we have too many\n            if sum_fg[i] > num_fg:\n                fg_inds = torch.nonzero(labels[i] == 1).view(-1)\n                # torch.randperm seems has a bug on multi-gpu setting that cause the segfault.\n                # See https://github.com/pytorch/pytorch/issues/1868 for more details.\n                # use numpy instead.\n                #rand_num = torch.randperm(fg_inds.size(0)).type_as(gt_boxes).long()\n                rand_num = torch.from_numpy(np.random.permutation(fg_inds.size(0))).type_as(gt_boxes).long()\n                disable_inds = fg_inds[rand_num[:fg_inds.size(0)-num_fg]]\n                labels[i][disable_inds] = -1\n                \n            # num_bg = cfg.TRAIN.RPN_BATCHSIZE - sum_fg[i]\n            num_bg = cfg.TRAIN.RPN_BATCHSIZE - torch.sum((labels == 1).int(), 1)[i]\n\n            # subsample negative labels if we have too many\n            if sum_bg[i] > num_bg:\n                bg_inds = torch.nonzero(labels[i] == 0).view(-1)\n                #rand_num = torch.randperm(bg_inds.size(0)).type_as(gt_boxes).long()\n\n                rand_num = torch.from_numpy(np.random.permutation(bg_inds.size(0))).type_as(gt_boxes).long()\n                disable_inds = bg_inds[rand_num[:bg_inds.size(0)-num_bg]]\n                labels[i][disable_inds] = -1\n\n        offset = torch.arange(0, batch_size)*gt_boxes.size(1)\n\n        argmax_overlaps = argmax_overlaps + offset.view(batch_size, 1).type_as(argmax_overlaps)\n        bbox_targets = _compute_targets_batch(anchors, gt_boxes.view(-1,5)[argmax_overlaps.view(-1), :].view(batch_size, -1, 5))\n\n        # use a single value instead of 4 values for easy index.\n        bbox_inside_weights[labels==1] = cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS[0]\n\n        if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:\n            num_examples = torch.sum(labels[i] >= 0)\n            positive_weights = 1.0 / num_examples.item()\n            negative_weights = 1.0 / num_examples.item()\n        else:\n            assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) &\n                    (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1))\n\n        bbox_outside_weights[labels == 1] = positive_weights\n        bbox_outside_weights[labels == 0] = negative_weights\n\n        labels = _unmap(labels, total_anchors, inds_inside, batch_size, fill=-1)\n        bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, batch_size, fill=0)\n        bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, batch_size, fill=0)\n        bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, batch_size, fill=0)\n\n        outputs = []\n\n        labels = labels.view(batch_size, height, width, A).permute(0,3,1,2).contiguous()\n        labels = labels.view(batch_size, 1, A * height, width)\n        outputs.append(labels)\n\n        bbox_targets = bbox_targets.view(batch_size, height, width, A*4).permute(0,3,1,2).contiguous()\n        outputs.append(bbox_targets)\n\n        anchors_count = bbox_inside_weights.size(1)\n        bbox_inside_weights = bbox_inside_weights.view(batch_size,anchors_count,1).expand(batch_size, anchors_count, 4)\n\n        bbox_inside_weights = bbox_inside_weights.contiguous().view(batch_size, height, width, 4*A)\\\n                            .permute(0,3,1,2).contiguous()\n\n        outputs.append(bbox_inside_weights)\n\n        bbox_outside_weights = bbox_outside_weights.view(batch_size,anchors_count,1).expand(batch_size, anchors_count, 4)\n        bbox_outside_weights = bbox_outside_weights.contiguous().view(batch_size, height, width, 4*A)\\\n                            .permute(0,3,1,2).contiguous()\n        outputs.append(bbox_outside_weights)\n\n        return outputs\n\n    def backward(self, top, propagate_down, bottom):\n        """"""This layer does not propagate gradients.""""""\n        pass\n\n    def reshape(self, bottom, top):\n        """"""Reshaping happens during the call to forward.""""""\n        pass\n\ndef _unmap(data, count, inds, batch_size, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n\n    if data.dim() == 2:\n        ret = torch.Tensor(batch_size, count).fill_(fill).type_as(data)\n        ret[:, inds] = data\n    else:\n        ret = torch.Tensor(batch_size, count, data.size(2)).fill_(fill).type_as(data)\n        ret[:, inds,:] = data\n    return ret\n\n\ndef _compute_targets_batch(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    return bbox_transform_batch(ex_rois, gt_rois[:, :, :4])\n'"
lib/model/rpn/bbox_transform.py,22,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n# --------------------------------------------------------\n# Reorganized and modified by Jianwei Yang and Jiasen Lu\n# --------------------------------------------------------\n\nimport torch\nimport numpy as np\nimport pdb\n\ndef bbox_transform(ex_rois, gt_rois):\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = torch.log(gt_widths / ex_widths)\n    targets_dh = torch.log(gt_heights / ex_heights)\n\n    targets = torch.stack(\n        (targets_dx, targets_dy, targets_dw, targets_dh),1)\n\n    return targets\n\ndef bbox_transform_batch(ex_rois, gt_rois):\n\n    if ex_rois.dim() == 2:\n        ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n        ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n        ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n        ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n        gt_widths = gt_rois[:, :, 2] - gt_rois[:, :, 0] + 1.0\n        gt_heights = gt_rois[:, :, 3] - gt_rois[:, :, 1] + 1.0\n        gt_ctr_x = gt_rois[:, :, 0] + 0.5 * gt_widths\n        gt_ctr_y = gt_rois[:, :, 1] + 0.5 * gt_heights\n\n        targets_dx = (gt_ctr_x - ex_ctr_x.view(1,-1).expand_as(gt_ctr_x)) / ex_widths\n        targets_dy = (gt_ctr_y - ex_ctr_y.view(1,-1).expand_as(gt_ctr_y)) / ex_heights\n        targets_dw = torch.log(gt_widths / ex_widths.view(1,-1).expand_as(gt_widths))\n        targets_dh = torch.log(gt_heights / ex_heights.view(1,-1).expand_as(gt_heights))\n\n    elif ex_rois.dim() == 3:\n        ex_widths = ex_rois[:, :, 2] - ex_rois[:, :, 0] + 1.0\n        ex_heights = ex_rois[:,:, 3] - ex_rois[:,:, 1] + 1.0\n        ex_ctr_x = ex_rois[:, :, 0] + 0.5 * ex_widths\n        ex_ctr_y = ex_rois[:, :, 1] + 0.5 * ex_heights\n\n        gt_widths = gt_rois[:, :, 2] - gt_rois[:, :, 0] + 1.0\n        gt_heights = gt_rois[:, :, 3] - gt_rois[:, :, 1] + 1.0\n        gt_ctr_x = gt_rois[:, :, 0] + 0.5 * gt_widths\n        gt_ctr_y = gt_rois[:, :, 1] + 0.5 * gt_heights\n\n        targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n        targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n        targets_dw = torch.log(gt_widths / ex_widths)\n        targets_dh = torch.log(gt_heights / ex_heights)\n    else:\n        raise ValueError(\'ex_roi input dimension is not correct.\')\n\n    targets = torch.stack(\n        (targets_dx, targets_dy, targets_dw, targets_dh),2)\n\n    return targets\n\ndef bbox_transform_inv(boxes, deltas, batch_size):\n    widths = boxes[:, :, 2] - boxes[:, :, 0] + 1.0\n    heights = boxes[:, :, 3] - boxes[:, :, 1] + 1.0\n    ctr_x = boxes[:, :, 0] + 0.5 * widths\n    ctr_y = boxes[:, :, 1] + 0.5 * heights\n\n    dx = deltas[:, :, 0::4]\n    dy = deltas[:, :, 1::4]\n    dw = deltas[:, :, 2::4]\n    dh = deltas[:, :, 3::4]\n\n    pred_ctr_x = dx * widths.unsqueeze(2) + ctr_x.unsqueeze(2)\n    pred_ctr_y = dy * heights.unsqueeze(2) + ctr_y.unsqueeze(2)\n    pred_w = torch.exp(dw) * widths.unsqueeze(2)\n    pred_h = torch.exp(dh) * heights.unsqueeze(2)\n\n    pred_boxes = deltas.clone()\n    # x1\n    pred_boxes[:, :, 0::4] = pred_ctr_x - 0.5 * pred_w\n    # y1\n    pred_boxes[:, :, 1::4] = pred_ctr_y - 0.5 * pred_h\n    # x2\n    pred_boxes[:, :, 2::4] = pred_ctr_x + 0.5 * pred_w\n    # y2\n    pred_boxes[:, :, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n    return pred_boxes\n\ndef clip_boxes_batch(boxes, im_shape, batch_size):\n    """"""\n    Clip boxes to image boundaries.\n    """"""\n    num_rois = boxes.size(1)\n\n    boxes[boxes < 0] = 0\n    # batch_x = (im_shape[:,0]-1).view(batch_size, 1).expand(batch_size, num_rois)\n    # batch_y = (im_shape[:,1]-1).view(batch_size, 1).expand(batch_size, num_rois)\n\n    batch_x = im_shape[:, 1] - 1\n    batch_y = im_shape[:, 0] - 1\n\n    boxes[:,:,0][boxes[:,:,0] > batch_x] = batch_x\n    boxes[:,:,1][boxes[:,:,1] > batch_y] = batch_y\n    boxes[:,:,2][boxes[:,:,2] > batch_x] = batch_x\n    boxes[:,:,3][boxes[:,:,3] > batch_y] = batch_y\n\n    return boxes\n\ndef clip_boxes(boxes, im_shape, batch_size):\n\n    for i in range(batch_size):\n        boxes[i,:,0::4].clamp_(0, im_shape[i, 1]-1)\n        boxes[i,:,1::4].clamp_(0, im_shape[i, 0]-1)\n        boxes[i,:,2::4].clamp_(0, im_shape[i, 1]-1)\n        boxes[i,:,3::4].clamp_(0, im_shape[i, 0]-1)\n\n    return boxes\n\n\ndef bbox_overlaps(anchors, gt_boxes):\n    """"""\n    anchors: (N, 4) ndarray of float\n    gt_boxes: (K, 4) ndarray of float\n\n    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n    """"""\n    N = anchors.size(0)\n    K = gt_boxes.size(0)\n\n    gt_boxes_area = ((gt_boxes[:,2] - gt_boxes[:,0] + 1) *\n                (gt_boxes[:,3] - gt_boxes[:,1] + 1)).view(1, K)\n\n    anchors_area = ((anchors[:,2] - anchors[:,0] + 1) *\n                (anchors[:,3] - anchors[:,1] + 1)).view(N, 1)\n\n    boxes = anchors.view(N, 1, 4).expand(N, K, 4)\n    query_boxes = gt_boxes.view(1, K, 4).expand(N, K, 4)\n\n    iw = (torch.min(boxes[:,:,2], query_boxes[:,:,2]) -\n        torch.max(boxes[:,:,0], query_boxes[:,:,0]) + 1)\n    iw[iw < 0] = 0\n\n    ih = (torch.min(boxes[:,:,3], query_boxes[:,:,3]) -\n        torch.max(boxes[:,:,1], query_boxes[:,:,1]) + 1)\n    ih[ih < 0] = 0\n\n    ua = anchors_area + gt_boxes_area - (iw * ih)\n    overlaps = iw * ih / ua\n\n    return overlaps\n\ndef bbox_overlaps_batch(anchors, gt_boxes):\n    """"""\n    anchors: (N, 4) ndarray of float\n    gt_boxes: (b, K, 5) ndarray of float\n\n    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n    """"""\n    batch_size = gt_boxes.size(0)\n\n\n    if anchors.dim() == 2:\n\n        N = anchors.size(0)\n        K = gt_boxes.size(1)\n\n        anchors = anchors.view(1, N, 4).expand(batch_size, N, 4).contiguous()\n        gt_boxes = gt_boxes[:,:,:4].contiguous()\n\n\n        gt_boxes_x = (gt_boxes[:,:,2] - gt_boxes[:,:,0] + 1)\n        gt_boxes_y = (gt_boxes[:,:,3] - gt_boxes[:,:,1] + 1)\n        gt_boxes_area = (gt_boxes_x * gt_boxes_y).view(batch_size, 1, K)\n\n        anchors_boxes_x = (anchors[:,:,2] - anchors[:,:,0] + 1)\n        anchors_boxes_y = (anchors[:,:,3] - anchors[:,:,1] + 1)\n        anchors_area = (anchors_boxes_x * anchors_boxes_y).view(batch_size, N, 1)\n\n        gt_area_zero = (gt_boxes_x == 1) & (gt_boxes_y == 1)\n        anchors_area_zero = (anchors_boxes_x == 1) & (anchors_boxes_y == 1)\n\n        boxes = anchors.view(batch_size, N, 1, 4).expand(batch_size, N, K, 4)\n        query_boxes = gt_boxes.view(batch_size, 1, K, 4).expand(batch_size, N, K, 4)\n\n        iw = (torch.min(boxes[:,:,:,2], query_boxes[:,:,:,2]) -\n            torch.max(boxes[:,:,:,0], query_boxes[:,:,:,0]) + 1)\n        iw[iw < 0] = 0\n\n        ih = (torch.min(boxes[:,:,:,3], query_boxes[:,:,:,3]) -\n            torch.max(boxes[:,:,:,1], query_boxes[:,:,:,1]) + 1)\n        ih[ih < 0] = 0\n        ua = anchors_area + gt_boxes_area - (iw * ih)\n        overlaps = iw * ih / ua\n\n        # mask the overlap here.\n        overlaps.masked_fill_(gt_area_zero.view(batch_size, 1, K).expand(batch_size, N, K), 0)\n        overlaps.masked_fill_(anchors_area_zero.view(batch_size, N, 1).expand(batch_size, N, K), -1)\n\n    elif anchors.dim() == 3:\n        N = anchors.size(1)\n        K = gt_boxes.size(1)\n\n        if anchors.size(2) == 4:\n            anchors = anchors[:,:,:4].contiguous()\n        else:\n            anchors = anchors[:,:,1:5].contiguous()\n\n        gt_boxes = gt_boxes[:,:,:4].contiguous()\n\n        gt_boxes_x = (gt_boxes[:,:,2] - gt_boxes[:,:,0] + 1)\n        gt_boxes_y = (gt_boxes[:,:,3] - gt_boxes[:,:,1] + 1)\n        gt_boxes_area = (gt_boxes_x * gt_boxes_y).view(batch_size, 1, K)\n\n        anchors_boxes_x = (anchors[:,:,2] - anchors[:,:,0] + 1)\n        anchors_boxes_y = (anchors[:,:,3] - anchors[:,:,1] + 1)\n        anchors_area = (anchors_boxes_x * anchors_boxes_y).view(batch_size, N, 1)\n\n        gt_area_zero = (gt_boxes_x == 1) & (gt_boxes_y == 1)\n        anchors_area_zero = (anchors_boxes_x == 1) & (anchors_boxes_y == 1)\n\n        boxes = anchors.view(batch_size, N, 1, 4).expand(batch_size, N, K, 4)\n        query_boxes = gt_boxes.view(batch_size, 1, K, 4).expand(batch_size, N, K, 4)\n\n        iw = (torch.min(boxes[:,:,:,2], query_boxes[:,:,:,2]) -\n            torch.max(boxes[:,:,:,0], query_boxes[:,:,:,0]) + 1)\n        iw[iw < 0] = 0\n\n        ih = (torch.min(boxes[:,:,:,3], query_boxes[:,:,:,3]) -\n            torch.max(boxes[:,:,:,1], query_boxes[:,:,:,1]) + 1)\n        ih[ih < 0] = 0\n        ua = anchors_area + gt_boxes_area - (iw * ih)\n\n        overlaps = iw * ih / ua\n\n        # mask the overlap here.\n        overlaps.masked_fill_(gt_area_zero.view(batch_size, 1, K).expand(batch_size, N, K), 0)\n        overlaps.masked_fill_(anchors_area_zero.view(batch_size, N, 1).expand(batch_size, N, K), -1)\n    else:\n        raise ValueError(\'anchors input dimension is not correct.\')\n\n    return overlaps\n'"
lib/model/rpn/generate_anchors.py,0,"b'from __future__ import print_function\n# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\nimport pdb\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n#array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n\ndef generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2**np.arange(3, 6)):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.\n    """"""\n\n    base_anchor = np.array([1, 1, base_size, base_size]) - 1\n    ratio_anchors = _ratio_enum(base_anchor, ratios)\n    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                         for i in xrange(ratio_anchors.shape[0])])\n    return anchors\n\ndef _whctrs(anchor):\n    """"""\n    Return width, height, x center, and y center for an anchor (window).\n    """"""\n\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + 0.5 * (w - 1)\n    y_ctr = anchor[1] + 0.5 * (h - 1)\n    return w, h, x_ctr, y_ctr\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""\n    Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n\n    ws = ws[:, np.newaxis]\n    hs = hs[:, np.newaxis]\n    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                         y_ctr - 0.5 * (hs - 1),\n                         x_ctr + 0.5 * (ws - 1),\n                         y_ctr + 0.5 * (hs - 1)))\n    return anchors\n\ndef _ratio_enum(anchor, ratios):\n    """"""\n    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = np.round(np.sqrt(size_ratios))\n    hs = np.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\ndef _scale_enum(anchor, scales):\n    """"""\n    Enumerate a set of anchors for each scale wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\nif __name__ == \'__main__\':\n    import time\n    t = time.time()\n    a = generate_anchors()\n    print(time.time() - t)\n    print(a)\n    from IPython import embed; embed()\n'"
lib/model/rpn/proposal_layer.py,6,"b'from __future__ import absolute_import\n# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n# --------------------------------------------------------\n# Reorganized and modified by Jianwei Yang and Jiasen Lu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport math\nimport yaml\nfrom model.utils.config import cfg\nfrom .generate_anchors import generate_anchors\nfrom .bbox_transform import bbox_transform_inv, clip_boxes, clip_boxes_batch\n# from model.nms.nms_wrapper import nms\nfrom model.roi_layers import nms\nimport pdb\n\nDEBUG = False\n\nclass _ProposalLayer(nn.Module):\n    """"""\n    Outputs object detection proposals by applying estimated bounding-box\n    transformations to a set of regular boxes (called ""anchors"").\n    """"""\n\n    def __init__(self, feat_stride, scales, ratios):\n        super(_ProposalLayer, self).__init__()\n\n        self._feat_stride = feat_stride\n        self._anchors = torch.from_numpy(generate_anchors(scales=np.array(scales),\n            ratios=np.array(ratios))).float()\n        self._num_anchors = self._anchors.size(0)\n\n        # rois blob: holds R regions of interest, each is a 5-tuple\n        # (n, x1, y1, x2, y2) specifying an image batch index n and a\n        # rectangle (x1, y1, x2, y2)\n        # top[0].reshape(1, 5)\n        #\n        # # scores blob: holds scores for R regions of interest\n        # if len(top) > 1:\n        #     top[1].reshape(1, 1, 1, 1)\n\n    def forward(self, input):\n\n        # Algorithm:\n        #\n        # for each (H, W) location i\n        #   generate A anchor boxes centered on cell i\n        #   apply predicted bbox deltas at cell i to each of the A anchors\n        # clip predicted boxes to image\n        # remove predicted boxes with either height or width < threshold\n        # sort all (proposal, score) pairs by score from highest to lowest\n        # take top pre_nms_topN proposals before NMS\n        # apply NMS with threshold 0.7 to remaining proposals\n        # take after_nms_topN proposals after NMS\n        # return the top proposals (-> RoIs top, scores top)\n\n\n        # the first set of _num_anchors channels are bg probs\n        # the second set are the fg probs\n        scores = input[0][:, self._num_anchors:, :, :]\n        bbox_deltas = input[1]\n        im_info = input[2]\n        cfg_key = input[3]\n\n        pre_nms_topN  = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n        post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n        nms_thresh    = cfg[cfg_key].RPN_NMS_THRESH\n        min_size      = cfg[cfg_key].RPN_MIN_SIZE\n\n        batch_size = bbox_deltas.size(0)\n\n        feat_height, feat_width = scores.size(2), scores.size(3)\n        shift_x = np.arange(0, feat_width) * self._feat_stride\n        shift_y = np.arange(0, feat_height) * self._feat_stride\n        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n        shifts = torch.from_numpy(np.vstack((shift_x.ravel(), shift_y.ravel(),\n                                  shift_x.ravel(), shift_y.ravel())).transpose())\n        shifts = shifts.contiguous().type_as(scores).float()\n\n        A = self._num_anchors\n        K = shifts.size(0)\n\n        self._anchors = self._anchors.type_as(scores)\n        # anchors = self._anchors.view(1, A, 4) + shifts.view(1, K, 4).permute(1, 0, 2).contiguous()\n        anchors = self._anchors.view(1, A, 4) + shifts.view(K, 1, 4)\n        anchors = anchors.view(1, K * A, 4).expand(batch_size, K * A, 4)\n\n        # Transpose and reshape predicted bbox transformations to get them\n        # into the same order as the anchors:\n\n        bbox_deltas = bbox_deltas.permute(0, 2, 3, 1).contiguous()\n        bbox_deltas = bbox_deltas.view(batch_size, -1, 4)\n\n        # Same story for the scores:\n        scores = scores.permute(0, 2, 3, 1).contiguous()\n        scores = scores.view(batch_size, -1)\n\n        # Convert anchors into proposals via bbox transformations\n        proposals = bbox_transform_inv(anchors, bbox_deltas, batch_size)\n\n        # 2. clip predicted boxes to image\n        proposals = clip_boxes(proposals, im_info, batch_size)\n        # proposals = clip_boxes_batch(proposals, im_info, batch_size)\n\n        # assign the score to 0 if it\'s non keep.\n        # keep = self._filter_boxes(proposals, min_size * im_info[:, 2])\n\n        # trim keep index to make it euqal over batch\n        # keep_idx = torch.cat(tuple(keep_idx), 0)\n\n        # scores_keep = scores.view(-1)[keep_idx].view(batch_size, trim_size)\n        # proposals_keep = proposals.view(-1, 4)[keep_idx, :].contiguous().view(batch_size, trim_size, 4)\n\n        # _, order = torch.sort(scores_keep, 1, True)\n\n        scores_keep = scores\n        proposals_keep = proposals\n        _, order = torch.sort(scores_keep, 1, True)\n\n        output = scores.new(batch_size, post_nms_topN, 5).zero_()\n        for i in range(batch_size):\n            # # 3. remove predicted boxes with either height or width < threshold\n            # # (NOTE: convert min_size to input image scale stored in im_info[2])\n            proposals_single = proposals_keep[i]\n            scores_single = scores_keep[i]\n\n            # # 4. sort all (proposal, score) pairs by score from highest to lowest\n            # # 5. take top pre_nms_topN (e.g. 6000)\n            order_single = order[i]\n\n            if pre_nms_topN > 0 and pre_nms_topN < scores_keep.numel():\n                order_single = order_single[:pre_nms_topN]\n\n            proposals_single = proposals_single[order_single, :]\n            scores_single = scores_single[order_single].view(-1,1)\n\n            # 6. apply nms (e.g. threshold = 0.7)\n            # 7. take after_nms_topN (e.g. 300)\n            # 8. return the top proposals (-> RoIs top)\n            keep_idx_i = nms(proposals_single, scores_single.squeeze(1), nms_thresh)\n            keep_idx_i = keep_idx_i.long().view(-1)\n\n            if post_nms_topN > 0:\n                keep_idx_i = keep_idx_i[:post_nms_topN]\n            proposals_single = proposals_single[keep_idx_i, :]\n            scores_single = scores_single[keep_idx_i, :]\n\n            # padding 0 at the end.\n            num_proposal = proposals_single.size(0)\n            output[i,:,0] = i\n            output[i,:num_proposal,1:] = proposals_single\n\n        return output\n\n    def backward(self, top, propagate_down, bottom):\n        """"""This layer does not propagate gradients.""""""\n        pass\n\n    def reshape(self, bottom, top):\n        """"""Reshaping happens during the call to forward.""""""\n        pass\n\n    def _filter_boxes(self, boxes, min_size):\n        """"""Remove all boxes with any side smaller than min_size.""""""\n        ws = boxes[:, :, 2] - boxes[:, :, 0] + 1\n        hs = boxes[:, :, 3] - boxes[:, :, 1] + 1\n        keep = ((ws >= min_size.view(-1,1).expand_as(ws)) & (hs >= min_size.view(-1,1).expand_as(hs)))\n        return keep\n'"
lib/model/rpn/proposal_target_layer_cascade.py,21,"b'from __future__ import absolute_import\n# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n# --------------------------------------------------------\n# Reorganized and modified by Jianwei Yang and Jiasen Lu\n# --------------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport numpy.random as npr\nfrom ..utils.config import cfg\nfrom .bbox_transform import bbox_overlaps_batch, bbox_transform_batch\nimport pdb\n\nclass _ProposalTargetLayer(nn.Module):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    """"""\n\n    def __init__(self, nclasses):\n        super(_ProposalTargetLayer, self).__init__()\n        self._num_classes = nclasses\n        self.BBOX_NORMALIZE_MEANS = torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS)\n        self.BBOX_NORMALIZE_STDS = torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS)\n        self.BBOX_INSIDE_WEIGHTS = torch.FloatTensor(cfg.TRAIN.BBOX_INSIDE_WEIGHTS)\n\n    def forward(self, all_rois, gt_boxes, num_boxes):\n\n        self.BBOX_NORMALIZE_MEANS = self.BBOX_NORMALIZE_MEANS.type_as(gt_boxes)\n        self.BBOX_NORMALIZE_STDS = self.BBOX_NORMALIZE_STDS.type_as(gt_boxes)\n        self.BBOX_INSIDE_WEIGHTS = self.BBOX_INSIDE_WEIGHTS.type_as(gt_boxes)\n\n        gt_boxes_append = gt_boxes.new(gt_boxes.size()).zero_()\n        gt_boxes_append[:,:,1:5] = gt_boxes[:,:,:4]\n        \n\n        # Include ground-truth boxes in the set of candidate rois\n        all_rois = torch.cat([all_rois, gt_boxes_append], 1)\n\n        num_images = 1\n        rois_per_image = int(cfg.TRAIN.BATCH_SIZE / num_images)\n        fg_rois_per_image = int(np.round(cfg.TRAIN.FG_FRACTION * rois_per_image))\n        fg_rois_per_image = 1 if fg_rois_per_image == 0 else fg_rois_per_image\n\n        labels, rois, bbox_targets, bbox_inside_weights = self._sample_rois_pytorch(\n            all_rois, gt_boxes, fg_rois_per_image,\n            rois_per_image, self._num_classes)\n\n        bbox_outside_weights = (bbox_inside_weights > 0).float()\n\n        return rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n\n    def backward(self, top, propagate_down, bottom):\n        """"""This layer does not propagate gradients.""""""\n        pass\n\n    def reshape(self, bottom, top):\n        """"""Reshaping happens during the call to forward.""""""\n        pass\n\n    def _get_bbox_regression_labels_pytorch(self, bbox_target_data, labels_batch, num_classes):\n        """"""Bounding-box regression targets (bbox_target_data) are stored in a\n        compact form b x N x (class, tx, ty, tw, th)\n\n        This function expands those targets into the 4-of-4*K representation used\n        by the network (i.e. only one class has non-zero targets).\n\n        Returns:\n            bbox_target (ndarray): b x N x 4K blob of regression targets\n            bbox_inside_weights (ndarray): b x N x 4K blob of loss weights\n        """"""\n        batch_size = labels_batch.size(0)\n        rois_per_image = labels_batch.size(1)\n        clss = labels_batch\n        bbox_targets = bbox_target_data.new(batch_size, rois_per_image, 4).zero_()\n        bbox_inside_weights = bbox_target_data.new(bbox_targets.size()).zero_()\n\n        for b in range(batch_size):\n            # assert clss[b].sum() > 0\n            if clss[b].sum() == 0:\n                continue\n            inds = torch.nonzero(clss[b] > 0).view(-1)\n            for i in range(inds.numel()):\n                ind = inds[i]\n                bbox_targets[b, ind, :] = bbox_target_data[b, ind, :]\n                bbox_inside_weights[b, ind, :] = self.BBOX_INSIDE_WEIGHTS\n\n        return bbox_targets, bbox_inside_weights\n\n\n    def _compute_targets_pytorch(self, ex_rois, gt_rois):\n        """"""Compute bounding-box regression targets for an image.""""""\n\n        assert ex_rois.size(1) == gt_rois.size(1)\n        assert ex_rois.size(2) == 4\n        assert gt_rois.size(2) == 4\n\n        batch_size = ex_rois.size(0)\n        rois_per_image = ex_rois.size(1)\n\n        targets = bbox_transform_batch(ex_rois, gt_rois)\n\n        if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n            # Optionally normalize targets by a precomputed mean and stdev\n            targets = ((targets - self.BBOX_NORMALIZE_MEANS.expand_as(targets))\n                        / self.BBOX_NORMALIZE_STDS.expand_as(targets))\n\n        return targets\n\n\n    def _sample_rois_pytorch(self, all_rois, gt_boxes, fg_rois_per_image, rois_per_image, num_classes):\n        """"""Generate a random sample of RoIs comprising foreground and background\n        examples.\n        """"""\n        # overlaps: (rois x gt_boxes)\n\n        overlaps = bbox_overlaps_batch(all_rois, gt_boxes)\n\n        max_overlaps, gt_assignment = torch.max(overlaps, 2)\n\n        batch_size = overlaps.size(0)\n        num_proposal = overlaps.size(1)\n        num_boxes_per_img = overlaps.size(2)\n\n        offset = torch.arange(0, batch_size)*gt_boxes.size(1)\n        offset = offset.view(-1, 1).type_as(gt_assignment) + gt_assignment\n\n        # changed indexing way for pytorch 1.0\n        labels = gt_boxes[:,:,4].contiguous().view(-1)[(offset.view(-1),)].view(batch_size, -1)\n\n        labels_batch = labels.new(batch_size, rois_per_image).zero_()\n        rois_batch  = all_rois.new(batch_size, rois_per_image, 5).zero_()\n        gt_rois_batch = all_rois.new(batch_size, rois_per_image, 5).zero_()\n        # Guard against the case when an image has fewer than max_fg_rois_per_image\n        # foreground RoIs\n        for i in range(batch_size):\n\n            fg_inds = torch.nonzero(max_overlaps[i] >= cfg.TRAIN.FG_THRESH).view(-1)\n            fg_num_rois = fg_inds.numel()\n\n            # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n            bg_inds = torch.nonzero((max_overlaps[i] < cfg.TRAIN.BG_THRESH_HI) &\n                                    (max_overlaps[i] >= cfg.TRAIN.BG_THRESH_LO)).view(-1)\n            bg_num_rois = bg_inds.numel()\n\n            if fg_num_rois > 0 and bg_num_rois > 0:\n                # sampling fg\n                fg_rois_per_this_image = min(fg_rois_per_image, fg_num_rois)\n\n                # torch.randperm seems has a bug on multi-gpu setting that cause the segfault.\n                # See https://github.com/pytorch/pytorch/issues/1868 for more details.\n                # use numpy instead.\n                #rand_num = torch.randperm(fg_num_rois).long().cuda()\n                rand_num = torch.from_numpy(np.random.permutation(fg_num_rois)).type_as(gt_boxes).long()\n                fg_inds = fg_inds[rand_num[:fg_rois_per_this_image]]\n\n                # sampling bg\n                bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n\n                # Seems torch.rand has a bug, it will generate very large number and make an error.\n                # We use numpy rand instead.\n                #rand_num = (torch.rand(bg_rois_per_this_image) * bg_num_rois).long().cuda()\n                rand_num = np.floor(np.random.rand(bg_rois_per_this_image) * bg_num_rois)\n                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n                bg_inds = bg_inds[rand_num]\n\n            elif fg_num_rois > 0 and bg_num_rois == 0:\n                # sampling fg\n                #rand_num = torch.floor(torch.rand(rois_per_image) * fg_num_rois).long().cuda()\n                rand_num = np.floor(np.random.rand(rois_per_image) * fg_num_rois)\n                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n                fg_inds = fg_inds[rand_num]\n                fg_rois_per_this_image = rois_per_image\n                bg_rois_per_this_image = 0\n            elif bg_num_rois > 0 and fg_num_rois == 0:\n                # sampling bg\n                #rand_num = torch.floor(torch.rand(rois_per_image) * bg_num_rois).long().cuda()\n                rand_num = np.floor(np.random.rand(rois_per_image) * bg_num_rois)\n                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n\n                bg_inds = bg_inds[rand_num]\n                bg_rois_per_this_image = rois_per_image\n                fg_rois_per_this_image = 0\n            else:\n                raise ValueError(""bg_num_rois = 0 and fg_num_rois = 0, this should not happen!"")\n\n            # The indices that we\'re selecting (both fg and bg)\n            keep_inds = torch.cat([fg_inds, bg_inds], 0)\n\n            # Select sampled values from various arrays:\n            labels_batch[i].copy_(labels[i][keep_inds])\n\n            # Clamp labels for the background RoIs to 0\n            if fg_rois_per_this_image < rois_per_image:\n                labels_batch[i][fg_rois_per_this_image:] = 0\n\n            rois_batch[i] = all_rois[i][keep_inds]\n            rois_batch[i,:,0] = i\n\n            gt_rois_batch[i] = gt_boxes[i][gt_assignment[i][keep_inds]]\n\n        bbox_target_data = self._compute_targets_pytorch(\n                rois_batch[:,:,1:5], gt_rois_batch[:,:,:4])\n\n        bbox_targets, bbox_inside_weights = \\\n                self._get_bbox_regression_labels_pytorch(bbox_target_data, labels_batch, num_classes)\n\n        return labels_batch, rois_batch, bbox_targets, bbox_inside_weights\n'"
lib/model/rpn/rpn.py,6,"b'from __future__ import absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom model.utils.config import cfg\nfrom .proposal_layer import _ProposalLayer\nfrom .anchor_target_layer import _AnchorTargetLayer\nfrom model.utils.net_utils import _smooth_l1_loss\n\nimport numpy as np\nimport math\nimport pdb\nimport time\n\nclass _RPN(nn.Module):\n    """""" region proposal network """"""\n    def __init__(self, din):\n        super(_RPN, self).__init__()\n        \n        self.din = din  # get depth of input feature map, e.g., 512\n        self.anchor_scales = cfg.ANCHOR_SCALES\n        self.anchor_ratios = cfg.ANCHOR_RATIOS\n        self.feat_stride = cfg.FEAT_STRIDE[0]\n\n        # define the convrelu layers processing input feature map\n        # self.mix_Conv = nn.Sequential(\n        #         nn.Conv2d(self.din, 512, 3, 1, 1, bias=True),\n        #         nn.BatchNorm2d(512),\n        #         nn.ReLU(inplace=True)\n        #     )\n        self.RPN_Conv = nn.Conv2d(self.din, 512, 3, 1, 1, bias=True)\n\n        # define bg/fg classifcation score layer\n        self.nc_score_out = len(self.anchor_scales) * len(self.anchor_ratios) * 2 # 2(bg/fg) * 9 (anchors)\n        self.RPN_cls_score = nn.Conv2d(512, self.nc_score_out, 1, 1, 0)\n\n        # define anchor box offset prediction layer\n        self.nc_bbox_out = len(self.anchor_scales) * len(self.anchor_ratios) * 4 # 4(coords) * 9 (anchors)\n\n        self.RPN_bbox_pred = nn.Conv2d(512, self.nc_bbox_out, 1, 1, 0)\n\n        # define proposal layer\n        self.RPN_proposal = _ProposalLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios)\n\n        # define anchor target layer\n        self.RPN_anchor_target = _AnchorTargetLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios)\n\n        self.rpn_loss_cls = 0\n        self.rpn_loss_box = 0\n\n    @staticmethod\n    def reshape(x, d):\n        input_shape = x.size()\n        x = x.view(\n            input_shape[0],\n            int(d),\n            int(float(input_shape[1] * input_shape[2]) / float(d)),\n            input_shape[3]\n        )\n        return x\n\n    def forward(self, base_feat, im_info, gt_boxes, num_boxes):\n\n        batch_size = base_feat.size(0)\n\n        # return feature map after convrelu layer\n        rpn_conv1 = F.relu(self.RPN_Conv(base_feat), inplace=True)\n        # get rpn classification score\n        rpn_cls_score = self.RPN_cls_score(rpn_conv1)\n\n        rpn_cls_score_reshape = self.reshape(rpn_cls_score, 2)\n        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, 1)\n        rpn_cls_prob = self.reshape(rpn_cls_prob_reshape, self.nc_score_out)\n\n\n        # get rpn offsets to the anchor boxes\n        rpn_bbox_pred = self.RPN_bbox_pred(rpn_conv1)\n\n        # proposal layer\n        cfg_key = \'TRAIN\' if self.training else \'TEST\'\n\n        rois = self.RPN_proposal((rpn_cls_prob.data, rpn_bbox_pred.data,\n                                 im_info, cfg_key))\n\n        self.rpn_loss_cls = 0\n        self.rpn_loss_box = 0\n\n\n        # generating training labels and build the rpn loss\n        if self.training:\n            assert gt_boxes is not None\n\n            rpn_data = self.RPN_anchor_target((rpn_cls_score.data, gt_boxes, im_info, num_boxes))\n\n            # compute classification loss\n            rpn_cls_score = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 2)\n            rpn_label = rpn_data[0].view(batch_size, -1)\n\n            rpn_keep = Variable(rpn_label.view(-1).ne(-1).nonzero().view(-1))\n\n            rpn_cls_score = torch.index_select(rpn_cls_score.view(-1,2), 0, rpn_keep)\n            rpn_label = torch.index_select(rpn_label.view(-1), 0, rpn_keep.data)\n            rpn_label = Variable(rpn_label.long())\n\n            self.rpn_loss_cls = F.cross_entropy(rpn_cls_score, rpn_label)\n            fg_cnt = torch.sum(rpn_label.data.ne(0))\n\n            rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = rpn_data[1:]\n\n            # compute bbox regression loss\n            rpn_bbox_inside_weights = Variable(rpn_bbox_inside_weights)\n            rpn_bbox_outside_weights = Variable(rpn_bbox_outside_weights)\n            rpn_bbox_targets = Variable(rpn_bbox_targets)\n\n            self.rpn_loss_box = _smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights,\n                                                            rpn_bbox_outside_weights, sigma=3, dim=[1,2,3])\n        return rois, self.rpn_loss_cls, self.rpn_loss_box\n'"
lib/model/utils/__init__.py,0,b''
lib/model/utils/blob.py,1,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Blob helper functions.""""""\n\nimport numpy as np\n# from scipy.misc import imread, imresize\nimport cv2\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n\ndef im_list_to_blob(ims):\n    """"""Convert a list of images into a network input.\n\n    Assumes images are already prepared (means subtracted, BGR order, ...).\n    """"""\n    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n    num_images = len(ims)\n    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                    dtype=np.float32)\n    for i in xrange(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n    return blob\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n    """"""Mean subtract and scale an image for use in a blob.""""""\n\n    im = im.astype(np.float32, copy=False)\n    # changed to use pytorch models\n    im /= 255. # Convert range to [0,1]\n    # normalization for pytroch pretrained models.\n    # https://pytorch.org/docs/stable/torchvision/models.html\n    pixel_means = [0.485, 0.456, 0.406]\n    pixel_stdens = [0.229, 0.224, 0.225]\n    \n    # normalize manual\n    im -= pixel_means # Minus mean    \n    im /= pixel_stdens # divide by stddev\n\n\n    # im = im[:, :, ::-1]\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    # if np.round(im_scale * im_size_max) > max_size:\n    #     im_scale = float(max_size) / float(im_size_max)\n    # im = imresize(im, im_scale)\n    im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n\n    return im, im_scale\n\ndef crop(image, purpose, size):\n\n    h, w, c = image.shape\n\n    # add_h = int(purpose[4]-purpose[2])/5\n    # add_w = int(purpose[3]-purpose[1])/5\n\n    # purpose[2] = int(purpose[2])-add_h if (int(purpose[2])-add_h >0) else 0\n    # purpose[4] = int(purpose[4])+add_h if (int(purpose[4])+add_h <h) else h\n\n    # purpose[1] = int(purpose[1])-add_w if (int(purpose[1])-add_w >0) else 0\n    # purpose[3] = int(purpose[3])+add_w if (int(purpose[3])+add_w <w) else w\n    cut_image = image[int(purpose[1]):int(purpose[3]),int(purpose[0]):int(purpose[2]),:]\n\n\n    height, width = cut_image.shape[0:2]\n\n    max_hw   = max(height, width)\n    cty, ctx = [height // 2, width // 2]\n\n    cropped_image  = np.zeros((max_hw, max_hw, 3), dtype=cut_image.dtype)\n\n    x0, x1 = max(0, ctx - max_hw // 2), min(ctx + max_hw // 2, width)\n    y0, y1 = max(0, cty - max_hw // 2), min(cty + max_hw // 2, height)\n\n    left, right = ctx - x0, x1 - ctx\n    top, bottom = cty - y0, y1 - cty\n\n    cropped_cty, cropped_ctx = max_hw // 2, max_hw // 2\n    y_slice = slice(cropped_cty - top, cropped_cty + bottom)\n    x_slice = slice(cropped_ctx - left, cropped_ctx + right)\n    cropped_image[y_slice, x_slice, :] = cut_image[y0:y1, x0:x1, :]\n\n\n    return cv2.resize(cropped_image, (size,size), interpolation=cv2.INTER_LINEAR)\n'"
lib/model/utils/config.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\nimport numpy as np\n# `pip install easydict` if you don\'t have it\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n#\n# Training options\n#\n__C.TRAIN = edict()\n\n\n# Initial learning rate\n__C.TRAIN.MARGIN = -0.3\n\n# Initial learning rate\n__C.TRAIN.LEARNING_RATE = 0.001\n\n# Momentum\n__C.TRAIN.MOMENTUM = 0.9\n\n# Weight decay, for regularization\n__C.TRAIN.WEIGHT_DECAY = 0.0005\n\n# Factor for reducing the learning rate\n__C.TRAIN.GAMMA = 0.1\n\n# Step size for reducing the learning rate, currently only support one step\n__C.TRAIN.STEPSIZE = [30000]\n\n# Iteration intervals for showing the loss during training, on command line interface\n__C.TRAIN.DISPLAY = 10\n\n# Whether to double the learning rate for bias\n__C.TRAIN.DOUBLE_BIAS = True\n\n# Whether to initialize the weights with truncated normal distribution\n__C.TRAIN.TRUNCATED = False\n\n# Whether to have weight decay on bias as well\n__C.TRAIN.BIAS_DECAY = False\n\n# Whether to add ground truth boxes to the pool when sampling regions\n__C.TRAIN.USE_GT = False\n\n# Whether to use aspect-ratio grouping of training images, introduced merely for saving\n# GPU memory\n__C.TRAIN.ASPECT_GROUPING = False\n\n# The number of snapshots kept, older ones are deleted to save space\n__C.TRAIN.SNAPSHOT_KEPT = 3\n\n# The time interval for saving tensorflow summaries\n__C.TRAIN.SUMMARY_INTERVAL = 180\n\n# Scale to use during training (can list multiple scales)\n# The scale is the pixel size of an image\'s shortest side\n__C.TRAIN.SCALES = (600,)\n__C.TRAIN.query_size = 128\n\n# Max pixel size of the longest side of a scaled input image\n__C.TRAIN.MAX_SIZE = 1000\n\n# Trim size for input images to create minibatch\n__C.TRAIN.TRIM_HEIGHT = 600\n__C.TRAIN.TRIM_WIDTH = 600\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 1\n\n# Minibatch size (number of regions of interest [ROIs])\n__C.TRAIN.BATCH_SIZE = 128\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = 0.5\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = 0.5\n__C.TRAIN.BG_THRESH_LO = 0.1\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Train bounding-box regressors\n__C.TRAIN.BBOX_REG = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = 0.5\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 5000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_PREFIX = \'res101_faster_rcnn\'\n# __C.TRAIN.SNAPSHOT_INFIX = \'\'\n\n# Use a prefetch thread in roi_data_layer.layer\n# So far I haven\'t found this useful; likely more engineering work is required\n# __C.TRAIN.USE_PREFETCH = False\n\n# Normalize the targets (subtract empirical mean, divide by empirical stddev)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS = True\n# Deprecated (inside weights)\n__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Normalize the targets using ""precomputed"" (or made up) means and stdevs\n# (BBOX_NORMALIZE_TARGETS must also be True)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = True\n__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n\n# Train using these proposals\n__C.TRAIN.PROPOSAL_METHOD = \'gt\'\n\n# Make minibatches from images that have similar aspect ratios (i.e. both\n# tall and thin or both short and wide) in order to avoid wasting computation\n# on zero-padding.\n\n# Use RPN to detect objects\n__C.TRAIN.HAS_RPN = True\n# IOU >= thresh: positive example\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n# IOU < thresh: negative example\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n# If an anchor statisfied by positive and negative conditions set to negative\n__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n# Max number of foreground examples\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n# Total number of examples\n__C.TRAIN.RPN_BATCHSIZE = 256\n# NMS threshold used on RPN proposals\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TRAIN.RPN_MIN_SIZE = 8\n# Deprecated (outside weights)\n__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Give the positive RPN examples weight of p * 1 / {num positives}\n# and give negatives a weight of (1 - p)\n# Set to -1.0 to use uniform example weighting\n__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n# Whether to use all ground truth bounding boxes for training,\n# For COCO, setting USE_ALL_GT to False will exclude boxes that are flagged as \'\'iscrowd\'\'\n__C.TRAIN.USE_ALL_GT = True\n\n# Whether to tune the batch normalization parameters during training\n__C.TRAIN.BN_TRAIN = False\n\n#\n# Testing options\n#\n__C.TEST = edict()\n\n# Scale to use during testing (can NOT list multiple scales)\n# The scale is the pixel size of an image\'s shortest side\n__C.TEST.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TEST.MAX_SIZE = 1000\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.3\n\n# Experimental: treat the (K+1) units in the cls_score layer as linear\n# predictors (trained, eg, with one-vs-rest SVMs).\n__C.TEST.SVM = False\n\n# Test using bounding-box regressors\n__C.TEST.BBOX_REG = True\n\n# Propose boxes\n__C.TEST.HAS_RPN = False\n\n# Test using these proposals\n__C.TEST.PROPOSAL_METHOD = \'gt\'\n\n## NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n## Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n\n## Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TEST.RPN_POST_NMS_TOP_N = 300\n\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TEST.RPN_MIN_SIZE = 16\n\n# Testing mode, default to be \'nms\', \'top\' is slower but better\n# See report for details\n__C.TEST.MODE = \'nms\'\n\n# Only useful when TEST.MODE is \'top\', specifies the number of top proposals to select\n__C.TEST.RPN_TOP_N = 5000\n\n#\n# ResNet options\n#\n\n__C.RESNET = edict()\n\n# Option to set if max-pooling is appended after crop_and_resize.\n# if true, the region will be resized to a square of 2xPOOLING_SIZE,\n# then 2x2 max-pooling is applied; otherwise the region will be directly\n# resized to a square of POOLING_SIZE\n__C.RESNET.MAX_POOL = False\n\n# Number of fixed blocks during training, by default the first of all 4 blocks is fixed\n# Range: 0 (none) to 3 (all)\n__C.RESNET.FIXED_BLOCKS = 2\n\n#\n# MobileNet options\n#\n\n__C.MOBILENET = edict()\n\n# Whether to regularize the depth-wise filters during training\n__C.MOBILENET.REGU_DEPTH = False\n\n# Number of fixed layers during training, by default the first of all 14 layers is fixed\n# Range: 0 (none) to 12 (all)\n__C.MOBILENET.FIXED_LAYERS = 5\n\n# Weight decay for the mobilenet weights\n__C.MOBILENET.WEIGHT_DECAY = 0.00004\n\n# Depth multiplier\n__C.MOBILENET.DEPTH_MULTIPLIER = 1.\n\n#\n# MISC\n#\n\n__C.train_categories = [1]\n__C.test_categories = [1]\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1. / 16.\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# We use the same pixel mean for all networks even though it\'s not exactly what\n# they were trained with\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# A small number that\'s used many times\n__C.EPS = 1e-14\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\', \'..\'))\n\n# Data directory\n__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'../data\'))\n\n# Name (or path to) the matlab executable\n__C.MATLAB = \'matlab\'\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = \'default\'\n\n# Use GPU implementation of non-maximum suppression\n__C.USE_GPU_NMS = True\n\n# Default GPU device id\n__C.GPU_ID = 0\n\n__C.POOLING_MODE = \'crop\'\n\n# Size of the pooled region after RoI pooling\n__C.POOLING_SIZE = 7\n\n# Maximal number of gt rois in an image during Training\n__C.MAX_NUM_GT_BOXES = 20\n\n# Anchor scales for RPN\n__C.ANCHOR_SCALES = [8,16,32]\n\n# Anchor ratios for RPN\n__C.ANCHOR_RATIOS = [0.5,1,2]\n\n# Feature stride for RPN\n__C.FEAT_STRIDE = [16, ]\n\n__C.CUDA = False\n\n__C.CROP_RESIZE_WITH_MAX_POOL = True\n\nimport pdb\ndef get_output_dir(imdb, weights_filename):\n  """"""Return the directory where experimental artifacts are placed.\n  If the directory does not exist, it is created.\n\n  A canonical path is built using the name from an imdb and a network\n  (if not None).\n  """"""\n  outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'output\', __C.EXP_DIR, imdb.name))\n  if weights_filename is None:\n    weights_filename = \'default\'\n  outdir = osp.join(outdir, weights_filename)\n  if not os.path.exists(outdir):\n    os.makedirs(outdir)\n  return outdir\n\n\ndef get_output_tb_dir(imdb, weights_filename):\n  """"""Return the directory where tensorflow summaries are placed.\n  If the directory does not exist, it is created.\n\n  A canonical path is built using the name from an imdb and a network\n  (if not None).\n  """"""\n  outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'tensorboard\', __C.EXP_DIR, imdb.name))\n  if weights_filename is None:\n    weights_filename = \'default\'\n  outdir = osp.join(outdir, weights_filename)\n  if not os.path.exists(outdir):\n    os.makedirs(outdir)\n  return outdir\n\n\ndef _merge_a_into_b(a, b):\n  """"""Merge config dictionary a into config dictionary b, clobbering the\n  options in b whenever they are also specified in a.\n  """"""\n  if type(a) is not edict:\n    return\n\n  for k, v in a.items():\n    # a must specify keys that are in b\n    if k not in b:\n      raise KeyError(\'{} is not a valid config key\'.format(k))\n\n    # the types must match, too\n    old_type = type(b[k])\n    if old_type is not type(v):\n      if isinstance(b[k], np.ndarray):\n        v = np.array(v, dtype=b[k].dtype)\n      else:\n        raise ValueError((\'Type mismatch ({} vs. {}) \'\n                          \'for config key: {}\').format(type(b[k]),\n                                                       type(v), k))\n\n    # recursively merge dicts\n    if type(v) is edict:\n      try:\n        _merge_a_into_b(a[k], b[k])\n      except:\n        print((\'Error under config key: {}\'.format(k)))\n        raise\n    else:\n      b[k] = v\n\n\ndef cfg_from_file(filename):\n  """"""Load a config file and merge it into the default options.""""""\n  import yaml\n  with open(filename, \'r\') as f:\n    yaml_cfg = edict(yaml.load(f))\n\n  _merge_a_into_b(yaml_cfg, __C)\n\n\ndef cfg_from_list(cfg_list):\n  """"""Set config keys via list (e.g., from command line).""""""\n  from ast import literal_eval\n  assert len(cfg_list) % 2 == 0\n  for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n    key_list = k.split(\'.\')\n    d = __C\n    for subkey in key_list[:-1]:\n      assert subkey in d\n      d = d[subkey]\n    subkey = key_list[-1]\n    assert subkey in d\n    try:\n      value = literal_eval(v)\n    except:\n      # handle the case when v is a string literal\n      value = v\n    assert type(value) == type(d[subkey]), \\\n      \'type {} does not match original type {}\'.format(\n        type(value), type(d[subkey]))\n    d[subkey] = value\n'"
lib/model/utils/logger.py,0,"b'# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc \ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import BytesIO         # Python 3.x\n\n\nclass Logger(object):\n    \n    def __init__(self, log_dir):\n        """"""Create a summary writer logging to log_dir.""""""\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def scalar_summary(self, tag, value, step):\n        """"""Log a scalar variable.""""""\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def image_summary(self, tag, images, step):\n        """"""Log a list of images.""""""\n\n        img_summaries = []\n        for i, img in enumerate(images):\n            # Write the image to a string\n            try:\n                s = StringIO()\n            except:\n                s = BytesIO()\n            scipy.misc.toimage(img).save(s, format=""png"")\n\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                       height=img.shape[0],\n                                       width=img.shape[1])\n            # Create a Summary value\n            img_summaries.append(tf.Summary.Value(tag=\'%s/%d\' % (tag, i), image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=img_summaries)\n        self.writer.add_summary(summary, step)\n        \n    def histo_summary(self, tag, values, step, bins=1000):\n        """"""Log a histogram of the tensor of values.""""""\n\n        # Create a histogram using numpy\n        counts, bin_edges = np.histogram(values, bins=bins)\n\n        # Fill the fields of the histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values**2))\n\n        # Drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n        self.writer.add_summary(summary, step)\n        self.writer.flush()\n'"
lib/model/utils/net_utils.py,21,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision.models as models\nfrom model.utils.config import cfg\nimport cv2\nimport pdb\nimport random\n\ndef save_net(fname, net):\n    import h5py\n    h5f = h5py.File(fname, mode=\'w\')\n    for k, v in net.state_dict().items():\n        h5f.create_dataset(k, data=v.cpu().numpy())\n\ndef load_net(fname, net):\n    import h5py\n    h5f = h5py.File(fname, mode=\'r\')\n    for k, v in net.state_dict().items():\n        param = torch.from_numpy(np.asarray(h5f[k]))\n        v.copy_(param)\n\ndef weights_normal_init(model, dev=0.01):\n    if isinstance(model, list):\n        for m in model:\n            weights_normal_init(m, dev)\n    else:\n        for m in model.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0.0, dev)\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0.0, dev)\n\n\ndef clip_gradient(model, clip_norm):\n    """"""Computes a gradient clipping coefficient based on gradient norm.""""""\n    totalnorm = 0\n    for p in model.parameters():\n        if p.requires_grad:\n            modulenorm = p.grad.data.norm()\n            totalnorm += modulenorm ** 2\n    totalnorm = torch.sqrt(totalnorm).item()\n    norm = (clip_norm / max(totalnorm, clip_norm))\n    for p in model.parameters():\n        if p.requires_grad:\n            p.grad.mul_(norm)\n\ndef vis_detections(im, class_name, dets, thresh=0.5):\n    """"""Visual debugging of detections.""""""\n    for i in range(np.minimum(10, dets.shape[0])):\n        bbox = tuple(int(np.round(x)) for x in dets[i, :4])\n        score = dets[i, -1]\n        if score > 0.8:\n            cv2.rectangle(im, bbox[0:2], bbox[2:4], (0, 110, 255), 5)\n\n            text = \'%.3f\' % (score)\n\n            (text_width, text_height) = cv2.getTextSize(text, cv2.FONT_HERSHEY_TRIPLEX, fontScale=1.2, thickness=2)[0]\n\n            cv2.rectangle(im, (bbox[0], bbox[1] ), (bbox[0]+text_width, bbox[1] + text_height), (0, 255, 251), -1)\n\n            cv2.putText(im,text , (bbox[0], bbox[1]+text_height), cv2.FONT_HERSHEY_TRIPLEX, 1.2, (0, 0, 0), thickness=2)\n    return im\n\ndef adjust_learning_rate(optimizer, decay=0.1):\n    """"""Sets the learning rate to the initial LR decayed by 0.5 every 20 epochs""""""\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = decay * param_group[\'lr\']\n\ndef save_checkpoint(state, filename):\n    torch.save(state, filename)\n\ndef _smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n\n    sigma_2 = sigma ** 2\n    box_diff = bbox_pred - bbox_targets\n    in_box_diff = bbox_inside_weights * box_diff\n    abs_in_box_diff = torch.abs(in_box_diff)\n    smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\n    in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n                  + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n    out_loss_box = bbox_outside_weights * in_loss_box\n    loss_box = out_loss_box\n    for i in sorted(dim, reverse=True):\n      loss_box = loss_box.sum(i)\n    loss_box = loss_box.mean()\n    return loss_box\n\ndef _crop_pool_layer(bottom, rois, max_pool=True):\n    # code modified from\n    # https://github.com/ruotianluo/pytorch-faster-rcnn\n    # implement it using stn\n    # box to affine\n    # input (x1,y1,x2,y2)\n    """"""\n    [  x2-x1             x1 + x2 - W + 1  ]\n    [  -----      0      ---------------  ]\n    [  W - 1                  W - 1       ]\n    [                                     ]\n    [           y2-y1    y1 + y2 - H + 1  ]\n    [    0      -----    ---------------  ]\n    [           H - 1         H - 1      ]\n    """"""\n    rois = rois.detach()\n    batch_size = bottom.size(0)\n    D = bottom.size(1)\n    H = bottom.size(2)\n    W = bottom.size(3)\n    roi_per_batch = rois.size(0) / batch_size\n    x1 = rois[:, 1::4] / 16.0\n    y1 = rois[:, 2::4] / 16.0\n    x2 = rois[:, 3::4] / 16.0\n    y2 = rois[:, 4::4] / 16.0\n\n    height = bottom.size(2)\n    width = bottom.size(3)\n\n    # affine theta\n    zero = Variable(rois.data.new(rois.size(0), 1).zero_())\n    theta = torch.cat([\\\n      (x2 - x1) / (width - 1),\n      zero,\n      (x1 + x2 - width + 1) / (width - 1),\n      zero,\n      (y2 - y1) / (height - 1),\n      (y1 + y2 - height + 1) / (height - 1)], 1).view(-1, 2, 3)\n\n    if max_pool:\n      pre_pool_size = cfg.POOLING_SIZE * 2\n      grid = F.affine_grid(theta, torch.Size((rois.size(0), 1, pre_pool_size, pre_pool_size)))\n      bottom = bottom.view(1, batch_size, D, H, W).contiguous().expand(roi_per_batch, batch_size, D, H, W)\\\n                                                                .contiguous().view(-1, D, H, W)\n      crops = F.grid_sample(bottom, grid)\n      crops = F.max_pool2d(crops, 2, 2)\n    else:\n      grid = F.affine_grid(theta, torch.Size((rois.size(0), 1, cfg.POOLING_SIZE, cfg.POOLING_SIZE)))\n      bottom = bottom.view(1, batch_size, D, H, W).contiguous().expand(roi_per_batch, batch_size, D, H, W)\\\n                                                                .contiguous().view(-1, D, H, W)\n      crops = F.grid_sample(bottom, grid)\n\n    return crops, grid\n\ndef _affine_grid_gen(rois, input_size, grid_size):\n\n    rois = rois.detach()\n    x1 = rois[:, 1::4] / 16.0\n    y1 = rois[:, 2::4] / 16.0\n    x2 = rois[:, 3::4] / 16.0\n    y2 = rois[:, 4::4] / 16.0\n\n    height = input_size[0]\n    width = input_size[1]\n\n    zero = Variable(rois.data.new(rois.size(0), 1).zero_())\n    theta = torch.cat([\\\n      (x2 - x1) / (width - 1),\n      zero,\n      (x1 + x2 - width + 1) / (width - 1),\n      zero,\n      (y2 - y1) / (height - 1),\n      (y1 + y2 - height + 1) / (height - 1)], 1).view(-1, 2, 3)\n\n    grid = F.affine_grid(theta, torch.Size((rois.size(0), 1, grid_size, grid_size)))\n\n    return grid\n\ndef _affine_theta(rois, input_size):\n\n    rois = rois.detach()\n    x1 = rois[:, 1::4] / 16.0\n    y1 = rois[:, 2::4] / 16.0\n    x2 = rois[:, 3::4] / 16.0\n    y2 = rois[:, 4::4] / 16.0\n\n    height = input_size[0]\n    width = input_size[1]\n\n    zero = Variable(rois.data.new(rois.size(0), 1).zero_())\n\n    # theta = torch.cat([\\\n    #   (x2 - x1) / (width - 1),\n    #   zero,\n    #   (x1 + x2 - width + 1) / (width - 1),\n    #   zero,\n    #   (y2 - y1) / (height - 1),\n    #   (y1 + y2 - height + 1) / (height - 1)], 1).view(-1, 2, 3)\n\n    theta = torch.cat([\\\n      (y2 - y1) / (height - 1),\n      zero,\n      (y1 + y2 - height + 1) / (height - 1),\n      zero,\n      (x2 - x1) / (width - 1),\n      (x1 + x2 - width + 1) / (width - 1)], 1).view(-1, 2, 3)\n\n    return theta\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), \'kernel size must be 3 or 7\'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n        \nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass ChannelGate(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16, pool_types=[\'avg\', \'max\']):\n        super(ChannelGate, self).__init__()\n        self.gate_channels = gate_channels\n        self.mlp = nn.Sequential(\n            Flatten(),\n            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n            nn.ReLU(),\n            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n            )\n        self.pool_types = pool_types\n    def forward(self, x):\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type==\'avg\':\n                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                channel_att_raw = self.mlp( avg_pool )\n            elif pool_type==\'max\':\n                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                channel_att_raw = self.mlp( max_pool )\n            elif pool_type==\'lp\':\n                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                channel_att_raw = self.mlp( lp_pool )\n            elif pool_type==\'lse\':\n                # LSE pool only\n                lse_pool = logsumexp_2d(x)\n                channel_att_raw = self.mlp( lse_pool )\n\n            if channel_att_sum is None:\n                channel_att_sum = channel_att_raw\n            else:\n                channel_att_sum = channel_att_sum + channel_att_raw\n\n        scale = torch.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3)\n        return scale\n\nclass GroupNorm(nn.Module):\n    def __init__(self, num_features, num_groups=32, eps=1e-5):\n        super(GroupNorm, self).__init__()\n        self.weight = nn.Parameter(torch.ones(1,num_features,1,1))\n        self.bias = nn.Parameter(torch.zeros(1,num_features,1,1))\n        self.num_groups = num_groups\n        self.eps = eps\n\n    def forward(self, x):\n        N,C,H,W = x.size()\n        G = self.num_groups\n        assert C % G == 0\n\n        x = x.view(N,G,-1)\n        mean = x.mean(-1, keepdim=True)\n        var = x.var(-1, keepdim=True)\n\n        x = (x-mean) / (var+self.eps).sqrt()\n        x = x.view(N,C,H,W)\n        return x * self.weight + self.bias\n'"
lib/model/nms/_ext/__init__.py,0,b''
lib/model/roi_align/_ext/__init__.py,0,b''
lib/model/roi_align/functions/__init__.py,0,b''
lib/model/roi_align/functions/roi_align.py,1,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import roi_align\n\n\n# TODO use save_for_backward instead\nclass RoIAlignFunction(Function):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n        self.rois = None\n        self.feature_size = None\n\n    def forward(self, features, rois):\n        self.rois = rois\n        self.feature_size = features.size()\n\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size(0)\n\n        output = features.new(num_rois, num_channels, self.aligned_height, self.aligned_width).zero_()\n        if features.is_cuda:\n            roi_align.roi_align_forward_cuda(self.aligned_height,\n                                             self.aligned_width,\n                                             self.spatial_scale, features,\n                                             rois, output)\n        else:\n            roi_align.roi_align_forward(self.aligned_height,\n                                        self.aligned_width,\n                                        self.spatial_scale, features,\n                                        rois, output)\n#            raise NotImplementedError\n\n        return output\n\n    def backward(self, grad_output):\n        assert(self.feature_size is not None and grad_output.is_cuda)\n\n        batch_size, num_channels, data_height, data_width = self.feature_size\n\n        grad_input = self.rois.new(batch_size, num_channels, data_height,\n                                  data_width).zero_()\n        roi_align.roi_align_backward_cuda(self.aligned_height,\n                                          self.aligned_width,\n                                          self.spatial_scale, grad_output,\n                                          self.rois, grad_input)\n\n        # print grad_input\n\n        return grad_input, None\n'"
lib/model/roi_align/modules/__init__.py,0,b''
lib/model/roi_align/modules/roi_align.py,2,"b'from torch.nn.modules.module import Module\nfrom torch.nn.functional import avg_pool2d, max_pool2d\nfrom ..functions.roi_align import RoIAlignFunction\n\n\nclass RoIAlign(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        super(RoIAlign, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return RoIAlignFunction(self.aligned_height, self.aligned_width,\n                                self.spatial_scale)(features, rois)\n\nclass RoIAlignAvg(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        super(RoIAlignAvg, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        x =  RoIAlignFunction(self.aligned_height+1, self.aligned_width+1,\n                                self.spatial_scale)(features, rois)\n        return avg_pool2d(x, kernel_size=2, stride=1)\n\nclass RoIAlignMax(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        super(RoIAlignMax, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        x =  RoIAlignFunction(self.aligned_height+1, self.aligned_width+1,\n                                self.spatial_scale)(features, rois)\n        return max_pool2d(x, kernel_size=2, stride=1)\n'"
lib/model/roi_crop/_ext/__init__.py,0,b''
lib/model/roi_crop/functions/__init__.py,0,b''
lib/model/roi_crop/functions/crop_resize.py,6,"b'# functions/add.py\nimport torch\nfrom torch.autograd import Function\nfrom .._ext import roi_crop\nfrom cffi import FFI\nffi = FFI()\n\nclass RoICropFunction(Function):\n    def forward(self, input1, input2):\n        self.input1 = input1\n        self.input2 = input2\n        self.device_c = ffi.new(""int *"")\n        output = torch.zeros(input2.size()[0], input1.size()[1], input2.size()[1], input2.size()[2])\n        #print(\'decice %d\' % torch.cuda.current_device())\n        if input1.is_cuda:\n            self.device = torch.cuda.current_device()\n        else:\n            self.device = -1\n        self.device_c[0] = self.device\n        if not input1.is_cuda:\n            roi_crop.BilinearSamplerBHWD_updateOutput(input1, input2, output)\n        else:\n            output = output.cuda(self.device)\n            roi_crop.BilinearSamplerBHWD_updateOutput_cuda(input1, input2, output)\n        return output\n\n    def backward(self, grad_output):\n        grad_input1 = torch.zeros(self.input1.size())\n        grad_input2 = torch.zeros(self.input2.size())\n        #print(\'backward decice %d\' % self.device)\n        if not grad_output.is_cuda:\n            roi_crop.BilinearSamplerBHWD_updateGradInput(self.input1, self.input2, grad_input1, grad_input2, grad_output)\n        else:\n            grad_input1 = grad_input1.cuda(self.device)\n            grad_input2 = grad_input2.cuda(self.device)\n            roi_crop.BilinearSamplerBHWD_updateGradInput_cuda(self.input1, self.input2, grad_input1, grad_input2, grad_output)\n        return grad_input1, grad_input2\n'"
lib/model/roi_crop/functions/gridgen.py,6,"b'# functions/add.py\nimport torch\nfrom torch.autograd import Function\nimport numpy as np\n\n\nclass AffineGridGenFunction(Function):\n    def __init__(self, height, width,lr=1):\n        super(AffineGridGenFunction, self).__init__()\n        self.lr = lr\n        self.height, self.width = height, width\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/(self.height)), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/(self.width)), 0), repeats = self.height, axis = 0), 0)\n        # self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/(self.height - 1)), 0), repeats = self.width, axis = 0).T, 0)\n        # self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/(self.width - 1)), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n        #print(self.grid)\n\n    def forward(self, input1):\n        self.input1 = input1\n        output = input1.new(torch.Size([input1.size(0)]) + self.grid.size()).zero_()\n        self.batchgrid = input1.new(torch.Size([input1.size(0)]) + self.grid.size()).zero_()\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid.astype(self.batchgrid[i])\n\n        # if input1.is_cuda:\n        #    self.batchgrid = self.batchgrid.cuda()\n        #    output = output.cuda()\n\n        for i in range(input1.size(0)):\n            output = torch.bmm(self.batchgrid.view(-1, self.height*self.width, 3), torch.transpose(input1, 1, 2)).view(-1, self.height, self.width, 2)\n\n        return output\n\n    def backward(self, grad_output):\n\n        grad_input1 = self.input1.new(self.input1.size()).zero_()\n\n        # if grad_output.is_cuda:\n        #    self.batchgrid = self.batchgrid.cuda()\n        #    grad_input1 = grad_input1.cuda()\n\n        grad_input1 = torch.baddbmm(grad_input1, torch.transpose(grad_output.view(-1, self.height*self.width, 2), 1,2), self.batchgrid.view(-1, self.height*self.width, 3))\n        return grad_input1\n'"
lib/model/roi_crop/functions/roi_crop.py,1,"b'# functions/add.py\nimport torch\nfrom torch.autograd import Function\nfrom .._ext import roi_crop\nimport pdb\n\nclass RoICropFunction(Function):\n    def forward(self, input1, input2):\n        self.input1 = input1.clone()\n        self.input2 = input2.clone()\n        output = input2.new(input2.size()[0], input1.size()[1], input2.size()[1], input2.size()[2]).zero_()\n        assert output.get_device() == input1.get_device(), ""output and input1 must on the same device""\n        assert output.get_device() == input2.get_device(), ""output and input2 must on the same device""\n        roi_crop.BilinearSamplerBHWD_updateOutput_cuda(input1, input2, output)\n        return output\n\n    def backward(self, grad_output):\n        grad_input1 = self.input1.new(self.input1.size()).zero_()\n        grad_input2 = self.input2.new(self.input2.size()).zero_()\n        roi_crop.BilinearSamplerBHWD_updateGradInput_cuda(self.input1, self.input2, grad_input1, grad_input2, grad_output)\n        return grad_input1, grad_input2\n'"
lib/model/roi_crop/modules/__init__.py,0,b''
lib/model/roi_crop/modules/gridgen.py,80,"b'from torch.nn.modules.module import Module\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nfrom ..functions.gridgen import AffineGridGenFunction\n\nimport pyximport\npyximport.install(setup_args={""include_dirs"":np.get_include()},\n                  reload_support=True)\n\n\nclass _AffineGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(_AffineGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.f = AffineGridGenFunction(self.height, self.width, lr=lr)\n        self.lr = lr\n    def forward(self, input):\n        # if not self.aux_loss:\n        return self.f(input)\n        # else:\n        #     identity = torch.from_numpy(np.array([[1,0,0], [0,1,0]], dtype=np.float32))\n        #     batch_identity = torch.zeros([input.size(0), 2,3])\n        #     for i in range(input.size(0)):\n        #         batch_identity[i] = identity\n        #     batch_identity = Variable(batch_identity)\n        #     loss = torch.mul(input - batch_identity, input - batch_identity)\n        #     loss = torch.sum(loss,1)\n        #     loss = torch.sum(loss,2)\n\n        #       return self.f(input), loss.view(-1,1)\n\nclass CylinderGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(CylinderGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.f = CylinderGridGenFunction(self.height, self.width, lr=lr)\n        self.lr = lr\n    def forward(self, input):\n\n        if not self.aux_loss:\n            return self.f(input)\n        else:\n            return self.f(input), torch.mul(input, input).view(-1,1)\n\n\nclass AffineGridGenV2(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(AffineGridGenV2, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n\n    def forward(self, input1):\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n        self.batchgrid = Variable(self.batchgrid)\n\n        if input1.is_cuda:\n            self.batchgrid = self.batchgrid.cuda()\n\n        output = torch.bmm(self.batchgrid.view(-1, self.height*self.width, 3), torch.transpose(input1, 1, 2)).view(-1, self.height, self.width, 2)\n\n        return output\n\n\nclass CylinderGridGenV2(Module):\n    def __init__(self, height, width, lr = 1):\n        super(CylinderGridGenV2, self).__init__()\n        self.height, self.width = height, width\n        self.lr = lr\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n    def forward(self, input):\n        self.batchgrid = torch.zeros(torch.Size([input.size(0)]) + self.grid.size() )\n        #print(self.batchgrid.size())\n        for i in range(input.size(0)):\n            self.batchgrid[i,:,:,:] = self.grid\n        self.batchgrid = Variable(self.batchgrid)\n\n        #print(self.batchgrid.size())\n\n        input_u = input.view(-1,1,1,1).repeat(1,self.height, self.width,1)\n        #print(input_u.requires_grad, self.batchgrid)\n\n        output0 = self.batchgrid[:,:,:,0:1]\n        output1 = torch.atan(torch.tan(np.pi/2.0*(self.batchgrid[:,:,:,1:2] + self.batchgrid[:,:,:,2:] * input_u[:,:,:,:])))  /(np.pi/2)\n        #print(output0.size(), output1.size())\n\n        output = torch.cat([output0, output1], 3)\n        return output\n\n\nclass DenseAffineGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(DenseAffineGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n\n    def forward(self, input1):\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n        #print self.batchgrid,  input1[:,:,:,0:3]\n        #print self.batchgrid,  input1[:,:,:,4:6]\n        x = torch.mul(self.batchgrid, input1[:,:,:,0:3])\n        y = torch.mul(self.batchgrid, input1[:,:,:,3:6])\n\n        output = torch.cat([torch.sum(x,3),torch.sum(y,3)], 3)\n        return output\n\n\n\n\nclass DenseAffine3DGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(DenseAffine3DGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, input1):\n        self.batchgrid3d = torch.zeros(torch.Size([input1.size(0)]) + self.grid3d.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n        #print(self.batchgrid3d)\n\n        x = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,0:4]), 3)\n        y = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,4:8]), 3)\n        z = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,8:]), 3)\n        #print(x)\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n        phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n        phi = phi/np.pi\n\n\n        output = torch.cat([theta,phi], 3)\n\n        return output\n\n\n\n\n\nclass DenseAffine3DGridGen_rotate(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(DenseAffine3DGridGen_rotate, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, input1, input2):\n        self.batchgrid3d = torch.zeros(torch.Size([input1.size(0)]) + self.grid3d.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n\n        #print(self.batchgrid3d)\n\n        x = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,0:4]), 3)\n        y = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,4:8]), 3)\n        z = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,8:]), 3)\n        #print(x)\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n        phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n        phi = phi/np.pi\n\n        input_u = input2.view(-1,1,1,1).repeat(1,self.height, self.width,1)\n\n        output = torch.cat([theta,phi], 3)\n\n        output1 = torch.atan(torch.tan(np.pi/2.0*(output[:,:,:,1:2] + self.batchgrid[:,:,:,2:] * input_u[:,:,:,:])))  /(np.pi/2)\n        output2 = torch.cat([output[:,:,:,0:1], output1], 3)\n\n        return output2\n\n\nclass Depth3DGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(Depth3DGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, depth, trans0, trans1, rotate):\n        self.batchgrid3d = torch.zeros(torch.Size([depth.size(0)]) + self.grid3d.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n\n        self.batchgrid = torch.zeros(torch.Size([depth.size(0)]) + self.grid.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n\n        x = self.batchgrid3d[:,:,:,0:1] * depth + trans0.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n\n        y = self.batchgrid3d[:,:,:,1:2] * depth + trans1.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n        z = self.batchgrid3d[:,:,:,2:3] * depth\n        #print(x.size(), y.size(), z.size())\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n        phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n        phi = phi/np.pi\n\n        #print(theta.size(), phi.size())\n\n\n        input_u = rotate.view(-1,1,1,1).repeat(1,self.height, self.width,1)\n\n        output = torch.cat([theta,phi], 3)\n        #print(output.size())\n\n        output1 = torch.atan(torch.tan(np.pi/2.0*(output[:,:,:,1:2] + self.batchgrid[:,:,:,2:] * input_u[:,:,:,:])))  /(np.pi/2)\n        output2 = torch.cat([output[:,:,:,0:1], output1], 3)\n\n        return output2\n\n\n\n\n\nclass Depth3DGridGen_with_mask(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False, ray_tracing = False):\n        super(Depth3DGridGen_with_mask, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n        self.ray_tracing = ray_tracing\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, depth, trans0, trans1, rotate):\n        self.batchgrid3d = torch.zeros(torch.Size([depth.size(0)]) + self.grid3d.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n\n        self.batchgrid = torch.zeros(torch.Size([depth.size(0)]) + self.grid.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n\n        if depth.is_cuda:\n            self.batchgrid = self.batchgrid.cuda()\n            self.batchgrid3d = self.batchgrid3d.cuda()\n\n\n        x_ = self.batchgrid3d[:,:,:,0:1] * depth + trans0.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n\n        y_ = self.batchgrid3d[:,:,:,1:2] * depth + trans1.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n        z = self.batchgrid3d[:,:,:,2:3] * depth\n        #print(x.size(), y.size(), z.size())\n\n        rotate_z = rotate.view(-1,1,1,1).repeat(1,self.height, self.width,1) * np.pi\n\n        x = x_ * torch.cos(rotate_z) - y_ * torch.sin(rotate_z)\n        y = x_ * torch.sin(rotate_z) + y_ * torch.cos(rotate_z)\n\n\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n\n        if depth.is_cuda:\n            phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.cuda.FloatTensor) * (y.ge(0).type(torch.cuda.FloatTensor) - y.lt(0).type(torch.cuda.FloatTensor))\n        else:\n            phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n\n\n        phi = phi/np.pi\n\n        output = torch.cat([theta,phi], 3)\n        return output\n'"
lib/model/roi_crop/modules/roi_crop.py,1,"b""from torch.nn.modules.module import Module\nfrom ..functions.roi_crop import RoICropFunction\n\nclass _RoICrop(Module):\n    def __init__(self, layout = 'BHWD'):\n        super(_RoICrop, self).__init__()\n    def forward(self, input1, input2):\n        return RoICropFunction()(input1, input2)\n"""
lib/model/roi_pooling/_ext/__init__.py,0,b''
lib/model/roi_pooling/functions/__init__.py,0,b''
lib/model/roi_pooling/functions/roi_pool.py,1,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import roi_pooling\nimport pdb\n\nclass RoIPoolFunction(Function):\n    def __init__(ctx, pooled_height, pooled_width, spatial_scale):\n        ctx.pooled_width = pooled_width\n        ctx.pooled_height = pooled_height\n        ctx.spatial_scale = spatial_scale\n        ctx.feature_size = None\n\n    def forward(ctx, features, rois): \n        ctx.feature_size = features.size()           \n        batch_size, num_channels, data_height, data_width = ctx.feature_size\n        num_rois = rois.size(0)\n        output = features.new(num_rois, num_channels, ctx.pooled_height, ctx.pooled_width).zero_()\n        ctx.argmax = features.new(num_rois, num_channels, ctx.pooled_height, ctx.pooled_width).zero_().int()\n        ctx.rois = rois\n        if not features.is_cuda:\n            _features = features.permute(0, 2, 3, 1)\n            roi_pooling.roi_pooling_forward(ctx.pooled_height, ctx.pooled_width, ctx.spatial_scale,\n                                            _features, rois, output)\n        else:\n            roi_pooling.roi_pooling_forward_cuda(ctx.pooled_height, ctx.pooled_width, ctx.spatial_scale,\n                                                 features, rois, output, ctx.argmax)\n\n        return output\n\n    def backward(ctx, grad_output):\n        assert(ctx.feature_size is not None and grad_output.is_cuda)\n        batch_size, num_channels, data_height, data_width = ctx.feature_size\n        grad_input = grad_output.new(batch_size, num_channels, data_height, data_width).zero_()\n\n        roi_pooling.roi_pooling_backward_cuda(ctx.pooled_height, ctx.pooled_width, ctx.spatial_scale,\n                                              grad_output, ctx.rois, grad_input, ctx.argmax)\n\n        return grad_input, None\n'"
lib/model/roi_pooling/modules/__init__.py,0,b''
lib/model/roi_pooling/modules/roi_pool.py,1,"b'from torch.nn.modules.module import Module\nfrom ..functions.roi_pool import RoIPoolFunction\n\n\nclass _RoIPooling(Module):\n    def __init__(self, pooled_height, pooled_width, spatial_scale):\n        super(_RoIPooling, self).__init__()\n\n        self.pooled_width = int(pooled_width)\n        self.pooled_height = int(pooled_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return RoIPoolFunction(self.pooled_height, self.pooled_width, self.spatial_scale)(features, rois)\n'"
lib/model/nms/_ext/nms/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._nms import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/model/roi_align/_ext/roi_align/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_align import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/model/roi_crop/_ext/crop_resize/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._crop_resize import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        locals[symbol] = _wrap_function(fn, _ffi)\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/model/roi_crop/_ext/roi_crop/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_crop import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/model/roi_pooling/_ext/roi_pooling/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_pooling import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
