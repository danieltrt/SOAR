file_path,api_count,code
demo.py,5,"b'import torch\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom models import EfficientDet\nfrom torchvision import transforms\nimport numpy as np\nimport skimage\nfrom datasets import get_augumentation, VOC_CLASSES\nfrom timeit import default_timer as timer\nimport argparse\nimport copy\nfrom utils import vis_bbox, EFFICIENTDET\n\nparser = argparse.ArgumentParser(description=\'EfficientDet\')\n\nparser.add_argument(\'-n\', \'--network\', default=\'efficientdet-d0\',\n                    help=\'efficientdet-[d0, d1, ..]\')\nparser.add_argument(\'-s\', \'--score\', default=True,\n                    action=""store_true"", help=\'Show score\')\nparser.add_argument(\'-t\', \'--threshold\', default=0.6,\n                    type=float, help=\'Visualization threshold\')\nparser.add_argument(\'-it\', \'--iou_threshold\', default=0.6,\n                    type=float, help=\'Visualization threshold\')\nparser.add_argument(\'-w\', \'--weight\', default=\'./weights/voc0712.pth\',\n                    type=str, help=\'Weight model path\')\nparser.add_argument(\'-c\', \'--cam\',\n                    action=""store_true"", help=\'Use camera\')\nparser.add_argument(\'-f\', \'--file_name\', default=\'pic.jpg\',\n                    help=\'Image path\')\nparser.add_argument(\'--num_class\', default=21, type=int,\n                    help=\'Number of class used in model\')\nargs = parser.parse_args()\n\n\nclass Detect(object):\n    """"""\n        dir_name: Folder or image_file\n    """"""\n\n    def __init__(self, weights, num_class=21, network=\'efficientdet-d0\', size_image=(512, 512)):\n        super(Detect,  self).__init__()\n        self.weights = weights\n        self.size_image = size_image\n        self.device = torch.device(\n            ""cuda:0"" if torch.cuda.is_available() else \'cpu\')\n        self.transform = get_augumentation(phase=\'test\')\n        if(self.weights is not None):\n            print(\'Load pretrained Model\')\n            checkpoint = torch.load(\n                self.weights, map_location=lambda storage, loc: storage)\n            params = checkpoint[\'parser\']\n            num_class = params.num_class\n            network = params.network\n\n        self.model = EfficientDet(num_classes=num_class,\n                                  network=network,\n                                  W_bifpn=EFFICIENTDET[network][\'W_bifpn\'],\n                                  D_bifpn=EFFICIENTDET[network][\'D_bifpn\'],\n                                  D_class=EFFICIENTDET[network][\'D_class\'],\n                                  is_training=False\n                                  )\n\n        if(self.weights is not None):\n            state_dict = checkpoint[\'state_dict\']\n            self.model.load_state_dict(state_dict)\n        if torch.cuda.is_available():\n            self.model = self.model.cuda()\n        self.model.eval()\n\n    def process(self, file_name=None, img=None, show=False):\n        if file_name is not None:\n            img = cv2.imread(file_name)\n        origin_img = copy.deepcopy(img)\n        augmentation = self.transform(image=img)\n        img = augmentation[\'image\']\n        img = img.to(self.device)\n        img = img.unsqueeze(0)\n\n        with torch.no_grad():\n            scores, classification, transformed_anchors = self.model(img)\n            bboxes = list()\n            labels = list()\n            bbox_scores = list()\n            colors = list()\n            for j in range(scores.shape[0]):\n                bbox = transformed_anchors[[j], :][0].data.cpu().numpy()\n                x1 = int(bbox[0]*origin_img.shape[1]/self.size_image[1])\n                y1 = int(bbox[1]*origin_img.shape[0]/self.size_image[0])\n                x2 = int(bbox[2]*origin_img.shape[1]/self.size_image[1])\n                y2 = int(bbox[3]*origin_img.shape[0]/self.size_image[0])\n                bboxes.append([x1, y1, x2, y2])\n                label_name = VOC_CLASSES[int(classification[[j]])]\n                labels.append(label_name)\n\n                if(args.cam):\n                    cv2.rectangle(origin_img, (x1, y1),\n                                  (x2, y2), (179, 255, 179), 2, 1)\n                if args.score:\n                    score = np.around(\n                        scores[[j]].cpu().numpy(), decimals=2) * 100\n                    if(args.cam):\n                        labelSize, baseLine = cv2.getTextSize(\'{} {}\'.format(\n                            label_name, int(score)), cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n                        cv2.rectangle(\n                            origin_img, (x1, y1-labelSize[1]), (x1+labelSize[0], y1+baseLine), (223, 128, 255), cv2.FILLED)\n                        cv2.putText(\n                            origin_img, \'{} {}\'.format(label_name, int(score)),\n                            (x1, y1), cv2.FONT_HERSHEY_SIMPLEX,\n                            0.8, (0, 0, 0), 2\n                        )\n                    bbox_scores.append(int(score))\n                else:\n                    if(args.cam):\n                        labelSize, baseLine = cv2.getTextSize(\'{}\'.format(\n                            label_name), cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n                        cv2.rectangle(\n                            origin_img, (x1, y1-labelSize[1]), (x1+labelSize[0], y1+baseLine), (0, 102, 255), cv2.FILLED)\n                        cv2.putText(\n                            origin_img, \'{} {}\'.format(label_name, int(score)),\n                            (x1, y1), cv2.FONT_HERSHEY_SIMPLEX,\n                            0.8, (0, 0, 0), 2\n                        )\n            if show:\n                fig, ax = vis_bbox(img=origin_img, bbox=bboxes,\n                                   label=labels, score=bbox_scores)\n                fig.savefig(\'./docs/demo.png\')\n                plt.show()\n            else:\n                return origin_img\n\n    def camera(self):\n        cap = cv2.VideoCapture(0)\n        if not cap.isOpened():\n            print(""Unable to open camera"")\n            exit(-1)\n        count_tfps = 1\n        accum_time = 0\n        curr_fps = 0\n        fps = ""FPS: ??""\n        prev_time = timer()\n        while True:\n            res, img = cap.read()\n            curr_time = timer()\n            exec_time = curr_time - prev_time\n            prev_time = curr_time\n            accum_time = accum_time + exec_time\n            curr_fps = curr_fps + 1\n\n            if accum_time > 1:\n                accum_time = accum_time - 1\n                fps = curr_fps\n                curr_fps = 0\n            if res:\n                show_image = self.process(img=img)\n                cv2.putText(\n                    show_image, ""FPS: "" + str(fps), (10,  20),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (204, 51, 51), 2\n                )\n\n                cv2.imshow(""Detection"", show_image)\n                k = cv2.waitKey(1)\n                if k == 27:\n                    break\n            else:\n                print(""Unable to read image"")\n                exit(-1)\n            count_tfps += 1\n        cap.release()\n        cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    detect = Detect(weights=args.weight)\n    print(\'cam: \', args.cam)\n    if args.cam:\n        detect.camera()\n    else:\n        detect.process(file_name=args.file_name, show=True)\n'"
eval.py,4,"b'import argparse\nimport os\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom tqdm import tqdm\nfrom pycocotools.cocoeval import COCOeval\nimport json\n\nfrom datasets import (Augmenter, CocoDataset, Normalizer,\n                      Resizer, VOCDetection, collater, detection_collate,\n                      get_augumentation)\nfrom models.efficientdet import EfficientDet\nfrom utils import EFFICIENTDET, get_state_dict\n\n\ndef compute_overlap(a, b):\n    """"""\n    Parameters\n    ----------\n    a: (N, 4) ndarray of float\n    b: (K, 4) ndarray of float\n    Returns\n    -------\n    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n    """"""\n    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n\n    iw = np.minimum(np.expand_dims(\n        a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n    ih = np.minimum(np.expand_dims(\n        a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n\n    iw = np.maximum(iw, 0)\n    ih = np.maximum(ih, 0)\n\n    ua = np.expand_dims((a[:, 2] - a[:, 0]) *\n                        (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n\n    ua = np.maximum(ua, np.finfo(float).eps)\n\n    intersection = iw * ih\n\n    return intersection / ua\n\n\ndef _compute_ap(recall, precision):\n    """""" Compute the average precision, given the recall and precision curves.\n    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n    # Arguments\n        recall:    The recall curve (list).\n        precision: The precision curve (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], recall, [1.]))\n    mpre = np.concatenate(([0.], precision, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef _get_detections(dataset, retinanet, score_threshold=0.05, max_detections=100, save_path=None):\n    """""" Get the detections from the retinanet using the generator.\n    The result is a list of lists such that the size is:\n        all_detections[num_images][num_classes] = detections[num_detections, 4 + num_classes]\n    # Arguments\n        dataset         : The generator used to run images through the retinanet.\n        retinanet           : The retinanet to run on the images.\n        score_threshold : The score confidence threshold to use.\n        max_detections  : The maximum number of detections to use per image.\n        save_path       : The path to save the images with visualized detections to.\n    # Returns\n        A list of lists containing the detections for each image in the generator.\n    """"""\n    all_detections = [[None for i in range(\n        dataset.num_classes())] for j in range(len(dataset))]\n\n    retinanet.eval()\n\n    with torch.no_grad():\n\n        for index in range(len(dataset)):\n            data = dataset[index]\n            scale = data[\'scale\']\n\n            # run network\n            scores, labels, boxes = retinanet(data[\'img\'].permute(\n                2, 0, 1).cuda().float().unsqueeze(dim=0))\n            scores = scores.cpu().numpy()\n            labels = labels.cpu().numpy()\n            boxes = boxes.cpu().numpy()\n\n            # correct boxes for image scale\n            boxes /= scale\n\n            # select indices which have a score above the threshold\n            indices = np.where(scores > score_threshold)[0]\n            if indices.shape[0] > 0:\n                # select those scores\n                scores = scores[indices]\n\n                # find the order with which to sort the scores\n                scores_sort = np.argsort(-scores)[:max_detections]\n\n                # select detections\n                image_boxes = boxes[indices[scores_sort], :]\n                image_scores = scores[scores_sort]\n                image_labels = labels[indices[scores_sort]]\n                image_detections = np.concatenate([image_boxes, np.expand_dims(\n                    image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1)\n\n                # copy detections to all_detections\n                for label in range(dataset.num_classes()):\n                    all_detections[index][label] = image_detections[image_detections[:, -1] == label, :-1]\n            else:\n                # copy detections to all_detections\n                for label in range(dataset.num_classes()):\n                    all_detections[index][label] = np.zeros((0, 5))\n\n            print(\'{}/{}\'.format(index + 1, len(dataset)), end=\'\\r\')\n\n    return all_detections\n\n\ndef _get_annotations(generator):\n    """""" Get the ground truth annotations from the generator.\n    The result is a list of lists such that the size is:\n        all_detections[num_images][num_classes] = annotations[num_detections, 5]\n    # Arguments\n        generator : The generator used to retrieve ground truth annotations.\n    # Returns\n        A list of lists containing the annotations for each image in the generator.\n    """"""\n    all_annotations = [[None for i in range(\n        generator.num_classes())] for j in range(len(generator))]\n\n    for i in range(len(generator)):\n        # load the annotations\n        annotations = generator.load_annotations(i)\n\n        # copy detections to all_annotations\n        for label in range(generator.num_classes()):\n            all_annotations[i][label] = annotations[annotations[:, 4]\n                                                    == label, :4].copy()\n\n        print(\'{}/{}\'.format(i + 1, len(generator)), end=\'\\r\')\n\n    return all_annotations\n\n\ndef evaluate(\n    generator,\n    retinanet,\n    iou_threshold=0.5,\n    score_threshold=0.05,\n    max_detections=100,\n    save_path=None\n):\n    """""" Evaluate a given dataset using a given retinanet.\n    # Arguments\n        generator       : The generator that represents the dataset to evaluate.\n        retinanet           : The retinanet to evaluate.\n        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n        score_threshold : The score confidence threshold to use for detections.\n        max_detections  : The maximum number of detections to use per image.\n        save_path       : The path to save images with visualized detections to.\n    # Returns\n        A dict mapping class names to mAP scores.\n    """"""\n\n    # gather all detections and annotations\n\n    all_detections = _get_detections(\n        generator, retinanet, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)\n    all_annotations = _get_annotations(generator)\n\n    average_precisions = {}\n\n    for label in range(generator.num_classes()):\n        false_positives = np.zeros((0,))\n        true_positives = np.zeros((0,))\n        scores = np.zeros((0,))\n        num_annotations = 0.0\n\n        for i in range(len(generator)):\n            detections = all_detections[i][label]\n            annotations = all_annotations[i][label]\n            num_annotations += annotations.shape[0]\n            detected_annotations = []\n\n            for d in detections:\n                scores = np.append(scores, d[4])\n\n                if annotations.shape[0] == 0:\n                    false_positives = np.append(false_positives, 1)\n                    true_positives = np.append(true_positives, 0)\n                    continue\n\n                overlaps = compute_overlap(\n                    np.expand_dims(d, axis=0), annotations)\n                assigned_annotation = np.argmax(overlaps, axis=1)\n                max_overlap = overlaps[0, assigned_annotation]\n\n                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n                    false_positives = np.append(false_positives, 0)\n                    true_positives = np.append(true_positives, 1)\n                    detected_annotations.append(assigned_annotation)\n                else:\n                    false_positives = np.append(false_positives, 1)\n                    true_positives = np.append(true_positives, 0)\n\n        # no annotations -> AP for this class is 0 (is this correct?)\n        if num_annotations == 0:\n            average_precisions[label] = 0, 0\n            continue\n\n        # sort by score\n        indices = np.argsort(-scores)\n        false_positives = false_positives[indices]\n        true_positives = true_positives[indices]\n\n        # compute false positives and true positives\n        false_positives = np.cumsum(false_positives)\n        true_positives = np.cumsum(true_positives)\n\n        # compute recall and precision\n        recall = true_positives / num_annotations\n        precision = true_positives / \\\n            np.maximum(true_positives + false_positives,\n                       np.finfo(np.float64).eps)\n\n        # compute average precision\n        average_precision = _compute_ap(recall, precision)\n        average_precisions[label] = average_precision, num_annotations\n\n    print(\'\\nmAP:\')\n    avg_mAP = []\n    for label in range(generator.num_classes()):\n        label_name = generator.label_to_name(label)\n        print(\'{}: {}\'.format(label_name, average_precisions[label][0]))\n        avg_mAP.append(average_precisions[label][0])\n    print(\'avg mAP: {}\'.format(np.mean(avg_mAP)))\n    return np.mean(avg_mAP), average_precisions\n\n\ndef evaluate_coco(dataset, model, threshold=0.05):\n\n    model.eval()\n\n    with torch.no_grad():\n\n        # start collecting results\n        results = []\n        image_ids = []\n\n        for index in range(len(dataset)):\n            data = dataset[index]\n            scale = data[\'scale\']\n\n            # run network\n            scores, labels, boxes = model(data[\'img\'].permute(\n                2, 0, 1).cuda().float().unsqueeze(dim=0))\n            scores = scores.cpu()\n            labels = labels.cpu()\n            boxes = boxes.cpu()\n\n            # correct boxes for image scale\n            boxes /= scale\n\n            if boxes.shape[0] > 0:\n                # change to (x, y, w, h) (MS COCO standard)\n                boxes[:, 2] -= boxes[:, 0]\n                boxes[:, 3] -= boxes[:, 1]\n\n                # compute predicted labels and scores\n                # for box, score, label in zip(boxes[0], scores[0], labels[0]):\n                for box_id in range(boxes.shape[0]):\n                    score = float(scores[box_id])\n                    label = int(labels[box_id])\n                    box = boxes[box_id, :]\n\n                    # scores are sorted, so we can break\n                    if score < threshold:\n                        break\n\n                    # append detection for each positively labeled class\n                    image_result = {\n                        \'image_id\': dataset.image_ids[index],\n                        \'category_id\': dataset.label_to_coco_label(label),\n                        \'score\': float(score),\n                        \'bbox\': box.tolist(),\n                    }\n\n                    # append detection to results\n                    results.append(image_result)\n\n            # append image to list of processed images\n            image_ids.append(dataset.image_ids[index])\n\n            # print progress\n            print(\'{}/{}\'.format(index, len(dataset)), end=\'\\r\')\n\n        if not len(results):\n            return\n\n        # write output\n        json.dump(results, open(\'{}_bbox_results.json\'.format(\n            dataset.set_name), \'w\'), indent=4)\n\n        # load results in COCO evaluation tool\n        coco_true = dataset.coco\n        coco_pred = coco_true.loadRes(\n            \'{}_bbox_results.json\'.format(dataset.set_name))\n\n        # run COCO evaluation\n        coco_eval = COCOeval(coco_true, coco_pred, \'bbox\')\n        coco_eval.params.imgIds = image_ids\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n        model.train()\n\n        return\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'EfficientDet Training With Pytorch\')\n    train_set = parser.add_mutually_exclusive_group()\n    parser.add_argument(\'--dataset\', default=\'VOC\', choices=[\'VOC\', \'COCO\'],\n                        type=str, help=\'VOC or COCO\')\n    parser.add_argument(\'--dataset_root\', default=\'/root/data/VOCdevkit/\',\n                        help=\'Dataset root directory path [/root/data/VOCdevkit/, /root/data/coco/]\')\n    parser.add_argument(\'-t\', \'--threshold\', default=0.4,\n                        type=float, help=\'Visualization threshold\')\n    parser.add_argument(\'-it\', \'--iou_threshold\', default=0.5,\n                        type=float, help=\'Visualization threshold\')\n    parser.add_argument(\'--weight\', default=\'./checkpoint_VOC_efficientdet-d0_248.pth\', type=str,\n                        help=\'Checkpoint state_dict file to resume training from\')\n    args = parser.parse_args()\n\n    if(args.weight is not None):\n        resume_path = str(args.weight)\n        print(""Loading checkpoint: {} ..."".format(resume_path))\n        checkpoint = torch.load(\n            args.weight, map_location=lambda storage, loc: storage)\n        params = checkpoint[\'parser\']\n        args.num_class = params.num_class\n        args.network = params.network\n        model = EfficientDet(\n            num_classes=args.num_class,\n            network=args.network,\n            W_bifpn=EFFICIENTDET[args.network][\'W_bifpn\'],\n            D_bifpn=EFFICIENTDET[args.network][\'D_bifpn\'],\n            D_class=EFFICIENTDET[args.network][\'D_class\'],\n            is_training=False,\n            threshold=args.threshold,\n            iou_threshold=args.iou_threshold)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n    model = model.cuda()\n    if(args.dataset == \'VOC\'):\n        valid_dataset = VOCDetection(root=args.dataset_root, image_sets=[(\'2007\', \'test\')],\n                                     transform=transforms.Compose([Normalizer(), Resizer()]))\n        evaluate(valid_dataset, model)\n    else:\n        valid_dataset = CocoDataset(root_dir=args.dataset_root, set_name=\'val2017\',\n                                    transform=transforms.Compose([Normalizer(), Resizer()]))\n        evaluate_coco(valid_dataset, model)\n'"
test.py,2,"b""import torch\nfrom models import EfficientDet\nfrom models.efficientnet import EfficientNet\n\nif __name__ == '__main__':\n\n    inputs = torch.randn(5, 3, 512, 512)\n\n    # Test EfficientNet\n    model = EfficientNet.from_pretrained('efficientnet-b0')\n    inputs = torch.randn(4, 3, 512, 512)\n    P = model(inputs)\n    for idx, p in enumerate(P):\n        print('P{}: {}'.format(idx, p.size()))\n\n    # print('model: ', model)\n\n    # Test inference\n    model = EfficientDet(num_classes=20, is_training=False)\n    output = model(inputs)\n    for out in output:\n        print(out.size())\n"""
train.py,24,"b'from tqdm import tqdm\nimport argparse\nimport os\nimport random\nimport shutil\nimport time\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.optim\nimport torch.multiprocessing as mp\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nimport os\nimport sys\nimport time\nimport argparse\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader\n\nfrom models.efficientdet import EfficientDet\nfrom models.losses import FocalLoss\nfrom datasets import VOCDetection, CocoDataset, get_augumentation, detection_collate, Resizer, Normalizer, Augmenter, collater\nfrom utils import EFFICIENTDET, get_state_dict\nfrom eval import evaluate, evaluate_coco\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\nparser.add_argument(\'--dataset\', default=\'VOC\', choices=[\'VOC\', \'COCO\'],\n                    type=str, help=\'VOC or COCO\')\nparser.add_argument(\n    \'--dataset_root\',\n    default=\'/root/data/VOCdevkit/\',\n    help=\'Dataset root directory path [/root/data/VOCdevkit/, /root/data/coco/]\')\nparser.add_argument(\'--network\', default=\'efficientdet-d0\', type=str,\n                    help=\'efficientdet-[d0, d1, ..]\')\n\nparser.add_argument(\'--resume\', default=None, type=str,\n                    help=\'Checkpoint state_dict file to resume training from\')\nparser.add_argument(\'--num_epoch\', default=500, type=int,\n                    help=\'Num epoch for training\')\nparser.add_argument(\'--batch_size\', default=32, type=int,\n                    help=\'Batch size for training\')\nparser.add_argument(\'--num_class\', default=20, type=int,\n                    help=\'Number of class used in model\')\nparser.add_argument(\'--device\', default=[0, 1], type=list,\n                    help=\'Use CUDA to train model\')\nparser.add_argument(\'--grad_accumulation_steps\', default=1, type=int,\n                    help=\'Number of gradient accumulation steps\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=1e-4, type=float,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float,\n                    help=\'Momentum value for optim\')\nparser.add_argument(\'--weight_decay\', default=5e-4, type=float,\n                    help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1, type=float,\n                    help=\'Gamma update for SGD\')\nparser.add_argument(\'--save_folder\', default=\'./saved/weights/\', type=str,\n                    help=\'Directory for saving checkpoint models\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--start_epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--world-size\', default=1, type=int,\n                    help=\'number of nodes for distributed training\')\nparser.add_argument(\'--rank\', default=0, type=int,\n                    help=\'node rank for distributed training\')\nparser.add_argument(\'--dist-url\', default=\'env://\', type=str,\n                    help=\'url used to set up distributed training\')\nparser.add_argument(\'--dist-backend\', default=\'nccl\', type=str,\n                    help=\'distributed backend\')\nparser.add_argument(\'--seed\', default=24, type=int,\n                    help=\'seed for initializing training. \')\nparser.add_argument(\'--gpu\', default=None, type=int,\n                    help=\'GPU id to use.\')\nparser.add_argument(\n    \'--multiprocessing-distributed\',\n    action=\'store_true\',\n    help=\'Use multi-processing distributed training to launch \'\n    \'N processes per node, which has N GPUs. This is the \'\n    \'fastest way to use PyTorch for either single node or \'\n    \'multi node data parallel training\')\n\niteration = 1\n\n\ndef train(train_loader, model, scheduler, optimizer, epoch, args):\n    global iteration\n    print(""{} epoch: \\t start training...."".format(epoch))\n    start = time.time()\n    total_loss = []\n    model.train()\n    model.module.is_training = True\n    model.module.freeze_bn()\n    optimizer.zero_grad()\n    for idx, (images, annotations) in enumerate(train_loader):\n        images = images.cuda().float()\n        annotations = annotations.cuda()\n        classification_loss, regression_loss = model([images, annotations])\n        classification_loss = classification_loss.mean()\n        regression_loss = regression_loss.mean()\n        loss = classification_loss + regression_loss\n        if bool(loss == 0):\n            print(\'loss equal zero(0)\')\n            continue\n        loss.backward()\n        if (idx + 1) % args.grad_accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n            optimizer.step()\n            optimizer.zero_grad()\n\n        total_loss.append(loss.item())\n        if(iteration % 300 == 0):\n            print(\'{} iteration: training ...\'.format(iteration))\n            ans = {\n                \'epoch\': epoch,\n                \'iteration\': iteration,\n                \'cls_loss\': classification_loss.item(),\n                \'reg_loss\': regression_loss.item(),\n                \'mean_loss\': np.mean(total_loss)\n            }\n            for key, value in ans.items():\n                print(\'    {:15s}: {}\'.format(str(key), value))\n        iteration += 1\n    scheduler.step(np.mean(total_loss))\n    result = {\n        \'time\': time.time() - start,\n        \'loss\': np.mean(total_loss)\n    }\n    for key, value in result.items():\n        print(\'    {:15s}: {}\'.format(str(key), value))\n\n\ndef test(dataset, model, epoch, args):\n    print(""{} epoch: \\t start validation...."".format(epoch))\n    model = model.module\n    model.eval()\n    model.is_training = False\n    with torch.no_grad():\n        if(args.dataset == \'VOC\'):\n            evaluate(dataset, model)\n        else:\n            evaluate_coco(dataset, model)\n\n\ndef main_worker(gpu, ngpus_per_node, args):\n    args.gpu = gpu\n    if args.gpu is not None:\n        print(""Use GPU: {} for training"".format(args.gpu))\n\n    if args.distributed:\n        if args.dist_url == ""env://"" and args.rank == -1:\n            # args.rank = int(os.environ[""RANK""])\n            args.rank = 1\n        if args.multiprocessing_distributed:\n            # For multiprocessing distributed training, rank needs to be the\n            # global rank among all the processes\n            args.rank = args.rank * ngpus_per_node + gpu\n        dist.init_process_group(\n            backend=args.dist_backend,\n            init_method=args.dist_url,\n            world_size=args.world_size,\n            rank=args.rank)\n\n    # Training dataset\n    train_dataset = []\n    if(args.dataset == \'VOC\'):\n        train_dataset = VOCDetection(root=args.dataset_root, transform=transforms.Compose(\n            [Normalizer(), Augmenter(), Resizer()]))\n        valid_dataset = VOCDetection(root=args.dataset_root, image_sets=[(\n            \'2007\', \'test\')], transform=transforms.Compose([Normalizer(), Resizer()]))\n        args.num_class = train_dataset.num_classes()\n    elif(args.dataset == \'COCO\'):\n        train_dataset = CocoDataset(\n            root_dir=args.dataset_root,\n            set_name=\'train2017\',\n            transform=transforms.Compose(\n                [\n                    Normalizer(),\n                    Augmenter(),\n                    Resizer()]))\n        valid_dataset = CocoDataset(\n            root_dir=args.dataset_root,\n            set_name=\'val2017\',\n            transform=transforms.Compose(\n                [\n                    Normalizer(),\n                    Resizer()]))\n        args.num_class = train_dataset.num_classes()\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=args.batch_size,\n                              num_workers=args.workers,\n                              shuffle=True,\n                              collate_fn=collater,\n                              pin_memory=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=1,\n                              num_workers=args.workers,\n                              shuffle=False,\n                              collate_fn=collater,\n                              pin_memory=True)\n\n    checkpoint = []\n    if(args.resume is not None):\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            if args.gpu is None:\n                checkpoint = torch.load(args.resume)\n            else:\n                # Map model to be loaded to specified single gpu.\n                loc = \'cuda:{}\'.format(args.gpu)\n                checkpoint = torch.load(args.resume, map_location=loc)\n        params = checkpoint[\'parser\']\n        args.num_class = params.num_class\n        args.network = params.network\n        args.start_epoch = checkpoint[\'epoch\'] + 1\n        del params\n\n    model = EfficientDet(num_classes=args.num_class,\n                         network=args.network,\n                         W_bifpn=EFFICIENTDET[args.network][\'W_bifpn\'],\n                         D_bifpn=EFFICIENTDET[args.network][\'D_bifpn\'],\n                         D_class=EFFICIENTDET[args.network][\'D_class\']\n                         )\n    if(args.resume is not None):\n        model.load_state_dict(checkpoint[\'state_dict\'])\n    del checkpoint\n    if args.distributed:\n        # For multiprocessing distributed, DistributedDataParallel constructor\n        # should always set the single device scope, otherwise,\n        # DistributedDataParallel will use all available devices.\n        if args.gpu is not None:\n            torch.cuda.set_device(args.gpu)\n            model.cuda(args.gpu)\n            # When using a single GPU per process and per\n            # DistributedDataParallel, we need to divide the batch size\n            # ourselves based on the total number of GPUs we have\n            args.batch_size = int(args.batch_size / ngpus_per_node)\n            args.workers = int(\n                (args.workers + ngpus_per_node - 1) / ngpus_per_node)\n            model = torch.nn.parallel.DistributedDataParallel(\n                model, device_ids=[args.gpu], find_unused_parameters=True)\n            print(\'Run with DistributedDataParallel with divice_ids....\')\n        else:\n            model.cuda()\n            # DistributedDataParallel will divide and allocate batch_size to all\n            # available GPUs if device_ids are not set\n            model = torch.nn.parallel.DistributedDataParallel(model)\n            print(\'Run with DistributedDataParallel without device_ids....\')\n    elif args.gpu is not None:\n        torch.cuda.set_device(args.gpu)\n        model = model.cuda(args.gpu)\n    else:\n        model = model.cuda()\n        print(\'Run with DataParallel ....\')\n        model = torch.nn.DataParallel(model).cuda()\n\n    # define loss function (criterion) , optimizer, scheduler\n    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, patience=3, verbose=True)\n    cudnn.benchmark = True\n\n    for epoch in range(args.start_epoch, args.num_epoch):\n        train(train_loader, model, scheduler, optimizer, epoch, args)\n\n        if (epoch + 1) % 5 == 0:\n            test(valid_dataset, model, epoch, args)\n\n        state = {\n            \'epoch\': epoch,\n            \'parser\': args,\n            \'state_dict\': get_state_dict(model)\n        }\n\n        torch.save(\n            state,\n            os.path.join(\n                args.save_folder,\n                args.dataset,\n                args.network,\n                ""checkpoint_{}.pth"".format(epoch)))\n\n\ndef main():\n    args = parser.parse_args()\n    if(not os.path.exists(os.path.join(args.save_folder, args.dataset, args.network))):\n        os.makedirs(os.path.join(args.save_folder, args.dataset, args.network))\n    if args.seed is not None:\n        random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        cudnn.deterministic = True\n        warnings.warn(\'You have chosen to seed training. \'\n                      \'This will turn on the CUDNN deterministic setting, \'\n                      \'which can slow down your training considerably! \'\n                      \'You may see unexpected behavior when restarting \'\n                      \'from checkpoints.\')\n\n    if args.gpu is not None:\n        warnings.warn(\'You have chosen a specific GPU. This will completely \'\n                      \'disable data parallelism.\')\n    os.environ[\'MASTER_ADDR\'] = \'localhost\'\n    os.environ[\'MASTER_PORT\'] = \'12355\'\n    os.environ[\'WORLD_SIZE\'] = \'2\'\n    if args.dist_url == ""env://"" and args.world_size == -1:\n        args.world_size = int(os.environ[""WORLD_SIZE""])\n\n    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n    ngpus_per_node = torch.cuda.device_count()\n    if args.multiprocessing_distributed:\n        # Since we have ngpus_per_node processes per node, the total world_size\n        # needs to be adjusted accordingly\n        args.world_size = ngpus_per_node * args.world_size\n        # Use torch.multiprocessing.spawn to launch distributed processes: the\n        # main_worker process function\n        mp.spawn(main_worker, nprocs=ngpus_per_node,\n                 args=(ngpus_per_node, args))\n    else:\n        # Simply call main_worker function\n        main_worker(args.gpu, ngpus_per_node, args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
datasets/__init__.py,0,"b'from .voc0712 import VOCDetection, VOC_CLASSES\nfrom .augmentation import get_augumentation, detection_collate, Resizer, Normalizer, Augmenter, collater\nfrom .coco import CocoDataset'"
datasets/augmentation.py,7,"b'import albumentations as albu\nfrom albumentations.pytorch.transforms import ToTensor\nimport torch\nimport numpy as np\nimport cv2\n\n\ndef get_augumentation(phase, width=512, height=512, min_area=0., min_visibility=0.):\n    list_transforms = []\n    if phase == \'train\':\n        list_transforms.extend([\n            albu.augmentations.transforms.LongestMaxSize(\n                max_size=width, always_apply=True),\n            albu.PadIfNeeded(min_height=height, min_width=width,\n                             always_apply=True, border_mode=0, value=[0, 0, 0]),\n            albu.augmentations.transforms.RandomResizedCrop(\n                height=height,\n                width=width, p=0.3),\n            albu.augmentations.transforms.Flip(),\n            albu.augmentations.transforms.Transpose(),\n            albu.OneOf([\n                albu.RandomBrightnessContrast(brightness_limit=0.5,\n                                              contrast_limit=0.4),\n                albu.RandomGamma(gamma_limit=(50, 150)),\n                albu.NoOp()\n            ]),\n            albu.OneOf([\n                albu.RGBShift(r_shift_limit=20, b_shift_limit=15,\n                              g_shift_limit=15),\n                albu.HueSaturationValue(hue_shift_limit=5,\n                                        sat_shift_limit=5),\n                albu.NoOp()\n            ]),\n            albu.CLAHE(p=0.8),\n            albu.HorizontalFlip(p=0.5),\n            albu.VerticalFlip(p=0.5),\n        ])\n    if(phase == \'test\' or phase == \'valid\'):\n        list_transforms.extend([\n            albu.Resize(height=height, width=width)\n        ])\n    list_transforms.extend([\n        albu.Normalize(mean=(0.485, 0.456, 0.406),\n                       std=(0.229, 0.224, 0.225), p=1),\n        ToTensor()\n    ])\n    if(phase == \'test\'):\n        return albu.Compose(list_transforms)\n    return albu.Compose(list_transforms, bbox_params=albu.BboxParams(format=\'pascal_voc\', min_area=min_area,\n                                                                     min_visibility=min_visibility, label_fields=[\'category_id\']))\n\n\ndef detection_collate(batch):\n    imgs = [s[\'image\'] for s in batch]\n    annots = [s[\'bboxes\'] for s in batch]\n    labels = [s[\'category_id\'] for s in batch]\n\n    max_num_annots = max(len(annot) for annot in annots)\n    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n\n    if max_num_annots > 0:\n        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n            if len(annot) > 0:\n                annot_padded[idx, :len(annot), :4] = annot\n                annot_padded[idx, :len(annot), 4] = lab\n    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n\n\ndef collater(data):\n    imgs = [s[\'img\'] for s in data]\n    annots = [s[\'annot\'] for s in data]\n    scales = [s[\'scale\'] for s in data]\n\n    imgs = torch.from_numpy(np.stack(imgs, axis=0))\n\n    max_num_annots = max(annot.shape[0] for annot in annots)\n\n    if max_num_annots > 0:\n\n        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n\n        if max_num_annots > 0:\n            for idx, annot in enumerate(annots):\n                if annot.shape[0] > 0:\n                    annot_padded[idx, :annot.shape[0], :] = annot\n    else:\n        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n\n    imgs = imgs.permute(0, 3, 1, 2)\n\n    return (imgs, torch.FloatTensor(annot_padded))\n\n\nclass Resizer(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample, common_size=512):\n        image, annots = sample[\'img\'], sample[\'annot\']\n        height, width, _ = image.shape\n        if height > width:\n            scale = common_size / height\n            resized_height = common_size\n            resized_width = int(width * scale)\n        else:\n            scale = common_size / width\n            resized_height = int(height * scale)\n            resized_width = common_size\n\n        image = cv2.resize(image, (resized_width, resized_height))\n\n        new_image = np.zeros((common_size, common_size, 3))\n        new_image[0:resized_height, 0:resized_width] = image\n        annots[:, :4] *= scale\n\n        return {\'img\': torch.from_numpy(new_image), \'annot\': torch.from_numpy(annots), \'scale\': scale}\n\n\nclass Augmenter(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample, flip_x=0.5):\n        if np.random.rand() < flip_x:\n            image, annots = sample[\'img\'], sample[\'annot\']\n            image = image[:, ::-1, :]\n\n            rows, cols, channels = image.shape\n\n            x1 = annots[:, 0].copy()\n            x2 = annots[:, 2].copy()\n\n            x_tmp = x1.copy()\n\n            annots[:, 0] = cols - x2\n            annots[:, 2] = cols - x_tmp\n\n            sample = {\'img\': image, \'annot\': annots}\n\n        return sample\n\n\nclass Normalizer(object):\n\n    def __init__(self):\n        self.mean = np.array([[[0.485, 0.456, 0.406]]])\n        self.std = np.array([[[0.229, 0.224, 0.225]]])\n\n    def __call__(self, sample):\n        image, annots = sample[\'img\'], sample[\'annot\']\n\n        return {\'img\': ((image.astype(np.float32) - self.mean) / self.std), \'annot\': annots}\n'"
datasets/coco.py,2,"b'from __future__ import print_function, division\nimport sys\nimport os\nimport torch\nimport numpy as np\nimport random\nimport csv\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom torch.utils.data.sampler import Sampler\n\nfrom pycocotools.coco import COCO\n\nimport skimage.io\nimport skimage.transform\nimport skimage.color\nimport skimage\nimport cv2\nfrom PIL import Image\n\n\nclass CocoDataset(Dataset):\n    """"""Coco dataset.""""""\n\n    def __init__(self, root_dir, set_name=\'train2017\', transform=None):\n        """"""\n        Args:\n            root_dir (string): COCO directory.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n        self.root_dir = root_dir\n        self.set_name = set_name\n        self.transform = transform\n\n        self.coco = COCO(os.path.join(self.root_dir, \'annotations\',\n                                      \'instances_\' + self.set_name + \'.json\'))\n        self.image_ids = self.coco.getImgIds()\n\n        self.load_classes()\n\n    def load_classes(self):\n        # load class names (name -> label)\n        categories = self.coco.loadCats(self.coco.getCatIds())\n        categories.sort(key=lambda x: x[\'id\'])\n\n        self.classes = {}\n        self.coco_labels = {}\n        self.coco_labels_inverse = {}\n        for c in categories:\n            self.coco_labels[len(self.classes)] = c[\'id\']\n            self.coco_labels_inverse[c[\'id\']] = len(self.classes)\n            self.classes[c[\'name\']] = len(self.classes)\n\n        # also load the reverse (label -> name)\n        self.labels = {}\n        for key, value in self.classes.items():\n            self.labels[value] = key\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n\n        img = self.load_image(idx)\n        annot = self.load_annotations(idx)\n        sample = {\'img\': img, \'annot\': annot}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n    def load_image(self, image_index):\n        image_info = self.coco.loadImgs(self.image_ids[image_index])[0]\n        path = os.path.join(self.root_dir, \'images\',\n                            self.set_name, image_info[\'file_name\'])\n        img = cv2.imread(path)\n\n        if len(img.shape) == 2:\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        return img\n\n    def load_annotations(self, image_index):\n        # get ground truth annotations\n        annotations_ids = self.coco.getAnnIds(\n            imgIds=self.image_ids[image_index], iscrowd=False)\n        annotations = np.zeros((0, 5))\n\n        # some images appear to miss annotations (like image with id 257034)\n        if len(annotations_ids) == 0:\n            return annotations\n\n        # parse annotations\n        coco_annotations = self.coco.loadAnns(annotations_ids)\n        for idx, a in enumerate(coco_annotations):\n\n            # some annotations have basically no width / height, skip them\n            if a[\'bbox\'][2] < 1 or a[\'bbox\'][3] < 1:\n                continue\n\n            annotation = np.zeros((1, 5))\n            annotation[0, :4] = a[\'bbox\']\n            annotation[0, 4] = self.coco_label_to_label(a[\'category_id\'])\n            annotations = np.append(annotations, annotation, axis=0)\n\n        # transform from [x, y, w, h] to [x1, y1, x2, y2]\n        annotations[:, 2] = annotations[:, 0] + annotations[:, 2]\n        annotations[:, 3] = annotations[:, 1] + annotations[:, 3]\n\n        return annotations\n\n    def coco_label_to_label(self, coco_label):\n        return self.coco_labels_inverse[coco_label]\n\n    def label_to_coco_label(self, label):\n        return self.coco_labels[label]\n\n    def image_aspect_ratio(self, image_index):\n        image = self.coco.loadImgs(self.image_ids[image_index])[0]\n        return float(image[\'width\']) / float(image[\'height\'])\n\n    def num_classes(self):\n        return 80\n\n\nif __name__ == \'__main__\':\n    from augmentation import get_augumentation\n    dataset = CocoDataset(root_dir=\'/root/data/coco\', set_name=\'trainval35k\',\n                          transform=get_augumentation(phase=\'train\'))\n    sample = dataset[0]\n    print(\'sample: \', sample)\n'"
datasets/visual_aug.py,0,"b'import cv2\nfrom augmentation import get_augumentation\nfrom voc0712 import VOCDetection\nimport matplotlib.pyplot as plt\nEFFICIENTDET = {\n    \'efficientdet-d0\': {\'input_size\': 512,\n                        \'backbone\': \'B0\',\n                        \'W_bifpn\': 64,\n                        \'D_bifpn\': 2,\n                        \'D_class\': 3},\n    \'efficientdet-d1\': {\'input_size\': 640,\n                        \'backbone\': \'B1\',\n                        \'W_bifpn\': 88,\n                        \'D_bifpn\': 3,\n                        \'D_class\': 3},\n    \'efficientdet-d2\': {\'input_size\': 768,\n                        \'backbone\': \'B2\',\n                        \'W_bifpn\': 112,\n                        \'D_bifpn\': 4,\n                        \'D_class\': 3},\n}\n\n\n# Functions to visualize bounding boxes and class labels on an image.\n# Based on https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/vis.py\n\nBOX_COLOR = (255, 0, 0)\nTEXT_COLOR = (255, 255, 255)\n\n\ndef visualize_bbox(img, bbox, class_id, class_idx_to_name, color=BOX_COLOR, thickness=2):\n    x_min, y_min, x_max, y_max = bbox\n    x_min, x_max, y_min, y_max = int(x_min), int(x_max), int(y_min), int(y_max)\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max),\n                  color=color, thickness=thickness)\n    # class_name = class_idx_to_name[class_id]\n    # ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n    # cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n    # cv2.putText(img, class_name, (x_min, y_min - int(0.3 * text_height)), cv2.FONT_HERSHEY_SIMPLEX, 0.35,TEXT_COLOR, lineType=cv2.LINE_AA)\n    return img\n\n\ndef visualize(annotations, category_id_to_name):\n    img = annotations[\'image\'].copy()\n    for idx, bbox in enumerate(annotations[\'bboxes\']):\n        img = visualize_bbox(\n            img, bbox, annotations[\'category_id\'][idx], category_id_to_name)\n    # plt.figure(figsize=(12, 12))\n    # plt.imshow(img)\n    return img\n\n\ndataset_root = \'/root/data/VOCdevkit\'\nnetwork = \'efficientdet-d0\'\ndataset = VOCDetection(root=dataset_root,\n                       transform=get_augumentation(phase=\'train\', width=EFFICIENTDET[network][\'input_size\'], height=EFFICIENTDET[network][\'input_size\']))\n\n\ndef visual_data(data, name):\n    img = data[\'image\']\n    bboxes = data[\'bboxes\']\n    annotations = {\'image\': data[\'image\'], \'bboxes\': data[\'bboxes\'], \'category_id\': range(\n        len(data[\'bboxes\']))}\n    category_id_to_name = {v: v for v in range(len(data[\'bboxes\']))}\n\n    img = visualize(annotations, category_id_to_name)\n    cv2.imwrite(name, img)\n\n\nfor i in range(20, 25):\n    visual_data(dataset[i], ""name""+str(i)+"".png"")\n'"
datasets/voc0712.py,1,"b'import os.path as osp\nimport sys\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport numpy as np\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\nVOC_CLASSES = (  # always index 0\n    \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n    \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n    \'cow\', \'diningtable\', \'dog\', \'horse\',\n    \'motorbike\', \'person\', \'pottedplant\',\n    \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\n# note: if you used our download scripts, this should be right\nVOC_ROOT = osp.join(\'/home/toandm2\', ""data/VOCdevkit/"")\n\n\nclass VOCAnnotationTransform(object):\n    """"""Transforms a VOC annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n    Arguments:\n        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n            (default: alphabetic indexing of VOC\'s 20 classes)\n        keep_difficult (bool, optional): keep difficult instances or not\n            (default: False)\n        height (int): height\n        width (int): width\n    """"""\n\n    def __init__(self, class_to_ind=None, keep_difficult=False):\n        self.class_to_ind = class_to_ind or dict(\n            zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n        self.keep_difficult = keep_difficult\n\n    def __call__(self, target, width, height):\n        """"""\n        Arguments:\n            target (annotation) : the target annotation to be made usable\n                will be an ET.Element\n        Returns:\n            a list containing lists of bounding boxes  [bbox coords, class name]\n        """"""\n        res = []\n        for obj in target.iter(\'object\'):\n            difficult = int(obj.find(\'difficult\').text) == 1\n            if not self.keep_difficult and difficult:\n                continue\n            name = obj.find(\'name\').text.lower().strip()\n            bbox = obj.find(\'bndbox\')\n\n            pts = [\'xmin\', \'ymin\', \'xmax\', \'ymax\']\n            bndbox = []\n            for i, pt in enumerate(pts):\n                cur_pt = float(bbox.find(pt).text) - 1\n                # scale height or width\n                # cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n                bndbox.append(cur_pt)\n            label_idx = self.class_to_ind[name]\n            bndbox.append(label_idx)\n            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]\n            # img_id = target.find(\'filename\').text[:-4]\n\n        return res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n\n\nclass VOCDetection(data.Dataset):\n    """"""VOC Detection Dataset Object\n    input is image, target is annotation\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n        image_set (string): imageset to use (eg. \'train\', \'val\', \'test\')\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root,\n                 image_sets=[(\'2007\', \'trainval\'), (\'2012\', \'trainval\')],\n                 transform=None, target_transform=VOCAnnotationTransform(),\n                 dataset_name=\'VOC0712\'):\n        self.root = root\n        self.image_set = image_sets\n        self.transform = transform\n        self.target_transform = target_transform\n        self.name = dataset_name\n        self._annopath = osp.join(\'%s\', \'Annotations\', \'%s.xml\')\n        self._imgpath = osp.join(\'%s\', \'JPEGImages\', \'%s.jpg\')\n        self.ids = list()\n        for (year, name) in image_sets:\n            rootpath = osp.join(self.root, \'VOC\' + year)\n            for line in open(osp.join(rootpath, \'ImageSets\', \'Main\', name + \'.txt\')):\n                self.ids.append((rootpath, line.strip()))\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n\n        target = ET.parse(self._annopath % img_id).getroot()\n        img = cv2.imread(self._imgpath % img_id)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = img.astype(np.float32)/255.\n        height, width, channels = img.shape\n\n        if self.target_transform is not None:\n            target = self.target_transform(target, width, height)\n        target = np.array(target)\n        sample = {\'img\': img, \'annot\': target}\n        if self.transform is not None:\n            sample = self.transform(sample)\n        return sample\n\n        bbox = target[:, :4]\n        labels = target[:, 4]\n\n        if self.transform is not None:\n            annotation = {\'image\': img, \'bboxes\': bbox, \'category_id\': labels}\n            augmentation = self.transform(**annotation)\n            img = augmentation[\'image\']\n            bbox = augmentation[\'bboxes\']\n            labels = augmentation[\'category_id\']\n        return {\'image\': img, \'bboxes\': bbox, \'category_id\': labels}\n\n    def __len__(self):\n        return len(self.ids)\n\n    def num_classes(self):\n        return len(VOC_CLASSES)\n\n    def label_to_name(self, label):\n        return VOC_CLASSES[label]\n\n    def load_annotations(self, index):\n        img_id = self.ids[index]\n        anno = ET.parse(self._annopath % img_id).getroot()\n        gt = self.target_transform(anno, 1, 1)\n        gt = np.array(gt)\n        return gt\n'"
models/__init__.py,0,b'from .efficientdet import EfficientDet'
models/bifpn.py,6,"b""import torch.nn as nn\nimport torch.nn.functional as F\n\n\nfrom .module import ConvModule, xavier_init\nimport torch\n\n\nclass BIFPN(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 stack=1,\n                 add_extra_convs=False,\n                 extra_convs_on_inputs=True,\n                 relu_before_extra_convs=False,\n                 no_norm_on_lateral=False,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 activation=None):\n        super(BIFPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.activation = activation\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.no_norm_on_lateral = no_norm_on_lateral\n        self.stack = stack\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        self.extra_convs_on_inputs = extra_convs_on_inputs\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        self.stack_bifpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n                activation=self.activation,\n                inplace=False)\n            self.lateral_convs.append(l_conv)\n\n        for ii in range(stack):\n            self.stack_bifpn_convs.append(BiFPNModule(channels=out_channels,\n                                                      levels=self.backbone_end_level-self.start_level,\n                                                      conv_cfg=conv_cfg,\n                                                      norm_cfg=norm_cfg,\n                                                      activation=activation))\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.extra_convs_on_inputs:\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    activation=self.activation,\n                    inplace=False)\n                self.fpn_convs.append(extra_fpn_conv)\n        self.init_weights()\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n    def forward(self, inputs):\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # part 1: build top-down and down-top path with stack\n        used_backbone_levels = len(laterals)\n        for bifpn_module in self.stack_bifpn_convs:\n            laterals = bifpn_module(laterals)\n        outs = laterals\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.extra_convs_on_inputs:\n                    orig = inputs[self.backbone_end_level - 1]\n                    outs.append(self.fpn_convs[0](orig))\n                else:\n                    outs.append(self.fpn_convs[0](outs[-1]))\n                for i in range(1, self.num_outs - used_backbone_levels):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)\n\n\nclass BiFPNModule(nn.Module):\n    def __init__(self,\n                 channels,\n                 levels,\n                 init=0.5,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 activation=None,\n                 eps=0.0001):\n        super(BiFPNModule, self).__init__()\n        self.activation = activation\n        self.eps = eps\n        self.levels = levels\n        self.bifpn_convs = nn.ModuleList()\n        # weighted\n        self.w1 = nn.Parameter(torch.Tensor(2, levels).fill_(init))\n        self.relu1 = nn.ReLU()\n        self.w2 = nn.Parameter(torch.Tensor(3, levels - 2).fill_(init))\n        self.relu2 = nn.ReLU()\n        for jj in range(2):\n            for i in range(self.levels-1):  # 1,2,3\n                fpn_conv = nn.Sequential(\n                    ConvModule(\n                        channels,\n                        channels,\n                        3,\n                        padding=1,\n                        conv_cfg=conv_cfg,\n                        norm_cfg=norm_cfg,\n                        activation=self.activation,\n                        inplace=False)\n                )\n                self.bifpn_convs.append(fpn_conv)\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n    def forward(self, inputs):\n        assert len(inputs) == self.levels\n        # build top-down and down-top path with stack\n        levels = self.levels\n        # w relu\n        w1 = self.relu1(self.w1)\n        w1 /= torch.sum(w1, dim=0) + self.eps  # normalize\n        w2 = self.relu2(self.w2)\n        w2 /= torch.sum(w2, dim=0) + self.eps  # normalize\n        # build top-down\n        idx_bifpn = 0\n        pathtd = inputs\n        inputs_clone = []\n        for in_tensor in inputs:\n            inputs_clone.append(in_tensor.clone())\n\n        for i in range(levels - 1, 0, -1):\n            pathtd[i - 1] = (w1[0, i-1]*pathtd[i - 1] + w1[1, i-1]*F.interpolate(\n                pathtd[i], scale_factor=2, mode='nearest'))/(w1[0, i-1] + w1[1, i-1] + self.eps)\n            pathtd[i - 1] = self.bifpn_convs[idx_bifpn](pathtd[i - 1])\n            idx_bifpn = idx_bifpn + 1\n        # build down-top\n        for i in range(0, levels - 2, 1):\n            pathtd[i + 1] = (w2[0, i] * pathtd[i + 1] + w2[1, i] * F.max_pool2d(pathtd[i], kernel_size=2) +\n                             w2[2, i] * inputs_clone[i + 1])/(w2[0, i] + w2[1, i] + w2[2, i] + self.eps)\n            pathtd[i + 1] = self.bifpn_convs[idx_bifpn](pathtd[i + 1])\n            idx_bifpn = idx_bifpn + 1\n\n        pathtd[levels - 1] = (w1[0, levels-1] * pathtd[levels - 1] + w1[1, levels-1] * F.max_pool2d(\n            pathtd[levels - 2], kernel_size=2))/(w1[0, levels-1] + w1[1, levels-1] + self.eps)\n        pathtd[levels - 1] = self.bifpn_convs[idx_bifpn](pathtd[levels - 1])\n        return pathtd\n"""
models/efficientdet.py,5,"b'import torch\nimport torch.nn as nn\nimport math\nfrom models.efficientnet import EfficientNet\nfrom models.bifpn import BIFPN\nfrom .retinahead import RetinaHead\nfrom models.module import RegressionModel, ClassificationModel, Anchors, ClipBoxes, BBoxTransform\nfrom torchvision.ops import nms\nfrom .losses import FocalLoss\nMODEL_MAP = {\n    \'efficientdet-d0\': \'efficientnet-b0\',\n    \'efficientdet-d1\': \'efficientnet-b1\',\n    \'efficientdet-d2\': \'efficientnet-b2\',\n    \'efficientdet-d3\': \'efficientnet-b3\',\n    \'efficientdet-d4\': \'efficientnet-b4\',\n    \'efficientdet-d5\': \'efficientnet-b5\',\n    \'efficientdet-d6\': \'efficientnet-b6\',\n    \'efficientdet-d7\': \'efficientnet-b6\',\n}\n\n\nclass EfficientDet(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 network=\'efficientdet-d0\',\n                 D_bifpn=3,\n                 W_bifpn=88,\n                 D_class=3,\n                 is_training=True,\n                 threshold=0.01,\n                 iou_threshold=0.5):\n        super(EfficientDet, self).__init__()\n        self.backbone = EfficientNet.from_pretrained(MODEL_MAP[network])\n        self.is_training = is_training\n        self.neck = BIFPN(in_channels=self.backbone.get_list_features()[-5:],\n                          out_channels=W_bifpn,\n                          stack=D_bifpn,\n                          num_outs=5)\n        self.bbox_head = RetinaHead(num_classes=num_classes,\n                                    in_channels=W_bifpn)\n\n        self.anchors = Anchors()\n        self.regressBoxes = BBoxTransform()\n        self.clipBoxes = ClipBoxes()\n        self.threshold = threshold\n        self.iou_threshold = iou_threshold\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        self.freeze_bn()\n        self.criterion = FocalLoss()\n\n    def forward(self, inputs):\n        if self.is_training:\n            inputs, annotations = inputs\n        else:\n            inputs = inputs\n        x = self.extract_feat(inputs)\n        outs = self.bbox_head(x)\n        classification = torch.cat([out for out in outs[0]], dim=1)\n        regression = torch.cat([out for out in outs[1]], dim=1)\n        anchors = self.anchors(inputs)\n        if self.is_training:\n            return self.criterion(classification, regression, anchors, annotations)\n        else:\n            transformed_anchors = self.regressBoxes(anchors, regression)\n            transformed_anchors = self.clipBoxes(transformed_anchors, inputs)\n            scores = torch.max(classification, dim=2, keepdim=True)[0]\n            scores_over_thresh = (scores > self.threshold)[0, :, 0]\n\n            if scores_over_thresh.sum() == 0:\n                print(\'No boxes to NMS\')\n                # no boxes to NMS, just return\n                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n            classification = classification[:, scores_over_thresh, :]\n            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n            scores = scores[:, scores_over_thresh, :]\n            anchors_nms_idx = nms(\n                transformed_anchors[0, :, :], scores[0, :, 0], iou_threshold=self.iou_threshold)\n            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(\n                dim=1)\n            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n\n    def freeze_bn(self):\n        \'\'\'Freeze BatchNorm layers.\'\'\'\n        for layer in self.modules():\n            if isinstance(layer, nn.BatchNorm2d):\n                layer.eval()\n\n    def extract_feat(self, img):\n        """"""\n            Directly extract features from the backbone+neck\n        """"""\n        x = self.backbone(img)\n        x = self.neck(x[-5:])\n        return x\n'"
models/efficientnet.py,3,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom models.utils import (\n    round_filters,\n    round_repeats,\n    drop_connect,\n    get_same_padding_conv2d,\n    get_model_params,\n    efficientnet_params,\n    load_pretrained_weights,\n    Swish,\n    MemoryEfficientSwish,\n)\n\n\nclass MBConvBlock(nn.Module):\n    """"""\n    Mobile Inverted Residual Bottleneck Block\n    Args:\n        block_args (namedtuple): BlockArgs, see above\n        global_params (namedtuple): GlobalParam, see above\n    Attributes:\n        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n    """"""\n\n    def __init__(self, block_args, global_params):\n        super().__init__()\n        self._block_args = block_args\n        self._bn_mom = 1 - global_params.batch_norm_momentum\n        self._bn_eps = global_params.batch_norm_epsilon\n        self.has_se = (self._block_args.se_ratio is not None) and (\n            0 < self._block_args.se_ratio <= 1)\n        self.id_skip = block_args.id_skip  # skip connection and drop connect\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Expansion phase\n        inp = self._block_args.input_filters  # number of input channels\n        oup = self._block_args.input_filters * \\\n            self._block_args.expand_ratio  # number of output channels\n        if self._block_args.expand_ratio != 1:\n            self._expand_conv = Conv2d(\n                in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self._bn0 = nn.BatchNorm2d(\n                num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n        # Depthwise convolution phase\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        self._depthwise_conv = Conv2d(\n            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n            kernel_size=k, stride=s, bias=False)\n        self._bn1 = nn.BatchNorm2d(\n            num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Squeeze and Excitation layer, if desired\n        if self.has_se:\n            num_squeezed_channels = max(\n                1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            self._se_reduce = Conv2d(\n                in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n            self._se_expand = Conv2d(\n                in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n        # Output phase\n        final_oup = self._block_args.output_filters\n        self._project_conv = Conv2d(\n            in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n        self._bn2 = nn.BatchNorm2d(\n            num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n        self._swish = MemoryEfficientSwish()\n\n    def forward(self, inputs, drop_connect_rate=None):\n        """"""\n        :param inputs: input tensor\n        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n        :return: output of block\n        """"""\n\n        # Expansion and Depthwise Convolution\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = self._swish(self._bn0(self._expand_conv(inputs)))\n\n        x = self._swish(self._bn1(self._depthwise_conv(x)))\n\n        # Squeeze and Excitation\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_expand(\n                self._swish(self._se_reduce(x_squeezed)))\n            x = torch.sigmoid(x_squeezed) * x\n\n        x = self._bn2(self._project_conv(x))\n\n        # Skip connection and drop connect\n        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate,\n                                 training=self.training)\n            x = x + inputs  # skip connection\n        return x\n\n    def set_swish(self, memory_efficient=True):\n        """"""Sets swish function as memory efficient (for training) or standard (for export)""""""\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n\n\nclass EfficientNet(nn.Module):\n    """"""\n    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n    Args:\n        blocks_args (list): A list of BlockArgs to construct blocks\n        global_params (namedtuple): A set of GlobalParams shared between blocks\n    Example:\n        model = EfficientNet.from_pretrained(\'efficientnet-b0\')\n    """"""\n\n    def __init__(self, blocks_args=None, global_params=None):\n        super().__init__()\n        assert isinstance(blocks_args, list), \'blocks_args should be a list\'\n        assert len(blocks_args) > 0, \'block args must be greater than 0\'\n        self._global_params = global_params\n        self._blocks_args = blocks_args\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Batch norm parameters\n        bn_mom = 1 - self._global_params.batch_norm_momentum\n        bn_eps = self._global_params.batch_norm_epsilon\n\n        # Stem\n        in_channels = 3  # rgb\n        # number of output channels\n        out_channels = round_filters(32, self._global_params)\n        self._conv_stem = Conv2d(\n            in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        self._bn0 = nn.BatchNorm2d(\n            num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Build blocks\n        self._blocks = nn.ModuleList([])\n        for i in range(len(self._blocks_args)):\n            # Update block input and output filters based on depth multiplier.\n            self._blocks_args[i] = self._blocks_args[i]._replace(\n                input_filters=round_filters(\n                    self._blocks_args[i].input_filters, self._global_params),\n                output_filters=round_filters(\n                    self._blocks_args[i].output_filters, self._global_params),\n                num_repeat=round_repeats(\n                    self._blocks_args[i].num_repeat, self._global_params)\n            )\n\n            # The first block needs to take care of stride and filter size increase.\n            self._blocks.append(MBConvBlock(\n                self._blocks_args[i], self._global_params))\n            if self._blocks_args[i].num_repeat > 1:\n                self._blocks_args[i] = self._blocks_args[i]._replace(\n                    input_filters=self._blocks_args[i].output_filters, stride=1)\n            for _ in range(self._blocks_args[i].num_repeat - 1):\n                self._blocks.append(MBConvBlock(\n                    self._blocks_args[i], self._global_params))\n\n        # Head\'efficientdet-d0\': \'efficientnet-b0\',\n        # output of final block\n        in_channels = self._blocks_args[len(\n            self._blocks_args)-1].output_filters\n        out_channels = round_filters(1280, self._global_params)\n        self._conv_head = Conv2d(\n            in_channels, out_channels, kernel_size=1, bias=False)\n        self._bn1 = nn.BatchNorm2d(\n            num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Final linear layer\n        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n        self._dropout = nn.Dropout(self._global_params.dropout_rate)\n        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n        self._swish = MemoryEfficientSwish()\n\n    def set_swish(self, memory_efficient=True):\n        """"""Sets swish function as memory efficient (for training) or standard (for export)""""""\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n        for block in self._blocks:\n            block.set_swish(memory_efficient)\n\n    def extract_features(self, inputs):\n        """""" Returns output of the final convolution layer """"""\n        # Stem\n        x = self._swish(self._bn0(self._conv_stem(inputs)))\n\n        P = []\n        index = 0\n        num_repeat = 0\n        # Blocks\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self._blocks)\n            x = block(x, drop_connect_rate=drop_connect_rate)\n            num_repeat = num_repeat + 1\n            if(num_repeat == self._blocks_args[index].num_repeat):\n                num_repeat = 0\n                index = index + 1\n                P.append(x)\n        return P\n\n    def forward(self, inputs):\n        """""" Calls extract_features to extract features, applies final linear layer, and returns logits. """"""\n        # Convolution layers\n        P = self.extract_features(inputs)\n        return P\n\n    @classmethod\n    def from_name(cls, model_name, override_params=None):\n        cls._check_model_name_is_valid(model_name)\n        blocks_args, global_params = get_model_params(\n            model_name, override_params)\n        return cls(blocks_args, global_params)\n\n    @classmethod\n    def from_pretrained(cls, model_name, num_classes=1000, in_channels=3):\n        model = cls.from_name(model_name, override_params={\n                              \'num_classes\': num_classes})\n        load_pretrained_weights(\n            model, model_name, load_fc=(num_classes == 1000))\n        if in_channels != 3:\n            Conv2d = get_same_padding_conv2d(\n                image_size=model._global_params.image_size)\n            out_channels = round_filters(32, model._global_params)\n            model._conv_stem = Conv2d(\n                in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        return model\n\n    @classmethod\n    def from_pretrained(cls, model_name, num_classes=1000):\n        model = cls.from_name(model_name, override_params={\n                              \'num_classes\': num_classes})\n        load_pretrained_weights(\n            model, model_name, load_fc=(num_classes == 1000))\n\n        return model\n\n    @classmethod\n    def get_image_size(cls, model_name):\n        cls._check_model_name_is_valid(model_name)\n        _, _, res, _ = efficientnet_params(model_name)\n        return res\n\n    @classmethod\n    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n        """""" Validates model name. None that pretrained weights are only available for\n        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. """"""\n        num_models = 4 if also_need_pretrained_weights else 8\n        valid_models = [\'efficientnet-b\'+str(i) for i in range(num_models)]\n        if model_name not in valid_models:\n            raise ValueError(\'model_name should be one of: \' +\n                             \', \'.join(valid_models))\n\n    def get_list_features(self):\n        list_feature = []\n        for idx in range(len(self._blocks_args)):\n            list_feature.append(self._blocks_args[idx].output_filters)\n\n        return list_feature\n\n\nif __name__ == \'__main__\':\n    model = EfficientNet.from_pretrained(\'efficientnet-b0\')\n    inputs = torch.randn(4, 3, 640, 640)\n    P = model(inputs)\n    for idx, p in enumerate(P):\n        print(\'P{}: {}\'.format(idx, p.size()))\n    # print(\'model: \', model)\n'"
models/losses.py,40,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\n\ndef calc_iou(a, b):\n    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n\n    iw = torch.min(torch.unsqueeze(\n        a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n    ih = torch.min(torch.unsqueeze(\n        a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n\n    iw = torch.clamp(iw, min=0)\n    ih = torch.clamp(ih, min=0)\n\n    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) *\n                         (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n\n    ua = torch.clamp(ua, min=1e-8)\n\n    intersection = iw * ih\n\n    IoU = intersection / ua\n\n    return IoU\n\n\nclass FocalLoss(nn.Module):\n    # def __init__(self):\n\n    def forward(self, classifications, regressions, anchors, annotations):\n        alpha = 0.25\n        gamma = 2.0\n        batch_size = classifications.shape[0]\n        classification_losses = []\n        regression_losses = []\n\n        anchor = anchors[0, :, :]\n\n        anchor_widths = anchor[:, 2] - anchor[:, 0]\n        anchor_heights = anchor[:, 3] - anchor[:, 1]\n        anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths\n        anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights\n\n        for j in range(batch_size):\n\n            classification = classifications[j, :, :]\n            regression = regressions[j, :, :]\n\n            bbox_annotation = annotations[j, :, :]\n            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]\n\n            if bbox_annotation.shape[0] == 0:\n                regression_losses.append(torch.tensor(0).float().cuda())\n                classification_losses.append(torch.tensor(0).float().cuda())\n\n                continue\n\n            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n\n            # num_anchors x num_annotations\n            IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4])\n\n            IoU_max, IoU_argmax = torch.max(IoU, dim=1)  # num_anchors x 1\n\n            #import pdb\n            # pdb.set_trace()\n\n            # compute the loss for classification\n            targets = torch.ones(classification.shape) * -1\n            targets = targets.cuda()\n\n            targets[torch.lt(IoU_max, 0.4), :] = 0\n\n            positive_indices = torch.ge(IoU_max, 0.5)\n\n            num_positive_anchors = positive_indices.sum()\n\n            assigned_annotations = bbox_annotation[IoU_argmax, :]\n\n            targets[positive_indices, :] = 0\n            targets[positive_indices,\n                    assigned_annotations[positive_indices, 4].long()] = 1\n\n            alpha_factor = torch.ones(targets.shape).cuda() * alpha\n\n            alpha_factor = torch.where(\n                torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n            focal_weight = torch.where(\n                torch.eq(targets, 1.), 1. - classification, classification)\n            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n\n            bce = -(targets * torch.log(classification) +\n                    (1.0 - targets) * torch.log(1.0 - classification))\n\n            # cls_loss = focal_weight * torch.pow(bce, gamma)\n            cls_loss = focal_weight * bce\n\n            cls_loss = torch.where(\n                torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape).cuda())\n\n            classification_losses.append(\n                cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0))\n\n            # compute the loss for regression\n\n            if positive_indices.sum() > 0:\n                assigned_annotations = assigned_annotations[positive_indices, :]\n\n                anchor_widths_pi = anchor_widths[positive_indices]\n                anchor_heights_pi = anchor_heights[positive_indices]\n                anchor_ctr_x_pi = anchor_ctr_x[positive_indices]\n                anchor_ctr_y_pi = anchor_ctr_y[positive_indices]\n\n                gt_widths = assigned_annotations[:,\n                                                 2] - assigned_annotations[:, 0]\n                gt_heights = assigned_annotations[:,\n                                                  3] - assigned_annotations[:, 1]\n                gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths\n                gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights\n\n                # clip widths to 1\n                gt_widths = torch.clamp(gt_widths, min=1)\n                gt_heights = torch.clamp(gt_heights, min=1)\n\n                targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi\n                targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi\n                targets_dw = torch.log(gt_widths / anchor_widths_pi)\n                targets_dh = torch.log(gt_heights / anchor_heights_pi)\n\n                targets = torch.stack(\n                    (targets_dx, targets_dy, targets_dw, targets_dh))\n                targets = targets.t()\n\n                targets = targets/torch.Tensor([[0.1, 0.1, 0.2, 0.2]]).cuda()\n\n                negative_indices = 1 + (~positive_indices)\n\n                regression_diff = torch.abs(\n                    targets - regression[positive_indices, :])\n\n                regression_loss = torch.where(\n                    torch.le(regression_diff, 1.0 / 9.0),\n                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n                    regression_diff - 0.5 / 9.0\n                )\n                regression_losses.append(regression_loss.mean())\n            else:\n                regression_losses.append(torch.tensor(0).float().cuda())\n\n        return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0, keepdim=True)\n'"
models/module.py,12,"b'\nimport numpy as np\nimport torch\nimport warnings\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BBoxTransform(nn.Module):\n\n    def __init__(self, mean=None, std=None):\n        super(BBoxTransform, self).__init__()\n        if mean is None:\n            self.mean = torch.from_numpy(\n                np.array([0, 0, 0, 0]).astype(np.float32))\n        else:\n            self.mean = mean\n        if std is None:\n            self.std = torch.from_numpy(\n                np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32))\n        else:\n            self.std = std\n\n    def forward(self, boxes, deltas):\n\n        widths = boxes[:, :, 2] - boxes[:, :, 0]\n        heights = boxes[:, :, 3] - boxes[:, :, 1]\n        ctr_x = boxes[:, :, 0] + 0.5 * widths\n        ctr_y = boxes[:, :, 1] + 0.5 * heights\n\n        dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n        dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n        dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n        dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n\n        pred_ctr_x = ctr_x + dx * widths\n        pred_ctr_y = ctr_y + dy * heights\n        pred_w = torch.exp(dw) * widths\n        pred_h = torch.exp(dh) * heights\n\n        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n\n        pred_boxes = torch.stack(\n            [pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n\n        return pred_boxes\n\n\nclass ClipBoxes(nn.Module):\n\n    def __init__(self, width=None, height=None):\n        super(ClipBoxes, self).__init__()\n\n    def forward(self, boxes, img):\n\n        batch_size, num_channels, height, width = img.shape\n\n        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n\n        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n\n        return boxes\n\n\nclass RegressionModel(nn.Module):\n    def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n        super(RegressionModel, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            num_features_in, feature_size, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(feature_size, feature_size,\n                               kernel_size=3, padding=1)\n        self.act2 = nn.ReLU()\n        self.conv3 = nn.Conv2d(feature_size, feature_size,\n                               kernel_size=3, padding=1)\n        self.act3 = nn.ReLU()\n        self.conv4 = nn.Conv2d(feature_size, feature_size,\n                               kernel_size=3, padding=1)\n        self.act4 = nn.ReLU()\n        self.output = nn.Conv2d(\n            feature_size, num_anchors*4, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.act1(out)\n        out = self.conv2(out)\n        out = self.act2(out)\n        out = self.conv3(out)\n        out = self.act3(out)\n        out = self.conv4(out)\n        out = self.act4(out)\n        out = self.output(out)\n        # out is B x C x W x H, with C = 4*num_anchors\n        out = out.permute(0, 2, 3, 1)\n        return out.contiguous().view(out.shape[0], -1, 4)\n\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):\n        super(ClassificationModel, self).__init__()\n        self.num_classes = num_classes\n        self.num_anchors = num_anchors\n\n        self.conv1 = nn.Conv2d(\n            num_features_in, feature_size, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(feature_size, feature_size,\n                               kernel_size=3, padding=1)\n        self.act2 = nn.ReLU()\n        self.conv3 = nn.Conv2d(feature_size, feature_size,\n                               kernel_size=3, padding=1)\n        self.act3 = nn.ReLU()\n        self.conv4 = nn.Conv2d(feature_size, feature_size,\n                               kernel_size=3, padding=1)\n        self.act4 = nn.ReLU()\n        self.output = nn.Conv2d(\n            feature_size, num_anchors*num_classes, kernel_size=3, padding=1)\n        self.output_act = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.act1(out)\n        out = self.conv2(out)\n        out = self.act2(out)\n        out = self.conv3(out)\n        out = self.act3(out)\n        out = self.conv4(out)\n        out = self.act4(out)\n        out = self.output(out)\n        out = self.output_act(out)\n        # out is B x C x W x H, with C = n_classes + n_anchors\n        out1 = out.permute(0, 2, 3, 1)\n        batch_size, width, height, channels = out1.shape\n        out2 = out1.view(batch_size, width, height,\n                         self.num_anchors, self.num_classes)\n        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n\n\nclass Anchors(nn.Module):\n    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n        super(Anchors, self).__init__()\n\n        if pyramid_levels is None:\n            self.pyramid_levels = [3, 4, 5, 6, 7]\n        if strides is None:\n            self.strides = [2 ** x for x in self.pyramid_levels]\n        if sizes is None:\n            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n        if ratios is None:\n            self.ratios = np.array([0.5, 1, 2])\n        if scales is None:\n            self.scales = np.array(\n                [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n\n    def forward(self, image):\n\n        image_shape = image.shape[2:]\n        image_shape = np.array(image_shape)\n        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x)\n                        for x in self.pyramid_levels]\n\n        # compute anchors over all pyramid levels\n        all_anchors = np.zeros((0, 4)).astype(np.float32)\n\n        for idx, p in enumerate(self.pyramid_levels):\n            anchors = generate_anchors(\n                base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n            shifted_anchors = shift(\n                image_shapes[idx], self.strides[idx], anchors)\n            all_anchors = np.append(all_anchors, shifted_anchors, axis=0)\n\n        all_anchors = np.expand_dims(all_anchors, axis=0)\n\n        return torch.from_numpy(all_anchors.astype(np.float32)).to(image.device)\n\n\ndef generate_anchors(base_size=16, ratios=None, scales=None):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales w.r.t. a reference window.\n    """"""\n\n    if ratios is None:\n        ratios = np.array([0.5, 1, 2])\n\n    if scales is None:\n        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n\n    num_anchors = len(ratios) * len(scales)\n\n    # initialize output anchors\n    anchors = np.zeros((num_anchors, 4))\n\n    # scale base_size\n    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n\n    # compute areas of anchors\n    areas = anchors[:, 2] * anchors[:, 3]\n\n    # correct for ratios\n    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n\n    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n\n    return anchors\n\n\ndef compute_shape(image_shape, pyramid_levels):\n    """"""Compute shapes based on pyramid levels.\n    :param image_shape:\n    :param pyramid_levels:\n    :return:\n    """"""\n    image_shape = np.array(image_shape[:2])\n    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x)\n                    for x in pyramid_levels]\n    return image_shapes\n\n\ndef anchors_for_shape(\n    image_shape,\n    pyramid_levels=None,\n    ratios=None,\n    scales=None,\n    strides=None,\n    sizes=None,\n    shapes_callback=None,\n):\n\n    image_shapes = compute_shape(image_shape, pyramid_levels)\n\n    # compute anchors over all pyramid levels\n    all_anchors = np.zeros((0, 4))\n    for idx, p in enumerate(pyramid_levels):\n        anchors = generate_anchors(\n            base_size=sizes[idx], ratios=ratios, scales=scales)\n        shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n        all_anchors = np.append(all_anchors, shifted_anchors, axis=0)\n\n    return all_anchors\n\n\ndef shift(shape, stride, anchors):\n    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n\n    shifts = np.vstack((\n        shift_x.ravel(), shift_y.ravel(),\n        shift_x.ravel(), shift_y.ravel()\n    )).transpose()\n\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = anchors.shape[0]\n    K = shifts.shape[0]\n    all_anchors = (anchors.reshape((1, A, 4)) +\n                   shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n    all_anchors = all_anchors.reshape((K * A, 4))\n\n    return all_anchors\n\n\ndef conv_ws_2d(input,\n               weight,\n               bias=None,\n               stride=1,\n               padding=0,\n               dilation=1,\n               groups=1,\n               eps=1e-5):\n    c_in = weight.size(0)\n    weight_flat = weight.view(c_in, -1)\n    mean = weight_flat.mean(dim=1, keepdim=True).view(c_in, 1, 1, 1)\n    std = weight_flat.std(dim=1, keepdim=True).view(c_in, 1, 1, 1)\n    weight = (weight - mean) / (std + eps)\n    return F.conv2d(input, weight, bias, stride, padding, dilation, groups)\n\n\nclass ConvWS2d(nn.Conv2d):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=True,\n                 eps=1e-5):\n        super(ConvWS2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias)\n        self.eps = eps\n\n    def forward(self, x):\n        return conv_ws_2d(x, self.weight, self.bias, self.stride, self.padding,\n                          self.dilation, self.groups, self.eps)\n\n\nconv_cfg = {\n    \'Conv\': nn.Conv2d,\n    \'ConvWS\': ConvWS2d,\n    # TODO: octave conv\n}\n\n\ndef build_conv_layer(cfg, *args, **kwargs):\n    """""" Build convolution layer\n    Args:\n        cfg (None or dict): cfg should contain:\n            type (str): identify conv layer type.\n            layer args: args needed to instantiate a conv layer.\n    Returns:\n        layer (nn.Module): created conv layer\n    """"""\n    if cfg is None:\n        cfg_ = dict(type=\'Conv\')\n    else:\n        assert isinstance(cfg, dict) and \'type\' in cfg\n        cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in conv_cfg:\n        raise KeyError(\'Unrecognized norm type {}\'.format(layer_type))\n    else:\n        conv_layer = conv_cfg[layer_type]\n\n    layer = conv_layer(*args, **kwargs, **cfg_)\n\n    return layer\n\n\nnorm_cfg = {\n    # format: layer_type: (abbreviation, module)\n    \'BN\': (\'bn\', nn.BatchNorm2d),\n    \'SyncBN\': (\'bn\', nn.SyncBatchNorm),\n    \'GN\': (\'gn\', nn.GroupNorm),\n    # and potentially \'SN\'\n}\n\n\ndef build_norm_layer(cfg, num_features, postfix=\'\'):\n    """""" Build normalization layer\n    Args:\n        cfg (dict): cfg should contain:\n            type (str): identify norm layer type.\n            layer args: args needed to instantiate a norm layer.\n            requires_grad (bool): [optional] whether stop gradient updates\n        num_features (int): number of channels from input.\n        postfix (int, str): appended into norm abbreviation to\n            create named layer.\n    Returns:\n        name (str): abbreviation + postfix\n        layer (nn.Module): created norm layer\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in norm_cfg:\n        raise KeyError(\'Unrecognized norm type {}\'.format(layer_type))\n    else:\n        abbr, norm_layer = norm_cfg[layer_type]\n        if norm_layer is None:\n            raise NotImplementedError\n\n    assert isinstance(postfix, (int, str))\n    name = abbr + str(postfix)\n\n    requires_grad = cfg_.pop(\'requires_grad\', True)\n    cfg_.setdefault(\'eps\', 1e-5)\n    if layer_type != \'GN\':\n        layer = norm_layer(num_features, **cfg_)\n        if layer_type == \'SyncBN\':\n            layer._specify_ddp_gpu_num(1)\n    else:\n        assert \'num_groups\' in cfg_\n        layer = norm_layer(num_channels=num_features, **cfg_)\n\n    for param in layer.parameters():\n        param.requires_grad = requires_grad\n\n    return name, layer\n\n\nclass ConvModule(nn.Module):\n    """"""A conv block that contains conv/norm/activation layers.\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.\n        bias (bool or str): If specified as `auto`, it will be decided by the\n            norm_cfg. Bias will be set as True if norm_cfg is None, otherwise\n            False.\n        conv_cfg (dict): Config dict for convolution layer.\n        norm_cfg (dict): Config dict for normalization layer.\n        activation (str or None): Activation type, ""ReLU"" by default.\n        inplace (bool): Whether to use inplace mode for activation.\n        order (tuple[str]): The order of conv/norm/activation layers. It is a\n            sequence of ""conv"", ""norm"" and ""act"". Examples are\n            (""conv"", ""norm"", ""act"") and (""act"", ""conv"", ""norm"").\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=\'auto\',\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 activation=\'relu\',\n                 inplace=True,\n                 order=(\'conv\', \'norm\', \'act\')):\n        super(ConvModule, self).__init__()\n        assert conv_cfg is None or isinstance(conv_cfg, dict)\n        assert norm_cfg is None or isinstance(norm_cfg, dict)\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.activation = activation\n        self.inplace = inplace\n        self.order = order\n        assert isinstance(self.order, tuple) and len(self.order) == 3\n        assert set(order) == set([\'conv\', \'norm\', \'act\'])\n\n        self.with_norm = norm_cfg is not None\n        self.with_activatation = activation is not None\n        # if the conv layer is before a norm layer, bias is unnecessary.\n        if bias == \'auto\':\n            bias = False if self.with_norm else True\n        self.with_bias = bias\n\n        if self.with_norm and self.with_bias:\n            warnings.warn(\'ConvModule has norm and bias at the same time\')\n\n        # build convolution layer\n        self.conv = build_conv_layer(\n            conv_cfg,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias)\n        # export the attributes of self.conv to a higher level for convenience\n        self.in_channels = self.conv.in_channels\n        self.out_channels = self.conv.out_channels\n        self.kernel_size = self.conv.kernel_size\n        self.stride = self.conv.stride\n        self.padding = self.conv.padding\n        self.dilation = self.conv.dilation\n        self.transposed = self.conv.transposed\n        self.output_padding = self.conv.output_padding\n        self.groups = self.conv.groups\n\n        # build normalization layers\n        if self.with_norm:\n            # norm layer is after conv layer\n            if order.index(\'norm\') > order.index(\'conv\'):\n                norm_channels = out_channels\n            else:\n                norm_channels = in_channels\n            self.norm_name, norm = build_norm_layer(norm_cfg, norm_channels)\n            self.add_module(self.norm_name, norm)\n\n        # build activation layer\n        if self.with_activatation:\n            # TODO: introduce `act_cfg` and supports more activation layers\n            if self.activation not in [\'relu\']:\n                raise ValueError(\'{} is currently not supported.\'.format(\n                    self.activation))\n            if self.activation == \'relu\':\n                self.activate = nn.ReLU(inplace=inplace)\n\n    @property\n    def norm(self):\n        return getattr(self, self.norm_name)\n\n    def forward(self, x, activate=True, norm=True):\n        for layer in self.order:\n            if layer == \'conv\':\n                x = self.conv(x)\n            elif layer == \'norm\' and norm and self.with_norm:\n                x = self.norm(x)\n            elif layer == \'act\' and activate and self.with_activatation:\n                x = self.activate(x)\n        return x\n\n\ndef xavier_init(module, gain=1, bias=0, distribution=\'normal\'):\n    assert distribution in [\'uniform\', \'normal\']\n    if distribution == \'uniform\':\n        nn.init.xavier_uniform_(module.weight, gain=gain)\n    else:\n        nn.init.xavier_normal_(module.weight, gain=gain)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef normal_init(module, mean=0, std=1, bias=0):\n    nn.init.normal_(module.weight, mean, std)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef uniform_init(module, a=0, b=1, bias=0):\n    nn.init.uniform_(module.weight, a, b)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef kaiming_init(module,\n                 mode=\'fan_out\',\n                 nonlinearity=\'relu\',\n                 bias=0,\n                 distribution=\'normal\'):\n    assert distribution in [\'uniform\', \'normal\']\n    if distribution == \'uniform\':\n        nn.init.kaiming_uniform_(\n            module.weight, mode=mode, nonlinearity=nonlinearity)\n    else:\n        nn.init.kaiming_normal_(\n            module.weight, mode=mode, nonlinearity=nonlinearity)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef bias_init_with_prob(prior_prob):\n    """""" initialize conv/fc bias value according to giving probablity""""""\n    bias_init = float(-np.log((1 - prior_prob) / prior_prob))\n    return bias_init\n'"
models/retinahead.py,2,"b'from functools import partial\n\nimport numpy as np\nimport torch.nn as nn\n\nfrom .module import ConvModule, bias_init_with_prob, normal_init\nfrom six.moves import map, zip\n\n\ndef multi_apply(func, *args, **kwargs):\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\n\n\nclass RetinaHead(nn.Module):\n    """"""\n    An anchor-based head used in [1]_.\n    The head contains two subnetworks. The first classifies anchor boxes and\n    the second regresses deltas for the anchors.\n    References:\n        .. [1]  https://arxiv.org/pdf/1708.02002.pdf\n    Example:\n        >>> import torch\n        >>> self = RetinaHead(11, 7)\n        >>> x = torch.rand(1, 7, 32, 32)\n        >>> cls_score, bbox_pred = self.forward_single(x)\n        >>> # Each anchor predicts a score for each class except background\n        >>> cls_per_anchor = cls_score.shape[1] / self.num_anchors\n        >>> box_per_anchor = bbox_pred.shape[1] / self.num_anchors\n        >>> assert cls_per_anchor == (self.num_classes - 1)\n        >>> assert box_per_anchor == 4\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 feat_channels=256,\n                 anchor_scales=[8, 16, 32],\n                 anchor_ratios=[0.5, 1.0, 2.0],\n                 anchor_strides=[4, 8, 16, 32, 64],\n                 stacked_convs=4,\n                 octave_base_scale=4,\n                 scales_per_octave=3,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 **kwargs):\n        super(RetinaHead, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.anchor_scales = anchor_scales\n        self.anchor_ratios = anchor_ratios\n        self.anchor_strides = anchor_strides\n        self.stacked_convs = stacked_convs\n        self.octave_base_scale = octave_base_scale\n        self.scales_per_octave = scales_per_octave\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        octave_scales = np.array(\n            [2**(i / scales_per_octave) for i in range(scales_per_octave)])\n        anchor_scales = octave_scales * octave_base_scale\n        self.cls_out_channels = num_classes\n        self.num_anchors = len(self.anchor_ratios) * len(self.anchor_scales)\n        self._init_layers()\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n        self.retina_cls = nn.Conv2d(\n            self.feat_channels,\n            self.num_anchors * self.cls_out_channels,\n            3,\n            padding=1)\n        self.retina_reg = nn.Conv2d(\n            self.feat_channels, self.num_anchors * 4, 3, padding=1)\n        self.output_act = nn.Sigmoid()\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.retina_cls, std=0.01, bias=bias_cls)\n        normal_init(self.retina_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_feat = x\n        reg_feat = x\n        for cls_conv in self.cls_convs:\n            cls_feat = cls_conv(cls_feat)\n        for reg_conv in self.reg_convs:\n            reg_feat = reg_conv(reg_feat)\n\n        cls_score = self.retina_cls(cls_feat)\n        cls_score = self.output_act(cls_score)\n        # out is B x C x W x H, with C = n_classes + n_anchors\n        cls_score = cls_score.permute(0, 2, 3, 1)\n        batch_size, width, height, channels = cls_score.shape\n        cls_score = cls_score.view(\n            batch_size, width, height, self.num_anchors, self.num_classes)\n        cls_score = cls_score.contiguous().view(x.size(0), -1, self.num_classes)\n\n        bbox_pred = self.retina_reg(reg_feat)\n        bbox_pred = bbox_pred.permute(0, 2, 3, 1)\n        bbox_pred = bbox_pred.contiguous().view(bbox_pred.size(0), -1, 4)\n        return cls_score, bbox_pred\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n'"
models/utils.py,8,"b'import re\nimport math\nimport collections\nfrom functools import partial\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import model_zoo\n\n########################################################################\n############### HELPERS FUNCTIONS FOR MODEL ARCHITECTURE ###############\n########################################################################\n\n\n# Parameters for the entire model (stem, all blocks, and head)\nGlobalParams = collections.namedtuple(\'GlobalParams\', [\n    \'batch_norm_momentum\', \'batch_norm_epsilon\', \'dropout_rate\',\n    \'num_classes\', \'width_coefficient\', \'depth_coefficient\',\n    \'depth_divisor\', \'min_depth\', \'drop_connect_rate\', \'image_size\'])\n\n# Parameters for an individual model block\nBlockArgs = collections.namedtuple(\'BlockArgs\', [\n    \'kernel_size\', \'num_repeat\', \'input_filters\', \'output_filters\',\n    \'expand_ratio\', \'id_skip\', \'stride\', \'se_ratio\'])\n\n# Change namedtuple defaults\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\nclass SwishImplementation(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = torch.sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\nclass MemoryEfficientSwish(nn.Module):\n    def forward(self, x):\n        return SwishImplementation.apply(x)\n\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\ndef round_filters(filters, global_params):\n    """""" Calculate and round number of filters based on depth multiplier. """"""\n    multiplier = global_params.width_coefficient\n    if not multiplier:\n        return filters\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(\n        filters + divisor / 2) // divisor * divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    """""" Round number of filters based on depth multiplier. """"""\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef drop_connect(inputs, p, training):\n    """""" Drop connect. """"""\n    if not training:\n        return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1],\n                                dtype=inputs.dtype, device=inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs / keep_prob * binary_tensor\n    return output\n\n\ndef get_same_padding_conv2d(image_size=None):\n    """""" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n        Static padding is necessary for ONNX exporting of models. """"""\n    if image_size is None:\n        return Conv2dDynamicSamePadding\n    else:\n        return partial(Conv2dStaticSamePadding, image_size=image_size)\n\n\nclass Conv2dDynamicSamePadding(nn.Conv2d):\n    """""" 2D Convolutions like TensorFlow, for a dynamic image size """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n        super().__init__(in_channels, out_channels,\n                         kernel_size, stride, 0, dilation, groups, bias)\n        self.stride = self.stride if len(self.stride) == 2 else [\n            self.stride[0]] * 2\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] +\n                    (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] +\n                    (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, [pad_w // 2, pad_w - pad_w //\n                          2, pad_h // 2, pad_h - pad_h // 2])\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass Conv2dStaticSamePadding(nn.Conv2d):\n    """""" 2D Convolutions like TensorFlow, for a fixed image size""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n        self.stride = self.stride if len(self.stride) == 2 else [\n            self.stride[0]] * 2\n\n        # Calculate padding based on image size and save it\n        assert image_size is not None\n        ih, iw = image_size if type(image_size) == list else [\n            image_size, image_size]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] +\n                    (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] +\n                    (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            self.static_padding = nn.ZeroPad2d(\n                (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n        else:\n            self.static_padding = Identity()\n\n    def forward(self, x):\n        x = self.static_padding(x)\n        x = F.conv2d(x, self.weight, self.bias, self.stride,\n                     self.padding, self.dilation, self.groups)\n        return x\n\n\nclass Identity(nn.Module):\n    def __init__(self, ):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\n########################################################################\n############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n########################################################################\n\n\ndef efficientnet_params(model_name):\n    """""" Map EfficientNet model name to parameter coefficients. """"""\n    params_dict = {\n        # Coefficients:   width,depth,res,dropout\n        \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n        \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n        \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n        \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n        \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n        \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n        \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n        \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n    }\n    return params_dict[model_name]\n\n\nclass BlockDecoder(object):\n    """""" Block Decoder for readability, straight from the official TensorFlow repository """"""\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        """""" Gets a block through a string notation of arguments. """"""\n        assert isinstance(block_string, str)\n\n        ops = block_string.split(\'_\')\n        options = {}\n        for op in ops:\n            splits = re.split(r\'(\\d.*)\', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        # Check stride\n        assert ((\'s\' in options and len(options[\'s\']) == 1) or\n                (len(options[\'s\']) == 2 and options[\'s\'][0] == options[\'s\'][1]))\n\n        return BlockArgs(\n            kernel_size=int(options[\'k\']),\n            num_repeat=int(options[\'r\']),\n            input_filters=int(options[\'i\']),\n            output_filters=int(options[\'o\']),\n            expand_ratio=int(options[\'e\']),\n            id_skip=(\'noskip\' not in block_string),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=[int(options[\'s\'][0])])\n\n    @staticmethod\n    def _encode_block_string(block):\n        """"""Encodes a block to a string.""""""\n        args = [\n            \'r%d\' % block.num_repeat,\n            \'k%d\' % block.kernel_size,\n            \'s%d%d\' % (block.strides[0], block.strides[1]),\n            \'e%s\' % block.expand_ratio,\n            \'i%d\' % block.input_filters,\n            \'o%d\' % block.output_filters\n        ]\n        if 0 < block.se_ratio <= 1:\n            args.append(\'se%s\' % block.se_ratio)\n        if block.id_skip is False:\n            args.append(\'noskip\')\n        return \'_\'.join(args)\n\n    @staticmethod\n    def decode(string_list):\n        """"""\n        Decodes a list of string notations to specify blocks inside the network.\n        :param string_list: a list of strings, each string is a notation of block\n        :return: a list of BlockArgs namedtuples of block args\n        """"""\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n    @staticmethod\n    def encode(blocks_args):\n        """"""\n        Encodes a list of BlockArgs to a list of strings.\n        :param blocks_args: a list of BlockArgs namedtuples of block args\n        :return: a list of strings, each string is a notation of block\n        """"""\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(BlockDecoder._encode_block_string(block))\n        return block_strings\n\n\ndef efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n    """""" Creates a efficientnet model. """"""\n\n    blocks_args = [\n        \'r1_k3_s11_e1_i32_o16_se0.25\', \'r2_k3_s22_e6_i16_o24_se0.25\',\n        \'r2_k5_s22_e6_i24_o40_se0.25\', \'r3_k3_s22_e6_i40_o80_se0.25\',\n        \'r3_k5_s22_e6_i80_o112_se0.25\', \'r4_k5_s22_e6_i112_o192_se0.25\',\n        \'r1_k3_s22_e6_i192_o320_se0.25\',\n    ]\n    blocks_args = BlockDecoder.decode(blocks_args)\n\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=drop_connect_rate,\n        # data_format=\'channels_last\',  # removed, this is always true in PyTorch\n        num_classes=num_classes,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None,\n        image_size=image_size,\n    )\n\n    return blocks_args, global_params\n\n\ndef get_model_params(model_name, override_params):\n    """""" Get the block args and global params for a given model """"""\n    if model_name.startswith(\'efficientnet\'):\n        w, d, s, p = efficientnet_params(model_name)\n        # note: all models have drop connect rate = 0.2\n        blocks_args, global_params = efficientnet(\n            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n    else:\n        raise NotImplementedError(\n            \'model name is not pre-defined: %s\' % model_name)\n    if override_params:\n        # ValueError will be raised here if override_params has fields not included in global_params.\n        global_params = global_params._replace(**override_params)\n    return blocks_args, global_params\n\n\nurl_map = {\n    \'efficientnet-b0\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b0-355c32eb.pth\',\n    \'efficientnet-b1\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b1-f1951068.pth\',\n    \'efficientnet-b2\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b2-8bb594d6.pth\',\n    \'efficientnet-b3\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b3-5fb5a3c3.pth\',\n    \'efficientnet-b4\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b4-6ed6700e.pth\',\n    \'efficientnet-b5\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b5-b6417697.pth\',\n    \'efficientnet-b6\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b6-c76e70fd.pth\',\n    \'efficientnet-b7\': \'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b7-dcc49843.pth\',\n}\n\n\ndef load_pretrained_weights(model, model_name, load_fc=True):\n    """""" Loads pretrained weights, and downloads if loading for the first time. """"""\n    state_dict = model_zoo.load_url(url_map[model_name])\n    if load_fc:\n        model.load_state_dict(state_dict)\n    else:\n        state_dict.pop(\'_fc.weight\')\n        state_dict.pop(\'_fc.bias\')\n        res = model.load_state_dict(state_dict, strict=False)\n        assert set(res.missing_keys) == set(\n            [\'_fc.weight\', \'_fc.bias\']), \'issue loading pretrained weights\'\n    print(\'Loaded pretrained weights for {}\'.format(model_name))\n'"
utils/__init__.py,0,b'from .helper import *\nfrom .util import *\nfrom .visualization import *\nfrom .vis_bbox import vis_bbox\nfrom .config_eff import *'
utils/config_eff.py,0,"b""EFFICIENTDET = {\n    'efficientdet-d0': {'input_size': 512,\n                        'backbone': 'B0',\n                        'W_bifpn': 64,\n                        'D_bifpn': 2,\n                        'D_class': 3},\n    'efficientdet-d1': {'input_size': 640,\n                        'backbone': 'B1',\n                        'W_bifpn': 88,\n                        'D_bifpn': 3,\n                        'D_class': 3},\n    'efficientdet-d2': {'input_size': 768,\n                        'backbone': 'B2',\n                        'W_bifpn': 112,\n                        'D_bifpn': 4,\n                        'D_class': 3},\n    'efficientdet-d3': {'input_size': 896,\n                        'backbone': 'B3',\n                        'W_bifpn': 160,\n                        'D_bifpn': 5,\n                        'D_class': 4},\n    'efficientdet-d4': {'input_size': 1024,\n                        'backbone': 'B4',\n                        'W_bifpn': 224,\n                        'D_bifpn': 6,\n                        'D_class': 4},\n    'efficientdet-d5': {'input_size': 1280,\n                        'backbone': 'B5',\n                        'W_bifpn': 288,\n                        'D_bifpn': 7,\n                        'D_class': 4},\n    'efficientdet-d6': {'input_size': 1408,\n                        'backbone': 'B6',\n                        'W_bifpn': 384,\n                        'D_bifpn': 8,\n                        'D_class': 5},\n    'efficientdet-d7': {'input_size': 1636,\n                        'backbone': 'B6',\n                        'W_bifpn': 384,\n                        'D_bifpn': 8,\n                        'D_class': 5},\n}"""
utils/helper.py,4,"b""import yaml\nimport numpy as np\nimport torch\nimport os\nimport requests\nimport socket\nimport datetime\nimport json\n\n\ndef load_yaml(file_name):\n    with open(file_name, 'r') as stream:\n        config = yaml.load(stream, Loader=yaml.FullLoader)\n    return config\n\n\ndef init_seed(SEED=42):\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True\n\n\ndef get_state_dict(model):\n    if type(model) == torch.nn.DataParallel:\n        state_dict = model.module.state_dict()\n    else:\n        state_dict = model.state_dict()\n    return state_dict\n"""
utils/metric.py,1,"b'from __future__ import print_function\n\nimport numpy as np\nimport json\nimport os\n\nimport torch\n\n\ndef compute_overlap(a, b):\n    """"""\n    Parameters\n    ----------\n    a: (N, 4) ndarray of float\n    b: (K, 4) ndarray of float\n    Returns\n    -------\n    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n    """"""\n    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n\n    iw = np.minimum(np.expand_dims(\n        a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n    ih = np.minimum(np.expand_dims(\n        a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n\n    iw = np.maximum(iw, 0)\n    ih = np.maximum(ih, 0)\n\n    ua = np.expand_dims((a[:, 2] - a[:, 0]) *\n                        (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n\n    ua = np.maximum(ua, np.finfo(float).eps)\n\n    intersection = iw * ih\n\n    return intersection / ua\n\n\ndef _compute_ap(recall, precision):\n    """""" Compute the average precision, given the recall and precision curves.\n    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n    # Arguments\n        recall:    The recall curve (list).\n        precision: The precision curve (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], recall, [1.]))\n    mpre = np.concatenate(([0.], precision, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef _get_detections(dataset, model, score_threshold=0.05, max_detections=100, save_path=None):\n    """""" Get the detections from the retinanet using the generator.\n    The result is a list of lists such that the size is:\n        all_detections[num_images][num_classes] = detections[num_detections, 4 + num_classes]\n    # Arguments\n        dataset         : The generator used to run images through the retinanet.\n        retinanet           : The retinanet to run on the images.\n        score_threshold : The score confidence threshold to use.\n        max_detections  : The maximum number of detections to use per image.\n        save_path       : The path to save the images with visualized detections to.\n    # Returns\n        A list of lists containing the detections for each image in the generator.\n    """"""\n    all_detections = [[None for i in range(\n        dataset.num_classes())] for j in range(len(dataset))]\n\n    model.eval()\n\n    with torch.no_grad():\n        for index in range(len(dataset)):\n            data = dataset[index]\n            scale = data[\'scale\']\n\n            # run network\n            scores, labels, boxes = model(data[\'img\'].permute(\n                2, 0, 1).cuda().float().unsqueeze(dim=0))\n            scores = scores.cpu().numpy()\n            labels = labels.cpu().numpy()\n            boxes = boxes.cpu().numpy()\n\n            # correct boxes for image scale\n            boxes /= scale\n\n            # select indices which have a score above the threshold\n            indices = np.where(scores > score_threshold)[0]\n            if indices.shape[0] > 0:\n                # select those scores\n                scores = scores[indices]\n\n                # find the order with which to sort the scores\n                scores_sort = np.argsort(-scores)[:max_detections]\n\n                # select detections\n                image_boxes = boxes[indices[scores_sort], :]\n                image_scores = scores[scores_sort]\n                image_labels = labels[indices[scores_sort]]\n                image_detections = np.concatenate([image_boxes, np.expand_dims(\n                    image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1)\n\n                # copy detections to all_detections\n                for label in range(dataset.num_classes()):\n                    all_detections[index][label] = image_detections[image_detections[:, -1] == label, :-1]\n            else:\n                # copy detections to all_detections\n                for label in range(dataset.num_classes()):\n                    all_detections[index][label] = np.zeros((0, 5))\n\n            print(\'{}/{}\'.format(index + 1, len(dataset)), end=\'\\r\')\n\n    return all_detections\n\n\ndef _get_annotations(generator):\n    """""" Get the ground truth annotations from the generator.\n    The result is a list of lists such that the size is:\n        all_detections[num_images][num_classes] = annotations[num_detections, 5]\n    # Arguments\n        generator : The generator used to retrieve ground truth annotations.\n    # Returns\n        A list of lists containing the annotations for each image in the generator.\n    """"""\n    all_annotations = [[None for i in range(\n        generator.num_classes())] for j in range(len(generator))]\n\n    for i in range(len(generator)):\n        # load the annotations\n        annotations = generator.load_annotations(i)\n\n        # copy detections to all_annotations\n        for label in range(generator.num_classes()):\n            all_annotations[i][label] = annotations[annotations[:, 4]\n                                                    == label, :4].copy()\n\n        print(\'{}/{}\'.format(i + 1, len(generator)), end=\'\\r\')\n\n    return all_annotations\n\n\ndef evaluate(\n    generator,\n    retinanet,\n    iou_threshold=0.5,\n    score_threshold=0.05,\n    max_detections=100,\n    save_path=None\n):\n    """""" Evaluate a given dataset using a given retinanet.\n    # Arguments\n        generator       : The generator that represents the dataset to evaluate.\n        retinanet           : The retinanet to evaluate.\n        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n        score_threshold : The score confidence threshold to use for detections.\n        max_detections  : The maximum number of detections to use per image.\n        save_path       : The path to save images with visualized detections to.\n    # Returns\n        A dict mapping class names to mAP scores.\n    """"""\n\n    # gather all detections and annotations\n\n    all_detections = _get_detections(\n        generator, retinanet, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)\n    all_annotations = _get_annotations(generator)\n\n    average_precisions = {}\n\n    for label in range(generator.num_classes()):\n        false_positives = np.zeros((0,))\n        true_positives = np.zeros((0,))\n        scores = np.zeros((0,))\n        num_annotations = 0.0\n\n        for i in range(len(generator)):\n            detections = all_detections[i][label]\n            annotations = all_annotations[i][label]\n            num_annotations += annotations.shape[0]\n            detected_annotations = []\n\n            for d in detections:\n                scores = np.append(scores, d[4])\n\n                if annotations.shape[0] == 0:\n                    false_positives = np.append(false_positives, 1)\n                    true_positives = np.append(true_positives, 0)\n                    continue\n\n                overlaps = compute_overlap(\n                    np.expand_dims(d, axis=0), annotations)\n                assigned_annotation = np.argmax(overlaps, axis=1)\n                max_overlap = overlaps[0, assigned_annotation]\n\n                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n                    false_positives = np.append(false_positives, 0)\n                    true_positives = np.append(true_positives, 1)\n                    detected_annotations.append(assigned_annotation)\n                else:\n                    false_positives = np.append(false_positives, 1)\n                    true_positives = np.append(true_positives, 0)\n\n        # no annotations -> AP for this class is 0 (is this correct?)\n        if num_annotations == 0:\n            average_precisions[label] = 0, 0\n            continue\n\n        # sort by score\n        indices = np.argsort(-scores)\n        false_positives = false_positives[indices]\n        true_positives = true_positives[indices]\n\n        # compute false positives and true positives\n        false_positives = np.cumsum(false_positives)\n        true_positives = np.cumsum(true_positives)\n\n        # compute recall and precision\n        recall = true_positives / num_annotations\n        precision = true_positives / \\\n            np.maximum(true_positives + false_positives,\n                       np.finfo(np.float64).eps)\n\n        # compute average precision\n        average_precision = _compute_ap(recall, precision)\n        average_precisions[label] = average_precision, num_annotations\n\n    print(\'\\nmAP:\')\n    for label in range(generator.num_classes()):\n        label_name = generator.label_to_name(label)\n        print(\'{}: {}\'.format(label_name, average_precisions[label][0]))\n\n    return average_precisions\n'"
utils/util.py,0,"b""import pandas as pd\n\n\nclass MetricTracker:\n    def __init__(self, *keys, writer=None):\n        self.writer = writer\n        self._data = pd.DataFrame(\n            index=keys, columns=['total', 'counts', 'average'])\n        self.reset()\n\n    def reset(self):\n        for col in self._data.columns:\n            self._data[col].values[:] = 0\n\n    def update(self, key, value, n=1):\n        if self.writer is not None:\n            self.writer.add_scalar(key, value)\n        self._data.total[key] += value * n\n        self._data.counts[key] += n\n        self._data.average[key] = self._data.total[key] / \\\n            self._data.counts[key]\n\n    def avg(self, key):\n        return self._data.average[key]\n\n    def result(self):\n        return dict(self._data.average)\n"""
utils/vis_bbox.py,0,"b'import cv2\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef vis_bbox(img, bbox, label=None, score=None,\n             instance_colors=None, alpha=1., linewidth=2., ax=None):\n    """"""Visualize bounding boxes inside the image.\n    Args:\n        img (~numpy.ndarray): An array of shape :math:`(3, height, width)`.\n            This is in RGB format and the range of its value is\n            :math:`[0, 255]`. If this is :obj:`None`, no image is displayed.\n        bbox (~numpy.ndarray): An array of shape :math:`(R, 4)`, where\n            :math:`R` is the number of bounding boxes in the image.\n            Each element is organized\n            by :math:`(y_{min}, x_{min}, y_{max}, x_{max})` in the second axis.\n        label (~numpy.ndarray): An integer array of shape :math:`(R,)`.\n            The values correspond to id for label names stored in\n            :obj:`label_names`. This is optional.\n        score (~numpy.ndarray): A float array of shape :math:`(R,)`.\n             Each value indicates how confident the prediction is.\n             This is optional.\n        label_names (iterable of strings): Name of labels ordered according\n            to label ids. If this is :obj:`None`, labels will be skipped.\n        instance_colors (iterable of tuples): List of colors.\n            Each color is RGB format and the range of its values is\n            :math:`[0, 255]`. The :obj:`i`-th element is the color used\n            to visualize the :obj:`i`-th instance.\n            If :obj:`instance_colors` is :obj:`None`, the red is used for\n            all boxes.\n        alpha (float): The value which determines transparency of the\n            bounding boxes. The range of this value is :math:`[0, 1]`.\n        linewidth (float): The thickness of the edges of the bounding boxes.\n        ax (matplotlib.axes.Axis): The visualization is displayed on this\n            axis. If this is :obj:`None` (default), a new axis is created.\n    Returns:\n        ~matploblib.axes.Axes:\n        Returns the Axes object with the plot for further tweaking.\n    from: https://github.com/chainer/chainercv\n    """"""\n\n    if label is not None and not len(bbox) == len(label):\n        raise ValueError(\'The length of label must be same as that of bbox\')\n    if score is not None and not len(bbox) == len(score):\n        raise ValueError(\'The length of score must be same as that of bbox\')\n\n    # Returns newly instantiated matplotlib.axes.Axes object if ax is None\n    if ax is None:\n        fig = plt.figure()\n        # ax = fig.add_subplot(1, 1, 1)\n        h, w, _ = img.shape\n        w_ = w / 60.0\n        h_ = w_ * (h / w)\n        fig.set_size_inches((w_, h_))\n        ax = plt.axes([0, 0, 1, 1])\n    ax.imshow(img.astype(np.uint8))\n    ax.axis(\'off\')\n    # If there is no bounding box to display, visualize the image and exit.\n    if len(bbox) == 0:\n        return fig, ax\n\n    if instance_colors is None:\n        # Red\n        instance_colors = np.zeros((len(bbox), 3), dtype=np.float32)\n        instance_colors[:, 0] = 51\n        instance_colors[:, 1] = 51\n        instance_colors[:, 2] = 224\n    instance_colors = np.array(instance_colors)\n\n    for i, bb in enumerate(bbox):\n        xy = (bb[0], bb[1])\n        height = bb[3] - bb[1]\n        width = bb[2] - bb[0]\n        color = instance_colors[i % len(instance_colors)] / 255\n        ax.add_patch(plt.Rectangle(\n            xy, width, height, fill=False,\n            edgecolor=color, linewidth=linewidth, alpha=alpha))\n\n        caption = []\n        caption.append(label[i])\n        if(len(score) > 0):\n            sc = score[i]\n            caption.append(\'{}\'.format(sc))\n\n        if len(caption) > 0:\n            face_color = np.array([225, 51, 123])/255\n            ax.text(bb[0], bb[1],\n                    \': \'.join(caption),\n                    fontsize=12,\n                    color=\'black\',\n                    style=\'italic\',\n                    bbox={\'facecolor\': face_color, \'edgecolor\': face_color, \'alpha\': 1, \'pad\': 0})\n    return fig, ax\n\n\nif __name__ == \'__main__\':\n    img = cv2.imread(\'./../docs/output.png\')\n    print(\'img: \', img.shape)\n    img = np.array(img)\n    # img = img.convert(\'RGB\')\n    bbox = np.array([[50, 50, 200, 200]])\n    label = np.array([\'toan\'])\n    score = np.array([100])\n    ax, fig = vis_bbox(img=img,\n                       bbox=bbox,\n                       label=label,\n                       score=score,\n                       label_names=label_names\n                       )\n    fig.savefig(\'kaka.png\')\n    fig.show()\n    plt.show()\n'"
utils/visualization.py,2,"b'import importlib\nfrom datetime import datetime\n\n\nclass TensorboardWriter():\n    def __init__(self, log_dir, enabled):\n        self.writer = None\n        self.selected_module = """"\n\n        if enabled:\n            log_dir = str(log_dir)\n\n            # Retrieve vizualization writer.\n            succeeded = False\n            for module in [""torch.utils.tensorboard"", ""tensorboardX""]:\n                try:\n                    self.writer = importlib.import_module(\n                        module).SummaryWriter(log_dir)\n                    succeeded = True\n                    break\n                except ImportError:\n                    succeeded = False\n                self.selected_module = module\n\n            if not succeeded:\n                message = ""Warning: visualization (Tensorboard) is configured to use, but currently not installed on "" \\\n                    ""this machine. Please install TensorboardX with \'pip install tensorboardx\', upgrade PyTorch to "" \\\n                    ""version >= 1.1 to use \'torch.utils.tensorboard\' or turn off the option in the \'config.json\' file.""\n                print(message)\n\n        self.step = 0\n        self.mode = \'\'\n\n        self.tb_writer_ftns = {\n            \'add_scalar\', \'add_scalars\', \'add_image\', \'add_images\', \'add_audio\',\n            \'add_text\', \'add_histogram\', \'add_pr_curve\', \'add_embedding\', \'add_graph\'\n        }\n        self.tag_mode_exceptions = {\'add_histogram\', \'add_embedding\'}\n        self.timer = datetime.now()\n\n    def set_step(self, step, mode=\'train\'):\n        self.mode = mode\n        self.step = step\n        if step == 0:\n            self.timer = datetime.now()\n        else:\n            duration = datetime.now() - self.timer\n            self.add_scalar(\'steps_per_sec\', 1 / duration.total_seconds())\n            self.timer = datetime.now()\n\n    def __getattr__(self, name):\n        """"""\n        If visualization is configured to use:\n            return add_data() methods of tensorboard with additional information (step, tag) added.\n        Otherwise:\n            return a blank function handle that does nothing\n        """"""\n        if name in self.tb_writer_ftns:\n            add_data = getattr(self.writer, name, None)\n\n            def wrapper(tag, data, *args, **kwargs):\n                if add_data is not None:\n                    # add mode(train/valid) tag\n                    if name not in self.tag_mode_exceptions:\n                        tag = \'{}/{}\'.format(tag, self.mode)\n                    add_data(tag, data, self.step, *args, **kwargs)\n            return wrapper\n        else:\n            # default action for returning methods defined in this class, set_step() for instance.\n            try:\n                attr = object.__getattr__(name)\n            except AttributeError:\n                raise AttributeError(""type object \'{}\' has no attribute \'{}\'"".format(\n                    self.selected_module, name))\n            return attr\n'"
