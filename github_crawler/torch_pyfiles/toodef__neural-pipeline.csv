file_path,api_count,code
setup.py,0,"b'import setuptools\nimport neural_pipeline\n\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=""neural-pipeline"",\n    version=neural_pipeline.__version__,\n    author=""Anton Fedotov"",\n    author_email=""anton.fedotov.af@gmail.com"",\n    description=""Neural networks training pipeline based on PyTorch. Designed to standardize training process and to increase coding preformance"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/toodef/neural-pipeline"",\n    packages=setuptools.find_packages(exclude=[\'tests\']),\n    install_requires=[\'numpy\', \'tqdm\', \'torch>=0.4.1\'],\n    classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'Programming Language :: Python :: 3\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Science/Research\',\n        \'Operating System :: OS Independent\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Scientific/Engineering :: Image Recognition\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n)\n'"
neural_pipeline/__init__.py,0,"b""__version__ = '0.1.0'\n\nfrom .data_producer import *\nfrom .data_processor import *\nfrom .train_config import *\nfrom .utils import *\nfrom .monitoring import MonitorHub, AbstractMonitor, ConsoleMonitor\nfrom .train import Trainer\nfrom .predict import Predictor\n"""
neural_pipeline/gridsearch_train.py,1,"b'import os\nimport numpy as np\nimport torch\n\nfrom neural_pipeline import AbstractMetric\nfrom neural_pipeline.train_config.train_config import ComparableTrainConfig\nfrom neural_pipeline.utils.fsm import MultipleFSM\n\n\nclass GridSearchTrainer:\n    class MetricValAggregator:\n        def __init__(self, metric: AbstractMetric, method: str = \'min\'):\n            self._values = []\n            self._metric = metric\n\n            self._process_vals = None\n\n            if method == \'min\':\n                self._process_vals = self._calc_min\n            elif len(method) > 12 and method[:12] == \'calc_around_\':\n                self._process_vals = lambda: self._calc_around_min(int(method[:12]))\n            else:\n                raise NotImplementedError(""Methods for process metric must be \'min\' or \'calc_around_N\' where N is integer positive number"")\n\n        def update(self):\n            self._values.append(np.mean(self._metric.get_values()))\n\n        def _calc_min(self) -> float:\n            return self._values[np.argmin(self._values)]\n\n        def _calc_around_min(self, num_around: int) -> float:\n            min_idx = np.argmin(self._values)\n            num_back = min_idx - num_around\n            if num_back < 0:\n                num_back = 0\n            num_forward = min_idx + num_around\n            if num_forward > len(self._values) - 1:\n                num_forward = len(self._values) - 1\n            return np.mean(self._values[num_back: num_forward])\n\n        def get_val(self) -> float:\n            return self._process_vals()\n\n    def __init__(self, train_configs: [ComparableTrainConfig], workdir: str, device: torch.device = None, is_continue: bool = False):\n        self._train_configs = train_configs\n        self._device = device\n\n        self._workdir = workdir\n        self._state = {}\n\n        self._fsm = MultipleFSM(self._workdir, is_continue=is_continue)\n        self._init_monitor_clbks = []\n\n        self._epoch_num = 100\n\n        if is_continue:\n            self._load_state()\n\n    def _load_state(self) -> None:\n        """"""\n        Internal method for gridsearch state load\n\n        :return: self object\n        """"""\n        with open(self.__state_file_path(), \'r\') as file:\n            self._state = json.load(file)\n\n    def train(self):\n        if os.path.exists(self._workdir) or not os.path.isdir(self._workdir):\n            os.makedirs(self._workdir)\n\n        with open(self.__state_file_path(), \'w\') as file:\n            for i, comparable_train_config in enumerate(self._train_configs):\n                train_config_name = str(i)\n\n                if train_config_name in self._state:\n                    continue\n\n                print(""Train \'{}\'"".format(train_config_name))\n\n                self._fsm.set_namespace(train_config_name)\n                trainer = Trainer(comparable_train_config.get_train_config(), self._fsm, device=self._device)\n\n                for init_monitor in self._init_monitor_clbks:\n                    trainer.monitor_hub.add_monitor(init_monitor())\n                metric_aggregator = self.MetricValAggregator(comparable_train_config.get_metric_for_compare())\n                trainer.add_on_epoch_end_callback(metric_aggregator.update)\n                trainer.set_epoch_num(self._epoch_num)\n                trainer.train()\n\n                cur_state = {\'canceled\': True, \'params\': comparable_train_config.get_params(), \'metric_val\': metric_aggregator.get_val()}\n\n                self._state[train_config_name] = cur_state\n                json.dump(self._state, file)\n\n    def __state_file_path(self) -> str:\n        """"""\n        Internam method for compile state file path\n\n        :return: path\n        """"""\n        return os.path.join(self._workdir, \'gridsearch_trainer.json\')\n\n    def set_epoch_num(self, epoch_num: int) -> \'GridSearchTrainer\':\n        self._epoch_num = epoch_num\n        return self\n\n    def add_init_monitor_clbk(self, init_monitor_clbk: callable) -> \'GridSearchTrainer\':\n        self._init_monitor_clbks.append(init_monitor_clbk)\n        return self\n\n    def fsm(self) -> \'FileStructManager\':\n        return self._fsm\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        best_params = None\n        cur_best_metric = None\n        for exp_name, state in self._state.items():\n            if cur_best_metric is None or cur_best_metric < state[\'metric_val\']:\n                cur_best_metric = state[\'metric_val\']\n                best_params = state[\'params\']\n\n        print(""Best parameters:"", best_params)\n        print(""Best metric value:"", cur_best_metric)\n'"
neural_pipeline/monitoring.py,0,"b'""""""\nMain module for monitoring training process\n\nThere is:\n\n* :class:`MonitorHub` - monitors collection for connect all monitors to :class:`Trainer`\n* :class:`AbstractMonitor` - basic class for all monitors, that will be connected to :class:`MonitorHub`\n* :class:`ConsoleMonitor` - monitor, that used for write epoch results to console\n* :class:`LogMonitor` - monitor, used for metrics logging\n""""""\n\nimport json\nimport os\nfrom abc import ABCMeta\nimport numpy as np\n\nfrom neural_pipeline.train_config import MetricsGroup\nfrom neural_pipeline.utils import dict_recursive_bypass\nfrom neural_pipeline.utils.fsm import FileStructManager, FolderRegistrable\n\n__all__ = [\'MonitorHub\', \'AbstractMonitor\', \'ConsoleMonitor\', \'LogMonitor\']\n\n\nclass AbstractMonitor(metaclass=ABCMeta):\n    """"""\n    Basic class for every monitor.\n    """"""\n    def __init__(self):\n        self.epoch_num = 0\n\n    def set_epoch_num(self, epoch_num: int) -> None:\n        """"""\n        Set current epoch num\n\n        :param epoch_num: num of current epoch\n        """"""\n        self.epoch_num = epoch_num\n\n    def update_metrics(self, metrics: {}) -> None:\n        """"""\n        Update metrics on   monitor\n\n        :param metrics: metrics dict with keys \'metrics\' and \'groups\'\n        """"""\n        pass\n\n    def update_losses(self, losses: {}) -> None:\n        """"""\n        Update losses on monitor\n\n        :param losses: losses values dict with keys is names of stages in train pipeline (e.g. [train, validation])\n        """"""\n        pass\n\n    @staticmethod\n    def _iterate_by_losses(losses: {}, callback: callable) -> None:\n        """"""\n        Internal method for unify iteration by losses dict\n\n        :param losses: dic of losses\n        :param callback: callable, that call for every loss value and get params loss_name and loss_values: ``callback(name: str, values: np.ndarray)``\n        """"""\n        for m, v in losses.items():\n            callback(m, v)\n\n    def register_event(self, text: str) -> None:\n        pass\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\nclass ConsoleMonitor(AbstractMonitor):\n    """"""\n    Monitor, that used for write metrics to console.\n\n    Output looks like: ``Epoch: [#]; train: [-1, 0, 1]; validation: [-1, 0, 1]``. This 3 numbers is [min, mean, max] values of\n    training stage loss values\n    """"""\n    class ResStr:\n        def __init__(self, start: str):\n            self.res = start\n\n        def append(self, string: str):\n            self.res += string\n\n        def __str__(self):\n            return self.res[:len(self.res) - 1]\n\n    def update_losses(self, losses: {}) -> None:\n        def on_loss(name: str, values: np.ndarray, string) -> None:\n            string.append("" {}: [{:4f}, {:4f}, {:4f}];"".format(name, np.min(values), np.mean(values), np.max(values)))\n\n        res_string = self.ResStr(""Epoch: [{}];"".format(self.epoch_num))\n        self._iterate_by_losses(losses, lambda m, v: on_loss(m, v, res_string))\n        print(res_string)\n\n\nclass LogMonitor(AbstractMonitor, FolderRegistrable):\n    """"""\n    Monitor, used for logging metrics. It\'s write full log and can also write last metrics in separate file if required\n\n    All output files in JSON format and stores in ``<base_dir_path>/monitors/metrics_log``\n\n    :param fsm: :class:`FileStructManager` object\n    """"""\n    def __init__(self, fsm: FileStructManager):\n        super().__init__()\n\n        self._fsm = fsm\n        self._fsm.register_dir(self)\n        self._storage = {}\n        self._file = self._get_file_name(False)\n        self._final_metrics_file = None\n\n    def write_final_metrics(self, path: str = None) -> \'LogMonitor\':\n        """"""\n        Enable saving final metrics to separate file\n\n        :param path: path to result file. If not defined, file will placed near full metrics log and named \'metrics.json`\n        :return: self object\n        """"""\n        if path is not None:\n            self._final_metrics_file = path\n        else:\n            self._final_metrics_file = self._get_final_file_name(False)\n        return self\n\n    def get_final_metrics_file(self) -> str or None:\n        """"""\n        Get final metrics file path\n\n        :return: path or None if writing doesn\'t enabled by :meth:`write_final_metrics`\n        """"""\n        return self._final_metrics_file\n\n    def update_metrics(self, metrics: {}) -> None:\n        for metric in metrics[\'metrics\']:\n            self._process_metric(metric)\n\n        for metrics_group in metrics[\'groups\']:\n            for metric in metrics_group.metrics():\n                self._process_metric(metric, metrics_group.name())\n            for group in metrics_group.groups():\n                self._process_metric(group, metrics_group.name())\n\n    def update_losses(self, losses: {}) -> None:\n        def on_loss(name: str, values: np.ndarray):\n            store = self._cur_storage([name, \'loss\'])\n            store.append(float(np.mean(values)))\n\n        self._iterate_by_losses(losses, on_loss)\n\n    def _process_metric(self, cur_metric, parent_tag: str = None) -> None:\n        """"""\n        Internal method for processing metrics or metrics groups\n\n        :param cur_metric: :class:`AbstractMetric` or :class:`MetricsGroup` object\n        :param parent_tag: parent tag for place metric in storage\n        """"""\n        if isinstance(cur_metric, MetricsGroup):\n            for m in cur_metric.metrics():\n                if m.get_values().size > 0:\n                    store = self._cur_storage([parent_tag, cur_metric.name(), m.name()])\n                    store.append(float(np.mean(m.get_values())))\n        else:\n            values = cur_metric.get_values().astype(np.float32)\n            if values.size > 0:\n                store = self._cur_storage([parent_tag, cur_metric.name()])\n                store.append(float(np.mean(values)))\n\n    def _flush_metrics(self) -> None:\n        """"""\n        Flush metrics files\n        """"""\n        with open(self._get_file_name(True), \'w\') as out:\n            json.dump(self._storage, out)\n\n        if self._final_metrics_file is not None:\n            res = dict_recursive_bypass(self._storage, lambda v: v[-1])\n            with open(self._final_metrics_file, \'w\') as out:\n                json.dump(res, out)\n\n    def _cur_storage(self, names: [str]) -> [] or {}:\n        """"""\n        Get current substorage by path of names\n        :param names: list on names (path to target substorage)\n        :return: substorage\n        """"""\n        res = self._storage\n        for i, n in enumerate(names):\n            if n is None:\n                continue\n            if n not in res:\n                res[n] = {} if i < (len(names) - 1) else []\n            res = res[n]\n        return res\n\n    def close(self) -> None:\n        """"""\n        Close monitor\n        """"""\n        self._flush_metrics()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def _get_file_name(self, create: bool) -> str:\n        return os.path.join(self._fsm.get_path(self, create), \'metrics_log.json\')\n\n    def _get_final_file_name(self, create: bool) -> str:\n        return os.path.join(self._fsm.get_path(self, create), \'metrics.json\')\n\n    def _get_gir(self) -> str:\n        return os.path.join(\'monitors\', \'metrics_log\')\n\n    def _get_name(self) -> str:\n        return \'LogMonitor\'\n\n\nclass MonitorHub:\n    """"""\n    Aggregator of monitors. This class collect monitors and provide unified interface to it\'s\n    """"""\n    def __init__(self):\n        self.monitors = []\n\n    def set_epoch_num(self, epoch_num: int) -> None:\n        """"""\n        Set current epoch num\n\n        :param epoch_num: num of current epoch\n        """"""\n        for m in self.monitors:\n            m.set_epoch_num(epoch_num)\n\n    def add_monitor(self, monitor: AbstractMonitor) -> \'MonitorHub\':\n        """"""\n        Connect monitor to hub\n\n        :param monitor: :class:`AbstractMonitor` object\n        :return:\n        """"""\n        self.monitors.append(monitor)\n        return self\n\n    def update_metrics(self, metrics: {}) -> None:\n        """"""\n        Update metrics in all monitors\n\n        :param metrics: metrics dict with keys \'metrics\' and \'groups\'\n        """"""\n        for m in self.monitors:\n            m.update_metrics(metrics)\n\n    def update_losses(self, losses: {}) -> None:\n        """"""\n        Update monitor\n\n        :param losses: losses values with keys \'train\' and \'validation\'\n        """"""\n        for m in self.monitors:\n            m.update_losses(losses)\n\n    def register_event(self, text: str) -> None:\n        for m in self.monitors:\n            m.register_event(text)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        for m in self.monitors:\n            m.__exit__(exc_type, exc_val, exc_tb)\n'"
neural_pipeline/predict.py,2,"b'""""""\nThe main module for run inference\n""""""\nfrom abc import ABCMeta\n\nfrom tqdm import tqdm\nimport torch\n\nfrom neural_pipeline.utils import CheckpointsManager\nfrom neural_pipeline.data_producer.data_producer import DataProducer\nfrom neural_pipeline.data_processor import Model\nfrom neural_pipeline.utils.fsm import FileStructManager\nfrom neural_pipeline.data_processor.data_processor import DataProcessor\n\n__all__ = [\'Predictor\', \'DataProducerPredictor\']\n\n\nclass BasePredictor(metaclass=ABCMeta):\n    def __init__(self, model: Model, fsm: FileStructManager, from_best_state: bool = False):\n        self._fsm = fsm\n        self._data_processor = DataProcessor(model)\n        checkpoint_manager = CheckpointsManager(self._fsm, prefix=\'best\' if from_best_state else None)\n        self._data_processor.set_checkpoints_manager(checkpoint_manager)\n        checkpoint_manager.unpack()\n        self._data_processor.load()\n        checkpoint_manager.pack()\n\n\nclass Predictor(BasePredictor):\n    """"""\n    Predictor run inference by training parameters\n\n    :param model: model object, used for predict\n    :param fsm: :class:`FileStructManager` object\n    """"""\n\n    def __init__(self, model: Model, fsm: FileStructManager, from_best_state: bool = False):\n        super().__init__(model, fsm, from_best_state=from_best_state)\n\n    def predict(self, data: torch.Tensor or dict):\n        """"""\n        Predict ine data\n\n        :param data: data as :class:`torch.Tensor` or dict with key ``data``\n        :return: processed output\n        :rtype: model output type\n        """"""\n        return self._data_processor.predict(data)\n\n\nclass DataProducerPredictor(BasePredictor):\n    def __init__(self, model: Model, fsm: FileStructManager, from_best_state: bool = False):\n        super().__init__(model, fsm, from_best_state=from_best_state)\n\n    def predict(self, data_producer: DataProducer, callback: callable) -> None:\n        """"""\n        Run prediction iterates by ``data_producer``\n\n        :param data_producer: :class:`DataProducer` object\n        :param callback: callback, that call for every data prediction and get it\'s result as parameter\n        """"""\n        loader = data_producer.get_loader()\n\n        for img in tqdm(loader):\n            callback(self._data_processor.predict(img))\n            del img\n'"
neural_pipeline/train.py,2,"b'""""""\nThe main module for training process\n""""""\nimport json\nimport os\nimport numpy as np\nimport torch\nfrom torch.nn import Module\n\nfrom neural_pipeline import AbstractMetric\n\nfrom neural_pipeline.data_processor import TrainDataProcessor\nfrom neural_pipeline.utils import FileStructManager, CheckpointsManager\nfrom neural_pipeline.train_config.train_config import TrainConfig, ComparableTrainConfig\nfrom neural_pipeline.monitoring import MonitorHub, ConsoleMonitor\nfrom neural_pipeline.utils.fsm import MultipleFSM\n\n__all__ = [\'Trainer\', \'GridSearchTrainer\']\n\n\nclass LearningRate:\n    """"""\n    Basic learning rate class\n    """"""\n\n    def __init__(self, value: float):\n        self._value = value\n\n    def value(self) -> float:\n        """"""\n        Get value of current learning rate\n\n        :return: current value\n        """"""\n        return self._value\n\n    def set_value(self, value) -> None:\n        """"""\n        Set lr value\n\n        :param value: lr value\n        """"""\n        self._value = value\n\n\nclass DecayingLR(LearningRate):\n    """"""\n    This class provide lr decaying by defined metric value (by :arg:`target_value_clbk`).\n    If metric value doesn\'t update minimum after defined number of steps (:arg:`patience`) - lr was decaying\n    by defined coefficient (:arg:`decay_coefficient`).\n\n    :param start_value: start value\n    :param decay_coefficient: coefficient of decaying\n    :param patience: steps before decay\n    :param target_value_clbk: callable, that return target value for lr decaying\n    """"""\n\n    def __init__(self, start_value: float, decay_coefficient: float, patience: int, target_value_clbk: callable):\n        super().__init__(start_value)\n\n        self._decay_coefficient = decay_coefficient\n        self._patience = patience\n        self._cur_step = 1\n        self._target_value_clbk = target_value_clbk\n        self._cur_min_target_val = None\n\n    def value(self) -> float:\n        """"""\n        Get value of current learning rate\n\n        :return: learning rate value\n        """"""\n        metric_val = self._target_value_clbk()\n        if metric_val is None:\n            return self._value\n\n        if self._cur_min_target_val is None:\n            self._cur_min_target_val = metric_val\n\n        if metric_val < self._cur_min_target_val:\n            self._cur_step = 1\n            self._cur_min_target_val = metric_val\n\n        if self._cur_step > 0 and (self._cur_step % self._patience) == 0:\n            self._value *= self._decay_coefficient\n            self._cur_min_target_val = None\n            self._cur_step = 1\n            return self._value\n\n        self._cur_step += 1\n        return self._value\n\n    def set_value(self, value):\n        self._value = value\n        self._cur_step = 0\n        self._cur_min_target_val = None\n\n\nclass Trainer:\n    """"""\n    Class, that run drive process.\n\n    Trainer get list of training stages and every epoch loop over it.\n\n    Training process looks like:\n\n    .. highlight:: python\n    .. code-block:: python\n\n        for epoch in epochs_num:\n            for stage in training_stages:\n                stage.run()\n                monitor_hub.update_metrics(stage.metrics_processor().get_metrics())\n            save_state()\n            on_epoch_end_callback()\n\n    :param train_config: :class:`TrainConfig` object\n    :param fsm: :class:`FileStructManager` object\n    :param device: device for training process\n    """"""\n\n    class TrainerException(Exception):\n        def __init__(self, msg):\n            super().__init__()\n            self._msg = msg\n\n        def __str__(self):\n            return self._msg\n\n    def __init__(self, train_config: TrainConfig, fsm: FileStructManager, device: torch.device = None):\n        self._fsm = fsm\n        self.monitor_hub = MonitorHub()\n\n        self._checkpoint_manager = CheckpointsManager(self._fsm)\n\n        self.__epoch_num = 100\n        self._resume_from = None\n        self._on_epoch_end = []\n        self._best_state_rule = None\n\n        self._train_config = train_config\n        self._data_processor = TrainDataProcessor(self._train_config, device).set_checkpoints_manager(self._checkpoint_manager)\n        self._lr = LearningRate(self._data_processor.get_lr())\n\n        self._stop_rules = []\n\n    def set_epoch_num(self, epoch_number: int) -> \'Trainer\':\n        """"""\n        Define number of epoch for training. One epoch - one iteration over all train stages\n\n        :param epoch_number: number of training epoch\n        :return: self object\n        """"""\n        self.__epoch_num = epoch_number\n        return self\n\n    def resume(self, from_best_checkpoint: bool) -> \'Trainer\':\n        """"""\n        Resume train from last checkpoint\n\n        :param from_best_checkpoint: is need to continue from best checkpoint\n        :return: self object\n        """"""\n        self._resume_from = \'last\' if from_best_checkpoint is False else \'best\'\n        return self\n\n    def enable_lr_decaying(self, coeff: float, patience: int, target_val_clbk: callable) -> \'Trainer\':\n        """"""\n        Enable rearing rate decaying. Learning rate decay when `target_val_clbk` returns doesn\'t update\n        minimum for `patience` steps\n\n        :param coeff: lr decay coefficient\n        :param patience: number of steps\n        :param target_val_clbk: callback which returns the value that is used for lr decaying\n        :return: self object\n        """"""\n        self._lr = DecayingLR(self._data_processor.get_lr(), coeff, patience, target_val_clbk)\n        return self\n\n    def train(self) -> None:\n        """"""\n        Run training process\n        """"""\n        if len(self._train_config.stages()) < 1:\n            raise self.TrainerException(""There\'s no sages for training"")\n\n        best_checkpoints_manager = None\n        cur_best_state = None\n        if self._best_state_rule is not None:\n            best_checkpoints_manager = CheckpointsManager(self._fsm, \'best\')\n\n        start_epoch_idx = 1\n        if self._resume_from is not None:\n            start_epoch_idx += self._resume()\n        self.monitor_hub.add_monitor(ConsoleMonitor())\n\n        with self.monitor_hub:\n            for epoch_idx in range(start_epoch_idx, self.__epoch_num + start_epoch_idx):\n                if True in [stop_rule() for stop_rule in self._stop_rules]:\n                    break\n\n                self.monitor_hub.set_epoch_num(epoch_idx)\n                for stage in self._train_config.stages():\n                    stage.run(self._data_processor)\n\n                    if stage.metrics_processor() is not None:\n                        self.monitor_hub.update_metrics(stage.metrics_processor().get_metrics())\n\n                new_best_state = self._save_state(self._checkpoint_manager, best_checkpoints_manager, cur_best_state, epoch_idx)\n                if new_best_state is not None:\n                    cur_best_state = new_best_state\n\n                self._data_processor.update_lr(self._lr.value())\n\n                for clbk in self._on_epoch_end:\n                    clbk()\n\n                self._update_losses()\n                self.__iterate_by_stages(lambda s: s.on_epoch_end())\n\n    def _resume(self) -> int:\n        if self._resume_from == \'last\':\n            ckpts_manager = self._checkpoint_manager\n        elif self._checkpoint_manager == \'best\':\n            ckpts_manager = CheckpointsManager(self._fsm, \'best\')\n        else:\n            raise NotImplementedError(""Resume parameter may be only \'last\' or \'best\' not {}"".format(self._resume_from))\n        ckpts_manager.unpack()\n        self._data_processor.load()\n\n        with open(ckpts_manager.trainer_file(), \'r\') as file:\n            start_epoch_idx = json.load(file)[\'last_epoch\'] + 1\n\n        ckpts_manager.pack()\n        return start_epoch_idx\n\n    def _save_state(self, ckpts_manager: CheckpointsManager, best_ckpts_manager: CheckpointsManager or None,\n                    cur_best_state: float or None, epoch_idx: int) -> float or None:\n        """"""\n        Internal method used for save states after epoch end\n\n        :param ckpts_manager: ordinal checkpoints manager\n        :param best_ckpts_manager: checkpoints manager, used for store best stages\n        :param cur_best_state: current best stage metric value\n        :return: new best stage metric value or None if it not update\n        """"""\n        def save_trainer(ckp_manager):\n            with open(ckp_manager.trainer_file(), \'w\') as out:\n                json.dump({\'last_epoch\': epoch_idx}, out)\n\n        if self._best_state_rule is not None:\n            new_best_state = self._best_state_rule()\n            if cur_best_state is None:\n                self._data_processor.save_state()\n                save_trainer(ckpts_manager)\n                ckpts_manager.pack()\n                return new_best_state\n            else:\n                if new_best_state <= cur_best_state:\n                    self._data_processor.set_checkpoints_manager(best_ckpts_manager)\n                    self._data_processor.save_state()\n                    save_trainer(best_ckpts_manager)\n                    best_ckpts_manager.pack()\n                    self._data_processor.set_checkpoints_manager(ckpts_manager)\n                    return new_best_state\n\n        self._data_processor.save_state()\n        save_trainer(ckpts_manager)\n        ckpts_manager.pack()\n        return None\n\n    def _update_losses(self) -> None:\n        """"""\n        Update loses procedure\n        """"""\n        losses = {}\n        for stage in self._train_config.stages():\n            if stage.get_losses() is not None:\n                losses[stage.name()] = stage.get_losses()\n        self.monitor_hub.update_losses(losses)\n\n    def data_processor(self) -> TrainDataProcessor:\n        """"""\n        Get data processor object\n\n        :return: data processor\n        """"""\n        return self._data_processor\n\n    def enable_best_states_saving(self, rule: callable) -> \'Trainer\':\n        """"""\n        Enable best states saving\n\n        Best stages will save when return of `rule` update minimum\n\n        :param rule: callback which returns the value that is used for define when need store best metric\n        :return: self object\n        """"""\n        self._best_state_rule = rule\n        return self\n\n    def disable_best_states_saving(self) -> \'Trainer\':\n        """"""\n        Enable best states saving\n\n        :return: self object\n        """"""\n        self._best_state_rule = None\n        return self\n\n    def add_on_epoch_end_callback(self, callback: callable) -> \'Trainer\':\n        """"""\n        Add callback, that will be called after every epoch end\n\n        :param callback: method, that will be called. This method may not get any parameters\n        :return: self object\n        """"""\n        self._on_epoch_end.append(callback)\n        return self\n\n    def add_stop_rule(self, rule: callable) -> \'Trainer\':\n        """"""\n        Add the rule that control training process interruption\n\n        Params:\n            rule (callable): callable, that doesn\'t get params and return boolean. When one of rules returns `True` training loop will be interrupted\n\n        Returns:\n            self object\n\n        Examples:\n\n        .. highlight:: python\n        .. code-block:: python\n\n            trainer.add_stop_rule(lambda: trainer.data_processor().get_lr() < 1e-6)\n        """"""\n        self._stop_rules.append(rule)\n        return self\n\n    def train_config(self) -> TrainConfig:\n        """"""\n        Get train config\n\n        :return: TrainConfig object\n        """"""\n        return self._train_config\n\n    def __iterate_by_stages(self, func: callable) -> None:\n        """"""\n        Internal method, that used for iterate by stages\n\n        :param func: callback, that calls for every stage\n        """"""\n        for stage in self._train_config.stages():\n            func(stage)\n'"
tests/__init__.py,0,b''
tests/common.py,1,"b""import os\nimport shutil\nimport unittest\n\nfrom torch import Tensor\nfrom torch.nn import functional as F\nimport numpy as np\n\nfrom neural_pipeline.train_config import AbstractMetric\n\n__all__ = ['UseFileStructure', 'data_remove', 'SimpleMetric']\n\n\nclass UseFileStructure(unittest.TestCase):\n    base_dir = 'data'\n    monitors_dir = 'monitors'\n    checkpoints_dir = 'checkpoints_dir'\n\n    def tearDown(self):\n        if os.path.exists(self.base_dir):\n            shutil.rmtree(self.base_dir, ignore_errors=True)\n\n\ndef data_remove(func: callable) -> callable:\n    def res(*args, **kwargs):\n        ret = func(*args, **kwargs)\n        UseFileStructure().tearDown()\n        return ret\n\n    return res\n\n\nclass SimpleMetric(AbstractMetric):\n    def __init__(self, name: str = None, coeff: float = 1):\n        super().__init__('SimpleMetric' if name is None else name)\n        self._coeff = coeff\n\n    def calc(self, output: Tensor, target: Tensor) -> np.ndarray or float:\n        return F.pairwise_distance(output, target, p=2).numpy() * self._coeff\n"""
tests/data_processor_test.py,47,"b'import os\nimport shutil\nimport unittest\n\nimport torch\nimport numpy as np\n\nfrom neural_pipeline.data_processor import DataProcessor, TrainDataProcessor, Model\nfrom neural_pipeline.utils import FileStructManager, dict_pair_recursive_bypass, CheckpointsManager\nfrom neural_pipeline.train_config import TrainConfig\nfrom tests.common import UseFileStructure, data_remove\n\n__all__ = [\'ModelTest\', \'DataProcessorTest\', \'TrainDataProcessorTest\']\n\n\nclass SimpleModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(3, 1)\n\n    def forward(self, x):\n        return self.fc(x)\n\n    @staticmethod\n    def dummy_input():\n        return torch.rand(3)\n\n\ndef compare_two_models(unittest_obj: unittest.TestCase, model1: torch.nn.Module, model2: torch.nn.Module):\n    def on_node(n1, n2):\n        if n1.device == torch.device(\'cuda:0\'):\n            n1 = n1.to(\'cpu\')\n        if n2.device == torch.device(\'cuda:0\'):\n            n2 = n2.to(\'cpu\')\n        unittest_obj.assertTrue(np.array_equal(n1.numpy(), n2.numpy()))\n\n    state_dict1 = model1.state_dict().copy()\n    state_dict2 = model2.state_dict().copy()\n\n    dict_pair_recursive_bypass(state_dict1, state_dict2, on_node)\n\n\nclass NonStandardIOModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(3, 1)\n\n    def forward(self, x):\n        res1 = self.fc(x[\'data1\'])\n        res2 = self.fc(x[\'data2\'])\n        return {\'res1\': res1, \'res2\': res2}\n\n\nclass ModelTest(UseFileStructure):\n    def test_init(self):\n        try:\n            Model(SimpleModel())\n        except:\n            self.fail(\'Model initialization fail\')\n\n    def test_data_parallel(self):\n        try:\n            m = Model(torch.nn.DataParallel(SimpleModel()))\n            m(SimpleModel.dummy_input())\n        except:\n            self.fail(\'Model DataParallel pass fail\')\n\n        if torch.cuda.is_available():\n            try:\n                device = torch.device(\'cuda\')\n                m = Model(torch.nn.DataParallel(SimpleModel())).to_device(device)\n                m(SimpleModel.dummy_input().to(device))\n            except:\n                self.fail(\'Model DataParallel on gpu fail\')\n\n    def test_saving_and_loading(self):\n        m = Model(SimpleModel())\n        m.save_weights(\'weights.pth\')\n        m_new = Model(SimpleModel())\n        m_new.load_weights(\'weights.pth\')\n        compare_two_models(self, m.model(), m_new.model())\n        os.remove(\'weights.pth\')\n\n        m = SimpleModel()\n        Model(torch.nn.DataParallel(m)).save_weights(\'weights.pth\')\n        m_new = Model(SimpleModel())\n        m_new.load_weights(\'weights.pth\')\n        compare_two_models(self, m, m_new.model())\n        os.remove(\'weights.pth\')\n\n        m = SimpleModel()\n        Model(m).save_weights(\'weights.pth\')\n        m_new = SimpleModel()\n        Model(torch.nn.DataParallel(m_new)).load_weights(\'weights.pth\')\n        compare_two_models(self, m, m_new)\n        os.remove(\'weights.pth\')\n\n        m = SimpleModel()\n        Model(torch.nn.DataParallel(m)).save_weights(\'weights.pth\')\n        m_new = SimpleModel()\n        Model(torch.nn.DataParallel(m_new)).load_weights(\'weights.pth\')\n        compare_two_models(self, m, m_new)\n        os.remove(\'weights.pth\')\n\n\nclass DataProcessorTest(UseFileStructure):\n    def test_initialisation(self):\n        try:\n            DataProcessor(model=SimpleModel())\n        except:\n            self.fail(\'DataProcessor initialisation raises exception\')\n\n    def test_prediction_output(self):\n        model = SimpleModel()\n        dp = DataProcessor(model=model)\n        self.assertFalse(model.fc.weight.is_cuda)\n        res = dp.predict({\'data\': torch.rand(1, 3)})\n        self.assertIs(type(res), torch.Tensor)\n\n        model = NonStandardIOModel()\n        dp = DataProcessor(model=model)\n        self.assertFalse(model.fc.weight.is_cuda)\n        res = dp.predict({\'data\': {\'data1\': torch.rand(1, 3), \'data2\': torch.rand(1, 3)}})\n        self.assertIs(type(res), dict)\n        self.assertIn(\'res1\', res)\n        self.assertIs(type(res[\'res1\']), torch.Tensor)\n        self.assertIn(\'res2\', res)\n        self.assertIs(type(res[\'res2\']), torch.Tensor)\n\n    def test_predict(self):\n        model = SimpleModel().train()\n        dp = DataProcessor(model=model)\n        self.assertFalse(model.fc.weight.is_cuda)\n        self.assertTrue(model.training)\n        res = dp.predict({\'data\': torch.rand(1, 3)})\n        self.assertFalse(model.training)\n        self.assertFalse(res.requires_grad)\n        self.assertIsNone(res.grad)\n\n    @data_remove\n    def test_continue_from_checkpoint(self):\n        def on_node(n1, n2):\n            self.assertTrue(np.array_equal(n1.numpy(), n2.numpy()))\n\n        model = SimpleModel().train()\n        dp = DataProcessor(model=model)\n        with self.assertRaises(Model.ModelException):\n            dp.save_state()\n        try:\n            fsm = FileStructManager(self.base_dir, is_continue=False)\n            dp.set_checkpoints_manager(CheckpointsManager(fsm))\n            dp.save_state()\n        except:\n            self.fail(\'Fail to DataProcessor load when CheckpointsManager was defined\')\n\n        del dp\n\n        model_new = SimpleModel().train()\n        dp = DataProcessor(model=model_new)\n\n        with self.assertRaises(Model.ModelException):\n            dp.load()\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=True)\n        dp.set_checkpoints_manager(CheckpointsManager(fsm))\n        try:\n            fsm = FileStructManager(self.base_dir, is_continue=True)\n            dp.set_checkpoints_manager(CheckpointsManager(fsm))\n            dp.load()\n        except:\n            self.fail(\'Fail to DataProcessor load when CheckpointsManager was defined\')\n\n        compare_two_models(self, model, model_new)\n\n\nclass SimpleLoss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.module = torch.nn.Parameter(torch.Tensor([1]), requires_grad=True)\n        self.res = None\n\n    def forward(self, predict, target):\n        self.res = self.module * (predict - target)\n        return self.res\n\n\nclass TrainDataProcessorTest(UseFileStructure):\n    def test_initialisation(self):\n        model = SimpleModel()\n        train_config = TrainConfig(model, [], torch.nn.Module(), torch.optim.SGD(model.parameters(), lr=0.1))\n        try:\n            TrainDataProcessor(train_config=train_config)\n        except:\n            self.fail(\'DataProcessor initialisation raises exception\')\n\n    def test_prediction_train_output(self):\n        model = SimpleModel()\n        train_config = TrainConfig(model, [], torch.nn.Module(), torch.optim.SGD(model.parameters(), lr=0.1))\n        dp = TrainDataProcessor(train_config=train_config)\n        self.assertFalse(model.fc.weight.is_cuda)\n        res = dp.predict({\'data\': torch.rand(1, 3)}, is_train=True)\n        self.assertIs(type(res), torch.Tensor)\n\n        model = NonStandardIOModel()\n        train_config = TrainConfig(model, [], torch.nn.Module(), torch.optim.SGD(model.parameters(), lr=0.1))\n        dp = TrainDataProcessor(train_config=train_config)\n        self.assertFalse(model.fc.weight.is_cuda)\n        res = dp.predict({\'data\': {\'data1\': torch.rand(1, 3), \'data2\': torch.rand(1, 3)}}, is_train=True)\n        self.assertIs(type(res), dict)\n        self.assertIn(\'res1\', res)\n        self.assertIs(type(res[\'res1\']), torch.Tensor)\n        self.assertIn(\'res2\', res)\n        self.assertIs(type(res[\'res2\']), torch.Tensor)\n\n        self.assertTrue(model.training)\n        self.assertFalse(res[\'res1\'].requires_grad)\n        self.assertIsNone(res[\'res1\'].grad)\n        self.assertFalse(res[\'res2\'].requires_grad)\n        self.assertIsNone(res[\'res2\'].grad)\n\n    def test_prediction_notrain_output(self):\n        model = SimpleModel()\n        train_config = TrainConfig(model, [], torch.nn.Module(), torch.optim.SGD(model.parameters(), lr=0.1))\n        dp = TrainDataProcessor(train_config=train_config)\n        self.assertFalse(model.fc.weight.is_cuda)\n        res = dp.predict({\'data\': torch.rand(1, 3)}, is_train=False)\n        self.assertIs(type(res), torch.Tensor)\n\n        model = NonStandardIOModel()\n        train_config = TrainConfig(model, [], torch.nn.Module(), torch.optim.SGD(model.parameters(), lr=0.1))\n        dp = TrainDataProcessor(train_config=train_config)\n        self.assertFalse(model.fc.weight.is_cuda)\n        res = dp.predict({\'data\': {\'data1\': torch.rand(1, 3), \'data2\': torch.rand(1, 3)}}, is_train=False)\n        self.assertIs(type(res), dict)\n        self.assertIn(\'res1\', res)\n        self.assertIs(type(res[\'res1\']), torch.Tensor)\n        self.assertIn(\'res2\', res)\n        self.assertIs(type(res[\'res2\']), torch.Tensor)\n\n        self.assertFalse(model.training)\n        self.assertFalse(res[\'res1\'].requires_grad)\n        self.assertIsNone(res[\'res1\'].grad)\n        self.assertFalse(res[\'res2\'].requires_grad)\n        self.assertIsNone(res[\'res2\'].grad)\n\n    def test_predict(self):\n        model = SimpleModel().train()\n        train_config = TrainConfig(model, [], torch.nn.Module(), torch.optim.SGD(model.parameters(), lr=0.1))\n        dp = TrainDataProcessor(train_config=train_config)\n        self.assertFalse(model.fc.weight.is_cuda)\n        self.assertTrue(model.training)\n        res = dp.predict({\'data\': torch.rand(1, 3)})\n        self.assertFalse(model.training)\n        self.assertFalse(res.requires_grad)\n        self.assertIsNone(res.grad)\n\n    def test_train(self):\n        model = SimpleModel().train()\n        train_config = TrainConfig(model, [], torch.nn.Module(), torch.optim.SGD(model.parameters(), lr=0.1))\n        dp = TrainDataProcessor(train_config=train_config)\n\n        self.assertFalse(model.fc.weight.is_cuda)\n        self.assertTrue(model.training)\n        res = dp.predict({\'data\': torch.rand(1, 3)}, is_train=True)\n        self.assertTrue(model.training)\n        self.assertTrue(res.requires_grad)\n        self.assertIsNone(res.grad)\n\n        with self.assertRaises(NotImplementedError):\n            dp.process_batch({\'data\': torch.rand(1, 3), \'target\': torch.rand(1)}, is_train=True)\n\n        loss = SimpleLoss()\n        train_config = TrainConfig(model, [], loss, torch.optim.SGD(model.parameters(), lr=0.1))\n        dp = TrainDataProcessor(train_config=train_config)\n        res = dp.process_batch({\'data\': torch.rand(1, 3), \'target\': torch.rand(1)}, is_train=True)\n        self.assertTrue(model.training)\n        self.assertTrue(loss.module.requires_grad)\n        self.assertIsNotNone(loss.module.grad)\n        self.assertTrue(np.array_equal(res, loss.res.data.numpy()))\n\n    @data_remove\n    def test_continue_from_checkpoint(self):\n        def on_node(n1, n2):\n            self.assertTrue(np.array_equal(n1.numpy(), n2.numpy()))\n\n        model = SimpleModel().train()\n        loss = SimpleLoss()\n\n        for optim in [torch.optim.SGD(model.parameters(), lr=0.1), torch.optim.Adam(model.parameters(), lr=0.1)]:\n            train_config = TrainConfig(model, [], loss, optim)\n\n            dp_before = TrainDataProcessor(train_config=train_config)\n            before_state_dict = model.state_dict().copy()\n            dp_before.update_lr(0.023)\n\n            with self.assertRaises(Model.ModelException):\n                dp_before.save_state()\n            try:\n                fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n                dp_before.set_checkpoints_manager(CheckpointsManager(fsm))\n                dp_before.save_state()\n            except:\n                self.fail(""Exception on saving state when \'CheckpointsManager\' specified"")\n\n            fsm = FileStructManager(base_dir=self.base_dir, is_continue=True)\n            dp_after = TrainDataProcessor(train_config=train_config)\n            with self.assertRaises(Model.ModelException):\n                dp_after.load()\n            try:\n                cm = CheckpointsManager(fsm)\n                dp_after.set_checkpoints_manager(cm)\n                dp_after.load()\n            except:\n                self.fail(\'DataProcessor initialisation raises exception\')\n\n            after_state_dict = model.state_dict().copy()\n\n            dict_pair_recursive_bypass(before_state_dict, after_state_dict, on_node)\n            self.assertEqual(dp_before.get_lr(), dp_after.get_lr())\n\n            shutil.rmtree(self.base_dir)\n'"
tests/data_producer_test.py,1,"b""import unittest\nfrom random import randint\n\nfrom torch.utils.data import DataLoader\n\nfrom neural_pipeline.data_producer.data_producer import AbstractDataset, DataProducer\n\n__all__ = ['DataProducerTest']\n\n\nclass SampleDataset(AbstractDataset):\n    def __init__(self, numbers):\n        self.__numbers = numbers\n\n    def __len__(self):\n        return len(self.__numbers)\n\n    def __getitem__(self, item):\n        return self.__numbers[item]\n\n\nclass TestDataProducer(DataProducer):\n    def __init__(self, datasets: [AbstractDataset]):\n        super().__init__(datasets)\n\n\nclass DataProducerTest(unittest.TestCase):\n    def test_simple_getitem(self):\n        numbers = list(range(20))\n        dataset1 = SampleDataset(numbers[:4])\n        self.assertEqual(len(dataset1), 4)\n        dataset2 = SampleDataset(numbers[4:])\n        self.assertEqual(len(dataset2), 16)\n\n        for i, n in enumerate(numbers):\n            if i < len(dataset1):\n                self.assertEqual(dataset1[i], n)\n            else:\n                self.assertEqual(dataset2[i - len(dataset1)], n)\n\n        data_producer = TestDataProducer([dataset1, dataset2])\n\n        for i, n in enumerate(numbers):\n            self.assertEqual(data_producer[i], n)\n\n    def test_global_shuffle(self):\n        data_producer = DataProducer([list(range(10)), list(range(10, 20))])\n\n        prev_data = None\n        for data in data_producer:\n            if prev_data is None:\n                prev_data = data\n                continue\n            self.assertEqual(data, prev_data + 1)\n            prev_data = data\n\n        data_producer.global_shuffle(True)\n        prev_data = None\n        shuffled, non_shuffled = 1, 1\n        for data in data_producer:\n            if prev_data is None:\n                prev_data = data\n                continue\n            if prev_data + 1 == data:\n                non_shuffled += 1\n            else:\n                shuffled += 1\n            prev_data = data\n\n        self.assertEqual(non_shuffled, len(data_producer))\n\n        prev_data = None\n        shuffled, non_shuffled = 0, 0\n        loader = data_producer.get_loader()\n        for data in loader:\n            if prev_data is None:\n                prev_data = data\n                continue\n            if prev_data + 1 == data:\n                non_shuffled += 1\n            else:\n                shuffled += 1\n            prev_data = data\n\n        self.assertGreater(shuffled, non_shuffled)\n\n    def test_get_loader(self):\n        data_producer = DataProducer([list(range(1))])\n        self.assertIs(type(data_producer.get_loader()), DataLoader)\n        self.assertIs(type(data_producer.get_loader([('0_0',)])), DataLoader)\n\n    def test_shuffle_datasets_order(self):\n        data_producer = DataProducer([list(range(10)), list(range(10, 20))]) \\\n            .shuffle_datasets_order(True)\n\n        prev_data, first_dataset_first = None, None\n        for i, data in enumerate(data_producer):\n            if first_dataset_first is None:\n                first_dataset_first = data < 10\n                prev_data = data\n                continue\n\n            if i < 10:\n                self.assertEqual(data, prev_data + 1)\n                self.assertEqual(first_dataset_first, data < 10)\n\n            if i > 9:\n                self.assertEqual(first_dataset_first, data >= 10)\n                if i > 10:\n                    self.assertEqual(data, prev_data + 1)\n\n            prev_data = data\n\n    def test_pin_memory(self):\n        data_producer = DataProducer([list(range(1))]).pin_memory(False)\n        self.assertFalse(data_producer.get_loader().pin_memory)\n        self.assertFalse(data_producer.get_loader([('0_0',)]).pin_memory)\n\n        data_producer.pin_memory(True)\n        self.assertTrue(data_producer.get_loader().pin_memory)\n        self.assertTrue(data_producer.get_loader([('0_0',)]).pin_memory)\n\n    def test_pass_indices(self):\n        data_producer = DataProducer([list(range(10)), list(range(10, 20))])\n        loader = data_producer.global_shuffle(True).pass_indices(True).get_loader()\n\n        for i, item in enumerate(loader):\n            data, idx = item['data'], item['data_idx'][0]\n\n            self.assertEqual(data, 10 * int(idx[0]) + int(idx[2]))\n\n        indices = list(set([('{}_{}'.format(randint(0, 1), randint(0, 9)),) for _ in range(10)]))\n\n        for data in data_producer.get_loader(indices):\n            d = int(data)\n            idx = ('{}_{}'.format(int(d > 9), d if d < 10 else d % 10),)\n            self.assertIn(idx, indices)\n            indices.remove(idx)\n\n        self.assertEqual(len(indices), 0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/monitoring_test.py,1,"b""import json\nimport os\nimport shutil\n\nimport torch\nimport numpy as np\n\nfrom neural_pipeline.monitoring import LogMonitor\nfrom neural_pipeline.train_config import MetricsGroup\nfrom neural_pipeline.utils import FileStructManager\nfrom tests.common import UseFileStructure, SimpleMetric\n\n__all__ = ['MonitorLogTest']\n\n\nclass MonitorLogTest(UseFileStructure):\n    def test_base_execution(self):\n        fsm = FileStructManager(base_dir='data', is_continue=False)\n        expected_out = os.path.join('data', 'monitors', 'metrics_log', 'metrics_log.json')\n        try:\n            with LogMonitor(fsm) as m:\n                self.assertEqual(m._file, expected_out)\n        except:\n            self.fail('Fail initialisation')\n\n        self.assertTrue(os.path.exists(expected_out) and os.path.isfile(expected_out))\n\n    def metrics_processing(self, with_final_file: bool, final_file: str = None):\n        fsm = FileStructManager(base_dir='data', is_continue=False)\n        expected_out = os.path.join('data', 'monitors', 'metrics_log', 'metrics_log.json')\n\n        metrics_group_lv1 = MetricsGroup('lv1').add(SimpleMetric(name='a', coeff=1)).add(SimpleMetric(name='b', coeff=2))\n        metrics_group_lv2 = MetricsGroup('lv2').add(SimpleMetric(name='c', coeff=3))\n        metrics_group_lv1.add(metrics_group_lv2)\n        m = SimpleMetric(name='d', coeff=4)\n\n        values = []\n        with LogMonitor(fsm) as monitor:\n            if with_final_file:\n                monitor.write_final_metrics(final_file)\n            for epoch in range(10):\n                cur_vals = []\n                for i in range(10):\n                    output, target = torch.rand(1, 3), torch.rand(1, 3)\n                    metrics_group_lv1.calc(output, target)\n                    m._calc(output, target)\n                    cur_vals.append(np.linalg.norm(output.numpy() - target.numpy()))\n\n                values.append(float(np.mean(cur_vals)))\n                monitor.set_epoch_num(epoch)\n                monitor.update_metrics({'metrics': [m], 'groups': [metrics_group_lv1]})\n                m.reset()\n                metrics_group_lv1.reset()\n\n        self.assertTrue(os.path.exists(expected_out) and os.path.isfile(expected_out))\n\n        with open(expected_out, 'r') as file:\n            data = json.load(file)\n\n        self.assertIn('d', data)\n        self.assertIn('lv1', data)\n        self.assertIn('lv2', data['lv1'])\n        self.assertIn('a', data['lv1'])\n        self.assertIn('b', data['lv1'])\n        self.assertIn('c', data['lv1']['lv2'])\n\n        self.assertEqual(len(data['d']), len(values))\n        self.assertEqual(len(data['lv1']['a']), len(values))\n        self.assertEqual(len(data['lv1']['b']), len(values))\n        self.assertEqual(len(data['lv1']['lv2']['c']), len(values))\n\n        for i, v in enumerate(values):\n            self.assertAlmostEqual(data['d'][i], values[i] * 4, delta=1e-5)\n            self.assertAlmostEqual(data['lv1']['a'][i], values[i], delta=1e-5)\n            self.assertAlmostEqual(data['lv1']['b'][i], values[i] * 2, delta=1e-5)\n            self.assertAlmostEqual(data['lv1']['lv2']['c'][i], values[i] * 3, delta=1e-5)\n\n        return values\n\n    def test_metrics_processing(self):\n        def check_data():\n            self.assertIn('d', data)\n            self.assertIn('lv1', data)\n            self.assertIn('lv2', data['lv1'])\n            self.assertIn('a', data['lv1'])\n            self.assertIn('b', data['lv1'])\n            self.assertIn('c', data['lv1']['lv2'])\n\n            self.assertAlmostEqual(data['d'], values[-1] * 4, delta=1e-5)\n            self.assertAlmostEqual(data['lv1']['a'], values[-1], delta=1e-5)\n            self.assertAlmostEqual(data['lv1']['b'], values[-1] * 2, delta=1e-5)\n            self.assertAlmostEqual(data['lv1']['lv2']['c'], values[-1] * 3, delta=1e-5)\n\n        self.metrics_processing(with_final_file=False)\n\n        shutil.rmtree('data')\n\n        values = self.metrics_processing(with_final_file=True)\n        expected_out = os.path.join('data', 'monitors', 'metrics_log', 'metrics.json')\n        self.assertTrue(os.path.exists(expected_out) and os.path.isfile(expected_out))\n\n        with open(expected_out, 'r') as file:\n            data = json.load(file)\n\n        check_data()\n\n        shutil.rmtree('data')\n\n        values = self.metrics_processing(with_final_file=True, final_file='my_metrics.json')\n        expected_out = os.path.join('my_metrics.json')\n        self.assertTrue(os.path.exists(expected_out) and os.path.isfile(expected_out))\n\n        with open(expected_out, 'r') as file:\n            data = json.load(file)\n\n        check_data()\n\n    def tearDown(self):\n        super().tearDown()\n\n        if os.path.exists('my_metrics.json') and os.path.isfile('my_metrics.json'):\n            os.remove('my_metrics.json')\n"""
tests/predict_test.py,4,"b""import torch\n\nfrom neural_pipeline.train_config import MetricsProcessor, TrainStage\nfrom neural_pipeline.train_config.train_config import ValidationStage, TrainConfig\nfrom neural_pipeline.utils.fsm import FileStructManager\nfrom neural_pipeline import Predictor, Trainer\nfrom tests.common import UseFileStructure\n\nfrom tests.data_processor_test import SimpleModel, SimpleLoss\nfrom tests.data_producer_test import TestDataProducer\n\n\nclass PredictTest(UseFileStructure):\n    def test_predict(self):\n        model = SimpleModel()\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n\n        metrics_processor = MetricsProcessor()\n        stages = [TrainStage(TestDataProducer([[{'data': torch.rand(1, 3), 'target': torch.rand(1)}\n                                                for _ in list(range(20))]]), metrics_processor),\n                  ValidationStage(TestDataProducer([[{'data': torch.rand(1, 3), 'target': torch.rand(1)}\n                                                     for _ in list(range(20))]]), metrics_processor)]\n        Trainer(TrainConfig(model, stages, SimpleLoss(), torch.optim.SGD(model.parameters(), lr=1)), fsm)\\\n            .set_epoch_num(1).train()\n\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=True)\n        Predictor(model, fsm).predict({'data': torch.rand(1, 3)})\n"""
tests/test.py,0,"b""import unittest\n\nfrom .data_processor_test import *\nfrom .data_producer_test import *\nfrom .train_config_test import *\nfrom .utils_test import *\nfrom .train_test import *\nfrom .predict_test import *\nfrom .monitoring_test import *\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/train_config_test.py,8,"b'import unittest\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nfrom torch import Tensor\n\nfrom neural_pipeline import Trainer\nfrom neural_pipeline.data_producer import DataProducer\nfrom neural_pipeline.train_config.train_config import MetricsGroup, AbstractMetric, TrainStage, MetricsProcessor, TrainConfig\nfrom neural_pipeline.utils.fsm import FileStructManager\nfrom tests.common import UseFileStructure\nfrom tests.data_processor_test import SimpleModel, SimpleLoss\nfrom tests.data_producer_test import TestDataProducer\n\n__all__ = [\'TrainConfigTest\']\n\n\nclass SimpleMetric(AbstractMetric):\n    def __init__(self):\n        super().__init__(\'SimpleMetric\')\n\n    def calc(self, output: Tensor, target: Tensor) -> np.ndarray or float:\n        return F.pairwise_distance(output, target, p=2).numpy()\n\n\nclass FakeMetricsProcessor(MetricsProcessor):\n    def __init__(self):\n        super().__init__()\n        self.call_num = 0\n\n    def calc_metrics(self, output, target):\n        self.call_num += 1\n\n\nclass TrainConfigTest(UseFileStructure):\n    def test_metric(self):\n        metric = SimpleMetric()\n\n        for i in range(10):\n            output, target = torch.rand(1, 3), torch.rand(1, 3)\n            res = metric.calc(output, target)[0]\n            self.assertAlmostEqual(res, np.linalg.norm(output.numpy() - target.numpy()), delta=1e-5)\n\n        vals = metric.get_values()\n        self.assertEqual(vals.size, 0)\n\n        values = []\n        for i in range(10):\n            output, target = torch.rand(1, 3), torch.rand(1, 3)\n            metric._calc(output, target)\n            values.append(np.linalg.norm(output.numpy() - target.numpy()))\n\n        vals = metric.get_values()\n        self.assertEqual(vals.size, len(values))\n        for v1, v2 in zip(values, vals):\n            self.assertAlmostEqual(v1, v2, delta=1e-5)\n\n        metric.reset()\n        self.assertEqual(metric.get_values().size, 0)\n\n        self.assertEqual(metric.name(), ""SimpleMetric"")\n\n    def test_metrics_group_nested(self):\n        metrics_group_lv1 = MetricsGroup(\'lvl\')\n        metrics_group_lv2 = MetricsGroup(\'lv2\')\n        metrics_group_lv1.add(metrics_group_lv2)\n        self.assertTrue(metrics_group_lv1.have_groups())\n        self.assertRaises(MetricsGroup.MGException, lambda: metrics_group_lv2.add(MetricsGroup(\'lv3\')))\n\n        metrics_group_lv1 = MetricsGroup(\'lvl\')\n        metrics_group_lv2 = MetricsGroup(\'lv2\')\n        metrics_group_lv3 = MetricsGroup(\'lv2\')\n        metrics_group_lv2.add(metrics_group_lv3)\n        self.assertRaises(MetricsGroup.MGException, lambda: metrics_group_lv1.add(metrics_group_lv2))\n\n    def test_metrics_group_calculation(self):\n        metrics_group_lv1 = MetricsGroup(\'lvl\').add(SimpleMetric())\n        metrics_group_lv2 = MetricsGroup(\'lv2\').add(SimpleMetric())\n        metrics_group_lv1.add(metrics_group_lv2)\n\n        values = []\n        for i in range(10):\n            output, target = torch.rand(1, 3), torch.rand(1, 3)\n            metrics_group_lv1.calc(output, target)\n            values.append(np.linalg.norm(output.numpy() - target.numpy()))\n\n        for metrics_group in [metrics_group_lv1, metrics_group_lv2]:\n            for m in metrics_group.metrics():\n                for v1, v2 in zip(values, m.get_values()):\n                    self.assertAlmostEqual(v1, v2, delta=1e-5)\n\n        metrics_group_lv1.reset()\n        self.assertEqual(metrics_group_lv1.metrics()[0].get_values().size, 0)\n        self.assertEqual(metrics_group_lv2.metrics()[0].get_values().size, 0)\n\n    def test_metrics_pocessor_calculation(self):\n        metrics_group_lv11 = MetricsGroup(\'lvl\').add(SimpleMetric())\n        metrics_group_lv21 = MetricsGroup(\'lv2\').add(SimpleMetric())\n        metrics_group_lv11.add(metrics_group_lv21)\n        metrics_processor = MetricsProcessor()\n        metrics_group_lv12 = MetricsGroup(\'lvl\').add(SimpleMetric())\n        metrics_group_lv22 = MetricsGroup(\'lv2\').add(SimpleMetric())\n        metrics_group_lv12.add(metrics_group_lv22)\n        metrics_processor.add_metrics_group(metrics_group_lv11)\n        metrics_processor.add_metrics_group(metrics_group_lv12)\n        m1, m2 = SimpleMetric(), SimpleMetric()\n        metrics_processor.add_metric(m1)\n        metrics_processor.add_metric(m2)\n\n        values = []\n        for i in range(10):\n            output, target = torch.rand(1, 3), torch.rand(1, 3)\n            metrics_processor.calc_metrics(output, target)\n            values.append(np.linalg.norm(output.numpy() - target.numpy()))\n\n        for metrics_group in [metrics_group_lv11, metrics_group_lv21, metrics_group_lv12, metrics_group_lv22]:\n            for m in metrics_group.metrics():\n                for v1, v2 in zip(values, m.get_values()):\n                    self.assertAlmostEqual(v1, v2, delta=1e-5)\n        for m in [m1, m2]:\n            for v1, v2 in zip(values, m.get_values()):\n                self.assertAlmostEqual(v1, v2, delta=1e-5)\n\n        metrics_processor.reset_metrics()\n        self.assertEqual(metrics_group_lv11.metrics()[0].get_values().size, 0)\n        self.assertEqual(metrics_group_lv21.metrics()[0].get_values().size, 0)\n        self.assertEqual(metrics_group_lv12.metrics()[0].get_values().size, 0)\n        self.assertEqual(metrics_group_lv22.metrics()[0].get_values().size, 0)\n        self.assertEqual(m1.get_values().size, 0)\n        self.assertEqual(m2.get_values().size, 0)\n\n    def test_metrics_and_groups_collection(self):\n        m1 = SimpleMetric()\n        name = \'lv1\'\n        metrics_group_lv1 = MetricsGroup(name)\n        self.assertEqual(metrics_group_lv1.metrics(), [])\n        metrics_group_lv1.add(m1)\n        self.assertEqual(metrics_group_lv1.groups(), [])\n        self.assertEqual(metrics_group_lv1.metrics(), [m1])\n\n        metrics_group_lv2 = MetricsGroup(\'lv2\').add(SimpleMetric())\n        metrics_group_lv1.add(metrics_group_lv2)\n        self.assertEqual(metrics_group_lv1.groups(), [metrics_group_lv2])\n        self.assertEqual(metrics_group_lv1.metrics(), [m1])\n\n        metrics_group_lv22 = MetricsGroup(\'lv2\').add(SimpleMetric())\n        metrics_group_lv1.add(metrics_group_lv22)\n        self.assertEqual(metrics_group_lv1.groups(), [metrics_group_lv2, metrics_group_lv22])\n        self.assertEqual(metrics_group_lv1.metrics(), [m1])\n\n        self.assertEqual(metrics_group_lv1.name(), name)\n\n    def test_train_stage(self):\n        data_producer = DataProducer([[{\'data\': torch.rand(1, 3), \'target\': torch.rand(1)} for _ in list(range(20))]])\n        metrics_processor = FakeMetricsProcessor()\n        train_stage = TrainStage(data_producer, metrics_processor).enable_hard_negative_mining(0.1)\n\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        model = SimpleModel()\n        Trainer(TrainConfig(model, [train_stage], SimpleLoss(), torch.optim.SGD(model.parameters(), lr=1)), fsm) \\\n            .set_epoch_num(1).train()\n\n        self.assertEqual(metrics_processor.call_num, len(data_producer))\n\n    def test_hard_negatives_mining(self):\n        with self.assertRaises(ValueError):\n            stage = TrainStage(None).enable_hard_negative_mining(0)\n        with self.assertRaises(ValueError):\n            stage = TrainStage(None).enable_hard_negative_mining(1)\n        with self.assertRaises(ValueError):\n            stage = TrainStage(None).enable_hard_negative_mining(-1)\n        with self.assertRaises(ValueError):\n            stage = TrainStage(None).enable_hard_negative_mining(1.1)\n\n        dp = TestDataProducer([[{\'data\': torch.Tensor([i]), \'target\': torch.rand(1)}\n                                for i in list(range(20))]]).pass_indices(True)\n        stage = TrainStage(dp).enable_hard_negative_mining(0.1)\n        losses = np.random.rand(20)\n        samples = []\n\n        def on_batch(batch, data_processor):\n            samples.append(batch)\n            stage.hnm._losses = np.array([0])\n\n        stage.hnm._process_batch = on_batch\n        stage.hnm.exec(None, losses, [[\'0_{}\'.format(i)] for i in range(20)])\n\n        self.assertEqual(len(samples), 2)\n\n        losses = [float(v) for v in losses]\n        idxs = [int(s[\'data\']) for s in samples]\n        max_losses = [losses[i] for i in idxs]\n        idxs.sort(reverse=True)\n        for i in idxs:\n            del losses[i]\n\n        for l in losses:\n            self.assertLess(l, min(max_losses))\n\n        stage.on_epoch_end()\n\n        self.assertIsNone(stage.hnm._losses)\n\n        stage.disable_hard_negative_mining()\n        self.assertIsNone(stage.hnm)\n\n        for data in dp:\n            self.assertIn(\'data_idx\', data)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/train_test.py,12,"b""import os\nfrom random import randint\n\nimport torch\nimport numpy as np\n\nfrom neural_pipeline import Trainer\nfrom neural_pipeline.train import DecayingLR\nfrom neural_pipeline.train_config import TrainConfig, TrainStage, MetricsProcessor\nfrom neural_pipeline.train_config.train_config import ValidationStage\nfrom neural_pipeline.utils.fsm import FileStructManager\nfrom tests.common import UseFileStructure\nfrom tests.data_processor_test import SimpleModel\nfrom tests.data_producer_test import TestDataProducer\n\n__all__ = ['TrainTest']\n\n\nclass SimpleLoss(torch.nn.Module):\n    def forward(self, output, target):\n        return output / target\n\n\nclass TrainTest(UseFileStructure):\n    def test_base_ops(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        model = SimpleModel()\n\n        trainer = Trainer(TrainConfig(model, [], torch.nn.L1Loss(), torch.optim.SGD(model.parameters(), lr=1)),\n                          fsm)\n        with self.assertRaises(Trainer.TrainerException):\n            trainer.train()\n\n    def test_train(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        model = SimpleModel()\n        metrics_processor = MetricsProcessor()\n        stages = [TrainStage(TestDataProducer([[{'data': torch.rand(1, 3), 'target': torch.rand(1)}\n                                                for _ in list(range(20))]]), metrics_processor),\n                  ValidationStage(TestDataProducer([[{'data': torch.rand(1, 3), 'target': torch.rand(1)}\n                                                     for _ in list(range(20))]]), metrics_processor)]\n        Trainer(TrainConfig(model, stages, SimpleLoss(), torch.optim.SGD(model.parameters(), lr=1)), fsm) \\\n            .set_epoch_num(1).train()\n\n    def test_decaynig_lr(self):\n        step_num = 0\n\n        def target_value_clbk() -> float:\n            return 1 / step_num\n\n        lr = DecayingLR(0.1, 0.5, 3, target_value_clbk)\n        old_val = None\n        for i in range(1, 30):\n            step_num = i\n            value = lr.value()\n            if old_val is None:\n                old_val = value\n                continue\n\n            self.assertAlmostEqual(value, old_val, delta=1e-6)\n            old_val = value\n\n        step_num = 0\n\n        def target_value_clbk() -> float:\n            return 1\n\n        lr = DecayingLR(0.1, 0.5, 3, target_value_clbk)\n        old_val = None\n        for i in range(1, 30):\n            step_num = i\n            value = lr.value()\n            if old_val is None:\n                old_val = value\n                continue\n\n            if i % 3 == 0:\n                self.assertAlmostEqual(value, old_val * 0.5, delta=1e-6)\n            old_val = value\n\n        val = randint(1, 1000)\n        lr.set_value(val)\n        self.assertEqual(val, lr.value())\n\n    def test_lr_decaying(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        model = SimpleModel()\n        metrics_processor = MetricsProcessor()\n        stages = [TrainStage(TestDataProducer([[{'data': torch.rand(1, 3), 'target': torch.rand(1)}\n                                                for _ in list(range(20))]]), metrics_processor),\n                  ValidationStage(TestDataProducer([[{'data': torch.rand(1, 3), 'target': torch.rand(1)}\n                                                     for _ in list(range(20))]]), metrics_processor)]\n        trainer = Trainer(TrainConfig(model, stages, SimpleLoss(), torch.optim.SGD(model.parameters(), lr=0.1)),\n                          fsm).set_epoch_num(10)\n\n        def target_value_clbk() -> float:\n            return 1\n\n        trainer.enable_lr_decaying(0.5, 3, target_value_clbk)\n        trainer.train()\n\n        self.assertAlmostEqual(trainer.data_processor().get_lr(), 0.1 * (0.5 ** 3), delta=1e-6)\n\n    def test_savig_states(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        model = SimpleModel()\n        metrics_processor = MetricsProcessor()\n        stages = [TrainStage(TestDataProducer([[{'data': torch.rand(1, 3), 'target': torch.rand(1)}\n                                                for _ in list(range(20))]]), metrics_processor)]\n        trainer = Trainer(TrainConfig(model, stages, SimpleLoss(), torch.optim.SGD(model.parameters(), lr=0.1)),\n                          fsm).set_epoch_num(3)\n\n        checkpoint_file = os.path.join(self.base_dir, 'checkpoints', 'last', 'last_checkpoint.zip')\n\n        def on_epoch_end():\n            self.assertTrue(os.path.exists(checkpoint_file))\n            os.remove(checkpoint_file)\n\n        trainer.add_on_epoch_end_callback(on_epoch_end)\n        trainer.train()\n\n    def test_savig_best_states(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        model = SimpleModel()\n        metrics_processor = MetricsProcessor()\n        stages = [TrainStage(TestDataProducer([[{'data': torch.rand(1, 3), 'target': torch.rand(1)}\n                                                for _ in list(range(20))]]), metrics_processor)]\n        trainer = Trainer(TrainConfig(model, stages, SimpleLoss(), torch.optim.SGD(model.parameters(), lr=0.1)),\n                          fsm).set_epoch_num(3).enable_best_states_saving(lambda: np.mean(stages[0].get_losses()))\n\n        checkpoint_file = os.path.join(self.base_dir, 'checkpoints', 'last', 'last_checkpoint.zip')\n        best_checkpoint_file = os.path.join(self.base_dir, 'checkpoints', 'best', 'best_checkpoint.zip')\n\n        class Val:\n            def __init__(self):\n                self.v = None\n\n        first_val = Val()\n\n        def on_epoch_end(val):\n            if val.v is not None and np.mean(stages[0].get_losses()) < val.v:\n                self.assertTrue(os.path.exists(best_checkpoint_file))\n                os.remove(best_checkpoint_file)\n                val.v = np.mean(stages[0].get_losses())\n                return\n\n            val.v = np.mean(stages[0].get_losses())\n\n            self.assertTrue(os.path.exists(checkpoint_file))\n            self.assertFalse(os.path.exists(best_checkpoint_file))\n            os.remove(checkpoint_file)\n\n        trainer.add_on_epoch_end_callback(lambda: on_epoch_end(first_val))\n        trainer.train()\n"""
tests/utils_test.py,1,"b'import os\nimport shutil\nimport sys\n\nfrom io import StringIO\nfrom contextlib import contextmanager\n\nimport torch\nfrom torch import Tensor\nimport numpy as np\nimport unittest\n\nfrom neural_pipeline.utils import FileStructManager, CheckpointsManager, dict_recursive_bypass\nfrom neural_pipeline.utils.fsm import FolderRegistrable\nfrom tests.common import UseFileStructure\n\n__all__ = [\'UtilsTest\', \'FileStructManagerTest\', \'CheckpointsManagerTests\']\n\n\n@contextmanager\ndef captured_output():\n    new_out, new_err = StringIO(), StringIO()\n    old_out, old_err = sys.stdout, sys.stderr\n    try:\n        sys.stdout, sys.stderr = new_out, new_err\n        yield sys.stdout, sys.stderr\n    finally:\n        sys.stdout, sys.stderr = old_out, old_err\n\n\nclass UtilsTest(unittest.TestCase):\n    def test_dict_recursive_bypass(self):\n        d = {\'data\': np.array([1]), \'target\': {\'a\': np.array([1]), \'b\': np.array([1])}}\n        d = dict_recursive_bypass(d, lambda v: torch.from_numpy(v))\n\n        self.assertTrue(isinstance(d[\'data\'], Tensor))\n        self.assertTrue(isinstance(d[\'target\'][\'a\'], Tensor))\n        self.assertTrue(isinstance(d[\'target\'][\'b\'], Tensor))\n\n\nclass FileStructManagerTest(UseFileStructure):\n    class TestObj(FolderRegistrable):\n        def __init__(self, m: \'FileStructManager\', dir: str, name: str):\n            super().__init__(m)\n            self.dir = dir\n            self.name = name\n\n        def _get_gir(self) -> str:\n            return self.dir\n\n        def _get_name(self) -> str:\n            return self.name\n\n    def test_creation(self):\n        if os.path.exists(self.base_dir):\n            shutil.rmtree(self.checkpoints_dir, ignore_errors=True)\n\n        try:\n            FileStructManager(base_dir=self.base_dir, is_continue=False)\n        except FileStructManager.FSMException as err:\n            self.fail(""Raise error when base directory exists: [{}]"".format(err))\n\n        self.assertFalse(os.path.exists(self.base_dir))\n\n        try:\n            FileStructManager(base_dir=self.base_dir, is_continue=False)\n        except FileStructManager.FSMException as err:\n            self.fail(""Raise error when base directory exists but empty: [{}]"".format(err))\n\n        os.makedirs(os.path.join(self.base_dir, \'new_dir\'))\n        try:\n            FileStructManager(base_dir=self.base_dir, is_continue=False)\n        except:\n            self.fail(""Error initialize when exists non-registered folders in base directory"")\n\n        shutil.rmtree(self.base_dir, ignore_errors=True)\n\n    def test_object_registration(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        fsm_exist_ok = FileStructManager(base_dir=self.base_dir, is_continue=False, exists_ok=True)\n        o = self.TestObj(fsm, \'test_dir\', \'test_name\')\n        fsm.register_dir(o)\n\n        expected_path = os.path.join(self.base_dir, \'test_dir\')\n        self.assertFalse(os.path.exists(expected_path))\n        self.assertEqual(fsm.get_path(o), expected_path)\n\n        with self.assertRaises(FileStructManager.FSMException):\n            fsm.register_dir(self.TestObj(fsm, \'test_dir\', \'another_name\'))\n        try:\n            fsm.register_dir(self.TestObj(fsm, \'test_dir\', \'another_name\'), check_dir_registered=False)\n            fsm_exist_ok.register_dir(self.TestObj(fsm, \'test_dir\', \'another_name\'))\n            fsm_exist_ok.register_dir(self.TestObj(fsm, \'test_dir\', \'another_name2\'), check_dir_registered=False)\n        except:\n            self.fail(""Folder registrable test fail when it\'s disabled"")\n\n        with self.assertRaises(FileStructManager.FSMException):\n            fsm.register_dir(self.TestObj(fsm, \'another_dir\', \'test_name\'))\n            fsm.register_dir(self.TestObj(fsm, \'another_dir\', \'another_name\'))\n        with self.assertRaises(FileStructManager.FSMException):\n            fsm_exist_ok.register_dir(self.TestObj(fsm, \'another_dir\', \'test_name\'))\n            fsm_exist_ok.register_dir(self.TestObj(fsm, \'another_dir\', \'another_name\'))\n\n        try:\n            fsm.register_dir(self.TestObj(fsm, \'another_dir2\', \'test_name\'), check_name_registered=False)\n            fsm_exist_ok.register_dir(self.TestObj(fsm, \'another_dir2\', \'test_name\'), check_name_registered=False)\n        except:\n            self.fail(""Folder registrable test fail when it\'s disabled"")\n\n        os.makedirs(os.path.join(self.base_dir, \'dir_dir\', \'dir\'))\n        with self.assertRaises(FileStructManager.FSMException):\n            fsm.register_dir(self.TestObj(fsm, \'dir_dir\', \'name_name\'))\n\n        try:\n            fsm_exist_ok.register_dir(self.TestObj(fsm, \'dir_dir\', \'name_name\'))\n        except:\n            self.fail(""Folder registrable test fail when exists_ok=True"")\n\n\nclass CheckpointsManagerTests(UseFileStructure):\n    def test_initialisation(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n\n        try:\n            cm = CheckpointsManager(fsm)\n        except Exception as err:\n            self.fail(""Fail init CheckpointsManager; err: [\'{}\']"".format(err))\n\n        with self.assertRaises(FileStructManager.FSMException):\n            CheckpointsManager(fsm)\n\n        os.mkdir(os.path.join(fsm.get_path(cm), \'test_dir\'))\n        with self.assertRaises(FileStructManager.FSMException):\n            CheckpointsManager(fsm)\n\n    def test_pack(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        cm = CheckpointsManager(fsm)\n        with self.assertRaises(CheckpointsManager.SMException):\n            cm.pack()\n\n        os.mkdir(cm.weights_file())\n        os.mkdir(cm.optimizer_state_file())\n        with self.assertRaises(CheckpointsManager.SMException):\n            cm.pack()\n\n        shutil.rmtree(cm.weights_file())\n        shutil.rmtree(cm.optimizer_state_file())\n\n        f = open(cm.weights_file(), \'w\')\n        f.close()\n        f = open(cm.optimizer_state_file(), \'w\')\n        f.close()\n        f = open(cm.trainer_file(), \'w\')\n        f.close()\n\n        try:\n            cm.pack()\n        except Exception as err:\n            self.fail(\'Exception on packing files: [{}]\'.format(err))\n\n        for f in [cm.weights_file(), cm.optimizer_state_file()]:\n            if os.path.exists(f) and os.path.isfile(f):\n                self.fail(""File \'{}\' doesn\'t remove after pack"".format(f))\n\n        result = os.path.join(fsm.get_path(cm, check=False, create_if_non_exists=False), \'last_checkpoint.zip\')\n        self.assertTrue(os.path.exists(result) and os.path.isfile(result))\n\n        f = open(cm.weights_file(), \'w\')\n        f.close()\n        f = open(cm.optimizer_state_file(), \'w\')\n        f.close()\n        f = open(cm.trainer_file(), \'w\')\n        f.close()\n\n        try:\n            cm.pack()\n            result = os.path.join(fsm.get_path(cm, check=False, create_if_non_exists=False), \'last_checkpoint.zip.old\')\n            self.assertTrue(os.path.exists(result) and os.path.isfile(result))\n        except Exception as err:\n            self.fail(\'Fail to pack with existing previous state file\')\n\n    def test_unpack(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        cm = CheckpointsManager(fsm)\n\n        f = open(cm.weights_file(), \'w\')\n        f.write(\'1\')\n        f.close()\n        f = open(cm.optimizer_state_file(), \'w\')\n        f.write(\'2\')\n        f = open(cm.trainer_file(), \'w\')\n        f.write(\'3\')\n        f.close()\n\n        cm.pack()\n\n        try:\n            cm.unpack()\n        except Exception as err:\n            self.fail(\'Exception on unpacking\')\n\n        for i, f in enumerate([cm.weights_file(), cm.optimizer_state_file(), cm.trainer_file()]):\n            if not (os.path.exists(f) and os.path.isfile(f)):\n                self.fail(""File \'{}\' doesn\'t remove after pack"".format(f))\n            with open(f, \'r\') as file:\n                if file.read() != str(i + 1):\n                    self.fail(""File content corrupted"")\n\n    def test_clear_files(self):\n        fsm = FileStructManager(base_dir=self.base_dir, is_continue=False)\n        cm = CheckpointsManager(fsm)\n\n        f = open(cm.weights_file(), \'w\')\n        f.close()\n        f = open(cm.optimizer_state_file(), \'w\')\n        f.close()\n\n        cm.clear_files()\n\n        for f in [cm.weights_file(), cm.optimizer_state_file()]:\n            if os.path.exists(f) and os.path.isfile(f):\n                self.fail(""File \'{}\' doesn\'t remove after pack"".format(f))\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\'../../\'))\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Neural Pipeline\'\ncopyright = \'2019, Anton Fedotov\'\nauthor = \'Anton Fedotov\'\n\n# import importlib.util\n#\n# spec = importlib.util.spec_from_file_location(""neural_pipeline"",\n#                                               os.path.join(os.path.dirname(os.path.abspath(__file__)),\n#                                                            \'..\', \'..\', \'neural_pipeline\', \'__init__.py\'))\n# neural_pipeline = importlib.util.module_from_spec(spec)\n# spec.loader.exec_module(neural_pipeline)\n\nimport neural_pipeline\n\n# The short X.Y version\nversion = neural_pipeline.__version__\n# The full version, including alpha/beta/rc tags\nrelease = version\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'logo_only\': False,\n    \'display_version\': True,\n    \'prev_next_buttons_location\': \'both\',\n    \'style_external_links\': False,\n    \'style_nav_header_background\': \'white\',\n    # Toc options\n    \'collapse_navigation\': True,\n    \'sticky_navigation\': True,\n    \'navigation_depth\': 4,\n    \'includehidden\': True,\n    \'titles_only\': False\n  }\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'NeuralPipelinedoc\'\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NeuralPipeline.tex\', \'Neural Pipeline Documentation\',\n     \'Anton Fedotov\', \'manual\'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'neuralpipeline\', \'Neural Pipeline Documentation\',\n     [author], 1)\n]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NeuralPipeline\', \'Neural Pipeline Documentation\',\n     author, \'NeuralPipeline\', \'Neural networks training pipeline based on PyTorch. Designed to standardize training process and to increase coding preformance.\',\n     \'Miscellaneous\'),\n]\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n'"
examples/files/img_classification.py,4,"b""from neural_pipeline.builtin.monitors.tensorboard import TensorboardMonitor\nfrom neural_pipeline import DataProducer, AbstractDataset, TrainConfig, TrainStage,\\\n    ValidationStage, Trainer, FileStructManager\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nclass MNISTDataset(AbstractDataset):\n    transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n    def __init__(self, data_dir: str, is_train: bool):\n        self.dataset = datasets.MNIST(data_dir, train=is_train, download=True)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, item):\n        data, target = self.dataset[item]\n        return {'data': self.transforms(data), 'target': target}\n\n\nif __name__ == '__main__':\n    fsm = FileStructManager(base_dir='data', is_continue=False)\n    model = Net()\n\n    train_dataset = DataProducer([MNISTDataset('data/dataset', True)], batch_size=4, num_workers=2)\n    validation_dataset = DataProducer([MNISTDataset('data/dataset', False)], batch_size=4, num_workers=2)\n\n    train_config = TrainConfig(model, [TrainStage(train_dataset), ValidationStage(validation_dataset)], torch.nn.NLLLoss(),\n                               torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.5))\n\n    trainer = Trainer(train_config, fsm, torch.device('cuda:0')).set_epoch_num(5)\n    trainer.monitor_hub.add_monitor(TensorboardMonitor(fsm, is_continue=False))\n    trainer.train()\n"""
examples/files/img_segmentation.py,9,"b'""""""\nThe images human portrait segmentation example.\n\nImages dataset was taken from [PicsArt](https://picsart.com/) AI Hackathon.\n\nDataset may be downloaded [there](https://s3.eu-central-1.amazonaws.com/datasouls/public/picsart_hack_online_data.zip)\n\nFor this example need to install this dependencies:\n`pip install sklearn, albumentations, opencv-python`\n""""""\n\nimport torch\n\nimport cv2\nimport os\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nfrom albumentations import Compose, HorizontalFlip, VerticalFlip, RandomRotate90, RandomGamma, \\\n    RandomBrightnessContrast, RGBShift, Resize, RandomCrop, OneOf\n\nfrom neural_pipeline import Trainer\nfrom neural_pipeline.builtin.models.albunet import resnet18\nfrom neural_pipeline.data_producer import AbstractDataset, DataProducer\nfrom neural_pipeline.monitoring import LogMonitor\nfrom neural_pipeline.train_config import AbstractMetric, MetricsProcessor, MetricsGroup, TrainStage, ValidationStage, TrainConfig\nfrom neural_pipeline.utils.fsm import FileStructManager\nfrom neural_pipeline.builtin.monitors.tensorboard import TensorboardMonitor\n\n###################################\n# Define dataset and augmentations\n# The dataset used in this example is from PicsArt hackathon (https://picsart.ai/en/contest)\n###################################\n\ndatasets_dir = \'data/dataset\'\nbase_dir = os.path.join(datasets_dir, \'picsart_hack_online_data\')\n\npreprocess = OneOf([RandomCrop(height=224, width=224), Resize(width=224, height=224)], p=1)\ntransforms = Compose([HorizontalFlip(), VerticalFlip(), RandomRotate90()], p=0.5)\naug = Compose([RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4), RandomGamma(), RGBShift(), transforms])\n\n\ndef augmentate(item: {}):\n    res = preprocess(image=item[\'data\'], mask=item[\'target\'])\n    res = aug(image=res[\'image\'], mask=res[\'mask\'])\n    return {\'data\': res[\'image\'], \'target\': res[\'mask\']}\n\n\ndef augmentate_and_to_pytorch(item: {}):\n    res = augmentate(item)\n    return {\'data\': torch.from_numpy(np.moveaxis(res[\'data\'].astype(np.float32) / 255., -1, 0)),\n            \'target\': torch.from_numpy(np.expand_dims(res[\'target\'].astype(np.float32) / 255, axis=0))}\n\n\nclass PicsartDataset(AbstractDataset):\n    def __init__(self, images_pathes: [], aug: callable):\n        images_dir = os.path.join(base_dir, \'train\')\n        masks_dir = os.path.join(base_dir, \'train_mask\')\n        images_pathes = sorted(images_pathes, key=lambda p: int(os.path.splitext(p)[0]))\n        self.__image_pathes = []\n        self.__aug = aug\n        for p in images_pathes:\n            name = os.path.splitext(p)[0]\n            mask_img = os.path.join(masks_dir, name + \'.png\')\n            if os.path.exists(mask_img):\n                path = {\'data\': os.path.join(images_dir, p), \'target\': mask_img}\n                self.__image_pathes.append(path)\n\n    def __len__(self):\n        return len(self.__image_pathes)\n\n    def __getitem__(self, item):\n        img = cv2.imread(self.__image_pathes[item][\'data\'])\n        return self.__aug({\'data\': img,\n                           \'target\': cv2.imread(self.__image_pathes[item][\'target\'], cv2.IMREAD_UNCHANGED)})\n\n\nimg_dir = os.path.join(base_dir, \'train\')\nimg_pathes = [f for f in os.listdir(img_dir) if os.path.splitext(f)[1] == "".jpg""]\ntrain_pathes, val_pathes = train_test_split(img_pathes, shuffle=True, test_size=0.2)\n\ntrain_dataset, val_dataset = PicsartDataset(train_pathes, augmentate_and_to_pytorch), PicsartDataset(val_pathes,\n                                                                                                     augmentate_and_to_pytorch)\n\n###################################\n# define metrics\n###################################\n\neps = 1e-6\n\n\ndef dice(preds: torch.Tensor, trues: torch.Tensor) -> np.ndarray:\n    preds_inner = preds.data.cpu().numpy().copy()\n    trues_inner = trues.data.cpu().numpy().copy()\n\n    preds_inner = np.reshape(preds_inner, (preds_inner.shape[0], preds_inner.size // preds_inner.shape[0]))\n    trues_inner = np.reshape(trues_inner, (trues_inner.shape[0], trues_inner.size // trues_inner.shape[0]))\n\n    intersection = (preds_inner * trues_inner).sum(1)\n    scores = (2. * intersection + eps) / (preds_inner.sum(1) + trues_inner.sum(1) + eps)\n\n    return scores\n\n\ndef jaccard(preds: torch.Tensor, trues: torch.Tensor):\n    preds_inner = preds.cpu().data.numpy().copy()\n    trues_inner = trues.cpu().data.numpy().copy()\n\n    preds_inner = np.reshape(preds_inner, (preds_inner.shape[0], preds_inner.size // preds_inner.shape[0]))\n    trues_inner = np.reshape(trues_inner, (trues_inner.shape[0], trues_inner.size // trues_inner.shape[0]))\n    intersection = (preds_inner * trues_inner).sum(1)\n    scores = (intersection + eps) / ((preds_inner + trues_inner).sum(1) - intersection + eps)\n\n    return scores\n\n\nclass DiceMetric(AbstractMetric):\n    def __init__(self):\n        super().__init__(\'dice\')\n\n    def calc(self, output: torch.Tensor, target: torch.Tensor) -> np.ndarray or float:\n        return dice(output, target)\n\n\nclass JaccardMetric(AbstractMetric):\n    def __init__(self):\n        super().__init__(\'jaccard\')\n\n    def calc(self, output: torch.Tensor, target: torch.Tensor) -> np.ndarray or float:\n        return jaccard(output, target)\n\n\nclass SegmentationMetricsProcessor(MetricsProcessor):\n    def __init__(self, stage_name: str):\n        super().__init__()\n        self.add_metrics_group(MetricsGroup(stage_name).add(JaccardMetric()).add(DiceMetric()))\n\n\n###################################\n# define train config and train model\n###################################\n\ntrain_data_producer = DataProducer([train_dataset], batch_size=2, num_workers=3)\nval_data_producer = DataProducer([val_dataset], batch_size=2, num_workers=3)\n\ntrain_stage = TrainStage(train_data_producer, SegmentationMetricsProcessor(\'train\')).enable_hard_negative_mining(0.1)\nval_metrics_processor = SegmentationMetricsProcessor(\'validation\')\nval_stage = ValidationStage(val_data_producer, val_metrics_processor)\n\n\ndef train():\n    model = resnet18(classes_num=1, in_channels=3, pretrained=True)\n    train_config = TrainConfig(model, [train_stage, val_stage], torch.nn.BCEWithLogitsLoss(),\n                               torch.optim.Adam(model.parameters(), lr=1e-4))\n\n    file_struct_manager = FileStructManager(base_dir=\'data\', is_continue=False)\n\n    trainer = Trainer(train_config, file_struct_manager, torch.device(\'cuda:0\')).set_epoch_num(2)\n\n    tensorboard = TensorboardMonitor(file_struct_manager, is_continue=False, network_name=\'PortraitSegmentation\')\n    log = LogMonitor(file_struct_manager).write_final_metrics()\n    trainer.monitor_hub.add_monitor(tensorboard).add_monitor(log)\n    trainer.enable_best_states_saving(lambda: np.mean(train_stage.get_losses()))\n\n    trainer.enable_lr_decaying(coeff=0.5, patience=10, target_val_clbk=lambda: np.mean(train_stage.get_losses()))\n    trainer.add_on_epoch_end_callback(lambda: tensorboard.update_scalar(\'params/lr\', trainer.data_processor().get_lr()))\n    trainer.train()\n\n\nif __name__ == ""__main__"":\n    train()\n'"
examples/files/resume_train.py,3,"b'""""""\nThis file contain example for resume training described in img_segmentation.py\n""""""\n\nimport torch\nimport numpy as np\n\nfrom neural_pipeline import Trainer\nfrom neural_pipeline.builtin.models.albunet import resnet18\nfrom neural_pipeline.builtin.monitors.tensorboard import TensorboardMonitor\nfrom neural_pipeline.monitoring import LogMonitor\nfrom neural_pipeline.train_config import TrainConfig\nfrom neural_pipeline.utils import FileStructManager\n\nfrom examples.files.img_segmentation import train_stage, val_stage\n\n\ndef continue_training():\n    ########################################################\n    # Create needed parameters again\n    ########################################################\n\n    model = resnet18(classes_num=1, in_channels=3, pretrained=True)\n    train_config = TrainConfig([train_stage, val_stage], torch.nn.BCEWithLogitsLoss(),\n                               torch.optim.Adam(model.parameters(), lr=1e-4))\n\n    ########################################################\n    # If FileStructManager creates again - just \'set is_continue\' parameter to True\n    ########################################################\n    file_struct_manager = FileStructManager(base_dir=\'data\', is_continue=True)\n\n    trainer = Trainer(model, train_config, file_struct_manager, torch.device(\'cuda:0\')).set_epoch_num(10)\n\n    tensorboard = TensorboardMonitor(file_struct_manager, is_continue=False, network_name=\'PortraitSegmentation\')\n    log = LogMonitor(file_struct_manager).write_final_metrics()\n    trainer.monitor_hub.add_monitor(tensorboard).add_monitor(log)\n    trainer.enable_best_states_saving(lambda: np.mean(train_stage.get_losses()))\n\n    trainer.enable_lr_decaying(coeff=0.5, patience=10, target_val_clbk=lambda: np.mean(train_stage.get_losses()))\n    trainer.add_on_epoch_end_callback(lambda: tensorboard.update_scalar(\'params/lr\', trainer.data_processor().get_lr()))\n\n    ########################################################\n    # For set resume mode to Trainer just call \'resume\' method\n    ########################################################\n\n    trainer.resume(from_best_checkpoint=False).train()\n'"
neural_pipeline/builtin/__init__.py,0,b''
neural_pipeline/data_processor/__init__.py,0,"b'from .data_processor import DataProcessor, TrainDataProcessor\nfrom .model import Model\n'"
neural_pipeline/data_processor/data_processor.py,16,"b'import numpy as np\n\nimport torch\nfrom neural_pipeline.utils import dict_recursive_bypass\n\nfrom neural_pipeline.utils import CheckpointsManager\nfrom torch.nn import Module\n\nfrom neural_pipeline.data_processor.model import Model\n\n__all__ = [\'DataProcessor\', \'TrainDataProcessor\']\n\n\nclass DataProcessor:\n    """"""\n    DataProcessor manage: model, data processing, device choosing\n\n    Args:\n        model (Module): model, that will be used for process data\n        device (torch.device): what device pass data for processing\n    """"""\n\n    def __init__(self, model: Module, device: torch.device = None):\n        self._checkpoints_manager = None\n        self._model = Model(model)\n        self._device = device\n        self._pick_model_input = lambda data: data[\'data\']\n\n    def set_checkpoints_manager(self, checkpoint_manager: CheckpointsManager) -> \'DataProcessor\':\n        self._checkpoints_manager = checkpoint_manager\n        self._model.set_checkpoints_manager(checkpoint_manager)\n        return self\n\n    def model(self) -> Module:\n        """"""\n        Get current module\n        """"""\n        return self._model.model()\n\n    def set_pick_model_input(self, pick_model_input: callable) -> \'DataProcessor\':\n        """"""\n        Set callback, that will get output from :mod:`DataLoader` and return model input.\n\n        Default mode:\n\n        .. highlight:: python\n        .. code-block:: python\n\n        lambda data: data[\'data\']\n\n        Args:\n            pick_model_input (callable): pick model input callable. This callback need to get one parameter: dataset output\n\n        Returns:\n            self object\n\n        Examples:\n\n        .. highlight:: python\n        .. code-block:: python\n\n            data_processor.set_pick_model_input(lambda data: data[\'data\'])\n            data_processor.set_pick_model_input(lambda data: data[0])\n        """"""\n        self._pick_model_input = pick_model_input\n        return self\n\n    def predict(self, data: torch.Tensor or dict) -> object:\n        """"""\n        Make predict by data\n\n        :param data: data as :class:`torch.Tensor` or dict with key ``data``\n        :return: processed output\n        :rtype: the model output type\n        """"""\n        self.model().eval()\n        with torch.no_grad():\n            output = self._model(self._pick_model_input(data))\n        return output\n\n    def load(self) -> None:\n        """"""\n        Load model weights from checkpoint\n        """"""\n        self._model.load_weights()\n\n    def save_state(self) -> None:\n        """"""\n        Save state of optimizer and perform epochs number\n        """"""\n        self._model.save_weights()\n\n\nclass TrainDataProcessor(DataProcessor):\n    """"""\n    TrainDataProcessor is make all of DataProcessor but produce training process.\n\n    :param train_config: train config\n    """"""\n\n    class TDPException(Exception):\n        def __init__(self, msg):\n            self._msg = msg\n\n        def __str__(self):\n            return self._msg\n\n    def __init__(self, train_config: \'TrainConfig\', device: torch.device = None):\n        super().__init__(train_config.model(), device)\n\n        self._data_preprocess = (lambda data: data) if device is None else self._pass_data_to_device\n        self._pick_target = lambda data: data[\'target\']\n\n        self._loss_input_preproc = lambda data: data\n        self.__criterion = train_config.loss()\n        self.__optimizer = train_config.optimizer()\n\n    def predict(self, data, is_train=False) -> torch.Tensor or dict:\n        """"""\n        Make predict by data. If ``is_train`` is ``True`` - this operation will compute gradients. If\n        ``is_train`` is ``False`` - this will work with ``model.eval()`` and ``torch.no_grad``\n\n        :param data: data in dict\n        :param is_train: is data processor need train on data or just predict\n        :return: processed output\n        :rtype: model return type\n        """"""\n\n        if is_train:\n            self.model().train()\n            output = self._model(self._pick_model_input(data))\n        else:\n            output = super().predict(data)\n\n        return output\n\n    def process_batch(self, batch: {}, is_train: bool, metrics_processor: \'AbstractMetricsProcessor\' = None) -> np.ndarray:\n        """"""\n        Process one batch of data\n\n        :param batch: dict, contains \'data\' and \'target\' keys. The values for key must be instance of torch.Tensor or dict\n        :param is_train: is batch process for train\n        :param metrics_processor: metrics processor for collect metrics after batch is processed\n        :return: array of losses with shape (N, ...) where N is batch size\n        """"""\n        internal_batch = self._data_preprocess(batch)\n\n        if is_train:\n            self.__optimizer.zero_grad()\n\n        res = self.predict(internal_batch, is_train)\n        loss = self.__criterion(res, self._pick_target(internal_batch))\n\n        if is_train:\n            loss.backward()\n            self.__optimizer.step()\n\n        with torch.no_grad():\n            if metrics_processor is not None:\n                metrics_processor.calc_metrics(res, self._pick_target(internal_batch))\n\n        return loss.data.cpu().numpy()\n\n    def update_lr(self, lr: float) -> None:\n        """"""\n        Update learning rate straight to optimizer\n\n        :param lr: target learning rate\n        """"""\n        for param_group in self.__optimizer.param_groups:\n            param_group[\'lr\'] = lr\n\n    def get_lr(self) -> float:\n        """"""\n        Get learning rate from optimizer\n        """"""\n        for param_group in self.__optimizer.param_groups:\n            return param_group[\'lr\']\n\n    def get_state(self) -> {}:\n        """"""\n        Get model and optimizer state dicts\n\n        :return: dict with keys [weights, optimizer]\n        """"""\n        return {\'weights\': self._model.model().state_dict(), \'optimizer\': self.__optimizer.state_dict()}\n\n    def _get_checkpoints_manager(self) -> CheckpointsManager:\n        if self._checkpoints_manager is None:\n            raise self.TDPException(""Checkpoints manager doesn\'t specified. Use \'set_checkpoints_manager()\'"")\n        return self._checkpoints_manager\n\n    def load(self) -> None:\n        """"""\n        Load state of model, optimizer and TrainDataProcessor from checkpoint\n        """"""\n        super().load()\n        cp_manager = self._get_checkpoints_manager()\n        print(""Optimizer inited by file:"", cp_manager.optimizer_state_file(), end=\'; \')\n        state = torch.load(cp_manager.optimizer_state_file())\n        print(\'state dict len before:\', len(state), end=\'; \')\n        state = {k: v for k, v in state.items() if k in self.__optimizer.state_dict()}\n        print(\'state dict len after:\', len(state), end=\'; \')\n        self.__optimizer.load_state_dict(state)\n        print(\'done\')\n\n    def save_state(self) -> None:\n        """"""\n        Save state of optimizer and perform epochs number\n        """"""\n        super().save_state()\n        torch.save(self.__optimizer.state_dict(), self._get_checkpoints_manager().optimizer_state_file())\n\n    def set_pick_target(self, pick_target: callable) -> \'DataProcessor\':\n        """"""\n        Set callback, that will get output from :mod:`DataLoader` and return target.\n\n        Default mode:\n\n        .. highlight:: python\n        .. code-block:: python\n\n        lambda data: data[\'target\']\n\n        Args:\n            pick_target (callable): pick target callable. This callback need to get one parameter: dataset output\n\n        Returns:\n            self object\n\n        Examples:\n\n        .. highlight:: python\n        .. code-block:: python\n\n            data_processor.set_pick_target(lambda data: data[\'target\'])\n            data_processor.set_pick_target(lambda data: data[1])\n        """"""\n        self._pick_target = pick_target\n        return self\n\n    def set_data_preprocess(self, data_preprocess: callable) -> \'DataProcessor\':\n        """"""\n        Set callback, that will get output from :mod:`DataLoader` and return preprocessed data.\n        For example may be used for pass data to device.\n\n        Default mode:\n\n        .. highlight:: python\n        .. code-block:: python\n\n        :meth:`_pass_data_to_device`\n\n        Args:\n            data_preprocess (callable): preprocess callable. This callback need to get one parameter: dataset output\n\n        Returns:\n            self object\n\n        Examples:\n\n        .. highlight:: python\n        .. code-block:: python\n\n            from neural_pipeline.utils import dict_recursive_bypass\n            data_processor.set_data_preprocess(lambda data: dict_recursive_bypass(data, lambda v: v.cuda()))\n        """"""\n        self._data_preprocess = data_preprocess\n        return self\n\n    def _pass_data_to_device(self, data) -> torch.Tensor or dict:\n        """"""\n        Internal method, that pass data to specified device\n        :param data: data as any object type. If will passed to device if it\'s instance of :class:`torch.Tensor` or dict with key\n        ``data``. Otherwise data will be doesn\'t changed\n        :return: processed on target device\n        """"""\n        if isinstance(data, dict):\n            return dict_recursive_bypass(data, lambda v: v.to(self._device))\n        elif isinstance(data, torch.Tensor):\n            return data.to(self._device)\n        else:\n            return data\n'"
neural_pipeline/data_processor/model.py,12,"b'import torch\nfrom torch.nn import Module\n\nfrom neural_pipeline.utils.fsm import CheckpointsManager\n\n__all__ = [\'Model\']\n\n\nclass Model:\n    """"""\n    Wrapper for :class:`torch.nn.Module`. This class provide initialization, call and serialization for it\n\n    :param base_model: :class:`torch.nn.Module` object\n    """"""\n\n    class ModelException(Exception):\n        def __init__(self, msg):\n            self._msg = msg\n\n        def __str__(self):\n            return self._msg\n\n    def __init__(self, base_model: Module):\n        self._base_model = base_model\n        self._checkpoints_manager = None\n\n    def set_checkpoints_manager(self, manager: CheckpointsManager) -> \'Model\':\n        """"""\n        Set checkpoints manager, that will be used for identify path for weights file reading an writing\n\n        :param manager: :class:`CheckpointsManager` instance\n        :return: self object\n        """"""\n        self._checkpoints_manager = manager\n        return self\n\n    def model(self) -> Module:\n        """"""\n        Get internal :class:`torch.nn.Module` object\n\n        :return: internal :class:`torch.nn.Module` object\n        """"""\n        return self._base_model\n\n    def load_weights(self, weights_file: str = None) -> None:\n        """"""\n        Load weight from checkpoint\n        """"""\n        if weights_file is not None:\n            file = weights_file\n        else:\n            if self._checkpoints_manager is None:\n                raise self.ModelException(\'No weights file or CheckpointsManagement specified\')\n            file = self._checkpoints_manager.weights_file()\n        print(""Model inited by file:"", file, end=\'; \')\n        pretrained_weights = torch.load(file)\n        print(""dict len before:"", len(pretrained_weights), end=\'; \')\n        processed = {}\n        model_state_dict = self._base_model.state_dict()\n        for k, v in pretrained_weights.items():\n            if k.split(\'.\')[0] == \'module\' and not isinstance(self._base_model, torch.nn.DataParallel):\n                k = \'.\'.join(k.split(\'.\')[1:])\n            elif isinstance(self._base_model, torch.nn.DataParallel) and k.split(\'.\')[0] != \'module\':\n                k = \'module.\' + k\n            if k in model_state_dict:\n                if v.device != model_state_dict[k].device:\n                    v.to(model_state_dict[k].device)\n                processed[k] = v\n\n        self._base_model.load_state_dict(processed)\n        print(""dict len after:"", len(processed))\n\n    def save_weights(self, weights_file: str = None) -> None:\n        """"""\n        Serialize weights to file\n        """"""\n        state_dict = self._base_model.state_dict()\n        if weights_file is None:\n            if self._checkpoints_manager is None:\n                raise self.ModelException(""Checkpoints manager doesn\'t specified. Use \'set_checkpoints_manager()\'"")\n            torch.save(state_dict, self._checkpoints_manager.weights_file())\n        else:\n            torch.save(state_dict, weights_file)\n\n    def __call__(self, x):\n        """"""\n        Call torch.nn.Module __call__ method\n\n        :param x: model input data\n        """"""\n        return self._base_model(x)\n\n    def to_device(self, device: torch.device) -> \'Model\':\n        """"""\n        Pass model to specified device\n        """"""\n        self._base_model.to(device)\n        return self\n'"
neural_pipeline/data_producer/__init__.py,0,"b'from .data_producer import AbstractDataset, DataProducer, BasicDataset\n'"
neural_pipeline/data_producer/data_producer.py,2,"b'import itertools\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataloader import default_collate\nfrom abc import ABCMeta, abstractmethod\nimport numpy as np\n\n\n__all__ = [\'AbstractDataset\', \'DataProducer\', \'BasicDataset\']\n\n\nclass AbstractDataset(metaclass=ABCMeta):\n    @abstractmethod\n    def __getitem__(self, item):\n        pass\n\n    @abstractmethod\n    def __len__(self):\n        pass\n\n\nclass IndexesDataset(AbstractDataset):\n\nclass BasicDataset(AbstractDataset, metaclass=ABCMeta):\n    """"""\n    The standard dataset basic class.\n\n    Basic dataset get array of items and works with it. Array of items is just an array of shape [N, ?]\n    """"""\n    def __init__(self, items: list):\n        self._items = items\n        self._indices = None\n\n    @abstractmethod\n    def _interpret_item(self, item) -> any:\n        """"""\n        Interpret one item from dataset. This method get index of item and returns interpreted data? that will be passed from dataset\n\n        Args:\n            item: item of items array\n\n        Returns:\n            One item, that\n        """"""\n\n    def get_items(self) -> list:\n        """"""\n        Get array of items\n\n        :return: array of indices\n        """"""\n        return self._items\n\n    def set_indices(self, indices: [int], remove_unused: bool = False) -> \'AbstractDataset\':\n        if remove_unused:\n            self._items = [self._items[idx] for idx in indices]\n            self._indices = None\n        else:\n            self._indices = indices\n        return self\n\n    def get_indices(self) -> list:\n        return self._indices\n\n    def load_indices(self, path: str, remove_unused: bool = False) -> \'AbstractDataset\':\n        self.set_indices(np.load(path), remove_unused)\n        return self\n\n    def flush_indices(self, path: str) -> \'AbstractDataset\':\n        np.save(path, self._indices)\n        return self\n\n    def __getitem__(self, idx):\n        if self._indices is None:\n            return self._interpret_item(self._items[idx])\n        else:\n            return self._interpret_item(self._items[self._indices[idx]])\n\n    def __len__(self):\n        if self._indices is None:\n            return len(self._items)\n        else:\n            return len(self._indices)\n\n\nclass DataProducer:\n    """"""\n    Data Producer. Accumulate one or more datasets and pass it\'s data by batches for processing.\n    This use PyTorch builtin :class:`DataLoader` for increase performance of data delivery.\n\n    :param datasets: list of datasets. Every dataset might be iterable (contans methods ``__getitem__`` and ``__len__``)\n    :param batch_size: size of output batch\n    :param num_workers: number of processes, that load data from datasets and pass it for output\n    """"""\n\n    def __init__(self, dataset: [AbstractDataset], batch_size: int = 1, num_workers: int = 0):\n        self._dataset = dataset\n        self._batch_size = batch_size\n        self._num_workers = num_workers\n\n        self._glob_shuffle = True\n        self._pin_memory = False\n        self._collate_fn = default_collate\n        self._drop_last = False\n\n        self._need_pass_indices = False\n\n    def drop_last(self, need_drop: bool) -> \'DataProducer\':\n        self._drop_last = need_drop\n        return self\n\n    def global_shuffle(self, is_need: bool) -> \'DataProducer\':\n        """"""\n        Is need global shuffling. If global shuffling enable - batches will compile from random indices.\n\n        :param is_need: is need global shuffling\n        :return: self object\n        """"""\n        self._glob_shuffle = is_need\n        return self\n\n    def pin_memory(self, is_need: bool) -> \'DataProducer\':\n        """"""\n        Is need to pin memory on loading. Pinning memory was increase data loading performance (especially when data loads to GPU) but incompatible with swap\n\n        :param is_need: is need\n        :return: self object\n        """"""\n        self._pin_memory = is_need\n        return self\n\n    def pass_indices(self, need_pass: bool) -> \'DataProducer\':\n        """"""\n        Pass indices of data in every batch. By default disabled\n\n        :param need_pass: is need to pass indices\n        """"""\n        self._need_pass_indices = need_pass\n        return self\n\n    def _is_passed_indices(self) -> bool:\n        """"""\n        Internal method for know if :class:`DataProducer` passed indices\n\n        :return: is passed\n        """"""\n        return self._need_pass_indices\n\n    def get_data(self, dataset_idx: int, data_idx: int) -> object:\n        """"""\n        Get single data by dataset idx and data_idx\n\n        :param dataset_idx: index of dataset\n        :param data_idx: index of data in this dataset\n        :return: dataset output\n        """"""\n        data = self._dataset[data_idx]\n        if self._need_pass_indices:\n            if not isinstance(data, dict):\n                data = {\'data\': data}\n            return dict(data, **{\'data_idx\': str(dataset_idx) + ""_"" + str(data_idx)})\n        return data\n\n    def set_collate_func(self, func: callable) -> \'DataProducer\':\n        self._collate_fn = func\n        return self\n\n    def get_loader(self, indices: [str] = None) -> DataLoader:\n        """"""\n        Get PyTorch :class:`DataLoader` object, that aggregate :class:`DataProducer`.\n        If ``indices`` is specified - DataLoader will output data only by this indices. In this case indices will not passed.\n\n        :param indices: list of indices. Each item of list is a string in format \'{}_{}\'.format(dataset_idx, data_idx)\n        :return: :class:`DataLoader` object\n        """"""\n        if indices is not None:\n            return self._get_loader_by_indices(indices)\n        return DataLoader(self._dataset, batch_size=self._batch_size, num_workers=self._num_workers,\n                          shuffle=self._glob_shuffle, pin_memory=self._pin_memory, collate_fn=self._collate_fn,\n                          drop_last=self._drop_last)\n\n    def _get_loader_by_indices(self, indices: [str]) -> DataLoader:\n        """"""\n        Get loader, that produce data only by specified indices\n\n        :param indices: required indices\n        :return: :class:`DataLoader` object\n        """"""\n        return DataLoader(_ByIndices(self._dataset, indices), batch_size=self._batch_size, num_workers=self._num_workers,\n                          shuffle=self._glob_shuffle, pin_memory=self._pin_memory, collate_fn=self._collate_fn,\n                          drop_last=self._drop_last)\n\n\nclass _ByIndices(DataProducer):\n    def __init__(self, dataset:AbstractDataset, indices: list):\n        super().__init__(dataset)\n        self.indices = list(itertools.chain(*indices))\n\n    def __getitem__(self, item):\n        dataset_idx, data_idx = self.indices[item].split(\'_\')\n        return self.get_data(int(dataset_idx), int(data_idx))\n\n    def __len__(self):\n        return len(self.indices)\n'"
neural_pipeline/train_config/__init__.py,0,"b'from .train_config import AbstractMetric, MetricsGroup, MetricsProcessor, AbstractStage, TrainStage, ValidationStage, TrainConfig\n'"
neural_pipeline/train_config/registry.py,0,"b'from abc import ABCMeta, abstractmethod\n\nfrom torch import optim\nfrom torch import nn\n\n__all__ = [\'RegestryEntry\']\n\n\nclass RegestryEntry(metaclass=ABCMeta):\n    def __init__(self, instance_type: type):\n        self._type = instance_type\n        self._params = None\n        self._instance = None\n\n    @abstractmethod\n    def get_params(self) -> {}:\n        """"""\n        Get dict of parameters values\n\n        :return: dict of parameters with names in keys\n        """"""\n\n    def load_params(self, params: {}) -> \'RegestryEntry\':\n        """"""\n        Init object by parameters\n\n        :param params: dict of parameters\n        :return: instance of object\n        """"""\n        self._params = params.copy()\n        self._instance = self._init_by_params(params)\n        return self\n\n    def get_instance(self) -> object:\n        return self._instance\n\n    @abstractmethod\n    def _init_by_params(self, params: {}) -> object:\n        """"""\n        Init object by parameters\n\n        :param params: dict of parameters\n        :return: instance of object\n        """"""\n\n\nclass AdamEntry(RegestryEntry):\n    _param_names = [\'lr\', \'betas\', \'eps\', \'weight_decay\', \'amsgrad\']\n\n    def __init__(self):\n        super().__init__(optim.Adam)\n\n    def get_params(self):\n        res = {}\n        for param_name in self._param_names:\n            if param_name in self._param_names:\n                res[param_name] = self._params[param_name]\n        return res\n\n    def _init_by_params(self, params: {}) -> optim.Adam:\n        return optim.Adam(params=params[\'params\'], lr=params[\'lr\'] if \'lr\' in params else 1e-3,\n                          betas=params[\'betas\'] if \'betas\' in params else (0.9, 0.999), eps=params[\'eps\'] if \'eps\' in params else 1e-8,\n                          weight_decay=params[\'weight_decay\'] if \'weight_decay\' in params else 0,\n                          amsgrad=params[\'amsgrad\'] if \'amsgrad\' in params else False)\n\n\nclass BCELossEntry(RegestryEntry):\n    def __init__(self):\n        super().__init__(nn.BCELoss)\n\n    def get_params(self) -> {}:\n        return {}\n\n    def _init_by_params(self, params: {}) -> object:\n        return nn.BCELoss()\n\n\nregistry = {type(optim.Adam): AdamEntry(), type(nn.BCELoss): BCELossEntry()}\n'"
neural_pipeline/train_config/train_config.py,3,"b'from abc import ABCMeta, abstractmethod\n\nfrom torch import Tensor\nfrom torch.optim import Optimizer\nfrom torch.nn import Module\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\ntry:\n    from IPython import get_ipython\n\n    ip = get_ipython()\n    if ip is not None:\n        from tqdm import tqdm_notebook as tqdm\n    else:\n        from tqdm import tqdm\nexcept ImportError:\n    from tqdm import tqdm\n\nfrom neural_pipeline.data_producer.data_producer import DataProducer\nfrom neural_pipeline.data_processor.data_processor import TrainDataProcessor\n\n__all__ = [\'TrainConfig\', \'ComparableTrainConfig\', \'TrainStage\', \'ValidationStage\', \'AbstractMetric\', \'MetricsGroup\', \'MetricsProcessor\', \'AbstractStage\',\n           \'StandardStage\']\n\n\nclass AbstractMetric(metaclass=ABCMeta):\n    """"""\n    Abstract class for metrics. When it works in neural_pipeline, it store metric value for every call of :meth:`calc`\n\n    :param name: name of metric. Name wil be used in monitors, so be careful in use unsupported characters\n    """"""\n\n    def __init__(self, name: str):\n        self._name = name\n        self._values = np.array([])\n\n    @abstractmethod\n    def calc(self, output: Tensor, target: Tensor) -> np.ndarray or float:\n        """"""\n        Calculate metric by output from model and target\n\n        :param output: output from model\n        :param target: ground truth\n        """"""\n\n    def _calc(self, output: Tensor, target: Tensor):\n        """"""\n        Calculate metric by output from model and target. Method for internal use\n\n        :param output: output from model\n        :param target: ground truth\n        """"""\n        self._values = np.append(self._values, self.calc(output, target))\n\n    def name(self) -> str:\n        """"""\n        Get name of metric\n\n        :return: metric name\n        """"""\n        return self._name\n\n    def get_values(self) -> np.ndarray:\n        """"""\n        Get array of metric values\n\n        :return: array of values\n        """"""\n        return self._values\n\n    def reset(self) -> None:\n        """"""\n        Reset array of metric values\n        """"""\n        self._values = np.array([])\n\n    @staticmethod\n    def min_val() -> float:\n        """"""\n        Get minimum value of metric. This used for correct histogram visualisation in some monitors\n\n        :return: minimum value\n        """"""\n        return 0\n\n    @staticmethod\n    def max_val() -> float:\n        """"""\n        Get maximum value of metric. This used for correct histogram visualisation in some monitors\n\n        :return: maximum value\n        """"""\n        return 1\n\n\nclass MetricsGroup:\n    """"""\n    Class for unite metrics or another :class:`MetricsGroup`\'s in one namespace.\n    Note: MetricsGroup may contain only 2 level of :class:`MetricsGroup`\'s. So ``MetricsGroup().add(MetricsGroup().add(MetricsGroup()))``\n    will raises :class:`MGException`\n\n    :param name: group name. Name wil be used in monitors, so be careful in use unsupported characters\n    """"""\n\n    class MGException(Exception):\n        """"""\n        Exception for MetricsGroup\n        """"""\n\n        def __init__(self, msg: str):\n            self.__msg = msg\n\n        def __str__(self):\n            return self.__msg\n\n    def __init__(self, name: str):\n        self.__name = name\n        self.__metrics = []\n        self.__metrics_groups = []\n        self.__lvl = 1\n\n    def add(self, item: AbstractMetric or \'MetricsGroup\') -> \'MetricsGroup\':\n        """"""\n        Add :class:`AbstractMetric` or :class:`MetricsGroup`\n\n        :param item: object to add\n        :return: self object\n        :rtype: :class:`MetricsGroup`\n        """"""\n        if isinstance(item, type(self)):\n            item._set_level(self.__lvl + 1)\n            self.__metrics_groups.append(item)\n        else:\n            self.__metrics.append(item)\n        return self\n\n    def metrics(self) -> [AbstractMetric]:\n        """"""\n        Get list of metrics\n\n        :return: list of metrics\n        """"""\n        return self.__metrics\n\n    def groups(self) -> [\'MetricsGroup\']:\n        """"""\n        Get list of metrics groups\n\n        :return: list of metrics groups\n        """"""\n        return self.__metrics_groups\n\n    def name(self) -> str:\n        """"""\n        Get group name\n\n        :return: name\n        """"""\n        return self.__name\n\n    def have_groups(self) -> bool:\n        """"""\n        Is this group contains another metrics groups\n\n        :return: True if contains, otherwise - False\n        """"""\n        return len(self.__metrics_groups) > 0\n\n    def _set_level(self, level: int) -> None:\n        """"""\n        Internal method for set metrics group level\n        TODO: if metrics group contains in two groups with different levels - this is undefined case\n\n        :param level: parent group level\n        """"""\n        if level > 2:\n            raise self.MGException(""The metric group {} have {} level. There must be no more than 2 levels"".format(self.__name, self.__lvl))\n        self.__lvl = level\n        for group in self.__metrics_groups:\n            group._set_level(self.__lvl + 1)\n\n    def calc(self, output: Tensor, target: Tensor) -> None:\n        """"""\n        Recursive calculate all metrics in this group and all nested group\n\n        :param output: predict value\n        :param target: target value\n        """"""\n        for metric in self.__metrics:\n            metric._calc(output, target)\n        for group in self.__metrics_groups:\n            group.calc(output, target)\n\n    def reset(self) -> None:\n        """"""\n        Recursive reset all metrics in this group and all nested group\n        """"""\n        for metric in self.__metrics:\n            metric.reset()\n        for group in self.__metrics_groups:\n            group.reset()\n\n\nclass MetricsProcessor:\n    """"""\n    Collection for all :class:`AbstractMetric`\'s and :class:`MetricsGroup`\'s\n    """"""\n\n    def __init__(self):\n        self._metrics = []\n        self._metrics_groups = []\n\n    def add_metric(self, metric: AbstractMetric) -> AbstractMetric:\n        """"""\n        Add :class:`AbstractMetric` object\n\n        :param metric: metric to add\n        :return: metric object\n        :rtype: :class:`AbstractMetric`\n        """"""\n        self._metrics.append(metric)\n        return metric\n\n    def add_metrics_group(self, group: MetricsGroup) -> MetricsGroup:\n        """"""\n        Add :class:`MetricsGroup` object\n\n        :param group: metrics group to add\n        :return: metrics group object\n        :rtype: :class:`MetricsGroup`\n        """"""\n        self._metrics_groups.append(group)\n        return group\n\n    def calc_metrics(self, output, target) -> None:\n        """"""\n        Recursive calculate all metrics\n\n        :param output: predict value\n        :param target: target value\n        """"""\n        for metric in self._metrics:\n            metric.calc(output, target)\n        for group in self._metrics_groups:\n            group.calc(output, target)\n\n    def reset_metrics(self) -> None:\n        """"""\n        Recursive reset all metrics values\n        """"""\n        for metric in self._metrics:\n            metric.reset()\n        for group in self._metrics_groups:\n            group.reset()\n\n    def get_metrics(self) -> {}:\n        """"""\n        Get metrics and groups as dict\n\n        :return: dict of metrics and groups with keys [metrics, groups]\n        """"""\n        return {\'metrics\': self._metrics, \'groups\': self._metrics_groups}\n\n\nclass AbstractStage(metaclass=ABCMeta):\n    """"""\n    Stage of training process. For example there may be 2 stages: train and validation.\n    Every epochs in train loop is iteration by stages.\n\n    :param name: name of stage\n    """"""\n\n    def __init__(self, name: str):\n        self._name = name\n\n    def name(self) -> str:\n        """"""\n        Get name of stage\n\n        :return: name\n        """"""\n        return self._name\n\n    def metrics_processor(self) -> MetricsProcessor or None:\n        """"""\n        Get metrics processor\n\n        :return: :class:\'MetricsProcessor` object or None\n        """"""\n        return None\n\n    @abstractmethod\n    def run(self, data_processor: TrainDataProcessor) -> None:\n        """"""\n        Run stage\n        """"""\n\n    def get_losses(self) -> np.ndarray or None:\n        """"""\n        Get losses from this stage\n\n        :return: array of losses or None if this stage doesn\'t need losses\n        """"""\n        return None\n\n    def on_epoch_end(self) -> None:\n        """"""\n        Callback for train epoch end\n        """"""\n        pass\n\n\nclass StandardStage(AbstractStage):\n    """"""\n    Standard stage for train process.\n\n    When call :meth:`run` it\'s iterate :meth:`process_batch` of data processor by data loader\n\n    After stop iteration ValidationStage accumulate losses from :class:`DataProcessor`.\n\n    :param data_producer: :class:`DataProducer` object\n    :param metrics_processor: :class:`MetricsProcessor`\n    """"""\n\n    def __init__(self, stage_name: str, is_train: bool, data_producer: DataProducer, metrics_processor: MetricsProcessor = None):\n        super().__init__(name=stage_name)\n        self.data_loader = None\n        self.data_producer = data_producer\n        self._metrics_processor = metrics_processor\n        self._losses = None\n        self._is_train = is_train\n\n    def run(self, data_processor: TrainDataProcessor) -> None:\n        """"""\n        Run stage. This iterate by DataProducer and show progress in stdout\n\n        :param data_processor: :class:`DataProcessor` object\n        """"""\n        if self.data_loader is None:\n            self.data_loader = self.data_producer.get_loader()\n\n        self._run(self.data_loader, self.name(), data_processor)\n\n    def _run(self, data_loader: DataLoader, name: str, data_processor: TrainDataProcessor):\n        with tqdm(data_loader, desc=name, leave=False) as t:\n            self._losses = None\n            for batch in t:\n                self._process_batch(batch, data_processor)\n                t.set_postfix({\'loss\': \'[{:4f}]\'.format(np.mean(self._losses))})\n\n    def _process_batch(self, batch, data_processor: TrainDataProcessor):\n        cur_loss = data_processor.process_batch(batch, metrics_processor=self.metrics_processor(), is_train=self._is_train)\n        if self._losses is None:\n            self._losses = cur_loss\n        else:\n            self._losses = np.append(self._losses, cur_loss)\n\n    def metrics_processor(self) -> MetricsProcessor or None:\n        """"""\n        Get merics processor of this stage\n\n        :return: :class:`MetricsProcessor` if specified otherwise None\n        """"""\n        return self._metrics_processor\n\n    def get_losses(self) -> np.ndarray:\n        """"""\n        Get losses from this stage\n\n        :return: array of losses\n        """"""\n        return self._losses\n\n    def on_epoch_end(self) -> None:\n        """"""\n        Method, that calls after every epoch\n        """"""\n        self._losses = None\n        metrics_processor = self.metrics_processor()\n        if metrics_processor is not None:\n            metrics_processor.reset_metrics()\n\n\nclass TrainStage(StandardStage):\n    """"""\n    Standard training stage\n\n    When call :meth:`run` it\'s iterate :meth:`process_batch` of data processor by data loader with ``is_tran=True`` flag.\n\n    After stop iteration ValidationStage accumulate losses from :class:`DataProcessor`.\n\n    :param data_producer: :class:`DataProducer` object\n    :param metrics_processor: :class:`MetricsProcessor`\n    :param name: name of stage. By default \'train\'\n    """"""\n\n    class _HardNegativesTrainStage(StandardStage):\n        def __init__(self, stage_name: str, data_producer: DataProducer, part: float):\n            super().__init__(stage_name, True, data_producer)\n            self._part = part\n\n        def exec(self, data_processor: TrainDataProcessor, losses: np.ndarray, indices: []) -> None:\n            num_losses = int(losses.size * self._part)\n            idxs = np.argpartition(losses, -num_losses)[-num_losses:]\n            self._run(self.data_producer.get_loader([indices[i] for i in idxs]), self.name(), data_processor)\n\n    def __init__(self, data_producer: DataProducer, metrics_processor: MetricsProcessor = None, name: str = \'train\'):\n        super().__init__(name, True, data_producer, metrics_processor)\n        self.hnm = None\n        self.hn_indices = []\n        self._dp_pass_indices_earlier = False\n\n    def enable_hard_negative_mining(self, part: float) -> \'TrainStage\':\n        """"""\n        Enable hard negative mining. Hard negatives was taken by losses values\n\n        :param part: part of data that repeat after train stage\n        :return: self object\n        """"""\n\n        if not 0 < part < 1:\n            raise ValueError(\'Value of part for hard negative mining is out of range (0, 1)\')\n        self.hnm = self._HardNegativesTrainStage(self.name() + \'_hnm\', self.data_producer, part)\n        self._dp_pass_indices_earlier = self.data_producer._is_passed_indices()\n        self.data_producer.pass_indices(True)\n        return self\n\n    def disable_hard_negative_mining(self) -> \'TrainStage\':\n        """"""\n        Enable hard negative mining.\n\n        :return: self object\n        """"""\n        self.hnm = None\n        if not self._dp_pass_indices_earlier:\n            self.data_producer.pass_indices(False)\n        return self\n\n    def run(self, data_processor: TrainDataProcessor) -> None:\n        """"""\n        Run stage\n\n        :param data_processor: :class:`TrainDataProcessor` object\n        """"""\n        super().run(data_processor)\n        if self.hnm is not None:\n            self.hnm.exec(data_processor, self._losses, self.hn_indices)\n            self.hn_indices = []\n\n    def _process_batch(self, batch, data_processor: TrainDataProcessor) -> None:\n        """"""\n        Internal method for process one bathc\n\n        :param batch: batch\n        :param data_processor: :class:`TrainDataProcessor` instance\n        """"""\n        if self.hnm is not None:\n            self.hn_indices.append(batch[\'data_idx\'])\n        super()._process_batch(batch, data_processor)\n\n    def on_epoch_end(self):\n        """"""\n        Method, that calls after every epoch\n        """"""\n        super().on_epoch_end()\n        if self.hnm is not None:\n            self.hnm.on_epoch_end()\n\n\nclass ValidationStage(StandardStage):\n    """"""\n    Standard validation stage.\n\n    When call :meth:`run` it\'s iterate :meth:`process_batch` of data processor by data loader with ``is_tran=False`` flag.\n\n    After stop iteration ValidationStage accumulate losses from :class:`DataProcessor`.\n\n    :param data_producer: :class:`DataProducer` object\n    :param metrics_processor: :class:`MetricsProcessor`\n    :param name: name of stage. By default \'validation\'\n    """"""\n\n    def __init__(self, data_producer: DataProducer, metrics_processor: MetricsProcessor = None, name: str = \'validation\'):\n        super().__init__(name, False, data_producer, metrics_processor)\n\n\nclass TrainConfig:\n    """"""\n    Train process setting storage\n\n    :param train_stages: list of stages for train loop\n    :param loss: loss criterion\n    :param optimizer: optimizer object\n    """"""\n\n    def __init__(self, model: Module, train_stages: [], loss: Module, optimizer: Optimizer):\n        self._train_stages = train_stages\n        self._loss = loss\n        self._optimizer = optimizer\n        self._model = model\n\n    def loss(self) -> Module:\n        """"""\n        Get loss object\n\n        :return: loss object\n        """"""\n        return self._loss\n\n    def optimizer(self) -> Optimizer:\n        """"""\n        Get optimizer object\n\n        :return: optimizer object\n        """"""\n        return self._optimizer\n\n    def stages(self) -> [AbstractStage]:\n        """"""\n        Get list of stages\n\n        :return: list of stages\n        """"""\n        return self._train_stages\n\n    def model(self) -> Module:\n        return self._model\n\n\nclass ComparableTrainConfig:\n    """"""\n    Train process setting storage with name. Used for train with few train configs in one time\n\n    :param name: name of train config\n    """"""\n\n    def __init__(self, name: str = None):\n        self._name = name\n\n    @abstractmethod\n    def get_train_config(self) -> \'TrainConfig\':\n        """"""\n        Get train config\n\n        :return: TrainConfig object\n        """"""\n\n    @abstractmethod\n    def get_params(self) -> {}:\n        """"""\n        Get params of this config\n\n        :return:\n        """"""\n\n    def get_metric_for_compare(self) -> float or None:\n        """"""\n        Get metric for compare train configs\n\n        :return: metric value or None, if compare doesn\'t needed\n        """"""\n        return None\n'"
neural_pipeline/utils/__init__.py,0,"b'from .fsm import FileStructManager, CheckpointsManager\nfrom .utils import dict_recursive_bypass, dict_pair_recursive_bypass\n'"
neural_pipeline/utils/fsm.py,0,"b'""""""\nThis module contains all classes, that work with file structure\n\n* :class:`FileStructManager` provide all modules registration\n* :class:`CheckpointsManager` provide checkpoints management\n""""""\n\nimport os\nfrom abc import ABCMeta, abstractmethod\nfrom zipfile import ZipFile\n\n__all__ = [\'FileStructManager\', \'CheckpointsManager\', \'FolderRegistrable\', \'MultipleFSM\']\n\n\nclass FolderRegistrable(metaclass=ABCMeta):\n    """"""\n    Abstract class for implement classes, that use folders\n\n    :param fsm: FileStructureManager class instance\n    """"""\n\n    @abstractmethod\n    def __init__(self, fsm: \'FileStructManager\'):\n        pass\n\n    @abstractmethod\n    def _get_gir(self) -> str:\n        """"""\n        Get directory path to register\n\n        :return: path\n        """"""\n\n    @abstractmethod\n    def _get_name(self) -> str:\n        """"""\n        Get name of registrable object\n\n        :return: name\n        """"""\n\n\nclass CheckpointsManager(FolderRegistrable):\n    """"""\n    Class that manage checkpoints for DataProcessor.\n\n    All states pack to zip file. It contains few files: model weights, optimizer state, data processor state\n\n    :param fsm: :class:\'FileStructureManager\' instance\n    :param prefix: prefix of saved and loaded files\n    """"""\n\n    class SMException(Exception):\n        """"""\n        Exception for :class:`CheckpointsManager`\n        """"""\n\n        def __init__(self, message: str):\n            self.__message = message\n\n        def __str__(self):\n            return self.__message\n\n    def __init__(self, fsm: \'FileStructManager\', prefix: str = None):\n        super().__init__(fsm)\n\n        self._prefix = prefix if prefix is not None else \'last\'\n        fsm.register_dir(self)\n        self._checkpoints_dir = fsm.get_path(self, create_if_non_exists=True, check=False)\n\n        if (prefix is None) and (not (os.path.exists(self._checkpoints_dir) and os.path.isdir(self._checkpoints_dir))):\n            raise self.SMException(""Checkpoints dir doesn\'t exists: [{}]"".format(self._checkpoints_dir))\n\n        self._weights_file = os.path.join(self._checkpoints_dir, \'weights.pth\')\n        self._state_file = os.path.join(self._checkpoints_dir, \'state.pth\')\n        self._checkpoint_file = self._compile_path(self._checkpoints_dir, \'checkpoint.zip\')\n        self._trainer_file = os.path.join(self._checkpoints_dir, \'trainer.json\')\n\n        if not fsm.in_continue_mode() and os.path.exists(self._weights_file) and os.path.exists(self._state_file) and \\\n                os.path.isfile(self._weights_file) and os.path.isfile(self._state_file):\n            prev_prefix = self._prefix\n            self._prefix = ""prev_start""\n            self.pack()\n            self._prefix = prev_prefix\n\n    def unpack(self) -> None:\n        """"""\n        Unpack state files\n        """"""\n        with ZipFile(self._checkpoint_file, \'r\') as zipfile:\n            zipfile.extractall(self._checkpoints_dir)\n\n        self._check_files([self._weights_file, self._state_file, self._trainer_file])\n\n    def clear_files(self) -> None:\n        """"""\n        Clear unpacked files\n        """"""\n\n        def rm_file(file: str):\n            if os.path.exists(file) and os.path.isfile(file):\n                os.remove(file)\n\n        rm_file(self._weights_file)\n        rm_file(self._state_file)\n        rm_file(self._trainer_file)\n\n    def pack(self) -> None:\n        """"""\n        Pack all files in zip\n        """"""\n\n        def rm_file(file: str):\n            if os.path.exists(file) and os.path.isfile(file):\n                os.remove(file)\n\n        def rename_file(file: str):\n            target = file + "".old""\n            rm_file(target)\n            if os.path.exists(file) and os.path.isfile(file):\n                os.rename(file, target)\n\n        self._check_files([self._weights_file, self._state_file])\n\n        rename_file(self._checkpoint_file)\n        with ZipFile(self._checkpoint_file, \'w\') as zipfile:\n            zipfile.write(self._weights_file, os.path.basename(self._weights_file))\n            zipfile.write(self._state_file, os.path.basename(self._state_file))\n            zipfile.write(self._trainer_file, os.path.basename(self._trainer_file))\n\n        self.clear_files()\n\n    def optimizer_state_file(self) -> str:\n        """"""\n        Get optimizer state file path\n\n        :return: path\n        """"""\n        return self._state_file\n\n    def weights_file(self) -> str:\n        """"""\n        Get model weights file path\n\n        :return: path\n        """"""\n        return self._weights_file\n\n    def trainer_file(self) -> str:\n        """"""\n        Get trainer state file path\n\n        :return: path\n        """"""\n        return self._trainer_file\n\n    def _compile_path(self, directory: str, file: str) -> str:\n        """"""\n        Internal method for compile result file name\n\n        :return: path to result file\n        """"""\n        return os.path.join(directory, (self._prefix + ""_"" if self._prefix is not None else """") + file)\n\n    def _check_files(self, files) -> None:\n        """"""\n        Internal method for checking files for condition of existing\n\n        :param files: list of files pathes to check\n        :raises: SMException\n        """"""\n        failed = []\n        for f in files:\n            if not (os.path.exists(f) and os.path.isfile(f)):\n                failed.append(f)\n\n        if len(failed) > 0:\n            raise self.SMException(""Some files doesn\'t exists: [{}]"".format(\';\'.join(files)))\n\n    def _get_gir(self) -> str:\n        return os.path.join(\'checkpoints\', self._prefix)\n\n    def _get_name(self) -> str:\n        return \'CheckpointsManager\' + self._prefix\n\n\nclass FileStructManager:\n    """"""\n    Class, that provide directories registration in base directory.\n\n    All modules, that use file structure under base directory should register their paths in this class by pass module\n    to method :meth:`register_dir`.\n    If directory also registered registration method will raise exception :class:`FSMException`\n\n    :param base_dir: path to directory with checkpoints\n    :param is_continue: is `FileStructManager` used for continue training or predict\n    :param exists_ok: if `True` - all checks for existing directories will be disabled\n    """"""\n\n    class FSMException(Exception):\n        def __init__(self, message: str):\n            self.__message = message\n\n        def __str__(self):\n            return self.__message\n\n    class _Folder:\n        """"""\n        Internal class, that implements logic for single registrable directory\n\n        :param path: path to directory\n        :param fsm: :class:`FileStructManager` object\n        """"""\n\n        def __init__(self, path: str, fsm: \'FileStructManager\'):\n            self._path = path\n            self._fsm = fsm\n\n            self._path_first_request = True\n\n        def get_path_for_check(self) -> str:\n            """"""\n            Get folder path without any checking for existing\n\n            :return: path\n            """"""\n            return self._path\n\n        def _create_directories(self) -> None:\n            """"""\n            Internal method that create directory if this not exists and FileStructManager not in continue mode\n            """"""\n            if self._fsm._is_continue:\n                return\n            if not (os.path.exists(self._path) and os.path.isdir(self._path)):\n                os.makedirs(self._path, exist_ok=True)\n\n        def get_path(self, create_if_non_exists: bool = True) -> str:\n            """"""\n            Get folder path. This method create directory if it\'s not exists (if param ``create_if_non_exists == True``)\n\n            :param create_if_non_exists: is need to create directory if it\'s doesn\'t exists\n            :return: directory path\n            """"""\n            if create_if_non_exists and self._path_first_request:\n                self._create_directories()\n                self._path_first_request = False\n            return self._path\n\n        def check_path(self) -> None:\n            """"""\n            Check that directory doesn\'t contains any files\n\n            :raises: :class:`FileStructManager.FSMException`\n            """"""\n            if os.path.exists(self._path) and os.path.isdir(self._path):\n                if os.listdir(self._path):\n                    raise self._fsm.FSMException(""Checkpoint directory already exists [{}]"".format(self._path))\n\n    def __init__(self, base_dir: str, is_continue: bool, exists_ok: bool = False):\n        self._dirs = {}\n        self._is_continue = is_continue\n        self._base_dir = base_dir\n        self._exist_ok = exists_ok\n\n    def register_dir(self, obj: FolderRegistrable, check_name_registered: bool = False, check_dir_registered: bool = True) -> None:\n        """"""\n        Register directory in file structure\n\n        :param obj: object to registration\n        :param check_name_registered: is need to check if object name also registered\n        :param check_dir_registered: is need to check if object path also registered\n        :raise FileStructManager: if path or object name also registered and if path also exists (in depends of optional parameters values)\n        """"""\n        path = self._compile_path(obj)\n\n        if check_dir_registered:\n            for n, f in self._dirs.items():\n                if f.get_path_for_check() == path:\n                    raise self.FSMException(""Path {} already registered!"".format(path))\n\n        if check_name_registered:\n            if obj._get_name() in self._dirs:\n                raise self.FSMException(""Object {} already registered!"".format(obj._get_name()))\n\n        self._dirs[obj._get_name()] = self._Folder(path, self)\n        if not self._exist_ok and not self._is_continue:\n            self._dirs[obj._get_name()].check_path()\n\n    def get_path(self, obj: FolderRegistrable, create_if_non_exists: bool = False, check: bool = True) -> str:\n        """"""\n        Get path of registered object\n\n        :param obj: object\n        :param create_if_non_exists: is need to create object\'s directory if it doesn\'t exists\n        :param check: is need to check object\'s directory existing\n        :return: path to directory\n        :raise FSMException: if directory exists and ``check == True``\n        """"""\n        dir = self._dirs[obj._get_name()]\n        if not self._exist_ok and not self._is_continue and check:\n            dir.check_path()\n        return dir.get_path(create_if_non_exists)\n\n    def in_continue_mode(self) -> bool:\n        """"""\n        Is FileStructManager in continue mode\n\n        :return: True if in continue\n        """"""\n        return self._is_continue\n\n    def _compile_path(self, obj: FolderRegistrable) -> str:\n        return os.path.join(self._base_dir, obj._get_gir())\n\n\nclass MultipleFSM(FileStructManager):\n    def __init__(self, base_dir: str, is_continue: bool, exists_ok: bool = False):\n        super().__init__(base_dir, is_continue, exists_ok)\n        self._cur_experiment_name = None\n        self._objects_nums = {}\n\n    def set_namespace(self, name: str) -> \'MultipleFSM\':\n        self._cur_experiment_name = name\n        return self\n\n    def _compile_path(self, obj: FolderRegistrable) -> str:\n        if obj._get_name() not in self._objects_nums:\n            self._objects_nums[obj._get_name()] = 0\n        else:\n            self._objects_nums[obj._get_name()] += 1\n        return os.path.join(self._base_dir, str(self._objects_nums[obj._get_name()]), obj._get_gir())\n'"
neural_pipeline/utils/utils.py,0,"b'def dict_recursive_bypass(dictionary: dict, on_node: callable) -> dict:\n    """"""\n    Recursive bypass dictionary\n\n    :param dictionary:\n    :param on_node: callable for every node, that get value of dict end node as parameters\n    """"""\n    res = {}\n    for k, v in dictionary.items():\n        if isinstance(v, dict):\n            res[k] = dict_recursive_bypass(v, on_node)\n        else:\n            res[k] = on_node(v)\n\n    return res\n\n\ndef dict_pair_recursive_bypass(dictionary1: dict, dictionary2: dict, on_node: callable) -> dict:\n    """"""\n    Recursive bypass dictionary\n\n    :param dictionary1:\n    :param dictionary2:\n    :param on_node: callable for every node, that get value of dict end node as parameters\n    """"""\n    res = {}\n    for k, v in dictionary1.items():\n        if isinstance(v, dict):\n            res[k] = dict_pair_recursive_bypass(v, dictionary2[k], on_node)\n        else:\n            res[k] = on_node(v, dictionary2[k])\n\n    return res\n'"
neural_pipeline/builtin/models/__init__.py,0,b'from. import albunet\n'
neural_pipeline/builtin/models/albunet.py,11,"b'""""""\nThis module created AlbUNet: U-Net with ResNet encoder. This model writed by Alexander Buslaev and spoiled by me.\n\nThis model can be constructed with \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\' encoders.\n\nFor create model just call ``resnet<number>`` method\n""""""\n\nimport torch\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\n\nfrom torch.utils import model_zoo\n\n__all__ = [\'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\nclass UnetDecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.layer = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2)\n        return self.layer(x)\n\n\nclass ConvBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.seq = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, dec, enc):\n        x = torch.cat([dec, enc], dim=1)\n        return self.seq(x)\n\n\nclass AlbUNet(torch.nn.Module):\n    def __init__(self, base_model: torch.nn.Module, num_classes: int, weights_url: str = None):\n        super().__init__()\n        if not hasattr(self, \'decoder_block\'):\n            self.decoder_block = UnetDecoderBlock\n        if not hasattr(self, \'bottleneck_type\'):\n            self.bottleneck_type = ConvBottleneck\n\n        if weights_url is not None:\n            print(""Model weights inited by url"")\n\n            pretrained_weights = model_zoo.load_url(weights_url)\n            model_state_dict = base_model.state_dict()\n            pretrained_weights = {k: v for k, v in pretrained_weights.items() if k in model_state_dict}\n            base_model.load_state_dict(pretrained_weights)\n\n        filters = [64, 64, 128, 256, 512]\n\n        self.bottlenecks = nn.ModuleList([self.bottleneck_type(f * 2, f) for f in reversed(filters[:-1])])\n        self.decoder_stages = nn.ModuleList([self.get_decoder(filters, idx) for idx in range(1, len(filters))])\n\n        self.encoder_stages = nn.ModuleList([self.get_encoder(base_model, idx) for idx in range(len(filters))])\n\n        self.last_upsample = self.decoder_block(filters[0], filters[0])\n        self.final = self.make_final_classifier(filters[0], num_classes)\n\n    def forward(self, x):\n        # you better run this in debugger and see what happens.\n        # initial ideas are U-Net and LinkNet. see papers or blogposts for additional information\n        enc_results = []\n        for stage in self.encoder_stages:\n            x = stage(x)\n            enc_results.append(x.clone())\n\n        for idx, bottleneck in enumerate(self.bottlenecks):\n            rev_idx = - (idx + 1)\n            x = self.decoder_stages[rev_idx](x)\n            x = bottleneck(x, enc_results[rev_idx - 1])\n\n        x = self.last_upsample(x)\n        f = self.final(x)\n        return f\n\n    @staticmethod\n    def make_final_classifier(in_filters, num_classes):\n        return nn.Sequential(\n            nn.Conv2d(in_filters, num_classes, 3, padding=1)\n        )\n\n    @staticmethod\n    def get_encoder(encoder, layer):\n        """"""\n        encoder layers are different sized features from different net depth\n        """"""\n        if layer == 0:\n            return nn.Sequential(\n                encoder.conv1,\n                encoder.bn1,\n                encoder.relu)\n        elif layer == 1:\n            return nn.Sequential(\n                encoder.maxpool,\n                encoder.layer1)\n        elif layer == 2:\n            return encoder.layer2\n        elif layer == 3:\n            return encoder.layer3\n        elif layer == 4:\n            return encoder.layer4\n\n    def get_decoder(self, filters, layer):\n        return self.decoder_block(filters[layer], filters[max(layer - 1, 0)])\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, in_channels=3):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\n\ndef resnet18(classes_num: int, in_channels: int, pretrained: bool = True):\n    """"""\n    Constructs a AlbUNet with ResNet-18 encoder.\n\n    :param classes_num: number of classes (number of masks in output)\n    :param in_channels: number of input channels\n    :param pretrained: If True, returns a model with encoder pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], in_channels)\n    return AlbUNet(model, classes_num, weights_url=model_urls[\'resnet18\'] if pretrained else None)\n\n\ndef resnet34(classes_num: int, in_channels: int, pretrained: bool = True):\n    """"""\n    Constructs a AlbUNet with ResNet-34 encoder.\n\n    :param classes_num: number of classes (number of masks in output)\n    :param in_channels: number of input channels\n    :param pretrained: If True, returns a model with encoder pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], in_channels)\n    return AlbUNet(model, classes_num, weights_url=model_urls[\'resnet34\'] if pretrained else None)\n\n\ndef resnet50(classes_num: int, in_channels: int, pretrained: bool = True):\n    """"""\n    Constructs a AlbUNet with ResNet-50 encoder.\n\n    :param classes_num: number of classes (number of masks in output)\n    :param in_channels: number of input channels\n    :param pretrained: If True, returns a model with encoder pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], in_channels)\n    return AlbUNet(model, classes_num, weights_url=model_urls[\'resnet50\'] if pretrained else None)\n\n\ndef resnet101(classes_num: int, in_channels: int, pretrained: bool = True):\n    """"""\n    Constructs a AlbUNet with ResNet-101 encoder.\n\n    :param classes_num: number of classes (number of masks in output)\n    :param in_channels: number of input channels\n    :param pretrained: If True, returns a model with encoder pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], in_channels)\n    return AlbUNet(model, classes_num, weights_url=model_urls[\'resnet101\'] if pretrained else None)\n\n\ndef resnet152(classes_num: int, in_channels: int, pretrained: bool = True):\n    """"""\n    Constructs a AlbUNet with ResNet-152 encoder.\n\n    :param classes_num: number of classes (number of masks in output)\n    :param in_channels: number of input channels\n    :param pretrained: If True, returns a model with encoder pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], in_channels)\n    return AlbUNet(model, classes_num, weights_url=model_urls[\'resnet152\'] if pretrained else None)\n'"
neural_pipeline/builtin/monitors/__init__.py,0,b''
neural_pipeline/builtin/monitors/mpl.py,0,"b'""""""\nThis module contains Matplotlib monitor interface\n""""""\n\nfrom random import shuffle\n\ntry:\n    import matplotlib.pyplot as plt\n    from matplotlib.ticker import MaxNLocator\nexcept ImportError:\n    import sys\n    print(""Can\'t import Matplotlib in module neural-pipeline.builtin.mpl. Try perform \'pip install matplotlib\'"", file=sys.stderr)\n    sys.exit(1)\n\nimport numpy as np\n\nfrom neural_pipeline import AbstractMonitor\nfrom neural_pipeline.train_config import MetricsGroup\n\n\nclass MPLMonitor(AbstractMonitor):\n    """"""\n    This monitor show all data in Matplotlib plots\n    """"""\n    class _Plot:\n        __cmap = plt.cm.get_cmap(\'hsv\', 10)\n        __cmap_indices = [i for i in range(10)]\n        shuffle(__cmap_indices)\n\n        def __init__(self, names: [str]):\n            self._handle = names[0]\n\n            self._prev_values = {}\n            self._colors = {}\n            self._axis = None\n\n        def add_values(self, values: {}, epoch_idx: int) -> None:\n            for n, v in values.items():\n                self.add_value(n, v, epoch_idx)\n\n        def add_value(self, name: str, val: float, epoch_idx: int) -> None:\n            if name not in self._prev_values:\n                self._prev_values[name] = None\n                self._colors[name] = self.__cmap(self.__cmap_indices[len(self._colors)])\n            prev_value = self._prev_values[name]\n            if prev_value is not None and self._axis is not None:\n                self._axis.plot([prev_value[1], epoch_idx], [prev_value[0], val], label=name, c=self._colors[name])\n            self._prev_values[name] = [val, epoch_idx]\n\n        def place_plot(self, axis) -> None:\n            self._axis = axis\n\n            for n, v in self._prev_values.items():\n                self._axis.scatter(v[1], v[0], label=n, c=self._colors[n])\n\n            self._axis.set_ylabel(self._handle)\n            self._axis.set_xlabel(\'epoch\')\n            self._axis.xaxis.set_major_locator(MaxNLocator(integer=True))\n            self._axis.legend()\n            plt.grid()\n\n    def __init__(self):\n        super().__init__()\n\n        self._realtime = True\n        self._plots = {}\n        self._plots_placed = False\n\n    def update_losses(self, losses: {}):\n        def on_loss(name: str, values: np.ndarray):\n            plot = self._cur_plot([\'loss\', name])\n            plot.add_value(name, np.mean(values), self.epoch_num)\n\n        self._iterate_by_losses(losses, on_loss)\n\n        if not self._plots_placed:\n            self._place_plots()\n            self._plots_placed = True\n\n        if self._realtime:\n            plt.pause(0.01)\n\n    def update_metrics(self, metrics: {}) -> None:\n        for metric in metrics[\'metrics\']:\n            self._process_metric(metric)\n\n        for metrics_group in metrics[\'groups\']:\n            for metric in metrics_group.metrics():\n                self._process_metric(metric, metrics_group.name())\n            for group in metrics_group.groups():\n                self._process_metric(group)\n\n    def realtime(self, is_realtime: bool) -> \'MPLMonitor\':\n        """"""\n        Is need to show data updates in realtime\n\n        :param is_realtime: is need realtime\n        :return: self object\n        """"""\n        self._realtime = is_realtime\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        plt.show()\n\n    def _process_metric(self, cur_metric, parent_tag: str = None):\n        if isinstance(cur_metric, MetricsGroup):\n            for m in cur_metric.metrics():\n                names = self._compile_names(parent_tag, [cur_metric.name(), m.name()])\n                plot = self._cur_plot(names)\n                if m.get_values().size > 0:\n                    plot.add_value(m.name(), np.mean(m.get_values), self.epoch_num)\n        else:\n            values = cur_metric.get_values().astype(np.float32)\n            names = self._compile_names(parent_tag, [cur_metric.name()])\n            plot = self._cur_plot(names)\n            if values.size > 0:\n                plot.add_value(cur_metric.name(), np.mean(values), self.epoch_num)\n\n    @staticmethod\n    def _compile_names(parent_tag: str, names: [str]):\n        if parent_tag is not None:\n            return [parent_tag] + names\n        else:\n            return names\n\n    def _cur_plot(self, names: [str]) -> \'_Plot\':\n        if names[0] not in self._plots:\n            self._plots[names[0]] = self._Plot(names)\n        return self._plots[names[0]]\n\n    def _place_plots(self):\n        number_of_subplots = len(self._plots)\n        idx = 1\n        for n, v in self._plots.items():\n            v.place_plot(plt.subplot(number_of_subplots, 1, idx))\n            idx += 1\n'"
neural_pipeline/builtin/monitors/tensorboard.py,1,"b'""""""\nThis module contains Tensorboard monitor interface\n""""""\n\nimport os\nimport numpy as np\n\ntry:\n    from tensorboardX import SummaryWriter\nexcept ImportError:\n    print(""Looks like tensorboardX doesn\'t installed. Install in via \'pip install tensorboardX\' and try again"")\n\nfrom neural_pipeline.monitoring import AbstractMonitor\nfrom neural_pipeline.data_processor import Model\nfrom neural_pipeline.train_config import AbstractMetric, MetricsGroup\nfrom neural_pipeline.utils.fsm import FileStructManager, FolderRegistrable\n\nimport warnings\n\nwarnings.simplefilter(action=\'ignore\', category=FutureWarning)\n\n\nclass TensorboardMonitor(AbstractMonitor, FolderRegistrable):\n    """"""\n    Class, that manage metrics end events monitoring. It worked with tensorboard. Monitor get metrics after epoch ends and visualise it. Metrics may be float or np.array values. If\n    metric is np.array - it will be shown as histogram and scalars (scalar plots contains mean valuse from array).\n\n    :param fsm: file structure manager\n    :param is_continue: is data processor continue training\n    :param network_name: network name\n    """"""\n\n    def __init__(self, fsm: FileStructManager, is_continue: bool, network_name: str = None):\n        super().__init__()\n        self.__writer = None\n        self.__txt_log_file = None\n\n        fsm.register_dir(self)\n        dir = fsm.get_path(self)\n        if dir is None:\n            return\n\n        dir = os.path.join(dir, network_name) if network_name is not None else dir\n\n        if not (fsm.in_continue_mode() or is_continue) and os.path.exists(dir) and os.path.isdir(dir):\n            idx = 0\n            tmp_dir = dir + ""_v{}"".format(idx)\n            while os.path.exists(tmp_dir) and os.path.isdir(tmp_dir):\n                idx += 1\n                tmp_dir = dir + ""_v{}"".format(idx)\n            dir = tmp_dir\n\n        os.makedirs(dir, exist_ok=True)\n        self.__writer = SummaryWriter(dir)\n        self.__txt_log_file = open(os.path.join(dir, ""log.txt""), \'a\' if is_continue else \'w\')\n\n    def update_metrics(self, metrics: {}) -> None:\n        """"""\n        Update monitor\n\n        :param metrics: metrics dict with keys \'metrics\' and \'groups\'\n        """"""\n        self._update_metrics(metrics[\'metrics\'], metrics[\'groups\'])\n\n    def update_losses(self, losses: {}) -> None:\n        """"""\n        Update monitor\n\n        :param losses: losses values with keys \'train\' and \'validation\'\n        """"""\n        if self.__writer is None:\n            return\n\n        def on_loss(name: str, values: np.ndarray) -> None:\n            self.__writer.add_scalars(\'loss\', {name: np.mean(values)}, global_step=self.epoch_num)\n            self.__writer.add_histogram(\'{}/loss_hist\'.format(name), np.clip(values, -1, 1).astype(np.float32),\n                                        global_step=self.epoch_num, bins=np.linspace(-1, 1, num=11).astype(np.float32))\n\n        self._iterate_by_losses(losses, on_loss)\n\n    def _update_metrics(self, metrics: [AbstractMetric], metrics_groups: [MetricsGroup]) -> None:\n        """"""\n        Update console\n\n        :param metrics: metrics\n        """"""\n\n        def process_metric(cur_metric, parent_tag: str = None):\n            def add_histogram(name: str, vals, step_num, bins):\n                try:\n                    self.__writer.add_histogram(name, vals, step_num, bins)\n                except:\n                    pass\n\n            tag = lambda name: name if parent_tag is None else \'{}/{}\'.format(parent_tag, name)\n\n            if isinstance(cur_metric, MetricsGroup):\n                for m in cur_metric.metrics():\n                    if m.get_values().size > 0:\n                        self.__writer.add_scalars(tag(m.name()), {m.name(): np.mean(m.get_values())}, global_step=self.epoch_num)\n                        add_histogram(tag(m.name()) + \'_hist\',\n                                      np.clip(m.get_values(), m.min_val(), m.max_val()).astype(np.float32),\n                                      self.epoch_num, np.linspace(m.min_val(), m.max_val(), num=11).astype(np.float32))\n            else:\n                values = cur_metric.get_values().astype(np.float32)\n                if values.size > 0:\n                    self.__writer.add_scalar(tag(cur_metric.name()), float(np.mean(values)), global_step=self.epoch_num)\n                    add_histogram(tag(cur_metric.name()) + \'_hist\',\n                                  np.clip(values, cur_metric.min_val(), cur_metric.max_val()).astype(np.float32),\n                                  self.epoch_num, np.linspace(cur_metric.min_val(), cur_metric.max_val(), num=11).astype(np.float32))\n\n        if self.__writer is None:\n            return\n\n        for metric in metrics:\n            process_metric(metric)\n\n        for metrics_group in metrics_groups:\n            for metric in metrics_group.metrics():\n                process_metric(metric, metrics_group.name())\n            for group in metrics_group.groups():\n                process_metric(group, metrics_group.name())\n\n    def update_scalar(self, name: str, value: float, epoch_idx: int = None) -> None:\n        """"""\n        Update scalar on tensorboard\n\n        :param name: the classic tag for TensorboardX\n        :param value: scalar value\n        :param epoch_idx: epoch idx. If doesn\'t set - use last epoch idx stored in this class\n        """"""\n        self.__writer.add_scalar(name, value, global_step=(epoch_idx if epoch_idx is not None else self.epoch_num))\n\n    def write_to_txt_log(self, text: str, tag: str = None) -> None:\n        """"""\n        Write to txt log\n\n        :param text: text that will be writed\n        :param tag: tag\n        """"""\n        self.__writer.add_text(""log"" if tag is None else tag, text, self.epoch_num)\n        text = ""Epoch [{}]"".format(self.epoch_num) + "": "" + text\n        self.__txt_log_file.write(text + \'\\n\')\n        self.__txt_log_file.flush()\n\n    def visualize_model(self, model: Model, tensor) -> None:\n        """"""\n        Visualize model graph\n\n        :param model: :class:`torch.nn.Module` object\n        :param tensor: dummy input for trace model\n        """"""\n        self.__writer.add_graph(model, tensor)\n\n    def close(self):\n        if self.__txt_log_file is not None:\n            self.__txt_log_file.close()\n            self.__txt_log_file = None\n            del self.__txt_log_file\n        if self.__writer is not None:\n            self.__writer.close()\n            self.__writer = None\n            del self.__writer\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def _get_gir(self) -> str:\n        return os.path.join(\'monitors\', \'tensorboard\')\n\n    def _get_name(self) -> str:\n        return \'Tensorboard\'\n'"
