file_path,api_count,code
dqn_learn.py,12,"b'""""""\n    This file is copied/apdated from https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3\n""""""\nimport sys\nimport pickle\nimport numpy as np\nfrom collections import namedtuple\nfrom itertools import count\nimport random\nimport gym.spaces\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.autograd as autograd\n\nfrom utils.replay_buffer import ReplayBuffer\nfrom utils.gym import get_wrapper_by_name\n\nUSE_CUDA = torch.cuda.is_available()\ndtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n\nclass Variable(autograd.Variable):\n    def __init__(self, data, *args, **kwargs):\n        if USE_CUDA:\n            data = data.cuda()\n        super(Variable, self).__init__(data, *args, **kwargs)\n\n""""""\n    OptimizerSpec containing following attributes\n        constructor: The optimizer constructor ex: RMSprop\n        kwargs: {Dict} arguments for constructing optimizer\n""""""\nOptimizerSpec = namedtuple(""OptimizerSpec"", [""constructor"", ""kwargs""])\n\nStatistic = {\n    ""mean_episode_rewards"": [],\n    ""best_mean_episode_rewards"": []\n}\n\ndef dqn_learing(\n    env,\n    q_func,\n    optimizer_spec,\n    exploration,\n    stopping_criterion=None,\n    replay_buffer_size=1000000,\n    batch_size=32,\n    gamma=0.99,\n    learning_starts=50000,\n    learning_freq=4,\n    frame_history_len=4,\n    target_update_freq=10000\n    ):\n\n    """"""Run Deep Q-learning algorithm.\n\n    You can specify your own convnet using q_func.\n\n    All schedules are w.r.t. total number of steps taken in the environment.\n\n    Parameters\n    ----------\n    env: gym.Env\n        gym environment to train on.\n    q_func: function\n        Model to use for computing the q function. It should accept the\n        following named arguments:\n            input_channel: int\n                number of channel of input.\n            num_actions: int\n                number of actions\n    optimizer_spec: OptimizerSpec\n        Specifying the constructor and kwargs, as well as learning rate schedule\n        for the optimizer\n    exploration: Schedule (defined in utils.schedule)\n        schedule for probability of chosing random action.\n    stopping_criterion: (env) -> bool\n        should return true when it\'s ok for the RL algorithm to stop.\n        takes in env and the number of steps executed so far.\n    replay_buffer_size: int\n        How many memories to store in the replay buffer.\n    batch_size: int\n        How many transitions to sample each time experience is replayed.\n    gamma: float\n        Discount Factor\n    learning_starts: int\n        After how many environment steps to start replaying experiences\n    learning_freq: int\n        How many steps of environment to take between every experience replay\n    frame_history_len: int\n        How many past frames to include as input to the model.\n    target_update_freq: int\n        How many experience replay rounds (not steps!) to perform between\n        each update to the target Q network\n    """"""\n    assert type(env.observation_space) == gym.spaces.Box\n    assert type(env.action_space)      == gym.spaces.Discrete\n\n    ###############\n    # BUILD MODEL #\n    ###############\n\n    if len(env.observation_space.shape) == 1:\n        # This means we are running on low-dimensional observations (e.g. RAM)\n        input_arg = env.observation_space.shape[0]\n    else:\n        img_h, img_w, img_c = env.observation_space.shape\n        input_arg = frame_history_len * img_c\n    num_actions = env.action_space.n\n\n    # Construct an epilson greedy policy with given exploration schedule\n    def select_epilson_greedy_action(model, obs, t):\n        sample = random.random()\n        eps_threshold = exploration.value(t)\n        if sample > eps_threshold:\n            obs = torch.from_numpy(obs).type(dtype).unsqueeze(0) / 255.0\n            # Use volatile = True if variable is only used in inference mode, i.e. don\xe2\x80\x99t save the history\n            return model(Variable(obs, volatile=True)).data.max(1)[1].cpu()\n        else:\n            return torch.IntTensor([[random.randrange(num_actions)]])\n\n    # Initialize target q function and q function\n    Q = q_func(input_arg, num_actions).type(dtype)\n    target_Q = q_func(input_arg, num_actions).type(dtype)\n\n    # Construct Q network optimizer function\n    optimizer = optimizer_spec.constructor(Q.parameters(), **optimizer_spec.kwargs)\n\n    # Construct the replay buffer\n    replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n\n    ###############\n    # RUN ENV     #\n    ###############\n    num_param_updates = 0\n    mean_episode_reward = -float(\'nan\')\n    best_mean_episode_reward = -float(\'inf\')\n    last_obs = env.reset()\n    LOG_EVERY_N_STEPS = 10000\n\n    for t in count():\n        ### Check stopping criterion\n        if stopping_criterion is not None and stopping_criterion(env):\n            break\n\n        ### Step the env and store the transition\n        # Store lastest observation in replay memory and last_idx can be used to store action, reward, done\n        last_idx = replay_buffer.store_frame(last_obs)\n        # encode_recent_observation will take the latest observation\n        # that you pushed into the buffer and compute the corresponding\n        # input that should be given to a Q network by appending some\n        # previous frames.\n        recent_observations = replay_buffer.encode_recent_observation()\n\n        # Choose random action if not yet start learning\n        if t > learning_starts:\n            action = select_epilson_greedy_action(Q, recent_observations, t)[0, 0]\n        else:\n            action = random.randrange(num_actions)\n        # Advance one step\n        obs, reward, done, _ = env.step(action)\n        # clip rewards between -1 and 1\n        reward = max(-1.0, min(reward, 1.0))\n        # Store other info in replay memory\n        replay_buffer.store_effect(last_idx, action, reward, done)\n        # Resets the environment when reaching an episode boundary.\n        if done:\n            obs = env.reset()\n        last_obs = obs\n\n        ### Perform experience replay and train the network.\n        # Note that this is only done if the replay buffer contains enough samples\n        # for us to learn something useful -- until then, the model will not be\n        # initialized and random actions should be taken\n        if (t > learning_starts and\n                t % learning_freq == 0 and\n                replay_buffer.can_sample(batch_size)):\n            # Use the replay buffer to sample a batch of transitions\n            # Note: done_mask[i] is 1 if the next state corresponds to the end of an episode,\n            # in which case there is no Q-value at the next state; at the end of an\n            # episode, only the current state reward contributes to the target\n            obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(batch_size)\n            # Convert numpy nd_array to torch variables for calculation\n            obs_batch = Variable(torch.from_numpy(obs_batch).type(dtype) / 255.0)\n            act_batch = Variable(torch.from_numpy(act_batch).long())\n            rew_batch = Variable(torch.from_numpy(rew_batch))\n            next_obs_batch = Variable(torch.from_numpy(next_obs_batch).type(dtype) / 255.0)\n            not_done_mask = Variable(torch.from_numpy(1 - done_mask)).type(dtype)\n\n            if USE_CUDA:\n                act_batch = act_batch.cuda()\n                rew_batch = rew_batch.cuda()\n\n            # Compute current Q value, q_func takes only state and output value for every state-action pair\n            # We choose Q based on action taken.\n            current_Q_values = Q(obs_batch).gather(1, act_batch.unsqueeze(1))\n            # Compute next Q value based on which action gives max Q values\n            # Detach variable from the current graph since we don\'t want gradients for next Q to propagated\n            next_max_q = target_Q(next_obs_batch).detach().max(1)[0]\n            next_Q_values = not_done_mask * next_max_q\n            # Compute the target of the current Q values\n            target_Q_values = rew_batch + (gamma * next_Q_values)\n            # Compute Bellman error\n            bellman_error = target_Q_values - current_Q_values\n            # clip the bellman error between [-1 , 1]\n            clipped_bellman_error = bellman_error.clamp(-1, 1)\n            # Note: clipped_bellman_delta * -1 will be right gradient\n            d_error = clipped_bellman_error * -1.0\n            # Clear previous gradients before backward pass\n            optimizer.zero_grad()\n            # run backward pass\n            current_Q_values.backward(d_error.data.unsqueeze(1))\n\n            # Perfom the update\n            optimizer.step()\n            num_param_updates += 1\n\n            # Periodically update the target network by Q network to target Q network\n            if num_param_updates % target_update_freq == 0:\n                target_Q.load_state_dict(Q.state_dict())\n\n        ### 4. Log progress and keep track of statistics\n        episode_rewards = get_wrapper_by_name(env, ""Monitor"").get_episode_rewards()\n        if len(episode_rewards) > 0:\n            mean_episode_reward = np.mean(episode_rewards[-100:])\n        if len(episode_rewards) > 100:\n            best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n\n        Statistic[""mean_episode_rewards""].append(mean_episode_reward)\n        Statistic[""best_mean_episode_rewards""].append(best_mean_episode_reward)\n\n        if t % LOG_EVERY_N_STEPS == 0 and t > learning_starts:\n            print(""Timestep %d"" % (t,))\n            print(""mean reward (100 episodes) %f"" % mean_episode_reward)\n            print(""best mean reward %f"" % best_mean_episode_reward)\n            print(""episodes %d"" % len(episode_rewards))\n            print(""exploration %f"" % exploration.value(t))\n            sys.stdout.flush()\n\n            # Dump statistics to pickle\n            with open(\'statistics.pkl\', \'wb\') as f:\n                pickle.dump(Statistic, f)\n                print(""Saved to %s"" % \'statistics.pkl\')\n'"
dqn_model.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\nclass DQN(nn.Module):\n    def __init__(self, in_channels=4, num_actions=18):\n        """"""\n        Initialize a deep Q-learning network as described in\n        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n        Arguments:\n            in_channels: number of channel of input.\n                i.e The number of most recent frames stacked together as describe in the paper\n            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n        """"""\n        super(DQN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n        self.fc5 = nn.Linear(512, num_actions)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n        return self.fc5(x)\n\nclass DQN_RAM(nn.Module):\n    def __init__(self, in_features=4, num_actions=18):\n        """"""\n        Initialize a deep Q-learning network for testing algorithm\n            in_features: number of features of input.\n            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n        """"""\n        super(DQN_RAM, self).__init__()\n        self.fc1 = nn.Linear(in_features, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, num_actions)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n'"
main.py,1,"b'import gym\nimport torch.optim as optim\n\nfrom dqn_model import DQN\nfrom dqn_learn import OptimizerSpec, dqn_learing\nfrom utils.gym import get_env, get_wrapper_by_name\nfrom utils.schedule import LinearSchedule\n\nBATCH_SIZE = 32\nGAMMA = 0.99\nREPLAY_BUFFER_SIZE = 1000000\nLEARNING_STARTS = 50000\nLEARNING_FREQ = 4\nFRAME_HISTORY_LEN = 4\nTARGER_UPDATE_FREQ = 10000\nLEARNING_RATE = 0.00025\nALPHA = 0.95\nEPS = 0.01\n\ndef main(env, num_timesteps):\n\n    def stopping_criterion(env):\n        # notice that here t is the number of steps of the wrapped env,\n        # which is different from the number of steps in the underlying env\n        return get_wrapper_by_name(env, ""Monitor"").get_total_steps() >= num_timesteps\n\n    optimizer_spec = OptimizerSpec(\n        constructor=optim.RMSprop,\n        kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n    )\n\n    exploration_schedule = LinearSchedule(1000000, 0.1)\n\n    dqn_learing(\n        env=env,\n        q_func=DQN,\n        optimizer_spec=optimizer_spec,\n        exploration=exploration_schedule,\n        stopping_criterion=stopping_criterion,\n        replay_buffer_size=REPLAY_BUFFER_SIZE,\n        batch_size=BATCH_SIZE,\n        gamma=GAMMA,\n        learning_starts=LEARNING_STARTS,\n        learning_freq=LEARNING_FREQ,\n        frame_history_len=FRAME_HISTORY_LEN,\n        target_update_freq=TARGER_UPDATE_FREQ,\n    )\n\nif __name__ == \'__main__\':\n    # Get Atari games.\n    benchmark = gym.benchmark_spec(\'Atari40M\')\n\n    # Change the index to select a different game.\n    task = benchmark.tasks[3]\n\n    # Run training\n    seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n    env = get_env(task, seed)\n\n    main(env, task.max_timesteps)\n'"
ram.py,1,"b'import gym\nimport torch.optim as optim\n\nfrom dqn_model import DQN_RAM\nfrom dqn_learn import OptimizerSpec, dqn_learing\nfrom utils.gym import get_ram_env, get_wrapper_by_name\nfrom utils.schedule import LinearSchedule\n\nBATCH_SIZE = 32\nGAMMA = 0.99\nREPLAY_BUFFER_SIZE=1000000\nLEARNING_STARTS=50000\nLEARNING_FREQ=4\nFRAME_HISTORY_LEN=1\nTARGER_UPDATE_FREQ=10000\nLEARNING_RATE = 0.00025\nALPHA = 0.95\nEPS = 0.01\n\ndef main(env, num_timesteps=int(4e7)):\n\n    def stopping_criterion(env):\n        # notice that here t is the number of steps of the wrapped env,\n        # which is different from the number of steps in the underlying env\n        return get_wrapper_by_name(env, ""Monitor"").get_total_steps() >= num_timesteps\n\n    optimizer_spec = OptimizerSpec(\n        constructor=optim.RMSprop,\n        kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n    )\n\n    exploration_schedule = LinearSchedule(1000000, 0.1)\n\n    dqn_learing(\n        env=env,\n        q_func=DQN_RAM,\n        optimizer_spec=optimizer_spec,\n        exploration=exploration_schedule,\n        stopping_criterion=stopping_criterion,\n        replay_buffer_size=REPLAY_BUFFER_SIZE,\n        batch_size=BATCH_SIZE,\n        gamma=GAMMA,\n        learning_starts=LEARNING_STARTS,\n        learning_freq=LEARNING_FREQ,\n        frame_history_len=FRAME_HISTORY_LEN,\n        target_update_freq=TARGER_UPDATE_FREQ,\n    )\n\nif __name__ == \'__main__\':\n    # Get Atari games.\n    env = gym.make(\'Pong-ram-v0\')\n\n    # Run training\n    seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n    env = get_ram_env(env, seed)\n\n    main(env)\n'"
utils/__init__.py,0,b''
utils/atari_wrapper.py,0,"b'""""""\n    This file is copied/apdated from https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3\n""""""\nimport numpy as np\nfrom collections import deque\nimport gym\nfrom gym import spaces\nfrom PIL import Image\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env=None, noop_max=30):\n        """"""Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        super(NoopResetEnv, self).__init__(env)\n        self.noop_max = noop_max\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def _reset(self):\n        """""" Do no-op action for a number of steps in [1, noop_max].""""""\n        self.env.reset()\n        noops = np.random.randint(1, self.noop_max + 1)\n        for _ in range(noops):\n            obs, _, _, _ = self.env.step(0)\n        return obs\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env=None):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        super(FireResetEnv, self).__init__(env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def _reset(self):\n        self.env.reset()\n        obs, _, _, _ = self.env.step(1)\n        obs, _, _, _ = self.env.step(2)\n        return obs\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env=None):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        super(EpisodicLifeEnv, self).__init__(env)\n        self.lives = 0\n        self.was_real_done  = True\n        self.was_real_reset = False\n\n    def _step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n            # so its important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def _reset(self):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset()\n            self.was_real_reset = True\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n            self.was_real_reset = False\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env=None, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        super(MaxAndSkipEnv, self).__init__(env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = deque(maxlen=2)\n        self._skip       = skip\n\n    def _step(self, action):\n        total_reward = 0.0\n        done = None\n        for _ in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            self._obs_buffer.append(obs)\n            total_reward += reward\n            if done:\n                break\n\n        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def _reset(self):\n        """"""Clear past frame buffer and init. to first obs. from inner env.""""""\n        self._obs_buffer.clear()\n        obs = self.env.reset()\n        self._obs_buffer.append(obs)\n        return obs\n\ndef _process_frame84(frame):\n    img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n    img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n    img = Image.fromarray(img)\n    resized_screen = img.resize((84, 110), Image.BILINEAR)\n    resized_screen = np.array(resized_screen)\n    x_t = resized_screen[18:102, :]\n    x_t = np.reshape(x_t, [84, 84, 1])\n    return x_t.astype(np.uint8)\n\nclass ProcessFrame84(gym.Wrapper):\n    def __init__(self, env=None):\n        super(ProcessFrame84, self).__init__(env)\n        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1))\n\n    def _step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        return _process_frame84(obs), reward, done, info\n\n    def _reset(self):\n        return _process_frame84(self.env.reset())\n\nclass ClippedRewardsWrapper(gym.Wrapper):\n    def _step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        return obs, np.sign(reward), done, info\n\ndef wrap_deepmind_ram(env):\n    env = EpisodicLifeEnv(env)\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = ClippedRewardsWrapper(env)\n    return env\n\ndef wrap_deepmind(env):\n    assert \'NoFrameskip\' in env.spec.id\n    env = EpisodicLifeEnv(env)\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = ProcessFrame84(env)\n    env = ClippedRewardsWrapper(env)\n    return env\n'"
utils/gym.py,0,"b'""""""\n    This file is copied/apdated from https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3\n""""""\nimport gym\nfrom gym import wrappers\n\nfrom utils.seed import set_global_seeds\nfrom utils.atari_wrapper import wrap_deepmind, wrap_deepmind_ram\n\ndef get_env(task, seed):\n    env_id = task.env_id\n\n    env = gym.make(env_id)\n\n    set_global_seeds(seed)\n    env.seed(seed)\n\n    expt_dir = \'tmp/gym-results\'\n    env = wrappers.Monitor(env, expt_dir, force=True)\n    env = wrap_deepmind(env)\n\n    return env\n\ndef get_ram_env(env, seed):\n    set_global_seeds(seed)\n    env.seed(seed)\n\n    expt_dir = \'/tmp/gym-results\'\n    env = wrappers.Monitor(env, expt_dir, force=True)\n    env = wrap_deepmind_ram(env)\n\n    return env\n\ndef get_wrapper_by_name(env, classname):\n    currentenv = env\n    while True:\n        if classname in currentenv.__class__.__name__:\n            return currentenv\n        elif isinstance(env, gym.Wrapper):\n            currentenv = currentenv.env\n        else:\n            raise ValueError(""Couldn\'t find wrapper named %s""%classname)\n'"
utils/replay_buffer.py,0,"b'""""""\n    This file is copied/apdated from https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3\n""""""\nimport numpy as np\nimport random\n\ndef sample_n_unique(sampling_f, n):\n    """"""Helper function. Given a function `sampling_f` that returns\n    comparable objects, sample n such unique objects.\n    """"""\n    res = []\n    while len(res) < n:\n        candidate = sampling_f()\n        if candidate not in res:\n            res.append(candidate)\n    return res\n\nclass ReplayBuffer(object):\n    def __init__(self, size, frame_history_len):\n        """"""This is a memory efficient implementation of the replay buffer.\n\n        The sepecific memory optimizations use here are:\n            - only store each frame once rather than k times\n              even if every observation normally consists of k last frames\n            - store frames as np.uint8 (actually it is most time-performance\n              to cast them back to float32 on GPU to minimize memory transfer\n              time)\n            - store frame_t and frame_(t+1) in the same buffer.\n\n        For the typical use case in Atari Deep RL buffer with 1M frames the total\n        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes\n\n        Warning! Assumes that returning frame of zeros at the beginning\n        of the episode, when there is less frames than `frame_history_len`,\n        is acceptable.\n\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        frame_history_len: int\n            Number of memories to be retried for each observation.\n        """"""\n        self.size = size\n        self.frame_history_len = frame_history_len\n\n        self.next_idx      = 0\n        self.num_in_buffer = 0\n\n        self.obs      = None\n        self.action   = None\n        self.reward   = None\n        self.done     = None\n\n    def can_sample(self, batch_size):\n        """"""Returns true if `batch_size` different transitions can be sampled from the buffer.""""""\n        return batch_size + 1 <= self.num_in_buffer\n\n    def _encode_sample(self, idxes):\n        obs_batch      = np.concatenate([self._encode_observation(idx)[np.newaxis, :] for idx in idxes], 0)\n        act_batch      = self.action[idxes]\n        rew_batch      = self.reward[idxes]\n        next_obs_batch = np.concatenate([self._encode_observation(idx + 1)[np.newaxis, :] for idx in idxes], 0)\n        done_mask      = np.array([1.0 if self.done[idx] else 0.0 for idx in idxes], dtype=np.float32)\n\n        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask\n\n\n    def sample(self, batch_size):\n        """"""Sample `batch_size` different transitions.\n\n        i-th sample transition is the following:\n\n        when observing `obs_batch[i]`, action `act_batch[i]` was taken,\n        after which reward `rew_batch[i]` was received and subsequent\n        observation  next_obs_batch[i] was observed, unless the epsiode\n        was done which is represented by `done_mask[i]` which is equal\n        to 1 if episode has ended as a result of that action.\n\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n\n        Returns\n        -------\n        obs_batch: np.array\n            Array of shape\n            (batch_size, img_c * frame_history_len, img_h, img_w)\n            and dtype np.uint8\n        act_batch: np.array\n            Array of shape (batch_size,) and dtype np.int32\n        rew_batch: np.array\n            Array of shape (batch_size,) and dtype np.float32\n        next_obs_batch: np.array\n            Array of shape\n            (batch_size, img_c * frame_history_len, img_h, img_w)\n            and dtype np.uint8\n        done_mask: np.array\n            Array of shape (batch_size,) and dtype np.float32\n        """"""\n        assert self.can_sample(batch_size)\n        idxes = sample_n_unique(lambda: random.randint(0, self.num_in_buffer - 2), batch_size)\n        return self._encode_sample(idxes)\n\n    def encode_recent_observation(self):\n        """"""Return the most recent `frame_history_len` frames.\n\n        Returns\n        -------\n        observation: np.array\n            Array of shape (img_h, img_w, img_c * frame_history_len)\n            and dtype np.uint8, where observation[:, :, i*img_c:(i+1)*img_c]\n            encodes frame at time `t - frame_history_len + i`\n        """"""\n        assert self.num_in_buffer > 0\n        return self._encode_observation((self.next_idx - 1) % self.size)\n\n    def _encode_observation(self, idx):\n        end_idx   = idx + 1 # make noninclusive\n        start_idx = end_idx - self.frame_history_len\n        # this checks if we are using low-dimensional observations, such as RAM\n        # state, in which case we just directly return the latest RAM.\n        if len(self.obs.shape) == 2:\n            return self.obs[end_idx-1]\n        # if there weren\'t enough frames ever in the buffer for context\n        if start_idx < 0 and self.num_in_buffer != self.size:\n            start_idx = 0\n        for idx in range(start_idx, end_idx - 1):\n            if self.done[idx % self.size]:\n                start_idx = idx + 1\n        missing_context = self.frame_history_len - (end_idx - start_idx)\n        # if zero padding is needed for missing context\n        # or we are on the boundry of the buffer\n        if start_idx < 0 or missing_context > 0:\n            frames = [np.zeros_like(self.obs[0]) for _ in range(missing_context)]\n            for idx in range(start_idx, end_idx):\n                frames.append(self.obs[idx % self.size])\n            return np.concatenate(frames, 0)\n        else:\n            # this optimization has potential to saves about 30% compute time \\o/\n            img_h, img_w = self.obs.shape[2], self.obs.shape[3]\n            return self.obs[start_idx:end_idx].reshape(-1, img_h, img_w)\n\n    def store_frame(self, frame):\n        """"""Store a single frame in the buffer at the next available index, overwriting\n        old frames if necessary.\n\n        Parameters\n        ----------\n        frame: np.array\n            Array of shape (img_h, img_w, img_c) and dtype np.uint8\n            and the frame will transpose to shape (img_h, img_w, img_c) to be stored\n        Returns\n        -------\n        idx: int\n            Index at which the frame is stored. To be used for `store_effect` later.\n        """"""\n        # make sure we are not using low-dimensional observations, such as RAM\n        if len(frame.shape) > 1:\n            # transpose image frame into (img_c, img_h, img_w)\n            frame = frame.transpose(2, 0, 1)\n\n        if self.obs is None:\n            self.obs      = np.empty([self.size] + list(frame.shape), dtype=np.uint8)\n            self.action   = np.empty([self.size],                     dtype=np.int32)\n            self.reward   = np.empty([self.size],                     dtype=np.float32)\n            self.done     = np.empty([self.size],                     dtype=np.bool)\n\n        self.obs[self.next_idx] = frame\n\n        ret = self.next_idx\n        self.next_idx = (self.next_idx + 1) % self.size\n        self.num_in_buffer = min(self.size, self.num_in_buffer + 1)\n\n        return ret\n\n    def store_effect(self, idx, action, reward, done):\n        """"""Store effects of action taken after obeserving frame stored\n        at index idx. The reason `store_frame` and `store_effect` is broken\n        up into two functions is so that one can call `encode_recent_observation`\n        in between.\n\n        Paramters\n        ---------\n        idx: int\n            Index in buffer of recently observed frame (returned by `store_frame`).\n        action: int\n            Action that was performed upon observing this frame.\n        reward: float\n            Reward that was received when the actions was performed.\n        done: bool\n            True if episode was finished after performing that action.\n        """"""\n        self.action[idx] = action\n        self.reward[idx] = reward\n        self.done[idx]   = done\n'"
utils/schedule.py,0,"b'""""""\n    This file is copied/apdated from https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3\n""""""\nclass Schedule(object):\n    def value(self, t):\n        """"""Value of the schedule at time t""""""\n        raise NotImplementedError()\n\nclass ConstantSchedule(object):\n    def __init__(self, value):\n        """"""Value remains constant over time.\n        Parameters\n        ----------\n        value: float\n            Constant value of the schedule\n        """"""\n        self._v = value\n\n    def value(self, t):\n        """"""See Schedule.value""""""\n        return self._v\n\ndef linear_interpolation(l, r, alpha):\n    return l + alpha * (r - l)\n\nclass PiecewiseSchedule(object):\n    def __init__(self, endpoints, interpolation=linear_interpolation, outside_value=None):\n        """"""Piecewise schedule.\n        endpoints: [(int, int)]\n            list of pairs `(time, value)` meanining that schedule should output\n            `value` when `t==time`. All the values for time must be sorted in\n            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n            time passed between `time_a` and `time_b` for time `t`.\n        interpolation: lambda float, float, float: float\n            a function that takes value to the left and to the right of t according\n            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n            right endpoint that t has covered. See linear_interpolation for example.\n        outside_value: float\n            if the value is requested outside of all the intervals sepecified in\n            `endpoints` this value is returned. If None then AssertionError is\n            raised when outside value is requested.\n        """"""\n        idxes = [e[0] for e in endpoints]\n        assert idxes == sorted(idxes)\n        self._interpolation = interpolation\n        self._outside_value = outside_value\n        self._endpoints      = endpoints\n\n    def value(self, t):\n        """"""See Schedule.value""""""\n        for (l_t, l), (r_t, r) in zip(self._endpoints[:-1], self._endpoints[1:]):\n            if l_t <= t and t < r_t:\n                alpha = float(t - l_t) / (r_t - l_t)\n                return self._interpolation(l, r, alpha)\n\n        # t does not belong to any of the pieces, so doom.\n        assert self._outside_value is not None\n        return self._outside_value\n\nclass LinearSchedule(object):\n    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n        """"""Linear interpolation between initial_p and final_p over\n        schedule_timesteps. After this many timesteps pass final_p is\n        returned.\n        Parameters\n        ----------\n        schedule_timesteps: int\n            Number of timesteps for which to linearly anneal initial_p\n            to final_p\n        initial_p: float\n            initial output value\n        final_p: float\n            final output value\n        """"""\n        self.schedule_timesteps = schedule_timesteps\n        self.final_p            = final_p\n        self.initial_p          = initial_p\n\n    def value(self, t):\n        """"""See Schedule.value""""""\n        fraction  = min(float(t) / self.schedule_timesteps, 1.0)\n        return self.initial_p + fraction * (self.final_p - self.initial_p)\n'"
utils/seed.py,1,"b'""""""\n    This file is copied/apdated from https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3\n""""""\nimport numpy as np\nimport random\n\ndef set_global_seeds(i):\n    try:\n        import torch\n    except ImportError:\n        pass\n    else:\n        torch.manual_seed(i)\n    np.random.seed(i)\n    random.seed(i)\n'"
