file_path,api_count,code
swa.py,7,"b'#!/usr/bin/env python\n\n""""""\nStochastic Weight Averaging (SWA)\n\nAveraging Weights Leads to Wider Optima and Better Generalization\n\nhttps://github.com/timgaripov/swa\n""""""\nimport torch\nimport models\nfrom tqdm import tqdm\n\n\ndef moving_average(net1, net2, alpha=1.):\n    for param1, param2 in zip(net1.parameters(), net2.parameters()):\n        param1.data *= (1.0 - alpha)\n        param1.data += param2.data * alpha\n\n\ndef _check_bn(module, flag):\n    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n        flag[0] = True\n\n\ndef check_bn(model):\n    flag = [False]\n    model.apply(lambda module: _check_bn(module, flag))\n    return flag[0]\n\n\ndef reset_bn(module):\n    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n        module.running_mean = torch.zeros_like(module.running_mean)\n        module.running_var = torch.ones_like(module.running_var)\n\n\ndef _get_momenta(module, momenta):\n    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n        momenta[module] = module.momentum\n\n\ndef _set_momenta(module, momenta):\n    if issubclass(module.__class__, torch.nn.modules.batchnorm._BatchNorm):\n        module.momentum = momenta[module]\n\n\ndef bn_update(loader, model):\n    """"""\n        BatchNorm buffers update (if any).\n        Performs 1 epochs to estimate buffers average using train dataset.\n        :param loader: train dataset loader for buffers average estimation.\n        :param model: model being update\n        :return: None\n    """"""\n    if not check_bn(model):\n        return\n    model.train()\n    momenta = {}\n    model.apply(reset_bn)\n    model.apply(lambda module: _get_momenta(module, momenta))\n    n = 0\n\n    pbar = tqdm(loader, unit=""images"", unit_scale=loader.batch_size)\n    for batch in pbar:\n        image_ids, input, targets = batch[\'image_id\'], batch[\'input\'], batch[\'mask\']\n        input = input.cuda()\n        b = input.size(0)\n\n        momentum = b / (n + b)\n        for module in momenta.keys():\n            module.momentum = momentum\n\n        model(input)\n        n += b\n\n    model.apply(lambda module: _set_momenta(module, momenta))\n\n\nif __name__ == \'__main__\':\n    import argparse\n    from pathlib import Path\n    from torchvision.transforms import Compose\n    from torch.utils.data import DataLoader\n    from transforms import PrepareImageAndMask, PadToNxN, HWCtoCHW\n    from datasets import SaltIdentification\n\n    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""--input"", type=str, help=\'input directory\')\n    parser.add_argument(""--output"", type=str, default=\'swa_model.pth\', help=\'output model file\')\n    parser.add_argument(""--batch-size"", type=int, default=16, help=\'batch size\')\n    args = parser.parse_args()\n\n    directory = Path(args.input)\n    files = [f for f in directory.iterdir() if f.suffix == "".pth""]\n    assert(len(files) > 1)\n\n    net = models.load(files[0])\n    for i, f in enumerate(files[1:]):\n        net2 = models.load(f)\n        moving_average(net, net2, 1. / (i + 2))\n\n    img_size = 128\n    batch_size = 16\n    train_transform = Compose([PrepareImageAndMask(), PadToNxN(img_size), HWCtoCHW()])\n    train_dataset = SaltIdentification(mode=\'train\', transform=train_transform, preload=True)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n    net.cuda()\n    bn_update(train_dataloader, net)\n\n    models.save(net, args.output)'"
test.py,9,"b'#!/usr/bin/env python\n""""""Test UNet and create a Kaggle submission.""""""\n__author__ = \'Erdene-Ochir Tuguldur, Yuan Xu\'\n\nimport time\nimport argparse\nfrom tqdm import tqdm\n\nimport pandas as pd\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport models\nfrom torchvision.transforms import *\n\nfrom datasets import *\nfrom transforms import *\n\nfrom utils import rlenc, rlenc_np, FasterRle, gzip_save\n\norig_img_size = 101\nimg_size = 128\npadding = compute_padding(orig_img_size, orig_img_size, img_size)\nd_y0, d_y1, d_x0, d_x1 = padding\ny0, y1, x0, x1 = d_y0, d_y0 + orig_img_size, d_x0, d_x0 + orig_img_size\n\n\ndef predict(model, batch, flipped_batch, use_gpu):\n    image_ids, inputs = batch[\'image_id\'], batch[\'input\']\n    if use_gpu:\n        inputs = inputs.cuda()\n    outputs, _, _ = model(inputs)\n    probs = torch.sigmoid(outputs)\n\n    if flipped_batch is not None:\n        flipped_image_ids, flipped_inputs = flipped_batch[\'image_id\'], flipped_batch[\'input\']\n        # assert image_ids == flipped_image_ids\n        if use_gpu:\n            flipped_inputs = flipped_inputs.cuda()\n        flipped_outputs, _, _ = model(flipped_inputs)\n        flipped_probs = torch.sigmoid(flipped_outputs)\n\n        probs += torch.flip(flipped_probs, (3,))  # flip back and add\n        probs *= 0.5\n\n    probs = probs.squeeze(1).cpu().numpy()\n    if args.resize:\n        probs = np.swapaxes(probs, 0, 2)\n        probs = cv2.resize(probs, (orig_img_size, orig_img_size), interpolation=cv2.INTER_LINEAR)\n        probs = np.swapaxes(probs, 0, 2)\n    else:\n        probs = probs[:, y0:y1, x0:x1]\n    return probs\n\n\ndef test():\n    test_transform = Compose([PrepareImageAndMask(),\n                              ResizeToNxN(img_size) if args.resize else PadToNxN(img_size), HWCtoCHW()])\n    test_dataset = SaltIdentification(mode=\'test\', transform=test_transform, preload=False)\n    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.dataload_workers_nums)\n\n    flipped_test_transform = Compose([PrepareImageAndMask(), HorizontalFlip(),\n                                      ResizeToNxN(img_size) if args.resize else PadToNxN(img_size), HWCtoCHW()])\n    flipped_test_dataset = SaltIdentification(mode=\'test\', transform=flipped_test_transform, preload=False)\n    flipped_test_dataloader_iter = iter(DataLoader(flipped_test_dataset, batch_size=args.batch_size,\n                                                   num_workers=args.dataload_workers_nums))\n\n    model.eval()\n    torch.set_grad_enabled(False)\n\n    prediction = {}\n    submission = {}\n    pbar = tqdm(test_dataloader, unit=""images"", unit_scale=test_dataloader.batch_size, disable=None)\n\n    empty_images_count = 0\n    for batch in pbar:\n        if args.tta:\n            flipped_batch = next(flipped_test_dataloader_iter)\n        else:\n            flipped_batch = None\n\n        probs = predict(model, batch, flipped_batch, use_gpu=use_gpu)\n        pred = probs > args.threshold\n        empty_images_count += (pred.sum(axis=(1, 2)) == 0).sum()\n\n        probs_uint16 = (65535 * probs).astype(dtype=np.uint16)\n\n        image_ids = batch[\'image_id\']\n        prediction.update(dict(zip(image_ids, probs_uint16)))\n        rle = rlenc_np(pred)\n        submission.update(dict(zip(image_ids, rle)))\n\n    empty_images_percentage = empty_images_count / len(prediction)\n    print(""empty images: %.2f%% (in public LB 38%%)"" % (100 * empty_images_percentage))\n\n    gzip_save(\'-\'.join([args.output_prefix, \'probabilities.pkl.gz\']), prediction)\n    sub = pd.DataFrame.from_dict(submission, orient=\'index\')\n    sub.index.names = [\'id\']\n    sub.columns = [\'rle_mask\']\n    sub.to_csv(\'-\'.join([args.output_prefix, \'submission.csv\']))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""--batch-size"", type=int, default=30, help=\'batch size\')\n    parser.add_argument(""--dataload-workers-nums"", type=int, default=8, help=\'number of workers for dataloader\')\n    parser.add_argument(\'--tta\', action=\'store_true\', help=\'test time augmentation\')\n    parser.add_argument(\'--seed\', type=int, default=None, help=\'manual seed for deterministic\')\n    parser.add_argument(""--threshold"", type=float, default=0.5, help=\'probability threshold\')\n    parser.add_argument(""--output-prefix"", type=str, default=\'noprefix\', help=\'prefix string for output files\')\n    parser.add_argument(\'--resize\', action=\'store_true\', help=\'resize to 128x128 instead of reflective padding\')\n    parser.add_argument(""model"", help=\'a pretrained neural network model\')\n    args = parser.parse_args()\n\n    use_gpu = torch.cuda.is_available()\n    print(\'use_gpu\', use_gpu)\n\n    print(""loading model..."")\n    model = models.load(args.model)\n    model.float()\n\n    if use_gpu:\n        if args.seed is not None:\n            torch.manual_seed(args.seed)\n            torch.backends.cudnn.deterministic = True\n        else:\n            torch.backends.cudnn.benchmark = True\n        model.cuda()\n\n    print(""testing %s..."" % args.model)\n    since = time.time()\n    test()\n    time_elapsed = time.time() - since\n    time_str = \'total time elapsed: {:.0f}h {:.0f}m {:.0f}s \'.format(time_elapsed // 3600, time_elapsed % 3600 // 60,\n                                                                     time_elapsed % 60)\n    print(""finished"")\n'"
train.py,10,"b'#!/usr/bin/env python\n""""""Train UNet for the Kaggle TGS salt identification challenge: https://www.kaggle.com/c/tgs-salt-identification-challenge""""""\n__author__ = \'Erdene-Ochir Tuguldur, Yuan Xu\'\n\nimport time\nimport argparse\nimport os\nfrom datetime import datetime\nimport socket\nfrom pathlib import Path\nfrom tqdm import tqdm, trange\n\nfrom tensorboardX import SummaryWriter\nimport torch\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torchvision.transforms import *\nimport torchvision.utils as vutils\n\nfrom utils.metrics import calc_metric\nfrom datasets import *\nfrom transforms import *\nimport models\nfrom utils import create_optimizer, choose_device, create_lr_scheduler\n\nparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(""--comment"", type=str, default=\'\', help=\'comment in tensorboard title\')\nparser.add_argument(\'--device\', default=\'auto\', choices=[\'cuda\', \'cpu\'], help=\'running with cpu or cuda\')\nparser.add_argument(""--data-fold"", default=\'fold0\', choices=[\'fold{}\'.format(s) for s in [\'01\'] + list(range(10))],\n                    help=\'name of data split fold\')\nparser.add_argument(""--batch-size"", type=int, default=64, help=\'batch size\')\nparser.add_argument(""--dataload-workers-nums"", type=int, default=8, help=\'number of workers for dataloader\')\nparser.add_argument(""--weight-decay"", type=float, default=0.0001, help=\'weight decay\')\nparser.add_argument(""--optim"", choices=[\'sgd\', \'adam\', \'adamw\'], default=\'sgd\',\n                    help=\'choices of optimization algorithms\')\nparser.add_argument(\'--fp16-loss-scale\', default=None, type=float,\n                    help=\'loss scale factor for mixed-precision training, 0 means dynamic loss scale\')\nparser.add_argument(\'--gradient-accumulation\', type=int, default=1,\n                    help=\'accumulate gradients over number of batches\')\nparser.add_argument(""--learning-rate"", type=float, default=0.01, help=\'learning rate for optimization\')\nparser.add_argument(""--lr-scheduler"", choices=[\'plateau\', \'step\', \'milestones\', \'cos\', \'findlr\', \'noam\', \'clr\'],\n                    default=\'step\', help=\'method to adjust learning rate\')\nparser.add_argument(""--lr-scheduler-patience"", type=int, default=15,\n                    help=\'lr scheduler plateau: Number of epochs with no improvement after which learning rate will be reduced\')\nparser.add_argument(""--lr-scheduler-step-size"", type=int, default=100,\n                    help=\'lr scheduler step: number of epochs of learning rate decay.\')\nparser.add_argument(""--lr-scheduler-gamma"", type=float, default=0.1,\n                    help=\'learning rate is multiplied by the gamma to decrease it\')\nparser.add_argument(""--lr-scheduler-warmup"", type=int, default=10,\n                    help=\'The number of epochs to linearly increase the learning rate. (noam only)\')\nparser.add_argument(""--max-epochs"", type=int, default=350, help=\'max number of epochs\')\nparser.add_argument(""--resume"", type=str, help=\'checkpoint file to resume\')\nparser.add_argument(\'--resume-without-optimizer\', action=\'store_true\', help=\'resume but don\\\'t use optimizer state\')\nparser.add_argument(""--model"", choices=[\'unet\', \'danet\'], default=\'unet\', help=\'model of NN\')\nparser.add_argument(""--loss-on-center"", action=\'store_true\', help=\'loss on image without padding\')\nparser.add_argument(""--drop-mask-threshold"", type=int, default=0, help=\'drop problematic masks during training\')\nparser.add_argument(""--debug"", action=\'store_true\', help=\'write debug images\')\nparser.add_argument(""--disable-cutout"", action=\'store_true\', help=\'disable cutout data augmentation\')\nparser.add_argument(\'--pretrained\', default=\'imagenet\', choices=(\'imagenet\', \'coco\', \'oid\'),\n                    help=\'dataset name for pretrained model\')\nparser.add_argument(""--basenet"", choices=models.BASENET_CHOICES, default=\'resnet34\', help=\'model of basenet\')\ncurrent_time = datetime.now().strftime(\'%b%d_%H-%M-%S\')\ndefault_log_dir = os.path.join(\'runs\', current_time + \'_\' + socket.gethostname())\nparser.add_argument(\'--log-dir\', type=str, default=default_log_dir, help=\'Location to save logs and checkpoints\')\nparser.add_argument(\'--vtf\', action=\'store_true\', help=\'validation time flip augmentation\')\nparser.add_argument(\'--resize\', action=\'store_true\', help=\'resize to 128x128 instead of reflective padding\')\nargs = parser.parse_args()\n\nif args.resize:\n    # if resize is used, loss on center doesn\'t make sense\n    args.loss_on_center = False\n\ndevice = choose_device(args.device)\nuse_gpu = device.type == \'cuda\'\n\norig_img_size = 101\nimg_size = 128\npadding = compute_padding(orig_img_size, orig_img_size, img_size)\n\ngeometric_transform_prob = 0.5 * 0.25\ngeometric_transform = Compose([RandomApply([CropAndRescale(max_scale=0.2)], p=geometric_transform_prob),\n                               RandomApply([HorizontalShear(max_scale=0.07)], p=geometric_transform_prob),\n                               RandomApply([Rotation(max_angle=15)], p=geometric_transform_prob),\n                               RandomApply([ElasticDeformation(max_distort=0.15)], p=geometric_transform_prob)])\nbrightness_transform_prob = 0.5 * 0.33\nbrightness_transform = Compose([RandomApply([BrightnessShift(max_value=0.1)], p=brightness_transform_prob),\n                                RandomApply([BrightnessScaling(max_value=0.08)], p=brightness_transform_prob),\n                                RandomApply([GammaChange(max_value=0.08)], p=brightness_transform_prob)])\ntrain_transform = Compose([PrepareImageAndMask(),\n                           RandomApply([Cutout(1, 30)], p=0.0 if args.disable_cutout else 0.5),\n                           RandomApply([HorizontalFlip()]),\n                           geometric_transform,\n                           brightness_transform,\n                           ResizeToNxN(img_size) if args.resize else PadToNxN(img_size), HWCtoCHW()])\nvalid_transform = Compose([PrepareImageAndMask(),\n                           ResizeToNxN(img_size) if args.resize else PadToNxN(img_size), HWCtoCHW()])\ndata_fold_id = args.data_fold[len(\'fold\'):]\nif len(data_fold_id) == 1:\n    list_train = \'list_train{}_3600\'\n    list_vaild = \'list_valid{}_400\'\nelif len(data_fold_id) == 2:\n    list_train = \'list_train{}_3200\'\n    list_vaild = \'list_valid{}_800\'\nelse:\n    raise RuntimeError(""unknown fold {}"".format(args.data_fold))\n\ntrain_dataset = SaltIdentification(mode=\'train\', name=list_train.format(data_fold_id),\n                                   transform=train_transform, preload=True, mask_threshold=args.drop_mask_threshold)\nvalid_dataset = SaltIdentification(mode=\'train\', name=list_vaild.format(data_fold_id),\n                                   transform=valid_transform, preload=True)\nif args.vtf:\n    flipped_valid_transform = Compose([PrepareImageAndMask(), HorizontalFlip(),\n                                       ResizeToNxN(img_size) if args.resize else PadToNxN(img_size), HWCtoCHW()])\n    flipped_valid_dataset = SaltIdentification(mode=\'train\', name=\'list_valid{}_400\'.format(data_fold_id),\n                                               transform=flipped_valid_transform, preload=True)\n    valid_dataset = ConcatDataset([valid_dataset, flipped_valid_dataset])\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size,\n                              num_workers=args.dataload_workers_nums, drop_last=True)\nvalid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=args.batch_size,\n                              num_workers=args.dataload_workers_nums)\n\n# a name used to save checkpoints etc.\nfull_name = \'%s_%s_%s_%s_bs%d_lr%.1e_wd%.1e\' % (\n    args.model, args.data_fold, args.optim, args.lr_scheduler, args.batch_size, args.learning_rate, args.weight_decay)\nif args.comment:\n    full_name = \'%s_%s\' % (full_name, args.comment)\n\nmodel = models.create(args.model, basenet=args.basenet, pretrained=args.pretrained)\n\nmodel, optimizer = create_optimizer(model, args.optim, args.learning_rate, args.weight_decay,\n                                    momentum=0.9,\n                                    fp16_loss_scale=args.fp16_loss_scale,\n                                    device=device)\n\nlr_scheduler = create_lr_scheduler(optimizer, **vars(args))\n\nstart_timestamp = int(time.time() * 1000)\nstart_epoch = 0\nbest_loss = 1e10\nbest_metric = 0\nbest_accuracy = 0\nglobal_step = 0\n\nif args.resume:\n    print(""resuming a checkpoint \'%s\'"" % args.resume)\n    if os.path.exists(args.resume):\n        saved_checkpoint = torch.load(args.resume)\n        old_model = models.load(saved_checkpoint[\'model_file\'])\n        model.module.load_state_dict(old_model.state_dict())\n        model.float()\n\n        if not args.resume_without_optimizer:\n            optimizer.load_state_dict(saved_checkpoint[\'optimizer\'])\n            lr_scheduler.load_state_dict(saved_checkpoint[\'lr_scheduler\'])\n            best_loss = saved_checkpoint.get(\'best_loss\', best_loss)\n            best_metric = saved_checkpoint.get(\'best_metric\', best_metric)\n            best_accuracy = saved_checkpoint.get(\'best_accuracy\', best_accuracy)\n            start_epoch = saved_checkpoint.get(\'epoch\', start_epoch)\n            global_step = saved_checkpoint.get(\'step\', global_step)\n\n        del saved_checkpoint  # reduce memory\n        del old_model\n    else:\n        print("">\\n>\\n>\\n>\\n>\\n>"")\n        print("">Warning the checkpoint \'%s\' doesn\'t exist! training from scratch!"" % args.resume)\n        print("">\\n>\\n>\\n>\\n>\\n>"")\n\n\ndef get_lr():\n    return optimizer.param_groups[0][\'lr\']\n\n\nprint(""logging into {}"".format(args.log_dir))\nwriter = SummaryWriter(log_dir=args.log_dir)\ncheckpoint_dir = Path(args.log_dir) / \'checkpoints\'\ncheckpoint_dir.mkdir(parents=True, exist_ok=True)\nmodels_dir = Path(args.log_dir) / \'models\'\nmodels_dir.mkdir(parents=True, exist_ok=True)\n\n\ndef remove_padding(data):\n    d_y0, d_y1, d_x0, d_x1 = padding\n    y0, y1, x0, x1 = d_y0, d_y0 + orig_img_size, d_x0, d_x0 + orig_img_size\n    if data.dim() == 3:\n        return data[:, y0:y1, x0:x1]\n    elif data.dim() == 4:\n        return data[:, :, y0:y1, x0:x1]\n    raise RuntimeError(""unspported dim {}"".format(data.dim()))\n\n\ndef train(epoch, phase=\'train\'):\n    global global_step, best_loss, best_metric, best_accuracy\n\n    if phase == \'train\':\n        writer.add_scalar(\'%s/learning_rate\' % phase, get_lr(), epoch)\n\n    model.train() if phase == \'train\' else model.eval()\n    torch.set_grad_enabled(True) if phase == \'train\' else torch.set_grad_enabled(False)\n    dataloader = train_dataloader if phase == \'train\' else valid_dataloader\n\n    running_loss, running_metric, running_accuracy = 0.0, 0.0, 0.0\n    worst_loss, worst_metric = best_loss, best_metric\n    it, total = 0, 0\n\n    if phase == \'valid\':\n        total_probs = []\n        total_truth = []\n\n    pbar_disable = False if epoch == start_epoch else None\n    pbar = tqdm(dataloader, unit=""images"", unit_scale=dataloader.batch_size, disable=pbar_disable)\n    for batch in pbar:\n        image_ids, inputs, targets = batch[\'image_id\'], batch[\'input\'], batch[\'mask\']\n\n        if use_gpu:\n            inputs = inputs.cuda()\n            targets = targets.cuda()\n\n        # forward\n        logit, logit_pixel, logit_image = model(inputs)\n\n        # look at the center only\n        if args.loss_on_center:\n            logit = remove_padding(logit)\n            logit_pixel = (remove_padding(l) for l in logit_pixel)\n            targets = remove_padding(targets)\n\n        truth_pixel = targets\n        truth_image = (truth_pixel.sum(dim=(1, 2)) > 0).float()\n        loss = models.deep_supervised_criterion(logit, logit_pixel, logit_image, truth_pixel, truth_image)\n\n        if not args.loss_on_center and not args.resize:\n            logit = remove_padding(logit)\n            targets = remove_padding(targets)\n\n        probs = torch.sigmoid(logit).squeeze(1)\n        # predictions = probs.squeeze(1) > 0.5\n\n        if phase == \'train\':\n            # backward\n            optimizer.backward(loss / args.gradient_accumulation)\n            if it % args.gradient_accumulation == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n\n        # statistics\n        it += 1\n        global_step += 1\n        loss = loss.item()\n        running_loss += (loss * targets.size(0))\n        total += targets.size(0)\n\n        writer.add_scalar(\'%s/loss\' % phase, loss, global_step)\n\n        targets_numpy = targets.cpu().numpy()\n        probs_numpy = probs.cpu().detach().numpy()\n        predictions_numpy = probs_numpy > 0.5  # predictions.cpu().numpy()\n        metric_array = calc_metric(targets_numpy, predictions_numpy, type=\'iou\', size_average=False)\n        metric = metric_array.mean()\n        running_metric += metric_array.sum()\n\n        running_accuracy += calc_metric(targets_numpy, predictions_numpy, type=\'pixel_accuracy\',\n                                        size_average=False).sum()\n\n        if phase == \'valid\':\n            total_truth.append(targets_numpy)\n            total_probs.append(probs_numpy)\n\n        visualize_output = False\n        if worst_loss > loss:\n            worst_loss = loss\n            visualize_output = True\n        if worst_metric < metric:\n            worst_metric = metric\n            visualize_output = True\n        if visualize_output and args.debug:\n            # sort samples by metric\n            ind = np.argsort(metric_array)\n            images = remove_padding(inputs.cpu())\n            images = images[ind]\n            probs = probs[ind].cpu()\n            predictions = predictions[ind].cpu()\n            targets = targets[ind].cpu()\n\n            preds = torch.cat([probs] * 3, 1)\n            mask = torch.cat([targets.unsqueeze(1)] * 3, 1)\n            all = images.clone()\n            all[:, 0] = torch.max(images[:, 0], predictions.float())\n            all[:, 1] = torch.max(images[:, 1], targets)\n            all = torch.cat((torch.cat((all, images), 3), torch.cat((preds, mask), 3)), 2)\n            all_grid = vutils.make_grid(all, nrow=4, normalize=False, pad_value=1)\n            writer.add_image(\'%s/img-mask-pred\' % phase, all_grid, global_step)\n\n        # update the progress bar\n        pbar.set_postfix({\n            \'loss\': ""%.05f"" % (running_loss / total),\n            \'metric\': ""%.03f"" % (running_metric / total)\n        })\n\n    epoch_loss = running_loss / total\n    epoch_metric = running_metric / total\n    epoch_accuracy = running_accuracy / total\n    writer.add_scalar(\'%s/metric\' % phase, epoch_metric, epoch)\n    writer.add_scalar(\'%s/accuracy\' % phase, epoch_accuracy, epoch)\n    writer.add_scalar(\'%s/epoch_loss\' % phase, epoch_loss, epoch)\n\n    if phase == \'valid\':\n\n        def save_checkpoint(name):\n            cycle = (\'-cycle%d\' % (epoch // args.lr_scheduler_step_size)) if args.lr_scheduler == \'clr\' else \'\'\n            model_name = name + \'-model\'\n            model_file_name = \'%d-%s-%s%s.pth\' % (start_timestamp, model_name, full_name, cycle)\n            model_file = models_dir / model_file_name\n            models.save(model, model_file)\n            mode_file_simple = Path(models_dir / (model_name + \'-%s%s.pth\' % (args.data_fold, cycle)))\n            if mode_file_simple.is_symlink() or mode_file_simple.exists():\n                mode_file_simple.unlink()\n            mode_file_simple.symlink_to(model_file.relative_to(mode_file_simple.parent))\n\n            checkpoint = {\n                \'epoch\': epoch,\n                \'step\': global_step,\n                \'model_file\': str(model_file),\n                \'best_loss\': best_loss,\n                \'best_metric\': best_metric,\n                \'best_accuracy\': best_accuracy,\n                \'optimizer\': optimizer.state_dict(),\n                \'lr_scheduler\': lr_scheduler.state_dict()\n            }\n            checkpoint_filename = name + \'-checkpoint-%s%s.pth\' % (full_name, cycle)\n            checkpoint_file = checkpoint_dir / checkpoint_filename\n            torch.save(checkpoint, checkpoint_file)\n            checkpoint_file_simple = Path(checkpoint_dir / (name + \'-checkpoint-%s%s.pth\' % (args.data_fold, cycle)))\n            if checkpoint_file_simple.is_symlink() or checkpoint_file_simple.exists():\n                checkpoint_file_simple.unlink()\n            checkpoint_file_simple.symlink_to(checkpoint_file.relative_to(checkpoint_file_simple.parent))\n\n        if epoch_loss < best_loss:\n            best_loss = epoch_loss\n            save_checkpoint(\'best-loss\')\n        if epoch_metric > best_metric:\n            best_metric = epoch_metric\n            save_checkpoint(\'best-metric\')\n        if epoch_accuracy > best_accuracy:\n            best_accuracy = epoch_accuracy\n            save_checkpoint(\'best-accuracy\')\n\n        save_checkpoint(\'last\')\n    return epoch_loss, epoch_metric, epoch_accuracy\n\n\nprint(""training %s..."" % args.model)\npbar_epoch = trange(start_epoch, args.max_epochs)\n\n# import cProfile\n# pr = cProfile.Profile()\n# pr.enable()\n\nfor epoch in pbar_epoch:\n    if args.lr_scheduler != \'plateau\':\n        if args.lr_scheduler == \'clr\':\n            if epoch % args.lr_scheduler_step_size == 0:\n                # reset best loss and metric for every cycle\n                best_loss = 1e10\n                best_metric = 0\n            lr_scheduler.step(epoch % args.lr_scheduler_step_size)\n        else:\n            lr_scheduler.step()\n\n    train_epoch_loss, train_epoch_metric, train_epoch_epoch_accuracy = train(epoch, phase=\'train\')\n    valid_epoch_loss, valid_epoch_metric, valid_epoch_epoch_accuracy = train(epoch, phase=\'valid\')\n\n    if args.lr_scheduler == \'plateau\':\n        lr_scheduler.step(metrics=valid_epoch_loss)\n\n    pbar_epoch.set_postfix({\'lr\': \'%.02e\' % get_lr(),\n                            \'train\': \'%.03f/%.03f/%.03f\' % (\n                            train_epoch_loss, train_epoch_metric, train_epoch_epoch_accuracy),\n                            \'val\': \'%.03f/%.03f/%.03f\' % (\n                            valid_epoch_loss, valid_epoch_metric, valid_epoch_epoch_accuracy),\n                            \'best val\': \'%.03f/%.03f/%.03f\' % (best_loss, best_metric, best_accuracy)},\n                           refresh=False)\n#    break\n# pr.disable()\n# pr.print_stats(\'cumulative\')\n# pr.dump_stats(\'test.profile\')\n\nprint(""finished data fold {}"".format(args.data_fold))\nprint(""best valid loss: %.05f, best valid metric: %.03f%%"" % (best_loss, best_metric))\n'"
datasets/__init__.py,0,b'from .salt_identification import SaltIdentification\n'
datasets/salt_identification.py,1,"b'""""""Dataset class for the Kaggle Salt Identification Challenge.""""""\n\n__author__ = \'Erdene-Ochir Tuguldur, Yuan Xu\'\n\nimport os\nimport copy\nimport pandas as pd\nimport skimage.io\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset\n\n\ndef get_test_image_ids(name):\n    script_dir_path = os.path.dirname(os.path.realpath(__file__))\n    dataset_dir_path = os.path.join(script_dir_path, name)\n    all_files = os.listdir(os.path.join(dataset_dir_path, \'images\'))\n    image_ids = []\n    for file_name in all_files:\n        if file_name.endswith("".png""):\n            image_ids.append(file_name[:-4])\n    return image_ids\n\n\ndef get_train_image_ids(name):\n    script_dir_path = os.path.dirname(os.path.realpath(__file__))\n    csv_dir_path = script_dir_path if name == \'train\' else os.path.join(script_dir_path, \'folds\')\n    df = pd.read_csv(os.path.join(csv_dir_path, \'%s.csv\' % name))\n    image_ids = []\n    for row in df.itertuples():\n        image_ids.append(row[1])\n    return image_ids\n\n\ndef load_data(name, mode, preload, mask_threshold):\n    script_dir_path = os.path.dirname(os.path.realpath(__file__))\n    dataset_dir_path = os.path.join(script_dir_path, mode)\n\n    all_data = []\n    image_ids = get_train_image_ids(name) if mode == \'train\' else get_test_image_ids(name)\n    pbar = tqdm(image_ids, desc=""Load dataset "" + name, total=len(image_ids), unit=""images"")\n    for image_id in pbar:\n        data = {}\n        data[\'image_id\'] = image_id\n        data[\'dataset_dir\'] = dataset_dir_path\n        if preload:\n            data = load_images_and_masks(data, mode, mask_threshold)\n\n        if data is not None:\n            all_data.append(data)\n    return all_data\n\n\ndef load_images_and_masks(data, mode, mask_threshold):\n    image_id = data[\'image_id\']\n    dataset_dir = data[\'dataset_dir\']\n    image = skimage.io.imread(\'%s/images/%s.png\' % (dataset_dir, image_id))\n\n    assert image.ndim == 3\n    data[\'input\'] = image\n\n    if mode == \'train\':\n        mask = skimage.io.imread(\'%s/masks/%s.png\' % (dataset_dir, image_id), as_gray=True)\n        assert mask.ndim == 2\n        mask[mask > 0] = 1\n        data[\'mask\'] = mask\n        pixel_count = mask.sum()\n        if 0 < pixel_count <= mask_threshold:\n            return None\n\n    return data\n\n\nclass SaltIdentification(Dataset):\n\n    def __init__(self, mode=\'train\', transform=None, preload=False, name=None, data=None, mask_threshold=0):\n        Dataset.__init__(self)\n        self.mode = mode\n        self.transform = transform\n        if name is not None:\n            self.name = name\n        else:\n            self.name = mode\n\n        self.mask_threshold = mask_threshold\n        if data is None:\n            self.data = load_data(self.name, self.mode, preload, self.mask_threshold)\n        else:\n            self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        data = self.data[index]\n        if isinstance(index, slice):\n            return SaltIdentification(mode=self.mode, transform=self.transform, name=self.name, data=data,\n                                      mask_threshold=self.mask_threshold)\n        return self.__pull_item__(data)\n\n    def __pull_item__(self, data):\n        if \'input\' not in data:\n            data = load_images_and_masks(data, self.mode, self.mask_threshold)\n        data = copy.copy(data)\n        if self.transform:\n            data = self.transform(data)\n        return data\n\n\nif __name__ == \'__main__\':\n    train_dataset = SaltIdentification(mode=\'train\', name=\'list_valid0_400\')\n    print(len(train_dataset))\n    assert len(train_dataset) == 399  # there is a bug, we ignore the first element from the csv file because of pandas\n\n    train_dataset = SaltIdentification(mode=\'train\', preload=True)\n    valid_dataset = SaltIdentification(mode=\'train\', name=\'valid\', data=train_dataset.data)\n    train_dataset = train_dataset[:-100]\n    valid_dataset = valid_dataset[-100:]\n\n    assert len(train_dataset) == 4000 - 100\n    assert len(valid_dataset) == 100\n\n    test_dataset = SaltIdentification(mode=\'test\')\n    assert len(test_dataset) == 18000\n    test_dataset = SaltIdentification(mode=\'test\', preload=True)\n    assert len(test_dataset) == 18000\n'"
losses/__init__.py,0,"b'from .lovasz_losses import lovasz_hinge, lovasz_loss_ignore_empty\n'"
losses/lovasz_losses.py,8,"b'""""""\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nfrom __future__ import print_function, division\nimport functools\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\ntry:\n    from itertools import  ifilterfalse\nexcept ImportError: # py3k\n    from itertools import  filterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    """"""\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    """"""\n    Array of IoU for each (non ignored) class\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []    \n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious)) # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\ndef hinge(logits, labels):\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    return errors\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    return lovasz_loss(logits, labels, error_func=hinge, per_image=per_image, ignore=ignore)\n\n\ndef lovasz_loss(logits, labels, error_func, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(lovasz_loss_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore), error_func=error_func)\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_loss_flat(*flatten_binary_scores(logits, labels, ignore), error_func=error_func)\n    return loss\n\n\ndef lovasz_loss_flat(logits, labels, error_func):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n\n    errors = error_func(logits, labels)\n\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    #loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n    loss = torch.dot(F.elu(errors_sorted) + 1, Variable(grad))\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = scores.contiguous().view(-1)\n    labels = labels.contiguous().view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n         super(StableBCELoss, self).__init__()\n    def forward(self, input, target):\n         neg_abs = - input.abs()\n         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n         return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    """"""\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    """"""\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    """"""\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, only_present=False):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n    """"""\n    C = probas.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float() # foreground for class c\n        if only_present and fg.sum() == 0:\n            continue\n        errors = (Variable(fg) - probas[:, c]).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\ndef xloss(logits, labels, ignore=None):\n    """"""\n    Cross entropy loss\n    """"""\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\n\ndef mean(l, ignore_nan=False, empty=0):\n    """"""\n    nanmean compatible with generators.\n    """"""\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == \'raise\':\n            raise ValueError(\'Empty mean\')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n\n\n\n#\n# lovasz hinge for non empty images\n#\n\ndef lovasz_loss_ignore_empty(logits, labels, truth_image, ignore=None):\n    loss = mean_ignore_empty((lovasz_loss_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore), error_func=hinge)\n                          for log, lab in zip(logits, labels)), truth_image)\n    return loss\n\ndef mean_ignore_empty(l, truth_image, ignore_nan=False, empty=\'raise\'):\n    n = 0\n    total_loss = 0\n    for i, loss in enumerate(l):\n        if truth_image[i] > 0:\n            total_loss += loss\n            n += 1\n    if n > 0:\n        return 1.0 * total_loss / n\n    else:\n        return 0.0\n'"
models/__init__.py,3,"b""import torch\nimport torch.nn as nn\n\nfrom .unet import UNet, BASENET_CHOICES, deep_supervised_criterion\n\n\ndef create(model, basenet, pretrained):\n    if model == 'unet':\n        net = UNet(basenet=basenet, pretrained=pretrained)\n    else:\n        raise NotImplementedError(model)\n\n    net.args = dict(model=model, basenet=basenet, pretrained=pretrained)\n\n    return net\n\n\ndef save(net, filename):\n    if isinstance(net, nn.DataParallel):\n        net = net.module\n\n    data = dict(args=net.args,\n                state_dict=net.state_dict())\n    torch.save(data, filename)\n\n\ndef load(filename, use_gpu=True):\n    print('load {}'.format(filename))\n    data = torch.load(filename, map_location=None if use_gpu else 'cpu')\n    net = create(**data['args'])\n    net.load_state_dict(data['state_dict'])\n    return net\n"""
models/basenet.py,3,"b'""""""Encoders for the UNet.""""""\n\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport torchvision\nfrom .inplace_abn import ActivatedBatchNorm\n\nBASENET_CHOICES = (\'vgg11\', \'vgg13\', \'vgg16\', \'vgg19\',\n                   \'vgg11_bn\', \'vgg13_bn\', \'vgg16_bn\', \'vgg19_bn\',\n                   \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\',\n                   \'resnext101_32x4d\', \'resnext101_64x4d\',\n                   \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n                   \'se_resnext50_32x4d\', \'se_resnext101_32x4d\', \'senet154\',\n                   \'darknet\')\n\nMODEL_ZOO_URL = \'https://drontheimerstr.synology.me/model_zoo/\'\n\nMODEL_URLS = {\n    \'resnet50\': {\n        # \'voc\':  MODEL_ZOO_URL + \'SSDretina_resnet50_c21-fb6036d1.pth\',  # SSDretina_resnet50_c81-a584ead7.pth pretrained\n        \'voc\': MODEL_ZOO_URL + \'SSDretina_resnet50_c21-1c85a349.pth\',  # SSDretina_resnet50_c501-06095077.pth pretrained\n        \'coco\': MODEL_ZOO_URL + \'SSDretina_resnet50_c81-a584ead7.pth\',\n        \'oid\': MODEL_ZOO_URL + \'SSDretina_resnet50_c501-06095077.pth\'},\n    \'resnext101_32x4d\': {\'coco\': MODEL_ZOO_URL + \'SSDretina_resnext101_32x4d_c81-fdb37546.pth\'}\n}\n\n\ndef conv(*args, **kwargs):\n    return lambda last_layer: nn.Conv2d(last_layer.out_channels, *args, **kwargs)\n\n\ndef get_out_channels(layers):\n    """"""access out_channels from last layer of nn.Sequential/list""""""\n    if hasattr(layers, \'out_channels\'):\n        return layers.out_channels\n    elif isinstance(layers, int):\n        return layers\n    else:\n        for i in range(len(layers) - 1, -1, -1):\n            layer = layers[i]\n            if hasattr(layer, \'out_channels\'):\n                return layer.out_channels\n            elif isinstance(layer, nn.Sequential):\n                return get_out_channels(layer)\n    raise RuntimeError(""cant get_out_channels from {}"".format(layers))\n\n\ndef Sequential(*args):\n    f = nn.Sequential(*args)\n    f.out_channels = get_out_channels(args)\n    return f\n\n\ndef sequential(*args):\n    def create_sequential(last_layer):\n        layers = []\n        for a in args:\n            layers.append(a(last_layer))\n            last_layer = layers[-1]\n        return Sequential(*layers)\n\n    return create_sequential\n\n\ndef ConvBnRelu(*args, **kwargs):\n    """"""drop in block for nn.Conv2d with BatchNorm and ReLU""""""\n    c = nn.Conv2d(*args, **kwargs)\n    return Sequential(c,\n                      nn.BatchNorm2d(c.out_channels),\n                      nn.ReLU(inplace=True))\n\n\ndef conv_bn_relu(*args, **kwargs):\n    return lambda last_layer: ConvBnRelu(get_out_channels(last_layer), *args, **kwargs)\n\n\ndef ConvRelu(*args, **kwargs):\n    return Sequential(nn.Conv2d(*args, **kwargs),\n                      nn.ReLU(inplace=True))\n\n\ndef conv_relu(*args, **kwargs):\n    return lambda last_layer: ConvRelu(get_out_channels(last_layer), *args, **kwargs)\n\n\ndef ReluConv(*args, **kwargs):\n    return Sequential(nn.ReLU(inplace=True),\n                      nn.Conv2d(*args, **kwargs))\n\n\ndef relu_conv(*args, **kwargs):\n    return lambda last_layer: ReluConv(get_out_channels(last_layer), *args, **kwargs)\n\n\ndef BnReluConv(*args, **kwargs):\n    """"""drop in block for nn.Conv2d with BatchNorm and ReLU""""""\n    c = nn.Conv2d(*args, **kwargs)\n    return Sequential(nn.BatchNorm2d(c.in_channels),\n                      nn.ReLU(inplace=True),\n                      c)\n\n\ndef bn_relu_conv(*args, **kwargs):\n    return lambda last_layer: BnReluConv(get_out_channels(last_layer), *args, **kwargs)\n\n\ndef vgg_base_extra(bn):\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    block = ConvBnRelu if bn else ConvRelu\n    conv6 = block(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = block(1024, 1024, kernel_size=1)\n    return [pool5, conv6, conv7]\n\n\ndef vgg(name, pretrained):\n    if name == \'vgg11\':\n        net_class = torchvision.models.vgg11\n    elif name == \'vgg13\':\n        net_class = torchvision.models.vgg13\n    elif name == \'vgg16\':\n        net_class = torchvision.models.vgg16\n    elif name == \'vgg19\':\n        net_class = torchvision.models.vgg19\n    elif name == \'vgg11_bn\':\n        net_class = torchvision.models.vgg11_bn\n    elif name == \'vgg13_bn\':\n        net_class = torchvision.models.vgg13_bn\n    elif name == \'vgg16_bn\':\n        net_class = torchvision.models.vgg16_bn\n    elif name == \'vgg19_bn\':\n        net_class = torchvision.models.vgg19_bn\n    else:\n        raise RuntimeError(""unknown model {}"".format(name))\n\n    imagenet_pretrained = pretrained == \'imagenet\'\n    vgg = net_class(pretrained=imagenet_pretrained)\n\n    # for have exact same layout as original paper\n    if name == \'vgg16\':\n        vgg.features[16].ceil_mode = True\n\n    bn = name.endswith(\'bn\')\n    layers = []\n    l = []\n    for i in range(len(vgg.features) - 1):\n        if isinstance(vgg.features[i], nn.MaxPool2d):\n            layers.append(l)\n            l = []\n        l.append(vgg.features[i])\n    l += vgg_base_extra(bn=bn)\n    layers.append(l)\n\n    # layers of feature scaling 2**5\n    block = ConvBnRelu if bn else ConvRelu\n    layer5 = [block(1024, 256, 1, 1, 0),\n              block(256, 512, 3, 2, 1)]\n    layers.append(layer5)\n\n    layers = [Sequential(*l) for l in layers]\n    n_pretrained = 4 if imagenet_pretrained else 0\n    return layers, bn, n_pretrained\n\n\ndef resnet(name, pretrained):\n    if name == \'resnet18\':\n        net_class = torchvision.models.resnet18\n    elif name == \'resnet34\':\n        net_class = torchvision.models.resnet34\n    elif name == \'resnet50\':\n        net_class = torchvision.models.resnet50\n    elif name == \'resnet101\':\n        net_class = torchvision.models.resnet101\n    elif name == \'resnet152\':\n        net_class = torchvision.models.resnet152\n\n    imagenet_pretrained = pretrained == \'imagenet\'\n    resnet = net_class(pretrained=imagenet_pretrained)\n    layer0 = Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n\n    layer0[-1].out_channels = resnet.bn1.num_features\n\n    def get_out_channels_from_resnet_block(layer):\n        block = layer[-1]\n        if isinstance(block, torchvision.models.resnet.BasicBlock):\n            return block.conv2.out_channels\n        elif isinstance(block, torchvision.models.resnet.Bottleneck):\n            return block.conv3.out_channels\n        raise RuntimeError(""unknown resnet block: {}"".format(block))\n\n    resnet.layer1.out_channels = resnet.layer1[-1].out_channels = get_out_channels_from_resnet_block(resnet.layer1)\n    resnet.layer2.out_channels = resnet.layer2[-1].out_channels = get_out_channels_from_resnet_block(resnet.layer2)\n    resnet.layer3.out_channels = resnet.layer3[-1].out_channels = get_out_channels_from_resnet_block(resnet.layer3)\n    resnet.layer4.out_channels = resnet.layer4[-1].out_channels = get_out_channels_from_resnet_block(resnet.layer4)\n    n_pretrained = 5 if imagenet_pretrained else 0\n    return [layer0, resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4], True, n_pretrained\n\n\ndef resnext(name, pretrained):\n    import pretrainedmodels\n    if name in [\'resnext101_32x4d\', \'resnext101_64x4d\']:\n        imagenet_pretrained = \'imagenet\' if pretrained == \'imagenet\' else None\n        resnext = pretrainedmodels.__dict__[name](num_classes=1000, pretrained=imagenet_pretrained)\n    else:\n        return NotImplemented\n\n    resnext_features = resnext.features\n    layer0 = [resnext_features[i] for i in range(4)]\n    layer0 = nn.Sequential(*layer0)\n    layer0.out_channels = layer0[-1].out_channels = 64\n\n    layer1 = resnext_features[4]\n    layer1.out_channels = layer1[-1].out_channels = 256\n\n    layer2 = resnext_features[5]\n    layer2.out_channels = layer2[-1].out_channels = 512\n\n    layer3 = resnext_features[6]\n    layer3.out_channels = layer3[-1].out_channels = 1024\n\n    layer4 = resnext_features[7]\n    layer4.out_channels = layer4[-1].out_channels = 2048\n    n_pretrained = 5 if imagenet_pretrained else 0\n    return [layer0, layer1, layer2, layer3, layer4], True, n_pretrained\n\n\ndef replace_bn(bn, act=None):\n    slop = 0.01\n    if isinstance(act, nn.ReLU):\n        activation = \'leaky_relu\'  # approximate relu\n    elif isinstance(act, nn.LeakyReLU):\n        activation = \'leaky_relu\'\n        slope = act.negative_slope\n    elif isinstance(act, nn.ELU):\n        activation = \'elu\'\n    else:\n        activation = \'none\'\n    abn = ActivatedBatchNorm(num_features=bn.num_features,\n                             eps=bn.eps,\n                             momentum=bn.momentum,\n                             affine=bn.affine,\n                             track_running_stats=bn.track_running_stats,\n                             activation=activation,\n                             slope=slop)\n    abn.load_state_dict(bn.state_dict())\n    return abn\n\n\ndef replace_bn_in_sequential(layer0, block=None):\n    layer0_modules = []\n    last_bn = None\n    for n, m in layer0.named_children():\n        if isinstance(m, nn.BatchNorm2d):\n            last_bn = (n, m)\n        else:\n            activation = \'none\'\n            if last_bn:\n                abn = replace_bn(last_bn[1], m)\n                activation = abn.activation\n                layer0_modules.append((last_bn[0], abn))\n                last_bn = None\n            if activation == \'none\':\n                if block and isinstance(m, block):\n                    m = replace_bn_in_block(m)\n                elif isinstance(m, nn.Sequential):\n                    m = replace_bn_in_sequential(m, block)\n                layer0_modules.append((n, m))\n    if last_bn:\n        abn = replace_bn(last_bn[1])\n        layer0_modules.append((last_bn[0], abn))\n    return nn.Sequential(OrderedDict(layer0_modules))\n\n\nclass DummyModule(nn.Module):\n    def forward(self, x):\n        return x\n\n\ndef replace_bn_in_block(block):\n    block.bn1 = replace_bn(block.bn1, block.relu)\n    block.bn2 = replace_bn(block.bn2, block.relu)\n    block.bn3 = replace_bn(block.bn3)\n    block.relu = DummyModule()\n    if block.downsample:\n        block.downsample = replace_bn_in_sequential(block.downsample)\n    return nn.Sequential(block,\n                         nn.ReLU(inplace=True))\n\n\ndef se_net(name, pretrained):\n    import pretrainedmodels\n    if name in [\'se_resnet50\', \'se_resnet101\', \'se_resnet152\', \'se_resnext50_32x4d\', \'se_resnext101_32x4d\', \'senet154\']:\n        imagenet_pretrained = \'imagenet\' if pretrained == \'imagenet\' else None\n        senet = pretrainedmodels.__dict__[name](num_classes=1000, pretrained=imagenet_pretrained)\n    else:\n        return NotImplemented\n\n    layer0 = replace_bn_in_sequential(senet.layer0)\n\n    block = senet.layer1[0].__class__\n    layer1 = replace_bn_in_sequential(senet.layer1, block=block)\n    layer1.out_channels = layer1[-1].out_channels = senet.layer1[-1].conv3.out_channels\n    layer0.out_channels = layer0[-1].out_channels = senet.layer1[0].conv1.in_channels\n\n    layer2 = replace_bn_in_sequential(senet.layer2, block=block)\n    layer2.out_channels = layer2[-1].out_channels = senet.layer2[-1].conv3.out_channels\n\n    layer3 = replace_bn_in_sequential(senet.layer3, block=block)\n    layer3.out_channels = layer3[-1].out_channels = senet.layer3[-1].conv3.out_channels\n\n    layer4 = replace_bn_in_sequential(senet.layer4, block=block)\n    layer4.out_channels = layer4[-1].out_channels = senet.layer4[-1].conv3.out_channels\n\n    n_pretrained = 5 if imagenet_pretrained else 0\n    return [layer0, layer1, layer2, layer3, layer4], True, n_pretrained\n\n\ndef darknet(pretrained):\n    from .darknet import KitModel as DarkNet\n    net = DarkNet()\n    if pretrained:\n        state_dict = torch.load(""/media/data/model_zoo/coco/pytorch_yolov3.pth"")\n        net.load_state_dict(state_dict)\n    n_pretrained = 3 if pretrained else 0\n    return [net.model0, net.model1, net.model2], True, n_pretrained\n\n\nclass MockModule(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.backbone = nn.ModuleList(layers)\n\n\ndef load_pretrained_weights(layers, name, dataset_name):\n    state_dict = model_zoo.load_url(MODEL_URLS[name][dataset_name])\n    mock_module = MockModule(layers)\n    mock_module.load_state_dict(state_dict, strict=False)\n\n\ndef create_basenet(name, pretrained):\n    """"""\n    Parameters\n    ----------\n    name: model name\n    pretrained: dataset name\n\n    Returns\n    -------\n    list of modules, is_batchnorm, num_of_pretrained_module\n    """"""\n    if name.startswith(\'vgg\'):\n        layers, bn, n_pretrained = vgg(name, pretrained)\n    elif name.startswith(\'resnet\'):\n        layers, bn, n_pretrained = resnet(name, pretrained)\n    elif name.startswith(\'resnext\'):\n        layers, bn, n_pretrained = resnext(name, pretrained)\n    elif name.startswith(\'se\'):\n        layers, bn, n_pretrained = se_net(name, pretrained)\n    elif name == \'darknet\':\n        layers, bn, n_pretrained = darknet(pretrained)\n    else:\n        raise NotImplemented(name)\n\n    if pretrained in (\'coco\', \'oid\'):\n        load_pretrained_weights(layers, name, pretrained)\n        n_pretrained = len(layers)\n\n    return layers, bn, n_pretrained\n\n\nif __name__ == \'__main__\':\n    import argparse\n\n    parser = argparse.ArgumentParser(description=""show network"", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'model\', nargs=1, choices=BASENET_CHOICES, help=\'load saved model\')\n    parser.add_argument(\'--pretrained\', default=False, type=str, choices=(\'imagenet\', \'voc\', \'coco\', \'oid\'),\n                        help=\'pretrained dataset\')\n\n    args = parser.parse_args()\n    model = create_basenet(args.model[0], args.pretrained)\n    print(model)\n'"
models/oc_net.py,4,"b'""""""\nOCNet: Object Context Network for Scene Parsing\nhttps://github.com/PkuRainBow/OCNet\n""""""\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom .inplace_abn import ActivatedBatchNorm\n\n\nclass SelfAttentionBlock2D(nn.Module):\n    """"""\n    The basic implementation for self-attention block/non-local block\n    Input:\n        N X C X H X W\n    Parameters:\n        in_channels       : the dimension of the input feature map\n        key_channels      : the dimension after the key/query transform\n        value_channels    : the dimension after the value transform\n        scale             : choose the scale to downsample the input feature maps (save memory cost)\n    Return:\n        N X C X H X W\n        position-aware context features.(w/o concate or add with the input)\n    """"""\n    def __init__(self, in_channels, key_channels, value_channels, out_channels=None, scale=1):\n        super().__init__()\n        self.scale = scale\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.key_channels = key_channels\n        self.value_channels = value_channels\n        if out_channels is None:\n            self.out_channels = in_channels\n        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))\n        self.f_key = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels, kernel_size=1, stride=1, padding=0),\n            ActivatedBatchNorm(self.key_channels)\n        )\n        self.f_query = self.f_key\n        self.f_value = nn.Conv2d(in_channels=self.in_channels, out_channels=self.value_channels,\n                                 kernel_size=1, stride=1, padding=0)\n        self.W = nn.Conv2d(in_channels=self.value_channels, out_channels=self.out_channels,\n                           kernel_size=1, stride=1, padding=0)\n        nn.init.constant(self.W.weight, 0)\n        nn.init.constant(self.W.bias, 0)\n\n    def forward(self, x):\n        batch_size, h, w = x.size(0), x.size(2), x.size(3)\n        if self.scale > 1:\n            x = self.pool(x)\n\n        value = self.f_value(x).view(batch_size, self.value_channels, -1)\n        value = value.permute(0, 2, 1)\n        query = self.f_query(x).view(batch_size, self.key_channels, -1)\n        query = query.permute(0, 2, 1)\n        key = self.f_key(x).view(batch_size, self.key_channels, -1)\n\n        sim_map = torch.matmul(query, key)\n        sim_map = (self.key_channels**-.5) * sim_map\n        sim_map = F.softmax(sim_map, dim=-1)\n\n        context = torch.matmul(sim_map, value)\n        context = context.permute(0, 2, 1).contiguous()\n        context = context.view(batch_size, self.value_channels, *x.size()[2:])\n        context = self.W(context)\n        if self.scale > 1:\n            context = F.upsample(input=context, size=(h, w), mode=\'bilinear\', align_corners=True)\n        return context\n\n\nclass BaseOC(nn.Module):\n    """"""\n    Implementation of the BaseOC module\n    Parameters:\n        in_features / out_features: the channels of the input / output feature maps.\n        dropout: we choose 0.05 as the default value.\n        size: you can apply multiple sizes. Here we only use one size.\n    Return:\n        features fused with Object context information.\n    """"""\n\n    def __init__(self, in_channels, out_channels, key_channels, value_channels, dropout, sizes=(1,)):\n        super().__init__()\n        self.stages = nn.ModuleList(\n            [SelfAttentionBlock2D(in_channels, key_channels, value_channels, out_channels, size) for size in sizes])\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(2 * in_channels, out_channels, kernel_size=1, padding=0),\n            ActivatedBatchNorm(out_channels),\n            nn.Dropout2d(dropout))\n\n    def forward(self, feats):\n        priors = [stage(feats) for stage in self.stages]\n        context = priors[0]\n        for i in range(1, len(priors)):\n            context += priors[i]\n        output = self.conv_bn_dropout(torch.cat([context, feats], 1))\n        return output\n'"
models/unet.py,11,"b'""""""UNet supporting different encoders.""""""\nimport torch\nimport torch.nn as nn\n\nfrom losses.lovasz_losses import lovasz_loss_ignore_empty, lovasz_hinge\nfrom .basenet import create_basenet, BASENET_CHOICES\nfrom .oc_net import BaseOC\nfrom .inplace_abn import ActivatedBatchNorm\nimport torch.nn.functional as F\n\n\ndef upsample(size=None, scale_factor=None):\n    return nn.Upsample(size=size, scale_factor=scale_factor, mode=\'bilinear\', align_corners=False)\n    # return nn.Upsample(size=size, scale_factor=scale_factor, mode=\'nearest\')\n\n\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            # nn.Dropout2d(p=0.1, inplace=True),\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n            ActivatedBatchNorm(middle_channels),\n            # DANetHead(middle_channels, middle_channels),\n            BaseOC(in_channels=middle_channels, out_channels=middle_channels,\n                   key_channels=middle_channels // 2,\n                   value_channels=middle_channels // 2,\n                   dropout=0.2),\n            # Parameters were chosen to avoid artifacts, suggested by https://distill.pub/2016/deconv-checkerboard/\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2, padding=1),\n            # upsample(scale_factor=2)\n        )\n\n    def forward(self, *args):\n        x = torch.cat(args, 1)\n        return self.block(x)\n\n\nclass ConcatPool2d(nn.Module):\n    def __init__(self, kernel_size, stride=None):\n        super().__init__()\n        self.ap = nn.AvgPool2d(kernel_size, stride)\n        self.mp = nn.MaxPool2d(kernel_size, stride)\n\n    def forward(self, x):\n        return torch.cat([self.mp(x), self.ap(x)], 1)\n\n\nclass AdaptiveConcatPool2d(nn.Module):\n    def __init__(self, size=(1, 1)):\n        super().__init__()\n        self.ap = nn.AdaptiveAvgPool2d(size)\n        self.mp = nn.AdaptiveMaxPool2d(size)\n\n    def forward(self, x):\n        return torch.cat([self.mp(x), self.ap(x)], 1)\n\n\n# https://github.com/ternaus/TernausNet/blob/master/unet_models.py\nclass UNet(nn.Module):\n    def __init__(self, basenet=\'vgg11\', num_filters=16, pretrained=\'imagenet\'):\n        super().__init__()\n        net, bn, n_pretrained = create_basenet(basenet, pretrained)\n\n        if basenet.startswith(\'vgg\'):\n            self.encoder1 = net[0]  # 64\n        else:\n            # add upsample\n            self.encoder1 = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=False),\n                net[0])\n            self.encoder1.out_channels = net[0].out_channels\n\n        self.encoder2 = net[1]  # 64\n        self.encoder3 = net[2]  # 128\n        self.encoder4 = net[3]  # 256\n\n        context_channels = num_filters * 8 * 4\n        self.encoder5 = nn.Sequential(\n            net[4],\n            nn.Conv2d(net[4].out_channels, context_channels, kernel_size=3, stride=1, padding=1),\n            ActivatedBatchNorm(context_channels, activation=\'none\'),\n            BaseOC(in_channels=context_channels, out_channels=context_channels,\n                   key_channels=context_channels // 2,\n                   value_channels=context_channels // 2,\n                   dropout=0.05)\n        )\n        self.encoder5.out_channels = context_channels\n\n        self.fuse_image = nn.Sequential(\n            nn.Linear(512, 32),\n            nn.ReLU(inplace=True)\n        )\n        self.logit_image = nn.Sequential(\n            nn.Linear(32, 1)\n        )\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.center = Decoder(self.encoder5.out_channels, num_filters * 8 * 2, num_filters * 8)\n\n        self.decoder5 = Decoder(self.encoder5.out_channels + num_filters * 8, num_filters * 8 * 2, num_filters * 8)\n        self.decoder4 = Decoder(self.encoder4.out_channels + num_filters * 8, num_filters * 8 * 2, num_filters * 4)\n        self.decoder3 = Decoder(self.encoder3.out_channels + num_filters * 4, num_filters * 4 * 2, num_filters * 2)\n\n        if basenet.startswith(\'vgg\'):\n            self.decoder2 = Decoder(self.encoder2.out_channels + num_filters * 2, num_filters * 2 * 2, num_filters)\n            self.decoder1 = nn.Sequential(\n                nn.Conv2d(self.encoder1.out_channels + num_filters, num_filters, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True))\n        else:\n            self.decoder2 = nn.Sequential(\n                nn.Conv2d(self.encoder2.out_channels + num_filters * 2, num_filters * 2 * 2, kernel_size=3, padding=1),\n                ActivatedBatchNorm(num_filters * 2 * 2),\n                nn.Conv2d(num_filters * 2 * 2, num_filters, kernel_size=3, padding=1),\n                ActivatedBatchNorm(num_filters))\n            self.decoder1 = Decoder(self.encoder1.out_channels + num_filters, num_filters * 2, num_filters)\n\n        self.logit = nn.Sequential(\n            nn.Dropout2d(p=0.5, inplace=True),\n            nn.Conv2d(96, num_filters, kernel_size=3, padding=1),\n            ActivatedBatchNorm(num_filters),\n            nn.Conv2d(num_filters, 1, kernel_size=1),\n        )\n\n        self.fuse_pixel = nn.Sequential(\n            nn.Dropout2d(p=0.5, inplace=True),\n            nn.Conv2d(num_filters * (8 + 4 + 2 + 1 + 1), 64, kernel_size=1, padding=0)\n        )\n        self.logit_pixel5 = nn.Sequential(\n            nn.Dropout2d(p=0.5, inplace=True),\n            nn.Conv2d(num_filters * 8, num_filters, kernel_size=3, padding=1),\n            ActivatedBatchNorm(num_filters),\n            nn.Conv2d(num_filters, 1, kernel_size=1),\n        )\n        self.logit_pixel4 = nn.Sequential(\n            nn.Dropout2d(p=0.5, inplace=True),\n            nn.Conv2d(num_filters * 4, num_filters, kernel_size=3, padding=1),\n            ActivatedBatchNorm(num_filters),\n            nn.Conv2d(num_filters, 1, kernel_size=1),\n        )\n        self.logit_pixel3 = nn.Sequential(\n            nn.Dropout2d(p=0.5, inplace=True),\n            nn.Conv2d(num_filters * 2, num_filters, kernel_size=3, padding=1),\n            ActivatedBatchNorm(num_filters),\n            nn.Conv2d(num_filters, 1, kernel_size=1),\n        )\n        self.logit_pixel2 = nn.Sequential(\n            nn.Dropout2d(p=0.5, inplace=True),\n            nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n            ActivatedBatchNorm(num_filters),\n            nn.Conv2d(num_filters, 1, kernel_size=1),\n        )\n        self.logit_pixel1 = nn.Sequential(\n            nn.Dropout2d(p=0.5, inplace=True),\n            nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n            ActivatedBatchNorm(num_filters),\n            nn.Conv2d(num_filters, 1, kernel_size=1),\n        )\n\n    def forward(self, x):\n        batch_size, _, _, _ = x.shape\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n        x[:, 0, :, :] -= mean[0]\n        x[:, 0, :, :] /= std[0]\n        x[:, 1, :, :] -= mean[1]\n        x[:, 1, :, :] /= std[1]\n        x[:, 2, :, :] -= mean[2]\n        x[:, 2, :, :] /= std[2]\n\n        # depth encoding / CoordConv\n        # coord_scale = 1 / std[0]\n        # coord_x = (torch.abs(torch.linspace(-1, 1, steps=x.size(3))) - 0.5) * coord_scale\n        # x[:, 1] = coord_x.unsqueeze(0).expand_as(x[:, 1])\n        # coord_y = (torch.linspace(-1, 1, steps=x.size(2))) * coord_scale\n        # x[:, 2] = coord_y.unsqueeze(-1).expand_as(x[:, 2])\n\n        e1 = self.encoder1(x)  # ; print(\'e1\', e1.size())\n        e2 = self.encoder2(e1)  # ; print(\'e2\', e2.size())\n        e3 = self.encoder3(e2)  # ; print(\'e3\', e3.size())\n        e4 = self.encoder4(e3)  # ; print(\'e4\', e4.size())\n        e5 = self.encoder5(e4)  # ; print(\'e5\', e5.size())\n\n        c = self.center(self.pool(e5))  # ; print(\'c\', c.size())\n\n        d5 = self.decoder5(c, e5)  # ; print(\'d5\', d5.size())\n        d4 = self.decoder4(d5, e4)  # ; print(\'d4\', d4.size())\n        d3 = self.decoder3(d4, e3)  # ; print(\'d3\', d3.size())\n        d2 = self.decoder2(torch.cat((d3, e2), 1))  # ; print(\'d2\', d2.size())\n        d1 = self.decoder1(d2, e1)  # ; print(\'d1\', d1.size())\n\n        d1_size = d1.size()[2:]\n        upsampler = upsample(size=d1_size)\n        u5 = upsampler(d5)\n        u4 = upsampler(d4)\n        u3 = upsampler(d3)\n        u2 = upsampler(d2)\n\n        d = torch.cat((d1, u2, u3, u4, u5), 1)\n        # logit = self.logit(d)#;print(logit.size())\n\n        fuse_pixel = self.fuse_pixel(d)\n\n        logit_pixel = (\n            self.logit_pixel1(d1), self.logit_pixel2(u2), self.logit_pixel3(u3), self.logit_pixel4(u4),\n            self.logit_pixel5(u5),\n        )\n\n        e = F.adaptive_avg_pool2d(e5, output_size=1).view(batch_size, -1)  # image pool\n        e = F.dropout(e, p=0.50, training=self.training)\n        fuse_image = self.fuse_image(e)\n        logit_image = self.logit_image(fuse_image).view(-1)\n\n        # print(fuse_pixel.size())\n        # print(fuse_image.size())\n        logit = self.logit(torch.cat([  # fuse\n            fuse_pixel,\n            F.upsample(fuse_image.view(batch_size, -1, 1, 1, ), scale_factor=128, mode=\'nearest\')\n        ], 1))\n\n        return logit, logit_pixel, logit_image\n\n\ndef symmetric_lovasz(outputs, targets):\n    return (lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1 - targets)) / 2\n\n\ndef symmetric_lovasz_ignore_empty(outputs, targets, truth_image):\n    return (lovasz_loss_ignore_empty(outputs, targets, truth_image) +\n            lovasz_loss_ignore_empty(-outputs, 1 - targets, truth_image)) / 2\n\n\ndef deep_supervised_criterion(logit, logit_pixel, logit_image, truth_pixel, truth_image, is_average=True):\n    loss_image = F.binary_cross_entropy_with_logits(logit_image, truth_image, reduce=is_average)\n    loss_pixel = 0\n    for l in logit_pixel:\n        loss_pixel += symmetric_lovasz_ignore_empty(l.squeeze(1), truth_pixel, truth_image)\n    loss = symmetric_lovasz(logit.squeeze(1), truth_pixel)\n    return 0.05 * loss_image + 0.1 * loss_pixel + 1 * loss\n\n\nif __name__ == \'__main__\':\n    import argparse\n\n    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""--basenet"", choices=BASENET_CHOICES, default=\'vgg11\', help=\'model of basenet\')\n    parser.add_argument(""--num-filters"", type=int, default=16, help=\'num filters for decoder\')\n\n    args = parser.parse_args()\n\n    net = UNet(**vars(args))\n    # print(net)\n    parameters = [p for p in net.parameters() if p.requires_grad]\n    n_params = sum(p.numel() for p in parameters)\n    print(\'N of parameters {} ({} tensors)\'.format(n_params, len(parameters)))\n    encoder_parameters = [p for name, p in net.named_parameters() if p.requires_grad and name.startswith(\'encoder\')]\n    n_encoder_params = sum(p.numel() for p in encoder_parameters)\n    print(\'N of encoder parameters {} ({} tensors)\'.format(n_encoder_params, len(encoder_parameters)))\n    print(\'N of decoder parameters {} ({} tensors)\'.format(n_params - n_encoder_params,\n                                                           len(parameters) - len(encoder_parameters)))\n\n    x = torch.empty((1, 3, 128, 128))\n    y = net(x)\n    print(x.size(), \'-->\', y.size())\n'"
transforms/__init__.py,0,b'from .unet_transforms import *\n'
transforms/unet_transforms.py,0,"b'""""""Transforms for UNet.\n\nMany methods are taken from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63974.\n""""""\n\n__author__ = \'Erdene-Ochir Tuguldur, Yuan Xu\'\n\nimport cv2\nimport random\n\nimport numpy as np\n\n\nclass PrepareImageAndMask(object):\n    """"""Prepare images and masks like fixing channel numbers.""""""\n\n    def __call__(self, data):\n        img = data[\'input\']\n        img = img[:, :, :3]  # max 3 channels\n        img = img / 255\n\n        if \'mask\' in data:\n            mask = data[\'mask\']\n        else:\n            mask = np.zeros(img.shape[:2], dtype=img.dtype)\n\n        data[\'input\'] = img.astype(np.float32)\n        data[\'mask\'] = mask.astype(np.float32)\n        return data\n\n\nclass ResizeToNxN(object):\n    """"""Resize input images to rgb NxN and the masks into gray NxN.""""""\n\n    def __init__(self, n=128):\n        self.n = n\n\n    def __call__(self, data):\n        n = self.n\n        data[\'input\'] = cv2.resize(data[\'input\'], (n, n), interpolation=cv2.INTER_LINEAR)\n        data[\'mask\'] = cv2.resize(data[\'mask\'], (n, n), interpolation=cv2.INTER_NEAREST)\n        return data\n\n\ndef compute_padding(h, w, n=128):\n    if h % n == 0:\n        dy0, dy1 = 0, 0\n    else:\n        dy = n - h % n\n        dy0 = dy // 2\n        dy1 = dy - dy0\n\n    if w % n == 0:\n        dx0, dx1 = 0, 0\n    else:\n        dx = n - w % n\n        dx0 = dx // 2\n        dx1 = dx - dx0\n\n    return dy0, dy1, dx0, dx1\n\n\nclass PadToNxN(object):\n    """"""Pad to image size NxN using border reflection.""""""\n\n    def __init__(self, n=128):\n        self.n = n\n\n    def __call__(self, data):\n        n = self.n\n        h, w = data[\'input\'].shape[:2]\n        dy0, dy1, dx0, dx1 = compute_padding(h, w, n)\n\n        data[\'input\'] = cv2.copyMakeBorder(data[\'input\'], dy0, dy1, dx0, dx1, cv2.BORDER_REFLECT_101)\n        data[\'mask\'] = cv2.copyMakeBorder(data[\'mask\'], dy0, dy1, dx0, dx1, cv2.BORDER_REFLECT_101)\n        return data\n\n\nclass HorizontalFlip(object):\n    """"""Flip input and masks horizontally.""""""\n\n    def __call__(self, data):\n        data[\'input\'] = cv2.flip(data[\'input\'], 1)\n        data[\'mask\'] = cv2.flip(data[\'mask\'], 1)\n        return data\n\n\nclass BrightnessShift(object):\n    """"""Brightness shift.""""""\n\n    def __init__(self, max_value=0.1):\n        self.max_value = max_value\n\n    def __call__(self, data):\n        img = data[\'input\']\n        img += np.random.uniform(-self.max_value, self.max_value)\n        data[\'input\'] = np.clip(img, 0, 1)\n        return data\n\n\nclass BrightnessScaling(object):\n    """"""Brightness scaling.""""""\n\n    def __init__(self, max_value=0.08):\n        self.max_value = max_value\n\n    def __call__(self, data):\n        img = data[\'input\']\n        img *= np.random.uniform(1 - self.max_value, 1 + self.max_value)\n        data[\'input\'] = np.clip(img, 0, 1)\n        return data\n\n\nclass GammaChange(object):\n    """"""Gamma change.""""""\n\n    def __init__(self, max_value=0.08):\n        self.max_value = max_value\n\n    def __call__(self, data):\n        img = data[\'input\']\n        img = img ** (1.0 / np.random.uniform(1 - self.max_value, 1 + self.max_value))\n        data[\'input\'] = np.clip(img, 0, 1)\n        return data\n\n\ndef do_elastic_transform(image, mask, grid=10, distort=0.2):\n    # https://www.kaggle.com/ori226/data-augmentation-with-elastic-deformations\n    # https://github.com/letmaik/lensfunpy/blob/master/lensfunpy/util.py\n    height, width = image.shape[:2]\n\n    x_step = int(grid)\n    xx = np.zeros(width, np.float32)\n    prev = 0\n    for x in range(0, width, x_step):\n        start = x\n        end = x + x_step\n        if end > width:\n            end = width\n            cur = width\n        else:\n            cur = prev + x_step * (1 + random.uniform(-distort, distort))\n\n        xx[start:end] = np.linspace(prev, cur, end - start)\n        prev = cur\n\n    y_step = int(grid)\n    yy = np.zeros(height, np.float32)\n    prev = 0\n    for y in range(0, height, y_step):\n        start = y\n        end = y + y_step\n        if end > height:\n            end = height\n            cur = height\n        else:\n            cur = prev + y_step * (1 + random.uniform(-distort, distort))\n\n        yy[start:end] = np.linspace(prev, cur, end - start)\n        prev = cur\n\n    # grid\n    map_x, map_y = np.meshgrid(xx, yy)\n    map_x = map_x.astype(np.float32)\n    map_y = map_y.astype(np.float32)\n\n    image = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101,\n                      borderValue=(0, 0, 0,))\n    mask = cv2.remap(mask, map_x, map_y, interpolation=cv2.INTER_NEAREST, borderMode=cv2.BORDER_REFLECT_101,\n                     borderValue=(0, 0, 0,))\n\n    # mask = (mask > 0.5).astype(np.float32)\n    return image, mask\n\n\nclass ElasticDeformation(object):\n    """"""Elastic deformation.""""""\n\n    def __init__(self, grid=10, max_distort=0.15):\n        self.grid = grid\n        self.max_distort = max_distort\n\n    def __call__(self, data):\n        distort = np.random.uniform(0, self.max_distort)\n        img, mask = do_elastic_transform(data[\'input\'], data[\'mask\'], self.grid, distort)\n\n        data[\'input\'] = img\n        data[\'mask\'] = mask\n        return data\n\n\ndef do_rotation_transform(image, mask, angle=0):\n    height, width = image.shape[:2]\n    cc = np.cos(angle / 180 * np.pi)\n    ss = np.sin(angle / 180 * np.pi)\n    rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n\n    box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ], np.float32)\n    box1 = box0 - np.array([width / 2, height / 2])\n    box1 = np.dot(box1, rotate_matrix.T) + np.array([width / 2, height / 2])\n\n    box0 = box0.astype(np.float32)\n    box1 = box1.astype(np.float32)\n    mat = cv2.getPerspectiveTransform(box0, box1)\n\n    image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR,\n                                borderMode=cv2.BORDER_REFLECT_101,\n                                borderValue=(0, 0, 0,))\n    mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_NEAREST,\n                               borderMode=cv2.BORDER_REFLECT_101,\n                               borderValue=(0, 0, 0,))\n    # mask = (mask > 0.5).astype(np.float32)\n    return image, mask\n\n\nclass Rotation(object):\n    """"""Rotation.""""""\n\n    def __init__(self, max_angle=15):\n        self.max_angle = max_angle\n\n    def __call__(self, data):\n        angle = np.random.uniform(-self.max_angle, self.max_angle)\n        img, mask = do_rotation_transform(data[\'input\'], data[\'mask\'], angle)\n\n        data[\'input\'] = img\n        data[\'mask\'] = mask\n        return data\n\n\ndef do_crop_and_rescale(image, mask, x0, y0, x1, y1):\n    height, width = image.shape[:2]\n    image = image[y0:y1, x0:x1]\n    mask = mask[y0:y1, x0:x1]\n\n    image = cv2.resize(image, (width, height), interpolation=cv2.INTER_LINEAR)\n    mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n    # mask = (mask > 0.5).astype(np.float32)\n    return image, mask\n\n\nclass CropAndRescale(object):\n    """"""Crop and rescale back to the initial size.""""""\n\n    def __init__(self, max_scale=0.2):\n        self.max_scale = max_scale\n\n    def __call__(self, data):\n        img, mask = data[\'input\'], data[\'mask\']\n        h, w = img.shape[:2]\n\n        dy = int(h * self.max_scale)\n        dx = int(w * self.max_scale)\n\n        img, mask = do_crop_and_rescale(img, mask, np.random.randint(0, dx), np.random.randint(0, dy),\n                                        w - np.random.randint(0, dx), h - np.random.randint(0, dy))\n\n        data[\'input\'] = img\n        data[\'mask\'] = mask\n        return data\n\n\ndef do_horizontal_shear(image, mask, scale=0):\n    height, width = image.shape[:2]\n    dx = int(scale * width)\n\n    box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ], np.float32)\n    box1 = np.array([[+dx, 0], [width + dx, 0], [width - dx, height], [-dx, height], ], np.float32)\n\n    box0 = box0.astype(np.float32)\n    box1 = box1.astype(np.float32)\n    mat = cv2.getPerspectiveTransform(box0, box1)\n\n    image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR,\n                                borderMode=cv2.BORDER_REFLECT_101, borderValue=(0, 0, 0,))\n    mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_NEAREST,\n                               borderMode=cv2.BORDER_REFLECT_101, borderValue=(0, 0, 0,))\n    # mask = (mask > 0.5).astype(np.float32)\n    return image, mask\n\n\nclass HorizontalShear(object):\n\n    def __init__(self, max_scale=0.2):\n        self.max_scale = max_scale\n\n    def __call__(self, data):\n        scale = np.random.uniform(-self.max_scale, self.max_scale)\n        img, mask = do_horizontal_shear(data[\'input\'], data[\'mask\'], scale)\n\n        data[\'input\'] = img\n        data[\'mask\'] = mask\n        return data\n\n\nclass HWCtoCHW(object):\n    def __call__(self, data):\n        data[\'input\'] = data[\'input\'].transpose((2, 0, 1))\n        return data\n\n\n# adopt from https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py\nclass Cutout(object):\n    """"""Randomly mask out one or more patches from an image.\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n    """"""\n\n    def __init__(self, n_holes, length, mean=(0.485, 0.456, 0.406)):\n        self.n_holes = n_holes\n        self.length = length\n        self.mean = mean\n\n    def __call__(self, data):\n        """"""\n        Args:\n            img (Tensor): Tensor image of size (C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        """"""\n        img = data[\'input\']\n        mask = data[\'mask\']\n        mask_pixel_count = mask.sum()\n\n        h, w, _ = img.shape\n\n        cut_mask = np.zeros((h, w), np.bool)\n\n        for n in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            if mask_pixel_count > 0:\n                cut_mask_pixel_count = mask[y1: y2, x1: x2].sum()\n                if cut_mask_pixel_count / mask_pixel_count > 0.5:\n                    continue  # don\'t cut more than half of mask\n\n            cut_mask[y1: y2, x1: x2] = 1\n\n        img[cut_mask] = self.mean\n        return data\n\n\nclass SaltAndPepper(object):\n\n    def __init__(self, probability=0.01):\n        self.probability = probability\n        self.threshold = 1 - probability\n\n    def __call__(self, data):\n        img = data[\'input\']\n        h, w = img.shape[:2]\n        noise = np.random.rand(h, w)\n        img[noise < self.probability] = 0\n        img[noise > self.threshold] = 1\n        data[\'input\'] = img\n        return data\n'"
utils/__init__.py,12,"b'import os\nimport gzip\nimport pickle\nfrom contextlib import contextmanager\nimport torch\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom .lr_scheduler import FindLR, NoamLR\n\nfrom .rle import RLenc as rlenc\nfrom .rle import toRunLength as rlenc_np\nfrom .rle import FasterRle\n\n\ndef choose_device(device):\n    if not isinstance(device, str):\n        return device\n\n    if device not in [\'cuda\', \'cpu\']:\n        device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n    elif device == ""cuda"":\n        assert torch.cuda.is_available()\n\n    device = torch.device(device)\n    return device\n\n\n@contextmanager\ndef save_cuda_memory(model):\n    # from cuda to cpu if necessary, for saving cuda RAM\n    # NOTE: assume model in single device and single dtype\n    param = next(model.parameters())\n    device = param.device\n    model.to(device=\'cpu\')\n    yield\n    model.to(device=device)\n\n\ndef optimizer_cpu_state_dict(optimizer):\n    # save cuda RAM\n    optimizer_state_dict = optimizer.state_dict()\n\n    dict_value_to_cpu = lambda d: {k: v.cpu() if isinstance(v, torch.Tensor) else v\n                                   for k, v in d.items()}\n\n    if \'optimizer_state_dict\' in optimizer_state_dict:\n        #  FP16_Optimizer\n        cuda_state_dict = optimizer_state_dict[\'optimizer_state_dict\']\n    else:\n        cuda_state_dict = optimizer_state_dict\n\n    if \'state\' in cuda_state_dict:\n        cuda_state_dict[\'state\'] = {k: dict_value_to_cpu(v)\n                                    for k, v in cuda_state_dict[\'state\'].items()}\n\n    return optimizer_state_dict\n\n\ndef get_num_workers(jobs):\n    """"""\n    Parameters\n    ----------\n    jobs How many jobs to be paralleled. Negative or 0 means number of cpu cores left.\n\n    Returns\n    -------\n    How many subprocess to be used\n    """"""\n    num_workers = jobs\n    if num_workers <= 0:\n        num_workers = os.cpu_count() + jobs\n    if num_workers < 0 or num_workers > os.cpu_count():\n        raise RuntimeError(""System doesn\'t have so many cpu cores: {} vs {}"".format(jobs, os.cpu_count()))\n    return num_workers\n\n\ndef create_optimizer(net, name, learning_rate, weight_decay, momentum=0, fp16_loss_scale=None,\n                     optimizer_state=None, device=None):\n    net.float()\n\n    use_fp16 = fp16_loss_scale is not None\n    if use_fp16:\n        from apex import fp16_utils\n        net = fp16_utils.network_to_half(net)\n\n    device = choose_device(device)\n    print(\'use\', device)\n    if device.type == \'cuda\':\n        net = torch.nn.DataParallel(net)\n        cudnn.benchmark = True\n    net = net.to(device)\n\n    # optimizer\n    parameters = [p for p in net.parameters() if p.requires_grad]\n    print(\'N of parameters\', len(parameters))\n\n    if name == \'sgd\':\n        optimizer = optim.SGD(parameters, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n    elif name == \'adamw\':\n        from .adamw import AdamW\n        optimizer = AdamW(parameters, lr=learning_rate, weight_decay=weight_decay)\n    elif name == \'adam\':\n        optimizer = optim.Adam(parameters, lr=learning_rate, weight_decay=weight_decay)\n    else:\n        raise NotImplementedError(name)\n\n    if use_fp16:\n        from apex import fp16_utils\n        if fp16_loss_scale == 0:\n            opt_args = dict(dynamic_loss_scale=True)\n        else:\n            opt_args = dict(static_loss_scale=fp16_loss_scale)\n        print(\'FP16_Optimizer\', opt_args)\n        optimizer = fp16_utils.FP16_Optimizer(optimizer, **opt_args)\n    else:\n        optimizer.backward = lambda loss: loss.backward()\n\n    if optimizer_state:\n        if use_fp16 and \'optimizer_state_dict\' not in optimizer_state:\n            # resume FP16_Optimizer.optimizer only\n            optimizer.optimizer.load_state_dict(optimizer_state)\n        elif use_fp16 and \'optimizer_state_dict\' in optimizer_state:\n            # resume optimizer from FP16_Optimizer.optimizer\n            optimizer.load_state_dict(optimizer_state[\'optimizer_state_dict\'])\n        else:\n            optimizer.load_state_dict(optimizer_state)\n\n    return net, optimizer\n\n\ndef create_lr_scheduler(optimizer, lr_scheduler, **kwargs):\n    if not isinstance(optimizer, optim.Optimizer):\n        # assume FP16_Optimizer\n        optimizer = optimizer.optimizer\n\n    if lr_scheduler == \'plateau\':\n        patience = kwargs.get(\'lr_scheduler_patience\', 10) // kwargs.get(\'validation_interval\', 1)\n        factor = kwargs.get(\'lr_scheduler_gamma\', 0.1)\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=factor, eps=0)\n    elif lr_scheduler == \'step\':\n        step_size = kwargs[\'lr_scheduler_step_size\']\n        gamma = kwargs.get(\'lr_scheduler_gamma\', 0.1)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n    elif lr_scheduler == \'cos\':\n        max_epochs = kwargs[\'max_epochs\']\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_epochs)\n    elif lr_scheduler == \'milestones\':\n        milestones = kwargs[\'lr_scheduler_milestones\']\n        gamma = kwargs.get(\'lr_scheduler_gamma\', 0.1)\n        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n    elif lr_scheduler == \'findlr\':\n        max_steps = kwargs[\'max_steps\']\n        lr_scheduler = FindLR(optimizer, max_steps)\n    elif lr_scheduler == \'noam\':\n        warmup_steps = kwargs[\'lr_scheduler_warmup\']\n        lr_scheduler = NoamLR(optimizer, warmup_steps=warmup_steps)\n    elif lr_scheduler == \'clr\':\n        step_size = kwargs[\'lr_scheduler_step_size\']\n        learning_rate = kwargs[\'learning_rate\']\n        lr_scheduler_gamma = kwargs[\'lr_scheduler_gamma\']\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                                  T_max=step_size,\n                                                                  eta_min=learning_rate * lr_scheduler_gamma)\n    else:\n        raise NotImplementedError(""unknown lr_scheduler "" + lr_scheduler)\n    return lr_scheduler\n\n\ndef gzip_save(filename, obj):\n    """"""save objects into a compressed diskfile""""""\n    with gzip.open(filename, \'wb\') as fil:\n        pickle.dump(obj, fil)\n\n\ndef gzip_load(filename):\n    """"""reload objects from a compressed diskfile""""""\n    with gzip.open(filename, \'rb\') as fil:\n        return pickle.load(fil)\n\n\ndef pickle_load(filename):\n    """"""load pickle""""""\n    with open(filename, \'rb\') as fil:\n        return pickle.load(fil)\n\n'"
utils/adamw.py,7,"b'\'\'\'\nhttps://github.com/pytorch/pytorch/pull/4429/files\nhttps://github.com/pytorch/pytorch/pull/3740/files\nhope it will be merged into pytorch\n\'\'\'\nimport torch\nif torch.__version__ not in [\'0.3.1\']:\n    import warnings\n    warnings.warn(""Please check if AdamW has been implemented in pytorch %s"" % torch.__version__, DeprecationWarning)\n\nimport math\nfrom torch.optim.optimizer import Optimizer\n\n\n\nclass AdamW(Optimizer):\n    """"""Implements Adam algorithm.\n\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay using the method from\n            the paper `Fixing Weight Decay Regularization in Adam` (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _Fixing Weight Decay Regularization in Adam:\n        https://arxiv.org/abs/1711.05101\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        super().__init__(params, defaults)\n        self._initial_lr = lr\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsgrad\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsgrad = group[\'amsgrad\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsgrad:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                if group[\'weight_decay\'] != 0:\n                    #eta = group[\'lr\'] / self._initial_lr # scheduler changes lr only\n                    #p.data.add_(-group[\'weight_decay\'] * eta, p.data)\n                    w = group[\'weight_decay\'] * group[\'lr\']\n                    p.data.add_(-w, p.data)\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n'"
utils/lr_scheduler.py,1,"b'""""""Noam Scheduler.""""""\n\n\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass FindLR(_LRScheduler):\n    """"""\n    inspired by fast.ai @https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n    """"""\n    def __init__(self, optimizer, max_steps, max_lr=10):\n        self.max_steps = max_steps\n        self.max_lr = max_lr\n        super().__init__(optimizer)\n\n    def get_lr(self):\n        return [base_lr * ((self.max_lr / base_lr) ** (self.last_epoch / (self.max_steps - 1)))\n                for base_lr in self.base_lrs]\n\n\nclass NoamLR(_LRScheduler):\n    """"""\n    Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate\n    linearly for the first ``warmup_steps`` training steps, and decreasing it thereafter proportionally\n    to the inverse square root of the step number, scaled by the inverse square root of the\n    dimensionality of the model. Time will tell if this is just madness or it\'s actually important.\n    Parameters\n    ----------\n    warmup_steps: ``int``, required.\n        The number of steps to linearly increase the learning rate.\n    """"""\n    def __init__(self, optimizer, warmup_steps):\n        self.warmup_steps = warmup_steps\n        super().__init__(optimizer)\n\n    def get_lr(self):\n        last_epoch = max(1, self.last_epoch)\n        scale = self.warmup_steps ** 0.5 * min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n        return [base_lr * scale for base_lr in self.base_lrs]'"
utils/metrics.py,0,"b'""""""Copied from https://www.kaggle.com/robertkag/metric-script""""""\n\nimport numpy as np\nimport scipy.stats\n\n\ndef calc_enptropy(probs, base=2):\n    _, counts = np.unique(probs, return_counts=True)\n    return scipy.stats.entropy(counts, base=base)\n\n\ndef calc_iou(actual, pred):\n    intersection = np.count_nonzero(actual * pred)\n    union = np.count_nonzero(actual + pred)\n    iou_result = intersection / union if union != 0 else 0.\n    return iou_result\n\n\ndef calc_ious(actuals, preds):\n    ious_ = np.array([calc_iou(a, p) for a, p in zip(actuals, preds)])\n    return ious_\n\n\ndef calc_precisions(thresholds, ious):\n    thresholds = np.reshape(thresholds, (1, -1))\n    ious = np.reshape(ious, (-1, 1))\n    ps = ious > thresholds\n    mps = ps.mean(axis=1)\n    return mps\n\n\ndef indiv_scores(masks, preds):\n    ious = calc_ious(masks, preds)\n    thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n    precisions = calc_precisions(thresholds, ious)\n\n    ###### Adjust score for empty masks\n    emptyMasks = np.count_nonzero(masks.reshape((len(masks), -1)), axis=1) == 0\n    emptyPreds = np.count_nonzero(preds.reshape((len(preds), -1)), axis=1) == 0\n    adjust = (emptyMasks == emptyPreds).astype(np.float)\n    precisions[emptyMasks] = adjust[emptyMasks]\n    ###################\n    return precisions\n\n\ndef calc_metric(masks, preds, type=\'iou\', size_average=True):\n    if type == \'iou\':\n        m = indiv_scores(masks, preds)\n    elif type == \'pixel_accuracy\':\n        m = pixel_accuracy(masks, preds)\n    else:\n        raise NotImplementedError(type)\n    if size_average:\n        m = m.mean()\n    return m\n\n\ndef pixel_accuracy(masks, preds):\n    correct = (preds == masks).astype(np.float).sum(axis=(1,2)) / masks[0, :, :].size\n    return correct\n'"
utils/rle.py,0,"b'""""""\nfrom Even Faster RLE\nhttps://www.kaggle.com/danmoller/even-faster-rle\n""""""\n\nimport numpy as np\nimport multiprocessing\nfrom collections import ChainMap\n\n\ndef RLenc(img, order=\'F\', format=True):\n    """"""\n    Source https://www.kaggle.com/bguberfain/unet-with-depth\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    """"""\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = \'\'\n\n        for rr in runs:\n            z += \'{} {} \'.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs\n\n\nclass Consumer(multiprocessing.Process):\n    """"""Consumer for performing a specific task.""""""\n\n    def __init__(self, task_queue, result_queue):\n        """"""Initialize consumer, it has a task and result queues.""""""\n        multiprocessing.Process.__init__(self)\n        self.task_queue = task_queue\n        self.result_queue = result_queue\n        self.daemon = True\n\n    def run(self):\n        """"""Actual run of the consumer.""""""\n        #print(\'start rle consumer {}\'.format(self.pid))\n        while True:\n            next_task = self.task_queue.get()\n            if next_task is None:\n                # Poison pill means shutdown\n                self.task_queue.task_done()\n                break\n            # Fetch answer from task\n            answer = next_task()\n            # Put into result queue\n            self.result_queue.put(answer)\n            self.task_queue.task_done()\n        #print(\'stop rle consumer {}\'.format(self.pid))\n        return\n\n\nclass RleTask(object):\n    """"""Wrap the RLE Encoder into a Task.""""""\n\n    def __init__(self, idx, img):\n        """"""Save image to self.""""""\n        self.img = img\n        self.idx = idx\n\n    def __call__(self):\n        """"""When object is called, encode.""""""\n        return {self.idx: RLenc(self.img)}\n\n\nclass FastRle(object):\n    """"""Perform RLE in paralell.""""""\n\n    def __init__(self, num_consumers=2):\n        """"""Initialize class.""""""\n        self._tasks = multiprocessing.JoinableQueue()\n        self._results = multiprocessing.Queue()\n        self._n_consumers = num_consumers\n\n        # Initialize consumers\n        self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n        for w in self._consumers:\n            w.start()\n\n    def add(self, idx, img):\n        """"""Add a task to perform.""""""\n        self._tasks.put(RleTask(idx, img))\n\n    def get_results(self):\n        """"""Close all tasks.""""""\n        # Provide poison pill\n        [self._tasks.put(None) for _ in range(self._n_consumers)]\n        # Wait for finish\n        self._tasks.join()\n        # Return results\n        singles = []\n        while not self._results.empty():\n            singles.append(self._results.get())\n        return dict(ChainMap({}, *singles))\n\n\n# even faster RLE encoder\ndef toRunLength(x, firstDim=2):\n    if firstDim == 2:\n        x = np.swapaxes(x, 1, 2)\n\n    x = (x > 0.5).astype(int)\n    x = x.reshape((x.shape[0], -1))\n    x = np.pad(x, ((0, 0), (1, 1)), \'constant\')\n\n    x = x[:, 1:] - x[:, :-1]\n    starts = x > 0\n    ends = x < 0\n\n    rang = np.arange(x.shape[1])\n\n    results = []\n\n    for image, imStarts, imEnds in zip(x, starts, ends):\n        st = rang[imStarts]\n        en = rang[imEnds]\n\n        #         counts = (en-st).astype(str)\n        #         st = (st+1).astype(str)\n\n        #         res = np.stack([st,counts], axis=-1).reshape((-1,))\n        #         res = np.core.defchararray.join("" "", res)\n\n        res = """"\n        for s, e in zip(st, en):\n            res += str(s + 1) + "" "" + str(e - s) + "" ""\n\n        results.append(res[:-1])\n    # print(""called"")\n\n    return results\n\n\nclass FasterTask(object):\n    """"""Wrap the RLE Encoder into a Task.""""""\n\n    def __init__(self, array, startIndex):\n        """"""Save array to self.""""""\n        self.array = array\n        self.startIndex = startIndex\n\n    def __call__(self):\n        """"""When object is called, encode.""""""\n        return (toRunLength(self.array), self.startIndex)\n\n\nclass FasterRle(object):\n    """"""Perform RLE in paralell.""""""\n\n    def __init__(self, num_consumers=2):\n        """"""Initialize class.""""""\n        self._tasks = multiprocessing.JoinableQueue()\n        self._results = multiprocessing.Queue()\n        self._n_consumers = num_consumers\n\n        # Initialize consumers\n        self._consumers = [Consumer(self._tasks, self._results) for i in range(self._n_consumers)]\n        for w in self._consumers:\n            w.start()\n        self.startIndex = 0\n\n    def add(self, array):\n        """"""Add a task to perform.""""""\n        self._tasks.put(FasterTask(array, self.startIndex))\n        self.startIndex += len(array)\n\n    def get_results(self):\n        """"""Close all tasks.""""""\n        # Provide poison pill\n        [self._tasks.put(None) for _ in range(self._n_consumers)]\n        # Wait for finish\n        self._tasks.join()\n        # Return results\n        singles = []\n        while not self._results.empty():\n            singles.append(self._results.get())\n\n        resultDic = dict()\n        for rles, start in singles:\n            # print(\'start:\', start)\n            for i, rle in enumerate(rles):\n                # print(\'i:\', i)\n                resultDic[start + i] = rle\n        return resultDic\n\n\nif __name__ == \'__main__\':\n    import timeit\n\n    example_image = np.random.uniform(0, 1, size=(1000, 101, 101)) > 0.5\n\n    # Wrap the FastRle class into a method so we measure the time\n    def original(array):\n        results = {}\n        for i, arr in enumerate(array):\n            results[i] = RLenc(arr)\n        return results\n\n\n    def faster(array):\n        rle = FastRle(4)\n        for i, arr in enumerate(array):\n            rle.add(i, arr)\n        return rle.get_results()\n\n\n    def pureNumpy(array):\n        rle = toRunLength(array)\n        rle = dict(enumerate(rle))\n        return rle\n\n\n    def evenFaster(array):\n        # make sure you treat this properly when len(array) % 4 != 0\n        rle = FasterRle(4)\n        subSize = len(array) // 4\n\n        for i in range(0, len(array), subSize):\n            rle.add(array[i:i + subSize])\n        return rle.get_results()\n\n    x = original(example_image)\n    y = faster(example_image)\n    z = pureNumpy(example_image)\n    w = evenFaster(example_image)\n\n    # Make sure they are the same\n    print(""Comparing values:\\n"")\n\n    #comparison = []\n    #for key in x:\n    #    comparison.append(x[key] == y[key])\n    #print(\'Original vs Parallel:\', np.all(comparison))\n\n    comparison = []\n    for key in x:\n        comparison.append(x[key] == z[key])\n    print(""Original vs pure numpy:"", np.all(comparison))\n\n    comparison = []\n    for key in x:\n        comparison.append(x[key] == w[key])\n    print(""Original vs even faster:"", np.all(comparison))\n\n    print(""Measuring times: \\n"")\n    for f_name in [""original"", ""faster"", ""pureNumpy"", ""evenFaster""]:\n        stmt = f_name + \'(example_image)\'\n        print(f_name, timeit.timeit(stmt=stmt, number=1, globals=globals()))\n'"
models/inplace_abn/__init__.py,0,b'\nfrom .abn import ABN as ActivatedBatchNorm\n#from .bn import InPlaceABN as ActivatedBatchNorm\n#from .bn import InPlaceABNSync as ActivatedBatchNorm\n'
models/inplace_abn/abn.py,2,"b'\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\n\nclass ABN(nn.BatchNorm2d):\n    """"""Activated Batch Normalization\n\n    This gathers a `BatchNorm2d` and an activation function in a single module\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True,\n                 activation=""leaky_relu"", slope=0.01):\n        """"""Creates an Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine,\n                         track_running_stats=track_running_stats)\n\n        if activation not in (""leaky_relu"", ""elu"", ""none""):\n            raise NotImplementedError(activation)\n\n        self.activation = activation\n        self.slope = slope\n\n    def forward(self, x):\n        x = super().forward(x)\n\n        if self.activation == ""leaky_relu"":\n            return functional.leaky_relu(x, negative_slope=self.slope, inplace=True)\n        elif self.activation == ""elu"":\n            return functional.elu(x, inplace=True)\n        else:\n            return x\n\n    def extra_repr(self):\n        rep = super().extra_repr()\n        rep += \', activation={activation}\'.format(**self.__dict__)\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope}\'.format(**self.__dict__)\n        return rep'"
models/inplace_abn/bn.py,1,"b'\nimport torch\nfrom queue import Queue\n\nfrom .functions import *\nfrom .abn import ABN\n\n\nclass InPlaceABN(ABN):\n    """"""InPlace Activated Batch Normalization""""""\n\n    def forward(self, x):\n        exponential_average_factor = 0.0\n\n        if self.training and self.track_running_stats:\n            self.num_batches_tracked += 1\n            if self.momentum is None:  # use cumulative moving average\n                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n            else:  # use exponential moving average\n                exponential_average_factor = self.momentum\n\n        return inplace_abn(x, self.weight, self.bias, self.running_mean, self.running_var,\n                           self.training or not self.track_running_stats,\n                           exponential_average_factor, self.eps, self.activation, self.slope)\n\n\nclass InPlaceABNSync(ABN):\n    """"""InPlace Activated Batch Normalization with cross-GPU synchronization\n\n    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DataParallel`.\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True,\n                 activation=""leaky_relu"", slope=0.01, devices=None):\n        """"""Creates a synchronized, InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        devices : list of int or None\n            IDs of the GPUs that will run the replicas of this module.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine,\n                         track_running_stats=track_running_stats, activation=activation, slope=slope)\n        self.devices = devices if devices else list(range(torch.cuda.device_count()))\n\n        # Initialize queues\n        self.worker_ids = self.devices[1:]\n        self.master_queue = Queue(len(self.worker_ids))\n        self.worker_queues = [Queue(1) for _ in self.worker_ids]\n\n    def forward(self, x):\n        if len(self.devices) < 2:\n            # fallback for CPU mode or single GPU mode\n            return super().forward(x)\n\n        if x.get_device() == self.devices[0]:\n            # Master mode\n            extra = {\n                ""is_master"": True,\n                ""master_queue"": self.master_queue,\n                ""worker_queues"": self.worker_queues,\n                ""worker_ids"": self.worker_ids\n            }\n        else:\n            # Worker mode\n            extra = {\n                ""is_master"": False,\n                ""master_queue"": self.master_queue,\n                ""worker_queue"": self.worker_queues[self.worker_ids.index(x.get_device())]\n            }\n\n        return inplace_abn_sync(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                extra, self.training, self.momentum, self.eps, self.activation, self.slope)\n\n    def extra_repr(self):\n        rep = super().extra_repr()\n        rep += \', devices={devices}\'.format(**self.__dict__)\n        return rep\n'"
models/inplace_abn/functions.py,4,"b'from os import path\n\nimport torch.autograd as autograd\nimport torch.cuda.comm as comm\nfrom torch.autograd.function import once_differentiable\nfrom torch.utils.cpp_extension import load\n\n_src_path = path.join(path.dirname(path.abspath(__file__)), ""src"")\n_backend = load(name=""inplace_abn"",\n                extra_cflags=[""-O3""],\n                sources=[path.join(_src_path, f) for f in [\n                    ""inplace_abn.cpp"",\n                    ""inplace_abn_cpu.cpp"",\n                    ""inplace_abn_cuda.cu""\n                ]],\n                extra_cuda_cflags=[""--expt-extended-lambda""])\n\n# Activation names\nACT_LEAKY_RELU = ""leaky_relu""\nACT_ELU = ""elu""\nACT_NONE = ""none""\n\n\ndef _check(fn, *args, **kwargs):\n    success = fn(*args, **kwargs)\n    if not success:\n        raise RuntimeError(""CUDA Error encountered in {}"".format(fn))\n\n\ndef _broadcast_shape(x):\n    out_size = []\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            out_size.append(1)\n        else:\n            out_size.append(s)\n    return out_size\n\n\ndef _reduce(x):\n    if len(x.size()) == 2:\n        return x.sum(dim=0)\n    else:\n        n, c = x.size()[0:2]\n        return x.contiguous().view((n, c, -1)).sum(2).sum(0)\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_forward(x, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_forward(x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_backward(x, dx, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_backward(x, dx)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\nclass InPlaceABN(autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        count = _count_samples(x)\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n        else:\n            # TODO: implement simplified CUDA backward for inference mode\n            edz = dz.new_zeros(dz.size(1))\n            eydz = dz.new_zeros(dz.size(1))\n\n        dx, dweight, dbias = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = dweight if ctx.affine else None\n        dbias = dbias if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\n\nclass InPlaceABNSync(autograd.Function):\n    @classmethod\n    def forward(cls, ctx, x, weight, bias, running_mean, running_var,\n                extra, training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        cls._parse_extra(ctx, extra)\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        count = _count_samples(x) * (ctx.master_queue.maxsize + 1)\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n\n            if ctx.is_master:\n                means, vars = [mean.unsqueeze(0)], [var.unsqueeze(0)]\n                for _ in range(ctx.master_queue.maxsize):\n                    mean_w, var_w = ctx.master_queue.get()\n                    ctx.master_queue.task_done()\n                    means.append(mean_w.unsqueeze(0))\n                    vars.append(var_w.unsqueeze(0))\n\n                means = comm.gather(means)\n                vars = comm.gather(vars)\n\n                mean = means.mean(0)\n                var = (vars + (mean - means) ** 2).mean(0)\n\n                tensors = comm.broadcast_coalesced((mean, var), [mean.get_device()] + ctx.worker_ids)\n                for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                    queue.put(ts)\n            else:\n                ctx.master_queue.put((mean, var))\n                mean, var = ctx.worker_queue.get()\n                ctx.worker_queue.task_done()\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n\n            if ctx.is_master:\n                edzs, eydzs = [edz], [eydz]\n                for _ in range(len(ctx.worker_queues)):\n                    edz_w, eydz_w = ctx.master_queue.get()\n                    ctx.master_queue.task_done()\n                    edzs.append(edz_w)\n                    eydzs.append(eydz_w)\n\n                edz = comm.reduce_add(edzs) / (ctx.master_queue.maxsize + 1)\n                eydz = comm.reduce_add(eydzs) / (ctx.master_queue.maxsize + 1)\n\n                tensors = comm.broadcast_coalesced((edz, eydz), [edz.get_device()] + ctx.worker_ids)\n                for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                    queue.put(ts)\n            else:\n                ctx.master_queue.put((edz, eydz))\n                edz, eydz = ctx.worker_queue.get()\n                ctx.worker_queue.task_done()\n        else:\n            edz = dz.new_zeros(dz.size(1))\n            eydz = dz.new_zeros(dz.size(1))\n\n        dx, dweight, dbias = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = dweight if ctx.affine else None\n        dbias = dbias if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def _parse_extra(ctx, extra):\n        ctx.is_master = extra[""is_master""]\n        if ctx.is_master:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queues = extra[""worker_queues""]\n            ctx.worker_ids = extra[""worker_ids""]\n        else:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queue = extra[""worker_queue""]\n\n\ninplace_abn = InPlaceABN.apply\ninplace_abn_sync = InPlaceABNSync.apply\n\n__all__ = [""inplace_abn"", ""inplace_abn_sync"", ""ACT_LEAKY_RELU"", ""ACT_ELU"", ""ACT_NONE""]\n'"
