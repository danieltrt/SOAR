file_path,api_count,code
dataset.py,2,"b'import torch.utils.data as data\r\nimport torch\r\nimport numpy as np\r\nimport h5py\r\n\r\nclass DatasetFromHdf5(data.Dataset):\r\n    def __init__(self, file_path):\r\n        super(DatasetFromHdf5, self).__init__()\r\n        hf = h5py.File(file_path)\r\n        self.data = hf.get(""data"")\r\n        self.label_x2 = hf.get(""label_x2"")\r\n        self.label_x4 = hf.get(""label_x4"")\r\n\r\n    def __getitem__(self, index):\r\n        return torch.from_numpy(self.data[index,:,:,:]).float(), torch.from_numpy(self.label_x2[index,:,:,:]).float(), torch.from_numpy(self.label_x4[index,:,:,:]).float()\r\n\r\n    def __len__(self):\r\n        return self.data.shape[0]'"
demo.py,4,"b'import argparse\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport time, math\r\nimport scipy.io as sio\r\nimport matplotlib.pyplot as plt\r\n\r\nparser = argparse.ArgumentParser(description=""PyTorch LapSRN Demo"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""use cuda?"")\r\nparser.add_argument(""--model"", default=""model/model_epoch_100.pth"", type=str, help=""model path"")\r\nparser.add_argument(""--image"", default=""butterfly_GT"", type=str, help=""image name"")\r\nparser.add_argument(""--scale"", default=4, type=int, help=""scale factor, Default: 4"")\r\n\r\ndef PSNR(pred, gt, shave_border=0):\r\n    height, width = pred.shape[:2]\r\n    pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    imdff = pred - gt\r\n    rmse = math.sqrt(np.mean(imdff ** 2))\r\n    if rmse == 0:\r\n        return 100\r\n    return 20 * math.log10(255.0 / rmse)\r\n\r\nopt = parser.parse_args()\r\ncuda = opt.cuda\r\n\r\nif cuda and not torch.cuda.is_available():\r\n    raise Exception(""No GPU found, please run without --cuda"")\r\n\r\nmodel = torch.load(opt.model)[""model""]\r\n\r\nim_gt_y = sio.loadmat(""Set5/"" + opt.image + "".mat"")[\'im_gt_y\']\r\nim_b_y = sio.loadmat(""Set5/"" + opt.image + "".mat"")[\'im_b_y\']\r\nim_l_y = sio.loadmat(""Set5/"" + opt.image + "".mat"")[\'im_l_y\']\r\n\r\nim_gt_y = im_gt_y.astype(float)\r\nim_b_y = im_b_y.astype(float)\r\nim_l_y = im_l_y.astype(float)\r\n\r\npsnr_bicubic = PSNR(im_gt_y, im_b_y,shave_border=opt.scale)\r\n\r\nim_input = im_l_y/255.\r\n\r\nim_input = Variable(torch.from_numpy(im_input).float()).view(1, -1, im_input.shape[0], im_input.shape[1])\r\n\r\nif cuda:\r\n    model = model.cuda()\r\n    im_input = im_input.cuda()\r\nelse:\r\n    model = model.cpu()\r\n    \r\nstart_time = time.time()\r\nHR_2x, HR_4x = model(im_input)\r\nelapsed_time = time.time() - start_time\r\n\r\nHR_4x = HR_4x.cpu()\r\n\r\nim_h_y = HR_4x.data[0].numpy().astype(np.float32)\r\n\r\nim_h_y = im_h_y*255.\r\nim_h_y[im_h_y<0] = 0\r\nim_h_y[im_h_y>255.] = 255.\r\nim_h_y = im_h_y[0,:,:]\r\n\r\npsnr_predicted = PSNR(im_gt_y, im_h_y,shave_border=opt.scale)\r\n\r\nprint(""Scale="",opt.scale)\r\nprint(""PSNR_predicted="", psnr_predicted)\r\nprint(""PSNR_bicubic="", psnr_bicubic)\r\nprint(""It takes {}s for processing"".format(elapsed_time))\r\n\r\nfig = plt.figure()\r\nax = plt.subplot(""131"")\r\nax.imshow(im_gt_y, cmap=\'gray\')\r\nax.set_title(""GT"")\r\n\r\nax = plt.subplot(""132"")\r\nax.imshow(im_b_y, cmap=\'gray\')\r\nax.set_title(""Input(Bicubic)"")\r\n\r\nax = plt.subplot(""133"")\r\nax.imshow(im_h_y, cmap=\'gray\')\r\nax.set_title(""Output(LapSRN)"")\r\nplt.show()\r\n'"
eval.py,4,"b'import argparse\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport time, math, glob\r\nimport scipy.io as sio\r\nimport matplotlib.pyplot as plt\r\n\r\nparser = argparse.ArgumentParser(description=""PyTorch LapSRN Eval"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""use cuda?"")\r\nparser.add_argument(""--model"", default=""model/model_epoch_100.pth"", type=str, help=""model path"")\r\nparser.add_argument(""--dataset"", default=""Set5"", type=str, help=""dataset name, Default: Set5"")\r\nparser.add_argument(""--scale"", default=4, type=int, help=""scale factor, Default: 4"")\r\n\r\ndef PSNR(pred, gt, shave_border=0):\r\n    height, width = pred.shape[:2]\r\n    pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    imdff = pred - gt\r\n    rmse = math.sqrt(np.mean(imdff ** 2))\r\n    if rmse == 0:\r\n        return 100\r\n    return 20 * math.log10(255.0 / rmse)\r\n\r\nopt = parser.parse_args()\r\ncuda = opt.cuda\r\n\r\nif cuda and not torch.cuda.is_available():\r\n    raise Exception(""No GPU found, please run without --cuda"")\r\n\r\nmodel = torch.load(opt.model)[""model""]\r\n\r\nimage_list = glob.glob(opt.dataset+""/*.*"") \r\n\r\navg_psnr_predicted = 0.0\r\navg_psnr_bicubic = 0.0\r\navg_elapsed_time = 0.0\r\n\r\nfor image_name in image_list:\r\n    print(""Processing "", image_name)\r\n    im_gt_y = sio.loadmat(image_name)[\'im_gt_y\']\r\n    im_b_y = sio.loadmat(image_name)[\'im_b_y\']\r\n    im_l_y = sio.loadmat(image_name)[\'im_l_y\']\r\n\r\n    im_gt_y = im_gt_y.astype(float)\r\n    im_b_y = im_b_y.astype(float)\r\n    im_l_y = im_l_y.astype(float)\r\n\r\n    psnr_bicubic = PSNR(im_gt_y, im_b_y,shave_border=opt.scale)\r\n    avg_psnr_bicubic += psnr_bicubic\r\n\r\n    im_input = im_l_y/255.\r\n\r\n    im_input = Variable(torch.from_numpy(im_input).float()).view(1, -1, im_input.shape[0], im_input.shape[1])\r\n\r\n    if cuda:\r\n        model = model.cuda()\r\n        im_input = im_input.cuda()\r\n    else:\r\n        model = model.cpu()\r\n\r\n    start_time = time.time()\r\n    HR_2x, HR_4x = model(im_input)\r\n    elapsed_time = time.time() - start_time\r\n    avg_elapsed_time += elapsed_time\r\n\r\n    HR_4x = HR_4x.cpu()\r\n\r\n    im_h_y = HR_4x.data[0].numpy().astype(np.float32)\r\n\r\n    im_h_y = im_h_y*255.\r\n    im_h_y[im_h_y<0] = 0\r\n    im_h_y[im_h_y>255.] = 255.\r\n    im_h_y = im_h_y[0,:,:]\r\n\r\n    psnr_predicted = PSNR(im_gt_y, im_h_y,shave_border=opt.scale)\r\n    avg_psnr_predicted += psnr_predicted\r\n\r\nprint(""Scale="", opt.scale)\r\nprint(""Dataset="", opt.dataset)\r\nprint(""PSNR_predicted="", avg_psnr_predicted/len(image_list))\r\nprint(""PSNR_bicubic="", avg_psnr_bicubic/len(image_list))\r\nprint(""It takes average {}s for processing"".format(avg_elapsed_time/len(image_list)))\r\n'"
lapsrn.py,5,"b'import torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nimport math\r\n\r\ndef get_upsample_filter(size):\r\n    """"""Make a 2D bilinear kernel suitable for upsampling""""""\r\n    factor = (size + 1) // 2\r\n    if size % 2 == 1:\r\n        center = factor - 1\r\n    else:\r\n        center = factor - 0.5\r\n    og = np.ogrid[:size, :size]\r\n    filter = (1 - abs(og[0] - center) / factor) * \\\r\n             (1 - abs(og[1] - center) / factor)\r\n    return torch.from_numpy(filter).float()\r\n\r\nclass _Conv_Block(nn.Module):    \r\n    def __init__(self):\r\n        super(_Conv_Block, self).__init__()\r\n        \r\n        self.cov_block = nn.Sequential(\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n        \r\n    def forward(self, x):  \r\n        output = self.cov_block(x)\r\n        return output \r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        \r\n        self.conv_input = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.relu = nn.LeakyReLU(0.2, inplace=True)\r\n        \r\n        self.convt_I1 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False)\r\n        self.convt_R1 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.convt_F1 = self.make_layer(_Conv_Block)\r\n  \r\n        self.convt_I2 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False)\r\n        self.convt_R2 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.convt_F2 = self.make_layer(_Conv_Block)        \r\n        \r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n            if isinstance(m, nn.ConvTranspose2d):\r\n                c1, c2, h, w = m.weight.data.size()\r\n                weight = get_upsample_filter(h)\r\n                m.weight.data = weight.view(1, 1, h, w).repeat(c1, c2, 1, 1)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n                    \r\n    def make_layer(self, block):\r\n        layers = []\r\n        layers.append(block())\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):    \r\n        out = self.relu(self.conv_input(x))\r\n        \r\n        convt_F1 = self.convt_F1(out)\r\n        convt_I1 = self.convt_I1(x)\r\n        convt_R1 = self.convt_R1(convt_F1)\r\n        HR_2x = convt_I1 + convt_R1\r\n        \r\n        convt_F2 = self.convt_F2(convt_F1)\r\n        convt_I2 = self.convt_I2(HR_2x)\r\n        convt_R2 = self.convt_R2(convt_F2)\r\n        HR_4x = convt_I2 + convt_R2\r\n       \r\n        return HR_2x, HR_4x\r\n        \r\nclass L1_Charbonnier_loss(nn.Module):\r\n    """"""L1 Charbonnierloss.""""""\r\n    def __init__(self):\r\n        super(L1_Charbonnier_loss, self).__init__()\r\n        self.eps = 1e-6\r\n\r\n    def forward(self, X, Y):\r\n        diff = torch.add(X, -Y)\r\n        error = torch.sqrt( diff * diff + self.eps )\r\n        loss = torch.sum(error) \r\n        return loss'"
lapsrn_wgan.py,5,"b'import torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nimport math\r\n\r\ndef get_upsample_filter(size):\r\n    """"""Make a 2D bilinear kernel suitable for upsampling""""""\r\n    factor = (size + 1) // 2\r\n    if size % 2 == 1:\r\n        center = factor - 1\r\n    else:\r\n        center = factor - 0.5\r\n    og = np.ogrid[:size, :size]\r\n    filter = (1 - abs(og[0] - center) / factor) * \\\r\n             (1 - abs(og[1] - center) / factor)\r\n    return torch.from_numpy(filter).float()\r\n\r\nclass _Conv_Block(nn.Module):    \r\n    def __init__(self):\r\n        super(_Conv_Block, self).__init__()\r\n        \r\n        self.cov_block = nn.Sequential(\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n        \r\n    def forward(self, x):  \r\n        output = self.cov_block(x)\r\n        return output \r\n\r\nclass _netG(nn.Module):\r\n    def __init__(self):\r\n        super(_netG, self).__init__()\r\n        \r\n        self.conv_input = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.relu = nn.LeakyReLU(0.2, inplace=True)\r\n        \r\n        self.convt_I1 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False)\r\n        self.convt_R1 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.convt_F1 = self.make_layer(_Conv_Block)\r\n  \r\n        self.convt_I2 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False)\r\n        self.convt_R2 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.convt_F2 = self.make_layer(_Conv_Block)        \r\n        \r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n            if isinstance(m, nn.ConvTranspose2d):\r\n                c1, c2, h, w = m.weight.data.size()\r\n                weight = get_upsample_filter(h)\r\n                m.weight.data = weight.view(1, 1, h, w).repeat(c1, c2, 1, 1)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n                    \r\n    def make_layer(self, block):\r\n        layers = []\r\n        layers.append(block())\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        out = self.relu(self.conv_input(x))\r\n        \r\n        convt_F1 = self.convt_F1(out)\r\n        convt_I1 = self.convt_I1(x)\r\n        convt_R1 = self.convt_R1(convt_F1)\r\n        HR_2x = convt_I1 + convt_R1\r\n        \r\n        convt_F2 = self.convt_F2(convt_F1)\r\n        convt_I2 = self.convt_I2(HR_2x)\r\n        convt_R2 = self.convt_R2(convt_F2)\r\n        HR_4x = convt_I2 + convt_R2\r\n       \r\n        return HR_2x, HR_4x\r\n        \r\nclass L1_Charbonnier_loss(nn.Module):\r\n    """"""L1 Charbonnierloss.""""""\r\n    def __init__(self):\r\n        super(L1_Charbonnier_loss, self).__init__()\r\n        self.eps = 1e-6\r\n\r\n    def forward(self, X, Y):\r\n        diff = torch.add(X, -Y)\r\n        error = torch.sqrt( diff * diff + self.eps )\r\n        loss = torch.sum(error) \r\n        return loss\r\n        \r\n\r\nclass _netD(nn.Module):\r\n    def __init__(self):\r\n        super(_netD, self).__init__()\r\n\r\n        self.features = nn.Sequential(\r\n        \r\n            # input is (1) x 128 x 128\r\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (64) x 128 x 128\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\r\n            #nn.BatchNorm2d(64),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (64) x 128 x 128\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\r\n            #nn.BatchNorm2d(128),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            \r\n            # state size. (64) x 64 x 64\r\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\r\n            #nn.BatchNorm2d(128),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (128) x 64 x 64\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\r\n            #nn.BatchNorm2d(256),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (256) x 32 x 32\r\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\r\n            #nn.BatchNorm2d(256),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (256) x 16 x 16\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False),\r\n            #nn.BatchNorm2d(512),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (512) x 16 x 16\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\r\n            #nn.BatchNorm2d(512),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n\r\n        self.LeakyReLU = nn.LeakyReLU(0.2, inplace=True)\r\n        self.fc1 = nn.Linear(512 * 8 * 8, 1024)\r\n        self.fc2 = nn.Linear(1024, 1)\r\n        \r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                m.weight.data.normal_(1.0, 0.02)\r\n                m.bias.data.fill_(0)\r\n\r\n        #self.sigmoid = nn.Sigmoid()\r\n\r\n    def forward(self, input):\r\n\r\n        out = self.features(input)\r\n\r\n        # state size. (512) x 8 x 8\r\n        out = out.view(out.size(0), -1)\r\n        \r\n        # state size. (512 x 8 x 8)\r\n        out = self.fc1(out)\r\n        \r\n        # state size. (1024)\r\n        out = self.LeakyReLU(out)\r\n        \r\n        out = self.fc2(out)\r\n        # state size. (1)\r\n\r\n        out = out.mean(0)\r\n\r\n        #out = self.sigmoid(out)\r\n        return out.view(1) '"
main_lapsrn.py,11,"b'import argparse, os\r\nimport torch\r\nimport random\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\nfrom torch.utils.data import DataLoader\r\nfrom lapsrn import Net, L1_Charbonnier_loss\r\nfrom dataset import DatasetFromHdf5\r\n\r\n# Training settings\r\nparser = argparse.ArgumentParser(description=""PyTorch LapSRN"")\r\nparser.add_argument(""--batchSize"", type=int, default=64, help=""training batch size"")\r\nparser.add_argument(""--nEpochs"", type=int, default=100, help=""number of epochs to train for"")\r\nparser.add_argument(""--lr"", type=float, default=1e-4, help=""Learning Rate. Default=1e-4"")\r\nparser.add_argument(""--step"", type=int, default=100, help=""Sets the learning rate to the initial LR decayed by momentum every n epochs, Default: n=10"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""Use cuda?"")\r\nparser.add_argument(""--resume"", default="""", type=str, help=""Path to checkpoint (default: none)"")\r\nparser.add_argument(""--start-epoch"", default=1, type=int, help=""Manual epoch number (useful on restarts)"")\r\nparser.add_argument(""--threads"", type=int, default=1, help=""Number of threads for data loader to use, Default: 1"")\r\nparser.add_argument(""--momentum"", default=0.9, type=float, help=""Momentum, Default: 0.9"")\r\nparser.add_argument(""--weight-decay"", ""--wd"", default=1e-4, type=float, help=""weight decay, Default: 1e-4"")\r\nparser.add_argument(""--pretrained"", default="""", type=str, help=""path to pretrained model (default: none)"")\r\n\r\ndef main():\r\n\r\n    global opt, model\r\n    opt = parser.parse_args()\r\n    print(opt)\r\n\r\n    cuda = opt.cuda\r\n    if cuda and not torch.cuda.is_available():\r\n        raise Exception(""No GPU found, please run without --cuda"")\r\n\r\n    opt.seed = random.randint(1, 10000)\r\n    print(""Random Seed: "", opt.seed)\r\n    torch.manual_seed(opt.seed)\r\n    if cuda:\r\n        torch.cuda.manual_seed(opt.seed)\r\n\r\n    cudnn.benchmark = True\r\n\r\n    print(""===> Loading datasets"")\r\n    train_set = DatasetFromHdf5(""data/lap_pry_x4_small.h5"")\r\n    training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=True)\r\n\r\n    print(""===> Building model"")\r\n    model = Net()\r\n    criterion = L1_Charbonnier_loss()\r\n\r\n    print(""===> Setting GPU"")\r\n    if cuda:\r\n        model = model.cuda()\r\n        criterion = criterion.cuda()\r\n    else:\r\n        model = model.cpu()\r\n\r\n    # optionally resume from a checkpoint\r\n    if opt.resume:\r\n        if os.path.isfile(opt.resume):\r\n            print(""=> loading checkpoint \'{}\'"".format(opt.resume))\r\n            checkpoint = torch.load(opt.resume)\r\n            opt.start_epoch = checkpoint[""epoch""] + 1\r\n            model.load_state_dict(checkpoint[""model""].state_dict())\r\n        else:\r\n            print(""=> no checkpoint found at \'{}\'"".format(opt.resume))\r\n\r\n    # optionally copy weights from a checkpoint\r\n    if opt.pretrained:\r\n        if os.path.isfile(opt.pretrained):\r\n            print(""=> loading model \'{}\'"".format(opt.pretrained))\r\n            weights = torch.load(opt.pretrained)\r\n            model.load_state_dict(weights[\'model\'].state_dict())\r\n        else:\r\n            print(""=> no model found at \'{}\'"".format(opt.pretrained)) \r\n\r\n    print(""===> Setting Optimizer"")\r\n    optimizer = optim.Adam(model.parameters(), lr=opt.lr)\r\n\r\n    print(""===> Training"")\r\n    for epoch in range(opt.start_epoch, opt.nEpochs + 1): \r\n        train(training_data_loader, optimizer, model, criterion, epoch)\r\n        save_checkpoint(model, epoch)\r\n\r\ndef adjust_learning_rate(optimizer, epoch):\r\n    """"""Sets the learning rate to the initial LR decayed by 10 every 10 epochs""""""\r\n    lr = opt.lr * (0.1 ** (epoch // opt.step))\r\n    return lr\r\n\r\ndef train(training_data_loader, optimizer, model, criterion, epoch):\r\n\r\n    lr = adjust_learning_rate(optimizer, epoch-1)\r\n\r\n    for param_group in optimizer.param_groups:\r\n        param_group[""lr""] = lr\r\n\r\n    print(""Epoch={}, lr={}"".format(epoch, optimizer.param_groups[0][""lr""]))\r\n\r\n    model.train()\r\n\r\n    for iteration, batch in enumerate(training_data_loader, 1):\r\n\r\n        input, label_x2, label_x4 = Variable(batch[0]), Variable(batch[1], requires_grad=False), Variable(batch[2], requires_grad=False)\r\n\r\n        if opt.cuda:\r\n            input = input.cuda()\r\n            label_x2 = label_x2.cuda()\r\n            label_x4 = label_x4.cuda()\r\n\r\n        HR_2x, HR_4x = model(input)\r\n\r\n        loss_x2 = criterion(HR_2x, label_x2)\r\n        loss_x4 = criterion(HR_4x, label_x4)\r\n        loss = loss_x2 + loss_x4\r\n\r\n        optimizer.zero_grad()\r\n\r\n        loss_x2.backward(retain_graph=True)\r\n\r\n        loss_x4.backward()\r\n\r\n        optimizer.step()\r\n\r\n        if iteration%100 == 0:\r\n            print(""===> Epoch[{}]({}/{}): Loss: {:.10f}"".format(epoch, iteration, len(training_data_loader), loss.data[0]))\r\n\r\ndef save_checkpoint(model, epoch):\r\n    model_folder = ""checkpoint/""\r\n    model_out_path = model_folder + ""lapsrn_model_epoch_{}.pth"".format(epoch)\r\n    state = {""epoch"": epoch ,""model"": model}\r\n    if not os.path.exists(model_folder):\r\n        os.makedirs(model_folder)\r\n\r\n    torch.save(state, model_out_path)\r\n\r\n    print(""Checkpoint saved to {}"".format(model_out_path))\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
main_lapwgan.py,17,"b'import argparse, os\r\nimport pdb\r\nimport torch\r\nimport math, random\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\nfrom torch.utils.data import DataLoader\r\nfrom lapsrn_wgan import _netG, _netD, L1_Charbonnier_loss\r\nfrom dataset import DatasetFromHdf5\r\nfrom torchvision import models, transforms\r\nimport torch.utils.model_zoo as model_zoo\r\n\r\n# Training settings\r\nparser = argparse.ArgumentParser(description=""PyTorch LapSRN WGAN"")\r\nparser.add_argument(""--batchSize"", type=int, default=32, help=""training batch size"")\r\nparser.add_argument(""--nEpochs"", type=int, default=400, help=""number of epochs to train for"")\r\nparser.add_argument(\'--lrG\', type=float, default=1e-4, help=\'Learning Rate. Default=1e-4\')\r\nparser.add_argument(\'--lrD\', type=float, default=1e-4, help=\'Learning Rate. Default=1e-4\')\r\nparser.add_argument(""--step"", type=int, default=50, help=""Sets the learning rate to the initial LR decayed by momentum every n epochs, Default: n=10"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""Use cuda?"")\r\nparser.add_argument(""--resume"", default="""", type=str, help=""Path to checkpoint (default: none)"")\r\nparser.add_argument(""--start-epoch"", default=1, type=int, help=""Manual epoch number (useful on restarts)"")\r\nparser.add_argument(""--threads"", type=int, default=1, help=""Number of threads for data loader to use, Default: 1"")\r\nparser.add_argument(""--momentum"", default=0.9, type=float, help=""Momentum, Default: 0.9"")\r\nparser.add_argument(""--weight-decay"", ""--wd"", default=1e-4, type=float, help=""weight decay, Default: 1e-4"")\r\nparser.add_argument(""--pretrained"", default="""", type=str, help=""path to pretrained model (default: none)"")\r\nparser.add_argument(\'--clamp_lower\', type=float, default=-0.01)\r\nparser.add_argument(\'--clamp_upper\', type=float, default=0.01)\r\n\r\ndef main():\r\n\r\n    global opt, model \r\n    opt = parser.parse_args()\r\n    print(opt)\r\n\r\n    cuda = opt.cuda\r\n    if cuda and not torch.cuda.is_available():\r\n        raise Exception(""No GPU found, please run without --cuda"")\r\n\r\n    opt.seed = random.randint(1, 10000)\r\n    print(""Random Seed: "", opt.seed)\r\n    torch.manual_seed(opt.seed)\r\n    if cuda:\r\n        torch.cuda.manual_seed(opt.seed)\r\n\r\n    cudnn.benchmark = True\r\n\r\n    print(""===> Loading datasets"")\r\n    train_set = DatasetFromHdf5(""data/lap_pry_x4_small.h5"")\r\n    training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=True)\r\n\r\n    print(\'===> Building generator model\')\r\n    netG = _netG()\r\n\r\n    print(\'===> Building discriminator model\')    \r\n    netD = _netD()\r\n\r\n    print(\'===> Loading VGG model\') \r\n    model_urls = {\r\n        ""vgg19"": ""https://download.pytorch.org/models/vgg19-dcbb9e9d.pth""\r\n    }\r\n\r\n    netVGG = models.vgg19()\r\n    netVGG.load_state_dict(model_zoo.load_url(model_urls[\'vgg19\']))\r\n\r\n    weight = torch.FloatTensor(64,1,3,3)\r\n    parameters = list(netVGG.parameters())\r\n    for i in range(64):\r\n        weight[i,:,:,:] = parameters[0].data[i].mean(0)\r\n    bias = parameters[1].data\r\n\r\n    class _content_model(nn.Module):\r\n        def __init__(self):\r\n            super(_content_model, self).__init__()\r\n            self.conv = conv2d = nn.Conv2d(1, 64, kernel_size=3, padding=1)\r\n            self.feature = nn.Sequential(*list(netVGG.features.children())[1:-1])\r\n            self._initialize_weights()\r\n\r\n        def forward(self, x):\r\n            out = self.conv(x)\r\n            out = self.feature(out)\r\n            return out\r\n\r\n        def _initialize_weights(self):\r\n            self.conv.weight.data.copy_(weight)\r\n            self.conv.bias.data.copy_(bias)\r\n\r\n    netContent = _content_model()\r\n\r\n    print(\'===> Building Loss\')\r\n    criterion = L1_Charbonnier_loss()\r\n\r\n    print(""===> Setting GPU"")\r\n    if cuda:\r\n        netG = netG.cuda()\r\n        netD = netD.cuda()\r\n        netContent = netContent.cuda()\r\n        criterion = criterion.cuda()\r\n\r\n    # optionally resume from a checkpoint\r\n    if opt.resume:\r\n        if os.path.isfile(opt.resume):\r\n            print(""=> loading checkpoint \'{}\'"".format(opt.resume))\r\n            checkpoint = torch.load(opt.resume)\r\n            opt.start_epoch = checkpoint[""epoch""] + 1\r\n            netG.load_state_dict(checkpoint[""model""].state_dict())\r\n        else:\r\n            print(""=> no checkpoint found at \'{}\'"".format(opt.resume))\r\n\r\n    # optionally copy weights from a checkpoint\r\n    if opt.pretrained:\r\n        if os.path.isfile(opt.pretrained):\r\n            print(""=> loading model \'{}\'"".format(opt.pretrained))\r\n            weights = torch.load(opt.pretrained)\r\n            netG.load_state_dict(weights[\'model\'].state_dict())\r\n        else:\r\n            print(""=> no model found at \'{}\'"".format(opt.pretrained))\r\n\r\n    print(""===> Setting Optimizer"")\r\n    optimizerD = optim.RMSprop(netD.parameters(), lr = opt.lrD)\r\n    optimizerG = optim.RMSprop(netG.parameters(), lr = opt.lrG)\r\n\r\n    print(""===> Training"")\r\n    for epoch in range(opt.start_epoch, opt.nEpochs + 1): \r\n        train(training_data_loader, optimizerG, optimizerD, netG, netD, netContent, criterion, epoch)\r\n        save_checkpoint(netG, epoch)\r\n\r\ndef adjust_learning_rate(optimizer, epoch):\r\n    """"""Sets the learning rate to the initial LR decayed by 10 every 10 epochs""""""\r\n    lr = opt.lr * (0.1 ** (epoch // opt.step))\r\n    return lr\r\n\r\ndef train(training_data_loader, optimizerG, optimizerD, netG, netD, netContent, criterion, epoch):\r\n\r\n    netG.train()\r\n    netD.train()\r\n\r\n    one = torch.FloatTensor([1.])\r\n    mone = one * -1\r\n    content_weight = torch.FloatTensor([1.])\r\n    adversarial_weight = torch.FloatTensor([1.])\r\n\r\n    for iteration, batch in enumerate(training_data_loader, 1):\r\n\r\n        input, label_x2, label_x4 = Variable(batch[0]), Variable(batch[1], requires_grad=False), Variable(batch[2], requires_grad=False)\r\n\r\n        if opt.cuda:\r\n            input = input.cuda()\r\n            label_x2 = label_x2.cuda()\r\n            label_x4 = label_x4.cuda()\r\n            one, mone, content_weight, adversarial_weight = one.cuda(), mone.cuda(), content_weight.cuda(), adversarial_weight.cuda()\r\n\r\n        ############################\r\n        # (1) Update D network: loss = D(x)) - D(G(z))\r\n        ###########################\r\n\r\n        # train with real\r\n        errD_real = netD(label_x4)\r\n        errD_real.backward(one, retain_graph=True)\r\n\r\n        # train with fake\r\n        input_G = Variable(input.data, volatile = True)\r\n        fake_x4 = Variable(netG(input_G)[1].data)\r\n        fake_D = fake_x4\r\n        errD_fake = netD(fake_D)\r\n\r\n        errD_fake.backward(mone)\r\n\r\n        errD = errD_real - errD_fake\r\n        optimizerD.step()\r\n\r\n        for p in netD.parameters(): # reset requires_grad\r\n            p.data.clamp_(opt.clamp_lower, opt.clamp_upper)\r\n\r\n        netD.zero_grad()\r\n        netG.zero_grad()\r\n        netContent.zero_grad()\r\n        \r\n        ############################\r\n        # (2) Update G network: loss = D(G(z))\r\n        ###########################      \r\n      \r\n        fake_D_x2, fake_D_x4 = netG(input)\r\n        content_fake_x2 = netContent(fake_D_x2)\r\n        content_real_x2 = netContent(label_x2)\r\n        content_real_x2 = Variable(content_real_x2.data)       \r\n        content_loss_x2 = criterion(content_fake_x2, content_real_x2)\r\n        content_loss_x2.backward(content_weight, retain_graph=True)\r\n\r\n        content_fake_x4 = netContent(fake_D_x4)\r\n        content_real_x4 = netContent(label_x4)\r\n        content_real_x4 = Variable(content_real_x4.data)       \r\n        content_loss_x4 = criterion(content_fake_x4, content_real_x4)\r\n        content_loss_x4.backward(content_weight, retain_graph=True)\r\n\r\n        content_loss = content_loss_x2 + content_loss_x4\r\n\r\n        adversarial_loss = netD(fake_D_x4)\r\n        adversarial_loss.backward(adversarial_weight)\r\n\r\n        optimizerG.step()\r\n\r\n        netD.zero_grad()\r\n        netG.zero_grad()\r\n        netContent.zero_grad()\r\n        if iteration%10 == 0:\r\n            print(""===> Epoch[{}]({}/{}): LossD: {:.10f} [{:.10f} - {:.10f}] LossG: [{:.10f} + {:.10f}]"".format(epoch, iteration, len(training_data_loader), \r\n                  errD.data[0], errD_real.data[0], errD_fake.data[0], adversarial_loss.data[0], content_loss.data[0]))   \r\n\r\ndef save_checkpoint(model, epoch):\r\n    model_folder = ""checkpoint/""\r\n    model_out_path = model_folder + ""lapwgan_model_epoch_{}.pth"".format(epoch)\r\n    state = {""epoch"": epoch ,""model"": model}\r\n    if not os.path.exists(model_folder):\r\n        os.makedirs(model_folder)\r\n\r\n    torch.save(state, model_out_path)\r\n\r\n    print(""Checkpoint saved to {}"".format(model_out_path))\r\n\r\nif __name__ == ""__main__"":\r\n    main()'"
