file_path,api_count,code
dataset.py,2,"b'import torch.utils.data as data\r\nimport torch\r\nimport h5py\r\n\r\nclass DatasetFromHdf5(data.Dataset):\r\n    def __init__(self, file_path):\r\n        super(DatasetFromHdf5, self).__init__()\r\n        hf = h5py.File(file_path)\r\n        self.data = hf.get(""data"")\r\n        self.target = hf.get(""label"")\r\n\r\n    def __getitem__(self, index):\r\n        return torch.from_numpy(self.data[index,:,:,:]).float(), torch.from_numpy(self.target[index,:,:,:]).float()\r\n\r\n    def __len__(self):\r\n        return self.data.shape[0]'"
demo.py,4,"b'import argparse, os\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport time, math\r\nimport scipy.io as sio\r\nimport matplotlib.pyplot as plt\r\n\r\nparser = argparse.ArgumentParser(description=""PyTorch SRResNet Demo"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""use cuda?"")\r\nparser.add_argument(""--model"", default=""model/model_srresnet.pth"", type=str, help=""model path"")\r\nparser.add_argument(""--image"", default=""butterfly_GT"", type=str, help=""image name"")\r\nparser.add_argument(""--dataset"", default=""Set5"", type=str, help=""dataset name"")\r\nparser.add_argument(""--scale"", default=4, type=int, help=""scale factor, Default: 4"")\r\nparser.add_argument(""--gpus"", default=""0"", type=str, help=""gpu ids (default: 0)"")\r\n\r\ndef PSNR(pred, gt, shave_border=0):\r\n    height, width = pred.shape[:2]\r\n    pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    imdff = pred - gt\r\n    rmse = math.sqrt(np.mean(imdff ** 2))\r\n    if rmse == 0:\r\n        return 100\r\n    return 20 * math.log10(255.0 / rmse)\r\n\r\nopt = parser.parse_args()\r\ncuda = opt.cuda\r\n\r\nif cuda:\r\n    print(""=> use gpu id: \'{}\'"".format(opt.gpus))\r\n    os.environ[""CUDA_VISIBLE_DEVICES""] = opt.gpus\r\n    if not torch.cuda.is_available():\r\n            raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\r\n\r\nmodel = torch.load(opt.model)[""model""]\r\n\r\nim_gt = sio.loadmat(""testsets/"" + opt.dataset + ""/"" + opt.image + "".mat"")[\'im_gt\']\r\nim_b = sio.loadmat(""testsets/"" + opt.dataset + ""/"" + opt.image + "".mat"")[\'im_b\']\r\nim_l = sio.loadmat(""testsets/"" + opt.dataset + ""/"" + opt.image + "".mat"")[\'im_l\']\r\n           \r\nim_gt = im_gt.astype(float).astype(np.uint8)\r\nim_b = im_b.astype(float).astype(np.uint8)\r\nim_l = im_l.astype(float).astype(np.uint8)      \r\n\r\nim_input = im_l.astype(np.float32).transpose(2,0,1)\r\nim_input = im_input.reshape(1,im_input.shape[0],im_input.shape[1],im_input.shape[2])\r\nim_input = Variable(torch.from_numpy(im_input/255.).float())\r\n\r\nif cuda:\r\n    model = model.cuda()\r\n    im_input = im_input.cuda()\r\nelse:\r\n    model = model.cpu()\r\n    \r\nstart_time = time.time()\r\nout = model(im_input)\r\nelapsed_time = time.time() - start_time\r\n\r\nout = out.cpu()\r\n\r\nim_h = out.data[0].numpy().astype(np.float32)\r\n\r\nim_h = im_h*255.\r\nim_h[im_h<0] = 0\r\nim_h[im_h>255.] = 255.            \r\nim_h = im_h.transpose(1,2,0)\r\n\r\nprint(""Dataset="",opt.dataset)\r\nprint(""Scale="",opt.scale)\r\nprint(""It takes {}s for processing"".format(elapsed_time))\r\n\r\nfig = plt.figure()\r\nax = plt.subplot(""131"")\r\nax.imshow(im_gt)\r\nax.set_title(""GT"")\r\n\r\nax = plt.subplot(""132"")\r\nax.imshow(im_b)\r\nax.set_title(""Input(Bicubic)"")\r\n\r\nax = plt.subplot(""133"")\r\nax.imshow(im_h.astype(np.uint8))\r\nax.set_title(""Output(SRResNet)"")\r\nplt.show()\r\n'"
eval.py,4,"b'import matlab.engine\r\nimport argparse, os\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport time, math, glob\r\nimport scipy.io as sio\r\nimport cv2\r\n\r\nparser = argparse.ArgumentParser(description=""PyTorch SRResNet Eval"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""use cuda?"")\r\nparser.add_argument(""--model"", default=""model/model_srresnet.pth"", type=str, help=""model path"")\r\nparser.add_argument(""--dataset"", default=""Set5"", type=str, help=""dataset name, Default: Set5"")\r\nparser.add_argument(""--scale"", default=4, type=int, help=""scale factor, Default: 4"")\r\nparser.add_argument(""--gpus"", default=""0"", type=str, help=""gpu ids (default: 0)"")\r\n\r\ndef PSNR(pred, gt, shave_border=0):\r\n    height, width = pred.shape[:2]\r\n    pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    imdff = pred - gt\r\n    rmse = math.sqrt(np.mean(imdff ** 2))\r\n    if rmse == 0:\r\n        return 100\r\n    return 20 * math.log10(255.0 / rmse)\r\n\r\nopt = parser.parse_args()\r\ncuda = opt.cuda\r\neng = matlab.engine.start_matlab()\r\n\r\nif cuda:\r\n    print(""=> use gpu id: \'{}\'"".format(opt.gpus))\r\n    os.environ[""CUDA_VISIBLE_DEVICES""] = opt.gpus\r\n    if not torch.cuda.is_available():\r\n            raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\r\n\r\nmodel = torch.load(opt.model)[""model""]\r\n\r\nimage_list = glob.glob(""./testsets/"" + opt.dataset + ""/*.*"") \r\n\r\navg_psnr_predicted = 0.0\r\navg_psnr_bicubic = 0.0\r\navg_elapsed_time = 0.0\r\n\r\nfor image_name in image_list:\r\n    print(""Processing "", image_name)\r\n    im_gt_y = sio.loadmat(image_name)[\'im_gt_y\']\r\n    im_b_y = sio.loadmat(image_name)[\'im_b_y\']\r\n    im_l = sio.loadmat(image_name)[\'im_l\']\r\n\r\n    im_gt_y = im_gt_y.astype(float)\r\n    im_b_y = im_b_y.astype(float)\r\n    im_l = im_l.astype(float)\r\n\r\n    psnr_bicubic = PSNR(im_gt_y, im_b_y,shave_border=opt.scale)\r\n    avg_psnr_bicubic += psnr_bicubic\r\n\r\n    im_input = im_l.astype(np.float32).transpose(2,0,1)\r\n    im_input = im_input.reshape(1,im_input.shape[0],im_input.shape[1],im_input.shape[2])\r\n    im_input = Variable(torch.from_numpy(im_input/255.).float())\r\n\r\n    if cuda:\r\n        model = model.cuda()\r\n        im_input = im_input.cuda()\r\n    else:\r\n        model = model.cpu()\r\n\r\n    start_time = time.time()\r\n    HR_4x = model(im_input)\r\n    elapsed_time = time.time() - start_time\r\n    avg_elapsed_time += elapsed_time\r\n\r\n    HR_4x = HR_4x.cpu()\r\n\r\n    im_h = HR_4x.data[0].numpy().astype(np.float32)\r\n\r\n    im_h = im_h*255.\r\n    im_h = np.clip(im_h, 0., 255.)\r\n    im_h = im_h.transpose(1,2,0).astype(np.float32)\r\n\r\n    im_h_matlab = matlab.double((im_h / 255.).tolist())\r\n    im_h_ycbcr = eng.rgb2ycbcr(im_h_matlab)\r\n    im_h_ycbcr = np.array(im_h_ycbcr._data).reshape(im_h_ycbcr.size, order=\'F\').astype(np.float32) * 255.\r\n    im_h_y = im_h_ycbcr[:,:,0]\r\n\r\n    psnr_predicted = PSNR(im_gt_y, im_h_y,shave_border=opt.scale)\r\n    avg_psnr_predicted += psnr_predicted\r\n\r\nprint(""Scale="", opt.scale)\r\nprint(""Dataset="", opt.dataset)\r\nprint(""PSNR_predicted="", avg_psnr_predicted/len(image_list))\r\nprint(""PSNR_bicubic="", avg_psnr_bicubic/len(image_list))\r\nprint(""It takes average {}s for processing"".format(avg_elapsed_time/len(image_list)))\r\n'"
main_srresnet.py,13,"b'import argparse, os\r\nimport torch\r\nimport math, random\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\nfrom torch.utils.data import DataLoader\r\nfrom srresnet import _NetG\r\nfrom dataset import DatasetFromHdf5\r\nfrom torchvision import models\r\nimport torch.utils.model_zoo as model_zoo\r\n\r\n# Training settings\r\nparser = argparse.ArgumentParser(description=""PyTorch SRResNet"")\r\nparser.add_argument(""--batchSize"", type=int, default=16, help=""training batch size"")\r\nparser.add_argument(""--nEpochs"", type=int, default=500, help=""number of epochs to train for"")\r\nparser.add_argument(""--lr"", type=float, default=1e-4, help=""Learning Rate. Default=1e-4"")\r\nparser.add_argument(""--step"", type=int, default=200, help=""Sets the learning rate to the initial LR decayed by momentum every n epochs, Default: n=500"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""Use cuda?"")\r\nparser.add_argument(""--resume"", default="""", type=str, help=""Path to checkpoint (default: none)"")\r\nparser.add_argument(""--start-epoch"", default=1, type=int, help=""Manual epoch number (useful on restarts)"")\r\nparser.add_argument(""--threads"", type=int, default=0, help=""Number of threads for data loader to use, Default: 1"")\r\nparser.add_argument(""--pretrained"", default="""", type=str, help=""path to pretrained model (default: none)"")\r\nparser.add_argument(""--vgg_loss"", action=""store_true"", help=""Use content loss?"")\r\nparser.add_argument(""--gpus"", default=""0"", type=str, help=""gpu ids (default: 0)"")\r\n\r\ndef main():\r\n\r\n    global opt, model, netContent\r\n    opt = parser.parse_args()\r\n    print(opt)\r\n\r\n    cuda = opt.cuda\r\n    if cuda:\r\n        print(""=> use gpu id: \'{}\'"".format(opt.gpus))\r\n        os.environ[""CUDA_VISIBLE_DEVICES""] = opt.gpus\r\n        if not torch.cuda.is_available():\r\n                raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\r\n\r\n    opt.seed = random.randint(1, 10000)\r\n    print(""Random Seed: "", opt.seed)\r\n    torch.manual_seed(opt.seed)\r\n    if cuda:\r\n        torch.cuda.manual_seed(opt.seed)\r\n\r\n    cudnn.benchmark = True\r\n\r\n    print(""===> Loading datasets"")\r\n    train_set = DatasetFromHdf5(""/path/to/your/hdf5/data/like/rgb_srresnet_x4.h5"")\r\n    training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, \\\r\n        batch_size=opt.batchSize, shuffle=True)\r\n\r\n    if opt.vgg_loss:\r\n        print(\'===> Loading VGG model\')\r\n        netVGG = models.vgg19()\r\n        netVGG.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\'))\r\n        class _content_model(nn.Module):\r\n            def __init__(self):\r\n                super(_content_model, self).__init__()\r\n                self.feature = nn.Sequential(*list(netVGG.features.children())[:-1])\r\n                \r\n            def forward(self, x):\r\n                out = self.feature(x)\r\n                return out\r\n\r\n        netContent = _content_model()\r\n\r\n    print(""===> Building model"")\r\n    model = _NetG()\r\n    criterion = nn.MSELoss(size_average=False)\r\n\r\n    print(""===> Setting GPU"")\r\n    if cuda:\r\n        model = model.cuda()\r\n        criterion = criterion.cuda()\r\n        if opt.vgg_loss:\r\n            netContent = netContent.cuda() \r\n\r\n    # optionally resume from a checkpoint\r\n    if opt.resume:\r\n        if os.path.isfile(opt.resume):\r\n            print(""=> loading checkpoint \'{}\'"".format(opt.resume))\r\n            checkpoint = torch.load(opt.resume)\r\n            opt.start_epoch = checkpoint[""epoch""] + 1\r\n            model.load_state_dict(checkpoint[""model""].state_dict())\r\n        else:\r\n            print(""=> no checkpoint found at \'{}\'"".format(opt.resume))\r\n\r\n    # optionally copy weights from a checkpoint\r\n    if opt.pretrained:\r\n        if os.path.isfile(opt.pretrained):\r\n            print(""=> loading model \'{}\'"".format(opt.pretrained))\r\n            weights = torch.load(opt.pretrained)\r\n            model.load_state_dict(weights[\'model\'].state_dict())\r\n        else:\r\n            print(""=> no model found at \'{}\'"".format(opt.pretrained))\r\n\r\n    print(""===> Setting Optimizer"")\r\n    optimizer = optim.Adam(model.parameters(), lr=opt.lr)\r\n\r\n    print(""===> Training"")\r\n    for epoch in range(opt.start_epoch, opt.nEpochs + 1):\r\n        train(training_data_loader, optimizer, model, criterion, epoch)\r\n        save_checkpoint(model, epoch)\r\n\r\ndef adjust_learning_rate(optimizer, epoch):\r\n    """"""Sets the learning rate to the initial LR decayed by 10""""""\r\n    lr = opt.lr * (0.1 ** (epoch // opt.step))\r\n    return lr \r\n\r\ndef train(training_data_loader, optimizer, model, criterion, epoch):\r\n\r\n    lr = adjust_learning_rate(optimizer, epoch-1)\r\n    \r\n    for param_group in optimizer.param_groups:\r\n        param_group[""lr""] = lr\r\n\r\n    print(""Epoch={}, lr={}"".format(epoch, optimizer.param_groups[0][""lr""]))\r\n    model.train()\r\n\r\n    for iteration, batch in enumerate(training_data_loader, 1):\r\n\r\n        input, target = Variable(batch[0]), Variable(batch[1], requires_grad=False)\r\n\r\n        if opt.cuda:\r\n            input = input.cuda()\r\n            target = target.cuda()\r\n\r\n        output = model(input)\r\n        loss = criterion(output, target)\r\n\r\n        if opt.vgg_loss:\r\n            content_input = netContent(output)\r\n            content_target = netContent(target)\r\n            content_target = content_target.detach()\r\n            content_loss = criterion(content_input, content_target)\r\n\r\n        optimizer.zero_grad()\r\n\r\n        if opt.vgg_loss:\r\n            netContent.zero_grad()\r\n            content_loss.backward(retain_graph=True)\r\n\r\n        loss.backward()\r\n\r\n        optimizer.step()\r\n\r\n        if iteration%100 == 0:\r\n            if opt.vgg_loss:\r\n                print(""===> Epoch[{}]({}/{}): Loss: {:.5} Content_loss {:.5}"".format(epoch, iteration, len(training_data_loader), loss.data[0], content_loss.data[0]))\r\n            else:\r\n                print(""===> Epoch[{}]({}/{}): Loss: {:.5}"".format(epoch, iteration, len(training_data_loader), loss.data[0]))\r\n\r\ndef save_checkpoint(model, epoch):\r\n    model_out_path = ""checkpoint/"" + ""model_epoch_{}.pth"".format(epoch)\r\n    state = {""epoch"": epoch ,""model"": model}\r\n    if not os.path.exists(""checkpoint/""):\r\n        os.makedirs(""checkpoint/"")\r\n\r\n    torch.save(state, model_out_path)\r\n\r\n    print(""Checkpoint saved to {}"".format(model_out_path))\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
srresnet.py,3,"b'import torch\r\nimport torch.nn as nn\r\nimport math\r\n\r\nclass _Residual_Block(nn.Module):\r\n    def __init__(self):\r\n        super(_Residual_Block, self).__init__()\r\n\r\n        self.conv1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.in1 = nn.InstanceNorm2d(64, affine=True)\r\n        self.relu = nn.LeakyReLU(0.2, inplace=True)\r\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.in2 = nn.InstanceNorm2d(64, affine=True)\r\n\r\n    def forward(self, x):\r\n        identity_data = x\r\n        output = self.relu(self.in1(self.conv1(x)))\r\n        output = self.in2(self.conv2(output))\r\n        output = torch.add(output,identity_data)\r\n        return output \r\n\r\nclass _NetG(nn.Module):\r\n    def __init__(self):\r\n        super(_NetG, self).__init__()\r\n\r\n        self.conv_input = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, stride=1, padding=4, bias=False)\r\n        self.relu = nn.LeakyReLU(0.2, inplace=True)\r\n        \r\n        self.residual = self.make_layer(_Residual_Block, 16)\r\n\r\n        self.conv_mid = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.bn_mid = nn.InstanceNorm2d(64, affine=True)\r\n\r\n        self.upscale4x = nn.Sequential(\r\n            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.PixelShuffle(2),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.PixelShuffle(2),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n\r\n        self.conv_output = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=9, stride=1, padding=4, bias=False)\r\n        \r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n\r\n    def make_layer(self, block, num_of_layer):\r\n        layers = []\r\n        for _ in range(num_of_layer):\r\n            layers.append(block())\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        out = self.relu(self.conv_input(x))\r\n        residual = out\r\n        out = self.residual(out)\r\n        out = self.bn_mid(self.conv_mid(out))\r\n        out = torch.add(out,residual)\r\n        out = self.upscale4x(out)\r\n        out = self.conv_output(out)\r\n        return out\r\n\r\nclass _NetD(nn.Module):\r\n    def __init__(self):\r\n        super(_NetD, self).__init__()\r\n\r\n        self.features = nn.Sequential(\r\n        \r\n            # input is (3) x 96 x 96\r\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (64) x 96 x 96\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),            \r\n            nn.BatchNorm2d(64),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (64) x 96 x 96\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),            \r\n            nn.BatchNorm2d(128),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            \r\n            # state size. (64) x 48 x 48\r\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\r\n            nn.BatchNorm2d(128),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (128) x 48 x 48\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.BatchNorm2d(256),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (256) x 24 x 24\r\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\r\n            nn.BatchNorm2d(256),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (256) x 12 x 12\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False),            \r\n            nn.BatchNorm2d(512),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (512) x 12 x 12\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),            \r\n            nn.BatchNorm2d(512),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n\r\n        self.LeakyReLU = nn.LeakyReLU(0.2, inplace=True)\r\n        self.fc1 = nn.Linear(512 * 6 * 6, 1024)\r\n        self.fc2 = nn.Linear(1024, 1)\r\n        self.sigmoid = nn.Sigmoid()\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                m.weight.data.normal_(0.0, 0.02)\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                m.weight.data.normal_(1.0, 0.02)\r\n                m.bias.data.fill_(0)\r\n\r\n    def forward(self, input):\r\n\r\n        out = self.features(input)\r\n\r\n        # state size. (512) x 6 x 6\r\n        out = out.view(out.size(0), -1)\r\n\r\n        # state size. (512 x 6 x 6)\r\n        out = self.fc1(out)\r\n\r\n        # state size. (1024)\r\n        out = self.LeakyReLU(out)\r\n\r\n        out = self.fc2(out)\r\n        out = self.sigmoid(out)\r\n        return out.view(-1, 1).squeeze(1)'"
