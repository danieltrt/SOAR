file_path,api_count,code
DifferentiablePatchMatch/demo_script.py,6,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nfrom PIL import Image\nimport torch\nimport random\nimport skimage\nimport numpy as np\nfrom models.image_reconstruction import ImageReconstruction\nimport os\nimport torchvision.transforms as transforms\nimport argparse\nimport matplotlib.pyplot as plt\n\nparser = argparse.ArgumentParser(description=\'Differentiable PatchMatch\')\nparser.add_argument(\'--base_dir\', default=\'./\',\n                    help=\'path of base directory where images are stored.\')\nparser.add_argument(\'--save_dir\',  default=\'./\',\n                     help=\'save directory\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\n\nargs = parser.parse_args()\ntorch.backends.cudnn.benchmark=True\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nif args.cuda:\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n\nmodel = ImageReconstruction()\n\nif args.cuda:\n    model.cuda()\n\n\ndef main():\n    \n    base_dir = args.base_dir\n    for file1, file2 in zip(sorted(os.listdir(base_dir+\'/image_1\')), sorted(os.listdir(base_dir+\'/image_2\'))):\n\n        image_1_image_path = base_dir + \'/image_1/\' + file1\n        image_2_image_path = base_dir + \'/image_2/\' + file2\n\n        image_1 = np.asarray(Image.open(image_1_image_path).convert(\'RGB\'))\n        image_2 = np.asarray(Image.open(image_2_image_path).convert(\'RGB\'))\n        \n        image_1 = transforms.ToTensor()(image_1).unsqueeze(0).cuda().float()\n        image_2 = transforms.ToTensor()(image_2).unsqueeze(0).cuda().float()\n\n        reconstruction = model(image_1, image_2)\n\n        plt.imsave(os.path.join(args.save_dir, image_1_image_path.split(\'/\')[-1]),\n                np.asarray(reconstruction[0].permute(1,2,0).data.cpu()*256).astype(\'uint16\'))\n\n\n\nif __name__ == \'__main__\':\n    with torch.no_grad():\n        main()'"
deeppruner/finetune_kitti.py,24,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\n\nfrom __future__ import print_function\nimport argparse\nimport os\nimport random\nfrom collections import namedtuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport skimage\nimport skimage.transform\nimport numpy as np\nfrom dataloader import kitti_collector as ls\nfrom dataloader import kitti_loader as DA\nfrom models.deeppruner import DeepPruner\nfrom tensorboardX import SummaryWriter\nfrom torchvision import transforms\nfrom loss_evaluation import loss_evaluation\nfrom models.config import config as config_args\nimport matplotlib.pyplot as plt\nimport logging\nfrom setup_logging import setup_logging\n\nparser = argparse.ArgumentParser(description=\'DeepPruner\')\nparser.add_argument(\'--train_datapath_2015\', default=None,\n                    help=\'training data path of KITTI 2015\')\nparser.add_argument(\'--datapath_2012\', default=None,\n                    help=\'data path of KITTI 2012 (all used for training)\')\nparser.add_argument(\'--val_datapath_2015\', default=None,\n                    help=\'validation data path of KITTI 2015\')\nparser.add_argument(\'--epochs\', type=int, default=1040,\n                    help=\'number of epochs to train\')\nparser.add_argument(\'--loadmodel\', default=None,\n                    help=\'load model\')\nparser.add_argument(\'--savemodel\', default=\'./\',\n                    help=\'save model\')\nparser.add_argument(\'--logging_filename\', default=\'./finetune_kitti.log\',\n                    help=\'filename for logs\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n\n\nargs.cost_aggregator_scale = config_args.cost_aggregator_scale\n\nsetup_logging(args.logging_filename)\n\nall_left_img, all_right_img, all_left_disp, test_left_img, test_right_img, test_left_disp = ls.datacollector(\n    args.train_datapath_2015, args.val_datapath_2015, args.datapath_2012)\n\n\nTrainImgLoader = torch.utils.data.DataLoader(\n         DA.KITTILoader(all_left_img, all_right_img, all_left_disp, True),\n         batch_size=16, shuffle=True, num_workers=8, drop_last=False)\n\nTestImgLoader = torch.utils.data.DataLoader(\n         DA.KITTILoader(test_left_img, test_right_img, test_left_disp, False),\n         batch_size=8, shuffle=False, num_workers=4, drop_last=False)\n\nmodel = DeepPruner()\nwriter = SummaryWriter()\nmodel = nn.DataParallel(model)\n    \n\nif args.cuda:\n    model.cuda()\n\nif args.loadmodel is not None:\n    logging.info(""loading model..."")\n    state_dict = torch.load(args.loadmodel)\n    model.load_state_dict(state_dict[\'state_dict\'], strict=True)\n\n\nlogging.info(\'Number of model parameters: {}\'.format(sum([p.data.nelement() for p in model.parameters()])))\noptimizer = optim.Adam(model.parameters(), lr=0.1, betas=(0.9, 0.999))\n\n\ndef train(imgL, imgR, disp_L, iteration, epoch):\n    if epoch >= 800:\n\t\tmodel.eval()\n    else:\t\n        model.train()\n\n    imgL   = Variable(torch.FloatTensor(imgL))\n    imgR   = Variable(torch.FloatTensor(imgR))   \n    disp_L = Variable(torch.FloatTensor(disp_L))\n\n    if args.cuda:\n        imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_L.cuda()\n\n    mask = (disp_true > 0)\n    mask.detach_()\n\n    optimizer.zero_grad()\n    result = model(imgL,imgR)\n\n    loss, _ = loss_evaluation(result, disp_true, mask, args.cost_aggregator_scale)\n\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n\n    \n    \ndef test(imgL,imgR,disp_L,iteration):\n\n        model.eval()\n        with torch.no_grad():\n            imgL   = Variable(torch.FloatTensor(imgL))\n            imgR   = Variable(torch.FloatTensor(imgR))   \n            disp_L = Variable(torch.FloatTensor(disp_L))\n\n            if args.cuda:\n                imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_L.cuda()\n\n            mask = (disp_true > 0)\n            mask.detach_()\n            \n            optimizer.zero_grad()\n            \n            result = model(imgL,imgR)\n            loss, output_disparity = loss_evaluation(result, disp_true, mask, args.cost_aggregator_scale)\n\n            #computing 3-px error: (source psmnet)#\n            true_disp = disp_true.data.cpu()\n            disp_true = true_disp\n            pred_disp = output_disparity.data.cpu()\n\n            index = np.argwhere(true_disp>0)\n            disp_true[index[0][:], index[1][:], index[2][:]] = np.abs(true_disp[index[0][:], index[1][:], index[2][:]]-pred_disp[index[0][:], index[1][:], index[2][:]])\n            correct = (disp_true[index[0][:], index[1][:], index[2][:]] < 3)|(disp_true[index[0][:], index[1][:], index[2][:]] < true_disp[index[0][:], index[1][:], index[2][:]]*0.05)      \n            torch.cuda.empty_cache()             \n            \n            loss = 1-(float(torch.sum(correct))/float(len(index[0])))\n                \n        return loss\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    if epoch <= 500:\n        lr = 0.0001\n    elif epoch<=1000:\n        lr = 0.00005\n    else:\n        lr = 0.00001\n    logging.info(\'learning rate = %.5f\' %(lr))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef main():\n\n    for epoch in range(0, args.epochs):\n        total_train_loss = 0\n        total_test_loss = 0\n        adjust_learning_rate(optimizer,epoch)\n    \n        if epoch %1==0 and epoch!=0:\n            for batch_idx, (imgL, imgR, disp_L) in enumerate(TestImgLoader):\n                test_loss = test(imgL,imgR,disp_L,batch_idx)\n                total_test_loss += test_loss\n                logging.info(\'Iter %d 3-px error in val = %.3f \\n\' %(batch_idx, test_loss))\n                \n            logging.info(\'epoch %d total test loss = %.3f\' %(epoch, total_test_loss/len(TestImgLoader)))\n            writer.add_scalar(""val-loss"",total_test_loss/len(TestImgLoader),epoch)\n            \n        for batch_idx, (imgL_crop, imgR_crop, disp_crop_L) in enumerate(TrainImgLoader):\n            loss = train(imgL_crop,imgR_crop,disp_crop_L,batch_idx,epoch)\n            total_train_loss += loss\n            logging.info(\'Iter %d training loss = %.3f \\n\' %(batch_idx, loss))\n            \n        logging.info(\'epoch %d total training loss = %.3f\' %(epoch, total_train_loss/len(TrainImgLoader)))\n        writer.add_scalar(""loss"",total_train_loss/len(TrainImgLoader),epoch)\n\n        # SAVE\n        if epoch%1==0:\n            savefilename = args.savemodel+\'finetune_\'+str(epoch)+\'.tar\'\n            torch.save({\n                    \'epoch\': epoch,\n                    \'state_dict\': model.state_dict(),\n                    \'train_loss\': total_train_loss,\n                    \'test_loss\': total_test_loss,\n                }, savefilename)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
deeppruner/loss_evaluation.py,1,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch.nn.functional as F\nfrom collections import namedtuple\nimport logging\n\nloss_weights = {\n    \'alpha_super_refined\': 1.6,\n    \'alpha_refined\': 1.3,\n    \'alpha_ca\': 1.0,\n    \'alpha_quantile\': 1.0,\n    \'alpha_min_max\': 0.7\n}\n\nloss_weights = namedtuple(\'loss_weights\', loss_weights.keys())(*loss_weights.values())\n\ndef loss_evaluation(result, disp_true, mask, cost_aggregator_scale=4):\n\n    # forces min_disparity to be equal or slightly lower than the true disparity\n    quantile_mask1 = ((disp_true[mask] - result[-1][mask]) < 0).float()\n    quantile_loss1 = (disp_true[mask] - result[-1][mask]) * (0.05 - quantile_mask1)\n    quantile_min_disparity_loss = quantile_loss1.mean()\n\n    # forces max_disparity to be equal or slightly larger than the true disparity\n    quantile_mask2 = ((disp_true[mask] - result[-2][mask]) < 0).float()\n    quantile_loss2 = (disp_true[mask] - result[-2][mask]) * (0.95 - quantile_mask2)\n    quantile_max_disparity_loss = quantile_loss2.mean()\n\n    min_disparity_loss = F.smooth_l1_loss(result[-1][mask], disp_true[mask], size_average=True)\n    max_disparity_loss = F.smooth_l1_loss(result[-2][mask], disp_true[mask], size_average=True)\n    ca_depth_loss = F.smooth_l1_loss(result[-3][mask], disp_true[mask], size_average=True)\n    refined_depth_loss = F.smooth_l1_loss(result[-4][mask], disp_true[mask], size_average=True)\n\n    logging.info(""============== evaluated losses =================="")\n    if cost_aggregator_scale == 8:\n        refined_depth_loss_1 = F.smooth_l1_loss(result[-5][mask], disp_true[mask], size_average=True)\n        loss = (loss_weights.alpha_super_refined * refined_depth_loss_1)\n        output_disparity = result[-5]\n        logging.info(\'refined_depth_loss_1: %.6f\', refined_depth_loss_1)\n    else:\n        loss = 0\n        output_disparity = result[-4]\n\n    loss += (loss_weights.alpha_refined * refined_depth_loss) + \\\n            (loss_weights.alpha_ca * ca_depth_loss) + \\\n            (loss_weights.alpha_quantile * (quantile_max_disparity_loss + quantile_min_disparity_loss)) + \\\n            (loss_weights.alpha_min_max * (min_disparity_loss + max_disparity_loss))\n\n    logging.info(\'refined_depth_loss: %.6f\' % refined_depth_loss)\n    logging.info(\'ca_depth_loss: %.6f\' % ca_depth_loss)\n    logging.info(\'quantile_loss_max_disparity: %.6f\' % quantile_max_disparity_loss)\n    logging.info(\'quantile_loss_min_disparity: %.6f\' % quantile_min_disparity_loss)\n    logging.info(\'max_disparity_loss: %.6f\' % max_disparity_loss)\n    logging.info(\'min_disparity_loss: %.6f\' % min_disparity_loss)\n    logging.info(""==================================================\\n"")\n\n    return loss, output_disparity\n'"
deeppruner/setup_logging.py,0,"b""import logging\n\ndef setup_logging(filename):\n\n    log_format = '%(filename)s: %(message)s'\n    logging.basicConfig(format=log_format, level=logging.INFO)\n\n    file_handler = logging.FileHandler(filename)\n    file_handler.setFormatter(logging.Formatter(fmt=log_format))\n    file_handler.setLevel(logging.INFO)\n    logging.getLogger().addHandler(file_handler)\n    """
deeppruner/submission_kitti.py,16,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport skimage.io\nimport numpy as np\nimport logging\nfrom dataloader import kitti_submission_collector as ls\nfrom dataloader import preprocess\nfrom PIL import Image\nfrom models.deeppruner import DeepPruner\nfrom models.config import config as config_args\nfrom setup_logging import setup_logging\n\nparser = argparse.ArgumentParser(description=\'DeepPruner\')\nparser.add_argument(\'--datapath\', default=\'/\',\n                    help=\'datapath\')\nparser.add_argument(\'--loadmodel\', default=None,\n                    help=\'load model\')\nparser.add_argument(\'--save_dir\', default=\'./\',\n                    help=\'save directory\')\nparser.add_argument(\'--logging_filename\', default=\'./submission_kitti.log\',\n                    help=\'filename for logs\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\n\nargs = parser.parse_args()\ntorch.backends.cudnn.benchmark = True\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nargs.cost_aggregator_scale = config_args.cost_aggregator_scale\nargs.downsample_scale = args.cost_aggregator_scale * 8.0\n\nsetup_logging(args.logging_filename)\n\nif args.cuda:\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n\n\ntest_left_img, test_right_img = ls.datacollector(args.datapath)\n\nmodel = DeepPruner()\nmodel = nn.DataParallel(model)\n    \nif args.cuda:\n    model.cuda()\n\nlogging.info(\'Number of model parameters: {}\'.format(sum([p.data.nelement() for p in model.parameters()])))\n\n\nif args.loadmodel is not None:\n    logging.info(""loading model..."")\n    state_dict = torch.load(args.loadmodel)\n    model.load_state_dict(state_dict[\'state_dict\'], strict=True)\n\n\ndef test(imgL, imgR):\n    model.eval()\n    with torch.no_grad():\n        imgL = Variable(torch.FloatTensor(imgL))\n        imgR = Variable(torch.FloatTensor(imgR))\n\n        if args.cuda:\n            imgL, imgR = imgL.cuda(), imgR.cuda()\n\n        refined_disparity = model(imgL, imgR)\n        return refined_disparity\n\n\ndef main():\n\n    for left_image_path, right_image_path in zip(test_left_img, test_right_img):\n        imgL = np.asarray(Image.open(left_image_path))\n        imgR = np.asarray(Image.open(right_image_path))\n\n        processed = preprocess.get_transform()\n        imgL = processed(imgL).numpy()\n        imgR = processed(imgR).numpy()\n\n        imgL = np.reshape(imgL, [1, 3, imgL.shape[1], imgL.shape[2]])\n        imgR = np.reshape(imgR, [1, 3, imgR.shape[1], imgR.shape[2]])\n\n        w = imgL.shape[3]\n        h = imgL.shape[2]\n        dw = int(args.downsample_scale - (w%args.downsample_scale + (w%args.downsample_scale==0)*args.downsample_scale))\n        dh = int(args.downsample_scale - (h%args.downsample_scale + (h%args.downsample_scale==0)*args.downsample_scale))\n\n        top_pad = dh\n        left_pad = dw\n        imgL = np.lib.pad(imgL, ((0, 0), (0, 0), (top_pad, 0), (0, left_pad)), mode=\'constant\', constant_values=0)\n        imgR = np.lib.pad(imgR, ((0, 0), (0, 0), (top_pad, 0), (0, left_pad)), mode=\'constant\', constant_values=0)\n\n        disparity = test(imgL, imgR)\n        disparity = disparity[0, top_pad:, :-left_pad].data.cpu().numpy()\n        skimage.io.imsave(os.path.join(args.save_dir, left_image_path.split(\'/\')\n                                       [-1]), (disparity * 256).astype(\'uint16\'))\n\n        logging.info(""Disparity for {} generated at: {}"".format(left_image_path, os.path.join(args.save_dir, \n                                                                left_image_path.split(\'/\')[-1])))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
deeppruner/train_sceneflow.py,22,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport numpy as np\nfrom dataloader import sceneflow_collector as lt\nfrom dataloader import sceneflow_loader as DA\nfrom models.deeppruner import DeepPruner\nfrom loss_evaluation import loss_evaluation\nfrom tensorboardX import SummaryWriter\nimport skimage\nimport time\nimport logging\nfrom models.config import config as config_args\nfrom setup_logging import setup_logging\n\nparser = argparse.ArgumentParser(description=\'DeepPruner\')\nparser.add_argument(\'--datapath_monkaa\', default=\'/\',\n                    help=\'datapath for sceneflow monkaa dataset\')\nparser.add_argument(\'--datapath_flying\', default=\'/\',\n                    help=\'datapath for sceneflow flying dataset\')\nparser.add_argument(\'--datapath_driving\', default=\'/\',\n                    help=\'datapath for sceneflow driving dataset\')\nparser.add_argument(\'--epochs\', type=int, default=100,\n                    help=\'number of epochs to train\')\nparser.add_argument(\'--loadmodel\', default=None,\n                    help=\'load model\')\nparser.add_argument(\'--save_dir\', default=\'./\',\n                    help=\'save directory\')\nparser.add_argument(\'--savemodel\', default=\'./\',\n                    help=\'save model\')\nparser.add_argument(\'--logging_filename\', default=\'./train_sceneflow.log\',\n                    help=\'save model\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n\n\nargs.cost_aggregator_scale = config_args.cost_aggregator_scale\nargs.maxdisp = config_args.max_disp\n\nsetup_logging(args.logging_filename)\n\n\nall_left_img, all_right_img, all_left_disp, test_left_img, test_right_img, test_left_disp = lt.dataloader(args.datapath_monkaa,\nargs.datapath_flying, args.datapath_driving)\n\n\nTrainImgLoader = torch.utils.data.DataLoader(\n    DA.SceneflowLoader(all_left_img, all_right_img, all_left_disp, args.cost_aggregator_scale*8.0, True),\n    batch_size=1, shuffle=True, num_workers=8, drop_last=False)\n\nTestImgLoader = torch.utils.data.DataLoader(\n    DA.SceneflowLoader(test_left_img, test_right_img, test_left_disp, args.cost_aggregator_scale*8.0, False),\n    batch_size=1, shuffle=False, num_workers=4, drop_last=False)\n\n\nmodel = DeepPruner()\nwriter = SummaryWriter()\n\nif args.cuda:\n    model = nn.DataParallel(model)\n    model.cuda()\n\n\nif args.loadmodel is not None:\n    state_dict = torch.load(args.loadmodel)\n    model.load_state_dict(state_dict[\'state_dict\'], strict=True)\n\nlogging.info(\'Number of model parameters: {}\'.format(sum([p.data.nelement() for p in model.parameters()])))\n\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n\n\ndef train(imgL, imgR, disp_L, iteration):\n    model.train()\n    imgL = Variable(torch.FloatTensor(imgL))\n    imgR = Variable(torch.FloatTensor(imgR))\n    disp_L = Variable(torch.FloatTensor(disp_L))\n\n    if args.cuda:\n        imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_L.cuda()\n\n    mask = disp_true < args.maxdisp\n    mask.detach_()\n\n    optimizer.zero_grad()\n    result = model(imgL, imgR)\n\n    loss, _ = loss_evaluation(result, disp_true, mask, args.cost_aggregator_scale)\n\n    loss.backward()\n    optimizer.step()\n\n    return loss.item()\n\n\ndef test(imgL, imgR, disp_L, iteration, pad_w, pad_h):\n\n    model.eval()\n    with torch.no_grad():\n        imgL = Variable(torch.FloatTensor(imgL))\n        imgR = Variable(torch.FloatTensor(imgR))\n        disp_L = Variable(torch.FloatTensor(disp_L))\n\n        if args.cuda:\n            imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_L.cuda()\n\n        mask = disp_true < args.maxdisp\n        mask.detach_()\n\n        if len(disp_true[mask]) == 0:\n            logging.info(""invalid GT disaprity..."")\n            return 0, 0\n\n        optimizer.zero_grad()\n\n        result = model(imgL, imgR)\n        output = []\n        for ind in range(len(result)):\n            output.append(result[ind][:, pad_h:, pad_w:])\n        result = output\n\n        loss, output_disparity = loss_evaluation(result, disp_true, mask, args.cost_aggregator_scale)\n        epe_loss = torch.mean(torch.abs(output_disparity[mask] - disp_true[mask]))\n\n    return loss.item(), epe_loss.item()\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    if epoch <= 20:\n        lr = 0.001\n    elif epoch <= 40:\n        lr = 0.0007\n    elif epoch <= 60:\n        lr = 0.0003\n    else:\n        lr = 0.0001\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef main():\n    for epoch in range(0, args.epochs):\n        total_train_loss = 0\n        total_test_loss = 0\n        total_epe_loss = 0\n        adjust_learning_rate(optimizer, epoch)\n\n        if epoch % 1 == 0 and epoch != 0:\n            logging.info(""testing..."")\n            for batch_idx, (imgL, imgR, disp_L, pad_w, pad_h) in enumerate(TestImgLoader):\n                start_time = time.time()\n                test_loss, epe_loss = test(imgL, imgR, disp_L, batch_idx, int(pad_w[0].item()), int(pad_h[0].item()))\n                total_test_loss += test_loss\n                total_epe_loss += epe_loss\n\n                logging.info(\'Iter %d 3-px error in val = %.3f, time = %.2f \\n\' %\n                      (batch_idx, epe_loss, time.time() - start_time))\n\n                writer.add_scalar(""val-loss-iter"", test_loss, epoch * 4370 + batch_idx)\n                writer.add_scalar(""val-epe-loss-iter"", epe_loss, epoch * 4370 + batch_idx)\n\n            logging.info(\'epoch %d total test loss = %.3f\' % (epoch, total_test_loss / len(TestImgLoader)))\n            writer.add_scalar(""val-loss"", total_test_loss / len(TestImgLoader), epoch)\n            logging.info(\'epoch %d total epe loss = %.3f\' % (epoch, total_epe_loss / len(TestImgLoader)))\n            writer.add_scalar(""epe-loss"", total_epe_loss / len(TestImgLoader), epoch)\n\n        for batch_idx, (imgL_crop, imgR_crop, disp_crop_L) in enumerate(TrainImgLoader):\n            start_time = time.time()\n            loss = train(imgL_crop, imgR_crop, disp_crop_L, batch_idx)\n            total_train_loss += loss\n\n            writer.add_scalar(""loss-iter"", loss, batch_idx + 35454 * epoch)\n            logging.info(\'Iter %d training loss = %.3f , time = %.2f \\n\' % (batch_idx, loss, time.time() - start_time))\n\n        logging.info(\'epoch %d total training loss = %.3f\' % (epoch, total_train_loss / len(TrainImgLoader)))\n        writer.add_scalar(""loss"", total_train_loss / len(TrainImgLoader), epoch)\n\n        # SAVE\n        if epoch % 1 == 0:\n            savefilename = args.savemodel + \'finetune_\' + str(epoch) + \'.tar\'\n            torch.save({\n                \'epoch\': epoch,\n                \'state_dict\': model.state_dict(),\n                \'train_loss\': total_train_loss,\n                \'test_loss\': total_test_loss,\n            }, savefilename)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
DifferentiablePatchMatch/models/__init__.py,0,b''
DifferentiablePatchMatch/models/config.py,0,"b'\n# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\n\nclass obj(object):\n    def __init__(self, d):\n        for key, value in d.items():\n            if isinstance(value, (list, tuple)):\n               setattr(self, key, [obj(x) if isinstance(x, dict) else x for x in value])\n            else:\n               setattr(self, key, obj(value) if isinstance(value, dict) else value)\n               \nconfig = {\n    ""patch_match_args"": {\n        # sample count refers to random sampling stage of generalized PM.\n        # Number of random samples generated: (sample_count+1) * (sample_count+1)\n        # we generate (sample_count+1) samples in x direction, and (sample_count+1) samples in y direction,\n        # and then perform meshgrid like opertaion to generate (sample_count+1) * (sample_count+1) samples.\n        ""sample_count"": 1,\n\n        ""iteration_count"": 21,\n        ""propagation_filter_size"": 3,\n        ""propagation_type"": ""faster_filter_3_propagation"", # for better code for PM propagation, set it to None\n        ""softmax_temperature"": 10000000000, # softmax temperature for evaluation. Larger temp. lead to sharper output.\n        ""random_search_window_size"": [100,100], # search range around evaluated offsets after every iteration.\n        ""evaluation_type"": ""softmax""\n    },\n\n    ""feature_extractor_filter_size"": 7\n \n}\n\n\nconfig = obj(config)'"
DifferentiablePatchMatch/models/feature_extractor.py,4,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass feature_extractor(nn.Module):\n    def __init__(self, filter_size):\n        super(feature_extractor, self).__init__()\n\n        self.filter_size = filter_size\n\n    def forward(self, left_input, right_input):\n        """"""\n        Feature Extractor\n\n        Description: Aggregates the RGB values from the neighbouring pixels in the window (filter_size * filter_size).\n                    No weights are learnt for this feature extractor.\n\n        Args:\n            :param left_input: Left Image\n            :param right_input: Right Image\n\n        Returns:\n            :left_features: Left Image features\n            :right_features: Right Image features\n            :one_hot_filter: Convolution filter used to aggregate neighbour RGB features to the center pixel.\n                             one_hot_filter.shape = (filter_size * filter_size)\n        """"""\n\n        device = left_input.get_device()\n\n        label = torch.arange(0, self.filter_size * self.filter_size, device=device).repeat(\n            self.filter_size * self.filter_size).view(\n            self.filter_size * self.filter_size, 1, 1, self.filter_size, self.filter_size)\n\n        one_hot_filter = torch.zeros_like(label).scatter_(0, label, 1).float()\n\n        left_features = F.conv3d(left_input.unsqueeze(1), one_hot_filter,\n                                 padding=(0, self.filter_size // 2, self.filter_size // 2))\n        right_features = F.conv3d(right_input.unsqueeze(1), one_hot_filter,\n                                  padding=(0, self.filter_size // 2, self.filter_size // 2))\n\n        left_features = left_features.view(left_features.size()[0],\n                                           left_features.size()[1] * left_features.size()[2],\n                                           left_features.size()[3],\n                                           left_features.size()[4])\n\n        right_features = right_features.view(right_features.size()[0],\n                                             right_features.size()[1] * right_features.size()[2],\n                                             right_features.size()[3],\n                                             right_features.size()[4])\n\n        return left_features, right_features, one_hot_filter\n'"
DifferentiablePatchMatch/models/image_reconstruction.py,6,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom models.patch_match import PatchMatch\nfrom models.feature_extractor import feature_extractor\nfrom models.config import config as args\n\n\nclass Reconstruct(nn.Module):\n    def __init__(self, filter_size):\n        super(Reconstruct, self).__init__()\n        self.filter_size = filter_size\n\n    def forward(self, right_input, offset_x, offset_y, x_coordinate, y_coordinate, neighbour_extraction_filter):\n        """"""\n        Reconstruct the left image using the NNF(NNF represented by the offsets and the xy_coordinates)\n        We did Patch Voting on the offset field, before reconstruction, in order to\n                generate smooth reconstruction.\n        Args:\n            :right_input: Right Image\n            :offset_x: horizontal offset to generate the NNF.\n            :offset_y: vertical offset to generate the NNF.\n            :x_coordinate: X coordinate\n            :y_coordinate: Y coordinate\n\n        Returns:\n            :reconstruction: Right image reconstruction\n        """"""\n\n        pad_size = self.filter_size // 2\n        smooth_offset_x = nn.ReflectionPad2d(\n            (pad_size, pad_size, pad_size, pad_size))(offset_x)\n        smooth_offset_y = nn.ReflectionPad2d(\n            (pad_size, pad_size, pad_size, pad_size))(offset_y)\n\n        smooth_offset_x = F.conv2d(smooth_offset_x,\n                                   neighbour_extraction_filter,\n                                   padding=(pad_size, pad_size))[:, :, pad_size:-pad_size, pad_size:-pad_size]\n\n        smooth_offset_y = F.conv2d(smooth_offset_y,\n                                   neighbour_extraction_filter,\n                                   padding=(pad_size, pad_size))[:, :, pad_size:-pad_size, pad_size:-pad_size]\n\n        coord_x = torch.clamp(\n            x_coordinate - smooth_offset_x,\n            min=0,\n            max=smooth_offset_x.size()[3] - 1)\n\n        coord_y = torch.clamp(\n            y_coordinate - smooth_offset_y,\n            min=0,\n            max=smooth_offset_x.size()[2] - 1)\n\n        coord_x -= coord_x.size()[3] / 2\n        coord_x /= (coord_x.size()[3] / 2)\n\n        coord_y -= coord_y.size()[2] / 2\n        coord_y /= (coord_y.size()[2] / 2)\n\n        grid = torch.cat((coord_x.unsqueeze(4), coord_y.unsqueeze(4)), dim=4)\n        grid = grid.view(grid.size()[0] * grid.size()[1], grid.size()[2], grid.size()[3], grid.size()[4])\n        reconstruction = F.grid_sample(right_input.repeat(grid.size()[0], 1, 1, 1), grid)\n        reconstruction = torch.mean(reconstruction, dim=0).unsqueeze(0)\n\n        return reconstruction\n\n\nclass ImageReconstruction(nn.Module):\n    def __init__(self):\n        super(ImageReconstruction, self).__init__()\n\n        self.patch_match = PatchMatch(args.patch_match_args)\n\n        filter_size = args.feature_extractor_filter_size\n        self.feature_extractor = feature_extractor(filter_size)\n        self.reconstruct = Reconstruct(filter_size)\n\n    def forward(self, left_input, right_input):\n        """"""\n        ImageReconstruction:\n        Description: This class performs the task of reconstruction the left image using the data of the other image,,\n            by fidning correspondences (nnf) between the two fields.\n            The images acan be any random images with some overlap between the two to assist\n            the correspondence matching.\n            For feature_extractor, we just use the RGB features of a (self.filter_size * self.filter_size) patch\n            around each pixel.\n            For finding the correspondences, we use the Differentiable PatchMatch.\n            ** Note: There is no assumption of rectification between the two images. **\n            ** Note: The words \'left\' and \'right\' do not have any significance.**\n\n\n        Args:\n            :left_input:  Left Image (Image 1)\n            :right_input:  Right Image (Image 2)\n\n        Returns:\n            :reconstruction: Reconstructed left image.\n        """"""\n\n        left_features, right_features, neighbour_extraction_filter = self.feature_extractor(left_input, right_input)\n        offset_x, offset_y, x_coordinate, y_coordinate = self.patch_match(left_features, right_features)\n\n        reconstruction = self.reconstruct(right_input,\n                                          offset_x, offset_y,\n                                          x_coordinate, y_coordinate,\n                                          neighbour_extraction_filter.squeeze(1))\n\n        return reconstruction\n'"
DifferentiablePatchMatch/models/patch_match.py,38,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RandomSampler(nn.Module):\n    def __init__(self, device, number_of_samples):\n        super(RandomSampler, self).__init__()\n\n        # Number of offset samples generated by this function: (number_of_samples+1) * (number_of_samples+1)\n        # we generate (number_of_samples+1) samples in x direction, and (number_of_samples+1) samples in y direction,\n        # and then perform meshgrid like opertaion to generate (number_of_samples+1) * (number_of_samples+1) samples\n        self.number_of_samples = number_of_samples\n        self.range_multiplier = torch.arange(0.0, number_of_samples + 1, 1, device=device).view(\n            number_of_samples + 1, 1, 1)\n\n    def forward(self, min_offset_x, max_offset_x, min_offset_y, max_offset_y):\n        """"""\n        Random Sampler:\n            Given the search range per pixel (defined by: [[lx(i), ux(i)], [ly(i), uy(i)]]),\n            where lx = lower_bound of the hoizontal offset,\n                  ux = upper_bound of the horizontal offset,\n                  ly = lower_bound of the vertical offset,\n                  uy = upper_bound of teh vertical offset, for all pixel i. )\n            random sampler generates samples from this search range.\n            First the search range is discretized into `number_of_samples` buckets,\n            then a random sample is generated from each random bucket.\n            ** Discretization is done in both xy directions. ** (similar to meshgrid)\n\n        Args:\n            :min_offset_x: Min horizontal offset of the search range.\n            :max_offset_x: Max horizontal offset of the search range.\n            :min_offset_y: Min vertical offset of the search range.\n            :max_offset_y: Max vertical offset of the search range.\n        Returns:\n            :offset_x: samples representing offset in the horizontal direction.\n            :offset_y: samples representing offset in the vertical direction.\n        """"""\n\n        device = min_offset_x.get_device()\n        noise = torch.rand(min_offset_x.repeat(1, self.number_of_samples + 1, 1, 1).size(), device=device)\n\n        offset_x = min_offset_x + ((max_offset_x - min_offset_x) / (self.number_of_samples + 1)) * \\\n            (self.range_multiplier + noise)\n        offset_y = min_offset_y + ((max_offset_y - min_offset_y) / (self.number_of_samples + 1)) * \\\n            (self.range_multiplier + noise)\n\n        offset_x = offset_x.unsqueeze_(1).expand(-1, offset_y.size()[1], -1, -1, -1)\n        offset_x = offset_x.contiguous().view(\n            offset_x.size()[0], offset_x.size()[1] * offset_x.size()[2], offset_x.size()[3], offset_x.size()[4])\n\n        offset_y = offset_y.unsqueeze_(2).expand(-1, -1, offset_y.size()[1], -1, -1)\n        offset_y = offset_y.contiguous().view(\n            offset_y.size()[0], offset_y.size()[1] * offset_y.size()[2], offset_y.size()[3], offset_y.size()[4])\n\n        return offset_x, offset_y\n\n\nclass Evaluate(nn.Module):\n    def __init__(self, left_features, filter_size, evaluation_type=\'softmax\', temperature=10000):\n        super(Evaluate, self).__init__()\n        self.temperature = temperature\n        self.filter_size = filter_size\n        self.softmax = torch.nn.Softmax(dim=1)\n        self.evaluation_type = evaluation_type\n\n        device = left_features.get_device()\n        self.left_x_coordinate = torch.arange(0.0, left_features.size()[3], device=device).repeat(\n            left_features.size()[2]).view(left_features.size()[2], left_features.size()[3])\n\n        self.left_x_coordinate = torch.clamp(self.left_x_coordinate, min=0, max=left_features.size()[3] - 1)\n        self.left_x_coordinate = self.left_x_coordinate.expand(left_features.size()[0], -1, -1).unsqueeze(1)\n\n        self.left_y_coordinate = torch.arange(0.0, left_features.size()[2], device=device).unsqueeze(1).repeat(\n            1, left_features.size()[3]).view(left_features.size()[2], left_features.size()[3])\n\n        self.left_y_coordinate = torch.clamp(self.left_y_coordinate, min=0, max=left_features.size()[3] - 1)\n        self.left_y_coordinate = self.left_y_coordinate.expand(left_features.size()[0], -1, -1).unsqueeze(1)\n\n    def forward(self, left_features, right_features, offset_x, offset_y):\n        """"""\n        PatchMatch Evaluation Block\n        Description:    For each pixel i, matching scores are computed by taking the inner product between the\n                left feature and the right feature: score(i,j) = feature_left(i), feature_right(i+disparity(i,j))\n                for all candidates j. The best k disparity value for each pixel is carried towards the next iteration.\n\n                As per implementation,\n                the complete disparity search range is discretized into intervals in\n                DisparityInitialization() function. Corresponding to each disparity interval, we have multiple samples\n                to evaluate. The best disparity sample per interval is the output of the function.\n\n        Args:\n            :left_features: Left Image Feature Map\n            :right_features: Right Image Feature Map\n            :offset_x: samples representing offset in the horizontal direction.\n            :offset_y: samples representing offset in the vertical direction.\n\n        Returns:\n            :offset_x: horizontal offset evaluated as the best offset to generate NNF.\n            :offset_y: vertical offset evaluated as the best offset to generate NNF.\n\n        """"""\n\n        right_x_coordinate = torch.clamp(self.left_x_coordinate - offset_x, min=0, max=left_features.size()[3] - 1)\n        right_y_coordinate = torch.clamp(self.left_y_coordinate - offset_y, min=0, max=left_features.size()[2] - 1)\n\n        right_x_coordinate -= right_x_coordinate.size()[3] / 2\n        right_x_coordinate /= (right_x_coordinate.size()[3] / 2)\n        right_y_coordinate -= right_y_coordinate.size()[2] / 2\n        right_y_coordinate /= (right_y_coordinate.size()[2] / 2)\n\n        samples = torch.cat((right_x_coordinate.unsqueeze(4), right_y_coordinate.unsqueeze(4)), dim=4)\n        samples = samples.view(samples.size()[0] * samples.size()[1],\n                               samples.size()[2],\n                               samples.size()[3],\n                               samples.size()[4])\n\n        offset_strength = torch.mean(-1.0 * (torch.abs(left_features.expand(\n            offset_x.size()[1], -1, -1, -1) - F.grid_sample(right_features.expand(\n                offset_x.size()[1], -1, -1, -1), samples))), dim=1) * self.temperature\n\n        offset_strength = offset_strength.view(left_features.size()[0],\n                                               offset_strength.size()[0] // left_features.size()[0],\n                                               offset_strength.size()[1],\n                                               offset_strength.size()[2])\n\n        if self.evaluation_type == ""softmax"":\n            offset_strength = torch.softmax(offset_strength, dim=1)\n            offset_x = torch.sum(offset_x * offset_strength, dim=1).unsqueeze(1)\n            offset_y = torch.sum(offset_y * offset_strength, dim=1).unsqueeze(1)\n        else:\n            offset_strength = torch.argmax(offset_strength, dim=1).unsqueeze(1)\n            offset_x = torch.gather(offset_x, index=offset_strength, dim=1)\n            offset_y = torch.gather(offset_y, index=offset_strength, dim=1)\n\n        return offset_x, offset_y\n\n\nclass Propagation(nn.Module):\n    def __init__(self, device, filter_size):\n        super(Propagation, self).__init__()\n        self.filter_size = filter_size\n        label = torch.arange(0, self.filter_size, device=device).repeat(self.filter_size).view(\n            self.filter_size, 1, 1, 1, self.filter_size)\n\n        self.one_hot_filter_h = torch.zeros_like(label).scatter_(0, label, 1).float()\n\n        label = torch.arange(0, self.filter_size, device=device).repeat(self.filter_size).view(\n            self.filter_size, 1, 1, self.filter_size, 1).long()\n\n        self.one_hot_filter_v = torch.zeros_like(label).scatter_(0, label, 1).float()\n\n    def forward(self, offset_x, offset_y, propagation_type=""horizontal""):\n        """"""\n        PatchMatch Propagation Block\n        Description:    Particles from adjacent pixels are propagated together through convolution with a\n                        one-hot filter, which en-codes the fact that we allow each pixel\n                        to propagate particles to its 4-neighbours.\n        Args:\n            :offset_x: samples representing offset in the horizontal direction.\n            :offset_y: samples representing offset in the vertical direction.\n            :device: Cuda/ CPU device\n            :propagation_type (default:""horizontal""): In order to be memory efficient, we use separable convolutions\n                                                    for propagtaion.\n\n        Returns:\n            :aggregated_offset_x: Horizontal offset samples aggregated from the neighbours.\n            :aggregated_offset_y: Vertical offset samples aggregated from the neighbours.\n\n        """"""\n\n        offset_x = offset_x.view(offset_x.size()[0], 1, offset_x.size()[1], offset_x.size()[2], offset_x.size()[3])\n        offset_y = offset_y.view(offset_y.size()[0], 1, offset_y.size()[1], offset_y.size()[2], offset_y.size()[3])\n\n        if propagation_type is ""horizontal"":\n            aggregated_offset_x = F.conv3d(offset_x, self.one_hot_filter_h, padding=(0, 0, self.filter_size // 2))\n            aggregated_offset_y = F.conv3d(offset_y, self.one_hot_filter_h, padding=(0, 0, self.filter_size // 2))\n\n        else:\n            aggregated_offset_x = F.conv3d(offset_x, self.one_hot_filter_v, padding=(0, self.filter_size // 2, 0))\n            aggregated_offset_y = F.conv3d(offset_y, self.one_hot_filter_v, padding=(0, self.filter_size // 2, 0))\n\n        aggregated_offset_x = aggregated_offset_x.permute([0, 2, 1, 3, 4])\n        aggregated_offset_x = aggregated_offset_x.contiguous().view(\n            aggregated_offset_x.size()[0],\n            aggregated_offset_x.size()[1] * aggregated_offset_x.size()[2],\n            aggregated_offset_x.size()[3],\n            aggregated_offset_x.size()[4])\n\n        aggregated_offset_y = aggregated_offset_y.permute([0, 2, 1, 3, 4])\n        aggregated_offset_y = aggregated_offset_y.contiguous().view(\n            aggregated_offset_y.size()[0],\n            aggregated_offset_y.size()[1] * aggregated_offset_y.size()[2],\n            aggregated_offset_y.size()[3],\n            aggregated_offset_y.size()[4])\n\n        return aggregated_offset_x, aggregated_offset_y\n\n\nclass PropagationFaster(nn.Module):\n    def __init__(self):\n        super(PropagationFaster, self).__init__()\n\n    def forward(self, offset_x, offset_y, device, propagation_type=""horizontal""):\n        """"""\n        Faster version of PatchMatch Propagation Block\n        This version uses a fixed propagation filter size of size 3. This implementation is not recommended\n        and is used only to do the propagation faster.\n\n        Description:    Particles from adjacent pixels are propagated together through convolution with a\n                        one-hot filter, which en-codes the fact that we allow each pixel\n                        to propagate particles to its 4-neighbours.\n        Args:\n            :offset_x: samples representing offset in the horizontal direction.\n            :offset_y: samples representing offset in the vertical direction.\n            :device: Cuda/ CPU device\n            :propagation_type (default:""horizontal""): In order to be memory efficient, we use separable convolutions\n                                                    for propagtaion.\n\n        Returns:\n            :aggregated_offset_x: Horizontal offset samples aggregated from the neighbours.\n            :aggregated_offset_y: Vertical offset samples aggregated from the neighbours.\n\n        """"""\n\n        self.vertical_zeros = torch.zeros((offset_x.size()[0], offset_x.size()[1], 1, offset_x.size()[3])).to(device)\n        self.horizontal_zeros = torch.zeros((offset_x.size()[0], offset_x.size()[1], offset_x.size()[2], 1)).to(device)\n\n        if propagation_type is ""horizontal"":\n            offset_x = torch.cat((torch.cat((self.horizontal_zeros, offset_x[:, :, :, :-1]), dim=3),\n                                  offset_x,\n                                  torch.cat((offset_x[:, :, :, 1:], self.horizontal_zeros), dim=3)), dim=1)\n            offset_y = torch.cat((torch.cat((self.horizontal_zeros, offset_y[:, :, :, :-1]), dim=3),\n                                  offset_y,\n                                  torch.cat((offset_y[:, :, :, 1:], self.horizontal_zeros), dim=3)), dim=1)\n\n        else:\n            offset_x = torch.cat((torch.cat((self.vertical_zeros, offset_x[:, :, :-1, :]), dim=2),\n                                  offset_x,\n                                  torch.cat((offset_x[:, :, 1:, :], self.vertical_zeros), dim=2)), dim=1)\n            offset_y = torch.cat((torch.cat((self.vertical_zeros, offset_y[:, :, :-1, :]), dim=2),\n                                  offset_y,\n                                  torch.cat((offset_y[:, :, 1:, :], self.vertical_zeros), dim=2)), dim=1)\n\n        return offset_x, offset_y\n\n\nclass PatchMatch(nn.Module):\n    def __init__(self, patch_match_args):\n        super(PatchMatch, self).__init__()\n        self.propagation_filter_size = patch_match_args.propagation_filter_size\n        self.number_of_samples = patch_match_args.sample_count\n        self.iteration_count = patch_match_args.iteration_count\n        self.evaluation_type = patch_match_args.evaluation_type\n        self.softmax_temperature = patch_match_args.softmax_temperature\n        self.propagation_type = patch_match_args.propagation_type\n\n        self.window_size_x = patch_match_args.random_search_window_size[0]\n        self.window_size_y = patch_match_args.random_search_window_size[1]\n\n    def forward(self, left_features, right_features):\n        """"""\n        Differential PatchMatch Block\n        Description:    In this work, we unroll generalized PatchMatch as a recurrent neural network,\n                        where each unrolling step is equivalent to each iteration of the algorithm.\n                        This is important as it allow us to train our full model end-to-end.\n                        Specifically, we design the following layers:\n                            - Initialization or Paticle Sampling\n                            - Propagation\n                            - Evaluation\n        Args:\n            :left_features: Left Image feature map\n            :right_features: Right image feature map\n\n        Returns:\n            :offset_x: offset for each pixel in the left_features corresponding to the\n                                                        right_features in the horizontal direction.\n            :offset_y: offset for each pixel in the left_features corresponding to the\n                                                        right_features in the vertical direction.\n\n            :x_coordinate: X coordinate corresponding to each pxiel.\n            :y_coordinate: Y coordinate corresponding to each pxiel.\n\n            (Offsets and the xy_cooridnates returned are used to generated the NNF field later for reconstruction.)\n\n        """"""\n\n        device = left_features.get_device()\n        if self.propagation_type is ""faster_filter_3_propagation"":\n            self.propagation = PropagationFaster()\n        else:\n            self.propagation = Propagation(device, self.propagation_filter_size)\n\n        self.evaluate = Evaluate(left_features, self.propagation_filter_size,\n                                 self.evaluation_type, self.softmax_temperature)\n        self.uniform_sampler = RandomSampler(device, self.number_of_samples)\n\n        min_offset_x = torch.zeros((left_features.size()[0], 1, left_features.size()[2],\n                                    left_features.size()[3])).to(device) - left_features.size()[3]\n        max_offset_x = min_offset_x + 2 * left_features.size()[3]\n        min_offset_y = min_offset_x + left_features.size()[3] - left_features.size()[2]\n        max_offset_y = min_offset_y + 2 * left_features.size()[2]\n\n        for prop_iter in range(self.iteration_count):\n            offset_x, offset_y = self.uniform_sampler(min_offset_x, max_offset_x,\n                                                      min_offset_y, max_offset_y)\n\n            offset_x, offset_y = self.propagation(offset_x, offset_y, device, ""horizontal"")\n            offset_x, offset_y = self.evaluate(left_features,\n                                               right_features,\n                                               offset_x, offset_y)\n\n            offset_x, offset_y = self.propagation(offset_x, offset_y, device, ""vertical"")\n            offset_x, offset_y = self.evaluate(left_features,\n                                               right_features,\n                                               offset_x, offset_y)\n\n            min_offset_x = torch.clamp(offset_x - self.window_size_x // 2, min=-left_features.size()[3],\n                                       max=left_features.size()[3])\n            max_offset_x = torch.clamp(offset_x + self.window_size_x // 2, min=-left_features.size()[3],\n                                       max=left_features.size()[3])\n            min_offset_y = torch.clamp(offset_y - self.window_size_y // 2, min=-left_features.size()[2],\n                                       max=left_features.size()[2])\n            max_offset_y = torch.clamp(offset_y + self.window_size_y // 2, min=-left_features.size()[2],\n                                       max=left_features.size()[2])\n\n        return offset_x, offset_y, self.evaluate.left_x_coordinate, self.evaluate.left_y_coordinate\n'"
deeppruner/dataloader/__init__.py,0,b''
deeppruner/dataloader/kitti_collector.py,1,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch.utils.data as data\nimport os\nimport glob\n\n\ndef datacollector(train_filepath, val_filepath, filepath_2012):\n\n    left_fold = \'image_2/\'\n    right_fold = \'image_3/\'\n    disp_L = \'disp_occ_0/\'\n    disp_R = \'disp_occ_1/\'\n\n    left_fold_2012 = \'colored_0/\'\n    right_fold_2012 = \'colored_1/\'\n    disp_L_2012 = \'disp_occ/\'\n\n    left_train = []\n    right_train = []\n    disp_train_L = []\n    \n    left_val = []\n    right_val = []\n    disp_val_L = []\n\n\n    if train_filepath is not None:\n        left_train = sorted(glob.glob(os.path.join(train_filepath, left_fold, \'*.png\'))) \n        right_train = sorted(glob.glob(os.path.join(train_filepath, right_fold, \'*.png\')))\n        disp_train_L = sorted(glob.glob(os.path.join(train_filepath, disp_L, \'*.png\')))\n\n    if filepath_2012 is not None:\n        left_train +=sorted(glob.glob(os.path.join(filepath_2012, left_fold_2012, \'*_10.png\')))\n        right_train += sorted(glob.glob(os.path.join(filepath_2012, right_fold_2012, \'*_10.png\')))\n        disp_train_L += sorted(glob.glob(os.path.join(filepath_2012, disp_L_2012, \'*_10.png\')))\n\n    if val_filepath is not None:\n        left_val = sorted(glob.glob(os.path.join(val_filepath, left_fold, \'*.png\')))\n        right_val = sorted(glob.glob(os.path.join(val_filepath, right_fold, \'*.png\')))\n        disp_val_L = sorted(glob.glob(os.path.join(val_filepath, disp_L, \'*.png\')))\n\n    return left_train, right_train, disp_train_L, left_val, right_val, disp_val_L\n'"
deeppruner/dataloader/kitti_loader.py,1,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch.utils.data as data\nimport random\nfrom PIL import Image\nimport numpy as np\nfrom dataloader import preprocess\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\ndef disparity_loader(path):\n    return Image.open(path)\n\n# train/ validation image crop size constants\nDEFAULT_TRAIN_IMAGE_HEIGHT = 256\nDEFAULT_TRAIN_IMAGE_WIDTH = 512\n\nDEFAULT_VAL_IMAGE_HEIGHT = 320\nDEFAULT_VAL_IMAGE_WIDTH = 1216\n\n\nclass KITTILoader(data.Dataset):\n    def __init__(self, left_images, right_images, left_disparity, training, loader=default_loader, dploader=disparity_loader):\n\n        self.left_img = left_images\n        self.right_img = right_images\n        self.left_disp = left_disparity\n        self.loader = loader\n        self.dploader = dploader\n        self.training = training\n\n    def __getitem__(self, index):\n        left_img = self.left_img[index]\n        right_img = self.right_img[index]\n        left_disp = self.left_disp[index]\n\n        left_img = self.loader(left_img)\n        right_img = self.loader(right_img)\n        left_disp = self.dploader(left_disp)\n        w, h = left_img.size\n\n\n        if self.training:\n            th, tw = DEFAULT_TRAIN_IMAGE_HEIGHT, DEFAULT_TRAIN_IMAGE_WIDTH\n            x1 = random.randint(0, w - tw)\n            y1 = random.randint(0, h - th)\n\n        else:\n            th, tw = DEFAULT_VAL_IMAGE_HEIGHT, DEFAULT_VAL_IMAGE_WIDTH\n            x1 = w - DEFAULT_VAL_IMAGE_WIDTH\n            y1 = h - DEFAULT_VAL_IMAGE_HEIGHT\n\n        left_img = left_img.crop((x1, y1, x1 + tw, y1 + th))\n        right_img = right_img.crop((x1, y1, x1 + tw, y1 + th))\n        left_disp = left_disp.crop((x1, y1, x1 + tw, y1 + th)) \n        left_disp = np.ascontiguousarray(left_disp, dtype=np.float32) / 256\n   \n\n        processed = preprocess.get_transform()\n        left_img = processed(left_img)\n        right_img = processed(right_img)\n\n        return left_img, right_img, left_disp\n\n    def __len__(self):\n        return len(self.left_img)\n'"
deeppruner/dataloader/kitti_submission_collector.py,0,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport os\n\n\ndef datacollector(filepath):\n    left_fold = \'image_2/\'\n    right_fold = \'image_3/\'\n    disp = \'disp_occ_0/\'\n\n    image = [img for img in sorted(os.listdir(os.path.join(filepath,left_fold))) if img.find(\'.png\') > -1]\n\n    left_test = [os.path.join(filepath, left_fold, img) for img in image]\n    right_test = [os.path.join(filepath, right_fold, img) for img in image]\n\n    return left_test, right_test\n'"
deeppruner/dataloader/preprocess.py,0,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torchvision.transforms as transforms\n\n__imagenet_stats = {\'mean\': [0.485, 0.456, 0.406],\n                    \'std\': [0.229, 0.224, 0.225]}\n\n\ndef get_transform():\n\n    normalize = __imagenet_stats\n    t_list = [\n        transforms.ToTensor(),\n        transforms.Normalize(**normalize),\n    ]\n\n    return transforms.Compose(t_list)\n'"
deeppruner/dataloader/readpfm.py,0,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport re\nimport numpy as np\nimport sys\n\n\ndef readPFM(file):\n    file = open(file, \'rb\')\n\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n\n    header = file.readline().rstrip()\n    if header == \'PF\':\n        color = True\n    elif header == \'Pf\':\n        color = False\n    else:\n        raise Exception(\'Not a PFM file.\')\n\n    dim_match = re.match(r\'^(\\d+)\\s(\\d+)\\s$\', file.readline())\n    if dim_match:\n        width, height = map(int, dim_match.groups())\n    else:\n        raise Exception(\'Malformed PFM header.\')\n\n    scale = float(file.readline().rstrip())\n    if scale < 0:  # little-endian\n        endian = \'<\'\n        scale = -scale\n    else:\n        endian = \'>\'  # big-endian\n\n    data = np.fromfile(file, endian + \'f\')\n    shape = (height, width, 3) if color else (height, width)\n\n    data = np.reshape(data, shape)\n    data = np.flipud(data)\n    return data, scale\n'"
deeppruner/dataloader/sceneflow_collector.py,1,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\nimport os.path\nimport logging\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef dataloader(filepath_monkaa, filepath_flying, filepath_driving):\n\n    try:\n        monkaa_path = os.path.join(filepath_monkaa, \'monkaa_frames_cleanpass\')\n        monkaa_disp = os.path.join(filepath_monkaa, \'monkaa_disparity\')\n        monkaa_dir = os.listdir(monkaa_path)\n\n        all_left_img = []\n        all_right_img = []\n        all_left_disp = []\n        test_left_img = []\n        test_right_img = []\n        test_left_disp = []\n\n        for dd in monkaa_dir:\n            for im in os.listdir(os.path.join(monkaa_path, dd, \'left\')):\n                if is_image_file(os.path.join(monkaa_path, dd, \'left\', im)) and is_image_file(\n                        os.path.join(monkaa_path, dd, \'right\', im)):\n                    all_left_img.append(os.path.join(monkaa_path, dd, \'left\', im))\n                    all_left_disp.append(os.path.join(monkaa_disp, dd, \'left\', im.split(""."")[0] + \'.pfm\'))\n                    all_right_img.append(os.path.join(monkaa_path, dd, \'right\', im))\n\n    except:\n        logging.error(""Some error in Monkaa, Monkaa might not be loaded correctly in this case..."")\n        raise Exception(\'Monkaa dataset couldn\\\'t be loaded correctly.\')\n\n    \n    try:\n        flying_path = os.path.join(filepath_flying, \'frames_cleanpass\')\n        flying_disp = os.path.join(filepath_flying, \'disparity\')\n        flying_dir = flying_path + \'/TRAIN/\'\n        subdir = [\'A\', \'B\', \'C\']\n\n        for ss in subdir:\n            flying = os.listdir(os.path.join(flying_dir, ss))\n\n            for ff in flying:\n                imm_l = os.listdir(os.path.join(flying_dir, ss, ff, \'left\'))\n                for im in imm_l:\n                    if is_image_file(os.path.join(flying_dir, ss, ff, \'left\', im)):\n                        all_left_img.append(os.path.join(flying_dir, ss, ff, \'left\', im))\n\n                    all_left_disp.append(os.path.join(flying_disp, \'TRAIN\', ss, ff, \'left\', im.split(""."")[0] + \'.pfm\'))\n\n                    if is_image_file(os.path.join(flying_dir, ss, ff, \'right\', im)):\n                        all_right_img.append(os.path.join(flying_dir, ss, ff, \'right\', im))\n\n        flying_dir = flying_path + \'/TEST/\'\n        subdir = [\'A\', \'B\', \'C\']\n\n        for ss in subdir:\n            flying = os.listdir(os.path.join(flying_dir, ss))\n\n            for ff in flying:\n                imm_l = os.listdir(os.path.join(flying_dir, ss, ff, \'left\'))\n                for im in imm_l:\n                    if is_image_file(os.path.join(flying_dir, ss, ff, \'left\', im)):\n                        test_left_img.append(os.path.join(flying_dir, ss, ff, \'left\', im))\n\n                    test_left_disp.append(os.path.join(flying_disp, \'TEST\', ss, ff, \'left\', im.split(""."")[0] + \'.pfm\'))\n\n                    if is_image_file(os.path.join(flying_dir, ss, ff, \'right\', im)):\n                        test_right_img.append(os.path.join(flying_dir, ss, ff, \'right\', im))\n    \n    except:\n        logging.error(""Some error in Flying Things, Flying Things might not be loaded correctly in this case..."")\n        raise Exception(\'Flying Things dataset couldn\\\'t be loaded correctly.\')\n\n    try:\n        driving_dir = os.path.join(filepath_driving, \'driving_frames_cleanpass/\')\n        driving_disp = os.path.join(filepath_driving, \'driving_disparity/\')\n\n        subdir1 = [\'35mm_focallength\', \'15mm_focallength\']\n        subdir2 = [\'scene_backwards\', \'scene_forwards\']\n        subdir3 = [\'fast\', \'slow\']\n\n        for i in subdir1:\n            for j in subdir2:\n                for k in subdir3:\n                    imm_l = os.listdir(os.path.join(driving_dir, i, j, k, \'left\'))\n                    for im in imm_l:\n                        if is_image_file(os.path.join(driving_dir, i, j, k, \'left\', im)):\n                            all_left_img.append(os.path.join(driving_dir, i, j, k, \'left\', im))\n                        all_left_disp.append(os.path.join(driving_disp, i, j, k, \'left\', im.split(""."")[0] + \'.pfm\'))\n\n                        if is_image_file(os.path.join(driving_dir, i, j, k, \'right\', im)):\n                            all_right_img.append(os.path.join(driving_dir, i, j, k, \'right\', im))\n    except:\n        logging.error(""Some error in Driving, Driving might not be loaded correctly in this case..."")\n        raise Exception(\'Driving dataset couldn\\\'t be loaded correctly.\')\n\n    return all_left_img, all_right_img, all_left_disp, test_left_img, test_right_img, test_left_disp\n'"
deeppruner/dataloader/sceneflow_loader.py,1,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch.utils.data as data\nimport random\nfrom PIL import Image\nfrom dataloader import preprocess\nfrom dataloader import readpfm as rp\nimport numpy as np\nimport math\n\n# train/ validation image crop size constants\nDEFAULT_TRAIN_IMAGE_HEIGHT = 256\nDEFAULT_TRAIN_IMAGE_WIDTH = 512\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\ndef disparity_loader(path):\n    return rp.readPFM(path)\n\n\nclass SceneflowLoader(data.Dataset):\n    def __init__(self, left_images, right_images, left_disparity, downsample_scale, training, loader=default_loader, dploader=disparity_loader):\n\n        self.left_images = left_images\n        self.right_images = right_images\n        self.left_disparity = left_disparity\n        self.loader = loader\n        self.dploader = dploader\n        self.training = training\n\n        # downsample_scale denotes maximum times the image features are downsampled \n        # by the network.\n        # Since the image size used for evaluation may not be divisible by the downsample_scale,\n        # we pad it with zeros, so that it becomes divible and later unpad the extra zeros.\n        self.downsample_scale = downsample_scale\n\n    def __getitem__(self, index):\n        left_img = self.left_images[index]\n        right_img = self.right_images[index]\n        left_disp = self.left_disparity[index]\n\n        left_img = self.loader(left_img)\n        right_img = self.loader(right_img)\n        left_disp, left_scale = self.dploader(left_disp)\n        left_disp = np.ascontiguousarray(left_disp, dtype=np.float32)\n\n        if self.training:\n            w, h = left_img.size\n            th, tw = DEFAULT_TRAIN_IMAGE_HEIGHT, DEFAULT_TRAIN_IMAGE_WIDTH\n\n            x1 = random.randint(0, w - tw)\n            y1 = random.randint(0, h - th)\n\n            left_img = left_img.crop((x1, y1, x1 + tw, y1 + th))\n            right_img = right_img.crop((x1, y1, x1 + tw, y1 + th))\n            left_disp = left_disp[y1:y1 + th, x1:x1 + tw]\n\n            processed = preprocess.get_transform()\n            left_img = processed(left_img)\n            right_img = processed(right_img)\n\n            return left_img, right_img, left_disp\n        else:\n            w, h = left_img.size\n\n            dw = w + (self.downsample_scale - (w%self.downsample_scale + (w%self.downsample_scale==0)*self.downsample_scale))\n            dh = h + (self.downsample_scale - (h%self.downsample_scale + (h%self.downsample_scale==0)*self.downsample_scale))\n\n            left_img = left_img.crop((w - dw, h - dh, w, h))\n            right_img = right_img.crop((w - dw, h - dh, w, h))\n\n            processed = preprocess.get_transform()\n            left_img = processed(left_img)\n            right_img = processed(right_img)\n\n            return left_img, right_img, left_disp, dw-w, dh-h\n\n    def __len__(self):\n        return len(self.left_images)\n'"
deeppruner/models/__init__.py,0,b''
deeppruner/models/config.py,0,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\n\nfrom __future__ import print_function\n\nclass obj(object):\n    def __init__(self, d):\n        for key, value in d.items():\n            if isinstance(value, (list, tuple)):\n               setattr(self, key, [obj(x) if isinstance(x, dict) else x for x in value])\n            else:\n               setattr(self, key, obj(value) if isinstance(value, dict) else value)\n\n\nconfig = {\n    ""max_disp"": 192,\n    ""cost_aggregator_scale"": 4, # for DeepPruner-fast change this to 8.\n    ""mode"": ""training"", # for evaluation/ submission, change this to evaluation.\n\n    \n    # The code allows the user to change the feature extrcator to any feature extractor of their choice.\n    # The only requirements of the feature extractor are:\n    #     1.  For cost_aggregator_scale == 4:\n    #             features at downsample-level X4 (feature_extractor_ca_level) \n    #             and downsample-level X2 (feature_extractor_refinement_level) should be the output.\n    #         For cost_aggregator_scale == 8:\n    #             features at downsample-level X8 (feature_extractor_ca_level),\n    #             downsample-level X4 (feature_extractor_refinement_level),\n    #             downsample-level X2 (feature_extractor_refinement_level_1) should be the output, \n        \n    #     2.  If the feature extractor is modified, change the ""feature_extractor_outplanes_*"" key in the config\n    #         accordingly.\n\n    ""feature_extractor_ca_level_outplanes"": 32,\n    ""feature_extractor_refinement_level_outplanes"": 32, # for DeepPruner-fast change this to 64.\n    ""feature_extractor_refinement_level_1_outplanes"": 32,\n\n    ""patch_match_args"": {\n        ""sample_count"": 12,\n        ""iteration_count"": 2,\n        ""propagation_filter_size"": 3\n    },\n\n    ""post_CRP_sample_count"": 7,\n    ""post_CRP_sampler_type"": ""uniform"", #change to patch_match for Sceneflow model. \n\n    ""hourglass_inplanes"": 16\n}\n\nconfig = obj(config)\n'"
deeppruner/models/deeppruner.py,13,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nfrom models.submodules3d import MinDisparityPredictor, MaxDisparityPredictor, CostAggregator\nfrom models.submodules2d import RefinementNet\nfrom models.submodules import SubModule, conv_relu, convbn_2d_lrelu, convbn_3d_lrelu\nfrom models.utils import SpatialTransformer, UniformSampler\nfrom models.patch_match import PatchMatch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom models.config import config as args\n\nclass DeepPruner(SubModule):\n    def __init__(self):\n        super(DeepPruner, self).__init__()\n        self.scale = args.cost_aggregator_scale\n        self.max_disp = args.max_disp // self.scale\n        self.mode = args.mode\n\n        self.patch_match_args = args.patch_match_args\n        self.patch_match_sample_count = self.patch_match_args.sample_count\n        self.patch_match_iteration_count = self.patch_match_args.iteration_count\n        self.patch_match_propagation_filter_size = self.patch_match_args.propagation_filter_size\n        \n        self.post_CRP_sample_count = args.post_CRP_sample_count\n        self.post_CRP_sampler_type = args.post_CRP_sampler_type\n        hourglass_inplanes = args.hourglass_inplanes\n\n        #   refinement input features are composed of:\n        #                                       left image low level features +\n        #                                       CA output features + CA output disparity\n\n        if self.scale == 8:\n            from models.feature_extractor_fast import feature_extraction\n            refinement_inplanes_1 = args.feature_extractor_refinement_level_1_outplanes + 1\n            self.refinement_net1 = RefinementNet(refinement_inplanes_1)\n        else:\n            from models.feature_extractor_best import feature_extraction\n\n        refinement_inplanes = args.feature_extractor_refinement_level_outplanes + self.post_CRP_sample_count + 2 + 1\n        self.refinement_net = RefinementNet(refinement_inplanes)\n\n        # cost_aggregator_inplanes are composed of:  \n        #                            left and right image features from feature_extractor (ca_level) + \n        #                            features from min/max predictors + \n        #                            min_disparity + max_disparity + disparity_samples\n\n        cost_aggregator_inplanes = 2 * (args.feature_extractor_ca_level_outplanes +\n                                        self.patch_match_sample_count + 2) + 1\n        self.cost_aggregator = CostAggregator(cost_aggregator_inplanes, hourglass_inplanes)\n\n        self.feature_extraction = feature_extraction()\n        self.min_disparity_predictor = MinDisparityPredictor(hourglass_inplanes)\n        self.max_disparity_predictor = MaxDisparityPredictor(hourglass_inplanes)\n        self.spatial_transformer = SpatialTransformer()\n        self.patch_match = PatchMatch(self.patch_match_propagation_filter_size)\n        self.uniform_sampler = UniformSampler()\n\n        # Confidence Range Predictor(CRP) input features are composed of:  \n        #                            left and right image features from feature_extractor (ca_level) + \n        #                            disparity_samples\n\n        CRP_feature_count = 2 * args.feature_extractor_ca_level_outplanes + 1\n        self.dres0 = nn.Sequential(convbn_3d_lrelu(CRP_feature_count, 64, 3, 1, 1),\n                                   convbn_3d_lrelu(64, 32, 3, 1, 1))\n\n        self.dres1 = nn.Sequential(convbn_3d_lrelu(32, 32, 3, 1, 1),\n                                   convbn_3d_lrelu(32, hourglass_inplanes, 3, 1, 1))\n\n        self.min_disparity_conv = conv_relu(1, 1, 5, 1, 2)\n        self.max_disparity_conv = conv_relu(1, 1, 5, 1, 2)\n        self.ca_disparity_conv = conv_relu(1, 1, 5, 1, 2)\n\n        self.ca_features_conv = convbn_2d_lrelu(self.post_CRP_sample_count + 2,\n                                          self. post_CRP_sample_count + 2, 5, 1, 2, dilation=1, bias=True)\n        self.min_disparity_features_conv = convbn_2d_lrelu(self.patch_match_sample_count + 2,\n                                                     self.patch_match_sample_count + 2, 5, 1, 2, dilation=1, bias=True)\n        self.max_disparity_features_conv = convbn_2d_lrelu(self.patch_match_sample_count + 2,\n                                                     self.patch_match_sample_count + 2, 5, 1, 2, dilation=1, bias=True)\n\n        self.weight_init()\n\n\n    def generate_search_range(self, left_input, sample_count, stage,\n                              input_min_disparity=None, input_max_disparity=None):\n        """"""\n        Description:    Generates the disparity search range depending upon the stage it is called.\n                    If stage is ""pre"" (Pre-PatchMatch and Pre-ConfidenceRangePredictor), the search range is\n                    the entire disparity search range.\n                    If stage is ""post"" (Post-ConfidenceRangePredictor), then the ConfidenceRangePredictor search range\n                    is adjusted for maximum efficiency.\n        Args:\n            :left_input: Left Image Features\n            :sample_count: number of samples to be generated from the search range. Used to adjust the search range.\n            :stage: ""pre""(Pre-PatchMatch) or ""post""(Post-ConfidenceRangePredictor)\n            :input_min_disparity (default:None): ConfidenceRangePredictor disparity lowerbound (for stage==""post"")\n            :input_max_disparity (default:None): ConfidenceRangePredictor disparity upperbound (for stage==""post"")\n\n        Returns:\n            :min_disparity: Lower bound of disparity search range\n            :max_disparity: Upper bound of disaprity search range.\n        """"""\n\n        device = left_input.get_device()\n        if stage is ""pre"":\n            min_disparity = torch.zeros((left_input.size()[0], 1, left_input.size()[2], left_input.size()[3]),\n                                        device=device)\n            max_disparity = torch.zeros((left_input.size()[0], 1, left_input.size()[2], left_input.size()[3]),\n                                        device=device) + self.max_disp\n\n        else:\n            min_disparity1 = torch.min(input_min_disparity, input_max_disparity)\n            max_disparity1 = torch.max(input_min_disparity, input_max_disparity)\n\n            # if (max_disparity1 - min_disparity1) > sample_count:\n            #     sample uniformly ""sample_count"" number of samples from (min_disparity1, max_disparity1)\n            # else:\n            #     stretch min_disparity1 and max_disparity1 such that (max_disparity1 - min_disparity1) == sample_count\n\n            min_disparity = torch.clamp(min_disparity1 - torch.clamp((\n                sample_count - max_disparity1 + min_disparity1), min=0) / 2.0, min=0, max=self.max_disp)\n            max_disparity = torch.clamp(max_disparity1 + torch.clamp(\n                sample_count - max_disparity1 + min_disparity, min=0), min=0, max=self.max_disp)\n\n        return min_disparity, max_disparity\n\n    def generate_disparity_samples(self, left_input, right_input, min_disparity,\n                                   max_disparity, sample_count=12, sampler_type=""patch_match""):\n        """"""\n        Description:    Generates ""sample_count"" number of disparity samples from the\n                                                            search range (min_disparity, max_disparity)\n                        Samples are generated either uniformly from the search range\n                                                            or are generated using PatchMatch.\n\n        Args:\n            :left_input: Left Image features.\n            :right_input: Right Image features.\n            :min_disparity: LowerBound of the disaprity search range.\n            :max_disparity: UpperBound of the disparity search range.\n            :sample_count (default:12): Number of samples to be generated from the input search range.\n            :sampler_type (default:""patch_match""): samples are generated either using\n                                                                    ""patch_match"" or ""uniform"" sampler.\n        Returns:\n            :disparity_samples:\n        """"""\n        if sampler_type is ""patch_match"":\n            disparity_samples = self.patch_match(left_input, right_input, min_disparity,\n                                                 max_disparity, sample_count, self.patch_match_iteration_count)\n        else:\n            disparity_samples = self.uniform_sampler(min_disparity, max_disparity, sample_count)\n\n        disparity_samples = torch.cat((torch.floor(min_disparity), disparity_samples, torch.ceil(max_disparity)),\n                                      dim=1).long()\n        return disparity_samples\n\n    def cost_volume_generator(self, left_input, right_input, disparity_samples):\n        """"""\n        Description: Generates cost-volume using left image features, disaprity samples\n                                                            and warped right image features.\n        Args:\n            :left_input: Left Image fetaures\n            :right_input: Right Image features\n            :disparity_samples: Disaprity samples\n\n        Returns:\n            :cost_volume:\n            :disaprity_samples:\n            :left_feature_map:\n        """"""\n\n        right_feature_map, left_feature_map = self.spatial_transformer(left_input,\n                                                                       right_input, disparity_samples)\n        disparity_samples = disparity_samples.unsqueeze(1).float()\n\n        cost_volume = torch.cat((left_feature_map, right_feature_map, disparity_samples), dim=1)\n\n        return cost_volume, disparity_samples, left_feature_map\n\n    def confidence_range_predictor(self, cost_volume, disparity_samples):\n        """"""\n        Description:    The original search space for all pixels is identical. However, in practice, for each\n                        pixel, the highly probable disparities lie in a narrow region. Using the small subset\n                        of disparities estimated from the PatchMatch stage, we have sufficient information to\n                        predict the range in which the true disparity lies. We thus exploit a confidence range\n                        prediction network to adjust the search space for each pixel.\n\n        Args:\n            :cost_volume: Input Cost-Volume\n            :disparity_samples: Initial Disparity samples.\n\n        Returns:\n            :min_disparity: ConfidenceRangePredictor disparity lowerbound\n            :max_disparity: ConfidenceRangePredictor disparity upperbound\n            :min_disparity_features: features from ConfidenceRangePredictor-Min\n            :max_disparity_features: features from ConfidenceRangePredictor-Max\n        """"""\n        # cost-volume bottleneck layers\n        cost_volume = self.dres0(cost_volume)\n        cost_volume = self.dres1(cost_volume)\n\n        min_disparity, min_disparity_features = self.min_disparity_predictor(cost_volume,\n                                                                             disparity_samples.squeeze(1))\n\n        max_disparity, max_disparity_features = self.max_disparity_predictor(cost_volume,\n                                                                             disparity_samples.squeeze(1))\n\n        min_disparity = self.min_disparity_conv(min_disparity)\n        max_disparity = self.max_disparity_conv(max_disparity)\n        min_disparity_features = self.min_disparity_features_conv(min_disparity_features)\n        max_disparity_features = self.max_disparity_features_conv(max_disparity_features)\n\n        return min_disparity, max_disparity, min_disparity_features, max_disparity_features\n\n    def forward(self, left_input, right_input):\n        """"""\n        DeepPruner\n        Description: DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n\n        Args:\n            :left_input: Left Stereo Image\n            :right_input: Right Stereo Image\n        Returns:\n            outputs depend of args.mode (""evaluation or ""training""), and\n            also on args.cost_aggregator_scale (8 or 4)\n\n            All possible outputs can be:\n            :refined_disparity_1: DeepPruner disparity output after Refinement1 stage.\n                                                                s (only when args.cost_aggregator_scale==8)\n            :refined_disparity: DeepPruner disparity output after Refinement stage.\n            :ca_disparity: DeepPruner disparity output after 3D-Cost Aggregation stage.\n            :max_disparity: DeepPruner disparity by Confidence Range Predictor (Max)\n            :min_disparity: DeepPruner disparity by Confidence Range Predictor (Min)\n\n        """"""\n\n        if self.scale == 8:\n            left_spp_features, left_low_level_features, left_low_level_features_1 = self.feature_extraction(left_input)\n            right_spp_features, right_low_level_features, _ = self.feature_extraction(\n                right_input)\n        else:\n            left_spp_features, left_low_level_features = self.feature_extraction(left_input)\n            right_spp_features, right_low_level_features = self.feature_extraction(right_input)\n\n        min_disparity, max_disparity = self.generate_search_range(\n            left_spp_features,\n            sample_count=self.patch_match_sample_count, stage=""pre"")\n\n        disparity_samples = self.generate_disparity_samples(\n            left_spp_features,\n            right_spp_features, min_disparity, max_disparity,\n            sample_count=self.patch_match_sample_count, sampler_type=""patch_match"")\n\n        cost_volume, disparity_samples, _ = self.cost_volume_generator(left_spp_features,\n                                                                       right_spp_features,\n                                                                       disparity_samples)\n\n        min_disparity, max_disparity, min_disparity_features, max_disparity_features = \\\n            self.confidence_range_predictor(cost_volume, disparity_samples)\n\n        stretched_min_disparity, stretched_max_disparity = self.generate_search_range(\n            left_spp_features,\n            sample_count=self.post_CRP_sample_count, stage=\'post\',\n            input_min_disparity=min_disparity, input_max_disparity=max_disparity)\n\n        disparity_samples = self.generate_disparity_samples(\n            left_spp_features,\n            right_spp_features, stretched_min_disparity, stretched_max_disparity,\n            sample_count=self.post_CRP_sample_count, sampler_type=self.post_CRP_sampler_type)\n\n        cost_volume, disparity_samples, expanded_left_feature_map = self.cost_volume_generator(\n            left_spp_features,\n            right_spp_features,\n            disparity_samples)\n\n        min_disparity_features = min_disparity_features.unsqueeze(2).expand(-1, -1,\n                                                                            expanded_left_feature_map.size()[2], -1, -1)\n        max_disparity_features = max_disparity_features.unsqueeze(2).expand(-1, -1,\n                                                                            expanded_left_feature_map.size()[2], -1, -1)\n\n        cost_volume = torch.cat((cost_volume, min_disparity_features, max_disparity_features), dim=1)\n        ca_disparity, ca_features = self.cost_aggregator(cost_volume, disparity_samples.squeeze(1))\n\n        ca_disparity = F.interpolate(ca_disparity * 2, scale_factor=(2, 2), mode=\'bilinear\')\n        ca_features = F.interpolate(ca_features, scale_factor=(2, 2), mode=\'bilinear\')\n        ca_disparity = self.ca_disparity_conv(ca_disparity)\n        ca_features = self.ca_features_conv(ca_features)\n\n        refinement_net_input = torch.cat((left_low_level_features, ca_features, ca_disparity), dim=1)\n        refined_disparity = self.refinement_net(refinement_net_input, ca_disparity)\n\n        refined_disparity = F.interpolate(refined_disparity * 2, scale_factor=(2, 2), mode=\'bilinear\')\n\n        if self.scale == 8:\n            refinement_net_input = torch.cat((left_low_level_features_1, refined_disparity), dim=1)\n            refined_disparity_1 = self.refinement_net1(refinement_net_input, refined_disparity)\n\n        if self.mode == \'evaluation\':\n            if self.scale == 8:\n                refined_disparity_1 = F.interpolate(refined_disparity_1 * 2, scale_factor=(2, 2),\n                                                    mode=\'bilinear\').squeeze(1)\n                return refined_disparity_1\n            return refined_disparity.squeeze(1)\n\n        min_disparity = F.interpolate(min_disparity * self.scale, scale_factor=(self.scale, self.scale),\n                                      mode=\'bilinear\').squeeze(1)\n        max_disparity = F.interpolate(max_disparity * self.scale, scale_factor=(self.scale, self.scale),\n                                      mode=\'bilinear\').squeeze(1)\n        ca_disparity = F.interpolate(ca_disparity * (self.scale // 2),\n                                     scale_factor=((self.scale // 2), (self.scale // 2)), mode=\'bilinear\').squeeze(1)\n\n        if self.scale == 8:\n            refined_disparity = F.interpolate(refined_disparity * 2, scale_factor=(2, 2), mode=\'bilinear\').squeeze(1)\n            refined_disparity_1 = F.interpolate(refined_disparity_1 * 2,\n                                                scale_factor=(2, 2), mode=\'bilinear\').squeeze(1)\n\n            return refined_disparity_1, refined_disparity, ca_disparity, max_disparity, min_disparity\n\n        return refined_disparity.squeeze(1), ca_disparity, max_disparity, min_disparity\n'"
deeppruner/models/feature_extractor_best.py,4,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom models.submodules import BasicBlock, convbn_relu\n\n\nclass feature_extraction(nn.Module):\n    def __init__(self):\n        super(feature_extraction, self).__init__()\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn_relu(3, 32, 3, 2, 1, 1),\n                                       convbn_relu(32, 32, 3, 1, 1, 1),\n                                       convbn_relu(32, 32, 3, 1, 1, 1))\n\n        self.layer1 = self._make_layer(BasicBlock, 32, 3, 1, 1, 1)\n        self.layer2 = self._make_layer(BasicBlock, 64, 16, 2, 1, 1)\n        self.layer3 = self._make_layer(BasicBlock, 128, 3, 1, 1, 1)\n        self.layer4 = self._make_layer(BasicBlock, 128, 3, 1, 1, 2)\n\n        self.branch1 = nn.Sequential(nn.AvgPool2d((64, 64), stride=(64, 64)),\n                                     convbn_relu(128, 32, 1, 1, 0, 1))\n\n        self.branch2 = nn.Sequential(nn.AvgPool2d((32, 32), stride=(32, 32)),\n                                     convbn_relu(128, 32, 1, 1, 0, 1))\n\n        self.branch3 = nn.Sequential(nn.AvgPool2d((16, 16), stride=(16, 16)),\n                                     convbn_relu(128, 32, 1, 1, 0, 1))\n\n        self.branch4 = nn.Sequential(nn.AvgPool2d((8, 8), stride=(8, 8)),\n                                     convbn_relu(128, 32, 1, 1, 0, 1))\n\n        self.lastconv = nn.Sequential(convbn_relu(320, 128, 3, 1, 1, 1),\n                                      nn.Conv2d(128, 32, kernel_size=1, padding=0, stride=1, bias=False))\n\n    def _make_layer(self, block, planes, blocks, stride, pad, dilation):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),)\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, pad, dilation))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, 1, None, pad, dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, input):\n        """"""\n        Feature Extractor\n        Description:    The goal of the feature extraction network is to produce a reliable pixel-wise\n                        feature representation from the input image. Specifically, we employ four residual blocks\n                        and use X2 dilated convolution for the last block to enlarge the receptive field.\n                        We then apply spatial pyramid pooling to build a 4-level pyramid feature.\n                        Through multi-scale information, the model is able to capture large context while\n                        maintaining a high spatial resolution. The size of the final feature map is 1/4 of\n                        the originalinput image size. We share the parameters for the left and right feature network.\n\n        Args:\n            :input: Input image (RGB)\n\n        Returns:\n            :output_feature: spp_features (downsampled X4)\n            :output1: low_level_features (downsampled X2)\n        """"""\n\n        output0 = self.firstconv(input)\n        output1 = self.layer1(output0)\n        output_raw = self.layer2(output1)\n        output = self.layer3(output_raw)\n        output_skip = self.layer4(output)\n\n        output_branch1 = self.branch1(output_skip)\n        output_branch1 = F.upsample(output_branch1, (output_skip.size()[2], output_skip.size()[3]), mode=\'bilinear\')\n\n        output_branch2 = self.branch2(output_skip)\n        output_branch2 = F.upsample(output_branch2, (output_skip.size()[2], output_skip.size()[3]), mode=\'bilinear\')\n\n        output_branch3 = self.branch3(output_skip)\n        output_branch3 = F.upsample(output_branch3, (output_skip.size()[2], output_skip.size()[3]), mode=\'bilinear\')\n\n        output_branch4 = self.branch4(output_skip)\n        output_branch4 = F.upsample(output_branch4, (output_skip.size()[2], output_skip.size()[3]), mode=\'bilinear\')\n\n        output_feature = torch.cat(\n            (output_raw,\n             output_skip,\n             output_branch4,\n             output_branch3,\n             output_branch2,\n             output_branch1),\n            1)\n        output_feature = self.lastconv(output_feature)\n\n        return output_feature, output1\n'"
deeppruner/models/feature_extractor_fast.py,4,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom models.submodules import BasicBlock, convbn_relu\n\n\nclass feature_extraction(nn.Module):\n    def __init__(self):\n        super(feature_extraction, self).__init__()\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn_relu(3, 32, 3, 2, 1, 1),\n                                       convbn_relu(32, 32, 3, 1, 1, 1),\n                                       convbn_relu(32, 32, 3, 1, 1, 1))\n\n        self.layer1 = self._make_layer(BasicBlock, 32, 3, 1, 1, 1)\n        self.layer2 = self._make_layer(BasicBlock, 64, 16, 2, 1, 1)\n        self.layer3 = self._make_layer(BasicBlock, 128, 3, 2, 1, 1)\n        self.layer4 = self._make_layer(BasicBlock, 128, 3, 1, 1, 1)\n\n        self.branch2 = nn.Sequential(nn.AvgPool2d((32, 32), stride=(32, 32)),\n                                     convbn_relu(128, 32, 1, 1, 0, 1))\n\n        self.branch3 = nn.Sequential(nn.AvgPool2d((16, 16), stride=(16, 16)),\n                                     convbn_relu(128, 32, 1, 1, 0, 1))\n\n        self.branch4 = nn.Sequential(nn.AvgPool2d((8, 8), stride=(8, 8)),\n                                     convbn_relu(128, 32, 1, 1, 0, 1))\n\n        self.lastconv = nn.Sequential(convbn_relu(352, 128, 3, 1, 1, 1),\n                                      nn.Conv2d(128, 32, kernel_size=1, padding=0, stride=1, bias=False))\n\n    def _make_layer(self, block, planes, blocks, stride, pad, dilation):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),)\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, pad, dilation))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, 1, None, pad, dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, input):\n        """"""\n        Feature Extractor\n        Description:    The goal of the feature extraction network is to produce a reliable pixel-wise\n                        feature representation from the input image. Specifically, we employ four residual blocks\n                        and use X2 dilated convolution for the last block to enlarge the receptive field.\n                        We then apply spatial pyramid pooling to build a 4-level pyramid feature.\n                        Through multi-scale information, the model is able to capture large context while\n                        maintaining a high spatial resolution. The size of the final feature map is 1/4 of\n                        the originalinput image size. We share the parameters for the left and right feature network.\n\n        Args:\n            :input: Input image (RGB)\n\n        Returns:\n            :output_feature: spp_features (downsampled X8)\n            :output_raw: features (downsampled X4)\n            :output1: low_level_features (downsampled X2)\n        """"""\n\n        output0 = self.firstconv(input)\n        output1 = self.layer1(output0)\n        output_raw = self.layer2(output1)\n        output = self.layer3(output_raw)\n        output_skip = self.layer4(output)\n\n        output_branch2 = self.branch2(output_skip)\n        output_branch2 = F.upsample(output_branch2, (output_skip.size()[2], output_skip.size()[3]), mode=\'bilinear\')\n\n        output_branch3 = self.branch3(output_skip)\n        output_branch3 = F.upsample(output_branch3, (output_skip.size()[2], output_skip.size()[3]), mode=\'bilinear\')\n\n        output_branch4 = self.branch4(output_skip)\n        output_branch4 = F.upsample(output_branch4, (output_skip.size()[2], output_skip.size()[3]), mode=\'bilinear\')\n\n        output_feature = torch.cat((output, output_skip, output_branch4, output_branch3, output_branch2), 1)\n        output_feature = self.lastconv(output_feature)\n\n        return output_feature, output_raw, output1\n'"
deeppruner/models/patch_match.py,18,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DisparityInitialization(nn.Module):\n\n    def __init__(self):\n        super(DisparityInitialization, self).__init__()\n\n    def forward(self, min_disparity, max_disparity, number_of_intervals=10):\n        """"""\n        PatchMatch Initialization Block\n        Description:    Rather than allowing each sample/ particle to reside in the full disparity space,\n                        we divide the search space into \'number_of_intervals\' intervals, and force the\n                        i-th particle to be in a i-th interval. This guarantees the diversity of the\n                        particles and helps improve accuracy for later computations.\n\n                        As per implementation,\n                        this function divides the complete disparity search space into multiple intervals.\n\n        Args:\n            :min_disparity: Min Disparity of the disparity search range.\n            :max_disparity: Max Disparity of the disparity search range.\n            :number_of_intervals (default: 10): Number of samples to be generated.\n        Returns:\n            :interval_noise: Random value between 0-1. Represents offset of the from the interval_min_disparity.\n            :interval_min_disparity: disparity_sample = interval_min_disparity + interval_noise\n            :multiplier: 1.0 / number_of_intervals\n        """"""\n\n        device = min_disparity.get_device()\n\n        multiplier = 1.0 / number_of_intervals\n        range_multiplier = torch.arange(0.0, 1, multiplier, device=device).view(number_of_intervals, 1, 1)\n        range_multiplier = range_multiplier.repeat(1, min_disparity.size()[2], min_disparity.size()[3])\n\n        interval_noise = min_disparity.new_empty(min_disparity.size()[0], number_of_intervals, min_disparity.size()[2],\n                                                 min_disparity.size()[3]).uniform_(0, 1)\n        interval_min_disparity = min_disparity + (max_disparity - min_disparity) * range_multiplier\n\n        return interval_noise, interval_min_disparity, multiplier\n\n\nclass Evaluate(nn.Module):\n    def __init__(self, filter_size=3, temperature=7):\n        super(Evaluate, self).__init__()\n        self.temperature = temperature\n        self.filter_size = filter_size\n        self.softmax = torch.nn.Softmax(dim=1)\n\n    def forward(self, left_input, right_input, disparity_samples, normalized_disparity_samples):\n        """"""\n        PatchMatch Evaluation Block\n        Description:    For each pixel i, matching scores are computed by taking the inner product between the\n            left feature and the right feature: score(i,j) = feature_left(i), feature_right(i+disparity(i,j))\n            for all candidates j. The best k disparity value for each pixel is carried towards the next iteration.\n\n            As per implementation,\n            the complete disparity search range is discretized into intervals in\n            DisparityInitialization() function. Corresponding to each disparity interval, we have multiple samples\n            to evaluate. The best disparity sample per interval is the output of the function.\n\n        Args:\n            :left_input: Left Image Feature Map\n            :right_input: Right Image Feature Map\n            :disparity_samples: Disparity Samples to be evaluated. For each pixel, we have\n                                (""number of intervals"" X ""number_of_samples_per_intervals"") samples.\n\n            :normalized_disparity_samples:\n        Returns:\n            :disparity_samples: Evaluated disparity sample, one per disparity interval.\n            :normalized_disparity_samples: Evaluated normaized disparity sample, one per disparity interval.\n        """"""\n        device = left_input.get_device()\n        left_y_coordinate = torch.arange(0.0, left_input.size()[3], device=device).repeat(\n            left_input.size()[2]).view(left_input.size()[2], left_input.size()[3])\n\n        left_y_coordinate = torch.clamp(left_y_coordinate, min=0, max=left_input.size()[3] - 1)\n        left_y_coordinate = left_y_coordinate.expand(left_input.size()[0], -1, -1)\n\n        right_feature_map = right_input.expand(disparity_samples.size()[1], -1, -1, -1, -1).permute([1, 2, 0, 3, 4])\n        left_feature_map = left_input.expand(disparity_samples.size()[1], -1, -1, -1, -1).permute([1, 2, 0, 3, 4])\n\n        disparity_sample_strength = disparity_samples.new(disparity_samples.size()[0],\n                                                          disparity_samples.size()[1],\n                                                          disparity_samples.size()[2],\n                                                          disparity_samples.size()[3])\n\n        right_y_coordinate = left_y_coordinate.expand(\n            disparity_samples.size()[1], -1, -1, -1).permute([1, 0, 2, 3]).float()\n        right_y_coordinate = right_y_coordinate - disparity_samples\n        right_y_coordinate = torch.clamp(right_y_coordinate, min=0, max=right_input.size()[3] - 1)\n\n        warped_right_feature_map = torch.gather(right_feature_map,\n                                                dim=4,\n                                                index=right_y_coordinate.expand(\n                                                    right_input.size()[1], -1, -1, -1, -1).permute([1, 0, 2, 3, 4]).long())\n\n        disparity_sample_strength = torch.mean(left_feature_map * warped_right_feature_map, dim=1) * self.temperature\n\n        disparity_sample_strength = disparity_sample_strength.view(\n            disparity_sample_strength.size()[0],\n            disparity_sample_strength.size()[1] // (self.filter_size),\n            (self.filter_size),\n            disparity_sample_strength.size()[2],\n            disparity_sample_strength.size()[3])\n\n        disparity_samples = disparity_samples.view(disparity_samples.size()[0],\n                                                   disparity_samples.size()[1] // (self.filter_size),\n                                                   (self.filter_size),\n                                                   disparity_samples.size()[2],\n                                                   disparity_samples.size()[3])\n\n        normalized_disparity_samples = normalized_disparity_samples.view(\n            normalized_disparity_samples.size()[0],\n            normalized_disparity_samples.size()[1] // (self.filter_size),\n            (self.filter_size),\n            normalized_disparity_samples.size()[2],\n            normalized_disparity_samples.size()[3])\n\n        disparity_sample_strength = disparity_sample_strength.permute([0, 2, 1, 3, 4])\n        disparity_samples = disparity_samples.permute([0, 2, 1, 3, 4])\n        normalized_disparity_samples = normalized_disparity_samples.permute([0, 2, 1, 3, 4])\n\n        disparity_sample_strength = torch.softmax(disparity_sample_strength, dim=1)\n        disparity_samples = torch.sum(disparity_samples * disparity_sample_strength, dim=1)\n        normalized_disparity_samples = torch.sum(normalized_disparity_samples * disparity_sample_strength, dim=1)\n\n        return normalized_disparity_samples, disparity_samples\n\n\nclass Propagation(nn.Module):\n    def __init__(self, filter_size=3):\n        super(Propagation, self).__init__()\n        self.filter_size = filter_size\n\n    def forward(self, disparity_samples, device, propagation_type=""horizontal""):\n        """"""\n        PatchMatch Propagation Block\n        Description:    Particles from adjacent pixels are propagated together through convolution with a\n            pre-defined one-hot filter pattern, which en-codes the fact that we allow each pixel\n            to propagate particles to its 4-neighbours.\n\n            As per implementation, the complete disparity search range is discretized into intervals in\n            DisparityInitialization() function.\n            Now, propagation of samples from neighbouring pixels, is done per interval. This implies that after\n            propagation, number of samples per pixel = (filter_size X number_of_intervals)\n\n        Args:\n            :disparity_samples:\n            :device: Cuda device\n            :propagation_type (default:""horizontal""): In order to be memory efficient, we use separable convolutions\n                                                    for propagtaion.\n\n        Returns:\n            :aggregated_disparity_samples: Disparity Samples aggregated from the neighbours.\n\n        """"""\n\n        disparity_samples = disparity_samples.view(disparity_samples.size()[0],\n                                                   1,\n                                                   disparity_samples.size()[1],\n                                                   disparity_samples.size()[2],\n                                                   disparity_samples.size()[3])\n\n        if propagation_type is ""horizontal"":\n            label = torch.arange(0, self.filter_size, device=device).repeat(self.filter_size).view(\n                self.filter_size, 1, 1, 1, self.filter_size)\n\n            one_hot_filter = torch.zeros_like(label).scatter_(0, label, 1).float()\n            aggregated_disparity_samples = F.conv3d(disparity_samples,\n                                                    one_hot_filter, padding=(0, 0, self.filter_size // 2))\n\n        else:\n            label = torch.arange(0, self.filter_size, device=device).repeat(self.filter_size).view(\n                self.filter_size, 1, 1, self.filter_size, 1).long()\n\n            one_hot_filter = torch.zeros_like(label).scatter_(0, label, 1).float()\n            aggregated_disparity_samples = F.conv3d(disparity_samples,\n                                                    one_hot_filter, padding=(0, self.filter_size // 2, 0))\n\n        aggregated_disparity_samples = aggregated_disparity_samples.permute([0, 2, 1, 3, 4])\n        aggregated_disparity_samples = aggregated_disparity_samples.contiguous().view(\n            aggregated_disparity_samples.size()[0],\n            aggregated_disparity_samples.size()[1] * aggregated_disparity_samples.size()[2],\n            aggregated_disparity_samples.size()[3],\n            aggregated_disparity_samples.size()[4])\n\n        return aggregated_disparity_samples\n\n\nclass PatchMatch(nn.Module):\n    def __init__(self, propagation_filter_size=3):\n        super(PatchMatch, self).__init__()\n\n        self.propagation_filter_size = propagation_filter_size\n        self.propagation = Propagation(filter_size=propagation_filter_size)\n        self.disparity_initialization = DisparityInitialization()\n        self.evaluate = Evaluate(filter_size=propagation_filter_size)\n\n    def forward(self, left_input, right_input, min_disparity, max_disparity, sample_count=10, iteration_count=3):\n        """"""\n        Differntail PatchMatch Block\n        Description:    In this work, we unroll generalized PatchMatch as a recurrent neural network,\n                        where each unrolling step is equivalent to each iteration of the algorithm.\n                        This is important as it allow us to train our full model end-to-end.\n                        Specifically, we design the following layers:\n                            - Initialization or Paticle Sampling\n                            - Propagation\n                            - Evaluation\n        Args:\n            :left_input: Left Image feature map\n            :right_input: Right image feature map\n            :min_disparity: Min of the disparity search range\n            :max_disparity: Max of the disparity search range\n            :sample_count (default:10): Number of disparity samples per pixel. (similar to generalized PatchMatch)\n            :iteration_count (default:3) : Number of PatchMatch iterations\n\n        Returns:\n            :disparity_samples: For each pixel, this function returns ""sample_count"" disparity samples.\n        """"""\n\n        device = left_input.get_device()\n        min_disparity = torch.floor(min_disparity)\n        max_disparity = torch.ceil(max_disparity)\n\n        # normalized_disparity_samples: Disparity samples normalized by the corresponding interval size.\n        #                               i.e (disparity_sample - interval_min_disparity) / interval_size\n\n        normalized_disparity_samples, min_disp_tensor, multiplier = self.disparity_initialization(\n            min_disparity, max_disparity, sample_count)\n        min_disp_tensor = min_disp_tensor.unsqueeze(2).repeat(1, 1, self.propagation_filter_size, 1, 1).view(\n            min_disp_tensor.size()[0],\n            min_disp_tensor.size()[1] * self.propagation_filter_size,\n            min_disp_tensor.size()[2],\n            min_disp_tensor.size()[3])\n\n        for prop_iter in range(iteration_count):\n            normalized_disparity_samples = self.propagation(normalized_disparity_samples, device, propagation_type=""horizontal"")\n            disparity_samples = normalized_disparity_samples * \\\n                (max_disparity - min_disparity) * multiplier + min_disp_tensor\n\n            normalized_disparity_samples, disparity_samples = self.evaluate(left_input,\n                                                                            right_input,\n                                                                            disparity_samples,\n                                                                            normalized_disparity_samples)\n\n            normalized_disparity_samples = self.propagation(normalized_disparity_samples, device, propagation_type=""vertical"")\n            disparity_samples = normalized_disparity_samples * \\\n                (max_disparity - min_disparity) * multiplier + min_disp_tensor\n\n            normalized_disparity_samples, disparity_samples = self.evaluate(left_input,\n                                                                            right_input,\n                                                                            disparity_samples,\n                                                                            normalized_disparity_samples)\n\n        return disparity_samples\n'"
deeppruner/models/submodules.py,1,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch.nn as nn\nimport math\n\n\ndef convbn_2d_lrelu(in_planes, out_planes, kernel_size, stride, pad, dilation=1, bias=False):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=(kernel_size, kernel_size),\n                  stride=(stride, stride), padding=(pad, pad), dilation=(dilation, dilation), bias=bias),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.1, inplace=True))\n\n\ndef convbn_3d_lrelu(in_planes, out_planes, kernel_size, stride, pad):\n\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=(pad, pad, pad),\n                                   stride=(1, stride, stride), bias=False),\n                         nn.BatchNorm3d(out_planes),\n                         nn.LeakyReLU(0.1, inplace=True))\n\n\ndef conv_relu(in_planes, out_planes, kernel_size, stride, pad, bias=True):\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size, stride, pad, bias=bias),\n                         nn.ReLU(inplace=True))\n\n\ndef convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_planes))\n\n\ndef convbn_relu(in_planes, out_planes, kernel_size, stride, pad, dilation):\n\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_planes),\n                         nn.ReLU(inplace=True))\n\n\ndef convbn_transpose_3d(inplanes, outplanes, kernel_size, padding, output_padding, stride, bias):\n    return nn.Sequential(nn.ConvTranspose3d(inplanes, outplanes, kernel_size, padding=padding,\n                                            output_padding=output_padding, stride=stride, bias=bias),\n                         nn.BatchNorm3d(outplanes))\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = convbn_relu(inplanes, planes, 3, stride, pad, dilation)\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n\n        out += x\n\n        return out\n\n\nclass SubModule(nn.Module):\n    def __init__(self):\n        super(SubModule, self).__init__()\n\n    def weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n'"
deeppruner/models/submodules2d.py,1,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch.nn as nn\nfrom models.submodules import SubModule, convbn_2d_lrelu\n\n\nclass RefinementNet(SubModule):\n    def __init__(self, inplanes):\n        super(RefinementNet, self).__init__()\n\n        self.conv1 = nn.Sequential(\n            convbn_2d_lrelu(inplanes, 32, kernel_size=3, stride=1, pad=1),\n            convbn_2d_lrelu(32, 32, kernel_size=3, stride=1, pad=1, dilation=1),\n            convbn_2d_lrelu(32, 32, kernel_size=3, stride=1, pad=1, dilation=1),\n            convbn_2d_lrelu(32, 16, kernel_size=3, stride=1, pad=2, dilation=2),\n            convbn_2d_lrelu(16, 16, kernel_size=3, stride=1, pad=4, dilation=4),\n            convbn_2d_lrelu(16, 16, kernel_size=3, stride=1, pad=1, dilation=1))\n\n        self.classif1 = nn.Conv2d(16, 1, kernel_size=3, padding=1, stride=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.weight_init()\n\n    def forward(self, input, disparity):\n        """"""\n        Refinement Block\n        Description:    The network takes left image convolutional features from the second residual block\n                        of the feature network and the current disparity estimation as input.\n                        It then outputs the finetuned disparity prediction. The low-level feature\n                        information serves as a guidance to reduce noise and improve the quality of the final\n                        disparity map, especially on sharp boundaries.\n\n        Args:\n            :input: Input features composed of left image low-level features, cost-aggregator features, and\n                    cost-aggregator disparity.\n\n            :disparity: predicted disparity\n        """"""\n\n        output0 = self.conv1(input)\n        output0 = self.classif1(output0)\n        output = self.relu(output0 + disparity)\n\n        return output'"
deeppruner/models/submodules3d.py,5,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom models.submodules import SubModule, convbn_3d_lrelu, convbn_transpose_3d\n\n\nclass HourGlass(SubModule):\n    def __init__(self, inplanes=16):\n        super(HourGlass, self).__init__()\n\n        self.conv1 = convbn_3d_lrelu(inplanes, inplanes * 2, kernel_size=3, stride=2, pad=1)\n        self.conv2 = convbn_3d_lrelu(inplanes * 2, inplanes * 2, kernel_size=3, stride=1, pad=1)\n\n        self.conv1_1 = convbn_3d_lrelu(inplanes * 2, inplanes * 4, kernel_size=3, stride=2, pad=1)\n        self.conv2_1 = convbn_3d_lrelu(inplanes * 4, inplanes * 4, kernel_size=3, stride=1, pad=1)\n\n        self.conv3 = convbn_3d_lrelu(inplanes * 4, inplanes * 8, kernel_size=3, stride=2, pad=1)\n        self.conv4 = convbn_3d_lrelu(inplanes * 8, inplanes * 8, kernel_size=3, stride=1, pad=1)\n\n        self.conv5 = convbn_transpose_3d(inplanes * 8, inplanes * 4, kernel_size=3, padding=1,\n                                         output_padding=(0, 1, 1), stride=(1, 2, 2), bias=False)\n        self.conv6 = convbn_transpose_3d(inplanes * 4, inplanes * 2, kernel_size=3, padding=1,\n                                         output_padding=(0, 1, 1), stride=(1, 2, 2), bias=False)\n        self.conv7 = convbn_transpose_3d(inplanes * 2, inplanes, kernel_size=3, padding=1,\n                                         output_padding=(0, 1, 1), stride=(1, 2, 2), bias=False)\n\n        self.last_conv3d_layer = nn.Sequential(\n            convbn_3d_lrelu(inplanes, inplanes * 2, 3, 1, 1),\n            nn.Conv3d(inplanes * 2, 1, kernel_size=3, padding=1, stride=1, bias=False))\n\n        self.softmax = nn.Softmax(dim=1)\n\n        self.weight_init()\n\n\nclass MaxDisparityPredictor(HourGlass):\n\n    def __init__(self, hourglass_inplanes=16):\n        super(MaxDisparityPredictor, self).__init__(hourglass_inplanes)\n\n    def forward(self, input, input_disparity):\n        """"""\n        Confidence Range Prediction (Max Disparity):\n        Description:    The network has a convolutional encoder-decoder structure. It takes the sparse\n                disparity estimations from the differentiable PatchMatch, the left image and the warped right image\n                (warped according to the sparse disparity estimations) as input and outputs the upper bound of\n                the confidence range for each pixel i.\n        Args:\n            :input: Left and Warped right Image features as Cost Volume.\n            :input_disparity: PatchMatch predicted disparity samples.\n        Returns:\n            :disparity_output: Max Disparity of the reduced disaprity search range.\n            :feature_output:   High-level features of the MaxDisparityPredictor\n        """"""\n\n        output0 = self.conv1(input)\n        output0_a = self.conv2(output0) + output0\n\n        output0 = self.conv1_1(output0_a)\n        output0_c = self.conv2_1(output0) + output0\n\n        output0 = self.conv3(output0_c)\n        output0 = self.conv4(output0) + output0\n\n        output1 = self.conv5(output0) + output0_c\n        output1 = self.conv6(output1) + output0_a\n        output1 = self.conv7(output1)\n\n        output2 = self.last_conv3d_layer(output1).squeeze(1)\n        feature_output = output2\n\n        confidence_output = self.softmax(output2)\n        disparity_output = torch.sum(confidence_output * input_disparity, dim=1).unsqueeze(1)\n\n        return disparity_output, feature_output\n\n\nclass MinDisparityPredictor(HourGlass):\n\n    def __init__(self, hourglass_inplanes=16):\n        super(MinDisparityPredictor, self).__init__(hourglass_inplanes)\n\n    def forward(self, input, input_disparity):\n        """"""\n        Confidence Range Prediction (Min Disparity):\n        Description:    The network has a convolutional encoder-decoder structure. It takes the sparse\n                disparity estimations from the differentiable PatchMatch, the left image and the warped right image\n                (warped according to the sparse disparity estimations) as input and outputs the lower bound of\n                the confidence range for each pixel i.\n        Args:\n            :input: Left and Warped right Image features as Cost Volume.\n            :input_disparity: PatchMatch predicted disparity samples.\n        Returns:\n            :disparity_output: Min Disparity of the reduced disaprity search range.\n            :feature_output:   High-level features of the MaxDisparityPredictor\n        """"""\n\n        output0 = self.conv1(input)\n        output0_a = self.conv2(output0) + output0\n\n        output0 = self.conv1_1(output0_a)\n        output0_c = self.conv2_1(output0) + output0\n\n        output0 = self.conv3(output0_c)\n        output0 = self.conv4(output0) + output0\n\n        output1 = self.conv5(output0) + output0_c\n        output1 = self.conv6(output1) + output0_a\n        output1 = self.conv7(output1)\n\n        output2 = self.last_conv3d_layer(output1).squeeze(1)\n        feature_output = output2\n\n        confidence_output = self.softmax(output2)\n        disparity_output = torch.sum(confidence_output * input_disparity, dim=1).unsqueeze(1)\n\n        return disparity_output, feature_output\n\n\nclass CostAggregator(HourGlass):\n\n    def __init__(self, cost_aggregator_inplanes, hourglass_inplanes=16):\n        super(CostAggregator, self).__init__(inplanes=16)\n\n        self.dres0 = nn.Sequential(convbn_3d_lrelu(cost_aggregator_inplanes, 64, 3, 1, 1),\n                                   convbn_3d_lrelu(64, 32, 3, 1, 1))\n\n        self.dres1 = nn.Sequential(convbn_3d_lrelu(32, 32, 3, 1, 1),\n                                   convbn_3d_lrelu(32, hourglass_inplanes, 3, 1, 1))\n\n    def forward(self, input, input_disparity):\n        """"""\n        3D Cost Aggregator\n        Description:    Based on the predicted range in the pruning module,\n                we build the 3D cost volume estimator and conduct spatial aggregation.\n                Following common practice, we take the left image, the warped right image and corresponding disparities\n                as input and output the cost over the disparity range at the size B X R X H X W , where R is the number\n                of disparities per pixel. Compared to prior work, our R is more than 10 times smaller, making\n                this module very efficient. Soft-arg max is again used to predict the disparity value ,\n                so that our approach is end-to-end trainable.\n\n        Args:\n            :input:   Cost-Volume composed of left image features, warped right image features,\n                      Confidence range Predictor features and input disparity samples/\n\n            :input_disparity: input disparity samples.\n\n        Returns:\n            :disparity_output: Predicted disparity\n            :feature_output: High-level features of 3d-Cost Aggregator\n\n        """"""\n\n        output0 = self.dres0(input)\n        output0_b = self.dres1(output0)\n\n        output0 = self.conv1(output0_b)\n        output0_a = self.conv2(output0) + output0\n\n        output0 = self.conv1_1(output0_a)\n        output0_c = self.conv2_1(output0) + output0\n\n        output0 = self.conv3(output0_c)\n        output0 = self.conv4(output0) + output0\n\n        output1 = self.conv5(output0) + output0_c\n        output1 = self.conv6(output1) + output0_a\n        output1 = self.conv7(output1) + output0_b\n\n        output2 = self.last_conv3d_layer(output1).squeeze(1)\n        feature_output = output2\n\n        confidence_output = self.softmax(output2)\n        disparity_output = torch.sum(confidence_output * input_disparity, dim=1)\n\n        return disparity_output.unsqueeze(1), feature_output\n'"
deeppruner/models/utils.py,7,"b'# ---------------------------------------------------------------------------\n# DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n#\n# Copyright (c) 2019 Uber Technologies, Inc.\n#\n# Licensed under the Uber Non-Commercial License (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at the root directory of this project. \n#\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Written by Shivam Duggal\n# ---------------------------------------------------------------------------\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\n\n\nclass UniformSampler(nn.Module):\n    def __init__(self):\n        super(UniformSampler, self).__init__()\n\n    def forward(self, min_disparity, max_disparity, number_of_samples=10):\n        """"""\n        Uniform Sampler\n        Description:    The Confidence Range Predictor predicts a reduced disparity search range R(i) = [l(i), u(i)]\n            for each pixel i. We then, generate disparity samples from this reduced search range for Cost Aggregation\n            or second stage of Patch Match. From experiments, we found Uniform sampling to work better.\n\n        Args:\n            :min_disparity: lower bound of disparity search range (predicted by Confidence Range Predictor)\n            :max_disparity: upper bound of disparity range predictor (predicted by Confidence Range Predictor)\n            :number_of_samples (default:10): number of samples to be genearted.\n        Returns:\n            :sampled_disparities: Uniformly generated disparity samples from the input search range.\n        """"""\n\n        device = min_disparity.get_device()\n\n        multiplier = (max_disparity - min_disparity) / (number_of_samples + 1)\n        range_multiplier = torch.arange(1.0, number_of_samples + 1, 1, device=device).view(number_of_samples, 1, 1)\n        sampled_disparities = min_disparity + multiplier * range_multiplier\n\n        return sampled_disparities\n\n\nclass SpatialTransformer(nn.Module):\n    def __init__(self):\n        super(SpatialTransformer, self).__init__()\n\n    def forward(self, left_input, right_input, disparity_samples):\n        """"""\n        Disparity Sample Cost Evaluator\n        Description:\n                Given the left image features, right iamge features and teh disparity samples, generates:\n                    - Per sample cost as <left_image_features, right_image_features>, <.,.> denotes scalar-product.\n                    - Warped righ image features\n\n        Args:\n            :left_input: Left Image Features\n            :right_input: Right Image Features\n            :disparity_samples:  Disparity Samples genearted by PatchMatch\n\n        Returns:\n            :disparity_samples_strength_1: Cost associated with each disaprity sample.\n            :warped_right_feature_map: right iamge features warped according to input disparity.\n            :left_feature_map: expanded left image features.\n        """"""\n\n        device = left_input.get_device()\n        left_y_coordinate = torch.arange(0.0, left_input.size()[3], device=device).repeat(left_input.size()[2])\n        left_y_coordinate = left_y_coordinate.view(left_input.size()[2], left_input.size()[3])\n        left_y_coordinate = torch.clamp(left_y_coordinate, min=0, max=left_input.size()[3] - 1)\n        left_y_coordinate = left_y_coordinate.expand(left_input.size()[0], -1, -1)\n\n        right_feature_map = right_input.expand(disparity_samples.size()[1], -1, -1, -1, -1).permute([1, 2, 0, 3, 4])\n        left_feature_map = left_input.expand(disparity_samples.size()[1], -1, -1, -1, -1).permute([1, 2, 0, 3, 4])\n\n        disparity_samples = disparity_samples.float()\n\n        right_y_coordinate = left_y_coordinate.expand(\n            disparity_samples.size()[1], -1, -1, -1).permute([1, 0, 2, 3]) - disparity_samples\n\n        right_y_coordinate_1 = right_y_coordinate\n        right_y_coordinate = torch.clamp(right_y_coordinate, min=0, max=right_input.size()[3] - 1)\n\n        warped_right_feature_map = torch.gather(right_feature_map, dim=4, index=right_y_coordinate.expand(\n            right_input.size()[1], -1, -1, -1, -1).permute([1, 0, 2, 3, 4]).long())\n\n        right_y_coordinate_1 = right_y_coordinate_1.unsqueeze(1)\n        warped_right_feature_map = (1 - ((right_y_coordinate_1 < 0) +\n                                         (right_y_coordinate_1 > right_input.size()[3] - 1)).float()) * \\\n            (warped_right_feature_map) + torch.zeros_like(warped_right_feature_map)\n\n        return warped_right_feature_map, left_feature_map\n'"
