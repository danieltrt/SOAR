file_path,api_count,code
1_1_Image_Representation/helpers.py,0,"b'# Helper functions\n\nimport os\nimport glob # library for loading images from a directory\nimport matplotlib.image as mpimg\n\nimport cv2\n\n\n\n# This function loads in images and their labels and places them in a list\n# The list contains all images and their associated labels\n# For example, after data is loaded, im_list[0][:] will be the first image-label pair in the list\ndef load_dataset(image_dir):\n    \n    # Populate this empty image list\n    im_list = []\n    image_types = [""day"", ""night""]\n    \n    # Iterate through each color folder\n    for im_type in image_types:\n        \n        # Iterate through each image file in each image_type folder\n        # glob reads in any image with the extension ""image_dir/im_type/*""\n        for file in glob.glob(os.path.join(image_dir, im_type, ""*"")):\n            \n            # Read in the image\n            im = mpimg.imread(file)\n            \n            # Check if the image exists/if it\'s been correctly read-in\n            if not im is None:\n                # Append the image, and it\'s type (red, green, yellow) to the image list\n                im_list.append((im, im_type))\n    \n    return im_list\n\n\n\n## Standardize the input images\n# Resize each image to the desired input size: 600x1100px (hxw).\n\n## Standardize the output\n# With each loaded image, we also specify the expected output.\n# For this, we use binary numerical values 0/1 = night/day.\n\n\n# This function should take in an RGB image and return a new, standardized version\n# 600 height x 1100 width image size (px x px)\ndef standardize_input(image):\n    \n    # Resize image and pre-process so that all ""standard"" images are the same size\n    standard_im = cv2.resize(image, (1100, 600))\n    \n    return standard_im\n\n\n# Examples:\n# encode(""day"") should return: 1\n# encode(""night"") should return: 0\ndef encode(label):\n    \n    numerical_val = 0\n    if(label == \'day\'):\n        numerical_val = 1\n    # else it is night and can stay 0\n    \n    return numerical_val\n\n# using both functions above, standardize the input images and output labels\ndef standardize(image_list):\n    \n    # Empty image data array\n    standard_list = []\n    \n    # Iterate through all the image-label pairs\n    for item in image_list:\n        image = item[0]\n        label = item[1]\n        \n        # Standardize the image\n        standardized_im = standardize_input(image)\n        \n        # Create a numerical label\n        binary_label = encode(label)\n        \n        # Append the image, and it\'s one hot encoded label to the full, processed list of image data\n        standard_list.append((standardized_im, binary_label))\n    \n    return standard_list\n\n\n\n\n'"
2_2_YOLO/darknet.py,24,"b""import torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass YoloLayer(nn.Module):\n    def __init__(self, anchor_mask=[], num_classes=0, anchors=[], num_anchors=1):\n        super(YoloLayer, self).__init__()\n        self.anchor_mask = anchor_mask\n        self.num_classes = num_classes\n        self.anchors = anchors\n        self.num_anchors = num_anchors\n        self.anchor_step = len(anchors)/num_anchors\n        self.coord_scale = 1\n        self.noobject_scale = 1\n        self.object_scale = 5\n        self.class_scale = 1\n        self.thresh = 0.6\n        self.stride = 32\n        self.seen = 0\n\n    def forward(self, output, nms_thresh):\n        self.thresh = nms_thresh\n        masked_anchors = []\n            \n        for m in self.anchor_mask:\n            masked_anchors += self.anchors[m*self.anchor_step:(m+1)*self.anchor_step]\n                \n        masked_anchors = [anchor/self.stride for anchor in masked_anchors]\n        boxes = get_region_boxes(output.data, self.thresh, self.num_classes, masked_anchors, len(self.anchor_mask))\n            \n        return boxes\n\n    \nclass Upsample(nn.Module):\n    def __init__(self, stride=2):\n        super(Upsample, self).__init__()\n        self.stride = stride\n    def forward(self, x):\n        stride = self.stride\n        assert(x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H, 1, W, 1).expand(B, C, H, stride, W, stride).contiguous().view(B, C, H*stride, W*stride)\n        return x\n\n\n#for route and shortcut\nclass EmptyModule(nn.Module):\n    def __init__(self):\n        super(EmptyModule, self).__init__()\n\n    def forward(self, x):\n        return x\n\n# support route shortcut\nclass Darknet(nn.Module):\n    def __init__(self, cfgfile):\n        super(Darknet, self).__init__()\n        self.blocks = parse_cfg(cfgfile)\n        self.models = self.create_network(self.blocks) # merge conv, bn,leaky\n        self.loss = self.models[len(self.models)-1]\n\n        self.width = int(self.blocks[0]['width'])\n        self.height = int(self.blocks[0]['height'])\n\n        self.header = torch.IntTensor([0,0,0,0])\n        self.seen = 0\n\n    def forward(self, x, nms_thresh):            \n        ind = -2\n        self.loss = None\n        outputs = dict()\n        out_boxes = []\n        \n        for block in self.blocks:\n            ind = ind + 1\n            if block['type'] == 'net':\n                continue\n            elif block['type'] in ['convolutional', 'upsample']: \n                x = self.models[ind](x)\n                outputs[ind] = x\n            elif block['type'] == 'route':\n                layers = block['layers'].split(',')\n                layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n                if len(layers) == 1:\n                    x = outputs[layers[0]]\n                    outputs[ind] = x\n                elif len(layers) == 2:\n                    x1 = outputs[layers[0]]\n                    x2 = outputs[layers[1]]\n                    x = torch.cat((x1,x2),1)\n                    outputs[ind] = x\n            elif block['type'] == 'shortcut':\n                from_layer = int(block['from'])\n                activation = block['activation']\n                from_layer = from_layer if from_layer > 0 else from_layer + ind\n                x1 = outputs[from_layer]\n                x2 = outputs[ind-1]\n                x  = x1 + x2\n                outputs[ind] = x\n            elif block['type'] == 'yolo':\n                boxes = self.models[ind](x, nms_thresh)\n                out_boxes.append(boxes)\n            else:\n                print('unknown type %s' % (block['type']))\n            \n        return out_boxes\n    \n\n    def print_network(self):\n        print_cfg(self.blocks)\n\n    def create_network(self, blocks):\n        models = nn.ModuleList()\n    \n        prev_filters = 3\n        out_filters =[]\n        prev_stride = 1\n        out_strides = []\n        conv_id = 0\n        for block in blocks:\n            if block['type'] == 'net':\n                prev_filters = int(block['channels'])\n                continue\n            elif block['type'] == 'convolutional':\n                conv_id = conv_id + 1\n                batch_normalize = int(block['batch_normalize'])\n                filters = int(block['filters'])\n                kernel_size = int(block['size'])\n                stride = int(block['stride'])\n                is_pad = int(block['pad'])\n                pad = (kernel_size-1)//2 if is_pad else 0\n                activation = block['activation']\n                model = nn.Sequential()\n                if batch_normalize:\n                    model.add_module('conv{0}'.format(conv_id), nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=False))\n                    model.add_module('bn{0}'.format(conv_id), nn.BatchNorm2d(filters))\n                else:\n                    model.add_module('conv{0}'.format(conv_id), nn.Conv2d(prev_filters, filters, kernel_size, stride, pad))\n                if activation == 'leaky':\n                    model.add_module('leaky{0}'.format(conv_id), nn.LeakyReLU(0.1, inplace=True))\n                prev_filters = filters\n                out_filters.append(prev_filters)\n                prev_stride = stride * prev_stride\n                out_strides.append(prev_stride)\n                models.append(model)\n            elif block['type'] == 'upsample':\n                stride = int(block['stride'])\n                out_filters.append(prev_filters)\n                prev_stride = prev_stride // stride\n                out_strides.append(prev_stride)\n                models.append(Upsample(stride))\n            elif block['type'] == 'route':\n                layers = block['layers'].split(',')\n                ind = len(models)\n                layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n                if len(layers) == 1:\n                    prev_filters = out_filters[layers[0]]\n                    prev_stride = out_strides[layers[0]]\n                elif len(layers) == 2:\n                    assert(layers[0] == ind - 1)\n                    prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n                    prev_stride = out_strides[layers[0]]\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(EmptyModule())\n            elif block['type'] == 'shortcut':\n                ind = len(models)\n                prev_filters = out_filters[ind-1]\n                out_filters.append(prev_filters)\n                prev_stride = out_strides[ind-1]\n                out_strides.append(prev_stride)\n                models.append(EmptyModule())\n            elif block['type'] == 'yolo':\n                yolo_layer = YoloLayer()\n                anchors = block['anchors'].split(',')\n                anchor_mask = block['mask'].split(',')\n                yolo_layer.anchor_mask = [int(i) for i in anchor_mask]\n                yolo_layer.anchors = [float(i) for i in anchors]\n                yolo_layer.num_classes = int(block['classes'])\n                yolo_layer.num_anchors = int(block['num'])\n                yolo_layer.anchor_step = len(yolo_layer.anchors)//yolo_layer.num_anchors\n                yolo_layer.stride = prev_stride\n                out_filters.append(prev_filters)\n                out_strides.append(prev_stride)\n                models.append(yolo_layer)\n            else:\n                print('unknown type %s' % (block['type']))\n    \n        return models\n\n    def load_weights(self, weightfile):\n        print()\n        fp = open(weightfile, 'rb')\n        header = np.fromfile(fp, count=5, dtype=np.int32)\n        self.header = torch.from_numpy(header)\n        self.seen = self.header[3]\n        buf = np.fromfile(fp, dtype = np.float32)\n        fp.close()\n\n        start = 0\n        ind = -2\n        counter = 3\n        for block in self.blocks:\n            if start >= buf.size:\n                break\n            ind = ind + 1\n            if block['type'] == 'net':\n                continue\n            elif block['type'] == 'convolutional':\n                model = self.models[ind]\n                batch_normalize = int(block['batch_normalize'])\n                if batch_normalize:\n                    start = load_conv_bn(buf, start, model[0], model[1])\n                else:\n                    start = load_conv(buf, start, model[0])\n            elif block['type'] == 'upsample':\n                pass\n            elif block['type'] == 'route':\n                pass\n            elif block['type'] == 'shortcut':\n                pass\n            elif block['type'] == 'yolo':\n                pass\n            else:\n                print('unknown type %s' % (block['type']))\n            \n            percent_comp = (counter / len(self.blocks)) * 100\n\n            print('Loading weights. Please Wait...{:.2f}% Complete'.format(percent_comp), end = '\\r', flush = True)\n\n            counter += 1\n\n            \n            \ndef convert2cpu(gpu_matrix):\n    return torch.FloatTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\n\ndef convert2cpu_long(gpu_matrix):\n    return torch.LongTensor(gpu_matrix.size()).copy_(gpu_matrix)\n\n\ndef get_region_boxes(output, conf_thresh, num_classes, anchors, num_anchors, only_objectness = 1, validation = False):\n    anchor_step = len(anchors)//num_anchors\n    if output.dim() == 3:\n        output = output.unsqueeze(0)\n    batch = output.size(0)\n    assert(output.size(1) == (5+num_classes)*num_anchors)\n    h = output.size(2)\n    w = output.size(3)\n\n    all_boxes = []\n    output = output.view(batch*num_anchors, 5+num_classes, h*w).transpose(0,1).contiguous().view(5+num_classes, batch*num_anchors*h*w)\n\n    grid_x = torch.linspace(0, w-1, w).repeat(h,1).repeat(batch*num_anchors, 1, 1).view(batch*num_anchors*h*w).type_as(output) #cuda()\n    grid_y = torch.linspace(0, h-1, h).repeat(w,1).t().repeat(batch*num_anchors, 1, 1).view(batch*num_anchors*h*w).type_as(output) #cuda()\n    xs = torch.sigmoid(output[0]) + grid_x\n    ys = torch.sigmoid(output[1]) + grid_y\n\n    anchor_w = torch.Tensor(anchors).view(num_anchors, anchor_step).index_select(1, torch.LongTensor([0]))\n    anchor_h = torch.Tensor(anchors).view(num_anchors, anchor_step).index_select(1, torch.LongTensor([1]))\n    anchor_w = anchor_w.repeat(batch, 1).repeat(1, 1, h*w).view(batch*num_anchors*h*w).type_as(output) #cuda()\n    anchor_h = anchor_h.repeat(batch, 1).repeat(1, 1, h*w).view(batch*num_anchors*h*w).type_as(output) #cuda()\n    ws = torch.exp(output[2]) * anchor_w\n    hs = torch.exp(output[3]) * anchor_h\n\n    det_confs = torch.sigmoid(output[4])\n    cls_confs = torch.nn.Softmax(dim=1)(output[5:5+num_classes].transpose(0,1)).detach()\n    cls_max_confs, cls_max_ids = torch.max(cls_confs, 1)\n    cls_max_confs = cls_max_confs.view(-1)\n    cls_max_ids = cls_max_ids.view(-1)\n\n    \n    sz_hw = h*w\n    sz_hwa = sz_hw*num_anchors\n    det_confs = convert2cpu(det_confs)\n    cls_max_confs = convert2cpu(cls_max_confs)\n    cls_max_ids = convert2cpu_long(cls_max_ids)\n    xs = convert2cpu(xs)\n    ys = convert2cpu(ys)\n    ws = convert2cpu(ws)\n    hs = convert2cpu(hs)\n    if validation:\n        cls_confs = convert2cpu(cls_confs.view(-1, num_classes))\n\n    for b in range(batch):\n        boxes = []\n        for cy in range(h):\n            for cx in range(w):\n                for i in range(num_anchors):\n                    ind = b*sz_hwa + i*sz_hw + cy*w + cx\n                    det_conf =  det_confs[ind]\n                    if only_objectness:\n                        conf =  det_confs[ind]\n                    else:\n                        conf = det_confs[ind] * cls_max_confs[ind]\n    \n                    if conf > conf_thresh:\n                        bcx = xs[ind]\n                        bcy = ys[ind]\n                        bw = ws[ind]\n                        bh = hs[ind]\n                        cls_max_conf = cls_max_confs[ind]\n                        cls_max_id = cls_max_ids[ind]\n                        box = [bcx/w, bcy/h, bw/w, bh/h, det_conf, cls_max_conf, cls_max_id]\n                        if (not only_objectness) and validation:\n                            for c in range(num_classes):\n                                tmp_conf = cls_confs[ind][c]\n                                if c != cls_max_id and det_confs[ind]*tmp_conf > conf_thresh:\n                                    box.append(tmp_conf)\n                                    box.append(c)\n                        boxes.append(box)\n        all_boxes.append(boxes)\n\n    return all_boxes\n\n\ndef parse_cfg(cfgfile):\n    blocks = []\n    fp = open(cfgfile, 'r')\n    block =  None\n    line = fp.readline()\n    while line != '':\n        line = line.rstrip()\n        if line == '' or line[0] == '#':\n            line = fp.readline()\n            continue        \n        elif line[0] == '[':\n            if block:\n                blocks.append(block)\n            block = dict()\n            block['type'] = line.lstrip('[').rstrip(']')\n            # set default value\n            if block['type'] == 'convolutional':\n                block['batch_normalize'] = 0\n        else:\n            key,value = line.split('=')\n            key = key.strip()\n            if key == 'type':\n                key = '_type'\n            value = value.strip()\n            block[key] = value\n        line = fp.readline()\n\n    if block:\n        blocks.append(block)\n    fp.close()\n    return blocks\n\n\ndef print_cfg(blocks):\n    print('layer     filters    size              input                output');\n    prev_width = 416\n    prev_height = 416\n    prev_filters = 3\n    out_filters =[]\n    out_widths =[]\n    out_heights =[]\n    ind = -2\n    for block in blocks:\n        ind = ind + 1\n        if block['type'] == 'net':\n            prev_width = int(block['width'])\n            prev_height = int(block['height'])\n            continue\n        elif block['type'] == 'convolutional':\n            filters = int(block['filters'])\n            kernel_size = int(block['size'])\n            stride = int(block['stride'])\n            is_pad = int(block['pad'])\n            pad = (kernel_size-1)//2 if is_pad else 0\n            width = (prev_width + 2*pad - kernel_size)//stride + 1\n            height = (prev_height + 2*pad - kernel_size)//stride + 1\n            print('%5d %-6s %4d  %d x %d / %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'conv', filters, kernel_size, kernel_size, stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'upsample':\n            stride = int(block['stride'])\n            filters = prev_filters\n            width = prev_width*stride\n            height = prev_height*stride\n            print('%5d %-6s           * %d   %3d x %3d x%4d   ->   %3d x %3d x%4d' % (ind, 'upsample', stride, prev_width, prev_height, prev_filters, width, height, filters))\n            prev_width = width\n            prev_height = height\n            prev_filters = filters\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'route':\n            layers = block['layers'].split(',')\n            layers = [int(i) if int(i) > 0 else int(i)+ind for i in layers]\n            if len(layers) == 1:\n                print('%5d %-6s %d' % (ind, 'route', layers[0]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                prev_filters = out_filters[layers[0]]\n            elif len(layers) == 2:\n                print('%5d %-6s %d %d' % (ind, 'route', layers[0], layers[1]))\n                prev_width = out_widths[layers[0]]\n                prev_height = out_heights[layers[0]]\n                assert(prev_width == out_widths[layers[1]])\n                assert(prev_height == out_heights[layers[1]])\n                prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] in ['region', 'yolo']:\n            print('%5d %-6s' % (ind, 'detection'))\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        elif block['type'] == 'shortcut':\n            from_id = int(block['from'])\n            from_id = from_id if from_id > 0 else from_id+ind\n            print('%5d %-6s %d' % (ind, 'shortcut', from_id))\n            prev_width = out_widths[from_id]\n            prev_height = out_heights[from_id]\n            prev_filters = out_filters[from_id]\n            out_widths.append(prev_width)\n            out_heights.append(prev_height)\n            out_filters.append(prev_filters)\n        else:\n            print('unknown type %s' % (block['type']))\n\n            \ndef load_conv(buf, start, conv_model):\n    num_w = conv_model.weight.numel()\n    num_b = conv_model.bias.numel()\n    conv_model.bias.data.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b\n    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w]).view_as(conv_model.weight.data)); start = start + num_w\n    return start\n\n\ndef load_conv_bn(buf, start, conv_model, bn_model):\n    num_w = conv_model.weight.numel()\n    num_b = bn_model.bias.numel()\n    bn_model.bias.data.copy_(torch.from_numpy(buf[start:start+num_b]));     start = start + num_b\n    bn_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b\n    bn_model.running_mean.copy_(torch.from_numpy(buf[start:start+num_b]));  start = start + num_b\n    bn_model.running_var.copy_(torch.from_numpy(buf[start:start+num_b]));   start = start + num_b\n    conv_model.weight.data.copy_(torch.from_numpy(buf[start:start+num_w]).view_as(conv_model.weight.data)); start = start + num_w\n    return start\n"""
2_2_YOLO/utils.py,4,"b""import time\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\ndef boxes_iou(box1, box2):\n  \n    # Get the Width and Height of each bounding box\n    width_box1 = box1[2]\n    height_box1 = box1[3]\n    width_box2 = box2[2]\n    height_box2 = box2[3]\n    \n    # Calculate the area of the each bounding box\n    area_box1 = width_box1 * height_box1\n    area_box2 = width_box2 * height_box2\n    \n    # Find the vertical edges of the union of the two bounding boxes\n    mx = min(box1[0] - width_box1/2.0, box2[0] - width_box2/2.0)\n    Mx = max(box1[0] + width_box1/2.0, box2[0] + width_box2/2.0)\n    \n    # Calculate the width of the union of the two bounding boxes\n    union_width = Mx - mx\n    \n    # Find the horizontal edges of the union of the two bounding boxes\n    my = min(box1[1] - height_box1/2.0, box2[1] - height_box2/2.0)\n    My = max(box1[1] + height_box1/2.0, box2[1] + height_box2/2.0)    \n    \n    # Calculate the height of the union of the two bounding boxes\n    union_height = My - my\n    \n    # Calculate the width and height of the area of intersection of the two bounding boxes\n    intersection_width = width_box1 + width_box2 - union_width\n    intersection_height = height_box1 + height_box2 - union_height\n   \n    # If the the boxes don't overlap then their IOU is zero\n    if intersection_width <= 0 or intersection_height <= 0:\n        return 0.0\n\n    # Calculate the area of intersection of the two bounding boxes\n    intersection_area = intersection_width * intersection_height\n    \n    # Calculate the area of the union of the two bounding boxes\n    union_area = area_box1 + area_box2 - intersection_area\n    \n    # Calculate the IOU\n    iou = intersection_area/union_area\n    \n    return iou\n\n\ndef nms(boxes, iou_thresh):\n    \n    # If there are no bounding boxes do nothing\n    if len(boxes) == 0:\n        return boxes\n    \n    # Create a PyTorch Tensor to keep track of the detection confidence\n    # of each predicted bounding box\n    det_confs = torch.zeros(len(boxes))\n    \n    # Get the detection confidence of each predicted bounding box\n    for i in range(len(boxes)):\n        det_confs[i] = boxes[i][4]\n\n    # Sort the indices of the bounding boxes by detection confidence value in descending order.\n    # We ignore the first returned element since we are only interested in the sorted indices\n    _,sortIds = torch.sort(det_confs, descending = True)\n    \n    # Create an empty list to hold the best bounding boxes after\n    # Non-Maximal Suppression (NMS) is performed\n    best_boxes = []\n    \n    # Perform Non-Maximal Suppression \n    for i in range(len(boxes)):\n        \n        # Get the bounding box with the highest detection confidence first\n        box_i = boxes[sortIds[i]]\n        \n        # Check that the detection confidence is not zero\n        if box_i[4] > 0:\n            \n            # Save the bounding box \n            best_boxes.append(box_i)\n            \n            # Go through the rest of the bounding boxes in the list and calculate their IOU with\n            # respect to the previous selected box_i. \n            for j in range(i + 1, len(boxes)):\n                box_j = boxes[sortIds[j]]\n                \n                # If the IOU of box_i and box_j is higher than the given IOU threshold set\n                # box_j's detection confidence to zero. \n                if boxes_iou(box_i, box_j) > iou_thresh:\n                    box_j[4] = 0\n                    \n    return best_boxes\n\n\ndef detect_objects(model, img, iou_thresh, nms_thresh):\n    \n    # Start the time. This is done to calculate how long the detection takes.\n    start = time.time()\n    \n    # Set the model to evaluation mode.\n    model.eval()\n    \n    # Convert the image from a NumPy ndarray to a PyTorch Tensor of the correct shape.\n    # The image is transposed, then converted to a FloatTensor of dtype float32, then\n    # Normalized to values between 0 and 1, and finally unsqueezed to have the correct\n    # shape of 1 x 3 x 416 x 416\n    img = torch.from_numpy(img.transpose(2,0,1)).float().div(255.0).unsqueeze(0)\n    \n    # Feed the image to the neural network with the corresponding NMS threshold.\n    # The first step in NMS is to remove all bounding boxes that have a very low\n    # probability of detection. All predicted bounding boxes with a value less than\n    # the given NMS threshold will be removed.\n    list_boxes = model(img, nms_thresh)\n    \n    # Make a new list with all the bounding boxes returned by the neural network\n    boxes = list_boxes[0][0] + list_boxes[1][0] + list_boxes[2][0]\n    \n    # Perform the second step of NMS on the bounding boxes returned by the neural network.\n    # In this step, we only keep the best bounding boxes by eliminating all the bounding boxes\n    # whose IOU value is higher than the given IOU threshold\n    boxes = nms(boxes, iou_thresh)\n    \n    # Stop the time. \n    finish = time.time()\n    \n    # Print the time it took to detect objects\n    print('\\n\\nIt took {:.3f}'.format(finish - start), 'seconds to detect the objects in the image.\\n')\n    \n    # Print the number of objects detected\n    print('Number of Objects Detected:', len(boxes), '\\n')\n    \n    return boxes\n\n\ndef load_class_names(namesfile):\n    \n    # Create an empty list to hold the object classes\n    class_names = []\n    \n    # Open the file containing the COCO object classes in read-only mode\n    with open(namesfile, 'r') as fp:\n        \n        # The coco.names file contains only one object class per line.\n        # Read the file line by line and save all the lines in a list.\n        lines = fp.readlines()\n    \n    # Get the object class names\n    for line in lines:\n        \n        # Make a copy of each line with any trailing whitespace removed\n        line = line.rstrip()\n        \n        # Save the object class name into class_names\n        class_names.append(line)\n        \n    return class_names\n\n\ndef print_objects(boxes, class_names):    \n    print('Objects Found and Confidence Level:\\n')\n    for i in range(len(boxes)):\n        box = boxes[i]\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = box[6]\n            print('%i. %s: %f' % (i + 1, class_names[cls_id], cls_conf))\n\n            \ndef plot_boxes(img, boxes, class_names, plot_labels, color = None):\n    \n    # Define a tensor used to set the colors of the bounding boxes\n    colors = torch.FloatTensor([[1,0,1],[0,0,1],[0,1,1],[0,1,0],[1,1,0],[1,0,0]])\n    \n    # Define a function to set the colors of the bounding boxes\n    def get_color(c, x, max_val):\n        ratio = float(x) / max_val * 5\n        i = int(np.floor(ratio))\n        j = int(np.ceil(ratio))\n        \n        ratio = ratio - i\n        r = (1 - ratio) * colors[i][c] + ratio * colors[j][c]\n        \n        return int(r * 255)\n    \n    # Get the width and height of the image\n    width = img.shape[1]\n    height = img.shape[0]\n    \n    # Create a figure and plot the image\n    fig, a = plt.subplots(1,1)\n    a.imshow(img)\n    \n    # Plot the bounding boxes and corresponding labels on top of the image\n    for i in range(len(boxes)):\n        \n        # Get the ith bounding box\n        box = boxes[i]\n        \n        # Get the (x,y) pixel coordinates of the lower-left and lower-right corners\n        # of the bounding box relative to the size of the image. \n        x1 = int(np.around((box[0] - box[2]/2.0) * width))\n        y1 = int(np.around((box[1] - box[3]/2.0) * height))\n        x2 = int(np.around((box[0] + box[2]/2.0) * width))\n        y2 = int(np.around((box[1] + box[3]/2.0) * height))\n        \n        # Set the default rgb value to red\n        rgb = (1, 0, 0)\n            \n        # Use the same color to plot the bounding boxes of the same object class\n        if len(box) >= 7 and class_names:\n            cls_conf = box[5]\n            cls_id = box[6]\n            classes = len(class_names)\n            offset = cls_id * 123457 % classes\n            red   = get_color(2, offset, classes) / 255\n            green = get_color(1, offset, classes) / 255\n            blue  = get_color(0, offset, classes) / 255\n            \n            # If a color is given then set rgb to the given color instead\n            if color is None:\n                rgb = (red, green, blue)\n            else:\n                rgb = color\n        \n        # Calculate the width and height of the bounding box relative to the size of the image.\n        width_x = x2 - x1\n        width_y = y1 - y2\n        \n        # Set the postion and size of the bounding box. (x1, y2) is the pixel coordinate of the\n        # lower-left corner of the bounding box relative to the size of the image.\n        rect = patches.Rectangle((x1, y2),\n                                 width_x, width_y,\n                                 linewidth = 2,\n                                 edgecolor = rgb,\n                                 facecolor = 'none')\n\n        # Draw the bounding box on top of the image\n        a.add_patch(rect)\n        \n        # If plot_labels = True then plot the corresponding label\n        if plot_labels:\n            \n            # Create a string with the object class name and the corresponding object class probability\n            conf_tx = class_names[cls_id] + ': {:.1f}'.format(cls_conf)\n            \n            # Define x and y offsets for the labels\n            lxc = (img.shape[1] * 0.266) / 100\n            lyc = (img.shape[0] * 1.180) / 100\n            \n            # Draw the labels on top of the image\n            a.text(x1 + lxc, y1 - lyc, conf_tx, fontsize = 24, color = 'k',\n                   bbox = dict(facecolor = rgb, edgecolor = rgb, alpha = 0.8))        \n        \n    plt.show()\n"""
2_4_LSTMs/sample.py,0,"b""'''\nMIT License\n\nCopyright (c) 2018 Udacity\n\n'''\n\nimport argparse\n\nfrom model import CharRNN, load_model, sample\n\nparser = argparse.ArgumentParser(\n                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\nparser.add_argument('checkpoint', type=str, default=None,\n                    help='initialize network from checkpoint')\nparser.add_argument('--gpu', action='store_true', default=False,\n                    help='run the network on the GPU')\nparser.add_argument('--num_samples', type=int, default=200,\n                    help='number of samples for generating text')\nparser.add_argument('--prime', type=str, default='From afar',\n                    help='prime the network with characters for sampling')\nparser.add_argument('--top_k', type=int, default=10,\n                    help='sample from top K character probabilities')\n\n\nargs = parser.parse_args()\n\nnet = load_model(args.checkpoint)\n\nprint(sample(net, args.num_samples, cuda=args.gpu, top_k=args.top_k, prime=args.prime))"""
