file_path,api_count,code
ddpg-bipedal/ddpg_agent.py,11,"b'import numpy as np\nimport random\nimport copy\nfrom collections import namedtuple, deque\n\nfrom model import Actor, Critic\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nBUFFER_SIZE = int(1e6)  # replay buffer size\nBATCH_SIZE = 128        # minibatch size\nGAMMA = 0.99            # discount factor\nTAU = 1e-3              # for soft update of target parameters\nLR_ACTOR = 1e-4         # learning rate of the actor \nLR_CRITIC = 3e-4        # learning rate of the critic\nWEIGHT_DECAY = 0.0001   # L2 weight decay\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nclass Agent():\n    """"""Interacts with and learns from the environment.""""""\n    \n    def __init__(self, state_size, action_size, random_seed):\n        """"""Initialize an Agent object.\n        \n        Params\n        ======\n            state_size (int): dimension of each state\n            action_size (int): dimension of each action\n            random_seed (int): random seed\n        """"""\n        self.state_size = state_size\n        self.action_size = action_size\n        self.seed = random.seed(random_seed)\n\n        # Actor Network (w/ Target Network)\n        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n\n        # Critic Network (w/ Target Network)\n        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n\n        # Noise process\n        self.noise = OUNoise(action_size, random_seed)\n\n        # Replay memory\n        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n    \n    def step(self, state, action, reward, next_state, done):\n        """"""Save experience in replay memory, and use random sample from buffer to learn.""""""\n        # Save experience / reward\n        self.memory.add(state, action, reward, next_state, done)\n\n        # Learn, if enough samples are available in memory\n        if len(self.memory) > BATCH_SIZE:\n            experiences = self.memory.sample()\n            self.learn(experiences, GAMMA)\n\n    def act(self, state, add_noise=True):\n        """"""Returns actions for given state as per current policy.""""""\n        state = torch.from_numpy(state).float().to(device)\n        self.actor_local.eval()\n        with torch.no_grad():\n            action = self.actor_local(state).cpu().data.numpy()\n        self.actor_local.train()\n        if add_noise:\n            action += self.noise.sample()\n        return np.clip(action, -1, 1)\n\n    def reset(self):\n        self.noise.reset()\n\n    def learn(self, experiences, gamma):\n        """"""Update policy and value parameters using given batch of experience tuples.\n        Q_targets = r + \xce\xb3 * critic_target(next_state, actor_target(next_state))\n        where:\n            actor_target(state) -> action\n            critic_target(state, action) -> Q-value\n\n        Params\n        ======\n            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s\', done) tuples \n            gamma (float): discount factor\n        """"""\n        states, actions, rewards, next_states, dones = experiences\n\n        # ---------------------------- update critic ---------------------------- #\n        # Get predicted next-state actions and Q values from target models\n        actions_next = self.actor_target(next_states)\n        Q_targets_next = self.critic_target(next_states, actions_next)\n        # Compute Q targets for current states (y_i)\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n        # Compute critic loss\n        Q_expected = self.critic_local(states, actions)\n        critic_loss = F.mse_loss(Q_expected, Q_targets)\n        # Minimize the loss\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # ---------------------------- update actor ---------------------------- #\n        # Compute actor loss\n        actions_pred = self.actor_local(states)\n        actor_loss = -self.critic_local(states, actions_pred).mean()\n        # Minimize the loss\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # ----------------------- update target networks ----------------------- #\n        self.soft_update(self.critic_local, self.critic_target, TAU)\n        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n\n    def soft_update(self, local_model, target_model, tau):\n        """"""Soft update model parameters.\n        \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target\n\n        Params\n        ======\n            local_model: PyTorch model (weights will be copied from)\n            target_model: PyTorch model (weights will be copied to)\n            tau (float): interpolation parameter \n        """"""\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n\nclass OUNoise:\n    """"""Ornstein-Uhlenbeck process.""""""\n\n    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n        """"""Initialize parameters and noise process.""""""\n        self.mu = mu * np.ones(size)\n        self.theta = theta\n        self.sigma = sigma\n        self.seed = random.seed(seed)\n        self.reset()\n\n    def reset(self):\n        """"""Reset the internal state (= noise) to mean (mu).""""""\n        self.state = copy.copy(self.mu)\n\n    def sample(self):\n        """"""Update internal state and return it as a noise sample.""""""\n        x = self.state\n        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n        self.state = x + dx\n        return self.state\n\nclass ReplayBuffer:\n    """"""Fixed-size buffer to store experience tuples.""""""\n\n    def __init__(self, action_size, buffer_size, batch_size, seed):\n        """"""Initialize a ReplayBuffer object.\n        Params\n        ======\n            buffer_size (int): maximum size of buffer\n            batch_size (int): size of each training batch\n        """"""\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n        self.batch_size = batch_size\n        self.experience = namedtuple(""Experience"", field_names=[""state"", ""action"", ""reward"", ""next_state"", ""done""])\n        self.seed = random.seed(seed)\n    \n    def add(self, state, action, reward, next_state, done):\n        """"""Add a new experience to memory.""""""\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n    \n    def sample(self):\n        """"""Randomly sample a batch of experiences from memory.""""""\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n\n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        """"""Return the current size of internal memory.""""""\n        return len(self.memory)'"
ddpg-bipedal/model.py,5,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef hidden_init(layer):\n    fan_in = layer.weight.data.size()[0]\n    lim = 1. / np.sqrt(fan_in)\n    return (-lim, lim)\n\nclass Actor(nn.Module):\n    """"""Actor (Policy) Model.""""""\n\n    def __init__(self, state_size, action_size, seed, fc_units=256):\n        """"""Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n            fc1_units (int): Number of nodes in first hidden layer\n            fc2_units (int): Number of nodes in second hidden layer\n        """"""\n        super(Actor, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.fc1 = nn.Linear(state_size, fc_units)\n        self.fc2 = nn.Linear(fc_units, action_size)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n        self.fc2.weight.data.uniform_(-3e-3, 3e-3)\n\n    def forward(self, state):\n        """"""Build an actor (policy) network that maps states -> actions.""""""\n        x = F.relu(self.fc1(state))\n        return F.tanh(self.fc2(x))\n\n\nclass Critic(nn.Module):\n    """"""Critic (Value) Model.""""""\n\n    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=256, fc3_units=128):\n        """"""Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n            fcs1_units (int): Number of nodes in the first hidden layer\n            fc2_units (int): Number of nodes in the second hidden layer\n        """"""\n        super(Critic, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.fcs1 = nn.Linear(state_size, fcs1_units)\n        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n        self.fc3 = nn.Linear(fc2_units, fc3_units)\n        self.fc4 = nn.Linear(fc3_units, 1)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n        self.fc4.weight.data.uniform_(-3e-3, 3e-3)\n\n    def forward(self, state, action):\n        """"""Build a critic (value) network that maps (state, action) pairs -> Q-values.""""""\n        xs = F.leaky_relu(self.fcs1(state))\n        x = torch.cat((xs, action), dim=1)\n        x = F.leaky_relu(self.fc2(x))\n        x = F.leaky_relu(self.fc3(x))\n        return self.fc4(x)\n'"
ddpg-pendulum/ddpg_agent.py,11,"b'import numpy as np\nimport random\nimport copy\nfrom collections import namedtuple, deque\n\nfrom model import Actor, Critic\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nBUFFER_SIZE = int(1e5)  # replay buffer size\nBATCH_SIZE = 128        # minibatch size\nGAMMA = 0.99            # discount factor\nTAU = 1e-3              # for soft update of target parameters\nLR_ACTOR = 1e-4         # learning rate of the actor \nLR_CRITIC = 1e-3        # learning rate of the critic\nWEIGHT_DECAY = 0        # L2 weight decay\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nclass Agent():\n    """"""Interacts with and learns from the environment.""""""\n    \n    def __init__(self, state_size, action_size, random_seed):\n        """"""Initialize an Agent object.\n        \n        Params\n        ======\n            state_size (int): dimension of each state\n            action_size (int): dimension of each action\n            random_seed (int): random seed\n        """"""\n        self.state_size = state_size\n        self.action_size = action_size\n        self.seed = random.seed(random_seed)\n\n        # Actor Network (w/ Target Network)\n        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n\n        # Critic Network (w/ Target Network)\n        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n\n        # Noise process\n        self.noise = OUNoise(action_size, random_seed)\n\n        # Replay memory\n        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n    \n    def step(self, state, action, reward, next_state, done):\n        """"""Save experience in replay memory, and use random sample from buffer to learn.""""""\n        # Save experience / reward\n        self.memory.add(state, action, reward, next_state, done)\n\n        # Learn, if enough samples are available in memory\n        if len(self.memory) > BATCH_SIZE:\n            experiences = self.memory.sample()\n            self.learn(experiences, GAMMA)\n\n    def act(self, state, add_noise=True):\n        """"""Returns actions for given state as per current policy.""""""\n        state = torch.from_numpy(state).float().to(device)\n        self.actor_local.eval()\n        with torch.no_grad():\n            action = self.actor_local(state).cpu().data.numpy()\n        self.actor_local.train()\n        if add_noise:\n            action += self.noise.sample()\n        return np.clip(action, -1, 1)\n\n    def reset(self):\n        self.noise.reset()\n\n    def learn(self, experiences, gamma):\n        """"""Update policy and value parameters using given batch of experience tuples.\n        Q_targets = r + \xce\xb3 * critic_target(next_state, actor_target(next_state))\n        where:\n            actor_target(state) -> action\n            critic_target(state, action) -> Q-value\n\n        Params\n        ======\n            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s\', done) tuples \n            gamma (float): discount factor\n        """"""\n        states, actions, rewards, next_states, dones = experiences\n\n        # ---------------------------- update critic ---------------------------- #\n        # Get predicted next-state actions and Q values from target models\n        actions_next = self.actor_target(next_states)\n        Q_targets_next = self.critic_target(next_states, actions_next)\n        # Compute Q targets for current states (y_i)\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n        # Compute critic loss\n        Q_expected = self.critic_local(states, actions)\n        critic_loss = F.mse_loss(Q_expected, Q_targets)\n        # Minimize the loss\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # ---------------------------- update actor ---------------------------- #\n        # Compute actor loss\n        actions_pred = self.actor_local(states)\n        actor_loss = -self.critic_local(states, actions_pred).mean()\n        # Minimize the loss\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # ----------------------- update target networks ----------------------- #\n        self.soft_update(self.critic_local, self.critic_target, TAU)\n        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n\n    def soft_update(self, local_model, target_model, tau):\n        """"""Soft update model parameters.\n        \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target\n\n        Params\n        ======\n            local_model: PyTorch model (weights will be copied from)\n            target_model: PyTorch model (weights will be copied to)\n            tau (float): interpolation parameter \n        """"""\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n\nclass OUNoise:\n    """"""Ornstein-Uhlenbeck process.""""""\n\n    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n        """"""Initialize parameters and noise process.""""""\n        self.mu = mu * np.ones(size)\n        self.theta = theta\n        self.sigma = sigma\n        self.seed = random.seed(seed)\n        self.reset()\n\n    def reset(self):\n        """"""Reset the internal state (= noise) to mean (mu).""""""\n        self.state = copy.copy(self.mu)\n\n    def sample(self):\n        """"""Update internal state and return it as a noise sample.""""""\n        x = self.state\n        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n        self.state = x + dx\n        return self.state\n\nclass ReplayBuffer:\n    """"""Fixed-size buffer to store experience tuples.""""""\n\n    def __init__(self, action_size, buffer_size, batch_size, seed):\n        """"""Initialize a ReplayBuffer object.\n        Params\n        ======\n            buffer_size (int): maximum size of buffer\n            batch_size (int): size of each training batch\n        """"""\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n        self.batch_size = batch_size\n        self.experience = namedtuple(""Experience"", field_names=[""state"", ""action"", ""reward"", ""next_state"", ""done""])\n        self.seed = random.seed(seed)\n    \n    def add(self, state, action, reward, next_state, done):\n        """"""Add a new experience to memory.""""""\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n    \n    def sample(self):\n        """"""Randomly sample a batch of experiences from memory.""""""\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n\n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        """"""Return the current size of internal memory.""""""\n        return len(self.memory)'"
ddpg-pendulum/model.py,5,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef hidden_init(layer):\n    fan_in = layer.weight.data.size()[0]\n    lim = 1. / np.sqrt(fan_in)\n    return (-lim, lim)\n\nclass Actor(nn.Module):\n    """"""Actor (Policy) Model.""""""\n\n    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n        """"""Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n            fc1_units (int): Number of nodes in first hidden layer\n            fc2_units (int): Number of nodes in second hidden layer\n        """"""\n        super(Actor, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.fc1 = nn.Linear(state_size, fc1_units)\n        self.fc2 = nn.Linear(fc1_units, fc2_units)\n        self.fc3 = nn.Linear(fc2_units, action_size)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n\n    def forward(self, state):\n        """"""Build an actor (policy) network that maps states -> actions.""""""\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        return F.tanh(self.fc3(x))\n\n\nclass Critic(nn.Module):\n    """"""Critic (Value) Model.""""""\n\n    def __init__(self, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n        """"""Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n            fcs1_units (int): Number of nodes in the first hidden layer\n            fc2_units (int): Number of nodes in the second hidden layer\n        """"""\n        super(Critic, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.fcs1 = nn.Linear(state_size, fcs1_units)\n        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n        self.fc3 = nn.Linear(fc2_units, 1)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n\n    def forward(self, state, action):\n        """"""Build a critic (value) network that maps (state, action) pairs -> Q-values.""""""\n        xs = F.relu(self.fcs1(state))\n        x = torch.cat((xs, action), dim=1)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n'"
dynamic-programming/check_test.py,0,"b'import unittest\nimport copy\nfrom IPython.display import Markdown, display\nimport numpy as np\nfrom frozenlake import FrozenLakeEnv\n\ndef printmd(string):\n    display(Markdown(string))\n\ndef policy_evaluation_soln(env, policy, gamma=1, theta=1e-8):\n    V = np.zeros(env.nS)\n    while True:\n        delta = 0\n        for s in range(env.nS):\n            Vs = 0\n            for a, action_prob in enumerate(policy[s]):\n                for prob, next_state, reward, done in env.P[s][a]:\n                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n            delta = max(delta, np.abs(V[s]-Vs))\n            V[s] = Vs\n        if delta < theta:\n            break\n    return V\n\ndef q_from_v_soln(env, V, s, gamma=1):\n    q = np.zeros(env.nA)\n    for a in range(env.nA):\n        for prob, next_state, reward, done in env.P[s][a]:\n            q[a] += prob * (reward + gamma * V[next_state])\n    return q\n\ndef policy_improvement_soln(env, V, gamma=1):\n    policy = np.zeros([env.nS, env.nA]) / env.nA\n    for s in range(env.nS):\n        q = q_from_v_soln(env, V, s, gamma)\n        best_a = np.argwhere(q==np.max(q)).flatten()\n        policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n    return policy\n\ndef policy_iteration_soln(env, gamma=1, theta=1e-8):\n    policy = np.ones([env.nS, env.nA]) / env.nA\n    while True:\n        V = policy_evaluation_soln(env, policy, gamma, theta)\n        new_policy = policy_improvement_soln(env, V)\n        if (new_policy == policy).all():\n            break;\n        policy = copy.copy(new_policy)\n    return policy, V\n\nenv = FrozenLakeEnv()\nrandom_policy = np.ones([env.nS, env.nA]) / env.nA\n\nclass Tests(unittest.TestCase):\n\n    def policy_evaluation_check(self, policy_evaluation):\n        soln = policy_evaluation_soln(env, random_policy)\n        to_check = policy_evaluation(env, random_policy)\n        np.testing.assert_array_almost_equal(soln, to_check)\n\n    def q_from_v_check(self, q_from_v):\n        V = policy_evaluation_soln(env, random_policy)\n        soln = np.zeros([env.nS, env.nA])\n        to_check = np.zeros([env.nS, env.nA])\n        for s in range(env.nS):\n            soln[s] = q_from_v_soln(env, V, s)\n            to_check[s] = q_from_v(env, V, s)\n        np.testing.assert_array_almost_equal(soln, to_check)\n\n    def policy_improvement_check(self, policy_improvement):\n        V = policy_evaluation_soln(env, random_policy)\n        new_policy = policy_improvement(env, V)\n        new_V = policy_evaluation_soln(env, new_policy)\n        self.assertTrue(np.all(new_V >= V))\n\n    def policy_iteration_check(self, policy_iteration):\n        policy_soln, _ = policy_iteration_soln(env)\n        policy_to_check, _ = policy_iteration(env)\n        soln = policy_evaluation_soln(env, policy_soln)\n        to_check = policy_evaluation_soln(env, policy_to_check)\n        np.testing.assert_array_almost_equal(soln, to_check)\n\n    def truncated_policy_iteration_check(self, truncated_policy_iteration):\n        self.policy_iteration_check(truncated_policy_iteration)\n\n    def value_iteration_check(self, value_iteration):\n        self.policy_iteration_check(value_iteration)\n\ncheck = Tests()\n\ndef run_check(check_name, func):\n    try:\n        getattr(check, check_name)(func)\n    except check.failureException as e:\n        printmd(\'**<span style=""color: red;"">PLEASE TRY AGAIN</span>**\')\n        return\n    printmd(\'**<span style=""color: green;"">PASSED</span>**\')'"
dynamic-programming/frozenlake.py,0,"b'import numpy as np\nimport sys\nfrom six import StringIO, b\n\nfrom gym import utils\nfrom gym.envs.toy_text import discrete\n\nLEFT = 0\nDOWN = 1\nRIGHT = 2\nUP = 3\n\nMAPS = {\n    ""4x4"": [\n        ""SFFF"",\n        ""FHFH"",\n        ""FFFH"",\n        ""HFFG""\n    ],\n    ""8x8"": [\n        ""SFFFFFFF"",\n        ""FFFFFFFF"",\n        ""FFFHFFFF"",\n        ""FFFFFHFF"",\n        ""FFFHFFFF"",\n        ""FHHFFFHF"",\n        ""FHFFHFHF"",\n        ""FFFHFFFG""\n    ],\n}\n\nclass FrozenLakeEnv(discrete.DiscreteEnv):\n    """"""\n    Winter is here. You and your friends were tossing around a frisbee at the park\n    when you made a wild throw that left the frisbee out in the middle of the lake.\n    The water is mostly frozen, but there are a few holes where the ice has melted.\n    If you step into one of those holes, you\'ll fall into the freezing water.\n    At this time, there\'s an international frisbee shortage, so it\'s absolutely imperative that\n    you navigate across the lake and retrieve the disc.\n    However, the ice is slippery, so you won\'t always move in the direction you intend.\n    The surface is described using a grid like the following\n\n        SFFF\n        FHFH\n        FFFH\n        HFFG\n\n    S : starting point, safe\n    F : frozen surface, safe\n    H : hole, fall to your doom\n    G : goal, where the frisbee is located\n\n    The episode ends when you reach the goal or fall in a hole.\n    You receive a reward of 1 if you reach the goal, and zero otherwise.\n\n    """"""\n\n    metadata = {\'render.modes\': [\'human\', \'ansi\']}\n\n    def __init__(self, desc=None, map_name=""4x4"",is_slippery=True):\n        if desc is None and map_name is None:\n            raise ValueError(\'Must provide either desc or map_name\')\n        elif desc is None:\n            desc = MAPS[map_name]\n        self.desc = desc = np.asarray(desc,dtype=\'c\')\n        self.nrow, self.ncol = nrow, ncol = desc.shape\n\n        nA = 4\n        nS = nrow * ncol\n\n        isd = np.array(desc == b\'S\').astype(\'float64\').ravel()\n        isd /= isd.sum()\n\n        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n\n        def to_s(row, col):\n            return row*ncol + col\n        def inc(row, col, a):\n            if a==0: # left\n                col = max(col-1,0)\n            elif a==1: # down\n                row = min(row+1,nrow-1)\n            elif a==2: # right\n                col = min(col+1,ncol-1)\n            elif a==3: # up\n                row = max(row-1,0)\n            return (row, col)\n\n        for row in range(nrow):\n            for col in range(ncol):\n                s = to_s(row, col)\n                for a in range(4):\n                    li = P[s][a]\n                    letter = desc[row, col]\n                    if letter in b\'GH\':\n                        li.append((1.0, s, 0, True))\n                    else:\n                        if is_slippery:\n                            for b in [(a-1)%4, a, (a+1)%4]:\n                                newrow, newcol = inc(row, col, b)\n                                newstate = to_s(newrow, newcol)\n                                newletter = desc[newrow, newcol]\n                                done = bytes(newletter) in b\'GH\'\n                                rew = float(newletter == b\'G\')\n                                li.append((1.0/3.0, newstate, rew, done))\n                        else:\n                            newrow, newcol = inc(row, col, a)\n                            newstate = to_s(newrow, newcol)\n                            newletter = desc[newrow, newcol]\n                            done = bytes(newletter) in b\'GH\'\n                            rew = float(newletter == b\'G\')\n                            li.append((1.0, newstate, rew, done))\n        \n        # obtain one-step dynamics for dynamic programming setting\n        self.P = P\n\n        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n\n    def _render(self, mode=\'human\', close=False):\n        if close:\n            return\n        outfile = StringIO() if mode == \'ansi\' else sys.stdout\n\n        row, col = self.s // self.ncol, self.s % self.ncol\n        desc = self.desc.tolist()\n        desc = [[c.decode(\'utf-8\') for c in line] for line in desc]\n        desc[row][col] = utils.colorize(desc[row][col], ""red"", highlight=True)\n        if self.lastaction is not None:\n            outfile.write(""  ({})\\n"".format([""Left"",""Down"",""Right"",""Up""][self.lastaction]))\n        else:\n            outfile.write(""\\n"")\n        outfile.write(""\\n"".join(\'\'.join(line) for line in desc)+""\\n"")\n\n        if mode != \'human\':\n            return outfile\n'"
dynamic-programming/plot_utils.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_values(V):\n\t# reshape value function\n\tV_sq = np.reshape(V, (4,4))\n\n\t# plot the state-value function\n\tfig = plt.figure(figsize=(6, 6))\n\tax = fig.add_subplot(111)\n\tim = ax.imshow(V_sq, cmap='cool')\n\tfor (j,i),label in np.ndenumerate(V_sq):\n\t    ax.text(i, j, np.round(label, 5), ha='center', va='center', fontsize=14)\n\tplt.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n\tplt.title('State-Value Function')\n\tplt.show()\n"""
finance/ddpg_agent.py,11,"b'import numpy as np\nimport random\nimport copy\nfrom collections import namedtuple, deque\n\nfrom model import Actor, Critic\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nBUFFER_SIZE = int(1e4)  # replay buffer size\nBATCH_SIZE = 128        # minibatch size\nGAMMA = 0.99            # discount factor\nTAU = 1e-3              # for soft update of target parameters\nLR_ACTOR = 1e-4         # learning rate of the actor \nLR_CRITIC = 1e-3        # learning rate of the critic\nWEIGHT_DECAY = 0        # L2 weight decay\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nclass Agent():\n    """"""Interacts with and learns from the environment.""""""\n    \n    def __init__(self, state_size, action_size, random_seed):\n        """"""Initialize an Agent object.\n        \n        Params\n        ======\n            state_size (int): dimension of each state\n            action_size (int): dimension of each action\n            random_seed (int): random seed\n        """"""\n        self.state_size = state_size\n        self.action_size = action_size\n        self.seed = random.seed(random_seed)\n\n        # Actor Network (w/ Target Network)\n        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n\n        # Critic Network (w/ Target Network)\n        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n\n        # Noise process\n        self.noise = OUNoise(action_size, random_seed)\n\n        # Replay memory\n        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n    \n    def step(self, state, action, reward, next_state, done):\n        """"""Save experience in replay memory, and use random sample from buffer to learn.""""""\n        # Save experience / reward\n        self.memory.add(state, action, reward, next_state, done)\n\n        # Learn, if enough samples are available in memory\n        if len(self.memory) > BATCH_SIZE:\n            experiences = self.memory.sample()\n            self.learn(experiences, GAMMA)\n\n    def act(self, state, add_noise=True):\n        """"""Returns actions for given state as per current policy.""""""\n        state = torch.from_numpy(state).float().to(device)\n        self.actor_local.eval()\n        with torch.no_grad():\n            action = self.actor_local(state).cpu().data.numpy()\n        self.actor_local.train()\n        if add_noise:\n            action += self.noise.sample()\n        action = (action + 1.0) / 2.0\n        return np.clip(action, 0, 1)\n\n\n    def reset(self):\n        self.noise.reset()\n\n    def learn(self, experiences, gamma):\n        """"""Update policy and value parameters using given batch of experience tuples.\n        Q_targets = r + \xce\xb3 * critic_target(next_state, actor_target(next_state))\n        where:\n            actor_target(state) -> action\n            critic_target(state, action) -> Q-value\n\n        Params\n        ======\n            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s\', done) tuples \n            gamma (float): discount factor\n        """"""\n        states, actions, rewards, next_states, dones = experiences\n\n        # ---------------------------- update critic ---------------------------- #\n        # Get predicted next-state actions and Q values from target models\n        actions_next = self.actor_target(next_states)\n        Q_targets_next = self.critic_target(next_states, actions_next)\n        # Compute Q targets for current states (y_i)\n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n        # Compute critic loss\n        Q_expected = self.critic_local(states, actions)\n        critic_loss = F.mse_loss(Q_expected, Q_targets)\n        # Minimize the loss\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # ---------------------------- update actor ---------------------------- #\n        # Compute actor loss\n        actions_pred = self.actor_local(states)\n        actor_loss = -self.critic_local(states, actions_pred).mean()\n        # Minimize the loss\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # ----------------------- update target networks ----------------------- #\n        self.soft_update(self.critic_local, self.critic_target, TAU)\n        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n\n    def soft_update(self, local_model, target_model, tau):\n        """"""Soft update model parameters.\n        \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target\n\n        Params\n        ======\n            local_model: PyTorch model (weights will be copied from)\n            target_model: PyTorch model (weights will be copied to)\n            tau (float): interpolation parameter \n        """"""\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n\nclass OUNoise:\n    """"""Ornstein-Uhlenbeck process.""""""\n\n    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n        """"""Initialize parameters and noise process.""""""\n        self.mu = mu * np.ones(size)\n        self.theta = theta\n        self.sigma = sigma\n        self.seed = random.seed(seed)\n        self.reset()\n\n    def reset(self):\n        """"""Reset the internal state (= noise) to mean (mu).""""""\n        self.state = copy.copy(self.mu)\n\n    def sample(self):\n        """"""Update internal state and return it as a noise sample.""""""\n        x = self.state\n        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n        self.state = x + dx\n        return self.state\n\nclass ReplayBuffer:\n    """"""Fixed-size buffer to store experience tuples.""""""\n\n    def __init__(self, action_size, buffer_size, batch_size, seed):\n        """"""Initialize a ReplayBuffer object.\n        Params\n        ======\n            buffer_size (int): maximum size of buffer\n            batch_size (int): size of each training batch\n        """"""\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n        self.batch_size = batch_size\n        self.experience = namedtuple(""Experience"", field_names=[""state"", ""action"", ""reward"", ""next_state"", ""done""])\n        self.seed = random.seed(seed)\n    \n    def add(self, state, action, reward, next_state, done):\n        """"""Add a new experience to memory.""""""\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n    \n    def sample(self):\n        """"""Randomly sample a batch of experiences from memory.""""""\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n\n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        """"""Return the current size of internal memory.""""""\n        return len(self.memory)'"
finance/model.py,5,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef hidden_init(layer):\n    fan_in = layer.weight.data.size()[0]\n    lim = 1. / np.sqrt(fan_in)\n    return (-lim, lim)\n\nclass Actor(nn.Module):\n    """"""Actor (Policy) Model.""""""\n\n    def __init__(self, state_size, action_size, seed, fc1_units=24, fc2_units=48):\n        """"""Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n            fc1_units (int): Number of nodes in first hidden layer\n            fc2_units (int): Number of nodes in second hidden layer\n        """"""\n        super(Actor, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.fc1 = nn.Linear(state_size, fc1_units)\n        self.fc2 = nn.Linear(fc1_units, fc2_units)\n        self.fc3 = nn.Linear(fc2_units, action_size)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n\n    def forward(self, state):\n        """"""Build an actor (policy) network that maps states -> actions.""""""\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        return F.tanh(self.fc3(x))\n\n\nclass Critic(nn.Module):\n    """"""Critic (Value) Model.""""""\n\n    def __init__(self, state_size, action_size, seed, fcs1_units=24, fc2_units=48):\n        """"""Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n            fcs1_units (int): Number of nodes in the first hidden layer\n            fc2_units (int): Number of nodes in the second hidden layer\n        """"""\n        super(Critic, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.fcs1 = nn.Linear(state_size, fcs1_units)\n        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n        self.fc3 = nn.Linear(fc2_units, 1)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n\n    def forward(self, state, action):\n        """"""Build a critic (value) network that maps (state, action) pairs -> Q-values.""""""\n        xs = F.relu(self.fcs1(state))\n        x = torch.cat((xs, action), dim=1)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n'"
finance/syntheticChrissAlmgren.py,0,"b""import random\nimport numpy as np\nimport collections\n\n\n# ------------------------------------------------ Financial Parameters --------------------------------------------------- #\n\nANNUAL_VOLAT = 0.12                                # Annual volatility in stock price\nBID_ASK_SP = 1 / 8                                 # Bid-ask spread\nDAILY_TRADE_VOL = 5e6                              # Average Daily trading volume  \nTRAD_DAYS = 250                                    # Number of trading days in a year\nDAILY_VOLAT = ANNUAL_VOLAT / np.sqrt(TRAD_DAYS)    # Daily volatility in stock price\n\n\n# ----------------------------- Parameters for the Almgren and Chriss Optimal Execution Model ----------------------------- #\n\nTOTAL_SHARES = 1000000                                               # Total number of shares to sell\nSTARTING_PRICE = 50                                                  # Starting price per share\nLLAMBDA = 1e-6                                                       # Trader's risk aversion\nLIQUIDATION_TIME = 60                                                # How many days to sell all the shares. \nNUM_N = 60                                                           # Number of trades\nEPSILON = BID_ASK_SP / 2                                             # Fixed Cost of Selling.\nSINGLE_STEP_VARIANCE = (DAILY_VOLAT  * STARTING_PRICE) ** 2          # Calculate single step variance\nETA = BID_ASK_SP / (0.01 * DAILY_TRADE_VOL)                          # Price Impact for Each 1% of Daily Volume Traded\nGAMMA = BID_ASK_SP / (0.1 * DAILY_TRADE_VOL)                         # Permanent Impact Constant\n\n# ----------------------------------------------------------------------------------------------------------------------- #\n\n\n# Simulation Environment\n\nclass MarketEnvironment():\n    \n    def __init__(self, randomSeed = 0,\n                 lqd_time = LIQUIDATION_TIME,\n                 num_tr = NUM_N,\n                 lambd = LLAMBDA):\n        \n        # Set the random seed\n        random.seed(randomSeed)\n        \n        # Initialize the financial parameters so we can access them later\n        self.anv = ANNUAL_VOLAT\n        self.basp = BID_ASK_SP\n        self.dtv = DAILY_TRADE_VOL\n        self.dpv = DAILY_VOLAT\n        \n        # Initialize the Almgren-Chriss parameters so we can access them later\n        self.total_shares = TOTAL_SHARES\n        self.startingPrice = STARTING_PRICE\n        self.llambda = lambd\n        self.liquidation_time = lqd_time\n        self.num_n = num_tr\n        self.epsilon = EPSILON\n        self.singleStepVariance = SINGLE_STEP_VARIANCE\n        self.eta = ETA\n        self.gamma = GAMMA\n        \n        # Calculate some Almgren-Chriss parameters\n        self.tau = self.liquidation_time / self.num_n \n        self.eta_hat = self.eta - (0.5 * self.gamma * self.tau)\n        self.kappa_hat = np.sqrt((self.llambda * self.singleStepVariance) / self.eta_hat)\n        self.kappa = np.arccosh((((self.kappa_hat ** 2) * (self.tau ** 2)) / 2) + 1) / self.tau\n\n        # Set the variables for the initial state\n        self.shares_remaining = self.total_shares\n        self.timeHorizon = self.num_n\n        self.logReturns = collections.deque(np.zeros(6))\n        \n        # Set the initial impacted price to the starting price\n        self.prevImpactedPrice = self.startingPrice\n\n        # Set the initial transaction state to False\n        self.transacting = False\n        \n        # Set a variable to keep trak of the trade number\n        self.k = 0\n        \n        \n    def reset(self, seed = 0, liquid_time = LIQUIDATION_TIME, num_trades = NUM_N, lamb = LLAMBDA):\n        \n        # Initialize the environment with the given parameters\n        self.__init__(randomSeed = seed, lqd_time = liquid_time, num_tr = num_trades, lambd = lamb)\n        \n        # Set the initial state to [0,0,0,0,0,0,1,1]\n        self.initial_state = np.array(list(self.logReturns) + [self.timeHorizon / self.num_n, \\\n                                                               self.shares_remaining / self.total_shares])\n        return self.initial_state\n\n    \n    def start_transactions(self):\n        \n        # Set transactions on\n        self.transacting = True\n        \n        # Set the minimum number of stocks one can sell\n        self.tolerance = 1\n        \n        # Set the initial capture to zero\n        self.totalCapture = 0\n        \n        # Set the initial previous price to the starting price\n        self.prevPrice = self.startingPrice\n        \n        # Set the initial square of the shares to sell to zero\n        self.totalSSSQ = 0\n        \n        # Set the initial square of the remaing shares to sell to zero\n        self.totalSRSQ = 0\n        \n        # Set the initial AC utility\n        self.prevUtility = self.compute_AC_utility(self.total_shares)\n        \n\n    def step(self, action):\n        \n        # Create a class that will be used to keep track of information about the transaction\n        class Info(object):\n            pass        \n        info = Info()\n        \n        # Set the done flag to False. This indicates that we haven't sold all the shares yet.\n        info.done = False\n                \n        # During training, if the DDPG fails to sell all the stocks before the given \n        # number of trades or if the total number shares remaining is less than 1, then stop transacting,\n        # set the done Flag to True, return the current implementation shortfall, and give a negative reward.\n        # The negative reward is given in the else statement below.\n        if self.transacting and (self.timeHorizon == 0 or abs(self.shares_remaining) < self.tolerance):\n            self.transacting = False\n            info.done = True\n            info.implementation_shortfall = self.total_shares * self.startingPrice - self.totalCapture\n            info.expected_shortfall = self.get_expected_shortfall(self.total_shares)\n            info.expected_variance = self.singleStepVariance * self.tau * self.totalSRSQ\n            info.utility = info.expected_shortfall + self.llambda * info.expected_variance\n            \n        # We don't add noise before the first trade    \n        if self.k == 0:\n            info.price = self.prevImpactedPrice\n        else:\n            # Calculate the current stock price using arithmetic brownian motion\n            info.price = self.prevImpactedPrice + np.sqrt(self.singleStepVariance * self.tau) * random.normalvariate(0, 1)\n      \n        # If we are transacting, the stock price is affected by the number of shares we sell. The price evolves \n        # according to the Almgren and Chriss price dynamics model. \n        if self.transacting:\n            \n            # If action is an ndarray then extract the number from the array\n            if isinstance(action, np.ndarray):\n                action = action.item()            \n\n            # Convert the action to the number of shares to sell in the current step\n            sharesToSellNow = self.shares_remaining * action\n#             sharesToSellNow = min(self.shares_remaining * action, self.shares_remaining)\n    \n            if self.timeHorizon < 2:\n                sharesToSellNow = self.shares_remaining\n\n            # Since we are not selling fractions of shares, round up the total number of shares to sell to the nearest integer. \n            info.share_to_sell_now = np.around(sharesToSellNow)\n\n            # Calculate the permanent and temporary impact on the stock price according the AC price dynamics model\n            info.currentPermanentImpact = self.permanentImpact(info.share_to_sell_now)\n            info.currentTemporaryImpact = self.temporaryImpact(info.share_to_sell_now)\n                \n            # Apply the temporary impact on the current stock price    \n            info.exec_price = info.price - info.currentTemporaryImpact\n            \n            # Calculate the current total capture\n            self.totalCapture += info.share_to_sell_now * info.exec_price\n\n            # Calculate the log return for the current step and save it in the logReturn deque\n            self.logReturns.append(np.log(info.price/self.prevPrice))\n            self.logReturns.popleft()\n            \n            # Update the number of shares remaining\n            self.shares_remaining -= info.share_to_sell_now\n            \n            # Calculate the runnig total of the squares of shares sold and shares remaining\n            self.totalSSSQ += info.share_to_sell_now ** 2\n            self.totalSRSQ += self.shares_remaining ** 2\n                                        \n            # Update the variables required for the next step\n            self.timeHorizon -= 1\n            self.prevPrice = info.price\n            self.prevImpactedPrice = info.price - info.currentPermanentImpact\n            \n            # Calculate the reward\n            currentUtility = self.compute_AC_utility(self.shares_remaining)\n            reward = (abs(self.prevUtility) - abs(currentUtility)) / abs(self.prevUtility)\n            self.prevUtility = currentUtility\n            \n            # If all the shares have been sold calculate E, V, and U, and give a positive reward.\n            if self.shares_remaining <= 0:\n                \n                # Calculate the implementation shortfall\n                info.implementation_shortfall  = self.total_shares * self.startingPrice - self.totalCapture\n                   \n                # Set the done flag to True. This indicates that we have sold all the shares\n                info.done = True\n        else:\n            reward = 0.0\n        \n        self.k += 1\n            \n        # Set the new state\n        state = np.array(list(self.logReturns) + [self.timeHorizon / self.num_n, self.shares_remaining / self.total_shares])\n\n        return (state, np.array([reward]), info.done, info)\n\n   \n    def permanentImpact(self, sharesToSell):\n        # Calculate the permanent impact according to equations (6) and (1) of the AC paper\n        pi = self.gamma * sharesToSell\n        return pi\n\n    \n    def temporaryImpact(self, sharesToSell):\n        # Calculate the temporary impact according to equation (7) of the AC paper\n        ti = (self.epsilon * np.sign(sharesToSell)) + ((self.eta / self.tau) * sharesToSell)\n        return ti\n    \n    def get_expected_shortfall(self, sharesToSell):\n        # Calculate the expected shortfall according to equation (8) of the AC paper\n        ft = 0.5 * self.gamma * (sharesToSell ** 2)        \n        st = self.epsilon * sharesToSell\n        tt = (self.eta_hat / self.tau) * self.totalSSSQ\n        return ft + st + tt\n\n    \n    def get_AC_expected_shortfall(self, sharesToSell):\n        # Calculate the expected shortfall for the optimal strategy according to equation (20) of the AC paper\n        ft = 0.5 * self.gamma * (sharesToSell ** 2)        \n        st = self.epsilon * sharesToSell        \n        tt = self.eta_hat * (sharesToSell ** 2)       \n        nft = np.tanh(0.5 * self.kappa * self.tau) * (self.tau * np.sinh(2 * self.kappa * self.liquidation_time) \\\n                                                      + 2 * self.liquidation_time * np.sinh(self.kappa * self.tau))       \n        dft = 2 * (self.tau ** 2) * (np.sinh(self.kappa * self.liquidation_time) ** 2)   \n        fot = nft / dft       \n        return ft + st + (tt * fot)  \n        \n    \n    def get_AC_variance(self, sharesToSell):\n        # Calculate the variance for the optimal strategy according to equation (20) of the AC paper\n        ft = 0.5 * (self.singleStepVariance) * (sharesToSell ** 2)                        \n        nst  = self.tau * np.sinh(self.kappa * self.liquidation_time) * np.cosh(self.kappa * (self.liquidation_time - self.tau)) \\\n               - self.liquidation_time * np.sinh(self.kappa * self.tau)        \n        dst = (np.sinh(self.kappa * self.liquidation_time) ** 2) * np.sinh(self.kappa * self.tau)        \n        st = nst / dst\n        return ft * st\n        \n        \n    def compute_AC_utility(self, sharesToSell):    \n        # Calculate the AC Utility according to pg. 13 of the AC paper\n        if self.liquidation_time == 0:\n            return 0        \n        E = self.get_AC_expected_shortfall(sharesToSell)\n        V = self.get_AC_variance(sharesToSell)\n        return E + self.llambda * V\n    \n    \n    def get_trade_list(self):\n        # Calculate the trade list for the optimal strategy according to equation (18) of the AC paper\n        trade_list = np.zeros(self.num_n)\n        ftn = 2 * np.sinh(0.5 * self.kappa * self.tau)\n        ftd = np.sinh(self.kappa * self.liquidation_time)\n        ft = (ftn / ftd) * self.total_shares\n        for i in range(1, self.num_n + 1):       \n            st = np.cosh(self.kappa * (self.liquidation_time - (i - 0.5) * self.tau))\n            trade_list[i - 1] = st\n        trade_list *= ft\n        return trade_list\n     \n        \n    def observation_space_dimension(self):\n        # Return the dimension of the state\n        return 8\n    \n    \n    def action_space_dimension(self):\n        # Return the dimension of the action\n        return 1\n    \n    \n    def stop_transactions(self):\n        # Stop transacting\n        self.transacting = False            \n            \n           """
finance/utils.py,0,"b""import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\nimport syntheticChrissAlmgren as sca\n\nfrom statsmodels.iolib.table import SimpleTable\nfrom statsmodels.compat.python import zip_longest\nfrom statsmodels.iolib.tableformatting import fmt_2cols\n\n\ndef generate_table(left_col, right_col, table_title):\n    \n    # Do not use column headers\n    col_headers = None\n    \n    # Generate the right table\n    if right_col:\n        # Add padding\n        if len(right_col) < len(left_col):\n            right_col += [(' ', ' ')] * (len(left_col) - len(right_col))\n        elif len(right_col) > len(left_col):\n            left_col += [(' ', ' ')] * (len(right_col) - len(left_col))\n        right_col = [('%-21s' % ('  '+k), v) for k,v in right_col]\n        \n        # Generate the right table\n        gen_stubs_right, gen_data_right = zip_longest(*right_col)\n        gen_table_right = SimpleTable(gen_data_right,\n                                          col_headers,\n                                          gen_stubs_right,\n                                          title = table_title,\n                                          txt_fmt = fmt_2cols)\n    else:\n        # If there is no right table set the right table to empty\n        gen_table_right = []\n\n    # Generate the left table  \n    gen_stubs_left, gen_data_left = zip_longest(*left_col) \n    gen_table_left = SimpleTable(gen_data_left,\n                                 col_headers,\n                                 gen_stubs_left,\n                                 title = table_title,\n                                 txt_fmt = fmt_2cols)\n\n    \n    # Merge the left and right tables to make a single table\n    gen_table_left.extend_right(gen_table_right)\n    general_table = gen_table_left\n\n    return general_table\n\n\ndef get_env_param():\n    \n    # Create a simulation environment\n    env = sca.MarketEnvironment()\n\n    # Set the title for the financial parameters table\n    fp_title = 'Financial Parameters'\n\n    # Get the default financial parameters from the simulation environment\n    fp_left_col = [('Annual Volatility:', ['{:.0f}%'.format(env.anv * 100)]),\n                   ('Daily Volatility:', ['{:.1f}%'.format(env.dpv * 100)])]\n    \n    fp_right_col = [('Bid-Ask Spread:', ['{:.3f}'.format(env.basp)]),\n                    ('Daily Trading Volume:', ['{:,.0f}'.format(env.dtv)])]\n\n    # Set the title for the Almgren and Chriss Model parameters table\n    acp_title = 'Almgren and Chriss Model Parameters'\n\n    # Get the default Almgren and Chriss Model Parameters from the simulation environment\n    acp_left_col = [('Total Number of Shares to Sell:', ['{:,}'.format(env.total_shares)]),\n                    ('Starting Price per Share:', ['${:.2f}'.format(env.startingPrice)]),\n                    ('Price Impact for Each 1% of Daily Volume Traded:', ['${}'.format(env.eta)]),                    \n                    ('Number of Days to Sell All the Shares:', ['{}'.format(env.liquidation_time)]),\n                    ('Number of Trades:', ['{}'.format(env.num_n)])]\n\n    acp_right_col = [('Fixed Cost of Selling per Share:', ['${:.3f}'.format(env.epsilon)]),\n                     ('Trader\\'s Risk Aversion:', ['{}'.format(env.llambda)]),\n                     ('Permanent Impact Constant:', ['{}'.format(env.gamma)]),\n                     ('Single Step Variance:', ['{:.3f}'.format(env.singleStepVariance)]),\n                     ('Time Interval between trades:', ['{}'.format(env.tau)])]\n\n    # Generate tables with the default financial and AC Model parameters\n    fp_table = generate_table(fp_left_col, fp_right_col, fp_title)\n    acp_table = generate_table(acp_left_col, acp_right_col, acp_title)\n\n    return fp_table, acp_table\n\n\ndef plot_price_model(seed = 0, num_days = 1000):\n    \n    # Create a simulation environment\n    env = sca.MarketEnvironment()\n\n    # Reset the enviroment with the given seed\n    env.reset(seed)\n\n    # Create an array to hold the daily stock price for the given number of days\n    price_hist = np.zeros(num_days)\n\n    # Get the simulated stock price movement from the environment\n    for i in range(num_days):\n        _, _, _, info = env.step(i)    \n        price_hist[i] = info.price\n    \n    # Print Average and Standard Deviation in Stock Price\n    print('Average Stock Price: ${:,.2f}'.format(price_hist.mean()))\n    print('Standard Deviation in Stock Price: ${:,.2f}'.format(price_hist.std()))\n#     print('Standard Deviation of Random Noise: {:,.5f}'.format(np.sqrt(env.singleStepVariance * env.tau)))\n    \n    # Plot the price history for the given number of days\n    price_df = pd.DataFrame(data = price_hist,  columns = ['Stock'], dtype = 'float64')\n    ax = price_df.plot(colormap = 'cool', grid = False)\n    ax.set_facecolor(color = 'k')\n    ax = plt.gca()\n    yNumFmt = mticker.StrMethodFormatter('${x:,.2f}')\n    ax.yaxis.set_major_formatter(yNumFmt)\n    plt.ylabel('Stock Price')\n    plt.xlabel('days')\n    plt.show()\n    \n\n    \ndef get_optimal_vals(lq_time = 60, nm_trades = 60, tr_risk = 1e-6, title = ''):\n    \n    # Create a simulation environment\n    env = sca.MarketEnvironment()\n\n    # Reset the enviroment with the given parameters\n    env.reset(liquid_time = lq_time, num_trades = nm_trades, lamb = tr_risk)\n\n    # Set the title for the AC Optimal Strategy table\n    if title == '':\n        title = 'AC Optimal Strategy'\n    else:\n        title = 'AC Optimal Strategy for ' + title\n\n    # Get the AC optimal values from the environment\n    E = env.get_AC_expected_shortfall(env.total_shares)\n    V = env.get_AC_variance(env.total_shares)\n    U = env.compute_AC_utility(env.total_shares)\n\n    left_col = [('Number of Days to Sell All the Shares:', ['{}'.format(env.liquidation_time)]),\n                ('Half-Life of The Trade:', ['{:,.1f}'.format(1 / env.kappa)]),\n                ('Utility:', ['${:,.2f}'.format(U)])]\n\n    right_col = [('Initial Portfolio Value:', ['${:,.2f}'.format(env.total_shares * env.startingPrice)]),\n                 ('Expected Shortfall:', ['${:,.2f}'.format(E)]),\n                 ('Standard Deviation of Shortfall:', ['${:,.2f}'.format(np.sqrt(V))])]\n\n    # Generate the table with the AC optimal values\n    val_table = generate_table(left_col, right_col, title)\n\n    return val_table\n\n\ndef get_min_param():\n    \n    # Get the minimum impact AC parameters\n    min_impact = get_optimal_vals(lq_time = 250, nm_trades = 250, tr_risk = 1e-17, title = 'Minimum Impact')\n    \n    # Get the minimum variance AC parameters\n    min_var = get_optimal_vals(lq_time = 1, nm_trades = 1, tr_risk = 0.0058, title = 'Minimum Variance')\n    \n    return min_impact, min_var\n  \n            \ndef get_crfs(trisk):\n    \n    # Create the annotation label\n    tr_st = '{:.0e}'.format(trisk)   \n    lnum = tr_st.split('e')[0]   \n    lexp = tr_st.split('e')[1]\n    if np.abs(np.int(lexp)) < 10:\n        lexp = lexp.replace('0', '', 1)    \n    an_st = '$\\lambda = ' + lnum + ' \\\\times 10^{' + lexp + '}$'\n    \n    # Set the correction factors for the annotation label\n    if trisk >= 1e-7 and trisk <= 4e-7:\n        xcrf = 0.94\n        ycrf = 2.5\n        scrf = 0.1\n    elif trisk > 4e-7 and trisk <= 9e-7:\n        xcrf = 0.9\n        ycrf = 2.5\n        scrf = 0.06\n    elif trisk > 9e-7 and trisk <= 1e-6:\n        xcrf = 0.85\n        ycrf = 2.5\n        scrf = 0.06\n    elif trisk > 1e-6 and trisk < 2e-6:\n        xcrf = 1.2\n        ycrf = 2.5\n        scrf = 0.06\n    elif trisk >= 2e-6 and trisk < 3e-6:\n        xcrf = 0.8\n        ycrf = 2.5\n        scrf = 0.06\n    elif trisk >= 3e-6 and trisk < 4e-6:\n        xcrf = 0.7\n        ycrf = 2.5\n        scrf = 0.08\n    elif trisk >= 4e-6 and trisk < 7e-6:\n        xcrf = 1.4\n        ycrf = 2.0\n        scrf = 0.08\n    elif trisk >= 7e-6 and trisk <= 1e-5:\n        xcrf = 4.5\n        ycrf = 1.5\n        scrf = 0.08\n    elif trisk > 1e-5 and trisk <= 2e-5:\n        xcrf = 7.0\n        ycrf = 1.1\n        scrf = 0.08\n    elif trisk > 2e-5 and trisk <= 5e-5:\n        xcrf = 12.\n        ycrf = 1.1\n        scrf = 0.08\n    elif trisk > 5e-5 and trisk <= 1e-4:\n        xcrf = 30\n        ycrf = 0.99\n        scrf = 0.08\n    else:\n        xcrf = 1\n        ycrf = 1\n        scrf = 0.08\n    \n    return an_st, xcrf, ycrf, scrf\n    \n\ndef plot_efficient_frontier(tr_risk = 1e-6):\n    \n    # Create a simulation environment\n    env = sca.MarketEnvironment()\n    \n    # Reset the enviroment with the given trader's risk aversion\n    env.reset(lamb = tr_risk)\n\n    # Get the expected shortfall and corresponding variance for the given trader's risk aversion\n    tr_E = env.get_AC_expected_shortfall(env.total_shares)\n    tr_V = env.get_AC_variance(env.total_shares)\n    \n    # Create empty arrays to hold our values of E, V, and U\n    E = np.array([])\n    V = np.array([])\n    U = np.array([])\n    \n    # Set the number of plot points for our frontier\n    num_points = 7000\n    \n    # Set the values of the trader's risk aversion to plot\n    lambdas = np.linspace(1e-7, 1e-4, num_points)\n    \n    # Calclate E, V, U for each value of llambda\n    for llambda in lambdas:\n        env.reset(lamb = llambda)\n        E = np.append(E, env.get_AC_expected_shortfall(env.total_shares))\n        V = np.append(V, env.get_AC_variance(env.total_shares))\n        U = np.append(U, env.compute_AC_utility(env.total_shares))\n        \n    # Plot E vs V and use U for the colorbar    \n    cm = plt.cm.get_cmap('gist_rainbow')    \n    sc = plt.scatter(V, E, s = 20, c = U, cmap = cm)\n    plt.colorbar(sc, label = 'AC Utility', format = mticker.StrMethodFormatter('${x:,.0f}'))\n    ax = plt.gca()\n    ax.set_facecolor('k')\n    ymin = E.min() * 0.7\n    ymax = E.max() * 1.1\n    plt.ylim(ymin, ymax)\n    yNumFmt = mticker.StrMethodFormatter('${x:,.0f}')\n    xNumFmt = mticker.StrMethodFormatter('{x:,.0f}')\n    ax.yaxis.set_major_formatter(yNumFmt)\n    ax.xaxis.set_major_formatter(xNumFmt)\n    plt.xlabel('Variance of Shortfall')\n    plt.ylabel('Expected Shortfall')\n    \n    # Get the annotation label and the correction factors\n    an_st, xcrf, ycrf, scrf = get_crfs(tr_risk)\n    \n    # Plot the annotation in the above plot\n    plt.annotate(an_st, xy = (tr_V, tr_E), xytext = (tr_V * xcrf, tr_E  * ycrf), color = 'w', size = 'large', \n                 arrowprops = dict(facecolor = 'cyan', shrink = scrf, width = 3, headwidth = 10))\n    plt.show()\n    \n    \ndef round_trade_list(trl):\n    \n    # Round the shares in the trading list\n    trl_rd = np.around(trl)\n        \n    # Rounding the number of shares in the trading list sometimes results in selling more or less\n    # shares than we have available. We calculate the difference between to total number of shares\n    # sold in the original trading list and the number of shares sold in the rounded list.\n    # This difference will be used to correct for rounding errors. \n    res = np.around(trl.sum() - trl_rd.sum())\n        \n    # Correct the number of shares sold due to rounding errors if necessary\n    if res != 0:\n        idx = trl_rd.nonzero()[0][-1]      \n        trl_rd[idx] += res\n        \n    return trl_rd\n\n    \ndef plot_trade_list(lq_time = 60, nm_trades = 60, tr_risk = 1e-6, show_trl = False):\n    \n    # Create simulation environment\n    env = sca.MarketEnvironment()\n\n    # Reset the environment with the given parameters\n    env.reset(liquid_time = lq_time, num_trades = nm_trades, lamb = tr_risk)\n\n    # Get the trading list from the environment\n    trade_list = env.get_trade_list()\n    \n    # Add a zero at the beginning of the trade list to indicate that at time 0 we don't sell any stocks\n    new_trl = np.insert(trade_list, 0, 0)\n\n    # We create a dataframe with the trading list and trading trajectory\n    df = pd.DataFrame(data = list(range(nm_trades + 1)),  columns = ['Trade Number'], dtype = 'float64')\n    df['Stocks Sold'] = new_trl\n    df['Stocks Remaining'] = (np.ones(nm_trades + 1) * env.total_shares) - np.cumsum(new_trl)\n\n    # Create a figure with 2 plots in 1 row\n    fig, axes = plt.subplots(nrows = 1, ncols = 2)\n    \n    # Make a scatter plot of the trade list\n    df.iloc[1:].plot.scatter(x = 'Trade Number', y = 'Stocks Sold', c = 'Stocks Sold', colormap = 'gist_rainbow',\n                                                 alpha = 1, sharex = False, s = 50, colorbar = False, ax = axes[0])\n    \n    # Plot a line through the points of the scatter plot of the trade list\n    axes[0].plot(df['Trade Number'].iloc[1:], df['Stocks Sold'].iloc[1:], linewidth = 2.0, alpha = 0.5)\n    axes[0].set_facecolor(color = 'k')\n    yNumFmt = mticker.StrMethodFormatter('{x:,.0f}')\n    axes[0].yaxis.set_major_formatter(yNumFmt)\n    axes[0].set_title('Trading List')\n\n    # Make a scatter plot of the number of stocks remaining after each trade\n    df.plot.scatter(x = 'Trade Number', y = 'Stocks Remaining', c = 'Stocks Remaining', colormap = 'gist_rainbow',\n                                                 alpha = 1, sharex = False, s = 50, colorbar = False, ax = axes[1])\n    \n    # Plot a line through the points of the scatter plot of the number of stocks remaining after each trade\n    axes[1].plot(df['Trade Number'], df['Stocks Remaining'], linewidth = 2.0, alpha = 0.5)\n    axes[1].set_facecolor(color = 'k')\n    yNumFmt = mticker.StrMethodFormatter('{x:,.0f}')\n    axes[1].yaxis.set_major_formatter(yNumFmt)\n    axes[1].set_title('Trading Trajectory')\n    \n    # Set the spacing between plots\n    plt.subplots_adjust(wspace = 0.4)\n    plt.show()\n    \n    print('\\nNumber of Shares Sold: {:,.0f}\\n'.format(new_trl.sum()))\n    \n    if show_trl:\n        \n        # Since we are not selling fractional shares we round up the shares in the trading list\n        rd_trl = round_trade_list(new_trl)\n#         rd_trl = new_trl\n\n        # We create a dataframe with the modified trading list and trading trajectory\n        df2 = pd.DataFrame(data = list(range(nm_trades + 1)),  columns = ['Trade Number'], dtype = 'float64')\n        df2['Stocks Sold'] = rd_trl\n        df2['Stocks Remaining'] = (np.ones(nm_trades + 1) * env.total_shares) - np.cumsum(rd_trl)\n\n        return df2.style.hide_index().format({'Trade Number': '{:.0f}', 'Stocks Sold': '{:,.0f}', 'Stocks Remaining': '{:,.0f}'})\n#         return df2.style.hide_index().format({'Trade Number': '{:.0f}', 'Stocks Sold': '{:e}', 'Stocks Remaining': '{:e}'})\n    \n\ndef implement_trade_list(seed = 0, lq_time = 60, nm_trades = 60, tr_risk = 1e-6):\n    \n    # Create simulation environment\n    env = sca.MarketEnvironment()\n\n    # Reset the environment with the given parameters\n    env.reset(seed = seed, liquid_time = lq_time, num_trades = nm_trades, lamb = tr_risk)\n\n    # Get the trading list from the environment\n    trl = env.get_trade_list()\n    \n    # Since we are not selling fractional shares we round up the shares in the trading list\n    trade_list = round_trade_list(trl)\n \n    # set the environment to make transactions\n    env.start_transactions()\n    \n    # Create an array to hold the impacted stock price\n    price_hist = np.array([])\n\n    # Implement the trading list in our similation environment\n    for trade in trade_list:\n        \n        # Convert the number of shares to sell in each trade into an action\n        action = trade / env.shares_remaining\n        \n        # Take a step in the environment my selling the number of shares in the current trade\n        _, _, _, info = env.step(action)\n        \n        # Get the impacted price from the environment\n        price_hist = np.append(price_hist, info.exec_price)\n        \n        # If all shares have been sold, stop making transactions and get the implementation sortfall\n        if info.done:\n            print('Implementation Shortfall: ${:,.2f} \\n'.format(info.implementation_shortfall))\n            break\n\n    # Plot the impacted price\n    price_df = pd.DataFrame(data = price_hist,  columns = ['Stock'], dtype = 'float64')\n    ax = price_df.plot(colormap = 'cool', grid = False)\n    ax.set_facecolor(color = 'k')\n    ax.set_title('Impacted Stock Price')\n    ax = plt.gca()\n    yNumFmt = mticker.StrMethodFormatter('${x:,.2f}')\n    ax.yaxis.set_major_formatter(yNumFmt)\n    plt.plot(price_hist, 'o')\n    plt.ylabel('Stock Price')\n    plt.xlabel('Trade Number')\n    plt.show()\n\n\ndef get_av_std(lq_time = 60, nm_trades = 60, tr_risk = 1e-6, trs = 100):\n    \n    # Create simulation environment\n    env = sca.MarketEnvironment()\n\n    # Reset the enviroment\n    env.reset(liquid_time = lq_time, num_trades = nm_trades, lamb = tr_risk)\n\n    # Get the trading list\n    trl = env.get_trade_list()\n\n    # Since we are not selling fractional shares we round up the shares in the trading list\n    trade_list = round_trade_list(trl)\n\n    # Set the initial shortfall to zero\n    shortfall_hist = np.array([])\n\n    for episode in range(trs):\n        \n        # Print current episode every 100 episodes\n        if (episode + 1) % 100 == 0:\n            print('Episode [{}/{}]'.format(episode + 1, trs), end = '\\r', flush = True)\n        \n        # Reset the enviroment\n        env.reset(seed = episode, liquid_time = lq_time, num_trades = nm_trades, lamb = tr_risk)\n\n        # set the environment to make transactions\n        env.start_transactions()\n\n        for trade in trade_list:\n            action = trade / env.shares_remaining\n            _, _, _, info = env.step(action)\n\n            if info.done:\n                shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n                break\n\n    print('Average Implementation Shortfall: ${:,.2f}'.format(shortfall_hist.mean()))\n    print('Standard Deviation of the Implementation Shortfall: ${:,.2f}'.format(shortfall_hist.std()))\n    \n    plt.plot(shortfall_hist, 'cyan', label='')\n    plt.xlim(0, trs)\n    ax = plt.gca()\n    ax.set_facecolor('k')\n    ax.set_xlabel('Episode', fontsize = 15)\n    ax.set_ylabel('Implementation Shortfall (US $)', fontsize = 15)\n    ax.axhline(shortfall_hist.mean(),0, 1, color = 'm', label='Average')\n    yNumFmt = mticker.StrMethodFormatter('${x:,.0f}')\n    ax.yaxis.set_major_formatter(yNumFmt)\n    plt.legend()\n    plt.show"""
lab-taxi/agent.py,0,"b'import numpy as np\nfrom collections import defaultdict\n\nclass Agent:\n\n    def __init__(self, nA=6):\n        """""" Initialize agent.\n\n        Params\n        ======\n        - nA: number of actions available to the agent\n        """"""\n        self.nA = nA\n        self.Q = defaultdict(lambda: np.zeros(self.nA))\n\n    def select_action(self, state):\n        """""" Given the state, select an action.\n\n        Params\n        ======\n        - state: the current state of the environment\n\n        Returns\n        =======\n        - action: an integer, compatible with the task\'s action space\n        """"""\n        return np.random.choice(self.nA)\n\n    def step(self, state, action, reward, next_state, done):\n        """""" Update the agent\'s knowledge, using the most recently sampled tuple.\n\n        Params\n        ======\n        - state: the previous state of the environment\n        - action: the agent\'s previous choice of action\n        - reward: last reward received\n        - next_state: the current state of the environment\n        - done: whether the episode is complete (True or False)\n        """"""\n        self.Q[state][action] += 1'"
lab-taxi/main.py,0,"b""from agent import Agent\nfrom monitor import interact\nimport gym\nimport numpy as np\n\nenv = gym.make('Taxi-v2')\nagent = Agent()\navg_rewards, best_avg_reward = interact(env, agent)"""
lab-taxi/monitor.py,0,"b'from collections import deque\nimport sys\nimport math\nimport numpy as np\n\ndef interact(env, agent, num_episodes=20000, window=100):\n    """""" Monitor agent\'s performance.\n    \n    Params\n    ======\n    - env: instance of OpenAI Gym\'s Taxi-v1 environment\n    - agent: instance of class Agent (see Agent.py for details)\n    - num_episodes: number of episodes of agent-environment interaction\n    - window: number of episodes to consider when calculating average rewards\n\n    Returns\n    =======\n    - avg_rewards: deque containing average rewards\n    - best_avg_reward: largest value in the avg_rewards deque\n    """"""\n    # initialize average rewards\n    avg_rewards = deque(maxlen=num_episodes)\n    # initialize best average reward\n    best_avg_reward = -math.inf\n    # initialize monitor for most recent rewards\n    samp_rewards = deque(maxlen=window)\n    # for each episode\n    for i_episode in range(1, num_episodes+1):\n        # begin the episode\n        state = env.reset()\n        # initialize the sampled reward\n        samp_reward = 0\n        while True:\n            # agent selects an action\n            action = agent.select_action(state)\n            # agent performs the selected action\n            next_state, reward, done, _ = env.step(action)\n            # agent performs internal updates based on sampled experience\n            agent.step(state, action, reward, next_state, done)\n            # update the sampled reward\n            samp_reward += reward\n            # update the state (s <- s\') to next time step\n            state = next_state\n            if done:\n                # save final sampled reward\n                samp_rewards.append(samp_reward)\n                break\n        if (i_episode >= 100):\n            # get average reward from last 100 episodes\n            avg_reward = np.mean(samp_rewards)\n            # append to deque\n            avg_rewards.append(avg_reward)\n            # update best average reward\n            if avg_reward > best_avg_reward:\n                best_avg_reward = avg_reward\n        # monitor progress\n        print(""\\rEpisode {}/{} || Best average reward {}"".format(i_episode, num_episodes, best_avg_reward), end="""")\n        sys.stdout.flush()\n        # check if task is solved (according to OpenAI Gym)\n        if best_avg_reward >= 9.7:\n            print(\'\\nEnvironment solved in {} episodes.\'.format(i_episode), end="""")\n            break\n        if i_episode == num_episodes: print(\'\\n\')\n    return avg_rewards, best_avg_reward'"
monte-carlo/plot_utils.py,0,"b'import numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef plot_blackjack_values(V):\n\n    def get_Z(x, y, usable_ace):\n        if (x,y,usable_ace) in V:\n            return V[x,y,usable_ace]\n        else:\n            return 0\n\n    def get_figure(usable_ace, ax):\n        x_range = np.arange(11, 22)\n        y_range = np.arange(1, 11)\n        X, Y = np.meshgrid(x_range, y_range)\n        \n        Z = np.array([get_Z(x,y,usable_ace) for x,y in zip(np.ravel(X), np.ravel(Y))]).reshape(X.shape)\n\n        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0)\n        ax.set_xlabel(\'Player\\\'s Current Sum\')\n        ax.set_ylabel(\'Dealer\\\'s Showing Card\')\n        ax.set_zlabel(\'State Value\')\n        ax.view_init(ax.elev, -120)\n\n    fig = plt.figure(figsize=(20, 20))\n    ax = fig.add_subplot(211, projection=\'3d\')\n    ax.set_title(\'Usable Ace\')\n    get_figure(True, ax)\n    ax = fig.add_subplot(212, projection=\'3d\')\n    ax.set_title(\'No Usable Ace\')\n    get_figure(False, ax)\n    plt.show()\n\ndef plot_policy(policy):\n\n    def get_Z(x, y, usable_ace):\n        if (x,y,usable_ace) in policy:\n            return policy[x,y,usable_ace]\n        else:\n            return 1\n\n    def get_figure(usable_ace, ax):\n        x_range = np.arange(11, 22)\n        y_range = np.arange(10, 0, -1)\n        X, Y = np.meshgrid(x_range, y_range)\n        Z = np.array([[get_Z(x,y,usable_ace) for x in x_range] for y in y_range])\n        surf = ax.imshow(Z, cmap=plt.get_cmap(\'Pastel2\', 2), vmin=0, vmax=1, extent=[10.5, 21.5, 0.5, 10.5])\n        plt.xticks(x_range)\n        plt.yticks(y_range)\n        plt.gca().invert_yaxis()\n        ax.set_xlabel(\'Player\\\'s Current Sum\')\n        ax.set_ylabel(\'Dealer\\\'s Showing Card\')\n        ax.grid(color=\'w\', linestyle=\'-\', linewidth=1)\n        divider = make_axes_locatable(ax)\n        cax = divider.append_axes(""right"", size=""5%"", pad=0.1)\n        cbar = plt.colorbar(surf, ticks=[0,1], cax=cax)\n        cbar.ax.set_yticklabels([\'0 (STICK)\',\'1 (HIT)\'])\n            \n    fig = plt.figure(figsize=(15, 15))\n    ax = fig.add_subplot(121)\n    ax.set_title(\'Usable Ace\')\n    get_figure(True, ax)\n    ax = fig.add_subplot(122)\n    ax.set_title(\'No Usable Ace\')\n    get_figure(False, ax)\n    plt.show()'"
python/learn.py,0,"b'# # Unity ML-Agents Toolkit\n# ## ML-Agent Learning\n\nimport logging\n\nimport os\nfrom docopt import docopt\n\nfrom unitytrainers.trainer_controller import TrainerController\n\n\nif __name__ == \'__main__\':\n    print(\'\'\'\n    \n                    \xe2\x96\x84\xe2\x96\x84\xe2\x96\x84\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\n               \xe2\x95\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x88\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\n          ,\xe2\x96\x84\xe2\x96\x84\xe2\x96\x84m\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\'  ,\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x80\xe2\x96\x93\xe2\x96\x93\xe2\x96\x84                           \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93  \xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c\n        \xe2\x96\x84\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x80\'      \xe2\x96\x84\xe2\x96\x93\xe2\x96\x93\xe2\x96\x80  \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93      \xe2\x96\x84\xe2\x96\x84     \xe2\x96\x84\xe2\x96\x84 ,\xe2\x96\x84\xe2\x96\x84 \xe2\x96\x84\xe2\x96\x84\xe2\x96\x84\xe2\x96\x84   ,\xe2\x96\x84\xe2\x96\x84 \xe2\x96\x84\xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c\xe2\x96\x84 \xe2\x96\x84\xe2\x96\x84\xe2\x96\x84    ,\xe2\x96\x84\xe2\x96\x84\n      \xe2\x96\x84\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x80        \xe2\x96\x84\xe2\x96\x93\xe2\x96\x93\xe2\x96\x80   \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c     \xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c   \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93 \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93 \xe2\x96\x80\xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c\xe2\x96\x80 ^\xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c  \xe2\x95\x92\xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c\n    \xe2\x96\x84\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x84\xe2\x96\x84\xe2\x96\x84\xe2\x96\x84\xe2\x96\x84\xe2\x96\x84\xe2\x96\x84\xe2\x96\x84\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93      \xe2\x96\x93\xe2\x96\x80      \xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c   \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93 \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93    \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93 \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93  \xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c   \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93\xe2\x96\x84 \xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c\n    \xe2\x96\x80\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x93\xe2\x96\x93\xe2\x96\x84     \xe2\x96\x93\xe2\x96\x93      \xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c   \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93 \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93    \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93 \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93  \xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c    \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93\xe2\x96\x90\xe2\x96\x93\xe2\x96\x93\n      ^\xe2\x96\x88\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93        \xe2\x96\x80\xe2\x96\x93\xe2\x96\x93\xe2\x96\x84   \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c     \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x84\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93 \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93    \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93 \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93  \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x84    \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93`\n        \'\xe2\x96\x80\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x84      ^\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93  \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93       \xe2\x94\x94\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80 \xe2\x96\x80\xe2\x96\x80 ^\xe2\x96\x80\xe2\x96\x80    `\xe2\x96\x80\xe2\x96\x80 `\xe2\x96\x80\xe2\x96\x80   \'\xe2\x96\x80\xe2\x96\x80    \xe2\x96\x90\xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c\n           \xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x93\xe2\x96\x84\xe2\x96\x84\xe2\x96\x84   \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93,                                      \xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x80\n               `\xe2\x96\x80\xe2\x96\x88\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x93\xe2\x96\x8c\n                    \xc2\xac`\xe2\x96\x80\xe2\x96\x80\xe2\x96\x80\xe2\x96\x88\xe2\x96\x93\n                    \n\'\'\')\n\n    logger = logging.getLogger(""unityagents"")\n    _USAGE = \'\'\'\n    Usage:\n      learn (<env>) [options]\n      learn [options]\n      learn --help\n\n    Options:\n      --curriculum=<file>        Curriculum json file for environment [default: None].\n      --keep-checkpoints=<n>     How many model checkpoints to keep [default: 5].\n      --lesson=<n>               Start learning from this lesson [default: 0].\n      --load                     Whether to load the model or randomly initialize [default: False].\n      --run-id=<path>            The sub-directory name for model and summary statistics [default: ppo]. \n      --save-freq=<n>            Frequency at which to save model [default: 50000].\n      --seed=<n>                 Random seed used for training [default: -1].\n      --slow                     Whether to run the game at training speed [default: False].\n      --train                    Whether to train model, or only run inference [default: False].\n      --worker-id=<n>            Number to add to communication port (5005). Used for multi-environment [default: 0].\n      --docker-target-name=<dt>  Docker Volume to store curriculum, executable and model files [default: Empty].\n      --no-graphics              Whether to run the Unity simulator in no-graphics mode [default: False].\n    \'\'\'\n\n    options = docopt(_USAGE)\n    logger.info(options)\n    # Docker Parameters\n    if options[\'--docker-target-name\'] == \'Empty\':\n        docker_target_name = \'\'\n    else:\n        docker_target_name = options[\'--docker-target-name\']\n\n    # General parameters\n    run_id = options[\'--run-id\']\n    seed = int(options[\'--seed\'])\n    load_model = options[\'--load\']\n    train_model = options[\'--train\']\n    save_freq = int(options[\'--save-freq\'])\n    env_path = options[\'<env>\']\n    keep_checkpoints = int(options[\'--keep-checkpoints\'])\n    worker_id = int(options[\'--worker-id\'])\n    curriculum_file = str(options[\'--curriculum\'])\n    if curriculum_file == ""None"":\n        curriculum_file = None\n    lesson = int(options[\'--lesson\'])\n    fast_simulation = not bool(options[\'--slow\'])\n    no_graphics = options[\'--no-graphics\']\n\n    # Constants\n    # Assumption that this yaml is present in same dir as this file\n    base_path = os.path.dirname(__file__)\n    TRAINER_CONFIG_PATH = os.path.abspath(os.path.join(base_path, ""trainer_config.yaml""))\n\n    tc = TrainerController(env_path, run_id, save_freq, curriculum_file, fast_simulation, load_model, train_model,\n                           worker_id, keep_checkpoints, lesson, seed, docker_target_name, TRAINER_CONFIG_PATH,\n                           no_graphics)\n    tc.start_learning()\n'"
python/setup.py,0,"b'#!/usr/bin/env python\n\nfrom setuptools import setup, Command, find_packages\n\n\nwith open(\'requirements.txt\') as f:\n    required = f.read().splitlines()\n\nsetup(name=\'unityagents\',\n      version=\'0.4.0\',\n      description=\'Unity Machine Learning Agents\',\n      license=\'Apache License 2.0\',\n      author=\'Unity Technologies\',\n      author_email=\'ML-Agents@unity3d.com\',\n      url=\'https://github.com/Unity-Technologies/ml-agents\',\n      packages=find_packages(),\n      install_requires = required,\n      long_description= (""Unity Machine Learning Agents allows researchers and developers ""\n       ""to transform games and simulations created using the Unity Editor into environments ""\n       ""where intelligent agents can be trained using reinforcement learning, evolutionary "" \n       ""strategies, or other machine learning methods through a simple to use Python API."")\n     )\n'"
temporal-difference/check_test.py,0,"b'import unittest \nfrom IPython.display import Markdown, display\nimport numpy as np\n\ndef printmd(string):\n    display(Markdown(string))\n\nV_opt = np.zeros((4,12))\nV_opt[0:13][0] = -np.arange(3, 15)[::-1]\nV_opt[0:13][1] = -np.arange(3, 15)[::-1] + 1\nV_opt[0:13][2] = -np.arange(3, 15)[::-1] + 2\nV_opt[3][0] = -13\n\npol_opt = np.hstack((np.ones(11), 2, 0))\n\nV_true = np.zeros((4,12))\nfor i in range(3):\n    V_true[0:13][i] = -np.arange(3, 15)[::-1] - i\nV_true[1][11] = -2\nV_true[2][11] = -1\nV_true[3][0] = -17\n\ndef get_long_path(V):\n    return np.array(np.hstack((V[0:13][0], V[1][0], V[1][11], V[2][0], V[2][11], V[3][0], V[3][11])))\n\ndef get_optimal_path(policy):\n    return np.array(np.hstack((policy[2][:], policy[3][0])))\n\nclass Tests(unittest.TestCase):\n\n    def td_prediction_check(self, V):\n        to_check = get_long_path(V)\n        soln = get_long_path(V_true)\n        np.testing.assert_array_almost_equal(soln, to_check)\n\n    def td_control_check(self, policy):\n        to_check = get_optimal_path(policy)\n        np.testing.assert_equal(pol_opt, to_check)\n\ncheck = Tests()\n\ndef run_check(check_name, func):\n    try:\n        getattr(check, check_name)(func)\n    except check.failureException as e:\n        printmd(\'**<span style=""color: red;"">PLEASE TRY AGAIN</span>**\')\n        return\n    printmd(\'**<span style=""color: green;"">PASSED</span>**\')'"
temporal-difference/plot_utils.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(""white"")\n\ndef plot_values(V):\n\t# reshape the state-value function\n\tV = np.reshape(V, (4,12))\n\t# plot the state-value function\n\tfig = plt.figure(figsize=(15,5))\n\tax = fig.add_subplot(111)\n\tim = ax.imshow(V, cmap=\'cool\')\n\tfor (j,i),label in np.ndenumerate(V):\n\t    ax.text(i, j, np.round(label,3), ha=\'center\', va=\'center\', fontsize=14)\n\tplt.tick_params(bottom=\'off\', left=\'off\', labelbottom=\'off\', labelleft=\'off\')\n\tplt.title(\'State-Value Function\')\n\tplt.show()'"
dqn/exercise/dqn_agent.py,11,"b'import numpy as np\nimport random\nfrom collections import namedtuple, deque\n\nfrom model import QNetwork\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nBUFFER_SIZE = int(1e5)  # replay buffer size\nBATCH_SIZE = 64         # minibatch size\nGAMMA = 0.99            # discount factor\nTAU = 1e-3              # for soft update of target parameters\nLR = 5e-4               # learning rate \nUPDATE_EVERY = 4        # how often to update the network\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nclass Agent():\n    """"""Interacts with and learns from the environment.""""""\n\n    def __init__(self, state_size, action_size, seed):\n        """"""Initialize an Agent object.\n        \n        Params\n        ======\n            state_size (int): dimension of each state\n            action_size (int): dimension of each action\n            seed (int): random seed\n        """"""\n        self.state_size = state_size\n        self.action_size = action_size\n        self.seed = random.seed(seed)\n\n        # Q-Network\n        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n\n        # Replay memory\n        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n        # Initialize time step (for updating every UPDATE_EVERY steps)\n        self.t_step = 0\n    \n    def step(self, state, action, reward, next_state, done):\n        # Save experience in replay memory\n        self.memory.add(state, action, reward, next_state, done)\n        \n        # Learn every UPDATE_EVERY time steps.\n        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n        if self.t_step == 0:\n            # If enough samples are available in memory, get random subset and learn\n            if len(self.memory) > BATCH_SIZE:\n                experiences = self.memory.sample()\n                self.learn(experiences, GAMMA)\n\n    def act(self, state, eps=0.):\n        """"""Returns actions for given state as per current policy.\n        \n        Params\n        ======\n            state (array_like): current state\n            eps (float): epsilon, for epsilon-greedy action selection\n        """"""\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        self.qnetwork_local.eval()\n        with torch.no_grad():\n            action_values = self.qnetwork_local(state)\n        self.qnetwork_local.train()\n\n        # Epsilon-greedy action selection\n        if random.random() > eps:\n            return np.argmax(action_values.cpu().data.numpy())\n        else:\n            return random.choice(np.arange(self.action_size))\n\n    def learn(self, experiences, gamma):\n        """"""Update value parameters using given batch of experience tuples.\n\n        Params\n        ======\n            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s\', done) tuples \n            gamma (float): discount factor\n        """"""\n        states, actions, rewards, next_states, dones = experiences\n\n        ## TODO: compute and minimize the loss\n        ""*** YOUR CODE HERE ***""\n\n        # ------------------- update target network ------------------- #\n        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n\n    def soft_update(self, local_model, target_model, tau):\n        """"""Soft update model parameters.\n        \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target\n\n        Params\n        ======\n            local_model (PyTorch model): weights will be copied from\n            target_model (PyTorch model): weights will be copied to\n            tau (float): interpolation parameter \n        """"""\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n\n\nclass ReplayBuffer:\n    """"""Fixed-size buffer to store experience tuples.""""""\n\n    def __init__(self, action_size, buffer_size, batch_size, seed):\n        """"""Initialize a ReplayBuffer object.\n\n        Params\n        ======\n            action_size (int): dimension of each action\n            buffer_size (int): maximum size of buffer\n            batch_size (int): size of each training batch\n            seed (int): random seed\n        """"""\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)  \n        self.batch_size = batch_size\n        self.experience = namedtuple(""Experience"", field_names=[""state"", ""action"", ""reward"", ""next_state"", ""done""])\n        self.seed = random.seed(seed)\n    \n    def add(self, state, action, reward, next_state, done):\n        """"""Add a new experience to memory.""""""\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n    \n    def sample(self):\n        """"""Randomly sample a batch of experiences from memory.""""""\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n  \n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        """"""Return the current size of internal memory.""""""\n        return len(self.memory)'"
dqn/exercise/model.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass QNetwork(nn.Module):\n    """"""Actor (Policy) Model.""""""\n\n    def __init__(self, state_size, action_size, seed):\n        """"""Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n        """"""\n        super(QNetwork, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        ""*** YOUR CODE HERE ***""\n\n    def forward(self, state):\n        """"""Build a network that maps state -> action values.""""""\n        pass\n'"
dqn/solution/dqn_agent.py,11,"b'import numpy as np\nimport random\nfrom collections import namedtuple, deque\n\nfrom model import QNetwork\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nBUFFER_SIZE = int(1e5)  # replay buffer size\nBATCH_SIZE = 64         # minibatch size\nGAMMA = 0.99            # discount factor\nTAU = 1e-3              # for soft update of target parameters\nLR = 5e-4               # learning rate \nUPDATE_EVERY = 4        # how often to update the network\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nclass Agent():\n    """"""Interacts with and learns from the environment.""""""\n\n    def __init__(self, state_size, action_size, seed):\n        """"""Initialize an Agent object.\n        \n        Params\n        ======\n            state_size (int): dimension of each state\n            action_size (int): dimension of each action\n            seed (int): random seed\n        """"""\n        self.state_size = state_size\n        self.action_size = action_size\n        self.seed = random.seed(seed)\n\n        # Q-Network\n        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n\n        # Replay memory\n        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n        # Initialize time step (for updating every UPDATE_EVERY steps)\n        self.t_step = 0\n    \n    def step(self, state, action, reward, next_state, done):\n        # Save experience in replay memory\n        self.memory.add(state, action, reward, next_state, done)\n        \n        # Learn every UPDATE_EVERY time steps.\n        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n        if self.t_step == 0:\n            # If enough samples are available in memory, get random subset and learn\n            if len(self.memory) > BATCH_SIZE:\n                experiences = self.memory.sample()\n                self.learn(experiences, GAMMA)\n\n    def act(self, state, eps=0.):\n        """"""Returns actions for given state as per current policy.\n        \n        Params\n        ======\n            state (array_like): current state\n            eps (float): epsilon, for epsilon-greedy action selection\n        """"""\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        self.qnetwork_local.eval()\n        with torch.no_grad():\n            action_values = self.qnetwork_local(state)\n        self.qnetwork_local.train()\n\n        # Epsilon-greedy action selection\n        if random.random() > eps:\n            return np.argmax(action_values.cpu().data.numpy())\n        else:\n            return random.choice(np.arange(self.action_size))\n\n    def learn(self, experiences, gamma):\n        """"""Update value parameters using given batch of experience tuples.\n\n        Params\n        ======\n            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s\', done) tuples \n            gamma (float): discount factor\n        """"""\n        states, actions, rewards, next_states, dones = experiences\n\n        # Get max predicted Q values (for next states) from target model\n        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n        # Compute Q targets for current states \n        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n\n        # Get expected Q values from local model\n        Q_expected = self.qnetwork_local(states).gather(1, actions)\n\n        # Compute loss\n        loss = F.mse_loss(Q_expected, Q_targets)\n        # Minimize the loss\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # ------------------- update target network ------------------- #\n        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n\n    def soft_update(self, local_model, target_model, tau):\n        """"""Soft update model parameters.\n        \xce\xb8_target = \xcf\x84*\xce\xb8_local + (1 - \xcf\x84)*\xce\xb8_target\n\n        Params\n        ======\n            local_model (PyTorch model): weights will be copied from\n            target_model (PyTorch model): weights will be copied to\n            tau (float): interpolation parameter \n        """"""\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n\n\nclass ReplayBuffer:\n    """"""Fixed-size buffer to store experience tuples.""""""\n\n    def __init__(self, action_size, buffer_size, batch_size, seed):\n        """"""Initialize a ReplayBuffer object.\n\n        Params\n        ======\n            action_size (int): dimension of each action\n            buffer_size (int): maximum size of buffer\n            batch_size (int): size of each training batch\n            seed (int): random seed\n        """"""\n        self.action_size = action_size\n        self.memory = deque(maxlen=buffer_size)  \n        self.batch_size = batch_size\n        self.experience = namedtuple(""Experience"", field_names=[""state"", ""action"", ""reward"", ""next_state"", ""done""])\n        self.seed = random.seed(seed)\n    \n    def add(self, state, action, reward, next_state, done):\n        """"""Add a new experience to memory.""""""\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n    \n    def sample(self):\n        """"""Randomly sample a batch of experiences from memory.""""""\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n  \n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        """"""Return the current size of internal memory.""""""\n        return len(self.memory)'"
dqn/solution/model.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass QNetwork(nn.Module):\n    """"""Actor (Policy) Model.""""""\n\n    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n        """"""Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n            fc1_units (int): Number of nodes in first hidden layer\n            fc2_units (int): Number of nodes in second hidden layer\n        """"""\n        super(QNetwork, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.fc1 = nn.Linear(state_size, fc1_units)\n        self.fc2 = nn.Linear(fc1_units, fc2_units)\n        self.fc3 = nn.Linear(fc2_units, action_size)\n\n    def forward(self, state):\n        """"""Build a network that maps state -> action values.""""""\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n'"
python/communicator_objects/__init__.py,0,b'from .agent_action_proto_pb2 import *\nfrom .agent_info_proto_pb2 import *\nfrom .brain_parameters_proto_pb2 import *\nfrom .brain_type_proto_pb2 import *\nfrom .command_proto_pb2 import *\nfrom .engine_configuration_proto_pb2 import *\nfrom .environment_parameters_proto_pb2 import *\nfrom .header_pb2 import *\nfrom .resolution_proto_pb2 import *\nfrom .space_type_proto_pb2 import *\nfrom .unity_input_pb2 import *\nfrom .unity_message_pb2 import *\nfrom .unity_output_pb2 import *\nfrom .unity_rl_initialization_input_pb2 import *\nfrom .unity_rl_initialization_output_pb2 import *\nfrom .unity_rl_input_pb2 import *\nfrom .unity_rl_output_pb2 import *\nfrom .unity_to_external_pb2 import *\nfrom .unity_to_external_pb2_grpc import *\n'
python/communicator_objects/agent_action_proto_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/agent_action_proto.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/agent_action_proto.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n-communicator_objects/agent_action_proto.proto\\x12\\x14\\x63ommunicator_objects\\""R\\n\\x10\\x41gentActionProto\\x12\\x16\\n\\x0evector_actions\\x18\\x01 \\x03(\\x02\\x12\\x14\\n\\x0ctext_actions\\x18\\x02 \\x01(\\t\\x12\\x10\\n\\x08memories\\x18\\x03 \\x03(\\x02\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n)\n\n\n\n\n_AGENTACTIONPROTO = _descriptor.Descriptor(\n  name=\'AgentActionProto\',\n  full_name=\'communicator_objects.AgentActionProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'vector_actions\', full_name=\'communicator_objects.AgentActionProto.vector_actions\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'text_actions\', full_name=\'communicator_objects.AgentActionProto.text_actions\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'memories\', full_name=\'communicator_objects.AgentActionProto.memories\', index=2,\n      number=3, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=71,\n  serialized_end=153,\n)\n\nDESCRIPTOR.message_types_by_name[\'AgentActionProto\'] = _AGENTACTIONPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nAgentActionProto = _reflection.GeneratedProtocolMessageType(\'AgentActionProto\', (_message.Message,), dict(\n  DESCRIPTOR = _AGENTACTIONPROTO,\n  __module__ = \'communicator_objects.agent_action_proto_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.AgentActionProto)\n  ))\n_sym_db.RegisterMessage(AgentActionProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/agent_info_proto_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/agent_info_proto.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/agent_info_proto.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n+communicator_objects/agent_info_proto.proto\\x12\\x14\\x63ommunicator_objects\\""\\xfd\\x01\\n\\x0e\\x41gentInfoProto\\x12\\""\\n\\x1astacked_vector_observation\\x18\\x01 \\x03(\\x02\\x12\\x1b\\n\\x13visual_observations\\x18\\x02 \\x03(\\x0c\\x12\\x18\\n\\x10text_observation\\x18\\x03 \\x01(\\t\\x12\\x1d\\n\\x15stored_vector_actions\\x18\\x04 \\x03(\\x02\\x12\\x1b\\n\\x13stored_text_actions\\x18\\x05 \\x01(\\t\\x12\\x10\\n\\x08memories\\x18\\x06 \\x03(\\x02\\x12\\x0e\\n\\x06reward\\x18\\x07 \\x01(\\x02\\x12\\x0c\\n\\x04\\x64one\\x18\\x08 \\x01(\\x08\\x12\\x18\\n\\x10max_step_reached\\x18\\t \\x01(\\x08\\x12\\n\\n\\x02id\\x18\\n \\x01(\\x05\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n)\n\n\n\n\n_AGENTINFOPROTO = _descriptor.Descriptor(\n  name=\'AgentInfoProto\',\n  full_name=\'communicator_objects.AgentInfoProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'stacked_vector_observation\', full_name=\'communicator_objects.AgentInfoProto.stacked_vector_observation\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'visual_observations\', full_name=\'communicator_objects.AgentInfoProto.visual_observations\', index=1,\n      number=2, type=12, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'text_observation\', full_name=\'communicator_objects.AgentInfoProto.text_observation\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'stored_vector_actions\', full_name=\'communicator_objects.AgentInfoProto.stored_vector_actions\', index=3,\n      number=4, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'stored_text_actions\', full_name=\'communicator_objects.AgentInfoProto.stored_text_actions\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'memories\', full_name=\'communicator_objects.AgentInfoProto.memories\', index=5,\n      number=6, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'reward\', full_name=\'communicator_objects.AgentInfoProto.reward\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'done\', full_name=\'communicator_objects.AgentInfoProto.done\', index=7,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'max_step_reached\', full_name=\'communicator_objects.AgentInfoProto.max_step_reached\', index=8,\n      number=9, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'communicator_objects.AgentInfoProto.id\', index=9,\n      number=10, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=70,\n  serialized_end=323,\n)\n\nDESCRIPTOR.message_types_by_name[\'AgentInfoProto\'] = _AGENTINFOPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nAgentInfoProto = _reflection.GeneratedProtocolMessageType(\'AgentInfoProto\', (_message.Message,), dict(\n  DESCRIPTOR = _AGENTINFOPROTO,\n  __module__ = \'communicator_objects.agent_info_proto_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.AgentInfoProto)\n  ))\n_sym_db.RegisterMessage(AgentInfoProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/brain_parameters_proto_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/brain_parameters_proto.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import resolution_proto_pb2 as communicator__objects_dot_resolution__proto__pb2\nfrom communicator_objects import brain_type_proto_pb2 as communicator__objects_dot_brain__type__proto__pb2\nfrom communicator_objects import space_type_proto_pb2 as communicator__objects_dot_space__type__proto__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/brain_parameters_proto.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n1communicator_objects/brain_parameters_proto.proto\\x12\\x14\\x63ommunicator_objects\\x1a+communicator_objects/resolution_proto.proto\\x1a+communicator_objects/brain_type_proto.proto\\x1a+communicator_objects/space_type_proto.proto\\""\\xc6\\x03\\n\\x14\\x42rainParametersProto\\x12\\x1f\\n\\x17vector_observation_size\\x18\\x01 \\x01(\\x05\\x12\\\'\\n\\x1fnum_stacked_vector_observations\\x18\\x02 \\x01(\\x05\\x12\\x1a\\n\\x12vector_action_size\\x18\\x03 \\x01(\\x05\\x12\\x41\\n\\x12\\x63\\x61mera_resolutions\\x18\\x04 \\x03(\\x0b\\x32%.communicator_objects.ResolutionProto\\x12\\""\\n\\x1avector_action_descriptions\\x18\\x05 \\x03(\\t\\x12\\x46\\n\\x18vector_action_space_type\\x18\\x06 \\x01(\\x0e\\x32$.communicator_objects.SpaceTypeProto\\x12K\\n\\x1dvector_observation_space_type\\x18\\x07 \\x01(\\x0e\\x32$.communicator_objects.SpaceTypeProto\\x12\\x12\\n\\nbrain_name\\x18\\x08 \\x01(\\t\\x12\\x38\\n\\nbrain_type\\x18\\t \\x01(\\x0e\\x32$.communicator_objects.BrainTypeProtoB\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n  ,\n  dependencies=[communicator__objects_dot_resolution__proto__pb2.DESCRIPTOR,communicator__objects_dot_brain__type__proto__pb2.DESCRIPTOR,communicator__objects_dot_space__type__proto__pb2.DESCRIPTOR,])\n\n\n\n\n_BRAINPARAMETERSPROTO = _descriptor.Descriptor(\n  name=\'BrainParametersProto\',\n  full_name=\'communicator_objects.BrainParametersProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'vector_observation_size\', full_name=\'communicator_objects.BrainParametersProto.vector_observation_size\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'num_stacked_vector_observations\', full_name=\'communicator_objects.BrainParametersProto.num_stacked_vector_observations\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'vector_action_size\', full_name=\'communicator_objects.BrainParametersProto.vector_action_size\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'camera_resolutions\', full_name=\'communicator_objects.BrainParametersProto.camera_resolutions\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'vector_action_descriptions\', full_name=\'communicator_objects.BrainParametersProto.vector_action_descriptions\', index=4,\n      number=5, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'vector_action_space_type\', full_name=\'communicator_objects.BrainParametersProto.vector_action_space_type\', index=5,\n      number=6, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'vector_observation_space_type\', full_name=\'communicator_objects.BrainParametersProto.vector_observation_space_type\', index=6,\n      number=7, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'brain_name\', full_name=\'communicator_objects.BrainParametersProto.brain_name\', index=7,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'brain_type\', full_name=\'communicator_objects.BrainParametersProto.brain_type\', index=8,\n      number=9, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=211,\n  serialized_end=665,\n)\n\n_BRAINPARAMETERSPROTO.fields_by_name[\'camera_resolutions\'].message_type = communicator__objects_dot_resolution__proto__pb2._RESOLUTIONPROTO\n_BRAINPARAMETERSPROTO.fields_by_name[\'vector_action_space_type\'].enum_type = communicator__objects_dot_space__type__proto__pb2._SPACETYPEPROTO\n_BRAINPARAMETERSPROTO.fields_by_name[\'vector_observation_space_type\'].enum_type = communicator__objects_dot_space__type__proto__pb2._SPACETYPEPROTO\n_BRAINPARAMETERSPROTO.fields_by_name[\'brain_type\'].enum_type = communicator__objects_dot_brain__type__proto__pb2._BRAINTYPEPROTO\nDESCRIPTOR.message_types_by_name[\'BrainParametersProto\'] = _BRAINPARAMETERSPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nBrainParametersProto = _reflection.GeneratedProtocolMessageType(\'BrainParametersProto\', (_message.Message,), dict(\n  DESCRIPTOR = _BRAINPARAMETERSPROTO,\n  __module__ = \'communicator_objects.brain_parameters_proto_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.BrainParametersProto)\n  ))\n_sym_db.RegisterMessage(BrainParametersProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/brain_type_proto_pb2.py,0,"b""# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/brain_type_proto.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import resolution_proto_pb2 as communicator__objects_dot_resolution__proto__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='communicator_objects/brain_type_proto.proto',\n  package='communicator_objects',\n  syntax='proto3',\n  serialized_pb=_b('\\n+communicator_objects/brain_type_proto.proto\\x12\\x14\\x63ommunicator_objects\\x1a+communicator_objects/resolution_proto.proto*G\\n\\x0e\\x42rainTypeProto\\x12\\n\\n\\x06Player\\x10\\x00\\x12\\r\\n\\tHeuristic\\x10\\x01\\x12\\x0c\\n\\x08\\x45xternal\\x10\\x02\\x12\\x0c\\n\\x08Internal\\x10\\x03\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3')\n  ,\n  dependencies=[communicator__objects_dot_resolution__proto__pb2.DESCRIPTOR,])\n\n_BRAINTYPEPROTO = _descriptor.EnumDescriptor(\n  name='BrainTypeProto',\n  full_name='communicator_objects.BrainTypeProto',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='Player', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='Heuristic', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='External', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='Internal', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=114,\n  serialized_end=185,\n)\n_sym_db.RegisterEnumDescriptor(_BRAINTYPEPROTO)\n\nBrainTypeProto = enum_type_wrapper.EnumTypeWrapper(_BRAINTYPEPROTO)\nPlayer = 0\nHeuristic = 1\nExternal = 2\nInternal = 3\n\n\nDESCRIPTOR.enum_types_by_name['BrainTypeProto'] = _BRAINTYPEPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\\252\\002\\034MLAgents.CommunicatorObjects'))\n# @@protoc_insertion_point(module_scope)\n"""
python/communicator_objects/command_proto_pb2.py,0,"b""# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/command_proto.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='communicator_objects/command_proto.proto',\n  package='communicator_objects',\n  syntax='proto3',\n  serialized_pb=_b('\\n(communicator_objects/command_proto.proto\\x12\\x14\\x63ommunicator_objects*-\\n\\x0c\\x43ommandProto\\x12\\x08\\n\\x04STEP\\x10\\x00\\x12\\t\\n\\x05RESET\\x10\\x01\\x12\\x08\\n\\x04QUIT\\x10\\x02\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3')\n)\n\n_COMMANDPROTO = _descriptor.EnumDescriptor(\n  name='CommandProto',\n  full_name='communicator_objects.CommandProto',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='STEP', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='RESET', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='QUIT', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=66,\n  serialized_end=111,\n)\n_sym_db.RegisterEnumDescriptor(_COMMANDPROTO)\n\nCommandProto = enum_type_wrapper.EnumTypeWrapper(_COMMANDPROTO)\nSTEP = 0\nRESET = 1\nQUIT = 2\n\n\nDESCRIPTOR.enum_types_by_name['CommandProto'] = _COMMANDPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\\252\\002\\034MLAgents.CommunicatorObjects'))\n# @@protoc_insertion_point(module_scope)\n"""
python/communicator_objects/engine_configuration_proto_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/engine_configuration_proto.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/engine_configuration_proto.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n5communicator_objects/engine_configuration_proto.proto\\x12\\x14\\x63ommunicator_objects\\""\\x95\\x01\\n\\x18\\x45ngineConfigurationProto\\x12\\r\\n\\x05width\\x18\\x01 \\x01(\\x05\\x12\\x0e\\n\\x06height\\x18\\x02 \\x01(\\x05\\x12\\x15\\n\\rquality_level\\x18\\x03 \\x01(\\x05\\x12\\x12\\n\\ntime_scale\\x18\\x04 \\x01(\\x02\\x12\\x19\\n\\x11target_frame_rate\\x18\\x05 \\x01(\\x05\\x12\\x14\\n\\x0cshow_monitor\\x18\\x06 \\x01(\\x08\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n)\n\n\n\n\n_ENGINECONFIGURATIONPROTO = _descriptor.Descriptor(\n  name=\'EngineConfigurationProto\',\n  full_name=\'communicator_objects.EngineConfigurationProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'communicator_objects.EngineConfigurationProto.width\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'communicator_objects.EngineConfigurationProto.height\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'quality_level\', full_name=\'communicator_objects.EngineConfigurationProto.quality_level\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'time_scale\', full_name=\'communicator_objects.EngineConfigurationProto.time_scale\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'target_frame_rate\', full_name=\'communicator_objects.EngineConfigurationProto.target_frame_rate\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'show_monitor\', full_name=\'communicator_objects.EngineConfigurationProto.show_monitor\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=80,\n  serialized_end=229,\n)\n\nDESCRIPTOR.message_types_by_name[\'EngineConfigurationProto\'] = _ENGINECONFIGURATIONPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nEngineConfigurationProto = _reflection.GeneratedProtocolMessageType(\'EngineConfigurationProto\', (_message.Message,), dict(\n  DESCRIPTOR = _ENGINECONFIGURATIONPROTO,\n  __module__ = \'communicator_objects.engine_configuration_proto_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.EngineConfigurationProto)\n  ))\n_sym_db.RegisterMessage(EngineConfigurationProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/environment_parameters_proto_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/environment_parameters_proto.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/environment_parameters_proto.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n7communicator_objects/environment_parameters_proto.proto\\x12\\x14\\x63ommunicator_objects\\""\\xb5\\x01\\n\\x1a\\x45nvironmentParametersProto\\x12_\\n\\x10\\x66loat_parameters\\x18\\x01 \\x03(\\x0b\\x32\\x45.communicator_objects.EnvironmentParametersProto.FloatParametersEntry\\x1a\\x36\\n\\x14\\x46loatParametersEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x02:\\x02\\x38\\x01\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n)\n\n\n\n\n_ENVIRONMENTPARAMETERSPROTO_FLOATPARAMETERSENTRY = _descriptor.Descriptor(\n  name=\'FloatParametersEntry\',\n  full_name=\'communicator_objects.EnvironmentParametersProto.FloatParametersEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'communicator_objects.EnvironmentParametersProto.FloatParametersEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'communicator_objects.EnvironmentParametersProto.FloatParametersEntry.value\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=209,\n  serialized_end=263,\n)\n\n_ENVIRONMENTPARAMETERSPROTO = _descriptor.Descriptor(\n  name=\'EnvironmentParametersProto\',\n  full_name=\'communicator_objects.EnvironmentParametersProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'float_parameters\', full_name=\'communicator_objects.EnvironmentParametersProto.float_parameters\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_ENVIRONMENTPARAMETERSPROTO_FLOATPARAMETERSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=82,\n  serialized_end=263,\n)\n\n_ENVIRONMENTPARAMETERSPROTO_FLOATPARAMETERSENTRY.containing_type = _ENVIRONMENTPARAMETERSPROTO\n_ENVIRONMENTPARAMETERSPROTO.fields_by_name[\'float_parameters\'].message_type = _ENVIRONMENTPARAMETERSPROTO_FLOATPARAMETERSENTRY\nDESCRIPTOR.message_types_by_name[\'EnvironmentParametersProto\'] = _ENVIRONMENTPARAMETERSPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nEnvironmentParametersProto = _reflection.GeneratedProtocolMessageType(\'EnvironmentParametersProto\', (_message.Message,), dict(\n\n  FloatParametersEntry = _reflection.GeneratedProtocolMessageType(\'FloatParametersEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _ENVIRONMENTPARAMETERSPROTO_FLOATPARAMETERSENTRY,\n    __module__ = \'communicator_objects.environment_parameters_proto_pb2\'\n    # @@protoc_insertion_point(class_scope:communicator_objects.EnvironmentParametersProto.FloatParametersEntry)\n    ))\n  ,\n  DESCRIPTOR = _ENVIRONMENTPARAMETERSPROTO,\n  __module__ = \'communicator_objects.environment_parameters_proto_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.EnvironmentParametersProto)\n  ))\n_sym_db.RegisterMessage(EnvironmentParametersProto)\n_sym_db.RegisterMessage(EnvironmentParametersProto.FloatParametersEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n_ENVIRONMENTPARAMETERSPROTO_FLOATPARAMETERSENTRY.has_options = True\n_ENVIRONMENTPARAMETERSPROTO_FLOATPARAMETERSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/header_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/header.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/header.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n!communicator_objects/header.proto\\x12\\x14\\x63ommunicator_objects\\"")\\n\\x06Header\\x12\\x0e\\n\\x06status\\x18\\x01 \\x01(\\x05\\x12\\x0f\\n\\x07message\\x18\\x02 \\x01(\\tB\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n)\n\n\n\n\n_HEADER = _descriptor.Descriptor(\n  name=\'Header\',\n  full_name=\'communicator_objects.Header\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'status\', full_name=\'communicator_objects.Header.status\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'message\', full_name=\'communicator_objects.Header.message\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=59,\n  serialized_end=100,\n)\n\nDESCRIPTOR.message_types_by_name[\'Header\'] = _HEADER\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nHeader = _reflection.GeneratedProtocolMessageType(\'Header\', (_message.Message,), dict(\n  DESCRIPTOR = _HEADER,\n  __module__ = \'communicator_objects.header_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.Header)\n  ))\n_sym_db.RegisterMessage(Header)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/resolution_proto_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/resolution_proto.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/resolution_proto.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n+communicator_objects/resolution_proto.proto\\x12\\x14\\x63ommunicator_objects\\""D\\n\\x0fResolutionProto\\x12\\r\\n\\x05width\\x18\\x01 \\x01(\\x05\\x12\\x0e\\n\\x06height\\x18\\x02 \\x01(\\x05\\x12\\x12\\n\\ngray_scale\\x18\\x03 \\x01(\\x08\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n)\n\n\n\n\n_RESOLUTIONPROTO = _descriptor.Descriptor(\n  name=\'ResolutionProto\',\n  full_name=\'communicator_objects.ResolutionProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'communicator_objects.ResolutionProto.width\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'communicator_objects.ResolutionProto.height\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'gray_scale\', full_name=\'communicator_objects.ResolutionProto.gray_scale\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=69,\n  serialized_end=137,\n)\n\nDESCRIPTOR.message_types_by_name[\'ResolutionProto\'] = _RESOLUTIONPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nResolutionProto = _reflection.GeneratedProtocolMessageType(\'ResolutionProto\', (_message.Message,), dict(\n  DESCRIPTOR = _RESOLUTIONPROTO,\n  __module__ = \'communicator_objects.resolution_proto_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.ResolutionProto)\n  ))\n_sym_db.RegisterMessage(ResolutionProto)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/space_type_proto_pb2.py,0,"b""# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/space_type_proto.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import resolution_proto_pb2 as communicator__objects_dot_resolution__proto__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='communicator_objects/space_type_proto.proto',\n  package='communicator_objects',\n  syntax='proto3',\n  serialized_pb=_b('\\n+communicator_objects/space_type_proto.proto\\x12\\x14\\x63ommunicator_objects\\x1a+communicator_objects/resolution_proto.proto*.\\n\\x0eSpaceTypeProto\\x12\\x0c\\n\\x08\\x64iscrete\\x10\\x00\\x12\\x0e\\n\\ncontinuous\\x10\\x01\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3')\n  ,\n  dependencies=[communicator__objects_dot_resolution__proto__pb2.DESCRIPTOR,])\n\n_SPACETYPEPROTO = _descriptor.EnumDescriptor(\n  name='SpaceTypeProto',\n  full_name='communicator_objects.SpaceTypeProto',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='discrete', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='continuous', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=114,\n  serialized_end=160,\n)\n_sym_db.RegisterEnumDescriptor(_SPACETYPEPROTO)\n\nSpaceTypeProto = enum_type_wrapper.EnumTypeWrapper(_SPACETYPEPROTO)\ndiscrete = 0\ncontinuous = 1\n\n\nDESCRIPTOR.enum_types_by_name['SpaceTypeProto'] = _SPACETYPEPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\\252\\002\\034MLAgents.CommunicatorObjects'))\n# @@protoc_insertion_point(module_scope)\n"""
python/communicator_objects/unity_input_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/unity_input.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import unity_rl_input_pb2 as communicator__objects_dot_unity__rl__input__pb2\nfrom communicator_objects import unity_rl_initialization_input_pb2 as communicator__objects_dot_unity__rl__initialization__input__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/unity_input.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n&communicator_objects/unity_input.proto\\x12\\x14\\x63ommunicator_objects\\x1a)communicator_objects/unity_rl_input.proto\\x1a\\x38\\x63ommunicator_objects/unity_rl_initialization_input.proto\\""\\xb0\\x01\\n\\nUnityInput\\x12\\x34\\n\\x08rl_input\\x18\\x01 \\x01(\\x0b\\x32\\"".communicator_objects.UnityRLInput\\x12Q\\n\\x17rl_initialization_input\\x18\\x02 \\x01(\\x0b\\x32\\x30.communicator_objects.UnityRLInitializationInput\\x12\\x19\\n\\x11\\x63ustom_data_input\\x18\\x03 \\x01(\\x05\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n  ,\n  dependencies=[communicator__objects_dot_unity__rl__input__pb2.DESCRIPTOR,communicator__objects_dot_unity__rl__initialization__input__pb2.DESCRIPTOR,])\n\n\n\n\n_UNITYINPUT = _descriptor.Descriptor(\n  name=\'UnityInput\',\n  full_name=\'communicator_objects.UnityInput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'rl_input\', full_name=\'communicator_objects.UnityInput.rl_input\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'rl_initialization_input\', full_name=\'communicator_objects.UnityInput.rl_initialization_input\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'custom_data_input\', full_name=\'communicator_objects.UnityInput.custom_data_input\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=166,\n  serialized_end=342,\n)\n\n_UNITYINPUT.fields_by_name[\'rl_input\'].message_type = communicator__objects_dot_unity__rl__input__pb2._UNITYRLINPUT\n_UNITYINPUT.fields_by_name[\'rl_initialization_input\'].message_type = communicator__objects_dot_unity__rl__initialization__input__pb2._UNITYRLINITIALIZATIONINPUT\nDESCRIPTOR.message_types_by_name[\'UnityInput\'] = _UNITYINPUT\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nUnityInput = _reflection.GeneratedProtocolMessageType(\'UnityInput\', (_message.Message,), dict(\n  DESCRIPTOR = _UNITYINPUT,\n  __module__ = \'communicator_objects.unity_input_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.UnityInput)\n  ))\n_sym_db.RegisterMessage(UnityInput)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/unity_message_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/unity_message.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import unity_output_pb2 as communicator__objects_dot_unity__output__pb2\nfrom communicator_objects import unity_input_pb2 as communicator__objects_dot_unity__input__pb2\nfrom communicator_objects import header_pb2 as communicator__objects_dot_header__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/unity_message.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n(communicator_objects/unity_message.proto\\x12\\x14\\x63ommunicator_objects\\x1a\\\'communicator_objects/unity_output.proto\\x1a&communicator_objects/unity_input.proto\\x1a!communicator_objects/header.proto\\""\\xac\\x01\\n\\x0cUnityMessage\\x12,\\n\\x06header\\x18\\x01 \\x01(\\x0b\\x32\\x1c.communicator_objects.Header\\x12\\x37\\n\\x0cunity_output\\x18\\x02 \\x01(\\x0b\\x32!.communicator_objects.UnityOutput\\x12\\x35\\n\\x0bunity_input\\x18\\x03 \\x01(\\x0b\\x32 .communicator_objects.UnityInputB\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n  ,\n  dependencies=[communicator__objects_dot_unity__output__pb2.DESCRIPTOR,communicator__objects_dot_unity__input__pb2.DESCRIPTOR,communicator__objects_dot_header__pb2.DESCRIPTOR,])\n\n\n\n\n_UNITYMESSAGE = _descriptor.Descriptor(\n  name=\'UnityMessage\',\n  full_name=\'communicator_objects.UnityMessage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'header\', full_name=\'communicator_objects.UnityMessage.header\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'unity_output\', full_name=\'communicator_objects.UnityMessage.unity_output\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'unity_input\', full_name=\'communicator_objects.UnityMessage.unity_input\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=183,\n  serialized_end=355,\n)\n\n_UNITYMESSAGE.fields_by_name[\'header\'].message_type = communicator__objects_dot_header__pb2._HEADER\n_UNITYMESSAGE.fields_by_name[\'unity_output\'].message_type = communicator__objects_dot_unity__output__pb2._UNITYOUTPUT\n_UNITYMESSAGE.fields_by_name[\'unity_input\'].message_type = communicator__objects_dot_unity__input__pb2._UNITYINPUT\nDESCRIPTOR.message_types_by_name[\'UnityMessage\'] = _UNITYMESSAGE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nUnityMessage = _reflection.GeneratedProtocolMessageType(\'UnityMessage\', (_message.Message,), dict(\n  DESCRIPTOR = _UNITYMESSAGE,\n  __module__ = \'communicator_objects.unity_message_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.UnityMessage)\n  ))\n_sym_db.RegisterMessage(UnityMessage)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/unity_output_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/unity_output.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import unity_rl_output_pb2 as communicator__objects_dot_unity__rl__output__pb2\nfrom communicator_objects import unity_rl_initialization_output_pb2 as communicator__objects_dot_unity__rl__initialization__output__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/unity_output.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\\'communicator_objects/unity_output.proto\\x12\\x14\\x63ommunicator_objects\\x1a*communicator_objects/unity_rl_output.proto\\x1a\\x39\\x63ommunicator_objects/unity_rl_initialization_output.proto\\""\\xb6\\x01\\n\\x0bUnityOutput\\x12\\x36\\n\\trl_output\\x18\\x01 \\x01(\\x0b\\x32#.communicator_objects.UnityRLOutput\\x12S\\n\\x18rl_initialization_output\\x18\\x02 \\x01(\\x0b\\x32\\x31.communicator_objects.UnityRLInitializationOutput\\x12\\x1a\\n\\x12\\x63ustom_data_output\\x18\\x03 \\x01(\\tB\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n  ,\n  dependencies=[communicator__objects_dot_unity__rl__output__pb2.DESCRIPTOR,communicator__objects_dot_unity__rl__initialization__output__pb2.DESCRIPTOR,])\n\n\n\n\n_UNITYOUTPUT = _descriptor.Descriptor(\n  name=\'UnityOutput\',\n  full_name=\'communicator_objects.UnityOutput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'rl_output\', full_name=\'communicator_objects.UnityOutput.rl_output\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'rl_initialization_output\', full_name=\'communicator_objects.UnityOutput.rl_initialization_output\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'custom_data_output\', full_name=\'communicator_objects.UnityOutput.custom_data_output\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=169,\n  serialized_end=351,\n)\n\n_UNITYOUTPUT.fields_by_name[\'rl_output\'].message_type = communicator__objects_dot_unity__rl__output__pb2._UNITYRLOUTPUT\n_UNITYOUTPUT.fields_by_name[\'rl_initialization_output\'].message_type = communicator__objects_dot_unity__rl__initialization__output__pb2._UNITYRLINITIALIZATIONOUTPUT\nDESCRIPTOR.message_types_by_name[\'UnityOutput\'] = _UNITYOUTPUT\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nUnityOutput = _reflection.GeneratedProtocolMessageType(\'UnityOutput\', (_message.Message,), dict(\n  DESCRIPTOR = _UNITYOUTPUT,\n  __module__ = \'communicator_objects.unity_output_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.UnityOutput)\n  ))\n_sym_db.RegisterMessage(UnityOutput)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/unity_rl_initialization_input_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/unity_rl_initialization_input.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/unity_rl_initialization_input.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n8communicator_objects/unity_rl_initialization_input.proto\\x12\\x14\\x63ommunicator_objects\\""*\\n\\x1aUnityRLInitializationInput\\x12\\x0c\\n\\x04seed\\x18\\x01 \\x01(\\x05\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n)\n\n\n\n\n_UNITYRLINITIALIZATIONINPUT = _descriptor.Descriptor(\n  name=\'UnityRLInitializationInput\',\n  full_name=\'communicator_objects.UnityRLInitializationInput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'seed\', full_name=\'communicator_objects.UnityRLInitializationInput.seed\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=82,\n  serialized_end=124,\n)\n\nDESCRIPTOR.message_types_by_name[\'UnityRLInitializationInput\'] = _UNITYRLINITIALIZATIONINPUT\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nUnityRLInitializationInput = _reflection.GeneratedProtocolMessageType(\'UnityRLInitializationInput\', (_message.Message,), dict(\n  DESCRIPTOR = _UNITYRLINITIALIZATIONINPUT,\n  __module__ = \'communicator_objects.unity_rl_initialization_input_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.UnityRLInitializationInput)\n  ))\n_sym_db.RegisterMessage(UnityRLInitializationInput)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/unity_rl_initialization_output_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/unity_rl_initialization_output.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import brain_parameters_proto_pb2 as communicator__objects_dot_brain__parameters__proto__pb2\nfrom communicator_objects import environment_parameters_proto_pb2 as communicator__objects_dot_environment__parameters__proto__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/unity_rl_initialization_output.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n9communicator_objects/unity_rl_initialization_output.proto\\x12\\x14\\x63ommunicator_objects\\x1a\\x31\\x63ommunicator_objects/brain_parameters_proto.proto\\x1a\\x37\\x63ommunicator_objects/environment_parameters_proto.proto\\""\\xe6\\x01\\n\\x1bUnityRLInitializationOutput\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07version\\x18\\x02 \\x01(\\t\\x12\\x10\\n\\x08log_path\\x18\\x03 \\x01(\\t\\x12\\x44\\n\\x10\\x62rain_parameters\\x18\\x05 \\x03(\\x0b\\x32*.communicator_objects.BrainParametersProto\\x12P\\n\\x16\\x65nvironment_parameters\\x18\\x06 \\x01(\\x0b\\x32\\x30.communicator_objects.EnvironmentParametersProtoB\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n  ,\n  dependencies=[communicator__objects_dot_brain__parameters__proto__pb2.DESCRIPTOR,communicator__objects_dot_environment__parameters__proto__pb2.DESCRIPTOR,])\n\n\n\n\n_UNITYRLINITIALIZATIONOUTPUT = _descriptor.Descriptor(\n  name=\'UnityRLInitializationOutput\',\n  full_name=\'communicator_objects.UnityRLInitializationOutput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'communicator_objects.UnityRLInitializationOutput.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'communicator_objects.UnityRLInitializationOutput.version\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'log_path\', full_name=\'communicator_objects.UnityRLInitializationOutput.log_path\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'brain_parameters\', full_name=\'communicator_objects.UnityRLInitializationOutput.brain_parameters\', index=3,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'environment_parameters\', full_name=\'communicator_objects.UnityRLInitializationOutput.environment_parameters\', index=4,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=192,\n  serialized_end=422,\n)\n\n_UNITYRLINITIALIZATIONOUTPUT.fields_by_name[\'brain_parameters\'].message_type = communicator__objects_dot_brain__parameters__proto__pb2._BRAINPARAMETERSPROTO\n_UNITYRLINITIALIZATIONOUTPUT.fields_by_name[\'environment_parameters\'].message_type = communicator__objects_dot_environment__parameters__proto__pb2._ENVIRONMENTPARAMETERSPROTO\nDESCRIPTOR.message_types_by_name[\'UnityRLInitializationOutput\'] = _UNITYRLINITIALIZATIONOUTPUT\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nUnityRLInitializationOutput = _reflection.GeneratedProtocolMessageType(\'UnityRLInitializationOutput\', (_message.Message,), dict(\n  DESCRIPTOR = _UNITYRLINITIALIZATIONOUTPUT,\n  __module__ = \'communicator_objects.unity_rl_initialization_output_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.UnityRLInitializationOutput)\n  ))\n_sym_db.RegisterMessage(UnityRLInitializationOutput)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/unity_rl_input_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/unity_rl_input.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import agent_action_proto_pb2 as communicator__objects_dot_agent__action__proto__pb2\nfrom communicator_objects import environment_parameters_proto_pb2 as communicator__objects_dot_environment__parameters__proto__pb2\nfrom communicator_objects import command_proto_pb2 as communicator__objects_dot_command__proto__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/unity_rl_input.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n)communicator_objects/unity_rl_input.proto\\x12\\x14\\x63ommunicator_objects\\x1a-communicator_objects/agent_action_proto.proto\\x1a\\x37\\x63ommunicator_objects/environment_parameters_proto.proto\\x1a(communicator_objects/command_proto.proto\\""\\xb4\\x03\\n\\x0cUnityRLInput\\x12K\\n\\ragent_actions\\x18\\x01 \\x03(\\x0b\\x32\\x34.communicator_objects.UnityRLInput.AgentActionsEntry\\x12P\\n\\x16\\x65nvironment_parameters\\x18\\x02 \\x01(\\x0b\\x32\\x30.communicator_objects.EnvironmentParametersProto\\x12\\x13\\n\\x0bis_training\\x18\\x03 \\x01(\\x08\\x12\\x33\\n\\x07\\x63ommand\\x18\\x04 \\x01(\\x0e\\x32\\"".communicator_objects.CommandProto\\x1aM\\n\\x14ListAgentActionProto\\x12\\x35\\n\\x05value\\x18\\x01 \\x03(\\x0b\\x32&.communicator_objects.AgentActionProto\\x1al\\n\\x11\\x41gentActionsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x46\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x37.communicator_objects.UnityRLInput.ListAgentActionProto:\\x02\\x38\\x01\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n  ,\n  dependencies=[communicator__objects_dot_agent__action__proto__pb2.DESCRIPTOR,communicator__objects_dot_environment__parameters__proto__pb2.DESCRIPTOR,communicator__objects_dot_command__proto__pb2.DESCRIPTOR,])\n\n\n\n\n_UNITYRLINPUT_LISTAGENTACTIONPROTO = _descriptor.Descriptor(\n  name=\'ListAgentActionProto\',\n  full_name=\'communicator_objects.UnityRLInput.ListAgentActionProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'communicator_objects.UnityRLInput.ListAgentActionProto.value\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=463,\n  serialized_end=540,\n)\n\n_UNITYRLINPUT_AGENTACTIONSENTRY = _descriptor.Descriptor(\n  name=\'AgentActionsEntry\',\n  full_name=\'communicator_objects.UnityRLInput.AgentActionsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'communicator_objects.UnityRLInput.AgentActionsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'communicator_objects.UnityRLInput.AgentActionsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=542,\n  serialized_end=650,\n)\n\n_UNITYRLINPUT = _descriptor.Descriptor(\n  name=\'UnityRLInput\',\n  full_name=\'communicator_objects.UnityRLInput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'agent_actions\', full_name=\'communicator_objects.UnityRLInput.agent_actions\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'environment_parameters\', full_name=\'communicator_objects.UnityRLInput.environment_parameters\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'is_training\', full_name=\'communicator_objects.UnityRLInput.is_training\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'command\', full_name=\'communicator_objects.UnityRLInput.command\', index=3,\n      number=4, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_UNITYRLINPUT_LISTAGENTACTIONPROTO, _UNITYRLINPUT_AGENTACTIONSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=214,\n  serialized_end=650,\n)\n\n_UNITYRLINPUT_LISTAGENTACTIONPROTO.fields_by_name[\'value\'].message_type = communicator__objects_dot_agent__action__proto__pb2._AGENTACTIONPROTO\n_UNITYRLINPUT_LISTAGENTACTIONPROTO.containing_type = _UNITYRLINPUT\n_UNITYRLINPUT_AGENTACTIONSENTRY.fields_by_name[\'value\'].message_type = _UNITYRLINPUT_LISTAGENTACTIONPROTO\n_UNITYRLINPUT_AGENTACTIONSENTRY.containing_type = _UNITYRLINPUT\n_UNITYRLINPUT.fields_by_name[\'agent_actions\'].message_type = _UNITYRLINPUT_AGENTACTIONSENTRY\n_UNITYRLINPUT.fields_by_name[\'environment_parameters\'].message_type = communicator__objects_dot_environment__parameters__proto__pb2._ENVIRONMENTPARAMETERSPROTO\n_UNITYRLINPUT.fields_by_name[\'command\'].enum_type = communicator__objects_dot_command__proto__pb2._COMMANDPROTO\nDESCRIPTOR.message_types_by_name[\'UnityRLInput\'] = _UNITYRLINPUT\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nUnityRLInput = _reflection.GeneratedProtocolMessageType(\'UnityRLInput\', (_message.Message,), dict(\n\n  ListAgentActionProto = _reflection.GeneratedProtocolMessageType(\'ListAgentActionProto\', (_message.Message,), dict(\n    DESCRIPTOR = _UNITYRLINPUT_LISTAGENTACTIONPROTO,\n    __module__ = \'communicator_objects.unity_rl_input_pb2\'\n    # @@protoc_insertion_point(class_scope:communicator_objects.UnityRLInput.ListAgentActionProto)\n    ))\n  ,\n\n  AgentActionsEntry = _reflection.GeneratedProtocolMessageType(\'AgentActionsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _UNITYRLINPUT_AGENTACTIONSENTRY,\n    __module__ = \'communicator_objects.unity_rl_input_pb2\'\n    # @@protoc_insertion_point(class_scope:communicator_objects.UnityRLInput.AgentActionsEntry)\n    ))\n  ,\n  DESCRIPTOR = _UNITYRLINPUT,\n  __module__ = \'communicator_objects.unity_rl_input_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.UnityRLInput)\n  ))\n_sym_db.RegisterMessage(UnityRLInput)\n_sym_db.RegisterMessage(UnityRLInput.ListAgentActionProto)\n_sym_db.RegisterMessage(UnityRLInput.AgentActionsEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n_UNITYRLINPUT_AGENTACTIONSENTRY.has_options = True\n_UNITYRLINPUT_AGENTACTIONSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/unity_rl_output_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/unity_rl_output.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import agent_info_proto_pb2 as communicator__objects_dot_agent__info__proto__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/unity_rl_output.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n*communicator_objects/unity_rl_output.proto\\x12\\x14\\x63ommunicator_objects\\x1a+communicator_objects/agent_info_proto.proto\\""\\xa3\\x02\\n\\rUnityRLOutput\\x12\\x13\\n\\x0bglobal_done\\x18\\x01 \\x01(\\x08\\x12G\\n\\nagentInfos\\x18\\x02 \\x03(\\x0b\\x32\\x33.communicator_objects.UnityRLOutput.AgentInfosEntry\\x1aI\\n\\x12ListAgentInfoProto\\x12\\x33\\n\\x05value\\x18\\x01 \\x03(\\x0b\\x32$.communicator_objects.AgentInfoProto\\x1ai\\n\\x0f\\x41gentInfosEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x45\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x36.communicator_objects.UnityRLOutput.ListAgentInfoProto:\\x02\\x38\\x01\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n  ,\n  dependencies=[communicator__objects_dot_agent__info__proto__pb2.DESCRIPTOR,])\n\n\n\n\n_UNITYRLOUTPUT_LISTAGENTINFOPROTO = _descriptor.Descriptor(\n  name=\'ListAgentInfoProto\',\n  full_name=\'communicator_objects.UnityRLOutput.ListAgentInfoProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'communicator_objects.UnityRLOutput.ListAgentInfoProto.value\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=225,\n  serialized_end=298,\n)\n\n_UNITYRLOUTPUT_AGENTINFOSENTRY = _descriptor.Descriptor(\n  name=\'AgentInfosEntry\',\n  full_name=\'communicator_objects.UnityRLOutput.AgentInfosEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'communicator_objects.UnityRLOutput.AgentInfosEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'communicator_objects.UnityRLOutput.AgentInfosEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=300,\n  serialized_end=405,\n)\n\n_UNITYRLOUTPUT = _descriptor.Descriptor(\n  name=\'UnityRLOutput\',\n  full_name=\'communicator_objects.UnityRLOutput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'global_done\', full_name=\'communicator_objects.UnityRLOutput.global_done\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'agentInfos\', full_name=\'communicator_objects.UnityRLOutput.agentInfos\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_UNITYRLOUTPUT_LISTAGENTINFOPROTO, _UNITYRLOUTPUT_AGENTINFOSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=114,\n  serialized_end=405,\n)\n\n_UNITYRLOUTPUT_LISTAGENTINFOPROTO.fields_by_name[\'value\'].message_type = communicator__objects_dot_agent__info__proto__pb2._AGENTINFOPROTO\n_UNITYRLOUTPUT_LISTAGENTINFOPROTO.containing_type = _UNITYRLOUTPUT\n_UNITYRLOUTPUT_AGENTINFOSENTRY.fields_by_name[\'value\'].message_type = _UNITYRLOUTPUT_LISTAGENTINFOPROTO\n_UNITYRLOUTPUT_AGENTINFOSENTRY.containing_type = _UNITYRLOUTPUT\n_UNITYRLOUTPUT.fields_by_name[\'agentInfos\'].message_type = _UNITYRLOUTPUT_AGENTINFOSENTRY\nDESCRIPTOR.message_types_by_name[\'UnityRLOutput\'] = _UNITYRLOUTPUT\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nUnityRLOutput = _reflection.GeneratedProtocolMessageType(\'UnityRLOutput\', (_message.Message,), dict(\n\n  ListAgentInfoProto = _reflection.GeneratedProtocolMessageType(\'ListAgentInfoProto\', (_message.Message,), dict(\n    DESCRIPTOR = _UNITYRLOUTPUT_LISTAGENTINFOPROTO,\n    __module__ = \'communicator_objects.unity_rl_output_pb2\'\n    # @@protoc_insertion_point(class_scope:communicator_objects.UnityRLOutput.ListAgentInfoProto)\n    ))\n  ,\n\n  AgentInfosEntry = _reflection.GeneratedProtocolMessageType(\'AgentInfosEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _UNITYRLOUTPUT_AGENTINFOSENTRY,\n    __module__ = \'communicator_objects.unity_rl_output_pb2\'\n    # @@protoc_insertion_point(class_scope:communicator_objects.UnityRLOutput.AgentInfosEntry)\n    ))\n  ,\n  DESCRIPTOR = _UNITYRLOUTPUT,\n  __module__ = \'communicator_objects.unity_rl_output_pb2\'\n  # @@protoc_insertion_point(class_scope:communicator_objects.UnityRLOutput)\n  ))\n_sym_db.RegisterMessage(UnityRLOutput)\n_sym_db.RegisterMessage(UnityRLOutput.ListAgentInfoProto)\n_sym_db.RegisterMessage(UnityRLOutput.AgentInfosEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n_UNITYRLOUTPUT_AGENTINFOSENTRY.has_options = True\n_UNITYRLOUTPUT_AGENTINFOSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/unity_to_external_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: communicator_objects/unity_to_external.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom communicator_objects import unity_message_pb2 as communicator__objects_dot_unity__message__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'communicator_objects/unity_to_external.proto\',\n  package=\'communicator_objects\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n,communicator_objects/unity_to_external.proto\\x12\\x14\\x63ommunicator_objects\\x1a(communicator_objects/unity_message.proto2g\\n\\x0fUnityToExternal\\x12T\\n\\x08\\x45xchange\\x12\\"".communicator_objects.UnityMessage\\x1a\\"".communicator_objects.UnityMessage\\""\\x00\\x42\\x1f\\xaa\\x02\\x1cMLAgents.CommunicatorObjectsb\\x06proto3\')\n  ,\n  dependencies=[communicator__objects_dot_unity__message__pb2.DESCRIPTOR,])\n\n\n\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\252\\002\\034MLAgents.CommunicatorObjects\'))\n\n_UNITYTOEXTERNAL = _descriptor.ServiceDescriptor(\n  name=\'UnityToExternal\',\n  full_name=\'communicator_objects.UnityToExternal\',\n  file=DESCRIPTOR,\n  index=0,\n  options=None,\n  serialized_start=112,\n  serialized_end=215,\n  methods=[\n  _descriptor.MethodDescriptor(\n    name=\'Exchange\',\n    full_name=\'communicator_objects.UnityToExternal.Exchange\',\n    index=0,\n    containing_service=None,\n    input_type=communicator__objects_dot_unity__message__pb2._UNITYMESSAGE,\n    output_type=communicator__objects_dot_unity__message__pb2._UNITYMESSAGE,\n    options=None,\n  ),\n])\n_sym_db.RegisterServiceDescriptor(_UNITYTOEXTERNAL)\n\nDESCRIPTOR.services_by_name[\'UnityToExternal\'] = _UNITYTOEXTERNAL\n\n# @@protoc_insertion_point(module_scope)\n'"
python/communicator_objects/unity_to_external_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom communicator_objects import unity_message_pb2 as communicator__objects_dot_unity__message__pb2\n\n\nclass UnityToExternalStub(object):\n  # missing associated documentation comment in .proto file\n  pass\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.Exchange = channel.unary_unary(\n        \'/communicator_objects.UnityToExternal/Exchange\',\n        request_serializer=communicator__objects_dot_unity__message__pb2.UnityMessage.SerializeToString,\n        response_deserializer=communicator__objects_dot_unity__message__pb2.UnityMessage.FromString,\n        )\n\n\nclass UnityToExternalServicer(object):\n  # missing associated documentation comment in .proto file\n  pass\n\n  def Exchange(self, request, context):\n    """"""Sends the academy parameters\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_UnityToExternalServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'Exchange\': grpc.unary_unary_rpc_method_handler(\n          servicer.Exchange,\n          request_deserializer=communicator__objects_dot_unity__message__pb2.UnityMessage.FromString,\n          response_serializer=communicator__objects_dot_unity__message__pb2.UnityMessage.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'communicator_objects.UnityToExternal\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
python/tests/__init__.py,0,b'from unityagents import *\nfrom unitytrainers import *\n'
python/tests/mock_communicator.py,0,"b'\nfrom unityagents.communicator import Communicator\nfrom communicator_objects import UnityMessage, UnityOutput, UnityInput,\\\n    ResolutionProto, BrainParametersProto, UnityRLInitializationOutput,\\\n    AgentInfoProto, UnityRLOutput\n\n\nclass MockCommunicator(Communicator):\n    def __init__(self, discrete_action=False, visual_inputs=0):\n        """"""\n        Python side of the grpc communication. Python is the client and Unity the server\n\n        :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this.\n        :int worker_id: Number to add to communication port (5005) [0]. Used for asynchronous agent scenarios.\n        """"""\n        self.is_discrete = discrete_action\n        self.steps = 0\n        self.visual_inputs = visual_inputs\n        self.has_been_closed = False\n\n    def initialize(self, inputs: UnityInput) -> UnityOutput:\n        resolutions = [ResolutionProto(\n            width=30,\n            height=40,\n            gray_scale=False) for i in range(self.visual_inputs)]\n        bp = BrainParametersProto(\n            vector_observation_size=3,\n            num_stacked_vector_observations=2,\n            vector_action_size=2,\n            camera_resolutions=resolutions,\n            vector_action_descriptions=["""", """"],\n            vector_action_space_type=int(not self.is_discrete),\n            vector_observation_space_type=1,\n            brain_name=""RealFakeBrain"",\n            brain_type=2\n        )\n        rl_init = UnityRLInitializationOutput(\n            name=""RealFakeAcademy"",\n            version=""API-4"",\n            log_path="""",\n            brain_parameters=[bp]\n        )\n        return UnityOutput(\n            rl_initialization_output=rl_init\n        )\n\n    def exchange(self, inputs: UnityInput) -> UnityOutput:\n        dict_agent_info = {}\n        if self.is_discrete:\n            vector_action = [1]\n        else:\n            vector_action = [1, 2]\n        list_agent_info = []\n        for i in range(3):\n            list_agent_info.append(\n                AgentInfoProto(\n                    stacked_vector_observation=[1, 2, 3, 1, 2, 3],\n                    reward=1,\n                    stored_vector_actions=vector_action,\n                    stored_text_actions="""",\n                    text_observation="""",\n                    memories=[],\n                    done=(i == 2),\n                    max_step_reached=False,\n                    id=i\n                ))\n        dict_agent_info[""RealFakeBrain""] = \\\n            UnityRLOutput.ListAgentInfoProto(value=list_agent_info)\n        global_done = False\n        try:\n            global_done = (inputs.rl_input.agent_actions[""RealFakeBrain""].value[0].vector_actions[0] == -1)\n        except:\n            pass\n        result = UnityRLOutput(\n            global_done=global_done,\n            agentInfos=dict_agent_info\n        )\n        return UnityOutput(\n            rl_output=result\n        )\n\n    def close(self):\n        """"""\n        Sends a shutdown signal to the unity environment, and closes the grpc connection.\n        """"""\n        self.has_been_closed = True\n'"
python/tests/test_bc.py,0,"b'import unittest.mock as mock\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom unitytrainers.bc.models import BehavioralCloningModel\nfrom unityagents import UnityEnvironment\nfrom .mock_communicator import MockCommunicator\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_cc_bc_model(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=False, visual_inputs=0)\n            env = UnityEnvironment(\' \')\n            model = BehavioralCloningModel(env.brains[""RealFakeBrain""])\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.sample_action, model.policy]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                   [3, 4, 5, 3, 4, 5]])}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_dc_bc_model(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=True, visual_inputs=0)\n            env = UnityEnvironment(\' \')\n            model = BehavioralCloningModel(env.brains[""RealFakeBrain""])\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.sample_action, model.policy]\n            feed_dict = {model.batch_size: 2,\n                         model.dropout_rate: 1.0,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                   [3, 4, 5, 3, 4, 5]])}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_visual_dc_bc_model(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=True, visual_inputs=2)\n            env = UnityEnvironment(\' \')\n            model = BehavioralCloningModel(env.brains[""RealFakeBrain""])\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.sample_action, model.policy]\n            feed_dict = {model.batch_size: 2,\n                         model.dropout_rate: 1.0,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                   [3, 4, 5, 3, 4, 5]]),\n                         model.visual_in[0]: np.ones([2, 40, 30, 3]),\n                         model.visual_in[1]: np.ones([2, 40, 30, 3])}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_visual_cc_bc_model(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=False, visual_inputs=2)\n            env = UnityEnvironment(\' \')\n            model = BehavioralCloningModel(env.brains[""RealFakeBrain""])\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.sample_action, model.policy]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                   [3, 4, 5, 3, 4, 5]]),\n                         model.visual_in[0]: np.ones([2, 40, 30, 3]),\n                         model.visual_in[1]: np.ones([2, 40, 30, 3])}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\nif __name__ == \'__main__\':\n    pytest.main()\n'"
python/tests/test_ppo.py,0,"b'import unittest.mock as mock\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom unitytrainers.ppo.models import PPOModel\nfrom unitytrainers.ppo.trainer import discount_rewards\nfrom unityagents import UnityEnvironment\nfrom .mock_communicator import MockCommunicator\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_cc_vector(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=False, visual_inputs=0)\n            env = UnityEnvironment(\' \')\n\n            model = PPOModel(env.brains[""RealFakeBrain""])\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.probs, model.value, model.entropy,\n                        model.learning_rate]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]])}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_cc_visual(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=False, visual_inputs=2)\n            env = UnityEnvironment(\' \')\n\n            model = PPOModel(env.brains[""RealFakeBrain""])\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.probs, model.value, model.entropy,\n                        model.learning_rate]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]]),\n                         model.visual_in[0]: np.ones([2, 40, 30, 3]),\n                         model.visual_in[1]: np.ones([2, 40, 30, 3])}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_dc_visual(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=True, visual_inputs=2)\n            env = UnityEnvironment(\' \')\n            model = PPOModel(env.brains[""RealFakeBrain""])\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.all_probs, model.value, model.entropy,\n                        model.learning_rate]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]]),\n                         model.visual_in[0]: np.ones([2, 40, 30, 3]),\n                         model.visual_in[1]: np.ones([2, 40, 30, 3])\n                         }\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_dc_vector(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=True, visual_inputs=0)\n            env = UnityEnvironment(\' \')\n            model = PPOModel(env.brains[""RealFakeBrain""])\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.all_probs, model.value, model.entropy,\n                        model.learning_rate]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]])}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_dc_vector_rnn(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=True, visual_inputs=0)\n            env = UnityEnvironment(\' \')\n            memory_size = 128\n            model = PPOModel(env.brains[""RealFakeBrain""], use_recurrent=True, m_size=memory_size)\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.all_probs, model.value, model.entropy,\n                        model.learning_rate, model.memory_out]\n            feed_dict = {model.batch_size: 1,\n                         model.sequence_length: 2,\n                         model.prev_action: [0, 0],\n                         model.memory_in: np.zeros((1, memory_size)),\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]])}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_cc_vector_rnn(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=False, visual_inputs=0)\n            env = UnityEnvironment(\' \')\n            memory_size = 128\n            model = PPOModel(env.brains[""RealFakeBrain""], use_recurrent=True, m_size=memory_size)\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.all_probs, model.value, model.entropy,\n                        model.learning_rate, model.memory_out]\n            feed_dict = {model.batch_size: 1,\n                         model.sequence_length: 2,\n                         model.memory_in: np.zeros((1, memory_size)),\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]])}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_dc_vector_curio(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=True, visual_inputs=0)\n            env = UnityEnvironment(\' \')\n            model = PPOModel(env.brains[""RealFakeBrain""], use_curiosity=True)\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.all_probs, model.value, model.entropy,\n                        model.learning_rate, model.intrinsic_reward]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]]),\n                         model.next_vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                         [3, 4, 5, 3, 4, 5]]),\n                         model.action_holder: [0, 0]}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_cc_vector_curio(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=False, visual_inputs=0)\n            env = UnityEnvironment(\' \')\n            model = PPOModel(env.brains[""RealFakeBrain""], use_curiosity=True)\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.all_probs, model.value, model.entropy,\n                        model.learning_rate, model.intrinsic_reward]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]]),\n                         model.next_vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                         [3, 4, 5, 3, 4, 5]]),\n                         model.output: [[0.0, 0.0], [0.0, 0.0]]}\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_dc_visual_curio(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=True, visual_inputs=2)\n            env = UnityEnvironment(\' \')\n            model = PPOModel(env.brains[""RealFakeBrain""], use_curiosity=True)\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.all_probs, model.value, model.entropy,\n                        model.learning_rate, model.intrinsic_reward]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]]),\n                         model.next_vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                         [3, 4, 5, 3, 4, 5]]),\n                         model.action_holder: [0, 0],\n                         model.visual_in[0]: np.ones([2, 40, 30, 3]),\n                         model.visual_in[1]: np.ones([2, 40, 30, 3]),\n                         model.next_visual_in[0]: np.ones([2, 40, 30, 3]),\n                         model.next_visual_in[1]: np.ones([2, 40, 30, 3])\n                         }\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_ppo_model_cc_visual_curio(mock_communicator, mock_launcher):\n    tf.reset_default_graph()\n    with tf.Session() as sess:\n        with tf.variable_scope(""FakeGraphScope""):\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=False, visual_inputs=2)\n            env = UnityEnvironment(\' \')\n            model = PPOModel(env.brains[""RealFakeBrain""], use_curiosity=True)\n            init = tf.global_variables_initializer()\n            sess.run(init)\n\n            run_list = [model.output, model.all_probs, model.value, model.entropy,\n                        model.learning_rate, model.intrinsic_reward]\n            feed_dict = {model.batch_size: 2,\n                         model.sequence_length: 1,\n                         model.vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                    [3, 4, 5, 3, 4, 5]]),\n                         model.next_vector_in: np.array([[1, 2, 3, 1, 2, 3],\n                                                         [3, 4, 5, 3, 4, 5]]),\n                         model.output: [[0.0, 0.0], [0.0, 0.0]],\n                         model.visual_in[0]: np.ones([2, 40, 30, 3]),\n                         model.visual_in[1]: np.ones([2, 40, 30, 3]),\n                         model.next_visual_in[0]: np.ones([2, 40, 30, 3]),\n                         model.next_visual_in[1]: np.ones([2, 40, 30, 3])\n                         }\n            sess.run(run_list, feed_dict=feed_dict)\n            env.close()\n\n\ndef test_rl_functions():\n    rewards = np.array([0.0, 0.0, 0.0, 1.0])\n    gamma = 0.9\n    returns = discount_rewards(rewards, gamma, 0.0)\n    np.testing.assert_array_almost_equal(returns, np.array([0.729, 0.81, 0.9, 1.0]))\n\n\nif __name__ == \'__main__\':\n    pytest.main()\n'"
python/tests/test_unityagents.py,0,"b'import json\nimport unittest.mock as mock\nimport pytest\nimport struct\n\nimport numpy as np\n\nfrom unityagents import UnityEnvironment, UnityEnvironmentException, UnityActionException, \\\n    BrainInfo, Curriculum\nfrom .mock_communicator import MockCommunicator\n\n\ndummy_curriculum = json.loads(\'\'\'{\n    ""measure"" : ""reward"",\n    ""thresholds"" : [10, 20, 50],\n    ""min_lesson_length"" : 3,\n    ""signal_smoothing"" : true, \n    ""parameters"" : \n    {\n        ""param1"" : [0.7, 0.5, 0.3, 0.1],\n        ""param2"" : [100, 50, 20, 15],\n        ""param3"" : [0.2, 0.3, 0.7, 0.9]\n    }\n}\'\'\')\nbad_curriculum = json.loads(\'\'\'{\n    ""measure"" : ""reward"",\n    ""thresholds"" : [10, 20, 50],\n    ""min_lesson_length"" : 3,\n    ""signal_smoothing"" : false, \n    ""parameters"" : \n    {\n        ""param1"" : [0.7, 0.5, 0.3, 0.1],\n        ""param2"" : [100, 50, 20],\n        ""param3"" : [0.2, 0.3, 0.7, 0.9]\n    }\n}\'\'\')\n\n\ndef test_handles_bad_filename():\n    with pytest.raises(UnityEnvironmentException):\n        UnityEnvironment(\' \')\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_initialization(mock_communicator, mock_launcher):\n    mock_communicator.return_value = MockCommunicator(\n        discrete_action=False, visual_inputs=0)\n    env = UnityEnvironment(\' \')\n    with pytest.raises(UnityActionException):\n        env.step([0])\n    assert env.brain_names[0] == \'RealFakeBrain\'\n    env.close()\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_reset(mock_communicator, mock_launcher):\n    mock_communicator.return_value = MockCommunicator(\n        discrete_action=False, visual_inputs=0)\n    env = UnityEnvironment(\' \')\n    brain = env.brains[\'RealFakeBrain\']\n    brain_info = env.reset()\n    env.close()\n    assert not env.global_done\n    assert isinstance(brain_info, dict)\n    assert isinstance(brain_info[\'RealFakeBrain\'], BrainInfo)\n    assert isinstance(brain_info[\'RealFakeBrain\'].visual_observations, list)\n    assert isinstance(brain_info[\'RealFakeBrain\'].vector_observations, np.ndarray)\n    assert len(brain_info[\'RealFakeBrain\'].visual_observations) == brain.number_visual_observations\n    assert brain_info[\'RealFakeBrain\'].vector_observations.shape[0] == \\\n           len(brain_info[\'RealFakeBrain\'].agents)\n    assert brain_info[\'RealFakeBrain\'].vector_observations.shape[1] == \\\n           brain.vector_observation_space_size * brain.num_stacked_vector_observations\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_step(mock_communicator, mock_launcher):\n    mock_communicator.return_value = MockCommunicator(\n        discrete_action=False, visual_inputs=0)\n    env = UnityEnvironment(\' \')\n    brain = env.brains[\'RealFakeBrain\']\n    brain_info = env.reset()\n    brain_info = env.step([0] * brain.vector_action_space_size * len(brain_info[\'RealFakeBrain\'].agents))\n    with pytest.raises(UnityActionException):\n        env.step([0])\n    brain_info = env.step([-1] * brain.vector_action_space_size * len(brain_info[\'RealFakeBrain\'].agents))\n    with pytest.raises(UnityActionException):\n        env.step([0] * brain.vector_action_space_size * len(brain_info[\'RealFakeBrain\'].agents))\n    env.close()\n    assert env.global_done\n    assert isinstance(brain_info, dict)\n    assert isinstance(brain_info[\'RealFakeBrain\'], BrainInfo)\n    assert isinstance(brain_info[\'RealFakeBrain\'].visual_observations, list)\n    assert isinstance(brain_info[\'RealFakeBrain\'].vector_observations, np.ndarray)\n    assert len(brain_info[\'RealFakeBrain\'].visual_observations) == brain.number_visual_observations\n    assert brain_info[\'RealFakeBrain\'].vector_observations.shape[0] == \\\n           len(brain_info[\'RealFakeBrain\'].agents)\n    assert brain_info[\'RealFakeBrain\'].vector_observations.shape[1] == \\\n           brain.vector_observation_space_size * brain.num_stacked_vector_observations\n\n    print(""\\n\\n\\n\\n\\n\\n\\n"" + str(brain_info[\'RealFakeBrain\'].local_done))\n    assert not brain_info[\'RealFakeBrain\'].local_done[0]\n    assert brain_info[\'RealFakeBrain\'].local_done[2]\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_close(mock_communicator, mock_launcher):\n    comm = MockCommunicator(\n        discrete_action=False, visual_inputs=0)\n    mock_communicator.return_value = comm\n    env = UnityEnvironment(\' \')\n    assert env._loaded\n    env.close()\n    assert not env._loaded\n    assert comm.has_been_closed\n\n\ndef test_curriculum():\n    open_name = \'%s.open\' % __name__\n    with mock.patch(\'json.load\') as mock_load:\n        with mock.patch(open_name, create=True) as mock_open:\n            mock_open.return_value = 0\n            mock_load.return_value = bad_curriculum\n            with pytest.raises(UnityEnvironmentException):\n                Curriculum(\'tests/test_unityagents.py\', {""param1"": 1, ""param2"": 1, ""param3"": 1})\n            mock_load.return_value = dummy_curriculum\n            with pytest.raises(UnityEnvironmentException):\n                Curriculum(\'tests/test_unityagents.py\', {""param1"": 1, ""param2"": 1})\n            curriculum = Curriculum(\'tests/test_unityagents.py\', {""param1"": 1, ""param2"": 1, ""param3"": 1})\n            assert curriculum.get_lesson_number == 0\n            curriculum.set_lesson_number(1)\n            assert curriculum.get_lesson_number == 1\n            curriculum.increment_lesson(10)\n            assert curriculum.get_lesson_number == 1\n            curriculum.increment_lesson(30)\n            curriculum.increment_lesson(30)\n            assert curriculum.get_lesson_number == 1\n            assert curriculum.lesson_length == 3\n            curriculum.increment_lesson(30)\n            assert curriculum.get_config() == {\'param1\': 0.3, \'param2\': 20, \'param3\': 0.7}\n            assert curriculum.get_config(0) == {""param1"": 0.7, ""param2"": 100, ""param3"": 0.2}\n            assert curriculum.lesson_length == 0\n            assert curriculum.get_lesson_number == 2\n\n\nif __name__ == \'__main__\':\n    pytest.main()\n'"
python/tests/test_unitytrainers.py,0,"b'import yaml\nimport unittest.mock as mock\nimport pytest\n\nfrom unitytrainers.trainer_controller import TrainerController\nfrom unitytrainers.buffer import Buffer\nfrom unitytrainers.models import *\nfrom unitytrainers.ppo.trainer import PPOTrainer\nfrom unitytrainers.bc.trainer import BehavioralCloningTrainer\nfrom unityagents import UnityEnvironmentException\nfrom .mock_communicator import MockCommunicator\n\ndummy_start = \'\'\'{\n  ""AcademyName"": ""RealFakeAcademy"",\n  ""resetParameters"": {},\n  ""brainNames"": [""RealFakeBrain""],\n  ""externalBrainNames"": [""RealFakeBrain""],\n  ""logPath"":""RealFakePath"",\n  ""apiNumber"":""API-3"",\n  ""brainParameters"": [{\n      ""vectorObservationSize"": 3,\n      ""numStackedVectorObservations"" : 2,\n      ""vectorActionSize"": 2,\n      ""memorySize"": 0,\n      ""cameraResolutions"": [],\n      ""vectorActionDescriptions"": ["""",""""],\n      ""vectorActionSpaceType"": 1,\n      ""vectorObservationSpaceType"": 1\n      }]\n}\'\'\'.encode()\n\n\ndummy_config = yaml.load(\'\'\'\ndefault:\n    trainer: ppo\n    batch_size: 32\n    beta: 5.0e-3\n    buffer_size: 512\n    epsilon: 0.2\n    gamma: 0.99\n    hidden_units: 128\n    lambd: 0.95\n    learning_rate: 3.0e-4\n    max_steps: 5.0e4\n    normalize: true\n    num_epoch: 5\n    num_layers: 2\n    time_horizon: 64\n    sequence_length: 64\n    summary_freq: 1000\n    use_recurrent: false\n    memory_size: 8\n    use_curiosity: false\n    curiosity_strength: 0.0\n    curiosity_enc_size: 1\n\'\'\')\n\ndummy_bc_config = yaml.load(\'\'\'\ndefault:\n    trainer: imitation\n    brain_to_imitate: ExpertBrain\n    batches_per_epoch: 16\n    batch_size: 32\n    beta: 5.0e-3\n    buffer_size: 512\n    epsilon: 0.2\n    gamma: 0.99\n    hidden_units: 128\n    lambd: 0.95\n    learning_rate: 3.0e-4\n    max_steps: 5.0e4\n    normalize: true\n    num_epoch: 5\n    num_layers: 2\n    time_horizon: 64\n    sequence_length: 64\n    summary_freq: 1000\n    use_recurrent: false\n    memory_size: 8\n    use_curiosity: false\n    curiosity_strength: 0.0\n    curiosity_enc_size: 1\n\'\'\')\n\ndummy_bad_config = yaml.load(\'\'\'\ndefault:\n    trainer: incorrect_trainer\n    brain_to_imitate: ExpertBrain\n    batches_per_epoch: 16\n    batch_size: 32\n    beta: 5.0e-3\n    buffer_size: 512\n    epsilon: 0.2\n    gamma: 0.99\n    hidden_units: 128\n    lambd: 0.95\n    learning_rate: 3.0e-4\n    max_steps: 5.0e4\n    normalize: true\n    num_epoch: 5\n    num_layers: 2\n    time_horizon: 64\n    sequence_length: 64\n    summary_freq: 1000\n    use_recurrent: false\n    memory_size: 8\n\'\'\')\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_initialization(mock_communicator, mock_launcher):\n    mock_communicator.return_value = MockCommunicator(\n        discrete_action=True, visual_inputs=1)\n    tc = TrainerController(\' \', \' \', 1, None, True, True, False, 1,\n                           1, 1, 1, \'\', ""tests/test_unitytrainers.py"", False)\n    assert(tc.env.brain_names[0] == \'RealFakeBrain\')\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_load_config(mock_communicator, mock_launcher):\n    open_name = \'unitytrainers.trainer_controller\' + \'.open\'\n    with mock.patch(\'yaml.load\') as mock_load:\n        with mock.patch(open_name, create=True) as _:\n            mock_load.return_value = dummy_config\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=True, visual_inputs=1)\n            mock_load.return_value = dummy_config\n            tc = TrainerController(\' \', \' \', 1, None, True, True, False, 1,\n                                       1, 1, 1, \'\',\'\', False)\n            config = tc._load_config()\n            assert(len(config) == 1)\n            assert(config[\'default\'][\'trainer\'] == ""ppo"")\n\n\n@mock.patch(\'unityagents.UnityEnvironment.executable_launcher\')\n@mock.patch(\'unityagents.UnityEnvironment.get_communicator\')\ndef test_initialize_trainers(mock_communicator, mock_launcher):\n    open_name = \'unitytrainers.trainer_controller\' + \'.open\'\n    with mock.patch(\'yaml.load\') as mock_load:\n        with mock.patch(open_name, create=True) as _:\n            mock_communicator.return_value = MockCommunicator(\n                discrete_action=True, visual_inputs=1)\n            tc = TrainerController(\' \', \' \', 1, None, True, True, False, 1,\n                                   1, 1, 1, \'\', ""tests/test_unitytrainers.py"", False)\n\n            # Test for PPO trainer\n            mock_load.return_value = dummy_config\n            config = tc._load_config()\n            tf.reset_default_graph()\n            with tf.Session() as sess:\n                tc._initialize_trainers(config, sess)\n                assert(len(tc.trainers) == 1)\n                assert(isinstance(tc.trainers[\'RealFakeBrain\'], PPOTrainer))\n\n            # Test for Behavior Cloning Trainer\n            mock_load.return_value = dummy_bc_config\n            config = tc._load_config()\n            tf.reset_default_graph()\n            with tf.Session() as sess:\n                tc._initialize_trainers(config, sess)\n                assert(isinstance(tc.trainers[\'RealFakeBrain\'], BehavioralCloningTrainer))\n\n            # Test for proper exception when trainer name is incorrect\n            mock_load.return_value = dummy_bad_config\n            config = tc._load_config()\n            tf.reset_default_graph()\n            with tf.Session() as sess:\n                with pytest.raises(UnityEnvironmentException):\n                    tc._initialize_trainers(config, sess)\n\n\ndef assert_array(a, b):\n    assert a.shape == b.shape\n    la = list(a.flatten())\n    lb = list(b.flatten())\n    for i in range(len(la)):\n        assert la[i] == lb[i]\n\n\ndef test_buffer():\n    b = Buffer()\n    for fake_agent_id in range(4):\n        for step in range(9):\n            b[fake_agent_id][\'vector_observation\'].append(\n                [100 * fake_agent_id + 10 * step + 1,\n                 100 * fake_agent_id + 10 * step + 2,\n                 100 * fake_agent_id + 10 * step + 3]\n            )\n            b[fake_agent_id][\'action\'].append([100 * fake_agent_id + 10 * step + 4,\n                                               100 * fake_agent_id + 10 * step + 5])\n    a = b[1][\'vector_observation\'].get_batch(batch_size=2, training_length=1, sequential=True)\n    assert_array(a, np.array([[171, 172, 173], [181, 182, 183]]))\n    a = b[2][\'vector_observation\'].get_batch(batch_size=2, training_length=3, sequential=True)\n    assert_array(a, np.array([\n        [[231, 232, 233], [241, 242, 243], [251, 252, 253]],\n        [[261, 262, 263], [271, 272, 273], [281, 282, 283]]\n    ]))\n    a = b[2][\'vector_observation\'].get_batch(batch_size=2, training_length=3, sequential=False)\n    assert_array(a, np.array([\n        [[251, 252, 253], [261, 262, 263], [271, 272, 273]],\n        [[261, 262, 263], [271, 272, 273], [281, 282, 283]]\n    ]))\n    b[4].reset_agent()\n    assert len(b[4]) == 0\n    b.append_update_buffer(3,\n                           batch_size=None, training_length=2)\n    b.append_update_buffer(2,\n                           batch_size=None, training_length=2)\n    assert len(b.update_buffer[\'action\']) == 10\n    assert np.array(b.update_buffer[\'action\']).shape == (10, 2, 2)\n\n\nif __name__ == \'__main__\':\n    pytest.main()\n'"
python/unityagents/__init__.py,0,b'from .environment import *\nfrom .brain import *\nfrom .exception import *\nfrom .curriculum import *\n'
python/unityagents/brain.py,0,"b'from typing import Dict\n\n\nclass BrainInfo:\n    def __init__(self, visual_observation, vector_observation, text_observations, memory=None,\n                 reward=None, agents=None, local_done=None,\n                 vector_action=None, text_action=None, max_reached=None):\n        """"""\n        Describes experience at current step of all agents linked to a brain.\n        """"""\n        self.visual_observations = visual_observation\n        self.vector_observations = vector_observation\n        self.text_observations = text_observations\n        self.memories = memory\n        self.rewards = reward\n        self.local_done = local_done\n        self.max_reached = max_reached\n        self.agents = agents\n        self.previous_vector_actions = vector_action\n        self.previous_text_actions = text_action\n\n\nAllBrainInfo = Dict[str, BrainInfo]\n\n\nclass BrainParameters:\n    def __init__(self, brain_name, brain_param):\n        """"""\n        Contains all brain-specific parameters.\n        :param brain_name: Name of brain.\n        :param brain_param: Dictionary of brain parameters.\n        """"""\n        self.brain_name = brain_name\n        self.vector_observation_space_size = brain_param[""vectorObservationSize""]\n        self.num_stacked_vector_observations = brain_param[""numStackedVectorObservations""]\n        self.number_visual_observations = len(brain_param[""cameraResolutions""])\n        self.camera_resolutions = brain_param[""cameraResolutions""]\n        self.vector_action_space_size = brain_param[""vectorActionSize""]\n        self.vector_action_descriptions = brain_param[""vectorActionDescriptions""]\n        self.vector_action_space_type = [""discrete"", ""continuous""][brain_param[""vectorActionSpaceType""]]\n        self.vector_observation_space_type = [""discrete"", ""continuous""][brain_param[""vectorObservationSpaceType""]]\n\n    def __str__(self):\n        return \'\'\'Unity brain name: {0}\n        Number of Visual Observations (per agent): {1}\n        Vector Observation space type: {2}\n        Vector Observation space size (per agent): {3}\n        Number of stacked Vector Observation: {4}\n        Vector Action space type: {5}\n        Vector Action space size (per agent): {6}\n        Vector Action descriptions: {7}\'\'\'.format(self.brain_name,\n                                                  str(self.number_visual_observations),\n                                                  self.vector_observation_space_type,\n                                                  str(self.vector_observation_space_size),\n                                                  str(self.num_stacked_vector_observations),\n                                                  self.vector_action_space_type,\n                                                  str(self.vector_action_space_size),\n                                                  \', \'.join(self.vector_action_descriptions))\n'"
python/unityagents/communicator.py,0,"b'import logging\n\nfrom communicator_objects import UnityOutput, UnityInput\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(""unityagents"")\n\n\nclass Communicator(object):\n    def __init__(self, worker_id=0,\n                 base_port=5005):\n        """"""\n        Python side of the communication. Must be used in pair with the right Unity Communicator equivalent.\n\n        :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this.\n        :int worker_id: Number to add to communication port (5005) [0]. Used for asynchronous agent scenarios.\n        """"""\n\n    def initialize(self, inputs: UnityInput) -> UnityOutput:\n        """"""\n        Used to exchange initialization parameters between Python and the Environment\n        :param inputs: The initialization input that will be sent to the environment.\n        :return: UnityOutput: The initialization output sent by Unity\n        """"""\n\n    def exchange(self, inputs: UnityInput) -> UnityOutput:\n        """"""\n        Used to send an input and receive an output from the Environment\n        :param inputs: The UnityInput that needs to be sent the Environment\n        :return: The UnityOutputs generated by the Environment\n        """"""\n\n    def close(self):\n        """"""\n        Sends a shutdown signal to the unity environment, and closes the connection.\n        """"""\n\n'"
python/unityagents/curriculum.py,0,"b'import json\n\nfrom .exception import UnityEnvironmentException\n\nimport logging\n\nlogger = logging.getLogger(""unityagents"")\n\n\nclass Curriculum(object):\n    def __init__(self, location, default_reset_parameters):\n        """"""\n        Initializes a Curriculum object.\n        :param location: Path to JSON defining curriculum.\n        :param default_reset_parameters: Set of reset parameters for environment.\n        """"""\n        self.lesson_length = 0\n        self.max_lesson_number = 0\n        self.measure_type = None\n        if location is None:\n            self.data = None\n        else:\n            try:\n                with open(location) as data_file:\n                    self.data = json.load(data_file)\n            except IOError:\n                raise UnityEnvironmentException(\n                    ""The file {0} could not be found."".format(location))\n            except UnicodeDecodeError:\n                raise UnityEnvironmentException(""There was an error decoding {}"".format(location))\n            self.smoothing_value = 0\n            for key in [\'parameters\', \'measure\', \'thresholds\',\n                        \'min_lesson_length\', \'signal_smoothing\']:\n                if key not in self.data:\n                    raise UnityEnvironmentException(""{0} does not contain a ""\n                                                    ""{1} field."".format(location, key))\n            parameters = self.data[\'parameters\']\n            self.measure_type = self.data[\'measure\']\n            self.max_lesson_number = len(self.data[\'thresholds\'])\n            for key in parameters:\n                if key not in default_reset_parameters:\n                    raise UnityEnvironmentException(\n                        ""The parameter {0} in Curriculum {1} is not present in ""\n                        ""the Environment"".format(key, location))\n            for key in parameters:\n                if len(parameters[key]) != self.max_lesson_number + 1:\n                    raise UnityEnvironmentException(\n                        ""The parameter {0} in Curriculum {1} must have {2} values ""\n                        ""but {3} were found"".format(key, location,\n                                                    self.max_lesson_number + 1, len(parameters[key])))\n        self.set_lesson_number(0)\n\n    @property\n    def measure(self):\n        return self.measure_type\n\n    @property\n    def get_lesson_number(self):\n        return self.lesson_number\n\n    def set_lesson_number(self, value):\n        self.lesson_length = 0\n        self.lesson_number = max(0, min(value, self.max_lesson_number))\n\n    def increment_lesson(self, progress):\n        """"""\n        Increments the lesson number depending on the progree given.\n        :param progress: Measure of progress (either reward or percentage steps completed).\n        """"""\n        if self.data is None or progress is None:\n            return\n        if self.data[""signal_smoothing""]:\n            progress = self.smoothing_value * 0.25 + 0.75 * progress\n            self.smoothing_value = progress\n        self.lesson_length += 1\n        if self.lesson_number < self.max_lesson_number:\n            if ((progress > self.data[\'thresholds\'][self.lesson_number]) and\n                    (self.lesson_length > self.data[\'min_lesson_length\'])):\n                self.lesson_length = 0\n                self.lesson_number += 1\n                config = {}\n                parameters = self.data[""parameters""]\n                for key in parameters:\n                    config[key] = parameters[key][self.lesson_number]\n                logger.info(""\\nLesson changed. Now in Lesson {0} : \\t{1}""\n                            .format(self.lesson_number,\n                                    \', \'.join([str(x) + \' -> \' + str(config[x]) for x in config])))\n\n    def get_config(self, lesson=None):\n        """"""\n        Returns reset parameters which correspond to the lesson.\n        :param lesson: The lesson you want to get the config of. If None, the current lesson is returned.\n        :return: The configuration of the reset parameters.\n        """"""\n        if self.data is None:\n            return {}\n        if lesson is None:\n            lesson = self.lesson_number\n        lesson = max(0, min(lesson, self.max_lesson_number))\n        config = {}\n        parameters = self.data[""parameters""]\n        for key in parameters:\n            config[key] = parameters[key][lesson]\n        return config\n'"
python/unityagents/environment.py,0,"b'import atexit\nimport glob\nimport io\nimport logging\nimport numpy as np\nimport os\nimport subprocess\n\nfrom .brain import BrainInfo, BrainParameters, AllBrainInfo\nfrom .exception import UnityEnvironmentException, UnityActionException, UnityTimeOutException\nfrom .curriculum import Curriculum\n\nfrom communicator_objects import UnityRLInput, UnityRLOutput, AgentActionProto,\\\n    EnvironmentParametersProto, UnityRLInitializationInput, UnityRLInitializationOutput,\\\n    UnityInput, UnityOutput\n\nfrom .rpc_communicator import RpcCommunicator\nfrom .socket_communicator import SocketCommunicator\n\n\nfrom sys import platform\nfrom PIL import Image\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(""unityagents"")\n\n\nclass UnityEnvironment(object):\n    def __init__(self, file_name=None, worker_id=0,\n                 base_port=5005, curriculum=None,\n                 seed=0, docker_training=False, no_graphics=False):\n        """"""\n        Starts a new unity environment and establishes a connection with the environment.\n        Notice: Currently communication between Unity and Python takes place over an open socket without authentication.\n        Ensure that the network where training takes place is secure.\n\n        :string file_name: Name of Unity environment binary.\n        :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this.\n        :int worker_id: Number to add to communication port (5005) [0]. Used for asynchronous agent scenarios.\n        :param docker_training: Informs this class whether the process is being run within a container.\n        :param no_graphics: Whether to run the Unity simulator in no-graphics mode\n        """"""\n\n        atexit.register(self._close)\n        self.port = base_port + worker_id\n        self._buffer_size = 12000\n        self._version_ = ""API-4""\n        self._loaded = False    # If true, this means the environment was successfully loaded\n        self.proc1 = None       # The process that is started. If None, no process was started\n        self.communicator = self.get_communicator(worker_id, base_port)\n\n        # If the environment name is \'editor\', a new environment will not be launched\n        # and the communicator will directly try to connect to an existing unity environment.\n        if file_name is not None:\n            self.executable_launcher(file_name, docker_training, no_graphics)\n        else:\n            logger.info(""Start training by pressing the Play button in the Unity Editor."")\n        self._loaded = True\n\n        rl_init_parameters_in = UnityRLInitializationInput(\n            seed=seed\n        )\n        try:\n            aca_params = self.send_academy_parameters(rl_init_parameters_in)\n        except UnityTimeOutException:\n            self._close()\n            raise\n        # TODO : think of a better way to expose the academyParameters\n        self._unity_version = aca_params.version\n        if self._unity_version != self._version_:\n            raise UnityEnvironmentException(\n                ""The API number is not compatible between Unity and python. Python API : {0}, Unity API : ""\n                ""{1}.\\nPlease go to https://github.com/Unity-Technologies/ml-agents to download the latest version ""\n                ""of ML-Agents."".format(self._version_, self._unity_version))\n        self._n_agents = {}\n        self._global_done = None\n        self._academy_name = aca_params.name\n        self._log_path = aca_params.log_path\n        self._brains = {}\n        self._brain_names = []\n        self._external_brain_names = []\n        for brain_param in aca_params.brain_parameters:\n            self._brain_names += [brain_param.brain_name]\n            resolution = [{\n                ""height"": x.height,\n                ""width"": x.width,\n                ""blackAndWhite"": x.gray_scale\n            } for x in brain_param.camera_resolutions]\n            self._brains[brain_param.brain_name] = \\\n                BrainParameters(brain_param.brain_name, {\n                    ""vectorObservationSize"": brain_param.vector_observation_size,\n                    ""numStackedVectorObservations"": brain_param.num_stacked_vector_observations,\n                    ""cameraResolutions"": resolution,\n                    ""vectorActionSize"": brain_param.vector_action_size,\n                    ""vectorActionDescriptions"": brain_param.vector_action_descriptions,\n                    ""vectorActionSpaceType"": brain_param.vector_action_space_type,\n                    ""vectorObservationSpaceType"": brain_param.vector_observation_space_type\n                })\n            if brain_param.brain_type == 2:\n                self._external_brain_names += [brain_param.brain_name]\n        self._num_brains = len(self._brain_names)\n        self._num_external_brains = len(self._external_brain_names)\n        self._resetParameters = dict(aca_params.environment_parameters.float_parameters) # TODO\n        self._curriculum = Curriculum(curriculum, self._resetParameters)\n        logger.info(""\\n\'{0}\' started successfully!\\n{1}"".format(self._academy_name, str(self)))\n        if self._num_external_brains == 0:\n            logger.warning("" No External Brains found in the Unity Environment. ""\n                           ""You will not be able to pass actions to your agent(s)."")\n\n    @property\n    def curriculum(self):\n        return self._curriculum\n\n    @property\n    def logfile_path(self):\n        return self._log_path\n\n    @property\n    def brains(self):\n        return self._brains\n\n    @property\n    def global_done(self):\n        return self._global_done\n\n    @property\n    def academy_name(self):\n        return self._academy_name\n\n    @property\n    def number_brains(self):\n        return self._num_brains\n\n    @property\n    def number_external_brains(self):\n        return self._num_external_brains\n\n    @property\n    def brain_names(self):\n        return self._brain_names\n\n    @property\n    def external_brain_names(self):\n        return self._external_brain_names\n\n    def executable_launcher(self, file_name, docker_training, no_graphics):\n        cwd = os.getcwd()\n        file_name = (file_name.strip()\n                     .replace(\'.app\', \'\').replace(\'.exe\', \'\').replace(\'.x86_64\', \'\').replace(\'.x86\', \'\'))\n        true_filename = os.path.basename(os.path.normpath(file_name))\n        logger.debug(\'The true file name is {}\'.format(true_filename))\n        launch_string = None\n        if platform == ""linux"" or platform == ""linux2"":\n            candidates = glob.glob(os.path.join(cwd, file_name) + \'.x86_64\')\n            if len(candidates) == 0:\n                candidates = glob.glob(os.path.join(cwd, file_name) + \'.x86\')\n            if len(candidates) == 0:\n                candidates = glob.glob(file_name + \'.x86_64\')\n            if len(candidates) == 0:\n                candidates = glob.glob(file_name + \'.x86\')\n            if len(candidates) > 0:\n                launch_string = candidates[0]\n\n        elif platform == \'darwin\':\n            candidates = glob.glob(os.path.join(cwd, file_name + \'.app\', \'Contents\', \'MacOS\', true_filename))\n            if len(candidates) == 0:\n                candidates = glob.glob(os.path.join(file_name + \'.app\', \'Contents\', \'MacOS\', true_filename))\n            if len(candidates) == 0:\n                candidates = glob.glob(os.path.join(cwd, file_name + \'.app\', \'Contents\', \'MacOS\', \'*\'))\n            if len(candidates) == 0:\n                candidates = glob.glob(os.path.join(file_name + \'.app\', \'Contents\', \'MacOS\', \'*\'))\n            if len(candidates) > 0:\n                launch_string = candidates[0]\n        elif platform == \'win32\':\n            candidates = glob.glob(os.path.join(cwd, file_name + \'.exe\'))\n            if len(candidates) == 0:\n                candidates = glob.glob(file_name + \'.exe\')\n            if len(candidates) > 0:\n                launch_string = candidates[0]\n        if launch_string is None:\n            self._close()\n            raise UnityEnvironmentException(""Couldn\'t launch the {0} environment. ""\n                                            ""Provided filename does not match any environments.""\n                                            .format(true_filename))\n        else:\n            logger.debug(""This is the launch string {}"".format(launch_string))\n            # Launch Unity environment\n            if not docker_training:\n                if no_graphics:\n                    self.proc1 = subprocess.Popen(\n                        [launch_string,\'-nographics\', \'-batchmode\',\n                         \'--port\', str(self.port)])\n                else:\n                    self.proc1 = subprocess.Popen(\n                        [launch_string, \'--port\', str(self.port)])\n            else:\n                """"""\n                Comments for future maintenance:\n                    xvfb-run is a wrapper around Xvfb, a virtual xserver where all\n                    rendering is done to virtual memory. It automatically creates a\n                    new virtual server automatically picking a server number `auto-servernum`.\n                    The server is passed the arguments using `server-args`, we are telling\n                    Xvfb to create Screen number 0 with width 640, height 480 and depth 24 bits.\n                    Note that 640 X 480 are the default width and height. The main reason for\n                    us to add this is because we\'d like to change the depth from the default\n                    of 8 bits to 24.\n                    Unfortunately, this means that we will need to pass the arguments through\n                    a shell which is why we set `shell=True`. Now, this adds its own\n                    complications. E.g SIGINT can bounce off the shell and not get propagated\n                    to the child processes. This is why we add `exec`, so that the shell gets\n                    launched, the arguments are passed to `xvfb-run`. `exec` replaces the shell\n                    we created with `xvfb`.\n                """"""\n                docker_ls = (""exec xvfb-run --auto-servernum""\n                             "" --server-args=\'-screen 0 640x480x24\'""\n                             "" {0} --port {1}"").format(launch_string, str(self.port))\n                self.proc1 = subprocess.Popen(docker_ls,\n                                              stdout=subprocess.PIPE,\n                                              stderr=subprocess.PIPE,\n                                              shell=True)\n\n    def get_communicator(self, worker_id, base_port):\n        return RpcCommunicator(worker_id, base_port)\n        # return SocketCommunicator(worker_id, base_port)\n\n    def __str__(self):\n        _new_reset_param = self._curriculum.get_config()\n        for k in _new_reset_param:\n            self._resetParameters[k] = _new_reset_param[k]\n        return \'\'\'Unity Academy name: {0}\n        Number of Brains: {1}\n        Number of External Brains : {2}\n        Lesson number : {3}\n        Reset Parameters :\\n\\t\\t{4}\'\'\'.format(self._academy_name, str(self._num_brains),\n                                 str(self._num_external_brains), self._curriculum.get_lesson_number,\n                                  ""\\n\\t\\t"".join([str(k) + "" -> "" + str(self._resetParameters[k])\n                                         for k in self._resetParameters])) + \'\\n\' + \\\n               \'\\n\'.join([str(self._brains[b]) for b in self._brains])\n\n    def reset(self, train_mode=True, config=None, lesson=None) -> AllBrainInfo:\n        """"""\n        Sends a signal to reset the unity environment.\n        :return: AllBrainInfo  : A Data structure corresponding to the initial reset state of the environment.\n        """"""\n        if config is None:\n            config = self._curriculum.get_config(lesson)\n        elif config != {}:\n            logger.info(""\\nAcademy Reset with parameters : \\t{0}""\n                        .format(\', \'.join([str(x) + \' -> \' + str(config[x]) for x in config])))\n        for k in config:\n            if (k in self._resetParameters) and (isinstance(config[k], (int, float))):\n                self._resetParameters[k] = config[k]\n            elif not isinstance(config[k], (int, float)):\n                raise UnityEnvironmentException(\n                    ""The value for parameter \'{0}\'\' must be an Integer or a Float."".format(k))\n            else:\n                raise UnityEnvironmentException(""The parameter \'{0}\' is not a valid parameter."".format(k))\n\n        if self._loaded:\n            outputs = self.communicator.exchange(\n                self._generate_reset_input(train_mode, config)\n            )\n            if outputs is None:\n                raise KeyboardInterrupt\n            rl_output = outputs.rl_output\n            s = self._get_state(rl_output)\n            self._global_done = s[1]\n            for _b in self._external_brain_names:\n                self._n_agents[_b] = len(s[0][_b].agents)\n            return s[0]\n        else:\n            raise UnityEnvironmentException(""No Unity environment is loaded."")\n\n    def step(self,  vector_action=None, memory=None, text_action=None) -> AllBrainInfo:\n        """"""\n        Provides the environment with an action, moves the environment dynamics forward accordingly, and returns\n        observation, state, and reward information to the agent.\n        :param vector_action: Agent\'s vector action to send to environment. Can be a scalar or vector of int/floats.\n        :param memory: Vector corresponding to memory used for RNNs, frame-stacking, or other auto-regressive process.\n        :param text_action: Text action to send to environment for.\n        :return: AllBrainInfo  : A Data structure corresponding to the new state of the environment.\n        """"""\n        vector_action = {} if vector_action is None else vector_action\n        memory = {} if memory is None else memory\n        text_action = {} if text_action is None else text_action\n        if self._loaded and not self._global_done and self._global_done is not None:\n            if isinstance(vector_action, (int, np.int_, float, np.float_, list, np.ndarray)):\n                if self._num_external_brains == 1:\n                    vector_action = {self._external_brain_names[0]: vector_action}\n                elif self._num_external_brains > 1:\n                    raise UnityActionException(\n                        ""You have {0} brains, you need to feed a dictionary of brain names a keys, ""\n                        ""and vector_actions as values"".format(self._num_brains))\n                else:\n                    raise UnityActionException(\n                        ""There are no external brains in the environment, ""\n                        ""step cannot take a vector_action input"")\n\n            if isinstance(memory, (int, np.int_, float, np.float_, list, np.ndarray)):\n                if self._num_external_brains == 1:\n                    memory = {self._external_brain_names[0]: memory}\n                elif self._num_external_brains > 1:\n                    raise UnityActionException(\n                        ""You have {0} brains, you need to feed a dictionary of brain names as keys ""\n                        ""and memories as values"".format(self._num_brains))\n                else:\n                    raise UnityActionException(\n                        ""There are no external brains in the environment, ""\n                        ""step cannot take a memory input"")\n            if isinstance(text_action, (str, list, np.ndarray)):\n                if self._num_external_brains == 1:\n                    text_action = {self._external_brain_names[0]: text_action}\n                elif self._num_external_brains > 1:\n                    raise UnityActionException(\n                        ""You have {0} brains, you need to feed a dictionary of brain names as keys ""\n                        ""and text_actions as values"".format(self._num_brains))\n                else:\n                    raise UnityActionException(\n                        ""There are no external brains in the environment, ""\n                        ""step cannot take a value input"")\n\n            for brain_name in list(vector_action.keys()) + list(memory.keys()) + list(text_action.keys()):\n                if brain_name not in self._external_brain_names:\n                    raise UnityActionException(\n                        ""The name {0} does not correspond to an external brain ""\n                        ""in the environment"".format(brain_name))\n\n            for b in self._external_brain_names:\n                n_agent = self._n_agents[b]\n                if b not in vector_action:\n                    # raise UnityActionException(""You need to input an action for the brain {0}"".format(b))\n                    if self._brains[b].vector_action_space_type == ""discrete"":\n                        vector_action[b] = [0.0] * n_agent\n                    else:\n                        vector_action[b] = [0.0] * n_agent * self._brains[b].vector_action_space_size\n                else:\n                    vector_action[b] = self._flatten(vector_action[b])\n                if b not in memory:\n                    memory[b] = []\n                else:\n                    if memory[b] is None:\n                        memory[b] = []\n                    else:\n                        memory[b] = self._flatten(memory[b])\n                if b not in text_action:\n                    text_action[b] = [""""] * n_agent\n                else:\n                    if text_action[b] is None:\n                        text_action[b] = [""""] * n_agent\n                    if isinstance(text_action[b], str):\n                        text_action[b] = [text_action[b]] * n_agent\n                if not ((len(text_action[b]) == n_agent) or len(text_action[b]) == 0):\n                    raise UnityActionException(\n                        ""There was a mismatch between the provided text_action and environment\'s expectation: ""\n                        ""The brain {0} expected {1} text_action but was given {2}"".format(\n                            b, n_agent, len(text_action[b])))\n                if not ((self._brains[b].vector_action_space_type == ""discrete"" and len(vector_action[b]) == n_agent) or\n                            (self._brains[b].vector_action_space_type == ""continuous"" and len(\n                                vector_action[b]) == self._brains[b].vector_action_space_size * n_agent)):\n                    raise UnityActionException(\n                        ""There was a mismatch between the provided action and environment\'s expectation: ""\n                        ""The brain {0} expected {1} {2} action(s), but was provided: {3}""\n                        .format(b, n_agent if self._brains[b].vector_action_space_type == ""discrete"" else\n                        str(self._brains[b].vector_action_space_size * n_agent),\n                        self._brains[b].vector_action_space_type,\n                        str(vector_action[b])))\n\n            outputs = self.communicator.exchange(\n                self._generate_step_input(vector_action, memory, text_action)\n            )\n            if outputs is None:\n                raise KeyboardInterrupt\n            rl_output = outputs.rl_output\n            s = self._get_state(rl_output)\n            self._global_done = s[1]\n            for _b in self._external_brain_names:\n                self._n_agents[_b] = len(s[0][_b].agents)\n            return s[0]\n        elif not self._loaded:\n            raise UnityEnvironmentException(""No Unity environment is loaded."")\n        elif self._global_done:\n            raise UnityActionException(""The episode is completed. Reset the environment with \'reset()\'"")\n        elif self.global_done is None:\n            raise UnityActionException(\n                ""You cannot conduct step without first calling reset. Reset the environment with \'reset()\'"")\n\n    def close(self):\n        """"""\n        Sends a shutdown signal to the unity environment, and closes the socket connection.\n        """"""\n        if self._loaded:\n            self._close()\n        else:\n            raise UnityEnvironmentException(""No Unity environment is loaded."")\n\n    def _close(self):\n        self._loaded = False\n        self.communicator.close()\n        if self.proc1 is not None:\n            self.proc1.kill()\n\n    @staticmethod\n    def _flatten(arr):\n        """"""\n        Converts arrays to list.\n        :param arr: numpy vector.\n        :return: flattened list.\n        """"""\n        if isinstance(arr, (int, np.int_, float, np.float_)):\n            arr = [float(arr)]\n        if isinstance(arr, np.ndarray):\n            arr = arr.tolist()\n        if len(arr) == 0:\n            return arr\n        if isinstance(arr[0], np.ndarray):\n            arr = [item for sublist in arr for item in sublist.tolist()]\n        if isinstance(arr[0], list):\n            arr = [item for sublist in arr for item in sublist]\n        arr = [float(x) for x in arr]\n        return arr\n\n    @staticmethod\n    def _process_pixels(image_bytes, gray_scale):\n        """"""\n        Converts byte array observation image into numpy array, re-sizes it, and optionally converts it to grey scale\n        :param image_bytes: input byte array corresponding to image\n        :return: processed numpy array of observation from environment\n        """"""\n        s = bytearray(image_bytes)\n        image = Image.open(io.BytesIO(s))\n        s = np.array(image) / 255.0\n        if gray_scale:\n            s = np.mean(s, axis=2)\n            s = np.reshape(s, [s.shape[0], s.shape[1], 1])\n        return s\n\n    def _get_state(self, output: UnityRLOutput) -> (AllBrainInfo, bool):\n        """"""\n        Collects experience information from all external brains in environment at current step.\n        :return: a dictionary of BrainInfo objects.\n        """"""\n        _data = {}\n        global_done = output.global_done\n        for b in output.agentInfos:\n            agent_info_list = output.agentInfos[b].value\n            vis_obs = []\n            for i in range(self.brains[b].number_visual_observations):\n                obs = [self._process_pixels(x.visual_observations[i],\n                                            self.brains[b].camera_resolutions[i][\'blackAndWhite\'])\n                    for x in agent_info_list]\n                vis_obs += [np.array(obs)]\n            if len(agent_info_list) == 0:\n                memory_size = 0\n            else:\n                memory_size = max([len(x.memories) for x in agent_info_list])\n            if memory_size == 0:\n                memory = np.zeros((0, 0))\n            else:\n                [x.memories.extend([0] * (memory_size - len(x.memories))) for x in agent_info_list]\n                memory = np.array([x.memories for x in agent_info_list])\n            _data[b] = BrainInfo(\n                visual_observation=vis_obs,\n                vector_observation=np.array([x.stacked_vector_observation for x in agent_info_list]),\n                text_observations=[x.text_observation for x in agent_info_list],\n                memory=memory,\n                reward=[x.reward for x in agent_info_list],\n                agents=[x.id for x in agent_info_list],\n                local_done=[x.done for x in agent_info_list],\n                vector_action=np.array([x.stored_vector_actions for x in agent_info_list]),\n                text_action=[x.stored_text_actions for x in agent_info_list],\n                max_reached=[x.max_step_reached for x in agent_info_list]\n                )\n        return _data, global_done\n\n    def _generate_step_input(self, vector_action, memory, text_action) -> UnityRLInput:\n        rl_in = UnityRLInput()\n        for b in vector_action:\n            n_agents = self._n_agents[b]\n            if n_agents == 0:\n                continue\n            _a_s = len(vector_action[b]) // n_agents\n            _m_s = len(memory[b]) // n_agents\n            for i in range(n_agents):\n                action = AgentActionProto(\n                    vector_actions=vector_action[b][i*_a_s: (i+1)*_a_s],\n                    memories=memory[b][i*_m_s: (i+1)*_m_s],\n                    text_actions=text_action[b][i]\n                )\n                rl_in.agent_actions[b].value.extend([action])\n                rl_in.command = 0\n        return self.wrap_unity_input(rl_in)\n\n    def _generate_reset_input(self, training, config) -> UnityRLInput:\n        rl_in = UnityRLInput()\n        rl_in.is_training = training\n        rl_in.environment_parameters.CopyFrom(EnvironmentParametersProto())\n        for key in config:\n            rl_in.environment_parameters.float_parameters[key] = config[key]\n        rl_in.command = 1\n        return self.wrap_unity_input(rl_in)\n\n    def send_academy_parameters(self, init_parameters: UnityRLInitializationInput) -> UnityRLInitializationOutput:\n        inputs = UnityInput()\n        inputs.rl_initialization_input.CopyFrom(init_parameters)\n        return self.communicator.initialize(inputs).rl_initialization_output\n\n    def wrap_unity_input(self, rl_input: UnityRLInput) -> UnityOutput:\n        result = UnityInput()\n        result.rl_input.CopyFrom(rl_input)\n        return result\n'"
python/unityagents/exception.py,0,"b'import logging\nlogger = logging.getLogger(""unityagents"")\n\nclass UnityException(Exception):\n    """"""\n    Any error related to ml-agents environment.\n    """"""\n    pass\n\nclass UnityEnvironmentException(UnityException):\n    """"""\n    Related to errors starting and closing environment.\n    """"""\n    pass\n\n\nclass UnityActionException(UnityException):\n    """"""\n    Related to errors with sending actions.\n    """"""\n    pass\n\nclass UnityTimeOutException(UnityException):\n    """"""\n    Related to errors with communication timeouts.\n    """"""\n    def __init__(self, message, log_file_path = None):\n        if log_file_path is not None:\n            try:\n                with open(log_file_path, ""r"") as f:\n                    printing = False\n                    unity_error = \'\\n\'\n                    for l in f:\n                        l=l.strip()\n                        if (l == \'Exception\') or (l==\'Error\'):\n                            printing = True\n                            unity_error += \'----------------------\\n\'\n                        if (l == \'\'):\n                            printing = False\n                        if printing:\n                            unity_error += l + \'\\n\'\n                    logger.info(unity_error)\n                    logger.error(""An error might have occured in the environment. ""\n                        ""You can check the logfile for more information at {}"".format(log_file_path))\n            except:\n                logger.error(""An error might have occured in the environment. ""\n               ""No unity-environment.log file could be found."") \n        super(UnityTimeOutException, self).__init__(message)\n\n'"
python/unityagents/rpc_communicator.py,0,"b'import logging\nimport grpc\n\nfrom multiprocessing import Pipe\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom .communicator import Communicator\nfrom communicator_objects import UnityToExternalServicer, add_UnityToExternalServicer_to_server\nfrom communicator_objects import UnityMessage, UnityInput, UnityOutput\nfrom .exception import UnityTimeOutException\n\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(""unityagents"")\n\n\nclass UnityToExternalServicerImplementation(UnityToExternalServicer):\n    parent_conn, child_conn = Pipe()\n\n    def Initialize(self, request, context):\n        self.child_conn.send(request)\n        return self.child_conn.recv()\n\n    def Exchange(self, request, context):\n        self.child_conn.send(request)\n        return self.child_conn.recv()\n\n\nclass RpcCommunicator(Communicator):\n    def __init__(self, worker_id=0,\n                 base_port=5005):\n        """"""\n        Python side of the grpc communication. Python is the server and Unity the client\n\n\n        :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this.\n        :int worker_id: Number to add to communication port (5005) [0]. Used for asynchronous agent scenarios.\n        """"""\n        self.port = base_port + worker_id\n        self.worker_id = worker_id\n        self.server = None\n        self.unity_to_external = None\n        self.is_open = False\n\n    def initialize(self, inputs: UnityInput) -> UnityOutput:\n        try:\n            # Establish communication grpc\n            self.server = grpc.server(ThreadPoolExecutor(max_workers=10))\n            self.unity_to_external = UnityToExternalServicerImplementation()\n            add_UnityToExternalServicer_to_server(self.unity_to_external, self.server)\n            self.server.add_insecure_port(\'[::]:\'+str(self.port))\n            self.server.start()\n        except :\n            raise UnityTimeOutException(\n                ""Couldn\'t start socket communication because worker number {} is still in use. ""\n                ""You may need to manually close a previously opened environment ""\n                ""or use a different worker number."".format(str(self.worker_id)))\n        if not self.unity_to_external.parent_conn.poll(30):\n            raise UnityTimeOutException(\n                ""The Unity environment took too long to respond. Make sure that :\\n""\n                ""\\t The environment does not need user interaction to launch\\n""\n                ""\\t The Academy and the External Brain(s) are attached to objects in the Scene\\n""\n                ""\\t The environment and the Python interface have compatible versions."")\n        aca_param = self.unity_to_external.parent_conn.recv().unity_output\n        self.is_open = True\n        message = UnityMessage()\n        message.header.status = 200\n        message.unity_input.CopyFrom(inputs)\n        self.unity_to_external.parent_conn.send(message)\n        self.unity_to_external.parent_conn.recv()\n        return aca_param\n\n    def exchange(self, inputs: UnityInput) -> UnityOutput:\n        message = UnityMessage()\n        message.header.status = 200\n        message.unity_input.CopyFrom(inputs)\n        self.unity_to_external.parent_conn.send(message)\n        output = self.unity_to_external.parent_conn.recv()\n        if output.header.status != 200:\n            return None\n        return output.unity_output\n\n    def close(self):\n        """"""\n        Sends a shutdown signal to the unity environment, and closes the grpc connection.\n        """"""\n        if self.is_open:\n            message_input = UnityMessage()\n            message_input.header.status = 400\n            self.unity_to_external.parent_conn.send(message_input)\n            self.unity_to_external.parent_conn.close()\n            self.server.stop(False)\n            self.is_open = False\n\n\n\n\n'"
python/unityagents/socket_communicator.py,0,"b'import logging\nimport socket\nimport struct\n\nfrom .communicator import Communicator\nfrom communicator_objects import UnityMessage, UnityOutput, UnityInput\nfrom .exception import UnityTimeOutException\n\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(""unityagents"")\n\n\nclass SocketCommunicator(Communicator):\n    def __init__(self, worker_id=0,\n                 base_port=5005):\n        """"""\n        Python side of the socket communication\n\n        :int base_port: Baseline port number to connect to Unity environment over. worker_id increments over this.\n        :int worker_id: Number to add to communication port (5005) [0]. Used for asynchronous agent scenarios.\n        """"""\n\n        self.port = base_port + worker_id\n        self._buffer_size = 12000\n        self.worker_id = worker_id\n        self._socket = None\n        self._conn = None\n\n    def initialize(self, inputs: UnityInput) -> UnityOutput:\n        try:\n            # Establish communication socket\n            self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            self._socket.bind((""localhost"", self.port))\n        except:\n            raise UnityTimeOutException(""Couldn\'t start socket communication because worker number {} is still in use. ""\n                                        ""You may need to manually close a previously opened environment ""\n                                        ""or use a different worker number."".format(str(self.worker_id)))\n        try:\n            self._socket.settimeout(30)\n            self._socket.listen(1)\n            self._conn, _ = self._socket.accept()\n            self._conn.settimeout(30)\n        except :\n            raise UnityTimeOutException(\n                ""The Unity environment took too long to respond. Make sure that :\\n""\n                ""\\t The environment does not need user interaction to launch\\n""\n                ""\\t The Academy and the External Brain(s) are attached to objects in the Scene\\n""\n                ""\\t The environment and the Python interface have compatible versions."")\n        message = UnityMessage()\n        message.header.status = 200\n        message.unity_input.CopyFrom(inputs)\n        self._communicator_send(message.SerializeToString())\n        initialization_output = UnityMessage()\n        initialization_output.ParseFromString(self._communicator_receive())\n        return initialization_output.unity_output\n\n    def _communicator_receive(self):\n        try:\n            s = self._conn.recv(self._buffer_size)\n            message_length = struct.unpack(""I"", bytearray(s[:4]))[0]\n            s = s[4:]\n            while len(s) != message_length:\n                s += self._conn.recv(self._buffer_size)\n        except socket.timeout as e:\n            raise UnityTimeOutException(""The environment took too long to respond."")\n        return s\n\n    def _communicator_send(self, message):\n        self._conn.send(struct.pack(""I"", len(message)) + message)\n\n    def exchange(self, inputs: UnityInput) -> UnityOutput:\n        message = UnityMessage()\n        message.header.status = 200\n        message.unity_input.CopyFrom(inputs)\n        self._communicator_send(message.SerializeToString())\n        outputs = UnityMessage()\n        outputs.ParseFromString(self._communicator_receive())\n        if outputs.header.status != 200:\n            return None\n        return outputs.unity_output\n\n    def close(self):\n        """"""\n        Sends a shutdown signal to the unity environment, and closes the socket connection.\n        """"""\n        if self._socket is not None and self._conn is not None:\n            message_input = UnityMessage()\n            message_input.header.status = 400\n            self._communicator_send(message_input.SerializeToString())\n        if self._socket is not None:\n            self._socket.close()\n            self._socket = None\n        if self._socket is not None:\n            self._conn.close()\n            self._conn = None\n\n'"
python/unitytrainers/__init__.py,0,b'from .buffer import *\nfrom .models import *\nfrom .trainer_controller import *\nfrom .bc.models import *\nfrom .bc.trainer import *\nfrom .ppo.models import *\nfrom .ppo.trainer import *\n'
python/unitytrainers/buffer.py,0,"b'import numpy as np\n\nfrom unityagents.exception import UnityException\n\n\nclass BufferException(UnityException):\n    """"""\n    Related to errors with the Buffer.\n    """"""\n    pass\n\n\nclass Buffer(dict):\n    """"""\n    Buffer contains a dictionary of AgentBuffer. The AgentBuffers are indexed by agent_id.\n    Buffer also contains an update_buffer that corresponds to the buffer used when updating the model.\n    """"""\n\n    class AgentBuffer(dict):\n        """"""\n        AgentBuffer contains a dictionary of AgentBufferFields. Each agent has his own AgentBuffer.\n        The keys correspond to the name of the field. Example: state, action\n        """"""\n\n        class AgentBufferField(list):\n            """"""\n            AgentBufferField is a list of numpy arrays. When an agent collects a field, you can add it to his\n            AgentBufferField with the append method.\n            """"""\n\n            def __str__(self):\n                return str(np.array(self).shape)\n\n            def extend(self, data):\n                """"""\n                Ads a list of np.arrays to the end of the list of np.arrays.\n                :param data: The np.array list to append.\n                """"""\n                self += list(np.array(data))\n\n            def set(self, data):\n                """"""\n                Sets the list of np.array to the input data\n                :param data: The np.array list to be set.\n                """"""\n                self[:] = []\n                self[:] = list(np.array(data))\n\n            def get_batch(self, batch_size=None, training_length=1, sequential=True):\n                """"""\n                Retrieve the last batch_size elements of length training_length\n                from the list of np.array\n                :param batch_size: The number of elements to retrieve. If None:\n                All elements will be retrieved.\n                :param training_length: The length of the sequence to be retrieved. If\n                None: only takes one element.\n                :param sequential: If true and training_length is not None: the elements\n                will not repeat in the sequence. [a,b,c,d,e] with training_length = 2 and\n                sequential=True gives [[0,a],[b,c],[d,e]]. If sequential=False gives\n                [[a,b],[b,c],[c,d],[d,e]]\n                """"""\n                if training_length == 1:\n                    # When the training length is 1, the method returns a list of elements,\n                    # not a list of sequences of elements.\n                    if batch_size is None:\n                        # If batch_size is None : All the elements of the AgentBufferField are returned.\n                        return np.array(self)\n                    else:\n                        # return the batch_size last elements\n                        if batch_size > len(self):\n                            raise BufferException(""Batch size requested is too large"")\n                        return np.array(self[-batch_size:])\n                else:\n                    # The training_length is not None, the method returns a list of SEQUENCES of elements\n                    if not sequential:\n                        # The sequences will have overlapping elements\n                        if batch_size is None:\n                            # retrieve the maximum number of elements\n                            batch_size = len(self) - training_length + 1\n                        # The number of sequences of length training_length taken from a list of len(self) elements\n                        # with overlapping is equal to batch_size\n                        if (len(self) - training_length + 1) < batch_size:\n                            raise BufferException(""The batch size and training length requested for get_batch where""\n                                                  "" too large given the current number of data points."")\n                        tmp_list = []\n                        for end in range(len(self) - batch_size + 1, len(self) + 1):\n                            tmp_list += [np.array(self[end - training_length:end])]\n                        return np.array(tmp_list)\n                    if sequential:\n                        # The sequences will not have overlapping elements (this involves padding)\n                        leftover = len(self) % training_length\n                        # leftover is the number of elements in the first sequence (this sequence might need 0 padding)\n                        if batch_size is None:\n                            # retrieve the maximum number of elements\n                            batch_size = len(self) // training_length + 1 * (leftover != 0)\n                        # The maximum number of sequences taken from a list of length len(self) without overlapping\n                        # with padding is equal to batch_size\n                        if batch_size > (len(self) // training_length + 1 * (leftover != 0)):\n                            raise BufferException(""The batch size and training length requested for get_batch where""\n                                                  "" too large given the current number of data points."")\n                        tmp_list = []\n                        padding = np.array(self[-1]) * 0\n                        # The padding is made with zeros and its shape is given by the shape of the last element\n                        for end in range(len(self), len(self) % training_length, -training_length)[:batch_size]:\n                            tmp_list += [np.array(self[end - training_length:end])]\n                        if (leftover != 0) and (len(tmp_list) < batch_size):\n                            tmp_list += [np.array([padding] * (training_length - leftover) + self[:leftover])]\n                        tmp_list.reverse()\n                        return np.array(tmp_list)\n\n            def reset_field(self):\n                """"""\n                Resets the AgentBufferField\n                """"""\n                self[:] = []\n\n        def __init__(self):\n            self.last_brain_info = None\n            self.last_take_action_outputs = None\n            super(Buffer.AgentBuffer, self).__init__()\n\n        def __str__(self):\n            return "", "".join([""\'{0}\' : {1}"".format(k, str(self[k])) for k in self.keys()])\n\n        def reset_agent(self):\n            """"""\n            Resets the AgentBuffer\n            """"""\n            for k in self.keys():\n                self[k].reset_field()\n            self.last_brain_info = None\n            self.last_take_action_outputs = None\n\n        def __getitem__(self, key):\n            if key not in self.keys():\n                self[key] = self.AgentBufferField()\n            return super(Buffer.AgentBuffer, self).__getitem__(key)\n\n        def check_length(self, key_list):\n            """"""\n            Some methods will require that some fields have the same length.\n            check_length will return true if the fields in key_list\n            have the same length.\n            :param key_list: The fields which length will be compared\n            """"""\n            if len(key_list) < 2:\n                return True\n            l = None\n            for key in key_list:\n                if key not in self.keys():\n                    return False\n                if (l is not None) and (l != len(self[key])):\n                    return False\n                l = len(self[key])\n            return True\n\n        def shuffle(self, key_list=None):\n            """"""\n            Shuffles the fields in key_list in a consistent way: The reordering will\n            be the same across fields.\n            :param key_list: The fields that must be shuffled.\n            """"""\n            if key_list is None:\n                key_list = list(self.keys())\n            if not self.check_length(key_list):\n                raise BufferException(""Unable to shuffle if the fields are not of same length"")\n            s = np.arange(len(self[key_list[0]]))\n            np.random.shuffle(s)\n            for key in key_list:\n                self[key][:] = [self[key][i] for i in s]\n\n    def __init__(self):\n        self.update_buffer = self.AgentBuffer()\n        super(Buffer, self).__init__()\n\n    def __str__(self):\n        return ""update buffer :\\n\\t{0}\\nlocal_buffers :\\n{1}"".format(str(self.update_buffer),\n                                                                     \'\\n\'.join(\n                                                                         [\'\\tagent {0} :{1}\'.format(k, str(self[k])) for\n                                                                          k in self.keys()]))\n\n    def __getitem__(self, key):\n        if key not in self.keys():\n            self[key] = self.AgentBuffer()\n        return super(Buffer, self).__getitem__(key)\n\n    def reset_update_buffer(self):\n        """"""\n        Resets the update buffer\n        """"""\n        self.update_buffer.reset_agent()\n\n    def reset_all(self):\n        """"""\n        Resets all the local local_buffers\n        """"""\n        agent_ids = list(self.keys())\n        for k in agent_ids:\n            self[k].reset_agent()\n\n    def append_update_buffer(self, agent_id, key_list=None, batch_size=None, training_length=None):\n        """"""\n        Appends the buffer of an agent to the update buffer.\n        :param agent_id: The id of the agent which data will be appended\n        :param key_list: The fields that must be added. If None: all fields will be appended.\n        :param batch_size: The number of elements that must be appended. If None: All of them will be.\n        :param training_length: The length of the samples that must be appended. If None: only takes one element.\n        """"""\n        if key_list is None:\n            key_list = self[agent_id].keys()\n        if not self[agent_id].check_length(key_list):\n            raise BufferException(""The length of the fields {0} for agent {1} where not of same length""\n                                  .format(key_list, agent_id))\n        for field_key in key_list:\n            self.update_buffer[field_key].extend(\n                self[agent_id][field_key].get_batch(batch_size=batch_size, training_length=training_length)\n            )\n\n    def append_all_agent_batch_to_update_buffer(self, key_list=None, batch_size=None, training_length=None):\n        """"""\n        Appends the buffer of all agents to the update buffer.\n        :param key_list: The fields that must be added. If None: all fields will be appended.\n        :param batch_size: The number of elements that must be appended. If None: All of them will be.\n        :param training_length: The length of the samples that must be appended. If None: only takes one element.\n        """"""\n        for agent_id in self.keys():\n            self.append_update_buffer(agent_id, key_list, batch_size, training_length)\n'"
python/unitytrainers/models.py,0,"b'import logging\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.layers as c_layers\n\nlogger = logging.getLogger(""unityagents"")\n\n\nclass LearningModel(object):\n    def __init__(self, m_size, normalize, use_recurrent, brain):\n        self.brain = brain\n        self.vector_in = None\n        self.normalize = False\n        self.use_recurrent = False\n        self.global_step, self.increment_step = self.create_global_steps()\n        self.visual_in = []\n        self.batch_size = tf.placeholder(shape=None, dtype=tf.int32, name=\'batch_size\')\n        self.sequence_length = tf.placeholder(shape=None, dtype=tf.int32, name=\'sequence_length\')\n        self.mask_input = tf.placeholder(shape=[None], dtype=tf.float32, name=\'masks\')\n        self.mask = tf.cast(self.mask_input, tf.int32)\n        self.m_size = m_size\n        self.normalize = normalize\n        self.use_recurrent = use_recurrent\n        self.a_size = brain.vector_action_space_size\n        self.o_size = brain.vector_observation_space_size * brain.num_stacked_vector_observations\n        self.v_size = brain.number_visual_observations\n\n    @staticmethod\n    def create_global_steps():\n        """"""Creates TF ops to track and increment global training step.""""""\n        global_step = tf.Variable(0, name=""global_step"", trainable=False, dtype=tf.int32)\n        increment_step = tf.assign(global_step, tf.add(global_step, 1))\n        return global_step, increment_step\n\n    @staticmethod\n    def swish(input_activation):\n        """"""Swish activation function. For more info: https://arxiv.org/abs/1710.05941""""""\n        return tf.multiply(input_activation, tf.nn.sigmoid(input_activation))\n\n    @staticmethod\n    def create_visual_input(camera_parameters, name):\n        """"""\n        Creates image input op.\n        :param camera_parameters: Parameters for visual observation from BrainInfo.\n        :param name: Desired name of input op.\n        :return: input op.\n        """"""\n        o_size_h = camera_parameters[\'height\']\n        o_size_w = camera_parameters[\'width\']\n        bw = camera_parameters[\'blackAndWhite\']\n\n        if bw:\n            c_channels = 1\n        else:\n            c_channels = 3\n\n        visual_in = tf.placeholder(shape=[None, o_size_h, o_size_w, c_channels], dtype=tf.float32, name=name)\n        return visual_in\n\n    def create_vector_input(self, name=\'vector_observation\'):\n        """"""\n        Creates ops for vector observation input.\n        :param name: Name of the placeholder op.\n        :param o_size: Size of stacked vector observation.\n        :return:\n        """"""\n        if self.brain.vector_observation_space_type == ""continuous"":\n            self.vector_in = tf.placeholder(shape=[None, self.o_size], dtype=tf.float32, name=name)\n            if self.normalize:\n                self.running_mean = tf.get_variable(""running_mean"", [self.o_size], trainable=False, dtype=tf.float32,\n                                                    initializer=tf.zeros_initializer())\n                self.running_variance = tf.get_variable(""running_variance"", [self.o_size], trainable=False,\n                                                        dtype=tf.float32, initializer=tf.ones_initializer())\n                self.update_mean, self.update_variance = self.create_normalizer_update(self.vector_in)\n\n                self.normalized_state = tf.clip_by_value((self.vector_in - self.running_mean) / tf.sqrt(\n                    self.running_variance / (tf.cast(self.global_step, tf.float32) + 1)), -5, 5,\n                                                         name=""normalized_state"")\n                return self.normalized_state\n            else:\n                return self.vector_in\n        else:\n            self.vector_in = tf.placeholder(shape=[None, 1], dtype=tf.int32, name=\'vector_observation\')\n            return self.vector_in\n\n    def create_normalizer_update(self, vector_input):\n        mean_current_observation = tf.reduce_mean(vector_input, axis=0)\n        new_mean = self.running_mean + (mean_current_observation - self.running_mean) / \\\n                   tf.cast(self.global_step + 1, tf.float32)\n        new_variance = self.running_variance + (mean_current_observation - new_mean) * \\\n                       (mean_current_observation - self.running_mean)\n        update_mean = tf.assign(self.running_mean, new_mean)\n        update_variance = tf.assign(self.running_variance, new_variance)\n        return update_mean, update_variance\n\n    @staticmethod\n    def create_continuous_observation_encoder(observation_input, h_size, activation, num_layers, scope, reuse):\n        """"""\n        Builds a set of hidden state encoders.\n        :param reuse: Whether to re-use the weights within the same scope.\n        :param scope: Graph scope for the encoder ops.\n        :param observation_input: Input vector.\n        :param h_size: Hidden layer size.\n        :param activation: What type of activation function to use for layers.\n        :param num_layers: number of hidden layers to create.\n        :return: List of hidden layer tensors.\n        """"""\n        with tf.variable_scope(scope):\n            hidden = observation_input\n            for i in range(num_layers):\n                hidden = tf.layers.dense(hidden, h_size, activation=activation, reuse=reuse, name=""hidden_{}"".format(i),\n                                         kernel_initializer=c_layers.variance_scaling_initializer(1.0))\n        return hidden\n\n    def create_visual_observation_encoder(self, image_input, h_size, activation, num_layers, scope, reuse):\n        """"""\n        Builds a set of visual (CNN) encoders.\n        :param reuse: Whether to re-use the weights within the same scope.\n        :param scope: The scope of the graph within which to create the ops.\n        :param image_input: The placeholder for the image input to use.\n        :param h_size: Hidden layer size.\n        :param activation: What type of activation function to use for layers.\n        :param num_layers: number of hidden layers to create.\n        :return: List of hidden layer tensors.\n        """"""\n        with tf.variable_scope(scope):\n            conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], strides=[4, 4],\n                                     activation=tf.nn.elu, reuse=reuse, name=""conv_1"")\n            conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], strides=[2, 2],\n                                     activation=tf.nn.elu, reuse=reuse, name=""conv_2"")\n            hidden = c_layers.flatten(conv2)\n\n        with tf.variable_scope(scope+\'/\'+\'flat_encoding\'):\n            hidden_flat = self.create_continuous_observation_encoder(hidden, h_size, activation,\n                                                                     num_layers, scope, reuse)\n        return hidden_flat\n\n    @staticmethod\n    def create_discrete_observation_encoder(observation_input, s_size, h_size, activation,\n                                            num_layers, scope, reuse):\n        """"""\n        Builds a set of hidden state encoders from discrete state input.\n        :param reuse: Whether to re-use the weights within the same scope.\n        :param scope: The scope of the graph within which to create the ops.\n        :param observation_input: Discrete observation.\n        :param s_size: state input size (discrete).\n        :param h_size: Hidden layer size.\n        :param activation: What type of activation function to use for layers.\n        :param num_layers: number of hidden layers to create.\n        :return: List of hidden layer tensors.\n        """"""\n        with tf.variable_scope(scope):\n            vector_in = tf.reshape(observation_input, [-1])\n            state_onehot = tf.one_hot(vector_in, s_size)\n            hidden = state_onehot\n            for i in range(num_layers):\n                hidden = tf.layers.dense(hidden, h_size, use_bias=False, activation=activation,\n                                         reuse=reuse, name=""hidden_{}"".format(i))\n        return hidden\n\n    def create_observation_streams(self, num_streams, h_size, num_layers):\n        """"""\n        Creates encoding stream for observations.\n        :param num_streams: Number of streams to create.\n        :param h_size: Size of hidden linear layers in stream.\n        :param num_layers: Number of hidden linear layers in stream.\n        :return: List of encoded streams.\n        """"""\n        brain = self.brain\n        activation_fn = self.swish\n\n        self.visual_in = []\n        for i in range(brain.number_visual_observations):\n            visual_input = self.create_visual_input(brain.camera_resolutions[i], name=""visual_observation_"" + str(i))\n            self.visual_in.append(visual_input)\n        vector_observation_input = self.create_vector_input()\n\n        final_hiddens = []\n        for i in range(num_streams):\n            visual_encoders = []\n            hidden_state, hidden_visual = None, None\n            if self.v_size > 0:\n                for j in range(brain.number_visual_observations):\n                    encoded_visual = self.create_visual_observation_encoder(self.visual_in[j], h_size,\n                                                                            activation_fn, num_layers,\n                                                                            ""main_graph_{}_encoder{}""\n                                                                            .format(i, j), False)\n                    visual_encoders.append(encoded_visual)\n                hidden_visual = tf.concat(visual_encoders, axis=1)\n            if brain.vector_observation_space_size > 0:\n                if brain.vector_observation_space_type == ""continuous"":\n                    hidden_state = self.create_continuous_observation_encoder(vector_observation_input,\n                                                                              h_size, activation_fn, num_layers,\n                                                                              ""main_graph_{}"".format(i), False)\n                else:\n                    hidden_state = self.create_discrete_observation_encoder(vector_observation_input, self.o_size,\n                                                                            h_size, activation_fn, num_layers,\n                                                                            ""main_graph_{}"".format(i), False)\n            if hidden_state is not None and hidden_visual is not None:\n                final_hidden = tf.concat([hidden_visual, hidden_state], axis=1)\n            elif hidden_state is None and hidden_visual is not None:\n                final_hidden = hidden_visual\n            elif hidden_state is not None and hidden_visual is None:\n                final_hidden = hidden_state\n            else:\n                raise Exception(""No valid network configuration possible. ""\n                                ""There are no states or observations in this brain"")\n            final_hiddens.append(final_hidden)\n        return final_hiddens\n\n    @staticmethod\n    def create_recurrent_encoder(input_state, memory_in, sequence_length, name=\'lstm\'):\n        """"""\n        Builds a recurrent encoder for either state or observations (LSTM).\n        :param sequence_length: Length of sequence to unroll.\n        :param input_state: The input tensor to the LSTM cell.\n        :param memory_in: The input memory to the LSTM cell.\n        :param name: The scope of the LSTM cell.\n        """"""\n        s_size = input_state.get_shape().as_list()[1]\n        m_size = memory_in.get_shape().as_list()[1]\n        lstm_input_state = tf.reshape(input_state, shape=[-1, sequence_length, s_size])\n        memory_in = tf.reshape(memory_in[:, :], [-1, m_size])\n        _half_point = int(m_size / 2)\n        with tf.variable_scope(name):\n            rnn_cell = tf.contrib.rnn.BasicLSTMCell(_half_point)\n            lstm_vector_in = tf.contrib.rnn.LSTMStateTuple(memory_in[:, :_half_point], memory_in[:, _half_point:])\n            recurrent_output, lstm_state_out = tf.nn.dynamic_rnn(rnn_cell, lstm_input_state,\n                                                                 initial_state=lstm_vector_in)\n\n        recurrent_output = tf.reshape(recurrent_output, shape=[-1, _half_point])\n        return recurrent_output, tf.concat([lstm_state_out.c, lstm_state_out.h], axis=1)\n\n    def create_dc_actor_critic(self, h_size, num_layers):\n        """"""\n        Creates Discrete control actor-critic model.\n        :param h_size: Size of hidden linear layers.\n        :param num_layers: Number of hidden linear layers.\n        """"""\n        hidden_streams = self.create_observation_streams(1, h_size, num_layers)\n        hidden = hidden_streams[0]\n\n        if self.use_recurrent:\n            tf.Variable(self.m_size, name=""memory_size"", trainable=False, dtype=tf.int32)\n            self.prev_action = tf.placeholder(shape=[None], dtype=tf.int32, name=\'prev_action\')\n            prev_action_oh = tf.one_hot(self.prev_action, self.a_size)\n            hidden = tf.concat([hidden, prev_action_oh], axis=1)\n\n            self.memory_in = tf.placeholder(shape=[None, self.m_size], dtype=tf.float32, name=\'recurrent_in\')\n            hidden, memory_out = self.create_recurrent_encoder(hidden, self.memory_in, self.sequence_length)\n            self.memory_out = tf.identity(memory_out, name=\'recurrent_out\')\n\n        self.policy = tf.layers.dense(hidden, self.a_size, activation=None, use_bias=False,\n                                      kernel_initializer=c_layers.variance_scaling_initializer(factor=0.01))\n\n        self.all_probs = tf.nn.softmax(self.policy, name=""action_probs"")\n        output = tf.multinomial(self.policy, 1)\n        self.output = tf.identity(output, name=""action"")\n\n        value = tf.layers.dense(hidden, 1, activation=None)\n        self.value = tf.identity(value, name=""value_estimate"")\n        self.entropy = -tf.reduce_sum(self.all_probs * tf.log(self.all_probs + 1e-10), axis=1)\n        self.action_holder = tf.placeholder(shape=[None], dtype=tf.int32)\n        self.selected_actions = tf.one_hot(self.action_holder, self.a_size)\n\n        self.all_old_probs = tf.placeholder(shape=[None, self.a_size], dtype=tf.float32, name=\'old_probabilities\')\n\n        # We reshape these tensors to [batch x 1] in order to be of the same rank as continuous control probabilities.\n        self.probs = tf.expand_dims(tf.reduce_sum(self.all_probs * self.selected_actions, axis=1), 1)\n        self.old_probs = tf.expand_dims(tf.reduce_sum(self.all_old_probs * self.selected_actions, axis=1), 1)\n\n    def create_cc_actor_critic(self, h_size, num_layers):\n        """"""\n        Creates Continuous control actor-critic model.\n        :param h_size: Size of hidden linear layers.\n        :param num_layers: Number of hidden linear layers.\n        """"""\n        hidden_streams = self.create_observation_streams(2, h_size, num_layers)\n\n        if self.use_recurrent:\n            tf.Variable(self.m_size, name=""memory_size"", trainable=False, dtype=tf.int32)\n            self.memory_in = tf.placeholder(shape=[None, self.m_size], dtype=tf.float32, name=\'recurrent_in\')\n            _half_point = int(self.m_size / 2)\n            hidden_policy, memory_policy_out = self.create_recurrent_encoder(\n                hidden_streams[0], self.memory_in[:, :_half_point], self.sequence_length, name=\'lstm_policy\')\n\n            hidden_value, memory_value_out = self.create_recurrent_encoder(\n                hidden_streams[1], self.memory_in[:, _half_point:], self.sequence_length, name=\'lstm_value\')\n            self.memory_out = tf.concat([memory_policy_out, memory_value_out], axis=1, name=\'recurrent_out\')\n        else:\n            hidden_policy = hidden_streams[0]\n            hidden_value = hidden_streams[1]\n\n        mu = tf.layers.dense(hidden_policy, self.a_size, activation=None,\n                             kernel_initializer=c_layers.variance_scaling_initializer(factor=0.01))\n\n        log_sigma_sq = tf.get_variable(""log_sigma_squared"", [self.a_size], dtype=tf.float32,\n                                       initializer=tf.zeros_initializer())\n\n        sigma_sq = tf.exp(log_sigma_sq)\n\n        epsilon = tf.random_normal(tf.shape(mu), dtype=tf.float32)\n\n        # Clip and scale output to ensure actions are always within [-1, 1] range.\n        self.output_pre = mu + tf.sqrt(sigma_sq) * epsilon\n        output_post = tf.clip_by_value(self.output_pre, -3, 3) / 3\n        self.output = tf.identity(output_post, name=\'action\')\n        self.selected_actions = tf.stop_gradient(output_post)\n\n        # Compute probability of model output.\n        a = tf.exp(-1 * tf.pow(tf.stop_gradient(self.output_pre) - mu, 2) / (2 * sigma_sq))\n        b = 1 / tf.sqrt(2 * sigma_sq * np.pi)\n        all_probs = tf.multiply(a, b)\n        self.all_probs = tf.identity(all_probs, name=\'action_probs\')\n\n        self.entropy = tf.reduce_mean(0.5 * tf.log(2 * np.pi * np.e * sigma_sq))\n\n        value = tf.layers.dense(hidden_value, 1, activation=None)\n        self.value = tf.identity(value, name=""value_estimate"")\n\n        self.all_old_probs = tf.placeholder(shape=[None, self.a_size], dtype=tf.float32,\n                                            name=\'old_probabilities\')\n\n        # We keep these tensors the same name, but use new nodes to keep code parallelism with discrete control.\n        self.probs = tf.identity(self.all_probs)\n        self.old_probs = tf.identity(self.all_old_probs)\n'"
python/unitytrainers/trainer.py,0,"b'# # Unity ML-Agents Toolkit\nimport logging\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom unityagents import UnityException, AllBrainInfo\n\nlogger = logging.getLogger(""unityagents"")\n\n\nclass UnityTrainerException(UnityException):\n    """"""\n    Related to errors with the Trainer.\n    """"""\n    pass\n\n\nclass Trainer(object):\n    """"""This class is the abstract class for the unitytrainers""""""\n\n    def __init__(self, sess, env, brain_name, trainer_parameters, training):\n        """"""\n        Responsible for collecting experiences and training a neural network model.\n        :param sess: Tensorflow session.\n        :param env: The UnityEnvironment.\n        :param  trainer_parameters: The parameters for the trainer (dictionary).\n        :param training: Whether the trainer is set for training.\n        """"""\n        self.brain_name = brain_name\n        self.brain = env.brains[self.brain_name]\n        self.trainer_parameters = trainer_parameters\n        self.is_training = training\n        self.sess = sess\n        self.stats = {}\n        self.summary_writer = None\n\n    def __str__(self):\n        return \'\'\'Empty Trainer\'\'\'\n\n    @property\n    def parameters(self):\n        """"""\n        Returns the trainer parameters of the trainer.\n        """"""\n        raise UnityTrainerException(""The parameters property was not implemented."")\n\n    @property\n    def graph_scope(self):\n        """"""\n        Returns the graph scope of the trainer.\n        """"""\n        raise UnityTrainerException(""The graph_scope property was not implemented."")\n\n    @property\n    def get_max_steps(self):\n        """"""\n        Returns the maximum number of steps. Is used to know when the trainer should be stopped.\n        :return: The maximum number of steps of the trainer\n        """"""\n        raise UnityTrainerException(""The get_max_steps property was not implemented."")\n\n    @property\n    def get_step(self):\n        """"""\n        Returns the number of steps the trainer has performed\n        :return: the step count of the trainer\n        """"""\n        raise UnityTrainerException(""The get_step property was not implemented."")\n\n    @property\n    def get_last_reward(self):\n        """"""\n        Returns the last reward the trainer has had\n        :return: the new last reward\n        """"""\n        raise UnityTrainerException(""The get_last_reward property was not implemented."")\n\n    def increment_step_and_update_last_reward(self):\n        """"""\n        Increment the step count of the trainer and updates the last reward\n        """"""\n        raise UnityTrainerException(""The increment_step_and_update_last_reward method was not implemented."")\n\n    def take_action(self, all_brain_info: AllBrainInfo):\n        """"""\n        Decides actions given state/observation information, and takes them in environment.\n        :param all_brain_info: A dictionary of brain names and BrainInfo from environment.\n        :return: a tuple containing action, memories, values and an object\n        to be passed to add experiences\n        """"""\n        raise UnityTrainerException(""The take_action method was not implemented."")\n\n    def add_experiences(self, curr_info: AllBrainInfo, next_info: AllBrainInfo, take_action_outputs):\n        """"""\n        Adds experiences to each agent\'s experience history.\n        :param curr_info: Current AllBrainInfo.\n        :param next_info: Next AllBrainInfo.\n        :param take_action_outputs: The outputs of the take action method.\n        """"""\n        raise UnityTrainerException(""The add_experiences method was not implemented."")\n\n    def process_experiences(self, current_info: AllBrainInfo, next_info: AllBrainInfo):\n        """"""\n        Checks agent histories for processing condition, and processes them as necessary.\n        Processing involves calculating value and advantage targets for model updating step.\n        :param current_info: Dictionary of all current-step brains and corresponding BrainInfo.\n        :param next_info: Dictionary of all next-step brains and corresponding BrainInfo.\n        """"""\n        raise UnityTrainerException(""The process_experiences method was not implemented."")\n\n    def end_episode(self):\n        """"""\n        A signal that the Episode has ended. The buffer must be reset. \n        Get only called when the academy resets.\n        """"""\n        raise UnityTrainerException(""The end_episode method was not implemented."")\n\n    def is_ready_update(self):\n        """"""\n        Returns whether or not the trainer has enough elements to run update model\n        :return: A boolean corresponding to wether or not update_model() can be run\n        """"""\n        raise UnityTrainerException(""The is_ready_update method was not implemented."")\n\n    def update_model(self):\n        """"""\n        Uses training_buffer to update model.\n        """"""\n        raise UnityTrainerException(""The update_model method was not implemented."")\n\n    def write_summary(self, lesson_number):\n        """"""\n        Saves training statistics to Tensorboard.\n        :param lesson_number: The lesson the trainer is at.\n        """"""\n        if (self.get_step % self.trainer_parameters[\'summary_freq\'] == 0 and self.get_step != 0 and\n                self.is_training and self.get_step <= self.get_max_steps):\n            if len(self.stats[\'cumulative_reward\']) > 0:\n                mean_reward = np.mean(self.stats[\'cumulative_reward\'])\n                logger.info("" {}: Step: {}. Mean Reward: {:0.3f}. Std of Reward: {:0.3f}.""\n                            .format(self.brain_name, self.get_step,\n                                    mean_reward, np.std(self.stats[\'cumulative_reward\'])))\n            else:\n                logger.info("" {}: Step: {}. No episode was completed since last summary.""\n                            .format(self.brain_name, self.get_step))\n            summary = tf.Summary()\n            for key in self.stats:\n                if len(self.stats[key]) > 0:\n                    stat_mean = float(np.mean(self.stats[key]))\n                    summary.value.add(tag=\'Info/{}\'.format(key), simple_value=stat_mean)\n                    self.stats[key] = []\n            summary.value.add(tag=\'Info/Lesson\', simple_value=lesson_number)\n            self.summary_writer.add_summary(summary, self.get_step)\n            self.summary_writer.flush()\n\n    def write_tensorboard_text(self, key, input_dict):\n        """"""\n        Saves text to Tensorboard.\n        Note: Only works on tensorflow r1.2 or above.\n        :param key: The name of the text.\n        :param input_dict: A dictionary that will be displayed in a table on Tensorboard.\n        """"""\n        try:\n            s_op = tf.summary.text(key, tf.convert_to_tensor(([[str(x), str(input_dict[x])] for x in input_dict])))\n            s = self.sess.run(s_op)\n            self.summary_writer.add_summary(s, self.get_step)\n        except:\n            logger.info(""Cannot write text summary for Tensorboard. Tensorflow version must be r1.2 or above."")\n            pass\n'"
python/unitytrainers/trainer_controller.py,0,"b'# # Unity ML-Agents Toolkit\n# ## ML-Agent Learning\n# Launches unitytrainers for each External Brains in a Unity Environment\n\nimport logging\nimport numpy as np\nimport os\nimport re\nimport tensorflow as tf\nimport yaml\n\nfrom tensorflow.python.tools import freeze_graph\nfrom unitytrainers.ppo.trainer import PPOTrainer\nfrom unitytrainers.bc.trainer import BehavioralCloningTrainer\nfrom unityagents import UnityEnvironment, UnityEnvironmentException\n\n\nclass TrainerController(object):\n    def __init__(self, env_path, run_id, save_freq, curriculum_file, fast_simulation, load, train,\n                 worker_id, keep_checkpoints, lesson, seed, docker_target_name, trainer_config_path,\n                 no_graphics):\n        """"""\n        :param env_path: Location to the environment executable to be loaded.\n        :param run_id: The sub-directory name for model and summary statistics\n        :param save_freq: Frequency at which to save model\n        :param curriculum_file: Curriculum json file for environment\n        :param fast_simulation: Whether to run the game at training speed\n        :param load: Whether to load the model or randomly initialize\n        :param train: Whether to train model, or only run inference\n        :param worker_id: Number to add to communication port (5005). Used for multi-environment\n        :param keep_checkpoints: How many model checkpoints to keep\n        :param lesson: Start learning from this lesson\n        :param seed: Random seed used for training.\n        :param docker_target_name: Name of docker volume that will contain all data.\n        :param trainer_config_path: Fully qualified path to location of trainer configuration file\n        :param no_graphics: Whether to run the Unity simulator in no-graphics mode\n        """"""\n        self.trainer_config_path = trainer_config_path\n        if env_path is not None:\n            env_path = (env_path.strip()\n                        .replace(\'.app\', \'\')\n                        .replace(\'.exe\', \'\')\n                        .replace(\'.x86_64\', \'\')\n                        .replace(\'.x86\', \'\'))  # Strip out executable extensions if passed\n        # Recognize and use docker volume if one is passed as an argument\n        if docker_target_name == \'\':\n            self.docker_training = False\n            self.model_path = \'./models/{run_id}\'.format(run_id=run_id)\n            self.curriculum_file = curriculum_file\n            self.summaries_dir = \'./summaries\'\n        else:\n            self.docker_training = True\n            self.model_path = \'/{docker_target_name}/models/{run_id}\'.format(\n                docker_target_name=docker_target_name,\n                run_id=run_id)\n            if env_path is not None:\n                env_path = \'/{docker_target_name}/{env_name}\'.format(docker_target_name=docker_target_name,\n                                                                     env_name=env_path)\n            if curriculum_file is None:\n                self.curriculum_file = None\n            else:\n                self.curriculum_file = \'/{docker_target_name}/{curriculum_file}\'.format(\n                    docker_target_name=docker_target_name,\n                    curriculum_file=curriculum_file)\n            self.summaries_dir = \'/{docker_target_name}/summaries\'.format(docker_target_name=docker_target_name)\n        self.logger = logging.getLogger(""unityagents"")\n        self.run_id = run_id\n        self.save_freq = save_freq\n        self.lesson = lesson\n        self.fast_simulation = fast_simulation\n        self.load_model = load\n        self.train_model = train\n        self.worker_id = worker_id\n        self.keep_checkpoints = keep_checkpoints\n        self.trainers = {}\n        if seed == -1:\n            seed = np.random.randint(0, 999999)\n        self.seed = seed\n        np.random.seed(self.seed)\n        tf.set_random_seed(self.seed)\n        self.env = UnityEnvironment(file_name=env_path, worker_id=self.worker_id,\n                                    curriculum=self.curriculum_file, seed=self.seed,\n                                    docker_training=self.docker_training,\n                                    no_graphics=no_graphics)\n        if env_path is None:\n            self.env_name = \'editor_\'+self.env.academy_name\n        else:\n            self.env_name = os.path.basename(os.path.normpath(env_path))  # Extract out name of environment\n\n    def _get_progress(self):\n        if self.curriculum_file is not None:\n            progress = 0\n            if self.env.curriculum.measure_type == ""progress"":\n                for brain_name in self.env.external_brain_names:\n                    progress += self.trainers[brain_name].get_step / self.trainers[brain_name].get_max_steps\n                return progress / len(self.env.external_brain_names)\n            elif self.env.curriculum.measure_type == ""reward"":\n                for brain_name in self.env.external_brain_names:\n                    progress += self.trainers[brain_name].get_last_reward\n                return progress\n            else:\n                return None\n        else:\n            return None\n\n    def _process_graph(self):\n        nodes = []\n        scopes = []\n        for brain_name in self.trainers.keys():\n            if self.trainers[brain_name].graph_scope is not None:\n                scope = self.trainers[brain_name].graph_scope + \'/\'\n                if scope == \'/\':\n                    scope = \'\'\n                scopes += [scope]\n                if self.trainers[brain_name].parameters[""trainer""] == ""imitation"":\n                    nodes += [scope + x for x in [""action""]]\n                else:\n                    nodes += [scope + x for x in [""action"", ""value_estimate"", ""action_probs""]]\n                if self.trainers[brain_name].parameters[""use_recurrent""]:\n                    nodes += [scope + x for x in [""recurrent_out"", ""memory_size""]]\n        if len(scopes) > 1:\n            self.logger.info(""List of available scopes :"")\n            for scope in scopes:\n                self.logger.info(""\\t"" + scope)\n        self.logger.info(""List of nodes to export :"")\n        for n in nodes:\n            self.logger.info(""\\t"" + n)\n        return nodes\n\n    def _save_model(self, sess, saver, steps=0):\n        """"""\n        Saves current model to checkpoint folder.\n        :param sess: Current Tensorflow session.\n        :param steps: Current number of steps in training process.\n        :param saver: Tensorflow saver for session.\n        """"""\n        last_checkpoint = self.model_path + \'/model-\' + str(steps) + \'.cptk\'\n        saver.save(sess, last_checkpoint)\n        tf.train.write_graph(sess.graph_def, self.model_path, \'raw_graph_def.pb\', as_text=False)\n        self.logger.info(""Saved Model"")\n\n    def _export_graph(self):\n        """"""\n        Exports latest saved model to .bytes format for Unity embedding.\n        """"""\n        target_nodes = \',\'.join(self._process_graph())\n        ckpt = tf.train.get_checkpoint_state(self.model_path)\n        freeze_graph.freeze_graph(input_graph=self.model_path + \'/raw_graph_def.pb\',\n                                  input_binary=True,\n                                  input_checkpoint=ckpt.model_checkpoint_path,\n                                  output_node_names=target_nodes,\n                                  output_graph=self.model_path + \'/\' + self.env_name + ""_"" + self.run_id + \'.bytes\',\n                                  clear_devices=True, initializer_nodes="""", input_saver="""",\n                                  restore_op_name=""save/restore_all"", filename_tensor_name=""save/Const:0"")\n\n    def _initialize_trainers(self, trainer_config, sess):\n        trainer_parameters_dict = {}\n        self.trainers = {}\n        for brain_name in self.env.external_brain_names:\n            trainer_parameters = trainer_config[\'default\'].copy()\n            if len(self.env.external_brain_names) > 1:\n                graph_scope = re.sub(\'[^0-9a-zA-Z]+\', \'-\', brain_name)\n                trainer_parameters[\'graph_scope\'] = graph_scope\n                trainer_parameters[\'summary_path\'] = \'{basedir}/{name}\'.format(\n                    basedir=self.summaries_dir,\n                    name=str(self.run_id) + \'_\' + graph_scope)\n            else:\n                trainer_parameters[\'graph_scope\'] = \'\'\n                trainer_parameters[\'summary_path\'] = \'{basedir}/{name}\'.format(\n                    basedir=self.summaries_dir,\n                    name=str(self.run_id))\n            if brain_name in trainer_config:\n                _brain_key = brain_name\n                while not isinstance(trainer_config[_brain_key], dict):\n                    _brain_key = trainer_config[_brain_key]\n                for k in trainer_config[_brain_key]:\n                    trainer_parameters[k] = trainer_config[_brain_key][k]\n            trainer_parameters_dict[brain_name] = trainer_parameters.copy()\n        for brain_name in self.env.external_brain_names:\n            if trainer_parameters_dict[brain_name][\'trainer\'] == ""imitation"":\n                self.trainers[brain_name] = BehavioralCloningTrainer(sess, self.env, brain_name,\n                                                                     trainer_parameters_dict[brain_name],\n                                                                     self.train_model, self.seed)\n            elif trainer_parameters_dict[brain_name][\'trainer\'] == ""ppo"":\n                self.trainers[brain_name] = PPOTrainer(sess, self.env, brain_name, trainer_parameters_dict[brain_name],\n                                                       self.train_model, self.seed)\n            else:\n                raise UnityEnvironmentException(""The trainer config contains an unknown trainer type for brain {}""\n                                                .format(brain_name))\n\n    def _load_config(self):\n        try:\n            with open(self.trainer_config_path) as data_file:\n                trainer_config = yaml.load(data_file)\n                return trainer_config\n        except IOError:\n            raise UnityEnvironmentException(""""""Parameter file could not be found here {}.\n                                            Will use default Hyper parameters""""""\n                                            .format(self.trainer_config_path))\n        except UnicodeDecodeError:\n            raise UnityEnvironmentException(""There was an error decoding Trainer Config from this path : {}""\n                                            .format(self.trainer_config_path))\n\n    @staticmethod\n    def _create_model_path(model_path):\n        try:\n            if not os.path.exists(model_path):\n                os.makedirs(model_path)\n        except Exception:\n            raise UnityEnvironmentException(""The folder {} containing the generated model could not be accessed.""\n                                            "" Please make sure the permissions are set correctly.""\n                                            .format(model_path))\n\n    def start_learning(self):\n        self.env.curriculum.set_lesson_number(self.lesson)\n        trainer_config = self._load_config()\n        self._create_model_path(self.model_path)\n\n        tf.reset_default_graph()\n\n        with tf.Session() as sess:\n            self._initialize_trainers(trainer_config, sess)\n            for k, t in self.trainers.items():\n                self.logger.info(t)\n            init = tf.global_variables_initializer()\n            saver = tf.train.Saver(max_to_keep=self.keep_checkpoints)\n            # Instantiate model parameters\n            if self.load_model:\n                self.logger.info(\'Loading Model...\')\n                ckpt = tf.train.get_checkpoint_state(self.model_path)\n                if ckpt is None:\n                    self.logger.info(\'The model {0} could not be found. Make sure you specified the right \'\n                                     \'--run-id\'.format(self.model_path))\n                saver.restore(sess, ckpt.model_checkpoint_path)\n            else:\n                sess.run(init)\n            global_step = 0  # This is only for saving the model\n            self.env.curriculum.increment_lesson(self._get_progress())\n            curr_info = self.env.reset(train_mode=self.fast_simulation)\n            if self.train_model:\n                for brain_name, trainer in self.trainers.items():\n                    trainer.write_tensorboard_text(\'Hyperparameters\', trainer.parameters)\n            try:\n                while any([t.get_step <= t.get_max_steps for k, t in self.trainers.items()]) or not self.train_model:\n                    if self.env.global_done:\n                        self.env.curriculum.increment_lesson(self._get_progress())\n                        curr_info = self.env.reset(train_mode=self.fast_simulation)\n                        for brain_name, trainer in self.trainers.items():\n                            trainer.end_episode()\n                    # Decide and take an action\n                    take_action_vector, take_action_memories, take_action_text, take_action_outputs = {}, {}, {}, {}\n                    for brain_name, trainer in self.trainers.items():\n                        (take_action_vector[brain_name],\n                         take_action_memories[brain_name],\n                         take_action_text[brain_name],\n                         take_action_outputs[brain_name]) = trainer.take_action(curr_info)\n                    new_info = self.env.step(vector_action=take_action_vector, memory=take_action_memories,\n                                             text_action=take_action_text)\n                    for brain_name, trainer in self.trainers.items():\n                        trainer.add_experiences(curr_info, new_info, take_action_outputs[brain_name])\n                        trainer.process_experiences(curr_info, new_info)\n                        if trainer.is_ready_update() and self.train_model and trainer.get_step <= trainer.get_max_steps:\n                            # Perform gradient descent with experience buffer\n                            trainer.update_model()\n                        # Write training statistics to Tensorboard.\n                        trainer.write_summary(self.env.curriculum.lesson_number)\n                        if self.train_model and trainer.get_step <= trainer.get_max_steps:\n                            trainer.increment_step_and_update_last_reward()\n                    if self.train_model:\n                        global_step += 1\n                    if global_step % self.save_freq == 0 and global_step != 0 and self.train_model:\n                        # Save Tensorflow model\n                        self._save_model(sess, steps=global_step, saver=saver)\n                    curr_info = new_info\n                # Final save Tensorflow model\n                if global_step != 0 and self.train_model:\n                    self._save_model(sess, steps=global_step, saver=saver)\n            except KeyboardInterrupt:\n                print(\'--------------------------Now saving model-------------------------\')\n                if self.train_model:\n                    self.logger.info(""Learning was interrupted. Please wait while the graph is generated."")\n                    self._save_model(sess, steps=global_step, saver=saver)\n                pass\n        self.env.close()\n        if self.train_model:\n            self._export_graph()\n'"
python/unitytrainers/bc/__init__.py,0,b'from .models import *\nfrom .trainer import *\n'
python/unitytrainers/bc/models.py,0,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as c_layers\nfrom unitytrainers.models import LearningModel\n\n\nclass BehavioralCloningModel(LearningModel):\n    def __init__(self, brain, h_size=128, lr=1e-4, n_layers=2, m_size=128,\n                 normalize=False, use_recurrent=False):\n        LearningModel.__init__(self, m_size, normalize, use_recurrent, brain)\n\n        num_streams = 1\n        hidden_streams = self.create_observation_streams(num_streams, h_size, n_layers)\n        hidden = hidden_streams[0]\n        self.dropout_rate = tf.placeholder(dtype=tf.float32, shape=[], name=""dropout_rate"")\n        hidden_reg = tf.layers.dropout(hidden, self.dropout_rate)\n        if self.use_recurrent:\n            tf.Variable(self.m_size, name=""memory_size"", trainable=False, dtype=tf.int32)\n            self.memory_in = tf.placeholder(shape=[None, self.m_size], dtype=tf.float32, name=\'recurrent_in\')\n            hidden_reg, self.memory_out = self.create_recurrent_encoder(hidden_reg, self.memory_in,\n                                                                        self.sequence_length)\n            self.memory_out = tf.identity(self.memory_out, name=\'recurrent_out\')\n        self.policy = tf.layers.dense(hidden_reg, self.a_size, activation=None, use_bias=False, name=\'pre_action\',\n                                      kernel_initializer=c_layers.variance_scaling_initializer(factor=0.01))\n\n        if brain.vector_action_space_type == ""discrete"":\n            self.action_probs = tf.nn.softmax(self.policy)\n            self.sample_action_float = tf.multinomial(self.policy, 1)\n            self.sample_action_float = tf.identity(self.sample_action_float, name=""action"")\n            self.sample_action = tf.cast(self.sample_action_float, tf.int32)\n            self.true_action = tf.placeholder(shape=[None], dtype=tf.int32, name=""teacher_action"")\n            self.action_oh = tf.one_hot(self.true_action, self.a_size)\n            self.loss = tf.reduce_sum(-tf.log(self.action_probs + 1e-10) * self.action_oh)\n            self.action_percent = tf.reduce_mean(tf.cast(\n                tf.equal(tf.cast(tf.argmax(self.action_probs, axis=1), tf.int32), self.sample_action), tf.float32))\n        else:\n            self.clipped_sample_action = tf.clip_by_value(self.policy, -1, 1)\n            self.sample_action = tf.identity(self.clipped_sample_action, name=""action"")\n            self.true_action = tf.placeholder(shape=[None, self.a_size], dtype=tf.float32, name=""teacher_action"")\n            self.clipped_true_action = tf.clip_by_value(self.true_action, -1, 1)\n            self.loss = tf.reduce_sum(tf.squared_difference(self.clipped_true_action, self.sample_action))\n\n        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n        self.update = optimizer.minimize(self.loss)\n'"
python/unitytrainers/bc/trainer.py,0,"b'# # Unity ML-Agents Toolkit\n# ## ML-Agent Learning (Imitation)\n# Contains an implementation of Behavioral Cloning Algorithm\n\nimport logging\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom unityagents import AllBrainInfo\nfrom unitytrainers.bc.models import BehavioralCloningModel\nfrom unitytrainers.buffer import Buffer\nfrom unitytrainers.trainer import UnityTrainerException, Trainer\n\nlogger = logging.getLogger(""unityagents"")\n\n\nclass BehavioralCloningTrainer(Trainer):\n    """"""The ImitationTrainer is an implementation of the imitation learning.""""""\n\n    def __init__(self, sess, env, brain_name, trainer_parameters, training, seed):\n        """"""\n        Responsible for collecting experiences and training PPO model.\n        :param sess: Tensorflow session.\n        :param env: The UnityEnvironment.\n        :param  trainer_parameters: The parameters for the trainer (dictionary).\n        :param training: Whether the trainer is set for training.\n        """"""\n        self.param_keys = [\'brain_to_imitate\', \'batch_size\', \'time_horizon\', \'graph_scope\',\n                           \'summary_freq\', \'max_steps\', \'batches_per_epoch\', \'use_recurrent\', \'hidden_units\',\n                           \'num_layers\', \'sequence_length\', \'memory_size\']\n\n        for k in self.param_keys:\n            if k not in trainer_parameters:\n                raise UnityTrainerException(""The hyperparameter {0} could not be found for the Imitation trainer of ""\n                                            ""brain {1}."".format(k, brain_name))\n\n        super(BehavioralCloningTrainer, self).__init__(sess, env, brain_name, trainer_parameters, training)\n\n        self.variable_scope = trainer_parameters[\'graph_scope\']\n        self.brain_to_imitate = trainer_parameters[\'brain_to_imitate\']\n        self.batches_per_epoch = trainer_parameters[\'batches_per_epoch\']\n        self.use_recurrent = trainer_parameters[\'use_recurrent\']\n        self.sequence_length = 1\n        self.m_size = None\n        if self.use_recurrent:\n            self.m_size = trainer_parameters[""memory_size""]\n            self.sequence_length = trainer_parameters[""sequence_length""]\n        self.n_sequences = max(int(trainer_parameters[\'batch_size\'] / self.sequence_length), 1)\n        self.cumulative_rewards = {}\n        self.episode_steps = {}\n        self.stats = {\'losses\': [], \'episode_length\': [], \'cumulative_reward\': []}\n\n        self.training_buffer = Buffer()\n        self.is_continuous_action = (env.brains[brain_name].vector_action_space_type == ""continuous"")\n        self.is_continuous_observation = (env.brains[brain_name].vector_observation_space_type == ""continuous"")\n        self.use_visual_observations = (env.brains[brain_name].number_visual_observations > 0)\n        if self.use_visual_observations:\n            logger.info(\'Cannot use observations with imitation learning\')\n        self.use_vector_observations = (env.brains[brain_name].vector_observation_space_size > 0)\n        self.summary_path = trainer_parameters[\'summary_path\']\n        if not os.path.exists(self.summary_path):\n            os.makedirs(self.summary_path)\n\n        self.summary_writer = tf.summary.FileWriter(self.summary_path)\n        with tf.variable_scope(self.variable_scope):\n            tf.set_random_seed(seed)\n            self.model = BehavioralCloningModel(\n                h_size=int(trainer_parameters[\'hidden_units\']),\n                lr=float(trainer_parameters[\'learning_rate\']),\n                n_layers=int(trainer_parameters[\'num_layers\']),\n                m_size=self.m_size,\n                normalize=False,\n                use_recurrent=trainer_parameters[\'use_recurrent\'],\n                brain=self.brain)\n        self.inference_run_list = [self.model.sample_action]\n        if self.use_recurrent:\n            self.inference_run_list += [self.model.memory_out]\n\n    def __str__(self):\n\n        return \'\'\'Hyperparameters for the Imitation Trainer of brain {0}: \\n{1}\'\'\'.format(\n            self.brain_name, \'\\n\'.join([\'\\t{0}:\\t{1}\'.format(x, self.trainer_parameters[x]) for x in self.param_keys]))\n\n    @property\n    def parameters(self):\n        """"""\n        Returns the trainer parameters of the trainer.\n        """"""\n        return self.trainer_parameters\n\n    @property\n    def graph_scope(self):\n        """"""\n        Returns the graph scope of the trainer.\n        """"""\n        return self.variable_scope\n\n    @property\n    def get_max_steps(self):\n        """"""\n        Returns the maximum number of steps. Is used to know when the trainer should be stopped.\n        :return: The maximum number of steps of the trainer\n        """"""\n        return float(self.trainer_parameters[\'max_steps\'])\n\n    @property\n    def get_step(self):\n        """"""\n        Returns the number of steps the trainer has performed\n        :return: the step count of the trainer\n        """"""\n        return self.sess.run(self.model.global_step)\n\n    @property\n    def get_last_reward(self):\n        """"""\n        Returns the last reward the trainer has had\n        :return: the new last reward\n        """"""\n        if len(self.stats[\'cumulative_reward\']) > 0:\n            return np.mean(self.stats[\'cumulative_reward\'])\n        else:\n            return 0\n\n    def increment_step_and_update_last_reward(self):\n        """"""\n        Increment the step count of the trainer and Updates the last reward\n        """"""\n        self.sess.run(self.model.increment_step)\n        return\n\n    def take_action(self, all_brain_info: AllBrainInfo):\n        """"""\n        Decides actions given state/observation information, and takes them in environment.\n        :param all_brain_info: AllBrainInfo from environment.\n        :return: a tuple containing action, memories, values and an object\n        to be passed to add experiences\n        """"""\n        if len(all_brain_info[self.brain_name].agents) == 0:\n            return [], [], [], None\n\n        agent_brain = all_brain_info[self.brain_name]\n        feed_dict = {self.model.dropout_rate: 1.0, self.model.sequence_length: 1}\n\n        if self.use_visual_observations:\n            for i, _ in enumerate(agent_brain.visual_observations):\n                feed_dict[self.model.visual_in[i]] = agent_brain.visual_observations[i]\n        if self.use_vector_observations:\n            feed_dict[self.model.vector_in] = agent_brain.vector_observations\n        if self.use_recurrent:\n            if agent_brain.memories.shape[1] == 0:\n                agent_brain.memories = np.zeros((len(agent_brain.agents), self.m_size))\n            feed_dict[self.model.memory_in] = agent_brain.memories\n            agent_action, memories = self.sess.run(self.inference_run_list, feed_dict)\n            return agent_action, memories, None, None\n        else:\n            agent_action = self.sess.run(self.inference_run_list, feed_dict)\n        return agent_action, None, None, None\n\n    def add_experiences(self, curr_info: AllBrainInfo, next_info: AllBrainInfo, take_action_outputs):\n        """"""\n        Adds experiences to each agent\'s experience history.\n        :param curr_info: Current AllBrainInfo (Dictionary of all current brains and corresponding BrainInfo).\n        :param next_info: Next AllBrainInfo (Dictionary of all current brains and corresponding BrainInfo).\n        :param take_action_outputs: The outputs of the take action method.\n        """"""\n\n        # Used to collect teacher experience into training buffer\n        info_teacher = curr_info[self.brain_to_imitate]\n        next_info_teacher = next_info[self.brain_to_imitate]\n        for agent_id in info_teacher.agents:\n            self.training_buffer[agent_id].last_brain_info = info_teacher\n\n        for agent_id in next_info_teacher.agents:\n            stored_info_teacher = self.training_buffer[agent_id].last_brain_info\n            if stored_info_teacher is None:\n                continue\n            else:\n                idx = stored_info_teacher.agents.index(agent_id)\n                next_idx = next_info_teacher.agents.index(agent_id)\n                if stored_info_teacher.text_observations[idx] != """":\n                    info_teacher_record, info_teacher_reset = \\\n                        stored_info_teacher.text_observations[idx].lower().split("","")\n                    next_info_teacher_record, next_info_teacher_reset = next_info_teacher.text_observations[idx].\\\n                        lower().split("","")\n                    if next_info_teacher_reset == ""true"":\n                        self.training_buffer.reset_update_buffer()\n                else:\n                    info_teacher_record, next_info_teacher_record = ""true"", ""true""\n                if info_teacher_record == ""true"" and next_info_teacher_record == ""true"":\n                    if not stored_info_teacher.local_done[idx]:\n                        if self.use_visual_observations:\n                            for i, _ in enumerate(stored_info_teacher.visual_observations):\n                                self.training_buffer[agent_id][\'visual_observations%d\' % i]\\\n                                    .append(stored_info_teacher.visual_observations[i][idx])\n                        if self.use_vector_observations:\n                            self.training_buffer[agent_id][\'vector_observations\']\\\n                                .append(stored_info_teacher.vector_observations[idx])\n                        if self.use_recurrent:\n                            if stored_info_teacher.memories.shape[1] == 0:\n                                stored_info_teacher.memories = np.zeros((len(stored_info_teacher.agents), self.m_size))\n                            self.training_buffer[agent_id][\'memory\'].append(stored_info_teacher.memories[idx])\n                        self.training_buffer[agent_id][\'actions\'].append(next_info_teacher.\n                                                                         previous_vector_actions[next_idx])\n        info_student = curr_info[self.brain_name]\n        next_info_student = next_info[self.brain_name]\n        for agent_id in info_student.agents:\n            self.training_buffer[agent_id].last_brain_info = info_student\n\n        # Used to collect information about student performance.\n        for agent_id in next_info_student.agents:\n            stored_info_student = self.training_buffer[agent_id].last_brain_info\n            if stored_info_student is None:\n                continue\n            else:\n                next_idx = next_info_student.agents.index(agent_id)\n                if agent_id not in self.cumulative_rewards:\n                    self.cumulative_rewards[agent_id] = 0\n                self.cumulative_rewards[agent_id] += next_info_student.rewards[next_idx]\n                if not next_info_student.local_done[next_idx]:\n                    if agent_id not in self.episode_steps:\n                        self.episode_steps[agent_id] = 0\n                    self.episode_steps[agent_id] += 1\n\n    def process_experiences(self, current_info: AllBrainInfo, next_info: AllBrainInfo):\n        """"""\n        Checks agent histories for processing condition, and processes them as necessary.\n        Processing involves calculating value and advantage targets for model updating step.\n        :param current_info: Current AllBrainInfo\n        :param next_info: Next AllBrainInfo\n        """"""\n        info_teacher = next_info[self.brain_to_imitate]\n        for l in range(len(info_teacher.agents)):\n            if ((info_teacher.local_done[l] or\n                 len(self.training_buffer[info_teacher.agents[l]][\'actions\']) > self.trainer_parameters[\n                 \'time_horizon\'])\n                    and len(self.training_buffer[info_teacher.agents[l]][\'actions\']) > 0):\n                agent_id = info_teacher.agents[l]\n                self.training_buffer.append_update_buffer(agent_id, batch_size=None,\n                                                          training_length=self.sequence_length)\n                self.training_buffer[agent_id].reset_agent()\n\n        info_student = next_info[self.brain_name]\n        for l in range(len(info_student.agents)):\n            if info_student.local_done[l]:\n                agent_id = info_student.agents[l]\n                self.stats[\'cumulative_reward\'].append(\n                    self.cumulative_rewards.get(agent_id, 0))\n                self.stats[\'episode_length\'].append(\n                    self.episode_steps.get(agent_id, 0))\n                self.cumulative_rewards[agent_id] = 0\n                self.episode_steps[agent_id] = 0\n\n    def end_episode(self):\n        """"""\n        A signal that the Episode has ended. The buffer must be reset. \n        Get only called when the academy resets.\n        """"""\n        self.training_buffer.reset_all()\n        for agent_id in self.cumulative_rewards:\n            self.cumulative_rewards[agent_id] = 0\n        for agent_id in self.episode_steps:\n            self.episode_steps[agent_id] = 0\n\n    def is_ready_update(self):\n        """"""\n        Returns whether or not the trainer has enough elements to run update model\n        :return: A boolean corresponding to whether or not update_model() can be run\n        """"""\n        return len(self.training_buffer.update_buffer[\'actions\']) > self.n_sequences\n\n    def update_model(self):\n        """"""\n        Uses training_buffer to update model.\n        """"""\n        self.training_buffer.update_buffer.shuffle()\n        batch_losses = []\n        for j in range(\n                min(len(self.training_buffer.update_buffer[\'actions\']) // self.n_sequences, self.batches_per_epoch)):\n            _buffer = self.training_buffer.update_buffer\n            start = j * self.n_sequences\n            end = (j + 1) * self.n_sequences\n\n            feed_dict = {self.model.dropout_rate: 0.5,\n                         self.model.batch_size: self.n_sequences,\n                         self.model.sequence_length: self.sequence_length}\n            if self.is_continuous_action:\n                feed_dict[self.model.true_action] = np.array(_buffer[\'actions\'][start:end]).\\\n                    reshape([-1, self.brain.vector_action_space_size])\n            else:\n                feed_dict[self.model.true_action] = np.array(_buffer[\'actions\'][start:end]).reshape([-1])\n            if self.use_vector_observations:\n                if not self.is_continuous_observation:\n                    feed_dict[self.model.vector_in] = np.array(_buffer[\'vector_observations\'][start:end])\\\n                        .reshape([-1, self.brain.num_stacked_vector_observations])\n                else:\n                    feed_dict[self.model.vector_in] = np.array(_buffer[\'vector_observations\'][start:end])\\\n                        .reshape([-1, self.brain.vector_observation_space_size * self.brain.num_stacked_vector_observations])\n            if self.use_visual_observations:\n                for i, _ in enumerate(self.model.visual_in):\n                    _obs = np.array(_buffer[\'visual_observations%d\' % i][start:end])\n                    feed_dict[self.model.visual_in[i]] = _obs\n            if self.use_recurrent:\n                feed_dict[self.model.memory_in] = np.zeros([self.n_sequences, self.m_size])\n            loss, _ = self.sess.run([self.model.loss, self.model.update], feed_dict=feed_dict)\n            batch_losses.append(loss)\n        if len(batch_losses) > 0:\n            self.stats[\'losses\'].append(np.mean(batch_losses))\n        else:\n            self.stats[\'losses\'].append(0)\n'"
python/unitytrainers/ppo/__init__.py,0,b'from .models import *\nfrom .trainer import *\n'
python/unitytrainers/ppo/models.py,0,"b'import logging\n\nimport tensorflow as tf\nfrom unitytrainers.models import LearningModel\n\nlogger = logging.getLogger(""unityagents"")\n\n\nclass PPOModel(LearningModel):\n    def __init__(self, brain, lr=1e-4, h_size=128, epsilon=0.2, beta=1e-3, max_step=5e6,\n                 normalize=False, use_recurrent=False, num_layers=2, m_size=None, use_curiosity=False,\n                 curiosity_strength=0.01, curiosity_enc_size=128):\n        """"""\n        Takes a Unity environment and model-specific hyper-parameters and returns the\n        appropriate PPO agent model for the environment.\n        :param brain: BrainInfo used to generate specific network graph.\n        :param lr: Learning rate.\n        :param h_size: Size of hidden layers\n        :param epsilon: Value for policy-divergence threshold.\n        :param beta: Strength of entropy regularization.\n        :return: a sub-class of PPOAgent tailored to the environment.\n        :param max_step: Total number of training steps.\n        :param normalize: Whether to normalize vector observation input.\n        :param use_recurrent: Whether to use an LSTM layer in the network.\n        :param num_layers Number of hidden layers between encoded input and policy & value layers\n        :param m_size: Size of brain memory.\n        """"""\n        LearningModel.__init__(self, m_size, normalize, use_recurrent, brain)\n        self.use_curiosity = use_curiosity\n        if num_layers < 1:\n            num_layers = 1\n        self.last_reward, self.new_reward, self.update_reward = self.create_reward_encoder()\n        if brain.vector_action_space_type == ""continuous"":\n            self.create_cc_actor_critic(h_size, num_layers)\n            self.entropy = tf.ones_like(tf.reshape(self.value, [-1])) * self.entropy\n        else:\n            self.create_dc_actor_critic(h_size, num_layers)\n        if self.use_curiosity:\n            self.curiosity_enc_size = curiosity_enc_size\n            self.curiosity_strength = curiosity_strength\n            encoded_state, encoded_next_state = self.create_curiosity_encoders()\n            self.create_inverse_model(encoded_state, encoded_next_state)\n            self.create_forward_model(encoded_state, encoded_next_state)\n        self.create_ppo_optimizer(self.probs, self.old_probs, self.value,\n                                  self.entropy, beta, epsilon, lr, max_step)\n\n    @staticmethod\n    def create_reward_encoder():\n        """"""Creates TF ops to track and increment recent average cumulative reward.""""""\n        last_reward = tf.Variable(0, name=""last_reward"", trainable=False, dtype=tf.float32)\n        new_reward = tf.placeholder(shape=[], dtype=tf.float32, name=\'new_reward\')\n        update_reward = tf.assign(last_reward, new_reward)\n        return last_reward, new_reward, update_reward\n\n    def create_curiosity_encoders(self):\n        """"""\n        Creates state encoders for current and future observations.\n        Used for implementation of \xef\xbb\xbfCuriosity-driven Exploration by Self-supervised Prediction\n        See https://arxiv.org/abs/1705.05363 for more details.\n        :return: current and future state encoder tensors.\n        """"""\n        encoded_state_list = []\n        encoded_next_state_list = []\n\n        if self.v_size > 0:\n            self.next_visual_in = []\n            visual_encoders = []\n            next_visual_encoders = []\n            for i in range(self.v_size):\n                # Create input ops for next (t+1) visual observations.\n                next_visual_input = self.create_visual_input(self.brain.camera_resolutions[i],\n                                                             name=""next_visual_observation_"" + str(i))\n                self.next_visual_in.append(next_visual_input)\n\n                # Create the encoder ops for current and next visual input. Not that these encoders are siamese.\n                encoded_visual = self.create_visual_observation_encoder(self.visual_in[i], self.curiosity_enc_size,\n                                                                        self.swish, 1, ""stream_{}_visual_obs_encoder""\n                                                                        .format(i), False)\n\n                encoded_next_visual = self.create_visual_observation_encoder(self.next_visual_in[i],\n                                                                             self.curiosity_enc_size,\n                                                                             self.swish, 1,\n                                                                             ""stream_{}_visual_obs_encoder"".format(i),\n                                                                             True)\n                visual_encoders.append(encoded_visual)\n                next_visual_encoders.append(encoded_next_visual)\n\n            hidden_visual = tf.concat(visual_encoders, axis=1)\n            hidden_next_visual = tf.concat(next_visual_encoders, axis=1)\n            encoded_state_list.append(hidden_visual)\n            encoded_next_state_list.append(hidden_next_visual)\n\n        if self.o_size > 0:\n\n            # Create the encoder ops for current and next vector input. Not that these encoders are siamese.\n            if self.brain.vector_observation_space_type == ""continuous"":\n                # Create input op for next (t+1) vector observation.\n                self.next_vector_in = tf.placeholder(shape=[None, self.o_size], dtype=tf.float32,\n                                                     name=\'next_vector_observation\')\n\n                encoded_vector_obs = self.create_continuous_observation_encoder(self.vector_in,\n                                                                                self.curiosity_enc_size,\n                                                                                self.swish, 2, ""vector_obs_encoder"",\n                                                                                False)\n                encoded_next_vector_obs = self.create_continuous_observation_encoder(self.next_vector_in,\n                                                                                     self.curiosity_enc_size,\n                                                                                     self.swish, 2,\n                                                                                     ""vector_obs_encoder"",\n                                                                                     True)\n            else:\n                self.next_vector_in = tf.placeholder(shape=[None, 1], dtype=tf.int32,\n                                                     name=\'next_vector_observation\')\n\n                encoded_vector_obs = self.create_discrete_observation_encoder(self.vector_in, self.o_size,\n                                                                              self.curiosity_enc_size,\n                                                                              self.swish, 2, ""vector_obs_encoder"",\n                                                                              False)\n                encoded_next_vector_obs = self.create_discrete_observation_encoder(self.next_vector_in, self.o_size,\n                                                                                   self.curiosity_enc_size,\n                                                                                   self.swish, 2, ""vector_obs_encoder"",\n                                                                                   True)\n            encoded_state_list.append(encoded_vector_obs)\n            encoded_next_state_list.append(encoded_next_vector_obs)\n\n        encoded_state = tf.concat(encoded_state_list, axis=1)\n        encoded_next_state = tf.concat(encoded_next_state_list, axis=1)\n        return encoded_state, encoded_next_state\n\n    def create_inverse_model(self, encoded_state, encoded_next_state):\n        """"""\n        Creates inverse model TensorFlow ops for Curiosity module.\n        Predicts action taken given current and future encoded states.\n        :param encoded_state: Tensor corresponding to encoded current state.\n        :param encoded_next_state: Tensor corresponding to encoded next state.\n        """"""\n        combined_input = tf.concat([encoded_state, encoded_next_state], axis=1)\n        hidden = tf.layers.dense(combined_input, 256, activation=self.swish)\n        if self.brain.vector_action_space_type == ""continuous"":\n            pred_action = tf.layers.dense(hidden, self.a_size, activation=None)\n            squared_difference = tf.reduce_sum(tf.squared_difference(pred_action, self.selected_actions), axis=1)\n            self.inverse_loss = tf.reduce_mean(tf.dynamic_partition(squared_difference, self.mask, 2)[1])\n        else:\n            pred_action = tf.layers.dense(hidden, self.a_size, activation=tf.nn.softmax)\n            cross_entropy = tf.reduce_sum(-tf.log(pred_action + 1e-10) * self.selected_actions, axis=1)\n            self.inverse_loss = tf.reduce_mean(tf.dynamic_partition(cross_entropy, self.mask, 2)[1])\n\n    def create_forward_model(self, encoded_state, encoded_next_state):\n        """"""\n        Creates forward model TensorFlow ops for Curiosity module.\n        Predicts encoded future state based on encoded current state and given action.\n        :param encoded_state: Tensor corresponding to encoded current state.\n        :param encoded_next_state: Tensor corresponding to encoded next state.\n        """"""\n        combined_input = tf.concat([encoded_state, self.selected_actions], axis=1)\n        hidden = tf.layers.dense(combined_input, 256, activation=self.swish)\n        # We compare against the concatenation of all observation streams, hence `self.v_size + int(self.o_size > 0)`.\n        pred_next_state = tf.layers.dense(hidden, self.curiosity_enc_size * (self.v_size + int(self.o_size > 0)),\n                                          activation=None)\n\n        squared_difference = 0.5 * tf.reduce_sum(tf.squared_difference(pred_next_state, encoded_next_state), axis=1)\n        self.intrinsic_reward = tf.clip_by_value(self.curiosity_strength * squared_difference, 0, 1)\n        self.forward_loss = tf.reduce_mean(tf.dynamic_partition(squared_difference, self.mask, 2)[1])\n\n    def create_ppo_optimizer(self, probs, old_probs, value, entropy, beta, epsilon, lr, max_step):\n        """"""\n        Creates training-specific Tensorflow ops for PPO models.\n        :param probs: Current policy probabilities\n        :param old_probs: Past policy probabilities\n        :param value: Current value estimate\n        :param beta: Entropy regularization strength\n        :param entropy: Current policy entropy\n        :param epsilon: Value for policy-divergence threshold\n        :param lr: Learning rate\n        :param max_step: Total number of training steps.\n        """"""\n        self.returns_holder = tf.placeholder(shape=[None], dtype=tf.float32, name=\'discounted_rewards\')\n        self.advantage = tf.placeholder(shape=[None, 1], dtype=tf.float32, name=\'advantages\')\n        self.learning_rate = tf.train.polynomial_decay(lr, self.global_step, max_step, 1e-10, power=1.0)\n\n        self.old_value = tf.placeholder(shape=[None], dtype=tf.float32, name=\'old_value_estimates\')\n\n        decay_epsilon = tf.train.polynomial_decay(epsilon, self.global_step, max_step, 0.1, power=1.0)\n        decay_beta = tf.train.polynomial_decay(beta, self.global_step, max_step, 1e-5, power=1.0)\n        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n\n        clipped_value_estimate = self.old_value + tf.clip_by_value(tf.reduce_sum(value, axis=1) - self.old_value,\n                                                                   - decay_epsilon, decay_epsilon)\n\n        v_opt_a = tf.squared_difference(self.returns_holder, tf.reduce_sum(value, axis=1))\n        v_opt_b = tf.squared_difference(self.returns_holder, clipped_value_estimate)\n        self.value_loss = tf.reduce_mean(tf.dynamic_partition(tf.maximum(v_opt_a, v_opt_b), self.mask, 2)[1])\n\n        # Here we calculate PPO policy loss. In continuous control this is done independently for each action gaussian\n        # and then averaged together. This provides significantly better performance than treating the probability\n        # as an average of probabilities, or as a joint probability.\n        r_theta = probs / (old_probs + 1e-10)\n        p_opt_a = r_theta * self.advantage\n        p_opt_b = tf.clip_by_value(r_theta, 1.0 - decay_epsilon, 1.0 + decay_epsilon) * self.advantage\n        self.policy_loss = -tf.reduce_mean(tf.dynamic_partition(tf.minimum(p_opt_a, p_opt_b), self.mask, 2)[1])\n\n        self.loss = self.policy_loss + 0.5 * self.value_loss - decay_beta * tf.reduce_mean(\n            tf.dynamic_partition(entropy, self.mask, 2)[1])\n\n        if self.use_curiosity:\n            self.loss += 10 * (0.2 * self.forward_loss + 0.8 * self.inverse_loss)\n        self.update_batch = optimizer.minimize(self.loss)\n'"
python/unitytrainers/ppo/trainer.py,0,"b'# # Unity ML-Agents Toolkit\n# ## ML-Agent Learning (PPO)\n# Contains an implementation of PPO as described (https://arxiv.org/abs/1707.06347).\n\nimport logging\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom unityagents import AllBrainInfo, BrainInfo\nfrom unitytrainers.buffer import Buffer\nfrom unitytrainers.ppo.models import PPOModel\nfrom unitytrainers.trainer import UnityTrainerException, Trainer\n\nlogger = logging.getLogger(""unityagents"")\n\n\nclass PPOTrainer(Trainer):\n    """"""The PPOTrainer is an implementation of the PPO algorithm.""""""\n\n    def __init__(self, sess, env, brain_name, trainer_parameters, training, seed):\n        """"""\n        Responsible for collecting experiences and training PPO model.\n        :param sess: Tensorflow session.\n        :param env: The UnityEnvironment.\n        :param  trainer_parameters: The parameters for the trainer (dictionary).\n        :param training: Whether the trainer is set for training.\n        """"""\n        self.param_keys = [\'batch_size\', \'beta\', \'buffer_size\', \'epsilon\', \'gamma\', \'hidden_units\', \'lambd\',\n                           \'learning_rate\', \'max_steps\', \'normalize\', \'num_epoch\', \'num_layers\',\n                           \'time_horizon\', \'sequence_length\', \'summary_freq\', \'use_recurrent\',\n                           \'graph_scope\', \'summary_path\', \'memory_size\', \'use_curiosity\', \'curiosity_strength\',\n                           \'curiosity_enc_size\']\n\n        for k in self.param_keys:\n            if k not in trainer_parameters:\n                raise UnityTrainerException(""The hyperparameter {0} could not be found for the PPO trainer of ""\n                                            ""brain {1}."".format(k, brain_name))\n\n        super(PPOTrainer, self).__init__(sess, env, brain_name, trainer_parameters, training)\n\n        self.use_recurrent = trainer_parameters[""use_recurrent""]\n        self.use_curiosity = bool(trainer_parameters[\'use_curiosity\'])\n        self.sequence_length = 1\n        self.step = 0\n        self.has_updated = False\n        self.m_size = None\n        if self.use_recurrent:\n            self.m_size = trainer_parameters[""memory_size""]\n            self.sequence_length = trainer_parameters[""sequence_length""]\n            if self.m_size == 0:\n                raise UnityTrainerException(""The memory size for brain {0} is 0 even though the trainer uses recurrent.""\n                                            .format(brain_name))\n            elif self.m_size % 4 != 0:\n                raise UnityTrainerException(""The memory size for brain {0} is {1} but it must be divisible by 4.""\n                                            .format(brain_name, self.m_size))\n\n        self.variable_scope = trainer_parameters[\'graph_scope\']\n        with tf.variable_scope(self.variable_scope):\n            tf.set_random_seed(seed)\n            self.model = PPOModel(env.brains[brain_name],\n                                  lr=float(trainer_parameters[\'learning_rate\']),\n                                  h_size=int(trainer_parameters[\'hidden_units\']),\n                                  epsilon=float(trainer_parameters[\'epsilon\']),\n                                  beta=float(trainer_parameters[\'beta\']),\n                                  max_step=float(trainer_parameters[\'max_steps\']),\n                                  normalize=trainer_parameters[\'normalize\'],\n                                  use_recurrent=trainer_parameters[\'use_recurrent\'],\n                                  num_layers=int(trainer_parameters[\'num_layers\']),\n                                  m_size=self.m_size,\n                                  use_curiosity=bool(trainer_parameters[\'use_curiosity\']),\n                                  curiosity_strength=float(trainer_parameters[\'curiosity_strength\']),\n                                  curiosity_enc_size=float(trainer_parameters[\'curiosity_enc_size\']))\n\n        stats = {\'cumulative_reward\': [], \'episode_length\': [], \'value_estimate\': [],\n                 \'entropy\': [], \'value_loss\': [], \'policy_loss\': [], \'learning_rate\': []}\n        if self.use_curiosity:\n            stats[\'forward_loss\'] = []\n            stats[\'inverse_loss\'] = []\n            stats[\'intrinsic_reward\'] = []\n            self.intrinsic_rewards = {}\n        self.stats = stats\n\n        self.training_buffer = Buffer()\n        self.cumulative_rewards = {}\n        self.episode_steps = {}\n        self.is_continuous_action = (env.brains[brain_name].vector_action_space_type == ""continuous"")\n        self.is_continuous_observation = (env.brains[brain_name].vector_observation_space_type == ""continuous"")\n        self.use_visual_obs = (env.brains[brain_name].number_visual_observations > 0)\n        self.use_vector_obs = (env.brains[brain_name].vector_observation_space_size > 0)\n        self.summary_path = trainer_parameters[\'summary_path\']\n        if not os.path.exists(self.summary_path):\n            os.makedirs(self.summary_path)\n\n        self.summary_writer = tf.summary.FileWriter(self.summary_path)\n\n        self.inference_run_list = [self.model.output, self.model.all_probs, self.model.value,\n                                   self.model.entropy, self.model.learning_rate]\n        if self.is_continuous_action:\n            self.inference_run_list.append(self.model.output_pre)\n        if self.use_recurrent:\n            self.inference_run_list.extend([self.model.memory_out])\n        if (self.is_training and self.is_continuous_observation and\n                self.use_vector_obs and self.trainer_parameters[\'normalize\']):\n            self.inference_run_list.extend([self.model.update_mean, self.model.update_variance])\n\n    def __str__(self):\n        return \'\'\'Hyperparameters for the PPO Trainer of brain {0}: \\n{1}\'\'\'.format(\n            self.brain_name, \'\\n\'.join([\'\\t{0}:\\t{1}\'.format(x, self.trainer_parameters[x]) for x in self.param_keys]))\n\n    @property\n    def parameters(self):\n        """"""\n        Returns the trainer parameters of the trainer.\n        """"""\n        return self.trainer_parameters\n\n    @property\n    def graph_scope(self):\n        """"""\n        Returns the graph scope of the trainer.\n        """"""\n        return self.variable_scope\n\n    @property\n    def get_max_steps(self):\n        """"""\n        Returns the maximum number of steps. Is used to know when the trainer should be stopped.\n        :return: The maximum number of steps of the trainer\n        """"""\n        return float(self.trainer_parameters[\'max_steps\'])\n\n    @property\n    def get_step(self):\n        """"""\n        Returns the number of steps the trainer has performed\n        :return: the step count of the trainer\n        """"""\n        return self.step\n\n    @property\n    def get_last_reward(self):\n        """"""\n        Returns the last reward the trainer has had\n        :return: the new last reward\n        """"""\n        return self.sess.run(self.model.last_reward)\n\n    def increment_step_and_update_last_reward(self):\n        """"""\n        Increment the step count of the trainer and Updates the last reward\n        """"""\n        if len(self.stats[\'cumulative_reward\']) > 0:\n            mean_reward = np.mean(self.stats[\'cumulative_reward\'])\n            self.sess.run([self.model.update_reward,\n                           self.model.increment_step],\n                          feed_dict={self.model.new_reward: mean_reward})\n        else:\n            self.sess.run(self.model.increment_step)\n        self.step = self.sess.run(self.model.global_step)\n\n    def take_action(self, all_brain_info: AllBrainInfo):\n        """"""\n        Decides actions given observations information, and takes them in environment.\n        :param all_brain_info: A dictionary of brain names and BrainInfo from environment.\n        :return: a tuple containing action, memories, values and an object\n        to be passed to add experiences\n        """"""\n        curr_brain_info = all_brain_info[self.brain_name]\n        if len(curr_brain_info.agents) == 0:\n            return [], [], [], None\n\n        feed_dict = {self.model.batch_size: len(curr_brain_info.vector_observations),\n                     self.model.sequence_length: 1}\n        if self.use_recurrent:\n            if not self.is_continuous_action:\n                feed_dict[self.model.prev_action] = curr_brain_info.previous_vector_actions.flatten()\n            if curr_brain_info.memories.shape[1] == 0:\n                curr_brain_info.memories = np.zeros((len(curr_brain_info.agents), self.m_size))\n            feed_dict[self.model.memory_in] = curr_brain_info.memories\n        if self.use_visual_obs:\n            for i, _ in enumerate(curr_brain_info.visual_observations):\n                feed_dict[self.model.visual_in[i]] = curr_brain_info.visual_observations[i]\n        if self.use_vector_obs:\n            feed_dict[self.model.vector_in] = curr_brain_info.vector_observations\n\n        values = self.sess.run(self.inference_run_list, feed_dict=feed_dict)\n        run_out = dict(zip(self.inference_run_list, values))\n\n        self.stats[\'value_estimate\'].append(run_out[self.model.value].mean())\n        self.stats[\'entropy\'].append(run_out[self.model.entropy].mean())\n        self.stats[\'learning_rate\'].append(run_out[self.model.learning_rate])\n        if self.use_recurrent:\n            return run_out[self.model.output], run_out[self.model.memory_out], None, run_out\n        else:\n            return run_out[self.model.output], None, None, run_out\n\n    def construct_curr_info(self, next_info: BrainInfo) -> BrainInfo:\n        """"""\n        Constructs a BrainInfo which contains the most recent previous experiences for all agents info\n        which correspond to the agents in a provided next_info.\n        :BrainInfo next_info: A t+1 BrainInfo.\n        :return: curr_info: Reconstructed BrainInfo to match agents of next_info.\n        """"""\n        visual_observations = [[]]\n        vector_observations = []\n        text_observations = []\n        memories = []\n        rewards = []\n        local_dones = []\n        max_reacheds = []\n        agents = []\n        prev_vector_actions = []\n        prev_text_actions = []\n        for agent_id in next_info.agents:\n            agent_brain_info = self.training_buffer[agent_id].last_brain_info\n            agent_index = agent_brain_info.agents.index(agent_id)\n            if agent_brain_info is None:\n                agent_brain_info = next_info\n            for i in range(len(next_info.visual_observations)):\n                visual_observations[i].append(agent_brain_info.visual_observations[i][agent_index])\n            vector_observations.append(agent_brain_info.vector_observations[agent_index])\n            text_observations.append(agent_brain_info.text_observations[agent_index])\n            if self.use_recurrent:\n                memories.append(agent_brain_info.memories[agent_index])\n            rewards.append(agent_brain_info.rewards[agent_index])\n            local_dones.append(agent_brain_info.local_done[agent_index])\n            max_reacheds.append(agent_brain_info.max_reached[agent_index])\n            agents.append(agent_brain_info.agents[agent_index])\n            prev_vector_actions.append(agent_brain_info.previous_vector_actions[agent_index])\n            prev_text_actions.append(agent_brain_info.previous_text_actions[agent_index])\n        curr_info = BrainInfo(visual_observations, vector_observations, text_observations, memories, rewards,\n                              agents, local_dones, prev_vector_actions, prev_text_actions, max_reacheds)\n        return curr_info\n\n    def generate_intrinsic_rewards(self, curr_info, next_info):\n        """"""\n        Generates intrinsic reward used for Curiosity-based training.\n        :BrainInfo curr_info: Current BrainInfo.\n        :BrainInfo next_info: Next BrainInfo.\n        :return: Intrinsic rewards for all agents.\n        """"""\n        if self.use_curiosity:\n            feed_dict = {self.model.batch_size: len(next_info.vector_observations), self.model.sequence_length: 1}\n            if self.is_continuous_action:\n                feed_dict[self.model.output] = next_info.previous_vector_actions\n            else:\n                feed_dict[self.model.action_holder] = next_info.previous_vector_actions.flatten()\n\n            if curr_info.agents != next_info.agents:\n                curr_info = self.construct_curr_info(next_info)\n\n            if self.use_visual_obs:\n                for i in range(len(curr_info.visual_observations)):\n                    feed_dict[self.model.visual_in[i]] = curr_info.visual_observations[i]\n                    feed_dict[self.model.next_visual_in[i]] = next_info.visual_observations[i]\n            if self.use_vector_obs:\n                feed_dict[self.model.vector_in] = curr_info.vector_observations\n                feed_dict[self.model.next_vector_in] = next_info.vector_observations\n            if self.use_recurrent:\n                if curr_info.memories.shape[1] == 0:\n                    curr_info.memories = np.zeros((len(curr_info.agents), self.m_size))\n                feed_dict[self.model.memory_in] = curr_info.memories\n            intrinsic_rewards = self.sess.run(self.model.intrinsic_reward,\n                                              feed_dict=feed_dict) * float(self.has_updated)\n            return intrinsic_rewards\n        else:\n            return None\n\n    def generate_value_estimate(self, brain_info, idx):\n        """"""\n        Generates value estimates for bootstrapping.\n        :param brain_info: BrainInfo to be used for bootstrapping.\n        :param idx: Index in BrainInfo of agent.\n        :return: Value estimate.\n        """"""\n        feed_dict = {self.model.batch_size: 1, self.model.sequence_length: 1}\n        if self.use_visual_obs:\n            for i in range(len(brain_info.visual_observations)):\n                feed_dict[self.model.visual_in[i]] = [brain_info.visual_observations[i][idx]]\n        if self.use_vector_obs:\n            feed_dict[self.model.vector_in] = [brain_info.vector_observations[idx]]\n        if self.use_recurrent:\n            if brain_info.memories.shape[1] == 0:\n                brain_info.memories = np.zeros(\n                    (len(brain_info.vector_observations), self.m_size))\n            feed_dict[self.model.memory_in] = [brain_info.memories[idx]]\n        if not self.is_continuous_action and self.use_recurrent:\n            feed_dict[self.model.prev_action] = brain_info.previous_vector_actions[idx].flatten()\n        value_estimate = self.sess.run(self.model.value, feed_dict)\n        return value_estimate\n\n    def add_experiences(self, curr_all_info: AllBrainInfo, next_all_info: AllBrainInfo, take_action_outputs):\n        """"""\n        Adds experiences to each agent\'s experience history.\n        :param curr_all_info: Dictionary of all current brains and corresponding BrainInfo.\n        :param next_all_info: Dictionary of all current brains and corresponding BrainInfo.\n        :param take_action_outputs: The outputs of the take action method.\n        """"""\n        curr_info = curr_all_info[self.brain_name]\n        next_info = next_all_info[self.brain_name]\n\n        for agent_id in curr_info.agents:\n            self.training_buffer[agent_id].last_brain_info = curr_info\n            self.training_buffer[agent_id].last_take_action_outputs = take_action_outputs\n\n        intrinsic_rewards = self.generate_intrinsic_rewards(curr_info, next_info)\n\n        for agent_id in next_info.agents:\n            stored_info = self.training_buffer[agent_id].last_brain_info\n            stored_take_action_outputs = self.training_buffer[agent_id].last_take_action_outputs\n            if stored_info is not None:\n                idx = stored_info.agents.index(agent_id)\n                next_idx = next_info.agents.index(agent_id)\n                if not stored_info.local_done[idx]:\n                    if self.use_visual_obs:\n                        for i, _ in enumerate(stored_info.visual_observations):\n                            self.training_buffer[agent_id][\'visual_obs%d\' % i].append(\n                                stored_info.visual_observations[i][idx])\n                            self.training_buffer[agent_id][\'next_visual_obs%d\' % i].append(\n                                next_info.visual_observations[i][idx])\n                    if self.use_vector_obs:\n                        self.training_buffer[agent_id][\'vector_obs\'].append(stored_info.vector_observations[idx])\n                        self.training_buffer[agent_id][\'next_vector_in\'].append(\n                            next_info.vector_observations[next_idx])\n                    if self.use_recurrent:\n                        if stored_info.memories.shape[1] == 0:\n                            stored_info.memories = np.zeros((len(stored_info.agents), self.m_size))\n                        self.training_buffer[agent_id][\'memory\'].append(stored_info.memories[idx])\n                    actions = stored_take_action_outputs[self.model.output]\n                    if self.is_continuous_action:\n                        actions_pre = stored_take_action_outputs[self.model.output_pre]\n                        self.training_buffer[agent_id][\'actions_pre\'].append(actions_pre[idx])\n                    a_dist = stored_take_action_outputs[self.model.all_probs]\n                    value = stored_take_action_outputs[self.model.value]\n                    self.training_buffer[agent_id][\'actions\'].append(actions[idx])\n                    self.training_buffer[agent_id][\'prev_action\'].append(stored_info.previous_vector_actions[idx])\n                    self.training_buffer[agent_id][\'masks\'].append(1.0)\n                    if self.use_curiosity:\n                        self.training_buffer[agent_id][\'rewards\'].append(next_info.rewards[next_idx] +\n                                                                         intrinsic_rewards[next_idx])\n                    else:\n                        self.training_buffer[agent_id][\'rewards\'].append(next_info.rewards[next_idx])\n                    self.training_buffer[agent_id][\'action_probs\'].append(a_dist[idx])\n                    self.training_buffer[agent_id][\'value_estimates\'].append(value[idx][0])\n\n                    if agent_id not in self.cumulative_rewards:\n                        self.cumulative_rewards[agent_id] = 0\n                    self.cumulative_rewards[agent_id] += next_info.rewards[next_idx]\n                    if self.use_curiosity:\n                        if agent_id not in self.intrinsic_rewards:\n                            self.intrinsic_rewards[agent_id] = 0\n                        self.intrinsic_rewards[agent_id] += intrinsic_rewards[next_idx]\n                if not next_info.local_done[next_idx]:\n                    if agent_id not in self.episode_steps:\n                        self.episode_steps[agent_id] = 0\n                    self.episode_steps[agent_id] += 1\n\n    def process_experiences(self, current_info: AllBrainInfo, new_info: AllBrainInfo):\n        """"""\n        Checks agent histories for processing condition, and processes them as necessary.\n        Processing involves calculating value and advantage targets for model updating step.\n        :param current_info: Dictionary of all current brains and corresponding BrainInfo.\n        :param new_info: Dictionary of all next brains and corresponding BrainInfo.\n        """"""\n\n        info = new_info[self.brain_name]\n        for l in range(len(info.agents)):\n            agent_actions = self.training_buffer[info.agents[l]][\'actions\']\n            if ((info.local_done[l] or len(agent_actions) > self.trainer_parameters[\'time_horizon\'])\n                    and len(agent_actions) > 0):\n                agent_id = info.agents[l]\n                if info.local_done[l] and not info.max_reached[l]:\n                    value_next = 0.0\n                else:\n                    if info.max_reached[l]:\n                        bootstrapping_info = self.training_buffer[agent_id].last_brain_info\n                        idx = bootstrapping_info.agents.index(agent_id)\n                    else:\n                        bootstrapping_info = info\n                        idx = l\n                    value_next = self.generate_value_estimate(bootstrapping_info, idx)\n\n                self.training_buffer[agent_id][\'advantages\'].set(\n                    get_gae(\n                        rewards=self.training_buffer[agent_id][\'rewards\'].get_batch(),\n                        value_estimates=self.training_buffer[agent_id][\'value_estimates\'].get_batch(),\n                        value_next=value_next,\n                        gamma=self.trainer_parameters[\'gamma\'],\n                        lambd=self.trainer_parameters[\'lambd\']))\n                self.training_buffer[agent_id][\'discounted_returns\'].set(\n                    self.training_buffer[agent_id][\'advantages\'].get_batch()\n                    + self.training_buffer[agent_id][\'value_estimates\'].get_batch())\n\n                self.training_buffer.append_update_buffer(agent_id, batch_size=None,\n                                                          training_length=self.sequence_length)\n\n                self.training_buffer[agent_id].reset_agent()\n                if info.local_done[l]:\n                    self.stats[\'cumulative_reward\'].append(\n                        self.cumulative_rewards.get(agent_id, 0))\n                    self.stats[\'episode_length\'].append(\n                        self.episode_steps.get(agent_id, 0))\n                    self.cumulative_rewards[agent_id] = 0\n                    self.episode_steps[agent_id] = 0\n                    if self.use_curiosity:\n                        self.stats[\'intrinsic_reward\'].append(\n                            self.intrinsic_rewards.get(agent_id, 0))\n                        self.intrinsic_rewards[agent_id] = 0\n\n    def end_episode(self):\n        """"""\n        A signal that the Episode has ended. The buffer must be reset. \n        Get only called when the academy resets.\n        """"""\n        self.training_buffer.reset_all()\n        for agent_id in self.cumulative_rewards:\n            self.cumulative_rewards[agent_id] = 0\n        for agent_id in self.episode_steps:\n            self.episode_steps[agent_id] = 0\n        if self.use_curiosity:\n            for agent_id in self.intrinsic_rewards:\n                self.intrinsic_rewards[agent_id] = 0\n\n    def is_ready_update(self):\n        """"""\n        Returns whether or not the trainer has enough elements to run update model\n        :return: A boolean corresponding to whether or not update_model() can be run\n        """"""\n        size_of_buffer = len(self.training_buffer.update_buffer[\'actions\'])\n        return size_of_buffer > max(int(self.trainer_parameters[\'buffer_size\'] / self.sequence_length), 1)\n\n    def update_model(self):\n        """"""\n        Uses training_buffer to update model.\n        """"""\n        n_sequences = max(int(self.trainer_parameters[\'batch_size\'] / self.sequence_length), 1)\n        value_total, policy_total, forward_total, inverse_total = [], [], [], []\n        advantages = self.training_buffer.update_buffer[\'advantages\'].get_batch()\n        self.training_buffer.update_buffer[\'advantages\'].set(\n            (advantages - advantages.mean()) / (advantages.std() + 1e-10))\n        num_epoch = self.trainer_parameters[\'num_epoch\']\n        for k in range(num_epoch):\n            self.training_buffer.update_buffer.shuffle()\n            buffer = self.training_buffer.update_buffer\n            for l in range(len(self.training_buffer.update_buffer[\'actions\']) // n_sequences):\n                start = l * n_sequences\n                end = (l + 1) * n_sequences\n                feed_dict = {self.model.batch_size: n_sequences,\n                             self.model.sequence_length: self.sequence_length,\n                             self.model.mask_input: np.array(buffer[\'masks\'][start:end]).flatten(),\n                             self.model.returns_holder: np.array(buffer[\'discounted_returns\'][start:end]).flatten(),\n                             self.model.old_value: np.array(buffer[\'value_estimates\'][start:end]).flatten(),\n                             self.model.advantage: np.array(buffer[\'advantages\'][start:end]).reshape([-1, 1]),\n                             self.model.all_old_probs: np.array(buffer[\'action_probs\'][start:end]).reshape(\n                                 [-1, self.brain.vector_action_space_size])}\n                if self.is_continuous_action:\n                    feed_dict[self.model.output_pre] = np.array(buffer[\'actions_pre\'][start:end]).reshape(\n                        [-1, self.brain.vector_action_space_size])\n                else:\n                    feed_dict[self.model.action_holder] = np.array(buffer[\'actions\'][start:end]).flatten()\n                    if self.use_recurrent:\n                        feed_dict[self.model.prev_action] = np.array(buffer[\'prev_action\'][start:end]).flatten()\n                if self.use_vector_obs:\n                    if self.is_continuous_observation:\n                        total_observation_length = self.brain.vector_observation_space_size * \\\n                                                   self.brain.num_stacked_vector_observations\n                        feed_dict[self.model.vector_in] = np.array(buffer[\'vector_obs\'][start:end]).reshape(\n                            [-1, total_observation_length])\n                        if self.use_curiosity:\n                            feed_dict[self.model.next_vector_in] = np.array(buffer[\'next_vector_in\'][start:end]) \\\n                                .reshape([-1, total_observation_length])\n                    else:\n                        feed_dict[self.model.vector_in] = np.array(buffer[\'vector_obs\'][start:end]).reshape(\n                            [-1, self.brain.num_stacked_vector_observations])\n                        if self.use_curiosity:\n                            feed_dict[self.model.next_vector_in] = np.array(buffer[\'next_vector_in\'][start:end]) \\\n                                .reshape([-1, self.brain.num_stacked_vector_observations])\n                if self.use_visual_obs:\n                    for i, _ in enumerate(self.model.visual_in):\n                        _obs = np.array(buffer[\'visual_obs%d\' % i][start:end])\n                        if self.sequence_length > 1 and self.use_recurrent:\n                            (_batch, _seq, _w, _h, _c) = _obs.shape\n                            feed_dict[self.model.visual_in[i]] = _obs.reshape([-1, _w, _h, _c])\n                        else:\n                            feed_dict[self.model.visual_in[i]] = _obs\n                    if self.use_curiosity:\n                        for i, _ in enumerate(self.model.visual_in):\n                            _obs = np.array(buffer[\'next_visual_obs%d\' % i][start:end])\n                            if self.sequence_length > 1 and self.use_recurrent:\n                                (_batch, _seq, _w, _h, _c) = _obs.shape\n                                feed_dict[self.model.next_visual_in[i]] = _obs.reshape([-1, _w, _h, _c])\n                            else:\n                                feed_dict[self.model.next_visual_in[i]] = _obs\n                if self.use_recurrent:\n                    mem_in = np.array(buffer[\'memory\'][start:end])[:, 0, :]\n                    feed_dict[self.model.memory_in] = mem_in\n\n                run_list = [self.model.value_loss, self.model.policy_loss, self.model.update_batch]\n                if self.use_curiosity:\n                    run_list.extend([self.model.forward_loss, self.model.inverse_loss])\n                values = self.sess.run(run_list, feed_dict=feed_dict)\n                self.has_updated = True\n                run_out = dict(zip(run_list, values))\n                value_total.append(run_out[self.model.value_loss])\n                policy_total.append(np.abs(run_out[self.model.policy_loss]))\n                if self.use_curiosity:\n                    inverse_total.append(run_out[self.model.inverse_loss])\n                    forward_total.append(run_out[self.model.forward_loss])\n        self.stats[\'value_loss\'].append(np.mean(value_total))\n        self.stats[\'policy_loss\'].append(np.mean(policy_total))\n        if self.use_curiosity:\n            self.stats[\'forward_loss\'].append(np.mean(forward_total))\n            self.stats[\'inverse_loss\'].append(np.mean(inverse_total))\n        self.training_buffer.reset_update_buffer()\n\n\ndef discount_rewards(r, gamma=0.99, value_next=0.0):\n    """"""\n    Computes discounted sum of future rewards for use in updating value estimate.\n    :param r: List of rewards.\n    :param gamma: Discount factor.\n    :param value_next: T+1 value estimate for returns calculation.\n    :return: discounted sum of future rewards as list.\n    """"""\n    discounted_r = np.zeros_like(r)\n    running_add = value_next\n    for t in reversed(range(0, r.size)):\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r\n\n\ndef get_gae(rewards, value_estimates, value_next=0.0, gamma=0.99, lambd=0.95):\n    """"""\n    Computes generalized advantage estimate for use in updating policy.\n    :param rewards: list of rewards for time-steps t to T.\n    :param value_next: Value estimate for time-step T+1.\n    :param value_estimates: list of value estimates for time-steps t to T.\n    :param gamma: Discount factor.\n    :param lambd: GAE weighing factor.\n    :return: list of advantage estimates for time-steps t to T.\n    """"""\n    value_estimates = np.asarray(value_estimates.tolist() + [value_next])\n    delta_t = rewards + gamma * value_estimates[1:] - value_estimates[:-1]\n    advantage = discount_rewards(r=delta_t, gamma=gamma * lambd)\n    return advantage\n'"
