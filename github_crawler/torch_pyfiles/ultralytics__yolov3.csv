file_path,api_count,code
detect.py,10,"b""import argparse\n\nfrom models import *  # set ONNX_EXPORT in models.py\nfrom utils.datasets import *\nfrom utils.utils import *\n\n\ndef detect(save_img=False):\n    imgsz = (320, 192) if ONNX_EXPORT else opt.img_size  # (320, 192) or (416, 256) or (608, 352) for (height, width)\n    out, source, weights, half, view_img, save_txt = opt.output, opt.source, opt.weights, opt.half, opt.view_img, opt.save_txt\n    webcam = source == '0' or source.startswith('rtsp') or source.startswith('http') or source.endswith('.txt')\n\n    # Initialize\n    device = torch_utils.select_device(device='cpu' if ONNX_EXPORT else opt.device)\n    if os.path.exists(out):\n        shutil.rmtree(out)  # delete output folder\n    os.makedirs(out)  # make new output folder\n\n    # Initialize model\n    model = Darknet(opt.cfg, imgsz)\n\n    # Load weights\n    attempt_download(weights)\n    if weights.endswith('.pt'):  # pytorch format\n        model.load_state_dict(torch.load(weights, map_location=device)['model'])\n    else:  # darknet format\n        load_darknet_weights(model, weights)\n\n    # Second-stage classifier\n    classify = False\n    if classify:\n        modelc = torch_utils.load_classifier(name='resnet101', n=2)  # initialize\n        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model'])  # load weights\n        modelc.to(device).eval()\n\n    # Eval mode\n    model.to(device).eval()\n\n    # Fuse Conv2d + BatchNorm2d layers\n    # model.fuse()\n\n    # Export mode\n    if ONNX_EXPORT:\n        model.fuse()\n        img = torch.zeros((1, 3) + imgsz)  # (1, 3, 320, 192)\n        f = opt.weights.replace(opt.weights.split('.')[-1], 'onnx')  # *.onnx filename\n        torch.onnx.export(model, img, f, verbose=False, opset_version=11,\n                          input_names=['images'], output_names=['classes', 'boxes'])\n\n        # Validate exported model\n        import onnx\n        model = onnx.load(f)  # Load the ONNX model\n        onnx.checker.check_model(model)  # Check that the IR is well formed\n        print(onnx.helper.printable_graph(model.graph))  # Print a human readable representation of the graph\n        return\n\n    # Half precision\n    half = half and device.type != 'cpu'  # half precision only supported on CUDA\n    if half:\n        model.half()\n\n    # Set Dataloader\n    vid_path, vid_writer = None, None\n    if webcam:\n        view_img = True\n        torch.backends.cudnn.benchmark = True  # set True to speed up constant image size inference\n        dataset = LoadStreams(source, img_size=imgsz)\n    else:\n        save_img = True\n        dataset = LoadImages(source, img_size=imgsz)\n\n    # Get names and colors\n    names = load_classes(opt.names)\n    colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n\n    # Run inference\n    t0 = time.time()\n    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n    _ = model(img.half() if half else img.float()) if device.type != 'cpu' else None  # run once\n    for path, img, im0s, vid_cap in dataset:\n        img = torch.from_numpy(img).to(device)\n        img = img.half() if half else img.float()  # uint8 to fp16/32\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n\n        # Inference\n        t1 = torch_utils.time_synchronized()\n        pred = model(img, augment=opt.augment)[0]\n        t2 = torch_utils.time_synchronized()\n\n        # to float\n        if half:\n            pred = pred.float()\n\n        # Apply NMS\n        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres,\n                                   multi_label=False, classes=opt.classes, agnostic=opt.agnostic_nms)\n\n        # Apply Classifier\n        if classify:\n            pred = apply_classifier(pred, modelc, img, im0s)\n\n        # Process detections\n        for i, det in enumerate(pred):  # detections for image i\n            if webcam:  # batch_size >= 1\n                p, s, im0 = path[i], '%g: ' % i, im0s[i].copy()\n            else:\n                p, s, im0 = path, '', im0s\n\n            save_path = str(Path(out) / Path(p).name)\n            s += '%gx%g ' % img.shape[2:]  # print string\n            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # \xc2\xa0normalization gain whwh\n            if det is not None and len(det):\n                # Rescale boxes from imgsz to im0 size\n                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n\n                # Print results\n                for c in det[:, -1].unique():\n                    n = (det[:, -1] == c).sum()  # detections per class\n                    s += '%g %ss, ' % (n, names[int(c)])  # add to string\n\n                # Write results\n                for *xyxy, conf, cls in det:\n                    if save_txt:  # Write to file\n                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n                        with open(save_path[:save_path.rfind('.')] + '.txt', 'a') as file:\n                            file.write(('%g ' * 5 + '\\n') % (cls, *xywh))  # label format\n\n                    if save_img or view_img:  # Add bbox to image\n                        label = '%s %.2f' % (names[int(cls)], conf)\n                        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)])\n\n            # Print time (inference + NMS)\n            print('%sDone. (%.3fs)' % (s, t2 - t1))\n\n            # Stream results\n            if view_img:\n                cv2.imshow(p, im0)\n                if cv2.waitKey(1) == ord('q'):  # q to quit\n                    raise StopIteration\n\n            # Save results (image with detections)\n            if save_img:\n                if dataset.mode == 'images':\n                    cv2.imwrite(save_path, im0)\n                else:\n                    if vid_path != save_path:  # new video\n                        vid_path = save_path\n                        if isinstance(vid_writer, cv2.VideoWriter):\n                            vid_writer.release()  # release previous video writer\n\n                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*opt.fourcc), fps, (w, h))\n                    vid_writer.write(im0)\n\n    if save_txt or save_img:\n        print('Results saved to %s' % os.getcwd() + os.sep + out)\n        if platform == 'darwin':  # MacOS\n            os.system('open ' + save_path)\n\n    print('Done. (%.3fs)' % (time.time() - t0))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--cfg', type=str, default='cfg/yolov3-spp.cfg', help='*.cfg path')\n    parser.add_argument('--names', type=str, default='data/coco.names', help='*.names path')\n    parser.add_argument('--weights', type=str, default='weights/yolov3-spp-ultralytics.pt', help='weights path')\n    parser.add_argument('--source', type=str, default='data/samples', help='source')  # input file/folder, 0 for webcam\n    parser.add_argument('--output', type=str, default='output', help='output folder')  # output folder\n    parser.add_argument('--img-size', type=int, default=512, help='inference size (pixels)')\n    parser.add_argument('--conf-thres', type=float, default=0.3, help='object confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.6, help='IOU threshold for NMS')\n    parser.add_argument('--fourcc', type=str, default='mp4v', help='output video codec (verify ffmpeg support)')\n    parser.add_argument('--half', action='store_true', help='half precision FP16 inference')\n    parser.add_argument('--device', default='', help='device id (i.e. 0 or 0,1) or cpu')\n    parser.add_argument('--view-img', action='store_true', help='display results')\n    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n    parser.add_argument('--classes', nargs='+', type=int, help='filter by class')\n    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n    parser.add_argument('--augment', action='store_true', help='augmented inference')\n    opt = parser.parse_args()\n    opt.cfg = list(glob.iglob('./**/' + opt.cfg, recursive=True))[0]  # find file\n    opt.names = list(glob.iglob('./**/' + opt.names, recursive=True))[0]  # find file\n    print(opt)\n\n    with torch.no_grad():\n        detect()\n"""
models.py,31,"b'from utils.google_utils import *\nfrom utils.layers import *\nfrom utils.parse_config import *\n\nONNX_EXPORT = False\n\n\ndef create_modules(module_defs, img_size, cfg):\n    # Constructs module list of layer blocks from module configuration in module_defs\n\n    img_size = [img_size] * 2 if isinstance(img_size, int) else img_size  # expand if necessary\n    _ = module_defs.pop(0)  # cfg training hyperparams (unused)\n    output_filters = [3]  # input channels\n    module_list = nn.ModuleList()\n    routs = []  # list of layers which rout to deeper layers\n    yolo_index = -1\n\n    for i, mdef in enumerate(module_defs):\n        modules = nn.Sequential()\n\n        if mdef[\'type\'] == \'convolutional\':\n            bn = mdef[\'batch_normalize\']\n            filters = mdef[\'filters\']\n            k = mdef[\'size\']  # kernel size\n            stride = mdef[\'stride\'] if \'stride\' in mdef else (mdef[\'stride_y\'], mdef[\'stride_x\'])\n            if isinstance(k, int):  # single-size conv\n                modules.add_module(\'Conv2d\', nn.Conv2d(in_channels=output_filters[-1],\n                                                       out_channels=filters,\n                                                       kernel_size=k,\n                                                       stride=stride,\n                                                       padding=k // 2 if mdef[\'pad\'] else 0,\n                                                       groups=mdef[\'groups\'] if \'groups\' in mdef else 1,\n                                                       bias=not bn))\n            else:  # multiple-size conv\n                modules.add_module(\'MixConv2d\', MixConv2d(in_ch=output_filters[-1],\n                                                          out_ch=filters,\n                                                          k=k,\n                                                          stride=stride,\n                                                          bias=not bn))\n\n            if bn:\n                modules.add_module(\'BatchNorm2d\', nn.BatchNorm2d(filters, momentum=0.03, eps=1E-4))\n            else:\n                routs.append(i)  # detection output (goes into yolo layer)\n\n            if mdef[\'activation\'] == \'leaky\':  # activation study https://github.com/ultralytics/yolov3/issues/441\n                modules.add_module(\'activation\', nn.LeakyReLU(0.1, inplace=True))\n            elif mdef[\'activation\'] == \'swish\':\n                modules.add_module(\'activation\', Swish())\n            elif mdef[\'activation\'] == \'mish\':\n                modules.add_module(\'activation\', Mish())\n\n        elif mdef[\'type\'] == \'BatchNorm2d\':\n            filters = output_filters[-1]\n            modules = nn.BatchNorm2d(filters, momentum=0.03, eps=1E-4)\n            if i == 0 and filters == 3:  # normalize RGB image\n                # imagenet mean and var https://pytorch.org/docs/stable/torchvision/models.html#classification\n                modules.running_mean = torch.tensor([0.485, 0.456, 0.406])\n                modules.running_var = torch.tensor([0.0524, 0.0502, 0.0506])\n\n        elif mdef[\'type\'] == \'maxpool\':\n            k = mdef[\'size\']  # kernel size\n            stride = mdef[\'stride\']\n            maxpool = nn.MaxPool2d(kernel_size=k, stride=stride, padding=(k - 1) // 2)\n            if k == 2 and stride == 1:  # yolov3-tiny\n                modules.add_module(\'ZeroPad2d\', nn.ZeroPad2d((0, 1, 0, 1)))\n                modules.add_module(\'MaxPool2d\', maxpool)\n            else:\n                modules = maxpool\n\n        elif mdef[\'type\'] == \'upsample\':\n            if ONNX_EXPORT:  # explicitly state size, avoid scale_factor\n                g = (yolo_index + 1) * 2 / 32  # gain\n                modules = nn.Upsample(size=tuple(int(x * g) for x in img_size))  # img_size = (320, 192)\n            else:\n                modules = nn.Upsample(scale_factor=mdef[\'stride\'])\n\n        elif mdef[\'type\'] == \'route\':  # nn.Sequential() placeholder for \'route\' layer\n            layers = mdef[\'layers\']\n            filters = sum([output_filters[l + 1 if l > 0 else l] for l in layers])\n            routs.extend([i + l if l < 0 else l for l in layers])\n            modules = FeatureConcat(layers=layers)\n\n        elif mdef[\'type\'] == \'shortcut\':  # nn.Sequential() placeholder for \'shortcut\' layer\n            layers = mdef[\'from\']\n            filters = output_filters[-1]\n            routs.extend([i + l if l < 0 else l for l in layers])\n            modules = WeightedFeatureFusion(layers=layers, weight=\'weights_type\' in mdef)\n\n        elif mdef[\'type\'] == \'reorg3d\':  # yolov3-spp-pan-scale\n            pass\n\n        elif mdef[\'type\'] == \'yolo\':\n            yolo_index += 1\n            stride = [32, 16, 8]  # P5, P4, P3 strides\n            if any(x in cfg for x in [\'panet\', \'yolov4\', \'cd53\']):  # stride order reversed\n                stride = list(reversed(stride))\n            layers = mdef[\'from\'] if \'from\' in mdef else []\n            modules = YOLOLayer(anchors=mdef[\'anchors\'][mdef[\'mask\']],  # anchor list\n                                nc=mdef[\'classes\'],  # number of classes\n                                img_size=img_size,  # (416, 416)\n                                yolo_index=yolo_index,  # 0, 1, 2...\n                                layers=layers,  # output layers\n                                stride=stride[yolo_index])\n\n            # Initialize preceding Conv2d() bias (https://arxiv.org/pdf/1708.02002.pdf section 3.3)\n            try:\n                j = layers[yolo_index] if \'from\' in mdef else -1\n                bias_ = module_list[j][0].bias  # shape(255,)\n                bias = bias_[:modules.no * modules.na].view(modules.na, -1)  # shape(3,85)\n                bias[:, 4] += -4.5  # obj\n                bias[:, 5:] += math.log(0.6 / (modules.nc - 0.99))  # cls (sigmoid(p) = 1/nc)\n                module_list[j][0].bias = torch.nn.Parameter(bias_, requires_grad=bias_.requires_grad)\n            except:\n                print(\'WARNING: smart bias initialization failure.\')\n\n        else:\n            print(\'Warning: Unrecognized Layer Type: \' + mdef[\'type\'])\n\n        # Register module list and number of output filters\n        module_list.append(modules)\n        output_filters.append(filters)\n\n    routs_binary = [False] * (i + 1)\n    for i in routs:\n        routs_binary[i] = True\n    return module_list, routs_binary\n\n\nclass YOLOLayer(nn.Module):\n    def __init__(self, anchors, nc, img_size, yolo_index, layers, stride):\n        super(YOLOLayer, self).__init__()\n        self.anchors = torch.Tensor(anchors)\n        self.index = yolo_index  # index of this layer in layers\n        self.layers = layers  # model output layer indices\n        self.stride = stride  # layer stride\n        self.nl = len(layers)  # number of output layers (3)\n        self.na = len(anchors)  # number of anchors (3)\n        self.nc = nc  # number of classes (80)\n        self.no = nc + 5  # number of outputs (85)\n        self.nx, self.ny, self.ng = 0, 0, 0  # initialize number of x, y gridpoints\n        self.anchor_vec = self.anchors / self.stride\n        self.anchor_wh = self.anchor_vec.view(1, self.na, 1, 1, 2)\n\n        if ONNX_EXPORT:\n            self.training = False\n            self.create_grids((img_size[1] // stride, img_size[0] // stride))  # number x, y grid points\n\n    def create_grids(self, ng=(13, 13), device=\'cpu\'):\n        self.nx, self.ny = ng  # x and y grid size\n        self.ng = torch.tensor(ng, dtype=torch.float)\n\n        # build xy offsets\n        if not self.training:\n            yv, xv = torch.meshgrid([torch.arange(self.ny, device=device), torch.arange(self.nx, device=device)])\n            self.grid = torch.stack((xv, yv), 2).view((1, 1, self.ny, self.nx, 2)).float()\n\n        if self.anchor_vec.device != device:\n            self.anchor_vec = self.anchor_vec.to(device)\n            self.anchor_wh = self.anchor_wh.to(device)\n\n    def forward(self, p, out):\n        ASFF = False  # https://arxiv.org/abs/1911.09516\n        if ASFF:\n            i, n = self.index, self.nl  # index in layers, number of layers\n            p = out[self.layers[i]]\n            bs, _, ny, nx = p.shape  # bs, 255, 13, 13\n            if (self.nx, self.ny) != (nx, ny):\n                self.create_grids((nx, ny), p.device)\n\n            # outputs and weights\n            # w = F.softmax(p[:, -n:], 1)  # normalized weights\n            w = torch.sigmoid(p[:, -n:]) * (2 / n)  # sigmoid weights (faster)\n            # w = w / w.sum(1).unsqueeze(1)  # normalize across layer dimension\n\n            # weighted ASFF sum\n            p = out[self.layers[i]][:, :-n] * w[:, i:i + 1]\n            for j in range(n):\n                if j != i:\n                    p += w[:, j:j + 1] * \\\n                         F.interpolate(out[self.layers[j]][:, :-n], size=[ny, nx], mode=\'bilinear\', align_corners=False)\n\n        elif ONNX_EXPORT:\n            bs = 1  # batch size\n        else:\n            bs, _, ny, nx = p.shape  # bs, 255, 13, 13\n            if (self.nx, self.ny) != (nx, ny):\n                self.create_grids((nx, ny), p.device)\n\n        # p.view(bs, 255, 13, 13) -- > (bs, 3, 13, 13, 85)  # (bs, anchors, grid, grid, classes + xywh)\n        p = p.view(bs, self.na, self.no, self.ny, self.nx).permute(0, 1, 3, 4, 2).contiguous()  # prediction\n\n        if self.training:\n            return p\n\n        elif ONNX_EXPORT:\n            # Avoid broadcasting for ANE operations\n            m = self.na * self.nx * self.ny\n            ng = 1. / self.ng.repeat(m, 1)\n            grid = self.grid.repeat(1, self.na, 1, 1, 1).view(m, 2)\n            anchor_wh = self.anchor_wh.repeat(1, 1, self.nx, self.ny, 1).view(m, 2) * ng\n\n            p = p.view(m, self.no)\n            xy = torch.sigmoid(p[:, 0:2]) + grid  # x, y\n            wh = torch.exp(p[:, 2:4]) * anchor_wh  # width, height\n            p_cls = torch.sigmoid(p[:, 4:5]) if self.nc == 1 else \\\n                torch.sigmoid(p[:, 5:self.no]) * torch.sigmoid(p[:, 4:5])  # conf\n            return p_cls, xy * ng, wh\n\n        else:  # inference\n            io = p.clone()  # inference output\n            io[..., :2] = torch.sigmoid(io[..., :2]) + self.grid  # xy\n            io[..., 2:4] = torch.exp(io[..., 2:4]) * self.anchor_wh  # wh yolo method\n            io[..., :4] *= self.stride\n            torch.sigmoid_(io[..., 4:])\n            return io.view(bs, -1, self.no), p  # view [1, 3, 13, 13, 85] as [1, 507, 85]\n\n\nclass Darknet(nn.Module):\n    # YOLOv3 object detection model\n\n    def __init__(self, cfg, img_size=(416, 416), verbose=False):\n        super(Darknet, self).__init__()\n\n        self.module_defs = parse_model_cfg(cfg)\n        self.module_list, self.routs = create_modules(self.module_defs, img_size, cfg)\n        self.yolo_layers = get_yolo_layers(self)\n        # torch_utils.initialize_weights(self)\n\n        # Darknet Header https://github.com/AlexeyAB/darknet/issues/2914#issuecomment-496675346\n        self.version = np.array([0, 2, 5], dtype=np.int32)  # (int32) version info: major, minor, revision\n        self.seen = np.array([0], dtype=np.int64)  # (int64) number of images seen during training\n        self.info(verbose) if not ONNX_EXPORT else None  # print model description\n\n    def forward(self, x, augment=False, verbose=False):\n\n        if not augment:\n            return self.forward_once(x)\n        else:  # Augment images (inference and test only) https://github.com/ultralytics/yolov3/issues/931\n            img_size = x.shape[-2:]  # height, width\n            s = [0.83, 0.67]  # scales\n            y = []\n            for i, xi in enumerate((x,\n                                    torch_utils.scale_img(x.flip(3), s[0], same_shape=False),  # flip-lr and scale\n                                    torch_utils.scale_img(x, s[1], same_shape=False),  # scale\n                                    )):\n                # cv2.imwrite(\'img%g.jpg\' % i, 255 * xi[0].numpy().transpose((1, 2, 0))[:, :, ::-1])\n                y.append(self.forward_once(xi)[0])\n\n            y[1][..., :4] /= s[0]  # scale\n            y[1][..., 0] = img_size[1] - y[1][..., 0]  # flip lr\n            y[2][..., :4] /= s[1]  # scale\n\n            # for i, yi in enumerate(y):  # coco small, medium, large = < 32**2 < 96**2 <\n            #     area = yi[..., 2:4].prod(2)[:, :, None]\n            #     if i == 1:\n            #         yi *= (area < 96. ** 2).float()\n            #     elif i == 2:\n            #         yi *= (area > 32. ** 2).float()\n            #     y[i] = yi\n\n            y = torch.cat(y, 1)\n            return y, None\n\n    def forward_once(self, x, augment=False, verbose=False):\n        img_size = x.shape[-2:]  # height, width\n        yolo_out, out = [], []\n        if verbose:\n            print(\'0\', x.shape)\n            str = \'\'\n\n        # Augment images (inference and test only)\n        if augment:  # https://github.com/ultralytics/yolov3/issues/931\n            nb = x.shape[0]  # batch size\n            s = [0.83, 0.67]  # scales\n            x = torch.cat((x,\n                           torch_utils.scale_img(x.flip(3), s[0]),  # flip-lr and scale\n                           torch_utils.scale_img(x, s[1]),  # scale\n                           ), 0)\n\n        for i, module in enumerate(self.module_list):\n            name = module.__class__.__name__\n            if name in [\'WeightedFeatureFusion\', \'FeatureConcat\']:  # sum, concat\n                if verbose:\n                    l = [i - 1] + module.layers  # layers\n                    sh = [list(x.shape)] + [list(out[i].shape) for i in module.layers]  # shapes\n                    str = \' >> \' + \' + \'.join([\'layer %g %s\' % x for x in zip(l, sh)])\n                x = module(x, out)  # WeightedFeatureFusion(), FeatureConcat()\n            elif name == \'YOLOLayer\':\n                yolo_out.append(module(x, out))\n            else:  # run module directly, i.e. mtype = \'convolutional\', \'upsample\', \'maxpool\', \'batchnorm2d\' etc.\n                x = module(x)\n\n            out.append(x if self.routs[i] else [])\n            if verbose:\n                print(\'%g/%g %s -\' % (i, len(self.module_list), name), list(x.shape), str)\n                str = \'\'\n\n        if self.training:  # train\n            return yolo_out\n        elif ONNX_EXPORT:  # export\n            x = [torch.cat(x, 0) for x in zip(*yolo_out)]\n            return x[0], torch.cat(x[1:3], 1)  # scores, boxes: 3780x80, 3780x4\n        else:  # inference or test\n            x, p = zip(*yolo_out)  # inference output, training output\n            x = torch.cat(x, 1)  # cat yolo outputs\n            if augment:  # de-augment results\n                x = torch.split(x, nb, dim=0)\n                x[1][..., :4] /= s[0]  # scale\n                x[1][..., 0] = img_size[1] - x[1][..., 0]  # flip lr\n                x[2][..., :4] /= s[1]  # scale\n                x = torch.cat(x, 1)\n            return x, p\n\n    def fuse(self):\n        # Fuse Conv2d + BatchNorm2d layers throughout model\n        print(\'Fusing layers...\')\n        fused_list = nn.ModuleList()\n        for a in list(self.children())[0]:\n            if isinstance(a, nn.Sequential):\n                for i, b in enumerate(a):\n                    if isinstance(b, nn.modules.batchnorm.BatchNorm2d):\n                        # fuse this bn layer with the previous conv2d layer\n                        conv = a[i - 1]\n                        fused = torch_utils.fuse_conv_and_bn(conv, b)\n                        a = nn.Sequential(fused, *list(a.children())[i + 1:])\n                        break\n            fused_list.append(a)\n        self.module_list = fused_list\n        self.info() if not ONNX_EXPORT else None  # yolov3-spp reduced from 225 to 152 layers\n\n    def info(self, verbose=False):\n        torch_utils.model_info(self, verbose)\n\n\ndef get_yolo_layers(model):\n    return [i for i, m in enumerate(model.module_list) if m.__class__.__name__ == \'YOLOLayer\']  # [89, 101, 113]\n\n\ndef load_darknet_weights(self, weights, cutoff=-1):\n    # Parses and loads the weights stored in \'weights\'\n\n    # Establish cutoffs (load layers between 0 and cutoff. if cutoff = -1 all are loaded)\n    file = Path(weights).name\n    if file == \'darknet53.conv.74\':\n        cutoff = 75\n    elif file == \'yolov3-tiny.conv.15\':\n        cutoff = 15\n\n    # Read weights file\n    with open(weights, \'rb\') as f:\n        # Read Header https://github.com/AlexeyAB/darknet/issues/2914#issuecomment-496675346\n        self.version = np.fromfile(f, dtype=np.int32, count=3)  # (int32) version info: major, minor, revision\n        self.seen = np.fromfile(f, dtype=np.int64, count=1)  # (int64) number of images seen during training\n\n        weights = np.fromfile(f, dtype=np.float32)  # the rest are weights\n\n    ptr = 0\n    for i, (mdef, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n        if mdef[\'type\'] == \'convolutional\':\n            conv = module[0]\n            if mdef[\'batch_normalize\']:\n                # Load BN bias, weights, running mean and running variance\n                bn = module[1]\n                nb = bn.bias.numel()  # number of biases\n                # Bias\n                bn.bias.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.bias))\n                ptr += nb\n                # Weight\n                bn.weight.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.weight))\n                ptr += nb\n                # Running Mean\n                bn.running_mean.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.running_mean))\n                ptr += nb\n                # Running Var\n                bn.running_var.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.running_var))\n                ptr += nb\n            else:\n                # Load conv. bias\n                nb = conv.bias.numel()\n                conv_b = torch.from_numpy(weights[ptr:ptr + nb]).view_as(conv.bias)\n                conv.bias.data.copy_(conv_b)\n                ptr += nb\n            # Load conv. weights\n            nw = conv.weight.numel()  # number of weights\n            conv.weight.data.copy_(torch.from_numpy(weights[ptr:ptr + nw]).view_as(conv.weight))\n            ptr += nw\n\n\ndef save_weights(self, path=\'model.weights\', cutoff=-1):\n    # Converts a PyTorch model to Darket format (*.pt to *.weights)\n    # Note: Does not work if model.fuse() is applied\n    with open(path, \'wb\') as f:\n        # Write Header https://github.com/AlexeyAB/darknet/issues/2914#issuecomment-496675346\n        self.version.tofile(f)  # (int32) version info: major, minor, revision\n        self.seen.tofile(f)  # (int64) number of images seen during training\n\n        # Iterate through layers\n        for i, (mdef, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n            if mdef[\'type\'] == \'convolutional\':\n                conv_layer = module[0]\n                # If batch norm, load bn first\n                if mdef[\'batch_normalize\']:\n                    bn_layer = module[1]\n                    bn_layer.bias.data.cpu().numpy().tofile(f)\n                    bn_layer.weight.data.cpu().numpy().tofile(f)\n                    bn_layer.running_mean.data.cpu().numpy().tofile(f)\n                    bn_layer.running_var.data.cpu().numpy().tofile(f)\n                # Load conv bias\n                else:\n                    conv_layer.bias.data.cpu().numpy().tofile(f)\n                # Load conv weights\n                conv_layer.weight.data.cpu().numpy().tofile(f)\n\n\ndef convert(cfg=\'cfg/yolov3-spp.cfg\', weights=\'weights/yolov3-spp.weights\'):\n    # Converts between PyTorch and Darknet format per extension (i.e. *.weights convert to *.pt and vice versa)\n    # from models import *; convert(\'cfg/yolov3-spp.cfg\', \'weights/yolov3-spp.weights\')\n\n    # Initialize model\n    model = Darknet(cfg)\n\n    # Load weights and save\n    if weights.endswith(\'.pt\'):  # if PyTorch format\n        model.load_state_dict(torch.load(weights, map_location=\'cpu\')[\'model\'])\n        target = weights.rsplit(\'.\', 1)[0] + \'.weights\'\n        save_weights(model, path=target, cutoff=-1)\n        print(""Success: converted \'%s\' to \'%s\'"" % (weights, target))\n\n    elif weights.endswith(\'.weights\'):  # darknet format\n        _ = load_darknet_weights(model, weights)\n\n        chkpt = {\'epoch\': -1,\n                 \'best_fitness\': None,\n                 \'training_results\': None,\n                 \'model\': model.state_dict(),\n                 \'optimizer\': None}\n\n        target = weights.rsplit(\'.\', 1)[0] + \'.pt\'\n        torch.save(chkpt, target)\n        print(""Success: converted \'%s\' to \'s%\'"" % (weights, target))\n\n    else:\n        print(\'Error: extension not supported.\')\n\n\ndef attempt_download(weights):\n    # Attempt to download pretrained weights if not found locally\n    weights = weights.strip()\n    msg = weights + \' missing, try downloading from https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0\'\n\n    if len(weights) > 0 and not os.path.isfile(weights):\n        d = {\'yolov3-spp.weights\': \'16lYS4bcIdM2HdmyJBVDOvt3Trx6N3W2R\',\n             \'yolov3.weights\': \'1uTlyDWlnaqXcsKOktP5aH_zRDbfcDp-y\',\n             \'yolov3-tiny.weights\': \'1CCF-iNIIkYesIDzaPvdwlcf7H9zSsKZQ\',\n             \'yolov3-spp.pt\': \'1f6Ovy3BSq2wYq4UfvFUpxJFNDFfrIDcR\',\n             \'yolov3.pt\': \'1SHNFyoe5Ni8DajDNEqgB2oVKBb_NoEad\',\n             \'yolov3-tiny.pt\': \'10m_3MlpQwRtZetQxtksm9jqHrPTHZ6vo\',\n             \'darknet53.conv.74\': \'1WUVBid-XuoUBmvzBVUCBl_ELrzqwA8dJ\',\n             \'yolov3-tiny.conv.15\': \'1Bw0kCpplxUqyRYAJr9RY9SGnOJbo9nEj\',\n             \'yolov3-spp-ultralytics.pt\': \'1UcR-zVoMs7DH5dj3N1bswkiQTA4dmKF4\'}\n\n        file = Path(weights).name\n        if file in d:\n            r = gdrive_download(id=d[file], name=weights)\n        else:  # download from pjreddie.com\n            url = \'https://pjreddie.com/media/files/\' + file\n            print(\'Downloading \' + url)\n            r = os.system(\'curl -f \' + url + \' -o \' + weights)\n\n        # Error check\n        if not (r == 0 and os.path.exists(weights) and os.path.getsize(weights) > 1E6):  # weights exist and > 1MB\n            os.system(\'rm \' + weights)  # remove partial downloads\n            raise Exception(msg)\n'"
test.py,12,"b'import argparse\nimport json\n\nfrom torch.utils.data import DataLoader\n\nfrom models import *\nfrom utils.datasets import *\nfrom utils.utils import *\n\n\ndef test(cfg,\n         data,\n         weights=None,\n         batch_size=16,\n         imgsz=416,\n         conf_thres=0.001,\n         iou_thres=0.6,  # for nms\n         save_json=False,\n         single_cls=False,\n         augment=False,\n         model=None,\n         dataloader=None,\n         multi_label=True):\n    # Initialize/load model and set device\n    if model is None:\n        device = torch_utils.select_device(opt.device, batch_size=batch_size)\n        verbose = opt.task == \'test\'\n\n        # Remove previous\n        for f in glob.glob(\'test_batch*.jpg\'):\n            os.remove(f)\n\n        # Initialize model\n        model = Darknet(cfg, imgsz)\n\n        # Load weights\n        attempt_download(weights)\n        if weights.endswith(\'.pt\'):  # pytorch format\n            model.load_state_dict(torch.load(weights, map_location=device)[\'model\'])\n        else:  # darknet format\n            load_darknet_weights(model, weights)\n\n        # Fuse\n        model.fuse()\n        model.to(device)\n\n        if device.type != \'cpu\' and torch.cuda.device_count() > 1:\n            model = nn.DataParallel(model)\n    else:  # called by train.py\n        device = next(model.parameters()).device  # get model device\n        verbose = False\n\n    # Configure run\n    data = parse_data_cfg(data)\n    nc = 1 if single_cls else int(data[\'classes\'])  # number of classes\n    path = data[\'valid\']  # path to test images\n    names = load_classes(data[\'names\'])  # class names\n    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n    iouv = iouv[0].view(1)  # comment for mAP@0.5:0.95\n    niou = iouv.numel()\n\n    # Dataloader\n    if dataloader is None:\n        dataset = LoadImagesAndLabels(path, imgsz, batch_size, rect=True, single_cls=opt.single_cls)\n        batch_size = min(batch_size, len(dataset))\n        dataloader = DataLoader(dataset,\n                                batch_size=batch_size,\n                                num_workers=min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8]),\n                                pin_memory=True,\n                                collate_fn=dataset.collate_fn)\n\n    seen = 0\n    model.eval()\n    _ = model(torch.zeros((1, 3, imgsz, imgsz), device=device)) if device.type != \'cpu\' else None  # run once\n    coco91class = coco80_to_coco91_class()\n    s = (\'%20s\' + \'%10s\' * 6) % (\'Class\', \'Images\', \'Targets\', \'P\', \'R\', \'mAP@0.5\', \'F1\')\n    p, r, f1, mp, mr, map, mf1, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n    loss = torch.zeros(3, device=device)\n    jdict, stats, ap, ap_class = [], [], [], []\n    for batch_i, (imgs, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n        imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n        targets = targets.to(device)\n        nb, _, height, width = imgs.shape  # batch size, channels, height, width\n        whwh = torch.Tensor([width, height, width, height]).to(device)\n\n        # Disable gradients\n        with torch.no_grad():\n            # Run model\n            t = torch_utils.time_synchronized()\n            inf_out, train_out = model(imgs, augment=augment)  # inference and training outputs\n            t0 += torch_utils.time_synchronized() - t\n\n            # Compute loss\n            if hasattr(model, \'hyp\'):  # if model has loss hyperparameters\n                loss += compute_loss(train_out, targets, model)[1][:3]  # GIoU, obj, cls\n\n            # Run NMS\n            t = torch_utils.time_synchronized()\n            output = non_max_suppression(inf_out, conf_thres=conf_thres, iou_thres=iou_thres, multi_label=multi_label)\n            t1 += torch_utils.time_synchronized() - t\n\n        # Statistics per image\n        for si, pred in enumerate(output):\n            labels = targets[targets[:, 0] == si, 1:]\n            nl = len(labels)\n            tcls = labels[:, 0].tolist() if nl else []  # target class\n            seen += 1\n\n            if pred is None:\n                if nl:\n                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n                continue\n\n            # Append to text file\n            # with open(\'test.txt\', \'a\') as file:\n            #    [file.write(\'%11.5g\' * 7 % tuple(x) + \'\\n\') for x in pred]\n\n            # Clip boxes to image bounds\n            clip_coords(pred, (height, width))\n\n            # Append to pycocotools JSON dictionary\n            if save_json:\n                # [{""image_id"": 42, ""category_id"": 18, ""bbox"": [258.15, 41.29, 348.26, 243.78], ""score"": 0.236}, ...\n                image_id = int(Path(paths[si]).stem.split(\'_\')[-1])\n                box = pred[:, :4].clone()  # xyxy\n                scale_coords(imgs[si].shape[1:], box, shapes[si][0], shapes[si][1])  # to original shape\n                box = xyxy2xywh(box)  # xywh\n                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n                for p, b in zip(pred.tolist(), box.tolist()):\n                    jdict.append({\'image_id\': image_id,\n                                  \'category_id\': coco91class[int(p[5])],\n                                  \'bbox\': [round(x, 3) for x in b],\n                                  \'score\': round(p[4], 5)})\n\n            # Assign all predictions as incorrect\n            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n            if nl:\n                detected = []  # target indices\n                tcls_tensor = labels[:, 0]\n\n                # target boxes\n                tbox = xywh2xyxy(labels[:, 1:5]) * whwh\n\n                # Per target class\n                for cls in torch.unique(tcls_tensor):\n                    ti = (cls == tcls_tensor).nonzero().view(-1)  # prediction indices\n                    pi = (cls == pred[:, 5]).nonzero().view(-1)  # target indices\n\n                    # Search for detections\n                    if pi.shape[0]:\n                        # Prediction to target ious\n                        ious, i = box_iou(pred[pi, :4], tbox[ti]).max(1)  # best ious, indices\n\n                        # Append detections\n                        for j in (ious > iouv[0]).nonzero():\n                            d = ti[i[j]]  # detected target\n                            if d not in detected:\n                                detected.append(d)\n                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n                                if len(detected) == nl:  # all targets already located in image\n                                    break\n\n            # Append statistics (correct, conf, pcls, tcls)\n            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n\n        # Plot images\n        if batch_i < 1:\n            f = \'test_batch%g_gt.jpg\' % batch_i  # filename\n            plot_images(imgs, targets, paths=paths, names=names, fname=f)  # ground truth\n            f = \'test_batch%g_pred.jpg\' % batch_i\n            plot_images(imgs, output_to_target(output, width, height), paths=paths, names=names, fname=f)  # predictions\n\n    # Compute statistics\n    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n    if len(stats):\n        p, r, ap, f1, ap_class = ap_per_class(*stats)\n        if niou > 1:\n            p, r, ap, f1 = p[:, 0], r[:, 0], ap.mean(1), ap[:, 0]  # [P, R, AP@0.5:0.95, AP@0.5]\n        mp, mr, map, mf1 = p.mean(), r.mean(), ap.mean(), f1.mean()\n        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n    else:\n        nt = torch.zeros(1)\n\n    # Print results\n    pf = \'%20s\' + \'%10.3g\' * 6  # print format\n    print(pf % (\'all\', seen, nt.sum(), mp, mr, map, mf1))\n\n    # Print results per class\n    if verbose and nc > 1 and len(stats):\n        for i, c in enumerate(ap_class):\n            print(pf % (names[c], seen, nt[c], p[i], r[i], ap[i], f1[i]))\n\n    # Print speeds\n    if verbose or save_json:\n        t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n        print(\'Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g\' % t)\n\n    # Save JSON\n    if save_json and map and len(jdict):\n        print(\'\\nCOCO mAP with pycocotools...\')\n        imgIds = [int(Path(x).stem.split(\'_\')[-1]) for x in dataloader.dataset.img_files]\n        with open(\'results.json\', \'w\') as file:\n            json.dump(jdict, file)\n\n        try:\n            from pycocotools.coco import COCO\n            from pycocotools.cocoeval import COCOeval\n\n            # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n            cocoGt = COCO(glob.glob(\'../coco/annotations/instances_val*.json\')[0])  # initialize COCO ground truth api\n            cocoDt = cocoGt.loadRes(\'results.json\')  # initialize COCO pred api\n\n            cocoEval = COCOeval(cocoGt, cocoDt, \'bbox\')\n            cocoEval.params.imgIds = imgIds  # [:32]  # only evaluate these images\n            cocoEval.evaluate()\n            cocoEval.accumulate()\n            cocoEval.summarize()\n            # mf1, map = cocoEval.stats[:2]  # update to pycocotools results (mAP@0.5:0.95, mAP@0.5)\n        except:\n            print(\'WARNING: pycocotools must be installed with numpy==1.17 to run correctly. \'\n                  \'See https://github.com/cocodataset/cocoapi/issues/356\')\n\n    # Return results\n    maps = np.zeros(nc) + map\n    for i, c in enumerate(ap_class):\n        maps[c] = ap[i]\n    return (mp, mr, map, mf1, *(loss.cpu() / len(dataloader)).tolist()), maps\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'test.py\')\n    parser.add_argument(\'--cfg\', type=str, default=\'cfg/yolov3-spp.cfg\', help=\'*.cfg path\')\n    parser.add_argument(\'--data\', type=str, default=\'data/coco2014.data\', help=\'*.data path\')\n    parser.add_argument(\'--weights\', type=str, default=\'weights/yolov3-spp-ultralytics.pt\', help=\'weights path\')\n    parser.add_argument(\'--batch-size\', type=int, default=16, help=\'size of each image batch\')\n    parser.add_argument(\'--img-size\', type=int, default=512, help=\'inference size (pixels)\')\n    parser.add_argument(\'--conf-thres\', type=float, default=0.001, help=\'object confidence threshold\')\n    parser.add_argument(\'--iou-thres\', type=float, default=0.6, help=\'IOU threshold for NMS\')\n    parser.add_argument(\'--save-json\', action=\'store_true\', help=\'save a cocoapi-compatible JSON results file\')\n    parser.add_argument(\'--task\', default=\'test\', help=""\'test\', \'study\', \'benchmark\'"")\n    parser.add_argument(\'--device\', default=\'\', help=\'device id (i.e. 0 or 0,1) or cpu\')\n    parser.add_argument(\'--single-cls\', action=\'store_true\', help=\'train as single-class dataset\')\n    parser.add_argument(\'--augment\', action=\'store_true\', help=\'augmented inference\')\n    opt = parser.parse_args()\n    opt.save_json = opt.save_json or any([x in opt.data for x in [\'coco.data\', \'coco2014.data\', \'coco2017.data\']])\n    opt.cfg = list(glob.iglob(\'./**/\' + opt.cfg, recursive=True))[0]  # find file\n    opt.data = list(glob.iglob(\'./**/\' + opt.data, recursive=True))[0]  # find file\n    print(opt)\n\n    # task = \'test\', \'study\', \'benchmark\'\n    if opt.task == \'test\':  # (default) test normally\n        test(opt.cfg,\n             opt.data,\n             opt.weights,\n             opt.batch_size,\n             opt.img_size,\n             opt.conf_thres,\n             opt.iou_thres,\n             opt.save_json,\n             opt.single_cls,\n             opt.augment)\n\n    elif opt.task == \'benchmark\':  # mAPs at 256-640 at conf 0.5 and 0.7\n        y = []\n        for i in list(range(256, 640, 128)):  # img-size\n            for j in [0.6, 0.7]:  # iou-thres\n                t = time.time()\n                r = test(opt.cfg, opt.data, opt.weights, opt.batch_size, i, opt.conf_thres, j, opt.save_json)[0]\n                y.append(r + (time.time() - t,))\n        np.savetxt(\'benchmark.txt\', y, fmt=\'%10.4g\')  # y = np.loadtxt(\'study.txt\')\n'"
train.py,18,"b'import argparse\n\nimport torch.distributed as dist\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport test  # import test.py to get mAP after each epoch\nfrom models import *\nfrom utils.datasets import *\nfrom utils.utils import *\n\nmixed_precision = True\ntry:  # Mixed precision training https://github.com/NVIDIA/apex\n    from apex import amp\nexcept:\n    print(\'Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex\')\n    mixed_precision = False  # not installed\n\nwdir = \'weights\' + os.sep  # weights dir\nlast = wdir + \'last.pt\'\nbest = wdir + \'best.pt\'\nresults_file = \'results.txt\'\n\n# Hyperparameters\nhyp = {\'giou\': 3.54,  # giou loss gain\n       \'cls\': 37.4,  # cls loss gain\n       \'cls_pw\': 1.0,  # cls BCELoss positive_weight\n       \'obj\': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n       \'obj_pw\': 1.0,  # obj BCELoss positive_weight\n       \'iou_t\': 0.20,  # iou training threshold\n       \'lr0\': 0.01,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n       \'lrf\': 0.0005,  # final learning rate (with cos scheduler)\n       \'momentum\': 0.937,  # SGD momentum\n       \'weight_decay\': 0.0005,  # optimizer weight decay\n       \'fl_gamma\': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n       \'hsv_h\': 0.0138,  # image HSV-Hue augmentation (fraction)\n       \'hsv_s\': 0.678,  # image HSV-Saturation augmentation (fraction)\n       \'hsv_v\': 0.36,  # image HSV-Value augmentation (fraction)\n       \'degrees\': 1.98 * 0,  # image rotation (+/- deg)\n       \'translate\': 0.05 * 0,  # image translation (+/- fraction)\n       \'scale\': 0.05 * 0,  # image scale (+/- gain)\n       \'shear\': 0.641 * 0}  # image shear (+/- deg)\n\n# Overwrite hyp with hyp*.txt (optional)\nf = glob.glob(\'hyp*.txt\')\nif f:\n    print(\'Using %s\' % f[0])\n    for k, v in zip(hyp.keys(), np.loadtxt(f[0])):\n        hyp[k] = v\n\n# Print focal loss if gamma > 0\nif hyp[\'fl_gamma\']:\n    print(\'Using FocalLoss(gamma=%g)\' % hyp[\'fl_gamma\'])\n\n\ndef train(hyp):\n    cfg = opt.cfg\n    data = opt.data\n    epochs = opt.epochs  # 500200 batches at bs 64, 117263 images = 273 epochs\n    batch_size = opt.batch_size\n    accumulate = max(round(64 / batch_size), 1)  # accumulate n times before optimizer update (bs 64)\n    weights = opt.weights  # initial training weights\n    imgsz_min, imgsz_max, imgsz_test = opt.img_size  # img sizes (min, max, test)\n\n    # Image Sizes\n    gs = 64  # (pixels) grid size\n    assert math.fmod(imgsz_min, gs) == 0, \'--img-size %g must be a %g-multiple\' % (imgsz_min, gs)\n    opt.multi_scale |= imgsz_min != imgsz_max  # multi if different (min, max)\n    if opt.multi_scale:\n        if imgsz_min == imgsz_max:\n            imgsz_min //= 1.5\n            imgsz_max //= 0.667\n        grid_min, grid_max = imgsz_min // gs, imgsz_max // gs\n        imgsz_min, imgsz_max = int(grid_min * gs), int(grid_max * gs)\n    img_size = imgsz_max  # initialize with max size\n\n    # Configure run\n    init_seeds()\n    data_dict = parse_data_cfg(data)\n    train_path = data_dict[\'train\']\n    test_path = data_dict[\'valid\']\n    nc = 1 if opt.single_cls else int(data_dict[\'classes\'])  # number of classes\n    hyp[\'cls\'] *= nc / 80  # update coco-tuned hyp[\'cls\'] to current dataset\n\n    # Remove previous results\n    for f in glob.glob(\'*_batch*.jpg\') + glob.glob(results_file):\n        os.remove(f)\n\n    # Initialize model\n    model = Darknet(cfg).to(device)\n\n    # Optimizer\n    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n    for k, v in dict(model.named_parameters()).items():\n        if \'.bias\' in k:\n            pg2 += [v]  # biases\n        elif \'Conv2d.weight\' in k:\n            pg1 += [v]  # apply weight_decay\n        else:\n            pg0 += [v]  # all else\n\n    if opt.adam:\n        # hyp[\'lr0\'] *= 0.1  # reduce lr (i.e. SGD=5E-3, Adam=5E-4)\n        optimizer = optim.Adam(pg0, lr=hyp[\'lr0\'])\n        # optimizer = AdaBound(pg0, lr=hyp[\'lr0\'], final_lr=0.1)\n    else:\n        optimizer = optim.SGD(pg0, lr=hyp[\'lr0\'], momentum=hyp[\'momentum\'], nesterov=True)\n    optimizer.add_param_group({\'params\': pg1, \'weight_decay\': hyp[\'weight_decay\']})  # add pg1 with weight_decay\n    optimizer.add_param_group({\'params\': pg2})  # add pg2 (biases)\n    print(\'Optimizer groups: %g .bias, %g Conv2d.weight, %g other\' % (len(pg2), len(pg1), len(pg0)))\n    del pg0, pg1, pg2\n\n    start_epoch = 0\n    best_fitness = 0.0\n    attempt_download(weights)\n    if weights.endswith(\'.pt\'):  # pytorch format\n        # possible weights are \'*.pt\', \'yolov3-spp.pt\', \'yolov3-tiny.pt\' etc.\n        chkpt = torch.load(weights, map_location=device)\n\n        # load model\n        try:\n            chkpt[\'model\'] = {k: v for k, v in chkpt[\'model\'].items() if model.state_dict()[k].numel() == v.numel()}\n            model.load_state_dict(chkpt[\'model\'], strict=False)\n        except KeyError as e:\n            s = ""%s is not compatible with %s. Specify --weights \'\' or specify a --cfg compatible with %s. "" \\\n                ""See https://github.com/ultralytics/yolov3/issues/657"" % (opt.weights, opt.cfg, opt.weights)\n            raise KeyError(s) from e\n\n        # load optimizer\n        if chkpt[\'optimizer\'] is not None:\n            optimizer.load_state_dict(chkpt[\'optimizer\'])\n            best_fitness = chkpt[\'best_fitness\']\n\n        # load results\n        if chkpt.get(\'training_results\') is not None:\n            with open(results_file, \'w\') as file:\n                file.write(chkpt[\'training_results\'])  # write results.txt\n\n        start_epoch = chkpt[\'epoch\'] + 1\n        del chkpt\n\n    elif len(weights) > 0:  # darknet format\n        # possible weights are \'*.weights\', \'yolov3-tiny.conv.15\',  \'darknet53.conv.74\' etc.\n        load_darknet_weights(model, weights)\n\n    # Mixed precision training https://github.com/NVIDIA/apex\n    if mixed_precision:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\'O1\', verbosity=0)\n\n    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n    lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.95 + 0.05  # cosine\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n    scheduler.last_epoch = start_epoch - 1  # see link below\n    # https://discuss.pytorch.org/t/a-problem-occured-when-resuming-an-optimizer/28822\n\n    # Plot lr schedule\n    # y = []\n    # for _ in range(epochs):\n    #     scheduler.step()\n    #     y.append(optimizer.param_groups[0][\'lr\'])\n    # plt.plot(y, \'.-\', label=\'LambdaLR\')\n    # plt.xlabel(\'epoch\')\n    # plt.ylabel(\'LR\')\n    # plt.tight_layout()\n    # plt.savefig(\'LR.png\', dpi=300)\n\n    # Initialize distributed training\n    if device.type != \'cpu\' and torch.cuda.device_count() > 1 and torch.distributed.is_available():\n        dist.init_process_group(backend=\'nccl\',  # \'distributed backend\'\n                                init_method=\'tcp://127.0.0.1:9999\',  # distributed training init method\n                                world_size=1,  # number of nodes for distributed training\n                                rank=0)  # distributed training node rank\n        model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)\n        model.yolo_layers = model.module.yolo_layers  # move yolo layer indices to top level\n\n    # Dataset\n    dataset = LoadImagesAndLabels(train_path, img_size, batch_size,\n                                  augment=True,\n                                  hyp=hyp,  # augmentation hyperparameters\n                                  rect=opt.rect,  # rectangular training\n                                  cache_images=opt.cache_images,\n                                  single_cls=opt.single_cls)\n\n    # Dataloader\n    batch_size = min(batch_size, len(dataset))\n    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n    dataloader = torch.utils.data.DataLoader(dataset,\n                                             batch_size=batch_size,\n                                             num_workers=nw,\n                                             shuffle=not opt.rect,  # Shuffle=True unless rectangular training is used\n                                             pin_memory=True,\n                                             collate_fn=dataset.collate_fn)\n\n    # Testloader\n    testloader = torch.utils.data.DataLoader(LoadImagesAndLabels(test_path, imgsz_test, batch_size,\n                                                                 hyp=hyp,\n                                                                 rect=True,\n                                                                 cache_images=opt.cache_images,\n                                                                 single_cls=opt.single_cls),\n                                             batch_size=batch_size,\n                                             num_workers=nw,\n                                             pin_memory=True,\n                                             collate_fn=dataset.collate_fn)\n\n    # Model parameters\n    model.nc = nc  # attach number of classes to model\n    model.hyp = hyp  # attach hyperparameters to model\n    model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)\n    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\n\n    # Model EMA\n    ema = torch_utils.ModelEMA(model)\n\n    # Start training\n    nb = len(dataloader)  # number of batches\n    n_burn = max(3 * nb, 500)  # burn-in iterations, max(3 epochs, 500 iterations)\n    maps = np.zeros(nc)  # mAP per class\n    # torch.autograd.set_detect_anomaly(True)\n    results = (0, 0, 0, 0, 0, 0, 0)  # \'P\', \'R\', \'mAP\', \'F1\', \'val GIoU\', \'val Objectness\', \'val Classification\'\n    t0 = time.time()\n    print(\'Image sizes %g - %g train, %g test\' % (imgsz_min, imgsz_max, imgsz_test))\n    print(\'Using %g dataloader workers\' % nw)\n    print(\'Starting training for %g epochs...\' % epochs)\n    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n        model.train()\n\n        # Update image weights (optional)\n        if dataset.image_weights:\n            w = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights\n            image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)\n            dataset.indices = random.choices(range(dataset.n), weights=image_weights, k=dataset.n)  # rand weighted idx\n\n        mloss = torch.zeros(4).to(device)  # mean losses\n        print((\'\\n\' + \'%10s\' * 8) % (\'Epoch\', \'gpu_mem\', \'GIoU\', \'obj\', \'cls\', \'total\', \'targets\', \'img_size\'))\n        pbar = tqdm(enumerate(dataloader), total=nb)  # progress bar\n        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n            ni = i + nb * epoch  # number integrated batches (since train start)\n            imgs = imgs.to(device).float() / 255.0  # uint8 to float32, 0 - 255 to 0.0 - 1.0\n            targets = targets.to(device)\n\n            # Burn-in\n            if ni <= n_burn:\n                xi = [0, n_burn]  # x interp\n                model.gr = np.interp(ni, xi, [0.0, 1.0])  # giou loss ratio (obj_loss = 1.0 or giou)\n                accumulate = max(1, np.interp(ni, xi, [1, 64 / batch_size]).round())\n                for j, x in enumerate(optimizer.param_groups):\n                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n                    x[\'lr\'] = np.interp(ni, xi, [0.1 if j == 2 else 0.0, x[\'initial_lr\'] * lf(epoch)])\n                    x[\'weight_decay\'] = np.interp(ni, xi, [0.0, hyp[\'weight_decay\'] if j == 1 else 0.0])\n                    if \'momentum\' in x:\n                        x[\'momentum\'] = np.interp(ni, xi, [0.9, hyp[\'momentum\']])\n\n            # Multi-Scale\n            if opt.multi_scale:\n                if ni / accumulate % 1 == 0:  # \xc2\xa0adjust img_size (67% - 150%) every 1 batch\n                    img_size = random.randrange(grid_min, grid_max + 1) * gs\n                sf = img_size / max(imgs.shape[2:])  # scale factor\n                if sf != 1:\n                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to 32-multiple)\n                    imgs = F.interpolate(imgs, size=ns, mode=\'bilinear\', align_corners=False)\n\n            # Forward\n            pred = model(imgs)\n\n            # Loss\n            loss, loss_items = compute_loss(pred, targets, model)\n            if not torch.isfinite(loss):\n                print(\'WARNING: non-finite loss, ending training \', loss_items)\n                return results\n\n            # Backward\n            loss *= batch_size / 64  # scale loss\n            if mixed_precision:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            # Optimize\n            if ni % accumulate == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n                ema.update(model)\n\n            # Print\n            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n            mem = \'%.3gG\' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n            s = (\'%10s\' * 2 + \'%10.3g\' * 6) % (\'%g/%g\' % (epoch, epochs - 1), mem, *mloss, len(targets), img_size)\n            pbar.set_description(s)\n\n            # Plot\n            if ni < 1:\n                f = \'train_batch%g.jpg\' % i  # filename\n                res = plot_images(images=imgs, targets=targets, paths=paths, fname=f)\n                if tb_writer:\n                    tb_writer.add_image(f, res, dataformats=\'HWC\', global_step=epoch)\n                    # tb_writer.add_graph(model, imgs)  # add model to tensorboard\n\n            # end batch ------------------------------------------------------------------------------------------------\n\n        # Update scheduler\n        scheduler.step()\n\n        # Process epoch results\n        ema.update_attr(model)\n        final_epoch = epoch + 1 == epochs\n        if not opt.notest or final_epoch:  # Calculate mAP\n            is_coco = any([x in data for x in [\'coco.data\', \'coco2014.data\', \'coco2017.data\']]) and model.nc == 80\n            results, maps = test.test(cfg,\n                                      data,\n                                      batch_size=batch_size,\n                                      imgsz=imgsz_test,\n                                      model=ema.ema,\n                                      save_json=final_epoch and is_coco,\n                                      single_cls=opt.single_cls,\n                                      dataloader=testloader,\n                                      multi_label=ni > n_burn)\n\n        # Write\n        with open(results_file, \'a\') as f:\n            f.write(s + \'%10.3g\' * 7 % results + \'\\n\')  # P, R, mAP, F1, test_losses=(GIoU, obj, cls)\n        if len(opt.name) and opt.bucket:\n            os.system(\'gsutil cp results.txt gs://%s/results/results%s.txt\' % (opt.bucket, opt.name))\n\n        # Tensorboard\n        if tb_writer:\n            tags = [\'train/giou_loss\', \'train/obj_loss\', \'train/cls_loss\',\n                    \'metrics/precision\', \'metrics/recall\', \'metrics/mAP_0.5\', \'metrics/F1\',\n                    \'val/giou_loss\', \'val/obj_loss\', \'val/cls_loss\']\n            for x, tag in zip(list(mloss[:-1]) + list(results), tags):\n                tb_writer.add_scalar(tag, x, epoch)\n\n        # Update best mAP\n        fi = fitness(np.array(results).reshape(1, -1))  # fitness_i = weighted combination of [P, R, mAP, F1]\n        if fi > best_fitness:\n            best_fitness = fi\n\n        # Save model\n        save = (not opt.nosave) or (final_epoch and not opt.evolve)\n        if save:\n            with open(results_file, \'r\') as f:  # create checkpoint\n                chkpt = {\'epoch\': epoch,\n                         \'best_fitness\': best_fitness,\n                         \'training_results\': f.read(),\n                         \'model\': ema.ema.module.state_dict() if hasattr(model, \'module\') else ema.ema.state_dict(),\n                         \'optimizer\': None if final_epoch else optimizer.state_dict()}\n\n            # Save last, best and delete\n            torch.save(chkpt, last)\n            if (best_fitness == fi) and not final_epoch:\n                torch.save(chkpt, best)\n            del chkpt\n\n        # end epoch ----------------------------------------------------------------------------------------------------\n    # end training\n\n    n = opt.name\n    if len(n):\n        n = \'_\' + n if not n.isnumeric() else n\n        fresults, flast, fbest = \'results%s.txt\' % n, wdir + \'last%s.pt\' % n, wdir + \'best%s.pt\' % n\n        for f1, f2 in zip([wdir + \'last.pt\', wdir + \'best.pt\', \'results.txt\'], [flast, fbest, fresults]):\n            if os.path.exists(f1):\n                os.rename(f1, f2)  # rename\n                ispt = f2.endswith(\'.pt\')  # is *.pt\n                strip_optimizer(f2) if ispt else None  # strip optimizer\n                os.system(\'gsutil cp %s gs://%s/weights\' % (f2, opt.bucket)) if opt.bucket and ispt else None  # upload\n\n    if not opt.evolve:\n        plot_results()  # save as results.png\n    print(\'%g epochs completed in %.3f hours.\\n\' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n    dist.destroy_process_group() if torch.cuda.device_count() > 1 else None\n    torch.cuda.empty_cache()\n    return results\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--epochs\', type=int, default=300)  # 500200 batches at bs 16, 117263 COCO images = 273 epochs\n    parser.add_argument(\'--batch-size\', type=int, default=16)  # effective bs = batch_size * accumulate = 16 * 4 = 64\n    parser.add_argument(\'--cfg\', type=str, default=\'cfg/yolov3-spp.cfg\', help=\'*.cfg path\')\n    parser.add_argument(\'--data\', type=str, default=\'data/coco2017.data\', help=\'*.data path\')\n    parser.add_argument(\'--multi-scale\', action=\'store_true\', help=\'adjust (67%% - 150%%) img_size every 10 batches\')\n    parser.add_argument(\'--img-size\', nargs=\'+\', type=int, default=[320, 640], help=\'[min_train, max-train, test]\')\n    parser.add_argument(\'--rect\', action=\'store_true\', help=\'rectangular training\')\n    parser.add_argument(\'--resume\', action=\'store_true\', help=\'resume training from last.pt\')\n    parser.add_argument(\'--nosave\', action=\'store_true\', help=\'only save final checkpoint\')\n    parser.add_argument(\'--notest\', action=\'store_true\', help=\'only test final epoch\')\n    parser.add_argument(\'--evolve\', action=\'store_true\', help=\'evolve hyperparameters\')\n    parser.add_argument(\'--bucket\', type=str, default=\'\', help=\'gsutil bucket\')\n    parser.add_argument(\'--cache-images\', action=\'store_true\', help=\'cache images for faster training\')\n    parser.add_argument(\'--weights\', type=str, default=\'weights/yolov3-spp-ultralytics.pt\', help=\'initial weights path\')\n    parser.add_argument(\'--name\', default=\'\', help=\'renames results.txt to results_name.txt if supplied\')\n    parser.add_argument(\'--device\', default=\'\', help=\'device id (i.e. 0 or 0,1 or cpu)\')\n    parser.add_argument(\'--adam\', action=\'store_true\', help=\'use adam optimizer\')\n    parser.add_argument(\'--single-cls\', action=\'store_true\', help=\'train as single-class dataset\')\n    opt = parser.parse_args()\n    opt.weights = last if opt.resume else opt.weights\n    check_git_status()\n    opt.cfg = list(glob.iglob(\'./**/\' + opt.cfg, recursive=True))[0]  # find file\n    # opt.data = list(glob.iglob(\'./**/\' + opt.data, recursive=True))[0]  # find file\n    print(opt)\n    opt.img_size.extend([opt.img_size[-1]] * (3 - len(opt.img_size)))  # extend to 3 sizes (min, max, test)\n    device = torch_utils.select_device(opt.device, apex=mixed_precision, batch_size=opt.batch_size)\n    if device.type == \'cpu\':\n        mixed_precision = False\n\n    # scale hyp[\'obj\'] by img_size (evolved at 320)\n    # hyp[\'obj\'] *= opt.img_size[0] / 320.\n\n    tb_writer = None\n    if not opt.evolve:  # Train normally\n        print(\'Start Tensorboard with ""tensorboard --logdir=runs"", view at http://localhost:6006/\')\n        tb_writer = SummaryWriter(comment=opt.name)\n        train(hyp)  # train normally\n\n    else:  # Evolve hyperparameters (optional)\n        opt.notest, opt.nosave = True, True  # only test/save final epoch\n        if opt.bucket:\n            os.system(\'gsutil cp gs://%s/evolve.txt .\' % opt.bucket)  # download evolve.txt if exists\n\n        for _ in range(1):  # generations to evolve\n            if os.path.exists(\'evolve.txt\'):  # if evolve.txt exists: select best hyps and mutate\n                # Select parent(s)\n                parent = \'single\'  # parent selection method: \'single\' or \'weighted\'\n                x = np.loadtxt(\'evolve.txt\', ndmin=2)\n                n = min(5, len(x))  # number of previous results to consider\n                x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n                w = fitness(x) - fitness(x).min()  # weights\n                if parent == \'single\' or len(x) == 1:\n                    # x = x[random.randint(0, n - 1)]  # random selection\n                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n                elif parent == \'weighted\':\n                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n\n                # Mutate\n                method, mp, s = 3, 0.9, 0.2  # method, mutation probability, sigma\n                npr = np.random\n                npr.seed(int(time.time()))\n                g = np.array([1, 1, 1, 1, 1, 1, 1, 0, .1, 1, 0, 1, 1, 1, 1, 1, 1, 1])  # gains\n                ng = len(g)\n                if method == 1:\n                    v = (npr.randn(ng) * npr.random() * g * s + 1) ** 2.0\n                elif method == 2:\n                    v = (npr.randn(ng) * npr.random(ng) * g * s + 1) ** 2.0\n                elif method == 3:\n                    v = np.ones(ng)\n                    while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n                        # v = (g * (npr.random(ng) < mp) * npr.randn(ng) * s + 1) ** 2.0\n                        v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n                    hyp[k] = x[i + 7] * v[i]  # mutate\n\n            # Clip to limits\n            keys = [\'lr0\', \'iou_t\', \'momentum\', \'weight_decay\', \'hsv_s\', \'hsv_v\', \'translate\', \'scale\', \'fl_gamma\']\n            limits = [(1e-5, 1e-2), (0.00, 0.70), (0.60, 0.98), (0, 0.001), (0, .9), (0, .9), (0, .9), (0, .9), (0, 3)]\n            for k, v in zip(keys, limits):\n                hyp[k] = np.clip(hyp[k], v[0], v[1])\n\n            # Train mutation\n            results = train(hyp.copy())\n\n            # Write mutation results\n            print_mutation(hyp, results, opt.bucket)\n\n            # Plot results\n            # plot_evolution_results(hyp)\n'"
utils/__init__.py,0,b''
utils/adabound.py,12,"b'import math\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AdaBound(Optimizer):\n    """"""Implements AdaBound algorithm.\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https://openreview.net/forum?id=Bkg3g2R9FX\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n                 eps=1e-8, weight_decay=0, amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        if not 0.0 <= final_lr:\n            raise ValueError(""Invalid final learning rate: {}"".format(final_lr))\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(""Invalid gamma parameter: {}"".format(gamma))\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n                        weight_decay=weight_decay, amsbound=amsbound)\n        super(AdaBound, self).__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group[\'lr\'], self.param_groups))\n\n    def __setstate__(self, state):\n        super(AdaBound, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsbound\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsbound = group[\'amsbound\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsbound:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\n                final_lr = group[\'final_lr\'] * group[\'lr\'] / base_lr\n                lower_bound = final_lr * (1 - 1 / (group[\'gamma\'] * state[\'step\'] + 1))\n                upper_bound = final_lr * (1 + 1 / (group[\'gamma\'] * state[\'step\']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\n\n                p.data.add_(-step_size)\n\n        return loss\n\n\nclass AdaBoundW(Optimizer):\n    """"""Implements AdaBound algorithm with Decoupled Weight Decay (arxiv.org/abs/1711.05101)\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https://openreview.net/forum?id=Bkg3g2R9FX\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n                 eps=1e-8, weight_decay=0, amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        if not 0.0 <= final_lr:\n            raise ValueError(""Invalid final learning rate: {}"".format(final_lr))\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(""Invalid gamma parameter: {}"".format(gamma))\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n                        weight_decay=weight_decay, amsbound=amsbound)\n        super(AdaBoundW, self).__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group[\'lr\'], self.param_groups))\n\n    def __setstate__(self, state):\n        super(AdaBoundW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsbound\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsbound = group[\'amsbound\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsbound:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\n                final_lr = group[\'final_lr\'] * group[\'lr\'] / base_lr\n                lower_bound = final_lr * (1 - 1 / (group[\'gamma\'] * state[\'step\'] + 1))\n                upper_bound = final_lr * (1 + 1 / (group[\'gamma\'] * state[\'step\']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\n\n                if group[\'weight_decay\'] != 0:\n                    decayed_weights = torch.mul(p.data, group[\'weight_decay\'])\n                    p.data.add_(-step_size)\n                    p.data.sub_(decayed_weights)\n                else:\n                    p.data.add_(-step_size)\n\n        return loss\n'"
utils/datasets.py,5,"b'import glob\nimport math\nimport os\nimport random\nimport shutil\nimport time\nfrom pathlib import Path\nfrom threading import Thread\n\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image, ExifTags\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\nfrom utils.utils import xyxy2xywh, xywh2xyxy\n\nhelp_url = \'https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data\'\nimg_formats = [\'.bmp\', \'.jpg\', \'.jpeg\', \'.png\', \'.tif\', \'.dng\']\nvid_formats = [\'.mov\', \'.avi\', \'.mp4\']\n\n# Get orientation exif tag\nfor orientation in ExifTags.TAGS.keys():\n    if ExifTags.TAGS[orientation] == \'Orientation\':\n        break\n\n\ndef exif_size(img):\n    # Returns exif-corrected PIL size\n    s = img.size  # (width, height)\n    try:\n        rotation = dict(img._getexif().items())[orientation]\n        if rotation == 6:  # rotation 270\n            s = (s[1], s[0])\n        elif rotation == 8:  # rotation 90\n            s = (s[1], s[0])\n    except:\n        pass\n\n    return s\n\n\nclass LoadImages:  # for inference\n    def __init__(self, path, img_size=416):\n        path = str(Path(path))  # os-agnostic\n        files = []\n        if os.path.isdir(path):\n            files = sorted(glob.glob(os.path.join(path, \'*.*\')))\n        elif os.path.isfile(path):\n            files = [path]\n\n        images = [x for x in files if os.path.splitext(x)[-1].lower() in img_formats]\n        videos = [x for x in files if os.path.splitext(x)[-1].lower() in vid_formats]\n        nI, nV = len(images), len(videos)\n\n        self.img_size = img_size\n        self.files = images + videos\n        self.nF = nI + nV  # number of files\n        self.video_flag = [False] * nI + [True] * nV\n        self.mode = \'images\'\n        if any(videos):\n            self.new_video(videos[0])  # new video\n        else:\n            self.cap = None\n        assert self.nF > 0, \'No images or videos found in \' + path\n\n    def __iter__(self):\n        self.count = 0\n        return self\n\n    def __next__(self):\n        if self.count == self.nF:\n            raise StopIteration\n        path = self.files[self.count]\n\n        if self.video_flag[self.count]:\n            # Read video\n            self.mode = \'video\'\n            ret_val, img0 = self.cap.read()\n            if not ret_val:\n                self.count += 1\n                self.cap.release()\n                if self.count == self.nF:  # last video\n                    raise StopIteration\n                else:\n                    path = self.files[self.count]\n                    self.new_video(path)\n                    ret_val, img0 = self.cap.read()\n\n            self.frame += 1\n            print(\'video %g/%g (%g/%g) %s: \' % (self.count + 1, self.nF, self.frame, self.nframes, path), end=\'\')\n\n        else:\n            # Read image\n            self.count += 1\n            img0 = cv2.imread(path)  # BGR\n            assert img0 is not None, \'Image Not Found \' + path\n            print(\'image %g/%g %s: \' % (self.count, self.nF, path), end=\'\')\n\n        # Padded resize\n        img = letterbox(img0, new_shape=self.img_size)[0]\n\n        # Convert\n        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n        img = np.ascontiguousarray(img)\n\n        # cv2.imwrite(path + \'.letterbox.jpg\', 255 * img.transpose((1, 2, 0))[:, :, ::-1])  # save letterbox image\n        return path, img, img0, self.cap\n\n    def new_video(self, path):\n        self.frame = 0\n        self.cap = cv2.VideoCapture(path)\n        self.nframes = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    def __len__(self):\n        return self.nF  # number of files\n\n\nclass LoadWebcam:  # for inference\n    def __init__(self, pipe=0, img_size=416):\n        self.img_size = img_size\n\n        if pipe == \'0\':\n            pipe = 0  # local camera\n        # pipe = \'rtsp://192.168.1.64/1\'  # IP camera\n        # pipe = \'rtsp://username:password@192.168.1.64/1\'  # IP camera with login\n        # pipe = \'rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa\'  # IP traffic camera\n        # pipe = \'http://wmccpinetop.axiscam.net/mjpg/video.mjpg\'  # IP golf camera\n\n        # https://answers.opencv.org/question/215996/changing-gstreamer-pipeline-to-opencv-in-pythonsolved/\n        # pipe = \'""rtspsrc location=""rtsp://username:password@192.168.1.64/1"" latency=10 ! appsink\'  # GStreamer\n\n        # https://answers.opencv.org/question/200787/video-acceleration-gstremer-pipeline-in-videocapture/\n        # https://stackoverflow.com/questions/54095699/install-gstreamer-support-for-opencv-python-package  # install help\n        # pipe = ""rtspsrc location=rtsp://root:root@192.168.0.91:554/axis-media/media.amp?videocodec=h264&resolution=3840x2160 protocols=GST_RTSP_LOWER_TRANS_TCP ! rtph264depay ! queue ! vaapih264dec ! videoconvert ! appsink""  # GStreamer\n\n        self.pipe = pipe\n        self.cap = cv2.VideoCapture(pipe)  # video capture object\n        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)  # set buffer size\n\n    def __iter__(self):\n        self.count = -1\n        return self\n\n    def __next__(self):\n        self.count += 1\n        if cv2.waitKey(1) == ord(\'q\'):  # q to quit\n            self.cap.release()\n            cv2.destroyAllWindows()\n            raise StopIteration\n\n        # Read frame\n        if self.pipe == 0:  # local camera\n            ret_val, img0 = self.cap.read()\n            img0 = cv2.flip(img0, 1)  # flip left-right\n        else:  # IP camera\n            n = 0\n            while True:\n                n += 1\n                self.cap.grab()\n                if n % 30 == 0:  # skip frames\n                    ret_val, img0 = self.cap.retrieve()\n                    if ret_val:\n                        break\n\n        # Print\n        assert ret_val, \'Camera Error %s\' % self.pipe\n        img_path = \'webcam.jpg\'\n        print(\'webcam %g: \' % self.count, end=\'\')\n\n        # Padded resize\n        img = letterbox(img0, new_shape=self.img_size)[0]\n\n        # Convert\n        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n        img = np.ascontiguousarray(img)\n\n        return img_path, img, img0, None\n\n    def __len__(self):\n        return 0\n\n\nclass LoadStreams:  # multiple IP or RTSP cameras\n    def __init__(self, sources=\'streams.txt\', img_size=416):\n        self.mode = \'images\'\n        self.img_size = img_size\n\n        if os.path.isfile(sources):\n            with open(sources, \'r\') as f:\n                sources = [x.strip() for x in f.read().splitlines() if len(x.strip())]\n        else:\n            sources = [sources]\n\n        n = len(sources)\n        self.imgs = [None] * n\n        self.sources = sources\n        for i, s in enumerate(sources):\n            # Start the thread to read frames from the video stream\n            print(\'%g/%g: %s... \' % (i + 1, n, s), end=\'\')\n            cap = cv2.VideoCapture(0 if s == \'0\' else s)\n            assert cap.isOpened(), \'Failed to open %s\' % s\n            w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n            h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n            fps = cap.get(cv2.CAP_PROP_FPS) % 100\n            _, self.imgs[i] = cap.read()  # guarantee first frame\n            thread = Thread(target=self.update, args=([i, cap]), daemon=True)\n            print(\' success (%gx%g at %.2f FPS).\' % (w, h, fps))\n            thread.start()\n        print(\'\')  # newline\n\n        # check for common shapes\n        s = np.stack([letterbox(x, new_shape=self.img_size)[0].shape for x in self.imgs], 0)  # inference shapes\n        self.rect = np.unique(s, axis=0).shape[0] == 1  # rect inference if all shapes equal\n        if not self.rect:\n            print(\'WARNING: Different stream shapes detected. For optimal performance supply similarly-shaped streams.\')\n\n    def update(self, index, cap):\n        # Read next stream frame in a daemon thread\n        n = 0\n        while cap.isOpened():\n            n += 1\n            # _, self.imgs[index] = cap.read()\n            cap.grab()\n            if n == 4:  # read every 4th frame\n                _, self.imgs[index] = cap.retrieve()\n                n = 0\n            time.sleep(0.01)  # wait time\n\n    def __iter__(self):\n        self.count = -1\n        return self\n\n    def __next__(self):\n        self.count += 1\n        img0 = self.imgs.copy()\n        if cv2.waitKey(1) == ord(\'q\'):  # q to quit\n            cv2.destroyAllWindows()\n            raise StopIteration\n\n        # Letterbox\n        img = [letterbox(x, new_shape=self.img_size, auto=self.rect)[0] for x in img0]\n\n        # Stack\n        img = np.stack(img, 0)\n\n        # Convert\n        img = img[:, :, :, ::-1].transpose(0, 3, 1, 2)  # BGR to RGB, to bsx3x416x416\n        img = np.ascontiguousarray(img)\n\n        return self.sources, img, img0, None\n\n    def __len__(self):\n        return 0  # 1E12 frames = 32 streams at 30 FPS for 30 years\n\n\nclass LoadImagesAndLabels(Dataset):  # for training/testing\n    def __init__(self, path, img_size=416, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,\n                 cache_images=False, single_cls=False):\n        try:\n            path = str(Path(path))  # os-agnostic\n            parent = str(Path(path).parent) + os.sep\n            if os.path.isfile(path):  # file\n                with open(path, \'r\') as f:\n                    f = f.read().splitlines()\n                    f = [x.replace(\'./\', parent) if x.startswith(\'./\') else x for x in f]  # local to global path\n            elif os.path.isdir(path):  # folder\n                f = glob.iglob(path + os.sep + \'*.*\')\n            else:\n                raise Exception(\'%s does not exist\' % path)\n            self.img_files = [x.replace(\'/\', os.sep) for x in f if os.path.splitext(x)[-1].lower() in img_formats]\n        except:\n            raise Exception(\'Error loading data from %s. See %s\' % (path, help_url))\n\n        n = len(self.img_files)\n        assert n > 0, \'No images found in %s. See %s\' % (path, help_url)\n        bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # batch index\n        nb = bi[-1] + 1  # number of batches\n\n        self.n = n  # number of images\n        self.batch = bi  # batch index of image\n        self.img_size = img_size\n        self.augment = augment\n        self.hyp = hyp\n        self.image_weights = image_weights\n        self.rect = False if image_weights else rect\n        self.mosaic = self.augment and not self.rect  # load 4 images at a time into a mosaic (only during training)\n\n        # Define labels\n        self.label_files = [x.replace(\'images\', \'labels\').replace(os.path.splitext(x)[-1], \'.txt\')\n                            for x in self.img_files]\n\n        # Rectangular Training  https://github.com/ultralytics/yolov3/issues/232\n        if self.rect:\n            # Read image shapes (wh)\n            sp = path.replace(\'.txt\', \'\') + \'.shapes\'  # shapefile path\n            try:\n                with open(sp, \'r\') as f:  # read existing shapefile\n                    s = [x.split() for x in f.read().splitlines()]\n                    assert len(s) == n, \'Shapefile out of sync\'\n            except:\n                s = [exif_size(Image.open(f)) for f in tqdm(self.img_files, desc=\'Reading image shapes\')]\n                np.savetxt(sp, s, fmt=\'%g\')  # overwrites existing (if any)\n\n            # Sort by aspect ratio\n            s = np.array(s, dtype=np.float64)\n            ar = s[:, 1] / s[:, 0]  # aspect ratio\n            irect = ar.argsort()\n            self.img_files = [self.img_files[i] for i in irect]\n            self.label_files = [self.label_files[i] for i in irect]\n            self.shapes = s[irect]  # wh\n            ar = ar[irect]\n\n            # Set training image shapes\n            shapes = [[1, 1]] * nb\n            for i in range(nb):\n                ari = ar[bi == i]\n                mini, maxi = ari.min(), ari.max()\n                if maxi < 1:\n                    shapes[i] = [maxi, 1]\n                elif mini > 1:\n                    shapes[i] = [1, 1 / mini]\n\n            self.batch_shapes = np.ceil(np.array(shapes) * img_size / 64.).astype(np.int) * 64\n\n        # Cache labels\n        self.imgs = [None] * n\n        self.labels = [np.zeros((0, 5), dtype=np.float32)] * n\n        create_datasubset, extract_bounding_boxes, labels_loaded = False, False, False\n        nm, nf, ne, ns, nd = 0, 0, 0, 0, 0  # number missing, found, empty, datasubset, duplicate\n        np_labels_path = str(Path(self.label_files[0]).parent) + \'.npy\'  # saved labels in *.npy file\n        if os.path.isfile(np_labels_path):\n            s = np_labels_path  # print string\n            x = np.load(np_labels_path, allow_pickle=True)\n            if len(x) == n:\n                self.labels = x\n                labels_loaded = True\n        else:\n            s = path.replace(\'images\', \'labels\')\n\n        pbar = tqdm(self.label_files)\n        for i, file in enumerate(pbar):\n            if labels_loaded:\n                l = self.labels[i]\n                # np.savetxt(file, l, \'%g\')  # save *.txt from *.npy file\n            else:\n                try:\n                    with open(file, \'r\') as f:\n                        l = np.array([x.split() for x in f.read().splitlines()], dtype=np.float32)\n                except:\n                    nm += 1  # print(\'missing labels for image %s\' % self.img_files[i])  # file missing\n                    continue\n\n            if l.shape[0]:\n                assert l.shape[1] == 5, \'> 5 label columns: %s\' % file\n                assert (l >= 0).all(), \'negative labels: %s\' % file\n                assert (l[:, 1:] <= 1).all(), \'non-normalized or out of bounds coordinate labels: %s\' % file\n                if np.unique(l, axis=0).shape[0] < l.shape[0]:  # duplicate rows\n                    nd += 1  # print(\'WARNING: duplicate rows in %s\' % self.label_files[i])  # duplicate rows\n                if single_cls:\n                    l[:, 0] = 0  # force dataset into single-class mode\n                self.labels[i] = l\n                nf += 1  # file found\n\n                # Create subdataset (a smaller dataset)\n                if create_datasubset and ns < 1E4:\n                    if ns == 0:\n                        create_folder(path=\'./datasubset\')\n                        os.makedirs(\'./datasubset/images\')\n                    exclude_classes = 43\n                    if exclude_classes not in l[:, 0]:\n                        ns += 1\n                        # shutil.copy(src=self.img_files[i], dst=\'./datasubset/images/\')  # copy image\n                        with open(\'./datasubset/images.txt\', \'a\') as f:\n                            f.write(self.img_files[i] + \'\\n\')\n\n                # Extract object detection boxes for a second stage classifier\n                if extract_bounding_boxes:\n                    p = Path(self.img_files[i])\n                    img = cv2.imread(str(p))\n                    h, w = img.shape[:2]\n                    for j, x in enumerate(l):\n                        f = \'%s%sclassifier%s%g_%g_%s\' % (p.parent.parent, os.sep, os.sep, x[0], j, p.name)\n                        if not os.path.exists(Path(f).parent):\n                            os.makedirs(Path(f).parent)  # make new output folder\n\n                        b = x[1:] * [w, h, w, h]  # box\n                        b[2:] = b[2:].max()  # rectangle to square\n                        b[2:] = b[2:] * 1.3 + 30  # pad\n                        b = xywh2xyxy(b.reshape(-1, 4)).ravel().astype(np.int)\n\n                        b[[0, 2]] = np.clip(b[[0, 2]], 0, w)  # clip boxes outside of image\n                        b[[1, 3]] = np.clip(b[[1, 3]], 0, h)\n                        assert cv2.imwrite(f, img[b[1]:b[3], b[0]:b[2]]), \'Failure extracting classifier boxes\'\n            else:\n                ne += 1  # print(\'empty labels for image %s\' % self.img_files[i])  # file empty\n                # os.system(""rm \'%s\' \'%s\'"" % (self.img_files[i], self.label_files[i]))  # remove\n\n            pbar.desc = \'Caching labels %s (%g found, %g missing, %g empty, %g duplicate, for %g images)\' % (\n                s, nf, nm, ne, nd, n)\n        assert nf > 0 or n == 20288, \'No labels found in %s. See %s\' % (os.path.dirname(file) + os.sep, help_url)\n        if not labels_loaded and n > 1000:\n            print(\'Saving labels to %s for faster future loading\' % np_labels_path)\n            np.save(np_labels_path, self.labels)  # save for next time\n\n        # Cache images into memory for faster training (WARNING: large datasets may exceed system RAM)\n        if cache_images:  # if training\n            gb = 0  # Gigabytes of cached images\n            pbar = tqdm(range(len(self.img_files)), desc=\'Caching images\')\n            self.img_hw0, self.img_hw = [None] * n, [None] * n\n            for i in pbar:  # max 10k images\n                self.imgs[i], self.img_hw0[i], self.img_hw[i] = load_image(self, i)  # img, hw_original, hw_resized\n                gb += self.imgs[i].nbytes\n                pbar.desc = \'Caching images (%.1fGB)\' % (gb / 1E9)\n\n        # Detect corrupted images https://medium.com/joelthchao/programmatically-detect-corrupted-image-8c1b2006c3d3\n        detect_corrupted_images = False\n        if detect_corrupted_images:\n            from skimage import io  # conda install -c conda-forge scikit-image\n            for file in tqdm(self.img_files, desc=\'Detecting corrupted images\'):\n                try:\n                    _ = io.imread(file)\n                except:\n                    print(\'Corrupted image detected: %s\' % file)\n\n    def __len__(self):\n        return len(self.img_files)\n\n    # def __iter__(self):\n    #     self.count = -1\n    #     print(\'ran dataset iter\')\n    #     #self.shuffled_vector = np.random.permutation(self.nF) if self.augment else np.arange(self.nF)\n    #     return self\n\n    def __getitem__(self, index):\n        if self.image_weights:\n            index = self.indices[index]\n\n        hyp = self.hyp\n        if self.mosaic:\n            # Load mosaic\n            img, labels = load_mosaic(self, index)\n            shapes = None\n\n        else:\n            # Load image\n            img, (h0, w0), (h, w) = load_image(self, index)\n\n            # Letterbox\n            shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # final letterboxed shape\n            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)\n            shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\n\n            # Load labels\n            labels = []\n            x = self.labels[index]\n            if x.size > 0:\n                # Normalized xywh to pixel xyxy format\n                labels = x.copy()\n                labels[:, 1] = ratio[0] * w * (x[:, 1] - x[:, 3] / 2) + pad[0]  # pad width\n                labels[:, 2] = ratio[1] * h * (x[:, 2] - x[:, 4] / 2) + pad[1]  # pad height\n                labels[:, 3] = ratio[0] * w * (x[:, 1] + x[:, 3] / 2) + pad[0]\n                labels[:, 4] = ratio[1] * h * (x[:, 2] + x[:, 4] / 2) + pad[1]\n\n        if self.augment:\n            # Augment imagespace\n            if not self.mosaic:\n                img, labels = random_affine(img, labels,\n                                            degrees=hyp[\'degrees\'],\n                                            translate=hyp[\'translate\'],\n                                            scale=hyp[\'scale\'],\n                                            shear=hyp[\'shear\'])\n\n            # Augment colorspace\n            augment_hsv(img, hgain=hyp[\'hsv_h\'], sgain=hyp[\'hsv_s\'], vgain=hyp[\'hsv_v\'])\n\n            # Apply cutouts\n            # if random.random() < 0.9:\n            #     labels = cutout(img, labels)\n\n        nL = len(labels)  # number of labels\n        if nL:\n            # convert xyxy to xywh\n            labels[:, 1:5] = xyxy2xywh(labels[:, 1:5])\n\n            # Normalize coordinates 0 - 1\n            labels[:, [2, 4]] /= img.shape[0]  # height\n            labels[:, [1, 3]] /= img.shape[1]  # width\n\n        if self.augment:\n            # random left-right flip\n            lr_flip = True\n            if lr_flip and random.random() < 0.5:\n                img = np.fliplr(img)\n                if nL:\n                    labels[:, 1] = 1 - labels[:, 1]\n\n            # random up-down flip\n            ud_flip = False\n            if ud_flip and random.random() < 0.5:\n                img = np.flipud(img)\n                if nL:\n                    labels[:, 2] = 1 - labels[:, 2]\n\n        labels_out = torch.zeros((nL, 6))\n        if nL:\n            labels_out[:, 1:] = torch.from_numpy(labels)\n\n        # Convert\n        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n        img = np.ascontiguousarray(img)\n\n        return torch.from_numpy(img), labels_out, self.img_files[index], shapes\n\n    @staticmethod\n    def collate_fn(batch):\n        img, label, path, shapes = zip(*batch)  # transposed\n        for i, l in enumerate(label):\n            l[:, 0] = i  # add target image index for build_targets()\n        return torch.stack(img, 0), torch.cat(label, 0), path, shapes\n\n\ndef load_image(self, index):\n    # loads 1 image from dataset, returns img, original hw, resized hw\n    img = self.imgs[index]\n    if img is None:  # not cached\n        path = self.img_files[index]\n        img = cv2.imread(path)  # BGR\n        assert img is not None, \'Image Not Found \' + path\n        h0, w0 = img.shape[:2]  # orig hw\n        r = self.img_size / max(h0, w0)  # resize image to img_size\n        if r < 1 or (self.augment and r != 1):  # always resize down, only resize up if training with augmentation\n            interp = cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR\n            img = cv2.resize(img, (int(w0 * r), int(h0 * r)), interpolation=interp)\n        return img, (h0, w0), img.shape[:2]  # img, hw_original, hw_resized\n    else:\n        return self.imgs[index], self.img_hw0[index], self.img_hw[index]  # img, hw_original, hw_resized\n\n\ndef augment_hsv(img, hgain=0.5, sgain=0.5, vgain=0.5):\n    r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n    dtype = img.dtype  # uint8\n\n    x = np.arange(0, 256, dtype=np.int16)\n    lut_hue = ((x * r[0]) % 180).astype(dtype)\n    lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n    lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n\n    img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)\n    cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=img)  # no return needed\n\n    # Histogram equalization\n    # if random.random() < 0.2:\n    #     for i in range(3):\n    #         img[:, :, i] = cv2.equalizeHist(img[:, :, i])\n\n\ndef load_mosaic(self, index):\n    # loads images in a mosaic\n\n    labels4 = []\n    s = self.img_size\n    xc, yc = [int(random.uniform(s * 0.5, s * 1.5)) for _ in range(2)]  # mosaic center x, y\n    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]  # 3 additional image indices\n    for i, index in enumerate(indices):\n        # Load image\n        img, _, (h, w) = load_image(self, index)\n\n        # place img in img4\n        if i == 0:  # top left\n            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n        elif i == 1:  # top right\n            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n        elif i == 2:  # bottom left\n            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n        elif i == 3:  # bottom right\n            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n\n        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n        padw = x1a - x1b\n        padh = y1a - y1b\n\n        # Labels\n        x = self.labels[index]\n        labels = x.copy()\n        if x.size > 0:  # Normalized xywh to pixel xyxy format\n            labels[:, 1] = w * (x[:, 1] - x[:, 3] / 2) + padw\n            labels[:, 2] = h * (x[:, 2] - x[:, 4] / 2) + padh\n            labels[:, 3] = w * (x[:, 1] + x[:, 3] / 2) + padw\n            labels[:, 4] = h * (x[:, 2] + x[:, 4] / 2) + padh\n        labels4.append(labels)\n\n    # Concat/clip labels\n    if len(labels4):\n        labels4 = np.concatenate(labels4, 0)\n        # np.clip(labels4[:, 1:] - s / 2, 0, s, out=labels4[:, 1:])  # use with center crop\n        np.clip(labels4[:, 1:], 0, 2 * s, out=labels4[:, 1:])  # use with random_affine\n\n    # Augment\n    # img4 = img4[s // 2: int(s * 1.5), s // 2:int(s * 1.5)]  # center crop (WARNING, requires box pruning)\n    img4, labels4 = random_affine(img4, labels4,\n                                  degrees=self.hyp[\'degrees\'],\n                                  translate=self.hyp[\'translate\'],\n                                  scale=self.hyp[\'scale\'],\n                                  shear=self.hyp[\'shear\'],\n                                  border=-s // 2)  # border to remove\n\n    return img4, labels4\n\n\ndef letterbox(img, new_shape=(416, 416), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n    # Resize image to a 32-pixel-multiple rectangle https://github.com/ultralytics/yolov3/issues/232\n    shape = img.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n\n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle\n        dw, dh = np.mod(dw, 64), np.mod(dh, 64)  # wh padding\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = new_shape\n        ratio = new_shape[0] / shape[1], new_shape[1] / shape[0]  # width, height ratios\n\n    dw /= 2  # divide padding into 2 sides\n    dh /= 2\n\n    if shape[::-1] != new_unpad:  # resize\n        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n    return img, ratio, (dw, dh)\n\n\ndef random_affine(img, targets=(), degrees=10, translate=.1, scale=.1, shear=10, border=0):\n    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n    # https://medium.com/uruvideo/dataset-augmentation-with-random-homographies-a8f4b44830d4\n    # targets = [cls, xyxy]\n\n    height = img.shape[0] + border * 2\n    width = img.shape[1] + border * 2\n\n    # Rotation and Scale\n    R = np.eye(3)\n    a = random.uniform(-degrees, degrees)\n    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n    s = random.uniform(1 - scale, 1 + scale)\n    # s = 2 ** random.uniform(-scale, scale)\n    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(img.shape[1] / 2, img.shape[0] / 2), scale=s)\n\n    # Translation\n    T = np.eye(3)\n    T[0, 2] = random.uniform(-translate, translate) * img.shape[0] + border  # x translation (pixels)\n    T[1, 2] = random.uniform(-translate, translate) * img.shape[1] + border  # y translation (pixels)\n\n    # Shear\n    S = np.eye(3)\n    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n\n    # Combined rotation matrix\n    M = S @ T @ R  # ORDER IS IMPORTANT HERE!!\n    if (border != 0) or (M != np.eye(3)).any():  # image changed\n        img = cv2.warpAffine(img, M[:2], dsize=(width, height), flags=cv2.INTER_LINEAR, borderValue=(114, 114, 114))\n\n    # Transform label coordinates\n    n = len(targets)\n    if n:\n        # warp points\n        xy = np.ones((n * 4, 3))\n        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n        xy = (xy @ M.T)[:, :2].reshape(n, 8)\n\n        # create new boxes\n        x = xy[:, [0, 2, 4, 6]]\n        y = xy[:, [1, 3, 5, 7]]\n        xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n\n        # # apply angle-based reduction of bounding boxes\n        # radians = a * math.pi / 180\n        # reduction = max(abs(math.sin(radians)), abs(math.cos(radians))) ** 0.5\n        # x = (xy[:, 2] + xy[:, 0]) / 2\n        # y = (xy[:, 3] + xy[:, 1]) / 2\n        # w = (xy[:, 2] - xy[:, 0]) * reduction\n        # h = (xy[:, 3] - xy[:, 1]) * reduction\n        # xy = np.concatenate((x - w / 2, y - h / 2, x + w / 2, y + h / 2)).reshape(4, n).T\n\n        # reject warped points outside of image\n        xy[:, [0, 2]] = xy[:, [0, 2]].clip(0, width)\n        xy[:, [1, 3]] = xy[:, [1, 3]].clip(0, height)\n        w = xy[:, 2] - xy[:, 0]\n        h = xy[:, 3] - xy[:, 1]\n        area = w * h\n        area0 = (targets[:, 3] - targets[:, 1]) * (targets[:, 4] - targets[:, 2])\n        ar = np.maximum(w / (h + 1e-16), h / (w + 1e-16))  # aspect ratio\n        i = (w > 4) & (h > 4) & (area / (area0 * s + 1e-16) > 0.2) & (ar < 10)\n\n        targets = targets[i]\n        targets[:, 1:5] = xy[i]\n\n    return img, targets\n\n\ndef cutout(image, labels):\n    # https://arxiv.org/abs/1708.04552\n    # https://github.com/hysts/pytorch_cutout/blob/master/dataloader.py\n    # https://towardsdatascience.com/when-conventional-wisdom-fails-revisiting-data-augmentation-for-self-driving-cars-4831998c5509\n    h, w = image.shape[:2]\n\n    def bbox_ioa(box1, box2):\n        # Returns the intersection over box2 area given box1, box2. box1 is 4, box2 is nx4. boxes are x1y1x2y2\n        box2 = box2.transpose()\n\n        # Get the coordinates of bounding boxes\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n\n        # Intersection area\n        inter_area = (np.minimum(b1_x2, b2_x2) - np.maximum(b1_x1, b2_x1)).clip(0) * \\\n                     (np.minimum(b1_y2, b2_y2) - np.maximum(b1_y1, b2_y1)).clip(0)\n\n        # box2 area\n        box2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1) + 1e-16\n\n        # Intersection over box2 area\n        return inter_area / box2_area\n\n    # create random masks\n    scales = [0.5] * 1 + [0.25] * 2 + [0.125] * 4 + [0.0625] * 8 + [0.03125] * 16  # image size fraction\n    for s in scales:\n        mask_h = random.randint(1, int(h * s))\n        mask_w = random.randint(1, int(w * s))\n\n        # box\n        xmin = max(0, random.randint(0, w) - mask_w // 2)\n        ymin = max(0, random.randint(0, h) - mask_h // 2)\n        xmax = min(w, xmin + mask_w)\n        ymax = min(h, ymin + mask_h)\n\n        # apply random color mask\n        image[ymin:ymax, xmin:xmax] = [random.randint(64, 191) for _ in range(3)]\n\n        # return unobscured labels\n        if len(labels) and s > 0.03:\n            box = np.array([xmin, ymin, xmax, ymax], dtype=np.float32)\n            ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\n            labels = labels[ioa < 0.60]  # remove >60% obscured labels\n\n    return labels\n\n\ndef reduce_img_size(path=\'../data/sm4/images\', img_size=1024):  # from utils.datasets import *; reduce_img_size()\n    # creates a new ./images_reduced folder with reduced size images of maximum size img_size\n    path_new = path + \'_reduced\'  # reduced images path\n    create_folder(path_new)\n    for f in tqdm(glob.glob(\'%s/*.*\' % path)):\n        try:\n            img = cv2.imread(f)\n            h, w = img.shape[:2]\n            r = img_size / max(h, w)  # size ratio\n            if r < 1.0:\n                img = cv2.resize(img, (int(w * r), int(h * r)), interpolation=cv2.INTER_AREA)  # _LINEAR fastest\n            fnew = f.replace(path, path_new)  # .replace(Path(f).suffix, \'.jpg\')\n            cv2.imwrite(fnew, img)\n        except:\n            print(\'WARNING: image failure %s\' % f)\n\n\ndef convert_images2bmp():  # from utils.datasets import *; convert_images2bmp()\n    # Save images\n    formats = [x.lower() for x in img_formats] + [x.upper() for x in img_formats]\n    # for path in [\'../coco/images/val2014\', \'../coco/images/train2014\']:\n    for path in [\'../data/sm4/images\', \'../data/sm4/background\']:\n        create_folder(path + \'bmp\')\n        for ext in formats:  # [\'.bmp\', \'.jpg\', \'.jpeg\', \'.png\', \'.tif\', \'.dng\']\n            for f in tqdm(glob.glob(\'%s/*%s\' % (path, ext)), desc=\'Converting %s\' % ext):\n                cv2.imwrite(f.replace(ext.lower(), \'.bmp\').replace(path, path + \'bmp\'), cv2.imread(f))\n\n    # Save labels\n    # for path in [\'../coco/trainvalno5k.txt\', \'../coco/5k.txt\']:\n    for file in [\'../data/sm4/out_train.txt\', \'../data/sm4/out_test.txt\']:\n        with open(file, \'r\') as f:\n            lines = f.read()\n            # lines = f.read().replace(\'2014/\', \'2014bmp/\')  # coco\n            lines = lines.replace(\'/images\', \'/imagesbmp\')\n            lines = lines.replace(\'/background\', \'/backgroundbmp\')\n        for ext in formats:\n            lines = lines.replace(ext, \'.bmp\')\n        with open(file.replace(\'.txt\', \'bmp.txt\'), \'w\') as f:\n            f.write(lines)\n\n\ndef recursive_dataset2bmp(dataset=\'../data/sm4_bmp\'):  # from utils.datasets import *; recursive_dataset2bmp()\n    # Converts dataset to bmp (for faster training)\n    formats = [x.lower() for x in img_formats] + [x.upper() for x in img_formats]\n    for a, b, files in os.walk(dataset):\n        for file in tqdm(files, desc=a):\n            p = a + \'/\' + file\n            s = Path(file).suffix\n            if s == \'.txt\':  # replace text\n                with open(p, \'r\') as f:\n                    lines = f.read()\n                for f in formats:\n                    lines = lines.replace(f, \'.bmp\')\n                with open(p, \'w\') as f:\n                    f.write(lines)\n            elif s in formats:  # replace image\n                cv2.imwrite(p.replace(s, \'.bmp\'), cv2.imread(p))\n                if s != \'.bmp\':\n                    os.system(""rm \'%s\'"" % p)\n\n\ndef imagelist2folder(path=\'data/coco_64img.txt\'):  # from utils.datasets import *; imagelist2folder()\n    # Copies all the images in a text file (list of images) into a folder\n    create_folder(path[:-4])\n    with open(path, \'r\') as f:\n        for line in f.read().splitlines():\n            os.system(\'cp ""%s"" %s\' % (line, path[:-4]))\n            print(line)\n\n\ndef create_folder(path=\'./new_folder\'):\n    # Create folder\n    if os.path.exists(path):\n        shutil.rmtree(path)  # delete output folder\n    os.makedirs(path)  # make new output folder\n'"
utils/google_utils.py,0,"b'# This file contains google utils: https://cloud.google.com/storage/docs/reference/libraries\n# pip install --upgrade google-cloud-storage\n\nimport os\nimport time\n\n\n# from google.cloud import storage\n\n\ndef gdrive_download(id=\'1HaXkef9z6y5l4vUnCYgdmEAj61c6bfWO\', name=\'coco.zip\'):\n    # https://gist.github.com/tanaikech/f0f2d122e05bf5f971611258c22c110f\n    # Downloads a file from Google Drive, accepting presented query\n    # from utils.google_utils import *; gdrive_download()\n    t = time.time()\n\n    print(\'Downloading https://drive.google.com/uc?export=download&id=%s as %s... \' % (id, name), end=\'\')\n    os.remove(name) if os.path.exists(name) else None  # remove existing\n    os.remove(\'cookie\') if os.path.exists(\'cookie\') else None\n\n    # Attempt file download\n    os.system(""curl -c ./cookie -s -L \\""https://drive.google.com/uc?export=download&id=%s\\"" > /dev/null"" % id)\n    if os.path.exists(\'cookie\'):  # large file\n        s = ""curl -Lb ./cookie \\""https://drive.google.com/uc?export=download&confirm=`awk \'/download/ {print $NF}\' ./cookie`&id=%s\\"" -o %s"" % (\n            id, name)\n    else:  # small file\n        s = ""curl -s -L -o %s \'https://drive.google.com/uc?export=download&id=%s\'"" % (name, id)\n    r = os.system(s)  # execute, capture return values\n    os.remove(\'cookie\') if os.path.exists(\'cookie\') else None\n\n    # Error check\n    if r != 0:\n        os.remove(name) if os.path.exists(name) else None  # remove partial\n        print(\'Download error \')  # raise Exception(\'Download error\')\n        return r\n\n    # Unzip if archive\n    if name.endswith(\'.zip\'):\n        print(\'unzipping... \', end=\'\')\n        os.system(\'unzip -q %s\' % name)  # unzip\n        os.remove(name)  # remove zip to free space\n\n    print(\'Done (%.1fs)\' % (time.time() - t))\n    return r\n\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    # Uploads a file to a bucket\n    # https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    blob.upload_from_filename(source_file_name)\n\n    print(\'File {} uploaded to {}.\'.format(\n        source_file_name,\n        destination_blob_name))\n\n\ndef download_blob(bucket_name, source_blob_name, destination_file_name):\n    # Uploads a blob from a bucket\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(source_blob_name)\n\n    blob.download_to_filename(destination_file_name)\n\n    print(\'Blob {} downloaded to {}.\'.format(\n        source_blob_name,\n        destination_file_name))\n'"
utils/layers.py,14,"b""import torch.nn.functional as F\n\nfrom utils.utils import *\n\n\ndef make_divisible(v, divisor):\n    # Function ensures all layers have a channel number that is divisible by 8\n    # https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    return math.ceil(v / divisor) * divisor\n\n\nclass Flatten(nn.Module):\n    # Use after nn.AdaptiveAvgPool2d(1) to remove last 2 dimensions\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass Concat(nn.Module):\n    # Concatenate a list of tensors along dimension\n    def __init__(self, dimension=1):\n        super(Concat, self).__init__()\n        self.d = dimension\n\n    def forward(self, x):\n        return torch.cat(x, self.d)\n\n\nclass FeatureConcat(nn.Module):\n    def __init__(self, layers):\n        super(FeatureConcat, self).__init__()\n        self.layers = layers  # layer indices\n        self.multiple = len(layers) > 1  # multiple layers flag\n\n    def forward(self, x, outputs):\n        return torch.cat([outputs[i] for i in self.layers], 1) if self.multiple else outputs[self.layers[0]]\n\n\nclass WeightedFeatureFusion(nn.Module):  # weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070\n    def __init__(self, layers, weight=False):\n        super(WeightedFeatureFusion, self).__init__()\n        self.layers = layers  # layer indices\n        self.weight = weight  # apply weights boolean\n        self.n = len(layers) + 1  # number of layers\n        if weight:\n            self.w = nn.Parameter(torch.zeros(self.n), requires_grad=True)  # layer weights\n\n    def forward(self, x, outputs):\n        # Weights\n        if self.weight:\n            w = torch.sigmoid(self.w) * (2 / self.n)  # sigmoid weights (0-1)\n            x = x * w[0]\n\n        # Fusion\n        nx = x.shape[1]  # input channels\n        for i in range(self.n - 1):\n            a = outputs[self.layers[i]] * w[i + 1] if self.weight else outputs[self.layers[i]]  # feature to add\n            na = a.shape[1]  # feature channels\n\n            # Adjust channels\n            if nx == na:  # same shape\n                x = x + a\n            elif nx > na:  # slice input\n                x[:, :na] = x[:, :na] + a  # or a = nn.ZeroPad2d((0, 0, 0, 0, 0, dc))(a); x = x + a\n            else:  # slice feature\n                x = x + a[:, :nx]\n\n        return x\n\n\nclass MixConv2d(nn.Module):  # MixConv: Mixed Depthwise Convolutional Kernels https://arxiv.org/abs/1907.09595\n    def __init__(self, in_ch, out_ch, k=(3, 5, 7), stride=1, dilation=1, bias=True, method='equal_params'):\n        super(MixConv2d, self).__init__()\n\n        groups = len(k)\n        if method == 'equal_ch':  # equal channels per group\n            i = torch.linspace(0, groups - 1E-6, out_ch).floor()  # out_ch indices\n            ch = [(i == g).sum() for g in range(groups)]\n        else:  # 'equal_params': equal parameter count per group\n            b = [out_ch] + [0] * groups\n            a = np.eye(groups + 1, groups, k=-1)\n            a -= np.roll(a, 1, axis=1)\n            a *= np.array(k) ** 2\n            a[0] = 1\n            ch = np.linalg.lstsq(a, b, rcond=None)[0].round().astype(int)  # solve for equal weight indices, ax = b\n\n        self.m = nn.ModuleList([nn.Conv2d(in_channels=in_ch,\n                                          out_channels=ch[g],\n                                          kernel_size=k[g],\n                                          stride=stride,\n                                          padding=k[g] // 2,  # 'same' pad\n                                          dilation=dilation,\n                                          bias=bias) for g in range(groups)])\n\n    def forward(self, x):\n        return torch.cat([m(x) for m in self.m], 1)\n\n\n# Activation functions below -------------------------------------------------------------------------------------------\nclass SwishImplementation(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return x * torch.sigmoid(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        sx = torch.sigmoid(x)  # sigmoid(ctx)\n        return grad_output * (sx * (1 + x * (1 - sx)))\n\n\nclass MishImplementation(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return x.mul(torch.tanh(F.softplus(x)))  # x * tanh(ln(1 + exp(x)))\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        sx = torch.sigmoid(x)\n        fx = F.softplus(x).tanh()\n        return grad_output * (fx + x * sx * (1 - fx * fx))\n\n\nclass MemoryEfficientSwish(nn.Module):\n    def forward(self, x):\n        return SwishImplementation.apply(x)\n\n\nclass MemoryEfficientMish(nn.Module):\n    def forward(self, x):\n        return MishImplementation.apply(x)\n\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass HardSwish(nn.Module):  # https://arxiv.org/pdf/1905.02244.pdf\n    def forward(self, x):\n        return x * F.hardtanh(x + 3, 0., 6., True) / 6.\n\n\nclass Mish(nn.Module):  # https://github.com/digantamisra98/Mish\n    def forward(self, x):\n        return x * F.softplus(x).tanh()\n"""
utils/parse_config.py,0,"b'import os\n\nimport numpy as np\n\n\ndef parse_model_cfg(path):\n    # Parse the yolo *.cfg file and return module definitions path may be \'cfg/yolov3.cfg\', \'yolov3.cfg\', or \'yolov3\'\n    if not path.endswith(\'.cfg\'):  # add .cfg suffix if omitted\n        path += \'.cfg\'\n    if not os.path.exists(path) and os.path.exists(\'cfg\' + os.sep + path):  # add cfg/ prefix if omitted\n        path = \'cfg\' + os.sep + path\n\n    with open(path, \'r\') as f:\n        lines = f.read().split(\'\\n\')\n    lines = [x for x in lines if x and not x.startswith(\'#\')]\n    lines = [x.rstrip().lstrip() for x in lines]  # get rid of fringe whitespaces\n    mdefs = []  # module definitions\n    for line in lines:\n        if line.startswith(\'[\'):  # This marks the start of a new block\n            mdefs.append({})\n            mdefs[-1][\'type\'] = line[1:-1].rstrip()\n            if mdefs[-1][\'type\'] == \'convolutional\':\n                mdefs[-1][\'batch_normalize\'] = 0  # pre-populate with zeros (may be overwritten later)\n        else:\n            key, val = line.split(""="")\n            key = key.rstrip()\n\n            if key == \'anchors\':  # return nparray\n                mdefs[-1][key] = np.array([float(x) for x in val.split(\',\')]).reshape((-1, 2))  # np anchors\n            elif (key in [\'from\', \'layers\', \'mask\']) or (key == \'size\' and \',\' in val):  # return array\n                mdefs[-1][key] = [int(x) for x in val.split(\',\')]\n            else:\n                val = val.strip()\n                if val.isnumeric():  # return int or float\n                    mdefs[-1][key] = int(val) if (int(val) - float(val)) == 0 else float(val)\n                else:\n                    mdefs[-1][key] = val  # return string\n\n    # Check all fields are supported\n    supported = [\'type\', \'batch_normalize\', \'filters\', \'size\', \'stride\', \'pad\', \'activation\', \'layers\', \'groups\',\n                 \'from\', \'mask\', \'anchors\', \'classes\', \'num\', \'jitter\', \'ignore_thresh\', \'truth_thresh\', \'random\',\n                 \'stride_x\', \'stride_y\', \'weights_type\', \'weights_normalization\', \'scale_x_y\', \'beta_nms\', \'nms_kind\',\n                 \'iou_loss\', \'iou_normalizer\', \'cls_normalizer\', \'iou_thresh\']\n\n    f = []  # fields\n    for x in mdefs[1:]:\n        [f.append(k) for k in x if k not in f]\n    u = [x for x in f if x not in supported]  # unsupported fields\n    assert not any(u), ""Unsupported fields %s in %s. See https://github.com/ultralytics/yolov3/issues/631"" % (u, path)\n\n    return mdefs\n\n\ndef parse_data_cfg(path):\n    # Parses the data configuration file\n    if not os.path.exists(path) and os.path.exists(\'data\' + os.sep + path):  # add data/ prefix if omitted\n        path = \'data\' + os.sep + path\n\n    with open(path, \'r\') as f:\n        lines = f.readlines()\n\n    options = dict()\n    for line in lines:\n        line = line.strip()\n        if line == \'\' or line.startswith(\'#\'):\n            continue\n        key, val = line.split(\'=\')\n        options[key.strip()] = val.strip()\n\n    return options\n'"
utils/torch_utils.py,23,"b'import math\nimport os\nimport time\nfrom copy import deepcopy\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef init_seeds(seed=0):\n    torch.manual_seed(seed)\n\n    # Reduce randomness (may be slower on Tesla GPUs) # https://pytorch.org/docs/stable/notes/randomness.html\n    if seed == 0:\n        cudnn.deterministic = False\n        cudnn.benchmark = True\n\n\ndef select_device(device=\'\', apex=False, batch_size=None):\n    # device = \'cpu\' or \'0\' or \'0,1,2,3\'\n    cpu_request = device.lower() == \'cpu\'\n    if device and not cpu_request:  # if device requested other than \'cpu\'\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = device  # set environment variable\n        assert torch.cuda.is_available(), \'CUDA unavailable, invalid device %s requested\' % device  # check availablity\n\n    cuda = False if cpu_request else torch.cuda.is_available()\n    if cuda:\n        c = 1024 ** 2  # bytes to MB\n        ng = torch.cuda.device_count()\n        if ng > 1 and batch_size:  # check that batch_size is compatible with device_count\n            assert batch_size % ng == 0, \'batch-size %g not multiple of GPU count %g\' % (batch_size, ng)\n        x = [torch.cuda.get_device_properties(i) for i in range(ng)]\n        s = \'Using CUDA \' + (\'Apex \' if apex else \'\')  # apex for mixed precision https://github.com/NVIDIA/apex\n        for i in range(0, ng):\n            if i == 1:\n                s = \' \' * len(s)\n            print(""%sdevice%g _CudaDeviceProperties(name=\'%s\', total_memory=%dMB)"" %\n                  (s, i, x[i].name, x[i].total_memory / c))\n    else:\n        print(\'Using CPU\')\n\n    print(\'\')  # skip a line\n    return torch.device(\'cuda:0\' if cuda else \'cpu\')\n\n\ndef time_synchronized():\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n    return time.time()\n\n\ndef initialize_weights(model):\n    for m in model.modules():\n        t = type(m)\n        if t is nn.Conv2d:\n            pass  # nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n        elif t is nn.BatchNorm2d:\n            m.eps = 1e-4\n            m.momentum = 0.03\n        elif t in [nn.LeakyReLU, nn.ReLU, nn.ReLU6]:\n            m.inplace = True\n\n\ndef find_modules(model, mclass=nn.Conv2d):\n    # finds layer indices matching module class \'mclass\'\n    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]\n\n\ndef fuse_conv_and_bn(conv, bn):\n    # https://tehnokv.com/posts/fusing-batchnorm-and-conv/\n    with torch.no_grad():\n        # init\n        fusedconv = torch.nn.Conv2d(conv.in_channels,\n                                    conv.out_channels,\n                                    kernel_size=conv.kernel_size,\n                                    stride=conv.stride,\n                                    padding=conv.padding,\n                                    bias=True)\n\n        # prepare filters\n        w_conv = conv.weight.clone().view(conv.out_channels, -1)\n        w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n        fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.size()))\n\n        # prepare spatial bias\n        if conv.bias is not None:\n            b_conv = conv.bias\n        else:\n            b_conv = torch.zeros(conv.weight.size(0))\n        b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n        fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\n\n        return fusedconv\n\n\ndef model_info(model, verbose=False):\n    # Plots a line-by-line description of a PyTorch model\n    n_p = sum(x.numel() for x in model.parameters())  # number parameters\n    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients\n    if verbose:\n        print(\'%5s %40s %9s %12s %20s %10s %10s\' % (\'layer\', \'name\', \'gradient\', \'parameters\', \'shape\', \'mu\', \'sigma\'))\n        for i, (name, p) in enumerate(model.named_parameters()):\n            name = name.replace(\'module_list.\', \'\')\n            print(\'%5g %40s %9s %12g %20s %10.3g %10.3g\' %\n                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))\n\n    try:  # FLOPS\n        from thop import profile\n        macs, _ = profile(model, inputs=(torch.zeros(1, 3, 480, 640),), verbose=False)\n        fs = \', %.1f GFLOPS\' % (macs / 1E9 * 2)\n    except:\n        fs = \'\'\n\n    print(\'Model Summary: %g layers, %g parameters, %g gradients%s\' % (len(list(model.parameters())), n_p, n_g, fs))\n\n\ndef load_classifier(name=\'resnet101\', n=2):\n    # Loads a pretrained model reshaped to n-class output\n    import pretrainedmodels  # https://github.com/Cadene/pretrained-models.pytorch#torchvision\n    model = pretrainedmodels.__dict__[name](num_classes=1000, pretrained=\'imagenet\')\n\n    # Display model properties\n    for x in [\'model.input_size\', \'model.input_space\', \'model.input_range\', \'model.mean\', \'model.std\']:\n        print(x + \' =\', eval(x))\n\n    # Reshape output to n classes\n    filters = model.last_linear.weight.shape[1]\n    model.last_linear.bias = torch.nn.Parameter(torch.zeros(n))\n    model.last_linear.weight = torch.nn.Parameter(torch.zeros(n, filters))\n    model.last_linear.out_features = n\n    return model\n\n\ndef scale_img(img, ratio=1.0, same_shape=True):  # img(16,3,256,416), r=ratio\n    # scales img(bs,3,y,x) by ratio\n    h, w = img.shape[2:]\n    s = (int(h * ratio), int(w * ratio))  # new size\n    img = F.interpolate(img, size=s, mode=\'bilinear\', align_corners=False)  # resize\n    if not same_shape:  # pad/crop img\n        gs = 64  # (pixels) grid size\n        h, w = [math.ceil(x * ratio / gs) * gs for x in (h, w)]\n    return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean\n\n\nclass ModelEMA:\n    """""" Model Exponential Moving Average from https://github.com/rwightman/pytorch-image-models\n    Keep a moving average of everything in the model state_dict (parameters and buffers).\n    This is intended to allow functionality like\n    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n    A smoothed version of the weights is necessary for some training schemes to perform well.\n    E.g. Google\'s hyper-params for training MNASNet, MobileNet-V3, EfficientNet, etc that use\n    RMSprop with a short 2.4-3 epoch decay period and slow LR decay rate of .96-.99 requires EMA\n    smoothing of weights to match results. Pay attention to the decay constant you are using\n    relative to your update count per epoch.\n    To keep EMA from using GPU resources, set device=\'cpu\'. This will save a bit of memory but\n    disable validation of the EMA weights. Validation will have to be done manually in a separate\n    process, or after the training stops converging.\n    This class is sensitive where it is initialized in the sequence of model init,\n    GPU assignment and distributed training wrappers.\n    I\'ve tested with the sequence in my own train.py for torch.DataParallel, apex.DDP, and single-GPU.\n    """"""\n\n    def __init__(self, model, decay=0.9999, device=\'\'):\n        # make a copy of the model for accumulating moving average of weights\n        self.ema = deepcopy(model)\n        self.ema.eval()\n        self.updates = 0  # number of EMA updates\n        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))  # decay exponential ramp (to help early epochs)\n        self.device = device  # perform ema on different device from model if set\n        if device:\n            self.ema.to(device=device)\n        for p in self.ema.parameters():\n            p.requires_grad_(False)\n\n    def update(self, model):\n        self.updates += 1\n        d = self.decay(self.updates)\n        with torch.no_grad():\n            if type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel):\n                msd, esd = model.module.state_dict(), self.ema.module.state_dict()\n            else:\n                msd, esd = model.state_dict(), self.ema.state_dict()\n\n            for k, v in esd.items():\n                if v.dtype.is_floating_point:\n                    v *= d\n                    v += (1. - d) * msd[k].detach()\n\n    def update_attr(self, model):\n        # Assign attributes (which may change during training)\n        for k in model.__dict__.keys():\n            if not k.startswith(\'_\'):\n                setattr(self.ema, k, getattr(model, k))\n'"
utils/utils.py,51,"b'import glob\nimport math\nimport os\nimport random\nimport shutil\nimport subprocess\nimport time\nfrom copy import copy\nfrom pathlib import Path\nfrom sys import platform\n\nimport cv2\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom tqdm import tqdm\n\nfrom . import torch_utils  # , google_utils\n\n# Set printoptions\ntorch.set_printoptions(linewidth=320, precision=5, profile=\'long\')\nnp.set_printoptions(linewidth=320, formatter={\'float_kind\': \'{:11.5g}\'.format})  # format short g, %precision=5\nmatplotlib.rc(\'font\', **{\'size\': 11})\n\n# Prevent OpenCV from multithreading (to use PyTorch DataLoader)\ncv2.setNumThreads(0)\n\n\ndef init_seeds(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch_utils.init_seeds(seed=seed)\n\n\ndef check_git_status():\n    if platform in [\'linux\', \'darwin\']:\n        # Suggest \'git pull\' if repo is out of date\n        s = subprocess.check_output(\'if [ -d .git ]; then git fetch && git status -uno; fi\', shell=True).decode(\'utf-8\')\n        if \'Your branch is behind\' in s:\n            print(s[s.find(\'Your branch is behind\'):s.find(\'\\n\\n\')] + \'\\n\')\n\n\ndef load_classes(path):\n    # Loads *.names file at \'path\'\n    with open(path, \'r\') as f:\n        names = f.read().split(\'\\n\')\n    return list(filter(None, names))  # filter removes empty strings (such as last line)\n\n\ndef labels_to_class_weights(labels, nc=80):\n    # Get class weights (inverse frequency) from training labels\n    if labels[0] is None:  # no labels loaded\n        return torch.Tensor()\n\n    labels = np.concatenate(labels, 0)  # labels.shape = (866643, 5) for COCO\n    classes = labels[:, 0].astype(np.int)  # labels = [class xywh]\n    weights = np.bincount(classes, minlength=nc)  # occurences per class\n\n    # Prepend gridpoint count (for uCE trianing)\n    # gpi = ((320 / 32 * np.array([1, 2, 4])) ** 2 * 3).sum()  # gridpoints per image\n    # weights = np.hstack([gpi * len(labels)  - weights.sum() * 9, weights * 9]) ** 0.5  # prepend gridpoints to start\n\n    weights[weights == 0] = 1  # replace empty bins with 1\n    weights = 1 / weights  # number of targets per class\n    weights /= weights.sum()  # normalize\n    return torch.from_numpy(weights)\n\n\ndef labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):\n    # Produces image weights based on class mAPs\n    n = len(labels)\n    class_counts = np.array([np.bincount(labels[i][:, 0].astype(np.int), minlength=nc) for i in range(n)])\n    image_weights = (class_weights.reshape(1, nc) * class_counts).sum(1)\n    # index = random.choices(range(n), weights=image_weights, k=1)  # weight image sample\n    return image_weights\n\n\ndef coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)\n    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n    # a = np.loadtxt(\'data/coco.names\', dtype=\'str\', delimiter=\'\\n\')\n    # b = np.loadtxt(\'data/coco_paper.names\', dtype=\'str\', delimiter=\'\\n\')\n    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n    return x\n\n\ndef xyxy2xywh(x):\n    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n    y[:, 2] = x[:, 2] - x[:, 0]  # width\n    y[:, 3] = x[:, 3] - x[:, 1]  # height\n    return y\n\n\ndef xywh2xyxy(x):\n    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n    return y\n\n\ndef scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n    # Rescale coords (xyxy) from img1_shape to img0_shape\n    if ratio_pad is None:  # calculate from img0_shape\n        gain = max(img1_shape) / max(img0_shape)  # gain  = old / new\n        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n\n    coords[:, [0, 2]] -= pad[0]  # x padding\n    coords[:, [1, 3]] -= pad[1]  # y padding\n    coords[:, :4] /= gain\n    clip_coords(coords, img0_shape)\n    return coords\n\n\ndef clip_coords(boxes, img_shape):\n    # Clip bounding xyxy bounding boxes to image shape (height, width)\n    boxes[:, 0].clamp_(0, img_shape[1])  # x1\n    boxes[:, 1].clamp_(0, img_shape[0])  # y1\n    boxes[:, 2].clamp_(0, img_shape[1])  # x2\n    boxes[:, 3].clamp_(0, img_shape[0])  # y2\n\n\ndef ap_per_class(tp, conf, pred_cls, target_cls):\n    """""" Compute the average precision, given the recall and precision curves.\n    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n    # Arguments\n        tp:    True positives (nparray, nx1 or nx10).\n        conf:  Objectness value from 0-1 (nparray).\n        pred_cls: Predicted object classes (nparray).\n        target_cls: True object classes (nparray).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n\n    # Sort by objectness\n    i = np.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n    # Find unique classes\n    unique_classes = np.unique(target_cls)\n\n    # Create Precision-Recall curve and compute AP for each class\n    pr_score = 0.1  # score to evaluate P and R https://github.com/ultralytics/yolov3/issues/898\n    s = [unique_classes.shape[0], tp.shape[1]]  # number class, number iou thresholds (i.e. 10 for mAP0.5...0.95)\n    ap, p, r = np.zeros(s), np.zeros(s), np.zeros(s)\n    for ci, c in enumerate(unique_classes):\n        i = pred_cls == c\n        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n        n_p = i.sum()  # Number of predicted objects\n\n        if n_p == 0 or n_gt == 0:\n            continue\n        else:\n            # Accumulate FPs and TPs\n            fpc = (1 - tp[i]).cumsum(0)\n            tpc = tp[i].cumsum(0)\n\n            # Recall\n            recall = tpc / (n_gt + 1e-16)  # recall curve\n            r[ci] = np.interp(-pr_score, -conf[i], recall[:, 0])  # r at pr_score, negative x, xp because xp decreases\n\n            # Precision\n            precision = tpc / (tpc + fpc)  # precision curve\n            p[ci] = np.interp(-pr_score, -conf[i], precision[:, 0])  # p at pr_score\n\n            # AP from recall-precision curve\n            for j in range(tp.shape[1]):\n                ap[ci, j] = compute_ap(recall[:, j], precision[:, j])\n\n            # Plot\n            # fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n            # ax.plot(recall, precision)\n            # ax.set_xlabel(\'Recall\')\n            # ax.set_ylabel(\'Precision\')\n            # ax.set_xlim(0, 1.01)\n            # ax.set_ylim(0, 1.01)\n            # fig.tight_layout()\n            # fig.savefig(\'PR_curve.png\', dpi=300)\n\n    # Compute F1 score (harmonic mean of precision and recall)\n    f1 = 2 * p * r / (p + r + 1e-16)\n\n    return p, r, ap, f1, unique_classes.astype(\'int32\')\n\n\ndef compute_ap(recall, precision):\n    """""" Compute the average precision, given the recall and precision curves.\n    Source: https://github.com/rbgirshick/py-faster-rcnn.\n    # Arguments\n        recall:    The recall curve (list).\n        precision: The precision curve (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n\n    # Append sentinel values to beginning and end\n    mrec = np.concatenate(([0.], recall, [min(recall[-1] + 1E-3, 1.)]))\n    mpre = np.concatenate(([0.], precision, [0.]))\n\n    # Compute the precision envelope\n    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n\n    # Integrate area under curve\n    method = \'interp\'  # methods: \'continuous\', \'interp\'\n    if method == \'interp\':\n        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n    else:  # \'continuous\'\n        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x axis (recall) changes\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n\n    return ap\n\n\ndef bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False):\n    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n    box2 = box2.t()\n\n    # Get the coordinates of bounding boxes\n    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n    else:  # transform from xywh to xyxy\n        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n\n    # Intersection area\n    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n\n    # Union Area\n    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n    union = (w1 * h1 + 1e-16) + w2 * h2 - inter\n\n    iou = inter / union  # iou\n    if GIoU or DIoU or CIoU:\n        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n        if GIoU:  # Generalized IoU https://arxiv.org/pdf/1902.09630.pdf\n            c_area = cw * ch + 1e-16  # convex area\n            return iou - (c_area - union) / c_area  # GIoU\n        if DIoU or CIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n            # convex diagonal squared\n            c2 = cw ** 2 + ch ** 2 + 1e-16\n            # centerpoint distance squared\n            rho2 = ((b2_x1 + b2_x2) - (b1_x1 + b1_x2)) ** 2 / 4 + ((b2_y1 + b2_y2) - (b1_y1 + b1_y2)) ** 2 / 4\n            if DIoU:\n                return iou - rho2 / c2  # DIoU\n            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n                with torch.no_grad():\n                    alpha = v / (1 - iou + v)\n                return iou - (rho2 / c2 + v * alpha)  # CIoU\n\n    return iou\n\n\ndef box_iou(box1, box2):\n    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n    """"""\n    Return intersection-over-union (Jaccard index) of boxes.\n    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Arguments:\n        box1 (Tensor[N, 4])\n        box2 (Tensor[M, 4])\n    Returns:\n        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n            IoU values for every element in boxes1 and boxes2\n    """"""\n\n    def box_area(box):\n        # box = 4xn\n        return (box[2] - box[0]) * (box[3] - box[1])\n\n    area1 = box_area(box1.t())\n    area2 = box_area(box2.t())\n\n    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n\n\ndef wh_iou(wh1, wh2):\n    # Returns the nxm IoU matrix. wh1 is nx2, wh2 is mx2\n    wh1 = wh1[:, None]  # [N,1,2]\n    wh2 = wh2[None]  # [1,M,2]\n    inter = torch.min(wh1, wh2).prod(2)  # [N,M]\n    return inter / (wh1.prod(2) + wh2.prod(2) - inter)  # iou = inter / (area1 + area2 - inter)\n\n\nclass FocalLoss(nn.Module):\n    # Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)\n    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):\n        super(FocalLoss, self).__init__()\n        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = loss_fcn.reduction\n        self.loss_fcn.reduction = \'none\'  # required to apply FL to each element\n\n    def forward(self, pred, true):\n        loss = self.loss_fcn(pred, true)\n        # p_t = torch.exp(-loss)\n        # loss *= self.alpha * (1.000001 - p_t) ** self.gamma  # non-zero power for gradient stability\n\n        # TF implementation https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py\n        pred_prob = torch.sigmoid(pred)  # prob from logits\n        p_t = true * pred_prob + (1 - true) * (1 - pred_prob)\n        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)\n        modulating_factor = (1.0 - p_t) ** self.gamma\n        loss *= alpha_factor * modulating_factor\n\n        if self.reduction == \'mean\':\n            return loss.mean()\n        elif self.reduction == \'sum\':\n            return loss.sum()\n        else:  # \'none\'\n            return loss\n\n\ndef smooth_BCE(eps=0.1):  # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441\n    # return positive, negative label smoothing BCE targets\n    return 1.0 - 0.5 * eps, 0.5 * eps\n\n\ndef compute_loss(p, targets, model):  # predictions, targets, model\n    ft = torch.cuda.FloatTensor if p[0].is_cuda else torch.Tensor\n    lcls, lbox, lobj = ft([0]), ft([0]), ft([0])\n    tcls, tbox, indices, anchors = build_targets(p, targets, model)  # targets\n    h = model.hyp  # hyperparameters\n    red = \'mean\'  # Loss reduction (sum or mean)\n\n    # Define criteria\n    BCEcls = nn.BCEWithLogitsLoss(pos_weight=ft([h[\'cls_pw\']]), reduction=red)\n    BCEobj = nn.BCEWithLogitsLoss(pos_weight=ft([h[\'obj_pw\']]), reduction=red)\n\n    # class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3\n    cp, cn = smooth_BCE(eps=0.0)\n\n    # focal loss\n    g = h[\'fl_gamma\']  # focal loss gamma\n    if g > 0:\n        BCEcls, BCEobj = FocalLoss(BCEcls, g), FocalLoss(BCEobj, g)\n\n    # per output\n    nt = 0  # targets\n    for i, pi in enumerate(p):  # layer index, layer predictions\n        b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx\n        tobj = torch.zeros_like(pi[..., 0])  # target obj\n\n        nb = b.shape[0]  # number of targets\n        if nb:\n            nt += nb  # cumulative targets\n            ps = pi[b, a, gj, gi]  # prediction subset corresponding to targets\n\n            # GIoU\n            pxy = ps[:, :2].sigmoid()\n            pwh = ps[:, 2:4].exp().clamp(max=1E3) * anchors[i]\n            pbox = torch.cat((pxy, pwh), 1)  # predicted box\n            giou = bbox_iou(pbox.t(), tbox[i], x1y1x2y2=False, GIoU=True)  # giou(prediction, target)\n            lbox += (1.0 - giou).sum() if red == \'sum\' else (1.0 - giou).mean()  # giou loss\n\n            # Obj\n            tobj[b, a, gj, gi] = (1.0 - model.gr) + model.gr * giou.detach().clamp(0).type(tobj.dtype)  # giou ratio\n\n            # Class\n            if model.nc > 1:  # cls loss (only if multiple classes)\n                t = torch.full_like(ps[:, 5:], cn)  # targets\n                t[range(nb), tcls[i]] = cp\n                lcls += BCEcls(ps[:, 5:], t)  # BCE\n\n            # Append targets to text file\n            # with open(\'targets.txt\', \'a\') as file:\n            #     [file.write(\'%11.5g \' * 4 % tuple(x) + \'\\n\') for x in torch.cat((txy[i], twh[i]), 1)]\n\n        lobj += BCEobj(pi[..., 4], tobj)  # obj loss\n\n    lbox *= h[\'giou\']\n    lobj *= h[\'obj\']\n    lcls *= h[\'cls\']\n    if red == \'sum\':\n        bs = tobj.shape[0]  # batch size\n        g = 3.0  # loss gain\n        lobj *= g / bs\n        if nt:\n            lcls *= g / nt / model.nc\n            lbox *= g / nt\n\n    loss = lbox + lobj + lcls\n    return loss, torch.cat((lbox, lobj, lcls, loss)).detach()\n\n\ndef build_targets(p, targets, model):\n    # Build targets for compute_loss(), input targets(image,class,x,y,w,h)\n    nt = targets.shape[0]\n    tcls, tbox, indices, anch = [], [], [], []\n    gain = torch.ones(6, device=targets.device)  # normalized to gridspace gain\n    off = torch.tensor([[1, 0], [0, 1], [-1, 0], [0, -1]], device=targets.device).float()  # overlap offsets\n\n    style = None\n    multi_gpu = type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)\n    for i, j in enumerate(model.yolo_layers):\n        anchors = model.module.module_list[j].anchor_vec if multi_gpu else model.module_list[j].anchor_vec\n        gain[2:] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain\n        na = anchors.shape[0]  # number of anchors\n        at = torch.arange(na).view(na, 1).repeat(1, nt)  # anchor tensor, same as .repeat_interleave(nt)\n\n        # Match targets to anchors\n        a, t, offsets = [], targets * gain, 0\n        if nt:\n            # r = t[None, :, 4:6] / anchors[:, None]  # wh ratio\n            # j = torch.max(r, 1. / r).max(2)[0] < model.hyp[\'anchor_t\']  # compare\n            j = wh_iou(anchors, t[:, 4:6]) > model.hyp[\'iou_t\']  # iou(3,n) = wh_iou(anchors(3,2), gwh(n,2))\n            a, t = at[j], t.repeat(na, 1, 1)[j]  # filter\n\n            # overlaps\n            gxy = t[:, 2:4]  # grid xy\n            z = torch.zeros_like(gxy)\n            if style == \'rect2\':\n                g = 0.2  # offset\n                j, k = ((gxy % 1. < g) & (gxy > 1.)).T\n                a, t = torch.cat((a, a[j], a[k]), 0), torch.cat((t, t[j], t[k]), 0)\n                offsets = torch.cat((z, z[j] + off[0], z[k] + off[1]), 0) * g\n\n            elif style == \'rect4\':\n                g = 0.5  # offset\n                j, k = ((gxy % 1. < g) & (gxy > 1.)).T\n                l, m = ((gxy % 1. > (1 - g)) & (gxy < (gain[[2, 3]] - 1.))).T\n                a, t = torch.cat((a, a[j], a[k], a[l], a[m]), 0), torch.cat((t, t[j], t[k], t[l], t[m]), 0)\n                offsets = torch.cat((z, z[j] + off[0], z[k] + off[1], z[l] + off[2], z[m] + off[3]), 0) * g\n\n        # Define\n        b, c = t[:, :2].long().T  # image, class\n        gxy = t[:, 2:4]  # grid xy\n        gwh = t[:, 4:6]  # grid wh\n        gij = (gxy - offsets).long()\n        gi, gj = gij.T  # grid xy indices\n\n        # Append\n        indices.append((b, a, gj, gi))  # image, anchor, grid indices\n        tbox.append(torch.cat((gxy - gij, gwh), 1))  # box\n        anch.append(anchors[a])  # anchors\n        tcls.append(c)  # class\n        if c.shape[0]:  # if any targets\n            assert c.max() < model.nc, \'Model accepts %g classes labeled from 0-%g, however you labelled a class %g. \' \\\n                                       \'See https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data\' % (\n                                           model.nc, model.nc - 1, c.max())\n\n    return tcls, tbox, indices, anch\n\n\ndef non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, multi_label=True, classes=None, agnostic=False):\n    """"""\n    Performs  Non-Maximum Suppression on inference results\n    Returns detections with shape:\n        nx6 (x1, y1, x2, y2, conf, cls)\n    """"""\n\n    # Settings\n    merge = True  # merge for best mAP\n    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n    time_limit = 10.0  # seconds to quit after\n\n    t = time.time()\n    nc = prediction[0].shape[1] - 5  # number of classes\n    multi_label &= nc > 1  # multiple labels per box\n    output = [None] * prediction.shape[0]\n    for xi, x in enumerate(prediction):  # image index, image inference\n        # Apply constraints\n        x = x[x[:, 4] > conf_thres]  # confidence\n        x = x[((x[:, 2:4] > min_wh) & (x[:, 2:4] < max_wh)).all(1)]  # width-height\n\n        # If none remain process next image\n        if not x.shape[0]:\n            continue\n\n        # Compute conf\n        x[..., 5:] *= x[..., 4:5]  # conf = obj_conf * cls_conf\n\n        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n        box = xywh2xyxy(x[:, :4])\n\n        # Detections matrix nx6 (xyxy, conf, cls)\n        if multi_label:\n            i, j = (x[:, 5:] > conf_thres).nonzero().t()\n            x = torch.cat((box[i], x[i, j + 5].unsqueeze(1), j.float().unsqueeze(1)), 1)\n        else:  # best class only\n            conf, j = x[:, 5:].max(1)\n            x = torch.cat((box, conf.unsqueeze(1), j.float().unsqueeze(1)), 1)[conf > conf_thres]\n\n        # Filter by class\n        if classes:\n            x = x[(j.view(-1, 1) == torch.tensor(classes, device=j.device)).any(1)]\n\n        # Apply finite constraint\n        # if not torch.isfinite(x).all():\n        #     x = x[torch.isfinite(x).all(1)]\n\n        # If none remain process next image\n        n = x.shape[0]  # number of boxes\n        if not n:\n            continue\n\n        # Sort by confidence\n        # x = x[x[:, 4].argsort(descending=True)]\n\n        # Batched NMS\n        c = x[:, 5] * 0 if agnostic else x[:, 5]  # classes\n        boxes, scores = x[:, :4].clone() + c.view(-1, 1) * max_wh, x[:, 4]  # boxes (offset by class), scores\n        i = torchvision.ops.boxes.nms(boxes, scores, iou_thres)\n        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n            try:  # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n                weights = iou * scores[None]  # box weights\n                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n                # i = i[iou.sum(1) > 1]  # require redundancy\n            except:  # possible CUDA error https://github.com/ultralytics/yolov3/issues/1139\n                print(x, i, x.shape, i.shape)\n                pass\n\n        output[xi] = x[i]\n        if (time.time() - t) > time_limit:\n            break  # time limit exceeded\n\n    return output\n\n\ndef get_yolo_layers(model):\n    bool_vec = [x[\'type\'] == \'yolo\' for x in model.module_defs]\n    return [i for i, x in enumerate(bool_vec) if x]  # [82, 94, 106] for yolov3\n\n\ndef print_model_biases(model):\n    # prints the bias neurons preceding each yolo layer\n    print(\'\\nModel Bias Summary: %8s%18s%18s%18s\' % (\'layer\', \'regression\', \'objectness\', \'classification\'))\n    try:\n        multi_gpu = type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)\n        for l in model.yolo_layers:  # print pretrained biases\n            if multi_gpu:\n                na = model.module.module_list[l].na  # number of anchors\n                b = model.module.module_list[l - 1][0].bias.view(na, -1)  # bias 3x85\n            else:\n                na = model.module_list[l].na\n                b = model.module_list[l - 1][0].bias.view(na, -1)  # bias 3x85\n            print(\' \' * 20 + \'%8g %18s%18s%18s\' % (l, \'%5.2f+/-%-5.2f\' % (b[:, :4].mean(), b[:, :4].std()),\n                                                   \'%5.2f+/-%-5.2f\' % (b[:, 4].mean(), b[:, 4].std()),\n                                                   \'%5.2f+/-%-5.2f\' % (b[:, 5:].mean(), b[:, 5:].std())))\n    except:\n        pass\n\n\ndef strip_optimizer(f=\'weights/best.pt\'):  # from utils.utils import *; strip_optimizer()\n    # Strip optimizer from *.pt files for lighter files (reduced by 2/3 size)\n    x = torch.load(f, map_location=torch.device(\'cpu\'))\n    x[\'optimizer\'] = None\n    print(\'Optimizer stripped from %s\' % f)\n    torch.save(x, f)\n\n\ndef create_backbone(f=\'weights/best.pt\'):  # from utils.utils import *; create_backbone()\n    # create a backbone from a *.pt file\n    x = torch.load(f, map_location=torch.device(\'cpu\'))\n    x[\'optimizer\'] = None\n    x[\'training_results\'] = None\n    x[\'epoch\'] = -1\n    for p in x[\'model\'].parameters():\n        p.requires_grad = True\n    s = \'weights/backbone.pt\'\n    print(\'%s saved as %s\' % (f, s))\n    torch.save(x, s)\n\n\ndef coco_class_count(path=\'../coco/labels/train2014/\'):\n    # Histogram of occurrences per class\n    nc = 80  # number classes\n    x = np.zeros(nc, dtype=\'int32\')\n    files = sorted(glob.glob(\'%s/*.*\' % path))\n    for i, file in enumerate(files):\n        labels = np.loadtxt(file, dtype=np.float32).reshape(-1, 5)\n        x += np.bincount(labels[:, 0].astype(\'int32\'), minlength=nc)\n        print(i, len(files))\n\n\ndef coco_only_people(path=\'../coco/labels/train2017/\'):  # from utils.utils import *; coco_only_people()\n    # Find images with only people\n    files = sorted(glob.glob(\'%s/*.*\' % path))\n    for i, file in enumerate(files):\n        labels = np.loadtxt(file, dtype=np.float32).reshape(-1, 5)\n        if all(labels[:, 0] == 0):\n            print(labels.shape[0], file)\n\n\ndef crop_images_random(path=\'../images/\', scale=0.50):  # from utils.utils import *; crop_images_random()\n    # crops images into random squares up to scale fraction\n    # WARNING: overwrites images!\n    for file in tqdm(sorted(glob.glob(\'%s/*.*\' % path))):\n        img = cv2.imread(file)  # BGR\n        if img is not None:\n            h, w = img.shape[:2]\n\n            # create random mask\n            a = 30  # minimum size (pixels)\n            mask_h = random.randint(a, int(max(a, h * scale)))  # mask height\n            mask_w = mask_h  # mask width\n\n            # box\n            xmin = max(0, random.randint(0, w) - mask_w // 2)\n            ymin = max(0, random.randint(0, h) - mask_h // 2)\n            xmax = min(w, xmin + mask_w)\n            ymax = min(h, ymin + mask_h)\n\n            # apply random color mask\n            cv2.imwrite(file, img[ymin:ymax, xmin:xmax])\n\n\ndef coco_single_class_labels(path=\'../coco/labels/train2014/\', label_class=43):\n    # Makes single-class coco datasets. from utils.utils import *; coco_single_class_labels()\n    if os.path.exists(\'new/\'):\n        shutil.rmtree(\'new/\')  # delete output folder\n    os.makedirs(\'new/\')  # make new output folder\n    os.makedirs(\'new/labels/\')\n    os.makedirs(\'new/images/\')\n    for file in tqdm(sorted(glob.glob(\'%s/*.*\' % path))):\n        with open(file, \'r\') as f:\n            labels = np.array([x.split() for x in f.read().splitlines()], dtype=np.float32)\n        i = labels[:, 0] == label_class\n        if any(i):\n            img_file = file.replace(\'labels\', \'images\').replace(\'txt\', \'jpg\')\n            labels[:, 0] = 0  # reset class to 0\n            with open(\'new/images.txt\', \'a\') as f:  # add image to dataset list\n                f.write(img_file + \'\\n\')\n            with open(\'new/labels/\' + Path(file).name, \'a\') as f:  # write label\n                for l in labels[i]:\n                    f.write(\'%g %.6f %.6f %.6f %.6f\\n\' % tuple(l))\n            shutil.copyfile(src=img_file, dst=\'new/images/\' + Path(file).name.replace(\'txt\', \'jpg\'))  # copy images\n\n\ndef kmean_anchors(path=\'./data/coco64.txt\', n=9, img_size=(640, 640), thr=0.20, gen=1000):\n    # Creates kmeans anchors for use in *.cfg files: from utils.utils import *; _ = kmean_anchors()\n    # n: number of anchors\n    # img_size: (min, max) image size used for multi-scale training (can be same values)\n    # thr: IoU threshold hyperparameter used for training (0.0 - 1.0)\n    # gen: generations to evolve anchors using genetic algorithm\n    from utils.datasets import LoadImagesAndLabels\n\n    def print_results(k):\n        k = k[np.argsort(k.prod(1))]  # sort small to large\n        iou = wh_iou(wh, torch.Tensor(k))\n        max_iou = iou.max(1)[0]\n        bpr, aat = (max_iou > thr).float().mean(), (iou > thr).float().mean() * n  # best possible recall, anch > thr\n        print(\'%.2f iou_thr: %.3f best possible recall, %.2f anchors > thr\' % (thr, bpr, aat))\n        print(\'n=%g, img_size=%s, IoU_all=%.3f/%.3f-mean/best, IoU>thr=%.3f-mean: \' %\n              (n, img_size, iou.mean(), max_iou.mean(), iou[iou > thr].mean()), end=\'\')\n        for i, x in enumerate(k):\n            print(\'%i,%i\' % (round(x[0]), round(x[1])), end=\',  \' if i < len(k) - 1 else \'\\n\')  # use in *.cfg\n        return k\n\n    def fitness(k):  # mutation fitness\n        iou = wh_iou(wh, torch.Tensor(k))  # iou\n        max_iou = iou.max(1)[0]\n        return (max_iou * (max_iou > thr).float()).mean()  # product\n\n    # Get label wh\n    wh = []\n    dataset = LoadImagesAndLabels(path, augment=True, rect=True)\n    nr = 1 if img_size[0] == img_size[1] else 10  # number augmentation repetitions\n    for s, l in zip(dataset.shapes, dataset.labels):\n        wh.append(l[:, 3:5] * (s / s.max()))  # image normalized to letterbox normalized wh\n    wh = np.concatenate(wh, 0).repeat(nr, axis=0)  # augment 10x\n    wh *= np.random.uniform(img_size[0], img_size[1], size=(wh.shape[0], 1))  # normalized to pixels (multi-scale)\n    wh = wh[(wh > 2.0).all(1)]  # remove below threshold boxes (< 2 pixels wh)\n\n    # Kmeans calculation\n    from scipy.cluster.vq import kmeans\n    print(\'Running kmeans for %g anchors on %g points...\' % (n, len(wh)))\n    s = wh.std(0)  # sigmas for whitening\n    k, dist = kmeans(wh / s, n, iter=30)  # points, mean distance\n    k *= s\n    wh = torch.Tensor(wh)\n    k = print_results(k)\n\n    # # Plot\n    # k, d = [None] * 20, [None] * 20\n    # for i in tqdm(range(1, 21)):\n    #     k[i-1], d[i-1] = kmeans(wh / s, i)  # points, mean distance\n    # fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n    # ax = ax.ravel()\n    # ax[0].plot(np.arange(1, 21), np.array(d) ** 2, marker=\'.\')\n    # fig, ax = plt.subplots(1, 2, figsize=(14, 7))  # plot wh\n    # ax[0].hist(wh[wh[:, 0]<100, 0],400)\n    # ax[1].hist(wh[wh[:, 1]<100, 1],400)\n    # fig.tight_layout()\n    # fig.savefig(\'wh.png\', dpi=200)\n\n    # Evolve\n    npr = np.random\n    f, sh, mp, s = fitness(k), k.shape, 0.9, 0.1  # fitness, generations, mutation prob, sigma\n    for _ in tqdm(range(gen), desc=\'Evolving anchors\'):\n        v = np.ones(sh)\n        while (v == 1).all():  # mutate until a change occurs (prevent duplicates)\n            v = ((npr.random(sh) < mp) * npr.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0)\n        kg = (k.copy() * v).clip(min=2.0)\n        fg = fitness(kg)\n        if fg > f:\n            f, k = fg, kg.copy()\n            print_results(k)\n    k = print_results(k)\n\n    return k\n\n\ndef print_mutation(hyp, results, bucket=\'\'):\n    # Print mutation results to evolve.txt (for use with train.py --evolve)\n    a = \'%10s\' * len(hyp) % tuple(hyp.keys())  # hyperparam keys\n    b = \'%10.3g\' * len(hyp) % tuple(hyp.values())  # hyperparam values\n    c = \'%10.4g\' * len(results) % results  # results (P, R, mAP, F1, test_loss)\n    print(\'\\n%s\\n%s\\nEvolved fitness: %s\\n\' % (a, b, c))\n\n    if bucket:\n        os.system(\'gsutil cp gs://%s/evolve.txt .\' % bucket)  # download evolve.txt\n\n    with open(\'evolve.txt\', \'a\') as f:  # append result\n        f.write(c + b + \'\\n\')\n    x = np.unique(np.loadtxt(\'evolve.txt\', ndmin=2), axis=0)  # load unique rows\n    np.savetxt(\'evolve.txt\', x[np.argsort(-fitness(x))], \'%10.3g\')  # save sort by fitness\n\n    if bucket:\n        os.system(\'gsutil cp evolve.txt gs://%s\' % bucket)  # upload evolve.txt\n\n\ndef apply_classifier(x, model, img, im0):\n    # applies a second stage classifier to yolo outputs\n    im0 = [im0] if isinstance(im0, np.ndarray) else im0\n    for i, d in enumerate(x):  # per image\n        if d is not None and len(d):\n            d = d.clone()\n\n            # Reshape and pad cutouts\n            b = xyxy2xywh(d[:, :4])  # boxes\n            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square\n            b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad\n            d[:, :4] = xywh2xyxy(b).long()\n\n            # Rescale boxes from img_size to im0 size\n            scale_coords(img.shape[2:], d[:, :4], im0[i].shape)\n\n            # Classes\n            pred_cls1 = d[:, 5].long()\n            ims = []\n            for j, a in enumerate(d):  # per item\n                cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]\n                im = cv2.resize(cutout, (224, 224))  # BGR\n                # cv2.imwrite(\'test%i.jpg\' % j, cutout)\n\n                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32\n                im /= 255.0  # 0 - 255 to 0.0 - 1.0\n                ims.append(im)\n\n            pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction\n            x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections\n\n    return x\n\n\ndef fitness(x):\n    # Returns fitness (for use with results.txt or evolve.txt)\n    w = [0.0, 0.01, 0.99, 0.00]  # weights for [P, R, mAP, F1]@0.5 or [P, R, mAP@0.5, mAP@0.5:0.95]\n    return (x[:, :4] * w).sum(1)\n\n\ndef output_to_target(output, width, height):\n    """"""\n    Convert a YOLO model output to target format\n    [batch_id, class_id, x, y, w, h, conf]\n    """"""\n    if isinstance(output, torch.Tensor):\n        output = output.cpu().numpy()\n\n    targets = []\n    for i, o in enumerate(output):\n        if o is not None:\n            for pred in o:\n                box = pred[:4]\n                w = (box[2] - box[0]) / width\n                h = (box[3] - box[1]) / height\n                x = box[0] / width + w / 2\n                y = box[1] / height + h / 2\n                conf = pred[4]\n                cls = int(pred[5])\n\n                targets.append([i, cls, x, y, w, h, conf])\n\n    return np.array(targets)\n\n\n# Plotting functions ---------------------------------------------------------------------------------------------------\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\n\ndef plot_wh_methods():  # from utils.utils import *; plot_wh_methods()\n    # Compares the two methods for width-height anchor multiplication\n    # https://github.com/ultralytics/yolov3/issues/168\n    x = np.arange(-4.0, 4.0, .1)\n    ya = np.exp(x)\n    yb = torch.sigmoid(torch.from_numpy(x)).numpy() * 2\n\n    fig = plt.figure(figsize=(6, 3), dpi=150)\n    plt.plot(x, ya, \'.-\', label=\'yolo method\')\n    plt.plot(x, yb ** 2, \'.-\', label=\'^2 power method\')\n    plt.plot(x, yb ** 2.5, \'.-\', label=\'^2.5 power method\')\n    plt.xlim(left=-4, right=4)\n    plt.ylim(bottom=0, top=6)\n    plt.xlabel(\'input\')\n    plt.ylabel(\'output\')\n    plt.legend()\n    fig.tight_layout()\n    fig.savefig(\'comparison.png\', dpi=200)\n\n\ndef plot_images(images, targets, paths=None, fname=\'images.jpg\', names=None, max_size=640, max_subplots=16):\n    tl = 3  # line thickness\n    tf = max(tl - 1, 1)  # font thickness\n    if os.path.isfile(fname):  # do not overwrite\n        return None\n\n    if isinstance(images, torch.Tensor):\n        images = images.cpu().numpy()\n\n    if isinstance(targets, torch.Tensor):\n        targets = targets.cpu().numpy()\n\n    # un-normalise\n    if np.max(images[0]) <= 1:\n        images *= 255\n\n    bs, _, h, w = images.shape  # batch size, _, height, width\n    bs = min(bs, max_subplots)  # limit plot images\n    ns = np.ceil(bs ** 0.5)  # number of subplots (square)\n\n    # Check if we should resize\n    scale_factor = max_size / max(h, w)\n    if scale_factor < 1:\n        h = math.ceil(scale_factor * h)\n        w = math.ceil(scale_factor * w)\n\n    # Empty array for output\n    mosaic = np.full((int(ns * h), int(ns * w), 3), 255, dtype=np.uint8)\n\n    # Fix class - colour map\n    prop_cycle = plt.rcParams[\'axes.prop_cycle\']\n    # https://stackoverflow.com/questions/51350872/python-from-color-name-to-rgb\n    hex2rgb = lambda h: tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n    color_lut = [hex2rgb(h) for h in prop_cycle.by_key()[\'color\']]\n\n    for i, img in enumerate(images):\n        if i == max_subplots:  # if last batch has fewer images than we expect\n            break\n\n        block_x = int(w * (i // ns))\n        block_y = int(h * (i % ns))\n\n        img = img.transpose(1, 2, 0)\n        if scale_factor < 1:\n            img = cv2.resize(img, (w, h))\n\n        mosaic[block_y:block_y + h, block_x:block_x + w, :] = img\n        if len(targets) > 0:\n            image_targets = targets[targets[:, 0] == i]\n            boxes = xywh2xyxy(image_targets[:, 2:6]).T\n            classes = image_targets[:, 1].astype(\'int\')\n            gt = image_targets.shape[1] == 6  # ground truth if no conf column\n            conf = None if gt else image_targets[:, 6]  # check for confidence presence (gt vs pred)\n\n            boxes[[0, 2]] *= w\n            boxes[[0, 2]] += block_x\n            boxes[[1, 3]] *= h\n            boxes[[1, 3]] += block_y\n            for j, box in enumerate(boxes.T):\n                cls = int(classes[j])\n                color = color_lut[cls % len(color_lut)]\n                cls = names[cls] if names else cls\n                if gt or conf[j] > 0.3:  # 0.3 conf thresh\n                    label = \'%s\' % cls if gt else \'%s %.1f\' % (cls, conf[j])\n                    plot_one_box(box, mosaic, label=label, color=color, line_thickness=tl)\n\n        # Draw image filename labels\n        if paths is not None:\n            label = os.path.basename(paths[i])[:40]  # trim to 40 char\n            t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n            cv2.putText(mosaic, label, (block_x + 5, block_y + t_size[1] + 5), 0, tl / 3, [220, 220, 220], thickness=tf,\n                        lineType=cv2.LINE_AA)\n\n        # Image border\n        cv2.rectangle(mosaic, (block_x, block_y), (block_x + w, block_y + h), (255, 255, 255), thickness=3)\n\n    if fname is not None:\n        mosaic = cv2.resize(mosaic, (int(ns * w * 0.5), int(ns * h * 0.5)), interpolation=cv2.INTER_AREA)\n        cv2.imwrite(fname, cv2.cvtColor(mosaic, cv2.COLOR_BGR2RGB))\n\n    return mosaic\n\n\ndef plot_lr_scheduler(optimizer, scheduler, epochs=300):\n    # Plot LR simulating training for full epochs\n    optimizer, scheduler = copy(optimizer), copy(scheduler)  # do not modify originals\n    y = []\n    for _ in range(epochs):\n        scheduler.step()\n        y.append(optimizer.param_groups[0][\'lr\'])\n    plt.plot(y, \'.-\', label=\'LR\')\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'LR\')\n    plt.tight_layout()\n    plt.savefig(\'LR.png\', dpi=200)\n\n\ndef plot_test_txt():  # from utils.utils import *; plot_test()\n    # Plot test.txt histograms\n    x = np.loadtxt(\'test.txt\', dtype=np.float32)\n    box = xyxy2xywh(x[:, :4])\n    cx, cy = box[:, 0], box[:, 1]\n\n    fig, ax = plt.subplots(1, 1, figsize=(6, 6), tight_layout=True)\n    ax.hist2d(cx, cy, bins=600, cmax=10, cmin=0)\n    ax.set_aspect(\'equal\')\n    plt.savefig(\'hist2d.png\', dpi=300)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6), tight_layout=True)\n    ax[0].hist(cx, bins=600)\n    ax[1].hist(cy, bins=600)\n    plt.savefig(\'hist1d.png\', dpi=200)\n\n\ndef plot_targets_txt():  # from utils.utils import *; plot_targets_txt()\n    # Plot targets.txt histograms\n    x = np.loadtxt(\'targets.txt\', dtype=np.float32).T\n    s = [\'x targets\', \'y targets\', \'width targets\', \'height targets\']\n    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)\n    ax = ax.ravel()\n    for i in range(4):\n        ax[i].hist(x[i], bins=100, label=\'%.3g +/- %.3g\' % (x[i].mean(), x[i].std()))\n        ax[i].legend()\n        ax[i].set_title(s[i])\n    plt.savefig(\'targets.jpg\', dpi=200)\n\n\ndef plot_labels(labels):\n    # plot dataset labels\n    c, b = labels[:, 0], labels[:, 1:].transpose()  # classees, boxes\n\n    def hist2d(x, y, n=100):\n        xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)\n        hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))\n        xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)\n        yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)\n        return hist[xidx, yidx]\n\n    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)\n    ax = ax.ravel()\n    ax[0].hist(c, bins=int(c.max() + 1))\n    ax[0].set_xlabel(\'classes\')\n    ax[1].scatter(b[0], b[1], c=hist2d(b[0], b[1], 90), cmap=\'jet\')\n    ax[1].set_xlabel(\'x\')\n    ax[1].set_ylabel(\'y\')\n    ax[2].scatter(b[2], b[3], c=hist2d(b[2], b[3], 90), cmap=\'jet\')\n    ax[2].set_xlabel(\'width\')\n    ax[2].set_ylabel(\'height\')\n    plt.savefig(\'labels.png\', dpi=200)\n\n\ndef plot_evolution_results(hyp):  # from utils.utils import *; plot_evolution_results(hyp)\n    # Plot hyperparameter evolution results in evolve.txt\n    x = np.loadtxt(\'evolve.txt\', ndmin=2)\n    f = fitness(x)\n    # weights = (f - f.min()) ** 2  # for weighted results\n    fig = plt.figure(figsize=(12, 10), tight_layout=True)\n    matplotlib.rc(\'font\', **{\'size\': 8})\n    for i, (k, v) in enumerate(hyp.items()):\n        y = x[:, i + 7]\n        # mu = (y * weights).sum() / weights.sum()  # best weighted result\n        mu = y[f.argmax()]  # best single result\n        plt.subplot(4, 5, i + 1)\n        plt.plot(mu, f.max(), \'o\', markersize=10)\n        plt.plot(y, f, \'.\')\n        plt.title(\'%s = %.3g\' % (k, mu), fontdict={\'size\': 9})  # limit to 40 characters\n        print(\'%15s: %.3g\' % (k, mu))\n    plt.savefig(\'evolve.png\', dpi=200)\n\n\ndef plot_results_overlay(start=0, stop=0):  # from utils.utils import *; plot_results_overlay()\n    # Plot training results files \'results*.txt\', overlaying train and val losses\n    s = [\'train\', \'train\', \'train\', \'Precision\', \'mAP@0.5\', \'val\', \'val\', \'val\', \'Recall\', \'F1\']  # legends\n    t = [\'GIoU\', \'Objectness\', \'Classification\', \'P-R\', \'mAP-F1\']  # titles\n    for f in sorted(glob.glob(\'results*.txt\') + glob.glob(\'../../Downloads/results*.txt\')):\n        results = np.loadtxt(f, usecols=[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin=2).T\n        n = results.shape[1]  # number of rows\n        x = range(start, min(stop, n) if stop else n)\n        fig, ax = plt.subplots(1, 5, figsize=(14, 3.5), tight_layout=True)\n        ax = ax.ravel()\n        for i in range(5):\n            for j in [i, i + 5]:\n                y = results[j, x]\n                if i in [0, 1, 2]:\n                    y[y == 0] = np.nan  # dont show zero loss values\n                ax[i].plot(x, y, marker=\'.\', label=s[j])\n            ax[i].set_title(t[i])\n            ax[i].legend()\n            ax[i].set_ylabel(f) if i == 0 else None  # add filename\n        fig.savefig(f.replace(\'.txt\', \'.png\'), dpi=200)\n\n\ndef plot_results(start=0, stop=0, bucket=\'\', id=()):  # from utils.utils import *; plot_results()\n    # Plot training \'results*.txt\' as seen in https://github.com/ultralytics/yolov3#training\n    fig, ax = plt.subplots(2, 5, figsize=(12, 6), tight_layout=True)\n    ax = ax.ravel()\n    s = [\'GIoU\', \'Objectness\', \'Classification\', \'Precision\', \'Recall\',\n         \'val GIoU\', \'val Objectness\', \'val Classification\', \'mAP@0.5\', \'F1\']\n    if bucket:\n        os.system(\'rm -rf storage.googleapis.com\')\n        files = [\'https://storage.googleapis.com/%s/results%g.txt\' % (bucket, x) for x in id]\n    else:\n        files = glob.glob(\'results*.txt\') + glob.glob(\'../../Downloads/results*.txt\')\n    for f in sorted(files):\n        try:\n            results = np.loadtxt(f, usecols=[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin=2).T\n            n = results.shape[1]  # number of rows\n            x = range(start, min(stop, n) if stop else n)\n            for i in range(10):\n                y = results[i, x]\n                if i in [0, 1, 2, 5, 6, 7]:\n                    y[y == 0] = np.nan  # dont show zero loss values\n                    # y /= y[0]  # normalize\n                ax[i].plot(x, y, marker=\'.\', label=Path(f).stem, linewidth=2, markersize=8)\n                ax[i].set_title(s[i])\n                # if i in [5, 6, 7]:  # share train and val loss y axes\n                #     ax[i].get_shared_y_axes().join(ax[i], ax[i - 5])\n        except:\n            print(\'Warning: Plotting error for %s, skipping file\' % f)\n\n    ax[1].legend()\n    fig.savefig(\'results.png\', dpi=200)\n'"
