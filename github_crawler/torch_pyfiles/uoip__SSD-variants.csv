file_path,api_count,code
evaluation.py,0,"b'# from https://github.com/chainer/chainercv/blob/master/chainercv/evaluations/eval_detection_voc.py\n\n\nfrom collections import defaultdict \nimport itertools \nimport numpy as np\n\nfrom collections import defaultdict\nimport itertools\nimport numpy as np\nimport six\n\n\ndef jaccard(a, b):  \n    # pairwise jaccard(IoU) botween boxes a and boxes b\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n    inter = np.clip(rb - lt, 0, None)\n\n    area_i = np.prod(inter, axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n\n    area_u = area_a[:, np.newaxis] + area_b - area_i\n    return area_i / np.clip(area_u, 1e-7, None)   # len(a) x len(b)\n\n\n\n\ndef eval_voc_detection(\n    pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels,\n    gt_difficults=None,\n    iou_thresh=0.5, use_07_metric=False):\n\n    prec, rec = calc_detection_voc_prec_rec(\n        pred_bboxes, pred_labels, pred_scores,\n        gt_bboxes, gt_labels, gt_difficults,\n        iou_thresh=iou_thresh)\n\n    ap = calc_detection_voc_ap(prec, rec, use_07_metric=use_07_metric)\n\n    return {\'ap\': ap, \'map\': np.nanmean(ap)}\n\n\ndef calc_detection_voc_prec_rec(\n        pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels,\n        gt_difficults=None,\n        iou_thresh=0.5):\n    """"""Calculate precision and recall based on evaluation code of PASCAL VOC.\n    This function calculates precision and recall of\n    predicted bounding boxes obtained from a dataset which has :math:`N`\n    images.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n    Args:\n        pred_bboxes (iterable of numpy.ndarray): An iterable of :math:`N`\n            sets of bounding boxes.\n            Its index corresponds to an index for the base dataset.\n            Each element of :obj:`pred_bboxes` is a set of coordinates\n            of bounding boxes. This is an array whose shape is :math:`(R, 4)`,\n            where :math:`R` corresponds\n            to the number of bounding boxes, which may vary among boxes.\n            The second axis corresponds to :obj:`y_min, x_min, y_max, x_max`\n            of a bounding box.\n        pred_labels (iterable of numpy.ndarray): An iterable of labels.\n            Similar to :obj:`pred_bboxes`, its index corresponds to an\n            index for the base dataset. Its length is :math:`N`.\n        pred_scores (iterable of numpy.ndarray): An iterable of confidence\n            scores for predicted bounding boxes. Similar to :obj:`pred_bboxes`,\n            its index corresponds to an index for the base dataset.\n            Its length is :math:`N`.\n        gt_bboxes (iterable of numpy.ndarray): An iterable of ground truth\n            bounding boxes\n            whose length is :math:`N`. An element of :obj:`gt_bboxes` is a\n            bounding box whose shape is :math:`(R, 4)`. Note that the number of\n            bounding boxes in each image does not need to be same as the number\n            of corresponding predicted boxes.\n        gt_labels (iterable of numpy.ndarray): An iterable of ground truth\n            labels which are organized similarly to :obj:`gt_bboxes`.\n        gt_difficults (iterable of numpy.ndarray): An iterable of boolean\n            arrays which is organized similarly to :obj:`gt_bboxes`.\n            This tells whether the\n            corresponding ground truth bounding box is difficult or not.\n            By default, this is :obj:`None`. In that case, this function\n            considers all bounding boxes to be not difficult.\n        iou_thresh (float): A prediction is correct if its Intersection over\n            Union with the ground truth is above this value..\n    Returns:\n        tuple of two lists:\n        This function returns two lists: :obj:`prec` and :obj:`rec`.\n        * :obj:`prec`: A list of arrays. :obj:`prec[l]` is precision \\\n            for class :math:`l`. If class :math:`l` does not exist in \\\n            either :obj:`pred_labels` or :obj:`gt_labels`, :obj:`prec[l]` is \\\n            set to :obj:`None`.\n        * :obj:`rec`: A list of arrays. :obj:`rec[l]` is recall \\\n            for class :math:`l`. If class :math:`l` that is not marked as \\\n            difficult does not exist in \\\n            :obj:`gt_labels`, :obj:`rec[l]` is \\\n            set to :obj:`None`.\n    """"""\n\n    pred_bboxes = iter(pred_bboxes)\n    pred_labels = iter(pred_labels)\n    pred_scores = iter(pred_scores)\n    gt_bboxes = iter(gt_bboxes)\n    gt_labels = iter(gt_labels)\n    if gt_difficults is None:\n        gt_difficults = itertools.repeat(None)\n    else:\n        gt_difficults = iter(gt_difficults)\n\n    n_pos = defaultdict(int)\n    score = defaultdict(list)\n    match = defaultdict(list)\n\n    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label, gt_difficult in \\\n        six.moves.zip(\n            pred_bboxes, pred_labels, pred_scores,\n            gt_bboxes, gt_labels, gt_difficults):\n\n        if gt_difficult is None:\n            gt_difficult = np.zeros(gt_bbox.shape[0], dtype=bool)\n\n        for l in np.unique(np.concatenate((pred_label, gt_label)).astype(int)):\n            pred_mask_l = pred_label == l\n            pred_bbox_l = pred_bbox[pred_mask_l]\n            pred_score_l = pred_score[pred_mask_l]\n            # sort by score\n            order = pred_score_l.argsort()[::-1]\n            pred_bbox_l = pred_bbox_l[order]\n            pred_score_l = pred_score_l[order]\n\n            gt_mask_l = gt_label == l\n            gt_bbox_l = gt_bbox[gt_mask_l]\n            gt_difficult_l = gt_difficult[gt_mask_l]\n\n            n_pos[l] += np.logical_not(gt_difficult_l).sum()\n            score[l].extend(pred_score_l)\n\n            if len(pred_bbox_l) == 0:\n                continue\n            if len(gt_bbox_l) == 0:\n                match[l].extend((0,) * pred_bbox_l.shape[0])\n                continue\n\n            # VOC evaluation follows integer typed bounding boxes.\n            pred_bbox_l = pred_bbox_l.copy()\n            #pred_bbox_l[:, 2:] += 1\n            gt_bbox_l = gt_bbox_l.copy()\n            #gt_bbox_l[:, 2:] += 1\n\n            iou = jaccard(pred_bbox_l, gt_bbox_l)\n            gt_index = iou.argmax(axis=1)\n            # set -1 if there is no matching ground truth\n            gt_index[iou.max(axis=1) < iou_thresh] = -1\n            del iou\n\n            selec = np.zeros(gt_bbox_l.shape[0], dtype=bool)\n            for gt_idx in gt_index:\n                if gt_idx >= 0:\n                    if gt_difficult_l[gt_idx]:\n                        match[l].append(-1)\n                    else:\n                        if not selec[gt_idx]:\n                            match[l].append(1)\n                        else:\n                            match[l].append(0)\n                    selec[gt_idx] = True\n                else:\n                    match[l].append(0)\n\n    for iter_ in (\n            pred_bboxes, pred_labels, pred_scores,\n            gt_bboxes, gt_labels, gt_difficults):\n        if next(iter_, None) is not None:\n            raise ValueError(\'Length of input iterables need to be same.\')\n\n    n_fg_class = max(n_pos.keys()) + 1\n    prec = [None] * n_fg_class\n    rec = [None] * n_fg_class\n\n    for l in n_pos.keys():\n        score_l = np.array(score[l])\n        match_l = np.array(match[l], dtype=np.int8)\n\n        order = score_l.argsort()[::-1]\n        match_l = match_l[order]\n\n        tp = np.cumsum(match_l == 1)\n        fp = np.cumsum(match_l == 0)\n\n        # If an element of fp + tp is 0,\n        # the corresponding element of prec[l] is nan.\n        prec[l] = tp / (fp + tp)\n        # If n_pos[l] is 0, rec[l] is None.\n        if n_pos[l] > 0:\n            rec[l] = tp / n_pos[l]\n\n    return prec, rec\n\n\ndef calc_detection_voc_ap(prec, rec, use_07_metric=False):\n    """"""Calculate average precisions based on evaluation code of PASCAL VOC.\n    This function calculates average precisions\n    from given precisions and recalls.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n    Args:\n        prec (list of numpy.array): A list of arrays.\n            :obj:`prec[l]` indicates precision for class :math:`l`.\n            If :obj:`prec[l]` is :obj:`None`, this function returns\n            :obj:`numpy.nan` for class :math:`l`.\n        rec (list of numpy.array): A list of arrays.\n            :obj:`rec[l]` indicates recall for class :math:`l`.\n            If :obj:`rec[l]` is :obj:`None`, this function returns\n            :obj:`numpy.nan` for class :math:`l`.\n        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n            for calculating average precision. The default value is\n            :obj:`False`.\n    Returns:\n        ~numpy.ndarray:\n        This function returns an array of average precisions.\n        The :math:`l`-th value corresponds to the average precision\n        for class :math:`l`. If :obj:`prec[l]` or :obj:`rec[l]` is\n        :obj:`None`, the corresponding value is set to :obj:`numpy.nan`.\n    """"""\n\n    n_fg_class = len(prec)\n    ap = np.empty(n_fg_class)\n    for l in six.moves.range(n_fg_class):\n        if prec[l] is None or rec[l] is None:\n            ap[l] = np.nan\n            continue\n\n        if use_07_metric:\n            # 11 point metric\n            ap[l] = 0\n            for t in np.arange(0., 1.1, 0.1):\n                if np.sum(rec[l] >= t) == 0:\n                    p = 0\n                else:\n                    p = np.max(np.nan_to_num(prec[l])[rec[l] >= t])\n                ap[l] += p / 11\n        else:\n            # correct AP calculation\n            # first append sentinel values at the end\n            mpre = np.concatenate(([0], np.nan_to_num(prec[l]), [0]))\n            mrec = np.concatenate(([0], rec[l], [1]))\n\n            mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n\n            # to calculate area under PR curve, look for points\n            # where X axis (recall) changes value\n            i = np.where(mrec[1:] != mrec[:-1])[0]\n\n            # and sum (\\Delta recall) * prec\n            ap[l] = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n\n    return ap'"
loss.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\n\ndef log_sum_exp(x, dim, keepdim=False):\n    x_max = x.max(dim=dim, keepdim=True)[0]\n\n    if keepdim:\n        return (x - x_max).exp().sum(dim=dim, keepdim=True).log() + x_max\n    else:\n        return (x - x_max).exp().sum(dim=dim).log() + x_max.squeeze(dim)\n\n\ndef _softmax_cross_entropy_with_logits(x, t):\n    assert x.size()[:-1] == t.size()\n    xt = torch.gather(x, -1, t.long().unsqueeze(-1))\n    return log_sum_exp(x, dim=-1, keepdim=False) - xt.squeeze(-1)\n    \n\n\nclass MultiBoxLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def _hard_negative_mining(self, loss, pos, neg, k):\n        loss = loss.detach()\n        rank = (loss * (-1 * neg.float())).sort(dim=1)[1].sort(dim=1)[1]\n        hard_neg = rank < (pos.long().sum(dim=1, keepdim=True) * k)\n        return hard_neg\n\n    def forward(self, xloc, xconf, loc, label, k=3):   # xconf is logits\n        pos = label > 0\n        neg = label == 0\n        label = label.clamp(min=0)\n\n        pos_idx = pos.unsqueeze(-1).expand_as(xloc)\n        loc_loss = F.smooth_l1_loss(xloc[pos_idx].view(-1, 4), loc[pos_idx].view(-1, 4), \n                                    size_average=False) \n        \n        conf_loss = _softmax_cross_entropy_with_logits(xconf, label)\n        hard_neg = self._hard_negative_mining(conf_loss, pos, neg, k)\n        conf_loss = conf_loss * (pos + hard_neg).gt(0).float()\n        conf_loss = conf_loss.sum()\n\n        N = pos.data.float().sum() + 1e-3#.clamp(min=1e-3)\n        return loc_loss / N, conf_loss / N\n\n\n\n\n\ndef _softmax_focal_loss(x, t, gamma=2):\n    assert x.size()[:-1] == t.size()\n    logp = torch.gather(F.log_softmax(x), -1, t.long().unsqueeze(-1))\n    FL = - (1 - logp.exp()).pow(gamma) * logp\n    return FL.sum()\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n        self.count = 0\n        self.multiloss = MultiBoxLoss()\n\n    def forward(self, xloc, xconf, loc, label):\n        pos = label > 0\n        neg = label == 0\n\n        pos_idx = pos.unsqueeze(-1).expand_as(xloc)\n        loc_loss = F.smooth_l1_loss(xloc[pos_idx].view(-1, 4), loc[pos_idx].view(-1, 4), \n                                    size_average=False) \n\n        pos_idx = pos.unsqueeze(-1).expand_as(xconf)\n        pos_conf_loss = _softmax_focal_loss(xconf[pos_idx].view(-1, xconf.size(-1)), label[pos])\n        neg_idx = neg.unsqueeze(-1).expand_as(xconf)\n        neg_conf_loss = _softmax_focal_loss(xconf[neg_idx].view(-1, xconf.size(-1)), label[neg])\n\n        conf_loss = self.alpha * pos_conf_loss + (1 - self.alpha) * neg_conf_loss\n\n        self.count += 1\n        if self.count % 1000 == 0:\n            print('pos loss, neg loss', pos_conf_loss.data, neg_conf_loss.data)\n            print('multiloss', self.multiloss(xloc, xconf, loc, label, 3))\n        \n        N = pos.float().sum().clamp(min=1e-3)\n        return loc_loss / N, conf_loss / N\n\n\n\n\n# class SigmoidFocalLoss(nn.Module):\n#     def __init__(self, alpha=0.25, scale=4.):\n#         super().__init__()\n#         self.alpha = alpha\n#         self.scale = scale\n#         self.onehot = None\n\n#     def forward(self, xloc, xconf, loc, label):\n#         pos = label > 0\n#         neg = label == 0\n\n#         # loc\n#         pos_idx = pos.unsqueeze(-1).expand_as(xloc)\n#         loc_loss = F.smooth_l1_loss(xloc[pos_idx].view(-1, 4), loc[pos_idx].view(-1, 4), \n#                                     size_average=False) \n\n#         # conf\n#         if self.onehot is None or self.onehot.size() != xconf.size():\n#             self.onehot = Variable(torch.zeros(xconf.size())).detach()\n#             if xconf.is_cuda:\n#                 self.onehot = self.onehot.cuda()\n#         self.onehot.data.fill_(0)\n#         self.onehot.data.scatter_(-1, label.data.clamp(min=0).long().unsqueeze(-1), 1)\n\n#         pos_idx = pos.unsqueeze(-1).expand_as(xconf)\n#         pos_conf_loss = F.multilabel_soft_margin_loss(\n#             self.scale * xconf[pos_idx].view(-1, xconf.size(-1)), \n#             self.onehot[pos_idx].view(-1, xconf.size(-1)), \n#             size_average=False)\n#         neg_idx = neg.unsqueeze(-1).expand_as(xconf)\n#         neg_conf_loss = F.multilabel_soft_margin_loss(\n#             self.scale * xconf[neg_idx].view(-1, xconf.size(-1)), \n#             self.onehot[neg_idx].view(-1, xconf.size(-1)), \n#             size_average=False)\n\n#         conf_loss = self.alpha * pos_conf_loss + (1 - self.alpha) * neg_conf_loss\n\n#         N = pos.float().sum().clamp(min=1e-3)\n#         return loc_loss / N, conf_loss / N\n"""
multibox.py,3,"b""\nimport numpy as np\nimport torch\n\nimport itertools\nfrom numbers import Number\n\n\n\nclass MultiBox(object):\n    def __init__(self, cfg):\n        self.pos_thresh = cfg.get('pos_thresh', 0.5)\n        self.neg_thresh = cfg.get('neg_thresh', 0.5)\n        self.prior_variance = cfg.get('prior_variance', [0.1, 0.1, 0.2, 0.2])\n\n        steps = cfg.get('steps', None)\n        grids = cfg['grids']\n        sizes = cfg['sizes']\n        aspect_ratios = cfg['aspect_ratios']\n        if isinstance(aspect_ratios[0], Number):\n            aspect_ratios = [aspect_ratios] * len(grids)\n\n        anchor_boxes = []\n        for k in range(len(grids)):\n            w, h = (grids[k], grids[k]) if isinstance(grids[k], Number) else grids[k]\n            if steps is None:\n                step_w, step_h = 1. / w, 1. / h\n            else:\n                step_w, step_h = (steps[k], steps[k]) if isinstance(steps[k], Number) else steps[k]\n\n            for u, v in itertools.product(range(h), range(w)):  # mind the order\n                cx = (v + 0.5) * step_w\n                cy = (u + 0.5) * step_h\n\n                s = np.sqrt(sizes[k] * sizes[k+1])\n                anchor_boxes.append([cx, cy, s, s])\n\n                s = sizes[k]\n                for ar in aspect_ratios[k]:\n                    anchor_boxes.append([cx, cy, s * np.sqrt(ar), s * np.sqrt(1. / ar)])\n\n        self.anchor_boxes = np.array(anchor_boxes)      # x-y-w-h\n        self.anchor_boxes_ = np.hstack([                # l-t-r-b, normalized\n            self.anchor_boxes[:, :2] - self.anchor_boxes[:, 2:] / 2,\n            self.anchor_boxes[:, :2] + self.anchor_boxes[:, 2:] / 2])   # do NOT clip\n\n\n    def encode(self, boxes, labels):\n        if len(boxes) == 0:\n            return (\n                torch.FloatTensor(np.zeros(self.anchor_boxes.shape, dtype=np.float32)),\n                torch.LongTensor(np.zeros(self.anchor_boxes.shape[0], dtype=np.int)))\n\n        iou = batch_iou(self.anchor_boxes_, boxes) \n        idx = iou.argmax(axis=1)\n\n        # ensure each target box correspondes to at least one anchor box \n        iouc = iou.copy()\n        for _ in range(len(boxes)):\n            i, j = np.unravel_index(iouc.argmax(), iouc.shape)\n            if iouc[i, j] < 0.1:\n                continue\n            iouc[i, :] = 0\n            iouc[:, j] = 0\n\n            idx[i] = j \n            iou[i, j] = 1.\n        iou = iou.max(axis=1)\n\n        boxes = boxes[idx]\n        loc = np.hstack([\n                ((boxes[:, :2] + boxes[:, 2:]) / 2. - self.anchor_boxes[:, :2]) / self.anchor_boxes[:, 2:],\n                np.log((boxes[:, 2:] - boxes[:, :2]) / self.anchor_boxes[:, 2:]),\n                ]) / self.prior_variance\n        \n        labels = labels[idx]\n        labels = 1 + labels\n        labels[iou < self.neg_thresh] = 0\n        labels[(self.neg_thresh <= iou) & (iou < self.pos_thresh)] = -1   # ignored during training\n\n        return torch.FloatTensor(loc.astype(np.float32)), torch.LongTensor(labels.astype(np.int))\n\n    \n    def decode(self, loc, conf, nms_thresh=0.5, conf_thresh=0.5):\n        loc = loc * self.prior_variance\n        boxes = np.hstack([\n                    loc[:, :2] * self.anchor_boxes[:, 2:] + self.anchor_boxes[:, :2],\n                    np.exp(loc[:, 2:]) * self.anchor_boxes[:, 2:]])\n        boxes[:, :2], boxes[:, 2:] = (boxes[:, :2] - boxes[:, 2:] / 2., \n                                      boxes[:, :2] + boxes[:, 2:] / 2.)\n        boxes = np.clip(boxes, 0, 1)\n\n        conf = np.exp(conf)\n        conf /= conf.sum(axis=-1, keepdims=True)\n        scores = conf[:, 1:]\n\n        chosen = np.zeros(len(scores), dtype=bool)\n        for i in range(scores.shape[1]):\n            keep = nms(boxes, scores[:, i], nms_thresh, conf_thresh)\n            scores[:, i] *= keep\n            chosen |= keep\n\n        chosen &= (-scores.max(axis=1)).argsort().argsort() < 200\n        return boxes[chosen], scores.argmax(axis=1)[chosen], scores.max(axis=1)[chosen]\n\n\n\n\n\n\ndef batch_iou(a, b):  \n    # pairwise jaccard botween boxes a and boxes b\n    # box: [left, top, right, bottom]\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n    inter = np.clip(rb - lt, 0, None)\n\n    area_i = np.prod(inter, axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n\n    area_u = area_a[:, np.newaxis] + area_b - area_i\n    return area_i / np.clip(area_u, 1e-7, None)  # shape: (len(a) x len(b))\n\n\ndef nms(boxes, scores, nms_thresh=0.45, conf_thresh=0, topk=400, topk_after=50):\n    Keep = np.zeros(len(scores), dtype=bool)\n    idx =  (scores >= conf_thresh) & ((-scores).argsort().argsort() < topk)\n    if idx.sum() == 0:\n        return Keep\n\n    boxes = boxes[idx]\n    scores = scores[idx]\n\n    iou = batch_iou(boxes, boxes)\n    keep = np.zeros(len(scores), dtype=bool)\n    keep[scores.argmax()] = True\n    for i in scores.argsort()[::-1]:\n        if (iou[i, keep] < nms_thresh).all():\n            keep[i] = True\n            #if keep.sum() >= topk_after:\n            #    break\n\n    Keep[idx] = keep\n    return Keep\n\n\n\n\n# def soft_nms(boxes, scores, sigma=0.5, Nt=0.3, thresh=0.001, method=1):\n#     num = len(scores)\n#     keep = np.zeros(num, dtype=bool)\n#     for _ in range(num):\n#         i = np.argmax(scores)\n#         if scores[i] < thresh:\n#             break\n#         keep[i] = True\n\n#         iou = batch_iou(boxes[np.newaxis, i], boxes).reshape(-1)\n\n#         if method == 1:   # linear\n#             weight = np.ones_like(iou) * (1 - iou)\n#             weight[iou <= Nt] = 1\n#         elif method == 2:   # gaussian\n#             weight = np.exp(-(iou * iou)/sigma)\n#         else:   # original\n#             weight = np.zeros_like(iou)\n#             weight[iou <= Nt] = 1\n\n#         scores = scores * weight\n#         scores[i] = 0\n#     return keep"""
pascal_voc.py,3,"b""# https://github.com/amdegroot/ssd.pytorch/blob/master/data/voc0712.py\r\n# https://github.com/fmassa/vision/blob/voc_dataset/torchvision/datasets/voc.py\r\n\r\nimport os\r\nimport cv2\r\nimport numpy as np \r\n\r\nimport sys\r\nimport xml.etree.ElementTree as ET \r\n\r\nimport torch.utils.data\r\n\r\n\r\n\r\nclass VOC(object):\r\n    N_CLASSES = 20\r\n    CLASSES = (\r\n        'aeroplane', 'bicycle', 'bird', 'boat',\r\n        'bottle', 'bus', 'car', 'cat', 'chair',\r\n        'cow', 'diningtable', 'dog', 'horse',\r\n        'motorbike', 'person', 'pottedplant',\r\n        'sheep', 'sofa', 'train', 'tvmonitor',\r\n    )\r\n\r\n    MEAN = [123.68, 116.779, 103.939]   # R,G,B\r\n\r\n    label_to_id = dict(map(reversed, enumerate(CLASSES))) \r\n    id_to_label = dict(enumerate(CLASSES)) \r\n\r\n\r\n\r\n\r\nclass Viz(object):\r\n    def __init__(self):\r\n        voc = VOC()\r\n\r\n        classes = voc.CLASSES\r\n\r\n        self.id_to_label = voc.id_to_label\r\n        self.label_to_id = voc.label_to_id\r\n\r\n        colors = {}\r\n        for label in classes:\r\n            id = self.label_to_id[label]\r\n            color = self._to_color(id, len(classes))\r\n            colors[id] = color\r\n            colors[label] = color\r\n        self.colors =colors\r\n\r\n    def _to_color(self, indx, n_classes):\r\n        base = int(np.ceil(pow(n_classes, 1./3)))\r\n        base2 = base * base\r\n        b = 2 - indx / base2\r\n        r = 2 - (indx % base2) / base\r\n        g = 2 - (indx % base2) % base\r\n        #return (b * 127, r * 127, g * 127)\r\n        return (r * 127, g * 127, b * 127)\r\n\r\n    def draw_bbox(self, img, bboxes, labels, relative=False):\r\n        if len(labels) == 0:\r\n            return img\r\n        img = img.copy()\r\n        h, w = img.shape[:2]\r\n\r\n        if relative:\r\n            bboxes = bboxes * [w, h, w, h]\r\n\r\n        bboxes = bboxes.astype(np.int)\r\n        labels = labels.astype(np.int)\r\n\r\n        for bbox, label in zip(bboxes, labels):\r\n            left, top, right, bot = bbox\r\n            color = self.colors[label]\r\n            label = self.id_to_label[label]\r\n            cv2.rectangle(img, (left, top), (right, bot), color, 2)\r\n            #img[max(0,top-18):min(h+1,top+2), max(0,left-2):min(left + len(label)*7+5,w+1)] = 15\r\n            cv2.putText(img, label, (left+1, top-5), cv2.FONT_HERSHEY_DUPLEX, 0.4, color, 1, cv2.LINE_AA)\r\n\r\n        return img\r\n\r\n    def blend_segmentation(self, img, target):\r\n        mask = (target.max(axis=2) > 0)[..., np.newaxis] * 1.\r\n        blend = img * 0.3 +  target * 0.7\r\n        \r\n        img = (1 - mask) * img + mask * blend\r\n        return img.astype('uint8')\r\n\r\n\r\n\r\nclass ParseAnnotation(object):\r\n    def __init__(self, keep_difficult=True):\r\n        self.keep_difficult = keep_difficult\r\n\r\n        voc = VOC()\r\n        self.label_to_id = voc.label_to_id\r\n        self.classes = voc.CLASSES\r\n\r\n    def __call__(self, target):\r\n        tree = ET.parse(target).getroot()\r\n\r\n        bboxes = []\r\n        labels = []\r\n        for obj in tree.iter('object'):\r\n            difficult = int(obj.find('difficult').text) == 1\r\n            if not self.keep_difficult and difficult:\r\n                continue\r\n\r\n            label = obj.find('name').text.lower().strip()\r\n            if label not in self.classes:\r\n                continue\r\n            label = self.label_to_id[label]\r\n\r\n            bndbox = obj.find('bndbox')\r\n            bbox = [int(bndbox.find(_).text) - 1 for _ in ['xmin', 'ymin', 'xmax', 'ymax']]\r\n\r\n            bboxes.append(bbox)\r\n            labels.append(label)\r\n\r\n        return np.array(bboxes), np.array(labels)\r\n\r\n\r\n\r\n#class VOCDetection(object):\r\nclass VOCDetection(torch.utils.data.Dataset):\r\n    def __init__(self, root, image_set, keep_difficult=False, transform=None, target_transform=None):\r\n        self.root = root\r\n        self.image_set = image_set\r\n        self.transform = transform\r\n        self.target_transform = target_transform\r\n\r\n        self._imgpath = os.path.join('%s', 'JPEGImages', '%s.jpg')\r\n        self._annopath = os.path.join('%s', 'Annotations', '%s.xml')\r\n\r\n        self.parse_annotation = ParseAnnotation(keep_difficult=keep_difficult)\r\n\r\n        self.ids = []\r\n        for year, split in image_set:\r\n            basepath = os.path.join(self.root, 'VOC' + str(year))\r\n            path = os.path.join(basepath, 'ImageSets', 'Main')\r\n            for file in os.listdir(path):\r\n                if not file.endswith('_' + split + '.txt'):\r\n                    continue\r\n                with open(os.path.join(path, file)) as f:\r\n                    for line in f:\r\n                        self.ids.append((basepath, line.strip()[:-3]))\r\n\r\n        self.ids = sorted(list(set(self.ids)), key=lambda _:_[0]+_[1])  # deterministic \r\n\r\n    def __getitem__(self, index):\r\n        img_id = self.ids[index]\r\n\r\n        img = cv2.imread(self._imgpath % img_id)[:, :, ::-1]\r\n        bboxes, labels = self.parse_annotation(self._annopath % img_id)\r\n\r\n        if self.transform is not None:\r\n            img, bboxes = self.transform(img, bboxes)\r\n\r\n        bboxes, labels = self.filter(img, bboxes, labels)\r\n        if self.target_transform is not None:\r\n            bboxes, labels = self.target_transform(bboxes, labels)\r\n        return img, bboxes, labels\r\n\r\n\r\n    def __len__(self):\r\n        return len(self.ids)\r\n\r\n    def filter(self, img, boxes, labels):\r\n        shape = img.shape\r\n        if len(shape) == 2:\r\n            h, w = shape\r\n        else:   # !!\r\n            if shape[0] > shape[2]:   # HWC\r\n                h, w = img.shape[:2]\r\n            else:                     # CHW\r\n                h, w = img.shape[1:]\r\n\r\n        boxes_ = []\r\n        labels_ = []\r\n        for box, label in zip(boxes, labels):\r\n            if min(box[2] - box[0], box[3] - box[1]) <= 0:\r\n                continue\r\n            if np.max(boxes) < 1 and np.sqrt((box[2] - box[0]) * w * (box[3] - box[1]) * h) < 8:\r\n                #if np.max(boxes) < 1 and min((box[2] - box[0]) * w, (box[3] - box[1]) * h) < 5:\r\n                continue\r\n            boxes_.append(box)\r\n            labels_.append(label)\r\n        return np.array(boxes_), np.array(labels_)\r\n\r\n\r\n\r\n#class VOCSegmentation(object):\r\nclass VOCSegmentation(torch.utils.data.Dataset):\r\n    def __init__(self, root, image_set, instance=False, transform=None):\r\n        self.root = root\r\n        self.image_set = image_set\r\n        self.instance = instance\r\n        self.transform = transform\r\n\r\n        self._imgpath = os.path.join('%s', 'JPEGImages', '%s.jpg')\r\n\r\n        if self.instance:   # instance segmentation\r\n            self._segpath = os.path.join('%s', 'SegmentationObject', '%s.png')\r\n        else:               # semantic segmentation\r\n            self._segpath = os.path.join('%s', 'SegmentationClass', '%s.png')\r\n\r\n        self.ids = []\r\n        for year, split in image_set:\r\n            basepath = os.path.join(root, 'VOC' + str(year))\r\n            path = os.path.join(basepath, 'ImageSets', 'Segmentation')\r\n            for file in os.listdir(path):\r\n                if (split + '.txt') != file:\r\n                    continue\r\n                with open(os.path.join(path, file)) as f:\r\n                    for line in f:\r\n                        self.ids.append((basepath, line.strip()))\r\n\r\n\r\n    def __getitem__(self, index):\r\n        img_id = self.ids[index]\r\n\r\n        img = cv2.imread(self._imgpath % img_id)[:,:,::-1]\r\n        target = cv2.imread(self._segpath % img_id)[:,:,::-1]\r\n\r\n        if self.transform is not None:\r\n            img, target = self.transform([img, target])\r\n\r\n        return img, target\r\n\r\n    def __len__(self):\r\n        return len(self.ids)"""
train.py,9,"b'import os\r\nimport time\r\nimport argparse\r\nimport random\r\nimport numpy as np \r\nfrom numpy.random import RandomState\r\nimport cv2\r\n\r\nimport torch\r\nimport torch.optim as optim\r\nimport torch.backends.cudnn as cudnn\r\nfrom torch.autograd import Variable\r\nfrom torch.utils.data import DataLoader\r\n\r\nfrom pascal_voc import VOCDetection, VOC, Viz\r\nfrom transforms import *\r\nfrom evaluation import eval_voc_detection\r\n\r\nimport sys\r\nsys.path.append(\'./models\')\r\nfrom SSD import SSD300\r\n\r\nfrom loss import MultiBoxLoss\r\nfrom multibox import MultiBox\r\n\r\n\r\nfrom tensorboardX import SummaryWriter\r\nsummary = SummaryWriter()\r\n\r\n# Setup\r\nparser = argparse.ArgumentParser(description=\'PyTorch SSD variants implementation\')\r\nparser.add_argument(\'--checkpoint\', help=\'resume from checkpoint\', default=\'\')\r\nparser.add_argument(\'--voc_root\', help=\'PASCAL VOC dataset root path\', default=\'\')\r\nparser.add_argument(\'--batch_size\', type=int, help=\'input data batch size\', default=32)\r\nparser.add_argument(\'--lr\', \'--learning_rate\', type=float, help=\'initial learning rate\', default=1e-3)\r\nparser.add_argument(\'--start_iter\', type=int, help=\'start iteration\', default=0)\r\nparser.add_argument(\'--backbone\', help=\'pretrained backbone net weights file\', default=\'vgg16_reducedfc.pth\')\r\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enable cuda\')\r\nparser.add_argument(\'--test\', action=\'store_true\', help=\'test mode\')\r\nparser.add_argument(\'--demo\', action=\'store_true\', help=\'show detection result\')\r\nparser.add_argument(\'--seed\', type=int, help=\'random seed\', default=233)\r\nparser.add_argument(\'--threads\', type=int, help=\'number of data loader workers.\', default=4)\r\n\r\nopt = parser.parse_args()\r\nprint(\'argparser:\', opt)\r\n\r\n\r\n\r\n\r\n# random seed\r\nrandom.seed(opt.seed)\r\nnp.random.seed(opt.seed)\r\ntorch.manual_seed(opt.seed)\r\nif opt.cuda:\r\n    assert torch.cuda.is_available(), \'No GPU found, please run without --cuda\'\r\n    torch.cuda.manual_seed_all(opt.seed)\r\n\r\n\r\n# model\r\nmodel = SSD300(VOC.N_CLASSES)\r\ncfg = model.config\r\n\r\nif opt.checkpoint:\r\n    model.load_state_dict(torch.load(opt.checkpoint))\r\nelse:\r\n    model.init_parameters(opt.backbone)\r\n\r\nencoder = MultiBox(cfg)\r\ncriterion = MultiBoxLoss()\r\n\r\n# cuda\r\nif opt.cuda:\r\n    model.cuda()\r\n    criterion.cuda()\r\n    cudnn.benchmark = True\r\n\r\n# optimizer\r\noptimizer = optim.SGD(model.parameters(), lr=opt.lr, momentum=0.9, weight_decay=5e-4)\r\n\r\n# learning rate / iterations\r\ninit_lr = cfg.get(\'init_lr\', 1e-3)\r\nstepvalues = cfg.get(\'stepvalues\', (80000, 100000))\r\nmax_iter = cfg.get(\'max_iter\', 120000)\r\n\r\ndef adjust_learning_rate(optimizer, stage):\r\n    lr = init_lr * (0.1 ** stage)\r\n    for param_group in optimizer.param_groups:\r\n        param_group[\'lr\'] = lr\r\n\r\n\r\n# def warm_up(iteration):\r\n# \twarmup_steps = [0, 300, 600, 1000]\r\n# \tif iteration in warmup_steps:\r\n# \t\ti = warmup_steps.index(iteration)\r\n# \t\tlr = 10 ** np.linspace(-6, np.log10(init_lr), len(warmup_steps))[i]\r\n# \t\tfor param_group in optimizer.param_groups:\r\n# \t\t\tparam_group[\'lr\'] = lr\r\n\r\n\r\ndef learning_rate_schedule(iteration):\r\n    if iteration in stepvalues:\r\n        adjust_learning_rate(optimizer, stepvalues.index(iteration) + 1)\r\n\r\n\r\n\r\n\r\ndef train():   \r\n    model.train()\r\n    PRNG = RandomState(opt.seed)\r\n\r\n    # augmentation / transforms\r\n    transform = Compose([\r\n            [ColorJitter(prob=0.5)],  # or write [ColorJitter(), None]\r\n            BoxesToCoords(),\r\n            Expand((1, 4), prob=0.5),\r\n            ObjectRandomCrop(),\r\n            HorizontalFlip(),\r\n            Resize(300),\r\n            CoordsToBoxes(),\r\n            [SubtractMean(mean=VOC.MEAN)],\r\n            [RGB2BGR()],\r\n            [ToTensor()],\r\n            ], PRNG, mode=None, fillval=VOC.MEAN)\r\n    target_transform = encoder.encode\r\n\r\n    dataset = VOCDetection(\r\n            root=opt.voc_root, \r\n            image_set=[(\'2007\', \'trainval\'), (\'2012\', \'trainval\'),],\r\n            keep_difficult=True,\r\n            transform=transform,\r\n            target_transform=target_transform)\r\n    dataloader = DataLoader(dataset=dataset, batch_size=opt.batch_size, shuffle=True, \r\n            num_workers=opt.threads, pin_memory=True)\r\n\r\n    iteration = opt.start_iter\r\n    while True:\r\n        for input, loc, label in dataloader:\r\n            learning_rate_schedule(iteration)\r\n\r\n            input, loc, label = Variable(input), Variable(loc), Variable(label)\r\n            if opt.cuda:\r\n                input, loc, label = input.cuda(), loc.cuda(), label.cuda()\r\n\r\n            xloc, xconf = model(input)\r\n            loc_loss, conf_loss = criterion(xloc, xconf, loc, label)\r\n            loss = loc_loss + conf_loss\r\n\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n            if iteration % 10 == 0:\r\n                summary.add_scalar(\'loss/loc_loss\', loc_loss.data[0], iteration)\r\n                summary.add_scalar(\'loss/conf_loss\', conf_loss.data[0], iteration)\r\n                summary.add_scalars(\'loss/loss\', {""loc_loss"": loc_loss.data[0],\r\n                                                  ""conf_loss"": conf_loss.data[0],\r\n                                                  ""loss"": loss.data[0]}, iteration)\r\n\r\n            if iteration % 5000 == 0 and iteration != opt.start_iter:\r\n                print(\'Save state, iter: {}, Loss:{}\'.format(iteration, loss.data[0]))\r\n                if not os.path.isdir(\'weights\'):\r\n                        os.mkdir(\'weights\')\r\n                torch.save(model.state_dict(), \'weights/{}_0712_{}.pth\'.format(cfg.get(\'name\', \'SSD\'), iteration))\r\n            \r\n            iteration += 1\r\n            if iteration > max_iter:\r\n                return 0\r\n\r\n            \r\n\r\n\r\n\r\ndef test():\r\n    PRNG = RandomState()\r\n\r\n    dataset = VOCDetection(\r\n            root=opt.voc_root, \r\n            image_set=[(\'2007\', \'test\')],\r\n            transform=Compose([\r\n                BoxesToCoords(),\r\n                Resize(300),\r\n                CoordsToBoxes(),\r\n                [SubtractMean(mean=VOC.MEAN)],\r\n                [RGB2BGR()],\r\n                [ToTensor()]]),\r\n            target_transform=None)\r\n    print(len(dataset))\r\n\r\n    pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels = [], [], [], [], []\r\n    for i in range(len(dataset)):\r\n        img, loc, label = dataset[i]\r\n\r\n        gt_bboxes.append(loc)\r\n        gt_labels.append(label)\r\n\r\n        input = Variable(img.unsqueeze(0), volatile=True)\r\n        if opt.cuda:\r\n            input = input.cuda()\r\n\r\n        xloc, xconf = model(input)\r\n        xloc = xloc.data.cpu().numpy()[0]\r\n        xconf = xconf.data.cpu().numpy()[0]\r\n\r\n        boxes, labels, scores = encoder.decode(xloc, xconf, nms_thresh=0.5, conf_thresh=0.01)\r\n\r\n        pred_bboxes.append(boxes)\r\n        pred_labels.append(labels)\r\n        pred_scores.append(scores)\r\n\r\n    print(eval_voc_detection(pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels, iou_thresh=0.5, use_07_metric=True))\r\n\r\n        \r\n\r\ndef demo():\r\n    PRNG = RandomState()\r\n    voc_vis = Viz()\r\n\r\n    dataset = VOCDetection(\r\n            root=opt.voc_root, \r\n            image_set=[(\'2007\', \'test\')],\r\n            transform=Compose([\r\n                BoxesToCoords(),\r\n                Resize(300),\r\n                CoordsToBoxes(),\r\n                [SubtractMean(mean=VOC.MEAN)],\r\n                [RGB2BGR()],\r\n                [ToTensor()]]),\r\n            target_transform=None)\r\n    print(len(dataset))\r\n\r\n    i = PRNG.choice(len(dataset))\r\n    for _ in range(1000):\r\n        img, _, _ = dataset[i]\r\n\r\n        input = Variable(img.unsqueeze(0), volatile=True)\r\n        if opt.cuda:\r\n            input = input.cuda()\r\n\r\n        xloc, xconf = model(input)\r\n\r\n        imgs = input.data.cpu().numpy().transpose(0, 2, 3, 1)\r\n        xloc = xloc.data.cpu().numpy()\r\n        xconf = xconf.data.cpu().numpy()\r\n        \r\n        for img, loc, conf in zip(imgs, xloc, xconf):\r\n            #print(img.mean(axis=(0,1)))\r\n            img = ((img[:, :, ::-1] + VOC.MEAN)).astype(\'uint8\')\r\n            boxes, labels, scores = encoder.decode(loc, conf, conf_thresh=0.5)\r\n\r\n            img = voc_vis.draw_bbox(img, boxes, labels, True)\r\n            cv2.imshow(\'0\', img[:, :, ::-1])\r\n            c = cv2.waitKey(0)\r\n            if c == 27 or c == ord(\'q\'):   # ESC / \'q\'\r\n                return\r\n            else:\r\n                i = PRNG.choice(len(dataset))\r\n\r\n\r\n\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    if opt.test:\r\n        test()\r\n    elif opt.demo:\r\n        demo()\r\n    else:\r\n        train()'"
transforms.py,7,"b'from __future__ import print_function\r\n\r\nimport numpy as np \r\n\r\n# scipy.ndimage -> skimage -> cv2, \r\n# skimage is one or two orders of magnitude slower than cv2\r\nimport cv2\r\n\r\ntry:\r\n    import torch\r\nexcept ImportError:\r\n    pass\r\n\r\nimport collections\r\nimport numbers\r\nimport types\r\n\r\n\r\n\r\nInterpolationFlags = {\'nearest\':cv2.INTER_NEAREST, \'linear\':cv2.INTER_LINEAR, \r\n                       \'cubic\':cv2.INTER_CUBIC, \'area\':cv2.INTER_AREA, \r\n                       \'lanczos\':cv2.INTER_LANCZOS4}\r\n\r\nBorderTypes = {\'constant\':cv2.BORDER_CONSTANT, \r\n               \'replicate\':cv2.BORDER_REPLICATE, \'nearest\':cv2.BORDER_REPLICATE,\r\n               \'reflect\':cv2.BORDER_REFLECT, \'mirror\': cv2.BORDER_REFLECT,\r\n               \'wrap\':cv2.BORDER_WRAP, \'reflect_101\':cv2.BORDER_REFLECT_101,}\r\n\r\n\r\n\r\ndef _loguniform(interval, random_state=np.random):\r\n    low, high = interval\r\n    return np.exp(random_state.uniform(np.log(low), np.log(high)))\r\n\r\n\r\ndef _clamp(img, min=None, max=None, dtype=\'uint8\'):\r\n    if min is None and max is None:\r\n        if dtype == \'uint8\':\r\n            min, max = 0, 255\r\n        elif dtype == \'uint16\':\r\n            min, max = 0, 65535\r\n        else:\r\n            min, max = -np.inf, np.inf\r\n    img = np.clip(img, min, max)\r\n    return img.astype(dtype)\r\n\r\n\r\ndef _jaccard(boxes, rect):\r\n    def _intersect(boxes, rect):\r\n        lt = np.maximum(boxes[:, :2], rect[:2])\r\n        rb = np.minimum(boxes[:, 2:], rect[2:])\r\n        inter = np.clip(rb - lt, 0, None)\r\n        return inter[:, 0] * inter[:, 1]\r\n\r\n    inter = _intersect(boxes, rect)\r\n    \r\n    area1 = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\r\n    area2 = (rect[2] - rect[0]) * (rect[3] - rect[1])\r\n    union = area1 + area2 - inter \r\n    \r\n    jaccard  = inter / np.clip(union, 1e-10, None)\r\n    coverage = inter / np.clip(area1, 1e-10, None)\r\n    return jaccard, coverage, inter\r\n\r\n\r\ndef _coords_clamp(cds, shape, outside=None):\r\n    w, h = shape[1] - 1, shape[0] - 1\r\n    if outside == \'discard\':\r\n        cds_ = []\r\n        for x, y in cds:\r\n            x_ = x if 0 <= x <= w else np.sign(x) * np.inf\r\n            y_ = y if 0 <= y <= h else np.sign(y) * np.inf\r\n            cds_.append([x_, y_])\r\n        return np.array(cds_, dtype=np.float32)\r\n    else:\r\n        return np.array([[np.clip(cd[0], 0, w), np.clip(cd[1], 0, h)] for cd in cds], dtype=np.float32)\r\n\r\n\r\ndef _to_bboxes(cds, img_shape=None):\r\n    assert len(cds) % 4 == 0\r\n\r\n    h, w = img_shape if img_shape is not None else (np.inf, np.inf)\r\n    boxes = []\r\n    cds = np.array(cds)\r\n    for i in range(0, len(cds), 4):\r\n        xmin = np.clip(cds[i:i+4, 0].min(), 0, w - 1)\r\n        xmax = np.clip(cds[i:i+4, 0].max(), 0, w - 1)\r\n        ymin = np.clip(cds[i:i+4, 1].min(), 0, h - 1)\r\n        ymax = np.clip(cds[i:i+4, 1].max(), 0, h - 1)\r\n        boxes.append([xmin, ymin, xmax, ymax])\r\n    return np.array(boxes)\r\n\r\n\r\ndef _to_coords(boxes):\r\n    cds = []\r\n    for box in boxes:\r\n        xmin, ymin, xmax, ymax = box \r\n        cds += [\r\n            [xmin, ymin],\r\n            [xmax, ymin],\r\n            [xmax, ymax],\r\n            [xmin, ymax],\r\n        ]\r\n    return np.array(cds)\r\n\r\n\r\n# recursively reset transform\'s state\r\ndef transform_state(t, **kwargs):\r\n    if callable(t):\r\n        t_vars = vars(t)\r\n\r\n        if \'random_state\' in kwargs and \'random\' in t_vars:\r\n            t.__dict__[\'random\'] = kwargs[\'random_state\']\r\n\r\n        support = [\'fillval\', \'anchor\', \'prob\', \'mean\', \'std\', \'outside\']\r\n        for arg in kwargs:\r\n            if arg in t_vars and arg in support:\r\n                t.__dict__[arg] = kwargs[arg]\r\n\r\n        if \'mode\' in kwargs and \'mode\' in t_vars:\r\n            t.__dict__[\'mode\'] = kwargs[\'mode\']\r\n        if \'border\' in kwargs and \'border\' in t_vars:\r\n            t.__dict__[\'border\'] = BorderTypes.get(kwargs[\'border\'], cv2.BORDER_REPLICATE)\r\n\r\n        if \'transforms\' in t_vars:\r\n            t.__dict__[\'transforms\'] = transforms_state(t.transforms, **kwargs)\r\n    return t\r\n\r\n\r\ndef transforms_state(ts, **kwargs):\r\n    assert isinstance(ts, collections.Sequence)\r\n\r\n    transforms = []\r\n    for t in ts:\r\n        if isinstance(t, collections.Sequence):\r\n            transforms.append(transforms_state(t, **kwargs))\r\n        else:\r\n            transforms.append(transform_state(t, **kwargs))\r\n    return transforms\r\n\r\n\r\n\r\n# Operators\r\n\'\'\'\r\nclass Clamp(object):\r\n    def __init__(self, min=0, max=255, soft=True, dtype=\'uint8\'):\r\n        self.min, self.max = min, max\r\n        self.dtype = dtype\r\n        self.soft = soft\r\n        self.thresh =\r\n\r\n    def __call__(self, img):\r\n        if self.soft is None:\r\n            return _clamp(img, min=self.min, max=self.max, dtype=self.dtype)\r\n        else:\r\n\'\'\'\r\n\r\n\r\nclass Unsqueeze(object):\r\n    def __call__(self, img):\r\n        if img.ndim == 2:\r\n            return img[..., np.newaxis]\r\n        elif img.ndim == 3:\r\n            return img\r\n        else:\r\n            raise ValueError(\'input muse be image\')\r\n\r\n\r\n\r\nclass Normalize(object):\r\n    def __init__(self, mean, std):\r\n        self.mean = mean\r\n        self.std = std \r\n\r\n    def __call__(self, img):\r\n        # normalize np.ndarray or torch.FloatTensor\r\n        if isinstance(img, np.ndarray):\r\n            return (img - self.mean) / self.std\r\n        elif isinstance(img, torch.FloatTensor):\r\n            tensor = img\r\n            for t, m, s in zip(tensor, self.mean, self.std):\r\n                t.sub_(m).div_(s)\r\n                return tensor \r\n        else:\r\n            raise Exception(\'invalid input type\')\r\n\r\n\r\nclass SubtractMean(object):\r\n    # TODO: pytorch tensor\r\n    def __init__(self, mean):\r\n        self.mean = mean \r\n\r\n    def __call__(self, img):\r\n        return img.astype(np.float32) - self.mean\r\n\r\nclass DivideBy(object):\r\n    # TODO: pytorch tensor\r\n    def __init__(self, divisor):\r\n        self.divisor = divisor\r\n\r\n    def __call__(self, img):\r\n        return img.astype(np.float32) / self.divisor\r\n\r\n\r\ndef HalfBlood(img, anchor, f1, f2):\r\n    assert isinstance(f1, types.LambdaType) and isinstance(f2, types.LambdaType)\r\n\r\n    if isinstance(anchor, numbers.Number):\r\n        anchor = int(np.ceil(anchor))\r\n\r\n    if isinstance(anchor, int) and img.ndim == 3 and 0 < anchor < img.shape[2]:\r\n        img1, img2 = img[:,:,:anchor], img[:,:,anchor:]\r\n\r\n        if img1.shape[2] == 1:\r\n            img1 = img1[:, :, 0]\r\n        if img2.shape[2] == 1:\r\n            img2 = img2[:, :, 0]\r\n\r\n        img1 = f1(img1)\r\n        img2 = f2(img2)\r\n\r\n        if img1.ndim == 2:\r\n            img1 = img1[..., np.newaxis]\r\n        if img2.ndim == 2:\r\n            img2 = img2[..., np.newaxis]\r\n        \r\n        return np.concatenate((img1, img2), axis=2)\r\n    elif anchor == 0:\r\n        img = f2(img)\r\n        if img.ndim == 2:\r\n            img = img[..., np.newaxis]\r\n        return img\r\n    else:\r\n        img = f1(img)\r\n        if img.ndim == 2:\r\n            img = img[..., np.newaxis]\r\n        return img\r\n\r\n\r\n\r\n\r\n\r\n# Photometric Transform\r\n\r\n\r\nclass RGB2BGR(object):\r\n    def __call__(self, img):\r\n        assert img.ndim == 3 and img.shape[2] == 3\r\n        return img[:, :, ::-1]\r\n\r\nclass BGR2RGB(object):\r\n    def __call__(self, img):\r\n        assert img.ndim == 3 and img.shape[2] == 3\r\n        return img[:, :, ::-1]\r\n\r\n\r\nclass GrayScale(object):\r\n    # RGB to Gray\r\n    def __call__(self, img):\r\n        if img.ndim == 3 and img.shape[2] == 1:\r\n            return img\r\n\r\n        assert img.ndim == 3 and img.shape[2] == 3\r\n        dtype = img.dtype\r\n        gray = np.sum(img * [0.299, 0.587, 0.114], axis=2).astype(dtype)  #5x slower than cv2.cvtColor \r\n        \r\n        #gray = cv2.cvtColor(img.astype(\'uint8\'), cv2.COLOR_RGB2GRAY)\r\n        return gray[..., np.newaxis]\r\n\r\n\r\nclass Hue(object):\r\n    # skimage.color.rgb2hsv/hsv2rgb is almost 100x slower than cv2.cvtColor\r\n    def __init__(self, var=0.05, prob=0.5, random_state=np.random):\r\n        self.var = var\r\n        self.prob = prob\r\n        self.random = random_state\r\n\r\n    def __call__(self, img):\r\n        assert img.ndim == 3 and img.shape[2] == 3\r\n\r\n        if self.random.random_sample() >= self.prob:\r\n            return img\r\n\r\n        var = self.random.uniform(-self.var, self.var)\r\n\r\n        to_HSV, from_HSV = [(cv2.COLOR_RGB2HSV, cv2.COLOR_HSV2RGB),\r\n                            (cv2.COLOR_BGR2HSV, cv2.COLOR_HSV2BGR)][self.random.randint(2)]\r\n\r\n        hsv = cv2.cvtColor(img, to_HSV).astype(np.float32)\r\n\r\n        hue = hsv[:, :, 0] / 179. + var\r\n        hue = hue - np.floor(hue)\r\n        hsv[:, :, 0] = hue * 179.\r\n\r\n        img = cv2.cvtColor(hsv.astype(\'uint8\'), from_HSV)\r\n        return img\r\n\r\n\r\nclass Saturation(object):\r\n    def __init__(self, var=0.3, prob=0.5, random_state=np.random):\r\n        self.var = var \r\n        self.prob = prob\r\n        self.random = random_state\r\n\r\n        self.grayscale = GrayScale()\r\n\r\n    def __call__(self, img):\r\n        if self.random.random_sample() >= self.prob:\r\n            return img\r\n\r\n        dtype = img.dtype\r\n        gs = self.grayscale(img)\r\n\r\n        alpha = 1.0 + self.random.uniform(-self.var, self.var)\r\n        img = alpha * img.astype(np.float32) + (1 - alpha) * gs.astype(np.float32)\r\n        return _clamp(img, dtype=dtype)\r\n\r\n\r\n\r\nclass Brightness(object):\r\n    def __init__(self, delta=32, prob=0.5, random_state=np.random):\r\n        self.delta = delta\r\n        self.prob = prob\r\n        self.random = random_state\r\n\r\n    def __call__(self, img):\r\n        if self.random.random_sample() >= self.prob:\r\n            return img\r\n\r\n        dtype = img.dtype\r\n        #alpha = 1.0 + self.random.uniform(-self.var, self.var)\r\n        #img = alpha * img.astype(np.float32)\r\n        img = img.astype(np.float32) + self.random.uniform(-self.delta, self.delta)\r\n        return _clamp(img, dtype=dtype)\r\n\r\n\r\n\r\nclass Contrast(object):\r\n    def __init__(self, var=0.3, prob=0.5, random_state=np.random):\r\n        self.var = var \r\n        self.prob = prob\r\n        self.random = random_state\r\n\r\n        self.grayscale = GrayScale()\r\n\r\n    def __call__(self, img):\r\n        if self.random.random_sample() >= self.prob:\r\n            return img\r\n\r\n        dtype = img.dtype\r\n        gs = self.grayscale(img).mean()\r\n\r\n        alpha = 1.0 + self.random.uniform(-self.var, self.var)\r\n        img = alpha * img.astype(np.float32) + (1 - alpha) * gs\r\n        return _clamp(img, dtype=dtype)\r\n\r\n\r\nclass RandomOrder(object):\r\n    def __init__(self, transforms, random_state=None):  #, **kwargs):\r\n        if random_state is None:\r\n            self.random = np.random\r\n        else:\r\n            self.random = random_state\r\n            #kwargs[\'random_state\'] = random_state\r\n\r\n        self.transforms = transforms_state(transforms, random=random_state)\r\n\r\n    def __call__(self, img):\r\n        if self.transforms is None:\r\n            return img\r\n        order = self.random.permutation(len(self.transforms))\r\n        for i in order:\r\n            img = self.transforms[i](img)\r\n        return img\r\n\r\n\r\nclass ColorJitter(RandomOrder):\r\n    def __init__(self, brightness=32, contrast=0.5, saturation=0.5, hue=0.1,\r\n                     prob=0.5, random_state=np.random):\r\n        self.transforms = []\r\n        self.random = random_state\r\n\r\n        if brightness != 0:\r\n            self.transforms.append(\r\n                Brightness(brightness, prob=prob, random_state=random_state))\r\n        if contrast != 0:\r\n            self.transforms.append(\r\n                Contrast(contrast, prob=prob, random_state=random_state))\r\n        if saturation != 0:\r\n            self.transforms.append(\r\n                Saturation(saturation, prob=prob, random_state=random_state))\r\n        if hue != 0:\r\n            self.transforms.append(\r\n                Hue(hue, prob=prob, random_state=random_state))\r\n\r\n\r\n\r\n# ""ImageNet Classification with Deep Convolutional Neural Networks""\r\n# looks inferior to ColorJitter\r\nclass FancyPCA(object):\r\n    def __init__(self, var=0.2, random_state=np.random):\r\n        self.var = var\r\n        self.random = random_state\r\n\r\n        self.pca = None    # shape (channels, channels)\r\n\r\n    def __call__(self, img):\r\n        dtype = img.dtype\r\n        channels = img.shape[2]\r\n        alpha = self.random.randn(channels) * self.var\r\n\r\n        if self.pca is None:\r\n            pca = self._pca(img)\r\n        else:\r\n            pca = self.pca\r\n\r\n        img = img + (pca * alpha).sum(axis=1)\r\n        return _clamp(img, dtype=dtype)\r\n\r\n    def _pca(self, img):   # single image (hwc), or a batch (nhwc)\r\n        assert img.ndim >= 3\r\n        channels = img.shape[-1]\r\n        X = img.reshape(-1, channels)\r\n\r\n        cov = np.cov(X.T)   \r\n        evals, evecs = np.linalg.eigh(cov)\r\n        pca = np.sqrt(evals) * evecs\r\n        return pca\r\n\r\n    def fit(self, imgs):   # training\r\n        self.pca = self._pca(imgs)\r\n        print(self.pca)\r\n\r\n\r\nclass ShuffleChannels(object):\r\n    def __init__(self, prob=1., random_state=np.random):\r\n        self.prob = prob\r\n        self.random = random_state \r\n\r\n    def __call__(self, img):\r\n        if self.prob < 1 and self.random.random_sample() >= self.prob:\r\n            return img \r\n\r\n        assert img.ndim == 3\r\n        permut = self.random.permutation(img.shape[2])\r\n        img = img[:, :, permut]\r\n\r\n        return img\r\n\r\n\r\n# ""Improved Regularization of Convolutional Neural Networks with Cutout"". (arXiv:1708.04552)\r\n# fill with 0(if image is normalized) or dataset\'s per-channel mean.\r\nclass Cutout(object):\r\n    def __init__(self, size, fillval=0, prob=0.5, random_state=np.random): \r\n        if isinstance(size, numbers.Number):\r\n            size = (int(size), int(size))\r\n        self.size = size\r\n        \r\n        self.fillval = fillval\r\n        self.prob = prob\r\n        self.random = random_state\r\n\r\n    def __call__(self, img):\r\n        if self.random.random_sample() >= self.prob:\r\n            return img\r\n\r\n        h, w = img.shape[:2]\r\n        tw, th = self.size \r\n\r\n        cx = self.random.randint(0, w)\r\n        cy = self.random.randint(0, h)\r\n\r\n        x1 = int(np.clip(cx -       tw / 2, 0, w - 1))\r\n        x2 = int(np.clip(cx + (tw + 1) / 2, 0, w    ))\r\n        y1 = int(np.clip(cy -       th / 2, 0, h - 1))\r\n        y2 = int(np.clip(cy + (th + 1) / 2, 0, h    ))\r\n\r\n        img[y1:y2, x1:x2] = self.fillval\r\n\r\n        return img\r\n\r\n\r\n# ""Random Erasing Data Augmentation"". (arXiv:1708.04896).  fill with random value\r\nclass RandomErasing(object):\r\n    def __init__(self, area_range=(0.02, 0.2), ratio_range=[0.3, 1/0.3], fillval=None, \r\n                 prob=0.5, num=1, anchor=None, random_state=np.random):\r\n        self.area_range = area_range\r\n        self.ratio_range = ratio_range\r\n        self.fillval = fillval\r\n        self.prob = prob\r\n        self.num = num\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n    def __call__(self, img):\r\n        if self.random.random_sample() >= self.prob:\r\n            return img\r\n\r\n        h, w = img.shape[:2]\r\n\r\n        num = self.random.randint(self.num) + 1\r\n        count = 0\r\n        for _ in range(10):\r\n            area = h * w \r\n            target_area = _loguniform(self.area_range, self.random) * area\r\n            aspect_ratio = _loguniform(self.ratio_range, self.random)\r\n\r\n            tw = int(round(np.sqrt(target_area * aspect_ratio)))\r\n            th = int(round(np.sqrt(target_area / aspect_ratio)))\r\n\r\n            if tw <= w and th <= h:\r\n\r\n                x1 = self.random.randint(0, w - tw + 1)\r\n                y1 = self.random.randint(0, h - th + 1)\r\n\r\n                fillval = self.random.randint(0, 256) if self.fillval is None else self.fillval\r\n\r\n                erase = lambda im: self._fill(im, (x1, y1, x1+tw, y1+th), fillval)\r\n                cut = lambda im: self._fill(im, (x1, y1, x1+tw, y1+th), 0)\r\n                img = HalfBlood(img, self.anchor, erase, cut)\r\n\r\n                count += 1\r\n            if count >= num:\r\n                return img\r\n\r\n        # Fallback\r\n        return img\r\n\r\n    def _fill(self, img, rect, val):\r\n        l, t, r, b = rect\r\n        img[t:b, l:r] = val\r\n        return img\r\n\r\n\r\n#GaussianBlur\r\n#MotionBlue\r\n#RadialBlur\r\n#ResizeBlur \r\n#Sharpen\r\n\r\n\r\n\r\n\r\n# Geometric Transform\r\n\r\ndef _expand(img, size, lt, val):\r\n    h, w = img.shape[:2]\r\n    nw, nh = size \r\n    x1, y1 = lt \r\n    expand = np.zeros([nh, nw] + list(img.shape[2:]), dtype=img.dtype)\r\n    expand[...] = val\r\n    expand[y1: h + y1, x1: w + x1] = img\r\n    #expand = cv2.copyMakeBorder(img, y1, nh-h-y1, x1, nw-w-x1, \r\n    #\t\t\t\t\t\t\tcv2.BORDER_CONSTANT, value=val)  # slightly faster\r\n    return expand\r\n\r\n\r\nclass Pad(object):\r\n    def __init__(self, padding, fillval=0, anchor=None):\r\n        if isinstance(padding, numbers.Number):\r\n            padding = (padding, padding)\r\n        assert len(padding) == 2\r\n\r\n        self.padding = [int(np.clip(_), 0, None) for _ in padding]\r\n        self.fillval = fillval\r\n        self.anchor = anchor\r\n\r\n    def __call__(self, img, cds=None):\r\n        if max(self.padding) == 0:\r\n            return img if cds is None else (img, cds)\r\n\r\n        h, w = img.shape[:2]\r\n        pw, ph = self.padding\r\n\r\n        pad = lambda im: _expand(im, (w + pw*2, h + ph*2), (pw, ph), self.fillval)\r\n        purer = lambda im: _expand(im, (w + pw*2, h + ph*2), (pw, ph), 0)  \r\n        img = HalfBlood(img, self.anchor, pad, purer)\r\n\r\n        if cds is not None:\r\n            return img, np.array([[x + pw, y + ph] for x, y in cds])\r\n        else:\r\n            return img\r\n\r\n\r\n# ""SSD: Single Shot MultiBox Detector"".  generate multi-resolution image/ multi-scale objects\r\nclass Expand(object):\r\n    def __init__(self, scale_range=(1, 4), fillval=0, prob=1.0, anchor=None, random_state=np.random):\r\n        if isinstance(scale_range, numbers.Number):\r\n            scale_range = (1, scale_range)\r\n        assert max(scale_range) <= 5 \r\n\r\n        self.scale_range = scale_range\t\r\n        self.fillval = fillval\r\n        self.prob = prob\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None):\r\n        if self.prob < 1 and self.random.random_sample() >= self.prob:\r\n            return img if cds is None else (img, cds)\r\n\r\n        #multiple = _loguniform(self.scale_range, self.random)\r\n        multiple = self.random.uniform(*self.scale_range)\r\n\r\n        h, w = img.shape[:2]\r\n        nh, nw = int(multiple * h), int(multiple * w)\r\n\r\n        if multiple < 1:\r\n            return RandomCrop(size=(nw, nh), random_state=self.random)(img, cds)\r\n\r\n        y1 = self.random.randint(0, nh - h + 1)\r\n        x1 = self.random.randint(0, nw - w + 1)\r\n\r\n        expand = lambda im: _expand(im, (nw, nh), (x1, y1), self.fillval)\r\n        purer = lambda im: _expand(im, (nw, nh), (x1, y1), 0)\r\n        img = HalfBlood(img, self.anchor, expand, purer)\r\n\r\n        if cds is not None:\r\n            return img, np.array([[x + x1, y + y1] for x, y in cds])\r\n        else:\r\n            return img\r\n\r\n\r\n# scales the smaller edge to given size\r\nclass Scale(object):\r\n    def __init__(self, size, mode=\'linear\', lazy=False, anchor=None, random_state=np.random):\r\n        assert isinstance(size, int)\r\n\r\n        self.size = int(size)\r\n        self.mode = mode\r\n        self.lazy = lazy\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None):\r\n        interp_mode = (self.random.choice(list(InterpolationFlags.values())) if self.mode is None \r\n                                   else InterpolationFlags.get(self.mode, cv2.INTER_LINEAR))\r\n\r\n        h, w = img.shape[:2]\r\n\r\n        if self.lazy and min(h, w) >= self.size:\r\n            return img if cds is None else (img, cds)\r\n\r\n        if h < w:\r\n            tw, th = int(self.size / float(h) * w), self.size\r\n        else:\r\n            th, tw = int(self.size / float(w) * h), self.size\r\n\r\n        # skimage.transform.resize 10x slower than cv2.resize\r\n        resize = lambda im: cv2.resize(im, (tw, th), interpolation=interp_mode)\r\n        purer = lambda im: cv2.resize(im, (tw, th), interpolation=cv2.INTER_NEAREST)\r\n        img = HalfBlood(img, self.anchor, resize, purer)\r\n\r\n        if cds is not None:\r\n            s_x, s_y = tw / float(w), th / float(h)\r\n            return img, np.array([[x * s_x, y * s_y] for x, y in cds])\r\n        else:\r\n            return img\r\n\r\n\r\nclass RandomScale(object):\r\n    def __init__(self, size_range, mode=\'linear\', anchor=None, random_state=np.random):\r\n        assert isinstance(size_range, collections.Sequence) and len(size_range) == 2\r\n\r\n        self.size_range = size_range\r\n        self.mode = mode\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None):\r\n        interp_mode = (self.random.choice(list(InterpolationFlags.values())) if self.mode is None \r\n                                   else InterpolationFlags.get(self.mode, cv2.INTER_LINEAR))\r\n\r\n        h, w = img.shape[:2]\r\n        size = int(self.random.uniform(*self.size_range))\r\n        \r\n        if h < w:\r\n            tw, th = int(size / float(h) * w), size\r\n        else:\r\n            th, tw = int(size / float(w) * h), size\r\n\r\n        resize = lambda im: cv2.resize(im, (tw, th), interpolation=interp_mode)\r\n        purer = lambda im: cv2.resize(im, (tw, th), interpolation=cv2.INTER_NEAREST)\r\n        img = HalfBlood(img, self.anchor, resize, purer)\r\n\r\n        if cds is not None:\r\n            s_x, s_y = tw / float(w), th / float(h)\r\n            return img, np.array([[x * s_x, y * s_y] for x, y in cds])\r\n        else:\r\n            return img\r\n\r\n\r\nclass CenterCrop(object):\r\n    def __init__(self, size):\r\n        if isinstance(size, numbers.Number):\r\n            size = (int(size), int(size))\r\n        self.size = size\r\n\r\n    def __call__(self, img, cds=None):\r\n        h, w = img.shape[:2]\r\n        tw, th = self.size\r\n\r\n        if h == th and w == tw:\r\n            return img if cds is None else (img, cds)\r\n        elif h < th or w < tw:\r\n            raise Exception(\'invalid crop size\')\r\n\r\n        x1 = int(round((w - tw) / 2.))\r\n        y1 = int(round((h - th) / 2.))\r\n        img = img[y1:y1 + th, x1:x1 + tw]\r\n\r\n        if cds is not None:\r\n            return img, _coords_clamp([[x - x1, y - y1] for x, y in cds], img.shape)\r\n        else:\r\n            return img\r\n\r\n\r\nclass RandomCrop(object):\r\n    def __init__(self, size, fillval=0, random_state=np.random):\r\n        if isinstance(size, numbers.Number):\r\n            size = (int(size), int(size))\r\n        self.size = size\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None):\r\n        h, w = img.shape[:2]\r\n        tw, th = self.size\r\n\r\n        assert h >= th and w >= tw\r\n\r\n        x1 = self.random.randint(0, w - tw + 1)\r\n        y1 = self.random.randint(0, h - th + 1)\r\n        img = img[y1:y1 + th, x1:x1 + tw]\r\n\r\n        if cds is not None:\r\n            return img, _coords_clamp([[x - x1, y - y1] for x, y in cds], img.shape)\r\n        else:\r\n            return img\r\n\r\n\'\'\'\r\n# ""SSD: Single Shot MultiBox Detector"". \r\n# object-aware RandomCrop, crop multi-scale objects\r\nclass ObjectRandomCrop(object):\r\n    def __init__(self, final_size=None, prob=1., random_state=np.random):\r\n        self.final_size = final_size   # reference size\r\n        self.final_area = (final_size * final_size if isinstance(final_size, numbers.Number) \r\n                                    else np.prod(final_size))\r\n        self.prob = prob\r\n        self.random = random_state \r\n\r\n        self.options = [\r\n            None,                # keep original size\r\n            #(-np.inf, 0.1),      # large crop\r\n            (0.02, 0.1),\r\n            (0.1, 0.3),\r\n            (0.3, 0.5),\r\n            (0.5, 0.7),\r\n            (0.7, np.inf),       # small crop\r\n            (-np.inf, np.inf),   # arbitrary size  \r\n        ]\r\n\r\n    def __call__(self, img, cbs):\r\n        h, w = img.shape[:2]\r\n\r\n        # ad-hoc\r\n        if len(cbs) == 0:\r\n            return img, cbs\r\n\r\n        if len(cbs[0]) == 4:\r\n            boxes = cbs\r\n        elif len(cbs[0]) == 2:\r\n            boxes = _to_bboxes(cbs, img.shape[:2])\r\n        else:\r\n            raise Exception(\'invalid input\')\r\n\r\n        for attempt in range(30):\r\n            mode = self.random.choice(self.options)\r\n\r\n            if mode is None or (self.prob < 1 and self.random.random_sample() >= self.prob):\r\n                if self.final_size is not None:\r\n                    # area constraint\r\n                    box_areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\r\n                    size = np.sqrt(box_areas * self.final_area / (h * w))\r\n                    if not ((11 < size) * (size < 18)).any() and size.max() > 22:\r\n                        return img, cbs\r\n                    mode = self.options[self.random.randint(1, len(self.options))]\r\n                else:\r\n                    return img, cbs\r\n\r\n            min_iou, max_iou = mode\r\n\r\n            for _ in range(50):\r\n                tw = self.random.uniform(0.3 * w, w)\r\n                th = self.random.uniform(0.3 * h, h)\r\n                if max(th / tw, tw / th) > 2:\r\n                    continue\r\n\r\n                x1 = self.random.randint(0, w - tw + 1)\r\n                y1 = self.random.randint(0, h - th + 1)\r\n\r\n                rect = np.array([int(x1), int(y1), int(x1+tw), int(y1+th)])\r\n                jaccard, coverage, inter = _jaccard(boxes, rect)\r\n\r\n                # iou constraint\r\n                if jaccard.max() < min_iou or jaccard.max() > max_iou:\r\n                    continue\r\n\r\n                # coverage constraint\r\n                m1 = coverage > 1/9.\r\n                m2 = coverage < 0.45\r\n                if (m1 * m2).any():\r\n                    continue\r\n\r\n                mask = coverage >= 0.45\r\n                if not mask.any():\r\n                    continue\r\n\r\n                # area constraint\r\n                if self.final_size is not None:\r\n                    area = (rect[2] - rect[0]) * (rect[3] - rect[1]) * 1.\r\n                    size = np.sqrt(inter[mask] * self.final_area / area)\r\n                    if ((11 < size) * (size < 18)).any() or size.max() < 22:\r\n                        continue\r\n\r\n                img = img[rect[1]:rect[3], rect[0]:rect[2]]\r\n\r\n                boxes[:, :2] = np.clip(boxes[:, :2], rect[:2], rect[2:])\r\n                boxes[:, :2] = boxes[:, :2] - rect[:2]\r\n                boxes[:, 2:] = np.clip(boxes[:, 2:], rect[:2], rect[2:])\r\n                boxes[:, 2:] = boxes[:, 2:] - rect[:2]\r\n                boxes[np.logical_not(mask), :] = 0\r\n\r\n                #print(min_iou, max_iou)\r\n\r\n                if len(cbs[0]) == 4:\r\n                    return img, boxes\r\n                else:\r\n                    return img, _to_coords(boxes)\r\n\r\n        # Fallback\r\n        return img, cbs\r\n\'\'\'\r\n\r\nclass ObjectRandomCrop(object):\r\n    def __init__(self, prob=1., random_state=np.random):\r\n        self.prob = prob\r\n        self.random = random_state \r\n\r\n        self.options = [\r\n        #(0, None), \r\n        (0.1, None),     \r\n        (0.3, None),\r\n        (0.5, None),\r\n        (0.7, None),\r\n        (0.9, None),       \r\n        (None, 1), ]\r\n    \r\n\r\n    def __call__(self, img, cbs):\r\n        h, w = img.shape[:2]\r\n\r\n        if len(cbs) == 0:\r\n            return img, cbs\r\n\r\n        # ad-hoc\r\n        if len(cbs[0]) == 4:\r\n            boxes = cbs\r\n        elif len(cbs[0]) == 2:\r\n            boxes = _to_bboxes(cbs, img.shape[:2])\r\n        else:\r\n            raise Exception(\'invalid input\')\r\n\r\n        params = [(np.array([0, 0, w, h]), None)]\r\n\r\n        for min_iou, max_iou in self.options:\r\n            if min_iou is None:\r\n                min_iou = 0\r\n            if max_iou is None:\r\n                max_iou = 1\r\n\r\n            for _ in range(50):\r\n                scale = self.random.uniform(0.3, 1)\r\n                aspect_ratio = self.random.uniform(\r\n                    max(1 / 2., scale * scale),\r\n                    min(2., 1 / (scale * scale)))\r\n                th = int(h * scale / np.sqrt(aspect_ratio))\r\n                tw = int(w * scale * np.sqrt(aspect_ratio))\r\n\r\n                x1 = self.random.randint(0, w - tw + 1)\r\n                y1 = self.random.randint(0, h - th + 1)\r\n                rect = np.array([x1, y1, x1 + tw, y1 + th])\r\n\r\n                iou, coverage, _ = _jaccard(boxes, rect)\r\n\r\n                #m1 = coverage > 0.1\r\n                #m2 = coverage < 0.45\r\n                #if (m1 * m2).any():\r\n                #\tcontinue\r\n\r\n                center = (boxes[:, :2] + boxes[:, 2:]) / 2\r\n                mask = np.logical_and(rect[:2] <= center, center < rect[2:]).all(axis=1)\r\n\r\n                #mask = coverage >= 0.45\r\n                #mask\r\n                if not mask.any():\r\n                    continue\r\n\r\n                if min_iou <= iou.max() and iou.min() <= max_iou:\r\n                    params.append((rect, mask))\r\n                    break\r\n        rect, mask = params[self.random.randint(len(params))]\r\n\r\n        img = img[rect[1]:rect[3], rect[0]:rect[2]]\r\n        boxes[:, :2] = np.clip(boxes[:, :2], rect[:2], rect[2:])\r\n        boxes[:, :2] = boxes[:, :2] - rect[:2]\r\n        boxes[:, 2:] = np.clip(boxes[:, 2:], rect[:2], rect[2:])\r\n        boxes[:, 2:] = boxes[:, 2:] - rect[:2]\r\n        if mask is not None:\r\n            boxes[np.logical_not(mask), :] = 0\r\n\r\n        if len(cbs[0]) == 4:\r\n            return img, boxes\r\n        else:\r\n            return img, _to_coords(boxes)\r\n\r\n\r\n\r\n\r\n\r\n# Random crop with size 8%-100% and aspect ratio 3/4 - 4/3. (Inception-style)\r\nclass RandomSizedCrop(object):\r\n    def __init__(self, size, mode=\'linear\', anchor=None, random_state=np.random):\r\n        self.size = size \r\n        self.mode = mode\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n        self.scale = Scale(size, mode=mode, anchor=anchor)\r\n        self.crop = CenterCrop(size)\r\n\r\n    def __call__(self, img, cds=None):\r\n        interp_mode = (self.random.choice(list(InterpolationFlags.values())) if self.mode is None \r\n                                   else InterpolationFlags.get(self.mode, cv2.INTER_LINEAR))\r\n\r\n        h, w = img.shape[:2]\r\n\r\n        for _ in range(10):\r\n            area = h * w\r\n            target_area = self.random.uniform(0.16, 1.0) * area   # 0.08~1.0\r\n            aspect_ratio = self.random.uniform(3. / 4, 4. / 3)\r\n\r\n            tw = int(round(np.sqrt(target_area * aspect_ratio)))\r\n            th = int(round(np.sqrt(target_area / aspect_ratio)))\r\n\r\n            if self.random.random_sample() < 0.5:\r\n                tw, th = th, tw \r\n\r\n            if tw <= w and th <= h:\r\n                x1 = self.random.randint(0, w - tw + 1)\r\n                y1 = self.random.randint(0, h - th + 1)\r\n\r\n                img = img[y1:y1 + th, x1:x1 + tw]\r\n\r\n                resize = lambda im: cv2.resize(im, (self.size, self.size), interpolation=interp_mode)\r\n                purer = lambda im: cv2.resize(im, (self.size, self.size), interpolation=cv2.INTER_NEAREST)\r\n                img = HalfBlood(img, self.anchor, resize, purer)\r\n\r\n                if cds is not None:\r\n                    scale_x = self.size / float(tw)\r\n                    scale_y = self.size / float(th)\r\n\r\n                    return img, _coords_clamp([[scale_x*(x-x1), scale_y*(y-y1)] for x, y in cds], img.shape)\r\n                else:\r\n                    return img\r\n\r\n        # Fallback\r\n        return self.crop(self.scale(img, cds=cds), cds=cds)\r\n\r\n\r\nclass GridCrop(object):\r\n    def __init__(self, size, grid=5, random_state=np.random):\r\n        # 4 grids, 5 grids or 9 grids\r\n        if isinstance(size, numbers.Number):\r\n            size = (int(size), int(size))\r\n        self.size = size\r\n\r\n        self.grid = grid\r\n        self.random = random_state\r\n        self.map = {\r\n            0: lambda w, h, tw, th: (            0,            0),\r\n            1: lambda w, h, tw, th: (       w - tw,            0),\r\n            2: lambda w, h, tw, th: (       w - tw,       h - th),\r\n            3: lambda w, h, tw, th: (            0,       h - th),\r\n            4: lambda w, h, tw, th: ((w - tw) // 2, (h - th) // 2),\r\n            5: lambda w, h, tw, th: ((w - tw) // 2,            0),\r\n            6: lambda w, h, tw, th: (       w - tw, (h - th) // 2),\r\n            7: lambda w, h, tw, th: ((w - tw) // 2,       h - th),\r\n            8: lambda w, h, tw, th: (            0, (h - th) // 2),\r\n        }\r\n\r\n    def __call__(self, img, cds=None, index=None):\r\n        h, w = img.shape[:2]\r\n        tw, th = self.size\r\n        if index is None:\r\n            index = self.random.randint(0, self.grid)\r\n        if index not in self.map:\r\n            raise Exception(\'invalid index\')\r\n\r\n        x1, y1 = self.map[index](w, h, tw, th)\r\n        img = img[y1:y1 + th, x1:x1 + tw]\r\n\r\n        if cds is not None:\r\n            return img, _coords_clamp([[x - x1, y - y1] for x, y in cds], img.shape)\r\n        else:\r\n            return img\r\n\r\n\r\n\r\nclass Resize(object):\r\n    def __init__(self, size, mode=\'linear\', anchor=None, random_state=np.random):\r\n        if isinstance(size, numbers.Number):\r\n            size = (int(size), int(size))\r\n        self.size = size\r\n\r\n        self.mode = mode\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None):\r\n        interp_mode = (self.random.choice(list(InterpolationFlags.values())) if self.mode is None \r\n                                   else InterpolationFlags.get(self.mode, cv2.INTER_LINEAR))\r\n\r\n        h, w = img.shape[:2]\r\n        tw, th = self.size\r\n\r\n        resize = lambda im: cv2.resize(im, (tw, th), interpolation=interp_mode)\r\n        purer = lambda im: cv2.resize(im, (tw, th), interpolation=cv2.INTER_NEAREST)\r\n        img = HalfBlood(img, self.anchor, resize, purer)\r\n\r\n        if cds is not None:\r\n            s_x = tw / float(w)\r\n            s_y = th / float(h)\r\n            return img, np.array([[s_x * x, s_y * y] for x, y in cds])\r\n        else:\r\n            return img\r\n\r\n\r\nclass RandomResize(object):\r\n    def __init__(self, scale_range=(0.8, 1.2), ratio_range=1., mode=\'linear\', anchor=None,\r\n                 random_state=np.random):\r\n\r\n        sr = scale_range\r\n        if isinstance(sr, numbers.Number):\r\n            sr = (min(sr, 1. / sr), max(sr, 1. / sr))\r\n        assert  max(sr) <= 5\r\n        self.sr = sr\r\n\r\n        rr = ratio_range\r\n        if isinstance(rr, numbers.Number):\r\n            rr = (min(rr, 1. / rr), max(rr, 1. / rr))\r\n        assert  max(rr) <= 5\r\n        self.rr = rr\r\n        \r\n        self.mode = mode\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None):\r\n        interp_mode = (self.random.choice(list(InterpolationFlags.values())) if self.mode is None \r\n                                   else InterpolationFlags.get(self.mode, cv2.INTER_LINEAR))\r\n\r\n        h, w = img.shape[:2]\r\n\r\n        scale_factor = _loguniform(self.sr, self.random)\r\n        ratio_factor = _loguniform(self.rr, self.random)\r\n\r\n        th = int(h * scale_factor)\r\n        tw = int(w * scale_factor * ratio_factor)\r\n\r\n        resize = lambda im: cv2.resize(im, (tw, th), interpolation=interp_mode)\r\n        purer = lambda im: cv2.resize(im, (tw, th), interpolation=cv2.INTER_NEAREST)\r\n        img = HalfBlood(img, self.anchor, resize, purer)\r\n        \r\n        if cds is not None:\r\n            s_x = tw / float(w)\r\n            s_y = th / float(h)\r\n            return img, np.array([[s_x * x, s_y * y] for x, y in cds])\r\n        else:\r\n            return img\r\n\r\n\r\nclass ElasticTransform(object):\r\n    def __init__(self, alpha=1000, sigma=40, mode=\'linear\', border=\'constant\', fillval=0, \r\n                 anchor=None, random_state=np.random):\r\n\r\n        if isinstance(fillval, numbers.Number):\r\n            fillval = [fillval] * 3\r\n\r\n        self.alpha, self.sigma = alpha, sigma\r\n        self.mode = mode\r\n        self.border = BorderTypes.get(border, cv2.BORDER_REPLICATE)\r\n        self.fillval = fillval\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n\r\n    def __call__(self, img, cds=None):\r\n        interp_mode = (self.random.choice(list(InterpolationFlags.values())) if self.mode is None \r\n                                   else InterpolationFlags.get(self.mode, cv2.INTER_LINEAR))\r\n\r\n        shape = img.shape[:2]\r\n\r\n        ksize = self.sigma * 4 + 1\r\n        dx = cv2.GaussianBlur((self.random.rand(*img.shape[:2]) * 2 - 1).astype(np.float32), \r\n                              (ksize, ksize), 0) * self.alpha\r\n        dy = cv2.GaussianBlur((self.random.rand(*img.shape[:2]) * 2 - 1).astype(np.float32), \r\n                              (ksize, ksize), 0) * self.alpha\r\n\r\n        y, x = np.meshgrid(np.arange(img.shape[0]), np.arange(img.shape[1]), indexing=\'ij\')\r\n        mapy, mapx = (y + dy).astype(np.float32), (x + dx).astype(np.float32)\r\n\r\n        elastic = lambda im: cv2.remap(im, mapx, mapy, interpolation=interp_mode, borderMode=self.border, borderValue=self.fillval)\r\n        purer = lambda im: cv2.remap(im, mapx, mapy, interpolation=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT)\r\n        img = HalfBlood(img, self.anchor, elastic, purer)\r\n\r\n        if cds is None:\r\n            return img\r\n        else:\r\n            cds_from = np.hstack([mapx.reshape(-1, 1), mapy.reshape(-1, 1)])\r\n            cds_to = np.hstack([x.reshape(-1, 1), y.reshape(-1, 1)])\r\n            cds_ = []\r\n            for coord in cds:\r\n                # TODO: top-k\r\n                ind = np.argmin(np.sum((coord - cds_from)**2, axis=1))\r\n                cds_.append(cds_to[ind])\r\n            return img, _coords_clamp(cds_, img.shape)\r\n\r\n\r\nclass RandomRotate(object):\r\n    def __init__(self, angle_range=(-30.0, 30.0), mode=\'linear\', border=\'constant\', fillval=0, \r\n                 anchor=None, random_state=np.random):   \r\n        if isinstance(angle_range, numbers.Number):\r\n            angle_range = (-angle_range, angle_range)\r\n        self.angle_range = angle_range\r\n\r\n        if isinstance(fillval, numbers.Number):\r\n            fillval = [fillval] * 3\r\n\r\n        self.mode = mode\r\n        self.border = BorderTypes.get(border, cv2.BORDER_REPLICATE)\r\n        self.fillval = fillval\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None):\r\n        interp_mode = (self.random.choice(list(InterpolationFlags.values())) if self.mode is None \r\n                                   else InterpolationFlags.get(self.mode, cv2.INTER_LINEAR))\r\n\r\n        h, w = img.shape[:2]\r\n        angle = self.random.uniform(*self.angle_range)\r\n\r\n        M = cv2.getRotationMatrix2D((w/2., h/2.), angle, 1)\r\n\r\n        rotate = lambda im: cv2.warpAffine(im, M, dsize=(w, h), flags=self.mode, borderMode=self.border, borderValue=self.fillval)\r\n        purer = lambda im: cv2.warpAffine(im, M, dsize=(w, h), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT)\r\n        img = HalfBlood(img, self.anchor, rotate, purer)\r\n\r\n        if cds is not None:\r\n            cos = np.cos(angle * np.pi / 180.)\r\n            sin = np.sin(angle * np.pi / 180.)\r\n            cds_ = []\r\n            for x, y in cds:\r\n                x, y = x - w/2., -(y - h/2.)\r\n                x, y = cos*x - sin*y, sin*x + cos*y\r\n                x, y = x + w/2., -y + h/2.\r\n                cds_.append([x, y])\r\n            return img, _coords_clamp(cds_, img.shape)\r\n        else:\r\n            return img\r\n\r\n\r\nclass Rotate90(object):\r\n    def __init__(self, random_state=np.random):\r\n        # 4 directions\r\n        self.random = random_state\r\n\r\n        self.map = {\r\n            0: lambda x, y, w, h: (    x,     y),\r\n            1: lambda x, y, w, h: (    y, w-1-x),\r\n            2: lambda x, y, w, h: (w-1-x, h-1-y),\r\n            3: lambda x, y, w, h: (h-1-y,     x),\r\n        }\r\n\r\n    def __call__(self, img, cds=None, index=None):\r\n        h, w = img.shape[:2]\r\n        if index is None:\r\n            index = self.random.randint(0, 4)\r\n        if index not in self.map:\r\n            raise Exception(\'invalid index\')\r\n\r\n        img = np.rot90(img, index)\r\n\r\n        if cds is not None:\r\n            return img, np.array([self.map[index](x, y, w, h) for x, y in cds])\r\n        else:\r\n            return img\r\n\r\n\r\nclass RandomShift(object):\r\n    def __init__(self, tx=(-0.1, 0.1), ty=None, border=\'constant\', fillval=0, anchor=None, random_state=np.random):   \r\n        if isinstance(tx, numbers.Number):\r\n            tx = (-abs(tx), abs(tx))\r\n        assert isinstance(tx, tuple) and np.abs(tx).max() < 1\r\n        if ty is None:\r\n            ty = tx\r\n        elif isinstance(ty, numbers.Number):\r\n            ty = (-abs(ty), abs(ty))\r\n        assert isinstance(ty, tuple) and np.abs(ty).max() < 1\r\n        self.tx, self.ty = tx, ty\r\n\r\n        if isinstance(fillval, numbers.Number):\r\n            fillval = [fillval] * 3\r\n\r\n        self.border = BorderTypes.get(border, cv2.BORDER_REPLICATE)\r\n        self.fillval = fillval\r\n        self.anchor = anchor\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None):\r\n        h, w = img.shape[:2]\r\n        tx = self.random.uniform(*self.tx) * w \r\n        ty = self.random.uniform(*self.ty) * h\r\n\r\n        M = np.float32([[1,0,tx],[0,1,ty]])\r\n\r\n        shift = lambda im: cv2.warpAffine(im, M, dsize=(w, h), flags=cv2.INTER_NEAREST, borderMode=self.border, borderValue=self.fillval)\r\n        purer = lambda im: cv2.warpAffine(im, M, dsize=(w, h), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT)\r\n        img = HalfBlood(img, self.anchor, shift, purer)\r\n\r\n        if cds is not None:\r\n            return img, _coords_clamp([[x + tx, y + ty] for x, y in cds], img.shape)\r\n        else:\r\n            return img\r\n\r\n\r\nclass HorizontalFlip(object):\r\n    def __init__(self, prob=0.5, random_state=np.random):\r\n        self.prob = prob\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None, flip=None):\r\n        if flip is None:\r\n            flip = self.random.random_sample() < self.prob\r\n\r\n        if flip:\r\n            img = img[:, ::-1]\r\n        \r\n        if cds is not None:\r\n            h, w = img.shape[:2]\r\n            t = lambda x, y: [w-1-x, y] if flip else [x, y]\r\n            return img, np.array([t(x, y) for x, y in cds])\r\n        else:\r\n            return img\r\n\r\n\r\nclass VerticalFlip(object):\r\n    def __init__(self, prob=0.5, random_state=np.random):\r\n        self.prob = prob\r\n        self.random = random_state\r\n\r\n    def __call__(self, img, cds=None, flip=None):\r\n        if flip is None:\r\n            flip = self.random.random_sample() < self.prob\r\n\r\n        if flip:\r\n            img = img[::-1, :]\r\n        \r\n        if cds is not None:\r\n            h, w = img.shape[:2]\r\n            t = lambda x, y: [x, h-1-y] if flip else [x, y]\r\n            return img, np.array([t(x, y) for x, y in cds])\r\n        else:\r\n            return img\r\n\r\n\r\n\'\'\'class RandomFlip(object):\r\n    def __init__(self, random_state=np.random):\r\n        self.random = random_state\r\n        self.transforms = [HorizontalFlip(random_state=np.random), \r\n                           VerticalFlip(random_state=np.random)]\r\n\r\n    def __call__(self, img, cds=None, index=None):\r\n        hori, vert = self.transforms\r\n        if index is None:\r\n            index = self.random.randint(0, 4)\r\n\r\n        if index == 0:\r\n            return hori(img, cds=cds, flip=False)\r\n        elif index == 1:\r\n            return hori(img, cds=cds, flip=True)\r\n        elif index == 2:\r\n            return vert(img, cds=cds, flip=True)\r\n        elif index == 3:\r\n            if cds is None:\r\n                img = hori(img, flip=True)\r\n                return vert(img, flip=True)\r\n            else:\r\n                img, cds = hori(img, cds=cds, flip=True)\r\n                return vert(img, cds=cds, flip=False)\r\n        else:\r\n            raise Exception(\'invalid index\')\'\'\'\r\n\r\n\r\n\r\n\r\n# Pipeline\r\n\r\nclass Lambda(object):\r\n    def __init__(self, lambd):\r\n        assert isinstance(lambd, types.LambdaType)\r\n        self.lambd = lambd \r\n\r\n    def __call__(self, *args):\r\n        return self.lambd(*args)\r\n\r\n\r\nclass Merge(object):\r\n    def __init__(self, axis=-1):\r\n        self.axis = axis\r\n\r\n    def __call__(self, *imgs):\r\n        # ad-hoc \r\n        if len(imgs) > 1 and not isinstance(imgs[0], collections.Sequence):\r\n            pass\r\n        elif len(imgs) == 1 and isinstance(imgs[0], collections.Sequence):   # unreliable\r\n            imgs = imgs[0]\r\n        elif len(imgs) == 1:\r\n            return imgs[0]\r\n        else:\r\n            raise Exception(\'input must be a sequence (list, tuple, etc.)\')\r\n\r\n        assert len(imgs) > 0 and all([isinstance(_, np.ndarray)\r\n                    for _ in imgs]), \'only support numpy array\'\r\n\r\n        shapes = []\r\n        imgs_ = []\r\n        for i, img in enumerate(imgs):\r\n            if img.ndim == 2:\r\n                img = np.expand_dims(img, axis=self.axis)\r\n            imgs_.append(img)\r\n            shape = list(img.shape)\r\n            shape[self.axis] = None\r\n            shapes.append(shape)\r\n        assert all([_ == shapes[0] for _ in shapes]), \'shapes must match\'\r\n        return np.concatenate(imgs_, axis=self.axis)\r\n\r\n\r\nclass Split(object):\r\n    def __init__(self, *slices, **kwargs):\r\n        slices_ = []\r\n        for s in slices:\r\n            if isinstance(s, collections.Sequence):\r\n                slices_.append(slice(*s))\r\n            else:\r\n                slices_.append(s)\r\n            assert all([isinstance(s, slice) for s in slices_]), \'slices must consist of slice instances\'\r\n\r\n        self.slices = slices_\r\n        self.axis = kwargs.get(\'axis\', -1)\r\n\r\n    def __call__(self, img):\r\n        if isinstance(img, np.ndarray):\r\n            result = []\r\n            for s in self.slices:\r\n                sl = [slice(None)] * img.ndim \r\n                sl[self.axis] = s \r\n                result.append(img[sl])\r\n            return result\r\n        else:\r\n            raise Exception(\'object must be a numpy array\')\r\n\r\n\r\nclass Branching(object):\r\n    # TODO\r\n    pass\r\n\r\nclass Bracket(object):\r\n    # TODO\r\n    pass\r\n\r\nclass Flatten(object):\r\n    # TODO \r\n    pass\r\n\r\nclass Permute(object):\r\n    # TODO\r\n    pass\r\n\r\n\r\nclass Compose(object):\r\n    def __init__(self, transforms, random_state=None, **kwargs):\r\n        if random_state is not None:\r\n            kwargs[\'random_state\'] = random_state\r\n        self.transforms = transforms_state(transforms, **kwargs)\r\n\r\n    def __call__(self, *data):\r\n        # ad-hoc \r\n        if len(data) >= 1 and not isinstance(data[0], collections.Sequence):\r\n            pass\r\n        elif len(data) == 1 and isinstance(data[0], collections.Sequence) and len(data[0]) > 0:   # unreliable\r\n            data = list(data[0])\r\n        else:\r\n            raise Exception(\'invalid input\')\r\n\r\n        for t in self.transforms:\r\n            if not isinstance(data, collections.Sequence):   # unreliable\r\n                data = [data]\r\n\r\n            if isinstance(t, collections.Sequence):\r\n                if len(t) > 1:\r\n                    assert isinstance(data, collections.Sequence) and len(data) == len(t)\r\n                    ds = []\r\n                    for i, d in enumerate(data):\r\n                        if callable(t[i]):\r\n                            ds.append(t[i](d))\r\n                        else:\r\n                            ds.append(d)\r\n                    data = ds\r\n                elif len(t) == 1:\r\n                    if callable(t[0]):\r\n                        data = [t[0](data[0])] + list(data)[1:]\r\n            elif callable(t):\r\n                data = t(*data)\r\n            elif t is not None:\r\n                raise Exception(\'invalid transform type\')\r\n\r\n        if isinstance(data, collections.Sequence) and len(data) == 1:   # unreliable\r\n            return data[0]\r\n        else:\r\n            return data\r\n\r\n    def set_random_state(self, random_state):\r\n        self.transforms = transforms_state(self.transforms, random=random_state)\r\n\r\n\r\nclass RandomCompose(Compose):\r\n    def __init__(self, transforms, random_state=None, **kwargs):\r\n        if random_state is None:\r\n            random_state = np.random\r\n        else:\r\n            kwargs[\'random_state\'] = random_state\r\n\r\n        self.transforms = transforms_state(transforms, **kwargs)\r\n        self.random = random_state\r\n\r\n    def __call__(self, *data):\r\n        self.random.shuffle(self.transforms)\r\n\r\n        return super(RandomCompose, self).__call__(*data)\r\n\r\n\r\nclass ToNumpy(object):\r\n    def __call__(self, pic):\r\n        # torch.FloatTensor -> np.ndarray\r\n        # or PIL Image -> np.ndarray\r\n        # TODO\r\n        pass\r\n\r\n\r\nclass ToTensor(object):\r\n    def __init__(self):\r\n        pass\r\n\r\n    def __call__(self, img):\r\n        # np.ndarray -> torch.FloatTensor\r\n        # or PIL Image -> torch.FloatTensor\r\n\r\n        # TODO: add option to choose whether div(255)\r\n        if isinstance(img, np.ndarray):\r\n            img = img.transpose((2, 0, 1))\r\n            return torch.from_numpy(np.ascontiguousarray(img)).float()\r\n        else:\r\n            # TODO\r\n            pass\r\n\r\nclass ToLongTensor(object):\r\n    def __init__(self):\r\n        pass \r\n    \r\n    def __call_(self, label):\r\n        if isinstance(label, np.ndarray):\r\n            return torch.from_numpy(label).long()\r\n\r\n\r\nclass BoxesToCoords(object):\r\n    def __init__(self, relative=False):\r\n        self.relative = relative\r\n\r\n    def bbox2coords(self, bbox):\r\n        xmin, ymin, xmax, ymax = bbox \r\n        return np.array([\r\n            [xmin, ymin],\r\n            [xmax, ymin],\r\n            [xmax, ymax],\r\n            [xmin, ymax],\r\n        ])\r\n\r\n    def __call__(self, img, boxes):\r\n        if len(boxes) == 0:\r\n            return img, np.array([])\r\n\r\n        h, w = img.shape[:2]\r\n        if self.relative:\r\n            boxes[:, 0] *= w\r\n            boxes[:, 2] *= w\r\n            boxes[:, 1] *= h\r\n            boxes[:, 3] *= h\r\n        return img, np.vstack([self.bbox2coords(_) for _ in boxes])\r\n\r\n\r\nclass CoordsToBoxes(object):\r\n    def __init__(self, relative=True):\r\n        self.relative = relative\r\n\r\n    def coords2bbox(self, cds, w, h):\r\n        xmin = np.clip(cds[:, 0].min(), 0, w - 1)\r\n        xmax = np.clip(cds[:, 0].max(), 0, w - 1)\r\n        ymin = np.clip(cds[:, 1].min(), 0, h - 1)\r\n        ymax = np.clip(cds[:, 1].max(), 0, h - 1)\r\n        return np.array([xmin, ymin, xmax, ymax])\r\n\r\n    def __call__(self, img, cds):\r\n        if len(cds) == 0:\r\n            return img, np.array([])\r\n\r\n        assert len(cds) % 4 == 0\r\n        num = len(cds) // 4\r\n\r\n        h, w = img.shape[:2]\r\n        boxcds = np.split(np.array(cds), np.arange(1, num) * 4)\r\n        boxes = np.array([self.coords2bbox(_, w, h) for _ in boxcds])\r\n\r\n        if self.relative:\r\n            boxes[:, 0] /= float(w) \r\n            boxes[:, 2] /= float(w)\r\n            boxes[:, 1] /= float(h)\r\n            boxes[:, 3] /= float(h)\r\n\r\n        return img, boxes\r\n\r\n\r\nclass OneHotMask(object):\r\n    def __init__(self, n_classes):\r\n        self.n_classes = n_classes\r\n\r\n    def __call__(self, mask):\r\n        if mask.ndim == 3 and mask.shape[2] == 1:\r\n            mask = mask[:, :, 0]\r\n        assert mask.ndim == 2 and mask.max() < self.n_classes\r\n\r\n        onehot_mask = np.zeros((mask.shape[0], mask.shape[1], self.n_classes), dtype=np.uint8)\r\n        for i in range(self.n_classes):\r\n            onehot_mask[:, :, i] = mask == i\r\n        return onehot_mask'"
models/BlitzNet.py,4,"b'# Detection part of BlitzNet (""BlitzNet: A Real-Time Deep Network for Scene Understanding"" Nikita Dvornik et al. ICCV17)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F \n\nfrom torchvision.models import resnet50, Bottleneck\nfrom collections import OrderedDict\n\n\n\nclass BlitzNet(object):\n\n    def __init__(self, n_classes, image_size=300, x4=True):\n        super(BlitzNet, self).__init__()\n        if image_size == 300:\n            self.config300(x4)\n        elif image_size == 512:\n            self.config512(x4)\n\n        self.Base = resnet50(pretrained=True)\n        self.Down = nn.Sequential(OrderedDict([\n            (\'layer5\', nn.Sequential(Bottleneck(2014, 512, 2, shortcut()), \n                                     Bottleneck(2048, 512, 1))),\n            (\'layer6\', nn.Sequential(Bottleneck(2014, 512, 2, shortcut()), \n                                     Bottleneck(2048, 512, 1))),\n            (\'layer7\', nn.Sequential(Bottleneck(2014, 512, 2, shortcut()), \n                                     Bottleneck(2048, 512, 1))),\n            (\'layer8\', nn.Sequential(Bottleneck(2014, 512, 2, shortcut()), \n                                     Bottleneck(2048, 512, 1)))]))\n        self.Up = nn.Sequential(OrderedDict([\n            (\'rev_layer7\', BottleneckSkip(2048, 2048, 512)),\n            (\'rev_layer6\', BottleneckSkip(2048 if self.pred_layers[0] == \'rev_layer6\' else 512, 2048, 512)),\n            (\'rev_layer5\', BottleneckSkip(512,  2048, 512)),\n            (\'rev_layer4\', BottleneckSkip(512,  2048, 512)),\n            (\'rev_layer3\', BottleneckSkip(512,  1024, 512)),\n            (\'rev_layer2\', BottleneckSkip(512,  512,  512)),\n            (\'rev_layer1\', BottleneckSkip(512,  256,  512))]))\n\n        n_boxes = len(self.config[\'aspect_ratios\']) + 1\n        self.Loc = nn.ModuleList([])\n        self.Conf = nn.ModuleList([])\n        for i in range(len(self.config[\'grids\')):\n            self.Loc.append(nn.Conv2d(512, n_boxes * 4, 3, padding=1))\n            self.Conf.append(nn.Conv2d(512, n_boxes * (self.n_classes + 1), 3, padding=1))\n        #self.Loc  = nn.Conv2d(512, n_boxes * 4, 3, padding=1)\n        #self.Conf = nn.Conv2d(512, n_boxes * (self.n_classes+1), 3, padding=1)\n\n    def forward(self, x):\n        skips, xs = [], []\n        for name, m in self.Base._modules.items():\n            x = m(x)\n            if name in self.skip_layers:\n                skips.append(x)\n            if name == \'layer4\':\n                break\n\n        for name, m in self.Down._modules.items():\n            if name in self.skip_layers:\n                x = m(x)\n                skips.append(x)\n\n        ind = -2 \n        for name, m in self.Up._modules.items():\n            if name in self.pred_layers:\n                x = m(x, skips[ind])\n                xs.append(x)\n                ind -= 1\n\n        xs = [F.avg_pool2d(xs[0], xs[0].size()[2:])] + xs\n        return self._prediction(xs[::-1])\n\n\n    def _prediction(self, xs):\n        locs = []\n        confs = []\n        for i, x in enumerate(xs):\n            loc = self.Loc[i](x) if isinstance(self.Loc, nn.ModuleList) else self.Loc(x)\n            loc = loc.permute(0, 2, 3, 1).contiguous().view(loc.size(0), -1, 4)\n            locs.append(loc)\n\n            conf = self.Conf[i](x) if isinstance(self.Conf, nn.ModuleList) else self.Conf(x)\n            conf = conf.permute(0, 2, 3, 1).contiguous().view(conf.size(0), -1, self.n_classes + 1)\n            confs.append(conf)\n        return torch.cat(locs, dim=1), torch.cat(confs, dim=1)\n\n\n    def config300(x4=True):\n        self.skip_layers = [\'layer1\', \'layer2\', \'layer3\', \'layer4\', \'layer5\', \'layer6\', \'layer7\']\n        self.pred_layers = [\'rev_layer6\', \'rev_layer5\', \'rev_layer4\', \'rev_layer3\', \'rev_layer2\'] + [\'rev_layer1\']*x4\n        self.config = {\n            \'name\': \'BlitzNet300-resnet50-Det\' + \'-s4\' if x4 else \'-s8\',\n            \'image_size\': 300,\n            \'grids\': [75]*x4 + [38, 19, 10, 5, 3, 1],\n            \'sizes\': ,\n            \'aspect_ratios\': (1/3.,  1/2.,  1,  2,  3),\n\n            \'batch_size\': 32,\n            \'init_lr\' = 1e-4,\n            \'stepvalues\' = (35000, 50000),    \n            \'max_iter\' = 65000\n        }\n\n    def config512(x4=False):\n        self.skip_layers = [\'layer1\', \'layer2\', \'layer3\', \'layer4\', \'layer5\', \'layer6\', \'layer7\', \'layer8\']\n        self.pred_layers = [\'rev_layer7\', \'rev_layer6\', \'rev_layer5\', \'rev_layer4\', \'rev_layer3\', \'rev_layer2\'] + (\n                           [\'rev_layer1\']*x4)\n        self.config = {\n            \'name\': \'BlitzNet512-resnet50-Det\' + \'-s4\' if x4 else \'-s8\',\n            \'image_size\': 512,\n            \'grids\': [128]*x4 + [64, 32, 16, 8, 4, 2, 1],\n            \'sizes\': ,\n            \'aspect_ratios\': (1/3.,  1/2.,  1,  2,  3),\n\n            \'batch_size\': 16,\n            \'init_lr\' = 1e-4,\n            \'stepvalues\' = (45000, 60000),\n            \'max_iter\' = 75000\n        }\n\n\n\n\nclass BottleneckSkip(nn.Module):\n    def __init__(self, in_channels1, in_channels2, out_channels, stride=1, \n                    mode=\'bilinear\'):\n        super().__init__()\n        self.mode = mode\n\n        if stride != 1 or in_channels1 != out_channels:\n            self.shortcut = shortcut(in_channels1, out_channels, stride)\n        else:\n            self.shortcut = nn.Sequential()\n\n        self.residual = nn.Sequential(\n            nn.Conv2d(in_channels1 + in_channels2, out_channels // 4, 1, bias=False)\n            nn.BatchNorm2d(out_channels // 4)\n            nn.ReLU(inplace=True)\n            nn.Conv2d(out_channels // 4, out_channels // 4, 3, padding=1, stride=stride, bias=False)\n            nn.BatchNorm2d(out_channels // 4)\n            nn.ReLU(inplace=True)\n            nn.Conv2d(out_channels // 4, out_channels, 1, bias=False)\n            nn.BatchNorm2d(out_channels))\n\n    def forward(self, x, skip):\n        x = F.upsample(x, size=skip.size()[2:], mode=self.mode)\n        shortcut = self.shortcut(x)\n        residual = self.residual(torch.cat([x, skip], dim=1))\n\n        return F.relu(shortcut + residual, inplace=True)\n\n\n\ndef shortcut(in_channels=2048, out_channels=2048, stride=2):\n    #if in_channels == out_channels:    # in BlitzNet\'s tensorflow implementation\n    #    return nn.MaxPool2d(1, stride=stride)\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n        nn.BatchNorm2d(out_channels))\n'"
models/DSOD.py,7,"b'# ""DSOD: Learning Deeply Supervised Object Detectors from Scratch"" Zhiqiang Shen et al. ICCV17\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F \n\nimport numpy as np\nfrom collections import OrderedDict\n\n\n\nclass L2Norm(nn.Module):\n    def __init__(self, n_channels, scale=20):\n        super(L2Norm, self).__init__()\n        self.scale = nn.Parameter(torch.FloatTensor(np.ones((1, n_channels, 1, 1))))\n        nn.init.constant(self.scale, scale)\n\n    def forward(self, x):   \n        x = x * x.pow(2).sum(1, keepdim=True).clamp(min=1e-10).rsqrt()\n        return self.scale * x\n\n\n\nclass DSOD300(nn.Module):\n\n    config = {\n        \'name\': \'DSOD300-64-192-48-1\',\n        \'image_size\': 300,\n        \'grids\': (38, 19, 10, 5, 3, 1),\n        \'aspect_ratios\': ((1/2.,  1,  2),   \n                          (1/3.,  1/2.,  1,  2,  3), \n                          (1/3.,  1/2.,  1,  2,  3), \n                          (1/3.,  1/2.,  1,  2,  3), \n                          (1/2.,  1,  2),\n                          (1/2.,  1,  2)),\n        #\'sizes\': [s / 300. for s in (30, 60, 101, 152, 206, 264, 315)],\n        \'steps\': [s / 300. for s in [8, 16, 32, 64, 100, 300]],\n        \'sizes\': [s / 300. for s in [30, 60, 111, 162, 213, 264, 315]],\n\n        #\'n_gpus\': 8,\n        #\'batch_size\': 128,\n        #\'init_lr\': 0.1,\n        #\'stepvalues\': (20000, 40000, 60000, 80000),\n        #\'max_iter\': 100000,\n    }\n\n    def __init__(self, n_classes, growth_rate=48):\n        super(DSOD300, self).__init__()\n        self.n_classes = n_classes\n\n        depth = [6, 8, 8, 8]\n        channels = [128 + growth_rate * _ for _ in np.cumsum(depth)]   \n        # growth_rate 48: 416, 800, 1184, 1568\n\n        # backbone\n        self.Stem = nn.Sequential(\n            conv_bn_relu(3, 64, 3, stride=2, padding=1),\n            conv_bn_relu(64, 64, 3, padding=1),\n            conv_bn_relu(64, 128, 3, padding=1),\n            nn.MaxPool2d(2, ceil_mode=True))\n\n        self.Block12 = nn.Sequential(\n            DenseBlock(128, depth[0], growth_rate),\n            Transition(channels[0], channels[0], pool=True, ceil_mode=True),  # 75 -> 38\n            DenseBlock(channels[0], depth[1], growth_rate),\n            Transition(channels[1], channels[1]))\n\n        self.pool2 = nn.MaxPool2d(2, ceil_mode=True)   # 38 -> 19\n        self.conv2 = bn_relu_conv(channels[1], 256, 1)\n\n        self.Block34 = nn.Sequential(\n            DenseBlock(channels[1], depth[2], growth_rate),\n            Transition(channels[2], channels[2]),\n            DenseBlock(channels[2], depth[3], growth_rate),\n            Transition(channels[3], 256))\n\n        # extra layers\n        self.Extra = nn.ModuleList([\n            LHRH(512, 512, ceil_mode=True),            # 19 -> 10\n            LHRH(512, 256, ceil_mode=True),            # 10 -> 5\n            LHRH(256, 256, ceil_mode=True),            #  5 -> 3\n            LHRH(256, 256)])                           #  3 -> 1\n        n_channels = [channels[1], 512, 512, 256, 256, 256]\n\n        # prediction layers\n        self.L2Norm = nn.ModuleList()\n        self.Loc = nn.ModuleList()\n        self.Conf = nn.ModuleList()\n        for i, ar in enumerate(self.config[\'aspect_ratios\']):\n            n = len(ar) + 1\n            self.L2Norm.append(L2Norm(n_channels[i], 20))\n            self.Loc.append(nn.Conv2d(n_channels[i], n * 4, 3, padding=1))\n            self.Conf.append(nn.Conv2d(n_channels[i], n * (self.n_classes + 1), 3, padding=1))\n\n        # weights initialization\n        self.apply(self.weights_init)\n\n    def forward(self, x):\n        xs = []\n        x = self.Stem(x)\n        x = self.Block12(x)\n        xs.append(x)\n\n        x = self.pool2(x)\n        x2 = self.conv2(x)\n        x = self.Block34(x)\n        x = torch.cat([x2, x], dim=1)\n        xs.append(x)\n\n        for m in self.Extra:\n            x = m(x)\n            xs.append(x)\n\n        return self._prediction(xs)\n\n    def _prediction(self, xs):\n        locs = []\n        confs = []\n        for i, x in enumerate(xs):\n            x = self.L2Norm[i](x)\n\n            loc = self.Loc[i](x)\n            loc = loc.permute(0, 2, 3, 1).contiguous().view(loc.size(0), -1, 4)\n            locs.append(loc)\n\n            conf = self.Conf[i](x)\n            conf = conf.permute(0, 2, 3, 1).contiguous().view(conf.size(0), -1, self.n_classes + 1)\n            confs.append(conf)\n        return torch.cat(locs, dim=1), torch.cat(confs, dim=1)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.xavier_uniform(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.zero_()\n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n\n    def init_parameters(self, x):\n        pass\n            \n\n\n\n\ndef bn_relu_conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n    return nn.Sequential(\n        nn.BatchNorm2d(in_channels),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))\n\ndef conv_bn_relu(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True)\n    )\n\n\n\nclass DenseBlock(nn.Module):\n    def __init__(self, in_channels, block_depth, growth_rate=48):\n        super(DenseBlock, self).__init__()\n\n        class DenseLayer(nn.Module):\n            def __init__(self, in_channels, growth_rate, widen=1, dropout=0.):\n                super(DenseLayer, self).__init__()\n\n                self.conv1 = bn_relu_conv(in_channels, growth_rate * widen, 1)\n                self.conv2 = bn_relu_conv(growth_rate * widen, growth_rate, 3, padding=1)\n                self.dropout = dropout\n\n            def forward(self, x):\n                out = self.conv1(x)\n                out = self.conv2(out)\n                if self.dropout > 0:\n                    out = F.dropout(out, p=self.dropout, training=self.training)\n                return torch.cat([x, out], 1)\n\n        layers = []\n        for i in range(block_depth):\n            layers.append(DenseLayer(in_channels + i * growth_rate, growth_rate))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass Transition(nn.Module):\n    def __init__(self, in_channels, out_channels, pool=False, ceil_mode=False, dropout=0.):\n        super(Transition, self).__init__()\n\n        self.conv = bn_relu_conv(in_channels, out_channels, 1)\n        self.pool = nn.MaxPool2d(2, ceil_mode=ceil_mode) if pool else nn.Sequential()\n        self.dropout = dropout\n\n    def forward(self, x):\n        out = self.conv(x)\n        if self.dropout > 0:\n            out = F.dropout(out, p=self.dropout, training=self.training)\n        return self.pool(out)\n\n\n# Learning Half and Reusing Half\nclass LHRH(nn.Module):\n    def __init__(self, in_channels, out_channels, widen=1, dropout=0., ceil_mode=False):\n        super(LHRH, self).__init__()\n\n        self.conv1_1 = bn_relu_conv(in_channels, int(out_channels / 2 * widen), 1)\n        self.conv1_2 = bn_relu_conv(int(out_channels / 2 * widen), out_channels // 2, 3, \n                                    padding=1 * ceil_mode, stride=2)\n        self.pool2 = nn.MaxPool2d(2, ceil_mode=ceil_mode)\n        self.conv2 = bn_relu_conv(in_channels, out_channels // 2, 1)\n        self.dropout = dropout\n\n    def forward(self, x):\n        out1 = self.conv1_2(self.conv1_1(x))\n        out2 = self.conv2(self.pool2(x))\n        if self.dropout > 0:\n            out1 = F.dropout(out1, p=self.dropout, training=self.training)\n            out2 = F.dropout(out2, p=self.dropout, training=self.training)\n        return torch.cat([out1, out2], 1)'"
models/RRC.py,5,"b'# ""Accurate Single Stage Detector Using Recurrent Rolling Convolution"" Jimmy Ren et al. (2017) \'RRC\'\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functonal as F\n\nimport os\nimport numbers\nimport numpy as np\n\nfrom vgg import VGG16\n\n\n# KITTI object detection\nclass RRC(nn.Module):\n    \n    img_size = (1272, 375)\n    # ((159, 47), (80, 24), (40, 12), (20, 6), (10, 3))\n    config = {\n        \'name\': \'RRC-KITTI\',\n        \'image_size\' = img_size,\n        \'grids\' = _feature_maps(img_size, start_level=3, n_levels=5, ceil_mode=True),\n        \'aspect_ratios\': ((1/2.,  1,  2), \n                          (1/3.,  1/2.,  1,  2,  3),\n                          (1/3.,  1/2.,  1,  2,  3),\n                          (1/3.,  1/2.,  1,  2,  3),\n                          (1/3.,  1/2.,  1,  2,  3)),\n        \'sizes\': _sizes(img_size, start_level=3, n_levels=5, min_ratio=0.15, max_ratio=0.85),\n        \'discretize\': 2,   # more anchor boxes  # WIP\n\n        \'batch_size\': 1,\n        \'init_lr\': 0.0005,\n        \'stepvalues\': (30000,),\n        \'max_iter\': 60000,\n\n        \'pos_thresh\': 0.7,\n        \'neg_thresh\': 0.5,\n        \'loc_weight\': 2.,\n    }\n\n    def __init__(self, n_classes=1):\n        super().__init__()\n\n        self.n_classes = n_classes\n        self.rolling_times = 4\n        self.rolling_ratio = 0.075\n\n        self.Base = VGG16()\n        self.Extra = nn.Sequential(OrderedDict([\n            (\'extra1_1\', nn.Conv2d(1024, 256, 1)),\n            (\'extra1_2\', nn.Conv2d(256, 256, 3, padding=1, stride=2)),\n            (\'extra2_1\', nn.Conv2d(256, 128, 1)),\n            (\'extra2_2\', nn.Conv2d(128, 256, 3, padding=1, stride=2)),\n            (\'extra3_1\', nn.Conv2d(256, 128, 1)),\n            (\'extra3_2\', nn.Conv2d(128, 256, 3, padding=1, stride=2))]))\n        self.pred_layers = [\'conv4_3\', \'conv7\', \'extra1_2\', \'extra2_2\', \'extra3_2\']\n\n        self.L2Norm = nn.ModuleList([L2Norm(512, 20)])\n        self.l2norm_layers = [\'conv4_3\']\n\n        # intermediate layers\n        self.Inter = nn.ModuleList([\n            nn.Sequential(nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True))\n            nn.Sequential(nn.Conv2d(1024, 256, 3, padding=1), nn.ReLU(inplace=True))\n            nn.Sequential(),\n            nn.Sequential(),\n            nn.Sequential()])\n        n_channels = [256, 256, 256, 256, 256]\n\n        # Recurrent Rolling\n        self.RollLeft = nn.ModuleList([])\n        self.RollRight = nn.ModuleList([])\n        self.Roll = nn.ModuleList([])\n        for i in range(len(n_channels)):\n            n_out = int(n_channels[i] * self.rolling_ratio)\n            if i > 0:\n                self.RollLeft.append( nn.Sequential(\n                    nn.Conv2d(n_channels[i-1], n_out, 1), \n                    nn.ReLU(inplace=True), \n                    nn.MaxPool2d(2, ceil_mode=True)))\n            if i < len(n_channels) - 1:\n                self.RollRight.append( nn.Sequential(\n                    nn.Conv2d(n_channels[i+1], n_out, 1), \n                    nn.Relu(inplace=True), \n                    nn.ConvTranspose2d(n_out, n_out, kernel_size=4, stride=2, padding=1)))\n\n            n_out = n_out * (int(i>0) + int(i<len(n_channels)-1))\n            self.Roll.append(nn.Sequential(\n                    nn.Conv2d(n_channels[i] + n_out, n_channels[i], 1), \n                    nn.ReLU(inplace=True)))\n\n        # Prediction\n        self.Loc = nn.ModuleList([])\n        self.Conf = nn.ModuleList([])\n        for i in range(len(n_channels)):\n            n_boxes = len(self.config[\'aspect_ratios\'][i]) + 1\n            self.Loc.append(nn.Conv2d(n_channels[i], n_boxes * 4, 3, padding=1))\n            self.Conf.append(nn.Conv2d(n_channels[i], n_boxes * (self.n_classes + 1), 3, padding=1))\n\n    def forward(self, x):\n        xs = []\n        for name, m in itertools.Chain(self.Base._modules.items(),\n                                       self.Extra._modules.items())\n            if isinstance(m, nn.Conv2d):\n                x = F.relu(m(x), inplace=True)\n            else:\n                x = m(x)\n\n            if name in self.pred_layers:\n                i = self.pred_layers.index(name)\n                if name in self.l2norm_layers:\n                    y = self.L2Norm[self.l2norm_layers.index(name)](x)\n                    xs.append(self.Inter[i](y))\n                else:\n                    xs.append(self.Inter[i](x))\n\n        return [self._prediction(_) for _ in self.recurrent_rolling(xs)]\n\n    def recurrent_rolling(self, xs):\n        num = len(xs)\n\n        ys = [xs]\n        for _ in range(self.rolling_times):\n            xs = []\n            for i in range(num):\n                x = []\n                if i > 0:\n                    x.append(self.RollLeft[i-1](ys[-1][i-1]))\n                x.append(ys[-1][i])\n                if i < num - 1:\n                    x.append(self.RollRight[i](ys[-1][i+1]))\n                x = torch.cat(x, dim=1)\n                x = self.Roll[i](x)\n                xs.append(x)\n            ys.append(xs)\n        return ys\n\n    def _prediction(self, xs)\n        locs = []\n        confs = []\n        for i, x in enumerate(xs):\n            loc = self.Loc[i](x)\n            loc = loc.permute(0, 2, 3, 1).contiguous().view(loc.size(0), -1, 4)\n            locs.append(loc)\n\n            conf = self.Conf[i](x)\n            conf = conf.permute(0, 2, 3, 1).contiguous().view(conf.size(0), -1, self.n_classes + 1)\n            confs.append(conf)\n\n        return (torch.cat(locs, dim=1), torch.cat(confs, dim=1))\n\n    def init_parameters(self, backbone=None):\n        def weights_init(m):\n            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n                nn.init.xavier_uniform(m.weight.data)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        self.apply(weights_init)\n\n        if backbone is not None and os.path.isfile(backbone):\n            self.Base.load_pretrained(backbone)\n\n\n\n\n\n# TODO\n# class RRC300(nn.Module):\n# class BRRC(nn.Module):\n\n\n\nclass L2Norm(nn.Module):\n    def __init__(self, n_channels, scale):\n        super(L2Norm, self).__init__()\n        self.scale = nn.Parameter(torch.FloatTensor(np.ones((1, n_channels, 1, 1))))\n        nn.init.constant(self.scale, scale)\n\n    def forward(self, x):   \n        x = x * x.pow(2).sum(1, keepdim=True).clamp(min=1e-10).rsqrt()\n        return self.scale * x\n\n\n\ndef _feature_maps(img_size, start_level, n_levels, ceil_mode=False):\n    def repeat(f, n):\n        if n==0:\n            return (lambda x: x)\n        return (lambda x: f(repeat(f, n-1)(x)))\n    half = lambda x: (int(np.ceil(x / 2.)) if ceil_mode else int(np.floor(x / 2.)))\n\n    if isinstance(img_size, numbers.Number):\n        img_size = (img_size, img_size)\n    return [(repeat(half, n)(img_size[0]), repeat(half, n)(img_size[1])) for n in range(\n        start_level, start_level + n_levels)]\n\n\ndef _sizes(img_size, start_level=3, n_levels=5, min_ratio=0.15, max_ratio=0.85):\n    # TODO\n    pass'"
models/RUN.py,5,"b'# ""Residual Features and Unified Prediction Network for Single Stage Detection"" Kyoungmin Lee et al. (2017) \'RUN\'\n\nimport torch\nimport torch.nn as nn \nimport torch.nn.functional as F \nfrom torch.autograd import Variable \n\nimport numpy as np\nimport os \nimport itertools\nfrom collections import OrderedDict\n\nfrom vgg import VGG16\n\n\n\nclass L2Norm(nn.Module):\n    def __init__(self,n_channels, scale=20):\n        super(L2Norm,self).__init__()\n        self.weight = nn.Parameter(torch.Tensor(n_channels))\n        nn.init.constant(self.weight, scale)\n\n    def forward(self, x):\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() + 1e-10\n        x /= norm\n        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n        return out\n\n\n\nclass RUN300(nn.Module):\n    \n    config = {\n        \'name\': \'RUN300-VGG16\',\n        \'image_size\': 300,\n        \'grids\': (38, 19, 10, 5, 3, 1),\n        \'aspect_ratios\': (1/3.,  1/2.,  1,  2,  3),  # 4 or 6\n        \'steps\': [s / 300. for s in [8, 16, 32, 64, 100, 300]],\n        \'sizes\': [s / 300. for s in (30, 60, 111, 162, 213, 264, 315)],  \n        #\'sizes\': [s / 300. for s in (30, 60, 104, 157, 210, 264, 315)],\n    } \n\n    def __init__(self, n_classes):\n        super(RUN300, self).__init__()\n        self.n_classes = n_classes\n\n        self.Base = VGG16()\n        self.Extra = nn.Sequential(OrderedDict([\n            (\'extra1_1\', nn.Conv2d(1024, 256, 1)),\n            (\'extra1_2\', nn.Conv2d(256, 512, 3, padding=1, stride=2)),\n            (\'extra2_1\', nn.Conv2d(512, 128, 1)),\n            (\'extra2_2\', nn.Conv2d(128, 256, 3, padding=1, stride=2)),\n            (\'extra3_1\', nn.Conv2d(256, 128, 1)),\n            (\'extra3_2\', nn.Conv2d(128, 256, 3)),\n            (\'extra4_1\', nn.Conv2d(256, 128, 1)),\n            (\'extra4_2\', nn.Conv2d(128, 256, 3))]))\n        self.pred_layers = [\'conv4_3\', \'conv7\', \'extra1_2\', \'extra2_2\', \'extra3_2\',\'extra4_2\']\n        n_channels = [512, 1024, 512, 256, 256, 256]\n\n        self.L2Norm = nn.ModuleList([L2Norm(512, 20)])\n        self.l2norm_layers = [\'conv4_3\']\n\n        # Multi-Way Residual Blocks\n        self.ResBlocks = nn.ModuleList()\n        for i in range(len(n_channels) - 1):\n            self.ResBlocks.append(\n                ThreeWay(n_channels[i], n_channels[i+1], self.config[\'grids\'][i], self.config[\'grids\'][i+1], \n                         out_channels=256))\n        self.ResBlocks.append(TwoWay(n_channels[-1], out_channels=256))\n\n        # Unified Prediction Module\n        n_boxes = len(self.config[\'aspect_ratios\']) + 1\n        #self.Loc  = nn.Conv2d(256, n_boxes * 4, 3, padding=1)\n        #self.Conf = nn.Conv2d(256, n_boxes * (self.n_classes+1), 3, padding=1)\n        self.Loc = nn.Sequential(\n            nn.Conv2d(256, 256, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, n_boxes * 4, 3, padding=1))\n        self.Conf = nn.Sequential(\n            nn.Conv2d(256, 256, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, n_boxes * (self.n_classes+1), 3, padding=1))\n        \n\n    def forward(self, x):\n        xs = []\n        for name, m in itertools.chain(self.Base._modules.items(), \n                                       self.Extra._modules.items()):\n            if isinstance(m, nn.Conv2d):\n                x = F.relu(m(x), inplace=True)\n            else:\n                x = m(x)\n\n            if name in self.pred_layers:\n                if name in self.l2norm_layers:\n                    i = self.l2norm_layers.index(name)\n                    xs.append(self.L2Norm[i](x))\n                else:\n                    xs.append(x)\n\n        return self._prediction(self.multiway(xs))\n\n    def multiway(self, xs):\n        ys = []\n        for i in range(len(xs)):\n            block = self.ResBlocks[i]\n            if isinstance(block, ThreeWay):\n                y = block(xs[i], xs[i+1])\n                ys.append(y)\n            elif isinstance(block, TwoWay):\n                y = block(xs[i])\n                ys.append(y)\n        return ys\n\n    def _prediction(self, ys):\n        locs = []\n        confs = []\n        for y in ys:\n            loc = self.Loc(y)\n            loc = loc.permute(0, 2, 3, 1).contiguous().view(loc.size(0), -1, 4)\n            locs.append(loc)\n\n            conf = self.Conf(y)\n            conf = conf.permute(0, 2, 3, 1).contiguous().view(conf.size(0), -1, self.n_classes + 1)\n            confs.append(conf)\n        return torch.cat(locs, dim=1), torch.cat(confs, dim=1)\n\n\n    def init_parameters(self, backbone=None):\n        def weights_init(m):\n            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n                nn.init.xavier_uniform(m.weight.data)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        self.apply(weights_init)\n\n        if backbone is not None and os.path.isfile(backbone):\n            self.Base.load_pretrained(backbone)\n\n\n\n\n\n# pre-activation\nclass TwoWay(nn.Module):\n    def __init__(self, in_channels, out_channels=256, bypass=False):\n        super().__init__()\n\n        if bypass and in_channels == out_channels:\n            self.branch1 = nn.Sequential()\n        else:\n            self.branch1 = conv_relu(in_channels, out_channels, 1)\n\n        self.branch2 = nn.Sequential(\n            conv_relu(in_channels, out_channels // 2, 1),\n            conv_relu(out_channels // 2, out_channels // 2, 3, padding=1),\n            conv_relu(out_channels // 2, out_channels, 1))\n\n    def forward(self, x):\n        return self.branch1(x) + self.branch2(x)\n\n\nclass ThreeWay(TwoWay):\n    def __init__(self, in_channels1, in_channels2, in_size1, in_size2, out_channels=256):\n        super().__init__(in_channels1, out_channels)\n\n        self.branch3 = nn.Sequential(\n            conv_relu(in_channels2, out_channels // 2, 3, padding=1),\n            deconv_relu(out_channels // 2, out_channels // 2, in_size2, in_size1),\n            conv_relu(out_channels // 2, out_channels, 1))\n\n    def forward(self, x1, x2):\n        return self.branch1(x1) + self.branch2(x1) + self.branch3(x2)\n\n        \n\n\ndef deconv_relu(in_channels, out_channels, in_size, out_size):\n    # TODO: handle case when size is tuple\n    if out_size == 2 * in_size:\n        dconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n    elif out_size == 2 * in_size - 1:\n        dconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n    elif out_size == 2 * in_size + 1:\n        dconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=0)\n    else:\n        raise ValueError(\'invalid size\')\n    return nn.Sequential(dconv, nn.ReLU(inplace=True))\n\n\ndef conv_relu(in_channels, out_channels, kernel_size, stride=1, padding=0):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n        nn.ReLU(inplace=True))\n\n\n\'\'\'\ndef conv_relu(in_channels, out_channels, kernel_size, stride=1, padding=0):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n        nn.ReLU(inplace=True))\n\n\ndef conv_bn_relu(in_channels, out_channels, kernel_size, stride=1, padding=0):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True))\n\ndef bn_relu_conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n    return nn.Sequential(\n        nn.BatchNorm2d(in_channels),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n\'\'\''"
models/SSD.py,5,"b'# ""SSD: Single Shot MultiBox Detector"" Wei Liu et al. ECCV16\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F \r\nfrom torch.autograd import Variable\r\n\r\nimport numpy as np\r\nimport os\r\nimport itertools\r\nfrom collections import OrderedDict\r\n\r\nfrom vgg import VGG16   # VGG16 as backbone\r\n\r\n\r\n\r\n\r\nclass L2Norm(nn.Module):\r\n    def __init__(self,n_channels, scale=20):\r\n        super(L2Norm,self).__init__()\r\n        self.weight = nn.Parameter(torch.Tensor(n_channels))\r\n        nn.init.constant(self.weight, scale)\r\n\r\n    def forward(self, x):\r\n        x /= (x.pow(2).sum(dim=1, keepdim=True).sqrt() + 1e-10)\r\n        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\r\n        return out\r\n\r\n\r\n\r\nclass SSD300(nn.Module):\r\n\r\n    config = {\r\n        \'name\': \'SSD300-VGG16\',\r\n        \'image_size\': 300,\r\n        \'grids\': (38, 19, 10, 5, 3, 1),   # feature map size\r\n        \'aspect_ratios\': ((1/2.,  1,  2), \r\n                          (1/3.,  1/2.,  1,  2,  3), \r\n                          (1/3.,  1/2.,  1,  2,  3), \r\n                          (1/3.,  1/2.,  1,  2,  3), \r\n                          (1/2.,  1,  2),\r\n                          (1/2.,  1,  2)),\r\n        \'steps\': [s / 300. for s in [8, 16, 32, 64, 100, 300]],\r\n        \'sizes\': [s / 300. for s in [30, 60, 111, 162, 213, 264, 315]],\r\n        \'prior_variance\': [0.1, 0.1, 0.2, 0.2],       # prior_variance imply relative weights of position offset, \r\n    }                                                 # width/height and class confidence\r\n    \r\n    def __init__(self, n_classes):\r\n        super(SSD300, self).__init__()\r\n        self.n_classes = n_classes\r\n\r\n        self.Base = VGG16()\r\n        self.Extra = nn.Sequential(OrderedDict([\r\n            (\'extra1_1\', nn.Conv2d(1024, 256, 1)),\r\n            (\'extra1_2\', nn.Conv2d(256, 512, 3, padding=1, stride=2)),\r\n            (\'extra2_1\', nn.Conv2d(512, 128, 1)),\r\n            (\'extra2_2\', nn.Conv2d(128, 256, 3, padding=1, stride=2)),\r\n            (\'extra3_1\', nn.Conv2d(256, 128, 1)),\r\n            (\'extra3_2\', nn.Conv2d(128, 256, 3)),\r\n            (\'extra4_1\', nn.Conv2d(256, 128, 1)),\r\n            (\'extra4_2\', nn.Conv2d(128, 256, 3)),\r\n        ]))\r\n        self.pred_layers = [\'conv4_3\', \'conv7\', \'extra1_2\', \'extra2_2\', \'extra3_2\', \'extra4_2\']\r\n        n_channels = [512, 1024, 512, 256, 256, 256]\r\n\r\n        self.L2Norm = nn.ModuleList([L2Norm(512, 20)])\r\n        self.norm_layers = [\'conv4_3\']   # decrease prediction layers\' influence on backbone\r\n\r\n        self.Loc = nn.ModuleList([])\r\n        self.Conf = nn.ModuleList([])\r\n        for i, ar in enumerate(self.config[\'aspect_ratios\']):\r\n            n = len(ar) + 1\r\n            self.Loc.append(nn.Conv2d(n_channels[i], n * 4, 3, padding=1))\r\n            self.Conf.append(nn.Conv2d(n_channels[i], n * (self.n_classes + 1), 3, padding=1))\r\n\r\n    def forward(self, x):\r\n        xs = []\r\n        for name, m in itertools.chain(self.Base._modules.items(), \r\n                                       self.Extra._modules.items()):\r\n            if isinstance(m, nn.Conv2d):\r\n                x = F.relu(m(x), inplace=True)\r\n            else:\r\n                x = m(x)\r\n\r\n            if name in self.pred_layers:\r\n                if name in self.norm_layers:\r\n                    i = self.norm_layers.index(name)\r\n                    xs.append(self.L2Norm[i](x))\r\n                else:\r\n                    xs.append(x)\r\n\r\n        return self._prediction(xs)\r\n\r\n    def _prediction(self, xs):\r\n        locs = []\r\n        confs = []\r\n        for i, x in enumerate(xs):\r\n            loc = self.Loc[i](x)\r\n            loc = loc.permute(0, 2, 3, 1).contiguous().view(loc.size(0), -1, 4)\r\n            locs.append(loc)\r\n\r\n            conf = self.Conf[i](x)\r\n            conf = conf.permute(0, 2, 3, 1).contiguous().view(conf.size(0), -1, self.n_classes + 1)\r\n            confs.append(conf)\r\n        return torch.cat(locs, dim=1), torch.cat(confs, dim=1)\r\n\r\n\r\n    def init_parameters(self, backbone=None):\r\n        def weights_init(m):\r\n            if isinstance(m, nn.Conv2d):\r\n                nn.init.xavier_uniform(m.weight.data)\r\n                m.bias.data.zero_()\r\n        self.apply(weights_init)\r\n\r\n        if backbone is not None and os.path.isfile(backbone):\r\n            self.Base.load_pretrained(backbone)\r\n            print(\'Backbone loaded!\')\r\n        else:\r\n            print(\'No backbone file!\')\r\n\r\n\r\n\r\n\r\nclass SSD512(SSD300):\r\n\r\n    config = {\r\n        \'name\': \'SSD512-VGG16\',\r\n        \'image_size\': 512,\r\n        \'grids\': (64, 32, 16, 8, 4, 2, 1),\r\n        \'aspect_ratios\': ((1/2.,  1,  2), \r\n                          (1/3.,  1/2.,  1,  2,  3),\r\n                          (1/3.,  1/2.,  1,  2,  3), \r\n                          (1/3.,  1/2.,  1,  2,  3), \r\n                          (1/3.,  1/2.,  1,  2,  3), \r\n                          (1/2.,  1,  2),\r\n                          (1/2.,  1,  2)),\r\n        \'sizes\': [s / 512. for s in (20.48, 61.2, 133.12, 215.04, 296.96, \r\n                                     378.88, 460.8, 542.72)],\r\n\r\n        \'init_lr\': 0.001,\r\n        \'stepvalues\': (45000, 60000),\r\n        \'max_iter\': 75000,\r\n    }\r\n\r\n    def __init__(self, n_classes):\r\n        super(SSD300, self).__init__()\r\n        self.n_classes = n_classes\r\n\r\n        self.Base = VGG16()\r\n        self.Extra = nn.Sequential(OrderedDict([\r\n            (\'extra1_1\', nn.Conv2d(1024, 256, 1)),\r\n            (\'extra1_2\', nn.Conv2d(256, 512, 3, padding=1, stride=2)),\r\n            (\'extra2_1\', nn.Conv2d(512, 128, 1)),\r\n            (\'extra2_2\', nn.Conv2d(128, 256, 3, padding=1, stride=2)),\r\n            (\'extra3_1\', nn.Conv2d(256, 128, 1)),\r\n            (\'extra3_2\', nn.Conv2d(128, 256, 3, padding=1, stride=2)),\r\n            (\'extra4_1\', nn.Conv2d(256, 128, 1)),\r\n            (\'extra4_2\', nn.Conv2d(128, 256, 3, padding=1, stride=2)),\r\n            (\'extra5_1\', nn.Conv2d(256, 128, 1)),\r\n            (\'extra5_2\', nn.Conv2d(128, 256, 3)),\r\n        ]))\r\n        self.pred_layers = [\'conv4_3\', \'conv7\', \'extra1_2\', \'extra2_2\', \'extra3_2\', \r\n                            \'extra4_2\', \'extra5_2\']\r\n        n_channels = [512, 1024, 512, 256, 256, 256, 256]\r\n\r\n        self.L2Norm = nn.ModuleList([L2Norm(512, 20)])\r\n        self.l2norm_layers = [\'conv4_3\']\r\n\r\n        self.Loc = nn.ModuleList([])\r\n        self.Conf = nn.ModuleList([])\r\n        for i, ar in enumerate(self.config[\'aspect_ratios\']):\r\n            n = len(ar) + 1\r\n            self.Loc.append(nn.Conv2d(n_channels[i], n * 4, 3, padding=1))\r\n            self.Conf.append(nn.Conv2d(n_channels[i], n * (self.n_classes + 1), 3, padding=1))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# TODO\r\n#class SSDResNet\r\n#class SSDKITTI'"
models/vgg.py,2,"b""# fully conv reduced VGG16\n# https://gist.github.com/weiliu89/2ed6e13bfd5b57cf81d6 , converted to pytorch by amdegroot, https://github.com/amdegroot/ssd.pytorch\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass VGG16(nn.Module):\n    '''\n    input image: BGR format, range [0, 255], then subtract mean\n    '''\n    def __init__(self):\n        super(VGG16, self).__init__()\n\n        self.conv1_1 = nn.Conv2d(  3,  64, 3, padding=1)\n        self.conv1_2 = nn.Conv2d( 64,  64, 3, padding=1)\n        self.pool1 = nn.MaxPool2d(2, ceil_mode=True)\n\n        self.conv2_1 = nn.Conv2d( 64, 128, 3, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n        self.pool2 = nn.MaxPool2d(2, ceil_mode=True)\n\n        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n        self.pool3 = nn.MaxPool2d(2, ceil_mode=True)\n\n        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.pool4 = nn.MaxPool2d(2, ceil_mode=True)\n\n        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.pool5 = nn.MaxPool2d(3, stride=1, padding=1)\n\n        self.conv6 = nn.Conv2d(512, 1024, 3, padding=6, dilation=6)\n        self.conv7 = nn.Conv2d(1024, 1024, 1)\n\n    def load_pretrained(self, path):\n        weights = torch.load(path)\n\n        lookup = {'conv1_1':'0', 'conv1_2':'2', 'conv2_1':'5', 'conv2_2':'7', \n                  'conv3_1':'10', 'conv3_2':'12', 'conv3_3':'14', \n                  'conv4_1':'17', 'conv4_2':'19', 'conv4_3':'21',\n                  'conv5_1':'24', 'conv5_2':'26', 'conv5_3':'28',\n                  'conv6':'31', 'conv7':'33'}\n\n        model_dict = self.state_dict()\n        pretrained_dict = {}\n        for name, ind in lookup.items():\n            for ext in ['.weight', '.bias']:\n                pretrained_dict[name + ext] = weights[ind + ext]\n        model_dict.update(pretrained_dict)\n\n        self.load_state_dict(model_dict)"""
