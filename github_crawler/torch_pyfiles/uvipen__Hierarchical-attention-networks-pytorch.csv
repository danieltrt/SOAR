file_path,api_count,code
app.py,8,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport random\nimport string\nfrom flask import Flask, request, render_template\nimport torch\nimport torch.nn.functional as F\nimport csv\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport numpy as np\n\napp = Flask(__name__)\nAPP_ROOT = os.path.dirname(os.path.abspath(__file__))\nIMAGES_FOLDER = ""flask_images""\nrand_str = lambda n: """".join([random.choice(string.ascii_letters + string.digits) for _ in range(n)])\n\nmodel = None\nword2vec = None\nmax_length_sentences = 0\nmax_length_word = 0\nnum_classes = 0\ncategories = None\n\n\n@app.route(""/"")\ndef home():\n    return render_template(""main.html"")\n\n@app.route(""/input"")\ndef new_input():\n    return render_template(""input.html"")\n\n@app.route(""/show"", methods=[""POST""])\ndef show():\n    global model, dictionary, max_length_word, max_length_sentences, num_classes, categories\n    trained_model = request.files[""model""]\n    if torch.cuda.is_available():\n        model = torch.load(trained_model)\n    else:\n        model = torch.load(trained_model, map_location=lambda storage, loc: storage)\n    dictionary = pd.read_csv(filepath_or_buffer=request.files[""word2vec""], header=None, sep="" "", quoting=csv.QUOTE_NONE,\n                             usecols=[0]).values\n    dictionary = [word[0] for word in dictionary]\n    max_length_sentences = model.max_sent_length\n    max_length_word = model.max_word_length\n    num_classes = list(model.modules())[-1].out_features\n    if ""classes"" in request.files:\n        df = pd.read_csv(request.files[""classes""], header=None)\n        categories = [item[0] for item in df.values]\n    return render_template(""input.html"")\n\n\n@app.route(""/result"", methods=[""POST""])\ndef result():\n    global dictionary, model, max_length_sentences, max_length_word, categories\n    text = request.form[""message""]\n    document_encode = [\n        [dictionary.index(word) if word in dictionary else -1 for word in word_tokenize(text=sentences)] for sentences\n        in sent_tokenize(text=text)]\n\n    for sentences in document_encode:\n        if len(sentences) < max_length_word:\n            extended_words = [-1 for _ in range(max_length_word - len(sentences))]\n            sentences.extend(extended_words)\n\n    if len(document_encode) < max_length_sentences:\n        extended_sentences = [[-1 for _ in range(max_length_word)] for _ in\n                              range(max_length_sentences - len(document_encode))]\n        document_encode.extend(extended_sentences)\n\n    document_encode = [sentences[:max_length_word] for sentences in document_encode][\n                      :max_length_sentences]\n\n    document_encode = np.stack(arrays=document_encode, axis=0)\n    document_encode += 1\n    empty_array = np.zeros_like(document_encode, dtype=np.int64)\n    input_array = np.stack([document_encode, empty_array], axis=0)\n    feature = torch.from_numpy(input_array)\n    if torch.cuda.is_available():\n        feature = feature.cuda()\n    model.eval()\n    with torch.no_grad():\n        model._init_hidden_state(2)\n        prediction = model(feature)\n    prediction = F.softmax(prediction)\n    max_prob, max_prob_index = torch.max(prediction, dim=-1)\n    prob = ""{:.2f} %"".format(float(max_prob[0])*100)\n    if categories != None:\n        category = categories[int(max_prob_index[0])]\n    else:\n        category = int(max_prob_index[0]) + 1\n    return render_template(""result.html"", text=text, value=prob, index=category)\n\n\nif __name__ == ""__main__"":\n    app.secret_key = os.urandom(12)\n    app.run(host=""0.0.0.0"", port=4555, debug=True)\n'"
test.py,9,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom src.utils import get_evaluation\nfrom src.dataset import MyDataset\nimport argparse\nimport shutil\nimport csv\nimport numpy as np\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        """"""Implementation of the model described in the paper: Hierarchical Attention Networks for Document Classification"""""")\n    parser.add_argument(""--batch_size"", type=int, default=128)\n    parser.add_argument(""--data_path"", type=str, default=""data/test.csv"")\n    parser.add_argument(""--pre_trained_model"", type=str, default=""trained_models/whole_model_han"")\n    parser.add_argument(""--word2vec_path"", type=str, default=""data/glove.6B.50d.txt"")\n    parser.add_argument(""--output"", type=str, default=""predictions"")\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    test_params = {""batch_size"": opt.batch_size,\n                   ""shuffle"": False,\n                   ""drop_last"": False}\n    if os.path.isdir(opt.output):\n        shutil.rmtree(opt.output)\n    os.makedirs(opt.output)\n    if torch.cuda.is_available():\n        model = torch.load(opt.pre_trained_model)\n    else:\n        model = torch.load(opt.pre_trained_model, map_location=lambda storage, loc: storage)\n    test_set = MyDataset(opt.data_path, opt.word2vec_path, model.max_sent_length, model.max_word_length)\n    test_generator = DataLoader(test_set, **test_params)\n    if torch.cuda.is_available():\n        model.cuda()\n    model.eval()\n    te_label_ls = []\n    te_pred_ls = []\n    for te_feature, te_label in test_generator:\n        num_sample = len(te_label)\n        if torch.cuda.is_available():\n            te_feature = te_feature.cuda()\n            te_label = te_label.cuda()\n        with torch.no_grad():\n            model._init_hidden_state(num_sample)\n            te_predictions = model(te_feature)\n            te_predictions = F.softmax(te_predictions)\n        te_label_ls.extend(te_label.clone().cpu())\n        te_pred_ls.append(te_predictions.clone().cpu())\n    te_pred = torch.cat(te_pred_ls, 0).numpy()\n    te_label = np.array(te_label_ls)\n\n    fieldnames = [\'True label\', \'Predicted label\', \'Content\']\n    with open(opt.output + os.sep + ""predictions.csv"", \'w\') as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames, quoting=csv.QUOTE_NONNUMERIC)\n        writer.writeheader()\n        for i, j, k in zip(te_label, te_pred, test_set.texts):\n            writer.writerow(\n                {\'True label\': i + 1, \'Predicted label\': np.argmax(j) + 1, \'Content\': k})\n\n    test_metrics = get_evaluation(te_label, te_pred,\n                                  list_metrics=[""accuracy"", ""loss"", ""confusion_matrix""])\n    print(""Prediction:\\nLoss: {} Accuracy: {} \\nConfusion matrix: \\n{}"".format(test_metrics[""loss""],\n                                                                               test_metrics[""accuracy""],\n                                                                               test_metrics[""confusion_matrix""]))\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
train.py,13,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom src.utils import get_max_lengths, get_evaluation\nfrom src.dataset import MyDataset\nfrom src.hierarchical_att_model import HierAttNet\nfrom tensorboardX import SummaryWriter\nimport argparse\nimport shutil\nimport numpy as np\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        """"""Implementation of the model described in the paper: Hierarchical Attention Networks for Document Classification"""""")\n    parser.add_argument(""--batch_size"", type=int, default=128)\n    parser.add_argument(""--num_epoches"", type=int, default=100)\n    parser.add_argument(""--lr"", type=float, default=0.1)\n    parser.add_argument(""--momentum"", type=float, default=0.9)\n    parser.add_argument(""--word_hidden_size"", type=int, default=50)\n    parser.add_argument(""--sent_hidden_size"", type=int, default=50)\n    parser.add_argument(""--es_min_delta"", type=float, default=0.0,\n                        help=""Early stopping\'s parameter: minimum change loss to qualify as an improvement"")\n    parser.add_argument(""--es_patience"", type=int, default=5,\n                        help=""Early stopping\'s parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique."")\n    parser.add_argument(""--train_set"", type=str, default=""data/train.csv"")\n    parser.add_argument(""--test_set"", type=str, default=""data/test.csv"")\n    parser.add_argument(""--test_interval"", type=int, default=1, help=""Number of epoches between testing phases"")\n    parser.add_argument(""--word2vec_path"", type=str, default=""data/glove.6B.50d.txt"")\n    parser.add_argument(""--log_path"", type=str, default=""tensorboard/han_voc"")\n    parser.add_argument(""--saved_path"", type=str, default=""trained_models"")\n    args = parser.parse_args()\n    return args\n\n\ndef train(opt):\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(123)\n    else:\n        torch.manual_seed(123)\n    output_file = open(opt.saved_path + os.sep + ""logs.txt"", ""w"")\n    output_file.write(""Model\'s parameters: {}"".format(vars(opt)))\n    training_params = {""batch_size"": opt.batch_size,\n                       ""shuffle"": True,\n                       ""drop_last"": True}\n    test_params = {""batch_size"": opt.batch_size,\n                   ""shuffle"": False,\n                   ""drop_last"": False}\n\n    max_word_length, max_sent_length = get_max_lengths(opt.train_set)\n    training_set = MyDataset(opt.train_set, opt.word2vec_path, max_sent_length, max_word_length)\n    training_generator = DataLoader(training_set, **training_params)\n    test_set = MyDataset(opt.test_set, opt.word2vec_path, max_sent_length, max_word_length)\n    test_generator = DataLoader(test_set, **test_params)\n\n    model = HierAttNet(opt.word_hidden_size, opt.sent_hidden_size, opt.batch_size, training_set.num_classes,\n                       opt.word2vec_path, max_sent_length, max_word_length)\n\n\n    if os.path.isdir(opt.log_path):\n        shutil.rmtree(opt.log_path)\n    os.makedirs(opt.log_path)\n    writer = SummaryWriter(opt.log_path)\n    # writer.add_graph(model, torch.zeros(opt.batch_size, max_sent_length, max_word_length))\n\n    if torch.cuda.is_available():\n        model.cuda()\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=opt.lr, momentum=opt.momentum)\n    best_loss = 1e5\n    best_epoch = 0\n    model.train()\n    num_iter_per_epoch = len(training_generator)\n    for epoch in range(opt.num_epoches):\n        for iter, (feature, label) in enumerate(training_generator):\n            if torch.cuda.is_available():\n                feature = feature.cuda()\n                label = label.cuda()\n            optimizer.zero_grad()\n            model._init_hidden_state()\n            predictions = model(feature)\n            loss = criterion(predictions, label)\n            loss.backward()\n            optimizer.step()\n            training_metrics = get_evaluation(label.cpu().numpy(), predictions.cpu().detach().numpy(), list_metrics=[""accuracy""])\n            print(""Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss: {}, Accuracy: {}"".format(\n                epoch + 1,\n                opt.num_epoches,\n                iter + 1,\n                num_iter_per_epoch,\n                optimizer.param_groups[0][\'lr\'],\n                loss, training_metrics[""accuracy""]))\n            writer.add_scalar(\'Train/Loss\', loss, epoch * num_iter_per_epoch + iter)\n            writer.add_scalar(\'Train/Accuracy\', training_metrics[""accuracy""], epoch * num_iter_per_epoch + iter)\n        if epoch % opt.test_interval == 0:\n            model.eval()\n            loss_ls = []\n            te_label_ls = []\n            te_pred_ls = []\n            for te_feature, te_label in test_generator:\n                num_sample = len(te_label)\n                if torch.cuda.is_available():\n                    te_feature = te_feature.cuda()\n                    te_label = te_label.cuda()\n                with torch.no_grad():\n                    model._init_hidden_state(num_sample)\n                    te_predictions = model(te_feature)\n                te_loss = criterion(te_predictions, te_label)\n                loss_ls.append(te_loss * num_sample)\n                te_label_ls.extend(te_label.clone().cpu())\n                te_pred_ls.append(te_predictions.clone().cpu())\n            te_loss = sum(loss_ls) / test_set.__len__()\n            te_pred = torch.cat(te_pred_ls, 0)\n            te_label = np.array(te_label_ls)\n            test_metrics = get_evaluation(te_label, te_pred.numpy(), list_metrics=[""accuracy"", ""confusion_matrix""])\n            output_file.write(\n                ""Epoch: {}/{} \\nTest loss: {} Test accuracy: {} \\nTest confusion matrix: \\n{}\\n\\n"".format(\n                    epoch + 1, opt.num_epoches,\n                    te_loss,\n                    test_metrics[""accuracy""],\n                    test_metrics[""confusion_matrix""]))\n            print(""Epoch: {}/{}, Lr: {}, Loss: {}, Accuracy: {}"".format(\n                epoch + 1,\n                opt.num_epoches,\n                optimizer.param_groups[0][\'lr\'],\n                te_loss, test_metrics[""accuracy""]))\n            writer.add_scalar(\'Test/Loss\', te_loss, epoch)\n            writer.add_scalar(\'Test/Accuracy\', test_metrics[""accuracy""], epoch)\n            model.train()\n            if te_loss + opt.es_min_delta < best_loss:\n                best_loss = te_loss\n                best_epoch = epoch\n                torch.save(model, opt.saved_path + os.sep + ""whole_model_han"")\n\n            # Early stopping\n            if epoch - best_epoch > opt.es_patience > 0:\n                print(""Stop training at epoch {}. The lowest loss achieved is {}"".format(epoch, te_loss))\n                break\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    train(opt)\n'"
src/dataset.py,1,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport pandas as pd\nfrom torch.utils.data.dataset import Dataset\nimport csv\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport numpy as np\n\n\nclass MyDataset(Dataset):\n\n    def __init__(self, data_path, dict_path, max_length_sentences=30, max_length_word=35):\n        super(MyDataset, self).__init__()\n\n        texts, labels = [], []\n        with open(data_path) as csv_file:\n            reader = csv.reader(csv_file, quotechar=\'""\')\n            for idx, line in enumerate(reader):\n                text = """"\n                for tx in line[1:]:\n                    text += tx.lower()\n                    text += "" ""\n                label = int(line[0]) - 1\n                texts.append(text)\n                labels.append(label)\n\n        self.texts = texts\n        self.labels = labels\n        self.dict = pd.read_csv(filepath_or_buffer=dict_path, header=None, sep="" "", quoting=csv.QUOTE_NONE,\n                                usecols=[0]).values\n        self.dict = [word[0] for word in self.dict]\n        self.max_length_sentences = max_length_sentences\n        self.max_length_word = max_length_word\n        self.num_classes = len(set(self.labels))\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        label = self.labels[index]\n        text = self.texts[index]\n        document_encode = [\n            [self.dict.index(word) if word in self.dict else -1 for word in word_tokenize(text=sentences)] for sentences\n            in\n            sent_tokenize(text=text)]\n\n        for sentences in document_encode:\n            if len(sentences) < self.max_length_word:\n                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n                sentences.extend(extended_words)\n\n        if len(document_encode) < self.max_length_sentences:\n            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n                                  range(self.max_length_sentences - len(document_encode))]\n            document_encode.extend(extended_sentences)\n\n        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n                          :self.max_length_sentences]\n\n        document_encode = np.stack(arrays=document_encode, axis=0)\n        document_encode += 1\n\n        return document_encode.astype(np.int64), label\n\n\nif __name__ == \'__main__\':\n    test = MyDataset(data_path=""../data/test.csv"", dict_path=""../data/glove.6B.50d.txt"")\n    print (test.__getitem__(index=1)[0].shape)\n'"
src/hierarchical_att_model.py,5,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport torch\nimport torch.nn as nn\nfrom src.sent_att_model import SentAttNet\nfrom src.word_att_model import WordAttNet\n\n\nclass HierAttNet(nn.Module):\n    def __init__(self, word_hidden_size, sent_hidden_size, batch_size, num_classes, pretrained_word2vec_path,\n                 max_sent_length, max_word_length):\n        super(HierAttNet, self).__init__()\n        self.batch_size = batch_size\n        self.word_hidden_size = word_hidden_size\n        self.sent_hidden_size = sent_hidden_size\n        self.max_sent_length = max_sent_length\n        self.max_word_length = max_word_length\n        self.word_att_net = WordAttNet(pretrained_word2vec_path, word_hidden_size)\n        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes)\n        self._init_hidden_state()\n\n    def _init_hidden_state(self, last_batch_size=None):\n        if last_batch_size:\n            batch_size = last_batch_size\n        else:\n            batch_size = self.batch_size\n        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n        if torch.cuda.is_available():\n            self.word_hidden_state = self.word_hidden_state.cuda()\n            self.sent_hidden_state = self.sent_hidden_state.cuda()\n\n    def forward(self, input):\n\n        output_list = []\n        input = input.permute(1, 0, 2)\n        for i in input:\n            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0), self.word_hidden_state)\n            output_list.append(output)\n        output = torch.cat(output_list, 0)\n        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n\n        return output\n'"
src/sent_att_model.py,5,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom src.utils import matrix_mul, element_wise_mul\n\nclass SentAttNet(nn.Module):\n    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=14):\n        super(SentAttNet, self).__init__()\n\n        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n\n        self.gru = nn.GRU(2 * word_hidden_size, sent_hidden_size, bidirectional=True)\n        self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n        # self.sent_softmax = nn.Softmax()\n        # self.fc_softmax = nn.Softmax()\n        self._create_weights(mean=0.0, std=0.05)\n\n    def _create_weights(self, mean=0.0, std=0.05):\n        self.sent_weight.data.normal_(mean, std)\n        self.context_weight.data.normal_(mean, std)\n\n    def forward(self, input, hidden_state):\n\n        f_output, h_output = self.gru(input, hidden_state)\n        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n        output = matrix_mul(output, self.context_weight).permute(1, 0)\n        output = F.softmax(output)\n        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n        output = self.fc(output)\n\n        return output, h_output\n\n\nif __name__ == ""__main__"":\n    abc = SentAttNet()\n'"
src/utils.py,6,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport torch\nimport sys\nimport csv\ncsv.field_size_limit(sys.maxsize)\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn import metrics\nimport numpy as np\n\ndef get_evaluation(y_true, y_prob, list_metrics):\n    y_pred = np.argmax(y_prob, -1)\n    output = {}\n    if \'accuracy\' in list_metrics:\n        output[\'accuracy\'] = metrics.accuracy_score(y_true, y_pred)\n    if \'loss\' in list_metrics:\n        try:\n            output[\'loss\'] = metrics.log_loss(y_true, y_prob)\n        except ValueError:\n            output[\'loss\'] = -1\n    if \'confusion_matrix\' in list_metrics:\n        output[\'confusion_matrix\'] = str(metrics.confusion_matrix(y_true, y_pred))\n    return output\n\ndef matrix_mul(input, weight, bias=False):\n    feature_list = []\n    for feature in input:\n        feature = torch.mm(feature, weight)\n        if isinstance(bias, torch.nn.parameter.Parameter):\n            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n        feature = torch.tanh(feature).unsqueeze(0)\n        feature_list.append(feature)\n\n    return torch.cat(feature_list, 0).squeeze()\n\ndef element_wise_mul(input1, input2):\n\n    feature_list = []\n    for feature_1, feature_2 in zip(input1, input2):\n        feature_2 = feature_2.unsqueeze(1).expand_as(feature_1)\n        feature = feature_1 * feature_2\n        feature_list.append(feature.unsqueeze(0))\n    output = torch.cat(feature_list, 0)\n\n    return torch.sum(output, 0).unsqueeze(0)\n\ndef get_max_lengths(data_path):\n    word_length_list = []\n    sent_length_list = []\n    with open(data_path) as csv_file:\n        reader = csv.reader(csv_file, quotechar=\'""\')\n        for idx, line in enumerate(reader):\n            text = """"\n            for tx in line[1:]:\n                text += tx.lower()\n                text += "" ""\n            sent_list = sent_tokenize(text)\n            sent_length_list.append(len(sent_list))\n\n            for sent in sent_list:\n                word_list = word_tokenize(sent)\n                word_length_list.append(len(word_list))\n\n        sorted_word_length = sorted(word_length_list)\n        sorted_sent_length = sorted(sent_length_list)\n\n    return sorted_word_length[int(0.8*len(sorted_word_length))], sorted_sent_length[int(0.8*len(sorted_sent_length))]\n\nif __name__ == ""__main__"":\n    word, sent = get_max_lengths(""../data/test.csv"")\n    print (word)\n    print (sent)\n\n\n\n\n\n\n'"
src/word_att_model.py,6,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom src.utils import matrix_mul, element_wise_mul\nimport pandas as pd\nimport numpy as np\nimport csv\n\nclass WordAttNet(nn.Module):\n    def __init__(self, word2vec_path, hidden_size=50):\n        super(WordAttNet, self).__init__()\n        dict = pd.read_csv(filepath_or_buffer=word2vec_path, header=None, sep="" "", quoting=csv.QUOTE_NONE).values[:, 1:]\n        dict_len, embed_size = dict.shape\n        dict_len += 1\n        unknown_word = np.zeros((1, embed_size))\n        dict = torch.from_numpy(np.concatenate([unknown_word, dict], axis=0).astype(np.float))\n\n        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n\n        self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)\n        self._create_weights(mean=0.0, std=0.05)\n\n    def _create_weights(self, mean=0.0, std=0.05):\n\n        self.word_weight.data.normal_(mean, std)\n        self.context_weight.data.normal_(mean, std)\n\n    def forward(self, input, hidden_state):\n\n        output = self.lookup(input)\n        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n        output = matrix_mul(output, self.context_weight).permute(1,0)\n        output = F.softmax(output)\n        output = element_wise_mul(f_output,output.permute(1,0))\n\n        return output, h_output\n\n\nif __name__ == ""__main__"":\n    abc = WordAttNet(""../data/glove.6B.50d.txt"")\n'"
