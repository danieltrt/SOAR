file_path,api_count,code
test.py,11,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\n\nimport os\n\nos.environ[\'OMP_NUM_THREADS\'] = \'1\'\nimport argparse\nimport torch\nfrom src.env import create_train_env\nfrom src.model import ActorCritic\nimport torch.nn.functional as F\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        """"""Implementation of model described in the paper: Asynchronous Methods for Deep Reinforcement Learning for Super Mario Bros"""""")\n    parser.add_argument(""--world"", type=int, default=1)\n    parser.add_argument(""--stage"", type=int, default=1)\n    parser.add_argument(""--action_type"", type=str, default=""complex"")\n    parser.add_argument(""--saved_path"", type=str, default=""trained_models"")\n    parser.add_argument(""--output_path"", type=str, default=""output"")\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    torch.manual_seed(123)\n    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type,\n                                                    ""{}/video_{}_{}.mp4"".format(opt.output_path, opt.world, opt.stage))\n    model = ActorCritic(num_states, num_actions)\n    if torch.cuda.is_available():\n        model.load_state_dict(torch.load(""{}/a3c_super_mario_bros_{}_{}"".format(opt.saved_path, opt.world, opt.stage)))\n        model.cuda()\n    else:\n        model.load_state_dict(torch.load(""{}/a3c_super_mario_bros_{}_{}"".format(opt.saved_path, opt.world, opt.stage),\n                                         map_location=lambda storage, loc: storage))\n    model.eval()\n    state = torch.from_numpy(env.reset())\n    done = True\n    while True:\n        if done:\n            h_0 = torch.zeros((1, 512), dtype=torch.float)\n            c_0 = torch.zeros((1, 512), dtype=torch.float)\n            env.reset()\n        else:\n            h_0 = h_0.detach()\n            c_0 = c_0.detach()\n        if torch.cuda.is_available():\n            h_0 = h_0.cuda()\n            c_0 = c_0.cuda()\n            state = state.cuda()\n\n        logits, value, h_0, c_0 = model(state, h_0, c_0)\n        policy = F.softmax(logits, dim=1)\n        action = torch.argmax(policy).item()\n        action = int(action)\n        state, reward, done, info = env.step(action)\n        state = torch.from_numpy(state)\n        env.render()\n        if info[""flag_get""]:\n            print(""World {} stage {} completed"".format(opt.world, opt.stage))\n            break\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
train.py,3,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\n\nimport os\nos.environ[\'OMP_NUM_THREADS\'] = \'1\'\nimport argparse\nimport torch\nfrom src.env import create_train_env\nfrom src.model import ActorCritic\nfrom src.optimizer import GlobalAdam\nfrom src.process import local_train, local_test\nimport torch.multiprocessing as _mp\nimport shutil\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        """"""Implementation of model described in the paper: Asynchronous Methods for Deep Reinforcement Learning for Super Mario Bros"""""")\n    parser.add_argument(""--world"", type=int, default=1)\n    parser.add_argument(""--stage"", type=int, default=1)\n    parser.add_argument(""--action_type"", type=str, default=""complex"")\n    parser.add_argument(\'--lr\', type=float, default=1e-4)\n    parser.add_argument(\'--gamma\', type=float, default=0.9, help=\'discount factor for rewards\')\n    parser.add_argument(\'--tau\', type=float, default=1.0, help=\'parameter for GAE\')\n    parser.add_argument(\'--beta\', type=float, default=0.01, help=\'entropy coefficient\')\n    parser.add_argument(""--num_local_steps"", type=int, default=50)\n    parser.add_argument(""--num_global_steps"", type=int, default=5e6)\n    parser.add_argument(""--num_processes"", type=int, default=6)\n    parser.add_argument(""--save_interval"", type=int, default=500, help=""Number of steps between savings"")\n    parser.add_argument(""--max_actions"", type=int, default=200, help=""Maximum repetition steps in test phase"")\n    parser.add_argument(""--log_path"", type=str, default=""tensorboard/a3c_super_mario_bros"")\n    parser.add_argument(""--saved_path"", type=str, default=""trained_models"")\n    parser.add_argument(""--load_from_previous_stage"", type=bool, default=False,\n                        help=""Load weight from previous trained stage"")\n    parser.add_argument(""--use_gpu"", type=bool, default=True)\n    args = parser.parse_args()\n    return args\n\n\ndef train(opt):\n    torch.manual_seed(123)\n    if os.path.isdir(opt.log_path):\n        shutil.rmtree(opt.log_path)\n    os.makedirs(opt.log_path)\n    if not os.path.isdir(opt.saved_path):\n        os.makedirs(opt.saved_path)\n    mp = _mp.get_context(""spawn"")\n    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)\n    global_model = ActorCritic(num_states, num_actions)\n    if opt.use_gpu:\n        global_model.cuda()\n    global_model.share_memory()\n    if opt.load_from_previous_stage:\n        if opt.stage == 1:\n            previous_world = opt.world - 1\n            previous_stage = 4\n        else:\n            previous_world = opt.world\n            previous_stage = opt.stage - 1\n        file_ = ""{}/a3c_super_mario_bros_{}_{}"".format(opt.saved_path, previous_world, previous_stage)\n        if os.path.isfile(file_):\n            global_model.load_state_dict(torch.load(file_))\n\n    optimizer = GlobalAdam(global_model.parameters(), lr=opt.lr)\n    processes = []\n    for index in range(opt.num_processes):\n        if index == 0:\n            process = mp.Process(target=local_train, args=(index, opt, global_model, optimizer, True))\n        else:\n            process = mp.Process(target=local_train, args=(index, opt, global_model, optimizer))\n        process.start()\n        processes.append(process)\n    process = mp.Process(target=local_test, args=(opt.num_processes, opt, global_model))\n    process.start()\n    processes.append(process)\n    for process in processes:\n        process.join()\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    train(opt)\n'"
src/env.py,0,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\n\nimport gym_super_mario_bros\nfrom gym.spaces import Box\nfrom gym import Wrapper\nfrom nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\nimport cv2\nimport numpy as np\nimport subprocess as sp\n\n\nclass Monitor:\n    def __init__(self, width, height, saved_path):\n\n        self.command = [""ffmpeg"", ""-y"", ""-f"", ""rawvideo"", ""-vcodec"", ""rawvideo"", ""-s"", ""{}X{}"".format(width, height),\n                        ""-pix_fmt"", ""rgb24"", ""-r"", ""80"", ""-i"", ""-"", ""-an"", ""-vcodec"", ""mpeg4"", saved_path]\n        try:\n            self.pipe = sp.Popen(self.command, stdin=sp.PIPE, stderr=sp.PIPE)\n        except FileNotFoundError:\n            pass\n\n    def record(self, image_array):\n        self.pipe.stdin.write(image_array.tostring())\n\n\ndef process_frame(frame):\n    if frame is not None:\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(frame, (84, 84))[None, :, :] / 255.\n        return frame\n    else:\n        return np.zeros((1, 84, 84))\n\n\nclass CustomReward(Wrapper):\n    def __init__(self, env=None, monitor=None):\n        super(CustomReward, self).__init__(env)\n        self.observation_space = Box(low=0, high=255, shape=(1, 84, 84))\n        self.curr_score = 0\n        if monitor:\n            self.monitor = monitor\n        else:\n            self.monitor = None\n\n    def step(self, action):\n        state, reward, done, info = self.env.step(action)\n        if self.monitor:\n            self.monitor.record(state)\n        state = process_frame(state)\n        reward += (info[""score""] - self.curr_score) / 40.\n        self.curr_score = info[""score""]\n        if done:\n            if info[""flag_get""]:\n                reward += 50\n            else:\n                reward -= 50\n        return state, reward / 10., done, info\n\n    def reset(self):\n        self.curr_score = 0\n        return process_frame(self.env.reset())\n\n\nclass CustomSkipFrame(Wrapper):\n    def __init__(self, env, skip=4):\n        super(CustomSkipFrame, self).__init__(env)\n        self.observation_space = Box(low=0, high=255, shape=(4, 84, 84))\n        self.skip = skip\n\n    def step(self, action):\n        total_reward = 0\n        states = []\n        state, reward, done, info = self.env.step(action)\n        for i in range(self.skip):\n            if not done:\n                state, reward, done, info = self.env.step(action)\n                total_reward += reward\n                states.append(state)\n            else:\n                states.append(state)\n        states = np.concatenate(states, 0)[None, :, :, :]\n        return states.astype(np.float32), reward, done, info\n\n    def reset(self):\n        state = self.env.reset()\n        states = np.concatenate([state for _ in range(self.skip)], 0)[None, :, :, :]\n        return states.astype(np.float32)\n\n\ndef create_train_env(world, stage, action_type, output_path=None):\n    env = gym_super_mario_bros.make(""SuperMarioBros-{}-{}-v0"".format(world, stage))\n    if output_path:\n        monitor = Monitor(256, 240, output_path)\n    else:\n        monitor = None\n    if action_type == ""right"":\n        actions = RIGHT_ONLY\n    elif action_type == ""simple"":\n        actions = SIMPLE_MOVEMENT\n    else:\n        actions = COMPLEX_MOVEMENT\n    env = BinarySpaceToDiscreteSpaceEnv(env, actions)\n    env = CustomReward(env, monitor)\n    env = CustomSkipFrame(env)\n    return env, env.observation_space.shape[0], len(actions)\n'"
src/model.py,2,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ActorCritic(nn.Module):\n    def __init__(self, num_inputs, num_actions):\n        super(ActorCritic, self).__init__()\n        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n        self.lstm = nn.LSTMCell(32 * 6 * 6, 512)\n        self.critic_linear = nn.Linear(512, 1)\n        self.actor_linear = nn.Linear(512, num_actions)\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                # nn.init.kaiming_uniform_(module.weight)\n                nn.init.constant_(module.bias, 0)\n            elif isinstance(module, nn.LSTMCell):\n                nn.init.constant_(module.bias_ih, 0)\n                nn.init.constant_(module.bias_hh, 0)\n\n    def forward(self, x, hx, cx):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        hx, cx = self.lstm(x.view(x.size(0), -1), (hx, cx))\n        return self.actor_linear(hx), self.critic_linear(hx), hx, cx\n\n\n\n'"
src/optimizer.py,3,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\n\nimport torch\n\nclass GlobalAdam(torch.optim.Adam):\n    def __init__(self, params, lr):\n        super(GlobalAdam, self).__init__(params, lr=lr)\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = 0\n                state[\'exp_avg\'] = torch.zeros_like(p.data)\n                state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                state[\'exp_avg\'].share_memory_()\n                state[\'exp_avg_sq\'].share_memory_()\n'"
src/process.py,18,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\n\nimport torch\nfrom src.env import create_train_env\nfrom src.model import ActorCritic\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nfrom collections import deque\nfrom tensorboardX import SummaryWriter\nimport timeit\n\n\ndef local_train(index, opt, global_model, optimizer, save=False):\n    torch.manual_seed(123 + index)\n    if save:\n        start_time = timeit.default_timer()\n    writer = SummaryWriter(opt.log_path)\n    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)\n    local_model = ActorCritic(num_states, num_actions)\n    if opt.use_gpu:\n        local_model.cuda()\n    local_model.train()\n    state = torch.from_numpy(env.reset())\n    if opt.use_gpu:\n        state = state.cuda()\n    done = True\n    curr_step = 0\n    curr_episode = 0\n    while True:\n        if save:\n            if curr_episode % opt.save_interval == 0 and curr_episode > 0:\n                torch.save(global_model.state_dict(),\n                           ""{}/a3c_super_mario_bros_{}_{}"".format(opt.saved_path, opt.world, opt.stage))\n            print(""Process {}. Episode {}"".format(index, curr_episode))\n        curr_episode += 1\n        local_model.load_state_dict(global_model.state_dict())\n        if done:\n            h_0 = torch.zeros((1, 512), dtype=torch.float)\n            c_0 = torch.zeros((1, 512), dtype=torch.float)\n        else:\n            h_0 = h_0.detach()\n            c_0 = c_0.detach()\n        if opt.use_gpu:\n            h_0 = h_0.cuda()\n            c_0 = c_0.cuda()\n\n        log_policies = []\n        values = []\n        rewards = []\n        entropies = []\n\n        for _ in range(opt.num_local_steps):\n            curr_step += 1\n            logits, value, h_0, c_0 = local_model(state, h_0, c_0)\n            policy = F.softmax(logits, dim=1)\n            log_policy = F.log_softmax(logits, dim=1)\n            entropy = -(policy * log_policy).sum(1, keepdim=True)\n\n            m = Categorical(policy)\n            action = m.sample().item()\n\n            state, reward, done, _ = env.step(action)\n            state = torch.from_numpy(state)\n            if opt.use_gpu:\n                state = state.cuda()\n            if curr_step > opt.num_global_steps:\n                done = True\n\n            if done:\n                curr_step = 0\n                state = torch.from_numpy(env.reset())\n                if opt.use_gpu:\n                    state = state.cuda()\n\n            values.append(value)\n            log_policies.append(log_policy[0, action])\n            rewards.append(reward)\n            entropies.append(entropy)\n\n            if done:\n                break\n\n        R = torch.zeros((1, 1), dtype=torch.float)\n        if opt.use_gpu:\n            R = R.cuda()\n        if not done:\n            _, R, _, _ = local_model(state, h_0, c_0)\n\n        gae = torch.zeros((1, 1), dtype=torch.float)\n        if opt.use_gpu:\n            gae = gae.cuda()\n        actor_loss = 0\n        critic_loss = 0\n        entropy_loss = 0\n        next_value = R\n\n        for value, log_policy, reward, entropy in list(zip(values, log_policies, rewards, entropies))[::-1]:\n            gae = gae * opt.gamma * opt.tau\n            gae = gae + reward + opt.gamma * next_value.detach() - value.detach()\n            next_value = value\n            actor_loss = actor_loss + log_policy * gae\n            R = R * opt.gamma + reward\n            critic_loss = critic_loss + (R - value) ** 2 / 2\n            entropy_loss = entropy_loss + entropy\n\n        total_loss = -actor_loss + critic_loss - opt.beta * entropy_loss\n        writer.add_scalar(""Train_{}/Loss"".format(index), total_loss, curr_episode)\n        optimizer.zero_grad()\n        total_loss.backward()\n\n        for local_param, global_param in zip(local_model.parameters(), global_model.parameters()):\n            if global_param.grad is not None:\n                break\n            global_param._grad = local_param.grad\n\n        optimizer.step()\n\n        if curr_episode == int(opt.num_global_steps / opt.num_local_steps):\n            print(""Training process {} terminated"".format(index))\n            if save:\n                end_time = timeit.default_timer()\n                print(\'The code runs for %.2f s \' % (end_time - start_time))\n            return\n\n\ndef local_test(index, opt, global_model):\n    torch.manual_seed(123 + index)\n    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)\n    local_model = ActorCritic(num_states, num_actions)\n    local_model.eval()\n    state = torch.from_numpy(env.reset())\n    done = True\n    curr_step = 0\n    actions = deque(maxlen=opt.max_actions)\n    while True:\n        curr_step += 1\n        if done:\n            local_model.load_state_dict(global_model.state_dict())\n        with torch.no_grad():\n            if done:\n                h_0 = torch.zeros((1, 512), dtype=torch.float)\n                c_0 = torch.zeros((1, 512), dtype=torch.float)\n            else:\n                h_0 = h_0.detach()\n                c_0 = c_0.detach()\n\n        logits, value, h_0, c_0 = local_model(state, h_0, c_0)\n        policy = F.softmax(logits, dim=1)\n        action = torch.argmax(policy).item()\n        state, reward, done, _ = env.step(action)\n        env.render()\n        actions.append(action)\n        if curr_step > opt.num_global_steps or actions.count(actions[0]) == actions.maxlen:\n            done = True\n        if done:\n            curr_step = 0\n            actions.clear()\n            state = env.reset()\n        state = torch.from_numpy(state)\n'"
