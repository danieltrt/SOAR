file_path,api_count,code
test.py,10,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport argparse\nimport torch\nimport cv2\nfrom src.tetris import Tetris\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        """"""Implementation of Deep Q Network to play Tetris"""""")\n\n    parser.add_argument(""--width"", type=int, default=10, help=""The common width for all images"")\n    parser.add_argument(""--height"", type=int, default=20, help=""The common height for all images"")\n    parser.add_argument(""--block_size"", type=int, default=30, help=""Size of a block"")\n    parser.add_argument(""--fps"", type=int, default=300, help=""frames per second"")\n    parser.add_argument(""--saved_path"", type=str, default=""trained_models"")\n    parser.add_argument(""--output"", type=str, default=""output.mp4"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(123)\n    else:\n        torch.manual_seed(123)\n    if torch.cuda.is_available():\n        model = torch.load(""{}/tetris"".format(opt.saved_path))\n    else:\n        model = torch.load(""{}/tetris"".format(opt.saved_path), map_location=lambda storage, loc: storage)\n    model.eval()\n    env = Tetris(width=opt.width, height=opt.height, block_size=opt.block_size)\n    env.reset()\n    if torch.cuda.is_available():\n        model.cuda()\n    out = cv2.VideoWriter(opt.output, cv2.VideoWriter_fourcc(*""MJPG""), opt.fps,\n                          (int(1.5*opt.width*opt.block_size), opt.height*opt.block_size))\n    while True:\n        next_steps = env.get_next_states()\n        next_actions, next_states = zip(*next_steps.items())\n        next_states = torch.stack(next_states)\n        if torch.cuda.is_available():\n            next_states = next_states.cuda()\n        predictions = model(next_states)[:, 0]\n        index = torch.argmax(predictions).item()\n        action = next_actions[index]\n        _, done = env.step(action, render=True, video=out)\n\n        if done:\n            out.release()\n            break\n        \n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
train.py,20,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport argparse\nimport os\nimport shutil\nfrom random import random, randint, sample\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom tensorboardX import SummaryWriter\n\nfrom src.deep_q_network import DeepQNetwork\nfrom src.tetris import Tetris\nfrom collections import deque\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        """"""Implementation of Deep Q Network to play Tetris"""""")\n    parser.add_argument(""--width"", type=int, default=10, help=""The common width for all images"")\n    parser.add_argument(""--height"", type=int, default=20, help=""The common height for all images"")\n    parser.add_argument(""--block_size"", type=int, default=30, help=""Size of a block"")\n    parser.add_argument(""--batch_size"", type=int, default=512, help=""The number of images per batch"")\n    parser.add_argument(""--lr"", type=float, default=1e-3)\n    parser.add_argument(""--gamma"", type=float, default=0.99)\n    parser.add_argument(""--initial_epsilon"", type=float, default=1)\n    parser.add_argument(""--final_epsilon"", type=float, default=1e-3)\n    parser.add_argument(""--num_decay_epochs"", type=float, default=2000)\n    parser.add_argument(""--num_epochs"", type=int, default=3000)\n    parser.add_argument(""--save_interval"", type=int, default=1000)\n    parser.add_argument(""--replay_memory_size"", type=int, default=30000,\n                        help=""Number of epoches between testing phases"")\n    parser.add_argument(""--log_path"", type=str, default=""tensorboard"")\n    parser.add_argument(""--saved_path"", type=str, default=""trained_models"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef train(opt):\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(123)\n    else:\n        torch.manual_seed(123)\n    if os.path.isdir(opt.log_path):\n        shutil.rmtree(opt.log_path)\n    os.makedirs(opt.log_path)\n    writer = SummaryWriter(opt.log_path)\n    env = Tetris(width=opt.width, height=opt.height, block_size=opt.block_size)\n    model = DeepQNetwork()\n    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n    criterion = nn.MSELoss()\n\n    state = env.reset()\n    if torch.cuda.is_available():\n        model.cuda()\n        state = state.cuda()\n\n    replay_memory = deque(maxlen=opt.replay_memory_size)\n    epoch = 0\n    while epoch < opt.num_epochs:\n        next_steps = env.get_next_states()\n        # Exploration or exploitation\n        epsilon = opt.final_epsilon + (max(opt.num_decay_epochs - epoch, 0) * (\n                opt.initial_epsilon - opt.final_epsilon) / opt.num_decay_epochs)\n        u = random()\n        random_action = u <= epsilon\n        next_actions, next_states = zip(*next_steps.items())\n        next_states = torch.stack(next_states)\n        if torch.cuda.is_available():\n            next_states = next_states.cuda()\n        model.eval()\n        with torch.no_grad():\n            predictions = model(next_states)[:, 0]\n        model.train()\n        if random_action:\n            index = randint(0, len(next_steps) - 1)\n        else:\n            index = torch.argmax(predictions).item()\n\n        next_state = next_states[index, :]\n        action = next_actions[index]\n\n        reward, done = env.step(action, render=True)\n\n        if torch.cuda.is_available():\n            next_state = next_state.cuda()\n        replay_memory.append([state, reward, next_state, done])\n        if done:\n            final_score = env.score\n            final_tetrominoes = env.tetrominoes\n            final_cleared_lines = env.cleared_lines\n            state = env.reset()\n            if torch.cuda.is_available():\n                state = state.cuda()\n        else:\n            state = next_state\n            continue\n        if len(replay_memory) < opt.replay_memory_size / 10:\n            continue\n        epoch += 1\n        batch = sample(replay_memory, min(len(replay_memory), opt.batch_size))\n        state_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n        state_batch = torch.stack(tuple(state for state in state_batch))\n        reward_batch = torch.from_numpy(np.array(reward_batch, dtype=np.float32)[:, None])\n        next_state_batch = torch.stack(tuple(state for state in next_state_batch))\n\n        if torch.cuda.is_available():\n            state_batch = state_batch.cuda()\n            reward_batch = reward_batch.cuda()\n            next_state_batch = next_state_batch.cuda()\n\n        q_values = model(state_batch)\n        model.eval()\n        with torch.no_grad():\n            next_prediction_batch = model(next_state_batch)\n        model.train()\n\n        y_batch = torch.cat(\n            tuple(reward if done else reward + opt.gamma * prediction for reward, done, prediction in\n                  zip(reward_batch, done_batch, next_prediction_batch)))[:, None]\n\n        optimizer.zero_grad()\n        loss = criterion(q_values, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        print(""Epoch: {}/{}, Action: {}, Score: {}, Tetrominoes {}, Cleared lines: {}"".format(\n            epoch,\n            opt.num_epochs,\n            action,\n            final_score,\n            final_tetrominoes,\n            final_cleared_lines))\n        writer.add_scalar(\'Train/Score\', final_score, epoch - 1)\n        writer.add_scalar(\'Train/Tetrominoes\', final_tetrominoes, epoch - 1)\n        writer.add_scalar(\'Train/Cleared lines\', final_cleared_lines, epoch - 1)\n\n        if epoch > 0 and epoch % opt.save_interval == 0:\n            torch.save(model, ""{}/tetris_{}"".format(opt.saved_path, epoch))\n\n    torch.save(model, ""{}/tetris"".format(opt.saved_path))\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    train(opt)\n'"
src/deep_q_network.py,1,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport torch.nn as nn\n\nclass DeepQNetwork(nn.Module):\n    def __init__(self):\n        super(DeepQNetwork, self).__init__()\n\n        self.conv1 = nn.Sequential(nn.Linear(4, 64), nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(nn.Linear(64, 64), nn.ReLU(inplace=True))\n        self.conv3 = nn.Sequential(nn.Linear(64, 1))\n\n        self._create_weights()\n\n    def _create_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n\n        return x\n'"
src/tetris.py,1,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport numpy as np\nfrom PIL import Image\nimport cv2\nfrom matplotlib import style\nimport torch\nimport random\n\nstyle.use(""ggplot"")\n\n\nclass Tetris:\n    piece_colors = [\n        (0, 0, 0),\n        (255, 255, 0),\n        (147, 88, 254),\n        (54, 175, 144),\n        (255, 0, 0),\n        (102, 217, 238),\n        (254, 151, 32),\n        (0, 0, 255)\n    ]\n\n    pieces = [\n        [[1, 1],\n         [1, 1]],\n\n        [[0, 2, 0],\n         [2, 2, 2]],\n\n        [[0, 3, 3],\n         [3, 3, 0]],\n\n        [[4, 4, 0],\n         [0, 4, 4]],\n\n        [[5, 5, 5, 5]],\n\n        [[0, 0, 6],\n         [6, 6, 6]],\n\n        [[7, 0, 0],\n         [7, 7, 7]]\n    ]\n\n    def __init__(self, height=20, width=10, block_size=20):\n        self.height = height\n        self.width = width\n        self.block_size = block_size\n        self.extra_board = np.ones((self.height * self.block_size, self.width * int(self.block_size / 2), 3),\n                                   dtype=np.uint8) * np.array([204, 204, 255], dtype=np.uint8)\n        self.text_color = (200, 20, 220)\n        self.reset()\n\n    def reset(self):\n        self.board = [[0] * self.width for _ in range(self.height)]\n        self.score = 0\n        self.tetrominoes = 0\n        self.cleared_lines = 0\n        self.bag = list(range(len(self.pieces)))\n        random.shuffle(self.bag)\n        self.ind = self.bag.pop()\n        self.piece = [row[:] for row in self.pieces[self.ind]]\n        self.current_pos = {""x"": self.width // 2 - len(self.piece[0]) // 2, ""y"": 0}\n        self.gameover = False\n        return self.get_state_properties(self.board)\n\n    def rotate(self, piece):\n        num_rows_orig = num_cols_new = len(piece)\n        num_rows_new = len(piece[0])\n        rotated_array = []\n\n        for i in range(num_rows_new):\n            new_row = [0] * num_cols_new\n            for j in range(num_cols_new):\n                new_row[j] = piece[(num_rows_orig - 1) - j][i]\n            rotated_array.append(new_row)\n        return rotated_array\n\n    def get_state_properties(self, board):\n        lines_cleared, board = self.check_cleared_rows(board)\n        holes = self.get_holes(board)\n        bumpiness, height = self.get_bumpiness_and_height(board)\n\n        return torch.FloatTensor([lines_cleared, holes, bumpiness, height])\n\n    def get_holes(self, board):\n        num_holes = 0\n        for col in zip(*board):\n            row = 0\n            while row < self.height and col[row] == 0:\n                row += 1\n            num_holes += len([x for x in col[row + 1:] if x == 0])\n        return num_holes\n\n    def get_bumpiness_and_height(self, board):\n        board = np.array(board)\n        mask = board != 0\n        invert_heights = np.where(mask.any(axis=0), np.argmax(mask, axis=0), self.height)\n        heights = self.height - invert_heights\n        total_height = np.sum(heights)\n        currs = heights[:-1]\n        nexts = heights[1:]\n        diffs = np.abs(currs - nexts)\n        total_bumpiness = np.sum(diffs)\n        return total_bumpiness, total_height\n\n    def get_next_states(self):\n        states = {}\n        piece_id = self.ind\n        curr_piece = [row[:] for row in self.piece]\n        if piece_id == 0:  # O piece\n            num_rotations = 1\n        elif piece_id == 2 or piece_id == 3 or piece_id == 4:\n            num_rotations = 2\n        else:\n            num_rotations = 4\n\n        for i in range(num_rotations):\n            valid_xs = self.width - len(curr_piece[0])\n            for x in range(valid_xs + 1):\n                piece = [row[:] for row in curr_piece]\n                pos = {""x"": x, ""y"": 0}\n                while not self.check_collision(piece, pos):\n                    pos[""y""] += 1\n                self.truncate(piece, pos)\n                board = self.store(piece, pos)\n                states[(x, i)] = self.get_state_properties(board)\n            curr_piece = self.rotate(curr_piece)\n        return states\n\n    def get_current_board_state(self):\n        board = [x[:] for x in self.board]\n        for y in range(len(self.piece)):\n            for x in range(len(self.piece[y])):\n                board[y + self.current_pos[""y""]][x + self.current_pos[""x""]] = self.piece[y][x]\n        return board\n\n    def new_piece(self):\n        if not len(self.bag):\n            self.bag = list(range(len(self.pieces)))\n            random.shuffle(self.bag)\n        self.ind = self.bag.pop()\n        self.piece = [row[:] for row in self.pieces[self.ind]]\n        self.current_pos = {""x"": self.width // 2 - len(self.piece[0]) // 2,\n                            ""y"": 0\n                            }\n        if self.check_collision(self.piece, self.current_pos):\n            self.gameover = True\n\n    def check_collision(self, piece, pos):\n        future_y = pos[""y""] + 1\n        for y in range(len(piece)):\n            for x in range(len(piece[y])):\n                if future_y + y > self.height - 1 or self.board[future_y + y][pos[""x""] + x] and piece[y][x]:\n                    return True\n        return False\n\n    def truncate(self, piece, pos):\n        gameover = False\n        last_collision_row = -1\n        for y in range(len(piece)):\n            for x in range(len(piece[y])):\n                if self.board[pos[""y""] + y][pos[""x""] + x] and piece[y][x]:\n                    if y > last_collision_row:\n                        last_collision_row = y\n\n        if pos[""y""] - (len(piece) - last_collision_row) < 0 and last_collision_row > -1:\n            while last_collision_row >= 0 and len(piece) > 1:\n                gameover = True\n                last_collision_row = -1\n                del piece[0]\n                for y in range(len(piece)):\n                    for x in range(len(piece[y])):\n                        if self.board[pos[""y""] + y][pos[""x""] + x] and piece[y][x] and y > last_collision_row:\n                            last_collision_row = y\n        return gameover\n\n    def store(self, piece, pos):\n        board = [x[:] for x in self.board]\n        for y in range(len(piece)):\n            for x in range(len(piece[y])):\n                if piece[y][x] and not board[y + pos[""y""]][x + pos[""x""]]:\n                    board[y + pos[""y""]][x + pos[""x""]] = piece[y][x]\n        return board\n\n    def check_cleared_rows(self, board):\n        to_delete = []\n        for i, row in enumerate(board[::-1]):\n            if 0 not in row:\n                to_delete.append(len(board) - 1 - i)\n        if len(to_delete) > 0:\n            board = self.remove_row(board, to_delete)\n        return len(to_delete), board\n\n    def remove_row(self, board, indices):\n        for i in indices[::-1]:\n            del board[i]\n            board = [[0 for _ in range(self.width)]] + board\n        return board\n\n    def step(self, action, render=True, video=None):\n        x, num_rotations = action\n        self.current_pos = {""x"": x, ""y"": 0}\n        for _ in range(num_rotations):\n            self.piece = self.rotate(self.piece)\n\n        while not self.check_collision(self.piece, self.current_pos):\n            self.current_pos[""y""] += 1\n            if render:\n                self.render(video)\n\n        overflow = self.truncate(self.piece, self.current_pos)\n        if overflow:\n            self.gameover = True\n\n        self.board = self.store(self.piece, self.current_pos)\n\n        lines_cleared, self.board = self.check_cleared_rows(self.board)\n        score = 1 + (lines_cleared ** 2) * self.width\n        self.score += score\n        self.tetrominoes += 1\n        self.cleared_lines += lines_cleared\n        if not self.gameover:\n            self.new_piece()\n        if self.gameover:\n            self.score -= 2\n\n        return score, self.gameover\n\n    def render(self, video=None):\n        if not self.gameover:\n            img = [self.piece_colors[p] for row in self.get_current_board_state() for p in row]\n        else:\n            img = [self.piece_colors[p] for row in self.board for p in row]\n        img = np.array(img).reshape((self.height, self.width, 3)).astype(np.uint8)\n        img = img[..., ::-1]\n        img = Image.fromarray(img, ""RGB"")\n\n        img = img.resize((self.width * self.block_size, self.height * self.block_size))\n        img = np.array(img)\n        img[[i * self.block_size for i in range(self.height)], :, :] = 0\n        img[:, [i * self.block_size for i in range(self.width)], :] = 0\n\n        img = np.concatenate((img, self.extra_board), axis=1)\n\n\n        cv2.putText(img, ""Score:"", (self.width * self.block_size + int(self.block_size / 2), self.block_size),\n                    fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=1.0, color=self.text_color)\n        cv2.putText(img, str(self.score),\n                    (self.width * self.block_size + int(self.block_size / 2), 2 * self.block_size),\n                    fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=1.0, color=self.text_color)\n\n        cv2.putText(img, ""Pieces:"", (self.width * self.block_size + int(self.block_size / 2), 4 * self.block_size),\n                    fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=1.0, color=self.text_color)\n        cv2.putText(img, str(self.tetrominoes),\n                    (self.width * self.block_size + int(self.block_size / 2), 5 * self.block_size),\n                    fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=1.0, color=self.text_color)\n\n        cv2.putText(img, ""Lines:"", (self.width * self.block_size + int(self.block_size / 2), 7 * self.block_size),\n                    fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=1.0, color=self.text_color)\n        cv2.putText(img, str(self.cleared_lines),\n                    (self.width * self.block_size + int(self.block_size / 2), 8 * self.block_size),\n                    fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=1.0, color=self.text_color)\n\n        if video:\n            video.write(img)\n\n        cv2.imshow(""Deep Q-Learning Tetris"", img)\n        cv2.waitKey(1)\n'"
