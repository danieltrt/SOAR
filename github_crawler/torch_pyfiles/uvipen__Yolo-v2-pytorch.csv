file_path,api_count,code
test_coco_dataset.py,8,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport argparse\nimport shutil\nimport cv2\nimport numpy as np\nfrom src.utils import *\nimport pickle\nfrom src.yolo_net import Yolo\n\nCLASSES = [""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"",\n           ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"",\n           ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"",\n           ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"", ""sports ball"", ""kite"",\n           ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"",\n           ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n           ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"",\n           ""bed"", ""dining table"", ""toilet"", ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"",\n           ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"", ""vase"", ""scissors"",\n           ""teddy bear"", ""hair drier"", ""toothbrush""]\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""You Only Look Once: Unified, Real-Time Object Detection"")\n    parser.add_argument(""--image_size"", type=int, default=448, help=""The common width and height for all images"")\n    parser.add_argument(""--conf_threshold"", type=float, default=0.35)\n    parser.add_argument(""--nms_threshold"", type=float, default=0.5)\n    parser.add_argument(""--test_set"", type=str, default=""val"")\n    parser.add_argument(""--year"", type=str, default=""2017"", help=""The year of dataset (2014 or 2017)"")\n    parser.add_argument(""--root_path"", type=str, default=""data/COCO"", help=""the root folder of dataset"")\n    parser.add_argument(""--pre_trained_model_type"", type=str, choices=[""model"", ""params""], default=""model"")\n    parser.add_argument(""--pre_trained_model_path"", type=str, default=""trained_models/whole_model_trained_yolo_coco"")\n    parser.add_argument(""--output"", type=str, default=""predictions"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    input_image_folder = os.path.join(opt.root_path, ""images"", ""{}{}"".format(opt.test_set, opt.year))\n    anno_path = os.path.join(opt.root_path, ""anno_pickle"", ""COCO_{}{}.pkl"".format(opt.test_set, opt.year))\n    id_list_path = pickle.load(open(anno_path, ""rb""))\n    id_list_path = list(id_list_path.values())\n    output_folder = os.path.join(opt.output, ""COCO_{}{}"".format(opt.test_set, opt.year))\n    colors = pickle.load(open(""src/pallete"", ""rb""))\n    if os.path.isdir(output_folder):\n        shutil.rmtree(output_folder)\n    os.makedirs(output_folder)\n    if torch.cuda.is_available():\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path)\n        else:\n            model = Yolo(80)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path))\n    else:\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            model = Yolo(80)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage))\n    model.eval()\n\n    for id in id_list_path:\n        image_path = os.path.join(input_image_folder, id[""file_name""])\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        height, width = image.shape[:2]\n        image = cv2.resize(image, (opt.image_size, opt.image_size))\n        image = np.transpose(np.array(image, dtype=np.float32), (2, 0, 1))\n        image = image[None, :, :, :]\n        width_ratio = float(opt.image_size) / width\n        height_ratio = float(opt.image_size) / height\n        data = Variable(torch.FloatTensor(image))\n        if torch.cuda.is_available():\n            data = data.cuda()\n        with torch.no_grad():\n            logits = model(data)\n            predictions = post_processing(logits, opt.image_size, CLASSES, model.anchors, opt.conf_threshold,\n                                          opt.nms_threshold)\n        if len(predictions) == 0:\n            continue\n        else:\n            predictions = predictions[0]\n        output_image = cv2.imread(image_path)\n        for pred in predictions:\n            xmin = int(max(pred[0] / width_ratio, 0))\n            ymin = int(max(pred[1] / height_ratio, 0))\n            xmax = int(min((pred[0] + pred[2]) / width_ratio, width))\n            ymax = int(min((pred[1] + pred[3]) / height_ratio, height))\n            color = colors[CLASSES.index(pred[5])]\n            cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n            text_size = cv2.getTextSize(pred[5] + \' : %.2f\' % pred[4], cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n            cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)\n            cv2.putText(\n                output_image, pred[5] + \' : %.2f\' % pred[4],\n                (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n                (255, 255, 255), 1)\n            print(""Object: {}, Bounding box: ({},{}) ({},{})"".format(pred[5], xmin, xmax, ymin, ymax))\n        cv2.imwrite(""{}/{}_prediction.jpg"".format(output_folder, id[""file_name""][:-4]), output_image)\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
test_coco_images.py,8,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport glob\nimport argparse\nimport pickle\nimport cv2\nimport numpy as np\nfrom src.utils import *\nfrom src.yolo_net import Yolo\n\nCLASSES = [""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"",\n           ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"",\n           ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"",\n           ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"", ""sports ball"", ""kite"",\n           ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"",\n           ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n           ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"",\n           ""bed"", ""dining table"", ""toilet"", ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"",\n           ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"", ""vase"", ""scissors"",\n           ""teddy bear"", ""hair drier"", ""toothbrush""]\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""You Only Look Once: Unified, Real-Time Object Detection"")\n    parser.add_argument(""--image_size"", type=int, default=448, help=""The common width and height for all images"")\n    parser.add_argument(""--conf_threshold"", type=float, default=0.35)\n    parser.add_argument(""--nms_threshold"", type=float, default=0.5)\n    parser.add_argument(""--pre_trained_model_type"", type=str, choices=[""model"", ""params""], default=""model"")\n    parser.add_argument(""--pre_trained_model_path"", type=str, default=""trained_models/whole_model_trained_yolo_coco"")\n    parser.add_argument(""--input"", type=str, default=""test_images"")\n    parser.add_argument(""--output"", type=str, default=""test_images"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    if torch.cuda.is_available():\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path)\n        else:\n            model = Yolo(80)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path))\n    else:\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            model = Yolo(80)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage))\n    model.eval()\n    colors = pickle.load(open(""src/pallete"", ""rb""))\n\n    for image_path in glob.iglob(opt.input + os.sep + \'*.jpg\'):\n        if ""prediction"" in image_path:\n            continue\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        height, width = image.shape[:2]\n        image = cv2.resize(image, (opt.image_size, opt.image_size))\n        image = np.transpose(np.array(image, dtype=np.float32), (2, 0, 1))\n        image = image[None, :, :, :]\n        width_ratio = float(opt.image_size) / width\n        height_ratio = float(opt.image_size) / height\n        data = Variable(torch.FloatTensor(image))\n        if torch.cuda.is_available():\n            data = data.cuda()\n        with torch.no_grad():\n            logits = model(data)\n            predictions = post_processing(logits, opt.image_size, CLASSES, model.anchors, opt.conf_threshold,\n                                          opt.nms_threshold)\n        if len(predictions) != 0:\n            predictions = predictions[0]\n            output_image = cv2.imread(image_path)\n            for pred in predictions:\n                xmin = int(max(pred[0] / width_ratio, 0))\n                ymin = int(max(pred[1] / height_ratio, 0))\n                xmax = int(min((pred[0] + pred[2]) / width_ratio, width))\n                ymax = int(min((pred[1] + pred[3]) / height_ratio, height))\n                color = colors[CLASSES.index(pred[5])]\n                cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n                text_size = cv2.getTextSize(pred[5] + \' : %.2f\' % pred[4], cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n                cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)\n                cv2.putText(\n                    output_image, pred[5] + \' : %.2f\' % pred[4],\n                    (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n                    (255, 255, 255), 1)\n                print(""Object: {}, Bounding box: ({},{}) ({},{})"".format(pred[5], xmin, xmax, ymin, ymax))\n            cv2.imwrite(image_path[:-4] + ""_prediction.jpg"", output_image)\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
test_coco_video.py,8,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\n\nimport argparse\nimport pickle\nimport cv2\nimport numpy as np\nfrom src.utils import *\nfrom src.yolo_net import Yolo\n\nCLASSES = [""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"",\n           ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"",\n           ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"",\n           ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"", ""sports ball"", ""kite"",\n           ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"",\n           ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n           ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"",\n           ""bed"", ""dining table"", ""toilet"", ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"",\n           ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"", ""vase"", ""scissors"",\n           ""teddy bear"", ""hair drier"", ""toothbrush""]\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""You Only Look Once: Unified, Real-Time Object Detection"")\n    parser.add_argument(""--image_size"", type=int, default=448, help=""The common width and height for all images"")\n    parser.add_argument(""--conf_threshold"", type=float, default=0.35)\n    parser.add_argument(""--nms_threshold"", type=float, default=0.5)\n    parser.add_argument(""--test_set"", type=str, default=""test"",\n                        help=""For both VOC2007 and 2012, you could choose 3 different datasets: train, trainval and val. Additionally, for VOC2007, you could also pick the dataset name test"")\n    parser.add_argument(""--year"", type=str, default=""2007"", help=""The year of dataset (2007 or 2012)"")\n    parser.add_argument(""--data_path"", type=str, default=""data/VOCdevkit"", help=""the root folder of dataset"")\n    parser.add_argument(""--pre_trained_model_type"", type=str, choices=[""model"", ""params""], default=""model"")\n    parser.add_argument(""--pre_trained_model_path"", type=str, default=""trained_models/whole_model_trained_yolo_coco"")\n    parser.add_argument(""--input"", type=str, default=""test_videos/input2.mp4"")\n    parser.add_argument(""--output"", type=str, default=""test_videos/output_coco_2.mp4"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    if torch.cuda.is_available():\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path)\n        else:\n            model = Yolo(80)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path))\n    else:\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            model = Yolo(80)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage))\n    model.eval()\n    colors = pickle.load(open(""src/pallete"", ""rb""))\n\n    cap = cv2.VideoCapture(opt.input)\n    out = cv2.VideoWriter(opt.output,  cv2.VideoWriter_fourcc(*""MJPG""), int(cap.get(cv2.CAP_PROP_FPS)),\n                          (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n    while cap.isOpened():\n        flag, image = cap.read()\n        output_image = np.copy(image)\n        if flag:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        else:\n            break\n        height, width = image.shape[:2]\n        image = cv2.resize(image, (opt.image_size, opt.image_size))\n        image = np.transpose(np.array(image, dtype=np.float32), (2, 0, 1))\n        image = image[None, :, :, :]\n        width_ratio = float(opt.image_size) / width\n        height_ratio = float(opt.image_size) / height\n        data = Variable(torch.FloatTensor(image))\n        if torch.cuda.is_available():\n            data = data.cuda()\n        with torch.no_grad():\n            logits = model(data)\n            predictions = post_processing(logits, opt.image_size, CLASSES, model.anchors, opt.conf_threshold,\n                                          opt.nms_threshold)\n        if len(predictions) != 0:\n            predictions = predictions[0]\n            for pred in predictions:\n                xmin = int(max(pred[0] / width_ratio, 0))\n                ymin = int(max(pred[1] / height_ratio, 0))\n                xmax = int(min((pred[0] + pred[2]) / width_ratio, width))\n                ymax = int(min((pred[1] + pred[3]) / height_ratio, height))\n                color = colors[CLASSES.index(pred[5])]\n                cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n                text_size = cv2.getTextSize(pred[5] + \' : %.2f\' % pred[4], cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n                cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)\n                cv2.putText(\n                    output_image, pred[5] + \' : %.2f\' % pred[4],\n                    (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n                    (255, 255, 255), 1)\n        out.write(output_image)\n\n    cap.release()\n    out.release()\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
test_voc_dataset.py,8,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport argparse\nimport shutil\nimport cv2\nimport numpy as np\nfrom src.utils import *\nimport pickle\nfrom src.yolo_net import Yolo\n\nCLASSES = [\'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\',\n                        \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\', \'pottedplant\', \'sheep\', \'sofa\', \'train\',\n                        \'tvmonitor\']\n\ndef get_args():\n    parser = argparse.ArgumentParser(""You Only Look Once: Unified, Real-Time Object Detection"")\n    parser.add_argument(""--image_size"", type=int, default=448, help=""The common width and height for all images"")\n    parser.add_argument(""--conf_threshold"", type=float, default=0.35)\n    parser.add_argument(""--nms_threshold"", type=float, default=0.5)\n    parser.add_argument(""--test_set"", type=str, default=""val"",\n                        help=""For both VOC2007 and 2012, you could choose 3 different datasets: train, trainval and val. Additionally, for VOC2007, you could also pick the dataset name test"")\n    parser.add_argument(""--year"", type=str, default=""2012"", help=""The year of dataset (2007 or 2012)"")\n    parser.add_argument(""--data_path"", type=str, default=""data/VOCdevkit"", help=""the root folder of dataset"")\n    parser.add_argument(""--pre_trained_model_type"", type=str, choices=[""model"", ""params""], default=""model"")\n    parser.add_argument(""--pre_trained_model_path"", type=str, default=""trained_models/whole_model_trained_yolo_voc"")\n    parser.add_argument(""--output"", type=str, default=""predictions"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    input_list_path = os.path.join(opt.data_path, ""VOC{}"".format(opt.year), ""ImageSets/Main/{}.txt"".format(opt.test_set))\n    image_ids = [id.strip() for id in open(input_list_path)]\n    output_folder = os.path.join(opt.output, ""VOC{}_{}"".format(opt.year, opt.test_set))\n    colors = pickle.load(open(""src/pallete"", ""rb""))\n    if os.path.isdir(output_folder):\n        shutil.rmtree(output_folder)\n    os.makedirs(output_folder)\n    if torch.cuda.is_available():\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path)\n        else:\n            model = Yolo(20)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path))\n    else:\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            model = Yolo(20)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage))\n    model.eval()\n\n    for id in image_ids:\n        image_path = os.path.join(opt.data_path, ""VOC{}"".format(opt.year), ""JPEGImages"", ""{}.jpg"".format(id))\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        height, width = image.shape[:2]\n        image = cv2.resize(image, (opt.image_size, opt.image_size))\n        image = np.transpose(np.array(image, dtype=np.float32), (2, 0, 1))\n        image = image[None, :, :, :]\n        width_ratio = float(opt.image_size) / width\n        height_ratio = float(opt.image_size) / height\n        data = Variable(torch.FloatTensor(image))\n        if torch.cuda.is_available():\n            data = data.cuda()\n        with torch.no_grad():\n            logits = model(data)\n            predictions = post_processing(logits, opt.image_size, CLASSES, model.anchors, opt.conf_threshold,\n                                          opt.nms_threshold)\n        if len(predictions) == 0:\n            continue\n        else:\n            predictions = predictions[0]\n        output_image = cv2.imread(image_path)\n        for pred in predictions:\n            xmin = int(max(pred[0] / width_ratio, 0))\n            ymin = int(max(pred[1] / height_ratio, 0))\n            xmax = int(min((pred[0] + pred[2]) / width_ratio, width))\n            ymax = int(min((pred[1] + pred[3]) / height_ratio, height))\n            color = colors[CLASSES.index(pred[5])]\n            cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n            text_size = cv2.getTextSize(pred[5] + \' : %.2f\' % pred[4], cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n            cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)\n            cv2.putText(\n                output_image, pred[5] + \' : %.2f\' % pred[4],\n                (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n                (255, 255, 255), 1)\n            print(""Object: {}, Bounding box: ({},{}) ({},{})"".format(pred[5], xmin, xmax, ymin, ymax))\n        cv2.imwrite(""{}/{}_prediction.jpg"".format(output_folder, id), output_image)\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
test_voc_images.py,8,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport glob\nimport argparse\nimport pickle\nimport cv2\nimport numpy as np\nfrom src.utils import *\nfrom src.yolo_net import Yolo\n\nCLASSES = [\'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\',\n           \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\', \'pottedplant\', \'sheep\', \'sofa\', \'train\',\n           \'tvmonitor\']\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""You Only Look Once: Unified, Real-Time Object Detection"")\n    parser.add_argument(""--image_size"", type=int, default=448, help=""The common width and height for all images"")\n    parser.add_argument(""--conf_threshold"", type=float, default=0.35)\n    parser.add_argument(""--nms_threshold"", type=float, default=0.5)\n    parser.add_argument(""--pre_trained_model_type"", type=str, choices=[""model"", ""params""], default=""model"")\n    parser.add_argument(""--pre_trained_model_path"", type=str, default=""trained_models/whole_model_trained_yolo_voc"")\n    parser.add_argument(""--input"", type=str, default=""test_images"")\n    parser.add_argument(""--output"", type=str, default=""test_images"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    if torch.cuda.is_available():\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path)\n        else:\n            model = Yolo(20)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path))\n    else:\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            model = Yolo(20)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage))\n    model.eval()\n    colors = pickle.load(open(""src/pallete"", ""rb""))\n\n    for image_path in glob.iglob(opt.input + os.sep + \'*.jpg\'):\n        if ""prediction"" in image_path:\n            continue\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        height, width = image.shape[:2]\n        image = cv2.resize(image, (opt.image_size, opt.image_size))\n        image = np.transpose(np.array(image, dtype=np.float32), (2, 0, 1))\n        image = image[None, :, :, :]\n        width_ratio = float(opt.image_size) / width\n        height_ratio = float(opt.image_size) / height\n        data = Variable(torch.FloatTensor(image))\n        if torch.cuda.is_available():\n            data = data.cuda()\n        with torch.no_grad():\n            logits = model(data)\n            predictions = post_processing(logits, opt.image_size, CLASSES, model.anchors, opt.conf_threshold,\n                                          opt.nms_threshold)\n        if len(predictions) != 0:\n            predictions = predictions[0]\n            output_image = cv2.imread(image_path)\n            for pred in predictions:\n                xmin = int(max(pred[0] / width_ratio, 0))\n                ymin = int(max(pred[1] / height_ratio, 0))\n                xmax = int(min((pred[0] + pred[2]) / width_ratio, width))\n                ymax = int(min((pred[1] + pred[3]) / height_ratio, height))\n                color = colors[CLASSES.index(pred[5])]\n                cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n                text_size = cv2.getTextSize(pred[5] + \' : %.2f\' % pred[4], cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n                cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)\n                cv2.putText(\n                    output_image, pred[5] + \' : %.2f\' % pred[4],\n                    (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n                    (255, 255, 255), 1)\n                print(""Object: {}, Bounding box: ({},{}) ({},{})"".format(pred[5], xmin, xmax, ymin, ymax))\n            cv2.imwrite(image_path[:-4] + ""_prediction.jpg"", output_image)\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
test_voc_video.py,8,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport argparse\nimport pickle\nimport cv2\nimport numpy as np\nfrom src.utils import *\nfrom src.yolo_net import Yolo\n\nCLASSES = [\'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\',\n           \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\', \'pottedplant\', \'sheep\', \'sofa\', \'train\',\n           \'tvmonitor\']\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""You Only Look Once: Unified, Real-Time Object Detection"")\n    parser.add_argument(""--image_size"", type=int, default=448, help=""The common width and height for all images"")\n    parser.add_argument(""--conf_threshold"", type=float, default=0.35)\n    parser.add_argument(""--nms_threshold"", type=float, default=0.5)\n    parser.add_argument(""--test_set"", type=str, default=""test"",\n                        help=""For both VOC2007 and 2012, you could choose 3 different datasets: train, trainval and val. Additionally, for VOC2007, you could also pick the dataset name test"")\n    parser.add_argument(""--year"", type=str, default=""2007"", help=""The year of dataset (2007 or 2012)"")\n    parser.add_argument(""--data_path"", type=str, default=""data/VOCdevkit"", help=""the root folder of dataset"")\n    parser.add_argument(""--pre_trained_model_type"", type=str, choices=[""model"", ""params""], default=""model"")\n    parser.add_argument(""--pre_trained_model_path"", type=str, default=""trained_models/whole_model_trained_yolo_voc"")\n    parser.add_argument(""--input"", type=str, default=""test_videos/input.mp4"")\n    parser.add_argument(""--output"", type=str, default=""test_videos/output_voc.mp4"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    if torch.cuda.is_available():\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path)\n        else:\n            model = Yolo(20)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path))\n    else:\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            model = Yolo(20)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage))\n    model.eval()\n    colors = pickle.load(open(""src/pallete"", ""rb""))\n\n    cap = cv2.VideoCapture(opt.input)\n    out = cv2.VideoWriter(opt.output,  cv2.VideoWriter_fourcc(*""MJPG""), int(cap.get(cv2.CAP_PROP_FPS)),\n                          (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n    while cap.isOpened():\n        flag, image = cap.read()\n        output_image = np.copy(image)\n        if flag:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        else:\n            break\n        height, width = image.shape[:2]\n        image = cv2.resize(image, (opt.image_size, opt.image_size))\n        image = np.transpose(np.array(image, dtype=np.float32), (2, 0, 1))\n        image = image[None, :, :, :]\n        width_ratio = float(opt.image_size) / width\n        height_ratio = float(opt.image_size) / height\n        data = Variable(torch.FloatTensor(image))\n        if torch.cuda.is_available():\n            data = data.cuda()\n        with torch.no_grad():\n            logits = model(data)\n            predictions = post_processing(logits, opt.image_size, CLASSES, model.anchors, opt.conf_threshold,\n                                          opt.nms_threshold)\n        if len(predictions) != 0:\n            predictions = predictions[0]\n            for pred in predictions:\n                xmin = int(max(pred[0] / width_ratio, 0))\n                ymin = int(max(pred[1] / height_ratio, 0))\n                xmax = int(min((pred[0] + pred[2]) / width_ratio, width))\n                ymax = int(min((pred[1] + pred[3]) / height_ratio, height))\n                color = colors[CLASSES.index(pred[5])]\n                cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n                text_size = cv2.getTextSize(pred[5] + \' : %.2f\' % pred[4], cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n                cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)\n                cv2.putText(\n                    output_image, pred[5] + \' : %.2f\' % pred[4],\n                    (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n                    (255, 255, 255), 1)\n        out.write(output_image)\n\n    cap.release()\n    out.release()\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
train_coco.py,20,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport argparse\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom src.coco_dataset import COCODataset\nfrom src.utils import *\nfrom src.loss import YoloLoss\nfrom src.yolo_net import Yolo\nfrom tensorboardX import SummaryWriter\nimport shutil\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""You Only Look Once: Unified, Real-Time Object Detection"")\n    parser.add_argument(""--image_size"", type=int, default=448, help=""The common width and height for all images"")\n    parser.add_argument(""--batch_size"", type=int, default=10, help=""The number of images per batch"")\n    parser.add_argument(""--momentum"", type=float, default=0.9)\n    parser.add_argument(""--decay"", type=float, default=0.0005)\n    parser.add_argument(""--dropout"", type=float, default=0.5)\n    parser.add_argument(""--num_epoches"", type=int, default=160)\n    parser.add_argument(""--test_interval"", type=int, default=5, help=""Number of epoches between testing phases"")\n    parser.add_argument(""--object_scale"", type=float, default=1.0)\n    parser.add_argument(""--noobject_scale"", type=float, default=0.5)\n    parser.add_argument(""--class_scale"", type=float, default=1.0)\n    parser.add_argument(""--coord_scale"", type=float, default=5.0)\n    parser.add_argument(""--reduction"", type=int, default=32)\n    parser.add_argument(""--es_min_delta"", type=float, default=0.0,\n                        help=""Early stopping\'s parameter: minimum change loss to qualify as an improvement"")\n    parser.add_argument(""--es_patience"", type=int, default=0,\n                        help=""Early stopping\'s parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique."")\n    parser.add_argument(""--train_set"", type=str, default=""train"")\n    parser.add_argument(""--test_set"", type=str, default=""val"")\n    parser.add_argument(""--year"", type=str, default=""2014"", help=""The year of dataset (2014 or 2017)"")\n    parser.add_argument(""--data_path"", type=str, default=""data/COCO"", help=""the root folder of dataset"")\n    parser.add_argument(""--pre_trained_model_type"", type=str, choices=[""model"", ""params""], default=""model"")\n    parser.add_argument(""--pre_trained_model_path"", type=str, default=""trained_models/whole_model_trained_yolo_coco"")\n    parser.add_argument(""--log_path"", type=str, default=""tensorboard/yolo_coco"")\n    parser.add_argument(""--saved_path"", type=str, default=""trained_models"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef train(opt):\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(123)\n    else:\n        torch.manual_seed(123)\n    learning_rate_schedule = {""0"": 1e-5, ""5"": 1e-4,\n                              ""80"": 1e-5, ""110"": 1e-6}\n    training_params = {""batch_size"": opt.batch_size,\n                       ""shuffle"": True,\n                       ""drop_last"": True,\n                       ""collate_fn"": custom_collate_fn}\n\n    test_params = {""batch_size"": opt.batch_size,\n                   ""shuffle"": False,\n                   ""drop_last"": False,\n                   ""collate_fn"": custom_collate_fn}\n\n    training_set = COCODataset(opt.data_path, opt.year, opt.train_set, opt.image_size)\n    training_generator = DataLoader(training_set, **training_params)\n\n    test_set = COCODataset(opt.data_path, opt.year, opt.test_set, opt.image_size, is_training=False)\n    test_generator = DataLoader(test_set, **test_params)\n\n    if torch.cuda.is_available():\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path)\n        else:\n            model = Yolo(training_set.num_classes)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path))\n    else:\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            model = Yolo(training_set.num_classes)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage))\n    # The following line will re-initialize weight for the last layer, which is useful\n    # when you want to retrain the model based on my trained weights. if you uncomment it,\n    # you will see the loss is already very small at the beginning.\n    nn.init.normal_(list(model.modules())[-1].weight, 0, 0.01)\n    log_path = os.path.join(opt.log_path, ""{}"".format(opt.year))\n    if os.path.isdir(log_path):\n        shutil.rmtree(log_path)\n    os.makedirs(log_path)\n    writer = SummaryWriter(log_path)\n    if torch.cuda.is_available():\n        writer.add_graph(model.cpu(), torch.rand(opt.batch_size, 3, opt.image_size, opt.image_size))\n        model.cuda()\n    else:\n        writer.add_graph(model, torch.rand(opt.batch_size, 3, opt.image_size, opt.image_size))\n    criterion = YoloLoss(training_set.num_classes, model.anchors, opt.reduction)\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=opt.momentum, weight_decay=opt.decay)\n    best_loss = 1e10\n    best_epoch = 0\n    model.train()\n    num_iter_per_epoch = len(training_generator)\n    for epoch in range(opt.num_epoches):\n        if str(epoch) in learning_rate_schedule.keys():\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = learning_rate_schedule[str(epoch)]\n        for iter, batch in enumerate(training_generator):\n            image, label = batch\n            if torch.cuda.is_available():\n                image = Variable(image.cuda(), requires_grad=True)\n            else:\n                image = Variable(image, requires_grad=True)\n            optimizer.zero_grad()\n            logits = model(image)\n            loss, loss_coord, loss_conf, loss_cls = criterion(logits, label)\n            loss.backward()\n            optimizer.step()\n            print(""Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss:{:.2f} (Coord:{:.2f} Conf:{:.2f} Cls:{:.2f})"".format(\n                epoch + 1,\n                opt.num_epoches,\n                iter + 1,\n                num_iter_per_epoch,\n                optimizer.param_groups[0][\'lr\'],\n                loss,\n                loss_coord,\n                loss_conf,\n                loss_cls))\n            writer.add_scalar(\'Train/Total_loss\', loss, epoch * num_iter_per_epoch + iter)\n            writer.add_scalar(\'Train/Coordination_loss\', loss_coord, epoch * num_iter_per_epoch + iter)\n            writer.add_scalar(\'Train/Confidence_loss\', loss_conf, epoch * num_iter_per_epoch + iter)\n            writer.add_scalar(\'Train/Class_loss\', loss_cls, epoch * num_iter_per_epoch + iter)\n        if epoch % opt.test_interval == 0:\n            model.eval()\n            loss_ls = []\n            loss_coord_ls = []\n            loss_conf_ls = []\n            loss_cls_ls = []\n            for te_iter, te_batch in enumerate(test_generator):\n                te_image, te_label = te_batch\n                num_sample = len(te_label)\n                if torch.cuda.is_available():\n                    te_image = te_image.cuda()\n                with torch.no_grad():\n                    te_logits = model(te_image)\n                    batch_loss, batch_loss_coord, batch_loss_conf, batch_loss_cls = criterion(te_logits, te_label)\n                loss_ls.append(batch_loss * num_sample)\n                loss_coord_ls.append(batch_loss_coord * num_sample)\n                loss_conf_ls.append(batch_loss_conf * num_sample)\n                loss_cls_ls.append(batch_loss_cls * num_sample)\n            te_loss = sum(loss_ls) / test_set.__len__()\n            te_coord_loss = sum(loss_coord_ls) / test_set.__len__()\n            te_conf_loss = sum(loss_conf_ls) / test_set.__len__()\n            te_cls_loss = sum(loss_cls_ls) / test_set.__len__()\n            print(""Epoch: {}/{}, Lr: {}, Loss:{:.2f} (Coord:{:.2f} Conf:{:.2f} Cls:{:.2f})"".format(\n                epoch + 1,\n                opt.num_epoches,\n                optimizer.param_groups[0][\'lr\'],\n                te_loss,\n                te_coord_loss,\n                te_conf_loss,\n                te_cls_loss))\n            writer.add_scalar(\'Test/Total_loss\', te_loss, epoch)\n            writer.add_scalar(\'Test/Coordination_loss\', te_coord_loss, epoch)\n            writer.add_scalar(\'Test/Confidence_loss\', te_conf_loss, epoch)\n            writer.add_scalar(\'Test/Class_loss\', te_cls_loss, epoch)\n            model.train()\n            if te_loss + opt.es_min_delta < best_loss:\n                best_loss = te_loss\n                best_epoch = epoch\n                # torch.save(model, opt.saved_path + os.sep + ""trained_yolo_coco"")\n                torch.save(model.state_dict(), opt.saved_path + os.sep + ""only_params_trained_yolo_coco"")\n                torch.save(model, opt.saved_path + os.sep + ""whole_model_trained_yolo_coco"")\n\n            # Early stopping\n            if epoch - best_epoch > opt.es_patience > 0:\n                print(""Stop training at epoch {}. The lowest loss achieved is {}"".format(epoch, te_loss))\n                break\n    writer.export_scalars_to_json(log_path + os.sep + ""all_logs.json"")\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    train(opt)\n'"
train_coco_all.py,20,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport argparse\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom src.coco_dataset import COCODataset\nfrom src.utils import *\nfrom src.loss import YoloLoss\nfrom src.yolo_net import Yolo\nfrom tensorboardX import SummaryWriter\nimport shutil\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""You Only Look Once: Unified, Real-Time Object Detection"")\n    parser.add_argument(""--image_size"", type=int, default=448, help=""The common width and height for all images"")\n    parser.add_argument(""--batch_size"", type=int, default=10, help=""The number of images per batch"")\n    parser.add_argument(""--momentum"", type=float, default=0.9)\n    parser.add_argument(""--decay"", type=float, default=0.0005)\n    parser.add_argument(""--dropout"", type=float, default=0.5)\n    parser.add_argument(""--num_epoches"", type=int, default=160)\n    parser.add_argument(""--test_interval"", type=int, default=5, help=""Number of epoches between testing phases"")\n    parser.add_argument(""--object_scale"", type=float, default=1.0)\n    parser.add_argument(""--noobject_scale"", type=float, default=0.5)\n    parser.add_argument(""--class_scale"", type=float, default=1.0)\n    parser.add_argument(""--coord_scale"", type=float, default=5.0)\n    parser.add_argument(""--reduction"", type=int, default=32)\n    parser.add_argument(""--es_min_delta"", type=float, default=0.0,\n                        help=""Early stopping\'s parameter: minimum change loss to qualify as an improvement"")\n    parser.add_argument(""--es_patience"", type=int, default=0,\n                        help=""Early stopping\'s parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique."")\n    parser.add_argument(""--data_path"", type=str, default=""data/COCO"", help=""the root folder of dataset"")\n    parser.add_argument(""--pre_trained_model_type"", type=str, choices=[""model"", ""params""], default=""model"")\n    parser.add_argument(""--pre_trained_model_path"", type=str, default=""trained_models/whole_model_trained_yolo_coco"")\n    parser.add_argument(""--log_path"", type=str, default=""tensorboard/yolo_coco"")\n    parser.add_argument(""--saved_path"", type=str, default=""trained_models"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef train(opt):\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(123)\n    else:\n        torch.manual_seed(123)\n    learning_rate_schedule = {""0"": 1e-5, ""5"": 1e-4,\n                              ""80"": 1e-5, ""110"": 1e-6}\n    training_params = {""batch_size"": opt.batch_size,\n                       ""shuffle"": True,\n                       ""drop_last"": True,\n                       ""collate_fn"": custom_collate_fn}\n\n    test_params = {""batch_size"": opt.batch_size,\n                   ""shuffle"": False,\n                   ""drop_last"": False,\n                   ""collate_fn"": custom_collate_fn}\n\n    training_set = []\n    training_generator = []\n    training_set.append(COCODataset(opt.data_path, ""2014"", ""train"", opt.image_size))\n    training_set.append(COCODataset(opt.data_path, ""2014"", ""val"", opt.image_size))\n    training_set.append(COCODataset(opt.data_path, ""2017"", ""train"", opt.image_size))\n    training_generator.append(DataLoader(training_set[0], **training_params))\n    training_generator.append(DataLoader(training_set[1], **training_params))\n    training_generator.append(DataLoader(training_set[2], **training_params))\n\n    test_set = COCODataset(opt.data_path, ""2017"", ""val"", opt.image_size, is_training=False)\n    test_generator = DataLoader(test_set, **test_params)\n\n    if torch.cuda.is_available():\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path)\n        else:\n            model = Yolo(training_set[0].num_classes)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path))\n    else:\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            model = Yolo(training_set[0].num_classes)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage))\n    # The following line will re-initialize weight for the last layer, which is useful\n    # when you want to retrain the model based on my trained weights. if you uncomment it,\n    # you will see the loss is already very small at the beginning.\n    nn.init.normal_(list(model.modules())[-1].weight, 0, 0.01)\n    log_path = os.path.join(opt.log_path, ""{}"".format(""2014and2017""))\n    if os.path.isdir(log_path):\n        shutil.rmtree(log_path)\n    os.makedirs(log_path)\n    writer = SummaryWriter(log_path)\n    if torch.cuda.is_available():\n        writer.add_graph(model.cpu(), torch.rand(opt.batch_size, 3, opt.image_size, opt.image_size))\n        model.cuda()\n    else:\n        writer.add_graph(model, torch.rand(opt.batch_size, 3, opt.image_size, opt.image_size))\n    criterion = YoloLoss(training_set[0].num_classes, model.anchors, opt.reduction)\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=opt.momentum, weight_decay=opt.decay)\n    best_loss = 1e10\n    best_epoch = 0\n    model.train()\n    num_iter_per_epoch = 0\n    for generator in training_generator:\n        num_iter_per_epoch += len(generator)\n    for epoch in range(opt.num_epoches):\n        if str(epoch) in learning_rate_schedule.keys():\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = learning_rate_schedule[str(epoch)]\n        for generator in training_generator:\n            for iter, batch in enumerate(generator):\n                image, label = batch\n                if torch.cuda.is_available():\n                    image = Variable(image.cuda(), requires_grad=True)\n                else:\n                    image = Variable(image, requires_grad=True)\n                optimizer.zero_grad()\n                logits = model(image)\n                loss, loss_coord, loss_conf, loss_cls = criterion(logits, label)\n                loss.backward()\n                optimizer.step()\n                print(""Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss:{:.2f} (Coord:{:.2f} Conf:{:.2f} Cls:{:.2f})"".format(\n                    epoch + 1,\n                    opt.num_epoches,\n                    iter + 1,\n                    num_iter_per_epoch,\n                    optimizer.param_groups[0][\'lr\'],\n                    loss,\n                    loss_coord,\n                    loss_conf,\n                    loss_cls))\n                writer.add_scalar(\'Train/Total_loss\', loss, epoch * num_iter_per_epoch + iter)\n                writer.add_scalar(\'Train/Coordination_loss\', loss_coord, epoch * num_iter_per_epoch + iter)\n                writer.add_scalar(\'Train/Confidence_loss\', loss_conf, epoch * num_iter_per_epoch + iter)\n                writer.add_scalar(\'Train/Class_loss\', loss_cls, epoch * num_iter_per_epoch + iter)\n        if epoch % opt.test_interval == 0:\n            model.eval()\n            loss_ls = []\n            loss_coord_ls = []\n            loss_conf_ls = []\n            loss_cls_ls = []\n            for te_iter, te_batch in enumerate(test_generator):\n                te_image, te_label = te_batch\n                num_sample = len(te_label)\n                if torch.cuda.is_available():\n                    te_image = te_image.cuda()\n                with torch.no_grad():\n                    te_logits = model(te_image)\n                    batch_loss, batch_loss_coord, batch_loss_conf, batch_loss_cls = criterion(te_logits, te_label)\n                loss_ls.append(batch_loss * num_sample)\n                loss_coord_ls.append(batch_loss_coord * num_sample)\n                loss_conf_ls.append(batch_loss_conf * num_sample)\n                loss_cls_ls.append(batch_loss_cls * num_sample)\n            te_loss = sum(loss_ls) / test_set.__len__()\n            te_coord_loss = sum(loss_coord_ls) / test_set.__len__()\n            te_conf_loss = sum(loss_conf_ls) / test_set.__len__()\n            te_cls_loss = sum(loss_cls_ls) / test_set.__len__()\n            print(""Epoch: {}/{}, Lr: {}, Loss:{:.2f} (Coord:{:.2f} Conf:{:.2f} Cls:{:.2f})"".format(\n                epoch + 1,\n                opt.num_epoches,\n                optimizer.param_groups[0][\'lr\'],\n                te_loss,\n                te_coord_loss,\n                te_conf_loss,\n                te_cls_loss))\n            writer.add_scalar(\'Test/Total_loss\', te_loss, epoch)\n            writer.add_scalar(\'Test/Coordination_loss\', te_coord_loss, epoch)\n            writer.add_scalar(\'Test/Confidence_loss\', te_conf_loss, epoch)\n            writer.add_scalar(\'Test/Class_loss\', te_cls_loss, epoch)\n            model.train()\n            if te_loss + opt.es_min_delta < best_loss:\n                best_loss = te_loss\n                best_epoch = epoch\n                # torch.save(model, opt.saved_path + os.sep + ""trained_yolo_coco"")\n                torch.save(model.state_dict(), opt.saved_path + os.sep + ""only_params_trained_yolo_coco"")\n                torch.save(model, opt.saved_path + os.sep + ""whole_model_trained_yolo_coco"")\n\n            # Early stopping\n            if epoch - best_epoch > opt.es_patience > 0:\n                print(""Stop training at epoch {}. The lowest loss achieved is {}"".format(epoch, te_loss))\n                break\n    # writer.export_scalars_to_json(log_path + os.sep + ""all_logs.json"")\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    train(opt)\n'"
train_voc.py,20,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nimport argparse\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom src.voc_dataset import VOCDataset\nfrom src.utils import *\nfrom src.loss import YoloLoss\nfrom src.yolo_net import Yolo\nfrom tensorboardX import SummaryWriter\nimport shutil\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""You Only Look Once: Unified, Real-Time Object Detection"")\n    parser.add_argument(""--image_size"", type=int, default=448, help=""The common width and height for all images"")\n    parser.add_argument(""--batch_size"", type=int, default=10, help=""The number of images per batch"")\n    parser.add_argument(""--momentum"", type=float, default=0.9)\n    parser.add_argument(""--decay"", type=float, default=0.0005)\n    parser.add_argument(""--dropout"", type=float, default=0.5)\n    parser.add_argument(""--num_epoches"", type=int, default=160)\n    parser.add_argument(""--test_interval"", type=int, default=1, help=""Number of epoches between testing phases"")\n    parser.add_argument(""--object_scale"", type=float, default=1.0)\n    parser.add_argument(""--noobject_scale"", type=float, default=0.5)\n    parser.add_argument(""--class_scale"", type=float, default=1.0)\n    parser.add_argument(""--coord_scale"", type=float, default=5.0)\n    parser.add_argument(""--reduction"", type=int, default=32)\n    parser.add_argument(""--es_min_delta"", type=float, default=0.0,\n                        help=""Early stopping\'s parameter: minimum change loss to qualify as an improvement"")\n    parser.add_argument(""--es_patience"", type=int, default=0,\n                        help=""Early stopping\'s parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique."")\n    parser.add_argument(""--train_set"", type=str, default=""train"",\n                        help=""For both VOC2007 and 2012, you could choose 3 different datasets: train, trainval and val. Additionally, for VOC2007, you could also pick the dataset name test"")\n    parser.add_argument(""--test_set"", type=str, default=""val"",\n                        help=""For both VOC2007 and 2012, you could choose 3 different datasets: train, trainval and val. Additionally, for VOC2007, you could also pick the dataset name test"")\n    parser.add_argument(""--year"", type=str, default=""2012"", help=""The year of dataset (2007 or 2012)"")\n    parser.add_argument(""--data_path"", type=str, default=""data/VOCdevkit"", help=""the root folder of dataset"")\n    parser.add_argument(""--pre_trained_model_type"", type=str, choices=[""model"", ""params""], default=""model"")\n    parser.add_argument(""--pre_trained_model_path"", type=str, default=""trained_models/whole_model_trained_yolo_voc"")\n    parser.add_argument(""--log_path"", type=str, default=""tensorboard/yolo_voc"")\n    parser.add_argument(""--saved_path"", type=str, default=""trained_models"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef train(opt):\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(123)\n    else:\n        torch.manual_seed(123)\n    learning_rate_schedule = {""0"": 1e-5, ""5"": 1e-4,\n                              ""80"": 1e-5, ""110"": 1e-6}\n    training_params = {""batch_size"": opt.batch_size,\n                       ""shuffle"": True,\n                       ""drop_last"": True,\n                       ""collate_fn"": custom_collate_fn}\n\n    test_params = {""batch_size"": opt.batch_size,\n                   ""shuffle"": False,\n                   ""drop_last"": False,\n                   ""collate_fn"": custom_collate_fn}\n\n    training_set = VOCDataset(opt.data_path, opt.year, opt.train_set, opt.image_size)\n    training_generator = DataLoader(training_set, **training_params)\n\n    test_set = VOCDataset(opt.data_path, opt.year, opt.test_set, opt.image_size, is_training=False)\n    test_generator = DataLoader(test_set, **test_params)\n\n    if torch.cuda.is_available():\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path)\n        else:\n            model = Yolo(training_set.num_classes)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path))\n    else:\n        if opt.pre_trained_model_type == ""model"":\n            model = torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            model = Yolo(training_set.num_classes)\n            model.load_state_dict(torch.load(opt.pre_trained_model_path, map_location=lambda storage, loc: storage))\n    # The following line will re-initialize weight for the last layer, which is useful\n    # when you want to retrain the model based on my trained weights. if you uncomment it,\n    # you will see the loss is already very small at the beginning.\n    nn.init.normal_(list(model.modules())[-1].weight, 0, 0.01)\n    log_path = os.path.join(opt.log_path, ""{}"".format(opt.year))\n    if os.path.isdir(log_path):\n        shutil.rmtree(log_path)\n    os.makedirs(log_path)\n    writer = SummaryWriter(log_path)\n    if torch.cuda.is_available():\n        writer.add_graph(model.cpu(), torch.rand(opt.batch_size, 3, opt.image_size, opt.image_size))\n        model.cuda()\n    else:\n        writer.add_graph(model, torch.rand(opt.batch_size, 3, opt.image_size, opt.image_size))\n    criterion = YoloLoss(training_set.num_classes, model.anchors, opt.reduction)\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=opt.momentum, weight_decay=opt.decay)\n    best_loss = 1e10\n    best_epoch = 0\n    model.train()\n    num_iter_per_epoch = len(training_generator)\n    for epoch in range(opt.num_epoches):\n        if str(epoch) in learning_rate_schedule.keys():\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = learning_rate_schedule[str(epoch)]\n        for iter, batch in enumerate(training_generator):\n            image, label = batch\n            if torch.cuda.is_available():\n                image = Variable(image.cuda(), requires_grad=True)\n            else:\n                image = Variable(image, requires_grad=True)\n            optimizer.zero_grad()\n            logits = model(image)\n            loss, loss_coord, loss_conf, loss_cls = criterion(logits, label)\n            loss.backward()\n            optimizer.step()\n            print(""Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss:{:.2f} (Coord:{:.2f} Conf:{:.2f} Cls:{:.2f})"".format(\n                epoch + 1,\n                opt.num_epoches,\n                iter + 1,\n                num_iter_per_epoch,\n                optimizer.param_groups[0][\'lr\'],\n                loss,\n                loss_coord,\n                loss_conf,\n                loss_cls))\n            writer.add_scalar(\'Train/Total_loss\', loss, epoch * num_iter_per_epoch + iter)\n            writer.add_scalar(\'Train/Coordination_loss\', loss_coord, epoch * num_iter_per_epoch + iter)\n            writer.add_scalar(\'Train/Confidence_loss\', loss_conf, epoch * num_iter_per_epoch + iter)\n            writer.add_scalar(\'Train/Class_loss\', loss_cls, epoch * num_iter_per_epoch + iter)\n        if epoch % opt.test_interval == 0:\n            model.eval()\n            loss_ls = []\n            loss_coord_ls = []\n            loss_conf_ls = []\n            loss_cls_ls = []\n            for te_iter, te_batch in enumerate(test_generator):\n                te_image, te_label = te_batch\n                num_sample = len(te_label)\n                if torch.cuda.is_available():\n                    te_image = te_image.cuda()\n                with torch.no_grad():\n                    te_logits = model(te_image)\n                    batch_loss, batch_loss_coord, batch_loss_conf, batch_loss_cls = criterion(te_logits, te_label)\n                loss_ls.append(batch_loss * num_sample)\n                loss_coord_ls.append(batch_loss_coord * num_sample)\n                loss_conf_ls.append(batch_loss_conf * num_sample)\n                loss_cls_ls.append(batch_loss_cls * num_sample)\n            te_loss = sum(loss_ls) / test_set.__len__()\n            te_coord_loss = sum(loss_coord_ls) / test_set.__len__()\n            te_conf_loss = sum(loss_conf_ls) / test_set.__len__()\n            te_cls_loss = sum(loss_cls_ls) / test_set.__len__()\n            print(""Epoch: {}/{}, Lr: {}, Loss:{:.2f} (Coord:{:.2f} Conf:{:.2f} Cls:{:.2f})"".format(\n                epoch + 1,\n                opt.num_epoches,\n                optimizer.param_groups[0][\'lr\'],\n                te_loss,\n                te_coord_loss,\n                te_conf_loss,\n                te_cls_loss))\n            writer.add_scalar(\'Test/Total_loss\', te_loss, epoch)\n            writer.add_scalar(\'Test/Coordination_loss\', te_coord_loss, epoch)\n            writer.add_scalar(\'Test/Confidence_loss\', te_conf_loss, epoch)\n            writer.add_scalar(\'Test/Class_loss\', te_cls_loss, epoch)\n            model.train()\n            if te_loss + opt.es_min_delta < best_loss:\n                best_loss = te_loss\n                best_epoch = epoch\n                # torch.save(model, opt.saved_path + os.sep + ""trained_yolo_voc"")\n                torch.save(model.state_dict(), opt.saved_path + os.sep + ""only_params_trained_yolo_voc"")\n                torch.save(model, opt.saved_path + os.sep + ""whole_model_trained_yolo_voc"")\n\n            # Early stopping\n            if epoch - best_epoch > opt.es_patience > 0:\n                print(""Stop training at epoch {}. The lowest loss achieved is {}"".format(epoch, te_loss))\n                break\n    writer.export_scalars_to_json(log_path + os.sep + ""all_logs.json"")\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    train(opt)\n'"
convert_coco_data/convert_coco_to_pkl.py,0,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport json\nimport pickle\nimport argparse\nimport os\n\ndef get_args():\n    parser = argparse.ArgumentParser(""Parsing MS COCO dataset"")\n    parser.add_argument(""--input"", type=str, default=""data/COCO"")\n    parser.add_argument(""--type"", type=str, default=""val2014"")\n    parser.add_argument(""--output"", type=str, default=""data/COCO/anno_pickle"")\n    args = parser.parse_args()\n    return args\n\ndef main(opt):\n    ann_file = \'{}/annotations/instances_{}.json\'.format(opt.input, opt.type)\n    dataset = json.load(open(ann_file, \'r\'))\n    image_dict = {}\n    invalid_anno = 0\n\n    for image in dataset[""images""]:\n        if image[""id""] not in image_dict.keys():\n            image_dict[image[""id""]] = {""file_name"": image[""file_name""], ""objects"": []}\n\n    for ann in dataset[""annotations""]:\n        if ann[""image_id""] not in image_dict.keys():\n            invalid_anno += 1\n            continue\n        image_dict[ann[""image_id""]][""objects""].append(\n            [int(ann[""bbox""][0]), int(ann[""bbox""][1]), int(ann[""bbox""][0] + ann[""bbox""][2]),\n             int(ann[""bbox""][1] + ann[""bbox""][3]), ann[""category_id""]])\n\n    pickle.dump(image_dict, open(opt.output + os.sep + \'COCO_{}.pkl\'.format(opt.type), \'wb\'))\n    print (""There are {} invalid annotation(s)"".format(invalid_anno))\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    main(opt)\n'"
src/coco_dataset.py,1,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nfrom torch.utils.data import Dataset\nfrom src.data_augmentation import *\nimport pickle\nimport copy\n\n\nclass COCODataset(Dataset):\n    def __init__(self, root_path=""data/COCO"", year=""2014"", mode=""train"", image_size=448, is_training=True):\n        if mode in [""train"", ""val""] and year in [""2014"", ""2015"", ""2017""]:\n            self.image_path = os.path.join(root_path, ""images"", ""{}{}"".format(mode, year))\n            anno_path = os.path.join(root_path, ""anno_pickle"", ""COCO_{}{}.pkl"".format(mode, year))\n            id_list_path = pickle.load(open(anno_path, ""rb""))\n            self.id_list_path = list(id_list_path.values())\n        self.classes = [""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"",\n                        ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"",\n                        ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"",\n                        ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"", ""sports ball"", ""kite"",\n                        ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"",\n                        ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n                        ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"",\n                        ""bed"", ""dining table"", ""toilet"", ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"",\n                        ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"", ""vase"", ""scissors"",\n                        ""teddy bear"", ""hair drier"", ""toothbrush""]\n        self.class_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28,\n                          31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n                          55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n                          82, 84, 85, 86, 87, 88, 89, 90]\n        self.image_size = image_size\n        self.num_classes = len(self.classes)\n        self.num_images = len(self.id_list_path)\n        self.is_training = is_training\n\n    def __len__(self):\n        return self.num_images\n\n    def __getitem__(self, item):\n        image_path = os.path.join(self.image_path, self.id_list_path[item][""file_name""])\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        objects = copy.deepcopy(self.id_list_path[item][""objects""])\n        for idx in range(len(objects)):\n            objects[idx][4] = self.class_ids.index(objects[idx][4])\n        if self.is_training:\n            transformations = Compose([HSVAdjust(), VerticalFlip(), Crop(), Resize(self.image_size)])\n        else:\n            transformations = Compose([Resize(self.image_size)])\n        image, objects = transformations((image, objects))\n        return np.transpose(np.array(image, dtype=np.float32), (2, 0, 1)), np.array(objects, dtype=np.float32)\n    \n'"
src/data_augmentation.py,0,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport numpy as np\nfrom random import uniform\nimport cv2\n\n\nclass Compose(object):\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, data):\n        for function_ in self.transforms:\n            data = function_(data)\n        return data\n\n\nclass Crop(object):\n\n    def __init__(self, max_crop=0.1):\n        super().__init__()\n        self.max_crop = max_crop\n\n    def __call__(self, data):\n        image, label = data\n        height, width = image.shape[:2]\n        xmin = width\n        ymin = height\n        xmax = 0\n        ymax = 0\n        for lb in label:\n            xmin = min(xmin, lb[0])\n            ymin = min(ymin, lb[1])\n            xmax = max(xmax, lb[2])\n            ymax = max(ymax, lb[2])\n        cropped_left = uniform(0, self.max_crop)\n        cropped_right = uniform(0, self.max_crop)\n        cropped_top = uniform(0, self.max_crop)\n        cropped_bottom = uniform(0, self.max_crop)\n        new_xmin = int(min(cropped_left * width, xmin))\n        new_ymin = int(min(cropped_top * height, ymin))\n        new_xmax = int(max(width - 1 - cropped_right * width, xmax))\n        new_ymax = int(max(height - 1 - cropped_bottom * height, ymax))\n\n        image = image[new_ymin:new_ymax, new_xmin:new_xmax, :]\n        label = [[lb[0] - new_xmin, lb[1] - new_ymin, lb[2] - new_xmin, lb[3] - new_ymin, lb[4]] for lb in label]\n\n        return image, label\n\n\nclass VerticalFlip(object):\n\n    def __init__(self, prob=0.5):\n        super().__init__()\n        self.prob = prob\n\n    def __call__(self, data):\n        image, label = data\n        if uniform(0, 1) >= self.prob:\n            image = cv2.flip(image, 1)\n            width = image.shape[1]\n            label = [[width - lb[2], lb[1], width - lb[0], lb[3], lb[4]] for lb in label]\n        return image, label\n\n\nclass HSVAdjust(object):\n\n    def __init__(self, hue=30, saturation=1.5, value=1.5, prob=0.5):\n        super().__init__()\n        self.hue = hue\n        self.saturation = saturation\n        self.value = value\n        self.prob = prob\n\n    def __call__(self, data):\n\n        def clip_hue(hue_channel):\n            hue_channel[hue_channel >= 360] -= 360\n            hue_channel[hue_channel < 0] += 360\n            return hue_channel\n\n        image, label = data\n        adjust_hue = uniform(-self.hue, self.hue)\n        adjust_saturation = uniform(1, self.saturation)\n        if uniform(0, 1) >= self.prob:\n            adjust_saturation = 1 / adjust_saturation\n        adjust_value = uniform(1, self.value)\n        if uniform(0, 1) >= self.prob:\n            adjust_value = 1 / adjust_value\n        image = image.astype(np.float32) / 255\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        image[:, :, 0] += adjust_hue\n        image[:, :, 0] = clip_hue(image[:, :, 0])\n        image[:, :, 1] = np.clip(adjust_saturation * image[:, :, 1], 0.0, 1.0)\n        image[:, :, 2] = np.clip(adjust_value * image[:, :, 2], 0.0, 1.0)\n\n        image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n        image = (image * 255).astype(np.float32)\n\n        return image, label\n\n\nclass Resize(object):\n\n    def __init__(self, image_size):\n        super().__init__()\n        self.image_size = image_size\n\n    def __call__(self, data):\n        image, label = data\n        height, width = image.shape[:2]\n        image = cv2.resize(image, (self.image_size, self.image_size))\n        width_ratio = float(self.image_size) / width\n        height_ratio = float(self.image_size) / height\n        new_label = []\n        for lb in label:\n            resized_xmin = lb[0] * width_ratio\n            resized_ymin = lb[1] * height_ratio\n            resized_xmax = lb[2] * width_ratio\n            resized_ymax = lb[3] * height_ratio\n            resize_width = resized_xmax - resized_xmin\n            resize_height = resized_ymax - resized_ymin\n            new_label.append([resized_xmin, resized_ymin, resize_width, resize_height, lb[4]])\n\n        return image, new_label\n'"
src/loss.py,16,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport math\nimport torch\nimport torch.nn as nn\n\n\nclass YoloLoss(nn.modules.loss._Loss):\n    # The loss I borrow from LightNet repo.\n    def __init__(self, num_classes, anchors, reduction=32, coord_scale=1.0, noobject_scale=1.0,\n                 object_scale=5.0, class_scale=1.0, thresh=0.6):\n        super(YoloLoss, self).__init__()\n        self.num_classes = num_classes\n        self.num_anchors = len(anchors)\n        self.anchor_step = len(anchors[0])\n        self.anchors = torch.Tensor(anchors)\n        self.reduction = reduction\n\n        self.coord_scale = coord_scale\n        self.noobject_scale = noobject_scale\n        self.object_scale = object_scale\n        self.class_scale = class_scale\n        self.thresh = thresh\n\n    def forward(self, output, target):\n\n        batch_size = output.data.size(0)\n        height = output.data.size(2)\n        width = output.data.size(3)\n\n        # Get x,y,w,h,conf,cls\n        output = output.view(batch_size, self.num_anchors, -1, height * width)\n        coord = torch.zeros_like(output[:, :, :4, :])\n        coord[:, :, :2, :] = output[:, :, :2, :].sigmoid()  \n        coord[:, :, 2:4, :] = output[:, :, 2:4, :]\n        conf = output[:, :, 4, :].sigmoid()\n        cls = output[:, :, 5:, :].contiguous().view(batch_size * self.num_anchors, self.num_classes,\n                                                    height * width).transpose(1, 2).contiguous().view(-1,\n                                                                                                      self.num_classes)\n\n        # Create prediction boxes\n        pred_boxes = torch.FloatTensor(batch_size * self.num_anchors * height * width, 4)\n        lin_x = torch.range(0, width - 1).repeat(height, 1).view(height * width)\n        lin_y = torch.range(0, height - 1).repeat(width, 1).t().contiguous().view(height * width)\n        anchor_w = self.anchors[:, 0].contiguous().view(self.num_anchors, 1)\n        anchor_h = self.anchors[:, 1].contiguous().view(self.num_anchors, 1)\n\n        if torch.cuda.is_available():\n            pred_boxes = pred_boxes.cuda()\n            lin_x = lin_x.cuda()\n            lin_y = lin_y.cuda()\n            anchor_w = anchor_w.cuda()\n            anchor_h = anchor_h.cuda()\n\n        pred_boxes[:, 0] = (coord[:, :, 0].detach() + lin_x).view(-1)\n        pred_boxes[:, 1] = (coord[:, :, 1].detach() + lin_y).view(-1)\n        pred_boxes[:, 2] = (coord[:, :, 2].detach().exp() * anchor_w).view(-1)\n        pred_boxes[:, 3] = (coord[:, :, 3].detach().exp() * anchor_h).view(-1)\n        pred_boxes = pred_boxes.cpu()\n\n        # Get target values\n        coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls = self.build_targets(pred_boxes, target, height, width)\n        coord_mask = coord_mask.expand_as(tcoord)\n        tcls = tcls[cls_mask].view(-1).long()\n        cls_mask = cls_mask.view(-1, 1).repeat(1, self.num_classes)\n\n        if torch.cuda.is_available():\n            tcoord = tcoord.cuda()\n            tconf = tconf.cuda()\n            coord_mask = coord_mask.cuda()\n            conf_mask = conf_mask.cuda()\n            tcls = tcls.cuda()\n            cls_mask = cls_mask.cuda()\n\n        conf_mask = conf_mask.sqrt()\n        cls = cls[cls_mask].view(-1, self.num_classes)\n\n        # Compute losses\n        mse = nn.MSELoss(size_average=False)\n        ce = nn.CrossEntropyLoss(size_average=False)\n        self.loss_coord = self.coord_scale * mse(coord * coord_mask, tcoord * coord_mask) / batch_size\n        self.loss_conf = mse(conf * conf_mask, tconf * conf_mask) / batch_size\n        self.loss_cls = self.class_scale * 2 * ce(cls, tcls) / batch_size\n        self.loss_tot = self.loss_coord + self.loss_conf + self.loss_cls\n\n        return self.loss_tot, self.loss_coord, self.loss_conf, self.loss_cls\n\n    def build_targets(self, pred_boxes, ground_truth, height, width):\n        batch_size = len(ground_truth)\n\n        conf_mask = torch.ones(batch_size, self.num_anchors, height * width, requires_grad=False) * self.noobject_scale\n        coord_mask = torch.zeros(batch_size, self.num_anchors, 1, height * width, requires_grad=False)\n        cls_mask = torch.zeros(batch_size, self.num_anchors, height * width, requires_grad=False).byte()\n        tcoord = torch.zeros(batch_size, self.num_anchors, 4, height * width, requires_grad=False)\n        tconf = torch.zeros(batch_size, self.num_anchors, height * width, requires_grad=False)\n        tcls = torch.zeros(batch_size, self.num_anchors, height * width, requires_grad=False)\n\n        for b in range(batch_size):\n            if len(ground_truth[b]) == 0:\n                continue\n\n            # Build up tensors\n            cur_pred_boxes = pred_boxes[\n                             b * (self.num_anchors * height * width):(b + 1) * (self.num_anchors * height * width)]\n            if self.anchor_step == 4:\n                anchors = self.anchors.clone()\n                anchors[:, :2] = 0\n            else:\n                anchors = torch.cat([torch.zeros_like(self.anchors), self.anchors], 1)\n            gt = torch.zeros(len(ground_truth[b]), 4)\n            for i, anno in enumerate(ground_truth[b]):\n                gt[i, 0] = (anno[0] + anno[2] / 2) / self.reduction\n                gt[i, 1] = (anno[1] + anno[3] / 2) / self.reduction\n                gt[i, 2] = anno[2] / self.reduction\n                gt[i, 3] = anno[3] / self.reduction\n\n            # Set confidence mask of matching detections to 0\n            iou_gt_pred = bbox_ious(gt, cur_pred_boxes)\n            mask = (iou_gt_pred > self.thresh).sum(0) >= 1\n            conf_mask[b][mask.view_as(conf_mask[b])] = 0\n\n            # Find best anchor for each ground truth\n            gt_wh = gt.clone()\n            gt_wh[:, :2] = 0\n            iou_gt_anchors = bbox_ious(gt_wh, anchors)\n            _, best_anchors = iou_gt_anchors.max(1)\n\n            # Set masks and target values for each ground truth\n            for i, anno in enumerate(ground_truth[b]):\n                gi = min(width - 1, max(0, int(gt[i, 0])))\n                gj = min(height - 1, max(0, int(gt[i, 1])))\n                best_n = best_anchors[i]\n                iou = iou_gt_pred[i][best_n * height * width + gj * width + gi]\n                coord_mask[b][best_n][0][gj * width + gi] = 1\n                cls_mask[b][best_n][gj * width + gi] = 1\n                conf_mask[b][best_n][gj * width + gi] = self.object_scale\n                tcoord[b][best_n][0][gj * width + gi] = gt[i, 0] - gi\n                tcoord[b][best_n][1][gj * width + gi] = gt[i, 1] - gj\n                tcoord[b][best_n][2][gj * width + gi] = math.log(max(gt[i, 2], 1.0) / self.anchors[best_n, 0])\n                tcoord[b][best_n][3][gj * width + gi] = math.log(max(gt[i, 3], 1.0) / self.anchors[best_n, 1])\n                tconf[b][best_n][gj * width + gi] = iou\n                tcls[b][best_n][gj * width + gi] = int(anno[4])\n\n        return coord_mask, conf_mask, cls_mask, tcoord, tconf, tcls\n\n\ndef bbox_ious(boxes1, boxes2):\n    b1x1, b1y1 = (boxes1[:, :2] - (boxes1[:, 2:4] / 2)).split(1, 1)\n    b1x2, b1y2 = (boxes1[:, :2] + (boxes1[:, 2:4] / 2)).split(1, 1)\n    b2x1, b2y1 = (boxes2[:, :2] - (boxes2[:, 2:4] / 2)).split(1, 1)\n    b2x2, b2y2 = (boxes2[:, :2] + (boxes2[:, 2:4] / 2)).split(1, 1)\n\n    dx = (b1x2.min(b2x2.t()) - b1x1.max(b2x1.t())).clamp(min=0)\n    dy = (b1y2.min(b2y2.t()) - b1y1.max(b2y1.t())).clamp(min=0)\n    intersections = dx * dy\n\n    areas1 = (b1x2 - b1x1) * (b1y2 - b1y1)\n    areas2 = (b2x2 - b2x1) * (b2y2 - b2y1)\n    unions = (areas1 + areas2.t()) - intersections\n\n    return intersections / unions\n'"
src/utils.py,15,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport torch\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import default_collate\n\n\ndef custom_collate_fn(batch):\n    items = list(zip(*batch))\n    items[0] = default_collate(items[0])\n    items[1] = list(items[1])\n    return items\n\n\ndef post_processing(logits, image_size, gt_classes, anchors, conf_threshold, nms_threshold):\n    num_anchors = len(anchors)\n    anchors = torch.Tensor(anchors)\n    if isinstance(logits, Variable):\n        logits = logits.data\n\n    if logits.dim() == 3:\n        logits.unsqueeze_(0)\n\n    batch = logits.size(0)\n    h = logits.size(2)\n    w = logits.size(3)\n\n    # Compute xc,yc, w,h, box_score on Tensor\n    lin_x = torch.linspace(0, w - 1, w).repeat(h, 1).view(h * w)\n    lin_y = torch.linspace(0, h - 1, h).repeat(w, 1).t().contiguous().view(h * w)\n    anchor_w = anchors[:, 0].contiguous().view(1, num_anchors, 1)\n    anchor_h = anchors[:, 1].contiguous().view(1, num_anchors, 1)\n    if torch.cuda.is_available():\n        lin_x = lin_x.cuda()\n        lin_y = lin_y.cuda()\n        anchor_w = anchor_w.cuda()\n        anchor_h = anchor_h.cuda()\n\n    logits = logits.view(batch, num_anchors, -1, h * w)\n    logits[:, :, 0, :].sigmoid_().add_(lin_x).div_(w)\n    logits[:, :, 1, :].sigmoid_().add_(lin_y).div_(h)\n    logits[:, :, 2, :].exp_().mul_(anchor_w).div_(w)\n    logits[:, :, 3, :].exp_().mul_(anchor_h).div_(h)\n    logits[:, :, 4, :].sigmoid_()\n\n    with torch.no_grad():\n        cls_scores = torch.nn.functional.softmax(logits[:, :, 5:, :], 2)\n    cls_max, cls_max_idx = torch.max(cls_scores, 2)\n    cls_max_idx = cls_max_idx.float()\n    cls_max.mul_(logits[:, :, 4, :])\n\n    score_thresh = cls_max > conf_threshold\n    score_thresh_flat = score_thresh.view(-1)\n\n    if score_thresh.sum() == 0:\n        predicted_boxes = []\n        for i in range(batch):\n            predicted_boxes.append(torch.Tensor([]))\n    else:\n        coords = logits.transpose(2, 3)[..., 0:4]\n        coords = coords[score_thresh[..., None].expand_as(coords)].view(-1, 4)\n        scores = cls_max[score_thresh]\n        idx = cls_max_idx[score_thresh]\n        detections = torch.cat([coords, scores[:, None], idx[:, None]], dim=1)\n\n        max_det_per_batch = num_anchors * h * w\n        slices = [slice(max_det_per_batch * i, max_det_per_batch * (i + 1)) for i in range(batch)]\n        det_per_batch = torch.IntTensor([score_thresh_flat[s].int().sum() for s in slices])\n        split_idx = torch.cumsum(det_per_batch, dim=0)\n\n        # Group detections per image of batch\n        predicted_boxes = []\n        start = 0\n        for end in split_idx:\n            predicted_boxes.append(detections[start: end])\n            start = end\n\n    selected_boxes = []\n    for boxes in predicted_boxes:\n        if boxes.numel() == 0:\n            return boxes\n\n        a = boxes[:, :2]\n        b = boxes[:, 2:4]\n        bboxes = torch.cat([a - b / 2, a + b / 2], 1)\n        scores = boxes[:, 4]\n\n        # Sort coordinates by descending score\n        scores, order = scores.sort(0, descending=True)\n        x1, y1, x2, y2 = bboxes[order].split(1, 1)\n\n        # Compute dx and dy between each pair of boxes (these mat contain every pair twice...)\n        dx = (x2.min(x2.t()) - x1.max(x1.t())).clamp(min=0)\n        dy = (y2.min(y2.t()) - y1.max(y1.t())).clamp(min=0)\n\n        # Compute iou\n        intersections = dx * dy\n        areas = (x2 - x1) * (y2 - y1)\n        unions = (areas + areas.t()) - intersections\n        ious = intersections / unions\n\n        # Filter based on iou (and class)\n        conflicting = (ious > nms_threshold).triu(1)\n\n        keep = conflicting.sum(0).byte()\n        keep = keep.cpu()\n        conflicting = conflicting.cpu()\n\n        keep_len = len(keep) - 1\n        for i in range(1, keep_len):\n            if keep[i] > 0:\n                keep -= conflicting[i]\n        if torch.cuda.is_available():\n            keep = keep.cuda()\n\n        keep = (keep == 0)\n        selected_boxes.append(boxes[order][keep[:, None].expand_as(boxes)].view(-1, 6).contiguous())\n\n    final_boxes = []\n    for boxes in selected_boxes:\n        if boxes.dim() == 0:\n            final_boxes.append([])\n        else:\n            boxes[:, 0:3:2] *= image_size\n            boxes[:, 0] -= boxes[:, 2] / 2\n            boxes[:, 1:4:2] *= image_size\n            boxes[:, 1] -= boxes[:, 3] / 2\n\n            final_boxes.append([[box[0].item(), box[1].item(), box[2].item(), box[3].item(), box[4].item(),\n                                 gt_classes[int(box[5].item())]] for box in boxes])\n    return final_boxes\n'"
src/voc_dataset.py,1,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport os\nfrom torch.utils.data import Dataset\nimport xml.etree.ElementTree as ET\nfrom src.data_augmentation import *\n\n\nclass VOCDataset(Dataset):\n    def __init__(self, root_path=""data/VOCdevkit"", year=""2007"", mode=""train"", image_size=448, is_training = True):\n        if (mode in [""train"", ""val"", ""trainval"", ""test""] and year == ""2007"") or (\n                mode in [""train"", ""val"", ""trainval""] and year == ""2012""):\n            self.data_path = os.path.join(root_path, ""VOC{}"".format(year))\n        id_list_path = os.path.join(self.data_path, ""ImageSets/Main/{}.txt"".format(mode))\n        self.ids = [id.strip() for id in open(id_list_path)]\n        self.classes = [\'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\',\n                        \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\', \'pottedplant\', \'sheep\', \'sofa\', \'train\',\n                        \'tvmonitor\']\n        self.image_size = image_size\n        self.num_classes = len(self.classes)\n        self.num_images = len(self.ids)\n        self.is_training = is_training\n\n    def __len__(self):\n        return self.num_images\n\n    def __getitem__(self, item):\n        id = self.ids[item]\n        image_path = os.path.join(self.data_path, ""JPEGImages"", ""{}.jpg"".format(id))\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_xml_path = os.path.join(self.data_path, ""Annotations"", ""{}.xml"".format(id))\n        annot = ET.parse(image_xml_path)\n\n        objects = []\n        for obj in annot.findall(\'object\'):\n            xmin, xmax, ymin, ymax = [int(obj.find(\'bndbox\').find(tag).text) - 1 for tag in\n                                      [""xmin"", ""xmax"", ""ymin"", ""ymax""]]\n            label = self.classes.index(obj.find(\'name\').text.lower().strip())\n            objects.append([xmin, ymin, xmax, ymax, label])\n        if self.is_training:\n            transformations = Compose([HSVAdjust(), VerticalFlip(), Crop(), Resize(self.image_size)])\n        else:\n            transformations = Compose([Resize(self.image_size)])\n        image, objects = transformations((image, objects))\n\n        return np.transpose(np.array(image, dtype=np.float32), (2, 0, 1)), np.array(objects, dtype=np.float32)\n'"
src/yolo_net.py,2,"b'""""""\n@author: Viet Nguyen <nhviet1009@gmail.com>\n""""""\nimport torch.nn as nn\nimport torch\n\n\nclass Yolo(nn.Module):\n    def __init__(self, num_classes,\n                 anchors=[(1.3221, 1.73145), (3.19275, 4.00944), (5.05587, 8.09892), (9.47112, 4.84053),\n                          (11.2364, 10.0071)]):\n        super(Yolo, self).__init__()\n        self.num_classes = num_classes\n        self.anchors = anchors\n\n        self.stage1_conv1 = nn.Sequential(nn.Conv2d(3, 32, 3, 1, 1, bias=False), nn.BatchNorm2d(32),\n                                          nn.LeakyReLU(0.1, inplace=True), nn.MaxPool2d(2, 2))\n        self.stage1_conv2 = nn.Sequential(nn.Conv2d(32, 64, 3, 1, 1, bias=False), nn.BatchNorm2d(64),\n                                          nn.LeakyReLU(0.1, inplace=True), nn.MaxPool2d(2, 2))\n        self.stage1_conv3 = nn.Sequential(nn.Conv2d(64, 128, 3, 1, 1, bias=False), nn.BatchNorm2d(128),\n                                          nn.LeakyReLU(0.1, inplace=True))\n        self.stage1_conv4 = nn.Sequential(nn.Conv2d(128, 64, 1, 1, 0, bias=False), nn.BatchNorm2d(64),\n                                          nn.LeakyReLU(0.1, inplace=True))\n        self.stage1_conv5 = nn.Sequential(nn.Conv2d(64, 128, 3, 1, 1, bias=False), nn.BatchNorm2d(128),\n                                          nn.LeakyReLU(0.1, inplace=True), nn.MaxPool2d(2, 2))\n        self.stage1_conv6 = nn.Sequential(nn.Conv2d(128, 256, 3, 1, 1, bias=False), nn.BatchNorm2d(256),\n                                          nn.LeakyReLU(0.1, inplace=True))\n        self.stage1_conv7 = nn.Sequential(nn.Conv2d(256, 128, 1, 1, 0, bias=False), nn.BatchNorm2d(128),\n                                          nn.LeakyReLU(0.1, inplace=True))\n        self.stage1_conv8 = nn.Sequential(nn.Conv2d(128, 256, 3, 1, 1, bias=False), nn.BatchNorm2d(256),\n                                          nn.LeakyReLU(0.1, inplace=True), nn.MaxPool2d(2, 2))\n        self.stage1_conv9 = nn.Sequential(nn.Conv2d(256, 512, 3, 1, 1, bias=False), nn.BatchNorm2d(512),\n                                          nn.LeakyReLU(0.1, inplace=True))\n        self.stage1_conv10 = nn.Sequential(nn.Conv2d(512, 256, 1, 1, 0, bias=False), nn.BatchNorm2d(256),\n                                           nn.LeakyReLU(0.1, inplace=True))\n        self.stage1_conv11 = nn.Sequential(nn.Conv2d(256, 512, 3, 1, 1, bias=False), nn.BatchNorm2d(512),\n                                           nn.LeakyReLU(0.1, inplace=True))\n        self.stage1_conv12 = nn.Sequential(nn.Conv2d(512, 256, 1, 1, 0, bias=False), nn.BatchNorm2d(256),\n                                           nn.LeakyReLU(0.1, inplace=True))\n        self.stage1_conv13 = nn.Sequential(nn.Conv2d(256, 512, 3, 1, 1, bias=False), nn.BatchNorm2d(512),\n                                           nn.LeakyReLU(0.1, inplace=True))\n\n        self.stage2_a_maxpl = nn.MaxPool2d(2, 2)\n        self.stage2_a_conv1 = nn.Sequential(nn.Conv2d(512, 1024, 3, 1, 1, bias=False),\n                                            nn.BatchNorm2d(1024), nn.LeakyReLU(0.1, inplace=True))\n        self.stage2_a_conv2 = nn.Sequential(nn.Conv2d(1024, 512, 1, 1, 0, bias=False), nn.BatchNorm2d(512),\n                                            nn.LeakyReLU(0.1, inplace=True))\n        self.stage2_a_conv3 = nn.Sequential(nn.Conv2d(512, 1024, 3, 1, 1, bias=False), nn.BatchNorm2d(1024),\n                                            nn.LeakyReLU(0.1, inplace=True))\n        self.stage2_a_conv4 = nn.Sequential(nn.Conv2d(1024, 512, 1, 1, 0, bias=False), nn.BatchNorm2d(512),\n                                            nn.LeakyReLU(0.1, inplace=True))\n        self.stage2_a_conv5 = nn.Sequential(nn.Conv2d(512, 1024, 3, 1, 1, bias=False), nn.BatchNorm2d(1024),\n                                            nn.LeakyReLU(0.1, inplace=True))\n        self.stage2_a_conv6 = nn.Sequential(nn.Conv2d(1024, 1024, 3, 1, 1, bias=False), nn.BatchNorm2d(1024),\n                                            nn.LeakyReLU(0.1, inplace=True))\n        self.stage2_a_conv7 = nn.Sequential(nn.Conv2d(1024, 1024, 3, 1, 1, bias=False), nn.BatchNorm2d(1024),\n                                            nn.LeakyReLU(0.1, inplace=True))\n\n        self.stage2_b_conv = nn.Sequential(nn.Conv2d(512, 64, 1, 1, 0, bias=False), nn.BatchNorm2d(64),\n                                           nn.LeakyReLU(0.1, inplace=True))\n\n        self.stage3_conv1 = nn.Sequential(nn.Conv2d(256 + 1024, 1024, 3, 1, 1, bias=False), nn.BatchNorm2d(1024),\n                                          nn.LeakyReLU(0.1, inplace=True))\n        self.stage3_conv2 = nn.Conv2d(1024, len(self.anchors) * (5 + num_classes), 1, 1, 0, bias=False)\n\n    def forward(self, input):\n        output = self.stage1_conv1(input)\n        output = self.stage1_conv2(output)\n        output = self.stage1_conv3(output)\n        output = self.stage1_conv4(output)\n        output = self.stage1_conv5(output)\n        output = self.stage1_conv6(output)\n        output = self.stage1_conv7(output)\n        output = self.stage1_conv8(output)\n        output = self.stage1_conv9(output)\n        output = self.stage1_conv10(output)\n        output = self.stage1_conv11(output)\n        output = self.stage1_conv12(output)\n        output = self.stage1_conv13(output)\n\n        residual = output\n\n        output_1 = self.stage2_a_maxpl(output)\n        output_1 = self.stage2_a_conv1(output_1)\n        output_1 = self.stage2_a_conv2(output_1)\n        output_1 = self.stage2_a_conv3(output_1)\n        output_1 = self.stage2_a_conv4(output_1)\n        output_1 = self.stage2_a_conv5(output_1)\n        output_1 = self.stage2_a_conv6(output_1)\n        output_1 = self.stage2_a_conv7(output_1)\n\n        output_2 = self.stage2_b_conv(residual)\n        batch_size, num_channel, height, width = output_2.data.size()\n        output_2 = output_2.view(batch_size, int(num_channel / 4), height, 2, width, 2).contiguous()\n        output_2 = output_2.permute(0, 3, 5, 1, 2, 4).contiguous()\n        output_2 = output_2.view(batch_size, -1, int(height / 2), int(width / 2))\n\n        output = torch.cat((output_1, output_2), 1)\n        output = self.stage3_conv1(output)\n        output = self.stage3_conv2(output)\n\n        return output\n\n\nif __name__ == ""__main__"":\n    net = Yolo(20)\n    print(net.stage1_conv1[0])\n'"
