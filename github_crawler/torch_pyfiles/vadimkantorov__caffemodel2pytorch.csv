file_path,api_count,code
caffemodel2pytorch.py,17,"b""import os\nimport sys\nimport time\nimport argparse\nimport tempfile\nimport subprocess\nimport collections\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import reduce\n\ntry:\n\tfrom urllib.request import urlopen\nexcept:\n\tfrom urllib2 import urlopen # Python 2 support.\n\nimport google.protobuf.descriptor\nimport google.protobuf.descriptor_pool\nimport google.protobuf.symbol_database\nimport google.protobuf.text_format\nfrom google.protobuf.descriptor import FieldDescriptor as FD\n\nTRAIN = 0\n\nTEST = 1\n\ncaffe_pb2 = None\n\ndef initialize(caffe_proto = 'https://raw.githubusercontent.com/BVLC/caffe/master/src/caffe/proto/caffe.proto', codegen_dir = tempfile.mkdtemp(), shadow_caffe = True):\n\tglobal caffe_pb2\n\tif caffe_pb2 is None:\n\t\tlocal_caffe_proto = os.path.join(codegen_dir, os.path.basename(caffe_proto))\n\t\twith open(local_caffe_proto, 'w') as f:\n\t\t\tmybytes = urlopen(caffe_proto).read()\n\t\t\tmystr = mybytes.decode('ascii', 'ignore')\n\t\t\tf.write(mystr)\n\t\t\t#f.write((urlopen if 'http' in caffe_proto else open)(caffe_proto).read())\n\t\tsubprocess.check_call(['protoc', '--proto_path', os.path.dirname(local_caffe_proto), '--python_out', codegen_dir, local_caffe_proto])\n\t\tsys.path.insert(0, codegen_dir)\n\t\told_pool = google.protobuf.descriptor._message.default_pool\n\t\told_symdb = google.protobuf.symbol_database._DEFAULT\n\t\tgoogle.protobuf.descriptor._message.default_pool = google.protobuf.descriptor_pool.DescriptorPool()\n\t\tgoogle.protobuf.symbol_database._DEFAULT = google.protobuf.symbol_database.SymbolDatabase(pool = google.protobuf.descriptor._message.default_pool)\n\t\timport caffe_pb2 as caffe_pb2\n\t\tgoogle.protobuf.descriptor._message.default_pool = old_pool\n\t\tgoogle.protobuf.symbol_database._DEFAULT = old_symdb\n\t\tsys.modules[__name__ + '.proto'] = sys.modules[__name__]\n\t\tif shadow_caffe:\n\t\t\tsys.modules['caffe'] = sys.modules[__name__]\n\t\t\tsys.modules['caffe.proto'] = sys.modules[__name__]\n\treturn caffe_pb2\n\ndef set_mode_gpu():\n\tglobal convert_to_gpu_if_enabled\n\tconvert_to_gpu_if_enabled = lambda obj: obj.cuda()\n\ndef set_device(gpu_id):\n\ttorch.cuda.set_device(gpu_id)\n\nclass Net(nn.Module):\n\tdef __init__(self, prototxt, *args, **kwargs):\n\t\tsuper(Net, self).__init__()\n\t\t# to account for both constructors, see https://github.com/BVLC/caffe/blob/master/python/caffe/test/test_net.py#L145-L147\n\t\tcaffe_proto = kwargs.pop('caffe_proto', None) \n\t\tweights = kwargs.pop('weights', None)\n\t\tphase = kwargs.pop('phase', None)\n\t\tweights = weights or (args + (None, None))[0]\n\t\tphase = phase or (args + (None, None))[1]\n\n\t\tself.net_param = initialize(caffe_proto).NetParameter()\n\t\tgoogle.protobuf.text_format.Parse(open(prototxt).read(), self.net_param)\n\n\t\tfor layer in list(self.net_param.layer) + list(self.net_param.layers):\n\t\t\tlayer_type = layer.type if layer.type != 'Python' else layer.python_param.layer\n\t\t\tif isinstance(layer_type, int):\n\t\t\t\tlayer_type = layer.LayerType.Name(layer_type)\n\t\t\tmodule_constructor = ([v for k, v in modules.items() if k.replace('_', '').upper() in [layer_type.replace('_', '').upper(), layer.name.replace('_', '').upper()]] + [None])[0]\n\t\t\tif module_constructor is not None:\n\t\t\t\tparam = to_dict(([v for f, v in layer.ListFields() if f.name.endswith('_param')] + [None])[0])\n\t\t\t\tcaffe_input_variable_names = list(layer.bottom)\n\t\t\t\tcaffe_output_variable_names = list(layer.top)\n\t\t\t\tcaffe_loss_weight = (list(layer.loss_weight) or [1.0 if layer_type.upper().endswith('LOSS') else 0.0]) * len(layer.top)\n\t\t\t\tcaffe_propagate_down = list(getattr(layer, 'propagate_down', [])) or [True] * len(caffe_input_variable_names)\n\t\t\t\tcaffe_optimization_params = to_dict(layer.param)\n\t\t\t\tparam['inplace'] = len(caffe_input_variable_names) == 1 and caffe_input_variable_names == caffe_output_variable_names\n\t\t\t\tmodule = module_constructor(param)\n\t\t\t\tself.add_module(layer.name, module if isinstance(module, nn.Module) else CaffePythonLayerModule(module, caffe_input_variable_names, caffe_output_variable_names, param.get('param_str', '')) if type(module).__name__.endswith('Layer') else FunctionModule(module))\n\t\t\t\tmodule = getattr(self, layer.name)\n\t\t\t\tmodule.caffe_layer_name = layer.name\n\t\t\t\tmodule.caffe_layer_type = layer_type\n\t\t\t\tmodule.caffe_input_variable_names = caffe_input_variable_names\n\t\t\t\tmodule.caffe_output_variable_names = caffe_output_variable_names\n\t\t\t\tmodule.caffe_loss_weight = caffe_loss_weight\n\t\t\t\tmodule.caffe_propagate_down = caffe_propagate_down\n\t\t\t\tmodule.caffe_optimization_params = caffe_optimization_params\n\t\t\t\tfor optim_param, p in zip(caffe_optimization_params, module.parameters()):\n\t\t\t\t\tp.requires_grad = optim_param.get('lr_mult', 1) != 0\n\t\t\telse:\n\t\t\t\tprint('Skipping layer [{}, {}, {}]: not found in caffemodel2pytorch.modules dict'.format(layer.name, layer_type, layer.type))\n\n\t\tif weights is not None:\n\t\t\tself.copy_from(weights)\n\n\t\tself.blobs = collections.defaultdict(Blob)\n\t\tself.blob_loss_weights = {name : loss_weight for module in self.children() for name, loss_weight in zip(module.caffe_output_variable_names, module.caffe_loss_weight)}\n\n\t\tself.train(phase != TEST)\n\t\tconvert_to_gpu_if_enabled(self)\n\n\tdef forward(self, data = None, **variables):\n\t\tif data is not None:\n\t\t\tvariables['data'] = data\n\t\tnumpy = not all(map(torch.is_tensor, variables.values()))\n\t\tvariables = {k : convert_to_gpu_if_enabled(torch.from_numpy(v.copy()) if numpy else v) for k, v in variables.items()}\n\n\t\tfor module in [module for module in self.children() if not all(name in variables for name in module.caffe_output_variable_names)]:\n\t\t\tfor name in module.caffe_input_variable_names:\n\t\t\t\tassert name in variables, 'Variable [{}] does not exist. Pass it as a keyword argument or provide a layer which produces it.'.format(name)\n\t\t\tinputs = [variables[name] if propagate_down else variables[name].detach() for name, propagate_down in zip(module.caffe_input_variable_names, module.caffe_propagate_down)]\n\t\t\toutputs = module(*inputs)\n\t\t\tif not isinstance(outputs, tuple):\n\t\t\t\toutputs = (outputs, )\n\t\t\tvariables.update(dict(zip(module.caffe_output_variable_names, outputs)))\n\n\t\tself.blobs.update({k : Blob(data = v, numpy = numpy) for k, v in variables.items()})\n\t\tcaffe_output_variable_names = set([name for module in self.children() for name in module.caffe_output_variable_names]) - set([name for module in self.children() for name in module.caffe_input_variable_names if name not in module.caffe_output_variable_names])\n\t\treturn {k : v.detach().cpu().numpy() if numpy else v for k, v in variables.items() if k in caffe_output_variable_names}\n\n\tdef copy_from(self, weights):\n\t\ttry:\n\t\t\timport h5py, numpy\n\t\t\tstate_dict = self.state_dict()\n\t\t\tfor k, v in h5py.File(weights, 'r').items():\n\t\t\t\tif k in state_dict:\n\t\t\t\t\tstate_dict[k].resize_(v.shape).copy_(torch.from_numpy(numpy.array(v)))\n\t\t\tprint('caffemodel2pytorch: loaded model from [{}] in HDF5 format'.format(weights))\n\t\texcept Exception as e:\n\t\t\tprint('caffemodel2pytorch: loading model from [{}] in HDF5 format failed [{}], falling back to caffemodel format'.format(weights, e))\n\t\t\tbytes_weights = open(weights, 'rb').read()\n\t\t\tbytes_parsed = self.net_param.ParseFromString(bytes_weights)\n\t\t\tif bytes_parsed != len(bytes_weights):\n\t\t\t\tprint('caffemodel2pytorch: loading model from [{}] in caffemodel format, WARNING: file length [{}] is not equal to number of parsed bytes [{}]'.format(weights, len(bytes_weights), bytes_parsed))\n\t\t\tfor layer in list(self.net_param.layer) + list(self.net_param.layers):\n\t\t\t\tmodule = getattr(self, layer.name, None)\n\t\t\t\tif module is None:\n\t\t\t\t\tcontinue\n\t\t\t\tparameters = {name : convert_to_gpu_if_enabled(torch.FloatTensor(blob.data)).view(list(blob.shape.dim) if len(blob.shape.dim) > 0 else [blob.num, blob.channels, blob.height, blob.width]) for name, blob in zip(['weight', 'bias'], layer.blobs)}\n\t\t\t\tif len(parameters) > 0:\n\t\t\t\t\tmodule.set_parameters(**parameters)\n\t\t\tprint('caffemodel2pytorch: loaded model from [{}] in caffemodel format'.format(weights))\n\n\tdef save(self, weights):\n\t\timport h5py\n\t\twith h5py.File(weights, 'w') as h:\n\t\t\tfor k, v in self.state_dict().items():\n\t\t\t\th[k] = v.cpu().numpy()\n\t\tprint('caffemodel2pytorch: saved model to [{}] in HDF5 format'.format(weights))\n\n\t@property\n\tdef layers(self):\n\t\treturn list(self.children())\n\nclass Blob(object):\n\tAssignmentAdapter = type('', (object, ), dict(shape = property(lambda self: self.contents.shape), __setitem__ = lambda self, indices, values: setattr(self, 'contents', values)))\n\n\tdef __init__(self, data = None, diff = None, numpy = False):\n\t\tself.data_ = data if data is not None else Blob.AssignmentAdapter()\n\t\tself.diff_ = diff if diff is not None else Blob.AssignmentAdapter()\n\t\tself.shape_ = None\n\t\tself.numpy = numpy\n\n\tdef reshape(self, *args):\n\t\tself.shape_ = args\n\n\tdef count(self, *axis):\n\t\treturn reduce(lambda x, y: x * y, self.shape_[slice(*(axis + [-1])[:2])])\n\n\t@property\n\tdef data(self):\n\t\tif self.numpy and isinstance(self.data_, torch.autograd.Variable):\n\t\t\tself.data_ = self.data_.detach().cpu().numpy()\n\t\treturn self.data_\n\n\t@property\n\tdef diff(self):\n\t\tif self.numpy and isinstance(self.diff_, torch.autograd.Variable):\n\t\t\tself.diff_ = self.diff_.detach().cpu().numpy()\n\t\treturn self.diff_\n\n\t@property\n\tdef shape(self):\n\t\treturn self.shape_ if self.shape_ is not None else self.data_.shape\n\n\t@property\n\tdef num(self):\n\t\treturn self.shape[0]\n\n\t@property\n\tdef channels(self):\n\t\treturn self.shape[1]\n\n\t@property\n\tdef height(self):\n\t\treturn self.shape[2]\n\n\t@property\n\tdef width(self):\n\t\treturn self.shape[3]\n\nclass Layer(torch.autograd.Function):\n\tdef __init__(self, caffe_python_layer = None, caffe_input_variable_names = None, caffe_output_variable_names = None, caffe_propagate_down = None):\n\t\tself.caffe_python_layer = caffe_python_layer\n\t\tself.caffe_input_variable_names = caffe_input_variable_names\n\t\tself.caffe_output_variable_names = caffe_output_variable_names\n\t\tself.caffe_propagate_down = caffe_propagate_down\n\n\tdef forward(self, *inputs):\n\t\tbottom = [Blob(data = v.cpu().numpy()) for v in inputs]\n\t\ttop = [Blob() for name in self.caffe_output_variable_names]\n\n\t\t#self.caffe_python_layer.reshape()\n\t\tself.caffe_python_layer.setup(bottom, top)\n\t\tself.caffe_python_layer.setup = lambda *args: None\n\n\t\tself.caffe_python_layer.forward(bottom, top)\n\t\toutputs = tuple(convert_to_gpu_if_enabled(torch.from_numpy(v.data.contents.reshape(*v.shape))) for v in top)\n\t\tself.save_for_backward(*(inputs + outputs))\n\t\treturn outputs\n\n\tdef backward(self, grad_outputs):\n\t\tinputs, outputs = self.saved_tensors[:len(self.caffe_input_variable_names)], self.saved_tensors[len(self.caffe_input_variable_names):]\n\t\tbottom = [Blob(data = v.cpu().numpy()) for v in inputs]\n\t\ttop = [Blob(data = output.cpu().numpy(), diff = grad_output.cpu().numpy()) for grad_output, output in zip(grad_outputs, outputs)]\n\t\tself.caffe_python_layer.backward(top, self.caffe_propagate_down, bottom)\n\t\treturn tuple(convert_to_gpu_if_enabled(torch.from_numpy(blob.diff.contents.reshape(*v.reshape))) if propagate_down else None for v, propagate_down in zip(bottom, self.caffe_propagate_down))\n\t\t\t\nclass SGDSolver(object):\n\tdef __init__(self, solver_prototxt):\n\t\tsolver_param = initialize().SolverParameter()\n\t\tgoogle.protobuf.text_format.Parse(open(solver_prototxt).read(), solver_param)\n\t\tsolver_param = to_dict(solver_param)\n\t\tself.net = Net(solver_param.get('train_net') or solver_param.get('net'), phase = TRAIN)\n\t\tself.iter = 1\n\t\tself.iter_size = solver_param.get('iter_size', 1)\n\t\tself.optimizer_params = dict(lr = solver_param.get('base_lr') / self.iter_size, momentum = solver_param.get('momentum', 0), weight_decay = solver_param.get('weight_decay', 0))\n\t\tself.lr_scheduler_params = dict(policy = solver_param.get('lr_policy'), step_size = solver_param.get('stepsize'), gamma = solver_param.get('gamma'))\n\t\tself.optimizer, self.scheduler = None, None\n\n\tdef init_optimizer_scheduler(self):\n\t\tself.optimizer = torch.optim.SGD([dict(params = [param], lr = self.optimizer_params['lr'] * mult.get('lr_mult', 1), weight_decay = self.optimizer_params['weight_decay'] * mult.get('decay_mult', 1), momentum = self.optimizer_params['momentum']) for module in self.net.children() for param, mult in zip(module.parameters(), module.caffe_optimization_params + [{}, {}]) if param.requires_grad])\n\t\tself.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size = self.lr_scheduler_params['step_size'], gamma = self.lr_scheduler_params['gamma']) if self.lr_scheduler_params.get('policy') == 'step' else type('', (object, ), dict(step = lambda self: None))()\n\n\tdef step(self, iterations = 1, **inputs):\n\t\tloss_total = 0.0\n\t\tfor i in range(iterations):\n\t\t\ttic = time.time()\n\t\t\tif self.optimizer is not None:\n\t\t\t\tself.optimizer.zero_grad()\n\t\t\t\n\t\t\tloss_batch = 0\n\t\t\tlosses_batch = collections.defaultdict(float)\n\t\t\tfor j in range(self.iter_size):\n\t\t\t\toutputs = [kv for kv in self.net(**inputs).items() if self.net.blob_loss_weights[kv[0]] != 0]\n\t\t\t\tloss = sum([self.net.blob_loss_weights[k] * v.sum() for k, v in outputs])\n\t\t\t\tloss_batch += float(loss) / self.iter_size\n\t\t\t\tfor k, v in outputs:\n\t\t\t\t\tlosses_batch[k] += float(v.sum()) / self.iter_size\n\t\t\t\tif self.optimizer is None:\n\t\t\t\t\tself.init_optimizer_scheduler()\n\t\t\t\t\tself.optimizer.zero_grad()\n\t\t\t\tloss.backward()\n\n\t\t\tloss_total += loss_batch\n\t\t\tself.optimizer.step()\n\t\t\tself.scheduler.step()\n\t\t\tself.iter += 1\n\n\t\t\tlog_prefix = self.__module__ + '.' + type(self).__name__ \n\t\t\tprint('{}] Iteration {}, loss: {}'.format(log_prefix, self.iter, loss_batch))\n\t\t\tfor i, (name, loss) in enumerate(sorted(losses_batch.items())):\n\t\t\t\tprint('{}]     Train net output #{}: {} = {} (* {} = {} loss)'.format(log_prefix, i, name, loss, self.net.blob_loss_weights[name], self.net.blob_loss_weights[name] * loss))\n\t\t\tprint('{}] Iteration {}, lr = {}, time = {}'.format(log_prefix, self.iter, self.optimizer_params['lr'], time.time() - tic))\n\t\t\t\t\n\t\treturn loss_total\n\nmodules = dict(\n\tConvolution = lambda param: Convolution(param),\n\tInnerProduct = lambda param: InnerProduct(param),\n\tPooling = lambda param: [nn.MaxPool2d, nn.AvgPool2d][param['pool']](kernel_size = first_or(param, 'kernel_size', 1), stride = first_or(param, 'stride', 1), padding = first_or(param, 'pad', 0)),\n\tSoftmax = lambda param: nn.Softmax(dim = param.get('axis', -1)),\n\tReLU = lambda param: nn.ReLU(),\n\tDropout = lambda param: nn.Dropout(p = param['dropout_ratio']),\n\tEltwise = lambda param: [torch.mul, torch.add, torch.max][param.get('operation', 1)],\n\tLRN = lambda param: nn.LocalResponseNorm(size = param['local_size'], alpha = param['alpha'], beta = param['beta'])\n)\n\nclass FunctionModule(nn.Module):\n\tdef __init__(self, forward):\n\t\tsuper(FunctionModule, self).__init__()\n\t\tself.forward_func = forward\n\n\tdef forward(self, *inputs):\n\t\treturn self.forward_func(*inputs)\n\nclass CaffePythonLayerModule(nn.Module):\n\tdef __init__(self, caffe_python_layer, caffe_input_variable_names, caffe_output_variable_names, param_str):\n\t\tsuper(CaffePythonLayerModule, self).__init__()\n\t\tcaffe_python_layer.param_str = param_str\n\t\tself.caffe_python_layer = caffe_python_layer\n\t\tself.caffe_input_variable_names = caffe_input_variable_names\n\t\tself.caffe_output_variable_names = caffe_output_variable_names\n\n\tdef forward(self, *inputs):\n\t\treturn Layer(self.caffe_python_layer, self.caffe_input_variable_names, self.caffe_output_variable_names)(*inputs)\n\n\tdef __getattr__(self, name):\n\t\treturn nn.Module.__getattr__(self, name) if name in dir(self) else getattr(self.caffe_python_layer, name)\n\nclass Convolution(nn.Conv2d):\n\tdef __init__(self, param):\n\t\tsuper(Convolution, self).__init__(first_or(param,'group',1), param['num_output'], kernel_size = first_or(param, 'kernel_size', 1), stride = first_or(param, 'stride', 1), padding = first_or(param, 'pad', 0), dilation = first_or(param, 'dilation', 1), groups = first_or(param, 'group', 1))\n\t\tself.weight, self.bias = nn.Parameter(), nn.Parameter()\n\t\tself.weight_init, self.bias_init = param.get('weight_filler', {}), param.get('bias_filler', {})\n\n\tdef forward(self, x):\n\t\tif self.weight.numel() == 0 and self.bias.numel() == 0:\n\t\t\trequires_grad = [self.weight.requires_grad, self.bias.requires_grad]\n\t\t\tsuper(Convolution, self).__init__(x.size(1), self.out_channels, kernel_size = self.kernel_size, stride = self.stride, padding = self.padding, dilation = self.dilation)\n\t\t\tconvert_to_gpu_if_enabled(self)\n\t\t\tinit_weight_bias(self, requires_grad = requires_grad)\n\t\treturn super(Convolution, self).forward(x)\n\n\tdef set_parameters(self, weight = None, bias = None):\n\t\tinit_weight_bias(self, weight = weight, bias = bias.view(-1) if bias is not None else bias)\n\t\tself.in_channels = self.weight.size(1)\n\nclass InnerProduct(nn.Linear):\n\tdef __init__(self, param):\n\t\tsuper(InnerProduct, self).__init__(1, param['num_output'])\n\t\tself.weight, self.bias = nn.Parameter(), nn.Parameter()\n\t\tself.weight_init, self.bias_init = param.get('weight_filler', {}), param.get('bias_filler', {})\n\n\tdef forward(self, x):\n\t\tif self.weight.numel() == 0 and self.bias.numel() == 0:\n\t\t\trequires_grad = [self.weight.requires_grad, self.bias.requires_grad]\n\t\t\tsuper(InnerProduct, self).__init__(x.size(1), self.out_features)\n\t\t\tconvert_to_gpu_if_enabled(self)\n\t\t\tinit_weight_bias(self, requires_grad = requires_grad)\n\t\treturn super(InnerProduct, self).forward(x if x.size(-1) == self.in_features else x.view(len(x), -1))\n\n\tdef set_parameters(self, weight = None, bias = None):\n\t\tinit_weight_bias(self, weight = weight.view(weight.size(-2), weight.size(-1)) if weight is not None else None, bias = bias.view(-1) if bias is not None else None)\n\t\tself.in_features = self.weight.size(1)\n\ndef init_weight_bias(self, weight = None, bias = None, requires_grad = []):\n\tif weight is not None:\n\t\tself.weight = nn.Parameter(weight.type_as(self.weight), requires_grad = self.weight.requires_grad)\n\tif bias is not None:\n\t\tself.bias = nn.Parameter(bias.type_as(self.bias), requires_grad = self.bias.requires_grad)\n\tfor name, requires_grad in zip(['weight', 'bias'], requires_grad):\n\t\tparam, init = getattr(self, name), getattr(self, name + '_init')\n\t\tif init.get('type') == 'gaussian':\n\t\t\tnn.init.normal_(param, std = init['std'])\n\t\telif init.get('type') == 'constant':\n\t\t\tnn.init.constant_(param, val = init['value'])\n\t\tparam.requires_grad = requires_grad\n\ndef convert_to_gpu_if_enabled(obj):\n\treturn obj\n\ndef first_or(param, key, default):\n\treturn param[key] if isinstance(param.get(key), int) else (param.get(key, []) + [default])[0]\n\t\t\ndef to_dict(obj):\n\treturn list(map(to_dict, obj)) if isinstance(obj, collections.Iterable) else {} if obj is None else {f.name : converter(v) if f.label != FD.LABEL_REPEATED else list(map(converter, v)) for f, v in obj.ListFields() for converter in [{FD.TYPE_DOUBLE: float, FD.TYPE_SFIXED32: float, FD.TYPE_SFIXED64: float, FD.TYPE_SINT32: int, FD.TYPE_SINT64: int, FD.TYPE_FLOAT: float, FD.TYPE_ENUM: int, FD.TYPE_UINT32: int, FD.TYPE_INT64: int, FD.TYPE_UINT64: int, FD.TYPE_INT32: int, FD.TYPE_FIXED64: float, FD.TYPE_FIXED32: float, FD.TYPE_BOOL: bool, FD.TYPE_STRING: str, FD.TYPE_BYTES: lambda x: x.encode('string_escape'), FD.TYPE_MESSAGE: to_dict}[f.type]]}\n\nif __name__ == '__main__':\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(metavar = 'model.caffemodel', dest = 'model_caffemodel', help = 'Path to model.caffemodel')\n\tparser.add_argument('-o', dest = 'output_path', help = 'Path to converted model, supported file extensions are: h5, npy, npz, json, pt')\n\tparser.add_argument('--caffe.proto', metavar = '--caffe.proto', dest = 'caffe_proto', help = 'Path to caffe.proto (typically located at CAFFE_ROOT/src/caffe/proto/caffe.proto)', default = 'https://raw.githubusercontent.com/BVLC/caffe/master/src/caffe/proto/caffe.proto')\n\targs = parser.parse_args()\n\targs.output_path = args.output_path or args.model_caffemodel + '.pt'\n\n\tnet_param = initialize(args.caffe_proto).NetParameter()\n\tnet_param.ParseFromString(open(args.model_caffemodel, 'rb').read())\n\tblobs = {layer.name + '.' + name : dict(data = blob.data, shape = list(blob.shape.dim) if len(blob.shape.dim) > 0 else [blob.num, blob.channels, blob.height, blob.width]) for layer in list(net_param.layer) + list(net_param.layers) for name, blob in zip(['weight', 'bias'], layer.blobs)}\n\n\tif args.output_path.endswith('.json'):\n\t\timport json\n\t\twith open(args.output_path, 'w') as f:\n\t\t\tjson.dump(blobs, f)\n\telif args.output_path.endswith('.h5'):\n\t\timport h5py, numpy\n\t\twith h5py.File(args.output_path, 'w') as h:\n\t\t\th.update(**{k : numpy.array(blob['data'], dtype = numpy.float32).reshape(*blob['shape']) for k, blob in blobs.items()})\n\telif args.output_path.endswith('.npy') or args.output_path.endswith('.npz'):\n\t\timport numpy\n\t\t(numpy.savez if args.output_path[-1] == 'z' else numpy.save)(args.output_path, **{k : numpy.array(blob['data'], dtype = numpy.float32).reshape(*blob['shape']) for k, blob in blobs.items()})\n\telif args.output_path.endswith('.pt'):\n\t\ttorch.save({k : torch.FloatTensor(blob['data']).view(*blob['shape']) for k, blob in blobs.items()}, args.output_path)\n"""
