file_path,api_count,code
test_LRF.py,4,"b""from __future__ import print_function\nimport pickle\nimport argparse\n\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport numpy as np\n\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom data import COCOdir\nfrom data import COCODetection, BaseTransform, COCO_300, COCO_512\n\nfrom layers.functions import Detect, PriorBox\nfrom utils.nms_wrapper import nms\n\n\nparser = argparse.ArgumentParser(description='Testing Learning Rich Features Network')\nparser.add_argument('-s', '--size', default='300', help='300 or 512 input size.')\nparser.add_argument('-d', '--dataset', default='COCO', help='Currently we only provide results on MS COCO')\nparser.add_argument('-m', '--trained_model', default='weights/COCO/LRF_COCO_300/LRF_vgg_COCO_300.pth',\n                    type=str, help='Trained state_dict file path to open')\nparser.add_argument('--save_folder', default='eval/', type=str, help='Dir to save results')\nparser.add_argument('--cuda', default=True, type=bool, help='Use cuda to test model')\nargs = parser.parse_args()\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\ncfg = COCO_300 if args.size == '300' else COCO_512\n\nif args.dataset == 'COCO':\n    if args.size == '300':\n        from models.LRF_COCO_300 import build_net\n    else:\n        from models.LRF_COCO_512 import build_net\nelse:\n    print('Unkown Dataset!')\n\npriorbox = PriorBox(cfg)\npriors = Variable(priorbox.forward(), volatile=True)\npriors = priors.cpu() if not args.cuda else priors\n\n\ndef test_net(save_folder, net, detector, cuda, testset, transform, top_k=300, thresh=0.005):\n\n    if not os.path.exists(save_folder):\n        os.mkdir(save_folder)\n    num_images = len(testset)\n    num_classes = 81\n    all_boxes = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n\n    det_file = os.path.join(save_folder, 'detections.pkl')\n\n    for i in range(num_images):\n        img = testset.pull_image(i)\n        x = Variable(transform(img).unsqueeze(0), volatile=True)\n        x = x.cuda() if cuda else x\n\n        out = net(x)\n        boxes, scores = detector.forward(out, priors)\n\n        boxes = boxes[0]\n        scores = scores[0]\n        boxes = boxes.cpu().numpy()\n        scores = scores.cpu().numpy()\n        # scale   back up to the image\n        scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]]).cpu().numpy()\n        boxes *= scale\n\n        for j in range(1, num_classes):\n            inds = np.where(scores[:, j] > thresh)[0]\n            if len(inds) == 0:\n                all_boxes[j][i] = np.empty([0, 5], dtype=np.float32)\n                continue\n            c_bboxes = boxes[inds]\n            c_scores = scores[inds, j]\n            c_dets = np.hstack((c_bboxes, c_scores[:, np.newaxis])).astype(np.float32, copy=False)\n\n            cpu = False\n            keep = nms(c_dets, 0.45, force_cpu=cpu)\n            keep = keep[:50]\n            c_dets = c_dets[keep, :]\n            all_boxes[j][i] = c_dets\n\n        if top_k > 0:\n            image_scores = np.hstack([all_boxes[j][i][:, -1] for j in range(1,num_classes)])\n            if len(image_scores) > top_k:\n                image_thresh = np.sort(image_scores)[-top_k]\n                for j in range(1, num_classes):\n                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n\n    with open(det_file, 'wb') as f:\n        pickle.dump(all_boxes, f, protocol=2)\n    print('Evaluating detections')\n    testset.evaluate_detections(all_boxes, save_folder)\n\n\nif __name__ == '__main__':\n    # load net\n    img_dim = (300, 512)[args.size == '512']\n    num_classes = (21, 81)[args.dataset == 'COCO']\n    net = build_net('test', img_dim, num_classes)    # initialize detector\n\n    state_dict = torch.load(args.trained_model)\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = k[7:] if args.dataset == 'COCO' else k\n        new_state_dict[name] = v\n    net.load_state_dict(new_state_dict)\n    net.eval()\n    print('Finished loading LRFNet model!')\n    print(net)\n\n    if args.dataset == 'COCO':\n        testset = COCODetection(\n            COCOdir, [('2014', 'minival')], None)\n            # COCOdir, [('2015', 'test-dev')], None)\n    else:\n        print('Only COCO dataset is supported now!')\n\n    if args.cuda:\n        net = net.cuda()\n        cudnn.benchmark = True\n    else:\n        net = net.cpu()\n\n    top_k = 200\n    detector = Detect(num_classes, 0, cfg)\n    save_folder = os.path.join(args.save_folder, args.dataset)\n    rgb_means = (104, 117, 123)\n    test_net(save_folder, net, detector, args.cuda, testset, BaseTransform(net.size, rgb_means, (2, 0, 1)),\n             top_k, thresh=0.01)\n"""
data/__init__.py,0,"b'from .coco import COCODetection\nfrom .data_augment import *\nfrom .config import *\n\nimport cv2\nimport numpy as np\n\n\ndef base_transform(image, size, mean):\n    x = cv2.resize(image, (size, size)).astype(np.float32)\n    x -= mean\n    x = x.astype(np.float32)\n    return x\n\n\nclass BaseTransform_eval:\n    def __init__(self, size, mean):\n        self.size = size\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        return base_transform(image, self.size, self.mean), boxes, labels\n'"
data/coco.py,2,"b'""""""VOC Dataset Classes\n\nOriginal author: Francisco Massa\nhttps://github.com/fmassa/vision/blob/voc_dataset/torchvision/datasets/voc.py\n\nUpdated by: Ellis Brown, Max deGroot\n""""""\n\nimport os\nimport pickle\nimport os.path\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport cv2\nimport numpy as np\nimport json\n\nfrom utils.pycocotools.coco import COCO\nfrom utils.pycocotools.cocoeval import COCOeval\n\n\nclass COCODetection(data.Dataset):\n\n    """"""VOC Detection Dataset Object\n\n    input is image, target is annotation\n\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n        image_set (string): imageset to use (eg. \'train\', \'val\', \'test\')\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root, image_sets, preproc=None, target_transform=None,\n                 dataset_name=\'COCO\'):\n        self.root = root\n        self.cache_path = os.path.join(self.root, \'cache\')\n        self.image_set = image_sets\n        self.preproc = preproc\n        self.target_transform = target_transform\n        self.name = dataset_name\n        self.ids = list()\n        self.annotations = list()\n        self._view_map = {\n            \'minival2014\': \'val2014\',\n            \'valminusminival2014\' : \'val2014\',\n            \'test-dev2015\': \'test2015\',\n        }\n\n        for (year, image_set) in image_sets:\n            coco_name = image_set+year\n            data_name = (self._view_map[coco_name]\n                        if coco_name in self._view_map\n                        else coco_name)\n            annofile = self._get_ann_file(coco_name)\n            _COCO = COCO(annofile)\n            self._COCO = _COCO\n            self.coco_name = coco_name\n            cats = _COCO.loadCats(_COCO.getCatIds())\n            self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n            self.num_classes = len(self._classes)\n            self._class_to_ind = dict(zip(self._classes, range(self.num_classes)))\n            self._class_to_coco_cat_id = dict(zip([c[\'name\'] for c in cats],\n                                                  _COCO.getCatIds()))\n            indexes = _COCO.getImgIds()\n            self.image_indexes = indexes\n            self.ids.extend([self.image_path_from_index(data_name, index) for index in indexes ])\n            if image_set.find(\'test\') != -1:\n                print(\'test set will not load annotations!\')\n            else:\n                self.annotations.extend(self._load_coco_annotations(coco_name, indexes,_COCO))\n\n    def image_path_from_index(self, name, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # name = \'test2014\'\n        file_name = (\'COCO_\' + name + \'_\' +\n                     str(index).zfill(12) + \'.jpg\')\n        image_path = os.path.join(self.root, \'images\',\n                              name, file_name)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _get_ann_file(self, name):\n        prefix = \'instances\' if name.find(\'test\') == -1 \\\n                else \'image_info\'\n        # prefix = \'instances\'\n        return os.path.join(self.root, \'annotations\',\n                        prefix + \'_\' + name + \'.json\')\n\n    def _load_coco_annotations(self, coco_name, indexes, _COCO):\n        cache_file=os.path.join(self.cache_path,coco_name+\'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = pickle.load(fid)\n            print(\'{} gt roidb loaded from {}\'.format(coco_name,cache_file))\n            return roidb\n\n        gt_roidb = [self._annotation_from_index(index, _COCO)\n                    for index in indexes]\n        with open(cache_file, \'wb\') as fid:\n            pickle.dump(gt_roidb,fid,pickle.HIGHEST_PROTOCOL)\n        print(\'wrote gt roidb to {}\'.format(cache_file))\n        return gt_roidb\n\n    def _load_coco_annotations_test(self, coco_name):\n        cache_file = os.path.join(self.cache_path, coco_name+\'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = pickle.load(fid)\n            print(\'{} gt roidb loaded from {}\'.format(coco_name,cache_file))\n            return roidb\n\n    def _annotation_from_index(self, index, _COCO):\n        """"""\n        Loads COCO bounding-box instance annotations. Crowd instances are\n        handled by marking their overlaps (with all categories) to -1. This\n        overlap value means that crowd ""instances"" are excluded from training.\n        """"""\n        im_ann = _COCO.loadImgs(index)[0]\n        width = im_ann[\'width\']\n        height = im_ann[\'height\']\n\n        annIds = _COCO.getAnnIds(imgIds=index, iscrowd=None)\n        objs = _COCO.loadAnns(annIds)\n        # Sanitize bboxes -- some are invalid\n        valid_objs = []\n        for obj in objs:\n            x1 = np.max((0, obj[\'bbox\'][0]))\n            y1 = np.max((0, obj[\'bbox\'][1]))\n            x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n        objs = valid_objs\n        num_objs = len(objs)\n\n        res = np.zeros((num_objs, 5))\n\n        # Lookup table to map from COCO category ids to our internal class\n        # indices\n        coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                          self._class_to_ind[cls])\n                                         for cls in self._classes[1:]])\n\n        for ix, obj in enumerate(objs):\n            cls = coco_cat_id_to_class_ind[obj[\'category_id\']]\n            res[ix, 0:4] = obj[\'clean_bbox\']\n            res[ix, 4] = cls\n\n        return res\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n        target = self.annotations[index]\n        img = cv2.imread(img_id, cv2.IMREAD_COLOR)\n        height, width, _ = img.shape\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        if self.preproc is not None:\n            img, target = self.preproc(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_image(self, index):\n        \'\'\'Returns the original image object at index in PIL form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            PIL img\n        \'\'\'\n        img_id = self.ids[index]\n        return cv2.imread(img_id, cv2.IMREAD_COLOR)\n\n    def pull_tensor(self, index):\n        \'\'\'Returns the original image at an index in tensor form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            tensorized version of img, squeezed\n        \'\'\'\n        to_tensor = transforms.ToTensor()\n        return torch.Tensor(self.pull_image(index)).unsqueeze_(0)\n\n    def _print_detection_eval_metrics(self, coco_eval):\n        IoU_lo_thresh = 0.5\n        IoU_hi_thresh = 0.95\n\n        def _get_thr_ind(coco_eval, thr):\n            ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                           (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n            iou_thr = coco_eval.params.iouThrs[ind]\n            assert np.isclose(iou_thr, thr)\n            return ind\n\n        ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n        ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n\n        precision = \\\n            coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n        ap_default = np.mean(precision[precision > -1])\n        print(\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n               \'~~~~\'.format(IoU_lo_thresh, IoU_hi_thresh))\n        print(\'{:.1f}\'.format(100 * ap_default))\n        for cls_ind, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            # minus 1 because of __background__\n            precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n            ap = np.mean(precision[precision > -1])\n            print(\'{:.1f}\'.format(100 * ap))\n\n        print(\'~~~~ Summary metrics ~~~~\')\n        coco_eval.summarize()\n\n    def _do_detection_eval(self, res_file, output_dir):\n        ann_type = \'bbox\'\n        coco_dt = self._COCO.loadRes(res_file)\n        coco_eval = COCOeval(self._COCO, coco_dt)\n        coco_eval.params.useSegm = (ann_type == \'segm\')\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        self._print_detection_eval_metrics(coco_eval)\n        eval_file = os.path.join(output_dir, \'detection_results.pkl\')\n        with open(eval_file, \'wb\') as fid:\n            pickle.dump(coco_eval, fid, pickle.HIGHEST_PROTOCOL)\n        print(\'Wrote COCO eval results to: {}\'.format(eval_file))\n\n    def _coco_results_one_category(self, boxes, cat_id):\n        results = []\n        for im_ind, index in enumerate(self.image_indexes):\n            dets = boxes[im_ind].astype(np.float)\n            if dets == []:\n                continue\n            scores = dets[:, -1]\n            xs = dets[:, 0]\n            ys = dets[:, 1]\n            ws = dets[:, 2] - xs + 1\n            hs = dets[:, 3] - ys + 1\n            results.extend(\n              [{\'image_id\' : index,\n                \'category_id\' : cat_id,\n                \'bbox\' : [xs[k], ys[k], ws[k], hs[k]],\n                \'score\' : scores[k]} for k in range(dets.shape[0])])\n        return results\n\n    def _write_coco_results_file(self, all_boxes, res_file):\n        results = []\n        for cls_ind, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            print(\'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                          self.num_classes ))\n            coco_cat_id = self._class_to_coco_cat_id[cls]\n            results.extend(self._coco_results_one_category(all_boxes[cls_ind],\n                                                           coco_cat_id))\n        print(\'Writing results json to {}\'.format(res_file))\n        with open(res_file, \'w\') as fid:\n            json.dump(results, fid)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        res_file = os.path.join(output_dir, (\'detections_\' +\n                                         self.coco_name +\n                                         \'_results\'))\n        res_file += \'.json\'\n        self._write_coco_results_file(all_boxes, res_file)\n        # Only do evaluation on non-test sets\n        if self.coco_name.find(\'test\') == -1:\n            self._do_detection_eval(res_file, output_dir)\n        # Optionally cleanup results json file\n\n'"
data/config.py,0,"b'import os.path\n\nLRF_ROOT = os.path.dirname(os.getcwd())\n# access to the COCO dataset\nCOCOdir = os.path.join(LRF_ROOT, ""data/coco/"")\n\n# LRF CONFIGS\nCOCO_300 = {\n    \'feature_maps\': [38, 19, 10, 5, 3, 1],\n\n    \'min_dim\': 300,\n\n    \'steps\': [8, 16, 32, 64, 100, 300],\n\n    \'min_sizes\': [21, 45, 99, 153, 207, 261],\n\n    \'max_sizes\': [45, 99, 153, 207, 261, 315],\n\n    \'aspect_ratios\': [[2, 3], [2, 3], [2, 3], [2, 3], [2], [2]],\n\n    \'variance\': [0.1, 0.2],\n\n    \'clip\': True,\n}\n\nCOCO_512 = {\n    \'feature_maps\': [64, 32, 16, 8, 4, 2, 1],\n\n    \'min_dim\': 512,\n\n    \'steps\': [8, 16, 32, 64, 128, 256, 512],\n\n    \'min_sizes\': [20.48, 51.2, 133.12, 215.04, 296.96, 378.88, 460.8],\n\n    \'max_sizes\': [51.2, 133.12, 215.04, 296.96, 378.88, 460.8, 542.72],\n\n    \'aspect_ratios\': [[2, 3], [2, 3], [2, 3], [2, 3], [2,3], [2], [2]],\n\n    \'variance\': [0.1, 0.2],\n\n    \'clip\': True,\n}\n\n\n\n'"
data/data_augment.py,8,"b'""""""Data augmentation functionality. Passed as callable transformations to\nDataset classes.\n\nThe data augmentation procedures were interpreted from @weiliu89\'s SSD paper\nhttp://arxiv.org/abs/1512.02325\n\nTODO: implement data_augment for training\n\nEllis Brown, Max deGroot\n""""""\n\nimport torch\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport random\nimport math\nfrom utils.box_utils import matrix_iou\n# import torch_transforms\n\n\ndef _crop(image, boxes, labels):\n    height, width, _ = image.shape\n\n    if len(boxes)== 0:\n        return image, boxes, labels\n\n    while True:\n        mode = random.choice((\n            None,\n            (0.1, None),\n            (0.3, None),\n            (0.5, None),\n            (0.7, None),\n            (0.9, None),\n            (None, None),\n        ))\n\n        if mode is None:\n            return image, boxes, labels\n\n        min_iou, max_iou = mode\n        if min_iou is None:\n            min_iou = float(\'-inf\')\n        if max_iou is None:\n            max_iou = float(\'inf\')\n\n        for _ in range(50):\n            scale = random.uniform(0.3,1.)\n            min_ratio = max(0.5, scale*scale)\n            max_ratio = min(2, 1. / scale / scale)\n            ratio = math.sqrt(random.uniform(min_ratio, max_ratio))\n            w = int(scale * ratio * width)\n            h = int((scale / ratio) * height)\n\n            l = random.randrange(width - w)\n            t = random.randrange(height - h)\n            roi = np.array((l, t, l + w, t + h))\n\n            iou = matrix_iou(boxes, roi[np.newaxis])\n            \n            if not (min_iou <= iou.min() and iou.max() <= max_iou):\n                continue\n\n            image_t = image[roi[1]:roi[3], roi[0]:roi[2]]\n\n            centers = (boxes[:, :2] + boxes[:, 2:]) / 2\n            mask = np.logical_and(roi[:2] < centers, centers < roi[2:]) \\\n                     .all(axis=1)\n            boxes_t = boxes[mask].copy()\n            labels_t = labels[mask].copy()\n            if len(boxes_t) == 0:\n                continue\n\n            boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi[:2])\n            boxes_t[:, :2] -= roi[:2]\n            boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi[2:])\n            boxes_t[:, 2:] -= roi[:2]\n\n            return image_t, boxes_t,labels_t\n\n\ndef _distort(image):\n    def _convert(image, alpha=1, beta=0):\n        tmp = image.astype(float) * alpha + beta\n        tmp[tmp < 0] = 0\n        tmp[tmp > 255] = 255\n        image[:] = tmp\n\n    image = image.copy()\n\n    if random.randrange(2):\n        _convert(image, beta=random.uniform(-32, 32))\n\n    if random.randrange(2):\n        _convert(image, alpha=random.uniform(0.5, 1.5))\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    if random.randrange(2):\n        tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n        tmp %= 180\n        image[:, :, 0] = tmp\n\n    if random.randrange(2):\n        _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n\n    image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n\n    return image\n\n\ndef _expand(image, boxes,fill, p):\n    if random.random() > p:\n        return image, boxes\n\n    height, width, depth = image.shape\n    for _ in range(50):\n        scale = random.uniform(1, 4)\n\n        min_ratio = max(0.5, 1./scale/scale)\n        max_ratio = min(2, scale*scale)\n        ratio = math.sqrt(random.uniform(min_ratio, max_ratio))\n        ws = scale*ratio\n        hs = scale/ratio\n        if ws < 1 or hs < 1:\n            continue\n        w = int(ws * width)\n        h = int(hs * height)\n\n        left = random.randint(0, w - width)\n        top = random.randint(0, h - height)\n\n        boxes_t = boxes.copy()\n        boxes_t[:, :2] += (left, top)\n        boxes_t[:, 2:] += (left, top)\n\n        expand_image = np.empty(\n            (h, w, depth),\n            dtype=image.dtype)\n        expand_image[:, :] = fill\n        expand_image[top:top + height, left:left + width] = image\n        image = expand_image\n\n        return image, boxes_t\n\n\ndef _mirror(image, boxes):\n    _, width, _ = image.shape\n    if random.randrange(2):\n        image = image[:, ::-1]\n        boxes = boxes.copy()\n        boxes[:, 0::2] = width - boxes[:, 2::-2]\n    return image, boxes\n\n\ndef preproc_for_test(image, insize, mean):\n    interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n    interp_method = interp_methods[random.randrange(5)]\n    image = cv2.resize(image, (insize, insize), interpolation=interp_method)\n    image = image.astype(np.float32)\n    image -= mean\n    return image.transpose(2, 0, 1)\n\n\nclass preproc(object):\n\n    def __init__(self, resize, rgb_means, p):\n        self.means = rgb_means\n        self.resize = resize\n        self.p = p\n\n    def __call__(self, image, targets):\n        boxes = targets[:,:-1].copy()\n        labels = targets[:,-1].copy()\n        if len(boxes) == 0:\n            #boxes = np.empty((0, 4))\n            targets = np.zeros((1,5))\n            image = preproc_for_test(image, self.resize, self.means)\n            return torch.from_numpy(image), targets\n\n        image_o = image.copy()\n        targets_o = targets.copy()\n        height_o, width_o, _ = image_o.shape\n        boxes_o = targets_o[:,:-1]\n        labels_o = targets_o[:,-1]\n        boxes_o[:, 0::2] /= width_o\n        boxes_o[:, 1::2] /= height_o\n        labels_o = np.expand_dims(labels_o,1)\n        targets_o = np.hstack((boxes_o,labels_o))\n\n        image_t, boxes, labels = _crop(image, boxes, labels)\n        image_t = _distort(image_t)\n        image_t, boxes = _expand(image_t, boxes, self.means, self.p)\n        image_t, boxes = _mirror(image_t, boxes)\n        #image_t, boxes = _mirror(image, boxes)\n\n        height, width, _ = image_t.shape\n        image_t = preproc_for_test(image_t, self.resize, self.means)\n        # height, width, _ = image.shape\n        # image_t = preproc_for_test(image, self.resize, self.means)\n        boxes = boxes.copy()\n        boxes[:, 0::2] /= width\n        boxes[:, 1::2] /= height\n        b_w = (boxes[:, 2] - boxes[:, 0])*1.\n        b_h = (boxes[:, 3] - boxes[:, 1])*1.\n        mask_b= np.minimum(b_w, b_h) > 0.01\n        boxes_t = boxes[mask_b]\n        labels_t = labels[mask_b].copy()\n\n        if len(boxes_t)==0:\n            image = preproc_for_test(image_o, self.resize, self.means)\n            return torch.from_numpy(image),targets_o\n\n        labels_t = np.expand_dims(labels_t,1)\n        targets_t = np.hstack((boxes_t,labels_t))\n\n        return torch.from_numpy(image_t), targets_t\n\n\nclass preproc_pyramid(object):\n\n    def __init__(self, resize, rgb_means, p):\n        self.means = rgb_means\n        self.resize = resize\n        self.p = p\n\n    def __call__(self, image, targets):\n        boxes = targets[:,:-1].copy()\n        labels = targets[:,-1].copy()\n        if len(boxes) == 0:\n            #boxes = np.empty((0, 4))\n            targets = np.zeros((1,5))\n            image = preproc_for_test(image, self.resize, self.means)\n            return torch.from_numpy(image), targets\n\n        image_o = image.copy()\n        targets_o = targets.copy()\n        height_o, width_o, _ = image_o.shape\n        boxes_o = targets_o[:,:-1]\n        labels_o = targets_o[:,-1]\n        boxes_o[:, 0::2] /= width_o\n        boxes_o[:, 1::2] /= height_o\n        labels_o = np.expand_dims(labels_o,1)\n        targets_o = np.hstack((boxes_o,labels_o))\n\n        image_t, boxes, labels = _crop(image, boxes, labels)\n        image_t = _distort(image_t)\n        image_t, boxes = _expand(image_t, boxes, self.means, self.p)\n        image_t, boxes = _mirror(image_t, boxes)\n        #image_t, boxes = _mirror(image, boxes)\n\n        height, width, _ = image_t.shape\n        image_t = preproc_for_test(image_t, self.resize, self.means)\n        # height, width, _ = image.shape\n        # image_t = preproc_for_test(image, self.resize, self.means)\n        boxes = boxes.copy()\n        boxes[:, 0::2] /= width\n        boxes[:, 1::2] /= height\n        b_w = (boxes[:, 2] - boxes[:, 0])*1.\n        b_h = (boxes[:, 3] - boxes[:, 1])*1.\n        mask_b= np.minimum(b_w, b_h) > 0.01\n        boxes_t = boxes[mask_b]\n        labels_t = labels[mask_b].copy()\n\n        if len(boxes_t)==0:\n            image = preproc_for_test(image_o, self.resize, self.means)\n            return torch.from_numpy(image),targets_o\n\n        labels_t = np.expand_dims(labels_t,1)\n        targets_t = np.hstack((boxes_t,labels_t))\n\n        pr_conv4_3 = cv2.resize(image_t, (38, 38), interpolation=cv2.INTER_LINEAR)\n        pr_conv7 = cv2.resize(pr_conv4_3, (19, 19), interpolation=cv2.INTER_LINEAR)\n        pr_conv8 = cv2.resize(pr_conv7, (10, 10), interpolation=cv2.INTER_LINEAR)\n        pr_conv9 = cv2.resize(pr_conv8, (5, 5), interpolation=cv2.INTER_LINEAR)\n\n        return torch.from_numpy(image_t), torch.from_numpy(pr_conv4_3), torch.from_numpy(pr_conv7), \\\n               torch.from_numpy(pr_conv8), torch.from_numpy(pr_conv9), targets_t\n\n\nclass BaseTransform(object):\n    """"""Defines the transformations that should be applied to test PIL image\n        for input into the network\n\n    dimension -> tensorize -> color adj\n\n    Arguments:\n        resize (int): input dimension to SSD\n        rgb_means ((int,int,int)): average RGB of the dataset\n            (104,117,123)\n        swap ((int,int,int)): final order of channels\n    Returns:\n        transform (transform) : callable transform to be applied to test/val\n        data\n    """"""\n    def __init__(self, resize, rgb_means, swap=(2, 0, 1)):\n        self.means = rgb_means\n        self.resize = resize\n        self.swap = swap\n\n    # assume input is cv2 img for now\n    def __call__(self, img):\n\n        interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n        interp_method = interp_methods[0]\n        img = cv2.resize(np.array(img), (self.resize,\n                                         self.resize),interpolation = interp_method).astype(np.float32)\n        img -= self.means\n        img = img.transpose(self.swap)\n        return torch.from_numpy(img)\n'"
layers/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
models/LRF_COCO_300.py,10,"b'import torch\nimport torch.nn as nn\nimport os\nimport torch.nn.functional as F\n\n\nclass LDS(nn.Module):\n    def __init__(self,):\n        super(LDS, self).__init__()\n        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=1)\n\n    def forward(self, x):\n        x_pool1 = self.pool1(x)\n        x_pool2 = self.pool2(x_pool1)\n        x_pool3 = self.pool3(x_pool2)\n        return x_pool3\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(ConvBlock, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=False) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\nclass LSN_init(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(LSN_init, self).__init__()\n        self.out_channels = out_planes\n        inter_planes = out_planes // 4\n        self.part_a = nn.Sequential(\n                ConvBlock(in_planes, inter_planes, kernel_size=(3, 3), stride=stride, padding=1),\n                ConvBlock(inter_planes, inter_planes, kernel_size=1, stride=1),\n                ConvBlock(inter_planes, inter_planes, kernel_size=(3, 3), stride=stride, padding=1)\n                )\n        self.part_b = ConvBlock(inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n\n    def forward(self, x):\n        out1 = self.part_a(x)\n        out2 = self.part_b(out1)\n        return out1, out2\n\n\nclass LSN_later(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(LSN_later, self).__init__()\n        self.out_channels = out_planes\n        inter_planes = out_planes // 4\n        self.part_a = ConvBlock(in_planes, inter_planes, kernel_size=(3, 3), stride=stride, padding=1)\n        self.part_b = ConvBlock(inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n\n    def forward(self, x):\n        out1 = self.part_a(x)\n        out2 = self.part_b(out1)\n        return out1, out2\n\n\nclass IBN(nn.Module):\n    def __init__(self, out_planes, bn=True):\n        super(IBN, self).__init__()\n        self.out_channels = out_planes\n        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n\n    def forward(self, x):\n        if self.bn is not None:\n            x = self.bn(x)\n        return x\n\n\nclass One_Three_Conv(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(One_Three_Conv, self).__init__()\n        self.out_channels = out_planes\n        inter_planes = in_planes // 4\n        self.single_branch = nn.Sequential(\n                ConvBlock(in_planes, inter_planes, kernel_size=1, stride=1),\n                ConvBlock(inter_planes, out_planes, kernel_size=(3, 3), stride=stride, padding=1, relu=False)\n                )\n\n    def forward(self, x):\n        out = self.single_branch(x)\n        return out\n\n\nclass Relu_Conv(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(Relu_Conv, self).__init__()\n        self.out_channels = out_planes\n        self.relu = nn.ReLU(inplace=False)\n        self.single_branch = nn.Sequential(\n            ConvBlock(in_planes, out_planes, kernel_size=(3, 3), stride=stride, padding=1)\n        )\n\n    def forward(self, x):\n        x = self.relu(x)\n        out = self.single_branch(x)\n        return out\n\n\nclass Ds_Conv(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1, padding=(1, 1)):\n        super(Ds_Conv, self).__init__()\n        self.out_channels = out_planes\n        self.single_branch = nn.Sequential(\n            ConvBlock(in_planes, out_planes, kernel_size=(3, 3), stride=stride, padding=padding, relu=False)\n        )\n\n    def forward(self, x):\n        out = self.single_branch(x)\n        return out\n\n\nclass LRFNet(nn.Module):\n    """"""LRFNet for object detection\n    The network is based on the SSD architecture.\n    Each multibox layer branches into\n        1) conv2d for class conf scores\n        2) conv2d for localization predictions\n        3) associated priorbox layer to produce default bounding\n           boxes specific to the layer\'s feature map size.\n\n    Args:\n        phase: (string) Can be ""test"" or ""train""\n        base: VGG16 layers for input, size of either 300 or 512\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n    """"""\n\n    def __init__(self, phase, size, base, extras, head, num_classes):\n        super(LRFNet, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.size = size\n\n        # vgg network\n        self.base = nn.ModuleList(base)\n\n        self.lds = LDS()\n\n        # convs for merging the lsn and ssd features\n        self.Norm1 = Relu_Conv(512, 512, stride=1)\n        self.Norm2 = Relu_Conv(1024, 1024, stride=1)\n        self.Norm3 = Relu_Conv(512, 512, stride=1)\n        self.Norm4 = Relu_Conv(256, 256, stride=1)\n\n        # convs for generate the lsn features\n        self.icn1 = LSN_init(3, 512, stride=1)\n        self.icn2 = LSN_later(128, 1024, stride=2)\n        self.icn3 = LSN_later(256, 512, stride=2)\n\n        # convs with s=2 to downsample the features\n        self.dsc1 = Ds_Conv(512, 1024, stride=2, padding=(1, 1))\n        self.dsc2 = Ds_Conv(1024, 512, stride=2, padding=(1, 1))\n        self.dsc3 = Ds_Conv(512, 256, stride=2, padding=(1, 1))\n\n        # convs to reduce the feature dimensions of current level\n        self.agent1 = ConvBlock(512, 256, kernel_size=1, stride=1)\n        self.agent2 = ConvBlock(1024, 512, kernel_size=1, stride=1)\n        self.agent3 = ConvBlock(512, 256, kernel_size=1, stride=1)\n\n        # convs to reduce the feature dimensions of other levels\n        self.proj1 = ConvBlock(1024, 128, kernel_size=1, stride=1)\n        self.proj2 = ConvBlock(512, 128, kernel_size=1, stride=1)\n        self.proj3 = ConvBlock(256, 128, kernel_size=1, stride=1)\n\n        # convs to reduce the feature dimensions of other levels\n        self.convert1 = ConvBlock(384, 256, kernel_size=1)\n        self.convert2 = ConvBlock(256, 512, kernel_size=1)\n        self.convert3 = ConvBlock(128, 256, kernel_size=1)\n\n        # convs to merge the features of the current and higher level features\n        self.merge1 = ConvBlock(512, 512, kernel_size=3, stride=1, padding=1)\n        self.merge2 = ConvBlock(1024, 1024, kernel_size=3, stride=1, padding=1)\n        self.merge3 = ConvBlock(512, 512, kernel_size=3, stride=1, padding=1)\n\n        self.ibn1 = IBN(512, bn=True)\n        self.ibn2 = IBN(1024, bn=True)\n\n        self.relu = nn.ReLU(inplace=False)\n\n        self.extras = nn.ModuleList(extras)\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        if self.phase == \'test\':\n            self.softmax = nn.Softmax()\n\n    def forward(self, x):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                list of concat outputs from:\n                    1: softmax layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n        sources = list()\n        loc = list()\n        conf = list()\n        new_sources = list()\n\n        # apply lds to the initial image\n        x_pool = self.lds(x)\n\n        # apply vgg up to conv4_3\n        for k in range(22):\n            x = self.base[k](x)\n        conv4_3_bn = self.ibn1(x)\n        x_pool1_skip, x_pool1_icn = self.icn1(x_pool)\n        s = self.Norm1(conv4_3_bn * x_pool1_icn)\n\n        # apply vgg up to fc7\n        for k in range(22, 34):\n            x = self.base[k](x)\n        conv7_bn = self.ibn2(x)\n        x_pool2_skip, x_pool2_icn = self.icn2(x_pool1_skip)\n        p = self.Norm2(self.dsc1(s) + conv7_bn * x_pool2_icn)\n\n        x = self.base[34](x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = v(x)\n            if k == 0:\n                x_pool3_skip, x_pool3_icn = self.icn3(x_pool2_skip)\n                w = self.Norm3(self.dsc2(p) + x * x_pool3_icn)\n            elif k == 2:\n                q = self.Norm4(self.dsc3(w) + x)\n                sources.append(q)\n            elif k == 5 or k == 7:\n                sources.append(x)\n            else:\n                pass\n\n        # project the forward features into lower dimension.\n        tmp1 = self.proj1(p)\n        tmp2 = self.proj2(w)\n        tmp3 = self.proj3(q)\n\n        # The conv4_3 level\n        proj1 = F.upsample(tmp1, size=(38, 38), mode=\'bilinear\')\n        proj2 = F.upsample(tmp2, size=(38, 38), mode=\'bilinear\')\n        proj3 = F.upsample(tmp3, size=(38, 38), mode=\'bilinear\')\n        proj = torch.cat([proj1, proj2, proj3], dim=1)\n\n        agent1 = self.agent1(s)\n        convert1 = self.convert1(proj)\n        pred1 = torch.cat([agent1, convert1], dim=1)\n        pred1 = self.merge1(pred1)\n        new_sources.append(pred1)\n\n        # The fc_7 level\n        proj2 = F.upsample(tmp2, size=(19, 19), mode=\'bilinear\')\n        proj3 = F.upsample(tmp3, size=(19, 19), mode=\'bilinear\')\n        proj = torch.cat([proj2, proj3], dim=1)\n\n        agent2 = self.agent2(p)\n        convert2 = self.convert2(proj)\n        pred2 = torch.cat([agent2, convert2], dim=1)\n        pred2 = self.merge2(pred2)\n        new_sources.append(pred2)\n\n        # The conv8 level\n        proj3 = F.upsample(tmp3, size=(10, 10), mode=\'bilinear\')\n        proj = proj3\n\n        agent3 = self.agent3(w)\n        convert3 = self.convert3(proj)\n        pred3 = torch.cat([agent3, convert3], dim=1)\n        pred3 = self.merge3(pred3)\n        new_sources.append(pred3)\n\n        for prediction in sources:\n            new_sources.append(prediction)\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(new_sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if self.phase == ""test"":\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\n    def load_weights(self, base_file):\n        other, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            print(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file))\n            print(\'Finished!\')\n        else:\n            print(\'Sorry only .pth and .pkl files supported.\')\n\n\ndef vgg(cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == \'C\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=False)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=False)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=False), conv7, nn.ReLU(inplace=False)]\n    return layers\n\nbase = {\n    \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512]}\n\n\ndef add_extras(size, cfg, i, batch_norm=False):\n    # Extra layers added to VGG for feature scaling\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                if in_channels == 256 and size == 512:\n                    layers += [One_Three_Conv(in_channels, cfg[k+1], stride=2), nn.ReLU(inplace=False)]\n                else:\n                    layers += [One_Three_Conv(in_channels, cfg[k+1], stride=2), nn.ReLU(inplace=False)]\n        in_channels = v\n    layers += [ConvBlock(256, 128, kernel_size=1,stride=1)]\n    layers += [ConvBlock(128, 256, kernel_size=3,stride=1)]\n    layers += [ConvBlock(256, 128, kernel_size=1,stride=1)]\n    layers += [ConvBlock(128, 256, kernel_size=3,stride=1)]\n    return layers\n\n\nextras = {\n    \'300\': [1024, \'S\', 512, \'S\', 256]}\n\n\ndef multibox(size, vgg, extra_layers, cfg, num_classes):\n    loc_layers = []\n    conf_layers = []\n    vgg_source = [1, -2]\n    for k, v in enumerate(vgg_source):\n        if k == 0:\n            loc_layers += [nn.Conv2d(512,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n            conf_layers +=[nn.Conv2d(512,\n                                 cfg[k] * num_classes, kernel_size=3, padding=1)]\n        else:\n            loc_layers += [nn.Conv2d(vgg[v].out_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n            conf_layers += [nn.Conv2d(vgg[v].out_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    i = 2\n    indicator = 3\n\n    for k, v in enumerate(extra_layers):\n        if (k < indicator+1 and k % 2 == 0) or (k > indicator+1 and k % 2 != 0):\n            loc_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                 * 4, kernel_size=3, padding=1)]\n            conf_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                  * num_classes, kernel_size=3, padding=1)]\n            i += 1\n\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\n\nmbox = {\n    \'300\': [6, 6, 6, 6, 4, 4]}\n\n\ndef build_net(phase, size=300, num_classes=81):\n    if size != 300:\n        print(""Error: The input image size is not supported!"")\n        return\n\n    return LRFNet(phase, size, *multibox(size, vgg(base[str(size)], 3),\n                                add_extras(size, extras[str(size)], 1024),\n                                mbox[str(size)], num_classes), num_classes)\n'"
models/LRF_COCO_512.py,12,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\n\n\nclass LDS(nn.Module):\n    def __init__(self,):\n        super(LDS, self).__init__()\n        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n\n    def forward(self, x):\n        x_pool1 = self.pool1(x)\n        x_pool2 = self.pool2(x_pool1)\n        x_pool3 = self.pool3(x_pool2)\n        return x_pool3\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(ConvBlock, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=False) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\nclass LSN_init(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(LSN_init, self).__init__()\n        self.out_channels = out_planes\n        inter_planes = out_planes // 4\n        self.part_a = nn.Sequential(\n                ConvBlock(in_planes, inter_planes, kernel_size=(3, 3), stride=stride, padding=1),\n                ConvBlock(inter_planes, inter_planes, kernel_size=1, stride=1),\n                ConvBlock(inter_planes, inter_planes, kernel_size=(3, 3), stride=stride, padding=1)\n                )\n        self.part_b = ConvBlock(inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n\n    def forward(self, x):\n        out1 = self.part_a(x)\n        out2 = self.part_b(out1)\n        return out1, out2\n\n\nclass LSN_later(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(LSN_later, self).__init__()\n        self.out_channels = out_planes\n        inter_planes = out_planes // 4\n        self.part_a = ConvBlock(in_planes, inter_planes, kernel_size=(3, 3), stride=stride, padding=1)\n        self.part_b = ConvBlock(inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n\n    def forward(self, x):\n        out1 = self.part_a(x)\n        out2 = self.part_b(out1)\n        return out1, out2\n\n\nclass IBN(nn.Module):\n\n    def __init__(self, out_planes, bn=True):\n        super(IBN, self).__init__()\n        self.out_channels = out_planes\n        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n\n    def forward(self, x):\n        if self.bn is not None:\n            x = self.bn(x)\n        return x\n\n\nclass One_Three_Conv(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(One_Three_Conv, self).__init__()\n        self.out_channels = out_planes\n        inter_planes = in_planes // 4\n        self.single_branch = nn.Sequential(\n                ConvBlock(in_planes, inter_planes, kernel_size=1, stride=1),\n                ConvBlock(inter_planes, out_planes, kernel_size=(3, 3), stride=stride, padding=1, relu=False)\n                )\n\n    def forward(self, x):\n        out = self.single_branch(x)\n        return out\n\n\nclass Relu_Conv(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(Relu_Conv, self).__init__()\n        self.out_channels = out_planes\n        self.relu = nn.ReLU(inplace=False)\n        self.single_branch = nn.Sequential(\n            ConvBlock(in_planes, out_planes, kernel_size=(3, 3), stride=stride, padding=1)\n        )\n\n    def forward(self, x):\n        x = self.relu(x)\n        out = self.single_branch(x)\n        return out\n\n\nclass Ds_Conv(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1, padding=(1, 1)):\n        super(Ds_Conv, self).__init__()\n        self.out_channels = out_planes\n        self.single_branch = nn.Sequential(\n            ConvBlock(in_planes, out_planes, kernel_size=(3, 3), stride=stride, padding=padding, relu=False)\n        )\n\n    def forward(self, x):\n        out = self.single_branch(x)\n        return out\n\n\nclass LRFNet(nn.Module):\n    """"""LRFNet for object detection\n    The network is based on the SSD architecture.\n    Each multibox layer branches into\n        1) conv2d for class conf scores\n        2) conv2d for localization predictions\n        3) associated priorbox layer to produce default bounding\n           boxes specific to the layer\'s feature map size.\n\n    Args:\n        phase: (string) Can be ""test"" or ""train""\n        base: VGG16 layers for input, size of either 300 or 512\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n    """"""\n\n    def __init__(self, phase, size, base, extras, head, num_classes):\n        super(LRFNet, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.size = size\n\n        # vgg network\n        self.base = nn.ModuleList(base)\n\n        self.lds = LDS()\n\n        # convs for merging the lsn and ssd features\n        self.Norm1 = Relu_Conv(512, 512, stride=1)\n        self.Norm2 = Relu_Conv(1024, 1024, stride=1)\n        self.Norm3 = Relu_Conv(512, 512, stride=1)\n        self.Norm4 = Relu_Conv(256, 256, stride=1)\n        self.Norm5 = Relu_Conv(256, 256, stride=1)\n\n        # convs for generate the lsn features\n        self.icn1 = LSN_init(3, 512, stride=1)\n        self.icn2 = LSN_later(128, 1024, stride=2)\n        self.icn3 = LSN_later(256, 512, stride=2)\n        self.icn4 = LSN_later(128, 256, stride=2)\n\n        # convs with s=2 to downsample the features\n        self.dsc1 = Ds_Conv(512, 1024, stride=2, padding=(1, 1))\n        self.dsc2 = Ds_Conv(1024, 512, stride=2, padding=(1, 1))\n        self.dsc3 = Ds_Conv(512, 256, stride=2, padding=(1, 1))\n        self.dsc4 = Ds_Conv(256, 256, stride=2, padding=(1, 1))\n\n        # convs to reduce the feature dimensions of current level\n        self.agent1 = ConvBlock(512, 256, kernel_size=1, stride=1)\n        self.agent2 = ConvBlock(1024, 512, kernel_size=1, stride=1)\n        self.agent3 = ConvBlock(512, 256, kernel_size=1, stride=1)\n        self.agent4 = ConvBlock(256, 128, kernel_size=1, stride=1)\n\n        # convs to reduce the feature dimensions of other levels\n        self.proj1 = ConvBlock(1024, 128, kernel_size=1, stride=1)\n        self.proj2 = ConvBlock(512, 128, kernel_size=1, stride=1)\n        self.proj3 = ConvBlock(256, 128, kernel_size=1, stride=1)\n        self.proj4 = ConvBlock(256, 128, kernel_size=1, stride=1)\n\n        # convs to reduce the feature dimensions of other levels\n        self.convert1 = ConvBlock(512, 256, kernel_size=1)\n        self.convert2 = ConvBlock(384, 512, kernel_size=1)\n        self.convert3 = ConvBlock(256, 256, kernel_size=1)\n        self.convert4 = ConvBlock(128, 128, kernel_size=1)\n\n        # convs to merge the features of the current and higher level features\n        self.merge1 = ConvBlock(512, 512, kernel_size=3, stride=1, padding=1)\n        self.merge2 = ConvBlock(1024, 1024, kernel_size=3, stride=1, padding=1)\n        self.merge3 = ConvBlock(512, 512, kernel_size=3, stride=1, padding=1)\n        self.merge4 = ConvBlock(256, 256, kernel_size=3, stride=1, padding=1)\n\n        self.ibn1 = IBN(512, bn=True)\n        self.ibn2 = IBN(1024, bn=True)\n\n        self.relu = nn.ReLU(inplace=False)\n\n        self.extras = nn.ModuleList(extras)\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        if self.phase == \'test\':\n            self.softmax = nn.Softmax()\n\n    def forward(self, x):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,512,512].\n\n        Return:\n            Depending on phase:\n            test:\n                list of concat outputs from:\n                    1: softmax layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n        sources = list()\n        loc = list()\n        conf = list()\n        new_sources = list()\n\n        # apply lds to the initial image\n        x_pool = self.lds(x)\n\n        # apply vgg up to conv4_3\n        for k in range(22):\n            x = self.base[k](x)\n        conv4_3_bn = self.ibn1(x)\n        x_pool1_skip, x_pool1_icn = self.icn1(x_pool)\n        s = self.Norm1(conv4_3_bn * x_pool1_icn)\n\n        # apply vgg up to fc7\n        for k in range(22, 34):\n            x = self.base[k](x)\n        conv7_bn = self.ibn2(x)\n        x_pool2_skip, x_pool2_icn = self.icn2(x_pool1_skip)\n        p = self.Norm2(self.dsc1(s) + conv7_bn * x_pool2_icn)\n\n        x = self.base[34](x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = v(x)\n            if k == 0:\n                x_pool3_skip, x_pool3_icn = self.icn3(x_pool2_skip)\n                w = self.Norm3(self.dsc2(p) + x * x_pool3_icn)\n            elif k == 2:\n                x_pool4_skip, x_pool4_icn = self.icn4(x_pool3_skip)\n                q = self.Norm4(self.dsc3(w) + x * x_pool4_icn)\n            elif k == 4:\n                o = self.Norm5(self.dsc4(q) + x)\n                sources.append(o)\n            elif k == 7 or k == 9:\n                sources.append(x)\n            else:\n                pass\n\n        # project the forward features into lower dimension.\n        tmp1 = self.proj1(p)\n        tmp2 = self.proj2(w)\n        tmp3 = self.proj3(q)\n        tmp4 = self.proj4(o)\n\n        # The conv4_3 level\n        proj1 = F.upsample(tmp1, scale_factor=2, mode=\'bilinear\')\n        proj2 = F.upsample(tmp2, scale_factor=4, mode=\'bilinear\')\n        proj3 = F.upsample(tmp3, scale_factor=8, mode=\'bilinear\')\n        proj4 = F.upsample(tmp4, scale_factor=16, mode=\'bilinear\')\n        proj = torch.cat([proj1, proj2, proj3, proj4], dim=1)\n\n        agent1 = self.agent1(s)\n        convert1 = self.convert1(proj)\n        pred1 = torch.cat([agent1, convert1], dim=1)\n        pred1 = self.merge1(pred1)\n        new_sources.append(pred1)\n\n        # The fc_7 level\n        proj2 = F.upsample(tmp2, scale_factor=2, mode=\'bilinear\')\n        proj3 = F.upsample(tmp3, scale_factor=4, mode=\'bilinear\')\n        proj4 = F.upsample(tmp4, scale_factor=8, mode=\'bilinear\')\n        proj = torch.cat([proj2, proj3, proj4], dim=1)\n\n        agent2 = self.agent2(p)\n        convert2 = self.convert2(proj)\n        pred2 = torch.cat([agent2, convert2], dim=1)\n        pred2 = self.merge2(pred2)\n        new_sources.append(pred2)\n\n        # The conv8 level\n        proj3 = F.upsample(tmp3, scale_factor=2, mode=\'bilinear\')\n        proj4 = F.upsample(tmp4, scale_factor=4, mode=\'bilinear\')\n        proj = torch.cat([proj3, proj4], dim=1)\n\n        agent3 = self.agent3(w)\n        convert3 = self.convert3(proj)\n        pred3 = torch.cat([agent3, convert3], dim=1)\n        pred3 = self.merge3(pred3)\n        new_sources.append(pred3)\n\n        # The conv9 level\n        proj4 = F.upsample(tmp4, scale_factor=2, mode=\'bilinear\')\n        proj = proj4\n\n        agent4 = self.agent4(q)\n        convert4 = self.convert4(proj)\n        pred4 = torch.cat([agent4, convert4], dim=1)\n        pred4 = self.merge4(pred4)\n        new_sources.append(pred4)\n\n        for prediction in sources:\n            new_sources.append(prediction)\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(new_sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if self.phase == ""test"":\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\n    def load_weights(self, base_file):\n        other, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            print(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file))\n            print(\'Finished!\')\n        else:\n            print(\'Sorry only .pth and .pkl files supported.\')\n\n\ndef vgg(cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == \'C\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=False)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=False)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=False), conv7, nn.ReLU(inplace=False)]\n    return layers\n\n\nbase = {\n    \'512\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512]\n}\n\n\ndef add_extras(size, cfg, i, batch_norm=False):\n    # Extra layers added to VGG for feature scaling\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                if in_channels == 256 and size == 512:\n                    layers += [One_Three_Conv(in_channels, cfg[k+1], stride=2), nn.ReLU(inplace=False)]\n                else:\n                    layers += [One_Three_Conv(in_channels, cfg[k+1], stride=2), nn.ReLU(inplace=False)]\n        in_channels = v\n    layers += [ConvBlock(256, 128, kernel_size=1, stride=1)]\n    layers += [ConvBlock(128, 256, kernel_size=4, stride=1, padding=1)]\n    return layers\n\n\nextras = {\n    \'512\': [1024, \'S\', 512, \'S\', 256, \'S\', 256, \'S\', 256]}\n\n\ndef multibox(size, vgg, extra_layers, cfg, num_classes):\n    loc_layers = []\n    conf_layers = []\n    vgg_source = [1, -2]\n    for k, v in enumerate(vgg_source):\n        if k == 0:\n            loc_layers += [nn.Conv2d(512,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n            conf_layers +=[nn.Conv2d(512,\n                                 cfg[k] * num_classes, kernel_size=3, padding=1)]\n        else:\n            loc_layers += [nn.Conv2d(vgg[v].out_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n            conf_layers += [nn.Conv2d(vgg[v].out_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    i = 2\n    indicator = 5\n\n    for k, v in enumerate(extra_layers):\n        if k < indicator-2 and k % 2 == 0:\n            loc_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                 * 4, kernel_size=3, padding=1)]\n            conf_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                  * num_classes, kernel_size=3, padding=1)]\n            i += 1\n\n        elif k > indicator-1 and k % 2 != 0:\n            loc_layers += [nn.Conv2d(256, cfg[i]\n                                     * 4, kernel_size=3, padding=1)]\n            conf_layers += [nn.Conv2d(256, cfg[i]\n                                      * num_classes, kernel_size=3, padding=1)]\n            i += 1\n\n        else:\n            pass\n\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\n\nmbox = {\'512\': [6, 6, 6, 6, 6, 4, 4]}\n# number of boxes per feature map location\n\n\ndef build_net(phase, size=512, num_classes=81):\n    if size != 512:\n        print(""Error: The input image size is not supported!"")\n        return\n\n    return LRFNet(phase, size, *multibox(size, vgg(base[str(size)], 3),\n                                add_extras(size, extras[str(size)], 1024),\n                                mbox[str(size)], num_classes), num_classes)\n'"
models/__init__.py,0,b''
utils/__init__.py,0,b''
utils/box_utils.py,32,"b'import torch\nimport torch.nn as nn\nimport math\nimport numpy as np\nif torch.cuda.is_available():\n    import torch.backends.cudnn as cudnn\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef matrix_iou(a,b):\n    """"""\n    return iou of a and b, numpy version for data augenmentation\n    """"""\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n    return area_i / (area_a[:, np.newaxis] + area_b - area_i)\n\n\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    """"""\n    # jaccard index\n    overlaps = jaccard(\n        truths,\n        point_form(priors)\n    )\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n    conf = labels[best_truth_idx]          # Shape: [num_priors]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n    loc = encode(matches, priors, variances)\n    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n\n\ndef encode(matched, priors, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\ndef encode_multi(matched, priors, offsets, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2] - offsets[:,:2]\n    # encode variance\n    #g_cxcy /= (variances[0] * priors[:, 2:])\n    g_cxcy.div_(variances[0] * offsets[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef decode_multi(loc, priors, offsets, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + offsets[:,:2]+ loc[:, :2] * variances[0] * offsets[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    """"""Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    """"""\n\n    keep = torch.Tensor(scores.size(0)).fill_(0).long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n'"
utils/build.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                                   \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\': home, \'nvcc\': nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\n\n\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs=[numpy_include]\n    ),\n    Extension(\'nms.gpu_nms\',\n              [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with gcc\n              # the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_61\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']]\n              ),\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs=[numpy_include, \'pycocotools\'],\n        extra_compile_args={\n            \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    ),\n]\n\nsetup(\n    name=\'mot_utils\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
utils/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom .nms.cpu_nms import cpu_nms, cpu_soft_nms\nfrom .nms.gpu_nms import gpu_nms\n\n\n# def nms(dets, thresh, force_cpu=False):\n#     """"""Dispatch to either CPU or GPU NMS implementations.""""""\n#\n#     if dets.shape[0] == 0:\n#         return []\n#     if cfg.USE_GPU_NMS and not force_cpu:\n#         return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n#     else:\n#         return cpu_nms(dets, thresh)\n\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n    if dets.shape[0] == 0:\n        return []\n    if force_cpu:\n        #return cpu_soft_nms(dets, thresh, method = 0)\n        return cpu_nms(dets, thresh)\n    return gpu_nms(dets, thresh)\n'"
utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n\n    def clear(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n'"
layers/functions/__init__.py,0,"b""from .detection import Detect\nfrom .prior_box import PriorBox\n\n\n__all__ = ['Detect', 'PriorBox']\n"""
layers/functions/detection.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Function\nfrom torch.autograd import Variable\nfrom utils.box_utils import decode, nms\n\n\nclass Detect(Function):\n    """"""At test time, Detect is the final layer of SSD.  Decode location preds,\n    apply non-maximum suppression to location predictions based on conf\n    scores and threshold to a top_k number of output predictions for both\n    confidence score and locations.\n    """"""\n    def __init__(self, num_classes, bkg_label, cfg):\n        self.num_classes = num_classes\n        self.background_label = bkg_label\n        #self.thresh = thresh\n\n        # Parameters used in nms.\n        self.variance = cfg[\'variance\']\n\n    def forward(self, predictions, prior):\n        """"""\n        Args:\n            loc_data: (tensor) Loc preds from loc layers\n                Shape: [batch,num_priors*4]\n            conf_data: (tensor) Shape: Conf preds from conf layers\n                Shape: [batch*num_priors,num_classes]\n            prior_data: (tensor) Prior boxes and variances from priorbox layers\n                Shape: [1,num_priors,4]\n        """"""\n\n        loc, conf = predictions\n\n        loc_data = loc.data\n        conf_data = conf.data\n        prior_data = prior.data\n        num = loc_data.size(0)  # batch size\n        self.num_priors = prior_data.size(0)\n        self.boxes = torch.zeros(1, self.num_priors, 4)\n        self.scores = torch.zeros(1, self.num_priors, self.num_classes)\n\n        if num == 1:\n            # size batch x num_classes x num_priors\n            conf_preds = conf_data.unsqueeze(0)\n\n        else:\n            conf_preds = conf_data.view(num, num_priors,\n                                        self.num_classes)\n            self.boxes.expand_(num, self.num_priors, 4)\n            self.scores.expand_(num, self.num_priors, self.num_classes)\n\n        # Decode predictions into bboxes.\n        for i in range(num):\n            decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n            # For each class, perform nms\n            conf_scores = conf_preds[i].clone()\n            \'\'\'\n            c_mask = conf_scores.gt(self.thresh)\n            decoded_boxes = decoded_boxes[c_mask]\n            conf_scores = conf_scores[c_mask]\n            \'\'\'\n\n            self.boxes[i] = decoded_boxes\n            self.scores[i] = conf_scores\n\n            # _, class_ind = self.scores.max(2)\n\n        # return self.boxes, self.scores, class_ind\n        return self.boxes, self.scores\n\n\nclass Detect_eval(Function):\n    """"""At test time, Detect is the final layer of SSD.  Decode location preds,\n    apply non-maximum suppression to location predictions based on conf\n    scores and threshold to a top_k number of output predictions for both\n    confidence score and locations.\n    """"""\n    def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh):\n        self.num_classes = num_classes\n        self.background_label = bkg_label\n        self.top_k = top_k\n        # Parameters used in nms.\n        self.nms_thresh = nms_thresh\n        if nms_thresh <= 0:\n            raise ValueError(\'nms_threshold must be non negative.\')\n        self.conf_thresh = conf_thresh\n        self.variance = cfg[\'variance\']\n\n    def forward(self, loc_data, conf_data, prior_data):\n        """"""\n        Args:\n            loc_data: (tensor) Loc preds from loc layers\n                Shape: [batch,num_priors*4]\n            conf_data: (tensor) Shape: Conf preds from conf layers\n                Shape: [batch*num_priors,num_classes]\n            prior_data: (tensor) Prior boxes and variances from priorbox layers\n                Shape: [1,num_priors,4]\n        """"""\n        num = loc_data.size(0)  # batch size\n        num_priors = prior_data.size(0)\n        output = torch.zeros(num, self.num_classes, self.top_k, 5)\n        conf_preds = conf_data.view(num, num_priors,\n                                    self.num_classes).transpose(2, 1)\n\n        # Decode predictions into bboxes.\n        for i in range(num):\n            decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n            # For each class, perform nms\n            conf_scores = conf_preds[i].clone()\n\n            for cl in range(1, self.num_classes):\n                c_mask = conf_scores[cl].gt(self.conf_thresh)\n                scores = conf_scores[cl][c_mask]\n                if scores.dim() == 0:\n                    continue\n                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n                boxes = decoded_boxes[l_mask].view(-1, 4)\n                # idx of highest scoring and non-overlapping boxes per class\n                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n                output[i, cl, :count] = \\\n                    torch.cat((scores[ids[:count]].unsqueeze(1),\n                               boxes[ids[:count]]), 1)\n        flt = output.contiguous().view(num, -1, 5)\n        _, idx = flt[:, :, 0].sort(1, descending=True)\n        _, rank = idx.sort(1)\n        flt[(rank < self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0)\n        return output\n'"
layers/functions/prior_box.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom math import sqrt as sqrt\nfrom itertools import product as product\nif torch.cuda.is_available():\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n\n\nclass PriorBox(object):\n    """"""Compute priorbox coordinates in center-offset form for each source\n    feature map.\n    Note:\n    This \'layer\' has changed between versions of the original SSD\n    paper, so we include both versions, but note v2 is the most tested and most\n    recent version of the paper.\n\n    """"""\n    def __init__(self, cfg):\n        super(PriorBox, self).__init__()\n        self.image_size = cfg[\'min_dim\']\n        # number of priors for feature map location (either 4 or 6)\n        self.num_priors = len(cfg[\'aspect_ratios\'])\n        self.variance = cfg[\'variance\'] or [0.1]\n        self.feature_maps = cfg[\'feature_maps\']\n        self.min_sizes = cfg[\'min_sizes\']\n        self.max_sizes = cfg[\'max_sizes\']\n        self.steps = cfg[\'steps\']\n        self.aspect_ratios = cfg[\'aspect_ratios\']\n        self.clip = cfg[\'clip\']\n        for v in self.variance:\n            if v <= 0:\n                raise ValueError(\'Variances must be greater than 0\')\n\n    def forward(self):\n        mean = []\n        for k, f in enumerate(self.feature_maps):\n            for i, j in product(range(f), repeat=2):\n                f_k = self.image_size / self.steps[k]\n                cx = (j + 0.5) / f_k\n                cy = (i + 0.5) / f_k\n\n                s_k = self.min_sizes[k]/self.image_size\n                mean += [cx, cy, s_k, s_k]\n\n                # aspect_ratio: 1\n                # rel size: sqrt(s_k * s_(k+1))\n                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n                mean += [cx, cy, s_k_prime, s_k_prime]\n\n                # rest of aspect ratios\n                for ar in self.aspect_ratios[k]:\n                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n            # a = 1\n\n        # back to torch land\n        output = torch.Tensor(mean).view(-1, 4)\n        if self.clip:\n            output.clamp_(max=1, min=0)\n        return output\n'"
layers/modules/__init__.py,0,"b""from .l2norm import L2Norm\nfrom .multibox_loss import MultiBoxLoss\n\n__all__ = ['L2Norm', 'MultiBoxLoss']\n"""
layers/modules/l2norm.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd import Variable\nimport torch.nn.init as init\n\nclass L2Norm(nn.Module):\n    def __init__(self,n_channels, scale):\n        super(L2Norm,self).__init__()\n        self.n_channels = n_channels\n        self.gamma = scale or None\n        self.eps = 1e-10\n        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init.constant(self.weight,self.gamma)\n\n    def forward(self, x):\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n        #x /= norm\n        x = torch.div(x, norm)\n        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n        return out\n'"
layers/modules/multibox_loss.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom utils.box_utils import match, log_sum_exp\nGPU = False\nif torch.cuda.is_available():\n    GPU = True\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Compute Targets:\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n    Objective Loss:\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Args:\n            c: class confidences,\n            l: predicted boxes,\n            g: ground truth boxes\n            N: number of matched default boxes\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n\n    def __init__(self, num_classes,overlap_thresh,prior_for_matching,bkg_label,neg_mining,neg_pos,neg_overlap,encode_target):\n        super(MultiBoxLoss, self).__init__()\n        self.num_classes = num_classes\n        self.threshold = overlap_thresh\n        self.background_label = bkg_label\n        self.encode_target = encode_target\n        self.use_prior_for_matching  = prior_for_matching\n        self.do_neg_mining = neg_mining\n        self.negpos_ratio = neg_pos\n        self.neg_overlap = neg_overlap\n        self.variance = [0.1,0.2]\n\n    def forward(self, predictions, priors, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n\n            ground_truth (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n        """"""\n\n        loc_data, conf_data = predictions\n        priors = priors\n        num = loc_data.size(0)\n        num_priors = (priors.size(0))\n        num_classes = self.num_classes\n\n        # match priors (default boxes) and ground truth boxes\n        loc_t = torch.Tensor(num, num_priors, 4)\n        conf_t = torch.LongTensor(num, num_priors)\n        for idx in range(num):\n            truths = targets[idx][:,:-1].data\n            labels = targets[idx][:,-1].data\n            defaults = priors.data\n            match(self.threshold,truths,defaults,self.variance,labels,loc_t,conf_t,idx)\n        if GPU:\n            loc_t = loc_t.cuda()\n            conf_t = conf_t.cuda()\n        # wrap targets\n        loc_t = Variable(loc_t, requires_grad=False)\n        conf_t = Variable(conf_t,requires_grad=False)\n\n        pos = conf_t > 0\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1,4)\n        loc_t = loc_t[pos_idx].view(-1,4)\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n\n        # Compute max conf across batch for hard negative mining\n        batch_conf = conf_data.view(-1,self.num_classes)\n        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1,1))\n\n        # Hard Negative Mining\n        loss_c[pos.view(-1)] = 0 # filter out pos boxes for now\n        loss_c = loss_c.view(num, -1)\n        _,loss_idx = loss_c.sort(1, descending=True)\n        _,idx_rank = loss_idx.sort(1)\n        num_pos = pos.long().sum(1,keepdim=True)\n        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n        neg = idx_rank < num_neg.expand_as(idx_rank)\n\n        # Confidence Loss Including Positive and Negative Examples\n        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)\n        targets_weighted = conf_t[(pos+neg).gt(0)]\n        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n\n        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n\n        N = num_pos.data.sum()\n        loss_l/=N\n        loss_c/=N\n        return loss_l,loss_c\n'"
utils/nms/__init__.py,0,b''
utils/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
utils/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
utils/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
utils/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
utils/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\n#import pycocotools._mask as _mask\nfrom . import _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]\n'"
