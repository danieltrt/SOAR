file_path,api_count,code
setup.py,0,"b'import sys\n\nfrom distutils.core import setup\nfrom setuptools import find_packages\n\n\n# List of runtime dependencies required by this built package\ninstall_requires = []\nif sys.version_info <= (2, 7):\n    install_requires += [\'future\', \'typing\']\ninstall_requires += [""numpy"", ""protobuf""]\n\nsetup(\n    name=""tfrecord"",\n    version=""1.11"",\n    description=""TFRecord reader"",\n    author=""Vahid Kazemi"",\n    author_email=""vkazemi@gmail.com"",\n    url=""https://github.com/vahidk/tfrecord"",\n    packages=find_packages(),\n    license=""MIT"",\n    install_requires=install_requires\n)\n'"
tfrecord/__init__.py,0,b'from tfrecord import tools\nfrom tfrecord import torch\n\nfrom tfrecord import example_pb2\nfrom tfrecord import iterator_utils\nfrom tfrecord import reader\nfrom tfrecord import writer\n\nfrom tfrecord.iterator_utils import *\nfrom tfrecord.reader import *\nfrom tfrecord.writer import *\n'
tfrecord/example_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: example.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'example.proto\',\n  package=\'tfrecord\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\rexample.proto\\x12\\x08tfrecord\\""\\x1a\\n\\tBytesList\\x12\\r\\n\\x05value\\x18\\x01 \\x03(\\x0c\\""\\x1e\\n\\tFloatList\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x02\\x42\\x02\\x10\\x01\\""\\x1e\\n\\tInt64List\\x12\\x11\\n\\x05value\\x18\\x01 \\x03(\\x03\\x42\\x02\\x10\\x01\\""\\x92\\x01\\n\\x07\\x46\\x65\\x61ture\\x12)\\n\\nbytes_list\\x18\\x01 \\x01(\\x0b\\x32\\x13.tfrecord.BytesListH\\x00\\x12)\\n\\nfloat_list\\x18\\x02 \\x01(\\x0b\\x32\\x13.tfrecord.FloatListH\\x00\\x12)\\n\\nint64_list\\x18\\x03 \\x01(\\x0b\\x32\\x13.tfrecord.Int64ListH\\x00\\x42\\x06\\n\\x04kind\\""\\x7f\\n\\x08\\x46\\x65\\x61tures\\x12\\x30\\n\\x07\\x66\\x65\\x61ture\\x18\\x01 \\x03(\\x0b\\x32\\x1f.tfrecord.Features.FeatureEntry\\x1a\\x41\\n\\x0c\\x46\\x65\\x61tureEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12 \\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x11.tfrecord.Feature:\\x02\\x38\\x01\\""1\\n\\x0b\\x46\\x65\\x61tureList\\x12\\""\\n\\x07\\x66\\x65\\x61ture\\x18\\x01 \\x03(\\x0b\\x32\\x11.tfrecord.Feature\\""\\x98\\x01\\n\\x0c\\x46\\x65\\x61tureLists\\x12=\\n\\x0c\\x66\\x65\\x61ture_list\\x18\\x01 \\x03(\\x0b\\x32\\\'.tfrecord.FeatureLists.FeatureListEntry\\x1aI\\n\\x10\\x46\\x65\\x61tureListEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12$\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x15.tfrecord.FeatureList:\\x02\\x38\\x01\\""/\\n\\x07\\x45xample\\x12$\\n\\x08\\x66\\x65\\x61tures\\x18\\x01 \\x01(\\x0b\\x32\\x12.tfrecord.Features\\""e\\n\\x0fSequenceExample\\x12#\\n\\x07\\x63ontext\\x18\\x01 \\x01(\\x0b\\x32\\x12.tfrecord.Features\\x12-\\n\\rfeature_lists\\x18\\x02 \\x01(\\x0b\\x32\\x16.tfrecord.FeatureListsB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_BYTESLIST = _descriptor.Descriptor(\n  name=\'BytesList\',\n  full_name=\'tfrecord.BytesList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tfrecord.BytesList.value\', index=0,\n      number=1, type=12, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=27,\n  serialized_end=53,\n)\n\n\n_FLOATLIST = _descriptor.Descriptor(\n  name=\'FloatList\',\n  full_name=\'tfrecord.FloatList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tfrecord.FloatList.value\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=55,\n  serialized_end=85,\n)\n\n\n_INT64LIST = _descriptor.Descriptor(\n  name=\'Int64List\',\n  full_name=\'tfrecord.Int64List\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tfrecord.Int64List.value\', index=0,\n      number=1, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=87,\n  serialized_end=117,\n)\n\n\n_FEATURE = _descriptor.Descriptor(\n  name=\'Feature\',\n  full_name=\'tfrecord.Feature\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'bytes_list\', full_name=\'tfrecord.Feature.bytes_list\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'float_list\', full_name=\'tfrecord.Feature.float_list\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'int64_list\', full_name=\'tfrecord.Feature.int64_list\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'kind\', full_name=\'tfrecord.Feature.kind\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=120,\n  serialized_end=266,\n)\n\n\n_FEATURES_FEATUREENTRY = _descriptor.Descriptor(\n  name=\'FeatureEntry\',\n  full_name=\'tfrecord.Features.FeatureEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tfrecord.Features.FeatureEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tfrecord.Features.FeatureEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=330,\n  serialized_end=395,\n)\n\n_FEATURES = _descriptor.Descriptor(\n  name=\'Features\',\n  full_name=\'tfrecord.Features\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'feature\', full_name=\'tfrecord.Features.feature\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FEATURES_FEATUREENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=268,\n  serialized_end=395,\n)\n\n\n_FEATURELIST = _descriptor.Descriptor(\n  name=\'FeatureList\',\n  full_name=\'tfrecord.FeatureList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'feature\', full_name=\'tfrecord.FeatureList.feature\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=397,\n  serialized_end=446,\n)\n\n\n_FEATURELISTS_FEATURELISTENTRY = _descriptor.Descriptor(\n  name=\'FeatureListEntry\',\n  full_name=\'tfrecord.FeatureLists.FeatureListEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tfrecord.FeatureLists.FeatureListEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tfrecord.FeatureLists.FeatureListEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=528,\n  serialized_end=601,\n)\n\n_FEATURELISTS = _descriptor.Descriptor(\n  name=\'FeatureLists\',\n  full_name=\'tfrecord.FeatureLists\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'feature_list\', full_name=\'tfrecord.FeatureLists.feature_list\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FEATURELISTS_FEATURELISTENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=449,\n  serialized_end=601,\n)\n\n\n_EXAMPLE = _descriptor.Descriptor(\n  name=\'Example\',\n  full_name=\'tfrecord.Example\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'features\', full_name=\'tfrecord.Example.features\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=603,\n  serialized_end=650,\n)\n\n\n_SEQUENCEEXAMPLE = _descriptor.Descriptor(\n  name=\'SequenceExample\',\n  full_name=\'tfrecord.SequenceExample\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'context\', full_name=\'tfrecord.SequenceExample.context\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'feature_lists\', full_name=\'tfrecord.SequenceExample.feature_lists\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=652,\n  serialized_end=753,\n)\n\n_FEATURE.fields_by_name[\'bytes_list\'].message_type = _BYTESLIST\n_FEATURE.fields_by_name[\'float_list\'].message_type = _FLOATLIST\n_FEATURE.fields_by_name[\'int64_list\'].message_type = _INT64LIST\n_FEATURE.oneofs_by_name[\'kind\'].fields.append(\n  _FEATURE.fields_by_name[\'bytes_list\'])\n_FEATURE.fields_by_name[\'bytes_list\'].containing_oneof = _FEATURE.oneofs_by_name[\'kind\']\n_FEATURE.oneofs_by_name[\'kind\'].fields.append(\n  _FEATURE.fields_by_name[\'float_list\'])\n_FEATURE.fields_by_name[\'float_list\'].containing_oneof = _FEATURE.oneofs_by_name[\'kind\']\n_FEATURE.oneofs_by_name[\'kind\'].fields.append(\n  _FEATURE.fields_by_name[\'int64_list\'])\n_FEATURE.fields_by_name[\'int64_list\'].containing_oneof = _FEATURE.oneofs_by_name[\'kind\']\n_FEATURES_FEATUREENTRY.fields_by_name[\'value\'].message_type = _FEATURE\n_FEATURES_FEATUREENTRY.containing_type = _FEATURES\n_FEATURES.fields_by_name[\'feature\'].message_type = _FEATURES_FEATUREENTRY\n_FEATURELIST.fields_by_name[\'feature\'].message_type = _FEATURE\n_FEATURELISTS_FEATURELISTENTRY.fields_by_name[\'value\'].message_type = _FEATURELIST\n_FEATURELISTS_FEATURELISTENTRY.containing_type = _FEATURELISTS\n_FEATURELISTS.fields_by_name[\'feature_list\'].message_type = _FEATURELISTS_FEATURELISTENTRY\n_EXAMPLE.fields_by_name[\'features\'].message_type = _FEATURES\n_SEQUENCEEXAMPLE.fields_by_name[\'context\'].message_type = _FEATURES\n_SEQUENCEEXAMPLE.fields_by_name[\'feature_lists\'].message_type = _FEATURELISTS\nDESCRIPTOR.message_types_by_name[\'BytesList\'] = _BYTESLIST\nDESCRIPTOR.message_types_by_name[\'FloatList\'] = _FLOATLIST\nDESCRIPTOR.message_types_by_name[\'Int64List\'] = _INT64LIST\nDESCRIPTOR.message_types_by_name[\'Feature\'] = _FEATURE\nDESCRIPTOR.message_types_by_name[\'Features\'] = _FEATURES\nDESCRIPTOR.message_types_by_name[\'FeatureList\'] = _FEATURELIST\nDESCRIPTOR.message_types_by_name[\'FeatureLists\'] = _FEATURELISTS\nDESCRIPTOR.message_types_by_name[\'Example\'] = _EXAMPLE\nDESCRIPTOR.message_types_by_name[\'SequenceExample\'] = _SEQUENCEEXAMPLE\n\nBytesList = _reflection.GeneratedProtocolMessageType(\'BytesList\', (_message.Message,), dict(\n  DESCRIPTOR = _BYTESLIST,\n  __module__ = \'example_pb2\'\n  # @@protoc_insertion_point(class_scope:tfrecord.BytesList)\n  ))\n_sym_db.RegisterMessage(BytesList)\n\nFloatList = _reflection.GeneratedProtocolMessageType(\'FloatList\', (_message.Message,), dict(\n  DESCRIPTOR = _FLOATLIST,\n  __module__ = \'example_pb2\'\n  # @@protoc_insertion_point(class_scope:tfrecord.FloatList)\n  ))\n_sym_db.RegisterMessage(FloatList)\n\nInt64List = _reflection.GeneratedProtocolMessageType(\'Int64List\', (_message.Message,), dict(\n  DESCRIPTOR = _INT64LIST,\n  __module__ = \'example_pb2\'\n  # @@protoc_insertion_point(class_scope:tfrecord.Int64List)\n  ))\n_sym_db.RegisterMessage(Int64List)\n\nFeature = _reflection.GeneratedProtocolMessageType(\'Feature\', (_message.Message,), dict(\n  DESCRIPTOR = _FEATURE,\n  __module__ = \'example_pb2\'\n  # @@protoc_insertion_point(class_scope:tfrecord.Feature)\n  ))\n_sym_db.RegisterMessage(Feature)\n\nFeatures = _reflection.GeneratedProtocolMessageType(\'Features\', (_message.Message,), dict(\n\n  FeatureEntry = _reflection.GeneratedProtocolMessageType(\'FeatureEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FEATURES_FEATUREENTRY,\n    __module__ = \'example_pb2\'\n    # @@protoc_insertion_point(class_scope:tfrecord.Features.FeatureEntry)\n    ))\n  ,\n  DESCRIPTOR = _FEATURES,\n  __module__ = \'example_pb2\'\n  # @@protoc_insertion_point(class_scope:tfrecord.Features)\n  ))\n_sym_db.RegisterMessage(Features)\n_sym_db.RegisterMessage(Features.FeatureEntry)\n\nFeatureList = _reflection.GeneratedProtocolMessageType(\'FeatureList\', (_message.Message,), dict(\n  DESCRIPTOR = _FEATURELIST,\n  __module__ = \'example_pb2\'\n  # @@protoc_insertion_point(class_scope:tfrecord.FeatureList)\n  ))\n_sym_db.RegisterMessage(FeatureList)\n\nFeatureLists = _reflection.GeneratedProtocolMessageType(\'FeatureLists\', (_message.Message,), dict(\n\n  FeatureListEntry = _reflection.GeneratedProtocolMessageType(\'FeatureListEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FEATURELISTS_FEATURELISTENTRY,\n    __module__ = \'example_pb2\'\n    # @@protoc_insertion_point(class_scope:tfrecord.FeatureLists.FeatureListEntry)\n    ))\n  ,\n  DESCRIPTOR = _FEATURELISTS,\n  __module__ = \'example_pb2\'\n  # @@protoc_insertion_point(class_scope:tfrecord.FeatureLists)\n  ))\n_sym_db.RegisterMessage(FeatureLists)\n_sym_db.RegisterMessage(FeatureLists.FeatureListEntry)\n\nExample = _reflection.GeneratedProtocolMessageType(\'Example\', (_message.Message,), dict(\n  DESCRIPTOR = _EXAMPLE,\n  __module__ = \'example_pb2\'\n  # @@protoc_insertion_point(class_scope:tfrecord.Example)\n  ))\n_sym_db.RegisterMessage(Example)\n\nSequenceExample = _reflection.GeneratedProtocolMessageType(\'SequenceExample\', (_message.Message,), dict(\n  DESCRIPTOR = _SEQUENCEEXAMPLE,\n  __module__ = \'example_pb2\'\n  # @@protoc_insertion_point(class_scope:tfrecord.SequenceExample)\n  ))\n_sym_db.RegisterMessage(SequenceExample)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n_FLOATLIST.fields_by_name[\'value\'].has_options = True\n_FLOATLIST.fields_by_name[\'value\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_INT64LIST.fields_by_name[\'value\'].has_options = True\n_INT64LIST.fields_by_name[\'value\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\020\\001\'))\n_FEATURES_FEATUREENTRY.has_options = True\n_FEATURES_FEATUREENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_FEATURELISTS_FEATURELISTENTRY.has_options = True\n_FEATURELISTS_FEATURELISTENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
tfrecord/iterator_utils.py,0,"b'""""""Iterator utils.""""""\n\nfrom __future__ import division\n\nimport typing\nimport warnings\n\nimport numpy as np\n\n\ndef cycle(iterator_fn: typing.Callable) -> typing.Iterable[typing.Any]:\n    """"""Create a repeating iterator from an iterator generator.""""""\n    while True:\n        for element in iterator_fn():\n            yield element\n\n\ndef sample_iterators(iterators: typing.List[typing.Iterator],\n                     ratios: typing.List[int]) -> typing.Iterable[typing.Any]:\n    """"""Retrieve info generated from the iterator(s) according to their\n    sampling ratios.\n\n    Params:\n    -------\n    iterators: list of iterators\n        All iterators (one for each file).\n\n    ratios: list of int\n        The ratios with which to sample each iterator.\n\n    Yields:\n    -------\n    item: Any\n        Decoded bytes of features into its respective data types from\n        an iterator (based off their sampling ratio).\n    """"""\n    iterators = [cycle(iterator) for iterator in iterators]\n    ratios = np.array(ratios)\n    ratios = ratios / ratios.sum()\n    while True:\n        choice = np.random.choice(len(ratios), p=ratios)\n        yield next(iterators[choice])\n\n\ndef shuffle_iterator(iterator: typing.Iterator,\n                     queue_size: int) -> typing.Iterable[typing.Any]:\n    """"""Shuffle elements contained in an iterator.\n\n    Params:\n    -------\n    iterator: iterator\n        The iterator.\n\n    queue_size: int\n        Length of buffer. Determines how many records are queued to\n        sample from.\n\n    Yields:\n    -------\n    item: Any\n        Decoded bytes of the features into its respective data type (for\n        an individual record) from an iterator.\n    """"""\n    buffer = []\n    try:\n        for _ in range(queue_size):\n            buffer.append(next(iterator))\n    except StopIteration:\n        warnings.warn(""Number of elements in the iterator is less than the ""\n                      f""queue size (N={queue_size})."")\n    while buffer:\n        index = np.random.randint(len(buffer))\n        try:\n            item = buffer[index]\n            buffer[index] = next(iterator)\n            yield item\n        except StopIteration:\n            yield buffer.pop(index)\n'"
tfrecord/reader.py,0,"b'""""""Reader utils.""""""\n\nimport functools\nimport io\nimport os\nimport struct\nimport typing\n\nimport numpy as np\n\nfrom tfrecord import example_pb2\nfrom tfrecord import iterator_utils\n\n\ndef tfrecord_iterator(data_path: str,\n                      index_path: typing.Optional[str] = None,\n                      shard: typing.Optional[typing.Tuple[int, int]] = None\n                      ) -> typing.Iterable[memoryview]:\n    """"""Create an iterator over the tfrecord dataset.\n\n    Since the tfrecords file stores each example as bytes, we can\n    define an iterator over `datum_bytes_view`, which is a memoryview\n    object referencing the bytes.\n\n    Params:\n    -------\n    data_path: str\n        TFRecord file path.\n\n    index_path: str, optional, default=None\n        Index file path. Can be set to None if no file is available.\n\n    shard: tuple of ints, optional, default=None\n        A tuple (index, count) representing worker_id and num_workers\n        count. Necessary to evenly split/shard the dataset among many\n        workers (i.e. >1).\n\n    Yields:\n    -------\n    datum_bytes_view: memoryview\n        Object referencing the specified `datum_bytes` contained in the\n        file (for a single record).\n    """"""\n    file = io.open(data_path, ""rb"")\n\n    length_bytes = bytearray(8)\n    crc_bytes = bytearray(4)\n    datum_bytes = bytearray(1024 * 1024)\n\n    def read_records(start_offset=None, end_offset=None):\n        nonlocal length_bytes, crc_bytes, datum_bytes\n\n        if start_offset is not None:\n            file.seek(start_offset)\n        if end_offset is None:\n            end_offset = os.path.getsize(data_path)\n        while file.tell() < end_offset:\n            if file.readinto(length_bytes) != 8:\n                raise RuntimeError(""Failed to read the record size."")\n            if file.readinto(crc_bytes) != 4:\n                raise RuntimeError(""Failed to read the start token."")\n            length, = struct.unpack(""<Q"", length_bytes)\n            if length > len(datum_bytes):\n                datum_bytes = datum_bytes.zfill(int(length * 1.5))\n            datum_bytes_view = memoryview(datum_bytes)[:length]\n            if file.readinto(datum_bytes_view) != length:\n                raise RuntimeError(""Failed to read the record."")\n            if file.readinto(crc_bytes) != 4:\n                raise RuntimeError(""Failed to read the end token."")\n            yield datum_bytes_view\n\n    if index_path is None:\n        yield from read_records()\n    else:\n        index = np.loadtxt(index_path, dtype=np.int64)[:, 0]\n        if shard is None:\n            offset = np.random.choice(index)\n            yield from read_records(offset)\n            yield from read_records(0, offset)\n        else:\n            num_records = len(index)\n            shard_idx, shard_count = shard\n            start_index = (num_records * shard_idx) // shard_count\n            end_index = (num_records * (shard_idx + 1)) // shard_count\n            start_byte = index[start_index]\n            end_byte = index[end_index] if end_index < num_records else None\n            yield from read_records(start_byte, end_byte)\n\n    file.close()\n\n\ndef tfrecord_loader(data_path: str,\n                    index_path: typing.Union[str, None],\n                    description: typing.Union[typing.List[str], typing.Dict[str, str], None] = None,\n                    shard: typing.Optional[typing.Tuple[int, int]] = None,\n                    ) -> typing.Iterable[typing.Dict[str, np.ndarray]]:\n    """"""Create an iterator over the (decoded) examples contained within\n    the dataset.\n\n    Decodes raw bytes of the features (contained within the dataset)\n    into its respective format.\n\n    Params:\n    -------\n    data_path: str\n        TFRecord file path.\n\n    index_path: str or None\n        Index file path. Can be set to None if no file is available.\n\n    description: list or dict of str, optional, default=None\n        List of keys or dict of (key, value) pairs to extract from each\n        record. The keys represent the name of the features and the\n        values (""byte"", ""float"", or ""int"") correspond to the data type.\n        If dtypes are provided, then they are verified against the\n        inferred type for compatibility purposes. If None (default),\n        then all features contained in the file are extracted.\n\n    shard: tuple of ints, optional, default=None\n        A tuple (index, count) representing worker_id and num_workers\n        count. Necessary to evenly split/shard the dataset among many\n        workers (i.e. >1).\n\n    Yields:\n    -------\n    features: dict of {str, np.ndarray}\n        Decoded bytes of the features into its respective data type (for\n        an individual record).\n    """"""\n\n    typename_mapping = {\n        ""byte"": ""bytes_list"",\n        ""float"": ""float_list"",\n        ""int"": ""int64_list""\n    }\n\n    record_iterator = tfrecord_iterator(data_path, index_path, shard)\n\n    for record in record_iterator:\n        example = example_pb2.Example()\n        example.ParseFromString(record)\n\n        all_keys = list(example.features.feature.keys())\n        if description is None:\n            description = dict.fromkeys(all_keys, None)\n        elif isinstance(description, list):\n            description = dict.fromkeys(description, None)\n\n        features = {}\n        for key, typename in description.items():\n            if key not in all_keys:\n                raise KeyError(f""Key {key} doesn\'t exist (select from {all_keys})!"")\n            # NOTE: We assume that each key in the example has only one field\n            # (either ""bytes_list"", ""float_list"", or ""int64_list"")!\n            field = example.features.feature[key].ListFields()[0]\n            inferred_typename, value = field[0].name, field[1].value\n            if typename is not None:\n                tf_typename = typename_mapping[typename]\n                if tf_typename != inferred_typename:\n                    reversed_mapping = {v: k for k, v in typename_mapping.items()}\n                    raise TypeError(f""Incompatible type \'{typename}\' for `{key}` ""\n                                    f""(should be \'{reversed_mapping[inferred_typename]}\')."")\n\n            # Decode raw bytes into respective data types\n            if inferred_typename == ""bytes_list"":\n                value = np.frombuffer(value[0], dtype=np.uint8)\n            elif inferred_typename == ""float_list"":\n                value = np.array(value, dtype=np.float32)\n            elif inferred_typename == ""int64_list"":\n                value = np.array(value, dtype=np.int32)\n            features[key] = value\n\n        yield features\n\n\ndef multi_tfrecord_loader(data_pattern: str,\n                          index_pattern: typing.Union[str, None],\n                          splits: typing.Dict[str, float],\n                          description: typing.Union[typing.List[str], typing.Dict[str, str], None] = None,\n                          ) -> typing.Iterable[typing.Dict[str, np.ndarray]]:\n    """"""Create an iterator by reading and merging multiple tfrecord datasets.\n\n    NOTE: Sharding is currently unavailable for the multi tfrecord loader.\n\n    Params:\n    -------\n    data_pattern: str\n        Input data path pattern.\n\n    index_pattern: str or None\n        Input index path pattern.\n\n    splits: dict\n        Dictionary of (key, value) pairs, where the key is used to\n        construct the data and index path(s) and the value determines\n        the contribution of each split to the batch.\n\n    description: list or dict of str, optional, default=None\n        List of keys or dict of (key, value) pairs to extract from each\n        record. The keys represent the name of the features and the\n        values (""byte"", ""float"", or ""int"") correspond to the data type.\n        If dtypes are provided, then they are verified against the\n        inferred type for compatibility purposes. If None (default),\n        then all features contained in the file are extracted.\n\n    Returns:\n    --------\n    it: iterator\n        A repeating iterator that generates batches of data.\n    """"""\n    loaders = [functools.partial(tfrecord_loader, data_path=data_pattern.format(split),\n                                 index_path=index_pattern.format(split) \\\n                                     if index_pattern is not None else None,\n                                 description=description)\n               for split in splits.keys()]\n    return iterator_utils.sample_iterators(loaders, list(splits.values()))\n'"
tfrecord/writer.py,0,"b'""""""Writer utils.""""""\n\nimport io\nimport struct\nimport typing\n\nimport numpy as np\ntry:\n    import crc32c\nexcept ImportError:\n    crc32c = None\n\nfrom tfrecord import example_pb2\n\n\nclass TFRecordWriter:\n    """"""Opens a TFRecord file for writing.\n\n    Params:\n    -------\n    data_path: str\n        Path to the tfrecord file.\n    """"""\n\n    def __init__(self, data_path: str) -> None:\n        self.file = io.open(data_path, ""wb"")\n\n    def close(self) -> None:\n        """"""Close the tfrecord file.""""""\n        self.file.close()\n\n    def write(self, datum: typing.Dict[str, typing.Tuple[typing.Any, str]]) -> None:\n        """"""Write an example into tfrecord file.\n\n        Params:\n        -------\n        datum: dict\n            Dictionary of tuples of form (value, dtype). dtype can be\n            ""byte"", ""float"" or ""int"".\n        """"""\n        record = TFRecordWriter.serialize_tf_example(datum)\n        length = len(record)\n        length_bytes = struct.pack(""<Q"", length)\n        self.file.write(length_bytes)\n        self.file.write(TFRecordWriter.masked_crc(length_bytes))\n        self.file.write(record)\n        self.file.write(TFRecordWriter.masked_crc(record))\n\n    @staticmethod\n    def masked_crc(data: bytes) -> bytes:\n        """"""CRC checksum.""""""\n        mask = 0xa282ead8\n        crc = crc32c.crc32(data)\n        masked = ((crc >> 15) | (crc << 17)) + mask\n        masked = np.uint32(masked)\n        masked_bytes = struct.pack(""<I"", masked)\n        return masked_bytes\n\n    @staticmethod\n    def serialize_tf_example(datum: typing.Dict[str, typing.Tuple[typing.Any, str]]) -> bytes:\n        """"""Serialize example into tfrecord.Example proto.\n\n        Params:\n        -------\n        datum: dict\n            Dictionary of tuples of form (value, dtype). dtype can be\n            ""byte"", ""float"" or ""int"".\n\n        Returns:\n        --------\n        proto: bytes\n            Serialized tfrecord.example to bytes.\n        """"""\n        feature_map = {\n            ""byte"": lambda f: example_pb2.Feature(\n                bytes_list=example_pb2.BytesList(value=f)),\n            ""float"": lambda f: example_pb2.Feature(\n                float_list=example_pb2.FloatList(value=f)),\n            ""int"": lambda f: example_pb2.Feature(\n                int64_list=example_pb2.Int64List(value=f))\n        }\n\n        def serialize(value, dtype):\n            if not isinstance(value, (list, tuple, np.ndarray)):\n                value = [value]\n            return feature_map[dtype](value)\n\n        features = {key: serialize(value, dtype) for key, (value, dtype) in datum.items()}\n        example_proto = example_pb2.Example(features=example_pb2.Features(feature=features))\n        return example_proto.SerializeToString()\n'"
tfrecord/tools/__init__.py,0,b'from tfrecord.tools import tfrecord2idx\n\nfrom tfrecord.tools.tfrecord2idx import create_index\n'
tfrecord/tools/tfrecord2idx.py,0,"b'from __future__ import print_function\n\nimport sys\nimport struct\n\n\ndef create_index(tfrecord_file: str, index_file: str) -> None:\n    """"""Create index from the tfrecords file.\n\n    Stores starting location (byte) and length (in bytes) of each\n    serialized record.\n\n    Params:\n    -------\n    tfrecord_file: str\n        Path to the TFRecord file.\n\n    index_file: str\n        Path where to store the index file.\n    """"""\n    infile = open(tfrecord_file, ""rb"")\n    outfile = open(index_file, ""w"")\n\n    while True:\n        current = infile.tell()\n        try:\n            byte_len = infile.read(8)\n            if len(byte_len) == 0:\n                break\n            infile.read(4)\n            proto_len = struct.unpack(""q"", byte_len)[0]\n            infile.read(proto_len)\n            infile.read(4)\n            outfile.write(str(current) + "" "" + str(infile.tell() - current) + ""\\n"")\n        except:\n            print(""Failed to parse TFRecord."")\n            break\n    infile.close()\n    outfile.close()\n\n\ndef main():\n    if len(sys.argv) < 3:\n        print(""Usage: tfrecord2idx <tfrecord path> <index path>"")\n        sys.exit()\n\n    create_index(sys.argv[1], sys.argv[2])\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tfrecord/torch/__init__.py,2,b'from tfrecord.torch import dataset\n\nfrom tfrecord.torch.dataset import TFRecordDataset\nfrom tfrecord.torch.dataset import MultiTFRecordDataset\n'
tfrecord/torch/dataset.py,5,"b'""""""Load tfrecord files into torch datasets.""""""\n\nimport typing\nimport numpy as np\n\nimport torch.utils.data\n\nfrom tfrecord import reader\nfrom tfrecord import iterator_utils\n\n\nclass TFRecordDataset(torch.utils.data.IterableDataset):\n    """"""Parse (generic) TFRecords dataset into `IterableDataset` object,\n    which contain `np.ndarrays`s.\n\n    Params:\n    -------\n    data_path: str\n        The path to the tfrecords file.\n\n    index_path: str or None\n        The path to the index file.\n\n    description: list or dict of str, optional, default=None\n        List of keys or dict of (key, value) pairs to extract from each\n        record. The keys represent the name of the features and the\n        values (""byte"", ""float"", or ""int"") correspond to the data type.\n        If dtypes are provided, then they are verified against the\n        inferred type for compatibility purposes. If None (default),\n        then all features contained in the file are extracted.\n\n    shuffle_queue_size: int, optional, default=None\n        Length of buffer. Determines how many records are queued to\n        sample from.\n\n    transform : a callable, default = None\n        A function that takes in the input `features` i.e the dict\n        provided in the description, transforms it and returns a\n        desirable output.\n\n    """"""\n\n    def __init__(self,\n                 data_path: str,\n                 index_path: typing.Union[str, None],\n                 description: typing.Union[typing.List[str], typing.Dict[str, str], None] = None,\n                 shuffle_queue_size: typing.Optional[int] = None,\n                 transform: typing.Callable[[dict], typing.Any] = None\n                 ) -> None:\n        super(TFRecordDataset, self).__init__()\n        self.data_path = data_path\n        self.index_path = index_path\n        self.description = description\n        self.shuffle_queue_size = shuffle_queue_size\n        self.transform = transform or (lambda x: x)\n\n    def __iter__(self):\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is not None:\n            shard = worker_info.id, worker_info.num_workers\n            np.random.seed(worker_info.seed % np.iinfo(np.uint32).max)\n        else:\n            shard = None\n        it = reader.tfrecord_loader(\n            self.data_path, self.index_path, self.description, shard)\n        if self.shuffle_queue_size:\n            it = iterator_utils.shuffle_iterator(it, self.shuffle_queue_size)\n        if self.transform:\n            it = map(self.transform, it)\n        return it\n\n\nclass MultiTFRecordDataset(torch.utils.data.IterableDataset):\n    """"""Parse multiple (generic) TFRecords datasets into an `IterableDataset`\n    object, which contain `np.ndarrays`s.\n\n    Params:\n    -------\n    data_pattern: str\n        Input data path pattern.\n\n    index_pattern: str or None\n        Input index path pattern.\n\n    splits: dict\n        Dictionary of (key, value) pairs, where the key is used to\n        construct the data and index path(s) and the value determines\n        the contribution of each split to the batch.\n\n    description: list or dict of str, optional, default=None\n        List of keys or dict of (key, value) pairs to extract from each\n        record. The keys represent the name of the features and the\n        values (""byte"", ""float"", or ""int"") correspond to the data type.\n        If dtypes are provided, then they are verified against the\n        inferred type for compatibility purposes. If None (default),\n        then all features contained in the file are extracted.\n\n    shuffle_queue_size: int, optional, default=None\n        Length of buffer. Determines how many records are queued to\n        sample from.\n\n    transform : a callable, default = None\n        A function that takes in the input `features` i.e the dict\n        provided in the description, transforms it and returns a\n        desirable output.\n\n    """"""\n\n    def __init__(self,\n                 data_pattern: str,\n                 index_pattern: typing.Union[str, None],\n                 splits: typing.Dict[str, float],\n                 description: typing.Union[typing.List[str], typing.Dict[str, str], None] = None,\n                 shuffle_queue_size: typing.Optional[int] = None,\n                 transform: typing.Callable[[dict], typing.Any] = None) -> None:\n        super(MultiTFRecordDataset, self).__init__()\n        self.data_pattern = data_pattern\n        self.index_pattern = index_pattern\n        self.splits = splits\n        self.description = description\n        self.shuffle_queue_size = shuffle_queue_size\n        self.transform = transform\n\n    def __iter__(self):\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is not None:\n            np.random.seed(worker_info.seed % np.iinfo(np.uint32).max)\n        it = reader.multi_tfrecord_loader(\n            self.data_pattern, self.index_pattern, self.splits, self.description)\n        if self.shuffle_queue_size:\n            it = iterator_utils.shuffle_iterator(it, self.shuffle_queue_size)\n        if self.transform:\n            it = map(self.transform, it)\n        return it\n'"
