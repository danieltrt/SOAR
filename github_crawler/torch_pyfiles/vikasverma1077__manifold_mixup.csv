file_path,api_count,code
gan/GAN.py,16,"b'import utils, torch, time, os, pickle\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable, grad\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nclass generator(nn.Module):\n    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n    def __init__(self, dataset = \'mnist\'):\n        super(generator, self).__init__()\n        if dataset == \'mnist\' or dataset == \'fashion-mnist\':\n            self.input_height = 28\n            self.input_width = 28\n            self.input_dim = 62\n            self.output_dim = 1\n        elif dataset == \'celebA\':\n            self.input_height = 64\n            self.input_width = 64\n            self.input_dim = 62\n            self.output_dim = 3\n\n        self.fc = nn.Sequential(\n            nn.Linear(self.input_dim, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Linear(1024, 128 * (self.input_height // 4) * (self.input_width // 4)),\n            nn.BatchNorm1d(128 * (self.input_height // 4) * (self.input_width // 4)),\n            nn.ReLU(),\n        )\n        self.deconv = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n            nn.Sigmoid(),\n        )\n        utils.initialize_weights(self)\n\n    def forward(self, input):\n        x = self.fc(input)\n        x = x.view(-1, 128, (self.input_height // 4), (self.input_width // 4))\n        x = self.deconv(x)\n\n        return x\n\nclass discriminator(nn.Module):\n    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n    def __init__(self, dataset = \'mnist\'):\n        super(discriminator, self).__init__()\n        if dataset == \'mnist\' or dataset == \'fashion-mnist\':\n            self.input_height = 28\n            self.input_width = 28\n            self.input_dim = 1\n            self.output_dim = 1\n        elif dataset == \'celebA\':\n            self.input_height = 64\n            self.input_width = 64\n            self.input_dim = 3\n            self.output_dim = 1\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(128 * (self.input_height // 4) * (self.input_width // 4), 1024),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(0.2),\n            nn.Linear(1024, self.output_dim),\n            nn.Sigmoid(),\n        )\n        utils.initialize_weights(self)\n\n    def forward(self, input):\n        x = self.conv(input)\n        x = x.view(-1, 128 * (self.input_height // 4) * (self.input_width // 4))\n        x = self.fc(x)\n\n        return x\n\nclass GAN(object):\n    def __init__(self, args):\n        # parameters\n        self.epoch = args.epoch\n        self.sample_num = 16\n        self.batch_size = args.batch_size\n        self.save_dir = args.save_dir\n        self.result_dir = args.result_dir\n        self.dataset = args.dataset\n        self.log_dir = args.log_dir\n        self.gpu_mode = args.gpu_mode\n        self.model_name = args.gan_type\n\n        # networks init\n        self.G = generator(self.dataset)\n        self.D = discriminator(self.dataset)\n        self.G_optimizer = optim.Adam(self.G.parameters(), lr=args.lrG, betas=(args.beta1, args.beta2))\n        self.D_optimizer = optim.Adam(self.D.parameters(), lr=args.lrD, betas=(args.beta1, args.beta2))\n\n        if self.gpu_mode:\n            self.G.cuda()\n            self.D.cuda()\n            self.BCE_loss = nn.BCELoss().cuda()\n        else:\n            self.BCE_loss = nn.BCELoss()\n\n        print(\'---------- Networks architecture -------------\')\n        utils.print_network(self.G)\n        utils.print_network(self.D)\n        print(\'-----------------------------------------------\')\n\n        # load dataset\n        if self.dataset == \'mnist\':\n            self.data_loader = DataLoader(datasets.MNIST(\'data/mnist\', train=True, download=True,\n                                                                          transform=transforms.Compose(\n                                                                              [transforms.ToTensor()])),\n                                                           batch_size=self.batch_size, shuffle=True)\n        elif self.dataset == \'fashion-mnist\':\n            self.data_loader = DataLoader(\n                datasets.FashionMNIST(\'data/fashion-mnist\', train=True, download=True, transform=transforms.Compose(\n                    [transforms.ToTensor()])),\n                batch_size=self.batch_size, shuffle=True)\n        elif self.dataset == \'celebA\':\n            self.data_loader = utils.load_celebA(\'data/celebA\', transform=transforms.Compose(\n                [transforms.CenterCrop(160), transforms.Scale(64), transforms.ToTensor()]), batch_size=self.batch_size,\n                                                 shuffle=True)\n        self.z_dim = 62\n\n        # fixed noise\n        if self.gpu_mode:\n            self.sample_z_ = Variable(torch.rand((self.batch_size, self.z_dim)).cuda(), volatile=True)\n        else:\n            self.sample_z_ = Variable(torch.rand((self.batch_size, self.z_dim)), volatile=True)\n\n    def train(self):\n        self.train_hist = {}\n        self.train_hist[\'D_loss\'] = []\n        self.train_hist[\'G_loss\'] = []\n        self.train_hist[\'per_epoch_time\'] = []\n        self.train_hist[\'total_time\'] = []\n        self.train_hist[\'D_norm\'] = []\n\n        f = open(""%s/results.txt"" % self.log_dir, ""w"")\n        f.write(""d_loss,g_loss,d_norm\\n"")\n    \n        if self.gpu_mode:\n            self.y_real_, self.y_fake_ = Variable(torch.ones(self.batch_size, 1).cuda()), Variable(torch.zeros(self.batch_size, 1).cuda())\n        else:\n            self.y_real_, self.y_fake_ = Variable(torch.ones(self.batch_size, 1)), Variable(torch.zeros(self.batch_size, 1))\n\n        #for iter, ((x1_,_), (x2_,_)) in enumerate(zip(self.data_loader, self.data_loader)):\n        #    import pdb\n        #    pdb.set_trace()\n\n        self.D.train()\n        print(\'training start!!\')\n        start_time = time.time()\n        for epoch in range(self.epoch):\n            self.G.train()\n            epoch_start_time = time.time()\n            for iter, (x_, _) in enumerate(self.data_loader):\n                \n                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n                    break\n\n                z_ = torch.rand((self.batch_size, self.z_dim))\n\n                if self.gpu_mode:\n                    x_, z_ = Variable(x_.cuda(), requires_grad=True), \\\n                            Variable(z_.cuda())\n                else:\n                    x_, z_ = Variable(x_, requires_grad=True), \\\n                            Variable(z_)\n\n                # update D network\n\n                D_real = self.D(x_)\n                # compute gradient penalty\n                grad_wrt_x = grad(outputs=D_real, inputs=x_,\n                                 grad_outputs=torch.ones(D_real.size()).cuda(),\n                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n                g_norm  = ((grad_wrt_x.view(grad_wrt_x.size()[0], -1).norm(2, 1) - 1) ** 2).mean()\n                self.train_hist[\'D_norm\'].append(g_norm.data.item())\n\n                self.D_optimizer.zero_grad()\n\n                G_ = self.G(z_).detach()\n                alpha = float(np.random.random())\n                Xz = Variable(alpha*x_.data + (1.-alpha)*G_.data)\n                D_Xz = self.D(Xz)\n                D_loss = self.BCE_loss(D_Xz, alpha*self.y_real_)\n                \n                self.train_hist[\'D_loss\'].append(D_loss.data.item())\n\n                D_loss.backward()\n                self.D_optimizer.step()\n\n                # update G network\n                self.G_optimizer.zero_grad()\n\n                G_ = self.G(z_)\n                D_fake = self.D(G_)\n                G_loss = self.BCE_loss(D_fake, self.y_real_)\n                self.train_hist[\'G_loss\'].append(G_loss.data.item())\n\n                G_loss.backward()\n                self.G_optimizer.step()\n\n                if ((iter + 1) % 100) == 0:\n                    print(""Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f, D_norm: %.8f"" %\n                          ((epoch + 1),\n                           (iter + 1),\n                           self.data_loader.dataset.__len__() // self.batch_size,\n                           D_loss.data.item(),\n                           G_loss.data.item(),\n                           g_norm.data.item()))\n                    f.write(""%.8f,%.8f,%.8f\\n"" % (D_loss.data.item(), G_loss.data.item(), g_norm.data.item()))\n                    f.flush()\n\n            self.train_hist[\'per_epoch_time\'].append(time.time() - epoch_start_time)\n            self.visualize_results((epoch+1))\n\n        self.train_hist[\'total_time\'].append(time.time() - start_time)\n        print(""Avg one epoch time: %.2f, total %d epochs time: %.2f"" % (np.mean(self.train_hist[\'per_epoch_time\']),\n              self.epoch, self.train_hist[\'total_time\'][0]))\n        print(""Training finish!... save training results"")\n\n        f.close()\n\n        self.save()\n        utils.generate_animation(self.result_dir + \'/\' + self.dataset + \'/\' + self.model_name + \'/\' + self.model_name,\n                                 self.epoch)\n        utils.loss_plot(self.train_hist, os.path.join(self.save_dir, self.dataset, self.model_name), self.model_name)\n\n    def visualize_results(self, epoch, fix=True):\n        self.G.eval()\n\n        if not os.path.exists(self.result_dir + \'/\' + self.dataset + \'/\' + self.model_name):\n            os.makedirs(self.result_dir + \'/\' + self.dataset + \'/\' + self.model_name)\n\n        tot_num_samples = min(self.sample_num, self.batch_size)\n        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))\n\n        if fix:\n            """""" fixed noise """"""\n            samples = self.G(self.sample_z_)\n        else:\n            """""" random noise """"""\n            if self.gpu_mode:\n                sample_z_ = Variable(torch.rand((self.batch_size, self.z_dim)).cuda(), volatile=True)\n            else:\n                sample_z_ = Variable(torch.rand((self.batch_size, self.z_dim)), volatile=True)\n\n            samples = self.G(sample_z_)\n\n        if self.gpu_mode:\n            samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n        else:\n            samples = samples.data.numpy().transpose(0, 2, 3, 1)\n\n        utils.save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n                          self.result_dir + \'/\' + self.dataset + \'/\' + self.model_name + \'/\' + self.model_name + \'_epoch%03d\' % epoch + \'.png\')\n\n    def save(self):\n        save_dir = os.path.join(self.save_dir, self.dataset, self.model_name)\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n        torch.save(self.G.state_dict(), os.path.join(save_dir, self.model_name + \'_G.pkl\'))\n        torch.save(self.D.state_dict(), os.path.join(save_dir, self.model_name + \'_D.pkl\'))\n\n        with open(os.path.join(save_dir, self.model_name + \'_history.pkl\'), \'wb\') as f:\n            pickle.dump(self.train_hist, f)\n\n    def load(self):\n        save_dir = os.path.join(self.save_dir, self.dataset, self.model_name)\n\n        self.G.load_state_dict(torch.load(os.path.join(save_dir, self.model_name + \'_G.pkl\')))\n        self.D.load_state_dict(torch.load(os.path.join(save_dir, self.model_name + \'_D.pkl\')))\n'"
gan/fid_score.py,3,"b'#!/usr/bin/env python3\n""""""Calculates the Frechet Inception Distance (FID) to evalulate GANs\n\nThe FID metric calculates the distance between two distributions of images.\nTypically, we have summary statistics (mean & covariance matrix) of one\nof these distributions, while the 2nd distribution is given by a GAN.\n\nWhen run as a stand-alone program, it compares the distribution of\nimages that are stored as PNG/JPEG at a specified location with a\ndistribution given by summary statistics (in pickle format).\n\nThe FID is calculated by assuming that X_1 and X_2 are the activations of\nthe pool_3 layer of the inception net for generated samples and real world\nsamples respectivly.\n\nSee --help to see further details.\n\nCode apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\nof Tensorflow\n\nCopyright 2018 Institute of Bioinformatics, JKU Linz\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nimport pathlib\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\nimport torch\nimport numpy as np\nfrom scipy.misc import imread\nfrom scipy import linalg\nfrom torch.autograd import Variable\nfrom torch.nn.functional import adaptive_avg_pool2d\n\nfrom inception import InceptionV3\n\n\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'path\', type=str, nargs=2,\n                    help=(\'Path to the generated images or \'\n                          \'to .npz statistic files\'))\nparser.add_argument(\'--batch-size\', type=int, default=64,\n                    help=\'Batch size to use\')\nparser.add_argument(\'--dims\', type=int, default=2048,\n                    choices=list(InceptionV3.BLOCK_INDEX_BY_DIM),\n                    help=(\'Dimensionality of Inception features to use. \'\n                          \'By default, uses pool3 features\'))\nparser.add_argument(\'-c\', \'--gpu\', default=\'\', type=str,\n                    help=\'GPU to use (leave blank for CPU only)\')\n\n\ndef get_activations(images, model, batch_size=64, dims=2048,\n                    cuda=False, verbose=False):\n    """"""Calculates the activations of the pool_3 layer for all images.\n\n    Params:\n    -- images      : Numpy array of dimension (n_images, 3, hi, wi). The values\n                     must lie between 0 and 1.\n    -- model       : Instance of inception model\n    -- batch_size  : the images numpy array is split into batches with\n                     batch size batch_size. A reasonable batch size depends\n                     on the hardware.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the number\n                     of calculated batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, dims) that contains the\n       activations of the given tensor when feeding inception with the\n       query tensor.\n    """"""\n    model.eval()\n\n    d0 = images.shape[0]\n    if batch_size > d0:\n        print((\'Warning: batch size is bigger than the data size. \'\n               \'Setting batch size to data size\'))\n        batch_size = d0\n\n    n_batches = d0 // batch_size\n    n_used_imgs = n_batches * batch_size\n\n    pred_arr = np.empty((n_used_imgs, dims))\n    for i in range(n_batches):\n        if verbose:\n            print(\'\\rPropagating batch %d/%d\' % (i + 1, n_batches),\n                  end=\'\', flush=True)\n        start = i * batch_size\n        end = start + batch_size\n\n        batch = torch.from_numpy(images[start:end]).type(torch.FloatTensor)\n        batch = Variable(batch, volatile=True)\n        if cuda:\n            batch = batch.cuda()\n\n        pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n        if pred.shape[2] != 1 or pred.shape[3] != 1:\n            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n        pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)\n\n    if verbose:\n        print(\' done\')\n\n    return pred_arr\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    """"""Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n\n    Stable version by Dougal J. Sutherland.\n\n    Params:\n    -- mu1   : Numpy array containing the activations of a layer of the\n               inception net (like returned by the function \'get_predictions\')\n               for generated samples.\n    -- mu2   : The sample mean over activations, precalculated on an \n               representive data set.\n    -- sigma1: The covariance matrix over activations for generated samples.\n    -- sigma2: The covariance matrix over activations, precalculated on an \n               representive data set.\n\n    Returns:\n    --   : The Frechet Distance.\n    """"""\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        \'Training and test mean vectors have different lengths\'\n    assert sigma1.shape == sigma2.shape, \\\n        \'Training and test covariances have different dimensions\'\n\n    diff = mu1 - mu2\n\n    # Product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = (\'fid calculation produces singular product; \'\n               \'adding %s to diagonal of cov estimates\') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    # Numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError(\'Imaginary component {}\'.format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) +\n            np.trace(sigma2) - 2 * tr_covmean)\n\n\ndef calculate_activation_statistics(images, model, batch_size=64,\n                                    dims=2048, cuda=False, verbose=False):\n    """"""Calculation of the statistics used by the FID.\n    Params:\n    -- images      : Numpy array of dimension (n_images, 3, hi, wi). The values\n                     must lie between 0 and 1.\n    -- model       : Instance of inception model\n    -- batch_size  : The images numpy array is split into batches with\n                     batch size batch_size. A reasonable batch size\n                     depends on the hardware.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the\n                     number of calculated batches is reported.\n    Returns:\n    -- mu    : The mean over samples of the activations of the pool_3 layer of\n               the inception model.\n    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n               the inception model.\n    """"""\n    act = get_activations(images, model, batch_size, dims, cuda, verbose)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma\n\n\ndef _compute_statistics_of_path(path, model, batch_size, dims, cuda):\n    if path.endswith(\'.npz\'):\n        f = np.load(path)\n        m, s = f[\'mu\'][:], f[\'sigma\'][:]\n        f.close()\n    else:\n        path = pathlib.Path(path)\n        files = list(path.glob(\'*.jpg\')) + list(path.glob(\'*.png\'))\n\n        imgs = np.array([imread(str(fn)).astype(np.float32) for fn in files])\n\n        # Bring images to shape (B, 3, H, W)\n        imgs = imgs.transpose((0, 3, 1, 2))\n\n        # Rescale images to be between 0 and 1\n        imgs /= 255\n\n        m, s = calculate_activation_statistics(imgs, model, batch_size,\n                                               dims, cuda)\n\n    return m, s\n\n\ndef calculate_fid_given_paths(paths, batch_size, cuda, dims):\n    """"""Calculates the FID of two paths""""""\n    for p in paths:\n        if not os.path.exists(p):\n            raise RuntimeError(\'Invalid path: %s\' % p)\n\n    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n\n    model = InceptionV3([block_idx])\n    if cuda:\n        model.cuda()\n\n    m1, s1 = _compute_statistics_of_path(paths[0], model, batch_size,\n                                         dims, cuda)\n    m2, s2 = _compute_statistics_of_path(paths[1], model, batch_size,\n                                         dims, cuda)\n    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n\n    return fid_value\n\n#####################\n# Added by Chris B. #\n#####################\n\ndef _compute_statistics_of_imgs(imgs, model, batch_size, dims, cuda):\n\n    imgs = imgs.astype(np.float32)\n    \n    # Bring images to shape (B, 3, H, W)\n    #imgs = imgs.transpose((0, 3, 1, 2))\n\n    # Rescale images to be between 0 and 1\n    imgs /= 255\n\n    m, s = calculate_activation_statistics(imgs, model, batch_size,\n                                           dims, cuda)\n\n    return m, s\n\ndef calculate_fid_given_imgs(imgs1, imgs2, batch_size, cuda, dims):\n    """"""\n    Calculates the FID of two sets of images.\n    NOTE: imgs1,imgs2 should be uint8s\n    """"""\n\n    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n\n    model = InceptionV3([block_idx])\n    if cuda:\n        model.cuda()\n\n    m1, s1 = _compute_statistics_of_imgs(imgs1, model, batch_size,\n                                         dims, cuda)\n    m2, s2 = _compute_statistics_of_imgs(imgs2, model, batch_size,\n                                         dims, cuda)\n    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n\n    return fid_value\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    fid_value = calculate_fid_given_paths(args.path,\n                                          args.batch_size,\n                                          args.gpu != \'\',\n                                          args.dims)\n    print(\'FID: \', fid_value)\n'"
gan/inception.py,4,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n\nclass InceptionV3(nn.Module):\n    """"""Pretrained InceptionV3 network returning feature maps""""""\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=[DEFAULT_BLOCK_INDEX],\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False):\n        """"""Build pretrained InceptionV3\n\n        Parameters\n        ----------\n        output_blocks : list of int\n            Indices of blocks to return features of. Possible values are:\n                - 0: corresponds to output of first max pooling\n                - 1: corresponds to output of second max pooling\n                - 2: corresponds to output which is fed to aux classifier\n                - 3: corresponds to output of final average pooling\n        resize_input : bool\n            If true, bilinearly resizes input to width and height 299 before\n            feeding input to model. As the network without fully connected\n            layers is fully convolutional, it should be able to handle inputs\n            of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n            If true, normalizes the input to the statistics the pretrained\n            Inception network expects\n        requires_grad : bool\n            If true, parameters of the model require gradient. Possibly useful\n            for finetuning the network\n        """"""\n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            \'Last possible output block index is 3\'\n\n        self.blocks = nn.ModuleList()\n\n        inception = models.inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        """"""Get Inception feature maps\n\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in \n            range (0, 1)\n\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output \n        block, sorted ascending by index\n        """"""\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.upsample(x, size=(299, 299), mode=\'bilinear\')\n\n        if self.normalize_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp\n'"
gan/inception_score.py,8,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nimport torch.utils.data\n\nfrom torchvision.models.inception import inception_v3\n\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef inception_score(imgs, cuda=True, batch_size=32, resize=False, splits=1):\n    """"""Computes the inception score of the generated images imgs\n\n    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n    cuda -- whether or not to run on GPU\n    batch_size -- batch size for feeding into Inception v3\n    splits -- number of splits\n    """"""\n    N = len(imgs)\n\n    assert batch_size > 0\n    assert N > batch_size\n\n    # Set up dtype\n    if cuda:\n        dtype = torch.cuda.FloatTensor\n    else:\n        if torch.cuda.is_available():\n            print(""WARNING: You have a CUDA device, so you should probably set cuda=True"")\n        dtype = torch.FloatTensor\n\n    # Set up dataloader\n    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n\n    # Load inception model\n    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n    inception_model.eval();\n    up = nn.Upsample(size=(299, 299), mode=\'bilinear\').type(dtype)\n    def get_pred(x):\n        if resize:\n            x = up(x)\n        x = inception_model(x)\n        return F.softmax(x).data.cpu().numpy()\n\n    # Get predictions\n    preds = np.zeros((N, 1000))\n\n    for i, batch in enumerate(dataloader, 0):\n        batch = batch.type(dtype)\n        batchv = Variable(batch)\n        batch_size_i = batch.size()[0]\n\n        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n\n    # Now compute the mean kl-div\n    split_scores = []\n\n    for k in range(splits):\n        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n        py = np.mean(part, axis=0)\n        scores = []\n        for i in range(part.shape[0]):\n            pyx = part[i, :]\n            scores.append(entropy(pyx, py))\n        split_scores.append(np.exp(np.mean(scores)))\n\n    return np.mean(split_scores), np.std(split_scores)\n\nclass IgnoreLabelDataset(torch.utils.data.Dataset):\n    def __init__(self, orig):\n        self.orig = orig\n\n    def __getitem__(self, index):\n        return self.orig[index][0]\n\n    def __len__(self):\n        return len(self.orig)\n\nif __name__ == \'__main__\':\n\n    import torchvision.datasets as dset\n    import torchvision.transforms as transforms\n\n    cifar = dset.CIFAR10(root=\'data/\', download=True,\n                             transform=transforms.Compose([\n                                 transforms.Scale(32),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                             ])\n    )\n\n    IgnoreLabelDataset(cifar)\n\n    print (""Calculating Inception Score..."")\n    print (inception_score(IgnoreLabelDataset(cifar), cuda=True, batch_size=32, resize=True, splits=10))\n'"
gan/interactive.py,0,"b'import torch\nfrom torch import nn\nfrom torchvision.utils import save_image\n\ndef pp_interp(net, alpha):\n    """"""\n    Only works with model_resnet_preproc.py as your\n      architecture!!!\n    """"""\n    conv2d = net.d.preproc\n    deconv2d = nn.ConvTranspose2d(16, 3, 3, stride=1, padding=1)\n    deconv2d = deconv2d.cuda()\n    deconv2d.weight = conv2d.weight\n\n    gz1 = net.sample(bs=128)\n    gz2 = net.sample(bs=128)\n\n    #alpha = net.sample_lambda(gz1.size(0))\n    gz_mix = alpha*gz1 + (1.-alpha)*gz2\n\n    save_image(gz1*0.5 + 0.5, filename=""gz1.png"")\n    save_image(gz2*0.5 + 0.5, filename=""gz2.png"")\n    save_image(gz_mix*0.5 + 0.5, filename=""gz_mix.png"")\n\n    # Ok, do the mixup in hidden space.\n\n    gz1_h = conv2d(gz1)\n    gz2_h = conv2d(gz2)\n    #alpha = 0.05\n    gz_mix_h = alpha*gz1_h + (1.-alpha)*gz2_h\n    gz_mix_h_dec = deconv2d(gz_mix_h)\n    save_image(gz_mix_h_dec*0.5 + 0.5, filename=""gz_mix_h_dec.png"")\n\n    print(conv2d.weight == deconv2d.weight)\n\n    \n    import pdb\n    pdb.set_trace()\n    \n    \n'"
gan/main.py,0,"b'import argparse, os\nfrom GAN import GAN\n\'\'\'\nfrom CGAN import CGAN\nfrom LSGAN import LSGAN\nfrom DRAGAN import DRAGAN\nfrom ACGAN import ACGAN\nfrom WGAN import WGAN\nfrom WGAN_GP import WGAN_GP\nfrom infoGAN import infoGAN\nfrom EBGAN import EBGAN\nfrom BEGAN import BEGAN\n\'\'\'\n\n""""""parsing and configuration""""""\ndef parse_args():\n    desc = ""Pytorch implementation of GAN collections""\n    parser = argparse.ArgumentParser(description=desc)\n\n    parser.add_argument(\'--gan_type\', type=str, default=\'GAN\',\n                        choices=[\'GAN\', \'CGAN\', \'infoGAN\', \'ACGAN\', \'EBGAN\', \'BEGAN\', \'WGAN\', \'WGAN_GP\', \'DRAGAN\', \'LSGAN\'],\n                        help=\'The type of GAN\')#, required=True)\n    parser.add_argument(\'--dataset\', type=str, default=\'mnist\', choices=[\'mnist\', \'fashion-mnist\', \'celebA\'],\n                        help=\'The name of dataset\')\n    parser.add_argument(\'--epoch\', type=int, default=25, help=\'The number of epochs to run\')\n    parser.add_argument(\'--batch_size\', type=int, default=64, help=\'The size of batch\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'models\',\n                        help=\'Directory name to save the model\')\n    parser.add_argument(\'--result_dir\', type=str, default=\'results\',\n                        help=\'Directory name to save the generated images\')\n    parser.add_argument(\'--log_dir\', type=str, default=\'logs\',\n                        help=\'Directory name to save training logs\')\n    parser.add_argument(\'--lrG\', type=float, default=0.0002)\n    parser.add_argument(\'--lrD\', type=float, default=0.0002)\n    parser.add_argument(\'--beta1\', type=float, default=0.5)\n    parser.add_argument(\'--beta2\', type=float, default=0.999)\n    parser.add_argument(\'--gpu_mode\', type=bool, default=True)\n\n    return check_args(parser.parse_args())\n\n""""""checking arguments""""""\ndef check_args(args):\n    # --save_dir\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n\n    # --result_dir\n    if not os.path.exists(args.result_dir):\n        os.makedirs(args.result_dir)\n\n    # --result_dir\n    if not os.path.exists(args.log_dir):\n        os.makedirs(args.log_dir)\n\n    # --epoch\n    try:\n        assert args.epoch >= 1\n    except:\n        print(\'number of epochs must be larger than or equal to one\')\n\n    # --batch_size\n    try:\n        assert args.batch_size >= 1\n    except:\n        print(\'batch size must be larger than or equal to one\')\n\n    return args\n\n""""""main""""""\ndef main():\n    # parse arguments\n    args = parse_args()\n    if args is None:\n        exit()\n\n        # declare instance for GAN\n    if args.gan_type == \'GAN\':\n        gan = GAN(args)\n    elif args.gan_type == \'CGAN\':\n        gan = CGAN(args)\n    elif args.gan_type == \'ACGAN\':\n        gan = ACGAN(args)\n    elif args.gan_type == \'infoGAN\':\n        gan = infoGAN(args, SUPERVISED = True)\n    elif args.gan_type == \'EBGAN\':\n        gan = EBGAN(args)\n    elif args.gan_type == \'WGAN\':\n        gan = WGAN(args)\n    elif args.gan_type == \'WGAN_GP\':\n        gan = WGAN_GP(args)\n    elif args.gan_type == \'DRAGAN\':\n        gan = DRAGAN(args)\n    elif args.gan_type == \'LSGAN\':\n        gan = LSGAN(args)\n    elif args.gan_type == \'BEGAN\':\n        gan = BEGAN(args)\n    else:\n        raise Exception(""[!] There is no option for "" + args.gan_type)\n\n        # launch the graph in a session\n    gan.train()\n    print("" [*] Training finished!"")\n\n    # visualize learned generator\n    gan.visualize_results(args.epoch)\n    print("" [*] Testing finished!"")\n\nif __name__ == \'__main__\':\n    main()\n'"
gan/mugan.py,12,"b'import numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.autograd import Variable, grad\nfrom christorch.gan.base import GAN\n\ndef bce(prediction, target):\n    if not hasattr(target, \'__len__\'):\n        target = torch.ones_like(prediction)*target\n        if prediction.is_cuda:\n            target = target.cuda()\n        target = Variable(target)\n    loss = torch.nn.BCELoss()\n    if prediction.is_cuda:\n        loss = loss.cuda()\n    return loss(prediction, target)\n\ndef jsgan_d_fake_loss(d_fake):\n    return bce(d_fake, 0)\n\ndef jsgan_d_real_loss(d_real):\n    return bce(d_real, 1)\n\ndef jsgan_g_loss(d_fake):\n    return jsgan_d_real_loss(d_fake)\n\ndef wgan_d_fake_loss(d_fake):\n    return torch.mean(d_fake)\n\ndef wgan_d_real_loss(d_real):\n    return -torch.mean(d_real)\n\ndef wgan_g_loss(d_fake):\n    return wgan_d_real_loss(d_fake)\n\ndef hinge_d_fake_loss(d_fake):\n    return nn.ReLU()(1.0 + d_fake).mean()\n\ndef hinge_d_real_loss(d_real):\n    return nn.ReLU()(1.0 - d_real).mean()\n\ndef hinge_g_loss(d_fake):\n    return -torch.mean(d_fake)\n\nclass MUGAN(GAN):\n    """"""\n    JSGAN with the option of adding mixup.\n    Inherits from the base class `GAN` in christorch but adds\n      some extra args to init such as mixup and expontentially\n      decreasing LR schedule.\n    """"""\n    def __init__(self,\n                 loss=\'jsgan\',\n                 mixup=None,\n                 mixup_ff=False,\n                 lr_sched=None, alpha=0.2,\n                 update_g_every=1.,\n                 *args, **kwargs):\n        """"""\n        loss: which loss function to use? Can choose between\n          \'jsgan\', \'wgan\', and \'hinge\'.\n        mixup: which mixup to do. Can be None, \'pixel\', or \'hidden\'.\n        mixup_ff: whether or not to also generate fake-fake mixes as\n          well.\n        lr_sched:\n        alpha: mixup parameter, s.t. lambda ~ Beta(alpha,alpha)\n          and lambda := min(lambda, 1-lambda).\n        update_g_every: update the generator every this many\n          iterations.\n        """"""\n        if loss not in [\'jsgan\', \'wgan\', \'hinge\']:\n            raise Exception(""uknown loss"")\n        if loss == \'jsgan\':\n            self.g_loss = jsgan_g_loss\n            self.d_loss_real = jsgan_d_real_loss\n            self.d_loss_fake = jsgan_d_fake_loss\n        elif loss == \'wgan\':\n            self.g_loss = wgan_g_loss\n            self.d_loss_real = wgan_d_real_loss\n            self.d_loss_fake = wgan_d_fake_loss\n        else:\n            self.g_loss = hinge_g_loss\n            self.d_loss_real = hinge_d_real_loss\n            self.d_loss_fake = hinge_d_fake_loss\n        if mixup not in [None, \'pixel\', \'hidden\', \'vh1\', \'vh2\']:\n            raise Exception(""mixup must be either None, \'pixel\', or \'hidden\'"")\n        else:\n            # Backwards compatibility: if \'hidden\',\n            # assume it\'s vh1.\n            if mixup == \'hidden\':\n                self.mixup = \'vh1\'\n            else:\n                self.mixup = mixup\n        if lr_sched not in [None, \'exp\']:\n            raise Exception(""lr_sched must be either None or \'exp\'"")\n        self.mixup_ff = mixup_ff\n        self.lr_sched = lr_sched\n        self.alpha = 0.2\n        self.norm_verbosity = 0 # if == 1, then monitor fake/mix norms too\n        if lr_sched is not None:\n            self.scheduler = {}\n            self.scheduler[\'g\'] = optim.lr_scheduler.ExponentialLR(\n                self.optim[\'g\'], gamma=0.99)\n            self.scheduler[\'d\'] = optim.lr_scheduler.ExponentialLR(\n                self.optim[\'d\'], gamma=0.99)\n        self.update_g_every = update_g_every\n        super(MUGAN, self).__init__(*args, **kwargs)\n        # TODO :FIX\n        if self.dnorm is None:\n            self.dnorm = 0.\n        \n    def grad_norm(self, d_out, x):\n        ones = torch.ones(d_out.size())\n        if self.use_cuda:\n            ones = ones.cuda()\n        grad_wrt_x = grad(outputs=d_out, inputs=x,\n                          grad_outputs=ones,\n                          create_graph=True,\n                          retain_graph=True,\n                          only_inputs=True)[0]\n        g_norm = (grad_wrt_x.view(\n            grad_wrt_x.size()[0], -1).norm(2, 1)**2).mean()\n        return g_norm    \n    \n    def _train_on_instance(self, z, x, **kwargs):\n        """"""Training method for when mixup=False""""""\n        self._train()\n        x.requires_grad = True # for dnorm\n        # Train the generator.\n        self.optim[\'g\'].zero_grad()\n        fake = self.g(z)\n        _, d_fake = self.d(fake)\n        gen_loss = self.g_loss(d_fake)\n        if (kwargs[\'iter\']-1) % self.update_g_every == 0:\n            gen_loss.backward()\n            self.optim[\'g\'].step()\n        # Train the discriminator.\n        self.optim[\'d\'].zero_grad()\n        _, d_fake = self.d(fake.detach())\n        _, d_real = self.d(x)\n        d_loss = self.d_loss_real(d_real) + self.d_loss_fake(d_fake)\n        d_loss.backward()\n        self.optim[\'d\'].step()\n        ##################################\n        # Also compute the gradient norm.\n        # Grad norm for D_REAL\n        _, d_real = self.d(x)\n        g_norm_x = self.grad_norm(d_real, x)\n        if self.dnorm > 0.:\n            self.optim[\'d\'].zero_grad()\n            (g_norm_x*self.dnorm).backward()\n            self.optim[\'d\'].step()\n        self.optim[\'d\'].zero_grad()\n        ##################################\n        losses = {\n            \'g_loss\': gen_loss.data.item(),\n            \'d_loss\': d_loss.data.item(),\n            \'d_real_norm\': g_norm_x.data.item()\n        }\n        outputs = {\n            \'x\': x.detach(),\n            \'gz\': fake.detach(),\n        }\n        return losses, outputs\n\n    def sample_lambda(self, bs):\n        alphas = []\n        for i in range(bs):\n            alpha = np.random.beta(self.alpha, self.alpha)\n            alpha = min(alpha, 1.-alpha)\n            alphas.append(alpha)\n        alphas = np.asarray(alphas).reshape((bs,1,1,1))\n        alphas = torch.from_numpy(alphas).float()\n        if self.use_cuda:\n            alphas = alphas.cuda()\n        return alphas\n    \n    def _train_on_instance_mixup(self, z, x, **kwargs):\n        """"""Perform mixup in the pixel space""""""\n        self._train()\n        x.requires_grad = True # for dnorm\n        # Train the generator.\n        self.optim[\'g\'].zero_grad()\n        alpha = self.sample_lambda(x.size(0))\n        fake = self.g(z)\n        xz = Variable(alpha*x.data + (1.-alpha)*fake.data)\n        if self.mixup_ff:\n            perm = torch.randperm(fake.size(0)).view(-1).long()\n            fake_perm = fake[perm]\n            xz_ff = Variable(alpha*fake.data + (1.-alpha)*fake_perm.data)\n        _, d_fake = self.d(fake)\n        gen_loss = self.g_loss(d_fake)\n        if (kwargs[\'iter\']-1) % self.update_g_every == 0:\n            gen_loss.backward()\n            self.optim[\'g\'].step()\n        # Train the discriminator.\n        self.optim[\'d\'].zero_grad()\n        _, d_xz = self.d(xz.detach())\n        _, d_real = self.d(x)\n        _, d_fake = self.d(fake.detach())\n        d_loss = self.d_loss_fake(d_xz) + self.d_loss_real(d_real) + \\\n                 self.d_loss_fake(d_fake)\n        if self.mixup_ff:\n            _, d_xz_ff = self.d(xz_ff.detach())\n            d_loss += self.d_loss_fake(d_xz_ff)\n        d_loss.backward()\n        self.optim[\'d\'].step()\n        ##################################\n        # Also compute the gradient norm.\n        # Grad norm for D_REAL\n        _, d_real = self.d(x)\n        g_norm_x = self.grad_norm(d_real, x)\n        if self.dnorm > 0.:\n            self.optim[\'d\'].zero_grad()\n            (g_norm_x*self.dnorm).backward()\n            self.optim[\'d\'].step()\n        self.optim[\'d\'].zero_grad()\n        ##################################\n        losses = {\n            \'g_loss\': gen_loss.data.item(),\n            \'d_loss\': d_loss.data.item(),\n            \'d_real_norm\': g_norm_x.data.item(),\n        }\n        outputs = {\n            \'x\': x.detach(),\n            \'gz\': fake.detach(),\n        }\n        return losses, outputs\n\n    def _train_on_instance_mixup_hidden(self, z, x, **kwargs):\n        """"""Perform mixup in the hidden states""""""\n        self._train()\n        x.requires_grad = True # for dnorm\n        # Train the generator.\n        self.optim[\'g\'].zero_grad()\n        fake = self.g(z)\n        _, d_fake = self.d(fake)\n        #gen_loss = self.bce(d_fake, 1)\n        gen_loss = self.g_loss(d_fake)\n        gen_loss.backward()\n        self.optim[\'g\'].step()\n        # Train the discriminator.\n        self.optim[\'d\'].zero_grad()\n        hs_reals, d_real = self.d(x)\n        hs_fakes, d_fake = self.d(fake.detach())\n        #d_loss = self.bce(d_fake, 0) + self.bce(d_real, 1)\n        d_loss = self.d_loss_fake(d_fake) + self.d_loss_real(d_real)\n        # Do the mix.\n        alpha = self.sample_lambda(x.size(0))\n        # Sample the index. If it is == 0, do first hidden\n        # layer, else if 1, do second hidden, else do pixel.\n        if self.mixup == \'vh1\':\n            nc = 2 # sample in (0,1)\n        else:\n            nc = 3 # sample in (0,1,2)\n        idx = np.random.randint(0, nc)\n        if idx < (nc-1):\n            # Do mixup on either first or second hidden layer,\n            # (which is determined by the idx sampled)\n            hs_mix = alpha*hs_reals[idx] + (1.-alpha)*hs_fakes[idx]\n            d_xz = self.d.partial_forward(hs_mix, idx)\n            if self.mixup_ff:\n                # If fake-fake mixes are enabled\n                perm = torch.randperm(hs_fakes[idx].size(0)).view(-1).long()\n                hs_fakes_perm = hs_fakes[idx][perm]\n                hs_ff_mix = alpha*hs_fakes[idx] + (1.-alpha)*hs_fakes_perm\n                d_xz_ff = self.d.partial_forward(hs_ff_mix, idx)\n        else:\n            # Do mixup in the pixels.\n            xz = Variable(alpha*x.data + (1.-alpha)*fake.data)\n            _, d_xz = self.d(xz)\n            if self.mixup_ff:\n                # If fake-fake mixes are enabled\n                perm = torch.randperm(fake.size(0)).view(-1).long()\n                fake_perm = fake[perm]\n                xz_ff = Variable(alpha*fake.data + (1.-alpha)*fake_perm.data)\n                _, d_xz_ff = self.d(xz_ff)\n        d_loss += self.d_loss_fake(d_xz)\n        if self.mixup_ff:\n            d_loss += self.d_loss_fake(d_xz_ff)\n        d_loss.backward()\n        self.optim[\'d\'].step()\n        ##################################\n        # Also compute the gradient norm.\n        # Grad norm for D_REAL\n        _, d_real = self.d(x)\n        g_norm_x = self.grad_norm(d_real, x)\n        if self.dnorm > 0.:\n            self.optim[\'d\'].zero_grad()\n            (g_norm_x*self.dnorm).backward()\n            self.optim[\'d\'].step()\n        self.optim[\'d\'].zero_grad()\n        ##################################\n        losses = {\n            \'g_loss\': gen_loss.data.item(),\n            \'d_loss\': d_loss.data.item(),\n            \'d_real_norm\': g_norm_x.data.item()\n        }\n        outputs = {\n            \'x\': x.detach(),\n            \'gz\': fake.detach(),\n        }\n        return losses, outputs\n    \n    def train_on_instance(self, z, x, **kwargs):\n        if self.mixup == \'pixel\':\n            return self._train_on_instance_mixup(\n                z, x, **kwargs)\n        elif self.mixup is not None and self.mixup.startswith(\'vh\'):\n            return self._train_on_instance_mixup_hidden(\n                z, x, **kwargs)\n        else:\n            return self._train_on_instance(\n                z, x, **kwargs)\n'"
gan/task_launcher.py,1,"b'import numpy as np\nimport torch\nfrom torch.autograd import Variable, grad\nimport imp\nimport os\nimport argparse\nimport glob\nfrom mugan import MUGAN\nfrom skimage.io import imsave\n\ndef dump_samples_to_disk(folder, how_many, bs):\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    train_size = how_many\n    for b in range( (train_size // bs) + 1 ):\n        print(""Generating fake samples -- iteration %i"" % b)\n        samples_ = net.sample(bs).data.cpu().numpy()*0.5 + 0.5\n        samples_ = (samples_*255.).astype(""uint8"")\n        if b == 0:\n            samples = samples_\n        else:\n            samples = np.vstack((samples, samples_))\n    samples = samples[0:train_size]\n    assert len(samples) == train_size\n    for i in range(samples.shape[0]):\n        img = samples[i]\n        imsave(arr=img.swapaxes(0,1).swapaxes(1,2),\n               fname=""%s/%i.png"" % (folder,i))\n\ndef compute_fid_cifar10(use_samples=True, bs=512, use_tf=False):\n    """"""\n    Compute the FID score between the dataset and samples\n    from the model.\n    use_samples: compute FID using samples from our GAN. If\n      this is set to `False` then we use the training set of\n      CIFAR10.\n    bs: batch size\n    """"""\n    import fid_score\n    from iterators import cifar10\n    train_size = 50000 # cifar10 train size\n    if use_samples:\n        # Generate a bunch of samples of the same # as the size of\n        # the training set.\n        samples = None\n        for b in range( (train_size // bs) + 1 ):\n            print(""Generating fake samples -- iteration %i"" % b)\n            samples_ = net.sample(bs).data.cpu().numpy()*0.5 + 0.5\n            samples_ = (samples_*255.).astype(""uint8"")\n            if b == 0:\n                samples = samples_\n            else:\n                samples = np.vstack((samples, samples_))\n        samples = samples[0:train_size]\n        assert len(samples) == train_size\n    else:\n        samples = cifar10.get_data(train=True)\n        samples = samples.transpose((0,3,1,2))\n    print(""Loading entire test set..."")\n    cifar10_test = cifar10.get_data(train=False)\n    if not use_tf:\n        cifar10_test = cifar10_test.transpose((0, 3, 1, 2))\n        score = fid_score.calculate_fid_given_imgs(\n            imgs1=samples,\n            imgs2=cifar10_test,\n            batch_size=bs,\n            cuda=True,\n            dims=2048\n        )\n        print(""FID score: %f"" % score)\n    else:\n        from tf.fid import calculate_fid_given_imgs\n        samples_tf = samples.swapaxes(1, 2).swapaxes(2, 3)\n        score = calculate_fid_given_imgs(\n            samples_tf,\n            ""tf/cifar-10-fid.npz"")\n        print(""TF FID score: %f"" % score)\n\ndef compute_inception_cifar10(use_samples=True,\n                              how_many=50000,\n                              bs=512,\n                              seed=0,\n                              dump_only=False,\n                              use_tf=False):\n    """"""\n    Compute Inception score.\n    """"""\n    import inception_score\n    from iterators import cifar10\n    train_size = how_many # cifar10 train size\n    # Generate a bunch of samples of the same # as the size of\n    # the training set.\n    rs = np.random.RandomState(seed)\n    if use_samples:\n        samples = None\n        for b in range( (train_size // bs) + 1 ):\n            print(""Generating fake samples -- iteration %i"" % b)\n            samples_ = net.sample(bs, seed=rs.randint(1000000)).data.cpu().numpy()\n            if b == 0:\n                samples = samples_\n            else:\n                samples = np.vstack((samples, samples_))\n        samples = samples[0:train_size]\n        assert len(samples) == train_size\n    else:\n        samples = cifar10.get_data(train=True)\n        samples = samples.transpose((0, 3, 1, 2))\n        samples = ((samples/255.) - 0.5) / 0.5\n    # Expects samples in range [-1, 1]\n    if dump_only:\n        # Don\'t compute scores, just dump the samples\n        # to disk.\n        samples_uint8 = ((samples*0.5) + 0.5)*255.\n        samples_uint8 = samples_uint8.astype(np.uint8)\n        np.save(""%s.%i"" % (args.name, seed), samples_uint8)\n    else:\n        if not use_tf:\n            score_mu, score_std = inception_score.inception_score(\n                imgs=samples,\n                batch_size=64,\n                resize=True,\n                splits=10\n            )\n            print(""PyTorch Inception score: %f +/- %f"" % (score_mu, score_std))\n        else:\n            from tf.inception import get_inception_score\n            # This one actually expects samples in [0,255].\n            samples_tf = (((samples*0.5)+0.5)*255.).astype(np.uint8)\n            samples_tf = samples_tf.swapaxes(1, 2).swapaxes(2, 3)\n            samples_tf = [ samples_tf[i] for i in range(len(samples_tf)) ]\n            inception_score_mean, inception_score_std = get_inception_score(samples_tf)\n            print(""TF bugged Inception Score: Mean = {} \\tStd = {}."".format(\n                inception_score_mean, inception_score_std))\n\n    \n\'\'\'\nProcess arguments.\n\'\'\'\ndef parse_args():\n    parser = argparse.ArgumentParser(description="""")\n    parser.add_argument(\'--name\', type=str, default=""deleteme"")\n    parser.add_argument(\'--batch_size\', type=int, default=2)\n    parser.add_argument(\'--epochs\', type=int, default=100)\n    parser.add_argument(\'--loss\', type=str, default=\'jsgan\')\n    parser.add_argument(\'--z_dim\', type=int, default=62)\n    parser.add_argument(\'--lr\', type=float, default=2e-4)\n    parser.add_argument(\'--beta1\', type=float, default=0.5)\n    parser.add_argument(\'--beta2\', type=float, default=0.999)\n    parser.add_argument(\'--mixup\', type=str, default=None)\n    parser.add_argument(\'--mixup_ff\', action=\'store_true\')\n    parser.add_argument(\'--alpha\', type=float, default=0.2)\n    parser.add_argument(\'--dnorm\', type=float, default=0.0)\n    parser.add_argument(\'--update_g_every\', type=int, default=1)\n    # Iterator returns (it_train_a, it_train_b, it_val_a, it_val_b)\n    parser.add_argument(\'--iterator\', type=str, default=""iterators/mnist.py"")\n    parser.add_argument(\'--resume\', type=str, default=None)\n    parser.add_argument(\'--legacy\', action=\'store_true\')\n    parser.add_argument(\'--interactive\', type=str, default=None)\n    parser.add_argument(\'--network\', type=str, default=""networks/mnist.py"")\n    parser.add_argument(\'--save_path\', type=str, default=\'./results\')\n    parser.add_argument(\'--save_images_every\', type=int, default=100)\n    parser.add_argument(\'--save_every\', type=int, default=10)\n    parser.add_argument(\'--last_epoch\', type=int, default=None)\n    parser.add_argument(\'--cpu\', action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\nargs = parse_args()\n# Dynamically load network module.\nnet_module = imp.load_source(\'network\', args.network)\ngen_fn, disc_fn = getattr(net_module, \'get_network\')(args.z_dim)\n# Dynamically load iterator module.\nitr_module = imp.load_source(\'iterator\', args.iterator)\nitr = getattr(itr_module, \'get_iterators\')(args.batch_size)\n\ngan_class = MUGAN\ngan_kwargs = {\n    \'gen_fn\': gen_fn,\n    \'disc_fn\': disc_fn,\n    \'z_dim\': args.z_dim,\n    \'loss\': args.loss,\n    \'mixup\': args.mixup,\n    \'mixup_ff\': args.mixup_ff,\n    \'alpha\': args.alpha,\n    \'dnorm\': args.dnorm,\n    \'update_g_every\': args.update_g_every,\n    \'opt_d_args\': {\'lr\':args.lr, \'betas\':(args.beta1, args.beta2)},\n    \'opt_g_args\': {\'lr\':args.lr, \'betas\':(args.beta1, args.beta2)},\n    \'use_cuda\': False if args.cpu else True\n}\nnet = gan_class(**gan_kwargs)\n#net.alpha = args.alpha\n\nif args.resume is not None:\n    if args.resume == \'auto\':\n        # autoresume\n        model_dir = ""%s/%s/models"" % (args.save_path, args.name)\n        # List all the pkl files.\n        files = glob.glob(""%s/*.pkl"" % model_dir)\n        # Make them absolute paths.\n        files = [ os.path.abspath(key) for key in files ]\n        if len(files) > 0:\n            # Get creation time and use that.\n            latest_model = max(files, key=os.path.getctime)\n            print(""Auto-resume mode found latest model: %s"" %\n                  latest_model)\n            net.load(latest_model, legacy=args.legacy)\n    else:\n        ignore_d = True if args.interactive is not None else False\n        net.load(args.resume, legacy=args.legacy,\n                 ignore_d=ignore_d)\n    if args.last_epoch is not None:\n        net.last_epoch = args.last_epoch\nif args.interactive is not None:\n    how_many = 5000*10\n    if \'inception\' in args.interactive:\n        if args.interactive == \'inception\':\n            # Compute the Inception score and output\n            # mean and std.\n            compute_inception_cifar10(how_many=how_many)\n        elif args.interactive == \'inception_tf\':\n            compute_inception_cifar10(how_many=how_many, use_tf=True)\n        elif args.interactive == \'inception_both\':\n            compute_inception_cifar10(how_many=how_many)\n            compute_inception_cifar10(how_many=how_many, use_tf=True)\n    elif args.interactive == \'dump\':\n        # Dump the images to disk.\n        compute_inception_cifar10(how_many=how_many,\n                                  seed=0,\n                                  dump_only=True)\n    elif args.interactive == \'dump_to_disk\':\n        dump_samples_to_disk(folder=""img_dump"", how_many=50000, bs=512)\n    elif args.interactive == \'fid_tf\':\n        compute_fid_cifar10(use_tf=True)\n    elif args.interactive == \'free\':        \n        import pdb\n        pdb.set_trace()\nelse:\n    net.train(\n        itr=itr,\n        epochs=args.epochs,\n        model_dir=""%s/%s/models"" % (args.save_path, args.name),\n        result_dir=""%s/%s"" % (args.save_path, args.name),\n        append=True if args.resume is not None else False,\n        save_every=args.save_every\n    )\n'"
gan/utils.py,4,"b'import os, gzip, torch\nimport torch.nn as nn\nimport numpy as np\nimport scipy.misc\nimport imageio\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\n\ndef load_mnist(dataset):\n    data_dir = os.path.join(""./data"", dataset)\n\n    def extract_data(filename, num_data, head_size, data_size):\n        with gzip.open(filename) as bytestream:\n            bytestream.read(head_size)\n            buf = bytestream.read(data_size * num_data)\n            data = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n        return data\n\n    data = extract_data(data_dir + \'/train-images-idx3-ubyte.gz\', 60000, 16, 28 * 28)\n    trX = data.reshape((60000, 28, 28, 1))\n\n    data = extract_data(data_dir + \'/train-labels-idx1-ubyte.gz\', 60000, 8, 1)\n    trY = data.reshape((60000))\n\n    data = extract_data(data_dir + \'/t10k-images-idx3-ubyte.gz\', 10000, 16, 28 * 28)\n    teX = data.reshape((10000, 28, 28, 1))\n\n    data = extract_data(data_dir + \'/t10k-labels-idx1-ubyte.gz\', 10000, 8, 1)\n    teY = data.reshape((10000))\n\n    trY = np.asarray(trY).astype(np.int)\n    teY = np.asarray(teY)\n\n    X = np.concatenate((trX, teX), axis=0)\n    y = np.concatenate((trY, teY), axis=0).astype(np.int)\n\n    seed = 547\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(y)\n\n    y_vec = np.zeros((len(y), 10), dtype=np.float)\n    for i, label in enumerate(y):\n        y_vec[i, y[i]] = 1\n\n    X = X.transpose(0, 3, 1, 2) / 255.\n    # y_vec = y_vec.transpose(0, 3, 1, 2)\n\n    X = torch.from_numpy(X).type(torch.FloatTensor)\n    y_vec = torch.from_numpy(y_vec).type(torch.FloatTensor)\n    return X, y_vec\n\ndef load_celebA(dir, transform, batch_size, shuffle):\n    # transform = transforms.Compose([\n    #     transforms.CenterCrop(160),\n    #     transform.Scale(64)\n    #     transforms.ToTensor(),\n    #     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n    # ])\n\n    # data_dir = \'data/celebA\'  # this path depends on your computer\n    dset = datasets.ImageFolder(dir, transform)\n    data_loader = torch.utils.data.DataLoader(dset, batch_size, shuffle)\n\n    return data_loader\n\n\ndef print_network(net):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print(\'Total number of parameters: %d\' % num_params)\n\ndef save_images(images, size, image_path):\n    return imsave(images, size, image_path)\n\ndef imsave(images, size, path):\n    image = np.squeeze(merge(images, size))\n    return scipy.misc.imsave(path, image)\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n    if (images.shape[3] in (3,4)):\n        c = images.shape[3]\n        img = np.zeros((h * size[0], w * size[1], c))\n        for idx, image in enumerate(images):\n            i = idx % size[1]\n            j = idx // size[1]\n            img[j * h:j * h + h, i * w:i * w + w, :] = image\n        return img\n    elif images.shape[3]==1:\n        img = np.zeros((h * size[0], w * size[1]))\n        for idx, image in enumerate(images):\n            i = idx % size[1]\n            j = idx // size[1]\n            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n        return img\n    else:\n        raise ValueError(\'in merge(images,size) images parameter \'\'must have dimensions: HxW or HxWx3 or HxWx4\')\n\ndef generate_animation(path, num):\n    images = []\n    for e in range(num):\n        img_name = path + \'_epoch%03d\' % (e+1) + \'.png\'\n        images.append(imageio.imread(img_name))\n    imageio.mimsave(path + \'_generate_animation.gif\', images, fps=5)\n\ndef loss_plot(hist, path = \'Train_hist.png\', model_name = \'\'):\n    x = range(len(hist[\'D_loss\']))\n\n    y1 = hist[\'D_loss\']\n    y2 = hist[\'G_loss\']\n\n    plt.plot(x, y1, label=\'D_loss\')\n    plt.plot(x, y2, label=\'G_loss\')\n\n    plt.xlabel(\'Iter\')\n    plt.ylabel(\'Loss\')\n\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.tight_layout()\n\n    path = os.path.join(path, model_name + \'_loss.png\')\n\n    plt.savefig(path)\n\n    plt.close()\n\ndef initialize_weights(net):\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.ConvTranspose2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()'"
supervised/affine_transforms.py,13,"b'""""""\nAffine transforms implemented on torch tensors, and\nonly requiring one interpolation\n\nIncluded:\n- Affine()\n- AffineCompose()\n- Rotation()\n- Translation()\n- Shear()\n- Zoom()\n- Flip()\n\n""""""\n\nimport math\nimport random\nimport torch\n\n# necessary now, but should eventually not be\nimport scipy.ndimage as ndi\nimport numpy as np\n\n\ndef transform_matrix_offset_center(matrix, x, y):\n    """"""Apply offset to a transform matrix so that the image is\n    transformed about the center of the image. \n\n    NOTE: This is a fairly simple operaion, so can easily be\n    moved to full torch.\n\n    Arguments\n    ---------\n    matrix : 3x3 matrix/array\n\n    x : integer\n        height dimension of image to be transformed\n\n    y : integer\n        width dimension of image to be transformed\n    """"""\n    o_x = float(x) / 2 + 0.5\n    o_y = float(y) / 2 + 0.5\n    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n    return transform_matrix\n\ndef apply_transform(x, transform, fill_mode=\'nearest\', fill_value=0.):\n    """"""Applies an affine transform to a 2D array, or to each channel of a 3D array.\n\n    NOTE: this can and certainly should be moved to full torch operations.\n\n    Arguments\n    ---------\n    x : np.ndarray\n        array to transform. NOTE: array should be ordered CHW\n    \n    transform : 3x3 affine transform matrix\n        matrix to apply\n    """"""\n    x = x.astype(\'float32\')\n    transform = transform_matrix_offset_center(transform, x.shape[1], x.shape[2])\n    final_affine_matrix = transform[:2, :2]\n    final_offset = transform[:2, 2]\n    channel_images = [ndi.interpolation.affine_transform(x_channel, final_affine_matrix,\n            final_offset, order=0, mode=fill_mode, cval=fill_value) for x_channel in x]\n    x = np.stack(channel_images, axis=0)\n    return x\n\nclass Affine(object):\n\n    def __init__(self, \n                 rotation_range=None, \n                 translation_range=None,\n                 shear_range=None, \n                 zoom_range=None, \n                 fill_mode=\'constant\',\n                 fill_value=0., \n                 target_fill_mode=\'nearest\', \n                 target_fill_value=0.):\n        """"""Perform an affine transforms with various sub-transforms, using\n        only one interpolation and without having to instantiate each\n        sub-transform individually.\n\n        Arguments\n        ---------\n        rotation_range : one integer or float\n            image will be rotated between (-degrees, degrees) degrees\n\n        translation_range : a float or a tuple/list w/ 2 floats between [0, 1)\n            first value:\n                image will be horizontally shifted between \n                (-height_range * height_dimension, height_range * height_dimension)\n            second value:\n                Image will be vertically shifted between \n                (-width_range * width_dimension, width_range * width_dimension)\n\n        shear_range : float\n            radian bounds on the shear transform\n\n        zoom_range : list/tuple with two floats between [0, infinity).\n            first float should be less than the second\n            lower and upper bounds on percent zoom. \n            Anything less than 1.0 will zoom in on the image, \n            anything greater than 1.0 will zoom out on the image.\n            e.g. (0.7, 1.0) will only zoom in, \n                 (1.0, 1.4) will only zoom out,\n                 (0.7, 1.4) will randomly zoom in or out\n\n        fill_mode : string in {\'constant\', \'nearest\'}\n            how to fill the empty space caused by the transform\n            ProTip : use \'nearest\' for discrete images (e.g. segmentations)\n                    and use \'constant\' for continuous images\n\n        fill_value : float\n            the value to fill the empty space with if fill_mode=\'constant\'\n\n        target_fill_mode : same as fill_mode, but for target image\n\n        target_fill_value : same as fill_value, but for target image\n\n        """"""\n        self.transforms = []\n        if rotation_range:\n            rotation_tform = Rotation(rotation_range, lazy=True)\n            self.transforms.append(rotation_tform)\n\n        if translation_range:\n            translation_tform = Translation(translation_range, lazy=True)\n            self.transforms.append(translation_tform)\n\n        if shear_range:\n            shear_tform = Shear(shear_range, lazy=True)\n            self.transforms.append(shear_tform) \n\n        if zoom_range:\n            zoom_tform = Translation(zoom_range, lazy=True)\n            self.transforms.append(zoom_tform)\n\n        self.fill_mode = fill_mode\n        self.fill_value = fill_value\n        self.target_fill_mode = target_fill_mode\n        self.target_fill_value = target_fill_value\n\n    def __call__(self, x, y=None):\n        # collect all of the lazily returned tform matrices\n        tform_matrix = self.transforms[0](x)\n        for tform in self.transforms[1:]:\n            tform_matrix = np.dot(tform_matrix, tform(x)) \n\n        x = torch.from_numpy(apply_transform(x.numpy(), tform_matrix,\n            fill_mode=self.fill_mode, fill_value=self.fill_value))\n\n        if y:\n            y = torch.from_numpy(apply_transform(y.numpy(), tform_matrix,\n                fill_mode=self.target_fill_mode, fill_value=self.target_fill_value))\n            return x, y\n        else:\n            return x\n\nclass AffineCompose(object):\n\n    def __init__(self, \n                 transforms, \n                 fill_mode=\'constant\', \n                 fill_value=0., \n                 target_fill_mode=\'nearest\', \n                 target_fill_value=0.):\n        """"""Apply a collection of explicit affine transforms to an input image,\n        and to a target image if necessary\n\n        Arguments\n        ---------\n        transforms : list or tuple\n            each element in the list/tuple should be an affine transform.\n            currently supported transforms:\n                - Rotation()\n                - Translation()\n                - Shear()\n                - Zoom()\n\n        fill_mode : string in {\'constant\', \'nearest\'}\n            how to fill the empty space caused by the transform\n\n        fill_value : float\n            the value to fill the empty space with if fill_mode=\'constant\'\n\n        """"""\n        self.transforms = transforms\n        # set transforms to lazy so they only return the tform matrix\n        for t in self.transforms:\n            t.lazy = True\n        self.fill_mode = fill_mode\n        self.fill_value = fill_value\n        self.target_fill_mode = target_fill_mode\n        self.target_fill_value = target_fill_value\n\n    def __call__(self, x, y=None):\n        # collect all of the lazily returned tform matrices\n        tform_matrix = self.transforms[0](x)\n        for tform in self.transforms[1:]:\n            tform_matrix = np.dot(tform_matrix, tform(x)) \n\n        x = torch.from_numpy(apply_transform(x.numpy(), tform_matrix,\n            fill_mode=self.fill_mode, fill_value=self.fill_value))\n\n        if y:\n            y = torch.from_numpy(apply_transform(y.numpy(), tform_matrix,\n                fill_mode=self.target_fill_mode, fill_value=self.target_fill_value))\n            return x, y\n        else:\n            return x\n\n\nclass Rotation(object):\n\n    def __init__(self, \n                 rotation_range, \n                 fill_mode=\'constant\', \n                 fill_value=0., \n                 target_fill_mode=\'nearest\', \n                 target_fill_value=0., \n                 lazy=False):\n        """"""Randomly rotate an image between (-degrees, degrees). If the image\n        has multiple channels, the same rotation will be applied to each channel.\n\n        Arguments\n        ---------\n        rotation_range : integer or float\n            image will be rotated between (-degrees, degrees) degrees\n\n        fill_mode : string in {\'constant\', \'nearest\'}\n            how to fill the empty space caused by the transform\n\n        fill_value : float\n            the value to fill the empty space with if fill_mode=\'constant\'\n\n        lazy    : boolean\n            if true, perform the transform on the tensor and return the tensor\n            if false, only create the affine transform matrix and return that\n        """"""\n        self.rotation_range = rotation_range\n        self.fill_mode = fill_mode\n        self.fill_value = fill_value\n        self.target_fill_mode = target_fill_mode\n        self.target_fill_value = target_fill_value\n        self.lazy = lazy\n\n    def __call__(self, x, y=None):\n        degree = random.uniform(-self.rotation_range, self.rotation_range)\n        theta = math.pi / 180 * degree\n        rotation_matrix = np.array([[math.cos(theta), -math.sin(theta), 0],\n                                    [math.sin(theta), math.cos(theta), 0],\n                                    [0, 0, 1]])\n        if self.lazy:\n            return rotation_matrix\n        else:\n            x_transformed = torch.from_numpy(apply_transform(x.numpy(), rotation_matrix,\n                fill_mode=self.fill_mode, fill_value=self.fill_value))\n            if y:\n                y_transformed = torch.from_numpy(apply_transform(y.numpy(), rotation_matrix,\n                fill_mode=self.target_fill_mode, fill_value=self.target_fill_value))\n                return x_transformed, y_transformed\n            else:\n                return x_transformed\n\n\nclass Translation(object):\n\n    def __init__(self, \n                 translation_range, \n                 fill_mode=\'constant\',\n                 fill_value=0., \n                 target_fill_mode=\'nearest\', \n                 target_fill_value=0., \n                 lazy=False):\n        """"""Randomly translate an image some fraction of total height and/or\n        some fraction of total width. If the image has multiple channels,\n        the same translation will be applied to each channel.\n\n        Arguments\n        ---------\n        translation_range : two floats between [0, 1) \n            first value:\n                fractional bounds of total height to shift image\n                image will be horizontally shifted between \n                (-height_range * height_dimension, height_range * height_dimension)\n            second value:\n                fractional bounds of total width to shift image \n                Image will be vertically shifted between \n                (-width_range * width_dimension, width_range * width_dimension)\n\n        fill_mode : string in {\'constant\', \'nearest\'}\n            how to fill the empty space caused by the transform\n\n        fill_value : float\n            the value to fill the empty space with if fill_mode=\'constant\'\n\n        lazy    : boolean\n            if true, perform the transform on the tensor and return the tensor\n            if false, only create the affine transform matrix and return that\n        """"""\n        if isinstance(translation_range, float):\n            translation_range = (translation_range, translation_range)\n        self.height_range = translation_range[0]\n        self.width_range = translation_range[1]\n        self.fill_mode = fill_mode\n        self.fill_value = fill_value\n        self.target_fill_mode = target_fill_mode\n        self.target_fill_value = target_fill_value\n        self.lazy = lazy\n\n    def __call__(self, x, y=None):\n        # height shift\n        if self.height_range > 0:\n            tx = random.uniform(-self.height_range, self.height_range) * x.size(1)\n        else:\n            tx = 0\n        # width shift\n        if self.width_range > 0:\n            ty = random.uniform(-self.width_range, self.width_range) * x.size(2)\n        else:\n            ty = 0\n\n        translation_matrix = np.array([[1, 0, tx],\n                                       [0, 1, ty],\n                                       [0, 0, 1]])\n        if self.lazy:\n            return translation_matrix\n        else:\n            x_transformed = torch.from_numpy(apply_transform(x.numpy(), \n                translation_matrix, fill_mode=self.fill_mode, fill_value=self.fill_value))\n            if y:\n                y_transformed = torch.from_numpy(apply_transform(y.numpy(), translation_matrix,\n                fill_mode=self.target_fill_mode, fill_value=self.target_fill_value))\n                return x_transformed, y_transformed\n            else:\n                return x_transformed\n\n\nclass Shear(object):\n\n    def __init__(self, \n                 shear_range, \n                 fill_mode=\'constant\', \n                 fill_value=0., \n                 target_fill_mode=\'nearest\', \n                 target_fill_value=0., \n                 lazy=False):\n        """"""Randomly shear an image with radians (-shear_range, shear_range)\n\n        Arguments\n        ---------\n        shear_range : float\n            radian bounds on the shear transform\n        \n        fill_mode : string in {\'constant\', \'nearest\'}\n            how to fill the empty space caused by the transform\n\n        fill_value : float\n            the value to fill the empty space with if fill_mode=\'constant\'\n\n        lazy    : boolean\n            if true, perform the transform on the tensor and return the tensor\n            if false, only create the affine transform matrix and return that\n        """"""\n        self.shear_range = shear_range\n        self.fill_mode = fill_mode\n        self.fill_value = fill_value\n        self.target_fill_mode = target_fill_mode\n        self.target_fill_value = target_fill_value\n        self.lazy = lazy\n\n    def __call__(self, x, y=None):\n        shear = random.uniform(-self.shear_range, self.shear_range)\n        shear_matrix = np.array([[1, -math.sin(shear), 0],\n                                 [0, math.cos(shear), 0],\n                                 [0, 0, 1]])\n        if self.lazy:\n            return shear_matrix\n        else:\n            x_transformed = torch.from_numpy(apply_transform(x.numpy(), \n                shear_matrix, fill_mode=self.fill_mode, fill_value=self.fill_value))\n            if y:\n                y_transformed = torch.from_numpy(apply_transform(y.numpy(), shear_matrix,\n                fill_mode=self.target_fill_mode, fill_value=self.target_fill_value))\n                return x_transformed, y_transformed\n            else:\n                return x_transformed\n      \n\nclass Zoom(object):\n\n    def __init__(self, \n                 zoom_range, \n                 fill_mode=\'constant\', \n                 fill_value=0, \n                 target_fill_mode=\'nearest\', \n                 target_fill_value=0., \n                 lazy=False):\n        """"""Randomly zoom in and/or out on an image \n\n        Arguments\n        ---------\n        zoom_range : tuple or list with 2 values, both between (0, infinity)\n            lower and upper bounds on percent zoom. \n            Anything less than 1.0 will zoom in on the image, \n            anything greater than 1.0 will zoom out on the image.\n            e.g. (0.7, 1.0) will only zoom in, \n                 (1.0, 1.4) will only zoom out,\n                 (0.7, 1.4) will randomly zoom in or out\n\n        fill_mode : string in {\'constant\', \'nearest\'}\n            how to fill the empty space caused by the transform\n\n        fill_value : float\n            the value to fill the empty space with if fill_mode=\'constant\'\n\n        lazy    : boolean\n            if true, perform the transform on the tensor and return the tensor\n            if false, only create the affine transform matrix and return that\n        """"""\n        if not isinstance(zoom_range, list) and not isinstance(zoom_range, tuple):\n            raise ValueError(\'zoom_range must be tuple or list with 2 values\')\n        self.zoom_range = zoom_range\n        self.fill_mode = fill_mode\n        self.fill_value = fill_value\n        self.target_fill_mode = target_fill_mode\n        self.target_fill_value = target_fill_value\n        self.lazy = lazy\n\n    def __call__(self, x, y=None):\n        zx = random.uniform(self.zoom_range[0], self.zoom_range[1])\n        zy = random.uniform(self.zoom_range[0], self.zoom_range[1])\n        zoom_matrix = np.array([[zx, 0, 0],\n                                [0, zy, 0],\n                                [0, 0, 1]])\n        if self.lazy:\n            return zoom_matrix\n        else:\n            x_transformed = torch.from_numpy(apply_transform(x.numpy(), \n                zoom_matrix, fill_mode=self.fill_mode, fill_value=self.fill_value))\n            if y:\n                y_transformed = torch.from_numpy(apply_transform(y.numpy(), zoom_matrix,\n                fill_mode=self.target_fill_mode, fill_value=self.target_fill_value))\n                return x_transformed, y_transformed\n            else:\n                return x_transformed\n\n\n'"
supervised/analytical_helper_script.py,5,"b'\nfrom torch.autograd import Variable\nimport torch\nimport numpy as np\nfrom utils import to_one_hot\n\ncriterion = torch.nn.CrossEntropyLoss()\n\ndef mixup_criterion(y_a, y_b, lam):\n    return lambda criterion, pred: lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\ndef run_test_with_mixup(cuda, C, loader,mix_rate,mix_layer,num_trials=1):\n    correct = 0\n    total = 0\n\n    loss = 0.0\n    softmax = torch.nn.Softmax()\n    bce_loss = torch.nn.CrossEntropyLoss()#torch.nn.BCELoss()\n\n    lam = np.array(mix_rate)\n    lam = Variable(torch.from_numpy(np.array([lam]).astype(\'float32\')).cuda())\n\n    for i in range(0,num_trials):\n        for batch_idx, (data, target) in enumerate(loader):\n            if cuda:\n                data, target = data.cuda(), target.cuda()\n            data, target = Variable(data, volatile=True), Variable(target)\n\n            output,reweighted_target = C(data, lam=lam, target=target, layer_mix=mix_layer)\n\n            \'\'\'take original with probability lam.  First goal is to recover the target indices for the other batch.  \'\'\'\n\n            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.data.view_as(pred)).cpu().numpy().sum()\n            total += target.size(0)\n\n            \'\'\'These are the original targets in a one-hot space\'\'\'\n            target1_onehot = to_one_hot(target,10)\n\n            #print lam\n            #print reweighted_target[0:3]\n\n            target2 = (reweighted_target - target1_onehot*(lam)).max(1)[1]\n\n            #print ""reweighted target should put probability"", lam, ""on first set of indexed-values""\n            #print target[0:3]\n            #print target2[0:3]\n\n            loss += mixup_criterion(target, target2, lam)(bce_loss,output) * target.size(0)\n\n    #t_loss /= total\n    t_accuracy = 100. * correct / total\n    \n    average_loss = (loss / total)\n\n    #print ""Test with mixup"", mix_rate, ""on layer"", mix_layer, \', loss: \', average_loss\n    #print ""Accuracy"", t_accuracy\n\n    return t_accuracy, average_loss\n\n'"
supervised/helpers.py,0,"b""'''\nCreated on 16 Nov 2017\n\n'''\nfrom time import gmtime, strftime\nimport torch\nimport numpy as np\nimport pandas as pd\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\n\n\ndef copy_script_to_folder(caller_path, folder):\n    script_filename = caller_path.split('/')[-1]\n    script_relative_path = os.path.join(folder, script_filename)\n    # Copying script\n    shutil.copy(caller_path, script_relative_path)\n\ndef cyclic_lr(initial_lr,step,total_steps,num_cycles):\n    factor=np.ceil(float(total_steps)/num_cycles)\n    theta=np.pi*np.mod(step-1,factor)/factor\n    return (initial_lr/2)*(np.cos(theta)+1)\n\nif __name__ == '__main__':\n    lr_list=[]\n    for i in xrange(1000):\n        lr=cyclic_lr(0.1,i+1,1100,3)\n        lr_list.append(lr)\n    plt.plot(np.asarray(lr_list))\n    plt.show()\n        """
supervised/load_data.py,30,"b'\'\'\'\nCreated on 21 Nov 2017\n\n\n\'\'\'\nimport torch\nimport os\nfrom torchvision import datasets, transforms\nfrom affine_transforms import Rotation, Zoom\n\ndef per_image_standardization(x):\n    y = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n    mean = y.mean(dim=1, keepdim = True).expand_as(y)    \n    std = y.std(dim=1, keepdim = True).expand_as(y)      \n    adjusted_std = torch.max(std, 1.0/torch.sqrt(torch.cuda.FloatTensor([x.shape[1]*x.shape[2]*x.shape[3]])))    \n    y = (y- mean)/ adjusted_std\n    standarized_input =  y.view(x.shape[0],x.shape[1],x.shape[2],x.shape[3])  \n    return standarized_input  \n\ndef load_mnist(data_aug, batch_size, test_batch_size,cuda, data_target_dir):\n\n    if data_aug == 1:\n        hw_size = 24\n        transform_train = transforms.Compose([\n                            transforms.RandomCrop(hw_size),                \n                            transforms.ToTensor(),\n                            Rotation(15),                                            \n                            Zoom((0.85, 1.15)),       \n                            transforms.Normalize((0.1307,), (0.3081,))\n                       ])\n        transform_test = transforms.Compose([\n                            transforms.CenterCrop(hw_size),                       \n                            transforms.ToTensor(),\n                            transforms.Normalize((0.1307,), (0.3081,))\n                       ])\n    else:\n        hw_size = 28\n        transform_train = transforms.Compose([\n                            transforms.ToTensor(),       \n                            transforms.Normalize((0.1307,), (0.3081,))\n                       ])\n        transform_test = transforms.Compose([\n                            transforms.ToTensor(),\n                            transforms.Normalize((0.1307,), (0.3081,))\n                       ])\n    \n    \n    kwargs = {\'num_workers\': 0, \'pin_memory\': True} if cuda else {}       \n    \n    \n                \n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(data_target_dir, train=True, download=True, transform=transform_train),\n        batch_size=batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(data_target_dir, train=False, transform=transform_test),\n        batch_size=test_batch_size, shuffle=True, **kwargs)\n    \n    return train_loader, test_loader    \n\n\ndef load_data(data_aug, batch_size,workers,dataset, data_target_dir):\n    \n    if dataset == \'cifar10\':\n        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n    elif dataset == \'cifar100\':\n        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n        \n    elif dataset == \'svhn\':\n        mean = [x / 255 for x in [127.5, 127.5, 127.5]]\n        std = [x / 255 for x in [127.5, 127.5, 127.5]]\n    else:\n        assert False, ""Unknow dataset : {}"".format(dataset)\n    \n    if data_aug==1:\n        if dataset == \'svhn\':\n            train_transform = transforms.Compose(\n                                             [ transforms.RandomCrop(32, padding=2), transforms.ToTensor(),\n                                              transforms.Normalize(mean, std)])\n            test_transform = transforms.Compose(\n                                            [transforms.ToTensor(), transforms.Normalize(mean, std)])\n        else:\n            train_transform = transforms.Compose(\n                                                 [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n                                                  transforms.Normalize(mean, std)])\n            test_transform = transforms.Compose(\n                                                [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    else:\n        train_transform = transforms.Compose(\n                                             [ transforms.ToTensor(),\n                                              transforms.Normalize(mean, std)])\n        test_transform = transforms.Compose(\n                                            [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    if dataset == \'cifar10\':\n        train_data = datasets.CIFAR10(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.CIFAR10(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'cifar100\':\n        train_data = datasets.CIFAR100(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.CIFAR100(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 100\n    elif dataset == \'svhn\':\n        train_data = datasets.SVHN(data_target_dir, split=\'train\', transform=train_transform, download=True)\n        test_data = datasets.SVHN(data_target_dir, split=\'test\', transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'stl10\':\n        train_data = datasets.STL10(data_target_dir, split=\'train\', transform=train_transform, download=True)\n        test_data = datasets.STL10(data_target_dir, split=\'test\', transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'imagenet\':\n        assert False, \'Do not finish imagenet code\'\n    else:\n        assert False, \'Do not support dataset : {}\'.format(dataset)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,\n                         num_workers=workers, pin_memory=True)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False,\n                        num_workers=workers, pin_memory=True)\n    \n    return train_loader, test_loader, num_classes\n\n\ndef load_data_subset(data_aug, batch_size,workers,dataset, data_target_dir, labels_per_class=100, valid_labels_per_class = 500):\n    ## copied from GibbsNet_pytorch/load.py\n    import numpy as np\n    from functools import reduce\n    from operator import __or__\n    from torch.utils.data.sampler import SubsetRandomSampler\n        \n    if dataset == \'cifar10\':\n        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n    elif dataset == \'cifar100\':\n        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n    elif dataset == \'svhn\':\n        mean = [x / 255 for x in [127.5, 127.5, 127.5]]\n        std = [x / 255 for x in [127.5, 127.5, 127.5]]\n    elif dataset == \'tiny-imagenet-200\':\n        mean = [x / 255 for x in [127.5, 127.5, 127.5]]\n        std = [x / 255 for x in [127.5, 127.5, 127.5]]\n    elif dataset == \'mnist\':\n        pass \n    else:\n        assert False, ""Unknow dataset : {}"".format(dataset)\n    \n    if data_aug==1:\n        print (\'data aug\')\n        if dataset == \'svhn\':\n            train_transform = transforms.Compose(\n                                             [ transforms.RandomCrop(32, padding=2), transforms.ToTensor(),\n                                              transforms.Normalize(mean, std)])\n            test_transform = transforms.Compose(\n                                            [transforms.ToTensor(), transforms.Normalize(mean, std)])\n        elif dataset == \'mnist\':\n            hw_size = 24\n            train_transform = transforms.Compose([\n                                transforms.RandomCrop(hw_size),                \n                                transforms.ToTensor(),\n                                transforms.Normalize((0.1307,), (0.3081,))\n                           ])\n            test_transform = transforms.Compose([\n                                transforms.CenterCrop(hw_size),                       \n                                transforms.ToTensor(),\n                                transforms.Normalize((0.1307,), (0.3081,))\n                           ])\n        elif dataset == \'tiny-imagenet-200\':\n            train_transform = transforms.Compose(\n                                                 [transforms.RandomHorizontalFlip(),\n                                                  transforms.RandomCrop(64, padding=4),\n                                                  transforms.ToTensor(),\n                                                  transforms.Normalize(mean, std)])\n            test_transform = transforms.Compose(\n                                                [transforms.ToTensor(), transforms.Normalize(mean, std)])\n        else:    \n            train_transform = transforms.Compose(\n                                                 [transforms.RandomHorizontalFlip(),\n                                                  transforms.RandomCrop(32, padding=2),\n                                                  transforms.ToTensor(),\n                                                  transforms.Normalize(mean, std)])\n            test_transform = transforms.Compose(\n                                                [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    else:\n        print (\'no data aug\')\n        if dataset == \'mnist\':\n            hw_size = 28\n            train_transform = transforms.Compose([\n                                transforms.ToTensor(),       \n                                transforms.Normalize((0.1307,), (0.3081,))\n                           ])\n            test_transform = transforms.Compose([\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.1307,), (0.3081,))\n                           ])\n                \n        else:   \n            train_transform = transforms.Compose(\n                                                 [transforms.ToTensor(),\n                                                 transforms.Normalize(mean, std)])\n            test_transform = transforms.Compose(\n                                                [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    if dataset == \'cifar10\':\n        train_data = datasets.CIFAR10(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.CIFAR10(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'cifar100\':\n        train_data = datasets.CIFAR100(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.CIFAR100(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 100\n    elif dataset == \'svhn\':\n        train_data = datasets.SVHN(data_target_dir, split=\'train\', transform=train_transform, download=True)\n        test_data = datasets.SVHN(data_target_dir, split=\'test\', transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'mnist\':\n        train_data = datasets.MNIST(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.MNIST(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 10\n    #print (\'svhn\', train_data.labels.shape)\n    elif dataset == \'stl10\':\n        train_data = datasets.STL10(data_target_dir, split=\'train\', transform=train_transform, download=True)\n        test_data = datasets.STL10(data_target_dir, split=\'test\', transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'tiny-imagenet-200\':\n        train_root = os.path.join(data_target_dir, \'train\')  # this is path to training images folder\n        validation_root = os.path.join(data_target_dir, \'val/images\')  # this is path to validation images folder\n        train_data = datasets.ImageFolder(train_root, transform=train_transform)\n        test_data = datasets.ImageFolder(validation_root,transform=test_transform)\n        num_classes = 200\n    elif dataset == \'imagenet\':\n        assert False, \'Do not finish imagenet code\'\n    else:\n        assert False, \'Do not support dataset : {}\'.format(dataset)\n\n        \n    n_labels = num_classes\n    \n    def get_sampler(labels, n=None, n_valid= None):\n        # Only choose digits in n_labels\n        # n = number of labels per class for training\n        # n_val = number of lables per class for validation\n        #print type(labels)\n        #print (n_valid)\n        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n        # Ensure uniform distribution of labels\n        np.random.shuffle(indices)\n        \n        indices_valid = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n_valid] for i in range(n_labels)])\n        indices_train = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[n_valid:n_valid+n] for i in range(n_labels)])\n        indices_unlabelled = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:] for i in range(n_labels)])\n        #import pdb; pdb.set_trace()\n        #print (indices_train.shape)\n        #print (indices_valid.shape)\n        #print (indices_unlabelled.shape)\n        indices_train = torch.from_numpy(indices_train)\n        indices_valid = torch.from_numpy(indices_valid)\n        indices_unlabelled = torch.from_numpy(indices_unlabelled)\n        sampler_train = SubsetRandomSampler(indices_train)\n        sampler_valid = SubsetRandomSampler(indices_valid)\n        sampler_unlabelled = SubsetRandomSampler(indices_unlabelled)\n        return sampler_train, sampler_valid, sampler_unlabelled\n    \n    #print type(train_data.train_labels)\n    \n    # Dataloaders for MNIST\n    if dataset == \'svhn\':\n        train_sampler, valid_sampler, unlabelled_sampler = get_sampler(train_data.labels, labels_per_class, valid_labels_per_class)\n    elif dataset == \'mnist\':\n        train_sampler, valid_sampler, unlabelled_sampler = get_sampler(train_data.train_labels.numpy(), labels_per_class, valid_labels_per_class)\n    elif dataset == \'tiny-imagenet-200\':\n        pass\n    else: \n        train_sampler, valid_sampler, unlabelled_sampler = get_sampler(train_data.targets, labels_per_class, valid_labels_per_class)\n\n    if dataset == \'tiny-imagenet-200\':\n        labelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n        validation = None\n        unlabelled = None\n        test = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True)\n    else:\n        labelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler = train_sampler, shuffle=False, num_workers=workers, pin_memory=True)\n        validation = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler = valid_sampler, shuffle=False, num_workers=workers, pin_memory=True)\n        unlabelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler = unlabelled_sampler, shuffle=False, num_workers=workers, pin_memory=True)\n        test = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True)\n\n    return labelled, validation, unlabelled, test, num_classes\n\n### Alex code for data loading###\n""""""\ndef load_data_subset(data_aug, batch_size,workers,dataset, data_target_dir, validation_fraction, labels_per_class=100):\n    import numpy as np\n    from functools import reduce\n    from operator import __or__\n    from torch.utils.data.sampler import SubsetRandomSampler\n      \n    if dataset == \'cifar10\':\n        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n    elif dataset == \'cifar100\':\n        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n    else:\n        assert False, ""Unknow dataset : {}"".format(dataset)\n    \n    if data_aug==1:\n        train_transform = transforms.Compose(\n                                             [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n                                              transforms.Normalize(mean, std)])\n        test_transform = transforms.Compose(\n                                            [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    elif data_aug==""rotate_test"":\n        train_transform = transforms.Compose(\n                                             [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n                                              transforms.Normalize(mean, std)])\n        test_transform = transforms.Compose(\n                                            [transforms.ToTensor(), transforms.Normalize(mean, std),Shear(2.0)])\n\n    else:\n        train_transform = transforms.Compose(\n                                             [ transforms.ToTensor(),\n                                              transforms.Normalize(mean, std)])\n        test_transform = transforms.Compose(\n                                            [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    if dataset == \'cifar10\':\n        train_data = datasets.CIFAR10(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.CIFAR10(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'cifar100\':\n        train_data = datasets.CIFAR100(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.CIFAR100(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 100\n    elif dataset == \'svhn\':\n        train_data = datasets.SVHN(data_target_dir, split=\'train\', transform=train_transform, download=True)\n        test_data = datasets.SVHN(data_target_dir, split=\'test\', transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'stl10\':\n        train_data = datasets.STL10(data_target_dir, split=\'train\', transform=train_transform, download=True)\n        test_data = datasets.STL10(data_target_dir, split=\'test\', transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'imagenet\':\n        assert False, \'Do not finish imagenet code\'\n    else:\n        assert False, \'Do not support dataset : {}\'.format(dataset)\n\n        \n    #n_labels = num_classes\n    \n    def get_sampler(labels, n=None, validation_fraction=0.0):\n        # Only choose digits in n_labels\n        #print type(labels)\n        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(num_classes)]))\n        # Ensure uniform distribution of labels\n\n        if n is not None:\n            #n_total = int(0.5 + n / (1.0 - validation_fraction))\n            n_train = int(0.5 + n *(1.0 - validation_fraction)) # number of samples per class for training\n        else:\n            n_train = n\n        \n        np.random.shuffle(indices)\n        indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in range(num_classes)])\n        #print indices.shape\n        indices = torch.from_numpy(indices)\n        np.random.shuffle(indices)\n\n        if validation_fraction > 0.0:\n            #valid_cutoff = n_total*num_classes - n*num_classes\n            valid_cutoff = n*num_classes - n_train*num_classes \n            #print valid_cutoff\n            #valid_cutoff = int(indices.shape[0]*valid_percent)\n            print(""num_classes"", num_classes)\n            print(""n"", n)\n            print(""n_train"", n_train)\n            train_indices = indices[valid_cutoff:]\n            valid_indices = indices[:valid_cutoff]\n            print (""train indices"", train_indices.shape)\n            print (""valid indices"", valid_indices.shape)\n            train_sampler = SubsetRandomSampler(train_indices)\n            valid_sampler = SubsetRandomSampler(valid_indices)\n\n            return train_sampler, valid_sampler\n        else:\n\n            sampler = SubsetRandomSampler(indices)\n            return sampler\n    \n    #print type(train_data.train_labels)\n    \n    # Dataloaders for MNIST\n    if validation_fraction > 0.0:\n        train_sampler, valid_sampler = get_sampler(train_data.train_labels, labels_per_class, validation_fraction)\n        valid = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler,  num_workers=workers, pin_memory=True)\n    else:\n        train_sampler = get_sampler(train_data.train_labels, labels_per_class, validation_fraction)\n        valid = None\n\n    labelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler,  num_workers=workers, pin_memory=True)\n\n\n    unlabelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=get_sampler(train_data.train_labels),  num_workers=workers, pin_memory=True)\n    test = torch.utils.data.DataLoader(test_data, batch_size=batch_size, sampler=get_sampler(test_data.test_labels), num_workers=workers, pin_memory=True)\n\n    return labelled, valid, unlabelled, test, num_classes\n""""""\n\n\ndef load_data_subset_unpre(data_aug, batch_size,workers,dataset, data_target_dir, labels_per_class=100, valid_labels_per_class = 500):\n    ## loads the data without any preprocessing##\n    import numpy as np\n    from functools import reduce\n    from operator import __or__\n    from torch.utils.data.sampler import SubsetRandomSampler\n    \n    """"""\n    def per_image_standarize(x):\n        mean = x.mean()\n        std = x.std()\n        adjusted_std = torch.max(std, 1.0/torch.sqrt(torch.FloatTensor([x.shape[0]*x.shape[1]*x.shape[2]])))\n        standarized_input = (x- mean)/ adjusted_std\n        return standarized_input\n    """"""   \n    if data_aug==1:\n        print (\'data aug\')\n        if dataset == \'svhn\':\n            train_transform = transforms.Compose(\n                                            [transforms.RandomCrop(32, padding=2),\n                                            transforms.ToTensor(), \n                                            transforms.Lambda(lambda x : x.mul(255))\n                                            ])\n        else:    \n            train_transform = transforms.Compose(\n                                                [transforms.RandomHorizontalFlip(),\n                                                transforms.RandomCrop(32, padding=2),\n                                                transforms.ToTensor(), \n                                                transforms.Lambda(lambda x : x.mul(255))\n                                                ])\n        test_transform = transforms.Compose(\n                                            [transforms.ToTensor(),\n                                             transforms.Lambda(lambda x : x.mul(255))])\n    else:\n        print (\'no data aug\')\n        train_transform = transforms.Compose(\n                                            [transforms.ToTensor(),\n                                             transforms.Lambda(lambda x : x.mul(255))\n                                            ])\n        test_transform = transforms.Compose(\n                                            [transforms.ToTensor(),\n                                             transforms.Lambda(lambda x : x.mul(255))])\n    \n    if dataset == \'cifar10\':\n        train_data = datasets.CIFAR10(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.CIFAR10(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'cifar100\':\n        train_data = datasets.CIFAR100(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.CIFAR100(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 100\n    elif dataset == \'svhn\':\n        train_data = datasets.SVHN(data_target_dir, split=\'train\', transform=train_transform, download=True)\n        test_data = datasets.SVHN(data_target_dir, split=\'test\', transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'mnist\':\n        train_data = datasets.MNIST(data_target_dir, train=True, transform=train_transform, download=True)\n        test_data = datasets.MNIST(data_target_dir, train=False, transform=test_transform, download=True)\n        num_classes = 10\n    #print (\'svhn\', train_data.labels.shape)\n    elif dataset == \'stl10\':\n        train_data = datasets.STL10(data_target_dir, split=\'train\', transform=train_transform, download=True)\n        test_data = datasets.STL10(data_target_dir, split=\'test\', transform=test_transform, download=True)\n        num_classes = 10\n    elif dataset == \'imagenet\':\n        assert False, \'Do not finish imagenet code\'\n    else:\n        assert False, \'Do not support dataset : {}\'.format(dataset)\n        \n    n_labels = num_classes\n    \n    def get_sampler(labels, n=None, n_valid= None):\n        # Only choose digits in n_labels\n        # n = number of labels per class for training\n        # n_val = number of lables per class for validation\n        #print type(labels)\n        #print (n_valid)\n        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n        # Ensure uniform distribution of labels\n        np.random.shuffle(indices)\n        \n        indices_valid = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n_valid] for i in range(n_labels)])\n        indices_train = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[n_valid:n_valid+n] for i in range(n_labels)])\n        indices_unlabelled = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:] for i in range(n_labels)])\n        #print (indices_train.shape)\n        #print (indices_valid.shape)\n        #print (indices_unlabelled.shape)\n        indices_train = torch.from_numpy(indices_train)\n        indices_valid = torch.from_numpy(indices_valid)\n        indices_unlabelled = torch.from_numpy(indices_unlabelled)\n        sampler_train = SubsetRandomSampler(indices_train)\n        sampler_valid = SubsetRandomSampler(indices_valid)\n        sampler_unlabelled = SubsetRandomSampler(indices_unlabelled)\n        return sampler_train, sampler_valid, sampler_unlabelled\n    \n    #print type(train_data.train_labels)\n    \n    # Dataloaders for MNIST\n    if dataset == \'svhn\':\n        train_sampler, valid_sampler, unlabelled_sampler = get_sampler(train_data.labels, labels_per_class, valid_labels_per_class)\n    elif dataset == \'mnist\':\n        train_sampler, valid_sampler, unlabelled_sampler = get_sampler(train_data.train_labels.numpy(), labels_per_class, valid_labels_per_class)\n    else: \n        train_sampler, valid_sampler, unlabelled_sampler = get_sampler(train_data.train_labels, labels_per_class, valid_labels_per_class)\n    \n    labelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler = train_sampler, shuffle=False, num_workers=workers, pin_memory=True)\n    validation = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler = valid_sampler, shuffle=False, num_workers=workers, pin_memory=True)\n    unlabelled = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler = unlabelled_sampler,shuffle=False,  num_workers=workers, pin_memory=True)\n    test = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True)\n\n    return labelled, validation, unlabelled, test, num_classes\n\n\nif __name__ == \'__main__\':\n    labelled, validation, unlabelled, test, num_classes  = load_cifar10_subset(data_aug=1, batch_size=32,workers=1,dataset=\'cifar10\', data_target_dir=""/u/vermavik/data/DARC/cifar10"", labels_per_class=100, valid_labels_per_class = 500)\n    for (inputs, targets) in labelled:\n        import pdb; pdb.set_trace()\n        print (inputs)\n'"
supervised/logger.py,0,"b'# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc \nimport sys\nif sys.version[0] == \'2\':\n  from StringIO import StringIO  # Python 2.x\nelif sys.version[0] == \'3\':\n  from io import BytesIO       # Python 3.x\n\n\nclass Logger(object):\n  \n  def __init__(self, log_dir):\n    """"""Create a summary writer logging to log_dir.""""""\n    self.writer = tf.summary.FileWriter(log_dir)\n\n  def scalar_summary(self, tag, value, step):\n    """"""Log a scalar variable.""""""\n    summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n    self.writer.add_summary(summary, step)\n\n  def image_summary(self, tag, images, step):\n    """"""Log a list of images.""""""\n\n    img_summaries = []\n    for i, img in enumerate(images):\n      # Write the image to a string\n      try:\n        s = StringIO()\n      except:\n        s = BytesIO()\n      scipy.misc.toimage(img).save(s, format=""png"")\n\n      # Create an Image object\n      img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                 height=img.shape[0],\n                                 width=img.shape[1])\n      # Create a Summary value\n      img_summaries.append(tf.Summary.Value(tag=\'%s/%d\' % (tag, i), image=img_sum))\n\n    # Create and write Summary\n    summary = tf.Summary(value=img_summaries)\n    self.writer.add_summary(summary, step)\n    \n  def histo_summary(self, tag, values, step, bins=1000):\n    """"""Log a histogram of the tensor of values.""""""\n\n    # Create a histogram using numpy\n    counts, bin_edges = np.histogram(values, bins=bins)\n\n    # Fill the fields of the histogram proto\n    hist = tf.HistogramProto()\n    hist.min = float(np.min(values))\n    hist.max = float(np.max(values))\n    hist.num = int(np.prod(values.shape))\n    hist.sum = float(np.sum(values))\n    hist.sum_squares = float(np.sum(values**2))\n\n    # Drop the start of the first bin\n    bin_edges = bin_edges[1:]\n\n    # Add bin edges and counts\n    for edge in bin_edges:\n      hist.bucket_limit.append(edge)\n    for c in counts:\n      hist.bucket.append(c)\n\n    # Create and write Summary\n    summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n    self.writer.add_summary(summary, step)\n    self.writer.flush()\n'"
supervised/lr_scheduler.py,2,"b'#copied from https://github.com/Jiaming-Liu/pytorch-lr-scheduler/blob/master/lr_scheduler.py\n\nimport numpy as np\nimport warnings\nfrom torch.optim.optimizer import Optimizer\n\n\nclass ReduceLROnPlateau(object):\n    """"""Reduce learning rate when a metric has stopped improving.\n    Models often benefit from reducing the learning rate by a factor\n    of 2-10 once learning stagnates. This scheduler reads a metrics\n    quantity and if no improvement is seen for a \'patience\' number\n    of epochs, the learning rate is reduced.\n    \n    Args:\n        factor: factor by which the learning rate will\n            be reduced. new_lr = lr * factor\n        patience: number of epochs with no improvement\n            after which learning rate will be reduced.\n        verbose: int. 0: quiet, 1: update messages.\n        mode: one of {min, max}. In `min` mode,\n            lr will be reduced when the quantity\n            monitored has stopped decreasing; in `max`\n            mode it will be reduced when the quantity\n            monitored has stopped increasing.\n        epsilon: threshold for measuring the new optimum,\n            to only focus on significant changes.\n        cooldown: number of epochs to wait before resuming\n            normal operation after lr has been reduced.\n        min_lr: lower bound on the learning rate.\n        \n        \n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = ReduceLROnPlateau(optimizer, \'min\')\n        >>> for epoch in range(10):\n        >>>     train(...)\n        >>>     val_acc, val_loss = validate(...)\n        >>>     scheduler.step(val_loss, epoch)\n    """"""\n\n    def __init__(self, optimizer, mode=\'min\', factor=0.1, patience=10,\n                 verbose=0, epsilon=1e-4, cooldown=0, min_lr=0):\n        super(ReduceLROnPlateau, self).__init__()\n\n        if factor >= 1.0:\n            raise ValueError(\'ReduceLROnPlateau \'\n                             \'does not support a factor >= 1.0.\')\n        self.factor = factor\n        self.min_lr = min_lr\n        self.epsilon = epsilon\n        self.patience = patience\n        self.verbose = verbose\n        self.cooldown = cooldown\n        self.cooldown_counter = 0  # Cooldown counter.\n        self.monitor_op = None\n        self.wait = 0\n        self.best = 0\n        self.mode = mode\n        assert isinstance(optimizer, Optimizer)\n        self.optimizer = optimizer\n        self._reset()\n\n    def _reset(self):\n        """"""Resets wait counter and cooldown counter.\n        """"""\n        if self.mode not in [\'min\', \'max\']:\n            raise RuntimeError(\'Learning Rate Plateau Reducing mode %s is unknown!\')\n        if self.mode == \'min\' :\n            self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)\n            self.best = np.Inf\n        else:\n            self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)\n            self.best = -np.Inf\n        self.cooldown_counter = 0\n        self.wait = 0\n        self.lr_epsilon = self.min_lr * 1e-4\n\n    def reset(self):\n        self._reset()\n\n    def step(self, metrics, epoch):\n        current = metrics\n        if current is None:\n            warnings.warn(\'Learning Rate Plateau Reducing requires metrics available!\', RuntimeWarning)\n        else:\n            if self.in_cooldown():\n                self.cooldown_counter -= 1\n                self.wait = 0\n\n            if self.monitor_op(current, self.best):\n                self.best = current\n                self.wait = 0\n            elif not self.in_cooldown():\n                if self.wait >= self.patience:\n                    for param_group in self.optimizer.param_groups:\n                        old_lr = float(param_group[\'lr\'])\n                        if old_lr > self.min_lr + self.lr_epsilon:\n                            new_lr = old_lr * self.factor\n                            new_lr = max(new_lr, self.min_lr)\n                            param_group[\'lr\'] = new_lr\n                            if self.verbose > 0:\n                                print(\'\\nEpoch %05d: reducing learning rate to %s.\' % (epoch, new_lr))\n                            self.cooldown_counter = self.cooldown\n                            self.wait = 0\n                self.wait += 1\n\n    def in_cooldown(self):\n        return self.cooldown_counter > 0\n'"
supervised/main.py,17,"b'#!/usr/bin/env python\nfrom __future__ import division\n\nimport os, sys, shutil, time, random\nimport argparse\nfrom distutils.dir_util import copy_tree\nfrom shutil import rmtree\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nfrom utils import *\nimport models\n\nimport sys\nif sys.version_info[0] < 3:\n    import cPickle as pickle\nelse:\n    import _pickle as pickle\nimport numpy as np\nfrom collections import OrderedDict, Counter\nfrom load_data  import *\nfrom helpers import *\nfrom plots import *\nfrom analytical_helper_script import run_test_with_mixup\n#from attacks import run_test_adversarial, fgsm, pgd\n\n\nmodel_names = sorted(name for name in models.__dict__\n  if name.islower() and not name.startswith(""__"")\n  and callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'Trains ResNeXt on CIFAR or ImageNet\', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--dataset\', type=str, default=\'cifar10\', choices=[\'cifar10\', \'cifar100\', \'imagenet\', \'svhn\', \'stl10\', \'mnist\', \'tiny-imagenet-200\'], help=\'Choose between Cifar10/100 and ImageNet.\')\nparser.add_argument(\'--data_dir\', type = str, default = \'cifar10\',\n                        help=\'file where results are to be written\')\nparser.add_argument(\'--root_dir\', type = str, default = \'experiments\',\n                        help=\'folder where results are to be stored\')\nparser.add_argument(\'--labels_per_class\', type=int, default=5000, metavar=\'NL\',\n                    help=\'labels_per_class\')\nparser.add_argument(\'--valid_labels_per_class\', type=int, default=0, metavar=\'NL\',\n                    help=\'validation labels_per_class\')\n\nparser.add_argument(\'--arch\', metavar=\'ARCH\', default=\'resnext29_8_64\', choices=model_names, help=\'model architecture: \' + \' | \'.join(model_names) + \' (default: resnext29_8_64)\')\nparser.add_argument(\'--initial_channels\', type=int, default=64, choices=(16,64))\n# Optimization options\nparser.add_argument(\'--epochs\', type=int, default=300, help=\'Number of epochs to train.\')\nparser.add_argument(\'--train\', type=str, default = \'vanilla\', choices =[\'vanilla\',\'mixup\', \'mixup_hidden\',\'cutout\'])\nparser.add_argument(\'--mixup_alpha\', type=float, default=0.0, help=\'alpha parameter for mixup\')\nparser.add_argument(\'--cutout\', type=int, default=16, help=\'size of cut out\')\n\nparser.add_argument(\'--dropout\', action=\'store_true\', default=False,\n                    help=\'whether to use dropout or not in final layer\')\n#parser.add_argument(\'--batch_size\', type=int, default=128, help=\'Batch size.\')\nparser.add_argument(\'--batch_size\', type=int, default=100, help=\'Batch size.\')\nparser.add_argument(\'--learning_rate\', type=float, default=0.1, help=\'The Learning Rate.\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Momentum.\')\nparser.add_argument(\'--data_aug\', type=int, default=1)\nparser.add_argument(\'--adv_unpre\', action=\'store_true\', default=False,\n                     help= \'the adversarial examples will be calculated on real input space (not preprocessed)\')\nparser.add_argument(\'--decay\', type=float, default=0.0001, help=\'Weight decay (L2 penalty).\')\nparser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[150, 225], help=\'Decrease learning rate at these epochs.\')\nparser.add_argument(\'--gammas\', type=float, nargs=\'+\', default=[0.1, 0.1], help=\'LR is multiplied by gamma on schedule, number of gammas should be equal to schedule\')\n# Checkpoints\nparser.add_argument(\'--print_freq\', default=100, type=int, metavar=\'N\', help=\'print frequency (default: 200)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\', help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--start_epoch\', default=0, type=int, metavar=\'N\', help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--evaluate\', dest=\'evaluate\', action=\'store_true\', help=\'evaluate model on validation set\')\n# Acceleration\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\nparser.add_argument(\'--workers\', type=int, default=2, help=\'number of data loading workers (default: 2)\')\n# random seed\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--add_name\', type=str, default=\'\')\nparser.add_argument(\'--job_id\', type=str, default=\'\')\n\nargs = parser.parse_args()\nargs.use_cuda = args.ngpu>0 and torch.cuda.is_available()\n\nout_str = str(args)\nprint(out_str)\n\n\n""""""\nif args.manualSeed is None:\n    args.manualSeed = random.randint(1, 10000)\nrandom.seed(args.manualSeed)\ntorch.manual_seed(args.manualSeed)\n\nif args.use_cuda:\n    torch.cuda.manual_seed_all(args.manualSeed)\n""""""\ncudnn.benchmark = True\n\n\ndef experiment_name_non_mnist(dataset=\'cifar10\',\n                    arch=\'\',\n                    epochs=400,\n                    dropout=True,\n                    batch_size=64,\n                    lr=0.01,\n                    momentum=0.5,\n                    decay=0.0005,\n                    data_aug=1,\n                    train = \'vanilla\',\n                    mixup_alpha=0.0,\n                    job_id=None,\n                    add_name=\'\'):\n    exp_name = dataset\n    exp_name += \'_arch_\'+str(arch)\n    exp_name += \'_train_\'+str(train)\n    exp_name += \'_m_alpha_\'+str(mixup_alpha)\n    if dropout:\n        exp_name+=\'_do_\'+\'true\'\n    else:\n        exp_name+=\'_do_\'+\'False\'\n    exp_name += \'_eph_\'+str(epochs)\n    exp_name +=\'_bs_\'+str(batch_size)\n    exp_name += \'_lr_\'+str(lr)\n    exp_name += \'_mom_\'+str(momentum)\n    exp_name +=\'_decay_\'+str(decay)\n    exp_name += \'_data_aug_\'+str(data_aug)\n    if job_id!=None:\n        exp_name += \'_job_id_\'+str(job_id)\n    if add_name!=\'\':\n        exp_name += \'_add_name_\'+str(add_name)\n\n    # exp_name += strftime(""_%Y-%m-%d_%H:%M:%S"", gmtime())\n    print(\'experiement name: \' + exp_name)\n    return exp_name\n\n\ndef print_log(print_string, log):\n    print(""{}"".format(print_string))\n    log.write(\'{}\\n\'.format(print_string))\n    log.flush()\n\ndef save_checkpoint(state, is_best, save_path, filename):\n    filename = os.path.join(save_path, filename)\n    torch.save(state, filename)\n    if is_best:\n        bestname = os.path.join(save_path, \'model_best.pth.tar\')\n        shutil.copyfile(filename, bestname)\n\ndef adjust_learning_rate(optimizer, epoch, gammas, schedule):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = args.learning_rate\n    assert len(gammas) == len(schedule), ""length of gammas and schedule should be equal""\n    for (gamma, step) in zip(gammas, schedule):\n        if (epoch >= step):\n            lr = lr * gamma\n        else:\n            break\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\ndef mixup_criterion(y_a, y_b, lam):\n    return lambda criterion, pred: lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\nbce_loss = nn.BCELoss().cuda()\nsoftmax = nn.Softmax(dim=1).cuda()\ncriterion = nn.CrossEntropyLoss().cuda()\nmse_loss = nn.MSELoss().cuda()\n\n\ndef train(train_loader, model, optimizer, epoch, args, log):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # import pdb; pdb.set_trace()\n        # measure data loading time\n        #print (input)\n       \n        #unique, counts = np.unique(target.numpy(), return_counts=True)\n        #print (counts)\n        #print(Counter(target.numpy()))\n        #if i==100:\n        #    break\n        #import pdb; pdb.set_trace()\n        target = target.long()\n        input, target = input.cuda(), target.cuda()\n        data_time.update(time.time() - end)\n        #import pdb; pdb.set_trace()\n        ###  clean training####\n        if args.train == \'mixup\':\n            \n            input_var, target_var = Variable(input), Variable(target)\n            output, reweighted_target = model(input_var,target_var, mixup= True, mixup_alpha = args.mixup_alpha)\n            loss = bce_loss(softmax(output), reweighted_target)#mixup_criterion(target_a, target_b, lam)\n            \n            """"""\n            mixed_input, target_a, target_b, lam = mixup_data(input, target, args.mixup_alpha)\n            input_var, mixed_input_var, target_var, target_a_var, target_b_var = Variable(input),Variable(mixed_input), Variable(target), Variable(target_a), Variable(target_b)\n            \n            mixed_output = model(mixed_input_var)\n            output = model(input_var)\n            \n            loss_func = mixup_criterion(target_a_var, target_b_var, lam)\n            loss = loss_func(criterion, mixed_output)\n            """"""\n            \n        elif args.train== \'mixup_hidden\':\n            input_var, target_var = Variable(input), Variable(target)\n            output, reweighted_target = model(input_var, target_var, mixup_hidden= True, mixup_alpha = args.mixup_alpha)\n            loss = bce_loss(softmax(output), reweighted_target)#mixup_criterion(target_a, target_b, lam)\n            """"""\n            input_var, target_var = Variable(input), Variable(target)\n            mixed_output, target_a, target_b, lam = model(input_var, target_var, mixup_hidden = True,  mixup_alpha = args.mixup_alpha)\n            output = model(input_var)\n            \n            lam = lam[0]\n            target_a_one_hot = to_one_hot(target_a, args.num_classes)\n            target_b_one_hot = to_one_hot(target_b, args.num_classes)\n            mixed_target = target_a_one_hot * lam + target_b_one_hot * (1 - lam)\n            loss = bce_loss(softmax(output), mixed_target)\n            """"""\n        elif args.train == \'vanilla\':\n            input_var, target_var = Variable(input), Variable(target)\n            output, reweighted_target = model(input_var, target_var)\n            #loss = criterion(output, target_var)\n            #target_one_hot = to_one_hot(target_var, args.num_classes)\n            loss = bce_loss(softmax(output), reweighted_target)\n        \n        \n        elif args.train == \'cutout\':\n            cutout = Cutout(1, args.cutout)\n            cut_input = cutout.apply(input)\n                \n            input_var = torch.autograd.Variable(input)\n            target_var = torch.autograd.Variable(target)\n            cut_input_var = torch.autograd.Variable(cut_input)\n            #if dataname== \'mnist\':\n            #    input = input.view(-1, 784)\n            output, reweighted_target = model(cut_input_var, target_var)\n            #loss = criterion(output, target_var)\n            loss = bce_loss(softmax(output), reweighted_target)\n        \n        \n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec1.item(), input.size(0))\n        top5.update(prec5.item(), input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n\n        if i % args.print_freq == 0:\n            print_log(\'  Epoch: [{:03d}][{:03d}/{:03d}]   \'\n                \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})   \'\n                \'Data {data_time.val:.3f} ({data_time.avg:.3f})   \'\n                \'Loss {loss.val:.4f} ({loss.avg:.4f})   \'\n                \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})   \'\n                \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})   \'.format(\n                epoch, i, len(train_loader), batch_time=batch_time,\n                data_time=data_time, loss=losses, top1=top1, top5=top5) + time_string(), log)\n    \n                        \n      \n    print_log(\'  **Train** Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Error@1 {error1:.3f}\'.format(top1=top1, top5=top5, error1=100-top1.avg), log)\n    return top1.avg, top5.avg, losses.avg\n\n\ndef validate(val_loader, model, log):\n  losses = AverageMeter()\n  top1 = AverageMeter()\n  top5 = AverageMeter()\n\n  # switch to evaluate mode\n  model.eval()\n\n  for i, (input, target) in enumerate(val_loader):\n    if args.use_cuda:\n      target = target.cuda(async=True)\n      input = input.cuda()\n    with torch.no_grad():\n        input_var = Variable(input)\n        target_var = Variable(target)\n\n    # compute output\n    output = model(input_var)\n    loss = criterion(output, target_var)\n\n    # measure accuracy and record loss\n    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n    losses.update(loss.item(), input.size(0))\n    top1.update(prec1.item(), input.size(0))\n    top5.update(prec5.item(), input.size(0))\n\n  print_log(\'  **Test** Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Error@1 {error1:.3f} Loss: {losses.avg:.3f} \'.format(top1=top1, top5=top5, error1=100-top1.avg, losses=losses), log)\n\n  return top1.avg, losses.avg\n\nbest_acc = 0\ndef main():\n\n    ### set up the experiment directories########\n    exp_name=experiment_name_non_mnist(dataset=args.dataset,\n                    arch=args.arch,\n                    epochs=args.epochs,\n                    dropout=args.dropout,\n                    batch_size=args.batch_size,\n                    lr=args.learning_rate,\n                    momentum=args.momentum,\n                    decay= args.decay,\n                    data_aug=args.data_aug,\n                    train = args.train,\n                    mixup_alpha = args.mixup_alpha,\n                    job_id=args.job_id,\n                    add_name=args.add_name)\n    \n    exp_dir = args.root_dir+exp_name\n\n    if not os.path.exists(exp_dir):\n            os.makedirs(exp_dir)\n    \n    copy_script_to_folder(os.path.abspath(__file__), exp_dir)\n\n    result_png_path = os.path.join(exp_dir, \'results.png\')\n\n\n    global best_acc\n\n    log = open(os.path.join(exp_dir, \'log.txt\'.format(args.manualSeed)), \'w\')\n    print_log(\'save path : {}\'.format(exp_dir), log)\n    state = {k: v for k, v in args._get_kwargs()}\n    print_log(state, log)\n    print_log(""Random Seed: {}"".format(args.manualSeed), log)\n    print_log(""python version : {}"".format(sys.version.replace(\'\\n\', \' \')), log)\n    print_log(""torch  version : {}"".format(torch.__version__), log)\n    print_log(""cudnn  version : {}"".format(torch.backends.cudnn.version()), log)\n    \n    if args.adv_unpre:\n        per_img_std = True\n        train_loader, valid_loader, _ , test_loader, num_classes = load_data_subset_unpre(args.data_aug, args.batch_size, 2 ,args.dataset, args.data_dir,  labels_per_class = args.labels_per_class, valid_labels_per_class = args.valid_labels_per_class)\n    else:\n        per_img_std = False\n        train_loader, valid_loader, _ , test_loader, num_classes = load_data_subset(args.data_aug, args.batch_size, 2 ,args.dataset, args.data_dir,  labels_per_class = args.labels_per_class, valid_labels_per_class = args.valid_labels_per_class)\n    \n    if args.dataset == \'tiny-imagenet-200\':\n        stride = 2 \n    else:\n        stride = 1\n    #train_loader, valid_loader, _ , test_loader, num_classes = load_data_subset(args.data_aug, args.batch_size, 2, args.dataset, args.data_dir, 0.0, labels_per_class=5000)\n    print_log(""=> creating model \'{}\'"".format(args.arch), log)\n    net = models.__dict__[args.arch](num_classes,args.dropout,per_img_std, stride).cuda()\n    print_log(""=> network :\\n {}"".format(net), log)\n    args.num_classes = num_classes\n\n    #net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\n    optimizer = torch.optim.SGD(net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n                weight_decay=state[\'decay\'], nesterov=True)\n\n\n    recorder = RecorderMeter(args.epochs)\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print_log(""=> loading checkpoint \'{}\'"".format(args.resume), log)\n            checkpoint = torch.load(args.resume)\n            recorder = checkpoint[\'recorder\']\n            args.start_epoch = checkpoint[\'epoch\']\n            net.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            best_acc = recorder.max_accuracy(False)\n            print_log(""=> loaded checkpoint \'{}\' accuracy={} (epoch {})"" .format(args.resume, best_acc, checkpoint[\'epoch\']), log)\n        else:\n            print_log(""=> no checkpoint found at \'{}\'"".format(args.resume), log)\n    else:\n        print_log(""=> do not use any checkpoint for {} model"".format(args.arch), log)\n\n    if args.evaluate:\n        validate(test_loader, net, criterion, log)\n        return\n\n    # Main loop\n    start_time = time.time()\n    epoch_time = AverageMeter()\n    # Main loop\n    train_loss = []\n    train_acc=[]\n    test_loss=[]\n    test_acc=[]\n    \n    \n    for epoch in range(args.start_epoch, args.epochs):\n        current_learning_rate = adjust_learning_rate(optimizer, epoch, args.gammas, args.schedule)\n\n        need_hour, need_mins, need_secs = convert_secs2time(epoch_time.avg * (args.epochs-epoch))\n        need_time = \'[Need: {:02d}:{:02d}:{:02d}]\'.format(need_hour, need_mins, need_secs)\n\n        print_log(\'\\n==>>{:s} [Epoch={:03d}/{:03d}] {:s} [learning_rate={:6.4f}]\'.format(time_string(), epoch, args.epochs, need_time, current_learning_rate) \\\n                + \' [Best : Accuracy={:.2f}, Error={:.2f}]\'.format(recorder.max_accuracy(False), 100-recorder.max_accuracy(False)), log)\n\n        # train for one epoch\n        tr_acc, tr_acc5, tr_los  = train(train_loader, net, optimizer, epoch, args, log)\n\n        # evaluate on validation set\n        val_acc, val_los   = validate(test_loader, net, log)\n        \n        train_loss.append(tr_los)\n        train_acc.append(tr_acc)\n        test_loss.append(val_los)\n        test_acc.append(val_acc)\n        \n\n\n        dummy = recorder.update(epoch, tr_los, tr_acc, val_los, val_acc)\n\n        is_best = False\n        if val_acc > best_acc:\n            is_best = True\n            best_acc = val_acc\n\n        save_checkpoint({\n          \'epoch\': epoch + 1,\n          \'arch\': args.arch,\n          \'state_dict\': net.state_dict(),\n          \'recorder\': recorder,\n          \'optimizer\' : optimizer.state_dict(),\n        }, is_best, exp_dir, \'checkpoint.pth.tar\')\n\n        # measure elapsed time\n        epoch_time.update(time.time() - start_time)\n        start_time = time.time()\n        recorder.plot_curve(result_png_path)\n    \n        #import pdb; pdb.set_trace()\n        train_log = OrderedDict()\n        train_log[\'train_loss\'] = train_loss\n        train_log[\'train_acc\']=train_acc\n        train_log[\'test_loss\']=test_loss\n        train_log[\'test_acc\']=test_acc\n        \n                   \n        pickle.dump(train_log, open( os.path.join(exp_dir,\'log.pkl\'), \'wb\'))\n        plotting(exp_dir)\n    \n    log.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
supervised/plots.py,0,"b""'''\nCreated on 19 Oct 2017\n\n'''\nimport argparse\nimport sys\nif sys.version_info[0] < 3:\n    import cPickle as pickle\nelse:\n import _pickle as pickle\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nimport seaborn as sns\nsns.set(color_codes=True)\n\nplot_from_index=-10000\n\n\ndef plotting(exp_dir):\n    # Load the training log dictionary:\n    train_dict = pickle.load(open(os.path.join(exp_dir, 'log.pkl'), 'rb'))\n\n    ###########################################################\n    ### Make the vanilla train and test loss per epoch plot ###\n    ###########################################################\n   \n    plt.plot(np.asarray(train_dict['train_loss']), label='train_loss')\n    plt.plot(np.asarray(train_dict['test_loss']), label='test_loss')\n    \n    \n        \n    #plt.ylim(0,2000)\n    plt.xlabel('evaluation step')\n    plt.ylabel('metrics')\n    plt.tight_layout()\n    plt.legend(loc='upper right')\n    plt.savefig(os.path.join(exp_dir, 'loss.png' ))\n    plt.clf()\n    \n    \n   \n    ## accuracy###\n    plt.plot(np.asarray(train_dict['train_acc']), label='train_acc')\n    plt.plot(np.asarray(train_dict['test_acc']), label='test_acc')\n        \n    #plt.ylim(0,2000)\n    plt.xlabel('evaluation step')\n    plt.ylabel('metrics')\n    plt.tight_layout()\n    plt.legend(loc='upper right')\n    plt.savefig(os.path.join(exp_dir, 'acc.png' ))\n    plt.clf()\n    \n\n            \n    \n\n   \nif __name__ == '__main__':\n    plotting('temop')\n    #plotting_separate_theta('model', 'temp.pkl',3)"""
supervised/utils.py,4,"b'import os, sys, time\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\n\nclass AverageMeter(object):\n  """"""Computes and stores the average and current value""""""\n  def __init__(self):\n    self.reset()\n\n  def reset(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n\n  def update(self, val, n=1):\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n\n\nclass RecorderMeter(object):\n  """"""Computes and stores the minimum loss value and its epoch index""""""\n  def __init__(self, total_epoch):\n    self.reset(total_epoch)\n\n  def reset(self, total_epoch):\n    assert total_epoch > 0\n    self.total_epoch   = total_epoch\n    self.current_epoch = 0\n    self.epoch_losses  = np.zeros((self.total_epoch, 2), dtype=np.float32) # [epoch, train/val]\n    self.epoch_losses  = self.epoch_losses - 1\n\n    self.epoch_accuracy= np.zeros((self.total_epoch, 2), dtype=np.float32) # [epoch, train/val]\n    self.epoch_accuracy= self.epoch_accuracy\n\n  def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n    assert idx >= 0 and idx < self.total_epoch, \'total_epoch : {} , but update with the {} index\'.format(self.total_epoch, idx)\n    self.epoch_losses  [idx, 0] = train_loss\n    self.epoch_losses  [idx, 1] = val_loss\n    self.epoch_accuracy[idx, 0] = train_acc\n    self.epoch_accuracy[idx, 1] = val_acc\n    self.current_epoch = idx + 1\n    return self.max_accuracy(False) == val_acc\n\n  def max_accuracy(self, istrain):\n    if self.current_epoch <= 0: return 0\n    if istrain: return self.epoch_accuracy[:self.current_epoch, 0].max()\n    else:       return self.epoch_accuracy[:self.current_epoch, 1].max()\n  \n  def plot_curve(self, save_path):\n    title = \'the accuracy/loss curve of train/val\'\n    dpi = 80  \n    width, height = 1200, 800\n    legend_fontsize = 10\n    scale_distance = 48.8\n    figsize = width / float(dpi), height / float(dpi)\n\n    fig = plt.figure(figsize=figsize)\n    x_axis = np.array([i for i in range(self.total_epoch)]) # epochs\n    y_axis = np.zeros(self.total_epoch)\n\n    plt.xlim(0, self.total_epoch)\n    plt.ylim(0, 100)\n    interval_y = 5\n    interval_x = 5\n    plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n    plt.yticks(np.arange(0, 100 + interval_y, interval_y))\n    plt.grid()\n    plt.title(title, fontsize=20)\n    plt.xlabel(\'the training epoch\', fontsize=16)\n    plt.ylabel(\'accuracy\', fontsize=16)\n  \n    y_axis[:] = self.epoch_accuracy[:, 0]\n    plt.plot(x_axis, y_axis, color=\'g\', linestyle=\'-\', label=\'train-accuracy\', lw=2)\n    plt.legend(loc=4, fontsize=legend_fontsize)\n\n    y_axis[:] = self.epoch_accuracy[:, 1]\n    plt.plot(x_axis, y_axis, color=\'y\', linestyle=\'-\', label=\'valid-accuracy\', lw=2)\n    plt.legend(loc=4, fontsize=legend_fontsize)\n\n    \n    y_axis[:] = self.epoch_losses[:, 0]\n    plt.plot(x_axis, y_axis*50, color=\'g\', linestyle=\':\', label=\'train-loss-x50\', lw=2)\n    plt.legend(loc=4, fontsize=legend_fontsize)\n\n    y_axis[:] = self.epoch_losses[:, 1]\n    plt.plot(x_axis, y_axis*50, color=\'y\', linestyle=\':\', label=\'valid-loss-x50\', lw=2)\n    plt.legend(loc=4, fontsize=legend_fontsize)\n\n    if save_path is not None:\n      fig.savefig(save_path, dpi=dpi, bbox_inches=\'tight\')\n      print (\'---- save figure {} into {}\'.format(title, save_path))\n    plt.close(fig)\n    \n\ndef time_string():\n  ISOTIMEFORMAT=\'%Y-%m-%d %X\'\n  string = \'[{}]\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n  return string\n\ndef convert_secs2time(epoch_time):\n  need_hour = int(epoch_time / 3600)\n  need_mins = int((epoch_time - 3600*need_hour) / 60)\n  need_secs = int(epoch_time - 3600*need_hour - 60*need_mins)\n  return need_hour, need_mins, need_secs\n\ndef time_file_str():\n  ISOTIMEFORMAT=\'%Y-%m-%d\'\n  string = \'{}\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n  return string + \'-{}\'.format(random.randint(1, 10000))\n\ndef to_one_hot(inp,num_classes):\n    y_onehot = torch.FloatTensor(inp.size(0), num_classes)\n    y_onehot.zero_()\n\n    y_onehot.scatter_(1, inp.unsqueeze(1).data.cpu(), 1)\n    \n    return Variable(y_onehot.cuda(),requires_grad=False)\n\n\ndef mixup_process(out, target_reweighted, lam):\n    indices = np.random.permutation(out.size(0))\n    out = out*lam + out[indices]*(1-lam)\n    target_shuffled_onehot = target_reweighted[indices]\n    target_reweighted = target_reweighted * lam + target_shuffled_onehot * (1 - lam)\n    \n    #t1 = target.data.cpu().numpy()\n    #t2 = target[indices].data.cpu().numpy()\n    #print (np.sum(t1==t2))\n    return out, target_reweighted\n\n\ndef mixup_data(x, y, alpha):\n\n    \'\'\'Compute the mixup data. Return mixed inputs, pairs of targets, and lambda\'\'\'\n    if alpha > 0.:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).cuda()\n    mixed_x = lam * x + (1 - lam) * x[index,:]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef get_lambda(alpha=1.0):\n    \'\'\'Return lambda\'\'\'\n    if alpha > 0.:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.\n    return lam\n\nclass Cutout(object):\n    """"""Randomly mask out one or more patches from an image.\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n    """"""\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def apply(self, img):\n        """"""\n        Args:\n            img (Tensor): Tensor image of size (C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        """"""\n        h = img.size(2)\n        w = img.size(3)\n\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = int(np.clip(y - self.length / 2, 0, h))\n            y2 = int(np.clip(y + self.length / 2, 0, h))\n            x1 = int(np.clip(x - self.length / 2, 0, w))\n            x2 = int(np.clip(x + self.length / 2, 0, w))\n\n            mask[y1: y2, x1: x2] = 0.\n\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img).cuda()\n        img = img * mask\n\n        return img\n\n\ndef create_val_folder(data_set_path):\n    """"""\n    Used for Tiny-imagenet dataset\n    Copied from https://github.com/soumendukrg/BME595_DeepLearning/blob/master/Homework-06/train.py\n    This method is responsible for separating validation images into separate sub folders,\n    so that test and val data can be read by the pytorch dataloaders\n    """"""\n    path = os.path.join(data_set_path, \'val/images\')  # path where validation data is present now\n    filename = os.path.join(data_set_path, \'val/val_annotations.txt\')  # file where image2class mapping is present\n    fp = open(filename, ""r"")  # open file in read mode\n    data = fp.readlines()  # read line by line\n\n    # Create a dictionary with image names as key and corresponding classes as values\n    val_img_dict = {}\n    for line in data:\n        words = line.split(""\\t"")\n        val_img_dict[words[0]] = words[1]\n    fp.close()\n\n    # Create folder if not present, and move image into proper folder\n    for img, folder in val_img_dict.items():\n        newpath = (os.path.join(path, folder))\n        if not os.path.exists(newpath):  # check if folder exists\n            os.makedirs(newpath)\n\n        if os.path.exists(os.path.join(path, img)):  # Check if image exists in default directory\n            os.rename(os.path.join(path, img), os.path.join(newpath, img))\n\nif __name__ == ""__main__"":\n    create_val_folder(\'data/tiny-imagenet-200\')  # Call method to create validation image folders\n\n\n\n'"
gan/christorch/util.py,5,"b'from __future__ import print_function\n\nimport torch\nimport os\nimport numpy as np\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Dataset\n\n""""""\nCode borrowed from Pedro Costa\'s vess2ret repo:\nhttps://github.com/costapt/vess2ret\n""""""\ndef convert_to_rgb(img, is_grayscale=False):\n    """"""Given an image, make sure it has 3 channels and that it is between 0 and 1.""""""\n    if len(img.shape) != 3:\n        raise Exception(""""""Image must have 3 dimensions (channels x height x width). """"""\n                        """"""Given {0}"""""".format(len(img.shape)))\n    img_ch, _, _ = img.shape\n    if img_ch != 3 and img_ch != 1:\n        raise Exception(""""""Unsupported number of channels. """"""\n                        """"""Must be 1 or 3, given {0}."""""".format(img_ch))\n    imgp = img\n    if img_ch == 1:\n        imgp = np.repeat(img, 3, axis=0)\n    if not is_grayscale:\n        imgp = imgp * 127.5 + 127.5\n        imgp /= 255.\n    return np.clip(imgp.transpose((1, 2, 0)), 0, 1)\n\ndef rnd_crop(img, data_format=\'channels_last\'):\n    assert data_format in [\'channels_first\', \'channels_last\']\n    from skimage.transform import resize\n    if data_format == \'channels_last\':\n        # (h, w, f)\n        h, w = img.shape[0], img.shape[1]\n    else:\n        # (f, h, w)\n        h, w = img.shape[1], img.shape[2]\n    new_h, new_w = int(0.1*h + h), int(0.1*w + w)\n    # resize only works in the format (h, w, f)\n    if data_format == \'channels_first\':\n        img = img.swapaxes(0,1).swapaxes(1,2)\n    # resize\n    img_upsized = resize(img, (new_h, new_w))\n    # if channels first, swap back\n    if data_format == \'channels_first\':\n        img_upsized = img_upsized.swapaxes(2,1).swapaxes(1,0)\n    h_offset = np.random.randint(0, new_h-h)\n    w_offset = np.random.randint(0, new_w-w)\n    if data_format == \'channels_last\':\n        final_img = img_upsized[h_offset:h_offset+h, w_offset:w_offset+w, :]\n    else:\n        final_img = img_upsized[:, h_offset:h_offset+h, w_offset:w_offset+w]\n    return final_img\n\ndef min_max_then_tanh(img):\n    img2 = np.copy(img)\n    # old technique: if image is in [0,255],\n    # if grayscale then divide by 255 (putting it in [0,1]), or\n    # if colour then subtract 127.5 and divide by 127.5, putting it in [0,1].\n    # we do: (x - 0) / (255)\n    img2 = (img2 - np.min(img2)) / (np.max(img2) - np.min(img2))\n    img2 = (img2 - 0.5) / 0.5\n    return img2\n\ndef min_max(img):\n    img2 = np.copy(img)\n    img2 = (img2 - np.min(img2)) / (np.max(img2) - np.min(img2))\n    return img2\n\ndef zmuv(img):\n    img2 = np.copy(img)\n    for i in range(0, img2.shape[0]):\n        img2[i, ...] = (img2[i, ...] - np.mean(img2[i, ...])) / np.std(img2[i,...]) # zmuv\n    #print np.min(img2), np.max(img2)\n    return img2\n\ndef swap_axes(img):\n    img2 = np.copy(img)\n    img2 = img2.swapaxes(3,2).swapaxes(2,1)\n    return img2\n\ndef int_to_ord(labels, num_classes):\n    """"""\n    Convert integer label to ordinal label.\n    """"""\n    ords = np.ones((len(labels), num_classes-1))\n    for i in range(len(labels)):\n        if labels[i]==0:\n            continue\n        ords[i][0:labels[i]] *= 0.\n    return ords\n\ndef count_params(module, trainable_only=True):\n    """"""\n    Count the number of parameters in a module.\n    """"""\n    parameters = module.parameters()\n    if trainable_only:\n        parameters = filter(lambda p: p.requires_grad, parameters)\n    num = sum([np.prod(p.size()) for p in parameters])\n    return num\n\ndef get_gpu_mem_used():\n    try:\n        from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n        nvmlInit()\n        handle = nvmlDeviceGetHandleByIndex(0)\n        totalMemory = nvmlDeviceGetMemoryInfo(handle)\n        return totalMemory.used\n    except Exception:\n        return -1\n\n####################################################################\n\ndef test_image_folder(batch_size):\n    import torchvision.transforms as transforms\n    # loads images in [0,1] initially\n    loader = ImageFolder(root=""/data/lisa/data/beckhamc/dr-data/train_sample"",\n                         transform=transforms.Compose([\n                             transforms.Scale(256),\n                             transforms.CenterCrop(256),\n                             transforms.ToTensor(),\n                             transforms.Lambda(lambda img: (img-0.5)/0.5)\n                         ])\n    )\n    train_loader = DataLoader(\n        loader, batch_size=batch_size, shuffle=True, num_workers=1)\n    return train_loader\n\nimport torch.utils.data.dataset as dataset\n\nclass NumpyDataset(dataset.Dataset):\n    def __init__(self, X, ys, preprocessor=None, rnd_state=np.random.RandomState(0), reorder_channels=False):\n        self.X = X\n        if ys is not None:\n            # => we\'re dealing with classifier iterator\n            if type(ys) != list:\n                ys = [ys]\n            for y in ys:\n                assert len(y) == len(X)\n        self.ys = ys\n        self.N = len(X)\n        #self.keras_imgen = keras_imgen\n        self.preprocessor = preprocessor\n        self.rnd_state = rnd_state\n        self.reorder_channels = reorder_channels\n    def __getitem__(self, index):\n        xx = self.X[index]\n        if self.ys != None:\n            yy = []\n            for y in self.ys:\n                yy.append(y[index])\n            yy = np.asarray(yy)\n        if self.preprocessor is not None:\n            seed = self.rnd_state.randint(0, 100000)\n            #xx = self.keras_imgen.flow(xx[np.newaxis], None, batch_size=1, seed=seed, shuffle=False).next()[0]\n            xx = self.preprocessor(xx)\n        if self.reorder_channels:\n            xx = xx.swapaxes(2,1).swapaxes(1,0)\n        if self.ys is not None:\n            return xx, yy\n        else:\n            return xx\n    def __len__(self):\n        return self.N\n\nfrom PIL import Image\n\nclass DatasetFromFolder(Dataset):\n    """"""\n    Specify specific folders to load images from.\n\n    Notes\n    -----\n    Courtesy of:\n    https://github.com/togheppi/CycleGAN/blob/master/dataset.py\n    With some extra modifications done by me.\n    """"""\n    def __init__(self, image_dir, images=None, transform=None,\n                 append_label=None, bit16=False):\n        """"""\n        Parameters\n        ----------\n        image_dir: directory where the images are located\n        images: a list of images you want instead. If set to `None` it gets all\n          images in the directory specified by `image_dir`.\n        transform:\n        fliplr: enable left/right flip augmentation?\n        append_label: if an int is provided, then `__getitem__` will return\n          not just the image x, but (x,y), where y denotes the label. This\n          means that this iterator could also be used for classifiers.\n        """"""\n        super(DatasetFromFolder, self).__init__()\n        self.input_path = image_dir\n        if images is None:\n            self.image_filenames = [x for x in\n                                    sorted(os.listdir(self.input_path))]\n        else:\n            if type(images) != set:\n                images = set(images)\n            self.image_filenames = [os.path.join(image_dir, fname)\n                                    for fname in images]\n        self.transform = transform\n        self.append_label = append_label\n        self.bit16 = bit16\n    def __getitem__(self, index):\n        # Load Image\n        img_fn = os.path.join(self.input_path,\n                              self.image_filenames[index])\n        if self.bit16:\n            from skimage.io import imread\n            img = imread(img_fn)\n            img = img.astype(""float32"") / 65535.\n            img = (img*255.).astype(""uint8"")\n            img = Image.fromarray(img)\n        else:\n            img = Image.open(img_fn).convert(\'RGB\')\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.append_label is not None:\n            yy = np.asarray([self.append_label])\n            return img, yy\n        else:\n            return img\n    def __len__(self):\n        return len(self.image_filenames)\n\nclass ImagePool():\n    """"""\n    Used to implement a replay buffer for CycleGAN.\n\n    Notes\n    -----\n    Original code:\n    https://github.com/togheppi/CycleGAN/blob/master/utils.py\n    Unlike the original implementation, the buffer\'s images\n      are stored on the CPU, not the GPU. I am not sure whether\n      this is really worth the effort -- you\'d be doing a bit of\n      back and forth copying to/fro the GPU which could really\n      slow down the training loop I suspect.\n    """"""\n    def __init__(self, pool_size):\n        """"""\n        use_cuda: if `True`, store the buffer on GPU. This is\n          not recommended for large models!!!\n        """"""\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = [] # stored on cpu, NOT gpu\n\n    def query(self, images):\n        from torch.autograd import Variable\n        if self.pool_size == 0:\n            return images.detach()\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image.cpu())\n                return_images.append(image)\n            else:\n                p = np.random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = np.random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image.cpu()\n                    # since tmp is on cpu, cuda it when\n                    # we append it to return images\n                    return_images.append(tmp.cuda())\n                else:\n                    return_images.append(image)\n        return_images = torch.cat(return_images, 0)\n        return Variable(return_images)\n'"
gan/iterators/cifar10.py,2,"b'from torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom christorch.util import NumpyDataset\n\ndef get_iterators(bs):\n    def preproc(x):\n        return ((x/255.) - 0.5) / 0.5\n    data = CIFAR10(""./"", train=True, download=True)\n    X_train = data.train_data\n    train_ds = NumpyDataset(\n        X=X_train,\n        ys=None,\n        preprocessor=preproc,\n        reorder_channels=True\n    )\n    loader_train = DataLoader(train_ds, bs, shuffle=True)\n    return loader_train\n\ndef get_data(train=True):\n    data = CIFAR10(""./"", train=train, download=True)\n    if train:\n        return data.train_data\n    else:\n        return data.test_data\n\nif __name__ == \'__main__\':\n    #loader = get_iterators(32)\n    #for xx in loader:\n    #    assert xx.size(1) == 3\n\n    get_data(False)\n    \n    import pdb\n    pdb.set_trace()\n'"
gan/iterators/mnist.py,1,b'from christorch.gan.iterators.mnist import get_iterators\n'
gan/networks/base.py,0,"b'import torch\nfrom torch import nn\n\nclass discriminator(nn.Module):\n    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n    def __init__(self, input_width, input_height, input_dim, output_dim,\n                 out_nonlinearity=None):\n        super(discriminator, self).__init__()\n        assert out_nonlinearity in [None,\'sigmoid\']\n        self.input_height = input_height\n        self.input_width = input_width\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.conv = nn.Sequential(\n            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n        )\n        self.fc = [\n            nn.Linear(128 * (self.input_height // 4) * (self.input_width // 4), 1024),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(0.2),\n            nn.Linear(1024, self.output_dim),\n        ]\n        if out_nonlinearity == \'sigmoid\':\n            self.fc += [nn.Sigmoid()]\n        self.fc = nn.Sequential(*self.fc)\n        initialize_weights(self)\n\n    def forward(self, input):\n        """"""Returns a list of outputs where the last one is D(x)\n        and others are hidden states""""""\n        x = self.conv(input)\n        x = x.view(-1, 128 * (self.input_height // 4) * (self.input_width // 4))\n        preconv = x\n        x = self.fc(x)\n        return preconv, x\n\n    def partial_forward(self, preconv):\n        return self.fc(preconv)\n\ndef initialize_weights(net):\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.ConvTranspose2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n'"
gan/networks/mnist.py,1,"b""from christorch.gan.architectures import gen\nfrom networks import base # importlib works from root dir\n\ndef get_network(z_dim):\n    gen_fn = gen.generator(input_width=28,\n                           input_height=28,\n                           output_dim=1,\n                           z_dim=z_dim)\n    disc_fn = base.discriminator(input_width=28,\n                            input_height=28,\n                            input_dim=1,\n                            output_dim=1,\n                            out_nonlinearity='sigmoid')\n    return gen_fn, disc_fn\n"""
gan/networks/model_resnet.py,1,"b'# ResNet generator and discriminator\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom networks.spectral_normalization import SpectralNorm\nimport numpy as np\n\n\nchannels = 3\n\n\nclass ResBlockGenerator(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResBlockGenerator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n        nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n        nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n\n        self.model = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),\n            self.conv1,\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            self.conv2\n            )\n        self.bypass = nn.Sequential()\n        if stride != 1:\n            self.bypass = nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        return self.model(x) + self.bypass(x)\n\n\nclass ResBlockDiscriminator(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1, spec_norm=False):\n        super(ResBlockDiscriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n        nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n        nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n        if spec_norm:\n            self.spec_norm = SpectralNorm\n        else:\n            self.spec_norm = lambda x: x\n\n        if stride == 1:\n            self.model = nn.Sequential(\n                nn.ReLU(),\n                self.spec_norm(self.conv1),\n                nn.ReLU(),\n                self.spec_norm(self.conv2)\n                )\n        else:\n            self.model = nn.Sequential(\n                nn.ReLU(),\n                self.spec_norm(self.conv1),\n                nn.ReLU(),\n                self.spec_norm(self.conv2),\n                nn.AvgPool2d(2, stride=stride, padding=0)\n                )\n        self.bypass = nn.Sequential()\n        if stride != 1:\n\n            self.bypass_conv = nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)\n            nn.init.xavier_uniform(self.bypass_conv.weight.data, np.sqrt(2))\n\n            self.bypass = nn.Sequential(\n                self.spec_norm(self.bypass_conv),\n                nn.AvgPool2d(2, stride=stride, padding=0)\n            )\n            # if in_channels == out_channels:\n            #     self.bypass = nn.AvgPool2d(2, stride=stride, padding=0)\n            # else:\n            #     self.bypass = nn.Sequential(\n            #         SpectralNorm(nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)),\n            #         nn.AvgPool2d(2, stride=stride, padding=0)\n            #     )\n        else:\n            self.bypass = nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)\n\n\n    def forward(self, x):\n        return self.model(x) + self.bypass(x)\n\n# special ResBlock just for the first layer of the discriminator\nclass FirstResBlockDiscriminator(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1, spec_norm=False):\n        super(FirstResBlockDiscriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n        self.bypass_conv = nn.Conv2d(in_channels, out_channels, 1, 1, padding=0)\n        nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n        nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n        nn.init.xavier_uniform(self.bypass_conv.weight.data, np.sqrt(2))\n\n        if spec_norm:\n            self.spec_norm = SpectralNorm\n        else:\n            self.spec_norm = lambda x: x\n\n        # we don\'t want to apply ReLU activation to raw image before convolution transformation.\n        self.model = nn.Sequential(\n            self.spec_norm(self.conv1),\n            nn.ReLU(),\n            self.spec_norm(self.conv2),\n            nn.AvgPool2d(2)\n            )\n        self.bypass = nn.Sequential(\n            nn.AvgPool2d(2),\n            self.spec_norm(self.bypass_conv),\n        )\n\n    def forward(self, x):\n        return self.model(x) + self.bypass(x)\n\nGEN_SIZE=128\nDISC_SIZE=128\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim):\n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n\n        self.dense = nn.Linear(self.z_dim, 4 * 4 * GEN_SIZE)\n        self.final = nn.Conv2d(GEN_SIZE, channels, 3, stride=1, padding=1)\n        nn.init.xavier_uniform(self.dense.weight.data, 1.)\n        nn.init.xavier_uniform(self.final.weight.data, 1.)\n\n        self.model = nn.Sequential(\n            ResBlockGenerator(GEN_SIZE, GEN_SIZE, stride=2),\n            ResBlockGenerator(GEN_SIZE, GEN_SIZE, stride=2),\n            ResBlockGenerator(GEN_SIZE, GEN_SIZE, stride=2),\n            nn.BatchNorm2d(GEN_SIZE),\n            nn.ReLU(),\n            self.final,\n            nn.Tanh())\n\n    def forward(self, z):\n        return self.model(self.dense(z).view(-1, GEN_SIZE, 4, 4))\n\nclass Discriminator(nn.Module):\n    def __init__(self, spec_norm=False, sigmoid=False):\n        super(Discriminator, self).__init__()\n\n        if spec_norm:\n            self.spec_norm = SpectralNorm\n        else:\n            self.spec_norm = lambda x : x\n\n        self._init = FirstResBlockDiscriminator(channels, DISC_SIZE,\n                                                stride=2, spec_norm=spec_norm)\n        self._init2 = ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                             stride=2, spec_norm=spec_norm)\n        self.model = nn.Sequential(\n                ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                      spec_norm=spec_norm),\n                ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                      spec_norm=spec_norm),\n                nn.ReLU(),\n                nn.AvgPool2d(8),\n            )\n        self.fc = nn.Linear(DISC_SIZE, 1)\n        nn.init.xavier_uniform(self.fc.weight.data, 1.)\n        self.fc = self.spec_norm(self.fc)\n        if sigmoid:\n            self.sigm = nn.Sigmoid()\n        self.sigmoid = sigmoid\n\n    def forward(self, x):\n        """"""\n        Return a tuple of intermediate states, and also\n          the final output.\n        """"""\n        init = self._init(x)\n        init2 = self._init2(init)\n        return (init, init2), self.partial_forward(init2, 1)\n\n    def partial_forward(self, hs, idx):\n        """"""\n        Compute the output of the discriminator, given\n          either the result of the first or second layer.\n        """"""\n        assert idx in [0,1]\n        if idx == 0:\n            # hs == the result of self._init\n            result = self.fc(self.model(self._init2(hs)).view(-1,DISC_SIZE))\n        else:\n            # hs == the result of self._init2\n            result = self.fc(self.model(hs).view(-1,DISC_SIZE))\n        if self.sigmoid:\n            return self.sigm(result)\n        else:\n            return result\n        \n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(sigmoid=True)\n    return gen, disc\n\nif __name__ == \'__main__\':\n    gen, disc = get_network(128)\n    print(gen)\n    print(disc)\n'"
gan/networks/model_resnet_bnd.py,1,"b'# ResNet generator and discriminator\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n\nchannels = 3\n\n\nclass ResBlockGenerator(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResBlockGenerator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n        nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n        nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n\n        self.model = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),\n            self.conv1,\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            self.conv2\n            )\n        self.bypass = nn.Sequential()\n        if stride != 1:\n            self.bypass = nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        return self.model(x) + self.bypass(x)\n\n\nclass ResBlockDiscriminator(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResBlockDiscriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n        nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n        nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n\n        if stride == 1:\n            self.model = nn.Sequential(\n                nn.ReLU(),\n                self.conv1,\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(),\n                self.conv2,\n                nn.BatchNorm2d(out_channels)\n            )\n        else:\n            self.model = nn.Sequential(\n                nn.ReLU(),\n                self.conv1,\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(),\n                self.conv2,\n                nn.BatchNorm2d(out_channels),\n                nn.AvgPool2d(2, stride=stride, padding=0)\n            )\n        self.bypass = nn.Sequential()\n        if stride != 1:\n\n            self.bypass_conv = nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)\n            nn.init.xavier_uniform(self.bypass_conv.weight.data, np.sqrt(2))\n\n            self.bypass = nn.Sequential(\n                self.bypass_conv,\n                nn.AvgPool2d(2, stride=stride, padding=0)\n            )\n            # if in_channels == out_channels:\n            #     self.bypass = nn.AvgPool2d(2, stride=stride, padding=0)\n            # else:\n            #     self.bypass = nn.Sequential(\n            #         SpectralNorm(nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)),\n            #         nn.AvgPool2d(2, stride=stride, padding=0)\n            #     )\n\n\n    def forward(self, x):\n        return self.model(x) + self.bypass(x)\n\n# special ResBlock just for the first layer of the discriminator\nclass FirstResBlockDiscriminator(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(FirstResBlockDiscriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n        self.bypass_conv = nn.Conv2d(in_channels, out_channels, 1, 1, padding=0)\n        nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n        nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n        nn.init.xavier_uniform(self.bypass_conv.weight.data, np.sqrt(2))\n\n        # we don\'t want to apply ReLU activation to raw image before convolution transformation.\n        self.model = nn.Sequential(\n            self.conv1,\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            self.conv2,\n            nn.BatchNorm2d(out_channels),\n            nn.AvgPool2d(2)\n            )\n        self.bypass = nn.Sequential(\n            nn.AvgPool2d(2),\n            self.bypass_conv\n        )\n\n    def forward(self, x):\n        return self.model(x) + self.bypass(x)\n\nGEN_SIZE=128\nDISC_SIZE=128\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim):\n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n\n        self.dense = nn.Linear(self.z_dim, 4 * 4 * GEN_SIZE)\n        self.final = nn.Conv2d(GEN_SIZE, channels, 3, stride=1, padding=1)\n        nn.init.xavier_uniform(self.dense.weight.data, 1.)\n        nn.init.xavier_uniform(self.final.weight.data, 1.)\n\n        self.model = nn.Sequential(\n            ResBlockGenerator(GEN_SIZE, GEN_SIZE, stride=2),\n            ResBlockGenerator(GEN_SIZE, GEN_SIZE, stride=2),\n            ResBlockGenerator(GEN_SIZE, GEN_SIZE, stride=2),\n            nn.BatchNorm2d(GEN_SIZE),\n            nn.ReLU(),\n            self.final,\n            nn.Tanh())\n\n    def forward(self, z):\n        return self.model(self.dense(z).view(-1, GEN_SIZE, 4, 4))\n\nclass Discriminator(nn.Module):\n    def __init__(self, sigmoid=False):\n        super(Discriminator, self).__init__()\n\n        self._init = FirstResBlockDiscriminator(channels, DISC_SIZE,\n                                                stride=2)\n        self._init2 = ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                             stride=2)\n        self.model = nn.Sequential(\n                ResBlockDiscriminator(DISC_SIZE, DISC_SIZE),\n                ResBlockDiscriminator(DISC_SIZE, DISC_SIZE),\n                nn.ReLU(),\n                nn.AvgPool2d(8),\n            )\n        self.fc = nn.Linear(DISC_SIZE, 1)\n        nn.init.xavier_uniform(self.fc.weight.data, 1.)\n        if sigmoid:\n            self.sigm = nn.Sigmoid()\n        self.sigmoid = sigmoid\n\n    def forward(self, x):\n        """"""\n        Return a tuple of intermediate states, and also\n          the final output.\n        """"""\n        init = self._init(x)\n        init2 = self._init2(init)\n        return (init, init2), self.partial_forward(init2, 1)\n\n    def partial_forward(self, hs, idx):\n        """"""\n        Compute the output of the discriminator, given\n          either the result of the first or second layer.\n        """"""\n        assert idx in [0,1]\n        if idx == 0:\n            # hs == the result of self._init\n            result = self.fc(self.model(self._init2(hs)).view(-1,DISC_SIZE))\n        else:\n            # hs == the result of self._init2\n            result = self.fc(self.model(hs).view(-1,DISC_SIZE))\n        if self.sigmoid:\n            return self.sigm(result)\n        else:\n            return result\n        \n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(sigmoid=False)\n    return gen, disc\n\nif __name__ == \'__main__\':\n    gen, disc = get_network(128)\n    print(gen)\n    print(disc)\n'"
gan/networks/model_resnet_nosigm.py,0,"b'from networks.model_resnet import Generator, Discriminator\n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(sigmoid=False)\n    return gen, disc\n'"
gan/networks/model_resnet_old.py,1,"b""# ResNet generator and discriminator\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom networks.spectral_normalization import SpectralNorm\nimport numpy as np\n\n\nchannels = 3\n\n\nclass ResBlockGenerator(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResBlockGenerator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n        nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n        nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n\n        self.model = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2),\n            self.conv1,\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            self.conv2\n            )\n        self.bypass = nn.Sequential()\n        if stride != 1:\n            self.bypass = nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        return self.model(x) + self.bypass(x)\n\n\nclass ResBlockDiscriminator(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1, spec_norm=False):\n        super(ResBlockDiscriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n        nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n        nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n        if spec_norm:\n            self.spec_norm = SpectralNorm\n        else:\n            self.spec_norm = lambda x: x\n\n        if stride == 1:\n            self.model = nn.Sequential(\n                nn.ReLU(),\n                self.spec_norm(self.conv1),\n                nn.ReLU(),\n                self.spec_norm(self.conv2)\n                )\n        else:\n            self.model = nn.Sequential(\n                nn.ReLU(),\n                self.spec_norm(self.conv1),\n                nn.ReLU(),\n                self.spec_norm(self.conv2),\n                nn.AvgPool2d(2, stride=stride, padding=0)\n                )\n        self.bypass = nn.Sequential()\n        if stride != 1:\n\n            self.bypass_conv = nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)\n            nn.init.xavier_uniform(self.bypass_conv.weight.data, np.sqrt(2))\n\n            self.bypass = nn.Sequential(\n                self.spec_norm(self.bypass_conv),\n                nn.AvgPool2d(2, stride=stride, padding=0)\n            )\n            # if in_channels == out_channels:\n            #     self.bypass = nn.AvgPool2d(2, stride=stride, padding=0)\n            # else:\n            #     self.bypass = nn.Sequential(\n            #         SpectralNorm(nn.Conv2d(in_channels,out_channels, 1, 1, padding=0)),\n            #         nn.AvgPool2d(2, stride=stride, padding=0)\n            #     )\n\n\n    def forward(self, x):\n        return self.model(x) + self.bypass(x)\n\n# special ResBlock just for the first layer of the discriminator\nclass FirstResBlockDiscriminator(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1, spec_norm=False):\n        super(FirstResBlockDiscriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, padding=1)\n        self.bypass_conv = nn.Conv2d(in_channels, out_channels, 1, 1, padding=0)\n        nn.init.xavier_uniform(self.conv1.weight.data, 1.)\n        nn.init.xavier_uniform(self.conv2.weight.data, 1.)\n        nn.init.xavier_uniform(self.bypass_conv.weight.data, np.sqrt(2))\n\n        if spec_norm:\n            self.spec_norm = SpectralNorm\n        else:\n            self.spec_norm = lambda x: x\n\n        # we don't want to apply ReLU activation to raw image before convolution transformation.\n        self.model = nn.Sequential(\n            self.spec_norm(self.conv1),\n            nn.ReLU(),\n            self.spec_norm(self.conv2),\n            nn.AvgPool2d(2)\n            )\n        self.bypass = nn.Sequential(\n            nn.AvgPool2d(2),\n            self.spec_norm(self.bypass_conv),\n        )\n\n    def forward(self, x):\n        return self.model(x) + self.bypass(x)\n\nGEN_SIZE=128\nDISC_SIZE=128\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim):\n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n\n        self.dense = nn.Linear(self.z_dim, 4 * 4 * GEN_SIZE)\n        self.final = nn.Conv2d(GEN_SIZE, channels, 3, stride=1, padding=1)\n        nn.init.xavier_uniform(self.dense.weight.data, 1.)\n        nn.init.xavier_uniform(self.final.weight.data, 1.)\n\n        self.model = nn.Sequential(\n            ResBlockGenerator(GEN_SIZE, GEN_SIZE, stride=2),\n            ResBlockGenerator(GEN_SIZE, GEN_SIZE, stride=2),\n            ResBlockGenerator(GEN_SIZE, GEN_SIZE, stride=2),\n            nn.BatchNorm2d(GEN_SIZE),\n            nn.ReLU(),\n            self.final,\n            nn.Tanh())\n\n    def forward(self, z):\n        return self.model(self.dense(z).view(-1, GEN_SIZE, 4, 4))\n\nclass Discriminator(nn.Module):\n    def __init__(self, spec_norm=False, sigmoid=False):\n        super(Discriminator, self).__init__()\n\n        if spec_norm:\n            self.spec_norm = SpectralNorm\n        else:\n            self.spec_norm = lambda x : x\n\n        self.model = nn.Sequential(\n                FirstResBlockDiscriminator(channels, DISC_SIZE,\n                                           stride=2, spec_norm=spec_norm),\n                ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                      stride=2, spec_norm=spec_norm),\n                ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                      spec_norm=spec_norm),\n                ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                      spec_norm=spec_norm),\n                nn.ReLU(),\n                nn.AvgPool2d(8),\n            )\n        self.fc = nn.Linear(DISC_SIZE, 1)\n        nn.init.xavier_uniform(self.fc.weight.data, 1.)\n        self.fc = self.spec_norm(self.fc)\n        if sigmoid:\n            self.sigm = nn.Sigmoid()\n        self.sigmoid = sigmoid\n\n    def forward(self, x):\n        pre_fc = self.model(x).view(-1,DISC_SIZE)\n        result = self.fc(pre_fc)\n        if self.sigmoid:\n            return pre_fc, self.sigm(result)\n        else:\n            return pre_fc, result\n\n    def partial_forward(self, x):\n        if self.sigmoid:\n            return self.sigm(self.fc(x))\n        else:\n            return self.fc(x)\n        \n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(sigmoid=True)\n    return gen, disc\n\nif __name__ == '__main__':\n    gen, disc = get_network(128)\n    print(gen)\n    print(disc)\n"""
gan/networks/model_resnet_old_specnorm.py,0,"b'from networks.model_resnet_old import Generator, Discriminator\n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(spec_norm=True, sigmoid=True)\n    return gen, disc\n'"
gan/networks/model_resnet_ppresnet.py,0,"b'from torch import nn\nfrom networks.model_resnet import (FirstResBlockDiscriminator,\n                                   Generator,\n                                   ResBlockDiscriminator,\n                                   DISC_SIZE,\n                                   SpectralNorm,\n                                   channels)\n\n\nclass Discriminator(nn.Module):\n    """"""\n    This discriminator differs from the one in model_resnet\n      in that we have a preprocessor conv right before the\n      main model.\n    """"""\n    def __init__(self, spec_norm=False, sigmoid=False):\n        super(Discriminator, self).__init__()\n\n        if spec_norm:\n            self.spec_norm = SpectralNorm\n        else:\n            self.spec_norm = lambda x : x\n\n        #self.preproc = nn.Conv2d(channels, 16, 3, stride=1, padding=1)\n        self.preproc = ResBlockDiscriminator(3, 16, stride=1,\n                                             spec_norm=spec_norm)\n        \n        self.model = nn.Sequential(\n            FirstResBlockDiscriminator(16, DISC_SIZE, stride=2,\n                                       spec_norm=spec_norm),\n            ResBlockDiscriminator(DISC_SIZE, DISC_SIZE, stride=2,\n                                  spec_norm=spec_norm),\n            ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                  spec_norm=spec_norm),\n            ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                  spec_norm=spec_norm),\n            nn.ReLU(),\n            nn.AvgPool2d(8),\n        )\n        self.fc = nn.Linear(DISC_SIZE, 1)\n        nn.init.xavier_uniform(self.fc.weight.data, 1.)\n        self.fc = self.spec_norm(self.fc)\n        if sigmoid:\n            self.sigm = nn.Sigmoid()\n        self.sigmoid = sigmoid\n\n    def forward(self, x):\n        preproc = self.preproc(x)\n        return preproc, self.partial_forward(preproc)\n\n    def partial_forward(self, preproc, idx=-1):\n        pre_fc = self.model(preproc).view(-1,DISC_SIZE)\n        result = self.fc(pre_fc)\n        if self.sigmoid:\n            return self.sigm(result)\n        else:\n            return result\n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(sigmoid=True)\n    return gen, disc\n\nif __name__ == \'__main__\':\n    a,b = get_network(62)\n    print(a)\n    print(b)\n'"
gan/networks/model_resnet_preproc.py,0,"b'from torch import nn\nfrom networks.model_resnet import (FirstResBlockDiscriminator,\n                                   Generator,\n                                   ResBlockDiscriminator,\n                                   DISC_SIZE,\n                                   SpectralNorm,\n                                   channels)\n\n\nclass Discriminator(nn.Module):\n    """"""\n    This discriminator differs from the one in model_resnet\n      in that we have a preprocessor conv right before the\n      main model.\n    """"""\n    def __init__(self, spec_norm=False, sigmoid=False):\n        super(Discriminator, self).__init__()\n\n        if spec_norm:\n            self.spec_norm = SpectralNorm\n        else:\n            self.spec_norm = lambda x : x\n\n        self.preproc = nn.Conv2d(channels, 16, 3, stride=1, padding=1)\n        \n        self.model = nn.Sequential(\n            FirstResBlockDiscriminator(16, DISC_SIZE, stride=2,\n                                       spec_norm=spec_norm),\n            ResBlockDiscriminator(DISC_SIZE, DISC_SIZE, stride=2,\n                                  spec_norm=spec_norm),\n            ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                  spec_norm=spec_norm),\n            ResBlockDiscriminator(DISC_SIZE, DISC_SIZE,\n                                  spec_norm=spec_norm),\n            nn.ReLU(),\n            nn.AvgPool2d(8),\n        )\n        self.fc = nn.Linear(DISC_SIZE, 1)\n        nn.init.xavier_uniform(self.fc.weight.data, 1.)\n        self.fc = self.spec_norm(self.fc)\n        if sigmoid:\n            self.sigm = nn.Sigmoid()\n        self.sigmoid = sigmoid\n\n    def forward(self, x):\n        preproc = self.preproc(x)\n        return preproc, self.partial_forward(preproc)\n\n    def partial_forward(self, preproc, idx=-1):\n        pre_fc = self.model(preproc).view(-1,DISC_SIZE)\n        result = self.fc(pre_fc)\n        if self.sigmoid:\n            return self.sigm(result)\n        else:\n            return result\n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(sigmoid=True)\n    return gen, disc\n'"
gan/networks/model_resnet_specnorm.py,0,"b""from networks.model_resnet import Generator, Discriminator\n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(spec_norm=True, sigmoid=True)\n    return gen, disc\n\nif __name__ == '__main__':\n    dd, gg = get_network(z_dim=62)\n    print(dd)\n    print(gg)\n"""
gan/networks/model_resnet_specnorm_nosigm.py,0,"b""from networks.model_resnet import Generator, Discriminator\n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(spec_norm=True, sigmoid=False)\n    return gen, disc\n\nif __name__ == '__main__':\n    dd, gg = get_network(z_dim=62)\n    print(dd)\n    print(gg)\n"""
gan/networks/model_resnet_specnorm_preproc.py,0,"b""from networks.model_resnet_preproc import (Generator,\n                                           Discriminator)\n\ndef get_network(z_dim):\n    gen = Generator(z_dim)\n    disc = Discriminator(spec_norm=True,\n                         sigmoid=True)\n    return gen, disc\n\nif __name__ == '__main__':\n    a1, a2 = get_network(62)\n    print(a1)\n    print(a2)\n"""
gan/networks/models_64x64.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\n\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, num_features, eps=1e-5, affine=True):\n        super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))\n\n    def forward(self, x):\n        # This implementation is too slow!!!\n\n        shape = [-1] + [1] * (x.dim() - 1)\n        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n        std = x.view(x.size(0), -1).std(1).view(*shape)\n        y = (x - mean) / (std + self.eps)\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            y = self.gamma.view(*shape) * y + self.beta.view(*shape)\n        return y\n\n\nclass Generator(nn.Module):\n\n    def __init__(self, in_dim, dim=64):\n        super(Generator, self).__init__()\n\n        def dconv_bn_relu(in_dim, out_dim):\n            return nn.Sequential(\n                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n                                   padding=2, output_padding=1, bias=False),\n                nn.BatchNorm2d(out_dim),\n                nn.ReLU())\n\n        self.l1 = nn.Sequential(\n            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n            nn.BatchNorm1d(dim * 8 * 4 * 4),\n            nn.ReLU())\n        self.l2_5 = nn.Sequential(\n            dconv_bn_relu(dim * 8, dim * 4),\n            dconv_bn_relu(dim * 4, dim * 2),\n            #dconv_bn_relu(dim * 2, dim),\n            nn.ConvTranspose2d(dim*2, 3, 5, 2, padding=2, output_padding=1),\n            nn.Tanh())\n\n    def forward(self, x):\n        y = self.l1(x)\n        y = y.view(y.size(0), -1, 4, 4)\n        y = self.l2_5(y)\n        return y\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, in_dim, dim=64):\n        super(Discriminator, self).__init__()\n\n        def conv_bn_lrelu(in_dim, out_dim):\n            return nn.Sequential(\n                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n                nn.BatchNorm2d(out_dim),\n                nn.LeakyReLU(0.2))\n\n        self.ls = nn.Sequential(\n            nn.Conv2d(in_dim, dim, 5, 2, 2), nn.LeakyReLU(0.2),\n            conv_bn_lrelu(dim, dim * 2),\n            conv_bn_lrelu(dim * 2, dim * 4),\n            conv_bn_lrelu(dim * 4, dim * 8),\n            nn.Conv2d(dim * 8, 1, 4))\n\n    def forward(self, x):\n        y = self.ls(x)\n        y = y.view(-1)\n        return y\n\n\nclass DiscriminatorWGANGP(nn.Module):\n\n    def __init__(self, in_dim, dim=64):\n        super(DiscriminatorWGANGP, self).__init__()\n\n        def conv_ln_lrelu(in_dim, out_dim):\n            return nn.Sequential(\n                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n                # Since there is no effective implementation of LayerNorm,\n                # we use InstanceNorm2d instead of LayerNorm here.\n                nn.InstanceNorm2d(out_dim, affine=True),\n                nn.LeakyReLU(0.2))\n\n        self.ls = nn.Sequential(\n            nn.Conv2d(in_dim, dim, 5, 2, 2), nn.LeakyReLU(0.2),\n            conv_ln_lrelu(dim, dim * 2),\n            conv_ln_lrelu(dim * 2, dim * 4),\n            #conv_ln_lrelu(dim * 4, dim * 8),\n            nn.Conv2d(dim * 4, 1, 4),\n            nn.Sigmoid())\n\n    def forward(self, x):\n        y = self.ls(x)\n        y = y.view(-1, 1)\n        return None, y\n    \n\ndef get_network(z_dim):\n  g = Generator(in_dim=z_dim)\n  d = DiscriminatorWGANGP(in_dim=3)\n  return g,d\n'"
gan/networks/spectral_normalization.py,7,"b'import torch\nfrom torch.optim.optimizer import Optimizer, required\n\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch import Tensor\nfrom torch.nn import Parameter\n\ndef l2normalize(v, eps=1e-12):\n    return v / (v.norm() + eps)\n\n\nclass SpectralNorm(nn.Module):\n    def __init__(self, module, name=\'weight\', power_iterations=1):\n        super(SpectralNorm, self).__init__()\n        self.module = module\n        self.name = name\n        self.power_iterations = power_iterations\n        if not self._made_params():\n            self._make_params()\n\n    def _update_u_v(self):\n        u = getattr(self.module, self.name + ""_u"")\n        v = getattr(self.module, self.name + ""_v"")\n        w = getattr(self.module, self.name + ""_bar"")\n\n        height = w.data.shape[0]\n        for _ in range(self.power_iterations):\n            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n\n        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n        sigma = u.dot(w.view(height, -1).mv(v))\n        setattr(self.module, self.name, w / sigma.expand_as(w))\n\n    def _made_params(self):\n        try:\n            u = getattr(self.module, self.name + ""_u"")\n            v = getattr(self.module, self.name + ""_v"")\n            w = getattr(self.module, self.name + ""_bar"")\n            return True\n        except AttributeError:\n            return False\n\n\n    def _make_params(self):\n        w = getattr(self.module, self.name)\n\n        height = w.data.shape[0]\n        width = w.view(height, -1).data.shape[1]\n\n        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n        u.data = l2normalize(u.data)\n        v.data = l2normalize(v.data)\n        w_bar = Parameter(w.data)\n\n        del self.module._parameters[self.name]\n\n        self.module.register_parameter(self.name + ""_u"", u)\n        self.module.register_parameter(self.name + ""_v"", v)\n        self.module.register_parameter(self.name + ""_bar"", w_bar)\n\n\n    def forward(self, *args):\n        self._update_u_v()\n        return self.module.forward(*args)\n'"
gan/preprocessing/image.py,0,"b'""""""Fairly basic set of tools for real-time data augmentation on image data.\n\nCan easily be extended to include new transformations,\nnew preprocessing methods, etc...\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport re\nfrom scipy import linalg\nimport scipy.ndimage as ndi\nfrom six.moves import range\nimport os\nimport threading\nimport warnings\nimport multiprocessing.pool\nfrom functools import partial\n\nfrom .. import backend as K\nfrom ..utils.data_utils import Sequence\n\ntry:\n    from PIL import Image as pil_image\nexcept ImportError:\n    pil_image = None\n\n\nif pil_image is not None:\n    _PIL_INTERPOLATION_METHODS = {\n        \'nearest\': pil_image.NEAREST,\n        \'bilinear\': pil_image.BILINEAR,\n        \'bicubic\': pil_image.BICUBIC,\n    }\n    # These methods were only introduced in version 3.4.0 (2016).\n    if hasattr(pil_image, \'HAMMING\'):\n        _PIL_INTERPOLATION_METHODS[\'hamming\'] = pil_image.HAMMING\n    if hasattr(pil_image, \'BOX\'):\n        _PIL_INTERPOLATION_METHODS[\'box\'] = pil_image.BOX\n    # This method is new in version 1.1.3 (2013).\n    if hasattr(pil_image, \'LANCZOS\'):\n        _PIL_INTERPOLATION_METHODS[\'lanczos\'] = pil_image.LANCZOS\n\n\ndef random_rotation(x, rg, row_axis=1, col_axis=2, channel_axis=0,\n                    fill_mode=\'nearest\', cval=0.):\n    """"""Performs a random rotation of a Numpy image tensor.\n\n    # Arguments\n        x: Input tensor. Must be 3D.\n        rg: Rotation range, in degrees.\n        row_axis: Index of axis for rows in the input tensor.\n        col_axis: Index of axis for columns in the input tensor.\n        channel_axis: Index of axis for channels in the input tensor.\n        fill_mode: Points outside the boundaries of the input\n            are filled according to the given mode\n            (one of `{\'constant\', \'nearest\', \'reflect\', \'wrap\'}`).\n        cval: Value used for points outside the boundaries\n            of the input if `mode=\'constant\'`.\n\n    # Returns\n        Rotated Numpy image tensor.\n    """"""\n    theta = np.deg2rad(np.random.uniform(-rg, rg))\n    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n                                [np.sin(theta), np.cos(theta), 0],\n                                [0, 0, 1]])\n\n    h, w = x.shape[row_axis], x.shape[col_axis]\n    transform_matrix = transform_matrix_offset_center(rotation_matrix, h, w)\n    x = apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n    return x\n\n\ndef random_shift(x, wrg, hrg, row_axis=1, col_axis=2, channel_axis=0,\n                 fill_mode=\'nearest\', cval=0.):\n    """"""Performs a random spatial shift of a Numpy image tensor.\n\n    # Arguments\n        x: Input tensor. Must be 3D.\n        wrg: Width shift range, as a float fraction of the width.\n        hrg: Height shift range, as a float fraction of the height.\n        row_axis: Index of axis for rows in the input tensor.\n        col_axis: Index of axis for columns in the input tensor.\n        channel_axis: Index of axis for channels in the input tensor.\n        fill_mode: Points outside the boundaries of the input\n            are filled according to the given mode\n            (one of `{\'constant\', \'nearest\', \'reflect\', \'wrap\'}`).\n        cval: Value used for points outside the boundaries\n            of the input if `mode=\'constant\'`.\n\n    # Returns\n        Shifted Numpy image tensor.\n    """"""\n    h, w = x.shape[row_axis], x.shape[col_axis]\n    tx = np.random.uniform(-hrg, hrg) * h\n    ty = np.random.uniform(-wrg, wrg) * w\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n\n    transform_matrix = translation_matrix  # no need to do offset\n    x = apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n    return x\n\n\ndef random_shear(x, intensity, row_axis=1, col_axis=2, channel_axis=0,\n                 fill_mode=\'nearest\', cval=0.):\n    """"""Performs a random spatial shear of a Numpy image tensor.\n\n    # Arguments\n        x: Input tensor. Must be 3D.\n        intensity: Transformation intensity in degrees.\n        row_axis: Index of axis for rows in the input tensor.\n        col_axis: Index of axis for columns in the input tensor.\n        channel_axis: Index of axis for channels in the input tensor.\n        fill_mode: Points outside the boundaries of the input\n            are filled according to the given mode\n            (one of `{\'constant\', \'nearest\', \'reflect\', \'wrap\'}`).\n        cval: Value used for points outside the boundaries\n            of the input if `mode=\'constant\'`.\n\n    # Returns\n        Sheared Numpy image tensor.\n    """"""\n    shear = np.deg2rad(np.random.uniform(-intensity, intensity))\n    shear_matrix = np.array([[1, -np.sin(shear), 0],\n                             [0, np.cos(shear), 0],\n                             [0, 0, 1]])\n\n    h, w = x.shape[row_axis], x.shape[col_axis]\n    transform_matrix = transform_matrix_offset_center(shear_matrix, h, w)\n    x = apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n    return x\n\n\ndef random_zoom(x, zoom_range, row_axis=1, col_axis=2, channel_axis=0,\n                fill_mode=\'nearest\', cval=0.):\n    """"""Performs a random spatial zoom of a Numpy image tensor.\n\n    # Arguments\n        x: Input tensor. Must be 3D.\n        zoom_range: Tuple of floats; zoom range for width and height.\n        row_axis: Index of axis for rows in the input tensor.\n        col_axis: Index of axis for columns in the input tensor.\n        channel_axis: Index of axis for channels in the input tensor.\n        fill_mode: Points outside the boundaries of the input\n            are filled according to the given mode\n            (one of `{\'constant\', \'nearest\', \'reflect\', \'wrap\'}`).\n        cval: Value used for points outside the boundaries\n            of the input if `mode=\'constant\'`.\n\n    # Returns\n        Zoomed Numpy image tensor.\n\n    # Raises\n        ValueError: if `zoom_range` isn\'t a tuple.\n    """"""\n    if len(zoom_range) != 2:\n        raise ValueError(\'`zoom_range` should be a tuple or list of two floats. \'\n                         \'Received arg: \', zoom_range)\n\n    if zoom_range[0] == 1 and zoom_range[1] == 1:\n        zx, zy = 1, 1\n    else:\n        zx, zy = np.random.uniform(zoom_range[0], zoom_range[1], 2)\n    zoom_matrix = np.array([[zx, 0, 0],\n                            [0, zy, 0],\n                            [0, 0, 1]])\n\n    h, w = x.shape[row_axis], x.shape[col_axis]\n    transform_matrix = transform_matrix_offset_center(zoom_matrix, h, w)\n    x = apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n    return x\n\n\ndef random_channel_shift(x, intensity, channel_axis=0):\n    x = np.rollaxis(x, channel_axis, 0)\n    min_x, max_x = np.min(x), np.max(x)\n    channel_images = [np.clip(x_channel + np.random.uniform(-intensity, intensity), min_x, max_x)\n                      for x_channel in x]\n    x = np.stack(channel_images, axis=0)\n    x = np.rollaxis(x, 0, channel_axis + 1)\n    return x\n\n\ndef transform_matrix_offset_center(matrix, x, y):\n    o_x = float(x) / 2 + 0.5\n    o_y = float(y) / 2 + 0.5\n    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n    return transform_matrix\n\n\ndef apply_transform(x,\n                    transform_matrix,\n                    channel_axis=0,\n                    fill_mode=\'nearest\',\n                    cval=0.):\n    """"""Apply the image transformation specified by a matrix.\n\n    # Arguments\n        x: 2D numpy array, single image.\n        transform_matrix: Numpy array specifying the geometric transformation.\n        channel_axis: Index of axis for channels in the input tensor.\n        fill_mode: Points outside the boundaries of the input\n            are filled according to the given mode\n            (one of `{\'constant\', \'nearest\', \'reflect\', \'wrap\'}`).\n        cval: Value used for points outside the boundaries\n            of the input if `mode=\'constant\'`.\n\n    # Returns\n        The transformed version of the input.\n    """"""\n    x = np.rollaxis(x, channel_axis, 0)\n    final_affine_matrix = transform_matrix[:2, :2]\n    final_offset = transform_matrix[:2, 2]\n    channel_images = [ndi.interpolation.affine_transform(\n        x_channel,\n        final_affine_matrix,\n        final_offset,\n        order=0,\n        mode=fill_mode,\n        cval=cval) for x_channel in x]\n    x = np.stack(channel_images, axis=0)\n    x = np.rollaxis(x, 0, channel_axis + 1)\n    return x\n\n\ndef flip_axis(x, axis):\n    x = np.asarray(x).swapaxes(axis, 0)\n    x = x[::-1, ...]\n    x = x.swapaxes(0, axis)\n    return x\n\n\ndef array_to_img(x, data_format=None, scale=True):\n    """"""Converts a 3D Numpy array to a PIL Image instance.\n\n    # Arguments\n        x: Input Numpy array.\n        data_format: Image data format.\n        scale: Whether to rescale image values\n            to be within [0, 255].\n\n    # Returns\n        A PIL Image instance.\n\n    # Raises\n        ImportError: if PIL is not available.\n        ValueError: if invalid `x` or `data_format` is passed.\n    """"""\n    if pil_image is None:\n        raise ImportError(\'Could not import PIL.Image. \'\n                          \'The use of `array_to_img` requires PIL.\')\n    x = np.asarray(x, dtype=K.floatx())\n    if x.ndim != 3:\n        raise ValueError(\'Expected image array to have rank 3 (single image). \'\n                         \'Got array with shape:\', x.shape)\n\n    if data_format is None:\n        data_format = K.image_data_format()\n    if data_format not in {\'channels_first\', \'channels_last\'}:\n        raise ValueError(\'Invalid data_format:\', data_format)\n\n    # Original Numpy array x has format (height, width, channel)\n    # or (channel, height, width)\n    # but target PIL image has format (width, height, channel)\n    if data_format == \'channels_first\':\n        x = x.transpose(1, 2, 0)\n    if scale:\n        x = x + max(-np.min(x), 0)\n        x_max = np.max(x)\n        if x_max != 0:\n            x /= x_max\n        x *= 255\n    if x.shape[2] == 3:\n        # RGB\n        return pil_image.fromarray(x.astype(\'uint8\'), \'RGB\')\n    elif x.shape[2] == 1:\n        # grayscale\n        return pil_image.fromarray(x[:, :, 0].astype(\'uint8\'), \'L\')\n    else:\n        raise ValueError(\'Unsupported channel number: \', x.shape[2])\n\n\ndef img_to_array(img, data_format=None):\n    """"""Converts a PIL Image instance to a Numpy array.\n\n    # Arguments\n        img: PIL Image instance.\n        data_format: Image data format.\n\n    # Returns\n        A 3D Numpy array.\n\n    # Raises\n        ValueError: if invalid `img` or `data_format` is passed.\n    """"""\n    if data_format is None:\n        data_format = K.image_data_format()\n    if data_format not in {\'channels_first\', \'channels_last\'}:\n        raise ValueError(\'Unknown data_format: \', data_format)\n    # Numpy array x has format (height, width, channel)\n    # or (channel, height, width)\n    # but original PIL image has format (width, height, channel)\n    x = np.asarray(img, dtype=K.floatx())\n    if len(x.shape) == 3:\n        if data_format == \'channels_first\':\n            x = x.transpose(2, 0, 1)\n    elif len(x.shape) == 2:\n        if data_format == \'channels_first\':\n            x = x.reshape((1, x.shape[0], x.shape[1]))\n        else:\n            x = x.reshape((x.shape[0], x.shape[1], 1))\n    else:\n        raise ValueError(\'Unsupported image shape: \', x.shape)\n    return x\n\n\ndef load_img(path, grayscale=False, target_size=None,\n             interpolation=\'nearest\'):\n    """"""Loads an image into PIL format.\n\n    # Arguments\n        path: Path to image file\n        grayscale: Boolean, whether to load the image as grayscale.\n        target_size: Either `None` (default to original size)\n            or tuple of ints `(img_height, img_width)`.\n        interpolation: Interpolation method used to resample the image if the\n            target size is different from that of the loaded image.\n            Supported methods are ""nearest"", ""bilinear"", and ""bicubic"".\n            If PIL version 1.1.3 or newer is installed, ""lanczos"" is also\n            supported. If PIL version 3.4.0 or newer is installed, ""box"" and\n            ""hamming"" are also supported. By default, ""nearest"" is used.\n\n    # Returns\n        A PIL Image instance.\n\n    # Raises\n        ImportError: if PIL is not available.\n        ValueError: if interpolation method is not supported.\n    """"""\n    if pil_image is None:\n        raise ImportError(\'Could not import PIL.Image. \'\n                          \'The use of `array_to_img` requires PIL.\')\n    img = pil_image.open(path)\n    if grayscale:\n        if img.mode != \'L\':\n            img = img.convert(\'L\')\n    else:\n        if img.mode != \'RGB\':\n            img = img.convert(\'RGB\')\n    if target_size is not None:\n        width_height_tuple = (target_size[1], target_size[0])\n        if img.size != width_height_tuple:\n            if interpolation not in _PIL_INTERPOLATION_METHODS:\n                raise ValueError(\n                    \'Invalid interpolation method {} specified. Supported \'\n                    \'methods are {}\'.format(\n                        interpolation,\n                        "", "".join(_PIL_INTERPOLATION_METHODS.keys())))\n            resample = _PIL_INTERPOLATION_METHODS[interpolation]\n            img = img.resize(width_height_tuple, resample)\n    return img\n\n\ndef list_pictures(directory, ext=\'jpg|jpeg|bmp|png|ppm\'):\n    return [os.path.join(root, f)\n            for root, _, files in os.walk(directory) for f in files\n            if re.match(r\'([\\w]+\\.(?:\' + ext + \'))\', f)]\n\n\nclass ImageDataGenerator(object):\n    """"""Generate minibatches of image data with real-time data augmentation.\n\n    # Arguments\n        featurewise_center: set input mean to 0 over the dataset.\n        samplewise_center: set each sample mean to 0.\n        featurewise_std_normalization: divide inputs by std of the dataset.\n        samplewise_std_normalization: divide each input by its std.\n        zca_whitening: apply ZCA whitening.\n        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n        rotation_range: degrees (0 to 180).\n        width_shift_range: fraction of total width, if < 1, or pixels if >= 1.\n        height_shift_range: fraction of total height, if < 1, or pixels if >= 1.\n        shear_range: shear intensity (shear angle in degrees).\n        zoom_range: amount of zoom. if scalar z, zoom will be randomly picked\n            in the range [1-z, 1+z]. A sequence of two can be passed instead\n            to select this range.\n        channel_shift_range: shift range for each channel.\n        fill_mode: points outside the boundaries are filled according to the\n            given mode (\'constant\', \'nearest\', \'reflect\' or \'wrap\'). Default\n            is \'nearest\'.\n            Points outside the boundaries of the input are filled according to the given mode:\n                \'constant\': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n                \'nearest\':  aaaaaaaa|abcd|dddddddd\n                \'reflect\':  abcddcba|abcd|dcbaabcd\n                \'wrap\':  abcdabcd|abcd|abcdabcd\n        cval: value used for points outside the boundaries when fill_mode is\n            \'constant\'. Default is 0.\n        horizontal_flip: whether to randomly flip images horizontally.\n        vertical_flip: whether to randomly flip images vertically.\n        rescale: rescaling factor. If None or 0, no rescaling is applied,\n            otherwise we multiply the data by the value provided. This is\n            applied after the `preprocessing_function` (if any provided)\n            but before any other transformation.\n        preprocessing_function: function that will be implied on each input.\n            The function will run before any other modification on it.\n            The function should take one argument:\n            one image (Numpy tensor with rank 3),\n            and should output a Numpy tensor with the same shape.\n        data_format: \'channels_first\' or \'channels_last\'. In \'channels_first\' mode, the channels dimension\n            (the depth) is at index 1, in \'channels_last\' mode it is at index 3.\n            It defaults to the `image_data_format` value found in your\n            Keras config file at `~/.keras/keras.json`.\n            If you never set it, then it will be ""channels_last"".\n    """"""\n\n    def __init__(self,\n                 featurewise_center=False,\n                 samplewise_center=False,\n                 featurewise_std_normalization=False,\n                 samplewise_std_normalization=False,\n                 zca_whitening=False,\n                 zca_epsilon=1e-6,\n                 rotation_range=0.,\n                 width_shift_range=0.,\n                 height_shift_range=0.,\n                 shear_range=0.,\n                 zoom_range=0.,\n                 channel_shift_range=0.,\n                 fill_mode=\'nearest\',\n                 cval=0.,\n                 horizontal_flip=False,\n                 vertical_flip=False,\n                 rescale=None,\n                 preprocessing_function=None,\n                 data_format=None):\n        if data_format is None:\n            data_format = K.image_data_format()\n        self.featurewise_center = featurewise_center\n        self.samplewise_center = samplewise_center\n        self.featurewise_std_normalization = featurewise_std_normalization\n        self.samplewise_std_normalization = samplewise_std_normalization\n        self.zca_whitening = zca_whitening\n        self.zca_epsilon = zca_epsilon\n        self.rotation_range = rotation_range\n        self.width_shift_range = width_shift_range\n        self.height_shift_range = height_shift_range\n        self.shear_range = shear_range\n        self.zoom_range = zoom_range\n        self.channel_shift_range = channel_shift_range\n        self.fill_mode = fill_mode\n        self.cval = cval\n        self.horizontal_flip = horizontal_flip\n        self.vertical_flip = vertical_flip\n        self.rescale = rescale\n        self.preprocessing_function = preprocessing_function\n\n        if data_format not in {\'channels_last\', \'channels_first\'}:\n            raise ValueError(\'`data_format` should be `""channels_last""` (channel after row and \'\n                             \'column) or `""channels_first""` (channel before row and column). \'\n                             \'Received arg: \', data_format)\n        self.data_format = data_format\n        if data_format == \'channels_first\':\n            self.channel_axis = 1\n            self.row_axis = 2\n            self.col_axis = 3\n        if data_format == \'channels_last\':\n            self.channel_axis = 3\n            self.row_axis = 1\n            self.col_axis = 2\n\n        self.mean = None\n        self.std = None\n        self.principal_components = None\n\n        if np.isscalar(zoom_range):\n            self.zoom_range = [1 - zoom_range, 1 + zoom_range]\n        elif len(zoom_range) == 2:\n            self.zoom_range = [zoom_range[0], zoom_range[1]]\n        else:\n            raise ValueError(\'`zoom_range` should be a float or \'\n                             \'a tuple or list of two floats. \'\n                             \'Received arg: \', zoom_range)\n        if zca_whitening:\n            if not featurewise_center:\n                self.featurewise_center = True\n                warnings.warn(\'This ImageDataGenerator specifies \'\n                              \'`zca_whitening`, which overrides \'\n                              \'setting of `featurewise_center`.\')\n            if featurewise_std_normalization:\n                self.featurewise_std_normalization = False\n                warnings.warn(\'This ImageDataGenerator specifies \'\n                              \'`zca_whitening` \'\n                              \'which overrides setting of\'\n                              \'`featurewise_std_normalization`.\')\n        if featurewise_std_normalization:\n            if not featurewise_center:\n                self.featurewise_center = True\n                warnings.warn(\'This ImageDataGenerator specifies \'\n                              \'`featurewise_std_normalization`, \'\n                              \'which overrides setting of \'\n                              \'`featurewise_center`.\')\n        if samplewise_std_normalization:\n            if not samplewise_center:\n                self.samplewise_center = True\n                warnings.warn(\'This ImageDataGenerator specifies \'\n                              \'`samplewise_std_normalization`, \'\n                              \'which overrides setting of \'\n                              \'`samplewise_center`.\')\n\n    def flow(self, x, y=None, batch_size=32, shuffle=True, seed=None,\n             save_to_dir=None, save_prefix=\'\', save_format=\'png\'):\n        return NumpyArrayIterator(\n            x, y, self,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            seed=seed,\n            data_format=self.data_format,\n            save_to_dir=save_to_dir,\n            save_prefix=save_prefix,\n            save_format=save_format)\n\n    def flow_from_directory(self, directory,\n                            target_size=(256, 256), color_mode=\'rgb\',\n                            classes=None, class_mode=\'categorical\',\n                            batch_size=32, shuffle=True, seed=None,\n                            save_to_dir=None,\n                            save_prefix=\'\',\n                            save_format=\'png\',\n                            follow_links=False,\n                            interpolation=\'nearest\'):\n        return DirectoryIterator(\n            directory, self,\n            target_size=target_size, color_mode=color_mode,\n            classes=classes, class_mode=class_mode,\n            data_format=self.data_format,\n            batch_size=batch_size, shuffle=shuffle, seed=seed,\n            save_to_dir=save_to_dir,\n            save_prefix=save_prefix,\n            save_format=save_format,\n            follow_links=follow_links,\n            interpolation=interpolation)\n\n    def standardize(self, x):\n        """"""Apply the normalization configuration to a batch of inputs.\n\n        # Arguments\n            x: batch of inputs to be normalized.\n\n        # Returns\n            The inputs, normalized.\n        """"""\n        if self.preprocessing_function:\n            x = self.preprocessing_function(x)\n        if self.rescale:\n            x *= self.rescale\n        if self.samplewise_center:\n            x -= np.mean(x, keepdims=True)\n        if self.samplewise_std_normalization:\n            x /= (np.std(x, keepdims=True) + K.epsilon())\n\n        if self.featurewise_center:\n            if self.mean is not None:\n                x -= self.mean\n            else:\n                warnings.warn(\'This ImageDataGenerator specifies \'\n                              \'`featurewise_center`, but it hasn\\\'t \'\n                              \'been fit on any training data. Fit it \'\n                              \'first by calling `.fit(numpy_data)`.\')\n        if self.featurewise_std_normalization:\n            if self.std is not None:\n                x /= (self.std + K.epsilon())\n            else:\n                warnings.warn(\'This ImageDataGenerator specifies \'\n                              \'`featurewise_std_normalization`, but it hasn\\\'t \'\n                              \'been fit on any training data. Fit it \'\n                              \'first by calling `.fit(numpy_data)`.\')\n        if self.zca_whitening:\n            if self.principal_components is not None:\n                flatx = np.reshape(x, (-1, np.prod(x.shape[-3:])))\n                whitex = np.dot(flatx, self.principal_components)\n                x = np.reshape(whitex, x.shape)\n            else:\n                warnings.warn(\'This ImageDataGenerator specifies \'\n                              \'`zca_whitening`, but it hasn\\\'t \'\n                              \'been fit on any training data. Fit it \'\n                              \'first by calling `.fit(numpy_data)`.\')\n        return x\n\n    def random_transform(self, x, seed=None):\n        """"""Randomly augment a single image tensor.\n\n        # Arguments\n            x: 3D tensor, single image.\n            seed: random seed.\n\n        # Returns\n            A randomly transformed version of the input (same shape).\n        """"""\n        # x is a single image, so it doesn\'t have image number at index 0\n        img_row_axis = self.row_axis - 1\n        img_col_axis = self.col_axis - 1\n        img_channel_axis = self.channel_axis - 1\n\n        if seed is not None:\n            np.random.seed(seed)\n\n        # use composition of homographies\n        # to generate final transform that needs to be applied\n        if self.rotation_range:\n            theta = np.deg2rad(np.random.uniform(-self.rotation_range, self.rotation_range))\n        else:\n            theta = 0\n\n        if self.height_shift_range:\n            tx = np.random.uniform(-self.height_shift_range, self.height_shift_range)\n            if self.height_shift_range < 1:\n                tx *= x.shape[img_row_axis]\n        else:\n            tx = 0\n\n        if self.width_shift_range:\n            ty = np.random.uniform(-self.width_shift_range, self.width_shift_range)\n            if self.width_shift_range < 1:\n                ty *= x.shape[img_col_axis]\n        else:\n            ty = 0\n\n        if self.shear_range:\n            shear = np.deg2rad(np.random.uniform(-self.shear_range, self.shear_range))\n        else:\n            shear = 0\n\n        if self.zoom_range[0] == 1 and self.zoom_range[1] == 1:\n            zx, zy = 1, 1\n        else:\n            zx, zy = np.random.uniform(self.zoom_range[0], self.zoom_range[1], 2)\n\n        transform_matrix = None\n        if theta != 0:\n            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n                                        [np.sin(theta), np.cos(theta), 0],\n                                        [0, 0, 1]])\n            transform_matrix = rotation_matrix\n\n        if tx != 0 or ty != 0:\n            shift_matrix = np.array([[1, 0, tx],\n                                     [0, 1, ty],\n                                     [0, 0, 1]])\n            transform_matrix = shift_matrix if transform_matrix is None else np.dot(transform_matrix, shift_matrix)\n\n        if shear != 0:\n            shear_matrix = np.array([[1, -np.sin(shear), 0],\n                                    [0, np.cos(shear), 0],\n                                    [0, 0, 1]])\n            transform_matrix = shear_matrix if transform_matrix is None else np.dot(transform_matrix, shear_matrix)\n\n        if zx != 1 or zy != 1:\n            zoom_matrix = np.array([[zx, 0, 0],\n                                    [0, zy, 0],\n                                    [0, 0, 1]])\n            transform_matrix = zoom_matrix if transform_matrix is None else np.dot(transform_matrix, zoom_matrix)\n\n        if transform_matrix is not None:\n            h, w = x.shape[img_row_axis], x.shape[img_col_axis]\n            transform_matrix = transform_matrix_offset_center(transform_matrix, h, w)\n            x = apply_transform(x, transform_matrix, img_channel_axis,\n                                fill_mode=self.fill_mode, cval=self.cval)\n\n        if self.channel_shift_range != 0:\n            x = random_channel_shift(x,\n                                     self.channel_shift_range,\n                                     img_channel_axis)\n        if self.horizontal_flip:\n            if np.random.random() < 0.5:\n                x = flip_axis(x, img_col_axis)\n\n        if self.vertical_flip:\n            if np.random.random() < 0.5:\n                x = flip_axis(x, img_row_axis)\n\n        return x\n\n    def fit(self, x,\n            augment=False,\n            rounds=1,\n            seed=None):\n        """"""Fits internal statistics to some sample data.\n\n        Required for featurewise_center, featurewise_std_normalization\n        and zca_whitening.\n\n        # Arguments\n            x: Numpy array, the data to fit on. Should have rank 4.\n                In case of grayscale data,\n                the channels axis should have value 1, and in case\n                of RGB data, it should have value 3.\n            augment: Whether to fit on randomly augmented samples\n            rounds: If `augment`,\n                how many augmentation passes to do over the data\n            seed: random seed.\n\n        # Raises\n            ValueError: in case of invalid input `x`.\n        """"""\n        x = np.asarray(x, dtype=K.floatx())\n        if x.ndim != 4:\n            raise ValueError(\'Input to `.fit()` should have rank 4. \'\n                             \'Got array with shape: \' + str(x.shape))\n        if x.shape[self.channel_axis] not in {1, 3, 4}:\n            warnings.warn(\n                \'Expected input to be images (as Numpy array) \'\n                \'following the data format convention ""\' + self.data_format + \'"" \'\n                \'(channels on axis \' + str(self.channel_axis) + \'), i.e. expected \'\n                \'either 1, 3 or 4 channels on axis \' + str(self.channel_axis) + \'. \'\n                \'However, it was passed an array with shape \' + str(x.shape) +\n                \' (\' + str(x.shape[self.channel_axis]) + \' channels).\')\n\n        if seed is not None:\n            np.random.seed(seed)\n\n        x = np.copy(x)\n        if augment:\n            ax = np.zeros(tuple([rounds * x.shape[0]] + list(x.shape)[1:]), dtype=K.floatx())\n            for r in range(rounds):\n                for i in range(x.shape[0]):\n                    ax[i + r * x.shape[0]] = self.random_transform(x[i])\n            x = ax\n\n        if self.featurewise_center:\n            self.mean = np.mean(x, axis=(0, self.row_axis, self.col_axis))\n            broadcast_shape = [1, 1, 1]\n            broadcast_shape[self.channel_axis - 1] = x.shape[self.channel_axis]\n            self.mean = np.reshape(self.mean, broadcast_shape)\n            x -= self.mean\n\n        if self.featurewise_std_normalization:\n            self.std = np.std(x, axis=(0, self.row_axis, self.col_axis))\n            broadcast_shape = [1, 1, 1]\n            broadcast_shape[self.channel_axis - 1] = x.shape[self.channel_axis]\n            self.std = np.reshape(self.std, broadcast_shape)\n            x /= (self.std + K.epsilon())\n\n        if self.zca_whitening:\n            flat_x = np.reshape(x, (x.shape[0], x.shape[1] * x.shape[2] * x.shape[3]))\n            sigma = np.dot(flat_x.T, flat_x) / flat_x.shape[0]\n            u, s, _ = linalg.svd(sigma)\n            s_inv = 1. / np.sqrt(s[np.newaxis] + self.zca_epsilon)\n            self.principal_components = (u * s_inv).dot(u.T)\n\n\nclass Iterator(Sequence):\n    """"""Base class for image data iterators.\n\n    Every `Iterator` must implement the `_get_batches_of_transformed_samples`\n    method.\n\n    # Arguments\n        n: Integer, total number of samples in the dataset to loop over.\n        batch_size: Integer, size of a batch.\n        shuffle: Boolean, whether to shuffle the data between epochs.\n        seed: Random seeding for data shuffling.\n    """"""\n\n    def __init__(self, n, batch_size, shuffle, seed):\n        self.n = n\n        self.batch_size = batch_size\n        self.seed = seed\n        self.shuffle = shuffle\n        self.batch_index = 0\n        self.total_batches_seen = 0\n        self.lock = threading.Lock()\n        self.index_array = None\n        self.index_generator = self._flow_index()\n\n    def _set_index_array(self):\n        self.index_array = np.arange(self.n)\n        if self.shuffle:\n            self.index_array = np.random.permutation(self.n)\n\n    def __getitem__(self, idx):\n        if idx >= len(self):\n            raise ValueError(\'Asked to retrieve element {idx}, \'\n                             \'but the Sequence \'\n                             \'has length {length}\'.format(idx=idx,\n                                                          length=len(self)))\n        if self.seed is not None:\n            np.random.seed(self.seed + self.total_batches_seen)\n        self.total_batches_seen += 1\n        if self.index_array is None:\n            self._set_index_array()\n        index_array = self.index_array[self.batch_size * idx:\n                                       self.batch_size * (idx + 1)]\n        return self._get_batches_of_transformed_samples(index_array)\n\n    def __len__(self):\n        return (self.n + self.batch_size - 1) // self.batch_size  # round up\n\n    def on_epoch_end(self):\n        self._set_index_array()\n\n    def reset(self):\n        self.batch_index = 0\n\n    def _flow_index(self):\n        # Ensure self.batch_index is 0.\n        self.reset()\n        while 1:\n            if self.seed is not None:\n                np.random.seed(self.seed + self.total_batches_seen)\n            if self.batch_index == 0:\n                self._set_index_array()\n\n            current_index = (self.batch_index * self.batch_size) % self.n\n            if self.n > current_index + self.batch_size:\n                self.batch_index += 1\n            else:\n                self.batch_index = 0\n            self.total_batches_seen += 1\n            yield self.index_array[current_index:\n                                   current_index + self.batch_size]\n\n    def __iter__(self):\n        # Needed if we want to do something like:\n        # for x, y in data_gen.flow(...):\n        return self\n\n    def __next__(self, *args, **kwargs):\n        return self.next(*args, **kwargs)\n\n    def _get_batches_of_transformed_samples(self, index_array):\n        """"""Gets a batch of transformed samples.\n\n        # Arguments\n            index_array: array of sample indices to include in batch.\n\n        # Returns\n            A batch of transformed samples.\n        """"""\n        raise NotImplementedError\n\n\nclass NumpyArrayIterator(Iterator):\n    """"""Iterator yielding data from a Numpy array.\n\n    # Arguments\n        x: Numpy array of input data.\n        y: Numpy array of targets data.\n        image_data_generator: Instance of `ImageDataGenerator`\n            to use for random transformations and normalization.\n        batch_size: Integer, size of a batch.\n        shuffle: Boolean, whether to shuffle the data between epochs.\n        seed: Random seed for data shuffling.\n        data_format: String, one of `channels_first`, `channels_last`.\n        save_to_dir: Optional directory where to save the pictures\n            being yielded, in a viewable format. This is useful\n            for visualizing the random transformations being\n            applied, for debugging purposes.\n        save_prefix: String prefix to use for saving sample\n            images (if `save_to_dir` is set).\n        save_format: Format to use for saving sample images\n            (if `save_to_dir` is set).\n    """"""\n\n    def __init__(self, x, y, image_data_generator,\n                 batch_size=32, shuffle=False, seed=None,\n                 data_format=None,\n                 save_to_dir=None, save_prefix=\'\', save_format=\'png\'):\n        if y is not None and len(x) != len(y):\n            raise ValueError(\'x (images tensor) and y (labels) \'\n                             \'should have the same length. \'\n                             \'Found: x.shape = %s, y.shape = %s\' %\n                             (np.asarray(x).shape, np.asarray(y).shape))\n\n        if data_format is None:\n            data_format = K.image_data_format()\n        self.x = np.asarray(x, dtype=K.floatx())\n\n        if self.x.ndim != 4:\n            raise ValueError(\'Input data in `NumpyArrayIterator` \'\n                             \'should have rank 4. You passed an array \'\n                             \'with shape\', self.x.shape)\n        channels_axis = 3 if data_format == \'channels_last\' else 1\n        if self.x.shape[channels_axis] not in {1, 3, 4}:\n            warnings.warn(\'NumpyArrayIterator is set to use the \'\n                          \'data format convention ""\' + data_format + \'"" \'\n                          \'(channels on axis \' + str(channels_axis) + \'), i.e. expected \'\n                          \'either 1, 3 or 4 channels on axis \' + str(channels_axis) + \'. \'\n                          \'However, it was passed an array with shape \' + str(self.x.shape) +\n                          \' (\' + str(self.x.shape[channels_axis]) + \' channels).\')\n        if y is not None:\n            self.y = np.asarray(y)\n        else:\n            self.y = None\n        self.image_data_generator = image_data_generator\n        self.data_format = data_format\n        self.save_to_dir = save_to_dir\n        self.save_prefix = save_prefix\n        self.save_format = save_format\n        super(NumpyArrayIterator, self).__init__(x.shape[0], batch_size, shuffle, seed)\n\n    def _get_batches_of_transformed_samples(self, index_array):\n        batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]),\n                           dtype=K.floatx())\n        for i, j in enumerate(index_array):\n            x = self.x[j]\n            x = self.image_data_generator.random_transform(x.astype(K.floatx()))\n            x = self.image_data_generator.standardize(x)\n            batch_x[i] = x\n        if self.save_to_dir:\n            for i, j in enumerate(index_array):\n                img = array_to_img(batch_x[i], self.data_format, scale=True)\n                fname = \'{prefix}_{index}_{hash}.{format}\'.format(prefix=self.save_prefix,\n                                                                  index=j,\n                                                                  hash=np.random.randint(1e4),\n                                                                  format=self.save_format)\n                img.save(os.path.join(self.save_to_dir, fname))\n        if self.y is None:\n            return batch_x\n        batch_y = self.y[index_array]\n        return batch_x, batch_y\n\n    def next(self):\n        """"""For python 2.x.\n\n        # Returns\n            The next batch.\n        """"""\n        # Keeps under lock only the mechanism which advances\n        # the indexing of each batch.\n        with self.lock:\n            index_array = next(self.index_generator)\n        # The transformation of images is not under thread lock\n        # so it can be done in parallel\n        return self._get_batches_of_transformed_samples(index_array)\n\n\ndef _count_valid_files_in_directory(directory, white_list_formats, follow_links):\n    """"""Count files with extension in `white_list_formats` contained in directory.\n\n    # Arguments\n        directory: absolute path to the directory\n            containing files to be counted\n        white_list_formats: set of strings containing allowed extensions for\n            the files to be counted.\n        follow_links: boolean.\n\n    # Returns\n        the count of files with extension in `white_list_formats` contained in\n        the directory.\n    """"""\n    def _recursive_list(subpath):\n        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda x: x[0])\n\n    samples = 0\n    for _, _, files in _recursive_list(directory):\n        for fname in files:\n            is_valid = False\n            for extension in white_list_formats:\n                if fname.lower().endswith(\'.tiff\'):\n                    warnings.warn(\'Using \\\'.tiff\\\' files with multiple bands will cause distortion. \'\n                                  \'Please verify your output.\')\n                if fname.lower().endswith(\'.\' + extension):\n                    is_valid = True\n                    break\n            if is_valid:\n                samples += 1\n    return samples\n\n\ndef _list_valid_filenames_in_directory(directory, white_list_formats,\n                                       class_indices, follow_links):\n    """"""List paths of files in `subdir` with extensions in `white_list_formats`.\n\n    # Arguments\n        directory: absolute path to a directory containing the files to list.\n            The directory name is used as class label and must be a key of `class_indices`.\n        white_list_formats: set of strings containing allowed extensions for\n            the files to be counted.\n        class_indices: dictionary mapping a class name to its index.\n        follow_links: boolean.\n\n    # Returns\n        classes: a list of class indices\n        filenames: the path of valid files in `directory`, relative from\n            `directory`\'s parent (e.g., if `directory` is ""dataset/class1"",\n            the filenames will be [""class1/file1.jpg"", ""class1/file2.jpg"", ...]).\n    """"""\n    def _recursive_list(subpath):\n        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda x: x[0])\n\n    classes = []\n    filenames = []\n    subdir = os.path.basename(directory)\n    basedir = os.path.dirname(directory)\n    for root, _, files in _recursive_list(directory):\n        for fname in sorted(files):\n            is_valid = False\n            for extension in white_list_formats:\n                if fname.lower().endswith(\'.\' + extension):\n                    is_valid = True\n                    break\n            if is_valid:\n                classes.append(class_indices[subdir])\n                # add filename relative to directory\n                absolute_path = os.path.join(root, fname)\n                filenames.append(os.path.relpath(absolute_path, basedir))\n    return classes, filenames\n\n\nclass DirectoryIterator(Iterator):\n    """"""Iterator capable of reading images from a directory on disk.\n\n    # Arguments\n        directory: Path to the directory to read images from.\n            Each subdirectory in this directory will be\n            considered to contain images from one class,\n            or alternatively you could specify class subdirectories\n            via the `classes` argument.\n        image_data_generator: Instance of `ImageDataGenerator`\n            to use for random transformations and normalization.\n        target_size: tuple of integers, dimensions to resize input images to.\n        color_mode: One of `""rgb""`, `""grayscale""`. Color mode to read images.\n        classes: Optional list of strings, names of subdirectories\n            containing images from each class (e.g. `[""dogs"", ""cats""]`).\n            It will be computed automatically if not set.\n        class_mode: Mode for yielding the targets:\n            `""binary""`: binary targets (if there are only two classes),\n            `""categorical""`: categorical targets,\n            `""sparse""`: integer targets,\n            `""input""`: targets are images identical to input images (mainly\n                used to work with autoencoders),\n            `None`: no targets get yielded (only input images are yielded).\n        batch_size: Integer, size of a batch.\n        shuffle: Boolean, whether to shuffle the data between epochs.\n        seed: Random seed for data shuffling.\n        data_format: String, one of `channels_first`, `channels_last`.\n        save_to_dir: Optional directory where to save the pictures\n            being yielded, in a viewable format. This is useful\n            for visualizing the random transformations being\n            applied, for debugging purposes.\n        save_prefix: String prefix to use for saving sample\n            images (if `save_to_dir` is set).\n        save_format: Format to use for saving sample images\n            (if `save_to_dir` is set).\n        interpolation: Interpolation method used to resample the image if the\n            target size is different from that of the loaded image.\n            Supported methods are ""nearest"", ""bilinear"", and ""bicubic"".\n            If PIL version 1.1.3 or newer is installed, ""lanczos"" is also\n            supported. If PIL version 3.4.0 or newer is installed, ""box"" and\n            ""hamming"" are also supported. By default, ""nearest"" is used.\n    """"""\n\n    def __init__(self, directory, image_data_generator,\n                 target_size=(256, 256), color_mode=\'rgb\',\n                 classes=None, class_mode=\'categorical\',\n                 batch_size=32, shuffle=True, seed=None,\n                 data_format=None, save_to_dir=None,\n                 save_prefix=\'\', save_format=\'png\',\n                 follow_links=False, interpolation=\'nearest\'):\n        if data_format is None:\n            data_format = K.image_data_format()\n        self.directory = directory\n        self.image_data_generator = image_data_generator\n        self.target_size = tuple(target_size)\n        if color_mode not in {\'rgb\', \'grayscale\'}:\n            raise ValueError(\'Invalid color mode:\', color_mode,\n                             \'; expected ""rgb"" or ""grayscale"".\')\n        self.color_mode = color_mode\n        self.data_format = data_format\n        if self.color_mode == \'rgb\':\n            if self.data_format == \'channels_last\':\n                self.image_shape = self.target_size + (3,)\n            else:\n                self.image_shape = (3,) + self.target_size\n        else:\n            if self.data_format == \'channels_last\':\n                self.image_shape = self.target_size + (1,)\n            else:\n                self.image_shape = (1,) + self.target_size\n        self.classes = classes\n        if class_mode not in {\'categorical\', \'binary\', \'sparse\',\n                              \'input\', None}:\n            raise ValueError(\'Invalid class_mode:\', class_mode,\n                             \'; expected one of ""categorical"", \'\n                             \'""binary"", ""sparse"", ""input""\'\n                             \' or None.\')\n        self.class_mode = class_mode\n        self.save_to_dir = save_to_dir\n        self.save_prefix = save_prefix\n        self.save_format = save_format\n        self.interpolation = interpolation\n\n        white_list_formats = {\'png\', \'jpg\', \'jpeg\', \'bmp\', \'ppm\', \'tif\', \'tiff\'}\n\n        # first, count the number of samples and classes\n        self.samples = 0\n\n        if not classes:\n            classes = []\n            for subdir in sorted(os.listdir(directory)):\n                if os.path.isdir(os.path.join(directory, subdir)):\n                    classes.append(subdir)\n        self.num_classes = len(classes)\n        self.class_indices = dict(zip(classes, range(len(classes))))\n\n        pool = multiprocessing.pool.ThreadPool()\n        function_partial = partial(_count_valid_files_in_directory,\n                                   white_list_formats=white_list_formats,\n                                   follow_links=follow_links)\n        self.samples = sum(pool.map(function_partial,\n                                    (os.path.join(directory, subdir)\n                                     for subdir in classes)))\n\n        print(\'Found %d images belonging to %d classes.\' % (self.samples, self.num_classes))\n\n        # second, build an index of the images in the different class subfolders\n        results = []\n\n        self.filenames = []\n        self.classes = np.zeros((self.samples,), dtype=\'int32\')\n        i = 0\n        for dirpath in (os.path.join(directory, subdir) for subdir in classes):\n            results.append(pool.apply_async(_list_valid_filenames_in_directory,\n                                            (dirpath, white_list_formats,\n                                             self.class_indices, follow_links)))\n        for res in results:\n            classes, filenames = res.get()\n            self.classes[i:i + len(classes)] = classes\n            self.filenames += filenames\n            i += len(classes)\n        pool.close()\n        pool.join()\n        super(DirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n\n    def _get_batches_of_transformed_samples(self, index_array):\n        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n        grayscale = self.color_mode == \'grayscale\'\n        # build batch of image data\n        for i, j in enumerate(index_array):\n            fname = self.filenames[j]\n            img = load_img(os.path.join(self.directory, fname),\n                           grayscale=grayscale,\n                           target_size=self.target_size,\n                           interpolation=self.interpolation)\n            x = img_to_array(img, data_format=self.data_format)\n            x = self.image_data_generator.random_transform(x)\n            x = self.image_data_generator.standardize(x)\n            batch_x[i] = x\n        # optionally save augmented images to disk for debugging purposes\n        if self.save_to_dir:\n            for i, j in enumerate(index_array):\n                img = array_to_img(batch_x[i], self.data_format, scale=True)\n                fname = \'{prefix}_{index}_{hash}.{format}\'.format(prefix=self.save_prefix,\n                                                                  index=j,\n                                                                  hash=np.random.randint(1e7),\n                                                                  format=self.save_format)\n                img.save(os.path.join(self.save_to_dir, fname))\n        # build batch of labels\n        if self.class_mode == \'input\':\n            batch_y = batch_x.copy()\n        elif self.class_mode == \'sparse\':\n            batch_y = self.classes[index_array]\n        elif self.class_mode == \'binary\':\n            batch_y = self.classes[index_array].astype(K.floatx())\n        elif self.class_mode == \'categorical\':\n            batch_y = np.zeros((len(batch_x), self.num_classes), dtype=K.floatx())\n            for i, label in enumerate(self.classes[index_array]):\n                batch_y[i, label] = 1.\n        else:\n            return batch_x\n        return batch_x, batch_y\n\n    def next(self):\n        """"""For python 2.x.\n\n        # Returns\n            The next batch.\n        """"""\n        with self.lock:\n            index_array = next(self.index_generator)\n        # The transformation of images is not under thread lock\n        # so it can be done in parallel\n        return self._get_batches_of_transformed_samples(index_array)\n'"
gan/pt_utils/spectral_norm.py,8,"b'""""""\nSpectral Normalization from https://arxiv.org/abs/1802.05957\n""""""\nimport torch\nfrom torch.nn.functional import normalize\nfrom torch.nn.parameter import Parameter\n\n\nclass SpectralNorm(object):\n\n    def __init__(self, name=\'weight\', n_power_iterations=1, eps=1e-12):\n        self.name = name\n        self.n_power_iterations = n_power_iterations\n        self.eps = eps\n\n    def compute_weight(self, module):\n        weight = getattr(module, self.name + \'_org\')\n        u = getattr(module, self.name + \'_u\')\n        height = weight.size(0)\n        weight_mat = weight.view(height, -1)\n        with torch.no_grad():\n            for _ in range(self.n_power_iterations):\n                # Spectral norm of weight equals to `u^T W v`, where `u` and `v`\n                # are the first left and right singular vectors.\n                # This power iteration produces approximations of `u` and `v`.\n                v = normalize(torch.matmul(weight_mat.t(), u), dim=0, eps=self.eps)\n                u = normalize(torch.matmul(weight_mat, v), dim=0, eps=self.eps)\n\n            sigma = torch.dot(u, torch.matmul(weight_mat, v))\n        weight = weight / sigma\n        return weight, u\n\n    def remove(self, module):\n        weight = module._parameters[self.name + \'_org\']\n        delattr(module, self.name)\n        delattr(module, self.name + \'_u\')\n        delattr(module, self.name + \'_org\')\n        module.register_parameter(self.name, weight)\n\n    def __call__(self, module, inputs):\n        weight, u = self.compute_weight(module)\n        setattr(module, self.name, weight)\n        with torch.no_grad():\n            getattr(module, self.name).copy_(weight)\n\n    @staticmethod\n    def apply(module, name, n_power_iterations, eps):\n        fn = SpectralNorm(name, n_power_iterations, eps)\n        weight = module._parameters[name]\n        height = weight.size(0)\n\n        u = normalize(weight.new_empty(height).normal_(0, 1), dim=0, eps=fn.eps)\n        delattr(module, fn.name)\n        module.register_parameter(fn.name + ""_org"", weight)\n        module.register_buffer(fn.name, weight)\n        module.register_buffer(fn.name + ""_u"", u)\n\n        module.register_forward_pre_hook(fn)\n        return fn\n\n\ndef spectral_norm(module, name=\'weight\', n_power_iterations=1, eps=1e-12):\n    r""""""Applies spectral normalization to a parameter in the given module.\n\n    .. math::\n         \\mathbf{W} &= \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})} \\\\\n         \\sigma(\\mathbf{W}) &= \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0} \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}\n\n    Spectral normalization stabilizes the training of discriminators (critics)\n    in Generaive Adversarial Networks (GANs) by rescaling the weight tensor\n    with spectral norm :math:`\\sigma` of the weight matrix calculated using\n    power iteration method. If the dimension of the weight tensor is greater\n    than 2, it is reshaped to 2D in power iteration method to get spectral\n    norm. This is implemented via a hook that calculates spectral norm and\n    rescales weight before every :meth:`~Module.forward` call.\n\n    See `Spectral Normalization for Generative Adversarial Networks`_ .\n\n    .. _`Spectral Normalization for Generative Adversarial Networks`: https://arxiv.org/abs/1802.05957\n\n    Args:\n        module (nn.Module): containing module\n        name (str, optional): name of weight parameter\n        n_power_iterations (int, optional): number of power iterations to\n            calculate spectal norm\n        eps (float, optional): epsilon for numerical stability in\n            calculating norms\n\n    Returns:\n        The original module with the spectal norm hook\n\n    Example::\n\n        >>> m = spectral_norm(nn.Linear(20, 40))\n        Linear (20 -> 40)\n        >>> m.weight_u.size()\n        torch.Size([20])\n\n    """"""\n    SpectralNorm.apply(module, name, n_power_iterations, eps)\n    return module\n\n\ndef remove_spectral_norm(module, name=\'weight\'):\n    r""""""Removes the spectral normalization reparameterization from a module.\n\n    Args:\n        module (nn.Module): containing module\n        name (str, optional): name of weight parameter\n\n    Example:\n        >>> m = spectral_norm(nn.Linear(40, 10))\n        >>> remove_spectral_norm(m)\n    """"""\n    for k, hook in module._forward_pre_hooks.items():\n        if isinstance(hook, SpectralNorm) and hook.name == name:\n            hook.remove(module)\n            del module._forward_pre_hooks[k]\n            return module\n\n    raise ValueError(""spectral_norm of \'{}\' not found in {}"".format(\n        name, module))\n'"
gan/tf/fid.py,0,"b'#!/usr/bin/env python3\n\'\'\' Calculates the Frechet Inception Distance (FID) to evalulate GANs.\n\nThe FID metric calculates the distance between two distributions of images.\nTypically, we have summary statistics (mean & covariance matrix) of one\nof these distributions, while the 2nd distribution is given by a GAN.\n\nWhen run as a stand-alone program, it compares the distribution of\nimages that are stored as PNG/JPEG at a specified location with a\ndistribution given by summary statistics (in pickle format).\n\nThe FID is calculated by assuming that X_1 and X_2 are the activations of\nthe pool_3 layer of the inception net for generated samples and real world\nsamples respectivly.\n\nSee --help to see further details.\n\'\'\'\n\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np\nimport os\nimport gzip, pickle\nimport tensorflow as tf\nfrom scipy.misc import imread\nfrom scipy import linalg\nimport pathlib\nimport urllib\nimport warnings\n\n\nclass InvalidFIDException(Exception):\n    pass\n\n\ndef create_inception_graph(pth):\n    """"""Creates a graph from saved GraphDef file.""""""\n    # Creates graph from saved graph_def.pb.\n    with tf.gfile.FastGFile( pth, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString( f.read())\n        _ = tf.import_graph_def( graph_def, name=\'FID_Inception_Net\')\n#-------------------------------------------------------------------------------\n\n\n# code for handling inception net derived from\n#   https://github.com/openai/improved-gan/blob/master/inception_score/model.py\ndef _get_inception_layer(sess):\n    """"""Prepares inception net for batched usage and returns pool_3 layer. """"""\n    layername = \'FID_Inception_Net/pool_3:0\'\n    pool3 = sess.graph.get_tensor_by_name(layername)\n    ops = pool3.graph.get_operations()\n    for op_idx, op in enumerate(ops):\n        for o in op.outputs:\n            shape = o.get_shape()\n            if shape._dims is not None:\n              shape = [s.value for s in shape]\n              new_shape = []\n              for j, s in enumerate(shape):\n                if s == 1 and j == 0:\n                  new_shape.append(None)\n                else:\n                  new_shape.append(s)\n              #o._shape = tf.TensorShape(new_shape)\n              o.set_shape(tf.TensorShape(new_shape))\n    return pool3\n#-------------------------------------------------------------------------------\n\n\ndef get_activations(images, sess, batch_size=1, verbose=False):\n    """"""Calculates the activations of the pool_3 layer for all images.\n\n    Params:\n    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n                     must lie between 0 and 256.\n    -- sess        : current session\n    -- batch_size  : the images numpy array is split into batches with batch size\n                     batch_size. A reasonable batch size depends on the disposable hardware.\n    -- verbose    : If set to True and parameter out_step is given, the number of calculated\n                     batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, 2048) that contains the\n       activations of the given tensor when feeding inception with the query tensor.\n    """"""\n    inception_layer = _get_inception_layer(sess)\n    d0 = images.shape[0]\n    if batch_size > d0:\n        print(""warning: batch size is bigger than the data size. setting batch size to data size"")\n        batch_size = d0\n    n_batches = d0//batch_size\n    n_used_imgs = n_batches*batch_size\n    pred_arr = np.empty((n_used_imgs,2048))\n    for i in range(n_batches):\n        if verbose:\n            print(""\\rPropagating batch %d/%d"" % (i+1, n_batches), end="""", flush=True)\n        start = i*batch_size\n        end = start + batch_size\n        batch = images[start:end]\n        pred = sess.run(inception_layer, {\'FID_Inception_Net/ExpandDims:0\': batch})\n        pred_arr[start:end] = pred.reshape(batch_size,-1)\n    if verbose:\n        print("" done"")\n    return pred_arr\n#-------------------------------------------------------------------------------\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    """"""Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n            \n    Stable version by Dougal J. Sutherland.\n\n    Params:\n    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n             inception net ( like returned by the function \'get_predictions\')\n             for generated samples.\n    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n               on an representive data set.\n    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n               generated samples.\n    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n               precalcualted on an representive data set.\n\n    Returns:\n    --   : The Frechet Distance.\n    """"""\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, ""Training and test mean vectors have different lengths""\n    assert sigma1.shape == sigma2.shape, ""Training and test covariances have different dimensions""\n\n    diff = mu1 - mu2\n\n    # product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = ""fid calculation produces singular product; adding %s to diagonal of cov estimates"" % eps\n        warnings.warn(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    # numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError(""Imaginary component {}"".format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n#-------------------------------------------------------------------------------\n\n\ndef calculate_activation_statistics(images, sess, batch_size=1, verbose=False):\n    """"""Calculation of the statistics used by the FID.\n    Params:\n    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n                     must lie between 0 and 255.\n    -- sess        : current session\n    -- batch_size  : the images numpy array is split into batches with batch size\n                     batch_size. A reasonable batch size depends on the available hardware.\n    -- verbose     : If set to True and parameter out_step is given, the number of calculated\n                     batches is reported.\n    Returns:\n    -- mu    : The mean over samples of the activations of the pool_3 layer of\n               the incption model.\n    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n               the incption model.\n    """"""\n    act = get_activations(images, sess, batch_size, verbose)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma\n#-------------------------------------------------------------------------------\n\n\n#-------------------------------------------------------------------------------\n# The following functions aren\'t needed for calculating the FID\n# they\'re just here to make this module work as a stand-alone script\n# for calculating FID scores\n#-------------------------------------------------------------------------------\ndef check_or_download_inception(inception_path):\n    \'\'\' Checks if the path to the inception file is valid, or downloads\n        the file if it is not present. \'\'\'\n    INCEPTION_URL = \'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\'\n    if inception_path is None:\n        inception_path = \'/tmp\'\n    inception_path = pathlib.Path(inception_path)\n    model_file = inception_path / \'classify_image_graph_def.pb\'\n    if not model_file.exists():\n        print(""Downloading Inception model"")\n        from urllib import request\n        import tarfile\n        fn, _ = request.urlretrieve(INCEPTION_URL)\n        with tarfile.open(fn, mode=\'r\') as f:\n            f.extract(\'classify_image_graph_def.pb\', str(model_file.parent))\n    return str(model_file)\n\n\ndef _handle_path(path, sess):\n    if path.endswith(\'.npz\'):\n        f = np.load(path)\n        m, s = f[\'mu\'][:], f[\'sigma\'][:]\n        f.close()\n    else:\n        path = pathlib.Path(path)\n        files = list(path.glob(\'*.jpg\')) + list(path.glob(\'*.png\'))\n        x = np.array([imread(str(fn)).astype(np.float32) for fn in files])\n        m, s = calculate_activation_statistics(x, sess)\n    return m, s\n\n\ndef calculate_fid_given_paths(paths, inception_path):\n    \'\'\' Calculates the FID of two paths. \'\'\'\n    inception_path = check_or_download_inception(inception_path)\n\n    for p in paths:\n        if not os.path.exists(p):\n            raise RuntimeError(""Invalid path: %s"" % p)\n\n    create_inception_graph(str(inception_path))\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        m1, s1 = _handle_path(paths[0], sess)\n        m2, s2 = _handle_path(paths[1], sess)\n        fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n        return fid_value\n\n## Added by Chris\ndef _handle_imgs(imgs, sess):\n    imgs = imgs.astype(np.float32)\n    m, s = calculate_activation_statistics(imgs, sess)\n    return m, s\n\ndef calculate_fid_given_imgs(imgs1, imgs2, inception_path=None):    \n    \'\'\' Calculates the FID of two paths. \'\'\'\n    inception_path = check_or_download_inception(inception_path)\n    create_inception_graph(str(inception_path))\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        m1, s1 = _handle_imgs(imgs1, sess)\n        if type(imgs2) == str and imgs2.endswith("".npz""):\n            f = np.load(imgs2)\n            m2, s2 = f[\'mean\'], f[\'cov\']\n            f.close()\n        else:\n            m2, s2 = _handle_imgs(imgs2, sess)\n        fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n        return fid_value\n    \nif __name__ == ""__main__"":\n    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""path"", type=str, nargs=2,\n        help=\'Path to the generated images or to .npz statistic files\')\n    parser.add_argument(""-i"", ""--inception"", type=str, default=None,\n        help=\'Path to Inception model (will be downloaded if not provided)\')\n    parser.add_argument(""--gpu"", default="""", type=str,\n        help=\'GPU to use (leave blank for CPU only)\')\n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    fid_value = calculate_fid_given_paths(args.path, args.inception)\n    print(""FID: "", fid_value)\n'"
gan/tf/inception.py,0,"b'# Code derived from tensorflow/tensorflow/models/image/imagenet/classify_image.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\nimport glob\nimport scipy.misc\nimport math\nimport sys\n\n#MODEL_DIR = \'/tmp/imagenet\'\nMODEL_DIR = os.environ[""SLURM_TMP_DIR""]\nif not os.path.exists(MODEL_DIR):\n  os.makedirs(MODEL_DIR)\nDATA_URL = \'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\'\nsoftmax = None\n\n# Call this function with list of images. Each of elements should be a \n# numpy array with values ranging from 0 to 255.\ndef get_inception_score(images, splits=10):\n  assert(type(images) == list)\n  assert(type(images[0]) == np.ndarray)\n  assert(len(images[0].shape) == 3)\n  assert(np.max(images[0]) > 10)\n  assert(np.min(images[0]) >= 0.0)\n  inps = []\n  for img in images:\n    img = img.astype(np.float32)\n    inps.append(np.expand_dims(img, 0))\n  bs = 1\n  with tf.Session() as sess:\n    preds = []\n    n_batches = int(math.ceil(float(len(inps)) / float(bs)))\n    for i in range(n_batches):\n        sys.stdout.write(""."")\n        sys.stdout.flush()\n        inp = inps[(i * bs):min((i + 1) * bs, len(inps))]\n        inp = np.concatenate(inp, 0)\n        pred = sess.run(softmax, {\'ExpandDims:0\': inp})\n        preds.append(pred)\n    preds = np.concatenate(preds, 0)\n    scores = []\n    for i in range(splits):\n      part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n      kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n      kl = np.mean(np.sum(kl, 1))\n      scores.append(np.exp(kl))\n    return np.mean(scores), np.std(scores)\n\n# This function is called automatically.\ndef _init_inception():\n  global softmax\n  if not os.path.exists(MODEL_DIR):\n    os.makedirs(MODEL_DIR)\n  filename = DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(MODEL_DIR, filename)\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Succesfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(MODEL_DIR)\n  with tf.gfile.FastGFile(os.path.join(\n      MODEL_DIR, \'classify_image_graph_def.pb\'), \'rb\') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    _ = tf.import_graph_def(graph_def, name=\'\')\n  # Works with an arbitrary minibatch size.\n  with tf.Session() as sess:\n    pool3 = sess.graph.get_tensor_by_name(\'pool_3:0\')\n    ops = pool3.graph.get_operations()\n    for op_idx, op in enumerate(ops):\n        for o in op.outputs:\n            shape = o.get_shape()\n            shape = [s.value for s in shape]\n            new_shape = []\n            for j, s in enumerate(shape):\n                if s == 1 and j == 0:\n                    new_shape.append(None)\n                else:\n                    new_shape.append(s)\n            o.set_shape(tf.TensorShape(new_shape))\n    w = sess.graph.get_operation_by_name(""softmax/logits/MatMul"").inputs[1]\n    logits = tf.matmul(tf.squeeze(pool3, [1, 2]), w)\n    softmax = tf.nn.softmax(logits)\n\nif softmax is None:\n  _init_inception()\n'"
supervised/models/__init__.py,0,"b'""""""The models subpackage contains definitions for the following model\narchitectures:\n-  `ResNeXt` for CIFAR10 CIFAR100\nYou can construct a model with random weights by calling its constructor:\n.. code:: python\n    import models\n    resnext29_16_64 = models.ResNeXt29_16_64(num_classes)\n    resnext29_8_64 = models.ResNeXt29_8_64(num_classes)\n    resnet20 = models.ResNet20(num_classes)\n    resnet32 = models.ResNet32(num_classes)\n\n\n.. ResNext: https://arxiv.org/abs/1611.05431\n""""""\n\nfrom .resnext import resnext29_8_64, resnext29_16_64\n#from .resnet import resnet20, resnet32, resnet44, resnet56, resnet110\nfrom .resnet import resnet18, resnet34, resnet50, resnet101, resnet152\nfrom .preresnet import preactresnet18, preactresnet34, preactresnet50, preactresnet101, preactresnet152\n#from .preact_resnet_temp import preactresnet18, preactresnet34, preactresnet50, preactresnet101, preactresnet152\nfrom .caffe_cifar import caffe_cifar\nfrom .densenet import densenet100_12,densenet100_24\nfrom .wide_resnet import wrn28_10, wrn28_2\n\n#from .imagenet_resnet import resnet18, resnet34, resnet50, resnet101, resnet152\n'"
supervised/models/caffe_cifar.py,4,"b""from __future__ import division\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport math\n\n## http://torch.ch/blog/2015/07/30/cifar.html\nclass CifarCaffeNet(nn.Module):\n  def __init__(self, num_classes):\n    super(CifarCaffeNet, self).__init__()\n\n    self.num_classes = num_classes\n\n    self.block_1 = nn.Sequential(\n      nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n      nn.MaxPool2d(kernel_size=3, stride=2),\n      nn.ReLU(),\n      nn.BatchNorm2d(32))\n\n    self.block_2 = nn.Sequential(\n      nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n      nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n      nn.ReLU(),\n      nn.AvgPool2d(kernel_size=3, stride=2),\n      nn.BatchNorm2d(64))\n\n    self.block_3 = nn.Sequential(\n      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n      nn.Conv2d(64,128, kernel_size=3, stride=1, padding=1),\n      nn.ReLU(),\n      nn.AvgPool2d(kernel_size=3, stride=2),\n      nn.BatchNorm2d(128))\n\n    self.classifier = nn.Linear(128*9, self.num_classes)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        init.kaiming_normal(m.weight)\n        m.bias.data.zero_()\n\n  def forward(self, x):\n    x = self.block_1.forward(x)\n    x = self.block_2.forward(x)\n    x = self.block_3.forward(x)\n    x = x.view(x.size(0), -1)\n    #print ('{}'.format(x.size()))\n    return self.classifier(x)\n\ndef caffe_cifar(num_classes=10):\n  model = CifarCaffeNet(num_classes)\n  return model\n"""
supervised/models/densenet.py,5,"b'import math, torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Bottleneck(nn.Module):\n  def __init__(self, nChannels, growthRate):\n    super(Bottleneck, self).__init__()\n    interChannels = 4*growthRate\n    self.bn1 = nn.BatchNorm2d(nChannels)\n    self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1, bias=False)\n    self.bn2 = nn.BatchNorm2d(interChannels)\n    self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3, padding=1, bias=False)\n\n  def forward(self, x):\n    out = self.conv1(F.relu(self.bn1(x)))\n    out = self.conv2(F.relu(self.bn2(out)))\n    out = torch.cat((x, out), 1)\n    return out\n\nclass SingleLayer(nn.Module):\n  def __init__(self, nChannels, growthRate):\n    super(SingleLayer, self).__init__()\n    self.bn1 = nn.BatchNorm2d(nChannels)\n    self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3, padding=1, bias=False)\n\n  def forward(self, x):\n    out = self.conv1(F.relu(self.bn1(x)))\n    out = torch.cat((x, out), 1)\n    return out\n\nclass Transition(nn.Module):\n  def __init__(self, nChannels, nOutChannels):\n    super(Transition, self).__init__()\n    self.bn1 = nn.BatchNorm2d(nChannels)\n    self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1, bias=False)\n\n  def forward(self, x):\n    out = self.conv1(F.relu(self.bn1(x)))\n    out = F.avg_pool2d(out, 2)\n    return out\n\nclass DenseNet(nn.Module):\n  def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n    super(DenseNet, self).__init__()\n\n    if bottleneck:  nDenseBlocks = int( (depth-4) / 6 )\n    else         :  nDenseBlocks = int( (depth-4) / 3 )\n\n    nChannels = 2*growthRate\n    self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1, bias=False)\n\n    self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n    nChannels += nDenseBlocks*growthRate\n    nOutChannels = int(math.floor(nChannels*reduction))\n    self.trans1 = Transition(nChannels, nOutChannels)\n\n    nChannels = nOutChannels\n    self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n    nChannels += nDenseBlocks*growthRate\n    nOutChannels = int(math.floor(nChannels*reduction))\n    self.trans2 = Transition(nChannels, nOutChannels)\n\n    nChannels = nOutChannels\n    self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n    nChannels += nDenseBlocks*growthRate\n\n    self.bn1 = nn.BatchNorm2d(nChannels)\n    self.fc = nn.Linear(nChannels, nClasses)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        m.bias.data.zero_()\n\n  def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n    layers = []\n    for i in range(int(nDenseBlocks)):\n      if bottleneck:\n        layers.append(Bottleneck(nChannels, growthRate))\n      else:\n        layers.append(SingleLayer(nChannels, growthRate))\n      nChannels += growthRate\n    return nn.Sequential(*layers)\n\n  def forward(self, x):\n    out = self.conv1(x)\n    out = self.trans1(self.dense1(out))\n    out = self.trans2(self.dense2(out))\n    out = self.dense3(out)\n    out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n    out = F.log_softmax(self.fc(out))\n    return out\n\ndef densenet100_12(num_classes=10):\n  model = DenseNet(12, 100, 0.5, num_classes, False)\n  return model\n\n\ndef densenet100_24(num_classes=10):\n  model = DenseNet(24, 100, 0.5, num_classes, False)\n  return model\n  \n\n\n'"
supervised/models/imagenet_resnet.py,2,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(num_classes=1000):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n    return model\n\n\ndef resnet34(num_classes=1000):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n    return model\n\n\ndef resnet50(num_classes=1000):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n    return model\n\n\ndef resnet101(num_classes=1000):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n    return model\n\n\ndef resnet152(num_classes=1000):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes)\n    return model\n'"
supervised/models/preact_resnet_temp.py,7,"b'# forward pass to model returns target_a and target_b\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport sys, os\nimport numpy as np\nimport random\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom utils import mixup_data\nfrom load_data import per_image_standardization\n\nclass PreActBlock(nn.Module):\n    \'\'\'Pre-activation version of the BasicBlock.\'\'\'\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    \'\'\'Pre-activation version of the original Bottleneck module.\'\'\'\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        out += shortcut\n        return out\n\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks, initial_channels, num_classes, per_img_std= False):\n        super(PreActResNet, self).__init__()\n        self.in_planes = initial_channels\n        self.num_classes = num_classes\n        self.per_img_std = per_img_std\t\t\n\n        self.conv1 = nn.Conv2d(3, initial_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer1 = self._make_layer(block, initial_channels, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, initial_channels*2, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, initial_channels*4, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, initial_channels*8, num_blocks[3], stride=2)\n        self.linear = nn.Linear(initial_channels*8*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def compute_h1(self,x):\n        out = x\n        out = self.conv1(out)\n        out = self.layer1(out)\n        return out\n\n    def compute_h2(self,x):\n        out = x\n        out = self.conv1(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        return out\n\n    def forward(self, x, target=None, mixup_hidden = False,  mixup_alpha = 0.1, layer_mix=None):\n\n        if self.per_img_std:\n            x = per_image_standardization(x)\n            \n        if mixup_hidden == True:\n            if layer_mix == None:\n                layer_mix = random.randint(0,2)\n\n            out = x\n            \n            if layer_mix == 0:\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n            \n            out = self.conv1(x)\n            \n            out = self.layer1(out)\n    \n            if layer_mix == 1:\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n            \n            out = self.layer2(out)\n    \n            if layer_mix == 2:\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n           \n            out = self.layer3(out)\n            \n            if layer_mix == 3:\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n            \n            \n            \n            out = self.layer4(out)\n            \n            if layer_mix == 4:\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n            \n            \n            out = F.avg_pool2d(out, 4)\n            out = out.view(out.size(0), -1)\n            out = self.linear(out)            \n            #if layer_mix == 4:\n            #    out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n\n            lam = torch.tensor(lam).cuda()\n            lam = lam.repeat(y_a.size())\n            return out, y_a, y_b, lam\n\n        \n        else:\n            out = x\n            out = self.conv1(x)\n            out = self.layer1(out)\n            out = self.layer2(out)\n            out = self.layer3(out)\n            out = self.layer4(out)\n            out = F.avg_pool2d(out, 4)\n            out = out.view(out.size(0), -1)\n            out = self.linear(out)\n            return out\n\n        """"""\n        if layer_mix == \'rand\':\n            if self.mixup_hidden:\n                layer_mix = random.randint(0,2)\n            else:\n                layer_mix = 0\n\n        out = x\n\n        if lam is not None:\n            lam = torch.max(lam, 1-lam)\n            if target_reweighted is None:\n                target_reweighted = to_one_hot(target,self.num_classes)\n            else:\n                assert target is None\n            if layer_mix == 0:\n                out, target_reweighted = mixup_process(out, target_reweighted, lam)\n\n        out = self.conv1(out)\n        out = self.layer1(out)\n\n        if lam is not None and layer_mix == 1:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.layer2(out)\n\n        if lam is not None and layer_mix == 2:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n\n        if lam is None:\n            return out\n        else:\n            return out, target_reweighted\n\t""""""\n\n\ndef preactresnet18(num_classes=10, dropout = False, per_img_std = False):\n    return PreActResNet(PreActBlock, [2,2,2,2], 64, num_classes, per_img_std)\n\ndef preactresnet34(num_classes=10, dropout = False, per_img_std = False):\n    return PreActResNet(PreActBlock, [3,4,6,3], 64, num_classes, per_img_std)\n\ndef preactresnet50(num_classes=10, dropout = False, per_img_std = False):\n    return PreActResNet(PreActBottleneck, [3,4,6,3], 64, num_classes, per_img_std)\n\ndef preactresnet101(num_classes=10, dropout = False, per_img_std = False):\n    return PreActResNet(PreActBottleneck, [3,4,23,3], 64, num_classes, per_img_std)\n\ndef preactresnet152(num_classes=10, dropout = False, per_img_std = False):\n    return PreActResNet(PreActBottleneck, [3,8,36,3], 64, num_classes, per_img_std)\n\n\ndef test():\n    net = PreActResNet152(True,10)\n    y = net(Variable(torch.randn(1,3,32,32)))\n    print(y.size())\n\nif __name__ == ""__main__"":\n    test()\n# test()\n\n'"
supervised/models/preresnet.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\nimport sys,os\nimport numpy as np\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom utils import to_one_hot, mixup_process, get_lambda\nfrom load_data import per_image_standardization\nimport random\n\nclass PreActBlock(nn.Module):\n    \'\'\'Pre-activation version of the BasicBlock.\'\'\'\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    \'\'\'Pre-activation version of the original Bottleneck module.\'\'\'\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        out += shortcut\n        return out\n\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks, initial_channels, num_classes,  per_img_std= False, stride=1):\n        super(PreActResNet, self).__init__()\n        self.in_planes = initial_channels\n        self.num_classes = num_classes\n        self.per_img_std = per_img_std\n        #import pdb; pdb.set_trace()\n        self.conv1 = nn.Conv2d(3, initial_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.layer1 = self._make_layer(block, initial_channels, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, initial_channels*2, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, initial_channels*4, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, initial_channels*8, num_blocks[3], stride=2)\n        self.linear = nn.Linear(initial_channels*8*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def compute_h1(self,x):\n        out = x\n        out = self.conv1(out)\n        out = self.layer1(out)\n        return out\n\n    def compute_h2(self,x):\n        out = x\n        out = self.conv1(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        return out\n\n    def forward(self, x, target= None, mixup=False, mixup_hidden=False, mixup_alpha=None):\n        #import pdb; pdb.set_trace()\n        if self.per_img_std:\n            x = per_image_standardization(x)\n        \n        if mixup_hidden:\n            layer_mix = random.randint(0,2)\n        elif mixup:\n            layer_mix = 0\n        else:\n            layer_mix = None   \n        \n        out = x\n        \n        if mixup_alpha is not None:\n            lam = get_lambda(mixup_alpha)\n            lam = torch.from_numpy(np.array([lam]).astype(\'float32\')).cuda()\n            lam = Variable(lam)\n        \n        if target is not None :\n            target_reweighted = to_one_hot(target,self.num_classes)\n        \n        if layer_mix == 0:\n                out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.conv1(out)\n        out = self.layer1(out)\n\n        if layer_mix == 1:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.layer2(out)\n\n        if layer_mix == 2:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        \n        out = self.layer3(out)\n        if  layer_mix == 3:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        \n        if target is not None:\n            return out, target_reweighted\n        else: \n            return out\n\n\ndef preactresnet18(num_classes=10, dropout = False,  per_img_std = False, stride=1):\n    return PreActResNet(PreActBlock, [2,2,2,2], 64, num_classes,  per_img_std, stride= stride)\n\ndef preactresnet34(num_classes=10, dropout = False,  per_img_std = False, stride=1):\n    return PreActResNet(PreActBlock, [3,4,6,3], 64, num_classes,  per_img_std, stride= stride)\n\ndef preactresnet50(num_classes=10, dropout = False,  per_img_std = False, stride=1):\n    return PreActResNet(PreActBottleneck, [3,4,6,3], 64, num_classes,  per_img_std, stride= stride)\n\ndef preactresnet101(num_classes=10, dropout = False,  per_img_std = False, stride=1):\n    return PreActResNet(PreActBottleneck, [3,4,23,3], 64, num_classes, per_img_std, stride= stride)\n\ndef preactresnet152(num_classes=10, dropout = False,  per_img_std = False, stride=1):\n    return PreActResNet(PreActBottleneck, [3,8,36,3], 64, num_classes, per_img_std, stride= stride)\n\ndef test():\n    net = PreActResNet152(True,10)\n    y = net(Variable(torch.randn(1,3,32,32)))\n    print(y.size())\n\nif __name__ == ""__main__"":\n    test()\n# test()\n\n'"
supervised/models/res_utils.py,2,"b'import torch\nimport torch.nn as nn\n\nclass DownsampleA(nn.Module):  \n\n  def __init__(self, nIn, nOut, stride):\n    super(DownsampleA, self).__init__() \n    assert stride == 2    \n    self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)   \n\n  def forward(self, x):   \n    x = self.avg(x)  \n    return torch.cat((x, x.mul(0)), 1)  \n\nclass DownsampleC(nn.Module):     \n\n  def __init__(self, nIn, nOut, stride):\n    super(DownsampleC, self).__init__()\n    assert stride != 1 or nIn != nOut\n    self.conv = nn.Conv2d(nIn, nOut, kernel_size=1, stride=stride, padding=0, bias=False)\n\n  def forward(self, x):\n    x = self.conv(x)\n    return x\n\nclass DownsampleD(nn.Module):\n\n  def __init__(self, nIn, nOut, stride):\n    super(DownsampleD, self).__init__()\n    assert stride == 2\n    self.conv = nn.Conv2d(nIn, nOut, kernel_size=2, stride=stride, padding=0, bias=False)\n    self.bn   = nn.BatchNorm2d(nOut)\n\n  def forward(self, x):\n    x = self.conv(x)\n    x = self.bn(x)\n    return x\n'"
supervised/models/resnet.py,3,"b'## https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n\'\'\'ResNet in PyTorch.\nFor Pre-activation ResNet, see \'preact_resnet.py\'.\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport random\nimport sys, os\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom utils import mixup_data\nfrom load_data import per_image_standardization\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10, per_img_std = False):\n        super(ResNet, self).__init__()\n        self.per_img_std = per_img_std\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x, target=None, mixup_hidden = False,  mixup_alpha = 0.1, layer_mix=None):\n        if self.per_img_std:\n            x = per_image_standardization(x)\n        \n        if mixup_hidden == True:\n            if layer_mix == None:\n                layer_mix = random.randint(0,2)\n            \n            out = x\n            \n            if layer_mix == 0:\n                #out = lam * out + (1 - lam) * out[index,:]\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n            #print (out)       \n            \n            out = F.relu(self.bn1(self.conv1(x)))\n            \n            out = self.layer1(out)\n    \n            if layer_mix == 1:\n                #out = lam * out + (1 - lam) * out[index,:]\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n            \n            #print (out)\n\n            out = self.layer2(out)\n    \n            if layer_mix == 2:\n                #out = lam * out + (1 - lam) * out[index,:]\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n           #print (out)\n\n            out = self.layer3(out)\n            \n            if layer_mix == 3:\n                #out = lam * out + (1 - lam) * out[index,:]\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n            #print (out)\n\n            out = self.layer4(out)\n            \n            if layer_mix == 4:\n                #out = lam * out + (1 - lam) * out[index,:]\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n\n            #print (out)\n            out = F.avg_pool2d(out, 4)\n            out = out.view(out.size(0), -1)\n            out = self.linear(out)\n            \n            if layer_mix == 5:\n                #out = lam * out + (1 - lam) * out[index,:]\n                out, y_a, y_b, lam = mixup_data(out, target, mixup_alpha)\n            \n            lam = torch.tensor(lam).cuda()\n            lam = lam.repeat(y_a.size())\n            #d = {}\n            #d[\'out\'] = out\n            #d[\'target_a\'] = y_a\n            #d[\'target_b\'] = y_b\n            #d[\'lam\'] = lam\n            #print (out.shape)\n            #print (y_a.shape)\n            #print (y_b.size()) \n            #print (lam.size())\n            return out, y_a, y_b, lam\n\n        \n        else:\n            out = x\n            out = F.relu(self.bn1(self.conv1(x)))\n            out = self.layer1(out)\n            out = self.layer2(out)\n            out = self.layer3(out)\n            out = self.layer4(out)\n            out = F.avg_pool2d(out, 4)\n            out = out.view(out.size(0), -1)\n            out = self.linear(out)\n            return out\n        \n\ndef resnet18(num_classes=10, dropout = False, per_img_std = False):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes, per_img_std = per_img_std)\n    return model\n\n\ndef resnet34(num_classes=10, dropout = False, per_img_std = False):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], num_classes, per_img_std = per_img_std)\n    return model\n\n\ndef resnet50(num_classes=10, dropout = False, per_img_std = False):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes, per_img_std = per_img_std)\n    return model\n\n\ndef resnet101(num_classes=10, dropout = False, per_img_std = False):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes, per_img_std = per_img_std)\n    return model\n\n\ndef resnet152(num_classes=10, dropout = False, per_img_std = False):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes, per_img_std = per_img_std)\n    return model'"
supervised/models/resnet_old.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom .res_utils import DownsampleA, DownsampleC, DownsampleD\nimport math\n\n\nclass ResNetBasicblock(nn.Module):\n  expansion = 1\n  """"""\n  RexNet basicblock (https://github.com/facebook/fb.resnet.torch/blob/master/models/resnet.lua)\n  """"""\n  def __init__(self, inplanes, planes, stride=1, downsample=None):\n    super(ResNetBasicblock, self).__init__()\n\n    self.conv_a = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n    self.bn_a = nn.BatchNorm2d(planes)\n\n    self.conv_b = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn_b = nn.BatchNorm2d(planes)\n\n    self.downsample = downsample\n\n  def forward(self, x):\n    residual = x\n\n    basicblock = self.conv_a(x)\n    basicblock = self.bn_a(basicblock)\n    basicblock = F.relu(basicblock, inplace=True)\n\n    basicblock = self.conv_b(basicblock)\n    basicblock = self.bn_b(basicblock)\n\n    if self.downsample is not None:\n      residual = self.downsample(x)\n    \n    return F.relu(residual + basicblock, inplace=True)\n\nclass CifarResNet(nn.Module):\n  """"""\n  ResNet optimized for the Cifar dataset, as specified in\n  https://arxiv.org/abs/1512.03385.pdf\n  """"""\n  def __init__(self, block, depth, num_classes,dropout):\n    """""" Constructor\n    Args:\n      depth: number of layers.\n      num_classes: number of classes\n      base_width: base width\n    """"""\n    super(CifarResNet, self).__init__()\n\n    #Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n    assert (depth - 2) % 6 == 0, \'depth should be one of 20, 32, 44, 56, 110\'\n    layer_blocks = (depth - 2) // 6\n    print (\'CifarResNet : Depth : {} , Layers for each block : {}\'.format(depth, layer_blocks))\n\n    self.num_classes = num_classes\n    self.dropout = dropout\n    #print (dropout)\n    self.conv_1_3x3 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n    self.bn_1 = nn.BatchNorm2d(16)\n\n    self.inplanes = 16\n    self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n    self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n    self.stage_3 = self._make_layer(block, 64, layer_blocks, 2)\n    self.avgpool = nn.AvgPool2d(8)\n    self.classifier = nn.Linear(64*block.expansion, num_classes)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        #m.bias.data.zero_()\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        init.kaiming_normal(m.weight)\n        m.bias.data.zero_()\n\n  def _make_layer(self, block, planes, blocks, stride=1):\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n      downsample = DownsampleA(self.inplanes, planes * block.expansion, stride)\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n      layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n  def forward(self, x):\n    x = self.conv_1_3x3(x)\n    x = F.relu(self.bn_1(x), inplace=True)\n    x = self.stage_1(x)\n    x = self.stage_2(x)\n    x = self.stage_3(x)\n    x = self.avgpool(x)\n    x = x.view(x.size(0), -1)\n    if self.dropout:\n            x = F.dropout(x, p=0.5, training=self.training)\n    return self.classifier(x)\n\ndef resnet20(num_classes=10,dropout=True):\n  """"""Constructs a ResNet-20 model for CIFAR-10 (by default)\n  Args:\n    num_classes (uint): number of classes\n  """"""\n  model = CifarResNet(ResNetBasicblock, 20, num_classes,dropout)\n  return model\n\ndef resnet32(num_classes=10,dropout=True):\n  """"""Constructs a ResNet-32 model for CIFAR-10 (by default)\n  Args:\n    num_classes (uint): number of classes\n  """"""\n  model = CifarResNet(ResNetBasicblock, 32, num_classes,dropout)\n  return model\n\ndef resnet44(num_classes=10,dropout=True):\n  """"""Constructs a ResNet-44 model for CIFAR-10 (by default)\n  Args:\n    num_classes (uint): number of classes\n  """"""\n  model = CifarResNet(ResNetBasicblock, 44, num_classes,dropout)\n  return model\n\ndef resnet56(num_classes=10,dropout=True):\n  """"""Constructs a ResNet-56 model for CIFAR-10 (by default)\n  Args:\n    num_classes (uint): number of classes\n  """"""\n  model = CifarResNet(ResNetBasicblock, 56, num_classes,dropout)\n  return model\n\ndef resnet110(num_classes=10,dropout=True):\n  """"""Constructs a ResNet-110 model for CIFAR-10 (by default)\n  Args:\n    num_classes (uint): number of classes\n  """"""\n  model = CifarResNet(ResNetBasicblock, 110, num_classes,dropout)\n  return model\n'"
supervised/models/resnext.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport math\nfrom torch.autograd import Variable\nimport sys,os\nimport numpy as np\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom utils import to_one_hot, mixup_process, get_lambda\nfrom load_data import per_image_standardization\nimport random\n\nclass ResNeXtBottleneck(nn.Module):\n  expansion = 4\n  """"""\n  RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n  """"""\n  def __init__(self, inplanes, planes, cardinality, base_width, stride=1, downsample=None):\n    super(ResNeXtBottleneck, self).__init__()\n\n    D = int(math.floor(planes * (base_width/64.0)))\n    C = cardinality\n\n    self.conv_reduce = nn.Conv2d(inplanes, D*C, kernel_size=1, stride=1, padding=0, bias=False)\n    self.bn_reduce = nn.BatchNorm2d(D*C)\n\n    self.conv_conv = nn.Conv2d(D*C, D*C, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n    self.bn = nn.BatchNorm2d(D*C)\n\n    self.conv_expand = nn.Conv2d(D*C, planes*4, kernel_size=1, stride=1, padding=0, bias=False)\n    self.bn_expand = nn.BatchNorm2d(planes*4)\n\n    self.downsample = downsample\n\n  def forward(self, x):\n    residual = x\n\n    bottleneck = self.conv_reduce(x)\n    bottleneck = F.relu(self.bn_reduce(bottleneck), inplace=True)\n\n    bottleneck = self.conv_conv(bottleneck)\n    bottleneck = F.relu(self.bn(bottleneck), inplace=True)\n\n    bottleneck = self.conv_expand(bottleneck)\n    bottleneck = self.bn_expand(bottleneck)\n\n    if self.downsample is not None:\n      residual = self.downsample(x)\n    \n    return F.relu(residual + bottleneck, inplace=True)\n\n\nclass CifarResNeXt(nn.Module):\n  """"""\n  ResNext optimized for the Cifar dataset, as specified in\n  https://arxiv.org/pdf/1611.05431.pdf\n  """"""\n  def __init__(self, block, depth, cardinality, base_width, num_classes, dropout, per_img_std= False):\n    super(CifarResNeXt, self).__init__()\n    self.num_classes = num_classes\n    self.per_img_std = per_img_std\n    #Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n    assert (depth - 2) % 9 == 0, \'depth should be one of 29, 38, 47, 56, 101\'\n    layer_blocks = (depth - 2) // 9\n\n    self.cardinality = cardinality\n    self.base_width = base_width\n    self.num_classes = num_classes\n    self.dropout=dropout\n    self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n    self.bn_1 = nn.BatchNorm2d(64)\n\n    self.inplanes = 64\n    self.stage_1 = self._make_layer(block, 64 , layer_blocks, 1)\n    self.stage_2 = self._make_layer(block, 128, layer_blocks, 2)\n    self.stage_3 = self._make_layer(block, 256, layer_blocks, 2)\n    self.avgpool = nn.AvgPool2d(8)\n    self.classifier = nn.Linear(256*block.expansion, num_classes)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        init.kaiming_normal(m.weight)\n        m.bias.data.zero_()\n\n  def _make_layer(self, block, planes, blocks, stride=1):\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n      downsample = nn.Sequential(\n        nn.Conv2d(self.inplanes, planes * block.expansion,\n              kernel_size=1, stride=stride, bias=False),\n        nn.BatchNorm2d(planes * block.expansion),\n      )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, self.cardinality, self.base_width, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n      layers.append(block(self.inplanes, planes, self.cardinality, self.base_width))\n\n    return nn.Sequential(*layers)\n\n  def forward(self, x, target= None, mixup=False, mixup_hidden=False, mixup_alpha=None):\n        \n        if self.per_img_std:\n            x = per_image_standardization(x)\n        \n        if mixup_hidden:\n            layer_mix = random.randint(0,2)\n        elif mixup:\n            layer_mix = 0\n        else:\n            layer_mix = None   \n        \n        out = x\n        \n        if mixup_alpha is not None:\n            lam = get_lambda(mixup_alpha)\n            lam = torch.from_numpy(np.array([lam]).astype(\'float32\')).cuda()\n            lam = Variable(lam)\n        \n        if target is not None :\n            target_reweighted = to_one_hot(target,self.num_classes)\n        \n        if layer_mix == 0:\n                out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.conv_1_3x3(out)\n        out = F.relu(self.bn_1(out), inplace=True)\n        out = self.stage_1(out)\n\n        if layer_mix == 1:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.stage_2(out)\n\n        if layer_mix == 2:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.stage_3(out)\n        if  layer_mix == 3:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n        \n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        if self.dropout:\n                out = F.dropout(out, p=0.5, training=self.training)\n        out = self.classifier(out)\n        \n        if target is not None:\n            return out, target_reweighted\n        else: \n            return out\n        \ndef resnext29_16_64(num_classes=10,dropout=False, per_img_std = False):\n  """"""Constructs a ResNeXt-29, 16*64d model for CIdropoutFAR-10 (by default)\n  \n  Args:\n    num_classes (uint): number of classes\n  """"""\n  model = CifarResNeXt(ResNeXtBottleneck, 29, 16, 64, num_classes, dropout, per_img_std)\n  return model\n\ndef resnext29_8_64(num_classes=10, dropout=False, per_img_std = False):\n  """"""Constructs a ResNeXt-29, 8*64d model for CIFAR-10 (by default)\n  \n  Args:\n    num_classes (uint): number of classes\n  """"""\n  model = CifarResNeXt(ResNeXtBottleneck, 29, 8, 64, num_classes, dropout, per_img_std)\n  return model\n'"
supervised/models/utils.py,4,"b'import os, sys, time\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\n\nclass AverageMeter(object):\n  """"""Computes and stores the average and current value""""""\n  def __init__(self):\n    self.reset()\n\n  def reset(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n\n  def update(self, val, n=1):\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n\n\nclass RecorderMeter(object):\n  """"""Computes and stores the minimum loss value and its epoch index""""""\n  def __init__(self, total_epoch):\n    self.reset(total_epoch)\n\n  def reset(self, total_epoch):\n    assert total_epoch > 0\n    self.total_epoch   = total_epoch\n    self.current_epoch = 0\n    self.epoch_losses  = np.zeros((self.total_epoch, 2), dtype=np.float32) # [epoch, train/val]\n    self.epoch_losses  = self.epoch_losses - 1\n\n    self.epoch_accuracy= np.zeros((self.total_epoch, 2), dtype=np.float32) # [epoch, train/val]\n    self.epoch_accuracy= self.epoch_accuracy\n\n  def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n    assert idx >= 0 and idx < self.total_epoch, \'total_epoch : {} , but update with the {} index\'.format(self.total_epoch, idx)\n    self.epoch_losses  [idx, 0] = train_loss\n    self.epoch_losses  [idx, 1] = val_loss\n    self.epoch_accuracy[idx, 0] = train_acc\n    self.epoch_accuracy[idx, 1] = val_acc\n    self.current_epoch = idx + 1\n    return self.max_accuracy(False) == val_acc\n\n  def max_accuracy(self, istrain):\n    if self.current_epoch <= 0: return 0\n    if istrain: return self.epoch_accuracy[:self.current_epoch, 0].max()\n    else:       return self.epoch_accuracy[:self.current_epoch, 1].max()\n  \n  def plot_curve(self, save_path):\n    title = \'the accuracy/loss curve of train/val\'\n    dpi = 80  \n    width, height = 1200, 800\n    legend_fontsize = 10\n    scale_distance = 48.8\n    figsize = width / float(dpi), height / float(dpi)\n\n    fig = plt.figure(figsize=figsize)\n    x_axis = np.array([i for i in range(self.total_epoch)]) # epochs\n    y_axis = np.zeros(self.total_epoch)\n\n    plt.xlim(0, self.total_epoch)\n    plt.ylim(0, 100)\n    interval_y = 5\n    interval_x = 5\n    plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n    plt.yticks(np.arange(0, 100 + interval_y, interval_y))\n    plt.grid()\n    plt.title(title, fontsize=20)\n    plt.xlabel(\'the training epoch\', fontsize=16)\n    plt.ylabel(\'accuracy\', fontsize=16)\n  \n    y_axis[:] = self.epoch_accuracy[:, 0]\n    plt.plot(x_axis, y_axis, color=\'g\', linestyle=\'-\', label=\'train-accuracy\', lw=2)\n    plt.legend(loc=4, fontsize=legend_fontsize)\n\n    y_axis[:] = self.epoch_accuracy[:, 1]\n    plt.plot(x_axis, y_axis, color=\'y\', linestyle=\'-\', label=\'valid-accuracy\', lw=2)\n    plt.legend(loc=4, fontsize=legend_fontsize)\n\n    \n    y_axis[:] = self.epoch_losses[:, 0]\n    plt.plot(x_axis, y_axis*50, color=\'g\', linestyle=\':\', label=\'train-loss-x50\', lw=2)\n    plt.legend(loc=4, fontsize=legend_fontsize)\n\n    y_axis[:] = self.epoch_losses[:, 1]\n    plt.plot(x_axis, y_axis*50, color=\'y\', linestyle=\':\', label=\'valid-loss-x50\', lw=2)\n    plt.legend(loc=4, fontsize=legend_fontsize)\n\n    if save_path is not None:\n      fig.savefig(save_path, dpi=dpi, bbox_inches=\'tight\')\n      print (\'---- save figure {} into {}\'.format(title, save_path))\n    plt.close(fig)\n    \n\ndef time_string():\n  ISOTIMEFORMAT=\'%Y-%m-%d %X\'\n  string = \'[{}]\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n  return string\n\ndef convert_secs2time(epoch_time):\n  need_hour = int(epoch_time / 3600)\n  need_mins = int((epoch_time - 3600*need_hour) / 60)\n  need_secs = int(epoch_time - 3600*need_hour - 60*need_mins)\n  return need_hour, need_mins, need_secs\n\ndef time_file_str():\n  ISOTIMEFORMAT=\'%Y-%m-%d\'\n  string = \'{}\'.format(time.strftime( ISOTIMEFORMAT, time.gmtime(time.time()) ))\n  return string + \'-{}\'.format(random.randint(1, 10000))\n\n\ndef mixup_process(out, target_reweighted, lam):\n    indices = np.random.permutation(out.size(0))\n    out = out*lam + out[indices]*(1-lam)\n    target_shuffled_onehot = target_reweighted[indices]\n    target_reweighted = target_reweighted * lam + target_shuffled_onehot * (1 - lam)\n    return out, target_reweighted\n\n\ndef mixup_data(x, y, alpha):\n\n    \'\'\'Compute the mixup data. Return mixed inputs, pairs of targets, and lambda\'\'\'\n    if alpha > 0.:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).cuda()\n    mixed_x = lam * x + (1 - lam) * x[index,:]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\nclass Cutout(object):\n    """"""Randomly mask out one or more patches from an image.\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n    """"""\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def apply(self, img):\n        """"""\n        Args:\n            img (Tensor): Tensor image of size (C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        """"""\n        h = img.size(2)\n        w = img.size(3)\n\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = int(np.clip(y - self.length / 2, 0, h))\n            y2 = int(np.clip(y + self.length / 2, 0, h))\n            x1 = int(np.clip(x - self.length / 2, 0, w))\n            x2 = int(np.clip(x + self.length / 2, 0, w))\n\n            mask[y1: y2, x1: x2] = 0.\n\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img).cuda()\n        img = img * mask\n\n        return img\n\n\n\ndef to_one_hot(inp,num_classes):\n    y_onehot = torch.FloatTensor(inp.size(0), num_classes)\n    y_onehot.zero_()\n\n    y_onehot.scatter_(1, inp.unsqueeze(1).data.cpu(), 1)\n    \n    #return Variable(y_onehot.cuda(),requires_grad=False)\n    return y_onehot\n\n'"
supervised/models/wide_resnet.py,7,"b'### dropout has been removed in this code. original code had dropout#####\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport sys, os\nimport numpy as np\nimport random\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom utils import to_one_hot, mixup_process, get_lambda\nfrom load_data import per_image_standardization\nact = torch.nn.ReLU()\n\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\ndef conv_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n        init.constant(m.bias, 0)\n    elif classname.find(\'BatchNorm\') != -1:\n        init.constant(m.weight, 1)\n        init.constant(m.bias, 0)\n\nclass wide_basic(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(wide_basic, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n            )\n\n    def forward(self, x):\n        out = self.conv1(act(self.bn1(x)))\n        out = self.conv2(act(self.bn2(out)))\n        out += self.shortcut(x)\n\n        return out\n\nclass Wide_ResNet(nn.Module):\n    \n    def __init__(self, depth, widen_factor, num_classes, per_img_std= False, stride = 1):\n        super(Wide_ResNet, self).__init__()\n        self.num_classes = num_classes\n        self.per_img_std = per_img_std\n        self.in_planes = 16\n\n        assert ((depth-4)%6 ==0), \'Wide-resnet_v2 depth should be 6n+4\'\n        n = int((depth-4)/6)\n        k = widen_factor\n\n        print(\'| Wide-Resnet %dx%d\' %(depth, k))\n        nStages = [16, 16*k, 32*k, 64*k]\n\n        self.conv1 = conv3x3(3,nStages[0], stride = stride)\n        self.layer1 = self._wide_layer(wide_basic, nStages[1], n, stride=1)\n        self.layer2 = self._wide_layer(wide_basic, nStages[2], n, stride=2)\n        self.layer3 = self._wide_layer(wide_basic, nStages[3], n, stride=2)\n        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n        self.linear = nn.Linear(nStages[3], num_classes)\n\n    def _wide_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes\n\n        return nn.Sequential(*layers)\n    \n    """"""\n    ## Modified WRN architecture###\n    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n        super(Wide_ResNet, self).__init__()\n        self.in_planes = 16\n\n        assert ((depth-4)%6 ==0), \'Wide-resnet_v2 depth should be 6n+4\'\n        n = (depth-4)/6\n        k = widen_factor\n        #self.mixup_hidden = mixup_hidden\n\n        print(\'| Wide-Resnet %dx%d\' %(depth, k))\n        nStages = [16, 16*k, 32*k, 64*k]\n\n        self.conv1 = conv3x3(3,nStages[0])\n        self.bn1 = nn.BatchNorm2d(nStages[0])\n        self.layer1 = self._wide_layer(wide_basic, nStages[1], n, dropout_rate, stride=1)\n        self.layer2 = self._wide_layer(wide_basic, nStages[2], n, dropout_rate, stride=2)\n        self.layer3 = self._wide_layer(wide_basic, nStages[3], n, dropout_rate, stride=2)\n        #self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n        self.linear = nn.Linear(nStages[3], num_classes)\n\n    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n            self.in_planes = planes\n\n        return nn.Sequential(*layers)\n    """"""\n    def forward(self, x, target= None, mixup=False, mixup_hidden=False, mixup_alpha=None):\n        #print x.shape\n        if self.per_img_std:\n            x = per_image_standardization(x)\n        \n        if mixup_hidden:\n            layer_mix = random.randint(0,2)\n        elif mixup:\n            layer_mix = 0\n        else:\n            layer_mix = None   \n        \n        out = x\n        \n        if mixup_alpha is not None:\n            lam = get_lambda(mixup_alpha)\n            lam = torch.from_numpy(np.array([lam]).astype(\'float32\')).cuda()\n            lam = Variable(lam)\n        \n        if target is not None :\n            target_reweighted = to_one_hot(target,self.num_classes)\n            \n        if layer_mix == 0:\n                out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.conv1(out)\n        out = self.layer1(out)\n        \n        \n        if layer_mix == 1:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = self.layer2(out)\n\n        if layer_mix == 2:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        \n        out = self.layer3(out)\n        if  layer_mix == 3:\n            out, target_reweighted = mixup_process(out, target_reweighted, lam=lam)\n\n        out = act(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        \n        if target is not None:\n            return out, target_reweighted\n        else: \n            return out\n        \n                  \n        \ndef wrn28_10(num_classes=10, dropout = False, per_img_std = False, stride = 1):\n    #print (\'this\')\n    model = Wide_ResNet(depth=28, widen_factor=10, num_classes=num_classes, per_img_std = per_img_std, stride = stride)\n    return model\n\ndef wrn28_2(num_classes=10, dropout = False, per_img_std = False, stride = 1):\n    #print (\'this\')\n    model = Wide_ResNet(depth =28, widen_factor =2, num_classes = num_classes, per_img_std = per_img_std, stride = stride)\n    return model\n\n\n\nif __name__ == \'__main__\':\n    net=Wide_ResNet(28, 10, 0.3, 10)\n    y = net(Variable(torch.randn(1,3,32,32)))\n\n    print(y.size())\n'"
gan/christorch/gan/base.py,11,"b'import torch\nimport os\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nfrom torch import optim\nfrom skimage.io import imsave\nfrom christorch import util\n\nclass GAN:\n    """"""\n    Base model for GAN models\n    """"""\n    def __init__(self,\n                 gen_fn,\n                 disc_fn,\n                 z_dim,\n                 opt_g=optim.Adam,\n                 opt_d=optim.Adam,\n                 opt_d_args={\'lr\':0.0002, \'betas\':(0.5, 0.999)},\n                 opt_g_args={\'lr\':0.0002, \'betas\':(0.5, 0.999)},\n                 dnorm=None,\n                 handlers=[],\n                 scheduler_fn=None,\n                 scheduler_args={},\n                 use_cuda=\'detect\'):\n        assert use_cuda in [True, False, \'detect\']\n        if use_cuda == \'detect\':\n            use_cuda = True if torch.cuda.is_available() else False\n        self.z_dim = z_dim\n        self.dnorm = dnorm\n        self.g = gen_fn\n        self.d = disc_fn\n        optim_g = opt_g(filter(lambda p: p.requires_grad,\n                               self.g.parameters()), **opt_g_args)\n        optim_d = opt_d(filter(lambda p: p.requires_grad,\n                               self.d.parameters()), **opt_d_args)\n        self.optim = {\n            \'g\': optim_g,\n            \'d\': optim_d,\n        }\n        self.scheduler = {}\n        if scheduler_fn is not None:\n            for key in self.optim:\n                self.scheduler[key] = scheduler_fn(\n                    self.optim[key], **scheduler_args)\n        self.handlers = handlers\n        self.use_cuda = use_cuda\n        if self.use_cuda:\n            self.g.cuda()\n            self.d.cuda()\n        self.last_epoch = 0\n\n    def _get_stats(self, dict_, mode):\n        stats = OrderedDict({})\n        for key in dict_.keys():\n            stats[key] = np.mean(dict_[key])\n        return stats\n\n    def sample_z(self, bs, seed=None):\n        """"""Return a sample z ~ p(z)""""""\n        if seed is not None:\n            rnd_state = np.random.RandomState(seed)\n            z = torch.from_numpy(\n                rnd_state.normal(0, 1, size=(bs, self.z_dim))\n            ).float()\n        else:\n            z = torch.from_numpy(\n                np.random.normal(0, 1, size=(bs, self.z_dim))\n            ).float()\n        if self.use_cuda:\n            z = z.cuda()\n        z = Variable(z)\n        return z\n\n    def mse(self, prediction, target):\n        if not hasattr(target, \'__len__\'):\n            target = torch.ones_like(prediction)*target\n            if prediction.is_cuda:\n                target = target.cuda()\n            target = Variable(target)\n        return torch.nn.MSELoss()(prediction, target)\n\n    def _train(self):\n        self.g.train()\n        self.d.train()\n\n    def _eval(self):\n        self.g.eval()\n        self.d.eval()\n        \n    def train_on_instance(self, z, x):\n        self._train()\n        # Train the generator.\n        self.optim[\'g\'].zero_grad()\n        fake = self.g(z)\n        d_fake = self.d(fake)\n        gen_loss = self.mse(d_fake, 1)\n        gen_loss.backward()\n        self.optim[\'g\'].step()\n        # Train the discriminator.\n        self.optim[\'d\'].zero_grad()\n        d_fake = self.d(fake.detach())\n        d_real = self.d(x)\n        d_loss = self.mse(d_real, 1) + self.mse(d_fake, 0)\n        d_loss.backward()\n        self.optim[\'d\'].step()\n        losses = {\n            \'g_loss\': gen_loss.data.item(),\n            \'d_loss\': d_loss.data.item()\n        }\n        outputs = {\n            \'x\': x.detach(),\n            \'gz\': fake.detach(),\n        }\n        return losses, outputs\n\n    def sample(self, bs, seed=None):\n        """"""Return a sample G(z)""""""\n        self._eval()\n        with torch.no_grad():\n            z_batch = self.sample_z(bs, seed=seed)\n            gz = self.g(z_batch)\n        return gz\n\n    def visualise(self, gz_batch, out_file):\n        """"""Save samples g(z) to disk""""""\n        h, w = gz_batch.shape[-2], gz_batch.shape[-1]\n        vis_dim = int(np.floor(np.sqrt(gz_batch.shape[0])))\n        vis = np.zeros((vis_dim*h, vis_dim*w, 3))\n        c = 0\n        for i in range(vis_dim):\n            for j in range(vis_dim):\n                vis[i*h:(i+1)*h, j*w:(j+1)*w, :] = util.convert_to_rgb(\n                    gz_batch[c])\n                c += 1\n        imsave(arr=vis, fname=out_file)\n    \n    def prepare_batch(self, X_batch):\n        X_batch = X_batch.float()\n        if self.use_cuda:\n            X_batch = X_batch.cuda()\n        X_batch = Variable(X_batch)\n        return X_batch\n\n    def train(self,\n              itr,\n              epochs,\n              model_dir,\n              result_dir,\n              append=False,\n              save_every=1,\n              scheduler_fn=None,\n              scheduler_args={},\n              verbose=True):\n        for folder_name in [model_dir, result_dir]:\n            if folder_name is not None and not os.path.exists(folder_name):\n                os.makedirs(folder_name)\n        f_mode = \'w\' if not append else \'a\'\n        f = None\n        if result_dir is not None:\n            f = open(""%s/results.txt"" % result_dir, f_mode)\n        for epoch in range(self.last_epoch, epochs):\n            # Training\n            epoch_start_time = time.time()\n            if verbose:\n                pbar = tqdm(total=len(itr))\n            train_dict = OrderedDict({\'epoch\': epoch+1})\n            for b, X_batch in enumerate(itr):\n                X_batch = self.prepare_batch(X_batch)\n                Z_batch = self.sample_z(X_batch.size()[0])\n                losses, outputs = self.train_on_instance(Z_batch, X_batch,\n                                                         iter=b+1)\n                for key in losses:\n                    this_key = \'train_%s\' % key\n                    if this_key not in train_dict:\n                        train_dict[this_key] = []\n                    train_dict[this_key].append(losses[key])\n                pbar.update(1)\n                pbar.set_postfix(self._get_stats(train_dict, \'train\'))\n                # Process handlers.\n                for handler_fn in self.handlers:\n                    handler_fn(losses, (Z_batch, X_batch), outputs,\n                               {\'epoch\':epoch+1, \'iter\':b+1, \'mode\':\'train\'})\n            if verbose:\n                pbar.close()\n            # Step learning rates.\n            for key in self.scheduler:\n                self.scheduler[key].step()\n            all_dict = train_dict\n            for key in all_dict:\n                all_dict[key] = np.mean(all_dict[key])\n            for key in self.optim:\n                all_dict[""lr_%s"" % key] = \\\n                    self.optim[key].state_dict()[\'param_groups\'][0][\'lr\']\n            all_dict[\'time\'] = \\\n                time.time() - epoch_start_time\n            str_ = "","".join([str(all_dict[key]) for key in all_dict])\n            print(str_)\n            if f is not None:\n                if (epoch+1) == 1 and not append:\n                    # If we\'re not resuming, then write the header.\n                    f.write("","".join(all_dict.keys()) + ""\\n"")\n                f.write(str_ + ""\\n"")\n                f.flush()\n            if (epoch+1) % save_every == 0 and model_dir is not None:\n                self.save(filename=""%s/%i.pkl"" % (model_dir, epoch+1),\n                          epoch=epoch+1)\n            # Save some visualisations. We fix the z for this one\n            # so we can monitor the evolution over several epochs.\n            bs = itr.batch_size\n            gz_batch = self.sample(bs, seed=42).data.cpu().numpy()\n            self.visualise(gz_batch,\n                           out_file=""%s/samplef_%i.png"" % (result_dir, epoch+1))\n            \n        if f is not None:\n            f.close()\n\n    def save(self, filename, epoch, legacy=False):\n        if legacy:\n            torch.save(\n                (self.g.state_dict(),\n                 self.d.state_dict()),\n                filename)\n        else:\n            dd = {}\n            dd[\'g\'] = self.g.state_dict()\n            dd[\'d\'] = self.d.state_dict()\n            for key in self.optim:\n                dd[\'optim_\' + key] = self.optim[key].state_dict()\n            dd[\'epoch\'] = epoch\n            torch.save(dd, filename)\n\n    def load(self, filename, legacy=False, ignore_d=False):\n        """"""\n        ignore_d: if `True`, then don\'t load in the\n          discriminator.\n        """"""\n        if not self.use_cuda:\n            map_location = lambda storage, loc: storage\n        else:\n            map_location = None\n        if legacy:\n            g, d = torch.load(filename,\n                              map_location=map_location)\n            self.g.load_state_dict(g)\n            if not ignore_d:\n                self.d.load_state_dict(d)\n        else:\n            dd = torch.load(filename,\n                            map_location=map_location)\n            self.g.load_state_dict(dd[\'g\'])\n            if not ignore_d:\n                self.d.load_state_dict(dd[\'d\'])\n            for key in self.optim:\n                if ignore_d and key == \'d\':\n                    continue\n                self.optim[key].load_state_dict(dd[\'optim_\'+key])\n            self.last_epoch = dd[\'epoch\']\n            \n'"
