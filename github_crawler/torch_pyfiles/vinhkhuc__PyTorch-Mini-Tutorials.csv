file_path,api_count,code
0_multiply.py,2,"b'import torch\n\na = torch.IntTensor([2, 3, 4])\nb = torch.IntTensor([3, 4, 5])\nm = a * b  # element-wise product\nprint(m.numpy())  # convert to the numpy array [ 6 12 20]\n'"
1_linear_regression.py,7,"b'import torch\nfrom torch.autograd import Variable\nfrom torch import optim\n\n\ndef build_model():\n    model = torch.nn.Sequential()\n    model.add_module(""linear"", torch.nn.Linear(1, 1, bias=False))\n    return model\n\n\ndef train(model, loss, optimizer, x, y):\n    x = Variable(x, requires_grad=False)\n    y = Variable(y, requires_grad=False)\n\n    # Reset gradient\n    optimizer.zero_grad()\n\n    # Forward\n    fx = model.forward(x.view(len(x), 1)).squeeze()\n    output = loss.forward(fx, y)\n\n    # Backward\n    output.backward()\n\n    # Update parameters\n    optimizer.step()\n\n    return output.item()\n\n\ndef main():\n    torch.manual_seed(42)\n    X = torch.linspace(-1, 1, 101)\n    Y = 2 * X + torch.randn(X.size()) * 0.33\n\n    model = build_model()\n    loss = torch.nn.MSELoss(reduction=\'elementwise_mean\')\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    batch_size = 10\n\n    for i in range(100):\n        cost = 0.\n        num_batches = len(X) // batch_size\n        for k in range(num_batches):\n            start, end = k * batch_size, (k + 1) * batch_size\n            cost += train(model, loss, optimizer, X[start:end], Y[start:end])\n        print(""Epoch = %d, cost = %s"" % (i + 1, cost / num_batches))\n\n    w = next(model.parameters()).data  # model has only one parameter\n    print(""w = %.2f"" % w.numpy())  # will be approximately 2\n\nif __name__ == ""__main__"":\n    main()\n'"
2_logistic_regression.py,8,"b'import numpy as np\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch import optim\n\nfrom data_util import load_mnist\n\n\ndef build_model(input_dim, output_dim):\n    # We don\'t need the softmax layer here since CrossEntropyLoss already\n    # uses it internally.\n    model = torch.nn.Sequential()\n    model.add_module(""linear"",\n                     torch.nn.Linear(input_dim, output_dim, bias=False))\n    return model\n\n\ndef train(model, loss, optimizer, x_val, y_val):\n    x = Variable(x_val, requires_grad=False)\n    y = Variable(y_val, requires_grad=False)\n\n    # Reset gradient\n    optimizer.zero_grad()\n\n    # Forward\n    fx = model.forward(x)\n    output = loss.forward(fx, y)\n\n    # Backward\n    output.backward()\n\n    # Update parameters\n    optimizer.step()\n\n    return output.item()\n\n\ndef predict(model, x_val):\n    x = Variable(x_val, requires_grad=False)\n    output = model.forward(x)\n    return output.data.numpy().argmax(axis=1)\n\n\ndef main():\n    torch.manual_seed(42)\n    trX, teX, trY, teY = load_mnist(onehot=False)\n    trX = torch.from_numpy(trX).float()\n    teX = torch.from_numpy(teX).float()\n    trY = torch.from_numpy(trY).long()\n\n    n_examples, n_features = trX.size()\n    n_classes = 10\n    model = build_model(n_features, n_classes)\n    loss = torch.nn.CrossEntropyLoss(reduction=\'elementwise_mean\')\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    batch_size = 100\n\n    for i in range(100):\n        cost = 0.\n        num_batches = n_examples // batch_size\n        for k in range(num_batches):\n            start, end = k * batch_size, (k + 1) * batch_size\n            cost += train(model, loss, optimizer,\n                          trX[start:end], trY[start:end])\n        predY = predict(model, teX)\n        print(""Epoch %d, cost = %f, acc = %.2f%%""\n              % (i + 1, cost / num_batches, 100. * np.mean(predY == teY)))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
3_neural_net.py,10,"b'import numpy as np\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch import optim\n\nfrom data_util import load_mnist\n\n\ndef build_model(input_dim, output_dim):\n    model = torch.nn.Sequential()\n    model.add_module(""linear_1"", torch.nn.Linear(input_dim, 512, bias=False))\n    model.add_module(""sigmoid_1"", torch.nn.Sigmoid())\n    model.add_module(""linear_2"", torch.nn.Linear(512, output_dim, bias=False))\n    return model\n\n\ndef train(model, loss, optimizer, x_val, y_val):\n    x = Variable(x_val, requires_grad=False)\n    y = Variable(y_val, requires_grad=False)\n\n    # Reset gradient\n    optimizer.zero_grad()\n\n    # Forward\n    fx = model.forward(x)\n    output = loss.forward(fx, y)\n\n    # Backward\n    output.backward()\n\n    # Update parameters\n    optimizer.step()\n\n    return output.item()\n\n\ndef predict(model, x_val):\n    x = Variable(x_val, requires_grad=False)\n    output = model.forward(x)\n    return output.data.numpy().argmax(axis=1)\n\n\ndef main():\n    torch.manual_seed(42)\n    trX, teX, trY, teY = load_mnist(onehot=False)\n    trX = torch.from_numpy(trX).float()\n    teX = torch.from_numpy(teX).float()\n    trY = torch.from_numpy(trY).long()\n\n    n_examples, n_features = trX.size()\n    n_classes = 10\n    model = build_model(n_features, n_classes)\n    loss = torch.nn.CrossEntropyLoss(reduction=\'elementwise_mean\')\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    batch_size = 100\n\n    for i in range(100):\n        cost = 0.\n        num_batches = n_examples // batch_size\n        for k in range(num_batches):\n            start, end = k * batch_size, (k + 1) * batch_size\n            cost += train(model, loss, optimizer, trX[start:end], trY[start:end])\n        predY = predict(model, teX)\n        print(""Epoch %d, cost = %f, acc = %.2f%%""\n              % (i + 1, cost / num_batches, 100. * np.mean(predY == teY)))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
4_modern_neural_net.py,14,"b'import numpy as np\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch import optim\n\nfrom data_util import load_mnist\n\n\ndef build_model(input_dim, output_dim):\n    model = torch.nn.Sequential()\n    model.add_module(""linear_1"", torch.nn.Linear(input_dim, 512, bias=False))\n    model.add_module(""relu_1"", torch.nn.ReLU())\n    model.add_module(""dropout_1"", torch.nn.Dropout(0.2))\n    model.add_module(""linear_2"", torch.nn.Linear(512, 512, bias=False))\n    model.add_module(""relu_2"", torch.nn.ReLU())\n    model.add_module(""dropout_2"", torch.nn.Dropout(0.2))\n    model.add_module(""linear_3"", torch.nn.Linear(512, output_dim, bias=False))\n    return model\n\n\ndef train(model, loss, optimizer, x_val, y_val):\n    x = Variable(x_val, requires_grad=False)\n    y = Variable(y_val, requires_grad=False)\n\n    # Reset gradient\n    optimizer.zero_grad()\n\n    # Forward\n    fx = model.forward(x)\n    output = loss.forward(fx, y)\n\n    # Backward\n    output.backward()\n\n    # Update parameters\n    optimizer.step()\n\n    return output.item()\n\n\ndef predict(model, x_val):\n    x = Variable(x_val, requires_grad=False)\n    output = model.forward(x)\n    return output.data.numpy().argmax(axis=1)\n\n\ndef main():\n    torch.manual_seed(42)\n    trX, teX, trY, teY = load_mnist(onehot=False)\n    trX = torch.from_numpy(trX).float()\n    teX = torch.from_numpy(teX).float()\n    trY = torch.from_numpy(trY).long()\n\n    n_examples, n_features = trX.size()\n    n_classes = 10\n    model = build_model(n_features, n_classes)\n    loss = torch.nn.CrossEntropyLoss(reduction=\'elementwise_mean\')\n    optimizer = optim.Adam(model.parameters())\n    batch_size = 100\n\n    for i in range(100):\n        cost = 0.\n        num_batches = n_examples // batch_size\n        for k in range(num_batches):\n            start, end = k * batch_size, (k + 1) * batch_size\n            cost += train(model, loss, optimizer, trX[start:end], trY[start:end])\n        predY = predict(model, teX)\n        print(""Epoch %d, cost = %f, acc = %.2f%%""\n              % (i + 1, cost / num_batches, 100. * np.mean(predY == teY)))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
5_convolutional_net.py,20,"b'import numpy as np\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch import optim\n\nfrom data_util import load_mnist\n\n\n# We need to create two sequential models here since PyTorch doesn\'t have nn.View()\nclass ConvNet(torch.nn.Module):\n    def __init__(self, output_dim):\n        super(ConvNet, self).__init__()\n\n        self.conv = torch.nn.Sequential()\n        self.conv.add_module(""conv_1"", torch.nn.Conv2d(1, 10, kernel_size=5))\n        self.conv.add_module(""maxpool_1"", torch.nn.MaxPool2d(kernel_size=2))\n        self.conv.add_module(""relu_1"", torch.nn.ReLU())\n        self.conv.add_module(""conv_2"", torch.nn.Conv2d(10, 20, kernel_size=5))\n        self.conv.add_module(""dropout_2"", torch.nn.Dropout())\n        self.conv.add_module(""maxpool_2"", torch.nn.MaxPool2d(kernel_size=2))\n        self.conv.add_module(""relu_2"", torch.nn.ReLU())\n\n        self.fc = torch.nn.Sequential()\n        self.fc.add_module(""fc1"", torch.nn.Linear(320, 50))\n        self.fc.add_module(""relu_3"", torch.nn.ReLU())\n        self.fc.add_module(""dropout_3"", torch.nn.Dropout())\n        self.fc.add_module(""fc2"", torch.nn.Linear(50, output_dim))\n\n    def forward(self, x):\n        x = self.conv.forward(x)\n        x = x.view(-1, 320)\n        return self.fc.forward(x)\n\n\ndef train(model, loss, optimizer, x_val, y_val):\n    x = Variable(x_val, requires_grad=False)\n    y = Variable(y_val, requires_grad=False)\n\n    # Reset gradient\n    optimizer.zero_grad()\n\n    # Forward\n    fx = model.forward(x)\n    output = loss.forward(fx, y)\n\n    # Backward\n    output.backward()\n\n    # Update parameters\n    optimizer.step()\n\n    return output.item()\n\n\ndef predict(model, x_val):\n    x = Variable(x_val, requires_grad=False)\n    output = model.forward(x)\n    return output.data.numpy().argmax(axis=1)\n\n\ndef main():\n    torch.manual_seed(42)\n    trX, teX, trY, teY = load_mnist(onehot=False)\n    trX = trX.reshape(-1, 1, 28, 28)\n    teX = teX.reshape(-1, 1, 28, 28)\n\n    trX = torch.from_numpy(trX).float()\n    teX = torch.from_numpy(teX).float()\n    trY = torch.from_numpy(trY).long()\n\n    n_examples = len(trX)\n    n_classes = 10\n    model = ConvNet(output_dim=n_classes)\n    loss = torch.nn.CrossEntropyLoss(reduction=\'elementwise_mean\')\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    batch_size = 100\n\n    for i in range(20):\n        cost = 0.\n        num_batches = n_examples // batch_size\n        for k in range(num_batches):\n            start, end = k * batch_size, (k + 1) * batch_size\n            cost += train(model, loss, optimizer, trX[start:end], trY[start:end])\n        predY = predict(model, teX)\n        print(""Epoch %d, cost = %f, acc = %.2f%%""\n              % (i + 1, cost / num_batches, 100. * np.mean(predY == teY)))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
6_lstm.py,9,"b'from __future__ import division\nimport numpy as np\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch import optim, nn\n\nfrom data_util import load_mnist\n\n\nclass LSTMNet(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(LSTMNet, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_dim, hidden_dim)\n        self.linear = nn.Linear(hidden_dim, output_dim, bias=False)\n\n    def forward(self, x):\n        batch_size = x.size()[1]\n        h0 = Variable(torch.zeros([1, batch_size, self.hidden_dim]), requires_grad=False)\n        c0 = Variable(torch.zeros([1, batch_size, self.hidden_dim]), requires_grad=False)\n        fx, _ = self.lstm.forward(x, (h0, c0))\n        return self.linear.forward(fx[-1])\n\n\ndef train(model, loss, optimizer, x_val, y_val):\n    x = Variable(x_val, requires_grad=False)\n    y = Variable(y_val, requires_grad=False)\n\n    # Reset gradient\n    optimizer.zero_grad()\n\n    # Forward\n    fx = model.forward(x)\n    output = loss.forward(fx, y)\n\n    # Backward\n    output.backward()\n\n    # Update parameters\n    optimizer.step()\n\n    return output.item()\n\n\ndef predict(model, x_val):\n    x = Variable(x_val, requires_grad=False)\n    output = model.forward(x)\n    return output.data.numpy().argmax(axis=1)\n\n\ndef main():\n    torch.manual_seed(42)\n    trX, teX, trY, teY = load_mnist(onehot=False)\n\n    train_size = len(trY)\n    n_classes = 10\n    seq_length = 28\n    input_dim = 28\n    hidden_dim = 128\n    batch_size = 100\n    epochs = 20\n\n    trX = trX.reshape(-1, seq_length, input_dim)\n    teX = teX.reshape(-1, seq_length, input_dim)\n\n    # Convert to the shape (seq_length, num_samples, input_dim)\n    trX = np.swapaxes(trX, 0, 1)\n    teX = np.swapaxes(teX, 0, 1)\n\n    trX = torch.from_numpy(trX).float()\n    teX = torch.from_numpy(teX).float()\n    trY = torch.from_numpy(trY).long()\n\n    model = LSTMNet(input_dim, hidden_dim, n_classes)\n    loss = torch.nn.CrossEntropyLoss(reduction=\'elementwise_mean\')\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n    for i in range(epochs):\n        cost = 0.\n        num_batches = train_size // batch_size\n        for k in range(num_batches):\n            start, end = k * batch_size, (k + 1) * batch_size\n            cost += train(model, loss, optimizer, trX[:, start:end, :], trY[start:end])\n        predY = predict(model, teX)\n        print(""Epoch %d, cost = %f, acc = %.2f%%"" %\n              (i + 1, cost / num_batches, 100. * np.mean(predY == teY)))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
data_util.py,0,"b'import gzip\nimport os\nfrom os import path\nimport numpy as np\n\nimport sys\nif sys.version_info.major < 3:\n    import urllib\nelse:\n    import urllib.request as request\n\n\nDATASET_DIR = \'datasets/\'\n\nMNIST_FILES = [""train-images-idx3-ubyte.gz"", ""train-labels-idx1-ubyte.gz"",\n               ""t10k-images-idx3-ubyte.gz"", ""t10k-labels-idx1-ubyte.gz""]\n\n\ndef download_file(url, local_path):\n    dir_path = path.dirname(local_path)\n    if not path.exists(dir_path):\n        print(""Creating the directory \'%s\' ..."" % dir_path)\n        os.makedirs(dir_path)\n\n    print(""Downloading from \'%s\' ..."" % url)\n    if sys.version_info.major < 3:\n        urllib.URLopener().retrieve(url, local_path)\n    else:\n        request.urlretrieve(url, local_path)\n\n\ndef download_mnist(local_path):\n    url_root = ""http://yann.lecun.com/exdb/mnist/""\n    for f_name in MNIST_FILES:\n        f_path = os.path.join(local_path, f_name)\n        if not path.exists(f_path):\n            download_file(url_root + f_name, f_path)\n\n\ndef one_hot(x, n):\n    if type(x) == list:\n        x = np.array(x)\n    x = x.flatten()\n    o_h = np.zeros((len(x), n))\n    o_h[np.arange(len(x)), x] = 1\n    return o_h\n\n\ndef load_mnist(ntrain=60000, ntest=10000, onehot=True):\n    data_dir = os.path.join(DATASET_DIR, \'mnist/\')\n    if not path.exists(data_dir):\n        download_mnist(data_dir)\n    else:\n        # check all files\n        checks = [path.exists(os.path.join(data_dir, f)) for f in MNIST_FILES]\n        if not np.all(checks):\n            download_mnist(data_dir)\n\n    with gzip.open(os.path.join(data_dir, \'train-images-idx3-ubyte.gz\')) as fd:\n        buf = fd.read()\n        loaded = np.frombuffer(buf, dtype=np.uint8)\n        trX = loaded[16:].reshape((60000, 28 * 28)).astype(float)\n\n    with gzip.open(os.path.join(data_dir, \'train-labels-idx1-ubyte.gz\')) as fd:\n        buf = fd.read()\n        loaded = np.frombuffer(buf, dtype=np.uint8)\n        trY = loaded[8:].reshape((60000))\n\n    with gzip.open(os.path.join(data_dir, \'t10k-images-idx3-ubyte.gz\')) as fd:\n        buf = fd.read()\n        loaded = np.frombuffer(buf, dtype=np.uint8)\n        teX = loaded[16:].reshape((10000, 28 * 28)).astype(float)\n\n    with gzip.open(os.path.join(data_dir, \'t10k-labels-idx1-ubyte.gz\')) as fd:\n        buf = fd.read()\n        loaded = np.frombuffer(buf, dtype=np.uint8)\n        teY = loaded[8:].reshape((10000))\n\n    trX /= 255.\n    teX /= 255.\n\n    trX = trX[:ntrain]\n    trY = trY[:ntrain]\n\n    teX = teX[:ntest]\n    teY = teY[:ntest]\n\n    if onehot:\n        trY = one_hot(trY, 10)\n        teY = one_hot(teY, 10)\n    else:\n        trY = np.asarray(trY)\n        teY = np.asarray(teY)\n\n    return trX, teX, trY, teY\n'"
