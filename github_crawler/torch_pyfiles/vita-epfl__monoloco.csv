file_path,api_count,code
setup.py,0,"b""from setuptools import setup\n\n# extract version from __init__.py\nwith open('monoloco/__init__.py', 'r') as f:\n    VERSION_LINE = [l for l in f if l.startswith('__version__')][0]\n    VERSION = VERSION_LINE.split('=')[1].strip()[1:-1]\n\nsetup(\n    name='monoloco',\n    version=VERSION,\n    packages=[\n        'monoloco',\n        'monoloco.network',\n        'monoloco.eval',\n        'monoloco.train',\n        'monoloco.prep',\n        'monoloco.visuals',\n        'monoloco.utils'\n    ],\n    license='GNU AGPLv3',\n    description='MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation',\n    long_description=open('README.md').read(),\n    long_description_content_type='text/markdown',\n    author='Lorenzo Bertoni',\n    author_email='lorenzo.bertoni@epfl.ch',\n    url='https://github.com/vita-epfl/monoloco',\n    zip_safe=False,\n\n    install_requires=[\n        'torch<=1.1.0',\n        'Pillow<=6.3',\n        'torchvision<=0.3.0',\n        'openpifpaf<=0.9.0',\n        'tabulate<=0.8.3',   # For evaluation\n    ],\n    extras_require={\n        'test': [\n            'pylint<=2.4.2',\n            'pytest<=4.6.3',\n        ],\n        'prep': [\n            'nuscenes-devkit<=1.0.2',\n        ],\n    },\n)\n"""
monoloco/__init__.py,0,"b'\n""""""Open implementation of MonoLoco.""""""\n\n__version__ = \'0.4.8\'\n'"
monoloco/predict.py,1,"b'\nimport os\nimport json\n\nimport torch\nfrom PIL import Image\nfrom openpifpaf import show\n\nfrom .visuals.printer import Printer\nfrom .network import PifPaf, ImageList, MonoLoco\nfrom .network.process import factory_for_gt, preprocess_pifpaf\n\n\ndef predict(args):\n\n    cnt = 0\n\n    # load pifpaf and monoloco models\n    pifpaf = PifPaf(args)\n    monoloco = MonoLoco(model=args.model, device=args.device, n_dropout=args.n_dropout, p_dropout=args.dropout)\n\n    # data\n    data = ImageList(args.images, scale=args.scale)\n    data_loader = torch.utils.data.DataLoader(\n        data, batch_size=1, shuffle=False,\n        pin_memory=args.pin_memory, num_workers=args.loader_workers)\n\n    for idx, (image_paths, image_tensors, processed_images_cpu) in enumerate(data_loader):\n        images = image_tensors.permute(0, 2, 3, 1)\n\n        processed_images = processed_images_cpu.to(args.device, non_blocking=True)\n        fields_batch = pifpaf.fields(processed_images)\n\n        # unbatch\n        for image_path, image, processed_image_cpu, fields in zip(\n                image_paths, images, processed_images_cpu, fields_batch):\n\n            if args.output_directory is None:\n                output_path = image_path\n            else:\n                file_name = os.path.basename(image_path)\n                output_path = os.path.join(args.output_directory, file_name)\n            print(\'image\', idx, image_path, output_path)\n\n            keypoint_sets, scores, pifpaf_out = pifpaf.forward(image, processed_image_cpu, fields)\n            pifpaf_outputs = [keypoint_sets, scores, pifpaf_out]  # keypoints_sets and scores for pifpaf printing\n            images_outputs = [image]  # List of 1 or 2 elements with pifpaf tensor (resized) and monoloco original image\n\n            if \'monoloco\' in args.networks:\n                im_size = (float(image.size()[1] / args.scale),\n                           float(image.size()[0] / args.scale))  # Width, Height (original)\n\n                # Extract calibration matrix and ground truth file if present\n                with open(image_path, \'rb\') as f:\n                    pil_image = Image.open(f).convert(\'RGB\')\n                    images_outputs.append(pil_image)\n\n                im_name = os.path.basename(image_path)\n\n                kk, dic_gt = factory_for_gt(im_size, name=im_name, path_gt=args.path_gt)\n\n                # Preprocess pifpaf outputs and run monoloco\n                boxes, keypoints = preprocess_pifpaf(pifpaf_out, im_size)\n                outputs, varss = monoloco.forward(keypoints, kk)\n                dic_out = monoloco.post_process(outputs, varss, boxes, keypoints, kk, dic_gt)\n\n            else:\n                dic_out = None\n                kk = None\n\n            factory_outputs(args, images_outputs, output_path, pifpaf_outputs, dic_out=dic_out, kk=kk)\n            print(\'Image {}\\n\'.format(cnt) + \'-\' * 120)\n            cnt += 1\n\n\ndef factory_outputs(args, images_outputs, output_path, pifpaf_outputs, dic_out=None, kk=None):\n    """"""Output json files or images according to the choice""""""\n\n    # Save json file\n    if \'pifpaf\' in args.networks:\n        keypoint_sets, scores, pifpaf_out = pifpaf_outputs[:]\n\n        # Visualizer\n        keypoint_painter = show.KeypointPainter(show_box=False)\n        skeleton_painter = show.KeypointPainter(show_box=False, color_connections=True,\n                                                markersize=1, linewidth=4)\n\n        if \'json\' in args.output_types and keypoint_sets.size > 0:\n            with open(output_path + \'.pifpaf.json\', \'w\') as f:\n                json.dump(pifpaf_out, f)\n\n        if \'keypoints\' in args.output_types:\n            with show.image_canvas(images_outputs[0],\n                                   output_path + \'.keypoints.png\',\n                                   show=args.show,\n                                   fig_width=args.figure_width,\n                                   dpi_factor=args.dpi_factor) as ax:\n                keypoint_painter.keypoints(ax, keypoint_sets)\n\n        if \'skeleton\' in args.output_types:\n            with show.image_canvas(images_outputs[0],\n                                   output_path + \'.skeleton.png\',\n                                   show=args.show,\n                                   fig_width=args.figure_width,\n                                   dpi_factor=args.dpi_factor) as ax:\n                skeleton_painter.keypoints(ax, keypoint_sets, scores=scores)\n\n    if \'monoloco\' in args.networks:\n        if any((xx in args.output_types for xx in [\'front\', \'bird\', \'combined\'])):\n            epistemic = False\n            if args.n_dropout > 0:\n                epistemic = True\n\n            if dic_out[\'boxes\']:  # Only print in case of detections\n                printer = Printer(images_outputs[1], output_path, kk, output_types=args.output_types\n                                  , z_max=args.z_max, epistemic=epistemic)\n                figures, axes = printer.factory_axes()\n                printer.draw(figures, axes, dic_out, images_outputs[1], draw_box=args.draw_box,\n                             save=True, show=args.show)\n\n        if \'json\' in args.output_types:\n            with open(os.path.join(output_path + \'.monoloco.json\'), \'w\') as ff:\n                json.dump(dic_out, ff)\n'"
monoloco/run.py,0,"b'# pylint: disable=too-many-branches, too-many-statements\nimport argparse\n\nfrom openpifpaf.network import nets\nfrom openpifpaf import decoder\n\n\ndef cli():\n    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # Subparser definition\n    subparsers = parser.add_subparsers(help=\'Different parsers for main actions\', dest=\'command\')\n    predict_parser = subparsers.add_parser(""predict"")\n    prep_parser = subparsers.add_parser(""prep"")\n    training_parser = subparsers.add_parser(""train"")\n    eval_parser = subparsers.add_parser(""eval"")\n\n    # Preprocess input data\n    prep_parser.add_argument(\'--dir_ann\', help=\'directory of annotations of 2d joints\', required=True)\n    prep_parser.add_argument(\'--dataset\',\n                             help=\'datasets to preprocess: nuscenes, nuscenes_teaser, nuscenes_mini, kitti\',\n                             default=\'nuscenes\')\n    prep_parser.add_argument(\'--dir_nuscenes\', help=\'directory of nuscenes devkit\', default=\'data/nuscenes/\')\n    prep_parser.add_argument(\'--iou_min\', help=\'minimum iou to match ground truth\', type=float, default=0.3)\n\n    # Predict (2D pose and/or 3D location from images)\n    # General\n    predict_parser.add_argument(\'--networks\', nargs=\'+\', help=\'Run pifpaf and/or monoloco\', default=[\'monoloco\'])\n    predict_parser.add_argument(\'images\', nargs=\'*\', help=\'input images\')\n    predict_parser.add_argument(\'--glob\', help=\'glob expression for input images (for many images)\')\n    predict_parser.add_argument(\'-o\', \'--output-directory\', help=\'Output directory\')\n    predict_parser.add_argument(\'--output_types\', nargs=\'+\', default=[\'json\'],\n                                help=\'what to output: json keypoints skeleton for Pifpaf\'\n                                     \'json bird front combined for Monoloco\')\n    predict_parser.add_argument(\'--show\', help=\'to show images\', action=\'store_true\')\n\n    # Pifpaf\n    nets.cli(predict_parser)\n    decoder.cli(predict_parser, force_complete_pose=True, instance_threshold=0.15)\n    predict_parser.add_argument(\'--scale\', default=1.0, type=float, help=\'change the scale of the image to preprocess\')\n\n    # Monoloco\n    predict_parser.add_argument(\'--model\', help=\'path of MonoLoco model to load\', required=True)\n    predict_parser.add_argument(\'--hidden_size\', type=int, help=\'Number of hidden units in the model\', default=512)\n    predict_parser.add_argument(\'--path_gt\', help=\'path of json file with gt 3d localization\',\n                                default=\'data/arrays/names-kitti-190513-1754.json\')\n    predict_parser.add_argument(\'--transform\', help=\'transformation for the pose\', default=\'None\')\n    predict_parser.add_argument(\'--draw_box\', help=\'to draw box in the images\', action=\'store_true\')\n    predict_parser.add_argument(\'--predict\', help=\'whether to make prediction\', action=\'store_true\')\n    predict_parser.add_argument(\'--z_max\', type=int, help=\'maximum meters distance for predictions\', default=22)\n    predict_parser.add_argument(\'--n_dropout\', type=int, help=\'Epistemic uncertainty evaluation\', default=0)\n    predict_parser.add_argument(\'--dropout\', type=float, help=\'dropout parameter\', default=0.2)\n    predict_parser.add_argument(\'--webcam\', help=\'monoloco streaming\', action=\'store_true\')\n\n    # Training\n    training_parser.add_argument(\'--joints\', help=\'Json file with input joints\',\n                                 default=\'data/arrays/joints-nuscenes_teaser-190513-1846.json\')\n    training_parser.add_argument(\'--save\', help=\'whether to not save model and log file\', action=\'store_false\')\n    training_parser.add_argument(\'-e\', \'--epochs\', type=int, help=\'number of epochs to train for\', default=150)\n    training_parser.add_argument(\'--bs\', type=int, default=256, help=\'input batch size\')\n    training_parser.add_argument(\'--baseline\', help=\'whether to train using the baseline\', action=\'store_true\')\n    training_parser.add_argument(\'--dropout\', type=float, help=\'dropout. Default no dropout\', default=0.2)\n    training_parser.add_argument(\'--lr\', type=float, help=\'learning rate\', default=0.002)\n    training_parser.add_argument(\'--sched_step\', type=float, help=\'scheduler step time (epochs)\', default=20)\n    training_parser.add_argument(\'--sched_gamma\', type=float, help=\'Scheduler multiplication every step\', default=0.9)\n    training_parser.add_argument(\'--hidden_size\', type=int, help=\'Number of hidden units in the model\', default=256)\n    training_parser.add_argument(\'--n_stage\', type=int, help=\'Number of stages in the model\', default=3)\n    training_parser.add_argument(\'--hyp\', help=\'run hyperparameters tuning\', action=\'store_true\')\n    training_parser.add_argument(\'--multiplier\', type=int, help=\'Size of the grid of hyp search\', default=1)\n    training_parser.add_argument(\'--r_seed\', type=int, help=\'specify the seed for training and hyp tuning\', default=1)\n\n    # Evaluation\n    eval_parser.add_argument(\'--dataset\', help=\'datasets to evaluate, kitti or nuscenes\', default=\'kitti\')\n    eval_parser.add_argument(\'--geometric\', help=\'to evaluate geometric distance\', action=\'store_true\')\n    eval_parser.add_argument(\'--generate\', help=\'create txt files for KITTI evaluation\', action=\'store_true\')\n    eval_parser.add_argument(\'--dir_ann\', help=\'directory of annotations of 2d joints (for KITTI evaluation\')\n    eval_parser.add_argument(\'--model\', help=\'path of MonoLoco model to load\', required=True)\n    eval_parser.add_argument(\'--joints\', help=\'Json file with input joints to evaluate (for nuScenes evaluation)\')\n    eval_parser.add_argument(\'--n_dropout\', type=int, help=\'Epistemic uncertainty evaluation\', default=0)\n    eval_parser.add_argument(\'--dropout\', type=float, help=\'dropout. Default no dropout\', default=0.2)\n    eval_parser.add_argument(\'--hidden_size\', type=int, help=\'Number of hidden units in the model\', default=256)\n    eval_parser.add_argument(\'--n_stage\', type=int, help=\'Number of stages in the model\', default=3)\n    eval_parser.add_argument(\'--show\', help=\'whether to show statistic graphs\', action=\'store_true\')\n    eval_parser.add_argument(\'--save\', help=\'whether to save statistic graphs\', action=\'store_true\')\n    eval_parser.add_argument(\'--verbose\', help=\'verbosity of statistics\', action=\'store_true\')\n    eval_parser.add_argument(\'--stereo\', help=\'include stereo baseline results\', action=\'store_true\')\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = cli()\n    if args.command == \'predict\':\n        if args.webcam:\n            from .visuals.webcam import webcam\n            webcam(args)\n        else:\n            from . import predict\n            predict.predict(args)\n\n    elif args.command == \'prep\':\n        if \'nuscenes\' in args.dataset:\n            from .prep.preprocess_nu import PreprocessNuscenes\n            prep = PreprocessNuscenes(args.dir_ann, args.dir_nuscenes, args.dataset, args.iou_min)\n            prep.run()\n        if \'kitti\' in args.dataset:\n            from .prep.preprocess_ki import PreprocessKitti\n            prep = PreprocessKitti(args.dir_ann, args.iou_min)\n            prep.run()\n\n    elif args.command == \'train\':\n        from .train import HypTuning\n        if args.hyp:\n            hyp_tuning = HypTuning(joints=args.joints, epochs=args.epochs,\n                                   baseline=args.baseline, dropout=args.dropout,\n                                   multiplier=args.multiplier, r_seed=args.r_seed)\n            hyp_tuning.train()\n        else:\n            from .train import Trainer\n            training = Trainer(joints=args.joints, epochs=args.epochs, bs=args.bs,\n                               baseline=args.baseline, dropout=args.dropout, lr=args.lr, sched_step=args.sched_step,\n                               n_stage=args.n_stage, sched_gamma=args.sched_gamma, hidden_size=args.hidden_size,\n                               r_seed=args.r_seed, save=args.save)\n\n            _ = training.train()\n            _ = training.evaluate()\n\n    elif args.command == \'eval\':\n        if args.geometric:\n            assert args.joints, ""joints argument not provided""\n            from .eval import geometric_baseline\n            geometric_baseline(args.joints)\n\n        if args.generate:\n            from .eval import GenerateKitti\n            kitti_txt = GenerateKitti(args.model, args.dir_ann, p_dropout=args.dropout, n_dropout=args.n_dropout,\n                                      stereo=args.stereo)\n            kitti_txt.run()\n\n        if args.dataset == \'kitti\':\n            from .eval import EvalKitti\n            kitti_eval = EvalKitti(verbose=args.verbose, stereo=args.stereo)\n            kitti_eval.run()\n            kitti_eval.printer(show=args.show, save=args.save)\n\n        if \'nuscenes\' in args.dataset:\n            from .train import Trainer\n            training = Trainer(joints=args.joints)\n            _ = training.evaluate(load=True, model=args.model, debug=False)\n\n    else:\n        raise ValueError(""Main subparser not recognized or not provided"")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tests/test_package.py,0,"b'""""""Test if the main modules of the package run correctly""""""\n\nimport os\nimport sys\nimport json\n\n# Python does not consider the current directory to be a package\nsys.path.insert(0, os.path.join(\'..\', \'monoloco\'))\n\nfrom PIL import Image\n\nfrom monoloco.train import Trainer\nfrom monoloco.network import MonoLoco\nfrom monoloco.network.process import preprocess_pifpaf, factory_for_gt\nfrom monoloco.visuals.printer import Printer\n\nJOINTS = \'tests/joints_sample.json\'\nPIFPAF_KEYPOINTS = \'tests/002282.png.pifpaf.json\'\nIMAGE = \'docs/002282.png\'\n\n\ndef tst_trainer(joints):\n    trainer = Trainer(joints=joints, epochs=150, lr=0.01)\n    _ = trainer.train()\n    dic_err, model = trainer.evaluate()\n    return dic_err[\'val\'][\'all\'][\'mean\'], model\n\n\ndef tst_prediction(model, path_keypoints):\n    with open(path_keypoints, \'r\') as f:\n        pifpaf_out = json.load(f)\n\n    kk, _ = factory_for_gt(im_size=[1240, 340])\n\n    # Preprocess pifpaf outputs and run monoloco\n    boxes, keypoints = preprocess_pifpaf(pifpaf_out)\n    monoloco = MonoLoco(model)\n    outputs, varss = monoloco.forward(keypoints, kk)\n    dic_out = monoloco.post_process(outputs, varss, boxes, keypoints, kk)\n    return dic_out, kk\n\n\ndef tst_printer(dic_out, kk, image_path):\n    """"""Draw a fake figure""""""\n    with open(image_path, \'rb\') as f:\n        pil_image = Image.open(f).convert(\'RGB\')\n    printer = Printer(image=pil_image, output_path=\'tests/test_image\', kk=kk, output_types=[\'combined\'], z_max=15)\n    figures, axes = printer.factory_axes()\n    printer.draw(figures, axes, dic_out, pil_image, save=True)\n\n\ndef test_package():\n\n    # Training test\n    val_acc, model = tst_trainer(JOINTS)\n    assert val_acc < 2.5\n\n    # Prediction test\n    dic_out, kk = tst_prediction(model, PIFPAF_KEYPOINTS)\n    assert dic_out[\'boxes\'] and kk\n\n    # Visualization test\n    tst_printer(dic_out, kk, IMAGE)\n\n\n\n\n\n\n'"
tests/test_utils.py,0,"b""import os\nimport sys\n\n# Python does not consider the current directory to be a package\nsys.path.insert(0, os.path.join('..', 'monoloco'))\n\n\ndef test_iou():\n    from monoloco.utils import get_iou_matrix\n    boxes_pred = [[1, 100, 1, 200]]\n    boxes_gt = [[100., 120., 150., 160.],[12, 110, 130., 160.]]\n    iou_matrix = get_iou_matrix(boxes_pred, boxes_gt)\n    assert iou_matrix.shape == (len(boxes_pred), len(boxes_gt))\n\n\ndef test_pixel_to_camera():\n    from monoloco.utils import pixel_to_camera\n    kk = [[718.3351, 0., 600.3891], [0., 718.3351, 181.5122], [0., 0., 1.]]\n    zz = 10\n    uv_vector = [1000., 400.]\n    xx_norm = pixel_to_camera(uv_vector, kk, 1)[0]\n    xx_1 = xx_norm * zz\n    xx_2 = pixel_to_camera(uv_vector, kk, zz)[0]\n    assert xx_1 == xx_2\n"""
tests/test_visuals.py,0,"b'import os\nimport sys\nfrom collections import defaultdict\n\nfrom PIL import Image\n\n# Python does not consider the current directory to be a package\nsys.path.insert(0, os.path.join(\'..\', \'monoloco\'))\n\n\ndef test_printer():\n    """"""Draw a fake figure""""""\n    from monoloco.visuals.printer import Printer\n    test_list = [[718.3351, 0., 600.3891], [0., 718.3351, 181.5122], [0., 0., 1.]]\n    boxes = [xx + [0] for xx in test_list]\n    kk = test_list\n    dict_ann = defaultdict(lambda: [1., 2., 3.], xyz_real=test_list, xyz_pred=test_list, uv_shoulders=test_list,\n                           boxes=boxes, boxes_gt=boxes)\n    with open(\'docs/002282.png\', \'rb\') as f:\n        pil_image = Image.open(f).convert(\'RGB\')\n    printer = Printer(image=pil_image, output_path=None, kk=kk, output_types=[\'combined\'])\n    figures, axes = printer.factory_axes()\n    printer.draw(figures, axes, dict_ann, pil_image)\n'"
monoloco/eval/__init__.py,0,b'\nfrom .eval_kitti import EvalKitti\nfrom .generate_kitti import GenerateKitti\nfrom .geom_baseline import geometric_baseline\n'
monoloco/eval/eval_kitti.py,0,"b'""""""Evaluate Monoloco code on KITTI dataset using ALE and ALP metrics with the following baselines:\n    - Mono3D\n    - 3DOP\n    - MonoDepth\n    """"""\n\nimport os\nimport math\nimport logging\nimport datetime\nfrom collections import defaultdict\nfrom itertools import chain\n\nfrom tabulate import tabulate\n\nfrom ..utils import get_iou_matches, get_task_error, get_pixel_error, check_conditions, get_category, split_training, \\\n    parse_ground_truth\nfrom ..visuals import show_results, show_spread, show_task_error\n\n\nclass EvalKitti:\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    CLUSTERS = (\'easy\', \'moderate\', \'hard\', \'all\', \'6\', \'10\', \'15\', \'20\', \'25\', \'30\', \'40\', \'50\', \'>50\')\n    ALP_THRESHOLDS = (\'<0.5m\', \'<1m\', \'<2m\')\n    METHODS_MONO = [\'m3d\', \'monodepth\', \'3dop\', \'monoloco\']\n    METHODS_STEREO = [\'ml_stereo\', \'pose\', \'reid\']\n    BASELINES = [\'geometric\', \'task_error\', \'pixel_error\']\n    HEADERS = (\'method\', \'<0.5\', \'<1m\', \'<2m\', \'easy\', \'moderate\', \'hard\', \'all\')\n    CATEGORIES = (\'pedestrian\',)\n\n    def __init__(self, thresh_iou_monoloco=0.3, thresh_iou_base=0.3, thresh_conf_monoloco=0.3, thresh_conf_base=0.3,\n                 verbose=False, stereo=False):\n\n        self.main_dir = os.path.join(\'data\', \'kitti\')\n        self.dir_gt = os.path.join(self.main_dir, \'gt\')\n        self.methods = self.METHODS_MONO\n        self.stereo = stereo\n        if self.stereo:\n            self.methods.extend(self.METHODS_STEREO)\n        path_train = os.path.join(\'splits\', \'kitti_train.txt\')\n        path_val = os.path.join(\'splits\', \'kitti_val.txt\')\n        dir_logs = os.path.join(\'data\', \'logs\')\n        assert dir_logs, ""No directory to save final statistics""\n\n        now = datetime.datetime.now()\n        now_time = now.strftime(""%Y%m%d-%H%M"")[2:]\n        self.path_results = os.path.join(dir_logs, \'eval-\' + now_time + \'.json\')\n        self.verbose = verbose\n\n        self.dic_thresh_iou = {method: (thresh_iou_monoloco if method[:8] == \'monoloco\' else thresh_iou_base)\n                               for method in self.methods}\n        self.dic_thresh_conf = {method: (thresh_conf_monoloco if method[:8] == \'monoloco\' else thresh_conf_base)\n                                for method in self.methods}\n\n        # Extract validation images for evaluation\n        names_gt = tuple(os.listdir(self.dir_gt))\n        _, self.set_val = split_training(names_gt, path_train, path_val)\n\n        # Define variables to save statistics\n        self.dic_methods = None\n        self.errors = None\n        self.dic_stds = None\n        self.dic_stats = None\n        self.dic_cnt = None\n        self.cnt_gt = 0\n\n    def run(self):\n        """"""Evaluate Monoloco performances on ALP and ALE metrics""""""\n\n        for category in self.CATEGORIES:\n\n            # Initialize variables\n            self.errors = defaultdict(lambda: defaultdict(list))\n            self.dic_stds = defaultdict(lambda: defaultdict(list))\n            self.dic_stats = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n            self.dic_cnt = defaultdict(int)\n            self.cnt_gt = 0\n\n            # Iterate over each ground truth file in the training set\n            for name in self.set_val:\n                path_gt = os.path.join(self.dir_gt, name)\n\n                # Iterate over each line of the gt file and save box location and distances\n                out_gt = parse_ground_truth(path_gt, category)\n                methods_out = defaultdict(tuple)  # Save all methods for comparison\n                self.cnt_gt += len(out_gt[0])\n\n                if out_gt[0]:\n                    for method in self.methods:\n                        # Extract annotations\n                        dir_method = os.path.join(self.main_dir, method)\n                        assert os.path.exists(dir_method), ""directory of the method %s does not exists"" % method\n                        path_method = os.path.join(dir_method, name)\n                        methods_out[method] = self._parse_txts(path_method, category, method=method)\n\n                        # Compute the error with ground truth\n                        self._estimate_error(out_gt, methods_out[method], method=method)\n\n                    # Iterate over all the files together to find a pool of common annotations\n                    self._compare_error(out_gt, methods_out)\n\n            # Update statistics of errors and uncertainty\n            for key in self.errors:\n                add_true_negatives(self.errors[key], self.cnt_gt)\n                for clst in self.CLUSTERS[:-2]:  # M3d and pifpaf does not have annotations above 40 meters\n                    get_statistics(self.dic_stats[\'test\'][key][clst], self.errors[key][clst], self.dic_stds[clst], key)\n\n            # Show statistics\n            print(\'\\n\' + category.upper() + \':\')\n            self.show_statistics()\n\n    def printer(self, show, save):\n        if save or show:\n            show_results(self.dic_stats, show, save, stereo=self.stereo)\n            show_spread(self.dic_stats, show, save)\n            show_task_error(show, save)\n\n    def _parse_txts(self, path, category, method):\n\n        boxes = []\n        dds = []\n        stds_ale = []\n        stds_epi = []\n        dds_geometric = []\n        output = (boxes, dds) if method != \'monoloco\' else (boxes, dds, stds_ale, stds_epi, dds_geometric)\n\n        try:\n            with open(path, ""r"") as ff:\n                for line_str in ff:\n                    line = line_str.split()\n                    if check_conditions(line, category, method=method, thresh=self.dic_thresh_conf[method]):\n                        if method == \'monodepth\':\n                            box = [float(x[:-1]) for x in line[0:4]]\n                            delta_h = (box[3] - box[1]) / 7\n                            delta_w = (box[2] - box[0]) / 3.5\n                            assert delta_h > 0 and delta_w > 0, ""Bounding box <=0""\n                            box[0] -= delta_w\n                            box[1] -= delta_h\n                            box[2] += delta_w\n                            box[3] += delta_h\n                            dd = float(line[5][:-1])\n                        else:\n                            box = [float(x) for x in line[4:8]]\n                            loc = ([float(x) for x in line[11:14]])\n                            dd = math.sqrt(loc[0] ** 2 + loc[1] ** 2 + loc[2] ** 2)\n                        boxes.append(box)\n                        dds.append(dd)\n                        self.dic_cnt[method] += 1\n                        if method == \'monoloco\':\n                            stds_ale.append(float(line[16]))\n                            stds_epi.append(float(line[17]))\n                            dds_geometric.append(float(line[18]))\n                            self.dic_cnt[\'geometric\'] += 1\n            return output\n        except FileNotFoundError:\n            return output\n\n    def _estimate_error(self, out_gt, out, method):\n        """"""Estimate localization error""""""\n\n        boxes_gt, _, dds_gt, zzs_gt, truncs_gt, occs_gt = out_gt\n        if method == \'monoloco\':\n            boxes, dds, stds_ale, stds_epi, dds_geometric = out\n        else:\n            boxes, dds = out\n\n        matches = get_iou_matches(boxes, boxes_gt, self.dic_thresh_iou[method])\n\n        for (idx, idx_gt) in matches:\n            # Update error if match is found\n            cat = get_category(boxes_gt[idx_gt], truncs_gt[idx_gt], occs_gt[idx_gt])\n            self.update_errors(dds[idx], dds_gt[idx_gt], cat, self.errors[method])\n\n            if method == \'monoloco\':\n                self.update_errors(dds_geometric[idx], dds_gt[idx_gt], cat, self.errors[\'geometric\'])\n                self.update_uncertainty(stds_ale[idx], stds_epi[idx], dds[idx], dds_gt[idx_gt], cat)\n                dd_task_error = dds_gt[idx_gt] + (get_task_error(dds_gt[idx_gt]))**2\n                self.update_errors(dd_task_error, dds_gt[idx_gt], cat, self.errors[\'task_error\'])\n                dd_pixel_error = dds_gt[idx_gt] + get_pixel_error(zzs_gt[idx_gt])\n                self.update_errors(dd_pixel_error, dds_gt[idx_gt], cat, self.errors[\'pixel_error\'])\n\n    def _compare_error(self, out_gt, methods_out):\n        """"""Compare the error for a pool of instances commonly matched by all methods""""""\n        boxes_gt, _, dds_gt, zzs_gt, truncs_gt, occs_gt = out_gt\n\n        # Find IoU matches\n        matches = []\n        boxes_monoloco = methods_out[\'monoloco\'][0]\n        matches_monoloco = get_iou_matches(boxes_monoloco, boxes_gt, self.dic_thresh_iou[\'monoloco\'])\n\n        base_methods = [method for method in self.methods if method != \'monoloco\']\n        for method in base_methods:\n            boxes = methods_out[method][0]\n            matches.append(get_iou_matches(boxes, boxes_gt, self.dic_thresh_iou[method]))\n\n        # Update error of commonly matched instances\n        for (idx, idx_gt) in matches_monoloco:\n            check, indices = extract_indices(idx_gt, *matches)\n            if check:\n                cat = get_category(boxes_gt[idx_gt], truncs_gt[idx_gt], occs_gt[idx_gt])\n                dd_gt = dds_gt[idx_gt]\n\n                for idx_indices, method in enumerate(base_methods):\n                    dd = methods_out[method][1][indices[idx_indices]]\n                    self.update_errors(dd, dd_gt, cat, self.errors[method + \'_merged\'])\n\n                dd_monoloco = methods_out[\'monoloco\'][1][idx]\n                dd_geometric = methods_out[\'monoloco\'][4][idx]\n                self.update_errors(dd_monoloco, dd_gt, cat, self.errors[\'monoloco_merged\'])\n                self.update_errors(dd_geometric, dd_gt, cat, self.errors[\'geometric_merged\'])\n                self.update_errors(dd_gt + get_task_error(dd_gt), dd_gt, cat, self.errors[\'task_error_merged\'])\n                dd_pixel = dd_gt + get_pixel_error(zzs_gt[idx_gt])\n                self.update_errors(dd_pixel, dd_gt, cat, self.errors[\'pixel_error_merged\'])\n\n                for key in self.methods:\n                    self.dic_cnt[key + \'_merged\'] += 1\n\n    def update_errors(self, dd, dd_gt, cat, errors):\n        """"""Compute and save errors between a single box and the gt box which match""""""\n        diff = abs(dd - dd_gt)\n        clst = find_cluster(dd_gt, self.CLUSTERS)\n        errors[\'all\'].append(diff)\n        errors[cat].append(diff)\n        errors[clst].append(diff)\n\n        # Check if the distance is less than one or 2 meters\n        if diff <= 0.5:\n            errors[\'<0.5m\'].append(1)\n        else:\n            errors[\'<0.5m\'].append(0)\n\n        if diff <= 1:\n            errors[\'<1m\'].append(1)\n        else:\n            errors[\'<1m\'].append(0)\n\n        if diff <= 2:\n            errors[\'<2m\'].append(1)\n        else:\n            errors[\'<2m\'].append(0)\n\n    def update_uncertainty(self, std_ale, std_epi, dd, dd_gt, cat):\n\n        clst = find_cluster(dd_gt, self.CLUSTERS)\n        self.dic_stds[\'all\'][\'ale\'].append(std_ale)\n        self.dic_stds[clst][\'ale\'].append(std_ale)\n        self.dic_stds[cat][\'ale\'].append(std_ale)\n        self.dic_stds[\'all\'][\'epi\'].append(std_epi)\n        self.dic_stds[clst][\'epi\'].append(std_epi)\n        self.dic_stds[cat][\'epi\'].append(std_epi)\n\n        # Number of annotations inside the confidence interval\n        std = std_epi if std_epi > 0 else std_ale  # consider aleatoric uncertainty if epistemic is not calculated\n        if abs(dd - dd_gt) <= std:\n            self.dic_stds[\'all\'][\'interval\'].append(1)\n            self.dic_stds[clst][\'interval\'].append(1)\n            self.dic_stds[cat][\'interval\'].append(1)\n        else:\n            self.dic_stds[\'all\'][\'interval\'].append(0)\n            self.dic_stds[clst][\'interval\'].append(0)\n            self.dic_stds[cat][\'interval\'].append(0)\n\n        # Annotations at risk inside the confidence interval\n        if dd_gt <= dd:\n            self.dic_stds[\'all\'][\'at_risk\'].append(1)\n            self.dic_stds[clst][\'at_risk\'].append(1)\n            self.dic_stds[cat][\'at_risk\'].append(1)\n\n            if abs(dd - dd_gt) <= std_epi:\n                self.dic_stds[\'all\'][\'at_risk-interval\'].append(1)\n                self.dic_stds[clst][\'at_risk-interval\'].append(1)\n                self.dic_stds[cat][\'at_risk-interval\'].append(1)\n            else:\n                self.dic_stds[\'all\'][\'at_risk-interval\'].append(0)\n                self.dic_stds[clst][\'at_risk-interval\'].append(0)\n                self.dic_stds[cat][\'at_risk-interval\'].append(0)\n\n        else:\n            self.dic_stds[\'all\'][\'at_risk\'].append(0)\n            self.dic_stds[clst][\'at_risk\'].append(0)\n            self.dic_stds[cat][\'at_risk\'].append(0)\n\n        # Precision of uncertainty\n        eps = 1e-4\n        task_error = get_task_error(dd)\n        prec_1 = abs(dd - dd_gt) / (std_epi + eps)\n\n        prec_2 = abs(std_epi - task_error)\n        self.dic_stds[\'all\'][\'prec_1\'].append(prec_1)\n        self.dic_stds[clst][\'prec_1\'].append(prec_1)\n        self.dic_stds[cat][\'prec_1\'].append(prec_1)\n        self.dic_stds[\'all\'][\'prec_2\'].append(prec_2)\n        self.dic_stds[clst][\'prec_2\'].append(prec_2)\n        self.dic_stds[cat][\'prec_2\'].append(prec_2)\n\n    def show_statistics(self):\n\n        all_methods = self.methods + self.BASELINES\n        print(\'-\'*90)\n        self.summary_table(all_methods)\n\n        if self.verbose:\n            all_methods_merged = list(chain.from_iterable((method, method + \'_merged\') for method in all_methods))\n            for key in all_methods_merged:\n                for clst in self.CLUSTERS[:4]:\n                    print("" {} Average error in cluster {}: {:.2f} with a max error of {:.1f}, ""\n                          ""for {} annotations""\n                          .format(key, clst, self.dic_stats[\'test\'][key][clst][\'mean\'],\n                                  self.dic_stats[\'test\'][key][clst][\'max\'],\n                                  self.dic_stats[\'test\'][key][clst][\'cnt\']))\n\n                    if key == \'monoloco\':\n                        print(""% of annotation inside the confidence interval: {:.1f} %, ""\n                              ""of which {:.1f} % at higher risk""\n                              .format(self.dic_stats[\'test\'][key][clst][\'interval\']*100,\n                                      self.dic_stats[\'test\'][key][clst][\'at_risk\']*100))\n\n                for perc in self.ALP_THRESHOLDS:\n                    print(""{} Instances with error {}: {:.2f} %""\n                          .format(key, perc, 100 * average(self.errors[key][perc])))\n\n                print(""\\nMatched annotations: {:.1f} %"".format(self.errors[key][\'matched\']))\n                print("" Detected annotations : {}/{} "".format(self.dic_cnt[key], self.cnt_gt))\n                print(""-"" * 100)\n\n            print(""\\n Annotations inside the confidence interval: {:.1f} %""\n                  .format(self.dic_stats[\'test\'][\'monoloco\'][\'all\'][\'interval\']))\n            print(""precision 1: {:.2f}"".format(self.dic_stats[\'test\'][\'monoloco\'][\'all\'][\'prec_1\']))\n            print(""precision 2: {:.2f}"".format(self.dic_stats[\'test\'][\'monoloco\'][\'all\'][\'prec_2\']))\n\n    def summary_table(self, all_methods):\n        """"""Tabulate table for ALP and ALE metrics""""""\n\n        alp = [[str(100 * average(self.errors[key][perc]))[:5]\n                for perc in [\'<0.5m\', \'<1m\', \'<2m\']]\n               for key in all_methods]\n\n        ale = [[str(self.dic_stats[\'test\'][key + \'_merged\'][clst][\'mean\'])[:4] + \' (\' +\n                str(self.dic_stats[\'test\'][key][clst][\'mean\'])[:4] + \')\'\n                for clst in self.CLUSTERS[:4]]\n               for key in all_methods]\n\n        results = [[key] + alp[idx] + ale[idx] for idx, key in enumerate(all_methods)]\n        print(tabulate(results, headers=self.HEADERS))\n        print(\'-\' * 90 + \'\\n\')\n\n\ndef get_statistics(dic_stats, errors, dic_stds, key):\n    """"""Update statistics of a cluster""""""\n\n    dic_stats[\'mean\'] = average(errors)\n    dic_stats[\'max\'] = max(errors)\n    dic_stats[\'cnt\'] = len(errors)\n\n    if key == \'monoloco\':\n        dic_stats[\'std_ale\'] = average(dic_stds[\'ale\'])\n        dic_stats[\'std_epi\'] = average(dic_stds[\'epi\'])\n        dic_stats[\'interval\'] = average(dic_stds[\'interval\'])\n        dic_stats[\'at_risk\'] = average(dic_stds[\'at_risk\'])\n        dic_stats[\'prec_1\'] = average(dic_stds[\'prec_1\'])\n        dic_stats[\'prec_2\'] = average(dic_stds[\'prec_2\'])\n\n\ndef add_true_negatives(err, cnt_gt):\n    """"""Update errors statistics of a specific method with missing detections""""""\n\n    matched = len(err[\'all\'])\n    missed = cnt_gt - matched\n    zeros = [0] * missed\n    err[\'<0.5m\'].extend(zeros)\n    err[\'<1m\'].extend(zeros)\n    err[\'<2m\'].extend(zeros)\n    err[\'matched\'] = 100 * matched / cnt_gt\n\n\ndef find_cluster(dd, clusters):\n    """"""Find the correct cluster. The first and the last one are not numeric""""""\n\n    for clst in clusters[4: -1]:\n        if dd <= int(clst):\n            return clst\n\n    return clusters[-1]\n\n\ndef extract_indices(idx_to_check, *args):\n    """"""\n    Look if a given index j_gt is present in all the other series of indices (_, j)\n    and return the corresponding one for argument\n\n    idx_check --> gt index to check for correspondences in other method\n    idx_method --> index corresponding to the method\n    idx_gt --> index gt of the method\n    idx_pred --> index of the predicted box of the method\n    indices --> list of predicted indices for each method corresponding to the ground truth index to check\n    """"""\n\n    checks = [False]*len(args)\n    indices = []\n    for idx_method, method in enumerate(args):\n        for (idx_pred, idx_gt) in method:\n            if idx_gt == idx_to_check:\n                checks[idx_method] = True\n                indices.append(idx_pred)\n    return all(checks), indices\n\n\ndef average(my_list):\n    """"""calculate mean of a list""""""\n    return sum(my_list) / len(my_list)\n'"
monoloco/eval/generate_kitti.py,2,"b'\n""""""Run monoloco over all the pifpaf joints of KITTI images\nand extract and save the annotations in txt files""""""\n\n\nimport os\nimport glob\nimport shutil\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\n\nfrom ..network import MonoLoco\nfrom ..network.process import preprocess_pifpaf\nfrom ..eval.geom_baseline import compute_distance\nfrom ..utils import get_keypoints, pixel_to_camera, xyz_from_distance, get_calibration, open_annotations, split_training\nfrom .stereo_baselines import baselines_association\nfrom .reid_baseline import ReID, get_reid_features\n\n\nclass GenerateKitti:\n\n    def __init__(self, model, dir_ann, p_dropout=0.2, n_dropout=0, stereo=True):\n\n        # Load monoloco\n        use_cuda = torch.cuda.is_available()\n        device = torch.device(""cuda"" if use_cuda else ""cpu"")\n        self.monoloco = MonoLoco(model=model, device=device, n_dropout=n_dropout, p_dropout=p_dropout)\n        self.dir_ann = dir_ann\n\n        # Extract list of pifpaf files in validation images\n        dir_gt = os.path.join(\'data\', \'kitti\', \'gt\')\n        self.set_basename = factory_basename(dir_ann, dir_gt)\n        self.dir_kk = os.path.join(\'data\', \'kitti\', \'calib\')\n\n        # Calculate stereo baselines\n        self.stereo = stereo\n        if stereo:\n            self.baselines = [\'ml_stereo\', \'pose\', \'reid\']\n            self.cnt_disparity = defaultdict(int)\n            self.cnt_no_stereo = 0\n\n            # ReID Baseline\n            weights_path = \'data/models/reid_model_market.pkl\'\n            self.reid_net = ReID(weights_path=weights_path, device=device, num_classes=751, height=256, width=128)\n            self.dir_images = os.path.join(\'data\', \'kitti\', \'images\')\n            self.dir_images_r = os.path.join(\'data\', \'kitti\', \'images_r\')\n\n    def run(self):\n        """"""Run Monoloco and save txt files for KITTI evaluation""""""\n\n        cnt_ann = cnt_file = cnt_no_file = 0\n        dir_out = {""monoloco"": os.path.join(\'data\', \'kitti\', \'monoloco\')}\n        make_new_directory(dir_out[""monoloco""])\n        print(""\\nCreated empty output directory for txt files"")\n\n        if self.stereo:\n            for key in self.baselines:\n                dir_out[key] = os.path.join(\'data\', \'kitti\', key)\n                make_new_directory(dir_out[key])\n                print(""Created empty output directory for {}"".format(key))\n            print(""\\n"")\n\n        # Run monoloco over the list of images\n        for basename in self.set_basename:\n            path_calib = os.path.join(self.dir_kk, basename + \'.txt\')\n            annotations, kk, tt = factory_file(path_calib, self.dir_ann, basename)\n            boxes, keypoints = preprocess_pifpaf(annotations, im_size=(1242, 374))\n            assert keypoints, ""all pifpaf files should have at least one annotation""\n            cnt_ann += len(boxes)\n            cnt_file += 1\n\n            # Run the network and the geometric baseline\n            outputs, varss = self.monoloco.forward(keypoints, kk)\n            dds_geom = eval_geometric(keypoints, kk, average_y=0.48)\n\n            # Save the file\n            uv_centers = get_keypoints(keypoints, mode=\'bottom\')  # Kitti uses the bottom center to calculate depth\n            xy_centers = pixel_to_camera(uv_centers, kk, 1)\n            outputs = outputs.detach().cpu()\n            zzs = xyz_from_distance(outputs[:, 0:1], xy_centers)[:, 2].tolist()\n\n            all_outputs = [outputs.detach().cpu(), varss.detach().cpu(), dds_geom, zzs]\n            all_inputs = [boxes, xy_centers]\n            all_params = [kk, tt]\n            path_txt = {\'monoloco\': os.path.join(dir_out[\'monoloco\'], basename + \'.txt\')}\n            save_txts(path_txt[\'monoloco\'], all_inputs, all_outputs, all_params)\n\n            # Correct using stereo disparity and save in different folder\n            if self.stereo:\n                zzs = self._run_stereo_baselines(basename, boxes, keypoints, zzs, path_calib)\n                for key in zzs:\n                    path_txt[key] = os.path.join(dir_out[key], basename + \'.txt\')\n                    save_txts(path_txt[key], all_inputs, zzs[key], all_params, mode=\'baseline\')\n\n        print(""\\nSaved in {} txt {} annotations. Not found {} images"".format(cnt_file, cnt_ann, cnt_no_file))\n\n        if self.stereo:\n            print(""STEREO:"")\n            for key in self.baselines:\n                print(""Annotations corrected using {} baseline: {:.1f}%"".format(\n                    key, self.cnt_disparity[key] / cnt_ann * 100))\n            print(""Maximum possible stereo associations: {:.1f}%"".format(self.cnt_disparity[\'max\'] / cnt_ann * 100))\n            print(""Not found {}/{} stereo files"".format(self.cnt_no_stereo, cnt_file))\n\n    def _run_stereo_baselines(self, basename, boxes, keypoints, zzs, path_calib):\n\n        annotations_r, _, _ = factory_file(path_calib, self.dir_ann, basename, mode=\'right\')\n        boxes_r, keypoints_r = preprocess_pifpaf(annotations_r, im_size=(1242, 374))\n\n        # Stereo baselines\n        if keypoints_r:\n            path_image = os.path.join(self.dir_images, basename + \'.png\')\n            path_image_r = os.path.join(self.dir_images_r, basename + \'.png\')\n            reid_features = get_reid_features(self.reid_net, boxes, boxes_r, path_image, path_image_r)\n            zzs, cnt = baselines_association(self.baselines, zzs, keypoints, keypoints_r, reid_features)\n\n            for key in cnt:\n                self.cnt_disparity[key] += cnt[key]\n\n        else:\n            self.cnt_no_stereo += 1\n            zzs = {key: zzs for key in self.baselines}\n        return zzs\n\n\ndef save_txts(path_txt, all_inputs, all_outputs, all_params, mode=\'monoloco\'):\n\n    assert mode in (\'monoloco\', \'baseline\')\n    if mode == \'monoloco\':\n        outputs, varss, dds_geom, zzs = all_outputs[:]\n    else:\n        zzs = all_outputs\n    uv_boxes, xy_centers = all_inputs[:]\n    kk, tt = all_params[:]\n\n    with open(path_txt, ""w+"") as ff:\n        for idx, zz_base in enumerate(zzs):\n\n            xx = float(xy_centers[idx][0]) * zzs[idx] + tt[0]\n            yy = float(xy_centers[idx][1]) * zzs[idx] + tt[1]\n            zz = zz_base + tt[2]\n            cam_0 = [xx, yy, zz]\n            output_list = [0.]*3 + uv_boxes[idx][:-1] + [0.]*3 + cam_0 + [0.] + uv_boxes[idx][-1:]  # kitti format\n            ff.write(""%s "" % \'pedestrian\')\n            for el in output_list:\n                ff.write(""%f "" % el)\n\n            # add additional uncertainty information\n            if mode == \'monoloco\':\n                ff.write(""%f "" % float(outputs[idx][1]))\n                ff.write(""%f "" % float(varss[idx]))\n                ff.write(""%f "" % dds_geom[idx])\n            ff.write(""\\n"")\n\n\ndef factory_file(path_calib, dir_ann, basename, mode=\'left\'):\n    """"""Choose the annotation and the calibration files. Stereo option with ite = 1""""""\n\n    assert mode in (\'left\', \'right\')\n    p_left, p_right = get_calibration(path_calib)\n\n    if mode == \'left\':\n        kk, tt = p_left[:]\n        path_ann = os.path.join(dir_ann, basename + \'.png.pifpaf.json\')\n\n    else:\n        kk, tt = p_right[:]\n        path_ann = os.path.join(dir_ann + \'_right\', basename + \'.png.pifpaf.json\')\n\n    annotations = open_annotations(path_ann)\n\n    return annotations, kk, tt\n\n\ndef eval_geometric(keypoints, kk, average_y=0.48):\n    """""" Evaluate geometric distance""""""\n\n    dds_geom = []\n\n    uv_centers = get_keypoints(keypoints, mode=\'center\')\n    uv_shoulders = get_keypoints(keypoints, mode=\'shoulder\')\n    uv_hips = get_keypoints(keypoints, mode=\'hip\')\n\n    xy_centers = pixel_to_camera(uv_centers, kk, 1)\n    xy_shoulders = pixel_to_camera(uv_shoulders, kk, 1)\n    xy_hips = pixel_to_camera(uv_hips, kk, 1)\n\n    for idx, xy_center in enumerate(xy_centers):\n        zz = compute_distance(xy_shoulders[idx], xy_hips[idx], average_y)\n        xyz_center = np.array([xy_center[0], xy_center[1], zz])\n        dd_geom = float(np.linalg.norm(xyz_center))\n        dds_geom.append(dd_geom)\n\n    return dds_geom\n\n\ndef make_new_directory(dir_out):\n    """"""Remove the output directory if already exists (avoid residual txt files)""""""\n    if os.path.exists(dir_out):\n        shutil.rmtree(dir_out)\n    os.makedirs(dir_out)\n\n\ndef factory_basename(dir_ann, dir_gt):\n    """""" Return all the basenames in the annotations folder corresponding to validation images""""""\n\n    # Extract ground truth validation images\n    names_gt = tuple(os.listdir(dir_gt))\n    path_train = os.path.join(\'splits\', \'kitti_train.txt\')\n    path_val = os.path.join(\'splits\', \'kitti_val.txt\')\n    _, set_val_gt = split_training(names_gt, path_train, path_val)\n    set_val_gt = {os.path.basename(x).split(\'.\')[0] for x in set_val_gt}\n\n    # Extract pifpaf files corresponding to validation images\n    list_ann = glob.glob(os.path.join(dir_ann, \'*.json\'))\n    set_basename = {os.path.basename(x).split(\'.\')[0] for x in list_ann}\n    set_val = set_basename.intersection(set_val_gt)\n    assert set_val, "" Missing json annotations file to create txt files for KITTI datasets""\n    return set_val\n'"
monoloco/eval/geom_baseline.py,0,"b'\nimport json\nimport logging\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\n\nfrom ..utils import pixel_to_camera, get_keypoints\n\nAVERAGE_Y = 0.48\nCLUSTERS = [\'10\', \'20\', \'30\', \'all\']\n\n\ndef geometric_baseline(joints):\n    """"""\n    List of json files --> 2 lists with mean and std for each segment and the total count of instances\n\n    For each annotation:\n    1. From gt boxes calculate the height (deltaY) for the segments head, shoulder, hip, ankle\n    2. From mask boxes calculate distance of people using average height of people and real pixel height\n\n    For left-right ambiguities we chose always the average of the joints\n\n    The joints are mapped from 0 to 16 in the following order:\n    [\'nose\', \'left_eye\', \'right_eye\', \'left_ear\', \'right_ear\', \'left_shoulder\', \'right_shoulder\', \'left_elbow\',\n    \'right_elbow\', \'left_wrist\', \'right_wrist\', \'left_hip\', \'right_hip\', \'left_knee\', \'right_knee\', \'left_ankle\',\n    \'right_ankle\']\n\n    """"""\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    cnt_tot = 0\n    dic_dist = defaultdict(lambda: defaultdict(list))\n\n    # Access the joints file\n    with open(joints, \'r\') as ff:\n        dic_joints = json.load(ff)\n\n    # Calculate distances for all the instances in the joints dictionary\n    for phase in [\'train\', \'val\']:\n        cnt = update_distances(dic_joints[phase], dic_dist, phase, AVERAGE_Y)\n        cnt_tot += cnt\n\n    # Calculate mean and std of each segment\n    dic_h_means = calculate_heights(dic_dist[\'heights\'], mode=\'mean\')\n    dic_h_stds = calculate_heights(dic_dist[\'heights\'], mode=\'std\')\n    errors = calculate_error(dic_dist[\'error\'])\n\n    # Show results\n    logger.info(""Computed distance of {} annotations"".format(cnt_tot))\n    for key in dic_h_means:\n        logger.info(""Average height of segment {} is {:.2f} with a std of {:.2f}"".\n                    format(key, dic_h_means[key], dic_h_stds[key]))\n    for clst in CLUSTERS:\n        logger.info(""Average error over the val set for clst {}: {:.2f}"".format(clst, errors[clst]))\n    logger.info(""Joints used: {}"".format(joints))\n\n\ndef update_distances(dic_fin, dic_dist, phase, average_y):\n\n    # Loop over each annotation in the json file corresponding to the image\n    cnt = 0\n    for idx, kps in enumerate(dic_fin[\'kps\']):\n\n        # Extract pixel coordinates of head, shoulder, hip, ankle and and save them\n        dic_uv = {mode: get_keypoints(kps, mode) for mode in [\'head\', \'shoulder\', \'hip\', \'ankle\']}\n\n        # Convert segments from pixel coordinate to camera coordinate\n        kk = dic_fin[\'K\'][idx]\n        z_met = dic_fin[\'boxes_3d\'][idx][2]\n\n        # Create a dict with all annotations in meters\n        dic_xyz = {key: pixel_to_camera(dic_uv[key], kk, z_met) for key in dic_uv}\n        dic_xyz_norm = {key: pixel_to_camera(dic_uv[key], kk, 1) for key in dic_uv}\n\n        # Compute real height\n        dy_met = abs(float((dic_xyz[\'hip\'][0][1] - dic_xyz[\'shoulder\'][0][1])))\n\n        # Estimate distance for a single annotation\n        z_met_real = compute_distance(dic_xyz_norm[\'shoulder\'][0], dic_xyz_norm[\'hip\'][0], average_y,\n                                      mode=\'real\', dy_met=dy_met)\n        z_met_approx = compute_distance(dic_xyz_norm[\'shoulder\'][0], dic_xyz_norm[\'hip\'][0], average_y, mode=\'average\')\n\n        # Compute distance with respect to the center of the 3D bounding box\n        d_real = math.sqrt(z_met_real ** 2 + dic_fin[\'boxes_3d\'][idx][0] ** 2 + dic_fin[\'boxes_3d\'][idx][1] ** 2)\n        d_approx = math.sqrt(z_met_approx ** 2 +\n                             dic_fin[\'boxes_3d\'][idx][0] ** 2 + dic_fin[\'boxes_3d\'][idx][1] ** 2)\n\n        # Update the dictionary with distance and heights metrics\n        dic_dist = update_dic_dist(dic_dist, dic_xyz, d_real, d_approx, phase)\n        cnt += 1\n\n    return cnt\n\n\ndef compute_distance(xyz_norm_1, xyz_norm_2, average_y, mode=\'average\', dy_met=0):\n    """"""\n    Compute distance Z of a mask annotation (solving a linear system) for 2 possible cases:\n    1. knowing specific height of the annotation (head-ankle) dy_met\n    2. using mean height of people (average_y)\n    """"""\n    assert mode in (\'average\', \'real\')\n\n    x1 = float(xyz_norm_1[0])\n    y1 = float(xyz_norm_1[1])\n    x2 = float(xyz_norm_2[0])\n    y2 = float(xyz_norm_2[1])\n    xx = (x1 + x2) / 2\n\n    # Choose if solving for provided height or average one.\n    if mode == \'average\':\n        cc = - average_y  # Y axis goes down\n    else:\n        cc = -dy_met\n\n    # Solving the linear system Ax = b\n    matrix = np.array([[y1, 0, -xx],\n                       [0, -y1, 1],\n                       [y2, 0, -xx],\n                       [0, -y2, 1]])\n\n    bb = np.array([cc * xx, -cc, 0, 0]).reshape(4, 1)\n    xx = np.linalg.lstsq(matrix, bb, rcond=None)\n    z_met = abs(np.float(xx[0][1]))  # Abs take into account specularity behind the observer\n\n    return z_met\n\n\ndef update_dic_dist(dic_dist, dic_xyz, d_real, d_approx, phase):\n    """""" For every annotation in a single image, update the final dictionary""""""\n\n    # Update the dict with heights metric\n    if phase == \'train\':\n        dic_dist[\'heights\'][\'head\'].append(float(dic_xyz[\'head\'][0][1]))\n        dic_dist[\'heights\'][\'shoulder\'].append(float(dic_xyz[\'shoulder\'][0][1]))\n        dic_dist[\'heights\'][\'hip\'].append(float(dic_xyz[\'hip\'][0][1]))\n        dic_dist[\'heights\'][\'ankle\'].append(float(dic_xyz[\'ankle\'][0][1]))\n\n    # Update the dict with distance metrics for the test phase\n    if phase == \'val\':\n        error = abs(d_real - d_approx)\n\n        if d_real <= 10:\n            dic_dist[\'error\'][\'10\'].append(error)\n        elif d_real <= 20:\n            dic_dist[\'error\'][\'20\'].append(error)\n        elif d_real <= 30:\n            dic_dist[\'error\'][\'30\'].append(error)\n        else:\n            dic_dist[\'error\'][\'>30\'].append(error)\n\n        dic_dist[\'error\'][\'all\'].append(error)\n\n    return dic_dist\n\n\ndef calculate_heights(heights, mode):\n    """"""\n     Compute statistics of heights based on the distance\n     """"""\n\n    assert mode in (\'mean\', \'std\', \'max\')\n    heights_fin = {}\n\n    head_shoulder = np.array(heights[\'shoulder\']) - np.array(heights[\'head\'])\n    shoulder_hip = np.array(heights[\'hip\']) - np.array(heights[\'shoulder\'])\n    hip_ankle = np.array(heights[\'ankle\']) - np.array(heights[\'hip\'])\n\n    if mode == \'mean\':\n        heights_fin[\'head_shoulder\'] = np.float(np.mean(head_shoulder)) * 100\n        heights_fin[\'shoulder_hip\'] = np.float(np.mean(shoulder_hip)) * 100\n        heights_fin[\'hip_ankle\'] = np.float(np.mean(hip_ankle)) * 100\n\n    elif mode == \'std\':\n        heights_fin[\'head_shoulder\'] = np.float(np.std(head_shoulder)) * 100\n        heights_fin[\'shoulder_hip\'] = np.float(np.std(shoulder_hip)) * 100\n        heights_fin[\'hip_ankle\'] = np.float(np.std(hip_ankle)) * 100\n\n    elif mode == \'max\':\n        heights_fin[\'head_shoulder\'] = np.float(np.max(head_shoulder)) * 100\n        heights_fin[\'shoulder_hip\'] = np.float(np.max(shoulder_hip)) * 100\n        heights_fin[\'hip_ankle\'] = np.float(np.max(hip_ankle)) * 100\n\n    return heights_fin\n\n\ndef calculate_error(dic_errors):\n    """"""\n     Compute statistics of distances based on the distance\n     """"""\n    errors = {}\n    for clst in dic_errors:\n        errors[clst] = np.float(np.mean(np.array(dic_errors[clst])))\n    return errors\n'"
monoloco/eval/reid_baseline.py,10,"b'\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as T\n\n\nfrom ..utils import open_image\n\n\ndef get_reid_features(reid_net, boxes, boxes_r, path_image, path_image_r):\n\n    pil_image = open_image(path_image)\n    pil_image_r = open_image(path_image_r)\n    assert boxes and boxes_r\n    cropped_img = []\n    for box in boxes:\n        cropped_img = cropped_img + [pil_image.crop((box[0], box[1], box[2], box[3]))]\n    cropped_img_r = []\n    for box in boxes_r:\n        cropped_img_r = cropped_img_r + [pil_image_r.crop((box[0], box[1], box[2], box[3]))]\n\n    features = reid_net.forward(cropped_img)\n    features_r = reid_net.forward(cropped_img_r)\n    return features.cpu(), features_r.cpu()\n\n\nclass ReID(object):\n    def __init__(self, weights_path, device, num_classes=751, height=256, width=128):\n        super(ReID, self).__init__()\n        torch.manual_seed(1)\n        self.device = device\n\n        if self.device.type == ""cuda"":\n            cudnn.benchmark = True\n            torch.cuda.manual_seed_all(1)\n        else:\n            print(""Currently using CPU (GPU is highly recommended)"")\n\n        self.transform_test = T.Compose([\n            T.Resize((height, width)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        print(""ReID Baseline:"")\n        print(""Initializing ResNet model"")\n        self.model = ResNet50(num_classes=num_classes, loss={\'xent\'})\n        self.model.to(device)\n        num_param = sum(p.numel() for p in self.model.parameters()) / 1e+06\n        print(""Model size: {:.3f} M"".format(num_param))\n\n        # load pretrained weights but ignore layers that don\'t match in size\n        checkpoint = torch.load(weights_path)\n        model_dict = self.model.state_dict()\n        pretrain_dict = {k: v for k, v in checkpoint.items() if k in model_dict and model_dict[k].size() == v.size()}\n        model_dict.update(pretrain_dict)\n        self.model.load_state_dict(model_dict)\n        print(""Loaded pretrained weights from \'{}\'"".format(weights_path))\n        self.model.eval()\n\n    def forward(self, images):\n        image = torch.stack([self.transform_test(image) for image in images], dim=0)\n\n        image = image.to(self.device)\n        with torch.no_grad():\n            features = self.model(image)\n        return features\n\n    @staticmethod\n    def calculate_distmat(features_1, features_2=None, use_cosine=False):\n        query = features_1\n        if features_2 is not None:\n            gallery = features_2\n        else:\n            gallery = features_1\n        m = query.size(0)\n        n = gallery.size(0)\n        if not use_cosine:\n            distmat = torch.pow(query, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n                      torch.pow(gallery, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n            distmat.addmm_(1, -2, query, gallery.t())\n        else:\n            features_norm = query/query.norm(dim=1)[:, None]\n            reference_norm = gallery/gallery.norm(dim=1)[:, None]\n            distmat = torch.mm(features_norm, reference_norm.transpose(0, 1))\n        return distmat\n\n\nclass ResNet50(nn.Module):\n    def __init__(self, num_classes, loss):\n        super(ResNet50, self).__init__()\n        self.loss = loss\n        resnet50 = torchvision.models.resnet50(pretrained=True)\n        self.base = nn.Sequential(*list(resnet50.children())[:-2])\n        self.classifier = nn.Linear(2048, num_classes)\n        self.feat_dim = 2048\n\n    def forward(self, x):\n        x = self.base(x)\n        x = F.avg_pool2d(x, x.size()[2:])\n        f = x.view(x.size(0), -1)\n        if not self.training:\n            return f\n        y = self.classifier(f)\n\n        if self.loss == {\'xent\'}:\n            return y\n        return y, f\n'"
monoloco/eval/stereo_baselines.py,0,"b'\n""""""""Generate stereo baselines for kitti evaluation""""""\n\nimport warnings\nfrom collections import defaultdict\n\nimport numpy as np\n\nfrom ..utils import get_keypoints\n\n\ndef baselines_association(baselines, zzs, keypoints, keypoints_right, reid_features):\n    """"""compute stereo depth for each of the given stereo baselines""""""\n\n    # Initialize variables\n    zzs_stereo = defaultdict()\n    cnt_stereo = defaultdict(int)\n\n    features, features_r, keypoints, keypoints_r = factory_features(\n        keypoints, keypoints_right, baselines, reid_features)\n\n    # count maximum possible associations\n    cnt_stereo[\'max\'] = min(keypoints.shape[0], keypoints_r.shape[0])  # pylint: disable=E1136\n\n    # Filter joints disparity and calculate avg disparity\n    avg_disparities, disparities_x, disparities_y = mask_joint_disparity(keypoints, keypoints_r)\n\n    # Iterate over each left pose\n    for key in baselines:\n\n        # Extract features of the baseline\n        similarity = features_similarity(features[key], features_r[key], key, avg_disparities, zzs)\n\n        # Compute the association based on features minimization and calculate depth\n        zzs_stereo[key] = np.empty((keypoints.shape[0]))\n\n        indices_stereo = []  # keep track of indices\n        best = np.nanmin(similarity)\n        while not np.isnan(best):\n            idx, arg_best = np.unravel_index(np.nanargmin(similarity), similarity.shape)  # pylint: disable=W0632\n            zz_stereo, flag = similarity_to_depth(avg_disparities[idx, arg_best])\n            zz_mono = zzs[idx]\n            similarity[idx, :] = np.nan\n            indices_stereo.append(idx)\n\n            # Filter stereo depth\n            if flag and verify_stereo(zz_stereo, zz_mono, disparities_x[idx, arg_best], disparities_y[idx, arg_best]):\n                zzs_stereo[key][idx] = zz_stereo\n                cnt_stereo[key] += 1\n                similarity[:, arg_best] = np.nan\n            else:\n                zzs_stereo[key][idx] = zz_mono\n\n            best = np.nanmin(similarity)\n        indices_mono = [idx for idx, _ in enumerate(zzs) if idx not in indices_stereo]\n        for idx in indices_mono:\n            zzs_stereo[key][idx] = zzs[idx]\n        zzs_stereo[key] = zzs_stereo[key].tolist()\n\n    return zzs_stereo, cnt_stereo\n\n\ndef factory_features(keypoints, keypoints_right, baselines, reid_features):\n\n    features = defaultdict()\n    features_r = defaultdict()\n\n    for key in baselines:\n        if key == \'reid\':\n            features[key] = np.array(reid_features[0])\n            features_r[key] = np.array(reid_features[1])\n        else:\n            features[key] = np.array(keypoints)\n            features_r[key] = np.array(keypoints_right)\n\n    return features, features_r, np.array(keypoints), np.array(keypoints_right)\n\n\ndef features_similarity(features, features_r, key, avg_disparities, zzs):\n\n    similarity = np.empty((features.shape[0], features_r.shape[0]))\n    for idx, zz_mono in enumerate(zzs):\n        feature = features[idx]\n\n        if key == \'ml_stereo\':\n            expected_disparity = 0.54 * 721. / zz_mono\n            sim_row = np.abs(expected_disparity - avg_disparities[idx])\n\n        elif key == \'pose\':\n            # Zero-center the keypoints\n            uv_center = np.array(get_keypoints(feature, mode=\'center\').reshape(-1, 1))  # (1, 2) --> (2, 1)\n            uv_centers_r = np.array(get_keypoints(features_r, mode=\'center\').unsqueeze(-1))  # (m,2) --> (m, 2, 1)\n            feature_0 = feature[:2, :] - uv_center\n            feature_0 = feature_0.reshape(1, -1)  # (1, 34)\n            features_r_0 = features_r[:, :2, :] - uv_centers_r\n            features_r_0 = features_r_0.reshape(features_r_0.shape[0], -1)  # (m, 34)\n            sim_row = np.linalg.norm(feature_0 - features_r_0, axis=1)\n\n        else:\n            sim_row = np.linalg.norm(feature - features_r, axis=1)\n\n        similarity[idx] = sim_row\n    return similarity\n\n\ndef similarity_to_depth(avg_disparity):\n\n    try:\n        zz_stereo = 0.54 * 721. / float(avg_disparity)\n        flag = True\n    except (ZeroDivisionError, ValueError):  # All nan-slices or zero division\n        zz_stereo = np.nan\n        flag = False\n\n    return zz_stereo, flag\n\n\ndef mask_joint_disparity(keypoints, keypoints_r):\n    """"""filter joints based on confidence and interquartile range of the distribution""""""\n\n    CONF_MIN = 0.3\n    with warnings.catch_warnings() and np.errstate(invalid=\'ignore\'):\n        disparity_x_mask = np.empty((keypoints.shape[0], keypoints_r.shape[0], 17))\n        disparity_y_mask = np.empty((keypoints.shape[0], keypoints_r.shape[0], 17))\n        avg_disparity = np.empty((keypoints.shape[0], keypoints_r.shape[0]))\n\n        for idx, kps in enumerate(keypoints):\n            disparity_x = kps[0, :] - keypoints_r[:, 0, :]\n            disparity_y = kps[1, :] - keypoints_r[:, 1, :]\n\n            # Mask for low confidence\n            mask_conf_left = kps[2, :] > CONF_MIN\n            mask_conf_right = keypoints_r[:, 2, :] > CONF_MIN\n            mask_conf = mask_conf_left & mask_conf_right\n            disparity_x_conf = np.where(mask_conf, disparity_x, np.nan)\n            disparity_y_conf = np.where(mask_conf, disparity_y, np.nan)\n\n            # Mask outliers using iqr\n            mask_outlier = interquartile_mask(disparity_x_conf)\n            x_mask_row = np.where(mask_outlier, disparity_x_conf, np.nan)\n            y_mask_row = np.where(mask_outlier, disparity_y_conf, np.nan)\n            avg_row = np.nanmedian(x_mask_row, axis=1)  # ignore the nan\n\n            # Append\n            disparity_x_mask[idx] = x_mask_row\n            disparity_y_mask[idx] = y_mask_row\n            avg_disparity[idx] = avg_row\n\n        return avg_disparity, disparity_x_mask, disparity_y_mask\n\n\ndef verify_stereo(zz_stereo, zz_mono, disparity_x, disparity_y):\n    """"""Verify disparities based on coefficient of variation, maximum y difference and z difference wrt monoloco""""""\n\n    COV_MIN = 0.1\n    y_max_difference = (50 / zz_mono)\n    z_max_difference = 0.6 * zz_mono\n\n    cov = float(np.nanstd(disparity_x) / np.abs(np.nanmean(disparity_x)))  # Coefficient of variation\n    avg_disparity_y = np.nanmedian(disparity_y)\n\n    if abs(zz_stereo - zz_mono) < z_max_difference and \\\n            avg_disparity_y < y_max_difference and \\\n            cov < COV_MIN\\\n            and 4 < zz_stereo < 40:\n        return True\n    # if not np.isnan(zz_stereo):\n    #     return True\n    return False\n\n\ndef interquartile_mask(distribution):\n    quartile_1, quartile_3 = np.nanpercentile(distribution, [25, 75], axis=1)\n    iqr = quartile_3 - quartile_1\n    lower_bound = quartile_1 - (iqr * 1.5)\n    upper_bound = quartile_3 + (iqr * 1.5)\n    return (distribution < upper_bound.reshape(-1, 1)) & (distribution > lower_bound.reshape(-1, 1))\n'"
monoloco/network/__init__.py,0,"b'\nfrom .pifpaf import PifPaf, ImageList\nfrom .losses import LaplacianLoss\nfrom .net import MonoLoco\n'"
monoloco/network/architectures.py,1,"b'\nimport torch.nn as nn\n\n\nclass LinearModel(nn.Module):\n    """"""\n    Architecture inspired by https://github.com/una-dinosauria/3d-pose-baseline\n    Pytorch implementation from: https://github.com/weigq/3d_pose_baseline_pytorch\n    """"""\n\n    def __init__(self, input_size, output_size=2, linear_size=256, p_dropout=0.2, num_stage=3):\n        super(LinearModel, self).__init__()\n\n        self.input_size = input_size\n        self.output_size = output_size\n        self.linear_size = linear_size\n        self.p_dropout = p_dropout\n        self.num_stage = num_stage\n\n        # process input to linear size\n        self.w1 = nn.Linear(self.input_size, self.linear_size)\n        self.batch_norm1 = nn.BatchNorm1d(self.linear_size)\n\n        self.linear_stages = []\n        for _ in range(num_stage):\n            self.linear_stages.append(Linear(self.linear_size, self.p_dropout))\n        self.linear_stages = nn.ModuleList(self.linear_stages)\n\n        # post processing\n        self.w2 = nn.Linear(self.linear_size, self.output_size)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(self.p_dropout)\n\n    def forward(self, x):\n        # pre-processing\n        y = self.w1(x)\n        y = self.batch_norm1(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n        # linear layers\n        for i in range(self.num_stage):\n            y = self.linear_stages[i](y)\n        y = self.w2(y)\n        return y\n\n\nclass Linear(nn.Module):\n    def __init__(self, linear_size, p_dropout=0.5):\n        super(Linear, self).__init__()\n        self.l_size = linear_size\n\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(p_dropout)\n\n        self.w1 = nn.Linear(self.l_size, self.l_size)\n        self.batch_norm1 = nn.BatchNorm1d(self.l_size)\n\n        self.w2 = nn.Linear(self.l_size, self.l_size)\n        self.batch_norm2 = nn.BatchNorm1d(self.l_size)\n\n    def forward(self, x):\n        y = self.w1(x)\n        y = self.batch_norm1(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n\n        y = self.w2(y)\n        y = self.batch_norm2(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n\n        out = x + y\n\n        return out\n'"
monoloco/network/losses.py,14,"b'\nimport math\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nclass CustomL1Loss(torch.nn.Module):\n    """"""\n    L1 loss with more weight to errors at a shorter distance\n    It inherits from nn.module so it supports backward\n    """"""\n\n    def __init__(self, dic_norm, device, beta=1):\n        super(CustomL1Loss, self).__init__()\n\n        self.dic_norm = dic_norm\n        self.device = device\n        self.beta = beta\n\n    @staticmethod\n    def compute_weights(xx, beta=1):\n        """"""\n        Return the appropriate weight depending on the distance and the hyperparameter chosen\n        alpha = 1 refers to the curve of A Photogrammetric Approach for Real-time...\n        It is made for unnormalized outputs (to be more understandable)\n        From 70 meters on every value is weighted the same (0.1**beta)\n        Alpha is optional value from Focal loss. Yet to be analyzed\n        """"""\n        # alpha = np.maximum(1, 10 ** (beta - 1))\n        alpha = 1\n        ww = np.maximum(0.1, 1 - xx / 78)**beta\n\n        return alpha * ww\n\n    def print_loss(self):\n        xx = np.linspace(0, 80, 100)\n        y1 = self.compute_weights(xx, beta=1)\n        y2 = self.compute_weights(xx, beta=2)\n        y3 = self.compute_weights(xx, beta=3)\n        plt.plot(xx, y1)\n        plt.plot(xx, y2)\n        plt.plot(xx, y3)\n        plt.xlabel(""Distance [m]"")\n        plt.ylabel(""Loss function Weight"")\n        plt.legend((""Beta = 1"", ""Beta = 2"", ""Beta = 3""))\n        plt.show()\n\n    def forward(self, output, target):\n\n        unnormalized_output = output.cpu().detach().numpy() * self.dic_norm[\'std\'][\'Y\'] + self.dic_norm[\'mean\'][\'Y\']\n        weights_np = self.compute_weights(unnormalized_output, self.beta)\n        weights = torch.from_numpy(weights_np).float().to(self.device)  # To make weights in the same cuda device\n        losses = torch.abs(output - target) * weights\n        loss = losses.mean()  # Mean over the batch\n        return loss\n\n\nclass LaplacianLoss(torch.nn.Module):\n    """"""1D Gaussian with std depending on the absolute distance\n    """"""\n    def __init__(self, size_average=True, reduce=True, evaluate=False):\n        super(LaplacianLoss, self).__init__()\n        self.size_average = size_average\n        self.reduce = reduce\n        self.evaluate = evaluate\n\n    def laplacian_1d(self, mu_si, xx):\n        """"""\n        1D Gaussian Loss. f(x | mu, sigma). The network outputs mu and sigma. X is the ground truth distance.\n        This supports backward().\n        Inspired by\n        https://github.com/naba89/RNN-Handwriting-Generation-Pytorch/blob/master/loss_functions.py\n\n        """"""\n        mu, si = mu_si[:, 0:1], mu_si[:, 1:2]\n        # norm = xx - mu\n        norm = 1 - mu / xx  # Relative\n\n        term_a = torch.abs(norm) * torch.exp(-si)\n        term_b = si\n        norm_bi = (np.mean(np.abs(norm.cpu().detach().numpy())), np.mean(torch.exp(si).cpu().detach().numpy()))\n\n        if self.evaluate:\n            return norm_bi\n        return term_a + term_b\n\n    def forward(self, outputs, targets):\n\n        values = self.laplacian_1d(outputs, targets)\n\n        if not self.reduce or self.evaluate:\n            return values\n        if self.size_average:\n            mean_values = torch.mean(values)\n            return mean_values\n        return torch.sum(values)\n\n\nclass GaussianLoss(torch.nn.Module):\n    """"""1D Gaussian with std depending on the absolute distance\n    """"""\n    def __init__(self, device, size_average=True, reduce=True, evaluate=False):\n        super(GaussianLoss, self).__init__()\n        self.size_average = size_average\n        self.reduce = reduce\n        self.evaluate = evaluate\n        self.device = device\n\n    def gaussian_1d(self, mu_si, xx):\n        """"""\n        1D Gaussian Loss. f(x | mu, sigma). The network outputs mu and sigma. X is the ground truth distance.\n        This supports backward().\n        Inspired by\n        https://github.com/naba89/RNN-Handwriting-Generation-Pytorch/blob/master/loss_functions.py\n        """"""\n        mu, si = mu_si[:, 0:1], mu_si[:, 1:2]\n\n        min_si = torch.ones(si.size()).cuda(self.device) * 0.1\n        si = torch.max(min_si, si)\n        norm = xx - mu\n        term_a = (norm / si)**2 / 2\n        term_b = torch.log(si * math.sqrt(2 * math.pi))\n\n        norm_si = (np.mean(np.abs(norm.cpu().detach().numpy())), np.mean(si.cpu().detach().numpy()))\n\n        if self.evaluate:\n            return norm_si\n\n        return term_a + term_b\n\n    def forward(self, outputs, targets):\n\n        values = self.gaussian_1d(outputs, targets)\n\n        if not self.reduce or self.evaluate:\n            return values\n        if self.size_average:\n            mean_values = torch.mean(values)\n            return mean_values\n        return torch.sum(values)\n'"
monoloco/network/net.py,7,"b'\n""""""\nMonoloco class. From 2D joints to real-world distances\n""""""\n\nimport logging\nfrom collections import defaultdict\n\nimport torch\n\nfrom ..utils import get_iou_matches, reorder_matches, get_keypoints, pixel_to_camera, xyz_from_distance\nfrom .process import preprocess_monoloco, unnormalize_bi, laplace_sampling\nfrom .architectures import LinearModel\n\n\nclass MonoLoco:\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    INPUT_SIZE = 17 * 2\n    LINEAR_SIZE = 256\n    N_SAMPLES = 100\n\n    def __init__(self, model, device=None, n_dropout=0, p_dropout=0.2):\n\n        if not device:\n            self.device = torch.device(\'cpu\')\n        else:\n            self.device = device\n        self.n_dropout = n_dropout\n        self.epistemic = bool(self.n_dropout > 0)\n\n        # if the path is provided load the model parameters\n        if isinstance(model, str):\n            model_path = model\n            self.model = LinearModel(p_dropout=p_dropout, input_size=self.INPUT_SIZE, linear_size=self.LINEAR_SIZE)\n            self.model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n\n        # if the model is directly provided\n        else:\n            self.model = model\n        self.model.eval()  # Default is train\n        self.model.to(self.device)\n\n    def forward(self, keypoints, kk):\n        """"""forward pass of monoloco network""""""\n        if not keypoints:\n            return None, None\n\n        with torch.no_grad():\n            inputs = preprocess_monoloco(torch.tensor(keypoints).to(self.device), torch.tensor(kk).to(self.device))\n            if self.n_dropout > 0:\n                self.model.dropout.training = True  # Manually reactivate dropout in eval\n                total_outputs = torch.empty((0, inputs.size()[0])).to(self.device)\n\n                for _ in range(self.n_dropout):\n                    outputs = self.model(inputs)\n                    outputs = unnormalize_bi(outputs)\n                    samples = laplace_sampling(outputs, self.N_SAMPLES)\n                    total_outputs = torch.cat((total_outputs, samples), 0)\n                varss = total_outputs.std(0)\n                self.model.dropout.training = False\n            else:\n                varss = torch.zeros(inputs.size()[0])\n\n            #  Don\'t use dropout for the mean prediction\n            outputs = self.model(inputs)\n            outputs = unnormalize_bi(outputs)\n        return outputs, varss\n\n    @staticmethod\n    def post_process(outputs, varss, boxes, keypoints, kk, dic_gt=None, iou_min=0.3):\n        """"""Post process monoloco to output final dictionary with all information for visualizations""""""\n\n        dic_out = defaultdict(list)\n        if outputs is None:\n            return dic_out\n\n        if dic_gt:\n            boxes_gt, dds_gt = dic_gt[\'boxes\'], dic_gt[\'dds\']\n            matches = get_iou_matches(boxes, boxes_gt, thresh=iou_min)\n            print(""found {} matches with ground-truth"".format(len(matches)))\n        else:\n            matches = [(idx, idx) for idx, _ in enumerate(boxes)]  # Replicate boxes\n\n        matches = reorder_matches(matches, boxes, mode=\'left_right\')\n        uv_shoulders = get_keypoints(keypoints, mode=\'shoulder\')\n        uv_centers = get_keypoints(keypoints, mode=\'center\')\n        xy_centers = pixel_to_camera(uv_centers, kk, 1)\n\n        # Match with ground truth if available\n        for idx, idx_gt in matches:\n            dd_pred = float(outputs[idx][0])\n            ale = float(outputs[idx][1])\n            var_y = float(varss[idx])\n            dd_real = dds_gt[idx_gt] if dic_gt else dd_pred\n\n            kps = keypoints[idx]\n            box = boxes[idx]\n            uu_s, vv_s = uv_shoulders.tolist()[idx][0:2]\n            uu_c, vv_c = uv_centers.tolist()[idx][0:2]\n            uv_shoulder = [round(uu_s), round(vv_s)]\n            uv_center = [round(uu_c), round(vv_c)]\n            xyz_real = xyz_from_distance(dd_real, xy_centers[idx])\n            xyz_pred = xyz_from_distance(dd_pred, xy_centers[idx])\n            dic_out[\'boxes\'].append(box)\n            dic_out[\'boxes_gt\'].append(boxes_gt[idx_gt] if dic_gt else boxes[idx])\n            dic_out[\'dds_real\'].append(dd_real)\n            dic_out[\'dds_pred\'].append(dd_pred)\n            dic_out[\'stds_ale\'].append(ale)\n            dic_out[\'stds_epi\'].append(var_y)\n            dic_out[\'xyz_real\'].append(xyz_real.squeeze().tolist())\n            dic_out[\'xyz_pred\'].append(xyz_pred.squeeze().tolist())\n            dic_out[\'uv_kps\'].append(kps)\n            dic_out[\'uv_centers\'].append(uv_center)\n            dic_out[\'uv_shoulders\'].append(uv_shoulder)\n\n        return dic_out\n'"
monoloco/network/pifpaf.py,4,"b'\nimport glob\n\nimport numpy as np\nimport torchvision\nimport torch\nfrom PIL import Image, ImageFile\nfrom openpifpaf.network import nets\nfrom openpifpaf import decoder\n\nfrom .process import image_transform\n\n\nclass ImageList(torch.utils.data.Dataset):\n    """"""It defines transformations to apply to images and outputs of the dataloader""""""\n    def __init__(self, image_paths, scale):\n        self.image_paths = image_paths\n        self.scale = scale\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        ImageFile.LOAD_TRUNCATED_IMAGES = True\n        with open(image_path, \'rb\') as f:\n            image = Image.open(f).convert(\'RGB\')\n\n        if self.scale > 1.01 or self.scale < 0.99:\n            image = torchvision.transforms.functional.resize(image,\n                                                             (round(self.scale * image.size[1]),\n                                                              round(self.scale * image.size[0])),\n                                                             interpolation=Image.BICUBIC)\n        # PIL images are not iterables\n        original_image = torchvision.transforms.functional.to_tensor(image)  # 0-255 --> 0-1\n        image = image_transform(image)\n\n        return image_path, original_image, image\n\n    def __len__(self):\n        return len(self.image_paths)\n\n\ndef factory_from_args(args):\n\n    # Merge the model_pifpaf argument\n    if not args.checkpoint:\n        args.checkpoint = \'resnet152\'  # Default model Resnet 152\n    # glob\n    if not args.webcam:\n        if args.glob:\n            args.images += glob.glob(args.glob)\n        if not args.images:\n            raise Exception(""no image files given"")\n\n    # add args.device\n    args.device = torch.device(\'cpu\')\n    args.pin_memory = False\n    if torch.cuda.is_available():\n        args.device = torch.device(\'cuda\')\n        args.pin_memory = True\n\n    # Add num_workers\n    args.loader_workers = 8\n\n    # Add visualization defaults\n    args.figure_width = 10\n    args.dpi_factor = 1.0\n\n    return args\n\n\nclass PifPaf:\n    def __init__(self, args):\n        """"""Instanciate the mdodel""""""\n        factory_from_args(args)\n        model_pifpaf, _ = nets.factory_from_args(args)\n        model_pifpaf = model_pifpaf.to(args.device)\n        self.processor = decoder.factory_from_args(args, model_pifpaf)\n        self.keypoints_whole = []\n\n        # Scale the keypoints to the original image size for printing (if not webcam)\n        if not args.webcam:\n            self.scale_np = np.array([args.scale, args.scale, 1] * 17).reshape(17, 3)\n        else:\n            self.scale_np = np.array([1, 1, 1] * 17).reshape(17, 3)\n\n    def fields(self, processed_images):\n        """"""Encoder for pif and paf fields""""""\n        fields_batch = self.processor.fields(processed_images)\n        return fields_batch\n\n    def forward(self, image, processed_image_cpu, fields):\n        """"""Decoder, from pif and paf fields to keypoints""""""\n        self.processor.set_cpu_image(image, processed_image_cpu)\n        keypoint_sets, scores = self.processor.keypoint_sets(fields)\n\n        if keypoint_sets.size > 0:\n            self.keypoints_whole.append(np.around((keypoint_sets / self.scale_np), 1)\n                                        .reshape(keypoint_sets.shape[0], -1).tolist())\n\n        pifpaf_out = [\n            {\'keypoints\': np.around(kps / self.scale_np, 1).reshape(-1).tolist(),\n             \'bbox\': [np.min(kps[:, 0]) / self.scale_np[0, 0], np.min(kps[:, 1]) / self.scale_np[0, 0],\n                      np.max(kps[:, 0]) / self.scale_np[0, 0], np.max(kps[:, 1]) / self.scale_np[0, 0]]}\n            for kps in keypoint_sets\n        ]\n        return keypoint_sets, scores, pifpaf_out\n'"
monoloco/network/process.py,7,"b'\nimport json\n\nimport numpy as np\nimport torch\nimport torchvision\n\nfrom ..utils import get_keypoints, pixel_to_camera\n\n\ndef preprocess_monoloco(keypoints, kk):\n\n    """""" Preprocess batches of inputs\n    keypoints = torch tensors of (m, 3, 17)  or list [3,17]\n    Outputs =  torch tensors of (m, 34) in meters normalized (z=1) and zero-centered using the center of the box\n    """"""\n    if isinstance(keypoints, list):\n        keypoints = torch.tensor(keypoints)\n    if isinstance(kk, list):\n        kk = torch.tensor(kk)\n    # Projection in normalized image coordinates and zero-center with the center of the bounding box\n    uv_center = get_keypoints(keypoints, mode=\'center\')\n    xy1_center = pixel_to_camera(uv_center, kk, 10)\n    xy1_all = pixel_to_camera(keypoints[:, 0:2, :], kk, 10)\n    kps_norm = xy1_all - xy1_center.unsqueeze(1)  # (m, 17, 3) - (m, 1, 3)\n    kps_out = kps_norm[:, :, 0:2].reshape(kps_norm.size()[0], -1)  # no contiguous for view\n    return kps_out\n\n\ndef factory_for_gt(im_size, name=None, path_gt=None):\n    """"""Look for ground-truth annotations file and define calibration matrix based on image size """"""\n\n    try:\n        with open(path_gt, \'r\') as f:\n            dic_names = json.load(f)\n        print(\'-\' * 120 + ""\\nGround-truth file opened"")\n    except (FileNotFoundError, TypeError):\n        print(\'-\' * 120 + ""\\nGround-truth file not found"")\n        dic_names = {}\n\n    try:\n        kk = dic_names[name][\'K\']\n        dic_gt = dic_names[name]\n        print(""Matched ground-truth file!"")\n    except KeyError:\n        dic_gt = None\n        x_factor = im_size[0] / 1600\n        y_factor = im_size[1] / 900\n        pixel_factor = (x_factor + y_factor) / 2   # TODO remove and check it\n        if im_size[0] / im_size[1] > 2.5:\n            kk = [[718.3351, 0., 600.3891], [0., 718.3351, 181.5122], [0., 0., 1.]]  # Kitti calibration\n        else:\n            kk = [[1266.4 * pixel_factor, 0., 816.27 * x_factor],\n                  [0, 1266.4 * pixel_factor, 491.5 * y_factor],\n                  [0., 0., 1.]]  # nuScenes calibration\n\n        print(""Using a standard calibration matrix..."")\n\n    return kk, dic_gt\n\n\ndef laplace_sampling(outputs, n_samples):\n\n    # np.random.seed(1)\n    mu = outputs[:, 0]\n    bi = torch.abs(outputs[:, 1])\n\n    # Analytical\n    # uu = np.random.uniform(low=-0.5, high=0.5, size=mu.shape[0])\n    # xx = mu - bi * np.sign(uu) * np.log(1 - 2 * np.abs(uu))\n\n    # Sampling\n    cuda_check = outputs.is_cuda\n    if cuda_check:\n        get_device = outputs.get_device()\n        device = torch.device(type=""cuda"", index=get_device)\n    else:\n        device = torch.device(""cpu"")\n\n    laplace = torch.distributions.Laplace(mu, bi)\n    xx = laplace.sample((n_samples,)).to(device)\n\n    return xx\n\n\ndef unnormalize_bi(outputs):\n    """"""Unnormalize relative bi of a nunmpy array""""""\n\n    outputs[:, 1] = torch.exp(outputs[:, 1]) * outputs[:, 0]\n    return outputs\n\n\ndef preprocess_pifpaf(annotations, im_size=None):\n    """"""\n    Preprocess pif annotations:\n    1. enlarge the box of 10%\n    2. Constraint it inside the image (if image_size provided)\n    """"""\n\n    boxes = []\n    keypoints = []\n\n    for dic in annotations:\n        box = dic[\'bbox\']\n        if box[3] < 0.5:  # Check for no detections (boxes 0,0,0,0)\n            return [], []\n\n        kps = prepare_pif_kps(dic[\'keypoints\'])\n        conf = float(np.sort(np.array(kps[2]))[-3])  # The confidence is the 3rd highest value for the keypoints\n\n        # Add 15% for y and 20% for x\n        delta_h = (box[3] - box[1]) / 7\n        delta_w = (box[2] - box[0]) / 3.5\n        assert delta_h > -5 and delta_w > -5, ""Bounding box <=0""\n        box[0] -= delta_w\n        box[1] -= delta_h\n        box[2] += delta_w\n        box[3] += delta_h\n\n        # Put the box inside the image\n        if im_size is not None:\n            box[0] = max(0, box[0])\n            box[1] = max(0, box[1])\n            box[2] = min(box[2], im_size[0])\n            box[3] = min(box[3], im_size[1])\n\n        box.append(conf)\n        boxes.append(box)\n        keypoints.append(kps)\n\n    return boxes, keypoints\n\n\ndef prepare_pif_kps(kps_in):\n    """"""Convert from a list of 51 to a list of 3, 17""""""\n\n    assert len(kps_in) % 3 == 0, ""keypoints expected as a multiple of 3""\n    xxs = kps_in[0:][::3]\n    yys = kps_in[1:][::3]  # from offset 1 every 3\n    ccs = kps_in[2:][::3]\n\n    return [xxs, yys, ccs]\n\n\ndef image_transform(image):\n\n    normalize = torchvision.transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n    transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), normalize, ])\n    return transforms(image)\n'"
monoloco/prep/__init__.py,0,b''
monoloco/prep/preprocess_ki.py,0,"b'""""""Preprocess annotations with KITTI ground-truth""""""\n\nimport os\nimport glob\nimport copy\nimport logging\nfrom collections import defaultdict\nimport json\nimport datetime\n\nfrom .transforms import transform_keypoints\nfrom ..utils import get_calibration, split_training, parse_ground_truth, get_iou_matches, append_cluster\nfrom ..network.process import preprocess_pifpaf, preprocess_monoloco\n\n\nclass PreprocessKitti:\n    """"""Prepare arrays with same format as nuScenes preprocessing but using ground truth txt files""""""\n\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    dic_jo = {\'train\': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[],\n                            clst=defaultdict(lambda: defaultdict(list))),\n              \'val\': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[],\n                          clst=defaultdict(lambda: defaultdict(list))),\n              \'test\': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[],\n                           clst=defaultdict(lambda: defaultdict(list)))}\n    dic_names = defaultdict(lambda: defaultdict(list))\n\n    def __init__(self, dir_ann, iou_min):\n\n        self.dir_ann = dir_ann\n        self.iou_min = iou_min\n        self.dir_gt = os.path.join(\'data\', \'kitti\', \'gt\')\n        self.names_gt = tuple(os.listdir(self.dir_gt))\n        self.dir_kk = os.path.join(\'data\', \'kitti\', \'calib\')\n        self.list_gt = glob.glob(self.dir_gt + \'/*.txt\')\n        assert os.path.exists(self.dir_gt), ""Ground truth dir does not exist""\n        assert os.path.exists(self.dir_ann), ""Annotation dir does not exist""\n\n        now = datetime.datetime.now()\n        now_time = now.strftime(""%Y%m%d-%H%M"")[2:]\n        dir_out = os.path.join(\'data\', \'arrays\')\n        self.path_joints = os.path.join(dir_out, \'joints-kitti-\' + now_time + \'.json\')\n        self.path_names = os.path.join(dir_out, \'names-kitti-\' + now_time + \'.json\')\n        path_train = os.path.join(\'splits\', \'kitti_train.txt\')\n        path_val = os.path.join(\'splits\', \'kitti_val.txt\')\n        self.set_train, self.set_val = split_training(self.names_gt, path_train, path_val)\n\n    def run(self):\n        """"""Save json files""""""\n\n        cnt_gt = cnt_files = cnt_files_ped = cnt_fnf = 0\n        dic_cnt = {\'train\': 0, \'val\': 0, \'test\': 0}\n\n        for name in self.names_gt:\n            path_gt = os.path.join(self.dir_gt, name)\n            basename, _ = os.path.splitext(name)\n\n            phase, flag = self._factory_phase(name)\n            if flag:\n                cnt_fnf += 1\n                continue\n\n            # Extract keypoints\n            path_txt = os.path.join(self.dir_kk, basename + \'.txt\')\n            p_left, _ = get_calibration(path_txt)\n            kk = p_left[0]\n\n            # Iterate over each line of the gt file and save box location and distances\n            boxes_gt, boxes_3d, dds_gt = parse_ground_truth(path_gt, category=\'all\')[:3]\n\n            self.dic_names[basename + \'.png\'][\'boxes\'] = copy.deepcopy(boxes_gt)\n            self.dic_names[basename + \'.png\'][\'dds\'] = copy.deepcopy(dds_gt)\n            self.dic_names[basename + \'.png\'][\'K\'] = copy.deepcopy(kk)\n            cnt_gt += len(boxes_gt)\n            cnt_files += 1\n            cnt_files_ped += min(len(boxes_gt), 1)  # if no boxes 0 else 1\n\n            # Find the annotations if exists\n            try:\n                with open(os.path.join(self.dir_ann, basename + \'.png.pifpaf.json\'), \'r\') as f:\n                    annotations = json.load(f)\n                boxes, keypoints = preprocess_pifpaf(annotations, im_size=(1238, 374))\n                keypoints_hflip = transform_keypoints(keypoints, mode=\'flip\')\n                inputs = preprocess_monoloco(keypoints, kk).tolist()\n                inputs_hflip = preprocess_monoloco(keypoints, kk).tolist()\n                all_keypoints = [keypoints, keypoints_hflip]\n                all_inputs = [inputs, inputs_hflip]\n\n            except FileNotFoundError:\n                boxes = []\n\n            # Match each set of keypoint with a ground truth\n            matches = get_iou_matches(boxes, boxes_gt, self.iou_min)\n            for (idx, idx_gt) in matches:\n                for nn, keypoints in enumerate(all_keypoints):\n                    inputs = all_inputs[nn]\n                    self.dic_jo[phase][\'kps\'].append(keypoints[idx])\n                    self.dic_jo[phase][\'X\'].append(inputs[idx])\n                    self.dic_jo[phase][\'Y\'].append([dds_gt[idx_gt]])  # Trick to make it (nn,1)\n                    self.dic_jo[phase][\'boxes_3d\'].append(boxes_3d[idx_gt])\n                    self.dic_jo[phase][\'K\'].append(kk)\n                    self.dic_jo[phase][\'names\'].append(name)  # One image name for each annotation\n                    append_cluster(self.dic_jo, phase, inputs[idx], dds_gt[idx_gt], keypoints[idx])\n                dic_cnt[phase] += 1\n\n        with open(self.path_joints, \'w\') as file:\n            json.dump(self.dic_jo, file)\n        with open(os.path.join(self.path_names), \'w\') as file:\n            json.dump(self.dic_names, file)\n        for phase in [\'train\', \'val\', \'test\']:\n            print(""Saved {} annotations for phase {}""\n                  .format(dic_cnt[phase], phase))\n        print(""Number of GT files: {}. Files with at least one pedestrian: {}.  Files not found: {}""\n              .format(cnt_files, cnt_files_ped, cnt_fnf))\n        print(""Matched : {:.1f} % of the ground truth instances""\n              .format(100 * (dic_cnt[\'train\'] + dic_cnt[\'val\']) / cnt_gt))\n        print(""\\nOutput files:\\n{}\\n{}\\n"".format(self.path_names, self.path_joints))\n\n    def _factory_phase(self, name):\n        """"""Choose the phase""""""\n\n        phase = None\n        flag = False\n        if name in self.set_train:\n            phase = \'train\'\n        elif name in self.set_val:\n            phase = \'val\'\n        else:\n            flag = True\n        return phase, flag\n'"
monoloco/prep/preprocess_nu.py,0,"b'""""""Extract joints annotations and match with nuScenes ground truths\n""""""\n\nimport os\nimport sys\nimport time\nimport json\nimport logging\nfrom collections import defaultdict\nimport datetime\n\nimport numpy as np\nfrom nuscenes.nuscenes import NuScenes\nfrom nuscenes.utils import splits\n\nfrom ..utils import get_iou_matches, append_cluster, select_categories, project_3d\nfrom ..network.process import preprocess_pifpaf, preprocess_monoloco\n\n\nclass PreprocessNuscenes:\n    """"""\n    Preprocess Nuscenes dataset\n    """"""\n    CAMERAS = (\'CAM_FRONT\', \'CAM_FRONT_LEFT\', \'CAM_FRONT_RIGHT\', \'CAM_BACK\', \'CAM_BACK_LEFT\', \'CAM_BACK_RIGHT\')\n    dic_jo = {\'train\': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[],\n                            clst=defaultdict(lambda: defaultdict(list))),\n              \'val\': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[],\n                          clst=defaultdict(lambda: defaultdict(list))),\n              \'test\': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[],\n                           clst=defaultdict(lambda: defaultdict(list)))\n              }\n    dic_names = defaultdict(lambda: defaultdict(list))\n\n    def __init__(self, dir_ann, dir_nuscenes, dataset, iou_min):\n\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n\n        self.iou_min = iou_min\n        self.dir_ann = dir_ann\n        dir_out = os.path.join(\'data\', \'arrays\')\n        assert os.path.exists(dir_nuscenes), ""Nuscenes directory does not exists""\n        assert os.path.exists(self.dir_ann), ""The annotations directory does not exists""\n        assert os.path.exists(dir_out), ""Joints directory does not exists""\n\n        now = datetime.datetime.now()\n        now_time = now.strftime(""%Y%m%d-%H%M"")[2:]\n        self.path_joints = os.path.join(dir_out, \'joints-\' + dataset + \'-\' + now_time + \'.json\')\n        self.path_names = os.path.join(dir_out, \'names-\' + dataset + \'-\' + now_time + \'.json\')\n\n        self.nusc, self.scenes, self.split_train, self.split_val = factory(dataset, dir_nuscenes)\n\n    def run(self):\n        """"""\n        Prepare arrays for training\n        """"""\n        cnt_scenes = cnt_samples = cnt_sd = cnt_ann = 0\n        start = time.time()\n        for ii, scene in enumerate(self.scenes):\n            end_scene = time.time()\n            current_token = scene[\'first_sample_token\']\n            cnt_scenes += 1\n            time_left = str((end_scene - start_scene) / 60 * (len(self.scenes) - ii))[:4] if ii != 0 else ""NaN""\n\n            sys.stdout.write(\'\\r\' + \'Elaborating scene {}, remaining time {} minutes\'\n                             .format(cnt_scenes, time_left) + \'\\t\\n\')\n            start_scene = time.time()\n            if scene[\'name\'] in self.split_train:\n                phase = \'train\'\n            elif scene[\'name\'] in self.split_val:\n                phase = \'val\'\n            else:\n                print(""phase name not in training or validation split"")\n                continue\n\n            while not current_token == """":\n                sample_dic = self.nusc.get(\'sample\', current_token)\n                cnt_samples += 1\n\n                # Extract all the sample_data tokens for each sample\n                for cam in self.CAMERAS:\n                    sd_token = sample_dic[\'data\'][cam]\n                    cnt_sd += 1\n\n                    # Extract all the annotations of the person\n                    name, boxes_gt, boxes_3d, dds, kk = self.extract_from_token(sd_token)\n\n                    # Run IoU with pifpaf detections and save\n                    path_pif = os.path.join(self.dir_ann, name + \'.pifpaf.json\')\n                    exists = os.path.isfile(path_pif)\n\n                    if exists:\n                        with open(path_pif, \'r\') as file:\n                            annotations = json.load(file)\n                            boxes, keypoints = preprocess_pifpaf(annotations, im_size=(1600, 900))\n                    else:\n                        continue\n\n                    if keypoints:\n                        inputs = preprocess_monoloco(keypoints, kk).tolist()\n\n                        matches = get_iou_matches(boxes, boxes_gt, self.iou_min)\n                        for (idx, idx_gt) in matches:\n                            self.dic_jo[phase][\'kps\'].append(keypoints[idx])\n                            self.dic_jo[phase][\'X\'].append(inputs[idx])\n                            self.dic_jo[phase][\'Y\'].append([dds[idx_gt]])  # Trick to make it (nn,1)\n                            self.dic_jo[phase][\'names\'].append(name)  # One image name for each annotation\n                            self.dic_jo[phase][\'boxes_3d\'].append(boxes_3d[idx_gt])\n                            self.dic_jo[phase][\'K\'].append(kk)\n                            append_cluster(self.dic_jo, phase, inputs[idx], dds[idx_gt], keypoints[idx])\n                            cnt_ann += 1\n                            sys.stdout.write(\'\\r\' + \'Saved annotations {}\'.format(cnt_ann) + \'\\t\')\n\n                current_token = sample_dic[\'next\']\n\n        with open(os.path.join(self.path_joints), \'w\') as f:\n            json.dump(self.dic_jo, f)\n        with open(os.path.join(self.path_names), \'w\') as f:\n            json.dump(self.dic_names, f)\n        end = time.time()\n\n        print(""\\nSaved {} annotations for {} samples in {} scenes. Total time: {:.1f} minutes""\n              .format(cnt_ann, cnt_samples, cnt_scenes, (end-start)/60))\n        print(""\\nOutput files:\\n{}\\n{}\\n"".format(self.path_names, self.path_joints))\n\n    def extract_from_token(self, sd_token):\n\n        boxes_gt = []\n        dds = []\n        boxes_3d = []\n        path_im, boxes_obj, kk = self.nusc.get_sample_data(sd_token, box_vis_level=1)  # At least one corner\n        kk = kk.tolist()\n        name = os.path.basename(path_im)\n        for box_obj in boxes_obj:\n            if box_obj.name[:6] != \'animal\':\n                general_name = box_obj.name.split(\'.\')[0] + \'.\' + box_obj.name.split(\'.\')[1]\n            else:\n                general_name = \'animal\'\n            if general_name in select_categories(\'all\'):\n                box = project_3d(box_obj, kk)\n                dd = np.linalg.norm(box_obj.center)\n                boxes_gt.append(box)\n                dds.append(dd)\n                box_3d = box_obj.center.tolist() + box_obj.wlh.tolist()\n                boxes_3d.append(box_3d)\n                self.dic_names[name][\'boxes\'].append(box)\n                self.dic_names[name][\'dds\'].append(dd)\n                self.dic_names[name][\'K\'] = kk\n\n        return name, boxes_gt, boxes_3d, dds, kk\n\n\ndef factory(dataset, dir_nuscenes):\n    """"""Define dataset type and split training and validation""""""\n\n    assert dataset in [\'nuscenes\', \'nuscenes_mini\', \'nuscenes_teaser\']\n    if dataset == \'nuscenes_mini\':\n        version = \'v1.0-mini\'\n    else:\n        version = \'v1.0-trainval\'\n\n    nusc = NuScenes(version=version, dataroot=dir_nuscenes, verbose=True)\n    scenes = nusc.scene\n\n    if dataset == \'nuscenes_teaser\':\n        with open(""splits/nuscenes_teaser_scenes.txt"", ""r"") as file:\n            teaser_scenes = file.read().splitlines()\n        scenes = [scene for scene in scenes if scene[\'token\'] in teaser_scenes]\n        with open(""splits/split_nuscenes_teaser.json"", ""r"") as file:\n            dic_split = json.load(file)\n        split_train = [scene[\'name\'] for scene in scenes if scene[\'token\'] in dic_split[\'train\']]\n        split_val = [scene[\'name\'] for scene in scenes if scene[\'token\'] in dic_split[\'val\']]\n    else:\n        split_scenes = splits.create_splits_scenes()\n        split_train, split_val = split_scenes[\'train\'], split_scenes[\'val\']\n\n    return nusc, scenes, split_train, split_val\n'"
monoloco/prep/transforms.py,0,"b'\nimport numpy as np\n\n\nCOCO_KEYPOINTS = [\n    \'nose\',            # 1\n    \'left_eye\',        # 2\n    \'right_eye\',       # 3\n    \'left_ear\',        # 4\n    \'right_ear\',       # 5\n    \'left_shoulder\',   # 6\n    \'right_shoulder\',  # 7\n    \'left_elbow\',      # 8\n    \'right_elbow\',     # 9\n    \'left_wrist\',      # 10\n    \'right_wrist\',     # 11\n    \'left_hip\',        # 12\n    \'right_hip\',       # 13\n    \'left_knee\',       # 14\n    \'right_knee\',      # 15\n    \'left_ankle\',      # 16\n    \'right_ankle\',     # 17\n]\n\n\nHFLIP = {\n    \'nose\': \'nose\',\n    \'left_eye\': \'right_eye\',\n    \'right_eye\': \'left_eye\',\n    \'left_ear\': \'right_ear\',\n    \'right_ear\': \'left_ear\',\n    \'left_shoulder\': \'right_shoulder\',\n    \'right_shoulder\': \'left_shoulder\',\n    \'left_elbow\': \'right_elbow\',\n    \'right_elbow\': \'left_elbow\',\n    \'left_wrist\': \'right_wrist\',\n    \'right_wrist\': \'left_wrist\',\n    \'left_hip\': \'right_hip\',\n    \'right_hip\': \'left_hip\',\n    \'left_knee\': \'right_knee\',\n    \'right_knee\': \'left_knee\',\n    \'left_ankle\': \'right_ankle\',\n    \'right_ankle\': \'left_ankle\',\n}\n\n\ndef transform_keypoints(keypoints, mode):\n\n    assert mode == \'flip\', ""mode not recognized""\n    kps = np.array(keypoints)\n    dic_kps = {key: kps[:, :, idx] for idx, key in enumerate(COCO_KEYPOINTS)}\n    kps_hflip = np.array([dic_kps[value] for key, value in HFLIP.items()])\n    kps_hflip = np.transpose(kps_hflip, (1, 2, 0))\n    return kps_hflip.tolist()\n'"
monoloco/train/__init__.py,0,b'\nfrom .hyp_tuning import HypTuning\nfrom .trainer import Trainer\n'
monoloco/train/datasets.py,6,"b'\nimport json\nimport torch\n\nfrom torch.utils.data import Dataset\n\n\nclass KeypointsDataset(Dataset):\n    """"""\n    Dataloader fro nuscenes or kitti datasets\n    """"""\n\n    def __init__(self, joints, phase):\n        """"""\n        Load inputs and outputs from the pickles files from gt joints, mask joints or both\n        """"""\n        assert(phase in [\'train\', \'val\', \'test\'])\n\n        with open(joints, \'r\') as f:\n            dic_jo = json.load(f)\n\n        # Define input and output for normal training and inference\n        self.inputs_all = torch.tensor(dic_jo[phase][\'X\'])\n        self.outputs_all = torch.tensor(dic_jo[phase][\'Y\']).view(-1, 1)\n        self.names_all = dic_jo[phase][\'names\']\n        self.kps_all = torch.tensor(dic_jo[phase][\'kps\'])\n\n        # Extract annotations divided in clusters\n        self.dic_clst = dic_jo[phase][\'clst\']\n\n    def __len__(self):\n        """"""\n        :return: number of samples (m)\n        """"""\n        return self.inputs_all.shape[0]\n\n    def __getitem__(self, idx):\n        """"""\n        Reading the tensors when required. E.g. Retrieving one element or one batch at a time\n        :param idx: corresponding to m\n        """"""\n        inputs = self.inputs_all[idx, :]\n        outputs = self.outputs_all[idx]\n        names = self.names_all[idx]\n        kps = self.kps_all[idx, :]\n\n        return inputs, outputs, names, kps\n\n    def get_cluster_annotations(self, clst):\n        """"""Return normalized annotations corresponding to a certain cluster\n        """"""\n        inputs = torch.tensor(self.dic_clst[clst][\'X\'])\n        outputs = torch.tensor(self.dic_clst[clst][\'Y\']).float()\n        count = len(self.dic_clst[clst][\'Y\'])\n\n        return inputs, outputs, count\n'"
monoloco/train/hyp_tuning.py,1,"b'\nimport math\nimport os\nimport json\nimport time\nimport logging\nimport random\nimport datetime\n\nimport torch\nimport numpy as np\n\nfrom .trainer import Trainer\n\n\nclass HypTuning:\n\n    def __init__(self, joints, epochs, baseline, dropout, multiplier=1, r_seed=1):\n        """"""\n        Initialize directories, load the data and parameters for the training\n        """"""\n\n        # Initialize Directories\n        self.joints = joints\n        self.baseline = baseline\n        self.dropout = dropout\n        self.num_epochs = epochs\n        self.baseline = baseline\n        self.r_seed = r_seed\n        dir_out = os.path.join(\'data\', \'models\')\n        dir_logs = os.path.join(\'data\', \'logs\')\n        assert os.path.exists(dir_out), ""Output directory not found""\n        if not os.path.exists(dir_logs):\n            os.makedirs(dir_logs)\n\n        name_out = \'hyp-baseline-\' if baseline else \'hyp-monoloco-\'\n\n        self.path_log = os.path.join(dir_logs, name_out)\n        self.path_model = os.path.join(dir_out, name_out)\n\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n\n        # Initialize grid of parameters\n        random.seed(r_seed)\n        np.random.seed(r_seed)\n        self.sched_gamma_list = [0.8, 0.9, 1, 0.8, 0.9, 1] * multiplier\n        random.shuffle(self.sched_gamma_list)\n        self.sched_step = [10, 20, 30, 40, 50, 60] * multiplier\n        random.shuffle(self.sched_step)\n        self.bs_list = [64, 128, 256, 512, 1024, 2048] * multiplier\n        random.shuffle(self.bs_list)\n        self.hidden_list = [256, 256, 256, 256, 256, 256] * multiplier\n        random.shuffle(self.hidden_list)\n        self.n_stage_list = [3, 3, 3, 3, 3, 3] * multiplier\n        random.shuffle(self.n_stage_list)\n        # Learning rate\n        aa = math.log(0.001, 10)\n        bb = math.log(0.03, 10)\n        log_lr_list = np.random.uniform(aa, bb, int(6 * multiplier)).tolist()\n        self.lr_list = [10 ** xx for xx in log_lr_list]\n        # plt.hist(self.lr_list, bins=50)\n        # plt.show()\n\n    def train(self):\n        """"""Train multiple times using log-space random search""""""\n\n        best_acc_val = 20\n        dic_best = {}\n        dic_err_best = {}\n        start = time.time()\n        cnt = 0\n        for idx, lr in enumerate(self.lr_list):\n            bs = self.bs_list[idx]\n            sched_gamma = self.sched_gamma_list[idx]\n            sched_step = self.sched_step[idx]\n            hidden_size = self.hidden_list[idx]\n            n_stage = self.n_stage_list[idx]\n\n            training = Trainer(joints=self.joints, epochs=self.num_epochs,\n                               bs=bs, baseline=self.baseline, dropout=self.dropout, lr=lr, sched_step=sched_step,\n                               sched_gamma=sched_gamma, hidden_size=hidden_size, n_stage=n_stage,\n                               save=False, print_loss=False, r_seed=self.r_seed)\n\n            best_epoch = training.train()\n            dic_err, model = training.evaluate()\n            acc_val = dic_err[\'val\'][\'all\'][\'mean\']\n            cnt += 1\n            print(""Combination number: {}"".format(cnt))\n\n            if acc_val < best_acc_val:\n                dic_best[\'lr\'] = lr\n                dic_best[\'joints\'] = self.joints\n                dic_best[\'bs\'] = bs\n                dic_best[\'baseline\'] = self.baseline\n                dic_best[\'sched_gamma\'] = sched_gamma\n                dic_best[\'sched_step\'] = sched_step\n                dic_best[\'hidden_size\'] = hidden_size\n                dic_best[\'n_stage\'] = n_stage\n                dic_best[\'acc_val\'] = dic_err[\'val\'][\'all\'][\'mean\']\n                dic_best[\'best_epoch\'] = best_epoch\n                dic_best[\'random_seed\'] = self.r_seed\n                # dic_best[\'acc_test\'] = dic_err[\'test\'][\'all\'][\'mean\']\n\n                dic_err_best = dic_err\n                best_acc_val = acc_val\n                model_best = model\n\n        # Save model and log\n        now = datetime.datetime.now()\n        now_time = now.strftime(""%Y%m%d-%H%M"")[2:]\n        self.path_model = self.path_model + now_time + \'.pkl\'\n        torch.save(model_best.state_dict(), self.path_model)\n        with open(self.path_log + now_time, \'w\') as f:\n            json.dump(dic_best, f)\n        end = time.time()\n        print(\'\\n\\n\\n\')\n        self.logger.info("" Tried {} combinations"".format(cnt))\n        self.logger.info("" Total time for hyperparameters search: {:.2f} minutes"".format((end - start) / 60))\n        self.logger.info("" Best hyperparameters are:"")\n        for key, value in dic_best.items():\n            self.logger.info("" {}: {}"".format(key, value))\n\n        print()\n        self.logger.info(""Accuracy in each cluster:"")\n\n        for key in dic_err_best[\'val\']:\n            self.logger.info(""Val: error in cluster {} = {} "".format(key, dic_err_best[\'val\'][key][\'mean\']))\n        self.logger.info(""Final accuracy Val: {:.2f}"".format(dic_best[\'acc_val\']))\n        self.logger.info(""\\nSaved the model: {}"".format(self.path_model))\n'"
monoloco/train/trainer.py,15,"b'\n# pylint: disable=too-many-statements\n""""""\nTraining and evaluation of a neural network which predicts 3D localization and confidence intervals\ngiven 2d joints\n""""""\n\nimport copy\nimport os\nimport datetime\nimport logging\nfrom collections import defaultdict\nimport sys\nimport time\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import lr_scheduler\n\nfrom .datasets import KeypointsDataset\nfrom ..network import LaplacianLoss\nfrom ..network.process import unnormalize_bi\nfrom ..network.architectures import LinearModel\nfrom ..utils import set_logger\n\n\nclass Trainer:\n    def __init__(self, joints, epochs=100, bs=256, dropout=0.2, lr=0.002,\n                 sched_step=20, sched_gamma=1, hidden_size=256, n_stage=3, r_seed=1, n_samples=100,\n                 baseline=False, save=False, print_loss=False):\n        """"""\n        Initialize directories, load the data and parameters for the training\n        """"""\n\n        # Initialize directories and parameters\n        dir_out = os.path.join(\'data\', \'models\')\n        if not os.path.exists(dir_out):\n            warnings.warn(""Warning: output directory not found, the model will not be saved"")\n        dir_logs = os.path.join(\'data\', \'logs\')\n        if not os.path.exists(dir_logs):\n            warnings.warn(""Warning: default logs directory not found"")\n        assert os.path.exists(joints), ""Input file not found""\n\n        self.joints = joints\n        self.num_epochs = epochs\n        self.save = save\n        self.print_loss = print_loss\n        self.baseline = baseline\n        self.lr = lr\n        self.sched_step = sched_step\n        self.sched_gamma = sched_gamma\n        n_joints = 17\n        input_size = n_joints * 2\n        self.output_size = 2\n        self.clusters = [\'10\', \'20\', \'30\', \'>30\']\n        self.hidden_size = hidden_size\n        self.n_stage = n_stage\n        self.dir_out = dir_out\n        self.n_samples = n_samples\n        self.r_seed = r_seed\n\n        # Loss functions and output names\n        now = datetime.datetime.now()\n        now_time = now.strftime(""%Y%m%d-%H%M"")[2:]\n\n        if baseline:\n            name_out = \'baseline-\' + now_time\n            self.criterion = nn.L1Loss().cuda()\n            self.output_size = 1\n        else:\n            name_out = \'monoloco-\' + now_time\n            self.criterion = LaplacianLoss().cuda()\n            self.output_size = 2\n        self.criterion_eval = nn.L1Loss().cuda()\n\n        if self.save:\n            self.path_model = os.path.join(dir_out, name_out + \'.pkl\')\n            self.logger = set_logger(os.path.join(dir_logs, name_out))\n            self.logger.info(""Training arguments: \\nepochs: {} \\nbatch_size: {} \\ndropout: {}""\n                             ""\\nbaseline: {} \\nlearning rate: {} \\nscheduler step: {} \\nscheduler gamma: {}  ""\n                             ""\\ninput_size: {} \\nhidden_size: {} \\nn_stages: {} \\nr_seed: {}""\n                             ""\\ninput_file: {}""\n                             .format(epochs, bs, dropout, baseline, lr, sched_step, sched_gamma, input_size,\n                                     hidden_size, n_stage, r_seed, self.joints))\n        else:\n            logging.basicConfig(level=logging.INFO)\n            self.logger = logging.getLogger(__name__)\n\n        # Select the device and load the data\n        use_cuda = torch.cuda.is_available()\n        self.device = torch.device(""cuda"" if use_cuda else ""cpu"")\n        print(\'Device: \', self.device)\n\n        # Set the seed for random initialization\n        torch.manual_seed(r_seed)\n        if use_cuda:\n            torch.cuda.manual_seed(r_seed)\n\n        # Dataloader\n        self.dataloaders = {phase: DataLoader(KeypointsDataset(self.joints, phase=phase),\n                                              batch_size=bs, shuffle=True) for phase in [\'train\', \'val\']}\n\n        self.dataset_sizes = {phase: len(KeypointsDataset(self.joints, phase=phase))\n                              for phase in [\'train\', \'val\', \'test\']}\n\n        # Define the model\n        self.logger.info(\'Sizes of the dataset: {}\'.format(self.dataset_sizes))\n        print("">>> creating model"")\n        self.model = LinearModel(input_size=input_size, output_size=self.output_size, linear_size=hidden_size,\n                                 p_dropout=dropout, num_stage=self.n_stage)\n        self.model.to(self.device)\n        print("">>> total params: {:.2f}M"".format(sum(p.numel() for p in self.model.parameters()) / 1000000.0))\n\n        # Optimizer and scheduler\n        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=lr)\n        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=self.sched_step, gamma=self.sched_gamma)\n\n    def train(self):\n\n        # Initialize the variable containing model weights\n        since = time.time()\n        best_model_wts = copy.deepcopy(self.model.state_dict())\n        best_acc = 1e6\n        best_epoch = 0\n        epoch_losses_tr = epoch_losses_val = epoch_norms = epoch_sis = []\n\n        for epoch in range(self.num_epochs):\n\n            # Each epoch has a training and validation phase\n            for phase in [\'train\', \'val\']:\n                if phase == \'train\':\n                    self.scheduler.step()\n                    self.model.train()  # Set model to training mode\n                else:\n                    self.model.eval()  # Set model to evaluate mode\n\n                running_loss_tr = running_loss_eval = norm_tr = bi_tr = 0.0\n\n                # Iterate over data.\n                for inputs, labels, _, _ in self.dataloaders[phase]:\n                    inputs = inputs.to(self.device)\n                    labels = labels.to(self.device)\n\n                    # zero the parameter gradients\n                    self.optimizer.zero_grad()\n\n                    # forward\n                    # track history if only in train\n                    with torch.set_grad_enabled(phase == \'train\'):\n                        outputs = self.model(inputs)\n\n                        outputs_eval = outputs[:, 0:1] if self.output_size == 2 else outputs\n\n                        loss = self.criterion(outputs, labels)\n                        loss_eval = self.criterion_eval(outputs_eval, labels)  # L1 loss to evaluation\n\n                        # backward + optimize only if in training phase\n                        if phase == \'train\':\n                            loss.backward()\n                            self.optimizer.step()\n\n                    # statistics\n                    running_loss_tr += loss.item() * inputs.size(0)\n                    running_loss_eval += loss_eval.item() * inputs.size(0)\n\n                epoch_loss = running_loss_tr / self.dataset_sizes[phase]\n                epoch_acc = running_loss_eval / self.dataset_sizes[phase]  # Average distance in meters\n                epoch_norm = float(norm_tr / self.dataset_sizes[phase])\n                epoch_si = float(bi_tr / self.dataset_sizes[phase])\n                if phase == \'train\':\n                    epoch_losses_tr.append(epoch_loss)\n                    epoch_norms.append(epoch_norm)\n                    epoch_sis.append(epoch_si)\n                else:\n                    epoch_losses_val.append(epoch_acc)\n\n                if epoch % 5 == 1:\n                    sys.stdout.write(\'\\r\' + \'Epoch: {:.0f}   Training Loss: {:.3f}   Val Loss {:.3f}\'\n                                     .format(epoch, epoch_losses_tr[-1], epoch_losses_val[-1]) + \'\\t\')\n\n                # deep copy the model\n                if phase == \'val\' and epoch_acc < best_acc:\n                    best_acc = epoch_acc\n                    best_epoch = epoch\n                    best_model_wts = copy.deepcopy(self.model.state_dict())\n\n        time_elapsed = time.time() - since\n        print(\'\\n\\n\' + \'-\'*120)\n        self.logger.info(\'Training:\\nTraining complete in {:.0f}m {:.0f}s\'\n                         .format(time_elapsed // 60, time_elapsed % 60))\n        self.logger.info(\'Best validation Accuracy: {:.3f}\'.format(best_acc))\n        self.logger.info(\'Saved weights of the model at epoch: {}\'.format(best_epoch))\n\n        if self.print_loss:\n            epoch_losses_val_scaled = [x - 4 for x in epoch_losses_val]  # to compare with L1 Loss\n            plt.plot(epoch_losses_tr[10:], label=\'Training Loss\')\n            plt.plot(epoch_losses_val_scaled[10:], label=\'Validation Loss\')\n            plt.legend()\n            plt.show()\n\n        # load best model weights\n        self.model.load_state_dict(best_model_wts)\n\n        return best_epoch\n\n    def evaluate(self, load=False, model=None, debug=False):\n\n        # To load a model instead of using the trained one\n        if load:\n            self.model.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n\n        # Average distance on training and test set after unnormalizing\n        self.model.eval()\n        dic_err = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))  # initialized to zero\n        phase = \'val\'\n        batch_size = 5000\n        dataset = KeypointsDataset(self.joints, phase=phase)\n        size_eval = len(dataset)\n        start = 0\n        with torch.no_grad():\n            for end in range(batch_size, size_eval+batch_size, batch_size):\n                end = end if end < size_eval else size_eval\n                inputs, labels, _, _ = dataset[start:end]\n                start = end\n                inputs = inputs.to(self.device)\n                labels = labels.to(self.device)\n\n                # Debug plot for input-output distributions\n                if debug:\n                    debug_plots(inputs, labels)\n                    sys.exit()\n\n                # Forward pass\n                outputs = self.model(inputs)\n                if not self.baseline:\n                    outputs = unnormalize_bi(outputs)\n\n                dic_err[phase][\'all\'] = self.compute_stats(outputs, labels, dic_err[phase][\'all\'], size_eval)\n\n            print(\'-\'*120)\n            self.logger.info(""Evaluation:\\nAverage distance on the {} set: {:.2f}""\n                             .format(phase, dic_err[phase][\'all\'][\'mean\']))\n\n            self.logger.info(""Aleatoric Uncertainty: {:.2f}, inside the interval: {:.1f}%\\n""\n                             .format(dic_err[phase][\'all\'][\'bi\'], dic_err[phase][\'all\'][\'conf_bi\']*100))\n\n            # Evaluate performances on different clusters and save statistics\n            for clst in self.clusters:\n                inputs, labels, size_eval = dataset.get_cluster_annotations(clst)\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n\n                # Forward pass on each cluster\n                outputs = self.model(inputs)\n                if not self.baseline:\n                    outputs = unnormalize_bi(outputs)\n\n                    dic_err[phase][clst] = self.compute_stats(outputs, labels, dic_err[phase][clst], size_eval)\n\n                self.logger.info(""{} error in cluster {} = {:.2f} for {} instances. ""\n                                 ""Aleatoric of {:.2f} with {:.1f}% inside the interval""\n                                 .format(phase, clst, dic_err[phase][clst][\'mean\'], size_eval,\n                                         dic_err[phase][clst][\'bi\'], dic_err[phase][clst][\'conf_bi\'] * 100))\n\n        # Save the model and the results\n        if self.save and not load:\n            torch.save(self.model.state_dict(), self.path_model)\n            print(\'-\'*120)\n            self.logger.info(""\\nmodel saved: {} \\n"".format(self.path_model))\n        else:\n            self.logger.info(""\\nmodel not saved\\n"")\n\n        return dic_err, self.model\n\n    def compute_stats(self, outputs, labels_orig, dic_err, size_eval):\n        """"""Compute mean, bi and max of torch tensors""""""\n\n        labels = labels_orig.view(-1, )\n        mean_mu = float(self.criterion_eval(outputs[:, 0], labels).item())\n        max_mu = float(torch.max(torch.abs((outputs[:, 0] - labels))).item())\n\n        if self.baseline:\n            return (mean_mu, max_mu), (0, 0, 0)\n\n        mean_bi = torch.mean(outputs[:, 1]).item()\n\n        low_bound_bi = labels >= (outputs[:, 0] - outputs[:, 1])\n        up_bound_bi = labels <= (outputs[:, 0] + outputs[:, 1])\n        bools_bi = low_bound_bi & up_bound_bi\n        conf_bi = float(torch.sum(bools_bi)) / float(bools_bi.shape[0])\n\n        dic_err[\'mean\'] += mean_mu * (outputs.size(0) / size_eval)\n        dic_err[\'bi\'] += mean_bi * (outputs.size(0) / size_eval)\n        dic_err[\'count\'] += (outputs.size(0) / size_eval)\n        dic_err[\'conf_bi\'] += conf_bi * (outputs.size(0) / size_eval)\n\n        return dic_err\n\n\ndef debug_plots(inputs, labels):\n    inputs_shoulder = inputs.cpu().numpy()[:, 5]\n    inputs_hip = inputs.cpu().numpy()[:, 11]\n    labels = labels.cpu().numpy()\n    heights = inputs_hip - inputs_shoulder\n    plt.figure(1)\n    plt.hist(heights, bins=\'auto\')\n    plt.show()\n    plt.figure(2)\n    plt.hist(labels, bins=\'auto\')\n    plt.show()\n'"
monoloco/utils/__init__.py,0,"b'\nfrom .iou import get_iou_matches, reorder_matches, get_iou_matrix\nfrom .misc import get_task_error, get_pixel_error, append_cluster, open_annotations\nfrom .kitti import check_conditions, get_category, split_training, parse_ground_truth, get_calibration\nfrom .camera import xyz_from_distance, get_keypoints, pixel_to_camera, project_3d, open_image\nfrom .logs import set_logger\nfrom ..utils.nuscenes import select_categories\n'"
monoloco/utils/camera.py,10,"b'\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\n\n\ndef pixel_to_camera(uv_tensor, kk, z_met):\n    """"""\n    Convert a tensor in pixel coordinate to absolute camera coordinates\n    It accepts lists or torch/numpy tensors of (m, 2) or (m, x, 2)\n    where x is the number of keypoints\n    """"""\n    if isinstance(uv_tensor, (list, np.ndarray)):\n        uv_tensor = torch.tensor(uv_tensor)\n    if isinstance(kk, list):\n        kk = torch.tensor(kk)\n    if uv_tensor.size()[-1] != 2:\n        uv_tensor = uv_tensor.permute(0, 2, 1)  # permute to have 2 as last dim to be padded\n        assert uv_tensor.size()[-1] == 2, ""Tensor size not recognized""\n    uv_padded = F.pad(uv_tensor, pad=(0, 1), mode=""constant"", value=1)  # pad only last-dim below with value 1\n\n    kk_1 = torch.inverse(kk)\n    xyz_met_norm = torch.matmul(uv_padded, kk_1.t())  # More general than torch.mm\n    xyz_met = xyz_met_norm * z_met\n\n    return xyz_met\n\n\ndef project_to_pixels(xyz, kk):\n    """"""Project a single point in space into the image""""""\n    xx, yy, zz = np.dot(kk, xyz)\n    uu = int(xx / zz)\n    vv = int(yy / zz)\n\n    return uu, vv\n\n\ndef project_3d(box_obj, kk):\n    """"""\n    Project a 3D bounding box into the image plane using the central corners\n    """"""\n    box_2d = []\n    # Obtain the 3d points of the box\n    xc, yc, zc = box_obj.center\n    ww, _, hh, = box_obj.wlh\n\n    # Points corresponding to a box at the z of the center\n    x1 = xc - ww/2\n    y1 = yc - hh/2  # Y axis directed below\n    x2 = xc + ww/2\n    y2 = yc + hh/2\n    xyz1 = np.array([x1, y1, zc])\n    xyz2 = np.array([x2, y2, zc])\n    corners_3d = np.array([xyz1, xyz2])\n\n    # Project them and convert into pixel coordinates\n    for xyz in corners_3d:\n        xx, yy, zz = np.dot(kk, xyz)\n        uu = xx / zz\n        vv = yy / zz\n        box_2d.append(uu)\n        box_2d.append(vv)\n\n    return box_2d\n\n\ndef get_keypoints(keypoints, mode):\n    """"""\n    Extract center, shoulder or hip points of a keypoint\n    Input --> list or torch/numpy tensor [(m, 3, 17) or (3, 17)]\n    Output --> torch.tensor [(m, 2)]\n    """"""\n    if isinstance(keypoints, (list, np.ndarray)):\n        keypoints = torch.tensor(keypoints)\n    if len(keypoints.size()) == 2:  # add batch dim\n        keypoints = keypoints.unsqueeze(0)\n    assert len(keypoints.size()) == 3 and keypoints.size()[1] == 3, ""tensor dimensions not recognized""\n    assert mode in [\'center\', \'bottom\', \'head\', \'shoulder\', \'hip\', \'ankle\']\n\n    kps_in = keypoints[:, 0:2, :]  # (m, 2, 17)\n    if mode == \'center\':\n        kps_max, _ = kps_in.max(2)  # returns value, indices\n        kps_min, _ = kps_in.min(2)\n        kps_out = (kps_max - kps_min) / 2 + kps_min   # (m, 2) as keepdims is False\n\n    elif mode == \'bottom\':  # bottom center for kitti evaluation\n        kps_max, _ = kps_in.max(2)\n        kps_min, _ = kps_in.min(2)\n        kps_out_x = (kps_max[:, 0:1] - kps_min[:, 0:1]) / 2 + kps_min[:, 0:1]\n        kps_out_y = kps_max[:, 1:2]\n        kps_out = torch.cat((kps_out_x, kps_out_y), -1)\n\n    elif mode == \'head\':\n        kps_out = kps_in[:, :, 0:5].mean(2)\n\n    elif mode == \'shoulder\':\n        kps_out = kps_in[:, :, 5:7].mean(2)\n\n    elif mode == \'hip\':\n        kps_out = kps_in[:, :, 11:13].mean(2)\n\n    elif mode == \'ankle\':\n        kps_out = kps_in[:, :, 15:17].mean(2)\n\n    return kps_out  # (m, 2)\n\n\ndef transform_kp(kps, tr_mode):\n    """"""Apply different transformations to the keypoints based on the tr_mode""""""\n\n    assert tr_mode in (""None"", ""singularity"", ""upper"", ""lower"", ""horizontal"", ""vertical"", ""lateral"",\n                       \'shoulder\', \'knee\', \'upside\', \'falling\', \'random\')\n\n    uu_c, vv_c = get_keypoints(kps, mode=\'center\')\n\n    if tr_mode == ""None"":\n        return kps\n\n    if tr_mode == ""singularity"":\n        uus = [uu_c for uu in kps[0]]\n        vvs = [vv_c for vv in kps[1]]\n\n    elif tr_mode == ""vertical"":\n        uus = [uu_c for uu in kps[0]]\n        vvs = kps[1]\n\n    elif tr_mode == \'horizontal\':\n        uus = kps[0]\n        vvs = [vv_c for vv in kps[1]]\n\n    elif tr_mode == \'shoulder\':\n        uus = kps[0]\n        vvs = kps[1][:7] + [kps[1][6] for vv in kps[1][7:]]\n\n    elif tr_mode == \'knee\':\n        uus = kps[0]\n        vvs = [kps[1][14] for vv in kps[1][:13]] + kps[1][13:]\n\n    elif tr_mode == \'up\':\n        uus = kps[0]\n        vvs = [kp - 300 for kp in kps[1]]\n\n    elif tr_mode == \'falling\':\n        uus = [kps[0][16] - kp + kps[1][16] for kp in kps[1]]\n        vvs = [kps[1][16] - kp + kps[0][16] for kp in kps[0]]\n\n    elif tr_mode == \'random\':\n        uu_min = min(kps[0])\n        uu_max = max(kps[0])\n        vv_min = min(kps[1])\n        vv_max = max(kps[1])\n        np.random.seed(6)\n        uus = np.random.uniform(uu_min, uu_max, len(kps[0])).tolist()\n        vvs = np.random.uniform(vv_min, vv_max, len(kps[1])).tolist()\n\n    return [uus, vvs, kps[2], []]\n\n\ndef xyz_from_distance(distances, xy_centers):\n    """"""\n    From distances and normalized image coordinates (z=1), extract the real world position xyz\n    distances --> tensor (m,1) or (m) or float\n    xy_centers --> tensor(m,3) or (3)\n    """"""\n\n    if isinstance(distances, float):\n        distances = torch.tensor(distances).unsqueeze(0)\n    if len(distances.size()) == 1:\n        distances = distances.unsqueeze(1)\n    if len(xy_centers.size()) == 1:\n        xy_centers = xy_centers.unsqueeze(0)\n\n    assert xy_centers.size()[-1] == 3 and distances.size()[-1] == 1, ""Size of tensor not recognized""\n\n    return xy_centers * distances / torch.sqrt(1 + xy_centers[:, 0:1].pow(2) + xy_centers[:, 1:2].pow(2))\n\n\ndef open_image(path_image):\n    with open(path_image, \'rb\') as f:\n        pil_image = Image.open(f).convert(\'RGB\')\n        return pil_image\n'"
monoloco/utils/iou.py,0,"b'\nimport numpy as np\n\n\ndef calculate_iou(box1, box2):\n\n    # Calculate the (x1, y1, x2, y2) coordinates of the intersection of box1 and box2. Calculate its Area.\n    xi1 = max(box1[0], box2[0])\n    yi1 = max(box1[1], box2[1])\n    xi2 = min(box1[2], box2[2])\n    yi2 = min(box1[3], box2[3])\n    inter_area = max((xi2 - xi1), 0) * max((yi2 - yi1), 0)  # Max keeps into account not overlapping box\n\n    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    union_area = box1_area + box2_area - inter_area\n\n    # compute the IoU\n    iou = inter_area / union_area\n\n    return iou\n\n\ndef get_iou_matrix(boxes, boxes_gt):\n    """"""\n    Get IoU matrix between predicted and ground truth boxes\n    Dim: (boxes, boxes_gt)\n    """"""\n    iou_matrix = np.zeros((len(boxes), len(boxes_gt)))\n    for idx, box in enumerate(boxes):\n        for idx_gt, box_gt in enumerate(boxes_gt):\n            iou_matrix[idx, idx_gt] = calculate_iou(box, box_gt)\n    return iou_matrix\n\n\ndef get_iou_matches(boxes, boxes_gt, thresh):\n    """"""From 2 sets of boxes and a minimum threshold, compute the matching indices for IoU matchings""""""\n\n    iou_matrix = get_iou_matrix(boxes, boxes_gt)\n    if not iou_matrix.size:\n        return []\n\n    matches = []\n    iou_max = np.max(iou_matrix)\n    while iou_max > thresh:\n        # Extract the indeces of the max\n        args_max = np.unravel_index(np.argmax(iou_matrix, axis=None), iou_matrix.shape)\n        matches.append(args_max)\n        iou_matrix[args_max[0], :] = 0\n        iou_matrix[:, args_max[1]] = 0\n        iou_max = np.max(iou_matrix)\n    return matches\n\n\ndef reorder_matches(matches, boxes, mode=\'left_rigth\'):\n    """"""\n    Reorder a list of (idx, idx_gt) matches based on position of the detections in the image\n    ordered_boxes = (5, 6, 7, 0, 1, 4, 2, 4)\n    matches = [(0, x), (2,x), (4,x), (3,x), (5,x)]\n    Output --> [(5, x), (0, x), (3, x), (2, x), (5, x)]\n    """"""\n\n    assert mode == \'left_right\'\n\n    # Order the boxes based on the left-right position in the image and\n    ordered_boxes = np.argsort([box[0] for box in boxes])  # indices of boxes ordered from left to right\n    matches_left = [idx for (idx, _) in matches]\n\n    return [matches[matches_left.index(idx_boxes)] for idx_boxes in ordered_boxes if idx_boxes in matches_left]\n'"
monoloco/utils/kitti.py,0,"b'\nimport math\n\nimport numpy as np\n\n\ndef get_calibration(path_txt):\n    """"""Read calibration parameters from txt file:\n    For the left color camera we use P2 which is K * [I|t]\n\n    P = [fu, 0, x0, fu*t1-x0*t3\n         0, fv, y0, fv*t2-y0*t3\n         0, 0,  1,          t3]\n\n    check also http://ksimek.github.io/2013/08/13/intrinsic/\n\n    Simple case test:\n    xyz = np.array([2, 3, 30, 1]).reshape(4, 1)\n    xyz_2 = xyz[0:-1] + tt\n    uv_temp = np.dot(kk, xyz_2)\n    uv_1 = uv_temp / uv_temp[-1]\n    kk_1 = np.linalg.inv(kk)\n    xyz_temp2 = np.dot(kk_1, uv_1)\n    xyz_new_2 = xyz_temp2 * xyz_2[2]\n    xyz_fin_2 = xyz_new_2 - tt\n    """"""\n\n    with open(path_txt, ""r"") as ff:\n        file = ff.readlines()\n    p2_str = file[2].split()[1:]\n    p2_list = [float(xx) for xx in p2_str]\n    p2 = np.array(p2_list).reshape(3, 4)\n\n    p3_str = file[3].split()[1:]\n    p3_list = [float(xx) for xx in p3_str]\n    p3 = np.array(p3_list).reshape(3, 4)\n\n    kk, tt = get_translation(p2)\n    kk_right, tt_right = get_translation(p3)\n\n    return [kk, tt], [kk_right, tt_right]\n\n\ndef get_translation(pp):\n    """"""Separate intrinsic matrix from translation and convert in lists""""""\n\n    kk = pp[:, :-1]\n    f_x = kk[0, 0]\n    f_y = kk[1, 1]\n    x0, y0 = kk[2, 0:2]\n    aa, bb, t3 = pp[0:3, 3]\n    t1 = float((aa - x0*t3) / f_x)\n    t2 = float((bb - y0*t3) / f_y)\n    tt = [t1, t2, float(t3)]\n    return kk.tolist(), tt\n\n\ndef get_simplified_calibration(path_txt):\n\n    with open(path_txt, ""r"") as ff:\n        file = ff.readlines()\n\n    for line in file:\n        if line[:4] == \'K_02\':\n            kk_str = line[4:].split()[1:]\n            kk_list = [float(xx) for xx in kk_str]\n            kk = np.array(kk_list).reshape(3, 3).tolist()\n            return kk\n\n    raise ValueError(\'Matrix K_02 not found in the file\')\n\n\ndef check_conditions(line, category, method, thresh=0.3):\n    """"""Check conditions of our or m3d txt file""""""\n\n    check = False\n    assert category in [\'pedestrian\', \'cyclist\', \'all\']\n\n    if method == \'gt\':\n        if category == \'all\':\n            categories_gt = [\'Pedestrian\', \'Person_sitting\', \'Cyclist\']\n        else:\n            categories_gt = [category.upper()[0] + category[1:]]  # Upper case names\n        if line.split()[0] in categories_gt:\n            check = True\n\n    elif method in (\'m3d\', \'3dop\'):\n        conf = float(line[15])\n        if line[0] == category and conf >= thresh:\n            check = True\n\n    elif method == \'monodepth\':\n        check = True\n\n    else:\n        zz = float(line[13])\n        conf = float(line[15])\n        if conf >= thresh and 0.5 < zz < 70:\n            check = True\n\n    return check\n\n\ndef get_category(box, trunc, occ):\n\n    hh = box[3] - box[1]\n    if hh >= 40 and trunc <= 0.15 and occ <= 0:\n        cat = \'easy\'\n    elif trunc <= 0.3 and occ <= 1 and hh >= 25:\n        cat = \'moderate\'\n    elif trunc <= 0.5 and occ <= 2 and hh >= 25:\n        cat = \'hard\'\n    else:\n        cat = \'excluded\'\n    return cat\n\n\ndef split_training(names_gt, path_train, path_val):\n    """"""Split training and validation images""""""\n    set_gt = set(names_gt)\n    set_train = set()\n    set_val = set()\n\n    with open(path_train, ""r"") as f_train:\n        for line in f_train:\n            set_train.add(line[:-1] + \'.txt\')\n    with open(path_val, ""r"") as f_val:\n        for line in f_val:\n            set_val.add(line[:-1] + \'.txt\')\n\n    set_train = tuple(set_gt.intersection(set_train))\n    set_val = tuple(set_gt.intersection(set_val))\n    assert set_train and set_val, ""No validation or training annotations""\n    return set_train, set_val\n\n\ndef parse_ground_truth(path_gt, category):\n    """"""Parse KITTI ground truth files""""""\n    boxes_gt = []\n    dds_gt = []\n    zzs_gt = []\n    truncs_gt = []  # Float from 0 to 1\n    occs_gt = []  # Either 0,1,2,3 fully visible, partly occluded, largely occluded, unknown\n    boxes_3d = []\n\n    with open(path_gt, ""r"") as f_gt:\n        for line_gt in f_gt:\n            if check_conditions(line_gt, category, method=\'gt\'):\n                truncs_gt.append(float(line_gt.split()[1]))\n                occs_gt.append(int(line_gt.split()[2]))\n                boxes_gt.append([float(x) for x in line_gt.split()[4:8]])\n                loc_gt = [float(x) for x in line_gt.split()[11:14]]\n                wlh = [float(x) for x in line_gt.split()[8:11]]\n                boxes_3d.append(loc_gt + wlh)\n                zzs_gt.append(loc_gt[2])\n                dds_gt.append(math.sqrt(loc_gt[0] ** 2 + loc_gt[1] ** 2 + loc_gt[2] ** 2))\n\n    return boxes_gt, boxes_3d, dds_gt, zzs_gt, truncs_gt, occs_gt\n'"
monoloco/utils/logs.py,0,"b'\nimport logging\n\n\ndef set_logger(log_path):\n    """"""Set the logger to log info in terminal and file `log_path`.\n    ```\n    logging.info(""Starting training..."")\n    ```\n    Args:\n        log_path: (string) where to log\n    """"""\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n\n    # Logging to a file\n    file_handler = logging.FileHandler(log_path)\n    file_handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n    logger.addHandler(file_handler)\n\n    # Logging to console\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(logging.Formatter(\'%(message)s\'))\n    logger.addHandler(stream_handler)\n\n    return logger\n'"
monoloco/utils/misc.py,0,"b'import json\n\ndef append_cluster(dic_jo, phase, xx, dd, kps):\n    """"""Append the annotation based on its distance""""""\n\n    if dd <= 10:\n        dic_jo[phase][\'clst\'][\'10\'][\'kps\'].append(kps)\n        dic_jo[phase][\'clst\'][\'10\'][\'X\'].append(xx)\n        dic_jo[phase][\'clst\'][\'10\'][\'Y\'].append([dd])\n\n    elif dd <= 20:\n        dic_jo[phase][\'clst\'][\'20\'][\'kps\'].append(kps)\n        dic_jo[phase][\'clst\'][\'20\'][\'X\'].append(xx)\n        dic_jo[phase][\'clst\'][\'20\'][\'Y\'].append([dd])\n\n    elif dd <= 30:\n        dic_jo[phase][\'clst\'][\'30\'][\'kps\'].append(kps)\n        dic_jo[phase][\'clst\'][\'30\'][\'X\'].append(xx)\n        dic_jo[phase][\'clst\'][\'30\'][\'Y\'].append([dd])\n\n    else:\n        dic_jo[phase][\'clst\'][\'>30\'][\'kps\'].append(kps)\n        dic_jo[phase][\'clst\'][\'>30\'][\'X\'].append(xx)\n        dic_jo[phase][\'clst\'][\'>30\'][\'Y\'].append([dd])\n\n\ndef get_task_error(dd):\n    """"""Get target error not knowing the gender, modeled through a Gaussian Mixure model""""""\n    mm = 0.046\n    return dd * mm\n\n\ndef get_pixel_error(zz_gt):\n    """"""calculate error in stereo distance due to 1 pixel mismatch (function of depth)""""""\n\n    disp = 0.54 * 721 / zz_gt\n    error = abs(zz_gt - 0.54 * 721 / (disp - 1))\n    return error\n\n\ndef open_annotations(path_ann):\n    try:\n        with open(path_ann, \'r\') as f:\n            annotations = json.load(f)\n    except FileNotFoundError:\n        annotations = []\n    return annotations\n'"
monoloco/utils/nuscenes.py,0,"b'\nimport random\nimport json\nimport os\n\nimport numpy as np\n\n\ndef get_unique_tokens(list_fin):\n    """"""\n    list of json files --> list of unique scene tokens\n    """"""\n    list_token_scene = []\n\n    # Open one json file at a time\n    for name_fin in list_fin:\n        with open(name_fin, \'r\') as f:\n            dict_fin = json.load(f)\n\n        # Check if the token scene is already in the list and if not add it\n        if dict_fin[\'token_scene\'] not in list_token_scene:\n            list_token_scene.append(dict_fin[\'token_scene\'])\n\n    return list_token_scene\n\n\ndef split_scenes(list_token_scene, train, val, dir_main, save=False, load=True):\n    """"""\n    Split the list according tr, val percentages (test percentage is a consequence) after shuffling the order\n    """"""\n\n    path_split = os.path.join(dir_main, \'scenes\', \'split_scenes.json\')\n\n    if save:\n        random.seed(1)\n        random.shuffle(list_token_scene)  # it shuffles in place\n        n_scenes = len(list_token_scene)\n        n_train = round(n_scenes * train / 100)\n        n_val = round(n_scenes * val / 100)\n        list_train = list_token_scene[0: n_train]\n        list_val = list_token_scene[n_train: n_train + n_val]\n        list_test = list_token_scene[n_train + n_val:]\n\n        dic_split = {\'train\': list_train, \'val\': list_val, \'test\': list_test}\n        with open(path_split, \'w\') as f:\n            json.dump(dic_split, f)\n\n    if load:\n        with open(path_split, \'r\') as f:\n            dic_split = json.load(f)\n\n    return dic_split\n\n\ndef select_categories(cat):\n    """"""\n    Choose the categories to extract annotations from\n    """"""\n    assert cat in [\'person\', \'all\', \'car\', \'cyclist\']\n\n    if cat == \'person\':\n        categories = [\'human.pedestrian\']\n    elif cat == \'all\':\n        categories = [\'human.pedestrian\', \'vehicle.bicycle\', \'vehicle.motorcycle\']\n    elif cat == \'cyclist\':\n        categories = [\'vehicle.bicycle\']\n    elif cat == \'car\':\n        categories = [\'vehicle\']\n    return categories\n\n\ndef update_with_tokens(dict_gt, nusc, token_sd):\n    """"""\n    Update with tokens corresponding to the token_sd\n    """"""\n\n    table_sample_data = nusc.get(\'sample_data\', token_sd)  # Extract the whole record to get the sample token\n    token_sample = table_sample_data[\'sample_token\']  # Extract the sample_token from the table\n    table_sample = nusc.get(\'sample\', token_sample)  # Get the record of the sample\n    token_scene = table_sample[\'scene_token\']\n    dict_gt[\'token_sample_data\'] = token_sd\n    dict_gt[\'token_sample\'] = token_sample\n    dict_gt[\'token_scene\'] = token_scene\n    return dict_gt\n\n\ndef update_with_box(dict_gt, box):\n\n    bbox = np.zeros(7, )\n    flag_child = False\n\n    # Save the 3D bbox\n    bbox[0:3] = box.center\n    bbox[3:6] = box.wlh\n    bbox[6] = box.orientation.degrees\n    dict_gt[\'boxes\'].append(bbox.tolist()) # Save as list to be serializable by a json file\n    if box.name == \'human.pedestrian.child\':\n        flag_child = True\n\n    return dict_gt, flag_child\n'"
monoloco/visuals/__init__.py,0,"b'\nfrom .printer import Printer\nfrom .figures import show_results, show_spread, show_task_error\n'"
monoloco/visuals/figures.py,0,"b'# pylint: disable=R0915\n\nimport math\nimport itertools\nimport os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\n\nfrom ..utils import get_task_error, get_pixel_error\n\n\ndef show_results(dic_stats, show=False, save=False, stereo=False):\n    """"""\n    Visualize error as function of the distance and compare it with target errors based on human height analyses\n    """"""\n\n    dir_out = \'docs\'\n    phase = \'test\'\n    x_min = 0\n    x_max = 38\n    y_min = 0\n    y_max = 4.7\n    xx = np.linspace(0, 60, 100)\n    excl_clusters = [\'all\', \'50\', \'>50\', \'easy\', \'moderate\', \'hard\']\n    clusters = tuple([clst for clst in dic_stats[phase][\'monoloco\'] if clst not in excl_clusters])\n    yy_gender = get_task_error(xx)\n\n    styles = printing_styles(stereo)\n    for idx_style, (key, style) in enumerate(styles.items()):\n        plt.figure(idx_style)\n        plt.grid(linewidth=0.2)\n        plt.xlim(x_min, x_max)\n        plt.ylim(y_min, y_max)\n        plt.xlabel(""Ground-truth distance [m]"")\n        plt.ylabel(""Average localization error [m]"")\n        for idx, method in enumerate(style[\'methods\']):\n            errs = [dic_stats[phase][method][clst][\'mean\'] for clst in clusters]\n            assert errs, ""method %s empty"" % method\n            xxs = get_distances(clusters)\n\n            plt.plot(xxs, errs, marker=style[\'mks\'][idx], markersize=style[\'mksizes\'][idx], linewidth=style[\'lws\'][idx],\n                     label=style[\'labels\'][idx], linestyle=style[\'lstyles\'][idx], color=style[\'colors\'][idx])\n        plt.plot(xx, yy_gender, \'--\', label=""Task error"", color=\'lightgreen\', linewidth=2.5)\n        if key == \'stereo\':\n            yy_stereo = get_pixel_error(xx)\n            plt.plot(xx, yy_stereo, linewidth=1.7, color=\'k\', label=\'Pixel error\')\n\n        plt.legend(loc=\'upper left\')\n        if save:\n            path_fig = os.path.join(dir_out, \'results_\' + key + \'.png\')\n            plt.savefig(path_fig)\n            print(""Figure of results "" + key + "" saved in {}"".format(path_fig))\n        if show:\n            plt.show()\n        plt.close()\n\n\ndef show_spread(dic_stats, show=False, save=False):\n    """"""Predicted confidence intervals and task error as a function of ground-truth distance""""""\n\n    phase = \'test\'\n    dir_out = \'docs\'\n    excl_clusters = [\'all\', \'50\', \'>50\', \'easy\', \'moderate\', \'hard\']\n    clusters = tuple([clst for clst in dic_stats[phase][\'our\'] if clst not in excl_clusters])\n\n    plt.figure(2)\n    fig, ax = plt.subplots(2, sharex=True)\n    plt.xlabel(""Distance [m]"")\n    plt.ylabel(""Aleatoric uncertainty [m]"")\n    ar = 0.5  # Change aspect ratio of ellipses\n    scale = 1.5  # Factor to scale ellipses\n    rec_c = 0  # Center of the rectangle\n    plots_line = True\n\n    bbs = np.array([dic_stats[phase][\'our\'][key][\'std_ale\'] for key in clusters])\n    xxs = get_distances(clusters)\n    yys = get_task_error(np.array(xxs))\n    ax[1].plot(xxs, bbs, marker=\'s\', color=\'b\', label=""Spread b"")\n    ax[1].plot(xxs, yys, \'--\', color=\'lightgreen\', label=""Task error"", linewidth=2.5)\n    yys_up = [rec_c + ar / 2 * scale * yy for yy in yys]\n    bbs_up = [rec_c + ar / 2 * scale * bb for bb in bbs]\n    yys_down = [rec_c - ar / 2 * scale * yy for yy in yys]\n    bbs_down = [rec_c - ar / 2 * scale * bb for bb in bbs]\n\n    if plots_line:\n        ax[0].plot(xxs, yys_up, \'--\', color=\'lightgreen\', markersize=5, linewidth=1.4)\n        ax[0].plot(xxs, yys_down, \'--\', color=\'lightgreen\', markersize=5, linewidth=1.4)\n        ax[0].plot(xxs, bbs_up, marker=\'s\', color=\'b\', markersize=5, linewidth=0.7)\n        ax[0].plot(xxs, bbs_down, marker=\'s\', color=\'b\', markersize=5, linewidth=0.7)\n\n    for idx, xx in enumerate(xxs):\n        te = Ellipse((xx, rec_c), width=yys[idx] * ar * scale, height=scale, angle=90, color=\'lightgreen\', fill=True)\n        bi = Ellipse((xx, rec_c), width=bbs[idx] * ar * scale, height=scale, angle=90, color=\'b\', linewidth=1.8,\n                     fill=False)\n\n        ax[0].add_patch(te)\n        ax[0].add_patch(bi)\n\n    fig.subplots_adjust(hspace=0.1)\n    plt.setp([aa.get_yticklabels() for aa in fig.axes[:-1]], visible=False)\n    plt.legend()\n    if save:\n        path_fig = os.path.join(dir_out, \'spread.png\')\n        plt.savefig(path_fig)\n        print(""Figure of confidence intervals saved in {}"".format(path_fig))\n    if show:\n        plt.show()\n    plt.close()\n\n\ndef show_task_error(show, save):\n    """"""Task error figure""""""\n    plt.figure(3)\n    dir_out = \'docs\'\n    xx = np.linspace(0.1, 50, 100)\n    mu_men = 178\n    mu_women = 165\n    mu_child_m = 164\n    mu_child_w = 156\n    mm_gmm, mm_male, mm_female = calculate_gmm()\n    mm_young_male = mm_male + (mu_men - mu_child_m) / mu_men\n    mm_young_female = mm_female + (mu_women - mu_child_w) / mu_women\n    yy_male = target_error(xx, mm_male)\n    yy_female = target_error(xx, mm_female)\n    yy_young_male = target_error(xx, mm_young_male)\n    yy_young_female = target_error(xx, mm_young_female)\n    yy_gender = target_error(xx, mm_gmm)\n    yy_stereo = get_pixel_error(xx)\n    plt.grid(linewidth=0.3)\n    plt.plot(xx, yy_young_male, linestyle=\'dotted\', linewidth=2.1, color=\'b\', label=\'Adult/young male\')\n    plt.plot(xx, yy_young_female, linestyle=\'dotted\', linewidth=2.1, color=\'darkorange\', label=\'Adult/young female\')\n    plt.plot(xx, yy_gender, \'--\', color=\'lightgreen\', linewidth=2.8, label=\'Generic adult (task error)\')\n    plt.plot(xx, yy_female, \'-.\', linewidth=1.7, color=\'darkorange\', label=\'Adult female\')\n    plt.plot(xx, yy_male, \'-.\', linewidth=1.7, color=\'b\', label=\'Adult male\')\n    plt.plot(xx, yy_stereo, linewidth=1.7, color=\'k\', label=\'Pixel error\')\n    plt.xlim(np.min(xx), np.max(xx))\n    plt.xlabel(""Ground-truth distance from the camera $d_{gt}$ [m]"")\n    plt.ylabel(""Localization error $\\hat{e}$  due to human height variation [m]"")  # pylint: disable=W1401\n    plt.legend(loc=(0.01, 0.55))  # Location from 0 to 1 from lower left\n    if save:\n        path_fig = os.path.join(dir_out, \'task_error.png\')\n        plt.savefig(path_fig)\n        print(""Figure of task error saved in {}"".format(path_fig))\n    if show:\n        plt.show()\n    plt.close()\n\n\ndef show_method(save):\n    """""" method figure""""""\n    dir_out = \'docs\'\n    std_1 = 0.75\n    fig = plt.figure(1)\n    ax = fig.add_subplot(1, 1, 1)\n    ell_3 = Ellipse((0, 2), width=std_1 * 2, height=0.3, angle=-90, color=\'b\', fill=False, linewidth=2.5)\n    ell_4 = Ellipse((0, 2), width=std_1 * 3, height=0.3, angle=-90, color=\'r\', fill=False,\n                    linestyle=\'dashed\', linewidth=2.5)\n    ax.add_patch(ell_4)\n    ax.add_patch(ell_3)\n    plt.plot(0, 2, marker=\'o\', color=\'skyblue\', markersize=9)\n    plt.plot([0, 3], [0, 4], \'k--\')\n    plt.plot([0, -3], [0, 4], \'k--\')\n    plt.xlim(-3, 3)\n    plt.ylim(0, 3.5)\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel(\'X [m]\')\n    plt.ylabel(\'Z [m]\')\n    if save:\n        path_fig = os.path.join(dir_out, \'output_method.png\')\n        plt.savefig(path_fig)\n        print(""Figure of method saved in {}"".format(path_fig))\n\n\ndef target_error(xx, mm):\n    return mm * xx\n\n\ndef calculate_gmm():\n    dist_gmm, dist_male, dist_female = height_distributions()\n    # get_percentile(dist_gmm)\n    mu_gmm = np.mean(dist_gmm)\n    mm_gmm = np.mean(np.abs(1 - mu_gmm / dist_gmm))\n    mm_male = np.mean(np.abs(1 - np.mean(dist_male) / dist_male))\n    mm_female = np.mean(np.abs(1 - np.mean(dist_female) / dist_female))\n\n    print(""Mean of GMM distribution: {:.4f}"".format(mu_gmm))\n    print(""coefficient for gmm: {:.4f}"".format(mm_gmm))\n    print(""coefficient for men: {:.4f}"".format(mm_male))\n    print(""coefficient for women: {:.4f}"".format(mm_female))\n    return mm_gmm, mm_male, mm_female\n\n\ndef get_confidence(xx, zz, std):\n    theta = math.atan2(zz, xx)\n\n    delta_x = std * math.cos(theta)\n    delta_z = std * math.sin(theta)\n    return (xx - delta_x, xx + delta_x), (zz - delta_z, zz + delta_z)\n\n\ndef get_distances(clusters):\n    """"""Extract distances as intermediate values between 2 clusters""""""\n\n    clusters_ext = list(clusters)\n    clusters_ext.insert(0, str(0))\n    distances = []\n    for idx, _ in enumerate(clusters_ext[:-1]):\n        clst_0 = float(clusters_ext[idx])\n        clst_1 = float(clusters_ext[idx + 1])\n        distances.append((clst_1 - clst_0) / 2 + clst_0)\n    return tuple(distances)\n\n\ndef get_confidence_points(confidences, distances, errors):\n    confidence_points = []\n    distance_points = []\n    for idx, dd in enumerate(distances):\n        conf_perc = confidences[idx]\n        confidence_points.append(errors[idx] + conf_perc)\n        confidence_points.append(errors[idx] - conf_perc)\n        distance_points.append(dd)\n        distance_points.append(dd)\n\n    return distance_points, confidence_points\n\n\ndef height_distributions():\n    mu_men = 178\n    std_men = 7\n    mu_women = 165\n    std_women = 7\n    dist_men = np.random.normal(mu_men, std_men, int(1e7))\n    dist_women = np.random.normal(mu_women, std_women, int(1e7))\n\n    dist_gmm = np.concatenate((dist_men, dist_women))\n    return dist_gmm, dist_men, dist_women\n\n\ndef expandgrid(*itrs):\n    mm = 0\n    combinations = list(itertools.product(*itrs))\n\n    for h_i, h_gt in combinations:\n        mm += abs(float(1 - h_i / h_gt))\n\n    mm /= len(combinations)\n\n    return combinations\n\n\ndef plot_dist(dist_gmm, dist_men, dist_women):\n    try:\n        import seaborn as sns  # pylint: disable=C0415\n        sns.distplot(dist_men, hist=False, rug=False, label=""Men"")\n        sns.distplot(dist_women, hist=False, rug=False, label=""Women"")\n        sns.distplot(dist_gmm, hist=False, rug=False, label=""GMM"")\n        plt.xlabel(""X [cm]"")\n        plt.ylabel(""Height distributions of men and women"")\n        plt.legend()\n        plt.show()\n        plt.close()\n    except ImportError:\n        print(""Import Seaborn first"")\n\n\ndef get_percentile(dist_gmm):\n    dd_gt = 1000\n    mu_gmm = np.mean(dist_gmm)\n    dist_d = dd_gt * mu_gmm / dist_gmm\n    perc_d, _ = np.nanpercentile(dist_d, [18.5, 81.5])  # Laplace bi => 63%\n    perc_d2, _ = np.nanpercentile(dist_d, [23, 77])\n    mu_d = np.mean(dist_d)\n    # mm_bi = (mu_d - perc_d) / mu_d\n    # mm_test = (mu_d - perc_d2) / mu_d\n    # mad_d = np.mean(np.abs(dist_d - mu_d))\n\n\ndef printing_styles(stereo):\n    style = {\'mono\': {""labels"": [\'Mono3D\', \'Geometric Baseline\', \'MonoDepth\', \'Our MonoLoco\', \'3DOP (stereo)\'],\n                      ""methods"": [\'m3d_merged\', \'geometric_merged\', \'monodepth_merged\', \'monoloco_merged\',\n                                  \'3dop_merged\'],\n                      ""mks"": [\'*\', \'^\', \'p\', \'s\', \'o\'],\n                      ""mksizes"": [6, 6, 6, 6, 6], ""lws"": [1.5, 1.5, 1.5, 2.2, 1.6],\n                      ""colors"": [\'r\', \'deepskyblue\', \'grey\', \'b\', \'darkorange\'],\n                      ""lstyles"": [\'solid\', \'solid\', \'solid\', \'solid\', \'dashdot\']}}\n    if stereo:\n        style[\'stereo\'] = {""labels"": [\'3DOP\', \'Pose Baseline\', \'ReiD Baseline\', \'Our MonoLoco (monocular)\',\n                                      \'Our Stereo Baseline\'],\n                           ""methods"": [\'3dop_merged\', \'pose_merged\', \'reid_merged\', \'monoloco_merged\',\n                                       \'ml_stereo_merged\'],\n                           ""mks"": [\'o\', \'^\', \'p\', \'s\', \'s\'],\n                           ""mksizes"": [6, 6, 6, 4, 6], ""lws"": [1.5, 1.5, 1.5, 1.2, 1.5],\n                           ""colors"": [\'darkorange\', \'lightblue\', \'red\', \'b\', \'b\'],\n                           ""lstyles"": [\'solid\', \'solid\', \'solid\', \'dashed\', \'solid\']}\n\n    return style\n'"
monoloco/visuals/printer.py,0,"b'\nimport math\nfrom collections import OrderedDict\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib.patches import Ellipse, Circle, Rectangle\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfrom ..utils import pixel_to_camera, get_task_error\n\n\nclass Printer:\n    """"""\n    Print results on images: birds eye view and computed distance\n    """"""\n    FONTSIZE_BV = 16\n    FONTSIZE = 18\n    TEXTCOLOR = \'darkorange\'\n    COLOR_KPS = \'yellow\'\n\n    def __init__(self, image, output_path, kk, output_types, epistemic=False, z_max=30, fig_width=10):\n\n        self.im = image\n        self.kk = kk\n        self.output_types = output_types\n        self.epistemic = epistemic\n        self.z_max = z_max  # To include ellipses in the image\n        self.y_scale = 1\n        self.width = self.im.size[0]\n        self.height = self.im.size[1]\n        self.fig_width = fig_width\n\n        # Define the output dir\n        self.output_path = output_path\n        self.cmap = cm.get_cmap(\'jet\')\n        self.extensions = []\n\n        # Define variables of the class to change for every image\n        self.mpl_im0 = self.stds_ale = self.stds_epi = self.xx_gt = self.zz_gt = self.xx_pred = self.zz_pred =\\\n            self.dds_real = self.uv_centers = self.uv_shoulders = self.uv_kps = self.boxes = self.boxes_gt = \\\n            self.uv_camera = self.radius = None\n\n    def _process_results(self, dic_ann):\n        # Include the vectors inside the interval given by z_max\n        self.stds_ale = dic_ann[\'stds_ale\']\n        self.stds_epi = dic_ann[\'stds_epi\']\n        self.xx_gt = [xx[0] for xx in dic_ann[\'xyz_real\']]\n        self.zz_gt = [xx[2] if xx[2] < self.z_max - self.stds_epi[idx] else 0\n                      for idx, xx in enumerate(dic_ann[\'xyz_real\'])]\n        self.xx_pred = [xx[0] for xx in dic_ann[\'xyz_pred\']]\n        self.zz_pred = [xx[2] if xx[2] < self.z_max - self.stds_epi[idx] else 0\n                        for idx, xx in enumerate(dic_ann[\'xyz_pred\'])]\n        self.dds_real = dic_ann[\'dds_real\']\n        self.uv_shoulders = dic_ann[\'uv_shoulders\']\n        self.boxes = dic_ann[\'boxes\']\n        self.boxes_gt = dic_ann[\'boxes_gt\']\n\n        self.uv_camera = (int(self.im.size[0] / 2), self.im.size[1])\n        self.radius = 11 / 1600 * self.width\n\n    def factory_axes(self):\n        """"""Create axes for figures: front bird combined""""""\n        axes = []\n        figures = []\n\n        #  Initialize combined figure, resizing it for aesthetic proportions\n        if \'combined\' in self.output_types:\n            assert \'bird\' and \'front\' not in self.output_types, \\\n                ""combined figure cannot be print together with front or bird ones""\n\n            self.y_scale = self.width / (self.height * 1.8)  # Defined proportion\n            if self.y_scale < 0.95 or self.y_scale > 1.05:  # allows more variation without resizing\n                self.im = self.im.resize((self.width, round(self.height * self.y_scale)))\n            self.width = self.im.size[0]\n            self.height = self.im.size[1]\n            fig_width = self.fig_width + 0.6 * self.fig_width\n            fig_height = self.fig_width * self.height / self.width\n\n            # Distinguish between KITTI images and general images\n            fig_ar_1 = 1.7 if self.y_scale > 1.7 else 1.3\n            width_ratio = 1.9\n            self.extensions.append(\'.combined.png\')\n\n            fig, (ax1, ax0) = plt.subplots(1, 2, sharey=False, gridspec_kw={\'width_ratios\': [1, width_ratio]},\n                                           figsize=(fig_width, fig_height))\n            ax1.set_aspect(fig_ar_1)\n            fig.set_tight_layout(True)\n            fig.subplots_adjust(left=0.02, right=0.98, bottom=0, top=1, hspace=0, wspace=0.02)\n\n            figures.append(fig)\n            assert \'front\' not in self.output_types and \'bird\' not in self.output_types, \\\n                ""--combined arguments is not supported with other visualizations""\n\n        # Initialize front figure\n        elif \'front\' in self.output_types:\n            width = self.fig_width\n            height = self.fig_width * self.height / self.width\n            self.extensions.append("".front.png"")\n            plt.figure(0)\n            fig0, ax0 = plt.subplots(1, 1, figsize=(width, height))\n            fig0.set_tight_layout(True)\n            figures.append(fig0)\n\n        # Create front figure axis\n        if any(xx in self.output_types for xx in [\'front\', \'combined\']):\n            ax0 = self.set_axes(ax0, axis=0)\n\n            divider = make_axes_locatable(ax0)\n            cax = divider.append_axes(\'right\', size=\'3%\', pad=0.05)\n            bar_ticks = self.z_max // 5 + 1\n            norm = matplotlib.colors.Normalize(vmin=0, vmax=self.z_max)\n            scalar_mappable = plt.cm.ScalarMappable(cmap=self.cmap, norm=norm)\n            scalar_mappable.set_array([])\n            plt.colorbar(scalar_mappable, ticks=np.linspace(0, self.z_max, bar_ticks),\n                         boundaries=np.arange(- 0.05, self.z_max + 0.1, .1), cax=cax, label=\'Z [m]\')\n\n            axes.append(ax0)\n        if not axes:\n            axes.append(None)\n\n        # Initialize bird-eye-view figure\n        if \'bird\' in self.output_types:\n            self.extensions.append("".bird.png"")\n            fig1, ax1 = plt.subplots(1, 1)\n            fig1.set_tight_layout(True)\n            figures.append(fig1)\n        if any(xx in self.output_types for xx in [\'bird\', \'combined\']):\n            ax1 = self.set_axes(ax1, axis=1)  # Adding field of view\n            axes.append(ax1)\n        return figures, axes\n\n    def draw(self, figures, axes, dic_out, image, draw_text=True, legend=True, draw_box=False,\n             save=False, show=False):\n\n        # Process the annotation dictionary of monoloco\n        self._process_results(dic_out)\n\n        # Draw the front figure\n        num = 0\n        self.mpl_im0.set_data(image)\n        for idx, uv in enumerate(self.uv_shoulders):\n            if any(xx in self.output_types for xx in [\'front\', \'combined\']) and \\\n                 min(self.zz_pred[idx], self.zz_gt[idx]) > 0:\n\n                color = self.cmap((self.zz_pred[idx] % self.z_max) / self.z_max)\n                self.draw_circle(axes, uv, color)\n                if draw_box:\n                    self.draw_boxes(axes, idx, color)\n\n                if draw_text:\n                    self.draw_text_front(axes, uv, num)\n                    num += 1\n\n        # Draw the bird figure\n        num = 0\n        for idx, _ in enumerate(self.xx_pred):\n            if any(xx in self.output_types for xx in [\'bird\', \'combined\']) and self.zz_gt[idx] > 0:\n\n                # Draw ground truth and predicted ellipses\n                self.draw_ellipses(axes, idx)\n\n                # Draw bird eye view text\n                if draw_text:\n                    self.draw_text_bird(axes, idx, num)\n                    num += 1\n        # Add the legend\n        if legend:\n            draw_legend(axes)\n\n        # Draw, save or/and show the figures\n        for idx, fig in enumerate(figures):\n            fig.canvas.draw()\n            if save:\n                fig.savefig(self.output_path + self.extensions[idx], bbox_inches=\'tight\')\n            if show:\n                fig.show()\n\n    def draw_ellipses(self, axes, idx):\n        """"""draw uncertainty ellipses""""""\n        target = get_task_error(self.dds_real[idx])\n        angle_gt = get_angle(self.xx_gt[idx], self.zz_gt[idx])\n        ellipse_real = Ellipse((self.xx_gt[idx], self.zz_gt[idx]), width=target * 2, height=1,\n                               angle=angle_gt, color=\'lightgreen\', fill=True, label=""Task error"")\n        axes[1].add_patch(ellipse_real)\n        if abs(self.zz_gt[idx] - self.zz_pred[idx]) > 0.001:\n            axes[1].plot(self.xx_gt[idx], self.zz_gt[idx], \'kx\', label=""Ground truth"", markersize=3)\n\n        angle = get_angle(self.xx_pred[idx], self.zz_pred[idx])\n        ellipse_ale = Ellipse((self.xx_pred[idx], self.zz_pred[idx]), width=self.stds_ale[idx] * 2,\n                              height=1, angle=angle, color=\'b\', fill=False, label=""Aleatoric Uncertainty"",\n                              linewidth=1.3)\n        ellipse_var = Ellipse((self.xx_pred[idx], self.zz_pred[idx]), width=self.stds_epi[idx] * 2,\n                              height=1, angle=angle, color=\'r\', fill=False, label=""Uncertainty"",\n                              linewidth=1, linestyle=\'--\')\n\n        axes[1].add_patch(ellipse_ale)\n        if self.epistemic:\n            axes[1].add_patch(ellipse_var)\n\n        axes[1].plot(self.xx_pred[idx], self.zz_pred[idx], \'ro\', label=""Predicted"", markersize=3)\n\n    def draw_boxes(self, axes, idx, color):\n        ww_box = self.boxes[idx][2] - self.boxes[idx][0]\n        hh_box = (self.boxes[idx][3] - self.boxes[idx][1]) * self.y_scale\n        ww_box_gt = self.boxes_gt[idx][2] - self.boxes_gt[idx][0]\n        hh_box_gt = (self.boxes_gt[idx][3] - self.boxes_gt[idx][1]) * self.y_scale\n\n        rectangle = Rectangle((self.boxes[idx][0], self.boxes[idx][1] * self.y_scale),\n                              width=ww_box, height=hh_box, fill=False, color=color, linewidth=3)\n        rectangle_gt = Rectangle((self.boxes_gt[idx][0], self.boxes_gt[idx][1] * self.y_scale),\n                                 width=ww_box_gt, height=hh_box_gt, fill=False, color=\'g\', linewidth=2)\n        axes[0].add_patch(rectangle_gt)\n        axes[0].add_patch(rectangle)\n\n    def draw_text_front(self, axes, uv, num):\n        axes[0].text(uv[0] + self.radius, uv[1] * self.y_scale - self.radius, str(num),\n                     fontsize=self.FONTSIZE, color=self.TEXTCOLOR, weight=\'bold\')\n\n    def draw_text_bird(self, axes, idx, num):\n        """"""Plot the number in the bird eye view map""""""\n\n        std = self.stds_epi[idx] if self.stds_epi[idx] > 0 else self.stds_ale[idx]\n        theta = math.atan2(self.zz_pred[idx], self.xx_pred[idx])\n\n        delta_x = std * math.cos(theta)\n        delta_z = std * math.sin(theta)\n\n        axes[1].text(self.xx_pred[idx] + delta_x, self.zz_pred[idx] + delta_z,\n                     str(num), fontsize=self.FONTSIZE_BV, color=\'darkorange\')\n\n    def draw_circle(self, axes, uv, color):\n\n        circle = Circle((uv[0], uv[1] * self.y_scale), radius=self.radius, color=color, fill=True)\n        axes[0].add_patch(circle)\n\n    def set_axes(self, ax, axis):\n        assert axis in (0, 1)\n\n        if axis == 0:\n            ax.set_axis_off()\n            ax.set_xlim(0, self.width)\n            ax.set_ylim(self.height, 0)\n            self.mpl_im0 = ax.imshow(self.im)\n            ax.get_xaxis().set_visible(False)\n            ax.get_yaxis().set_visible(False)\n\n        else:\n            uv_max = [0., float(self.height)]\n            xyz_max = pixel_to_camera(uv_max, self.kk, self.z_max)\n            x_max = abs(xyz_max[0])  # shortcut to avoid oval circles in case of different kk\n            ax.plot([0, x_max], [0, self.z_max], \'k--\')\n            ax.plot([0, -x_max], [0, self.z_max], \'k--\')\n            ax.set_ylim(0, self.z_max+1)\n            ax.set_xlabel(""X [m]"")\n            ax.set_ylabel(""Z [m]"")\n\n        return ax\n\n\ndef draw_legend(axes):\n    handles, labels = axes[1].get_legend_handles_labels()\n    by_label = OrderedDict(zip(labels, handles))\n    axes[1].legend(by_label.values(), by_label.keys())\n\n\ndef get_angle(xx, zz):\n    """"""Obtain the points to plot the confidence of each annotation""""""\n\n    theta = math.atan2(zz, xx)\n    angle = theta * (180 / math.pi)\n\n    return angle\n'"
monoloco/visuals/webcam.py,4,"b'# pylint: disable=W0212\n""""""\nWebcam demo application\n\nImplementation adapted from https://github.com/vita-epfl/openpifpaf/blob/master/openpifpaf/webcam.py\n\n""""""\n\nimport time\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\n\nfrom ..visuals import Printer\nfrom ..network import PifPaf, MonoLoco\nfrom ..network.process import preprocess_pifpaf, factory_for_gt, image_transform\n\n\ndef webcam(args):\n\n    # add args.device\n    args.device = torch.device(\'cpu\')\n    if torch.cuda.is_available():\n        args.device = torch.device(\'cuda\')\n\n    # load models\n    args.camera = True\n    pifpaf = PifPaf(args)\n    monoloco = MonoLoco(model=args.model, device=args.device)\n\n    # Start recording\n    cam = cv2.VideoCapture(0)\n    visualizer_monoloco = None\n\n    while True:\n        start = time.time()\n        ret, frame = cam.read()\n        image = cv2.resize(frame, None, fx=args.scale, fy=args.scale)\n        height, width, _ = image.shape\n        print(\'resized image size: {}\'.format(image.shape))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        processed_image_cpu = image_transform(image.copy())\n        processed_image = processed_image_cpu.contiguous().to(args.device, non_blocking=True)\n        fields = pifpaf.fields(torch.unsqueeze(processed_image, 0))[0]\n        _, _, pifpaf_out = pifpaf.forward(image, processed_image_cpu, fields)\n\n        if not ret:\n            break\n        key = cv2.waitKey(1)\n\n        if key % 256 == 27:\n            # ESC pressed\n            print(""Escape hit, closing..."")\n            break\n        pil_image = Image.fromarray(image)\n        intrinsic_size = [xx * 1.3 for xx in pil_image.size]\n        kk, dict_gt = factory_for_gt(intrinsic_size)  # better intrinsics for mac camera\n        if visualizer_monoloco is None:  # it is, at the beginning\n            visualizer_monoloco = VisualizerMonoloco(kk, args)(pil_image)  # create it with the first image\n            visualizer_monoloco.send(None)\n\n        boxes, keypoints = preprocess_pifpaf(pifpaf_out, (width, height))\n        outputs, varss = monoloco.forward(keypoints, kk)\n        dic_out = monoloco.post_process(outputs, varss, boxes, keypoints, kk, dict_gt)\n        print(dic_out)\n        visualizer_monoloco.send((pil_image, dic_out))\n\n        end = time.time()\n        print(""run-time: {:.2f} ms"".format((end-start)*1000))\n\n    cam.release()\n\n    cv2.destroyAllWindows()\n\n\nclass VisualizerMonoloco:\n    def __init__(self, kk, args, epistemic=False):\n        self.kk = kk\n        self.args = args\n        self.z_max = args.z_max\n        self.epistemic = epistemic\n        self.output_types = args.output_types\n\n    def __call__(self, first_image, fig_width=4.0, **kwargs):\n        if \'figsize\' not in kwargs:\n            kwargs[\'figsize\'] = (fig_width, fig_width * first_image.size[0] / first_image.size[1])\n\n        printer = Printer(first_image, output_path="""", kk=self.kk, output_types=self.output_types,\n                          z_max=self.z_max, epistemic=self.epistemic)\n        figures, axes = printer.factory_axes()\n\n        for fig in figures:\n            fig.show()\n\n        while True:\n            image, dict_ann = yield\n            while axes and (axes[-1] and axes[-1].patches):  # for front -1==0, for bird/combined -1 == 1\n                if axes[0]:\n                    del axes[0].patches[0]\n                    del axes[0].texts[0]\n                if len(axes) == 2:\n                    del axes[1].patches[0]\n                    del axes[1].patches[0]  # the one became the 0\n                    if len(axes[1].lines) > 2:\n                        del axes[1].lines[2]\n                        if axes[1].texts:  # in case of no text\n                            del axes[1].texts[0]\n            printer.draw(figures, axes, dict_ann, image)\n            mypause(0.01)\n\n\ndef mypause(interval):\n    manager = plt._pylab_helpers.Gcf.get_active()\n    if manager is not None:\n        canvas = manager.canvas\n        if canvas.figure.stale:\n            canvas.draw_idle()\n        canvas.start_event_loop(interval)\n    else:\n        time.sleep(interval)\n'"
