file_path,api_count,code
dataHelper.py,9,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport numpy as np\nimport string\nfrom collections import Counter\nimport pandas as pd\nfrom tqdm import tqdm\nimport random\nimport time\nfrom utils import log_time_delta\nfrom dataloader import Dataset\nimport torch\nfrom torch.autograd import Variable\nfrom codecs import open\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nclass Alphabet(dict):\n    def __init__(self, start_feature_id = 1, alphabet_type=""text""):\n        self.fid = start_feature_id\n        if alphabet_type==""text"":\n            self.add(\'[PADDING]\')\n            self.add(\'[UNK]\')\n            self.add(\'[END]\')\n            self.unknow_token = self.get(\'[UNK]\')\n            self.end_token = self.get(\'[END]\')\n            self.padding_token = self.get(\'[PADDING]\')\n\n    def add(self, item):\n        idx = self.get(item, None)\n        if idx is None:\n            idx = self.fid\n            self[item] = idx\n      # self[idx] = item\n            self.fid += 1\n        return idx\n    \n    def addAll(self,words):\n        for word in words:\n            self.add(word)\n            \n    def dump(self, fname,path=""temp""):\n        if not os.path.exists(path):\n            os.mkdir(path)\n        with open(os.path.join(path,fname), ""w"",encoding=""utf-8"") as out:\n            for k in sorted(self.keys()):\n                out.write(""{}\\t{}\\n"".format(k, self[k]))\n\nclass DottableDict(dict):\n    def __init__(self, *args, **kwargs):\n        dict.__init__(self, *args, **kwargs)\n        self.__dict__ = self\n        self.allowDotting()\n    def allowDotting(self, state=True):\n        if state:\n            self.__dict__ = self\n        else:\n            self.__dict__ = dict()\n            \nclass BucketIterator(object):\n    def __init__(self,data,opt=None,batch_size=2,shuffle=True,test=False,position=False):\n        self.shuffle=shuffle\n        self.data=data\n        self.batch_size=batch_size\n        self.test=test        \n        if opt is not None:\n            self.setup(opt)\n    def setup(self,opt):\n        \n        self.batch_size=opt.batch_size\n        self.shuffle=opt.__dict__.get(""shuffle"",self.shuffle)\n        self.position=opt.__dict__.get(""position"",False)\n        if self.position:\n            self.padding_token =  opt.alphabet.padding_token\n    \n    def transform(self,data):\n        if torch.cuda.is_available():\n            data=data.reset_index()\n            text= Variable(torch.LongTensor(data.text).cuda())\n            label= Variable(torch.LongTensor([int(i) for i in data.label.tolist()]).cuda())                \n        else:\n            data=data.reset_index()\n            text= Variable(torch.LongTensor(data.text))\n            label= Variable(torch.LongTensor(data.label.tolist()))\n        if self.position:\n            position_tensor = self.get_position(data.text)\n            return DottableDict({""text"":(text,position_tensor),""label"":label})\n        return DottableDict({""text"":text,""label"":label})\n    \n    def get_position(self,inst_data):\n        inst_position = np.array([[pos_i+1 if w_i != self.padding_token else 0 for pos_i, w_i in enumerate(inst)] for inst in inst_data])\n        inst_position_tensor = Variable( torch.LongTensor(inst_position), volatile=self.test) \n        if torch.cuda.is_available():\n            inst_position_tensor=inst_position_tensor.cuda()\n        return inst_position_tensor\n\n    def __iter__(self):\n        if self.shuffle:\n            self.data = self.data.sample(frac=1).reset_index(drop=True)\n        batch_nums = int(len(self.data)/self.batch_size)\n        for  i in range(batch_nums):\n            yield self.transform(self.data[i*self.batch_size:(i+1)*self.batch_size])\n        yield self.transform(self.data[-1*self.batch_size:])\n    \n\n        \n                \n@log_time_delta\ndef vectors_lookup(vectors,vocab,dim):\n    embedding = np.zeros((len(vocab),dim))\n    count = 1\n    for word in vocab:\n        if word in vectors:\n            count += 1\n            embedding[vocab[word]]= vectors[word]\n        else:\n            embedding[vocab[word]]= np.random.uniform(-0.5,+0.5,dim)#vectors[\'[UNKNOW]\'] #.tolist()\n    print( \'word in embedding\',count)\n    return embedding\n\n@log_time_delta\ndef load_text_vec(alphabet,filename="""",embedding_size=-1):\n    vectors = {}\n    with open(filename,encoding=\'utf-8\') as f:\n        for line in tqdm(f):\n            items = line.strip().split(\' \')\n            if len(items) == 2:\n                vocab_size, embedding_size= items[0],items[1]\n                print( \'embedding_size\',embedding_size)\n                print( \'vocab_size in pretrained embedding\',vocab_size)                \n            else:\n                word = items[0]\n                if word in alphabet:\n                    vectors[word] = items[1:]\n    print( \'words need to be found \',len(alphabet))\n    print( \'words found in wor2vec embedding \',len(vectors.keys()))\n    \n    if embedding_size==-1:\n        embedding_size = len(vectors[list(vectors.keys())[0]])\n    return vectors,embedding_size\n\ndef getEmbeddingFile(opt):\n    #""glove""  ""w2v""\n    embedding_name = opt.__dict__.get(""embedding"",""glove_6b_300"")\n    if embedding_name.startswith(""glove""):\n        return os.path.join( "".vector_cache"",""glove.6B.300d.txt"")\n    else:\n        return opt.embedding_dir\n    # please refer to   https://pypi.python.org/pypi/torchwordemb/0.0.7\n    return \n@log_time_delta\ndef getSubVectors(opt,alphabet):\n    pickle_filename = ""temp/""+opt.dataset+"".vec""\n    if not os.path.exists(pickle_filename) or opt.debug:    \n        glove_file = getEmbeddingFile(opt)\n        wordset= set(alphabet.keys())   # python 2.7\n        loaded_vectors,embedding_size = load_text_vec(wordset,glove_file) \n        \n        vectors = vectors_lookup(loaded_vectors,alphabet,embedding_size)\n        if opt.debug:\n            if not os.path.exists(""temp""):\n                os.mkdir(""temp"")\n            with open(""temp/oov.txt"",""w"",""utf-8"") as f:\n                unknown_set = set(alphabet.keys()) - set(loaded_vectors.keys())\n                f.write(""\\n"".join( unknown_set))\n        if  opt.debug:\n            pickle.dump(vectors,open(pickle_filename,""wb""))\n        return vectors\n    else:\n        print(""load cache for SubVector"")\n        return pickle.load(open(pickle_filename,""rb""))\n    \ndef getDataSet(opt):\n    import dataloader\n    dataset= dataloader.getDataset(opt)\n#    files=[os.path.join(data_dir,data_name)   for data_name in [\'train.txt\',\'test.txt\',\'dev.txt\']]\n    \n    return dataset.getFormatedData()\n    \n    #data_dir = os.path.join("".data/clean"",opt.dataset)\n    #if not os.path.exists(data_dir):\n    #     import dataloader\n    #     dataset= dataloader.getDataset(opt)\n    #     return dataset.getFormatedData()\n    #else:\n    #     for root, dirs, files in os.walk(data_dir):\n    #         for file in files:\n    #             yield os.path.join(root,file)\n         \n    \n#    files=[os.path.join(data_dir,data_name)   for data_name in [\'train.txt\',\'test.txt\',\'dev.txt\']]\n    \nimport re\ndef clean(text):\n#    text=""\'tycoon.<br\'""\n    for token in [""<br/>"",""<br>"",""<br""]:\n         text = re.sub(token,"" "",text)\n    text = re.sub(""[\\s+\\.\\!\\/_,$%^*()\\(\\)<>+\\""\\[\\]\\-\\?;:\\\'{}`]+|[+\xe2\x80\x94\xe2\x80\x94\xef\xbc\x81\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x9f\xe3\x80\x81~@#\xef\xbf\xa5%\xe2\x80\xa6\xe2\x80\xa6&*\xef\xbc\x88\xef\xbc\x89]+"", "" "",text)\n\n#    print(""%s $$$$$ %s"" %(pre,text))     \n\n    return text.lower().split()\n@log_time_delta\ndef get_clean_datas(opt):\n    pickle_filename = ""temp/""+opt.dataset+"".data""\n    if not os.path.exists(pickle_filename) or opt.debug: \n        datas = [] \n        for filename in getDataSet(opt):\n            df = pd.read_csv(filename,header = None,sep=""\\t"",names=[""text"",""label""]).fillna(\'0\')\n    \n        #        df[""text""]= df[""text""].apply(clean).str.lower().str.split() #replace(""[\\"",:#]"","" "")\n            df[""text""]= df[""text""].apply(clean)\n            datas.append(df)\n        if  opt.debug:\n            if not os.path.exists(""temp""):\n                os.mkdir(""temp"")\n            pickle.dump(datas,open(pickle_filename,""wb""))\n        return datas\n    else:\n        print(""load cache for data"")\n        return pickle.load(open(pickle_filename,""rb""))\n    \n    \n    \n\n\ndef load_vocab_from_bert(bert_base):\n    \n    \n    bert_vocab_dir = os.path.join(bert_base,""vocab.txt"")\n    alphabet = Alphabet(start_feature_id = 0,alphabet_type=""bert"")\n\n    from pytorch_pretrained_bert import BertTokenizer\n\n    # Load pre-trained model tokenizer (vocabulary)\n    tokenizer = BertTokenizer.from_pretrained(bert_vocab_dir)\n    for index,word in tokenizer.ids_to_tokens.items():\n        alphabet.add(word)\n    return alphabet,tokenizer\n        \n\ndef process_with_bert(text,tokenizer,max_seq_len) :\n    tokens =tokenizer.convert_tokens_to_ids(  tokenizer.tokenize("" "".join(text[:max_seq_len])))\n    \n    return tokens[:max_seq_len] + [0] *int(max_seq_len-len(tokens))\n\ndef loadData(opt,embedding=True):\n    if embedding==False:\n        return loadDataWithoutEmbedding(opt)\n    \n    datas =get_clean_datas(opt)\n    \n    alphabet = Alphabet(start_feature_id = 0)\n    label_alphabet= Alphabet(start_feature_id = 0,alphabet_type=""label"") \n    \n    df=pd.concat(datas)   \n    df.to_csv(""demo.text"",sep=""\\t"",index=False)\n    label_set = set(df[""label""])\n    label_alphabet.addAll(label_set)\n    opt.label_size= len(label_alphabet)\n    if opt.max_seq_len==-1:\n        opt.max_seq_len = df.apply(lambda row: row[""text""].__len__(),axis=1).max()\n        \n    if ""bert"" not in opt.model.lower(): \n\n        \n        word_set=set()\n        [word_set.add(word)  for l in df[""text""] if l is not None for word in l ]\n    #    from functools import reduce\n    #    word_set=set(reduce(lambda x,y :x+y,df[""text""]))            \n       \n        alphabet.addAll(word_set)\n    \n        vectors = getSubVectors(opt,alphabet) \n        \n        opt.vocab_size= len(alphabet)    \n    #    opt.label_size= len(label_alphabet)\n        opt.embedding_dim= vectors.shape[-1]\n        opt.embeddings = torch.FloatTensor(vectors)\n        \n    else:   \n        alphabet,tokenizer = load_vocab_from_bert(opt.bert_dir)\n    \n    opt.alphabet=alphabet\n    \n#    alphabet.dump(opt.dataset+"".alphabet"")     \n    for data in datas:\n        if ""bert"" not in opt.model.lower():\n            data[""text""]= data[""text""].apply(lambda text: [alphabet.get(word,alphabet.unknow_token)  for word in text[:opt.max_seq_len]] + [alphabet.padding_token] *int(opt.max_seq_len-len(text)) )\n        else :\n            data[""text""]= data[""text""].apply(process_with_bert,tokenizer=tokenizer,max_seq_len = opt.max_seq_len)\n        data[""label""]=data[""label""].apply(lambda text: label_alphabet.get(text)) \n        \n    return map(lambda x:BucketIterator(x,opt),datas)#map(BucketIterator,datas)  #\n\ndef loadDataWithoutEmbedding(opt):\n    datas=[]\n    for filename in getDataSet(opt):\n        df = pd.read_csv(filename,header = None,sep=""\\t"",names=[""text"",""label""]).fillna(\'0\')\n        df[""text""]= df[""text""].str.lower()\n        datas.append((df[""text""],df[""label""]))\n    return datas\n    \n\n\n    \n\nif __name__ ==""__main__"":\n    import opts\n    opt = opts.parse_opt()\n    opt.max_seq_len=-1\n    import dataloader\n    dataset= dataloader.getDataset(opt)\n    datas=loadData(opt)\n    \n\n'"
main.py,8,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nfrom six.moves import cPickle\nimport time,os,random\nimport itertools\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import NLLLoss,MultiLabelSoftMarginLoss,MultiLabelMarginLoss,BCELoss\n\nimport opts\nimport models\nimport utils\n\n\ntimeStamp = time.strftime(""%Y%m%d%H%M%S"", time.localtime(int(time.time()) ))\nperformance_log_file =  os.path.join(""log"",""result""+timeStamp+ "".csv"") \nif not os.path.exists(performance_log_file):\n    with open(performance_log_file,""w"") as f:\n        f.write(""argument\\n"")\n        f.close() \n      \n        \ndef train(opt,train_iter, test_iter,verbose=True):\n    global_start= time.time()\n    logger = utils.getLogger()\n    model=models.setup(opt)\n    if torch.cuda.is_available():\n        model.cuda()\n    params = [param for param in model.parameters() if param.requires_grad] #filter(lambda p: p.requires_grad, model.parameters())\n    \n    model_info ="";"".join( [str(k)+"":""+ str(v)  for k,v in opt.__dict__.items() if type(v) in (str,int,float,list,bool)])  \n    logger.info(""# parameters:"" + str(sum(param.numel() for param in params)))\n    logger.info(model_info)\n    \n    \n    model.train()\n    optimizer = utils.getOptimizer(params,name=opt.optimizer, lr=opt.learning_rate,scheduler= utils.get_lr_scheduler(opt.lr_scheduler))\n\n    loss_fun = F.cross_entropy\n\n    filename = None\n    percisions=[]\n    for i in range(opt.max_epoch):\n        for epoch,batch in enumerate(train_iter):\n            optimizer.zero_grad()\n            start= time.time()\n            \n            text = batch.text[0] if opt.from_torchtext else batch.text\n            predicted = model(text)\n    \n            loss= loss_fun(predicted,batch.label)\n    \n            loss.backward()\n            utils.clip_gradient(optimizer, opt.grad_clip)\n            optimizer.step()\n            \n            if verbose:\n                if  torch.cuda.is_available():\n                    logger.info(""%d iteration %d epoch with loss : %.5f in %.4f seconds"" % (i,epoch,loss.cpu().data.numpy(),time.time()-start))\n                else:\n                    logger.info(""%d iteration %d epoch with loss : %.5f in %.4f seconds"" % (i,epoch,loss.data.numpy()[0],time.time()-start))\n \n        percision=utils.evaluation(model,test_iter,opt.from_torchtext)\n        if verbose:\n            logger.info(""%d iteration with percision %.4f"" % (i,percision))\n        if len(percisions)==0 or percision > max(percisions):\n            if filename:\n                os.remove(filename)\n            filename = model.save(metric=percision)\n        percisions.append(percision)\n            \n#    while(utils.is_writeable(performance_log_file)):\n    df = pd.read_csv(performance_log_file,index_col=0,sep=""\\t"")\n    df.loc[model_info,opt.dataset] =  max(percisions) \n    df.to_csv(performance_log_file,sep=""\\t"")    \n    logger.info(model_info +"" with time :""+ str( time.time()-global_start)+"" ->"" +str( max(percisions) ) )\n    print(model_info +"" with time :""+ str( time.time()-global_start)+"" ->"" +str( max(percisions) ) )\n\n        \nif __name__==""__main__"": \n    parameter_pools = utils.parse_grid_parameters(""config/grid_search_cnn.ini"")\n    \n#    parameter_pools={\n#            ""model"":[""lstm"",""cnn"",""fasttext""],\n#            ""keep_dropout"":[0.8,0.9,1.0],\n#            ""batch_size"":[32,64,128],\n#            ""learning_rate"":[100,10,1,1e-1,1e-2,1e-3],\n#            ""optimizer"":[""adam""],\n#            ""lr_scheduler"":[None]            \n#                        }    \n    opt = opts.parse_opt()\n    if ""CUDA_VISIBLE_DEVICES"" not in os.environ.keys():\n        os.environ[""CUDA_VISIBLE_DEVICES""] =opt.gpu\n    train_iter, test_iter = utils.loadData(opt)\n#    if from_torchtext:\n#        train_iter, test_iter = utils.loadData(opt)\n#    else:\n#        import dataHelper \n#        train_iter, test_iter = dataHelper.loadData(opt)\n    if False:\n        model=models.setup(opt)\n        print(opt.model)\n        if torch.cuda.is_available():\n            model.cuda()\n        train(opt,train_iter, test_iter)\n    else:\n        \n        pool =[ arg for arg in itertools.product(*parameter_pools.values())]\n        random.shuffle(pool)\n        args=[arg for i,arg in enumerate(pool) if i%opt.gpu_num==opt.gpu]\n        \n        for arg in args:\n            olddataset = opt.dataset\n            for k,v in zip(parameter_pools.keys(),arg):\n                opt.__setattr__(k,v)\n            if ""dataset"" in parameter_pools and olddataset != opt.dataset:\n                train_iter, test_iter = utils.loadData(opt)\n            train(opt,train_iter, test_iter,verbose=False)\n   '"
opts.py,0,"b'import argparse,os,re\nimport configparser\n\nclass Params(object):\n    def __init__(self):\n        parser = argparse.ArgumentParser()\n        # Data input settings\n        \n        parser.add_argument(\'--config\', type=str, default=""no_file_exists"",\n                        help=\'gpu number\')\n            \n            \n        parser.add_argument(\'--hidden_dim\', type=int, default=128,\n                        help=\'hidden_dim\')     \n    \n        parser.add_argument(\'--max_seq_len\', type=int, default=200,\n                        help=\'max_seq_len\')\n        parser.add_argument(\'--batch_size\', type=int, default=64,\n                        help=\'batch_size\')\n        parser.add_argument(\'--embedding_dim\', type=int, default=-1,\n                        help=\'embedding_dim\')\n        parser.add_argument(\'--learning_rate\', type=float, default=2e-5,\n                        help=\'learning_rate\')\n        parser.add_argument(\'--grad_clip\', type=float, default=1e-1,\n                        help=\'grad_clip\')\n    \n        parser.add_argument(\'--model\', type=str, default=""cnn"",\n                        help=\'model name\')\n    \n        parser.add_argument(\'--dataset\', type=str, default=""imdb"",\n    \n                        help=\'dataset\')\n        parser.add_argument(\'--position\', type=bool, default=False,\n                        help=\'gpu number\')\n        \n        parser.add_argument(\'--keep_dropout\', type=float, default=0.8,\n                        help=\'keep_dropout\')\n        parser.add_argument(\'--max_epoch\', type=int, default=20,\n                        help=\'max_epoch\')\n        parser.add_argument(\'--embedding_file\', type=str, default=""glove.6b.300"",\n                        help=\'glove or w2v\')\n        parser.add_argument(\'--embedding_training\', type=str, default=""false"",\n                        help=\'embedding_training\')\n        #kim CNN\n        parser.add_argument(\'--kernel_sizes\', type=str, default=""1,2,3,5"",\n                        help=\'kernel_sizes\')\n        parser.add_argument(\'--kernel_nums\', type=str, default=""256,256,256,256"",\n                        help=\'kernel_nums\')\n        parser.add_argument(\'--embedding_type\', type=str, default=""non-static"",\n                        help=\'embedding_type\')\n        parser.add_argument(\'--lstm_mean\', type=str, default=""mean"",# last\n                        help=\'lstm_mean\')\n        parser.add_argument(\'--lstm_layers\', type=int, default=1,# last\n                        help=\'lstm_layers\')\n        parser.add_argument(\'--gpu\', type=int, default=0,\n                        help=\'gpu number\')\n        parser.add_argument(\'--proxy\', type=str, default=""null"",\n                        help=\'http://proxy.xx.com:8080\')\n        parser.add_argument(\'--debug\', type=str, default=""true"",\n                        help=\'gpu number\')\n    \n        parser.add_argument(\'--embedding_dir\', type=str, default="".glove/glove.6B.300d.txt"",\n                        help=\'embedding_dir\')\n        \n        parser.add_argument(\'--bert_dir\', type=str, default=""D:/dataset/bert/uncased_L-12_H-768_A-12"",\n                        help=\'bert dir\')\n        parser.add_argument(\'--bert_trained\', type=str, default=""false"",\n                        help=\'fine tune the bert or not\')\n        \n        parser.add_argument(\'--from_torchtext\', type=str, default=""false"",\n                        help=\'from torchtext or native data loader\')\n    #\n        args = parser.parse_args()\n        \n        if args.config != ""no_file_exists"":\n            if os.path.exists(args.config):\n                config = configparser.ConfigParser()\n                config_file_path=args.config\n                config.read(config_file_path)\n                config_common = config[\'COMMON\']\n                for key in config_common.keys():\n                    args.__dict__[key]=config_common[key]\n            else:\n                print(""config file named %s does not exist"" % args.config)\n    \n#        args.kernel_sizes = [int(i) for i in args.kernel_sizes.split("","")]\n#        args.kernel_nums = [int(i) for i in args.kernel_nums.split("","")]\n    #\n    #    # Check if args are valid\n    #    assert args.rnn_size > 0, ""rnn_size should be greater than 0""\n    \n        if ""CUDA_VISIBLE_DEVICES"" not in os.environ.keys():\n            os.environ[""CUDA_VISIBLE_DEVICES""] =str(args.gpu)\n        \n        if args.model==""transformer"":\n            args.position=True\n        else:\n            args.position=False\n            \n        # process the type for bool and list    \n        for arg in args.__dict__.keys():\n            if type(args.__dict__[arg])==str:\n                if args.__dict__[arg].lower()==""true"":\n                    args.__dict__[arg]=True\n                elif args.__dict__[arg].lower()==""false"":\n                    args.__dict__[arg]=False\n                elif "","" in args.__dict__[arg]:\n                    args.__dict__[arg]= [int(i) for i in args.__dict__[arg].split("","")]\n                else:\n                    pass\n    \n            \n        if os.path.exists(""proxy.config""):\n            with open(""proxy.config"") as f:\n    \n                args.proxy = f.read()\n                print(args.proxy)\n        \n        return args \n    \n    def parse_config(self, config_file_path):\n        config = configparser.ConfigParser()\n        config.read(config_file_path)\n        config_common = config[\'COMMON\']\n        is_numberic = re.compile(r\'^[-+]?[0-9.]+$\')\n        for key,value in config_common.items():\n            result = is_numberic.match(value)\n            if result:\n                if type(eval(value)) == int:\n                    value= int(value)\n                else :\n                    value= float(value)\n\n            self.__dict__.__setitem__(key,value)            \n\n    def export_to_config(self, config_file_path):\n        config = configparser.ConfigParser()\n        config[\'COMMON\'] = {}\n        config_common = config[\'COMMON\']\n        for k,v in self.__dict__.items():        \n            if not k == \'lookup_table\':    \n                config_common[k] = str(v)\n\n        with open(config_file_path, \'w\') as configfile:\n            config.write(configfile)\n\n    def parseArgs(self):\n        #required arguments:\n        parser = argparse.ArgumentParser(description=\'running the complex embedding network\')\n        parser.add_argument(\'-config\', action = \'store\', dest = \'config_file_path\', help = \'The configuration file path.\')\n        args = parser.parse_args()\n        self.parse_config(args.config_file_path)\n    \n    def setup(self,parameters):\n        for k, v in parameters:\n            self.__dict__.__setitem__(k,v)\n    def get_parameter_list(self):\n        info=[]\n        for k, v in self.__dict__.items():\n            if k in [""validation_split"",""batch_size"",""dropout_rate"",""hidden_unit_num"",""hidden_unit_num_second"",""cell_type"",""contatenate"",""model""]:\n                info.append(""%s-%s""%(k,str(v)))\n        return info\n    \n    def to_string(self):\n        return ""_"".join(self.get_parameter_list())\n\n\ndef parse_opt():\n\n    parser = argparse.ArgumentParser()\n    # Data input settings\n    \n    parser.add_argument(\'--config\', type=str, default=""no_file_exists"",\n                    help=\'gpu number\')\n        \n        \n    parser.add_argument(\'--hidden_dim\', type=int, default=128,\n                    help=\'hidden_dim\')     \n\n    parser.add_argument(\'--max_seq_len\', type=int, default=200,\n                    help=\'max_seq_len\')\n    parser.add_argument(\'--batch_size\', type=int, default=64,\n                    help=\'batch_size\')\n    parser.add_argument(\'--embedding_dim\', type=int, default=-1,\n                    help=\'embedding_dim\')\n    \n    \n    parser.add_argument(\'--learning_rate\', type=float, default=2e-5,\n                    help=\'learning_rate\')\n    parser.add_argument(\'--lr_scheduler\', type=str, default=""none"",\n                    help=\'lr_scheduler\')\n    parser.add_argument(\'--optimizer\', type=str, default=""adam"",\n                    help=\'optimizer\')\n    parser.add_argument(\'--grad_clip\', type=float, default=1e-1,\n                    help=\'grad_clip\')\n            \n    parser.add_argument(\'--model\', type=str, default=""bilstm"",\n                    help=\'model name\')\n\n    parser.add_argument(\'--dataset\', type=str, default=""imdb"",\n\n                    help=\'dataset\')\n    parser.add_argument(\'--position\', type=bool, default=False,\n                    help=\'gpu number\')\n    \n    parser.add_argument(\'--keep_dropout\', type=float, default=0.8,\n                    help=\'keep_dropout\')\n    parser.add_argument(\'--max_epoch\', type=int, default=20,\n                    help=\'max_epoch\')\n    parser.add_argument(\'--embedding_file\', type=str, default=""glove.6b.300"",\n                    help=\'glove or w2v\')\n    parser.add_argument(\'--embedding_training\', type=str, default=""false"",\n                    help=\'embedding_training\')\n    #kim CNN\n    parser.add_argument(\'--kernel_sizes\', type=str, default=""1,2,3,5"",\n                    help=\'kernel_sizes\')\n    parser.add_argument(\'--kernel_nums\', type=str, default=""256,256,256,256"",\n                    help=\'kernel_nums\')\n    parser.add_argument(\'--embedding_type\', type=str, default=""non-static"",\n                    help=\'embedding_type\')\n    parser.add_argument(\'--lstm_mean\', type=str, default=""mean"",# last\n                    help=\'lstm_mean\')\n    parser.add_argument(\'--lstm_layers\', type=int, default=1,# last\n                    help=\'lstm_layers\')\n    parser.add_argument(\'--gpu\', type=int, default=0,\n                    help=\'gpu number\')\n    parser.add_argument(\'--gpu_num\', type=int, default=1,\n                    help=\'gpu number\')\n    parser.add_argument(\'--proxy\', type=str, default=""null"",\n                    help=\'http://proxy.xx.com:8080\')\n    parser.add_argument(\'--debug\', type=str, default=""true"",\n                    help=\'gpu number\')\n    parser.add_argument(\'--bidirectional\', type=str, default=""true"",\n                    help=\'bidirectional\')\n    \n    parser.add_argument(\'--embedding_dir\', type=str, default="".glove/glove.6B.300d.txt"",\n                    help=\'embedding_dir\')\n    \n    parser.add_argument(\'--bert_dir\', type=str, default=""D:/dataset/bert/uncased_L-12_H-768_A-12"",\n                    help=\'bert dir\')\n    parser.add_argument(\'--bert_trained\', type=str, default=""false"",\n                    help=\'fine tune the bert or not\')\n    \n    parser.add_argument(\'--from_torchtext\', type=str, default=""false"",\n                    help=\'from torchtext or native data loader\')\n#\n    args = parser.parse_args()\n    \n    if args.config != ""no_file_exists"":\n        if os.path.exists(args.config):\n            config = configparser.ConfigParser()\n            config_file_path=args.config\n            config.read(config_file_path)\n            config_common = config[\'COMMON\']\n            for key in config_common.keys():\n                args.__dict__[key]=config_common[key]\n        else:\n            print(""config file named %s does not exist"" % args.config)\n\n#        args.kernel_sizes = [int(i) for i in args.kernel_sizes.split("","")]\n#        args.kernel_nums = [int(i) for i in args.kernel_nums.split("","")]\n#\n#    # Check if args are valid\n#    assert args.rnn_size > 0, ""rnn_size should be greater than 0""\n\n    if ""CUDA_VISIBLE_DEVICES"" not in os.environ.keys():\n        os.environ[""CUDA_VISIBLE_DEVICES""] =str(args.gpu)\n    \n    if args.model==""transformer"":\n        args.position=True\n    else:\n        args.position=False\n        \n    # process the type for bool and list    \n    for arg in args.__dict__.keys():\n        if type(args.__dict__[arg])==str:\n            if args.__dict__[arg].lower()==""true"":\n                args.__dict__[arg]=True\n            elif args.__dict__[arg].lower()==""false"":\n                args.__dict__[arg]=False\n            elif "","" in args.__dict__[arg]:\n                args.__dict__[arg]= [int(i) for i in args.__dict__[arg].split("","")]\n            else:\n                pass\n\n        \n    if os.path.exists(""proxy.config""):\n        with open(""proxy.config"") as f:\n\n            args.proxy = f.read()\n            print(args.proxy)\n    \n    return args '"
trandition.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport opts\nimport dataHelper\n#refer to ""https://zhuanlan.zhihu.com/p/26729228""\nopt = opts.parse_opt()\nimport dataHelper as helper\ntrain_iter, test_iter = dataHelper.loadData(opt,embedding=False)\n#categories = [\'good\', \'bad\', \'mid\']\nx_train,y_train=train_iter\nx_test,y_test = test_iter\n\n#opt.model =""haha""\nif opt.model == ""bayes"":\n    """""" Naive Bayes classifier """"""\n    # sklearn\xe6\x9c\x89\xe4\xb8\x80\xe5\xa5\x97\xe5\xbe\x88\xe6\x88\x90\xe7\x86\x9f\xe7\x9a\x84\xe7\xae\xa1\xe9\x81\x93\xe6\xb5\x81\xe7\xa8\x8bPipeline\xef\xbc\x8c\xe5\xbf\xab\xe9\x80\x9f\xe6\x90\xad\xe5\xbb\xba\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xa5\x9e\xe5\x99\xa8\n    bayes_clf = Pipeline([(\'vect\', CountVectorizer()), \n                          (\'tfidf\', TfidfTransformer()),\n                          (\'clf\', MultinomialNB()) \n                          ])\n    bayes_clf.fit(x_train, y_train)\n    """""" Predict the test dataset using Naive Bayes""""""\n    predicted = bayes_clf.predict(x_test)\n    print(\'Naive Bayes correct prediction: {:4.4f}\'.format(np.mean(predicted == y_test)))\n    # \xe8\xbe\x93\xe5\x87\xbaf1\xe5\x88\x86\xe6\x95\xb0\xef\xbc\x8c\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xef\xbc\x8c\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\xe7\xad\x89\xe6\x8c\x87\xe6\xa0\x87\n#    print(metrics.classification_report(y_test, predicted, target_names=categories))\nelif opt.model == ""svm"":\n    \n    """""" Support Vector Machine (SVM) classifier""""""\n    svm_clf = Pipeline([(\'vect\', CountVectorizer()),\n        (\'tfidf\', TfidfTransformer()),\n        (\'clf\', SGDClassifier(loss=\'hinge\', penalty=\'l2\', alpha=1e-3, random_state=42)),\n    ])\n    svm_clf.fit(x_train, y_train)\n    predicted = svm_clf.predict(x_test)\n    print(\'SVM correct prediction: {:4.4f}\'.format(np.mean(predicted == y_test)))\n#    print(metrics.classification_report(y_test, predicted, target_names=categories))\n    \nelse:\n    """""" 10-\xe6\x8a\x98\xe4\xba\xa4\xe5\x8f\x89\xe9\xaa\x8c\xe8\xaf\x81 """"""\n    clf_b = make_pipeline(CountVectorizer(), TfidfTransformer(), MultinomialNB())\n    clf_s= make_pipeline(CountVectorizer(), TfidfTransformer(), SGDClassifier(loss=\'hinge\', penalty=\'l2\', alpha=1e-3, n_iter= 5, random_state=42))\n    \n    bayes_10_fold = cross_val_score(clf_b, x_test, y_test, cv=10)\n    svm_10_fold = cross_val_score(clf_s, x_test, y_test, cv=10)\n    \n    print(\'Naives Bayes 10-fold correct prediction: {:4.4f}\'.format(np.mean(bayes_10_fold)))\n    print(\'SVM 10-fold correct prediction: {:4.4f}\'.format(np.mean(svm_10_fold)))\n# \xe8\xbe\x93\xe5\x87\xba\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\n#print(""Confusion Matrix:"")\n#print(metrics.confusion_matrix(y_test, predicted))\n#print(\'\\n\')\n\n\n\n\n'"
utils.py,19,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn.functional as F\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe, CharNGram, FastText\nimport numpy as np\nfrom functools import wraps\nimport time\nimport sys\nimport logging\nimport os,configparser,re\n\ndef log_time_delta(func):\n    @wraps(func)\n    def _deco(*args, **kwargs):\n        start = time.time()\n        ret = func(*args, **kwargs)\n        end = time.time()\n        delta = end - start\n        print( ""%s runed %.2f seconds""% (func.__name__,delta))\n        return ret\n    return _deco  \n\ndef clip_gradient(optimizer, grad_clip):\n    for group in optimizer.param_groups:\n        for param in group[\'params\']:       \n            if param.grad is not None and param.requires_grad:\n                param.grad.data.clamp_(-grad_clip, grad_clip)\n\n\ndef loadData(opt):\n    if not opt.from_torchtext:\n        import dataHelper as helper\n        return helper.loadData(opt)\n    device = 0 if  torch.cuda.is_available()  else -1\n\n    TEXT = data.Field(lower=True, include_lengths=True, batch_first=True,fix_length=opt.max_seq_len)\n    LABEL = data.Field(sequential=False)\n    if opt.dataset==""imdb"":\n        train, test = datasets.IMDB.splits(TEXT, LABEL)\n    elif opt.dataset==""sst"":\n        train, val, test = datasets.SST.splits( TEXT, LABEL, fine_grained=True, train_subtrees=True,\n                                               filter_pred=lambda ex: ex.label != \'neutral\')\n    elif opt.dataset==""trec"":\n        train, test = datasets.TREC.splits(TEXT, LABEL, fine_grained=True)\n    else:\n        print(""does not support this datset"")\n        \n    TEXT.build_vocab(train, vectors=GloVe(name=\'6B\', dim=300))\n    LABEL.build_vocab(train)    \n    # print vocab information\n    print(\'len(TEXT.vocab)\', len(TEXT.vocab))\n    print(\'TEXT.vocab.vectors.size()\', TEXT.vocab.vectors.size())\n\n    train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=opt.batch_size,device=device,repeat=False,shuffle=True)\n\n    opt.label_size= len(LABEL.vocab)    \n    opt.vocab_size = len(TEXT.vocab)\n    opt.embedding_dim= TEXT.vocab.vectors.size()[1]\n    opt.embeddings = TEXT.vocab.vectors\n    \n    return train_iter, test_iter\n\n\ndef evaluation(model,test_iter,from_torchtext=True):\n    model.eval()\n    accuracy=[]\n#    batch= next(iter(test_iter))\n    for index,batch in enumerate( test_iter):\n        text = batch.text[0] if from_torchtext else batch.text\n        predicted = model(text)\n        prob, idx = torch.max(predicted, 1) \n        percision=(idx== batch.label).float().mean()\n        \n        if torch.cuda.is_available():\n            accuracy.append(percision.data.item() )\n        else:\n            accuracy.append(percision.data.numpy()[0] )\n    model.train()\n    return np.mean(accuracy)\n\n\n\ndef getOptimizer(params,name=""adam"",lr=1,momentum=None,scheduler=None):\n    \n    name = name.lower().strip()          \n        \n    if name==""adadelta"":\n        optimizer=torch.optim.Adadelta(params, lr=1.0*lr, rho=0.9, eps=1e-06, weight_decay=0).param_groups()\n    elif name == ""adagrad"":\n        optimizer=torch.optim.Adagrad(params, lr=0.01*lr, lr_decay=0, weight_decay=0)\n    elif name == ""sparseadam"":        \n        optimizer=torch.optim.SparseAdam(params, lr=0.001*lr, betas=(0.9, 0.999), eps=1e-08)\n    elif name ==""adamax"":\n        optimizer=torch.optim.Adamax(params, lr=0.002*lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n    elif name ==""asgd"":\n        optimizer=torch.optim.ASGD(params, lr=0.01*lr, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)\n    elif name == ""lbfgs"":\n        optimizer=torch.optim.LBFGS(params, lr=1*lr, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None)\n    elif name == ""rmsprop"":\n        optimizer=torch.optim.RMSprop(params, lr=0.01*lr, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n    elif name ==""rprop"":\n        optimizer=torch.optim.Rprop(params, lr=0.01*lr, etas=(0.5, 1.2), step_sizes=(1e-06, 50))\n    elif name ==""sgd"":\n        optimizer=torch.optim.SGD(params, lr=0.1*lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n    elif name ==""adam"":\n         optimizer=torch.optim.Adam(params, lr=0.1*lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n    else:\n        print(""undefined optimizer, use adam in default"")\n        optimizer=torch.optim.Adam(params, lr=0.1*lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n    \n    if scheduler is not None:\n        if scheduler == ""lambdalr"":\n            lambda1 = lambda epoch: epoch // 30\n            lambda2 = lambda epoch: 0.95 ** epoch\n            return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n        elif scheduler==""steplr"":\n            return torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n        elif scheduler ==""multisteplr"":\n            return torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n        elif scheduler ==""reducelronplateau"":\n            return  torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \'min\')\n        else:\n            pass\n\n    else:\n        return optimizer  \n    return \n\ndef get_lr_scheduler(name):\n    # todo \n    return None\n    \n    \n    \ndef getLogger():\n    import random\n    random_str = str(random.randint(1,10000))\n    \n    now = int(time.time()) \n    timeArray = time.localtime(now)\n    timeStamp = time.strftime(""%Y%m%d%H%M%S"", timeArray)\n    log_filename = ""log/"" +time.strftime(""%Y%m%d"", timeArray)\n    \n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program) \n    if not os.path.exists(""log""):\n        os.mkdir(""log"")\n    if not os.path.exists(log_filename):\n        os.mkdir(log_filename)\n    logging.basicConfig(format=\'%(asctime)s: %(levelname)s: %(message)s\',datefmt=\'%a, %d %b %Y %H:%M:%S\',filename=log_filename+\'/qa\'+timeStamp+""_""+ random_str+\'.log\',filemode=\'w\')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(""running %s"" % \' \'.join(sys.argv))\n    \n    return logger\n\ndef parse_grid_parameters(file_path):\n    config = configparser.ConfigParser()\n    config.read(file_path)\n    config_common = config[\'COMMON\']\n    dictionary = {}\n    for key,value in config_common.items():\n        array = value.split(\';\')\n        is_numberic = re.compile(r\'^[-+]?[0-9.]+$\')\n        new_array = []\n    \n        for value in array:\n            value = value.strip()\n            result = is_numberic.match(value)\n            if result:\n                if type(eval(value)) == int:\n                    value= int(value)\n                else :\n                    value= float(value)\n            new_array.append(value)\n        dictionary[key] = new_array\n    return dictionary\n\ndef is_writeable(path, check_parent=False):\n    \'\'\'\n    Check if a given path is writeable by the current user.\n    :param path: The path to check\n    :param check_parent: If the path to check does not exist, check for the\n    ability to write to the parent directory instead\n    :returns: True or False\n    \'\'\'\n    if os.access(path, os.F_OK) and os.access(path, os.W_OK):\n    # The path exists and is writeable\n        return True\n    if os.access(path, os.F_OK) and not os.access(path, os.W_OK):\n    # The path exists and is not writeable\n        return False\n    # The path does not exists or is not writeable\n    if check_parent is False:\n    # We\'re not allowed to check the parent directory of the provided path\n        return False\n    # Lets get the parent directory of the provided path\n    parent_dir = os.path.dirname(path)\n    if not os.access(parent_dir, os.F_OK):\n    # Parent directory does not exit\n        return False\n    # Finally, return if we\'re allowed to write in the parent directory of the\n    # provided path\n    return os.access(parent_dir, os.W_OK)\ndef is_readable(path):\n    \'\'\'\n    Check if a given path is readable by the current user.\n    :param path: The path to check\n    :returns: True or False\n    \'\'\'\n    if os.access(path, os.F_OK) and os.access(path, os.R_OK):\n    # The path exists and is readable\n        return True\n    # The path does not exist\n    return False\n\n'"
dataloader/Dataset.py,0,"b'# -*- coding: utf-8 -*-\nimport os,urllib\nclass Dataset(object):\n    def __init__(self,opt=None):\n        if opt is not None:\n            self.setup(opt)\n            self.http_proxy= opt.__dict__.get(""proxy"",""null"")\n\n        else:\n            self.name=""demo""\n            self.dirname=""demo""\n            self.http_proxy=""null""\n               \n        self.urls=[]\n        self.root="".data""\n        self.saved_path= os.path.join(os.path.join(self.root,""clean""),self.name)\n        self.formated_files=None\n\n        \n        \n    def setup(self,opt):\n\n        self.name=opt.dataset\n        self.dirname=opt.dataset\n        self.http_proxy= opt.__dict__.get(""proxy"",""null"")\n        \n        \n    def process(self):\n        dirname=self.download()\n        print(""processing dirname: ""+ dirname)\n        raise Exception(""method in father class have been called in processing: {} dataset"".format(opt.dataset))\n        return dirname\n    \n    \n    def getFormatedData(self):\n        \n        if self.formated_files is not None:\n            return self.formated_files\n        \n        if os.path.exists(self.saved_path):\n            return [os.path.join(self.saved_path,filename) for filename in os.listdir(self.saved_path)]\n        self.formated_files = self.process()\n        return self.formated_files\n    \n    def download_from_url(self,url, path, schedule=None):\n        #if schedule is None:\n        #    schedule=lambda a,b,c : print(""%.1f""%(100.0 * a * b / c), end=\'\\r\',flush=True) if (int(a * b / c)*100)%10==0 else None\n        if self.http_proxy != ""null"":\n            proxy = urllib.request.ProxyHandler({\'http\': self.http_proxy,\'https\': self.http_proxy})\n    # construct a new opener using your proxy settings\n            opener = urllib.request.build_opener(proxy)\n    # install the openen on the module-level\n            urllib.request.install_opener(opener)\n            print(""proxy in %s"" % self.http_proxy)\n#        urllib.request.urlretrieve(url,path,lambda a,b,c : print(""%.1f""%(100.0 * a * b / c), end=\'\\r\',flush=True) if (int(a * b / c)*1000)%100==0 else None )a\n        try:\n            urllib.request.urlretrieve(url,path )\n        except:\n            import urllib2\n            urllib2.urlretrieve(url,path )\n        return path\n    \n    def download(self,check=None):\n        """"""Download and unzip an online archive (.zip, .gz, or .tgz).\n    \n        Arguments:\n            check (str or None): Folder whose existence indicates\n                that the dataset has already been downloaded, or\n                None to check the existence of root/{cls.name}.\n    \n        Returns:\n            dataset_path (str): Path to extracted dataset.\n        """"""\n        import zipfile,tarfile\n    \n        path = os.path.join(self.root, self.name)\n        check = path if check is None else check\n        if not os.path.isdir(check):\n            for url in self.urls:\n                if isinstance(url, tuple):\n                    url, filename = url\n                else:\n                    filename = os.path.basename(url)\n                zpath = os.path.join(path, filename)\n                if not os.path.isfile(zpath):\n                    if not os.path.exists(os.path.dirname(zpath)):\n                        os.makedirs(os.path.dirname(zpath))\n                    print(\'downloading {}\'.format(filename))\n                    \n                    self.download_from_url(url, zpath)\n                ext = os.path.splitext(filename)[-1]\n                if ext == \'.zip\':\n                    with zipfile.ZipFile(zpath, \'r\') as zfile:\n                        print(\'extracting\')\n                        zfile.extractall(path)\n                elif ext in [\'.gz\', \'.tgz\',"".bz2""]:\n                    with tarfile.open(zpath, \'r:gz\') as tar:\n                        dirs = [member for member in tar.getmembers()]\n                        tar.extractall(path=path, members=dirs)\n        else:\n            print(""%s do not need to be downloaded"" % path)\n        return path\n    \n\n\n\n'"
dataloader/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\n\nfrom .imdb import IMDBDataset\nfrom .mr import MRDataset\nfrom .glove import Glove\nfrom .sst import SSTDataset\nfrom .ag import AGDataset\n\nfrom .Dataset import Dataset\ndef getDataset(opt):\n    if opt.dataset==""imdb"":\n        dataset = IMDBDataset(opt)\n    elif opt.dataset==""mr"":\n        dataset = MRDataset(opt)\n    elif opt.dataset==""sst"":\n        dataset =SSTDataset(opt)\n    elif opt.dataset == ""ag"":\n        dataset =AGDataset(opt)\n    elif opt.dataset in [""cr"",""mpqa"",""mr"",""sst1"",""sst2"",""subj"",""trec""]:\n        dataset =Dataset(opt)\n\n\n    else:\n        raise Exception(""dataset not supported: {}"".format(opt.dataset))\n    return dataset\n\ndef getEmbedding(opt):\n    if opt.embedding_file.startswith(""glove""):\n        assert len(opt.embedding_file.split(""."")) ==3 , ""embedding_type format wrong""\n        _,corpus,dim=opt.embedding_file.split(""."")\n        return Glove(corpus,dim,opt)\n    else:\n        raise Exception(""embedding not supported: {}"".format(opt.embedding_type))\n\n'"
dataloader/ag.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom .Dataset import Dataset\nimport os\nimport pandas as pd\nimport numpy as np\nfrom codecs import open\n\nclass AGDataset(Dataset):\n    def __init__(self,opt=None,**kwargs):\n        super(AGDataset,self).__init__(opt,**kwargs)\n        self.urls=[\'http://www.di.unipi.it/~gulli/newsSpace.bz2\']\n        \n    \n    def process(self):\n        \n        root=self.download()\n#        root = os.path.join(root,""rt-polaritydata"")\n#        print(""processing into: ""+ root)\n###        root = ""D:\\code\\git\\TextClassificationBenchmark\\.data_waby\\\\imdb\\\\aclImdb""\n#        if not os.path.exists(self.saved_path):\n#            print(""mkdir "" + self.saved_path)\n#            os.makedirs(self.saved_path) # better than os.mkdir\n##            \n#        datas=[]\n#        for polarity in  (""neg"",""pos""):\n#            filename = os.path.join(root,""rt-polarity.""+polarity) \n#            records=[]\n#            with open(filename,encoding=""utf-8"",errors=""replace"") as f:\n#                for i,line in enumerate(f):\n#                    print(i)\n#                    print(line)\n#                    records.append({""text"":line.strip(),""label"": 1 if polarity == ""pos"" else 0})\n#            datas.append(pd.DataFrame(records))\n#        \n#            \n#           \n#        df = pd.concat(datas)\n#        from sklearn.utils import shuffle  \n#        df = shuffle(df).reset_index()\n#        \n#        split_index = [True] * int (len(df) *0.8) + [False] *(len(df)-int (len(df) *0.8))\n##        train=df.sample(frac=0.8)\n#        train = df[split_index]\n#        test = df[~np.array(split_index)]\n#                     \n#        train_filename=os.path.join(self.saved_path,""train.csv"")\n#        test_filename = os.path.join(self.saved_path,""test.csv"")\n#        train[[""text"",""label""]].to_csv(train_filename,encoding=""utf-8"",sep=""\\t"",index=False,header=None)\n#        test[[""text"",""label""]].to_csv(test_filename,encoding=""utf-8"",sep=""\\t"",index=False,header=None)\n#            \n        \n#        \n#        for data_folder in  (""train"",""test""):\n#            data = []  \n#            for polarity in (""pos"",""neg""):\n#                diranme=os.path.join( os.path.join(root,data_folder), polarity)\n#                for rt, dirs, files in os.walk(diranme):\n#                    for f in files:\n#                        filename= os.path.join(rt,f)\n#                        data.append( {""text"": open(filename,encoding=""utf-8"").read().strip(),""label"":int(polarity==""pos"")})\n#            df=pd.DataFrame(data)\n#            saved_filename=os.path.join(self.saved_path,data_folder+"".csv"")\n#            \n#            df[[""text"",""label""]].to_csv(saved_filename,index=False,header=None,sep=""\\t"",encoding=""utf-8"")\n#            print(""finished %s""%saved_filename)\n        print(""processing into formated files over"")        \n        \n#        return [train_filename,test_filename]\n\nif __name__==""__main__"":\n    import opts\n    opt = opts.parse_opt()\n    opt.dataset=""ag""\n    import dataloader\n    dataset= dataloader.getDataset(opt)\n    dataset.process()\n    \n\n    '"
dataloader/glove.py,0,"b'# -*- coding: utf-8 -*-\nimport os\n\nfrom .Dataset import Dataset\nclass Glove(Dataset):\n    def __init__(self,corpus,dim,opt=None,**kwargs):\n        super(Glove,self).__init__(opt,**kwargs)\n\n        self.root = "".vector_cache""\n       \n#        if not os.path.exists(self.root):\n#            os.makedirs(self.root)\n            \n        embeding_urls = {\n                \'42b\': \'http://nlp.stanford.edu/data/glove.42B.300d.zip\',\n                \'840b\': \'http://nlp.stanford.edu/data/glove.840B.300d.zip\',\n                \'twitter.27b\': \'http://nlp.stanford.edu/data/glove.twitter.27B.zip\',\n                \'6b\': \'http://nlp.stanford.edu/data/glove.6B.zip\',\n        }\n        \n\n        self.urls= [ embeding_urls[corpus.lower()] ]\n        print(self.urls)\n        self.name = corpus\n\n    \n    def process(self):\n       \n        root=self.download()\n        \n        return root\n    def getFilename(self):\n        return self.process()\n\nif __name__ ==""__main__"":\n    import opts\n    opt = opts.parse_opt()\n    \n               \n    import dataloader\n    glove=dataloader.getEmbedding(opt)\n    print(glove.getFilename())\n\n    '"
dataloader/imdb.py,0,"b'from .Dataset import Dataset\nimport os\nimport pandas as pd\nfrom codecs import open\n\nclass IMDBDataset(Dataset):\n    def __init__(self,opt=None,**kwargs):\n        super(IMDBDataset,self).__init__(opt,**kwargs)\n        self.urls=[\'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\']\n        \n    \n    def process(self):\n        \n        root=self.download()\n        root = os.path.join(root,""aclImdb"")\n        print(""processing into: ""+ root)\n#        root = ""D:\\code\\git\\TextClassificationBenchmark\\.data_waby\\\\imdb\\\\aclImdb""\n        if not os.path.exists(self.saved_path):\n            print(""mkdir "" + self.saved_path)\n            os.makedirs(self.saved_path) # better than os.mkdir\n            \n        datafiles=[]\n        \n        for data_folder in  (""train"",""test""):\n            data = []  \n            for polarity in (""pos"",""neg""):\n                diranme=os.path.join( os.path.join(root,data_folder), polarity)\n                for rt, dirs, files in os.walk(diranme):\n                    for f in files:\n                        filename= os.path.join(rt,f)\n                        data.append( {""text"": open(filename,encoding=""utf-8"").read().strip(),""label"":int(polarity==""pos"")})\n            df=pd.DataFrame(data)\n            saved_filename=os.path.join(self.saved_path,data_folder+"".csv"")\n            \n            df[[""text"",""label""]].to_csv(saved_filename,index=False,header=None,sep=""\\t"",encoding=""utf-8"")\n            print(""finished %s""%saved_filename)\n            datafiles.append(saved_filename)\n        print(""processing into formated files over"")\n        \n        \n        return datafiles\n    \n                                \n                    \n'"
dataloader/mr.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom .Dataset import Dataset\nimport os\nimport pandas as pd\nimport numpy as np\nfrom codecs import open\n\nclass MRDataset(Dataset):\n    def __init__(self,opt=None,**kwargs):\n        super(MRDataset,self).__init__(opt,**kwargs)\n        self.urls=[\'https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\']\n        \n    \n    def process(self):\n        \n        root=self.download()\n        root = os.path.join(root,""rt-polaritydata"")\n        print(""processing into: ""+ root)\n##        root = ""D:\\code\\git\\TextClassificationBenchmark\\.data_waby\\\\imdb\\\\aclImdb""\n        if not os.path.exists(self.saved_path):\n            print(""mkdir "" + self.saved_path)\n            os.makedirs(self.saved_path) # better than os.mkdir\n#            \n        datas=[]\n        for polarity in  (""neg"",""pos""):\n            filename = os.path.join(root,""rt-polarity.""+polarity) \n            records=[]\n            with open(filename,encoding=""utf-8"",errors=""replace"") as f:\n                for i,line in enumerate(f):\n                    print(i)\n                    print(line)\n                    records.append({""text"":line.strip(),""label"": 1 if polarity == ""pos"" else 0})\n            datas.append(pd.DataFrame(records))\n        \n            \n           \n        df = pd.concat(datas)\n        from sklearn.utils import shuffle  \n        df = shuffle(df).reset_index()\n        \n        split_index = [True] * int (len(df) *0.8) + [False] *(len(df)-int (len(df) *0.8))\n#        train=df.sample(frac=0.8)\n        train = df[split_index]\n        test = df[~np.array(split_index)]\n                     \n        train_filename=os.path.join(self.saved_path,""train.csv"")\n        test_filename = os.path.join(self.saved_path,""test.csv"")\n        train[[""text"",""label""]].to_csv(train_filename,encoding=""utf-8"",sep=""\\t"",index=False,header=None)\n        test[[""text"",""label""]].to_csv(test_filename,encoding=""utf-8"",sep=""\\t"",index=False,header=None)\n            \n        \n#        \n#        for data_folder in  (""train"",""test""):\n#            data = []  \n#            for polarity in (""pos"",""neg""):\n#                diranme=os.path.join( os.path.join(root,data_folder), polarity)\n#                for rt, dirs, files in os.walk(diranme):\n#                    for f in files:\n#                        filename= os.path.join(rt,f)\n#                        data.append( {""text"": open(filename,encoding=""utf-8"").read().strip(),""label"":int(polarity==""pos"")})\n#            df=pd.DataFrame(data)\n#            saved_filename=os.path.join(self.saved_path,data_folder+"".csv"")\n#            \n#            df[[""text"",""label""]].to_csv(saved_filename,index=False,header=None,sep=""\\t"",encoding=""utf-8"")\n#            print(""finished %s""%saved_filename)\n        print(""processing into formated files over"")        \n        \n        return [train_filename,test_filename]\n\nif __name__==""__main__"":\n    import opts\n    opt = opts.parse_opt()\n    opt.dataset=""mr""\n    import dataloader\n    dataset= dataloader.getDataset(opt)\n    dataset.process()\n    \n\n    '"
dataloader/sst.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom .Dataset import Dataset\nimport os\nimport pandas as pd\nimport numpy as np\nfrom codecs import open\n\nclass SSTDataset(Dataset):\n    def __init__(self,opt=None,**kwargs):\n        super(SSTDataset,self).__init__(opt,**kwargs)\n        self.urls=[\'http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\']\n        \n    \n    def process(self):\n        \n        root=self.download()\n        root = os.path.join(root,""rt-polaritydata"")\n        print(""processing into: ""+ root)\n##        root = ""D:\\code\\git\\TextClassificationBenchmark\\.data_waby\\\\imdb\\\\aclImdb""\n        if not os.path.exists(self.saved_path):\n            print(""mkdir "" + self.saved_path)\n            os.makedirs(self.saved_path) # better than os.mkdir\n#            \n        datas=[]\n        for polarity in  (""neg"",""pos""):\n            filename = os.path.join(root,""rt-polarity.""+polarity) \n            records=[]\n            with open(filename,encoding=""utf-8"",errors=""replace"") as f:\n                for i,line in enumerate(f):\n                    print(i)\n                    print(line)\n                    records.append({""text"":line.strip(),""label"": 1 if polarity == ""pos"" else 0})\n            datas.append(pd.DataFrame(records))\n        \n            \n           \n        df = pd.concat(datas)\n        from sklearn.utils import shuffle  \n        df = shuffle(df).reset_index()\n        \n        split_index = [True] * int (len(df) *0.8) + [False] *(len(df)-int (len(df) *0.8))\n#        train=df.sample(frac=0.8)\n        train = df[split_index]\n        test = df[~np.array(split_index)]\n                     \n        train_filename=os.path.join(self.saved_path,""train.csv"")\n        test_filename = os.path.join(self.saved_path,""test.csv"")\n        train[[""text"",""label""]].to_csv(train_filename,encoding=""utf-8"",sep=""\\t"",index=False,header=None)\n        test[[""text"",""label""]].to_csv(test_filename,encoding=""utf-8"",sep=""\\t"",index=False,header=None)\n            \n        \n#        \n#        for data_folder in  (""train"",""test""):\n#            data = []  \n#            for polarity in (""pos"",""neg""):\n#                diranme=os.path.join( os.path.join(root,data_folder), polarity)\n#                for rt, dirs, files in os.walk(diranme):\n#                    for f in files:\n#                        filename= os.path.join(rt,f)\n#                        data.append( {""text"": open(filename,encoding=""utf-8"").read().strip(),""label"":int(polarity==""pos"")})\n#            df=pd.DataFrame(data)\n#            saved_filename=os.path.join(self.saved_path,data_folder+"".csv"")\n#            \n#            df[[""text"",""label""]].to_csv(saved_filename,index=False,header=None,sep=""\\t"",encoding=""utf-8"")\n#            print(""finished %s""%saved_filename)\n        print(""processing into formated files over"")        \n        \n        return [train_filename,test_filename]\n\nif __name__==""__main__"":\n    import opts\n    opt = opts.parse_opt()\n    opt.dataset=""sst""\n    import dataloader\n    dataset= dataloader.getDataset(opt)\n    dataset.process()\n    \n\n    '"
models/BERTFast.py,0,"b'# -*- coding: utf-8 -*-\nimport torch as t\nimport numpy as np\nfrom torch import nn\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\nfrom models.BaseModel import BaseModel\nclass BERTFast(BaseModel): \n    def __init__(self, opt ):\n        super(BERTFast, self).__init__(opt)\n\n        self.bert_model = BertModel.from_pretrained(\'bert-base-uncased\')  \n        for param in self.bert_model.parameters():\n            param.requires_grad=self.opt.bert_trained\n        self.hidden2label = nn.Linear(768, opt.label_size)\n        self.properties.update(\n                {""bert_trained"":self.opt.bert_trained\n                })\n\n\n    def forward(self,  content):\n        encoded, _ = self.bert_model(content)\n        encoded_doc = t.mean(encoded[-1],dim=1)\n        logits = self.hidden2label(encoded_doc)\n        return logits\n\nimport argparse\n\ndef parse_opt():\n    parser = argparse.ArgumentParser()\n    # Data input settings\n    parser.add_argument(\'--hidden_dim\', type=int, default=128,\n                    help=\'hidden_dim\')   \n    \n    \n    parser.add_argument(\'--batch_size\', type=int, default=64,\n                    help=\'batch_size\')\n    parser.add_argument(\'--embedding_dim\', type=int, default=300,\n                    help=\'embedding_dim\')\n    parser.add_argument(\'--learning_rate\', type=float, default=4e-4,\n                    help=\'learning_rate\')\n    parser.add_argument(\'--grad_clip\', type=float, default=1e-1,\n                    help=\'grad_clip\')\n    parser.add_argument(\'--model\', type=str, default=""lstm"",\n                    help=\'model name\')\n    parser.add_argument(\'--label_size\', type=str, default=2,\n                    help=\'label_size\')\n\n\n#\n    args = parser.parse_args()\n    args.embedding_dim=300\n    args.vocab_size=10000\n    args.kernel_size=3\n    args.num_classes=3\n    args.content_dim=256\n    args.max_seq_len=50\n    \n#\n#    # Check if args are valid\n#    assert args.rnn_size > 0, ""rnn_size should be greater than 0""\n\n\n    return args\n \nif __name__ == \'__main__\':\n    \n\n    opt = parse_opt()\n    m = BERTFast(opt)\n    content = t.autograd.Variable(t.arange(0,3200).view(-1,50)).long()\n    o = m(content)\n    print(o.size())\n\n'"
models/BaseModel.py,0,"b'# -*- coding: utf-8 -*-\n\nimport torch as t\n\nimport numpy as np\nfrom torch import nn\nfrom collections import OrderedDict\nimport os\nclass BaseModel(nn.Module):\n    def __init__(self, opt ):\n        super(BaseModel, self).__init__()\n        self.model_name = \'BaseModel\'\n        self.opt=opt\n        \n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n        if opt.__dict__.get(""embeddings"",None) is not None:\n            self.encoder.weight=nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n        self.fc = nn.Linear(opt.embedding_dim, opt.label_size)\n        \n\n        self.properties = {""model_name"":self.__class__.__name__,\n#                ""embedding_dim"":self.opt.embedding_dim,\n#                ""embedding_training"":self.opt.embedding_training,\n#                ""max_seq_len"":self.opt.max_seq_len,\n                ""batch_size"":self.opt.batch_size,\n                ""learning_rate"":self.opt.learning_rate,\n                ""keep_dropout"":self.opt.keep_dropout,\n                }\n \n    def forward(self,content):\n        content_=t.mean(self.encoder(content),dim=1)\n        out=self.fc(content_.view(content_.size(0),-1))\n        return out\n    \n\n    \n    def save(self,save_dir=""saved_model"",metric=None):\n        if not os.path.exists(save_dir):\n            os.mkdir(save_dir)\n        self.model_info = ""__"".join([k+""_""+str(v) if type(v)!=list else k+""_""+str(v)[1:-1].replace("","",""_"").replace("","","""")  for k,v in self.properties.items() ])\n        if metric:\n            path = os.path.join(save_dir, str(metric)[2:] +""_""+ self.model_info)\n        else:\n            path = os.path.join(save_dir,self.model_info)\n        t.save(self,path)\n        return path\n    \n\n        \nif __name__ == \'__main__\':\n    import sys\n    sys.path.append(r"".."")\n    import opts\n    opt=opts.parse_opt()\n    opt.vocab_size=2501\n    opt.embedding_dim=300\n    opt.label_size=3\n    m = BaseModel(opt)\n\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(content)\n    print(o.size())\n    path = m.save()'"
models/BiBloSA.py,0,b'# -*- coding: utf-8 -*-\n\n#https://github.com/galsang/BiBloSA-pytorch/blob/master/model/model.py\n\n'
models/CNN.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom models.BaseModel import BaseModel\nclass CNN(BaseModel):\n    def __init__(self, opt):\n        super(CNN, self).__init__(opt)\n\n        self.embedding_type = opt.embedding_type\n        self.batch_size = opt.batch_size\n        self.max_sent_len = opt.max_sent_len\n        self.embedding_dim = opt.embedding_dim\n        self.vocab_size = opt.vocab_size\n        self.CLASS_SIZE = opt.label_size\n        self.FILTERS = opt[""FILTERS""]\n        self.FILTER_NUM = opt[""FILTER_NUM""]\n        self.keep_dropout = opt.keep_dropout\n        self.IN_CHANNEL = 1\n\n        assert (len(self.FILTERS) == len(self.FILTER_NUM))\n\n        # one for UNK and one for zero padding\n        self.embedding = nn.Embedding(self.vocab_size + 2, self.embedding_dim, padding_idx=self.vocab_size + 1)\n        if self.embedding_type == ""static"" or self.embedding_type == ""non-static"" or self.embedding_type == ""multichannel"":\n            self.WV_MATRIX = opt[""WV_MATRIX""]\n            self.embedding.weight.data.copy_(torch.from_numpy(self.WV_MATRIX))\n            if self.embedding_type == ""static"":\n                self.embedding.weight.requires_grad = False\n            elif self.embedding_type == ""multichannel"":\n                self.embedding2 = nn.Embedding(self.vocab_size + 2, self.embedding_dim, padding_idx=self.VOCAB_SIZE + 1)\n                self.embedding2.weight.data.copy_(torch.from_numpy(self.WV_MATRIX))\n                self.embedding2.weight.requires_grad = False\n                self.IN_CHANNEL = 2\n\n        for i in range(len(self.FILTERS)):\n            conv = nn.Conv1d(self.IN_CHANNEL, self.FILTER_NUM[i], self.embedding_dim * self.FILTERS[i], stride=self.WORD_DIM)\n            setattr(self, \'conv_%d\'%i, conv)\n\n        self.fc = nn.Linear(sum(self.FILTER_NUM), self.label_size)\n        \n        self.properties.update(\n                {""FILTER_NUM"":self.FILTER_NUM,\n                 ""FILTERS"":self.FILTERS,\n                })\n\n    def get_conv(self, i):\n        return getattr(self, \'conv_%d\'%i)\n\n    def forward(self, inp):\n        x = self.embedding(inp).view(-1, 1, self.embedding_dim * self.max_sent_len)\n        if self.embedding_type == ""multichannel"":\n            x2 = self.embedding2(inp).view(-1, 1, self.embedding_dim * self.max_sent_len)\n            x = torch.cat((x, x2), 1)\n\n        conv_results = [\n            F.max_pool1d(F.relu(self.get_conv(i)(x)), self.max_sent_len - self.FILTERS[i] + 1)\n                .view(-1, self.FILTER_NUM[i])\n            for i in range(len(self.FILTERS))]\n\n        x = torch.cat(conv_results, 1)\n        x = F.dropout(x, p=self.keep_dropout, training=self.training)\n        x = self.fc(x)\n        return x\n\n\n\n#https://github.com/zachAlbus/pyTorch-text-classification/blob/master/Yoon/model.py\nfrom models.BaseModel import BaseModel        \nclass  CNN1(BaseModel):\n    \n    def __init__(self, opt):\n        super(CNN1,self).__init__(opt)\n\n        V = opt.vocab_size\n        D = opt.embedding_dim\n        C = opt.label_size\n        Ci = 1\n        Co = opt.kernel_num\n        Ks = opt.kernel_sizes\n\n        self.embed = nn.Embedding(V, D)\n        #self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n        \'\'\'\n        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n        \'\'\'\n        self.dropout = nn.Dropout(opt.dropout)\n        self.fc1 = nn.Linear(len(Ks)*Co, C)\n        self.properties.update(\n                {""kernel_num"":opt.kernel_num,\n                 ""kernel_sizes"":opt.kernel_sizes,\n                })\n\n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3) #(N,Co,W)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n\n\n    def forward(self, x):\n        x = self.embed(x) # (N,W,D)\n        \n        if self.args.static:\n            x = Variable(x)\n\n        x = x.unsqueeze(1) # (N,Ci,W,D)\n\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n\n\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n\n        x = torch.cat(x, 1)\n\n        \'\'\'\n        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n        \'\'\'\n        x = self.dropout(x) # (N,len(Ks)*Co)\n        logit = self.fc1(x) # (N,C)\n        return logit\n\nimport torch.nn as nn\n\n\n#https://github.com/zachAlbus/pyTorch-text-classification/blob/master/Zhang/model.py\nfrom models.BaseModel import BaseModel \nclass CNN2(BaseModel):\n    def __init__(self, opt):\n        super(CNN2, self).__init__(opt)\n        self.embed = nn.Embedding(opt.vocab_size + 1, opt.embedding_dim)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv1d(opt.l0, 256, kernel_size=7, stride=1),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=3)\n        )\n\n        self.conv2 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=7, stride=1),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=3)\n        )\n\n        self.conv3 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n\n        self.conv4 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n\n        self.conv5 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n\n        self.conv6 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=3)\n        )\n\n        self.fc = nn.Linear(256, opt.label_size)\n        self.properties.update(\n                {})\n\n    def forward(self, x_input):\n        # Embedding\n        x = self.embed(x_input)  # dim: (batch_size, max_seq_len, embedding_size)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n\n        # collapse\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return F.log_softmax(x)\n    \nfrom models.BaseModel import BaseModel \nclass CNN3(BaseModel):\n    """"""\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    """"""\n    def __init__(self, args):\n        super(CNN3, self).__init__(opt)\n        self.args = args\n\n        embedding_dim = args.embed_dim\n        embedding_num = args.num_features\n        class_number = args.class_num\n        in_channel = 1\n        out_channel = args.kernel_num\n        kernel_sizes = args.kernel_sizes\n\n        self.embed = nn.Embedding(embedding_num+1, embedding_dim)\n        self.conv = nn.ModuleList([nn.Conv2d(in_channel, out_channel, (K, embedding_dim)) for K in kernel_sizes])\n\n        self.dropout = nn.Dropout(args.dropout)\n        self.fc = nn.Linear(len(kernel_sizes) * out_channel, class_number)\n        self.properties.update(\n                {""kernel_sizes"":kernel_sizes\n                })\n\n    def forward(self, input_x):\n        """"""\n        :param input_x: a list size having the number of batch_size elements with the same length\n        :return: batch_size X num_aspects tensor\n        """"""\n        # Embedding\n        x = self.embed(input_x)  # dim: (batch_size, max_seq_len, embedding_size)\n\n        if self.args.static:\n            x = F.Variable(input_x)\n\n        # Conv & max pool\n        x = x.unsqueeze(1)  # dim: (batch_size, 1, max_seq_len, embedding_size)\n\n        # turns to be a list: [ti : i \\in kernel_sizes] where ti: tensor of dim([batch, num_kernels, max_seq_len-i+1])\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv]\n\n        # dim: [(batch_size, num_kernels), ...]*len(kernel_sizes)\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n        x = torch.cat(x, 1)\n\n        # Dropout & output\n        x = self.dropout(x)  # (batch_size,len(kernel_sizes)*num_kernels)\n        logit = F.log_softmax(self.fc(x))  # (batch_size, num_aspects)\n\n        return logit'"
models/CNNBasic.py,1,"b'# -*- coding: utf-8 -*-\nimport torch as t\nimport numpy as np\nfrom torch import nn\nfrom models.BaseModel import BaseModel\nclass BasicCNN1D(BaseModel): \n    def __init__(self, opt ):\n        super(BasicCNN1D, self).__init__(opt)\n\n        self.content_dim=opt.__dict__.get(""content_dim"",256)\n        self.kernel_size=opt.__dict__.get(""kernel_size"",3)\n\n        \n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n        if opt.__dict__.get(""embeddings"",None) is not None:\n            self.encoder.weight=nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n\n        self.content_conv = nn.Sequential(\n            nn.Conv1d(in_channels = opt.embedding_dim,\n                      out_channels = self.content_dim, #256\n                      kernel_size = self.kernel_size), #3\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size = (opt.max_seq_len - self.kernel_size + 1))\n#            nn.AdaptiveMaxPool1d()\n        )\n        self.fc = nn.Linear(self.content_dim, opt.label_size)\n        self.properties.update(\n                {""content_dim"":self.content_dim,\n                 ""kernel_size"":self.kernel_size,\n                })\n\n    def forward(self,  content):\n\n        content = self.encoder(content) #64x200x300\n        content_out = self.content_conv(content.permute(0,2,1)) #64x256x1\n        reshaped = content_out.view(content_out.size(0), -1) #64x256\n        logits = self.fc(reshaped) #64x3\n        return logits\n\nfrom models.BaseModel import BaseModel\nclass BasicCNN2D(BaseModel):\n    """"""\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    """"""\n    def __init__(self, args):\n        super(BasicCNN2D, self).__init__(opt)\n\n        self.embedding_dim = opt.embedding_dim\n        self.vocab_size = opt.vocab_size\n        self.label_size = opt.label_size\n        self.keep_dropout = opt.keep_dropout\n        in_channel = 1\n        self.kernel_nums = opt.kernel_nums\n        self.kernel_sizes = opt.kernel_sizes\n\n        self.embed = nn.Embedding(self.vocab_size+1, self.embedding_dim)\n        \n        if opt.__dict__.get(""embeddings"",None) is not None:\n            self.embed.weight=nn.Parameter(opt.embeddings)\n            \n        self.conv = nn.ModuleList([nn.Conv2d(in_channel, out_channel, (K, self.embedding_dim)) for K,out_channel in zip(self.kernel_sizes,self.kernel_nums)])\n\n        self.dropout = nn.Dropout(self.keep_dropout)\n        self.fc = nn.Linear(len(self.kernel_sizes) * self.out_channel, self.label_size)\n        \n        self.properties.update(\n                {""kernel_nums"":self.kernel_nums,\n                 ""kernel_sizes"":self.kernel_sizes,\n                })\n\n    def forward(self, input_x):\n        """"""\n        :param input_x: a list size having the number of batch_size elements with the same length\n        :return: batch_size X num_aspects tensor\n        """"""\n        # Embedding\n        x = self.embed(input_x)  # dim: (batch_size, max_seq_len, embedding_size)\n\n        if self.opt.static:\n            x = F.Variable(input_x)\n\n        # Conv & max pool\n        x = x.unsqueeze(1)  # dim: (batch_size, 1, max_seq_len, embedding_size)\n\n        # turns to be a list: [ti : i \\in kernel_sizes] where ti: tensor of dim([batch, num_kernels, max_seq_len-i+1])\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv]\n\n        # dim: [(batch_size, num_kernels), ...]*len(kernel_sizes)\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n        x = torch.cat(x, 1)\n\n        # Dropout & output\n        x = self.dropout(x)  # (batch_size,len(kernel_sizes)*num_kernels)\n        logit = F.log_softmax(self.fc(x))  # (batch_size, num_aspects)\n\n        return logit\nimport argparse\n\ndef parse_opt():\n    parser = argparse.ArgumentParser()\n    # Data input settings\n    parser.add_argument(\'--hidden_dim\', type=int, default=128,\n                    help=\'hidden_dim\')   \n    \n    \n    parser.add_argument(\'--batch_size\', type=int, default=64,\n                    help=\'batch_size\')\n    parser.add_argument(\'--embedding_dim\', type=int, default=300,\n                    help=\'embedding_dim\')\n    parser.add_argument(\'--learning_rate\', type=float, default=4e-4,\n                    help=\'learning_rate\')\n    parser.add_argument(\'--grad_clip\', type=float, default=1e-1,\n                    help=\'grad_clip\')\n    parser.add_argument(\'--model\', type=str, default=""lstm"",\n                    help=\'model name\')\n    parser.add_argument(\'--model\', type=str, default=""lstm"",\n                    help=\'model name\')\n\n\n#\n    args = parser.parse_args()\n    args.embedding_dim=300\n    args.vocab_size=10000\n    args.kernel_size=3\n    args.num_classes=3\n    args.content_dim=256\n    args.max_seq_len=50\n    \n#\n#    # Check if args are valid\n#    assert args.rnn_size > 0, ""rnn_size should be greater than 0""\n\n\n    return args\n \nif __name__ == \'__main__\':    \n\n    opt = parse_opt()\n    m = CNNText(opt)\n    content = t.autograd.Variable(t.arange(0,3200).view(-1,50)).long()\n    o = m(content)\n    print(o.size())\n\n'"
models/CNNInception.py,1,"b'# -*- coding: utf-8 -*-\n\n\nimport torch as t\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom collections import OrderedDict\n\nclass Inception(nn.Module):\n    def __init__(self,cin,co,relu=True,norm=True):\n        super(Inception, self).__init__()\n        assert(co%4==0)\n        cos=[int(co/4)]*4\n        self.activa=nn.Sequential()\n        if norm:self.activa.add_module(\'norm\',nn.BatchNorm1d(co))\n        if relu:self.activa.add_module(\'relu\',nn.ReLU(True))\n        self.branch1 =nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv1d(cin,cos[0], 1,stride=1)),\n            ])) \n        self.branch2 =nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv1d(cin,cos[1], 1)),\n            (\'norm1\', nn.BatchNorm1d(cos[1])),\n            (\'relu1\', nn.ReLU(inplace=True)),\n            (\'conv3\', nn.Conv1d(cos[1],cos[1], 3,stride=1,padding=1)),\n            ]))\n        self.branch3 =nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv1d(cin,cos[2], 3,padding=1)),\n            (\'norm1\', nn.BatchNorm1d(cos[2])),\n            (\'relu1\', nn.ReLU(inplace=True)),\n            (\'conv3\', nn.Conv1d(cos[2],cos[2], 5,stride=1,padding=2)),\n            ]))\n        self.branch4 =nn.Sequential(OrderedDict([\n            #(\'pool\',nn.MaxPool1d(2)),\n            (\'conv3\', nn.Conv1d(cin,cos[3], 3,stride=1,padding=1)),\n            ]))\n    def forward(self,x):\n        branch1=self.branch1(x)\n        branch2=self.branch2(x)\n        branch3=self.branch3(x)\n        branch4=self.branch4(x)\n        result=self.activa(torch.cat((branch1,branch2,branch3,branch4),1))\n        return result\n    \nfrom models.BaseModel import BaseModel\nclass InceptionCNN(BaseModel):\n    def __init__(self, opt ):\n        super(InceptionCNN, self).__init__(opt)   \n        incept_dim=getattr(opt,""inception_dim"",512)\n        self.model_name = \'CNNText_inception\'\n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n\n        self.content_conv=nn.Sequential(\n            Inception(opt.embedding_dim,incept_dim),#(batch_size,64,opt.content_seq_len)->(batch_size,64,(opt.content_seq_len)/2)\n            #Inception(incept_dim,incept_dim),#(batch_size,64,opt.content_seq_len/2)->(batch_size,32,(opt.content_seq_len)/4)\n            Inception(incept_dim,incept_dim),\n            nn.MaxPool1d(opt.max_seq_len)\n        )\n        linear_hidden_size = getattr(opt,""linear_hidden_size"",2000)\n        self.fc = nn.Sequential(\n            nn.Linear(incept_dim,linear_hidden_size),\n            nn.BatchNorm1d(linear_hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Linear(linear_hidden_size ,opt.label_size)\n        )\n        if opt.__dict__.get(""embeddings"",None) is not None:\n            self.encoder.weight=nn.Parameter(opt.embeddings)\n        self.properties.update(\n                {""linear_hidden_size"":linear_hidden_size,\n                 ""incept_dim"":incept_dim,\n                })\n \n    def forward(self,content):\n     \n        content=self.encoder(content)\n        if self.opt.embedding_type==""static"":\n            content=content.detach(0)\n\n        content_out=self.content_conv(content.permute(0,2,1))\n        out=content_out.view(content_out.size(0), -1)\n        out=self.fc(out)\n        return out\n        \nif __name__ == \'__main__\':\n    import sys\n    sys.path.append(r"".."")\n    import opts\n    opt=opts.parse_opt()\n    opt.vocab_size=2501\n    opt.label_size=3\n    m = CNNText_inception(opt)\n\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(content)\n    print(o.size())        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        '"
models/CNNKim.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom models.BaseModel import BaseModel\nclass KIMCNN1D(BaseModel):\n    def __init__(self, opt):\n        super(KIMCNN1D, self).__init__(opt)\n\n        self.embedding_type = opt.embedding_type\n        self.batch_size = opt.batch_size\n        self.max_seq_len = opt.max_seq_len\n        self.embedding_dim = opt.embedding_dim\n        self.vocab_size = opt.vocab_size\n        self.label_size = opt.label_size\n        self.kernel_sizes = opt.kernel_sizes\n        self.kernel_nums = opt.kernel_nums        \n        self.keep_dropout = opt.keep_dropout\n        self.in_channel = 1\n\n        assert (len(self.kernel_sizes) == len(self.kernel_nums))\n\n        # one for UNK and one for zero padding\n        self.embedding = nn.Embedding(self.vocab_size + 2, self.embedding_dim) #, padding_idx=self.vocab_size + 1\n        if self.embedding_type == ""static"" or self.embedding_type == ""non-static"" or self.embedding_type == ""multichannel"":\n            self.embedding.weight=nn.Parameter(opt.embeddings)            \n            if self.embedding_type == ""static"":\n                self.embedding.weight.requires_grad = False\n            elif self.embedding_type == ""multichannel"":\n                self.embedding2 = nn.Embedding(self.vocab_size + 2, self.embedding_dim, padding_idx=self.vocab_size + 1)\n                self.embedding2.weight=nn.Parameter(opt.embeddings) \n                self.embedding2.weight.requires_grad = False\n                self.in_channel = 2\n            else:\n                pass\n#\n#        for i in range(len(self.kernel_sizes)):\n#            conv = nn.Conv1d(self.in_channel, self.kernel_nums[i], self.embedding_dim * self.kernel_sizes[i], stride=self.embedding_dim)\n#            setattr(self, \'conv_%d\'%i, conv)\n        self.convs = nn.ModuleList([nn.Conv1d(self.in_channel, num, self.embedding_dim * size, stride=self.embedding_dim) for size,num in zip(opt.kernel_sizes,opt.kernel_nums)])\n        self.fc = nn.Linear(sum(self.kernel_nums), self.label_size)\n        self.properties.update(\n                {""kernel_sizes"":self.kernel_sizes,\n                 ""kernel_nums"":self.kernel_nums,\n                })\n    def get_conv(self, i):\n        return getattr(self, \'conv_%d\'%i)\n\n    def forward(self, inp):\n        x = self.embedding(inp).view(-1, 1, self.embedding_dim * self.max_seq_len)\n        if self.embedding_type == ""multichannel"":\n            x2 = self.embedding2(inp).view(-1, 1, self.embedding_dim * self.max_seq_len)\n            x = torch.cat((x, x2), 1)\n\n#        conv_results = [\n#            F.max_pool1d(F.relu(self.get_conv(i)(x)), self.max_seq_len - self.kernel_sizes[i] + 1)\n#                .view(-1, self.kernel_nums[i])\n#            for i in range(len(self.kernel_sizes))]\n        conv_results = [\n            F.max_pool1d(F.relu(self.convs[i](x)), self.max_seq_len - self.kernel_sizes[i] + 1)\n                .view(-1, self.kernel_nums[i])\n            for i in range(len(self.convs))]\n\n        x = torch.cat(conv_results, 1)\n        x = F.dropout(x, p=self.keep_dropout, training=self.training)\n        x = self.fc(x)\n        return x\n\n\n\n#https://github.com/zachAlbus/pyTorch-text-classification/blob/master/Yoon/model.py\nclass  KIMCNN2D(nn.Module):\n    \n    def __init__(self, opt):\n        super(KIMCNN2D,self).__init__()\n        self.opt = opt   \n        self.embedding_type = opt.embedding_type\n        self.batch_size = opt.batch_size\n        self.max_seq_len = opt.max_seq_len\n        self.embedding_dim = opt.embedding_dim\n        self.vocab_size = opt.vocab_size\n        self.label_size = opt.label_size\n        self.kernel_sizes = opt.kernel_sizes\n        self.kernel_nums = opt.kernel_nums        \n        self.keep_dropout = opt.keep_dropout\n        \n        self.embedding = nn.Embedding(self.vocab_size + 2, self.embedding_dim) # padding_idx=self.vocab_size + 1\n        if self.embedding_type == ""static"" or self.embedding_type == ""non-static"" or self.embedding_type == ""multichannel"":\n            self.embedding.weight=nn.Parameter(opt.embeddings)            \n            if self.embedding_type == ""static"":\n                self.embedding.weight.requires_grad = False\n            elif self.embedding_type == ""multichannel"":\n                self.embedding2 = nn.Embedding(self.vocab_size + 2, self.embedding_dim, padding_idx=self.vocab_size + 1)\n                self.embedding2.weight=nn.Parameter(opt.embeddings) \n                self.embedding2.weight.requires_grad = False\n                self.in_channel = 2\n            else:\n                pass\n        #self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n        self.convs1 = nn.ModuleList([nn.Conv2d(1, num, (size, opt.embedding_dim)) for size,num in zip(opt.kernel_sizes,opt.kernel_nums)])\n        \'\'\'\n        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n        \'\'\'\n        self.dropout = nn.Dropout(opt.keep_dropout)\n        self.fc = nn.Linear(sum(opt.kernel_nums), opt.label_size)\n\n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3) #(N,Co,W)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n\n\n    def forward(self, x):\n        x = self.embedding(x) # (N,W,D)\n        \n\n\n        x = x.unsqueeze(1) # (N,Ci,W,D)\n\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n\n\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n\n        x = torch.cat(x, 1)\n\n        \'\'\'\n        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n        \'\'\'\n        x = self.dropout(x) # (N,len(Ks)*Co)\n        logit = self.fc(x) # (N,C)\n        return logit\n\nif __name__ == \'__main__\':\n    import sys\n    sys.path.append(r"".."")\n    import opts\n    import torch as t\n    opt=opts.parse_opt()\n    import dataHelper \n    train_iter, test_iter = dataHelper.loadData(opt)\n    m = KIMCNN2D(opt)\n\n\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(content)\n    print(o.size())\n    path = m.save()'"
models/CNNMultiLayer.py,2,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n#https://github.com/zachAlbus/pyTorch-text-classification/blob/master/Zhang/model.py\nfrom models.BaseModel import BaseModel\nclass MultiLayerCNN(BaseModel):\n    def __init__(self, opt):\n        super(MultiLayerCNN, self).__init__(opt)\n        self.embed = nn.Embedding(opt.vocab_size + 1, opt.embedding_dim)\n        \n        if opt.__dict__.get(""embeddings"",None) is not None:\n            self.embed.weight=nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n            \n        self.conv1 = nn.Sequential(\n            nn.Conv1d(opt.max_seq_len, 256, kernel_size=7, stride=1),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=3)\n        )\n\n        self.conv2 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=7, stride=1),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=3)\n        )\n\n        self.conv3 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n\n        self.conv4 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n\n        self.conv5 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n\n        self.conv6 = nn.Sequential(\n            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=3, stride=3)\n        )\n\n        self.fc = nn.Linear(256*7, opt.label_size)\n\n    def forward(self, x):\n        # Embedding\n        x = self.embed(x)  # dim: (batch_size, max_seq_len, embedding_size)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n\n        # collapse\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return F.log_softmax(x)\n'"
models/CNNText.py,0,"b'# -*- coding: utf-8 -*-\nimport torch as t\nimport numpy as np\nfrom torch import nn\nfrom models.BaseModel import BaseModel\nclass CNNText(BaseModel): \n    def __init__(self, opt ):\n        super(CNNText, self).__init__(opt)\n\n\n        self.content_dim=opt.__dict__.get(""content_dim"",256)\n        self.kernel_size=opt.__dict__.get(""kernel_size"",3)\n\n        \n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n        if opt.__dict__.get(""embeddings"",None) is not None:\n            self.encoder.weight=nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n\n\n        self.content_conv = nn.Sequential(\n            nn.Conv1d(in_channels = opt.embedding_dim,\n                      out_channels = self.content_dim,\n                      kernel_size = self.kernel_size),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size = (opt.max_seq_len - self.kernel_size + 1))\n#            nn.AdaptiveMaxPool1d()\n        )\n\n        self.fc = nn.Linear(self.content_dim, opt.label_size)\n        self.properties.update(\n                {""content_dim"":self.content_dim,\n                 ""kernel_size"":self.kernel_size,\n                })\n\n    def forward(self,  content):\n\n        content = self.encoder(content)\n        content_out = self.content_conv(content.permute(0,2,1))\n        reshaped = content_out.view(content_out.size(0), -1)\n        logits = self.fc(reshaped)\n        return logits\n\nimport argparse\n\ndef parse_opt():\n    parser = argparse.ArgumentParser()\n    # Data input settings\n    parser.add_argument(\'--hidden_dim\', type=int, default=128,\n                    help=\'hidden_dim\')   \n    \n    \n    parser.add_argument(\'--batch_size\', type=int, default=64,\n                    help=\'batch_size\')\n    parser.add_argument(\'--embedding_dim\', type=int, default=300,\n                    help=\'embedding_dim\')\n    parser.add_argument(\'--learning_rate\', type=float, default=4e-4,\n                    help=\'learning_rate\')\n    parser.add_argument(\'--grad_clip\', type=float, default=1e-1,\n                    help=\'grad_clip\')\n    parser.add_argument(\'--model\', type=str, default=""lstm"",\n                    help=\'model name\')\n\n\n#\n    args = parser.parse_args()\n    args.embedding_dim=300\n    args.vocab_size=10000\n    args.kernel_size=3\n    args.num_classes=3\n    args.content_dim=256\n    args.max_seq_len=50\n    \n#\n#    # Check if args are valid\n#    assert args.rnn_size > 0, ""rnn_size should be greater than 0""\n\n\n    return args\n \nif __name__ == \'__main__\':\n    \n\n    opt = parse_opt()\n    m = CNNText(opt)\n    content = t.autograd.Variable(t.arange(0,3200).view(-1,50)).long()\n    o = m(content)\n    print(o.size())\n\n'"
models/CNN_Inception.py,1,"b'# -*- coding: utf-8 -*-\n\n\nimport torch as t\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom collections import OrderedDict\nfrom models.BaseModel import BaseModel\nclass Inception(nn.Module):\n    def __init__(self,cin,co,relu=True,norm=True):\n        super(Inception, self).__init__()\n        assert(co%4==0)\n        cos=[co/4]*4\n        self.activa=nn.Sequential()\n        if norm:self.activa.add_module(\'norm\',nn.BatchNorm1d(co))\n        if relu:self.activa.add_module(\'relu\',nn.ReLU(True))\n        self.branch1 =nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv1d(cin,cos[0], 1,stride=1)),\n            ])) \n        self.branch2 =nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv1d(cin,cos[1], 1)),\n            (\'norm1\', nn.BatchNorm1d(cos[1])),\n            (\'relu1\', nn.ReLU(inplace=True)),\n            (\'conv3\', nn.Conv1d(cos[1],cos[1], 3,stride=1,padding=1)),\n            ]))\n        self.branch3 =nn.Sequential(OrderedDict([\n            (\'conv1\', nn.Conv1d(cin,cos[2], 3,padding=1)),\n            (\'norm1\', nn.BatchNorm1d(cos[2])),\n            (\'relu1\', nn.ReLU(inplace=True)),\n            (\'conv3\', nn.Conv1d(cos[2],cos[2], 5,stride=1,padding=2)),\n            ]))\n        self.branch4 =nn.Sequential(OrderedDict([\n            #(\'pool\',nn.MaxPool1d(2)),\n            (\'conv3\', nn.Conv1d(cin,cos[3], 3,stride=1,padding=1)),\n            ]))\n    def forward(self,x):\n        branch1=self.branch1(x)\n        branch2=self.branch2(x)\n        branch3=self.branch3(x)\n        branch4=self.branch4(x)\n        result=self.activa(torch.cat((branch1,branch2,branch3,branch4),1))\n        return result\nclass CNNText_inception(BaseModel):\n    def __init__(self, opt ):\n        super(CNNText_inception, self).__init__(opt)   \n        incept_dim=getattr(opt,""inception_dim"",512)\n\n\n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n\n        self.content_conv=nn.Sequential(\n            Inception(opt.embedding_dim,incept_dim),#(batch_size,64,opt.content_seq_len)->(batch_size,64,(opt.content_seq_len)/2)\n            #Inception(incept_dim,incept_dim),#(batch_size,64,opt.content_seq_len/2)->(batch_size,32,(opt.content_seq_len)/4)\n            Inception(incept_dim,incept_dim),\n            nn.MaxPool1d(opt.max_seq_len)\n        )\n        opt.hidden_size = getattr(opt,""linear_hidden_size"",2000)\n        self.fc = nn.Sequential(\n            nn.Linear(incept_dim,opt.hidden_size),\n            nn.BatchNorm1d(opt.hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Linear(opt.hidden_size ,opt.label_size)\n        )\n        if opt.__dict__.get(""embeddings"",None) is not None:\n            print(\'load embedding\')\n            self.encoder.weight.data.copy_(t.from_numpy(opt.embeddings))\n        self.properties.update(\n                {""inception_dim"":incept_dim,\n                 ""hidden_size"":opt.hidden_size,\n                })\n \n    def forward(self,content):\n     \n        content=self.encoder(content)\n        if self.opt.static:\n            content=content.detach(0)\n\n        content_out=self.content_conv(content.permute(0,2,1))\n        out=content_out.view(content_out.size(0), -1)\n        out=self.fc(out)\n        return out\n        \nif __name__ == \'__main__\':\n    import sys\n    sys.path.append(r"".."")\n    import opts\n    opt=opts.parse_opt()\n    opt.vocab_size=2501\n    opt.label_size=3\n    m = CNNText_inception(opt)\n\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(content)\n    print(o.size())        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        '"
models/Capsule.py,14,"b'# -*- coding: utf-8 -*-\n# paper \n\n\n#\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport numpy as np\n\nBATCH_SIZE = 100\n\nNUM_EPOCHS = 500\nNUM_ROUTING_ITERATIONS = 3\n\ncuda = torch.cuda.is_available()\n\ndef softmax(input, dim=1):\n    transposed_input = input.transpose(dim, len(input.size()) - 1)\n    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)))\n    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)\n\n\n\n\n\nclass CapsuleLayer(nn.Module):\n    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels, kernel_size=None, stride=None,\n                 num_iterations=NUM_ROUTING_ITERATIONS,padding=0):\n        super(CapsuleLayer, self).__init__()\n\n        self.num_route_nodes = num_route_nodes\n        self.num_iterations = num_iterations\n\n        self.num_capsules = num_capsules\n        \n\n        \n        if num_route_nodes != -1:\n            self.route_weights = nn.Parameter(torch.randn(num_capsules, num_route_nodes, in_channels, out_channels))\n        else:\n            prime=[3,5,7,9,11,13,17,19,23]\n            sizes=prime[:self.num_capsules]\n            self.capsules = nn.ModuleList(\n                [nn.Conv1d(in_channels, out_channels, kernel_size=i, stride=2, padding=int((i-1)/2)) for i in sizes])\n\n    def squash(self, tensor, dim=-1):\n        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n        scale = squared_norm / (1 + squared_norm)\n        return scale * tensor / torch.sqrt(squared_norm)\n\n    def forward(self, x):\n        \n        if self.num_route_nodes != -1:\n            priors =torch.matmul( x[None, :, :, None, :],self.route_weights[:, None, :, :, :])\n\n            if torch.cuda.is_available():\n                logits = torch.autograd.Variable(torch.zeros(priors.size())).cuda()\n            else:\n                logits = torch.autograd.Variable(torch.zeros(priors.size()))\n            for i in range(self.num_iterations):\n                probs = softmax(logits, dim=2)\n                outputs = self.squash((torch.mul(probs , priors)).sum(dim=2, keepdim=True))\n\n                if i != self.num_iterations - 1:\n                    delta_logits = (torch.mul(priors , outputs)).sum(dim=-1, keepdim=True)\n                    logits = logits + delta_logits\n        else:\n            outputs = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n            outputs = torch.cat(outputs, dim=-1)\n            outputs = self.squash(outputs)\n\n        return outputs\n\nfrom models.BaseModel import BaseModel\nclass CapsuleNet(BaseModel):\n    def __init__(self,opt):\n        super(CapsuleNet, self).__init__(opt)\n\n        self.label_size=opt.label_size\n        self.embed = nn.Embedding(opt.vocab_size+1, opt.embedding_dim)\n        self.opt.cnn_dim = 1\n        self.kernel_size = 3\n        self.kernel_size_primary=3\n        if opt.__dict__.get(""embeddings"",None) is not None:\n            self.embed.weight=nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n\n        self.primary_capsules = CapsuleLayer(num_capsules=8, num_route_nodes=-1, in_channels=256, out_channels=32)\n        self.digit_capsules = CapsuleLayer(num_capsules=opt.label_size, num_route_nodes=int(32 * opt.max_seq_len/2), in_channels=8,\n                                           out_channels=16)\n        if self.opt.cnn_dim == 2:\n            self.conv_2d = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=(self.kernel_size,opt.embedding_dim), stride=(1,opt.embedding_dim),padding=(int((self.kernel_size-1)/2),0))\n        else:\n            self.conv_1d = nn.Conv1d(in_channels=1, out_channels=256, kernel_size=opt.embedding_dim * self.kernel_size, stride=opt.embedding_dim, padding=opt.embedding_dim* int((self.kernel_size-1)/2) )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(16 * self.label_size, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 784),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x, y=None,reconstruct=False):\n        #x = next(iter(train_iter)).text[0]\n\n        x= self.embed(x)\n        if self.opt.cnn_dim == 1:\n            x=x.view(x.size(0),1,x.size(-1)*x.size(-2))                \n            x_conv = F.relu(self.conv_1d(x), inplace=True)\n        else:\n            \n            x=x.unsqueeze(1)        \n            x_conv = F.relu(self.conv_2d(x), inplace=True).squeeze(3)\n\n        x = self.primary_capsules(x_conv)\n        x = self.digit_capsules(x).squeeze().transpose(0, 1)\n\n        classes = (x ** 2).sum(dim=-1) ** 0.5\n        classes = F.softmax(classes)\n        if not reconstruct:\n            return classes\n        if y is None:\n            # In all batches, get the most active capsule.\n            _, max_length_indices = classes.max(dim=1)\n            if torch.cuda.is_available():\n                y = Variable(torch.sparse.torch.eye(self.label_size)).cuda().index_select(dim=0, index=max_length_indices.data)\n            else:\n                y = Variable(torch.sparse.torch.eye(self.label_size)).index_select(dim=0, index=max_length_indices.data)\n        reconstructions = self.decoder((x * y[:, :, None]).view(x.size(0), -1))\n\n        return classes, reconstructions\n'"
models/ConvS2S.py,0,b'# -*- coding: utf-8 -*-\n\n'
models/DiSAN.py,0,b'# -*- coding: utf-8 -*-\n\n# https://github.com/taoshen58/DiSAN/blob/master/SST_disan/src/model/model_disan.py'
models/FastText.py,0,"b'# -*- coding: utf-8 -*-\n\nimport torch as t\n\nimport numpy as np\nfrom torch import nn\nfrom collections import OrderedDict\nfrom models.BaseModel import BaseModel\nclass FastText(BaseModel):\n    def __init__(self, opt ):\n        super(FastText, self).__init__(opt)\n        \n        linear_hidden_size=getattr(opt,""linear_hidden_size"",2000)\n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n        if opt.__dict__.get(""embeddings"",None) is not None:\n            print(\'load embedding\')\n            self.encoder.weight=nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n        \n        \n        self.content_fc = nn.Sequential(\n            nn.Linear(opt.embedding_dim,linear_hidden_size),\n            nn.BatchNorm1d(linear_hidden_size),\n            nn.ReLU(inplace=True),\n            # nn.Linear(opt.linear_hidden_size,opt.linear_hidden_size),\n            # nn.BatchNorm1d(opt.linear_hidden_size),\n            # nn.ReLU(inplace=True),\n            nn.Linear(linear_hidden_size,opt.label_size)\n        )\n#        self.fc = nn.Linear(300, opt.label_size)\n        self.properties.update(\n                {""linear_hidden_size"":linear_hidden_size\n                })\n \n    def forward(self,content):\n       \n        content_=t.mean(self.encoder(content),dim=1)\n\n\n        out=self.content_fc(content_.view(content_.size(0),-1))\n\n        return out\nif __name__ == \'__main__\':\n    import sys\n    sys.path.append(r"".."")\n    import opts\n    opt=opts.parse_opt()\n    opt.vocab_size=2501\n    opt.label_size=3\n    m = FastText(opt)\n\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(content)\n    print(o.size())'"
models/LSTM.py,16,"b'# -*- coding: utf-8 -*-\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.autograd import Variable\n#from memory_profiler import profile\nfrom models.BaseModel import BaseModel\nclass LSTMClassifier(BaseModel):\n    # embedding_dim, hidden_dim, vocab_size, label_size, batch_size, use_gpu\n    def __init__(self,opt):\n\n        super(LSTMClassifier, self).__init__(opt)\n        self.hidden_dim = opt.hidden_dim\n        self.batch_size = opt.batch_size\n        self.use_gpu = torch.cuda.is_available()\n\n        self.word_embeddings = nn.Embedding(opt.vocab_size, opt.embedding_dim)\n        self.word_embeddings.weight = nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))\n        self.lstm = nn.LSTM(opt.embedding_dim, opt.hidden_dim)\n        self.hidden2label = nn.Linear(opt.hidden_dim, opt.label_size)\n        self.hidden = self.init_hidden()\n        self.lsmt_reduce_by_mean = opt.__dict__.get(""lstm_mean"",True) \n\n    def init_hidden(self,batch_size=None):\n        if batch_size is None:\n            batch_size= self.batch_size\n        \n        if self.use_gpu:\n            h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda())\n            c0 = Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda())\n        else:\n            h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim))\n            c0 = Variable(torch.zeros(1,batch_size, self.hidden_dim))\n        return (h0, c0)\n#    @profile\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence) #64x200x300\n\n#        x = embeds.view(sentence.size()[1], self.batch_size, -1)\n        x=embeds.permute(1,0,2) #200x64x300\n        self.hidden= self.init_hidden(sentence.size()[0]) #1x64x128\n        lstm_out, self.hidden = self.lstm(x, self.hidden) #200x64x128\n        if self.lsmt_reduce_by_mean==""mean"":\n            out = lstm_out.permute(1,0,2)\n            final = torch.mean(out,1)\n        else:\n            final=lstm_out[-1]\n        y  = self.hidden2label(final)  #64x3\n        return y\n\n#    def forward1(self, sentence):\n#       \n#        return torch.zeros(sentence.size()[0], self.opt.label_size)\n##    def __call__(self, **args):\n##        self.forward(args)\n#    def test():\n#        \n#        import numpy as np\n#        \n#        word_embeddings = nn.Embedding(10000, 300)\n#        lstm = nn.LSTM(300, 100)\n#        h0 = Variable(torch.zeros(1, 128, 100))\n#        c0 = Variable(torch.zeros(1, 128, 100))\n#        hidden=(h0, c0)\n#        sentence = Variable(torch.LongTensor(np.zeros((128,30),dtype=np.int64)))\n#        embeds = word_embeddings(sentence)\n#        torch.tile(sentence)\n#        sentence.size()[0]\n#       \n#        \n#        \n##        x= Variable(torch.zeros(30, 128, 300))\n#        x = embeds.view(sentence.size()[1], self.batch_size, -1)\n#        embeds=embeds.permute(1,0,2)\n#        lstm_out, hidden = lstm(embeds, hidden)\n##                  '"
models/LSTMBI.py,10,"b'# -*- coding: utf-8 -*-\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.autograd import Variable\n#from memory_profiler import profile\nfrom models.BaseModel import BaseModel\nclass LSTMBI(BaseModel):\n    # embedding_dim, hidden_dim, vocab_size, label_size, batch_size, use_gpu\n    def __init__(self,opt):\n        super(LSTMBI, self).__init__(opt)\n\n\n        self.word_embeddings = nn.Embedding(opt.vocab_size, opt.embedding_dim)\n        self.word_embeddings.weight = nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))\n  \n\n        #self.bidirectional = True\n\n        self.bilstm = nn.LSTM(opt.embedding_dim, opt.hidden_dim // 2, num_layers=self.opt.lstm_layers, dropout=self.opt.keep_dropout, bidirectional=self.opt.bidirectional)\n        self.hidden2label = nn.Linear(opt.hidden_dim, opt.label_size)\n        self.hidden = self.init_hidden()\n        self.lsmt_reduce_by_mean = opt.__dict__.get(""lstm_mean"",True) \n        \n        \n        self.properties.update(\n                {""hidden_dim"":self.opt.hidden_dim,\n                 ""lstm_mean"":self.lsmt_reduce_by_mean,\n                 ""lstm_layers"":self.opt.lstm_layers,\n#                 ""bidirectional"":str(self.opt.bidirectional)\n                })\n\n    def init_hidden(self,batch_size=None):\n        if batch_size is None:\n            batch_size= self.opt.batch_size\n        \n        if torch.cuda.is_available():\n            h0 = Variable(torch.zeros(2*self.opt.lstm_layers, batch_size, self.opt.hidden_dim // 2).cuda())\n            c0 = Variable(torch.zeros(2*self.opt.lstm_layers, batch_size, self.opt.hidden_dim // 2).cuda())\n        else:\n            h0 = Variable(torch.zeros(2*self.opt.lstm_layers, batch_size, self.opt.hidden_dim // 2))\n            c0 = Variable(torch.zeros(2*self.opt.lstm_layers, batch_size, self.opt.hidden_dim // 2))\n        return (h0, c0)\n#    @profile\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n\n#        x = embeds.view(sentence.size()[1], self.batch_size, -1)\n        x=embeds.permute(1,0,2)  # we do this because the default parameter of lstm is False \n        self.hidden= self.init_hidden(sentence.size()[0]) #2x64x64\n        lstm_out, self.hidden = self.bilstm(x, self.hidden)  #lstm_out:200x64x128\n        if self.lsmt_reduce_by_mean==""mean"":\n            out = lstm_out.permute(1,0,2)\n            final = torch.mean(out,1)\n        else:\n            final=lstm_out[-1]\n        y  = self.hidden2label(final) #64x3  #lstm_out[-1]\n        return y\n\n'"
models/LSTMStack.py,0,b''
models/LSTMTree.py,0,b'# -*- coding: utf-8 -*-\n\n# https://github.com/dasguptar/treelstm.pytorch'
models/LSTMwithAttention.py,14,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom sklearn.utils import shuffle\nfrom torch.autograd import Variable\nfrom models.BaseModel import BaseModel\nclass LSTMAttention(torch.nn.Module):\n    def __init__(self,opt):\n\n        super(LSTMAttention, self).__init__()\n        self.hidden_dim = opt.hidden_dim\n        self.batch_size = opt.batch_size\n        self.use_gpu = torch.cuda.is_available()\n\n        self.word_embeddings = nn.Embedding(opt.vocab_size, opt.embedding_dim)\n        self.word_embeddings.weight = nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))\n  \n        self.num_layers = opt.lstm_layers\n        #self.bidirectional = True\n        self.dropout = opt.keep_dropout\n        self.bilstm = nn.LSTM(opt.embedding_dim, opt.hidden_dim // 2, batch_first=True,num_layers=self.num_layers, dropout=self.dropout, bidirectional=True)\n        self.hidden2label = nn.Linear(opt.hidden_dim, opt.label_size)\n        self.hidden = self.init_hidden()\n        self.mean = opt.__dict__.get(""lstm_mean"",True) \n        self.attn_fc = torch.nn.Linear(opt.embedding_dim, 1)\n    def init_hidden(self,batch_size=None):\n        if batch_size is None:\n            batch_size= self.batch_size\n        \n        if self.use_gpu:\n            h0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2).cuda())\n            c0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2).cuda())\n        else:\n            h0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2))\n            c0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2))\n        return (h0, c0)\n\n\n    def attention(self, rnn_out, state):\n        merged_state = torch.cat([s for s in state],1)\n        merged_state = merged_state.squeeze(0).unsqueeze(2)\n        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n        weights = torch.bmm(rnn_out, merged_state)\n        weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n    # end method attention\n    \n\n    def forward(self, X):\n        embedded = self.word_embeddings(X)\n        hidden= self.init_hidden(X.size()[0]) #\n        rnn_out, hidden = self.bilstm(embedded, hidden)\n        h_n, c_n = hidden\n        attn_out = self.attention(rnn_out, h_n)\n        logits = self.hidden2label(attn_out)\n        return logits'"
models/MLP.py,10,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\n# https://github.com/nmhkahn/MemN2N-pytorch/blob/master/memn2n/model.py\n\ndef position_encoding(sentence_size, embedding_dim):\n    encoding = np.ones((embedding_dim, sentence_size), dtype=np.float32)\n    ls = sentence_size + 1\n    le = embedding_dim + 1\n    for i in range(1, le):\n        for j in range(1, ls):\n            encoding[i-1, j-1] = (i - (embedding_dim+1)/2) * (j - (sentence_size+1)/2)\n    encoding = 1 + 4 * encoding / embedding_dim / sentence_size\n    # Make position encoding of time words identity to avoid modifying them\n    encoding[:, -1] = 1.0\n    return np.transpose(encoding)\n\nclass AttrProxy(object):\n    """"""\n    Translates index lookups into attribute lookups.\n    To implement some trick which able to use list of nn.Module in a nn.Module\n    see https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219/2\n    """"""\n    def __init__(self, module, prefix):\n        self.module = module\n        self.prefix = prefix\n\n    def __getitem__(self, i):\n        return getattr(self.module, self.prefix + str(i))\n\n\nclass MemN2N(nn.Module):\n    def __init__(self, opt):\n        super(MemN2N, self).__init__()\n\n        use_cuda = opt[""use_cuda""]\n        num_vocab = opt[""num_vocab""]\n        embedding_dim = opt[""embedding_dim""]\n        sentence_size = opt[""sentence_size""]\n        self.max_hops = opt[""max_hops""]\n\n        for hop in range(self.max_hops+1):\n            C = nn.Embedding(num_vocab, embedding_dim, padding_idx=0)\n            C.weight.data.normal_(0, 0.1)\n            self.add_module(""C_{}"".format(hop), C)\n        self.C = AttrProxy(self, ""C_"")\n\n        self.softmax = nn.Softmax()\n        self.encoding = Variable(torch.FloatTensor(\n            position_encoding(sentence_size, embedding_dim)), requires_grad=False)\n\n        if use_cuda:\n            self.encoding = self.encoding.cuda()\n\n    def forward(self, story, query):\n        story_size = story.size()\n\n        u = list()\n        query_embed = self.C[0](query)\n        # weired way to perform reduce_dot\n        encoding = self.encoding.unsqueeze(0).expand_as(query_embed)\n        u.append(torch.sum(query_embed*encoding, 1))\n        \n        for hop in range(self.max_hops):\n            embed_A = self.C[hop](story.view(story.size(0), -1))\n            embed_A = embed_A.view(story_size+(embed_A.size(-1),))\n       \n            encoding = self.encoding.unsqueeze(0).unsqueeze(1).expand_as(embed_A)\n            m_A = torch.sum(embed_A*encoding, 2)\n       \n            u_temp = u[-1].unsqueeze(1).expand_as(m_A)\n            prob   = self.softmax(torch.sum(m_A*u_temp, 2))\n        \n            embed_C = self.C[hop+1](story.view(story.size(0), -1))\n            embed_C = embed_C.view(story_size+(embed_C.size(-1),))\n            m_C     = torch.sum(embed_C*encoding, 2)\n       \n            prob = prob.unsqueeze(2).expand_as(m_C)\n            o_k  = torch.sum(m_C*prob, 1)\n       \n            u_k = u[-1] + o_k\n            u.append(u_k)\n       \n        a_hat = u[-1]@self.C[self.max_hops].weight.transpose(0, 1)\n        return a_hat, self.softmax(a_hat)'"
models/MemoryNetwork.py,10,"b'# -*- coding: utf-8 -*-\n#https://github.com/nmhkahn/MemN2N-pytorch/blob/master/memn2n/model.py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\ndef position_encoding(sentence_size, embedding_dim):\n    encoding = np.ones((embedding_dim, sentence_size), dtype=np.float32)\n    ls = sentence_size + 1\n    le = embedding_dim + 1\n    for i in range(1, le):\n        for j in range(1, ls):\n            encoding[i-1, j-1] = (i - (embedding_dim+1)/2) * (j - (sentence_size+1)/2)\n    encoding = 1 + 4 * encoding / embedding_dim / sentence_size\n    # Make position encoding of time words identity to avoid modifying them\n    encoding[:, -1] = 1.0\n    return np.transpose(encoding)\n\nclass AttrProxy(object):\n    """"""\n    Translates index lookups into attribute lookups.\n    To implement some trick which able to use list of nn.Module in a nn.Module\n    see https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219/2\n    """"""\n    def __init__(self, module, prefix):\n        self.module = module\n        self.prefix = prefix\n\n    def __getitem__(self, i):\n        return getattr(self.module, self.prefix + str(i))\n\n\nclass MemN2N(nn.Module):\n    def __init__(self, settings):\n        super(MemN2N, self).__init__()\n\n        use_cuda = settings[""use_cuda""]\n        num_vocab = settings[""num_vocab""]\n        embedding_dim = settings[""embedding_dim""]\n        sentence_size = settings[""sentence_size""]\n        self.max_hops = settings[""max_hops""]\n\n        for hop in range(self.max_hops+1):\n            C = nn.Embedding(num_vocab, embedding_dim, padding_idx=0)\n            C.weight.data.normal_(0, 0.1)\n            self.add_module(""C_{}"".format(hop), C)\n        self.C = AttrProxy(self, ""C_"")\n\n        self.softmax = nn.Softmax()\n        self.encoding = Variable(torch.FloatTensor(\n            position_encoding(sentence_size, embedding_dim)), requires_grad=False)\n\n        if use_cuda:\n            self.encoding = self.encoding.cuda()\n\n    def forward(self,  query):\n        \n        story=query  # for text classfication\n        \n        story_size = story.size()\n\n        u = list()\n        query_embed = self.C[0](query)\n        # weired way to perform reduce_dot\n        encoding = self.encoding.unsqueeze(0).expand_as(query_embed)\n        u.append(torch.sum(query_embed*encoding, 1))\n        \n        for hop in range(self.max_hops):\n            embed_A = self.C[hop](story.view(story.size(0), -1))\n            embed_A = embed_A.view(story_size+(embed_A.size(-1),))\n       \n            encoding = self.encoding.unsqueeze(0).unsqueeze(1).expand_as(embed_A)\n            m_A = torch.sum(embed_A*encoding, 2)\n       \n            u_temp = u[-1].unsqueeze(1).expand_as(m_A)\n            prob   = self.softmax(torch.sum(m_A*u_temp, 2))\n        \n            embed_C = self.C[hop+1](story.view(story.size(0), -1))\n            embed_C = embed_C.view(story_size+(embed_C.size(-1),))\n            m_C     = torch.sum(embed_C*encoding, 2)\n       \n            prob = prob.unsqueeze(2).expand_as(m_C)\n            o_k  = torch.sum(m_C*prob, 1)\n       \n            u_k = u[-1] + o_k\n            u.append(u_k)\n       \n        a_hat = u[-1]@self.C[self.max_hops].weight.transpose(0, 1)\n        return a_hat, self.softmax(a_hat)'"
models/QuantumCNN.py,0,b'# -*- coding: utf-8 -*-\n\n'
models/RCNN.py,11,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.autograd import Variable\n#from memory_profiler import profile\n\n""""""\nLai S, Xu L, Liu K, et al. Recurrent Convolutional Neural Networks for Text Classification[C]//AAAI. 2015, 333: 2267-2273.\n""""""\nfrom models.BaseModel import BaseModel\nclass RCNN(BaseModel):\n    # embedding_dim, hidden_dim, vocab_size, label_size, batch_size, use_gpu\n    def __init__(self,opt):\n\n        super(RCNN, self).__init__(opt)\n        self.hidden_dim = opt.hidden_dim\n        self.batch_size = opt.batch_size\n        self.use_gpu = torch.cuda.is_available()\n\n        self.word_embeddings = nn.Embedding(opt.vocab_size, opt.embedding_dim)\n        self.word_embeddings.weight = nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))\n        \n        self.num_layers = 1\n        #self.bidirectional = True\n        self.dropout = opt.keep_dropout\n        self.bilstm = nn.LSTM(input_size=opt.embedding_dim, hidden_size=opt.hidden_dim // 2, num_layers=self.num_layers, dropout=self.dropout, bidirectional=True)\n\n        ###self.hidden2label = nn.Linear(opt.hidden_dim, opt.label_size)\n        self.hidden = self.init_hidden()\n        \n        self.max_pooling = nn.MaxPool1d(kernel_size=3, stride=2)\n        \n        self.content_dim = 256\n        #self.conv =  nn.Conv1d(opt.hidden_dim, self.content_dim, opt.hidden_dim * 2, stride=opt.embedding_dim)\n        self.hidden2label = nn.Linear( (2*opt.hidden_dim // 2+opt.embedding_dim), opt.label_size)\n        \n\n    def init_hidden(self,batch_size=None):\n        if batch_size is None:\n            batch_size= self.batch_size\n        \n        if self.use_gpu:\n            h0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2).cuda())\n            c0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2).cuda())\n        else:\n            h0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2))\n            c0 = Variable(torch.zeros(2*self.num_layers,batch_size, self.hidden_dim // 2))\n        return (h0, c0)\n#    @profile\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence) #64x200x300\n\n#        x = embeds.view(sentence.size()[1], self.batch_size, -1)\n        x=embeds.permute(1,0,2) #200x64x300\n        self.hidden= self.init_hidden(sentence.size()[0]) #2x64x128\n        lstm_out, self.hidden = self.bilstm(x, self.hidden) ###input (seq_len, batch, input_size) #Outupts:output, (h_n, c_n) output:(seq_len, batch, hidden_size * num_directions)\n        #lstm_out 200x64x128\n        \n        c_lr =  lstm_out.permute(1,0,2) #64x200x128\n        xi = torch.cat((c_lr[:,:,0:int(c_lr.size()[2]/2)],embeds,c_lr[:,:,int(c_lr.size()[2]/2):]),2) #64x200x428\n        yi = torch.tanh(xi.permute(0,2,1)) #64x428x200\n        y = self.max_pooling(yi) #64x428x99\n        y = y.permute(2,0,1)\n        \n        ##y = self.conv(lstm_out.permute(1,2,0)) ###64x256x1\n        \n        y  = self.hidden2label(y[-1])\n        #y  = self.hidden2label(y[:,-1,:].view(y[:,-1,:].size()[0],-1)) \n        return y'"
models/RNN_CNN.py,9,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.autograd import Variable\n#from memory_profiler import profile\nfrom models.BaseModel import BaseModel\nclass RNN_CNN(BaseModel):\n    # embedding_dim, hidden_dim, vocab_size, label_size, batch_size, use_gpu\n    def __init__(self,opt):\n\n        super(RNN_CNN, self).__init__(opt)\n        self.hidden_dim = opt.hidden_dim\n        self.batch_size = opt.batch_size\n        self.use_gpu = torch.cuda.is_available()\n\n        self.word_embeddings = nn.Embedding(opt.vocab_size, opt.embedding_dim)\n        self.word_embeddings.weight = nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))\n        self.lstm = nn.LSTM(opt.embedding_dim, opt.hidden_dim)\n        ###self.hidden2label = nn.Linear(opt.hidden_dim, opt.label_size)\n        self.hidden = self.init_hidden()\n        \n        self.content_dim = 256\n        self.conv =  nn.Conv1d(in_channels=opt.hidden_dim, out_channels=self.content_dim, kernel_size=opt.hidden_dim * 2, stride=opt.embedding_dim)\n        self.hidden2label = nn.Linear(self.content_dim, opt.label_size)\n        self.properties.update(\n                {""content_dim"":self.content_dim,\n                })\n\n    def init_hidden(self,batch_size=None):\n        if batch_size is None:\n            batch_size= self.batch_size\n        \n        if self.use_gpu:\n            h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda())\n            c0 = Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda())\n        else:\n            h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim))\n            c0 = Variable(torch.zeros(1,batch_size, self.hidden_dim))\n        return (h0, c0)\n#    @profile\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence) #64x200x300\n\n#        x = embeds.view(sentence.size()[1], self.batch_size, -1)\n        x=embeds.permute(1,0,2) #200x64x300\n        self.hidden= self.init_hidden(sentence.size()[0]) #1x64x128\n        lstm_out, self.hidden = self.lstm(x, self.hidden) ###input (seq_len, batch, input_size) #Outupts:output, (h_n, c_n) output:(seq_len, batch, hidden_size * num_directions)\n        #lstm_out 200x64x128  lstm_out.permute(1,2,0):64x128x200\n        y = self.conv(lstm_out.permute(1,2,0)) ###64x256x1\n        ###y = self.conv(lstm_out.permute(1,2,0).contiguous().view(self.batch_size,128,-1))\n        #y  = self.hidden2label(y.view(sentence.size()[0],-1))\n        y  = self.hidden2label(y.view(y.size()[0],-1)) #64x3\n        return y'"
models/SelfAttention.py,10,"b'# -*- coding: utf-8 -*-#\n# https://arxiv.org/pdf/1703.03130.pdf\n# A Structured Self-attentive Sentence Embedding\n# https://github.com/nn116003/self-attention-classification/blob/master/imdb_attn.py\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.autograd import Variable\n#from memory_profiler import profile\n\nclass SelfAttention(nn.Module):\n    # embedding_dim, hidden_dim, vocab_size, label_size, batch_size, use_gpu\n    def __init__(self,opt):\n        self.opt=opt\n        super(SelfAttention, self).__init__()\n        self.hidden_dim = opt.hidden_dim\n        self.batch_size = opt.batch_size\n        self.use_gpu = torch.cuda.is_available()\n\n        self.word_embeddings = nn.Embedding(opt.vocab_size, opt.embedding_dim)\n        self.word_embeddings.weight = nn.Parameter(opt.embeddings,requires_grad=opt.embedding_training)\n#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))\n  \n        self.num_layers = 1\n        #self.bidirectional = True\n        self.dropout = opt.keep_dropout\n        self.bilstm = nn.LSTM(opt.embedding_dim, opt.hidden_dim // 2, num_layers=self.num_layers, dropout=self.dropout, bidirectional=True)\n        self.hidden2label = nn.Linear(opt.hidden_dim, opt.label_size)\n        self.hidden = self.init_hidden()\n        self.self_attention = nn.Sequential(\n            nn.Linear(opt.hidden_dim, 24),\n            nn.ReLU(True),\n            nn.Linear(24,1)\n            )\n    def init_hidden(self,batch_size=None):\n        if batch_size is None:\n            batch_size= self.batch_size\n        \n        if self.use_gpu:\n            h0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2).cuda())\n            c0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2).cuda())\n        else:\n            h0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2))\n            c0 = Variable(torch.zeros(2*self.num_layers, batch_size, self.hidden_dim // 2))\n        return (h0, c0)\n#    @profile\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n\n#        x = embeds.view(sentence.size()[1], self.batch_size, -1)\n        x=embeds.permute(1,0,2)\n        self.hidden= self.init_hidden(sentence.size()[0]) #2x64x64\n        lstm_out, self.hidden = self.bilstm(x, self.hidden)  #lstm_out:200x64x128\n        final =lstm_out.permute(1,0,2)#torch.mean(,1) \n        attn_ene = self.self_attention(final)\n        attns =F.softmax(attn_ene.view(self.batch_size, -1))\n        feats = (final * attns).sum(dim=1)\n        y  = self.hidden2label(feats) #64x3\n        \n        return y'"
models/Transformer.py,18,"b'# -*- coding: utf-8 -*-\n\n\'\'\' Define the Transformer model \'\'\'\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.init as init\n\n\n\n__author__ = ""Yu-Hsiang Huang""\n#refer to ""https://github.com/jadore801120/attention-is-all-you-need-pytorch""\n\nclass  ConstantsClass():\n    def __init__(self):\n        self.PAD = 0\n        self.UNK = 1\n        self.BOS = 2\n        self.EOS = 3\n        self.PAD_WORD = \'<pad>\'\n        self.UNK_WORD = \'<unk>\'\n        self.BOS_WORD = \'<s>\'\n        self.EOS_WORD = \'</s>\'\nConstants =ConstantsClass()\n\nclass Linear(nn.Module):\n    \'\'\' Simple Linear layer with xavier init \'\'\'\n    def __init__(self, d_in, d_out, bias=True):\n        super(Linear, self).__init__()\n        self.linear = nn.Linear(d_in, d_out, bias=bias)\n        init.xavier_normal(self.linear.weight)\n\n    def forward(self, x):\n        return self.linear(x)\n\nclass Bottle(nn.Module):\n    \'\'\' Perform the reshape routine before and after an operation \'\'\'\n\n    def forward(self, input):\n        if len(input.size()) <= 2:\n            return super(Bottle, self).forward(input)\n        size = input.size()[:2]\n        out = super(Bottle, self).forward(input.view(size[0]*size[1], -1))\n        return out.view(size[0], size[1], -1)\n\nclass BottleLinear(Bottle, Linear):\n    \'\'\' Perform the reshape routine before and after a linear projection \'\'\'\n    pass\n\nclass BottleSoftmax(Bottle, nn.Softmax):\n    \'\'\' Perform the reshape routine before and after a softmax operation\'\'\'\n    pass\n\nclass LayerNormalization(nn.Module):\n    \'\'\' Layer normalization module \'\'\'\n\n    def __init__(self, d_hid, eps=1e-3):\n        super(LayerNormalization, self).__init__()\n\n        self.eps = eps\n        self.a_2 = nn.Parameter(torch.ones(d_hid), requires_grad=True)\n        self.b_2 = nn.Parameter(torch.zeros(d_hid), requires_grad=True)\n\n    def forward(self, z):\n        if z.size(1) == 1:\n            return z\n\n        mu = torch.mean(z, keepdim=True, dim=-1)\n        sigma = torch.std(z, keepdim=True, dim=-1)\n        ln_out = (z - mu.expand_as(z)) / (sigma.expand_as(z) + self.eps)\n        ln_out = ln_out * self.a_2.expand_as(ln_out) + self.b_2.expand_as(ln_out)\n\n        return ln_out\n\nclass BatchBottle(nn.Module):\n    \'\'\' Perform the reshape routine before and after an operation \'\'\'\n\n    def forward(self, input):\n        if len(input.size()) <= 2:\n            return super(BatchBottle, self).forward(input)\n        size = input.size()[1:]\n        out = super(BatchBottle, self).forward(input.view(-1, size[0]*size[1]))\n        return out.view(-1, size[0], size[1])\n\nclass BottleLayerNormalization(BatchBottle, LayerNormalization):\n    \'\'\' Perform the reshape routine before and after a layer normalization\'\'\'\n    pass\n\nclass ScaledDotProductAttention(nn.Module):\n    \'\'\' Scaled Dot-Product Attention \'\'\'\n\n    def __init__(self, d_model, attn_dropout=0.1):\n        super(ScaledDotProductAttention, self).__init__()\n        self.temper = np.power(d_model, 0.5)\n        self.dropout = nn.Dropout(attn_dropout)\n        self.softmax = BottleSoftmax()\n\n    def forward(self, q, k, v, attn_mask=None):\n\n        attn = torch.bmm(q, k.transpose(1, 2)) / self.temper\n\n        if attn_mask is not None:\n\n            assert attn_mask.size() == attn.size(), \\\n                    \'Attention mask shape {} mismatch \' \\\n                    \'with Attention logit tensor shape \' \\\n                    \'{}.\'.format(attn_mask.size(), attn.size())\n\n            attn.data.masked_fill_(attn_mask, -float(\'inf\'))\n\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n\n        return output, attn\nclass MultiHeadAttention(nn.Module):\n    \'\'\' Multi-Head Attention module \'\'\'\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Parameter(torch.FloatTensor(n_head, d_model, d_k))\n        self.w_ks = nn.Parameter(torch.FloatTensor(n_head, d_model, d_k))\n        self.w_vs = nn.Parameter(torch.FloatTensor(n_head, d_model, d_v))\n\n        self.attention = ScaledDotProductAttention(d_model)\n        self.layer_norm = LayerNormalization(d_model)\n        self.proj = Linear(n_head*d_v, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n        init.xavier_normal(self.w_qs)\n        init.xavier_normal(self.w_ks)\n        init.xavier_normal(self.w_vs)\n\n    def forward(self, q, k, v, attn_mask=None):\n\n        d_k, d_v = self.d_k, self.d_v\n        n_head = self.n_head\n\n        residual = q\n\n        mb_size, len_q, d_model = q.size()\n        mb_size, len_k, d_model = k.size()\n        mb_size, len_v, d_model = v.size()\n\n        # treat as a (n_head) size batch\n        q_s = q.repeat(n_head, 1, 1).view(n_head, -1, d_model) # n_head x (mb_size*len_q) x d_model\n        k_s = k.repeat(n_head, 1, 1).view(n_head, -1, d_model) # n_head x (mb_size*len_k) x d_model\n        v_s = v.repeat(n_head, 1, 1).view(n_head, -1, d_model) # n_head x (mb_size*len_v) x d_model\n\n        # treat the result as a (n_head * mb_size) size batch\n        q_s = torch.bmm(q_s, self.w_qs).view(-1, len_q, d_k)   # (n_head*mb_size) x len_q x d_k\n        k_s = torch.bmm(k_s, self.w_ks).view(-1, len_k, d_k)   # (n_head*mb_size) x len_k x d_k\n        v_s = torch.bmm(v_s, self.w_vs).view(-1, len_v, d_v)   # (n_head*mb_size) x len_v x d_v\n\n        # perform attention, result size = (n_head * mb_size) x len_q x d_v\n        outputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))\n\n        # back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)\n        outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1) \n\n        # project back to residual size\n        outputs = self.proj(outputs)\n        outputs = self.dropout(outputs)\n\n        return self.layer_norm(outputs + residual), attns\n\nclass PositionwiseFeedForward(nn.Module):\n    \'\'\' A two-feed-forward-layer module \'\'\'\n\n    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Conv1d(d_hid, d_inner_hid, 1) # position-wise\n        self.w_2 = nn.Conv1d(d_inner_hid, d_hid, 1) # position-wise\n        self.layer_norm = LayerNormalization(d_hid)\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        output = self.relu(self.w_1(x.transpose(1, 2)))\n        output = self.w_2(output).transpose(2, 1)\n        output = self.dropout(output)\n        return self.layer_norm(output + residual)\nclass EncoderLayer(nn.Module):\n    \'\'\' Compose with two layers \'\'\'\n\n    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(\n            n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n\n    def forward(self, enc_input, slf_attn_mask=None):\n        enc_output, enc_slf_attn = self.slf_attn(\n            enc_input, enc_input, enc_input, attn_mask=slf_attn_mask)\n        enc_output = self.pos_ffn(enc_output)\n        return enc_output, enc_slf_attn\n\nclass DecoderLayer(nn.Module):\n    \'\'\' Compose with three layers \'\'\'\n\n    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n\n    def forward(self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None):\n        dec_output, dec_slf_attn = self.slf_attn(\n            dec_input, dec_input, dec_input, attn_mask=slf_attn_mask)\n        dec_output, dec_enc_attn = self.enc_attn(\n            dec_output, enc_output, enc_output, attn_mask=dec_enc_attn_mask)\n        dec_output = self.pos_ffn(dec_output)\n\n        return dec_output, dec_slf_attn, dec_enc_attn\n    \ndef position_encoding_init(n_position, d_pos_vec):\n    \'\'\' Init the sinusoid position encoding table \'\'\'\n\n    # keep dim 0 for padding token position encoding zero vector\n    position_enc = np.array([\n        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n\n    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n\ndef get_attn_padding_mask(seq_q, seq_k):\n    \'\'\' Indicate the padding-related part to mask \'\'\'\n    assert seq_q.dim() == 2 and seq_k.dim() == 2\n    mb_size, len_q = seq_q.size()\n    mb_size, len_k = seq_k.size()\n    pad_attn_mask = seq_k.data.eq(Constants.PAD).unsqueeze(1)   # bx1xsk\n    pad_attn_mask = pad_attn_mask.expand(mb_size, len_q, len_k) # bxsqxsk\n    return pad_attn_mask\n\ndef get_attn_subsequent_mask(seq):\n    \'\'\' Get an attention mask to avoid using the subsequent info.\'\'\'\n    assert seq.dim() == 2\n    attn_shape = (seq.size(0), seq.size(1), seq.size(1))\n    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(\'uint8\')\n    subsequent_mask = torch.from_numpy(subsequent_mask)\n    if seq.is_cuda:\n        subsequent_mask = subsequent_mask.cuda()\n    return subsequent_mask\n\nclass Encoder(nn.Module):\n    \'\'\' A encoder model with self attention mechanism. \'\'\'\n\n    def __init__(\n            self, n_src_vocab, n_max_seq, n_layers=6, n_head=8, d_k=64, d_v=64,\n            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n\n        super(Encoder, self).__init__()\n\n        n_position = n_max_seq + 1\n        self.n_max_seq = n_max_seq\n        self.d_model = d_model\n\n        self.position_enc = nn.Embedding(n_position, d_word_vec, padding_idx=Constants.PAD)\n        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n\n        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=Constants.PAD)\n\n        self.layer_stack = nn.ModuleList([\n            EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n\n    def forward(self, src_seq, src_pos, return_attns=False):\n        # Word embedding look up\n        enc_input = self.src_word_emb(src_seq)\n\n        # Position Encoding addition\n        enc_input += self.position_enc(src_pos)\n        if return_attns:\n            enc_slf_attns = []\n\n        enc_output = enc_input\n        enc_slf_attn_mask = get_attn_padding_mask(src_seq, src_seq)\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(\n                enc_output, slf_attn_mask=enc_slf_attn_mask)\n            if return_attns:\n                enc_slf_attns += [enc_slf_attn]\n\n        if return_attns:\n            return enc_output, enc_slf_attns\n        else:\n            return enc_output\n\nclass Decoder(nn.Module):\n    \'\'\' A decoder model with self attention mechanism. \'\'\'\n    def __init__(\n            self, n_tgt_vocab, n_max_seq, n_layers=6, n_head=8, d_k=64, d_v=64,\n            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n\n        super(Decoder, self).__init__()\n        n_position = n_max_seq + 1\n        self.n_max_seq = n_max_seq\n        self.d_model = d_model\n\n        self.position_enc = nn.Embedding(\n            n_position, d_word_vec, padding_idx=Constants.PAD)\n        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n\n        self.tgt_word_emb = nn.Embedding(\n            n_tgt_vocab, d_word_vec, padding_idx=Constants.PAD)\n        self.dropout = nn.Dropout(dropout)\n\n        self.layer_stack = nn.ModuleList([\n            DecoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n\n    def forward(self, tgt_seq, tgt_pos, src_seq, enc_output, return_attns=False):\n        # Word embedding look up\n        dec_input = self.tgt_word_emb(tgt_seq)\n\n        # Position Encoding addition\n        dec_input += self.position_enc(tgt_pos)\n\n        # Decode\n        dec_slf_attn_pad_mask = get_attn_padding_mask(tgt_seq, tgt_seq)\n        dec_slf_attn_sub_mask = get_attn_subsequent_mask(tgt_seq)\n        dec_slf_attn_mask = torch.gt(dec_slf_attn_pad_mask + dec_slf_attn_sub_mask, 0)\n\n        dec_enc_attn_pad_mask = get_attn_padding_mask(tgt_seq, src_seq)\n\n        if return_attns:\n            dec_slf_attns, dec_enc_attns = [], []\n\n        dec_output = dec_input\n        for dec_layer in self.layer_stack:\n            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n                dec_output, enc_output,\n                slf_attn_mask=dec_slf_attn_mask,\n                dec_enc_attn_mask=dec_enc_attn_pad_mask)\n\n            if return_attns:\n                dec_slf_attns += [dec_slf_attn]\n                dec_enc_attns += [dec_enc_attn]\n\n        if return_attns:\n            return dec_output, dec_slf_attns, dec_enc_attns\n        else:\n            return dec_output,\n\nclass Transformer(nn.Module):\n    \'\'\' A sequence to sequence model with attention mechanism. \'\'\'\n\n    def __init__(\n            self, n_src_vocab, n_tgt_vocab, n_max_seq, n_layers=6, n_head=8,\n            d_word_vec=512, d_model=512, d_inner_hid=1024, d_k=64, d_v=64,\n            dropout=0.1, proj_share_weight=True, embs_share_weight=True):\n\n        super(Transformer, self).__init__()\n        self.encoder = Encoder(\n            n_src_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,\n            d_word_vec=d_word_vec, d_model=d_model,\n            d_inner_hid=d_inner_hid, dropout=dropout)\n        self.decoder = Decoder(\n            n_tgt_vocab, n_max_seq, n_layers=n_layers, n_head=n_head,\n            d_word_vec=d_word_vec, d_model=d_model,\n            d_inner_hid=d_inner_hid, dropout=dropout)\n        self.tgt_word_proj = Linear(d_model, n_tgt_vocab, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n        assert d_model == d_word_vec, \\\n        \'To facilitate the residual connections, \\\n         the dimensions of all module output shall be the same.\'\n\n        if proj_share_weight:\n            # Share the weight matrix between tgt word embedding/projection\n            assert d_model == d_word_vec\n            self.tgt_word_proj.weight = self.decoder.tgt_word_emb.weight\n\n        if embs_share_weight:\n            # Share the weight matrix between src/tgt word embeddings\n            # assume the src/tgt word vec size are the same\n            assert n_src_vocab == n_tgt_vocab, \\\n            ""To share word embedding table, the vocabulary size of src/tgt shall be the same.""\n            self.encoder.src_word_emb.weight = self.decoder.tgt_word_emb.weight\n\n    def get_trainable_parameters(self):\n        \'\'\' Avoid updating the position encoding \'\'\'\n        enc_freezed_param_ids = set(map(id, self.encoder.position_enc.parameters()))\n        dec_freezed_param_ids = set(map(id, self.decoder.position_enc.parameters()))\n        freezed_param_ids = enc_freezed_param_ids | dec_freezed_param_ids\n        return (p for p in self.parameters() if id(p) not in freezed_param_ids)\n\n    def forward(self, src, tgt):\n        src_seq, src_pos = src\n        tgt_seq, tgt_pos = tgt\n\n        tgt_seq = tgt_seq[:, :-1]\n        tgt_pos = tgt_pos[:, :-1]\n\n        enc_output, _ = self.encoder(src_seq, src_pos)\n        dec_output, _ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)\n        seq_logit = self.tgt_word_proj(dec_output)\n\n        return seq_logit.view(-1, seq_logit.size(2))\n    \nclass AttentionIsAllYouNeed(nn.Module):\n     def __init__(self, opt, n_layers=6, n_head=8,\n            d_word_vec=128, d_model=128, d_inner_hid=256, d_k=32, d_v=32,\n            dropout=0.1, proj_share_weight=True, embs_share_weight=True):\n#          self, opt, n_layers=6, n_head=8, d_word_vec=512, d_model=512, d_inner_hid=1024, d_k=64, d_v=64,\n            \n         super(AttentionIsAllYouNeed, self).__init__() \n         self.encoder = Encoder(\n            opt.vocab_size, opt.max_seq_len, n_layers=n_layers, n_head=n_head,\n            d_word_vec=d_word_vec, d_model=d_model,\n            d_inner_hid=d_inner_hid, dropout=dropout)\n         self.hidden2label = nn.Linear(opt.max_seq_len*d_model, opt.label_size)\n         self.batch_size=opt.batch_size\n     def forward(self, inp):\n\n         src_seq,src_pos = inp\n#         enc_output, *_ = self.encoder(src_seq, src_pos)   #64x200x512\n         enc_output = self.encoder(src_seq, src_pos)   #64x200x512\n         return  self.hidden2label(enc_output.view((self.batch_size,-1)))\n         \n         \n    '"
models/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nimport numpy as np\n\n\n\nfrom models.LSTM import LSTMClassifier\nfrom models.CNNBasic import BasicCNN1D,BasicCNN2D\nfrom models.CNNKim import KIMCNN1D,KIMCNN2D\nfrom models.CNNMultiLayer import MultiLayerCNN\nfrom models.CNNInception import InceptionCNN\nfrom models.FastText import FastText\nfrom models.Capsule import CapsuleNet\nfrom models.RCNN import RCNN\nfrom models.RNN_CNN import RNN_CNN\nfrom models.LSTMBI import LSTMBI\nfrom models.Transformer import AttentionIsAllYouNeed\nfrom models.SelfAttention import SelfAttention\nfrom models.LSTMwithAttention import LSTMAttention\nfrom models.BERTFast import BERTFast\ndef setup(opt):\n    \n    if opt.model == \'lstm\':\n        model = LSTMClassifier(opt)\n    elif opt.model == \'basic_cnn\' or opt.model == ""cnn"":\n        model = BasicCNN1D(opt)\n    elif opt.model == \'baisc_cnn_2d\' :\n        model = BasicCNN2D(opt)\n    elif opt.model == \'kim_cnn\' :\n        model = KIMCNN1D(opt)\n    elif opt.model ==  \'kim_cnn_2d\':\n        model = KIMCNN2D(opt)\n    elif opt.model ==  \'multi_cnn\':\n        model = MultiLayerCNN(opt)\n    elif opt.model ==  \'inception_cnn\':\n        model = InceptionCNN(opt) \n    elif opt.model ==  \'fasttext\':\n        model = FastText(opt)\n    elif opt.model ==  \'capsule\':\n        model = CapsuleNet(opt)\n    elif opt.model ==  \'rnn_cnn\':\n        model = RNN_CNN(opt)\n    elif opt.model ==  \'rcnn\':\n        model = RCNN(opt)\n    elif opt.model ==  \'bilstm\':\n        model = LSTMBI(opt)\n    elif opt.model == ""transformer"":\n        model = AttentionIsAllYouNeed(opt)\n    elif opt.model == ""selfattention"":\n        model = SelfAttention(opt)\n    elif opt.model == ""lstm_attention"":\n        model =LSTMAttention(opt)\n    elif opt.model == ""bert"":\n        model =BERTFast(opt)\n    else:\n        raise Exception(""model not supported: {}"".format(opt.model))\n    return model\n'"
dataloader/torch_text_demo/imdb.py,1,"b""# -*- coding: utf-8 -*-\n\n\n\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import GloVe\nimport torch\nif torch.cuda.is_available() :\n    device = -1\nelse:\n    device = 0\n# Approach 1:\n# set up fields\nTEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\nLABEL = data.Field(sequential=False)\n\n\n# make splits for data\ntrain, test = datasets.IMDB.splits(TEXT, LABEL)\n\n# print information about the data\nprint('train.fields', train.fields)\nprint('len(train)', len(train))\nprint('vars(train[0])', vars(train[0]))\n\n# build the vocabulary\nTEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\nLABEL.build_vocab(train)\n\n# print vocab information\nprint('len(TEXT.vocab)', len(TEXT.vocab))\nprint('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n\n# make iterator for splits\n#train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=3, device=0)\ntrain_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=3,device=-1)\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n\n# Approach 2:\ntrain_iter, test_iter = datasets.IMDB.iters(batch_size=4,device=-1)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)"""
dataloader/torch_text_demo/sst.py,0,"b""from torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n\n\n# Approach 1:\n# set up fields\nTEXT = data.Field()\nLABEL = data.Field(sequential=False)\n\n# make splits for data\ntrain, val, test = datasets.SST.splits(\n    TEXT, LABEL, fine_grained=True, train_subtrees=True,\n    filter_pred=lambda ex: ex.label != 'neutral')\n\n# print information about the data\nprint('train.fields', train.fields)\nprint('len(train)', len(train))\nprint('vars(train[0])', vars(train[0]))\n\n# build the vocabulary\nurl = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\nTEXT.build_vocab(train, vectors=Vectors('wiki.simple.vec', url=url))\nLABEL.build_vocab(train)\n\n# print vocab information\nprint('len(TEXT.vocab)', len(TEXT.vocab))\nprint('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n\n# make iterator for splits\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_size=3, device=0)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n\n# Approach 2:\nTEXT.build_vocab(train, vectors=[GloVe(name='840B', dim='300'), CharNGram(), FastText()])\nLABEL.build_vocab(train)\n\n# print vocab information\nprint('len(TEXT.vocab)', len(TEXT.vocab))\nprint('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n\ntrain_iter, val_iter, test_iter = datasets.SST.iters(batch_size=4)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n\n# Approach 3:\nf = FastText()\nTEXT.build_vocab(train, vectors=f)\nTEXT.vocab.extend(f)\nLABEL.build_vocab(train)\n\n# print vocab information\nprint('len(TEXT.vocab)', len(TEXT.vocab))\nprint('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n\ntrain_iter, val_iter, test_iter = datasets.SST.iters(batch_size=4)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)"""
dataloader/torch_text_demo/trec.py,1,"b""from torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import GloVe, CharNGram\nimport torch\nif not torch.cuda.is_available() :\n    device = -1\nelse:\n    device = 0\n\n# Approach 1:\n# set up fields\nTEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\nLABEL = data.Field(sequential=False)\n\n\n# make splits for data\ntrain, test = datasets.TREC.splits(TEXT, LABEL, fine_grained=True)\n\n# print information about the data\nprint('train.fields', train.fields)\nprint('len(train)', len(train))\nprint('vars(train[0])', vars(train[0]))\n\n# build the vocabulary\nTEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\nLABEL.build_vocab(train)\n\n# print vocab information\nprint('len(TEXT.vocab)', len(TEXT.vocab))\nprint('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n\n# make iterator for splits\ntrain_iter, test_iter = data.BucketIterator.splits(\n    (train, test), batch_size=3, device=device)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)\n\n# Approach 2:\nTEXT.build_vocab(train, vectors=[GloVe(name='840B', dim='300'), CharNGram()],device=device)\nLABEL.build_vocab(train)\n\ntrain_iter, test_iter = datasets.TREC.iters(batch_size=4)\n\n# print batch information\nbatch = next(iter(train_iter))\nprint(batch.text)\nprint(batch.label)"""
