file_path,api_count,code
compute_iou.py,0,"b'import numpy as np\nimport argparse\nimport json\nfrom PIL import Image\nfrom os.path import join\n\n\ndef fast_hist(a, b, n):\n    k = (a >= 0) & (a < n)\n    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n\n\ndef per_class_iu(hist):\n    return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n\n\ndef label_mapping(input, mapping):\n    output = np.copy(input)\n    for ind in range(len(mapping)):\n        output[input == mapping[ind][0]] = mapping[ind][1]\n    return np.array(output, dtype=np.int64)\n\n\ndef compute_mIoU(gt_dir, pred_dir, devkit_dir=\'\'):\n    """"""\n    Compute IoU given the predicted colorized images and \n    """"""\n    with open(join(devkit_dir, \'info.json\'), \'r\') as fp:\n      info = json.load(fp)\n    num_classes = np.int(info[\'classes\'])\n    print(\'Num classes\', num_classes)\n    name_classes = np.array(info[\'label\'], dtype=np.str)\n    mapping = np.array(info[\'label2train\'], dtype=np.int)\n    hist = np.zeros((num_classes, num_classes))\n\n    image_path_list = join(devkit_dir, \'val.txt\')\n    label_path_list = join(devkit_dir, \'label.txt\')\n    gt_imgs = open(label_path_list, \'r\').read().splitlines()\n    gt_imgs = [join(gt_dir, x) for x in gt_imgs]\n    pred_imgs = open(image_path_list, \'r\').read().splitlines()\n    pred_imgs = [join(pred_dir, x.split(\'/\')[-1]) for x in pred_imgs]\n\n    for ind in range(len(gt_imgs)):\n        pred = np.array(Image.open(pred_imgs[ind]))\n        label = np.array(Image.open(gt_imgs[ind]))\n        label = label_mapping(label, mapping)\n        if len(label.flatten()) != len(pred.flatten()):\n            print(\'Skipping: len(gt) = {:d}, len(pred) = {:d}, {:s}, {:s}\'.format(len(label.flatten()), len(pred.flatten()), gt_imgs[ind], pred_imgs[ind]))\n            continue\n        hist += fast_hist(label.flatten(), pred.flatten(), num_classes)\n        if ind > 0 and ind % 10 == 0:\n            print(\'{:d} / {:d}: {:0.2f}\'.format(ind, len(gt_imgs), 100*np.mean(per_class_iu(hist))))\n    \n    mIoUs = per_class_iu(hist)\n    for ind_class in range(num_classes):\n        print(\'===>\' + name_classes[ind_class] + \':\\t\' + str(round(mIoUs[ind_class] * 100, 2)))\n    print(\'===> mIoU: \' + str(round(np.nanmean(mIoUs) * 100, 2)))\n    return mIoUs\n\n\ndef main(args):\n   compute_mIoU(args.gt_dir, args.pred_dir, args.devkit_dir)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'gt_dir\', type=str, help=\'directory which stores CityScapes val gt images\')\n    parser.add_argument(\'pred_dir\', type=str, help=\'directory which stores CityScapes val pred images\')\n    parser.add_argument(\'--devkit_dir\', default=\'dataset/cityscapes_list\', help=\'base directory of cityscapes\')\n    args = parser.parse_args()\n    main(args)\n'"
evaluate_cityscapes.py,6,"b'import argparse\nimport scipy\nfrom scipy import ndimage\nimport numpy as np\nimport sys\nfrom packaging import version\n\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.models as models\nimport torch.nn.functional as F\nfrom torch.utils import data, model_zoo\nfrom model.deeplab import Res_Deeplab\nfrom model.deeplab_multi import DeeplabMulti\nfrom model.deeplab_vgg import DeeplabVGG\nfrom dataset.cityscapes_dataset import cityscapesDataSet\nfrom collections import OrderedDict\nimport os\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n\nDATA_DIRECTORY = \'./data/Cityscapes/data\'\nDATA_LIST_PATH = \'./dataset/cityscapes_list/val.txt\'\nSAVE_PATH = \'./result/cityscapes\'\n\nIGNORE_LABEL = 255\nNUM_CLASSES = 19\nNUM_STEPS = 500 # Number of images in the validation set.\nRESTORE_FROM = \'http://vllab.ucmerced.edu/ytsai/CVPR18/GTA2Cityscapes_multi-ed35151c.pth\'\nRESTORE_FROM_VGG = \'http://vllab.ucmerced.edu/ytsai/CVPR18/GTA2Cityscapes_vgg-ac4ac9f6.pth\'\nRESTORE_FROM_ORC = \'http://vllab1.ucmerced.edu/~whung/adaptSeg/cityscapes_oracle-b7b9934.pth\'\nSET = \'val\'\n\nMODEL = \'DeeplabMulti\'\n\npalette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\nzero_pad = 256 * 3 - len(palette)\nfor i in range(zero_pad):\n    palette.append(0)\n\n\ndef colorize_mask(mask):\n    # mask: numpy array of the mask\n    new_mask = Image.fromarray(mask.astype(np.uint8)).convert(\'P\')\n    new_mask.putpalette(palette)\n\n    return new_mask\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n\n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""DeepLab-ResNet Network"")\n    parser.add_argument(""--model"", type=str, default=MODEL,\n                        help=""Model Choice (DeeplabMulti/DeeplabVGG/Oracle)."")\n    parser.add_argument(""--data-dir"", type=str, default=DATA_DIRECTORY,\n                        help=""Path to the directory containing the Cityscapes dataset."")\n    parser.add_argument(""--data-list"", type=str, default=DATA_LIST_PATH,\n                        help=""Path to the file listing the images in the dataset."")\n    parser.add_argument(""--ignore-label"", type=int, default=IGNORE_LABEL,\n                        help=""The index of the label to ignore during the training."")\n    parser.add_argument(""--num-classes"", type=int, default=NUM_CLASSES,\n                        help=""Number of classes to predict (including background)."")\n    parser.add_argument(""--restore-from"", type=str, default=RESTORE_FROM,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--gpu"", type=int, default=0,\n                        help=""choose gpu device."")\n    parser.add_argument(""--set"", type=str, default=SET,\n                        help=""choose evaluation set."")\n    parser.add_argument(""--save"", type=str, default=SAVE_PATH,\n                        help=""Path to save result."")\n    return parser.parse_args()\n\n\ndef main():\n    """"""Create the model and start the evaluation process.""""""\n\n    args = get_arguments()\n\n    gpu0 = args.gpu\n\n    if not os.path.exists(args.save):\n        os.makedirs(args.save)\n\n    if args.model == \'DeeplabMulti\':\n        model = DeeplabMulti(num_classes=args.num_classes)\n    elif args.model == \'Oracle\':\n        model = Res_Deeplab(num_classes=args.num_classes)\n        if args.restore_from == RESTORE_FROM:\n            args.restore_from = RESTORE_FROM_ORC\n    elif args.model == \'DeeplabVGG\':\n        model = DeeplabVGG(num_classes=args.num_classes)\n        if args.restore_from == RESTORE_FROM:\n            args.restore_from = RESTORE_FROM_VGG\n\n    if args.restore_from[:4] == \'http\' :\n        saved_state_dict = model_zoo.load_url(args.restore_from)\n    else:\n        saved_state_dict = torch.load(args.restore_from)\n    ### for running different versions of pytorch\n    model_dict = model.state_dict()\n    saved_state_dict = {k: v for k, v in saved_state_dict.items() if k in model_dict}\n    model_dict.update(saved_state_dict)\n    ###\n    model.load_state_dict(saved_state_dict)\n\n    model.eval()\n    model.cuda(gpu0)\n\n    testloader = data.DataLoader(cityscapesDataSet(args.data_dir, args.data_list, crop_size=(1024, 512), mean=IMG_MEAN, scale=False, mirror=False, set=args.set),\n                                    batch_size=1, shuffle=False, pin_memory=True)\n\n\n    if version.parse(torch.__version__) >= version.parse(\'0.4.0\'):\n        interp = nn.Upsample(size=(1024, 2048), mode=\'bilinear\', align_corners=True)\n    else:\n        interp = nn.Upsample(size=(1024, 2048), mode=\'bilinear\')\n\n    for index, batch in enumerate(testloader):\n        if index % 100 == 0:\n            print \'%d processd\' % index\n        image, _, name = batch\n        if args.model == \'DeeplabMulti\':\n            output1, output2 = model(Variable(image, volatile=True).cuda(gpu0))\n            output = interp(output2).cpu().data[0].numpy()\n        elif args.model == \'DeeplabVGG\' or args.model == \'Oracle\':\n            output = model(Variable(image, volatile=True).cuda(gpu0))\n            output = interp(output).cpu().data[0].numpy()\n\n        output = output.transpose(1,2,0)\n        output = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n\n        output_col = colorize_mask(output)\n        output = Image.fromarray(output)\n\n        name = name[0].split(\'/\')[-1]\n        output.save(\'%s/%s\' % (args.save, name))\n        output_col.save(\'%s/%s_color.png\' % (args.save, name.split(\'.\')[0]))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train_gta2cityscapes_multi.py,21,"b'import argparse\nimport torch\nimport torch.nn as nn\nfrom torch.utils import data, model_zoo\nimport numpy as np\nimport pickle\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport scipy.misc\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport sys\nimport os\nimport os.path as osp\nimport matplotlib.pyplot as plt\nimport random\n\nfrom model.deeplab_multi import DeeplabMulti\nfrom model.discriminator import FCDiscriminator\nfrom utils.loss import CrossEntropy2d\nfrom dataset.gta5_dataset import GTA5DataSet\nfrom dataset.cityscapes_dataset import cityscapesDataSet\n\nIMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n\nMODEL = \'DeepLab\'\nBATCH_SIZE = 1\nITER_SIZE = 1\nNUM_WORKERS = 4\nDATA_DIRECTORY = \'./data/GTA5\'\nDATA_LIST_PATH = \'./dataset/gta5_list/train.txt\'\nIGNORE_LABEL = 255\nINPUT_SIZE = \'1280,720\'\nDATA_DIRECTORY_TARGET = \'./data/Cityscapes/data\'\nDATA_LIST_PATH_TARGET = \'./dataset/cityscapes_list/train.txt\'\nINPUT_SIZE_TARGET = \'1024,512\'\nLEARNING_RATE = 2.5e-4\nMOMENTUM = 0.9\nNUM_CLASSES = 19\nNUM_STEPS = 250000\nNUM_STEPS_STOP = 150000  # early stopping\nPOWER = 0.9\nRANDOM_SEED = 1234\nRESTORE_FROM = \'http://vllab.ucmerced.edu/ytsai/CVPR18/DeepLab_resnet_pretrained_init-f81d91e8.pth\'\nSAVE_NUM_IMAGES = 2\nSAVE_PRED_EVERY = 5000\nSNAPSHOT_DIR = \'./snapshots/\'\nWEIGHT_DECAY = 0.0005\n\nLEARNING_RATE_D = 1e-4\nLAMBDA_SEG = 0.1\nLAMBDA_ADV_TARGET1 = 0.0002\nLAMBDA_ADV_TARGET2 = 0.001\nGAN = \'Vanilla\'\n\nTARGET = \'cityscapes\'\nSET = \'train\'\n\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n\n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""DeepLab-ResNet Network"")\n    parser.add_argument(""--model"", type=str, default=MODEL,\n                        help=""available options : DeepLab"")\n    parser.add_argument(""--target"", type=str, default=TARGET,\n                        help=""available options : cityscapes"")\n    parser.add_argument(""--batch-size"", type=int, default=BATCH_SIZE,\n                        help=""Number of images sent to the network in one step."")\n    parser.add_argument(""--iter-size"", type=int, default=ITER_SIZE,\n                        help=""Accumulate gradients for ITER_SIZE iterations."")\n    parser.add_argument(""--num-workers"", type=int, default=NUM_WORKERS,\n                        help=""number of workers for multithread dataloading."")\n    parser.add_argument(""--data-dir"", type=str, default=DATA_DIRECTORY,\n                        help=""Path to the directory containing the source dataset."")\n    parser.add_argument(""--data-list"", type=str, default=DATA_LIST_PATH,\n                        help=""Path to the file listing the images in the source dataset."")\n    parser.add_argument(""--ignore-label"", type=int, default=IGNORE_LABEL,\n                        help=""The index of the label to ignore during the training."")\n    parser.add_argument(""--input-size"", type=str, default=INPUT_SIZE,\n                        help=""Comma-separated string with height and width of source images."")\n    parser.add_argument(""--data-dir-target"", type=str, default=DATA_DIRECTORY_TARGET,\n                        help=""Path to the directory containing the target dataset."")\n    parser.add_argument(""--data-list-target"", type=str, default=DATA_LIST_PATH_TARGET,\n                        help=""Path to the file listing the images in the target dataset."")\n    parser.add_argument(""--input-size-target"", type=str, default=INPUT_SIZE_TARGET,\n                        help=""Comma-separated string with height and width of target images."")\n    parser.add_argument(""--is-training"", action=""store_true"",\n                        help=""Whether to updates the running means and variances during the training."")\n    parser.add_argument(""--learning-rate"", type=float, default=LEARNING_RATE,\n                        help=""Base learning rate for training with polynomial decay."")\n    parser.add_argument(""--learning-rate-D"", type=float, default=LEARNING_RATE_D,\n                        help=""Base learning rate for discriminator."")\n    parser.add_argument(""--lambda-seg"", type=float, default=LAMBDA_SEG,\n                        help=""lambda_seg."")\n    parser.add_argument(""--lambda-adv-target1"", type=float, default=LAMBDA_ADV_TARGET1,\n                        help=""lambda_adv for adversarial training."")\n    parser.add_argument(""--lambda-adv-target2"", type=float, default=LAMBDA_ADV_TARGET2,\n                        help=""lambda_adv for adversarial training."")\n    parser.add_argument(""--momentum"", type=float, default=MOMENTUM,\n                        help=""Momentum component of the optimiser."")\n    parser.add_argument(""--not-restore-last"", action=""store_true"",\n                        help=""Whether to not restore last (FC) layers."")\n    parser.add_argument(""--num-classes"", type=int, default=NUM_CLASSES,\n                        help=""Number of classes to predict (including background)."")\n    parser.add_argument(""--num-steps"", type=int, default=NUM_STEPS,\n                        help=""Number of training steps."")\n    parser.add_argument(""--num-steps-stop"", type=int, default=NUM_STEPS_STOP,\n                        help=""Number of training steps for early stopping."")\n    parser.add_argument(""--power"", type=float, default=POWER,\n                        help=""Decay parameter to compute the learning rate."")\n    parser.add_argument(""--random-mirror"", action=""store_true"",\n                        help=""Whether to randomly mirror the inputs during the training."")\n    parser.add_argument(""--random-scale"", action=""store_true"",\n                        help=""Whether to randomly scale the inputs during the training."")\n    parser.add_argument(""--random-seed"", type=int, default=RANDOM_SEED,\n                        help=""Random seed to have reproducible results."")\n    parser.add_argument(""--restore-from"", type=str, default=RESTORE_FROM,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--save-num-images"", type=int, default=SAVE_NUM_IMAGES,\n                        help=""How many images to save."")\n    parser.add_argument(""--save-pred-every"", type=int, default=SAVE_PRED_EVERY,\n                        help=""Save summaries and checkpoint every often."")\n    parser.add_argument(""--snapshot-dir"", type=str, default=SNAPSHOT_DIR,\n                        help=""Where to save snapshots of the model."")\n    parser.add_argument(""--weight-decay"", type=float, default=WEIGHT_DECAY,\n                        help=""Regularisation parameter for L2-loss."")\n    parser.add_argument(""--gpu"", type=int, default=0,\n                        help=""choose gpu device."")\n    parser.add_argument(""--set"", type=str, default=SET,\n                        help=""choose adaptation set."")\n    parser.add_argument(""--gan"", type=str, default=GAN,\n                        help=""choose the GAN objective."")\n    return parser.parse_args()\n\n\nargs = get_arguments()\n\n\ndef loss_calc(pred, label, gpu):\n    """"""\n    This function returns cross entropy loss for semantic segmentation\n    """"""\n    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n    label = Variable(label.long()).cuda(gpu)\n    criterion = CrossEntropy2d().cuda(gpu)\n\n    return criterion(pred, label)\n\n\ndef lr_poly(base_lr, iter, max_iter, power):\n    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n\n\ndef adjust_learning_rate(optimizer, i_iter):\n    lr = lr_poly(args.learning_rate, i_iter, args.num_steps, args.power)\n    optimizer.param_groups[0][\'lr\'] = lr\n    if len(optimizer.param_groups) > 1:\n        optimizer.param_groups[1][\'lr\'] = lr * 10\n\n\ndef adjust_learning_rate_D(optimizer, i_iter):\n    lr = lr_poly(args.learning_rate_D, i_iter, args.num_steps, args.power)\n    optimizer.param_groups[0][\'lr\'] = lr\n    if len(optimizer.param_groups) > 1:\n        optimizer.param_groups[1][\'lr\'] = lr * 10\n\n\ndef main():\n    """"""Create the model and start the training.""""""\n\n    w, h = map(int, args.input_size.split(\',\'))\n    input_size = (w, h)\n\n    w, h = map(int, args.input_size_target.split(\',\'))\n    input_size_target = (w, h)\n\n    cudnn.enabled = True\n    gpu = args.gpu\n\n    # Create network\n    if args.model == \'DeepLab\':\n        model = DeeplabMulti(num_classes=args.num_classes)\n        if args.restore_from[:4] == \'http\' :\n            saved_state_dict = model_zoo.load_url(args.restore_from)\n        else:\n            saved_state_dict = torch.load(args.restore_from)\n\n        new_params = model.state_dict().copy()\n        for i in saved_state_dict:\n            # Scale.layer5.conv2d_list.3.weight\n            i_parts = i.split(\'.\')\n            # print i_parts\n            if not args.num_classes == 19 or not i_parts[1] == \'layer5\':\n                new_params[\'.\'.join(i_parts[1:])] = saved_state_dict[i]\n                # print i_parts\n        model.load_state_dict(new_params)\n\n    model.train()\n    model.cuda(args.gpu)\n\n    cudnn.benchmark = True\n\n    # init D\n    model_D1 = FCDiscriminator(num_classes=args.num_classes)\n    model_D2 = FCDiscriminator(num_classes=args.num_classes)\n\n    model_D1.train()\n    model_D1.cuda(args.gpu)\n\n    model_D2.train()\n    model_D2.cuda(args.gpu)\n\n    if not os.path.exists(args.snapshot_dir):\n        os.makedirs(args.snapshot_dir)\n\n    trainloader = data.DataLoader(\n        GTA5DataSet(args.data_dir, args.data_list, max_iters=args.num_steps * args.iter_size * args.batch_size,\n                    crop_size=input_size,\n                    scale=args.random_scale, mirror=args.random_mirror, mean=IMG_MEAN),\n        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n\n    trainloader_iter = enumerate(trainloader)\n\n    targetloader = data.DataLoader(cityscapesDataSet(args.data_dir_target, args.data_list_target,\n                                                     max_iters=args.num_steps * args.iter_size * args.batch_size,\n                                                     crop_size=input_size_target,\n                                                     scale=False, mirror=args.random_mirror, mean=IMG_MEAN,\n                                                     set=args.set),\n                                   batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers,\n                                   pin_memory=True)\n\n\n    targetloader_iter = enumerate(targetloader)\n\n    # implement model.optim_parameters(args) to handle different models\' lr setting\n\n    optimizer = optim.SGD(model.optim_parameters(args),\n                          lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n    optimizer.zero_grad()\n\n    optimizer_D1 = optim.Adam(model_D1.parameters(), lr=args.learning_rate_D, betas=(0.9, 0.99))\n    optimizer_D1.zero_grad()\n\n    optimizer_D2 = optim.Adam(model_D2.parameters(), lr=args.learning_rate_D, betas=(0.9, 0.99))\n    optimizer_D2.zero_grad()\n\n    if args.gan == \'Vanilla\':\n        bce_loss = torch.nn.BCEWithLogitsLoss()\n    elif args.gan == \'LS\':\n        bce_loss = torch.nn.MSELoss()\n\n    interp = nn.Upsample(size=(input_size[1], input_size[0]), mode=\'bilinear\')\n    interp_target = nn.Upsample(size=(input_size_target[1], input_size_target[0]), mode=\'bilinear\')\n\n    # labels for adversarial training\n    source_label = 0\n    target_label = 1\n\n    for i_iter in range(args.num_steps):\n\n        loss_seg_value1 = 0\n        loss_adv_target_value1 = 0\n        loss_D_value1 = 0\n\n        loss_seg_value2 = 0\n        loss_adv_target_value2 = 0\n        loss_D_value2 = 0\n\n        optimizer.zero_grad()\n        adjust_learning_rate(optimizer, i_iter)\n\n        optimizer_D1.zero_grad()\n        optimizer_D2.zero_grad()\n        adjust_learning_rate_D(optimizer_D1, i_iter)\n        adjust_learning_rate_D(optimizer_D2, i_iter)\n\n        for sub_i in range(args.iter_size):\n\n            # train G\n\n            # don\'t accumulate grads in D\n            for param in model_D1.parameters():\n                param.requires_grad = False\n\n            for param in model_D2.parameters():\n                param.requires_grad = False\n\n            # train with source\n\n            _, batch = trainloader_iter.next()\n            images, labels, _, _ = batch\n            images = Variable(images).cuda(args.gpu)\n\n            pred1, pred2 = model(images)\n            pred1 = interp(pred1)\n            pred2 = interp(pred2)\n\n            loss_seg1 = loss_calc(pred1, labels, args.gpu)\n            loss_seg2 = loss_calc(pred2, labels, args.gpu)\n            loss = loss_seg2 + args.lambda_seg * loss_seg1\n\n            # proper normalization\n            loss = loss / args.iter_size\n            loss.backward()\n            loss_seg_value1 += loss_seg1.data.cpu().numpy()[0] / args.iter_size\n            loss_seg_value2 += loss_seg2.data.cpu().numpy()[0] / args.iter_size\n\n            # train with target\n\n            _, batch = targetloader_iter.next()\n            images, _, _ = batch\n            images = Variable(images).cuda(args.gpu)\n\n            pred_target1, pred_target2 = model(images)\n            pred_target1 = interp_target(pred_target1)\n            pred_target2 = interp_target(pred_target2)\n\n            D_out1 = model_D1(F.softmax(pred_target1))\n            D_out2 = model_D2(F.softmax(pred_target2))\n\n            loss_adv_target1 = bce_loss(D_out1,\n                                       Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(\n                                           args.gpu))\n\n            loss_adv_target2 = bce_loss(D_out2,\n                                        Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda(\n                                            args.gpu))\n\n            loss = args.lambda_adv_target1 * loss_adv_target1 + args.lambda_adv_target2 * loss_adv_target2\n            loss = loss / args.iter_size\n            loss.backward()\n            loss_adv_target_value1 += loss_adv_target1.data.cpu().numpy()[0] / args.iter_size\n            loss_adv_target_value2 += loss_adv_target2.data.cpu().numpy()[0] / args.iter_size\n\n            # train D\n\n            # bring back requires_grad\n            for param in model_D1.parameters():\n                param.requires_grad = True\n\n            for param in model_D2.parameters():\n                param.requires_grad = True\n\n            # train with source\n            pred1 = pred1.detach()\n            pred2 = pred2.detach()\n\n            D_out1 = model_D1(F.softmax(pred1))\n            D_out2 = model_D2(F.softmax(pred2))\n\n            loss_D1 = bce_loss(D_out1,\n                              Variable(torch.FloatTensor(D_out1.data.size()).fill_(source_label)).cuda(args.gpu))\n\n            loss_D2 = bce_loss(D_out2,\n                               Variable(torch.FloatTensor(D_out2.data.size()).fill_(source_label)).cuda(args.gpu))\n\n            loss_D1 = loss_D1 / args.iter_size / 2\n            loss_D2 = loss_D2 / args.iter_size / 2\n\n            loss_D1.backward()\n            loss_D2.backward()\n\n            loss_D_value1 += loss_D1.data.cpu().numpy()[0]\n            loss_D_value2 += loss_D2.data.cpu().numpy()[0]\n\n            # train with target\n            pred_target1 = pred_target1.detach()\n            pred_target2 = pred_target2.detach()\n\n            D_out1 = model_D1(F.softmax(pred_target1))\n            D_out2 = model_D2(F.softmax(pred_target2))\n\n            loss_D1 = bce_loss(D_out1,\n                              Variable(torch.FloatTensor(D_out1.data.size()).fill_(target_label)).cuda(args.gpu))\n\n            loss_D2 = bce_loss(D_out2,\n                               Variable(torch.FloatTensor(D_out2.data.size()).fill_(target_label)).cuda(args.gpu))\n\n            loss_D1 = loss_D1 / args.iter_size / 2\n            loss_D2 = loss_D2 / args.iter_size / 2\n\n            loss_D1.backward()\n            loss_D2.backward()\n\n            loss_D_value1 += loss_D1.data.cpu().numpy()[0]\n            loss_D_value2 += loss_D2.data.cpu().numpy()[0]\n\n        optimizer.step()\n        optimizer_D1.step()\n        optimizer_D2.step()\n\n        print(\'exp = {}\'.format(args.snapshot_dir))\n        print(\n        \'iter = {0:8d}/{1:8d}, loss_seg1 = {2:.3f} loss_seg2 = {3:.3f} loss_adv1 = {4:.3f}, loss_adv2 = {5:.3f} loss_D1 = {6:.3f} loss_D2 = {7:.3f}\'.format(\n            i_iter, args.num_steps, loss_seg_value1, loss_seg_value2, loss_adv_target_value1, loss_adv_target_value2, loss_D_value1, loss_D_value2))\n\n        if i_iter >= args.num_steps_stop - 1:\n            print \'save model ...\'\n            torch.save(model.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(args.num_steps_stop) + \'.pth\'))\n            torch.save(model_D1.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(args.num_steps_stop) + \'_D1.pth\'))\n            torch.save(model_D2.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(args.num_steps_stop) + \'_D2.pth\'))\n            break\n\n        if i_iter % args.save_pred_every == 0 and i_iter != 0:\n            print \'taking snapshot ...\'\n            torch.save(model.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(i_iter) + \'.pth\'))\n            torch.save(model_D1.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(i_iter) + \'_D1.pth\'))\n            torch.save(model_D2.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(i_iter) + \'_D2.pth\'))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
dataset/__init__.py,0,b''
dataset/cityscapes_dataset.py,1,"b'import os\nimport os.path as osp\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport collections\nimport torch\nimport torchvision\nfrom torch.utils import data\nfrom PIL import Image\n\nclass cityscapesDataSet(data.Dataset):\n    def __init__(self, root, list_path, max_iters=None, crop_size=(321, 321), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=255, set=\'val\'):\n        self.root = root\n        self.list_path = list_path\n        self.crop_size = crop_size\n        self.scale = scale\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.is_mirror = mirror\n        # self.mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        if not max_iters==None:\n            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n        self.files = []\n        self.set = set\n        # for split in [""train"", ""trainval"", ""val""]:\n        for name in self.img_ids:\n            img_file = osp.join(self.root, ""leftImg8bit/%s/%s"" % (self.set, name))\n            self.files.append({\n                ""img"": img_file,\n                ""name"": name\n            })\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n\n        image = Image.open(datafiles[""img""]).convert(\'RGB\')\n        name = datafiles[""name""]\n\n        # resize\n        image = image.resize(self.crop_size, Image.BICUBIC)\n\n        image = np.asarray(image, np.float32)\n\n        size = image.shape\n        image = image[:, :, ::-1]  # change to BGR\n        image -= self.mean\n        image = image.transpose((2, 0, 1))\n\n        return image.copy(), np.array(size), name\n\n\nif __name__ == \'__main__\':\n    dst = GTA5DataSet(""./data"", is_transform=True)\n    trainloader = data.DataLoader(dst, batch_size=4)\n    for i, data in enumerate(trainloader):\n        imgs, labels = data\n        if i == 0:\n            img = torchvision.utils.make_grid(imgs).numpy()\n            img = np.transpose(img, (1, 2, 0))\n            img = img[:, :, ::-1]\n            plt.imshow(img)\n            plt.show()\n'"
dataset/gta5_dataset.py,1,"b'import os\nimport os.path as osp\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport collections\nimport torch\nimport torchvision\nfrom torch.utils import data\nfrom PIL import Image\n\n\nclass GTA5DataSet(data.Dataset):\n    def __init__(self, root, list_path, max_iters=None, crop_size=(321, 321), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=255):\n        self.root = root\n        self.list_path = list_path\n        self.crop_size = crop_size\n        self.scale = scale\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.is_mirror = mirror\n        # self.mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        if not max_iters==None:\n            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n        self.files = []\n\n        self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n\n        # for split in [""train"", ""trainval"", ""val""]:\n        for name in self.img_ids:\n            img_file = osp.join(self.root, ""images/%s"" % name)\n            label_file = osp.join(self.root, ""labels/%s"" % name)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": name\n            })\n\n    def __len__(self):\n        return len(self.files)\n\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n\n        image = Image.open(datafiles[""img""]).convert(\'RGB\')\n        label = Image.open(datafiles[""label""])\n        name = datafiles[""name""]\n\n        # resize\n        image = image.resize(self.crop_size, Image.BICUBIC)\n        label = label.resize(self.crop_size, Image.NEAREST)\n\n        image = np.asarray(image, np.float32)\n        label = np.asarray(label, np.float32)\n\n        # re-assign labels to match the format of Cityscapes\n        label_copy = 255 * np.ones(label.shape, dtype=np.float32)\n        for k, v in self.id_to_trainid.items():\n            label_copy[label == k] = v\n\n        size = image.shape\n        image = image[:, :, ::-1]  # change to BGR\n        image -= self.mean\n        image = image.transpose((2, 0, 1))\n\n        return image.copy(), label_copy.copy(), np.array(size), name\n\n\nif __name__ == \'__main__\':\n    dst = GTA5DataSet(""./data"", is_transform=True)\n    trainloader = data.DataLoader(dst, batch_size=4)\n    for i, data in enumerate(trainloader):\n        imgs, labels = data\n        if i == 0:\n            img = torchvision.utils.make_grid(imgs).numpy()\n            img = np.transpose(img, (1, 2, 0))\n            img = img[:, :, ::-1]\n            plt.imshow(img)\n            plt.show()\n'"
model/__init__.py,0,b''
model/deeplab.py,2,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport numpy as np\naffine_par = True\n\n\ndef outS(i):\n    i = int(i)\n    i = (i+1)/2\n    i = int(np.ceil((i+1)/2.0))\n    i = (i+1)/2\n    return i\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, affine = affine_par)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, affine = affine_par)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n        self.bn1 = nn.BatchNorm2d(planes,affine = affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n\n        padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n                               padding=padding, bias=False, dilation = dilation)\n        self.bn2 = nn.BatchNorm2d(planes,affine = affine_par)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4, affine = affine_par)\n        for i in self.bn3.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Classifier_Module(nn.Module):\n\n    def __init__(self, dilation_series, padding_series, num_classes):\n        super(Classifier_Module, self).__init__()\n        self.conv2d_list = nn.ModuleList()\n        for dilation, padding in zip(dilation_series, padding_series):\n            self.conv2d_list.append(nn.Conv2d(2048, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias = True))\n\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list)-1):\n            out += self.conv2d_list[i+1](x)\n            return out\n\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, affine = affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        self.layer5 = self._make_pred_layer(Classifier_Module, [6,12,18,24],[6,12,18,24],num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        #        for i in m.parameters():\n        #            i.requires_grad = False\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion or dilation == 2 or dilation == 4:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion,affine = affine_par))\n        for i in downsample._modules[\'1\'].parameters():\n            i.requires_grad = False\n        layers = []\n        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n    def _make_pred_layer(self,block, dilation_series, padding_series,num_classes):\n        return block(dilation_series,padding_series,num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n\n        return x\n\n    def get_1x_lr_params_NOscale(self):\n        """"""\n        This generator returns all the parameters of the net except for \n        the last classification layer. Note that for each batchnorm layer, \n        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return \n        any batchnorm parameter\n        """"""\n        b = []\n\n        b.append(self.conv1)\n        b.append(self.bn1)\n        b.append(self.layer1)\n        b.append(self.layer2)\n        b.append(self.layer3)\n        b.append(self.layer4)\n\n    \n        for i in range(len(b)):\n            for j in b[i].modules():\n                jj = 0\n                for k in j.parameters():\n                    jj+=1\n                    if k.requires_grad:\n                        yield k\n\n    def get_10x_lr_params(self):\n        """"""\n        This generator returns all the parameters for the last layer of the net,\n        which does the classification of pixel into classes\n        """"""\n        b = []\n        b.append(self.layer5.parameters())\n\n        for j in range(len(b)):\n            for i in b[j]:\n                yield i\n            \n\n\n    def optim_parameters(self, args):\n        return [{\'params\': self.get_1x_lr_params_NOscale(), \'lr\': args.learning_rate},\n                {\'params\': self.get_10x_lr_params(), \'lr\': 10*args.learning_rate}] \n\n\ndef Res_Deeplab(num_classes=21):\n    model = ResNet(Bottleneck,[3, 4, 23, 3], num_classes)\n    return model\n\n'"
model/deeplab_multi.py,2,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport numpy as np\n\naffine_par = True\n\n\ndef outS(i):\n    i = int(i)\n    i = (i + 1) / 2\n    i = int(np.ceil((i + 1) / 2.0))\n    i = (i + 1) / 2\n    return i\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)  # change\n        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n\n        padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,  # change\n                               padding=padding, bias=False, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4, affine=affine_par)\n        for i in self.bn3.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Classifier_Module(nn.Module):\n    def __init__(self, inplanes, dilation_series, padding_series, num_classes):\n        super(Classifier_Module, self).__init__()\n        self.conv2d_list = nn.ModuleList()\n        for dilation, padding in zip(dilation_series, padding_series):\n            self.conv2d_list.append(\n                nn.Conv2d(inplanes, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias=True))\n\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list) - 1):\n            out += self.conv2d_list[i + 1](x)\n            return out\n\n\nclass ResNetMulti(nn.Module):\n    def __init__(self, block, layers, num_classes):\n        self.inplanes = 64\n        super(ResNetMulti, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        self.layer5 = self._make_pred_layer(Classifier_Module, 1024, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)\n        self.layer6 = self._make_pred_layer(Classifier_Module, 2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                #        for i in m.parameters():\n                #            i.requires_grad = False\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion or dilation == 2 or dilation == 4:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, affine=affine_par))\n        for i in downsample._modules[\'1\'].parameters():\n            i.requires_grad = False\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def _make_pred_layer(self, block, inplanes, dilation_series, padding_series, num_classes):\n        return block(inplanes, dilation_series, padding_series, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n\n        x = self.layer3(x)\n        x1 = self.layer5(x)\n\n        x2 = self.layer4(x)\n        x2 = self.layer6(x2)\n\n        return x1, x2\n\n    def get_1x_lr_params_NOscale(self):\n        """"""\n        This generator returns all the parameters of the net except for\n        the last classification layer. Note that for each batchnorm layer,\n        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n        any batchnorm parameter\n        """"""\n        b = []\n\n        b.append(self.conv1)\n        b.append(self.bn1)\n        b.append(self.layer1)\n        b.append(self.layer2)\n        b.append(self.layer3)\n        b.append(self.layer4)\n\n        for i in range(len(b)):\n            for j in b[i].modules():\n                jj = 0\n                for k in j.parameters():\n                    jj += 1\n                    if k.requires_grad:\n                        yield k\n\n    def get_10x_lr_params(self):\n        """"""\n        This generator returns all the parameters for the last layer of the net,\n        which does the classification of pixel into classes\n        """"""\n        b = []\n        b.append(self.layer5.parameters())\n        b.append(self.layer6.parameters())\n\n        for j in range(len(b)):\n            for i in b[j]:\n                yield i\n\n    def optim_parameters(self, args):\n        return [{\'params\': self.get_1x_lr_params_NOscale(), \'lr\': args.learning_rate},\n                {\'params\': self.get_10x_lr_params(), \'lr\': 10 * args.learning_rate}]\n\n\ndef DeeplabMulti(num_classes=21):\n    model = ResNetMulti(Bottleneck, [3, 4, 23, 3], num_classes)\n    return model\n\n'"
model/deeplab_vgg.py,1,"b'import numpy as np\nimport torch\nfrom torch import nn\nfrom torchvision import models\n\nclass Classifier_Module(nn.Module):\n\n    def __init__(self, dims_in, dilation_series, padding_series, num_classes):\n        super(Classifier_Module, self).__init__()\n        self.conv2d_list = nn.ModuleList()\n        for dilation, padding in zip(dilation_series, padding_series):\n            self.conv2d_list.append(nn.Conv2d(dims_in, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias = True))\n\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list)-1):\n            out += self.conv2d_list[i+1](x)\n            return out\n\n\nclass DeeplabVGG(nn.Module):\n    def __init__(self, num_classes, vgg16_caffe_path=None, pretrained=False):\n        super(DeeplabVGG, self).__init__()\n        vgg = models.vgg16()\n        if pretrained:\n            vgg.load_state_dict(torch.load(vgg16_caffe_path))\n\n        features, classifier = list(vgg.features.children()), list(vgg.classifier.children())\n\n        #remove pool4/pool5\n        features = nn.Sequential(*(features[i] for i in range(23)+range(24,30)))\n\n        for i in [23,25,27]:\n            features[i].dilation = (2,2)\n            features[i].padding = (2,2)\n\n        fc6 = nn.Conv2d(512, 1024, kernel_size=3, padding=4, dilation=4)\n        fc7 = nn.Conv2d(1024, 1024, kernel_size=3, padding=4, dilation=4)\n\n        self.features = nn.Sequential(*([features[i] for i in range(len(features))] + [ fc6, nn.ReLU(inplace=True), fc7, nn.ReLU(inplace=True)]))\n\n        self.classifier = Classifier_Module(1024, [6,12,18,24],[6,12,18,24],num_classes)\n\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n    def optim_parameters(self, args):\n        return self.parameters()\n'"
model/discriminator.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FCDiscriminator(nn.Module):\n\n\tdef __init__(self, num_classes, ndf = 64):\n\t\tsuper(FCDiscriminator, self).__init__()\n\n\t\tself.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n\t\tself.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n\t\tself.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n\t\tself.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n\t\tself.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n\n\t\tself.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\t\t#self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n\t\t#self.sigmoid = nn.Sigmoid()\n\n\n\tdef forward(self, x):\n\t\tx = self.conv1(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv2(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv3(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.conv4(x)\n\t\tx = self.leaky_relu(x)\n\t\tx = self.classifier(x)\n\t\t#x = self.up_sample(x)\n\t\t#x = self.sigmoid(x) \n\n\t\treturn x\n"""
pytorch_0.4/evaluate_cityscapes.py,6,"b'import argparse\nimport scipy\nfrom scipy import ndimage\nimport numpy as np\nimport sys\n\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.models as models\nimport torch.nn.functional as F\nfrom torch.utils import data, model_zoo\nfrom model.deeplab import Res_Deeplab\nfrom model.deeplab_multi import DeeplabMulti\nfrom model.deeplab_vgg import DeeplabVGG\nfrom dataset.cityscapes_dataset import cityscapesDataSet\nfrom collections import OrderedDict\nimport os\nfrom PIL import Image\n\nimport torch.nn as nn\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n\nDATA_DIRECTORY = \'./data/Cityscapes/data\'\nDATA_LIST_PATH = \'./dataset/cityscapes_list/val.txt\'\nSAVE_PATH = \'./result/cityscapes\'\n\nIGNORE_LABEL = 255\nNUM_CLASSES = 19\nNUM_STEPS = 500 # Number of images in the validation set.\nRESTORE_FROM = \'http://vllab.ucmerced.edu/ytsai/CVPR18/GTA2Cityscapes_multi-ed35151c.pth\'\nRESTORE_FROM_VGG = \'http://vllab.ucmerced.edu/ytsai/CVPR18/GTA2Cityscapes_vgg-ac4ac9f6.pth\'\nRESTORE_FROM_ORC = \'http://vllab1.ucmerced.edu/~whung/adaptSeg/cityscapes_oracle-b7b9934.pth\'\nSET = \'val\'\n\nMODEL = \'DeeplabMulti\'\n\npalette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\nzero_pad = 256 * 3 - len(palette)\nfor i in range(zero_pad):\n    palette.append(0)\n\n\ndef colorize_mask(mask):\n    # mask: numpy array of the mask\n    new_mask = Image.fromarray(mask.astype(np.uint8)).convert(\'P\')\n    new_mask.putpalette(palette)\n\n    return new_mask\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n\n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""DeepLab-ResNet Network"")\n    parser.add_argument(""--model"", type=str, default=MODEL,\n                        help=""Model Choice (DeeplabMulti/DeeplabVGG/Oracle)."")\n    parser.add_argument(""--data-dir"", type=str, default=DATA_DIRECTORY,\n                        help=""Path to the directory containing the Cityscapes dataset."")\n    parser.add_argument(""--data-list"", type=str, default=DATA_LIST_PATH,\n                        help=""Path to the file listing the images in the dataset."")\n    parser.add_argument(""--ignore-label"", type=int, default=IGNORE_LABEL,\n                        help=""The index of the label to ignore during the training."")\n    parser.add_argument(""--num-classes"", type=int, default=NUM_CLASSES,\n                        help=""Number of classes to predict (including background)."")\n    parser.add_argument(""--restore-from"", type=str, default=RESTORE_FROM,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--set"", type=str, default=SET,\n                        help=""choose evaluation set."")\n    parser.add_argument(""--save"", type=str, default=SAVE_PATH,\n                        help=""Path to save result."")\n    parser.add_argument(""--cpu"", action=\'store_true\', help=""choose to use cpu device."")\n    return parser.parse_args()\n\n\ndef main():\n    """"""Create the model and start the evaluation process.""""""\n\n    args = get_arguments()\n\n    if not os.path.exists(args.save):\n        os.makedirs(args.save)\n\n    if args.model == \'DeeplabMulti\':\n        model = DeeplabMulti(num_classes=args.num_classes)\n    elif args.model == \'Oracle\':\n        model = Res_Deeplab(num_classes=args.num_classes)\n        if args.restore_from == RESTORE_FROM:\n            args.restore_from = RESTORE_FROM_ORC\n    elif args.model == \'DeeplabVGG\':\n        model = DeeplabVGG(num_classes=args.num_classes)\n        if args.restore_from == RESTORE_FROM:\n            args.restore_from = RESTORE_FROM_VGG\n\n    if args.restore_from[:4] == \'http\' :\n        saved_state_dict = model_zoo.load_url(args.restore_from)\n    else:\n        saved_state_dict = torch.load(args.restore_from)\n    ### for running different versions of pytorch\n    model_dict = model.state_dict()\n    saved_state_dict = {k: v for k, v in saved_state_dict.items() if k in model_dict}\n    model_dict.update(saved_state_dict)\n    ###\n    model.load_state_dict(saved_state_dict)\n\n    device = torch.device(""cuda"" if not args.cpu else ""cpu"")\n    model = model.to(device)\n\n    model.eval()\n\n    testloader = data.DataLoader(cityscapesDataSet(args.data_dir, args.data_list, crop_size=(1024, 512), mean=IMG_MEAN, scale=False, mirror=False, set=args.set),\n                                    batch_size=1, shuffle=False, pin_memory=True)\n\n    interp = nn.Upsample(size=(1024, 2048), mode=\'bilinear\', align_corners=True)\n\n    for index, batch in enumerate(testloader):\n        if index % 100 == 0:\n            print(\'%d processd\' % index)\n        image, _, name = batch\n        image = image.to(device)\n\n        if args.model == \'DeeplabMulti\':\n            output1, output2 = model(image)\n            output = interp(output2).cpu().data[0].numpy()\n        elif args.model == \'DeeplabVGG\' or args.model == \'Oracle\':\n            output = model(image)\n            output = interp(output).cpu().data[0].numpy()\n\n        output = output.transpose(1,2,0)\n        output = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n\n        output_col = colorize_mask(output)\n        output = Image.fromarray(output)\n\n        name = name[0].split(\'/\')[-1]\n        output.save(\'%s/%s\' % (args.save, name))\n        output_col.save(\'%s/%s_color.png\' % (args.save, name.split(\'.\')[0]))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch_0.4/train_gta2cityscapes_multi.py,23,"b'import argparse\nimport torch\nimport torch.nn as nn\nfrom torch.utils import data, model_zoo\nimport numpy as np\nimport pickle\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport scipy.misc\nimport torch.backends.cudnn as cudnn\nimport torch.nn.functional as F\nimport sys\nimport os\nimport os.path as osp\nimport random\nfrom tensorboardX import SummaryWriter\n\nfrom model.deeplab_multi import DeeplabMulti\nfrom model.discriminator import FCDiscriminator\nfrom utils.loss import CrossEntropy2d\nfrom dataset.gta5_dataset import GTA5DataSet\nfrom dataset.cityscapes_dataset import cityscapesDataSet\n\nIMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)\n\nMODEL = \'DeepLab\'\nBATCH_SIZE = 1\nITER_SIZE = 1\nNUM_WORKERS = 4\nDATA_DIRECTORY = \'./data/GTA5\'\nDATA_LIST_PATH = \'./dataset/gta5_list/train.txt\'\nIGNORE_LABEL = 255\nINPUT_SIZE = \'1280,720\'\nDATA_DIRECTORY_TARGET = \'./data/Cityscapes/data\'\nDATA_LIST_PATH_TARGET = \'./dataset/cityscapes_list/train.txt\'\nINPUT_SIZE_TARGET = \'1024,512\'\nLEARNING_RATE = 2.5e-4\nMOMENTUM = 0.9\nNUM_CLASSES = 19\nNUM_STEPS = 250000\nNUM_STEPS_STOP = 150000  # early stopping\nPOWER = 0.9\nRANDOM_SEED = 1234\nRESTORE_FROM = \'http://vllab.ucmerced.edu/ytsai/CVPR18/DeepLab_resnet_pretrained_init-f81d91e8.pth\'\nSAVE_NUM_IMAGES = 2\nSAVE_PRED_EVERY = 5000\nSNAPSHOT_DIR = \'./snapshots/\'\nWEIGHT_DECAY = 0.0005\nLOG_DIR = \'./log\'\n\nLEARNING_RATE_D = 1e-4\nLAMBDA_SEG = 0.1\nLAMBDA_ADV_TARGET1 = 0.0002\nLAMBDA_ADV_TARGET2 = 0.001\nGAN = \'Vanilla\'\n\nTARGET = \'cityscapes\'\nSET = \'train\'\n\n\ndef get_arguments():\n    """"""Parse all the arguments provided from the CLI.\n\n    Returns:\n      A list of parsed arguments.\n    """"""\n    parser = argparse.ArgumentParser(description=""DeepLab-ResNet Network"")\n    parser.add_argument(""--model"", type=str, default=MODEL,\n                        help=""available options : DeepLab"")\n    parser.add_argument(""--target"", type=str, default=TARGET,\n                        help=""available options : cityscapes"")\n    parser.add_argument(""--batch-size"", type=int, default=BATCH_SIZE,\n                        help=""Number of images sent to the network in one step."")\n    parser.add_argument(""--iter-size"", type=int, default=ITER_SIZE,\n                        help=""Accumulate gradients for ITER_SIZE iterations."")\n    parser.add_argument(""--num-workers"", type=int, default=NUM_WORKERS,\n                        help=""number of workers for multithread dataloading."")\n    parser.add_argument(""--data-dir"", type=str, default=DATA_DIRECTORY,\n                        help=""Path to the directory containing the source dataset."")\n    parser.add_argument(""--data-list"", type=str, default=DATA_LIST_PATH,\n                        help=""Path to the file listing the images in the source dataset."")\n    parser.add_argument(""--ignore-label"", type=int, default=IGNORE_LABEL,\n                        help=""The index of the label to ignore during the training."")\n    parser.add_argument(""--input-size"", type=str, default=INPUT_SIZE,\n                        help=""Comma-separated string with height and width of source images."")\n    parser.add_argument(""--data-dir-target"", type=str, default=DATA_DIRECTORY_TARGET,\n                        help=""Path to the directory containing the target dataset."")\n    parser.add_argument(""--data-list-target"", type=str, default=DATA_LIST_PATH_TARGET,\n                        help=""Path to the file listing the images in the target dataset."")\n    parser.add_argument(""--input-size-target"", type=str, default=INPUT_SIZE_TARGET,\n                        help=""Comma-separated string with height and width of target images."")\n    parser.add_argument(""--is-training"", action=""store_true"",\n                        help=""Whether to updates the running means and variances during the training."")\n    parser.add_argument(""--learning-rate"", type=float, default=LEARNING_RATE,\n                        help=""Base learning rate for training with polynomial decay."")\n    parser.add_argument(""--learning-rate-D"", type=float, default=LEARNING_RATE_D,\n                        help=""Base learning rate for discriminator."")\n    parser.add_argument(""--lambda-seg"", type=float, default=LAMBDA_SEG,\n                        help=""lambda_seg."")\n    parser.add_argument(""--lambda-adv-target1"", type=float, default=LAMBDA_ADV_TARGET1,\n                        help=""lambda_adv for adversarial training."")\n    parser.add_argument(""--lambda-adv-target2"", type=float, default=LAMBDA_ADV_TARGET2,\n                        help=""lambda_adv for adversarial training."")\n    parser.add_argument(""--momentum"", type=float, default=MOMENTUM,\n                        help=""Momentum component of the optimiser."")\n    parser.add_argument(""--not-restore-last"", action=""store_true"",\n                        help=""Whether to not restore last (FC) layers."")\n    parser.add_argument(""--num-classes"", type=int, default=NUM_CLASSES,\n                        help=""Number of classes to predict (including background)."")\n    parser.add_argument(""--num-steps"", type=int, default=NUM_STEPS,\n                        help=""Number of training steps."")\n    parser.add_argument(""--num-steps-stop"", type=int, default=NUM_STEPS_STOP,\n                        help=""Number of training steps for early stopping."")\n    parser.add_argument(""--power"", type=float, default=POWER,\n                        help=""Decay parameter to compute the learning rate."")\n    parser.add_argument(""--random-mirror"", action=""store_true"",\n                        help=""Whether to randomly mirror the inputs during the training."")\n    parser.add_argument(""--random-scale"", action=""store_true"",\n                        help=""Whether to randomly scale the inputs during the training."")\n    parser.add_argument(""--random-seed"", type=int, default=RANDOM_SEED,\n                        help=""Random seed to have reproducible results."")\n    parser.add_argument(""--restore-from"", type=str, default=RESTORE_FROM,\n                        help=""Where restore model parameters from."")\n    parser.add_argument(""--save-num-images"", type=int, default=SAVE_NUM_IMAGES,\n                        help=""How many images to save."")\n    parser.add_argument(""--save-pred-every"", type=int, default=SAVE_PRED_EVERY,\n                        help=""Save summaries and checkpoint every often."")\n    parser.add_argument(""--snapshot-dir"", type=str, default=SNAPSHOT_DIR,\n                        help=""Where to save snapshots of the model."")\n    parser.add_argument(""--weight-decay"", type=float, default=WEIGHT_DECAY,\n                        help=""Regularisation parameter for L2-loss."")\n    parser.add_argument(""--cpu"", action=\'store_true\', help=""choose to use cpu device."")\n    parser.add_argument(""--tensorboard"", action=\'store_true\', help=""choose whether to use tensorboard."")\n    parser.add_argument(""--log-dir"", type=str, default=LOG_DIR,\n                        help=""Path to the directory of log."")\n    parser.add_argument(""--set"", type=str, default=SET,\n                        help=""choose adaptation set."")\n    parser.add_argument(""--gan"", type=str, default=GAN,\n                        help=""choose the GAN objective."")\n    return parser.parse_args()\n\n\nargs = get_arguments()\n\n\ndef lr_poly(base_lr, iter, max_iter, power):\n    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n\n\ndef adjust_learning_rate(optimizer, i_iter):\n    lr = lr_poly(args.learning_rate, i_iter, args.num_steps, args.power)\n    optimizer.param_groups[0][\'lr\'] = lr\n    if len(optimizer.param_groups) > 1:\n        optimizer.param_groups[1][\'lr\'] = lr * 10\n\n\ndef adjust_learning_rate_D(optimizer, i_iter):\n    lr = lr_poly(args.learning_rate_D, i_iter, args.num_steps, args.power)\n    optimizer.param_groups[0][\'lr\'] = lr\n    if len(optimizer.param_groups) > 1:\n        optimizer.param_groups[1][\'lr\'] = lr * 10\n\n\ndef main():\n    """"""Create the model and start the training.""""""\n\n    device = torch.device(""cuda"" if not args.cpu else ""cpu"")\n\n    w, h = map(int, args.input_size.split(\',\'))\n    input_size = (w, h)\n\n    w, h = map(int, args.input_size_target.split(\',\'))\n    input_size_target = (w, h)\n\n    cudnn.enabled = True\n\n    # Create network\n    if args.model == \'DeepLab\':\n        model = DeeplabMulti(num_classes=args.num_classes)\n        if args.restore_from[:4] == \'http\' :\n            saved_state_dict = model_zoo.load_url(args.restore_from)\n        else:\n            saved_state_dict = torch.load(args.restore_from)\n\n        new_params = model.state_dict().copy()\n        for i in saved_state_dict:\n            # Scale.layer5.conv2d_list.3.weight\n            i_parts = i.split(\'.\')\n            # print i_parts\n            if not args.num_classes == 19 or not i_parts[1] == \'layer5\':\n                new_params[\'.\'.join(i_parts[1:])] = saved_state_dict[i]\n                # print i_parts\n        model.load_state_dict(new_params)\n\n    model.train()\n    model.to(device)\n\n    cudnn.benchmark = True\n\n    # init D\n    model_D1 = FCDiscriminator(num_classes=args.num_classes).to(device)\n    model_D2 = FCDiscriminator(num_classes=args.num_classes).to(device)\n\n    model_D1.train()\n    model_D1.to(device)\n\n    model_D2.train()\n    model_D2.to(device)\n\n    if not os.path.exists(args.snapshot_dir):\n        os.makedirs(args.snapshot_dir)\n\n    trainloader = data.DataLoader(\n        GTA5DataSet(args.data_dir, args.data_list, max_iters=args.num_steps * args.iter_size * args.batch_size,\n                    crop_size=input_size,\n                    scale=args.random_scale, mirror=args.random_mirror, mean=IMG_MEAN),\n        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n\n    trainloader_iter = enumerate(trainloader)\n\n    targetloader = data.DataLoader(cityscapesDataSet(args.data_dir_target, args.data_list_target,\n                                                     max_iters=args.num_steps * args.iter_size * args.batch_size,\n                                                     crop_size=input_size_target,\n                                                     scale=False, mirror=args.random_mirror, mean=IMG_MEAN,\n                                                     set=args.set),\n                                   batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers,\n                                   pin_memory=True)\n\n\n    targetloader_iter = enumerate(targetloader)\n\n    # implement model.optim_parameters(args) to handle different models\' lr setting\n\n    optimizer = optim.SGD(model.optim_parameters(args),\n                          lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n    optimizer.zero_grad()\n\n    optimizer_D1 = optim.Adam(model_D1.parameters(), lr=args.learning_rate_D, betas=(0.9, 0.99))\n    optimizer_D1.zero_grad()\n\n    optimizer_D2 = optim.Adam(model_D2.parameters(), lr=args.learning_rate_D, betas=(0.9, 0.99))\n    optimizer_D2.zero_grad()\n\n    if args.gan == \'Vanilla\':\n        bce_loss = torch.nn.BCEWithLogitsLoss()\n    elif args.gan == \'LS\':\n        bce_loss = torch.nn.MSELoss()\n    seg_loss = torch.nn.CrossEntropyLoss(ignore_index=255)\n\n    interp = nn.Upsample(size=(input_size[1], input_size[0]), mode=\'bilinear\', align_corners=True)\n    interp_target = nn.Upsample(size=(input_size_target[1], input_size_target[0]), mode=\'bilinear\', align_corners=True)\n\n    # labels for adversarial training\n    source_label = 0\n    target_label = 1\n\n    # set up tensor board\n    if args.tensorboard:\n        if not os.path.exists(args.log_dir):\n            os.makedirs(args.log_dir)\n\n        writer = SummaryWriter(args.log_dir)\n\n    for i_iter in range(args.num_steps):\n\n        loss_seg_value1 = 0\n        loss_adv_target_value1 = 0\n        loss_D_value1 = 0\n\n        loss_seg_value2 = 0\n        loss_adv_target_value2 = 0\n        loss_D_value2 = 0\n\n        optimizer.zero_grad()\n        adjust_learning_rate(optimizer, i_iter)\n\n        optimizer_D1.zero_grad()\n        optimizer_D2.zero_grad()\n        adjust_learning_rate_D(optimizer_D1, i_iter)\n        adjust_learning_rate_D(optimizer_D2, i_iter)\n\n        for sub_i in range(args.iter_size):\n\n            # train G\n\n            # don\'t accumulate grads in D\n            for param in model_D1.parameters():\n                param.requires_grad = False\n\n            for param in model_D2.parameters():\n                param.requires_grad = False\n\n            # train with source\n\n            _, batch = trainloader_iter.__next__()\n\n            images, labels, _, _ = batch\n            images = images.to(device)\n            labels = labels.long().to(device)\n\n            pred1, pred2 = model(images)\n            pred1 = interp(pred1)\n            pred2 = interp(pred2)\n\n            loss_seg1 = seg_loss(pred1, labels)\n            loss_seg2 = seg_loss(pred2, labels)\n            loss = loss_seg2 + args.lambda_seg * loss_seg1\n\n            # proper normalization\n            loss = loss / args.iter_size\n            loss.backward()\n            loss_seg_value1 += loss_seg1.item() / args.iter_size\n            loss_seg_value2 += loss_seg2.item() / args.iter_size\n\n            # train with target\n\n            _, batch = targetloader_iter.__next__()\n            images, _, _ = batch\n            images = images.to(device)\n\n            pred_target1, pred_target2 = model(images)\n            pred_target1 = interp_target(pred_target1)\n            pred_target2 = interp_target(pred_target2)\n\n            D_out1 = model_D1(F.softmax(pred_target1))\n            D_out2 = model_D2(F.softmax(pred_target2))\n\n            loss_adv_target1 = bce_loss(D_out1, torch.FloatTensor(D_out1.data.size()).fill_(source_label).to(device))\n\n            loss_adv_target2 = bce_loss(D_out2, torch.FloatTensor(D_out2.data.size()).fill_(source_label).to(device))\n\n            loss = args.lambda_adv_target1 * loss_adv_target1 + args.lambda_adv_target2 * loss_adv_target2\n            loss = loss / args.iter_size\n            loss.backward()\n            loss_adv_target_value1 += loss_adv_target1.item() / args.iter_size\n            loss_adv_target_value2 += loss_adv_target2.item() / args.iter_size\n\n            # train D\n\n            # bring back requires_grad\n            for param in model_D1.parameters():\n                param.requires_grad = True\n\n            for param in model_D2.parameters():\n                param.requires_grad = True\n\n            # train with source\n            pred1 = pred1.detach()\n            pred2 = pred2.detach()\n\n            D_out1 = model_D1(F.softmax(pred1))\n            D_out2 = model_D2(F.softmax(pred2))\n\n            loss_D1 = bce_loss(D_out1, torch.FloatTensor(D_out1.data.size()).fill_(source_label).to(device))\n\n            loss_D2 = bce_loss(D_out2, torch.FloatTensor(D_out2.data.size()).fill_(source_label).to(device))\n\n            loss_D1 = loss_D1 / args.iter_size / 2\n            loss_D2 = loss_D2 / args.iter_size / 2\n\n            loss_D1.backward()\n            loss_D2.backward()\n\n            loss_D_value1 += loss_D1.item()\n            loss_D_value2 += loss_D2.item()\n\n            # train with target\n            pred_target1 = pred_target1.detach()\n            pred_target2 = pred_target2.detach()\n\n            D_out1 = model_D1(F.softmax(pred_target1))\n            D_out2 = model_D2(F.softmax(pred_target2))\n\n            loss_D1 = bce_loss(D_out1, torch.FloatTensor(D_out1.data.size()).fill_(target_label).to(device))\n\n            loss_D2 = bce_loss(D_out2, torch.FloatTensor(D_out2.data.size()).fill_(target_label).to(device))\n\n            loss_D1 = loss_D1 / args.iter_size / 2\n            loss_D2 = loss_D2 / args.iter_size / 2\n\n            loss_D1.backward()\n            loss_D2.backward()\n\n            loss_D_value1 += loss_D1.item()\n            loss_D_value2 += loss_D2.item()\n\n        optimizer.step()\n        optimizer_D1.step()\n        optimizer_D2.step()\n\n        if args.tensorboard:\n            scalar_info = {\n                \'loss_seg1\': loss_seg_value1,\n                \'loss_seg2\': loss_seg_value2,\n                \'loss_adv_target1\': loss_adv_target_value1,\n                \'loss_adv_target2\': loss_adv_target_value2,\n                \'loss_D1\': loss_D_value1,\n                \'loss_D2\': loss_D_value2,\n            }\n\n            if i_iter % 10 == 0:\n                for key, val in scalar_info.items():\n                    writer.add_scalar(key, val, i_iter)\n\n        print(\'exp = {}\'.format(args.snapshot_dir))\n        print(\n        \'iter = {0:8d}/{1:8d}, loss_seg1 = {2:.3f} loss_seg2 = {3:.3f} loss_adv1 = {4:.3f}, loss_adv2 = {5:.3f} loss_D1 = {6:.3f} loss_D2 = {7:.3f}\'.format(\n            i_iter, args.num_steps, loss_seg_value1, loss_seg_value2, loss_adv_target_value1, loss_adv_target_value2, loss_D_value1, loss_D_value2))\n\n        if i_iter >= args.num_steps_stop - 1:\n            print(\'save model ...\')\n            torch.save(model.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(args.num_steps_stop) + \'.pth\'))\n            torch.save(model_D1.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(args.num_steps_stop) + \'_D1.pth\'))\n            torch.save(model_D2.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(args.num_steps_stop) + \'_D2.pth\'))\n            break\n\n        if i_iter % args.save_pred_every == 0 and i_iter != 0:\n            print(\'taking snapshot ...\')\n            torch.save(model.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(i_iter) + \'.pth\'))\n            torch.save(model_D1.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(i_iter) + \'_D1.pth\'))\n            torch.save(model_D2.state_dict(), osp.join(args.snapshot_dir, \'GTA5_\' + str(i_iter) + \'_D2.pth\'))\n\n    if args.tensorboard:\n        writer.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/__init__.py,0,b''
utils/loss.py,4,"b'import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\nclass CrossEntropy2d(nn.Module):\n\n    def __init__(self, size_average=True, ignore_label=255):\n        super(CrossEntropy2d, self).__init__()\n        self.size_average = size_average\n        self.ignore_label = ignore_label\n\n    def forward(self, predict, target, weight=None):\n        """"""\n            Args:\n                predict:(n, c, h, w)\n                target:(n, h, w)\n                weight (Tensor, optional): a manual rescaling weight given to each class.\n                                           If given, has to be a Tensor of size ""nclasses""\n        """"""\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 3\n        assert predict.size(0) == target.size(0), ""{0} vs {1} "".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(1), ""{0} vs {1} "".format(predict.size(2), target.size(1))\n        assert predict.size(3) == target.size(2), ""{0} vs {1} "".format(predict.size(3), target.size(3))\n        n, c, h, w = predict.size()\n        target_mask = (target >= 0) * (target != self.ignore_label)\n        target = target[target_mask]\n        if not target.data.dim():\n            return Variable(torch.zeros(1))\n        predict = predict.transpose(1, 2).transpose(2, 3).contiguous()\n        predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)\n        loss = F.cross_entropy(predict, target, weight=weight, size_average=self.size_average)\n        return loss\n'"
