file_path,api_count,code
dataset.py,3,"b'"""""" train and test dataset\n\nauthor baiyu\n""""""\nimport os\nimport sys\nimport pickle\n\nfrom skimage import io\nimport matplotlib.pyplot as plt\nimport numpy \nimport torch\nfrom torch.utils.data import Dataset\n\nclass CIFAR100Train(Dataset):\n    """"""cifar100 test dataset, derived from\n    torch.utils.data.DataSet\n    """"""\n\n    def __init__(self, path, transform=None):\n        #if transform is given, we transoform data using\n        with open(os.path.join(path, \'train\'), \'rb\') as cifar100:\n            self.data = pickle.load(cifar100, encoding=\'bytes\')\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data[\'fine_labels\'.encode()])\n\n    def __getitem__(self, index):\n        label = self.data[\'fine_labels\'.encode()][index]\n        r = self.data[\'data\'.encode()][index, :1024].reshape(32, 32)\n        g = self.data[\'data\'.encode()][index, 1024:2048].reshape(32, 32)\n        b = self.data[\'data\'.encode()][index, 2048:].reshape(32, 32)\n        image = numpy.dstack((r, g, b))\n\n        if self.transform:\n            image = self.transform(image)\n        return label, image\n\nclass CIFAR100Test(Dataset):\n    """"""cifar100 test dataset, derived from\n    torch.utils.data.DataSet\n    """"""\n\n    def __init__(self, path, transform=None):\n        with open(os.path.join(path, \'test\'), \'rb\') as cifar100:\n            self.data = pickle.load(cifar100, encoding=\'bytes\')\n        self.transform = transform \n\n    def __len__(self):\n        return len(self.data[\'data\'.encode()])\n    \n    def __getitem__(self, index):\n        label = self.data[\'fine_labels\'.encode()][index]\n        r = self.data[\'data\'.encode()][index, :1024].reshape(32, 32)\n        g = self.data[\'data\'.encode()][index, 1024:2048].reshape(32, 32)\n        b = self.data[\'data\'.encode()][index, 2048:].reshape(32, 32)\n        image = numpy.dstack((r, g, b))\n\n        if self.transform:\n            image = self.transform(image)\n        return label, image\n\n'"
lr_finder.py,5,"b'\nimport argparse\nimport glob\nimport os\n\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\n#from PIL import Image\n#import transforms \nfrom torchvision import transforms\n#from tensorboardX import SummaryWriter\nfrom conf import settings\nfrom utils import *\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\n\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass FindLR(_LRScheduler):\n    """"""exponentially increasing learning rate\n\n    Args:\n        optimizer: optimzier(e.g. SGD)\n        num_iter: totoal_iters \n        max_lr: maximum  learning rate\n    """"""\n    def __init__(self, optimizer, max_lr=10, num_iter=100, last_epoch=-1):\n        \n        self.total_iters = num_iter\n        self.max_lr = max_lr\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n\n        return [base_lr * (self.max_lr / base_lr) ** (self.last_epoch / (self.total_iters + 1e-32)) for base_lr in self.base_lrs]\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-net\', type=str, required=True, help=\'net type\')\n    parser.add_argument(\'-w\', type=int, default=2, help=\'number of workers for dataloader\')\n    parser.add_argument(\'-b\', type=int, default=64, help=\'batch size for dataloader\')\n    parser.add_argument(\'-base_lr\', type=float, default=1e-7, help=\'min learning rate\')\n    parser.add_argument(\'-max_lr\', type=float, default=10, help=\'max learning rate\')\n    parser.add_argument(\'-num_iter\', type=int, default=100, help=\'num of iteration\')\n    parser.add_argument(\'-gpus\', nargs=\'+\', type=int, default=0, help=\'gpu device\')\n    args = parser.parse_args()\n\n    cifar100_training_loader = get_training_dataloader(\n        settings.CIFAR100_TRAIN_MEAN,\n        settings.CIFAR100_TRAIN_STD,\n        num_workers=args.w,\n        batch_size=args.b,\n    )\n    \n    net = get_network(args)\n\n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=args.base_lr, momentum=0.9, weight_decay=1e-4, nesterov=True)\n\n    #set up warmup phase learning rate scheduler\n    lr_scheduler = FindLR(optimizer, max_lr=args.max_lr, num_iter=args.num_iter)\n    epoches = int(args.num_iter / len(cifar100_training_loader)) + 1\n\n    n = 0\n\n    learning_rate = []\n    losses = []\n    for epoch in range(epoches):\n\n        #training procedure\n        net.train()\n        \n        for batch_index, (images, labels) in enumerate(cifar100_training_loader):\n            if n > args.num_iter:\n                break\n\n            lr_scheduler.step()\n\n            images = images.cuda()\n            labels = labels.cuda()\n\n            optimizer.zero_grad()\n            predicts = net(images)\n            loss = loss_function(predicts, labels)\n            if torch.isnan(loss).any():\n                n += 1e8\n                break\n            loss.backward()\n            optimizer.step()\n\n            print(\'Iterations: {iter_num} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.8f}\'.format(\n                loss.item(),\n                optimizer.param_groups[0][\'lr\'],\n                iter_num=n,\n                trained_samples=batch_index * args.b + len(images),\n                total_samples=len(cifar100_training_loader.dataset),\n            ))\n\n            learning_rate.append(optimizer.param_groups[0][\'lr\'])\n            losses.append(loss.item())\n            n += 1\n\n    learning_rate = learning_rate[10:-5]\n    losses = losses[10:-5]\n\n    fig, ax = plt.subplots(1,1)\n    ax.plot(learning_rate, losses)\n    ax.set_xlabel(\'learning rate\')\n    ax.set_ylabel(\'losses\')\n    ax.set_xscale(\'log\')\n    ax.xaxis.set_major_formatter(plt.FormatStrFormatter(\'%.0e\'))\n\n    fig.savefig(\'result.jpg\')\n'"
test.py,3,"b'#test.py\n#!/usr/bin/env python3\n\n"""""" test neuron network performace\nprint top1 and top5 err on test dataset\nof a model\n\nauthor baiyu\n""""""\n\nimport argparse\n#from dataset import *\n\n#from skimage import io\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\n\nfrom conf import settings\nfrom utils import get_network, get_test_dataloader\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-net\', type=str, required=True, help=\'net type\')\n    parser.add_argument(\'-weights\', type=str, required=True, help=\'the weights file you want to test\')\n    parser.add_argument(\'-gpu\', type=bool, default=True, help=\'use gpu or not\')\n    parser.add_argument(\'-w\', type=int, default=2, help=\'number of workers for dataloader\')\n    parser.add_argument(\'-b\', type=int, default=16, help=\'batch size for dataloader\')\n    parser.add_argument(\'-s\', type=bool, default=True, help=\'whether shuffle the dataset\')\n    args = parser.parse_args()\n\n    net = get_network(args)\n\n    cifar100_test_loader = get_test_dataloader(\n        settings.CIFAR100_TRAIN_MEAN,\n        settings.CIFAR100_TRAIN_STD,\n        #settings.CIFAR100_PATH,\n        num_workers=args.w,\n        batch_size=args.b,\n        shuffle=args.s\n    )\n\n    net.load_state_dict(torch.load(args.weights), args.gpu)\n    print(net)\n    net.eval()\n\n    correct_1 = 0.0\n    correct_5 = 0.0\n    total = 0\n\n    for n_iter, (image, label) in enumerate(cifar100_test_loader):\n        print(""iteration: {}\\ttotal {} iterations"".format(n_iter + 1, len(cifar100_test_loader)))\n        image = Variable(image).cuda()\n        label = Variable(label).cuda()\n        output = net(image)\n        _, pred = output.topk(5, 1, largest=True, sorted=True)\n\n        label = label.view(label.size(0), -1).expand_as(pred)\n        correct = pred.eq(label).float()\n\n        #compute top 5\n        correct_5 += correct[:, :5].sum()\n\n        #compute top1 \n        correct_1 += correct[:, :1].sum()\n\n\n    print()\n    print(""Top 1 err: "", 1 - correct_1 / len(cifar100_test_loader.dataset))\n    print(""Top 5 err: "", 1 - correct_5 / len(cifar100_test_loader.dataset))\n    print(""Parameter numbers: {}"".format(sum(p.numel() for p in net.parameters())))'"
train.py,7,"b'# train.py\n#!/usr/bin/env\tpython3\n\n"""""" train network using pytorch\n\nauthor baiyu\n""""""\n\nimport os\nimport sys\nimport argparse\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import DataLoader\n#from dataset import *\nfrom torch.autograd import Variable\n\nfrom tensorboardX import SummaryWriter\n\nfrom conf import settings\nfrom utils import get_network, get_training_dataloader, get_test_dataloader, WarmUpLR\n\ndef train(epoch):\n\n    net.train()\n    for batch_index, (images, labels) in enumerate(cifar100_training_loader):\n        if epoch <= args.warm:\n            warmup_scheduler.step()\n\n        images = Variable(images)\n        labels = Variable(labels)\n\n        labels = labels.cuda()\n        images = images.cuda()\n\n        optimizer.zero_grad()\n        outputs = net(images)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        n_iter = (epoch - 1) * len(cifar100_training_loader) + batch_index + 1\n\n        last_layer = list(net.children())[-1]\n        for name, para in last_layer.named_parameters():\n            if \'weight\' in name:\n                writer.add_scalar(\'LastLayerGradients/grad_norm2_weights\', para.grad.norm(), n_iter)\n            if \'bias\' in name:\n                writer.add_scalar(\'LastLayerGradients/grad_norm2_bias\', para.grad.norm(), n_iter)\n\n        print(\'Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}\'.format(\n            loss.item(),\n            optimizer.param_groups[0][\'lr\'],\n            epoch=epoch,\n            trained_samples=batch_index * args.b + len(images),\n            total_samples=len(cifar100_training_loader.dataset)\n        ))\n\n        #update training loss for each iteration\n        writer.add_scalar(\'Train/loss\', loss.item(), n_iter)\n\n    for name, param in net.named_parameters():\n        layer, attr = os.path.splitext(name)\n        attr = attr[1:]\n        writer.add_histogram(""{}/{}"".format(layer, attr), param, epoch)\n\ndef eval_training(epoch):\n    net.eval()\n\n    test_loss = 0.0 # cost function error\n    correct = 0.0\n\n    for (images, labels) in cifar100_test_loader:\n        images = Variable(images)\n        labels = Variable(labels)\n\n        images = images.cuda()\n        labels = labels.cuda()\n\n        outputs = net(images)\n        loss = loss_function(outputs, labels)\n        test_loss += loss.item()\n        _, preds = outputs.max(1)\n        correct += preds.eq(labels).sum()\n\n    print(\'Test set: Average loss: {:.4f}, Accuracy: {:.4f}\'.format(\n        test_loss / len(cifar100_test_loader.dataset),\n        correct.float() / len(cifar100_test_loader.dataset)\n    ))\n    print()\n\n    #add informations to tensorboard\n    writer.add_scalar(\'Test/Average loss\', test_loss / len(cifar100_test_loader.dataset), epoch)\n    writer.add_scalar(\'Test/Accuracy\', correct.float() / len(cifar100_test_loader.dataset), epoch)\n\n    return correct.float() / len(cifar100_test_loader.dataset)\n\nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-net\', type=str, required=True, help=\'net type\')\n    parser.add_argument(\'-gpu\', type=bool, default=True, help=\'use gpu or not\')\n    parser.add_argument(\'-w\', type=int, default=2, help=\'number of workers for dataloader\')\n    parser.add_argument(\'-b\', type=int, default=128, help=\'batch size for dataloader\')\n    parser.add_argument(\'-s\', type=bool, default=True, help=\'whether shuffle the dataset\')\n    parser.add_argument(\'-warm\', type=int, default=1, help=\'warm up training phase\')\n    parser.add_argument(\'-lr\', type=float, default=0.1, help=\'initial learning rate\')\n    args = parser.parse_args()\n\n    net = get_network(args, use_gpu=args.gpu)\n        \n    #data preprocessing:\n    cifar100_training_loader = get_training_dataloader(\n        settings.CIFAR100_TRAIN_MEAN,\n        settings.CIFAR100_TRAIN_STD,\n        num_workers=args.w,\n        batch_size=args.b,\n        shuffle=args.s\n    )\n    \n    cifar100_test_loader = get_test_dataloader(\n        settings.CIFAR100_TRAIN_MEAN,\n        settings.CIFAR100_TRAIN_STD,\n        num_workers=args.w,\n        batch_size=args.b,\n        shuffle=args.s\n    )\n    \n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=settings.MILESTONES, gamma=0.2) #learning rate decay\n    iter_per_epoch = len(cifar100_training_loader)\n    warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * args.warm)\n    checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, args.net, settings.TIME_NOW)\n\n    #use tensorboard\n    if not os.path.exists(settings.LOG_DIR):\n        os.mkdir(settings.LOG_DIR)\n    writer = SummaryWriter(log_dir=os.path.join(\n            settings.LOG_DIR, args.net, settings.TIME_NOW))\n    input_tensor = torch.Tensor(12, 3, 32, 32).cuda()\n    writer.add_graph(net, Variable(input_tensor, requires_grad=True))\n\n    #create checkpoint folder to save model\n    if not os.path.exists(checkpoint_path):\n        os.makedirs(checkpoint_path)\n    checkpoint_path = os.path.join(checkpoint_path, \'{net}-{epoch}-{type}.pth\')\n\n    best_acc = 0.0\n    for epoch in range(1, settings.EPOCH):\n        if epoch > args.warm:\n            train_scheduler.step(epoch)\n\n        train(epoch)\n        acc = eval_training(epoch)\n\n        #start to save best performance model after learning rate decay to 0.01 \n        if epoch > settings.MILESTONES[1] and best_acc < acc:\n            torch.save(net.state_dict(), checkpoint_path.format(net=args.net, epoch=epoch, type=\'best\'))\n            best_acc = acc\n            continue\n\n        if not epoch % settings.SAVE_EPOCH:\n            torch.save(net.state_dict(), checkpoint_path.format(net=args.net, epoch=epoch, type=\'regular\'))\n\n    writer.close()\n'"
utils.py,3,"b'"""""" helper function\n\nauthor baiyu\n""""""\n\nimport sys\n\nimport numpy\n\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n#from dataset import CIFAR100Train, CIFAR100Test\n\ndef get_network(args, use_gpu=True):\n    """""" return given network\n    """"""\n\n    if args.net == \'vgg16\':\n        from models.vgg import vgg16_bn\n        net = vgg16_bn()\n    elif args.net == \'vgg13\':\n        from models.vgg import vgg13_bn\n        net = vgg13_bn()\n    elif args.net == \'vgg11\':\n        from models.vgg import vgg11_bn\n        net = vgg11_bn()\n    elif args.net == \'vgg19\':\n        from models.vgg import vgg19_bn\n        net = vgg19_bn()\n    elif args.net == \'densenet121\':\n        from models.densenet import densenet121\n        net = densenet121()\n    elif args.net == \'densenet161\':\n        from models.densenet import densenet161\n        net = densenet161()\n    elif args.net == \'densenet169\':\n        from models.densenet import densenet169\n        net = densenet169()\n    elif args.net == \'densenet201\':\n        from models.densenet import densenet201\n        net = densenet201()\n    elif args.net == \'googlenet\':\n        from models.googlenet import googlenet\n        net = googlenet()\n    elif args.net == \'inceptionv3\':\n        from models.inceptionv3 import inceptionv3\n        net = inceptionv3()\n    elif args.net == \'inceptionv4\':\n        from models.inceptionv4 import inceptionv4\n        net = inceptionv4()\n    elif args.net == \'inceptionresnetv2\':\n        from models.inceptionv4 import inception_resnet_v2\n        net = inception_resnet_v2()\n    elif args.net == \'xception\':\n        from models.xception import xception\n        net = xception()\n    elif args.net == \'resnet18\':\n        from models.resnet import resnet18\n        net = resnet18()\n    elif args.net == \'resnet34\':\n        from models.resnet import resnet34\n        net = resnet34()\n    elif args.net == \'resnet50\':\n        from models.resnet import resnet50\n        net = resnet50()\n    elif args.net == \'resnet101\':\n        from models.resnet import resnet101\n        net = resnet101()\n    elif args.net == \'resnet152\':\n        from models.resnet import resnet152\n        net = resnet152()\n    elif args.net == \'preactresnet18\':\n        from models.preactresnet import preactresnet18\n        net = preactresnet18()\n    elif args.net == \'preactresnet34\':\n        from models.preactresnet import preactresnet34\n        net = preactresnet34()\n    elif args.net == \'preactresnet50\':\n        from models.preactresnet import preactresnet50\n        net = preactresnet50()\n    elif args.net == \'preactresnet101\':\n        from models.preactresnet import preactresnet101\n        net = preactresnet101()\n    elif args.net == \'preactresnet152\':\n        from models.preactresnet import preactresnet152\n        net = preactresnet152()\n    elif args.net == \'resnext50\':\n        from models.resnext import resnext50\n        net = resnext50()\n    elif args.net == \'resnext101\':\n        from models.resnext import resnext101\n        net = resnext101()\n    elif args.net == \'resnext152\':\n        from models.resnext import resnext152\n        net = resnext152()\n    elif args.net == \'shufflenet\':\n        from models.shufflenet import shufflenet\n        net = shufflenet()\n    elif args.net == \'shufflenetv2\':\n        from models.shufflenetv2 import shufflenetv2\n        net = shufflenetv2()\n    elif args.net == \'squeezenet\':\n        from models.squeezenet import squeezenet\n        net = squeezenet()\n    elif args.net == \'mobilenet\':\n        from models.mobilenet import mobilenet\n        net = mobilenet()\n    elif args.net == \'mobilenetv2\':\n        from models.mobilenetv2 import mobilenetv2\n        net = mobilenetv2()\n    elif args.net == \'nasnet\':\n        from models.nasnet import nasnet\n        net = nasnet()\n    elif args.net == \'attention56\':\n        from models.attention import attention56\n        net = attention56()\n    elif args.net == \'attention92\':\n        from models.attention import attention92\n        net = attention92()\n    elif args.net == \'seresnet18\':\n        from models.senet import seresnet18\n        net = seresnet18()\n    elif args.net == \'seresnet34\':\n        from models.senet import seresnet34 \n        net = seresnet34()\n    elif args.net == \'seresnet50\':\n        from models.senet import seresnet50 \n        net = seresnet50()\n    elif args.net == \'seresnet101\':\n        from models.senet import seresnet101 \n        net = seresnet101()\n    elif args.net == \'seresnet152\':\n        from models.senet import seresnet152\n        net = seresnet152()\n\n    else:\n        print(\'the network name you have entered is not supported yet\')\n        sys.exit()\n    \n    if use_gpu:\n        net = net.cuda()\n\n    return net\n\n\ndef get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n    """""" return training dataloader\n    Args:\n        mean: mean of cifar100 training dataset\n        std: std of cifar100 training dataset\n        path: path to cifar100 training python dataset\n        batch_size: dataloader batchsize\n        num_workers: dataloader num_works\n        shuffle: whether to shuffle \n    Returns: train_data_loader:torch dataloader object\n    """"""\n\n    transform_train = transforms.Compose([\n        #transforms.ToPILImage(),\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(15),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n    #cifar100_training = CIFAR100Train(path, transform=transform_train)\n    cifar100_training = torchvision.datasets.CIFAR100(root=\'./data\', train=True, download=True, transform=transform_train)\n    cifar100_training_loader = DataLoader(\n        cifar100_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n\n    return cifar100_training_loader\n\ndef get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n    """""" return training dataloader\n    Args:\n        mean: mean of cifar100 test dataset\n        std: std of cifar100 test dataset\n        path: path to cifar100 test python dataset\n        batch_size: dataloader batchsize\n        num_workers: dataloader num_works\n        shuffle: whether to shuffle \n    Returns: cifar100_test_loader:torch dataloader object\n    """"""\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n    #cifar100_test = CIFAR100Test(path, transform=transform_test)\n    cifar100_test = torchvision.datasets.CIFAR100(root=\'./data\', train=False, download=True, transform=transform_test)\n    cifar100_test_loader = DataLoader(\n        cifar100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n\n    return cifar100_test_loader\n\ndef compute_mean_std(cifar100_dataset):\n    """"""compute the mean and std of cifar100 dataset\n    Args:\n        cifar100_training_dataset or cifar100_test_dataset\n        witch derived from class torch.utils.data\n    \n    Returns:\n        a tuple contains mean, std value of entire dataset\n    """"""\n\n    data_r = numpy.dstack([cifar100_dataset[i][1][:, :, 0] for i in range(len(cifar100_dataset))])\n    data_g = numpy.dstack([cifar100_dataset[i][1][:, :, 1] for i in range(len(cifar100_dataset))])\n    data_b = numpy.dstack([cifar100_dataset[i][1][:, :, 2] for i in range(len(cifar100_dataset))])\n    mean = numpy.mean(data_r), numpy.mean(data_g), numpy.mean(data_b)\n    std = numpy.std(data_r), numpy.std(data_g), numpy.std(data_b)\n\n    return mean, std\n\nclass WarmUpLR(_LRScheduler):\n    """"""warmup_training learning rate scheduler\n    Args:\n        optimizer: optimzier(e.g. SGD)\n        total_iters: totoal_iters of warmup phase\n    """"""\n    def __init__(self, optimizer, total_iters, last_epoch=-1):\n        \n        self.total_iters = total_iters\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        """"""we will use the first m batches, and set the learning\n        rate to base_lr * m / total_iters\n        """"""\n        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]'"
conf/__init__.py,0,"b'"""""" dynamically load settings\n\nauthor baiyu\n""""""\nimport conf.global_settings as settings\n\nclass Settings:\n    def __init__(self, settings):\n\n        for attr in dir(settings):\n            if attr.isupper():\n                setattr(self, attr, getattr(settings, attr))\n\nsettings = Settings(settings)'"
conf/global_settings.py,0,"b'"""""" configurations for this project\n\nauthor baiyu\n""""""\nimport os\nfrom datetime import datetime\n\n#CIFAR100 dataset path (python version)\n#CIFAR100_PATH = \'/nfs/private/cifar100/cifar-100-python\'\n\n#mean and std of cifar100 dataset\nCIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\nCIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n\n#CIFAR100_TEST_MEAN = (0.5088964127604166, 0.48739301317401956, 0.44194221124387256)\n#CIFAR100_TEST_STD = (0.2682515741720801, 0.2573637364478126, 0.2770957707973042)\n\n#directory to save weights file\nCHECKPOINT_PATH = \'checkpoint\'\n\n#total training epoches\nEPOCH = 200 \nMILESTONES = [60, 120, 160]\n\n#initial learning rate\n#INIT_LR = 0.1\n\n#time of we run the script\nTIME_NOW = datetime.now().isoformat()\n\n#tensorboard log dir\nLOG_DIR = \'runs\'\n\n#save weights file per SAVE_EPOCH epoch\nSAVE_EPOCH = 10\n\n\n\n\n\n\n\n\n'"
models/attention.py,2,"b'""""""residual attention network in pytorch\n\n\n\n[1] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang\n\n    Residual Attention Network for Image Classification\n    https://arxiv.org/abs/1704.06904\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#""""""The Attention Module is built by pre-activation Residual Unit [11] with the \n#number of channels in each stage is the same as ResNet [10].""""""\n\nclass PreActResidualUnit(nn.Module):\n    """"""PreAct Residual Unit\n    Args:\n        in_channels: residual unit input channel number\n        out_channels: residual unit output channel numebr\n        stride: stride of residual unit when stride = 2, downsample the featuremap\n    """"""\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        bottleneck_channels = int(out_channels / 4)\n        self.residual_function = nn.Sequential(\n            #1x1 conv\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, bottleneck_channels, 1, stride),\n\n            #3x3 conv\n            nn.BatchNorm2d(bottleneck_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(bottleneck_channels, bottleneck_channels, 3, padding=1),\n\n            #1x1 conv\n            nn.BatchNorm2d(bottleneck_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(bottleneck_channels, out_channels, 1)\n        )\n\n        self.shortcut = nn.Sequential()\n        if stride != 2 or (in_channels != out_channels):\n            self.shortcut = nn.Conv2d(in_channels, out_channels, 1, stride=stride)\n    \n    def forward(self, x):\n\n        res = self.residual_function(x)\n        shortcut = self.shortcut(x)\n\n        return res + shortcut\n\nclass AttentionModule1(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, p=1, t=2, r=1):\n        super().__init__()\n        #""""""The hyperparameter p denotes the number of preprocessing Residual \n        #Units before splitting into trunk branch and mask branch. t denotes \n        #the number of Residual Units in trunk branch. r denotes the number of \n        #Residual Units between adjacent pooling layer in the mask branch.""""""\n        assert in_channels == out_channels\n\n        self.pre = self._make_residual(in_channels, out_channels, p)\n        self.trunk = self._make_residual(in_channels, out_channels, t)\n        self.soft_resdown1 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resdown2 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resdown3 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resdown4 = self._make_residual(in_channels, out_channels, r)\n\n        self.soft_resup1 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resup2 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resup3 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resup4 = self._make_residual(in_channels, out_channels, r)\n\n        self.shortcut_short = PreActResidualUnit(in_channels, out_channels, 1)\n        self.shortcut_long = PreActResidualUnit(in_channels, out_channels, 1)\n\n        self.sigmoid = nn.Sequential(\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n            nn.Sigmoid()\n        ) \n        \n        self.last = self._make_residual(in_channels, out_channels, p)\n    \n    def forward(self, x):\n        ###We make the size of the smallest output map in each mask branch 7*7 to be consistent\n        #with the smallest trunk output map size.\n        ###Thus 3,2,1 max-pooling layers are used in mask branch with input size 56 * 56, 28 * 28, 14 * 14 respectively.\n        x = self.pre(x)\n        input_size = (x.size(2), x.size(3))\n\n        x_t = self.trunk(x)\n\n        #first downsample out 28\n        x_s = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x_s = self.soft_resdown1(x_s)\n\n        #28 shortcut\n        shape1 = (x_s.size(2), x_s.size(3))\n        shortcut_long = self.shortcut_long(x_s)\n\n        #seccond downsample out 14\n        x_s = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x_s = self.soft_resdown2(x_s)\n\n        #14 shortcut\n        shape2 = (x_s.size(2), x_s.size(3))\n        shortcut_short = self.soft_resdown3(x_s)\n\n        #third downsample out 7\n        x_s = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x_s = self.soft_resdown3(x_s)\n\n        #mid\n        x_s = self.soft_resdown4(x_s)\n        x_s = self.soft_resup1(x_s)\n\n        #first upsample out 14\n        x_s = self.soft_resup2(x_s)\n        x_s = F.interpolate(x_s, size=shape2)\n        x_s += shortcut_short\n\n        #second upsample out 28\n        x_s = self.soft_resup3(x_s)\n        x_s = F.interpolate(x_s, size=shape1)\n        x_s += shortcut_long\n\n        #thrid upsample out 54\n        x_s = self.soft_resup4(x_s)\n        x_s = F.interpolate(x_s, size=input_size)\n\n        x_s = self.sigmoid(x_s)\n        x = (1 + x_s) * x_t\n        x = self.last(x)\n\n        return x\n\n    def _make_residual(self, in_channels, out_channels, p):\n\n        layers = []\n        for _ in range(p):\n            layers.append(PreActResidualUnit(in_channels, out_channels, 1))\n\n        return nn.Sequential(*layers)\n\nclass AttentionModule2(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, p=1, t=2, r=1):\n        super().__init__()\n        #""""""The hyperparameter p denotes the number of preprocessing Residual \n        #Units before splitting into trunk branch and mask branch. t denotes \n        #the number of Residual Units in trunk branch. r denotes the number of \n        #Residual Units between adjacent pooling layer in the mask branch.""""""\n        assert in_channels == out_channels\n\n        self.pre = self._make_residual(in_channels, out_channels, p)\n        self.trunk = self._make_residual(in_channels, out_channels, t)\n        self.soft_resdown1 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resdown2 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resdown3 = self._make_residual(in_channels, out_channels, r)\n\n        self.soft_resup1 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resup2 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resup3 = self._make_residual(in_channels, out_channels, r)\n\n        self.shortcut = PreActResidualUnit(in_channels, out_channels, 1)\n\n        self.sigmoid = nn.Sequential(\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n            nn.Sigmoid()\n        ) \n        \n        self.last = self._make_residual(in_channels, out_channels, p)\n    \n    def forward(self, x):\n        x = self.pre(x)\n        input_size = (x.size(2), x.size(3))\n\n        x_t = self.trunk(x)\n\n        #first downsample out 14\n        x_s = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x_s = self.soft_resdown1(x_s)\n\n        #14 shortcut\n        shape1 = (x_s.size(2), x_s.size(3))\n        shortcut = self.shortcut(x_s)\n\n        #seccond downsample out 7 \n        x_s = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x_s = self.soft_resdown2(x_s)\n\n        #mid\n        x_s = self.soft_resdown3(x_s)\n        x_s = self.soft_resup1(x_s)\n\n        #first upsample out 14\n        x_s = self.soft_resup2(x_s)\n        x_s = F.interpolate(x_s, size=shape1)\n        x_s += shortcut\n\n        #second upsample out 28\n        x_s = self.soft_resup3(x_s)\n        x_s = F.interpolate(x_s, size=input_size)\n\n        x_s = self.sigmoid(x_s)\n        x = (1 + x_s) * x_t\n        x = self.last(x)\n\n        return x\n\n    def _make_residual(self, in_channels, out_channels, p):\n\n        layers = []\n        for _ in range(p):\n            layers.append(PreActResidualUnit(in_channels, out_channels, 1))\n\n        return nn.Sequential(*layers)\n\nclass AttentionModule3(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, p=1, t=2, r=1):\n        super().__init__()\n\n        assert in_channels == out_channels\n\n        self.pre = self._make_residual(in_channels, out_channels, p)\n        self.trunk = self._make_residual(in_channels, out_channels, t)\n        self.soft_resdown1 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resdown2 = self._make_residual(in_channels, out_channels, r)\n\n        self.soft_resup1 = self._make_residual(in_channels, out_channels, r)\n        self.soft_resup2 = self._make_residual(in_channels, out_channels, r)\n\n        self.shortcut = PreActResidualUnit(in_channels, out_channels, 1)\n\n        self.sigmoid = nn.Sequential(\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n            nn.Sigmoid()\n        ) \n        \n        self.last = self._make_residual(in_channels, out_channels, p)\n    \n    def forward(self, x):\n        x = self.pre(x)\n        input_size = (x.size(2), x.size(3))\n\n        x_t = self.trunk(x)\n\n        #first downsample out 14\n        x_s = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x_s = self.soft_resdown1(x_s)\n\n        #mid\n        x_s = self.soft_resdown2(x_s)\n        x_s = self.soft_resup1(x_s)\n\n        #first upsample out 14\n        x_s = self.soft_resup2(x_s)\n        x_s = F.interpolate(x_s, size=input_size)\n\n        x_s = self.sigmoid(x_s)\n        x = (1 + x_s) * x_t\n        x = self.last(x)\n\n        return x\n\n    def _make_residual(self, in_channels, out_channels, p):\n\n        layers = []\n        for _ in range(p):\n            layers.append(PreActResidualUnit(in_channels, out_channels, 1))\n\n        return nn.Sequential(*layers)\n\nclass Attention(nn.Module):\n    """"""residual attention netowrk\n    Args:\n        block_num: attention module number for each stage\n    """"""\n\n    def __init__(self, block_num, class_num=100):\n        \n        super().__init__()\n        self.pre_conv = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n\n        self.stage1 = self._make_stage(64, 256, block_num[0], AttentionModule1)\n        self.stage2 = self._make_stage(256, 512, block_num[1], AttentionModule2)\n        self.stage3 = self._make_stage(512, 1024, block_num[2], AttentionModule3)\n        self.stage4 = nn.Sequential(\n            PreActResidualUnit(1024, 2048, 2),\n            PreActResidualUnit(2048, 2048, 1),\n            PreActResidualUnit(2048, 2048, 1)\n        )\n        self.avg = nn.AdaptiveAvgPool2d(1)\n        self.linear = nn.Linear(2048, 100)\n    \n    def forward(self, x):\n        x = self.pre_conv(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.avg(x)\n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n\n        return x\n\n    def _make_stage(self, in_channels, out_channels, num, block):\n\n        layers = []\n        layers.append(PreActResidualUnit(in_channels, out_channels, 2))\n\n        for _ in range(num):\n            layers.append(block(out_channels, out_channels))\n\n        return nn.Sequential(*layers)\n    \ndef attention56():\n    return Attention([1, 1, 1])\n\ndef attention92():\n    return Attention([1, 2, 3])\n\n'"
models/densenet.py,2,"b'""""""dense net in pytorch\n\n\n\n[1] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger.\n\n    Densely Connected Convolutional Networks\n    https://arxiv.org/abs/1608.06993v5\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\n\n#""""""Bottleneck layers. Although each layer only produces k\n#output feature-maps, it typically has many more inputs. It\n#has been noted in [37, 11] that a 1\xc3\x971 convolution can be in-\n#troduced as bottleneck layer before each 3\xc3\x973 convolution\n#to reduce the number of input feature-maps, and thus to\n#improve computational efficiency.""""""\nclass Bottleneck(nn.Module):\n    def __init__(self, in_channels, growth_rate):\n        super().__init__()\n        #""""""In  our experiments, we let each 1\xc3\x971 convolution \n        #produce 4k feature-maps.""""""\n        inner_channel = 4 * growth_rate\n\n        #""""""We find this design especially effective for DenseNet and \n        #we refer to our network with such a bottleneck layer, i.e., \n        #to the BN-ReLU-Conv(1\xc3\x971)-BN-ReLU-Conv(3\xc3\x973) version of H ` , \n        #as DenseNet-B.""""""\n        self.bottle_neck = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, inner_channel, kernel_size=1, bias=False),\n            nn.BatchNorm2d(inner_channel),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(inner_channel, growth_rate, kernel_size=3, padding=1, bias=False)\n        )\n\n    def forward(self, x):\n        return torch.cat([x, self.bottle_neck(x)], 1)\n\n#""""""We refer to layers between blocks as transition\n#layers, which do convolution and pooling.""""""\nclass Transition(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        #""""""The transition layers used in our experiments \n        #consist of a batch normalization layer and an 1\xc3\x971 \n        #convolutional layer followed by a 2\xc3\x972 average pooling \n        #layer"""""".\n        self.down_sample = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.AvgPool2d(2, stride=2)\n        )\n\n    def forward(self, x):\n        return self.down_sample(x)\n\n#DesneNet-BC\n#B stands for bottleneck layer(BN-RELU-CONV(1x1)-BN-RELU-CONV(3x3))\n#C stands for compression factor(0<=theta<=1)\nclass DenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_class=100):\n        super().__init__()\n        self.growth_rate = growth_rate\n\n        #""""""Before entering the first dense block, a convolution \n        #with 16 (or twice the growth rate for DenseNet-BC) \n        #output channels is performed on the input images.""""""\n        inner_channels = 2 * growth_rate\n\n        #For convolutional layers with kernel size 3\xc3\x973, each \n        #side of the inputs is zero-padded by one pixel to keep \n        #the feature-map size fixed.\n        self.conv1 = nn.Conv2d(3, inner_channels, kernel_size=3, padding=1, bias=False) \n\n        self.features = nn.Sequential()\n\n        for index in range(len(nblocks) - 1):\n            self.features.add_module(""dense_block_layer_{}"".format(index), self._make_dense_layers(block, inner_channels, nblocks[index]))\n            inner_channels += growth_rate * nblocks[index]\n\n            #""""""If a dense block contains m feature-maps, we let the \n            #following transition layer generate \xce\xb8m output feature-\n            #maps, where 0 < \xce\xb8 \xe2\x89\xa4 1 is referred to as the compression \n            #fac-tor.\n            out_channels = int(reduction * inner_channels) # int() will automatic floor the value\n            self.features.add_module(""transition_layer_{}"".format(index), Transition(inner_channels, out_channels))\n            inner_channels = out_channels\n\n        self.features.add_module(""dense_block{}"".format(len(nblocks) - 1), self._make_dense_layers(block, inner_channels, nblocks[len(nblocks)-1]))\n        inner_channels += growth_rate * nblocks[len(nblocks) - 1]\n        self.features.add_module(\'bn\', nn.BatchNorm2d(inner_channels))\n        self.features.add_module(\'relu\', nn.ReLU(inplace=True))\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.linear = nn.Linear(inner_channels, num_class)\n\n    def forward(self, x):\n        output = self.conv1(x)\n        output = self.features(output)\n        output = self.avgpool(output)\n        output = output.view(output.size()[0], -1)\n        output = self.linear(output)\n        return output\n\n    def _make_dense_layers(self, block, in_channels, nblocks):\n        dense_block = nn.Sequential()\n        for index in range(nblocks):\n            dense_block.add_module(\'bottle_neck_layer_{}\'.format(index), block(in_channels, self.growth_rate))\n            in_channels += self.growth_rate\n        return dense_block\n\ndef densenet121():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n\ndef densenet169():\n    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n\ndef densenet201():\n    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n\ndef densenet161():\n    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n\n'"
models/googlenet.py,2,"b'""""""google net in pytorch\n\n\n\n[1] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, \n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n\n    Going Deeper with Convolutions\n    https://arxiv.org/abs/1409.4842v1\n""""""\n\nimport torch\nimport torch.nn as nn\n\nclass Inception(nn.Module):\n    def __init__(self, input_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj):\n        super().__init__()\n\n        #1x1conv branch\n        self.b1 = nn.Sequential(\n            nn.Conv2d(input_channels, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(inplace=True)\n        )\n\n        #1x1conv -> 3x3conv branch\n        self.b2 = nn.Sequential(\n            nn.Conv2d(input_channels, n3x3_reduce, kernel_size=1),\n            nn.BatchNorm2d(n3x3_reduce),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(n3x3_reduce, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(inplace=True)\n        )\n\n        #1x1conv -> 5x5conv branch\n        #we use 2 3x3 conv filters stacked instead\n        #of 1 5x5 filters to obtain the same receptive\n        #field with fewer parameters\n        self.b3 = nn.Sequential(\n            nn.Conv2d(input_channels, n5x5_reduce, kernel_size=1),\n            nn.BatchNorm2d(n5x5_reduce),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(n5x5_reduce, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5, n5x5),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(inplace=True)\n        )\n\n        #3x3pooling -> 1x1conv\n        #same conv\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(input_channels, pool_proj, kernel_size=1),\n            nn.BatchNorm2d(pool_proj),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)\n\n\nclass GoogleNet(nn.Module):\n\n    def __init__(self, num_class=100):\n        super().__init__()\n        self.prelayer = nn.Sequential(\n            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True)\n        )\n\n        #although we only use 1 conv layer as prelayer,\n        #we still use name a3, b3.......\n        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n\n        #""""""In general, an Inception network is a network consisting of\n        #modules of the above type stacked upon each other, with occasional \n        #max-pooling layers with stride 2 to halve the resolution of the \n        #grid""""""\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n\n        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n\n        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n\n        #input feature size: 8*8*1024\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout2d(p=0.4)\n        self.linear = nn.Linear(1024, num_class)\n    \n    def forward(self, x):\n        output = self.prelayer(x)\n        output = self.a3(output)\n        output = self.b3(output)\n        \n        output = self.maxpool(output)\n\n        output = self.a4(output)\n        output = self.b4(output)\n        output = self.c4(output)\n        output = self.d4(output)\n        output = self.e4(output)\n\n        output = self.maxpool(output)\n\n        output = self.a5(output)\n        output = self.b5(output)\n\n        #""""""It was found that a move from fully connected layers to\n        #average pooling improved the top-1 accuracy by about 0.6%, \n        #however the use of dropout remained essential even after \n        #removing the fully connected layers.""""""\n        output = self.avgpool(output)\n        output = self.dropout(output)\n        output = output.view(output.size()[0], -1)\n        output = self.linear(output)\n\n        return output\n\ndef googlenet():\n    return GoogleNet()\n\n\n'"
models/inceptionv3.py,8,"b'"""""" inceptionv3 in pytorch\n\n\n[1] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna\n\n    Rethinking the Inception Architecture for Computer Vision\n    https://arxiv.org/abs/1512.00567v3\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, input_channels, output_channels, **kwargs):\n        super().__init__()\n        self.conv = nn.Conv2d(input_channels, output_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n\n        return x\n\n#same naive inception module\nclass InceptionA(nn.Module):\n\n    def __init__(self, input_channels, pool_features):\n        super().__init__()\n        self.branch1x1 = BasicConv2d(input_channels, 64, kernel_size=1)\n\n        self.branch5x5 = nn.Sequential(\n            BasicConv2d(input_channels, 48, kernel_size=1),\n            BasicConv2d(48, 64, kernel_size=5, padding=2)\n        )\n\n        self.branch3x3 = nn.Sequential(\n            BasicConv2d(input_channels, 64, kernel_size=1),\n            BasicConv2d(64, 96, kernel_size=3, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, padding=1)\n        )\n\n        self.branchpool = nn.Sequential(\n            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n            BasicConv2d(input_channels, pool_features, kernel_size=3, padding=1)\n        )\n    \n    def forward(self, x):\n        \n        #x -> 1x1(same)\n        branch1x1 = self.branch1x1(x)\n\n        #x -> 1x1 -> 5x5(same)\n        branch5x5 = self.branch5x5(x)\n        #branch5x5 = self.branch5x5_2(branch5x5)\n\n        #x -> 1x1 -> 3x3 -> 3x3(same)\n        branch3x3 = self.branch3x3(x)\n\n        #x -> pool -> 1x1(same)\n        branchpool = self.branchpool(x)\n\n        outputs = [branch1x1, branch5x5, branch3x3, branchpool]\n\n        return torch.cat(outputs, 1)\n\n#downsample\n#Factorization into smaller convolutions\nclass InceptionB(nn.Module):\n\n    def __init__(self, input_channels):\n        super().__init__()\n\n        self.branch3x3 = BasicConv2d(input_channels, 384, kernel_size=3, stride=2)\n\n        self.branch3x3stack = nn.Sequential(\n            BasicConv2d(input_channels, 64, kernel_size=1),\n            BasicConv2d(64, 96, kernel_size=3, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=2)\n        )\n\n        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n\n    def forward(self, x):\n\n        #x - > 3x3(downsample)\n        branch3x3 = self.branch3x3(x)\n\n        #x -> 3x3 -> 3x3(downsample)\n        branch3x3stack = self.branch3x3stack(x)\n\n        #x -> avgpool(downsample)\n        branchpool = self.branchpool(x)\n\n        #""""""We can use two parallel stride 2 blocks: P and C. P is a pooling \n        #layer (either average or maximum pooling) the activation, both of \n        #them are stride 2 the filter banks of which are concatenated as in \n        #figure 10.""""""\n        outputs = [branch3x3, branch3x3stack, branchpool]\n\n        return torch.cat(outputs, 1)\n    \n#Factorizing Convolutions with Large Filter Size\nclass InceptionC(nn.Module):\n    def __init__(self, input_channels, channels_7x7):\n        super().__init__()\n        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n\n        c7 = channels_7x7\n\n        #In theory, we could go even further and argue that one can replace any n \xc3\x97 n \n        #convolution by a 1 \xc3\x97 n convolution followed by a n \xc3\x97 1 convolution and the \n        #computational cost saving increases dramatically as n grows (see figure 6).\n        self.branch7x7 = nn.Sequential(\n            BasicConv2d(input_channels, c7, kernel_size=1),\n            BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n        )\n\n        self.branch7x7stack = nn.Sequential(\n            BasicConv2d(input_channels, c7, kernel_size=1),\n            BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n        )\n\n        self.branch_pool = nn.Sequential(\n            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n            BasicConv2d(input_channels, 192, kernel_size=1),\n        )\n\n    def forward(self, x):\n\n        #x -> 1x1(same)\n        branch1x1 = self.branch1x1(x)\n\n        #x -> 1layer 1*7 and 7*1 (same)\n        branch7x7 = self.branch7x7(x)\n\n        #x-> 2layer 1*7 and 7*1(same)\n        branch7x7stack = self.branch7x7stack(x)\n\n        #x-> avgpool (same)\n        branchpool = self.branch_pool(x)\n\n        outputs = [branch1x1, branch7x7, branch7x7stack, branchpool]\n\n        return torch.cat(outputs, 1)\n\nclass InceptionD(nn.Module):\n\n    def __init__(self, input_channels):\n        super().__init__()\n\n        self.branch3x3 = nn.Sequential(\n            BasicConv2d(input_channels, 192, kernel_size=1),\n            BasicConv2d(192, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch7x7 = nn.Sequential(\n            BasicConv2d(input_channels, 192, kernel_size=1),\n            BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(192, 192, kernel_size=3, stride=2)\n        )\n\n        self.branchpool = nn.AvgPool2d(kernel_size=3, stride=2)\n    \n    def forward(self, x):\n\n        #x -> 1x1 -> 3x3(downsample)\n        branch3x3 = self.branch3x3(x)\n\n        #x -> 1x1 -> 1x7 -> 7x1 -> 3x3 (downsample)\n        branch7x7 = self.branch7x7(x)\n\n        #x -> avgpool (downsample)\n        branchpool = self.branchpool(x)\n\n        outputs = [branch3x3, branch7x7, branchpool]\n\n        return torch.cat(outputs, 1)\n    \n\n#same\nclass InceptionE(nn.Module):\n    def __init__(self, input_channels):\n        super().__init__()\n        self.branch1x1 = BasicConv2d(input_channels, 320, kernel_size=1)\n\n        self.branch3x3_1 = BasicConv2d(input_channels, 384, kernel_size=1)\n        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n            \n        self.branch3x3stack_1 = BasicConv2d(input_channels, 448, kernel_size=1)\n        self.branch3x3stack_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n        self.branch3x3stack_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3stack_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch_pool = nn.Sequential(\n            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n            BasicConv2d(input_channels, 192, kernel_size=1)\n        )\n\n    def forward(self, x):\n\n        #x -> 1x1 (same)\n        branch1x1 = self.branch1x1(x)\n\n        # x -> 1x1 -> 3x1\n        # x -> 1x1 -> 1x3\n        # concatenate(3x1, 1x3)\n        #""""""7. Inception modules with expanded the filter bank outputs. \n        #This architecture is used on the coarsest (8 \xc3\x97 8) grids to promote \n        #high dimensional representations, as suggested by principle \n        #2 of Section 2.""""""\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3)\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        # x -> 1x1 -> 3x3 -> 1x3\n        # x -> 1x1 -> 3x3 -> 3x1\n        #concatenate(1x3, 3x1)\n        branch3x3stack = self.branch3x3stack_1(x)\n        branch3x3stack = self.branch3x3stack_2(branch3x3stack)\n        branch3x3stack = [\n            self.branch3x3stack_3a(branch3x3stack),\n            self.branch3x3stack_3b(branch3x3stack)\n        ]\n        branch3x3stack = torch.cat(branch3x3stack, 1)\n\n        branchpool = self.branch_pool(x)\n\n        outputs = [branch1x1, branch3x3, branch3x3stack, branchpool]\n\n        return torch.cat(outputs, 1)\n\nclass InceptionV3(nn.Module):\n    \n    def __init__(self, num_classes=100):\n        super().__init__()\n        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, padding=1)\n        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3, padding=1)\n        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n\n        #naive inception module\n        self.Mixed_5b = InceptionA(192, pool_features=32)\n        self.Mixed_5c = InceptionA(256, pool_features=64)\n        self.Mixed_5d = InceptionA(288, pool_features=64)\n\n        #downsample\n        self.Mixed_6a = InceptionB(288)\n\n        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n\n        #downsample\n        self.Mixed_7a = InceptionD(768)\n\n        self.Mixed_7b = InceptionE(1280)\n        self.Mixed_7c = InceptionE(2048)\n        \n        #6*6 feature size\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout2d()\n        self.linear = nn.Linear(2048, num_classes)\n\n    def forward(self, x):\n\n        #32 -> 30\n        x = self.Conv2d_1a_3x3(x)\n        x = self.Conv2d_2a_3x3(x)\n        x = self.Conv2d_2b_3x3(x)\n        x = self.Conv2d_3b_1x1(x)\n        x = self.Conv2d_4a_3x3(x)\n\n        #30 -> 30\n        x = self.Mixed_5b(x)\n        x = self.Mixed_5c(x)\n        x = self.Mixed_5d(x)\n\n        #30 -> 14\n        #Efficient Grid Size Reduction to avoid representation\n        #bottleneck\n        x = self.Mixed_6a(x)\n\n        #14 -> 14\n        #""""""In practice, we have found that employing this factorization does not \n        #work well on early layers, but it gives very good results on medium \n        #grid-sizes (On m \xc3\x97 m feature maps, where m ranges between 12 and 20). \n        #On that level, very good results can be achieved by using 1 \xc3\x97 7 convolutions \n        #followed by 7 \xc3\x97 1 convolutions.""""""\n        x = self.Mixed_6b(x)\n        x = self.Mixed_6c(x)\n        x = self.Mixed_6d(x)\n        x = self.Mixed_6e(x)\n\n        #14 -> 6\n        #Efficient Grid Size Reduction\n        x = self.Mixed_7a(x)\n\n        #6 -> 6\n        #We are using this solution only on the coarsest grid, \n        #since that is the place where producing high dimensional \n        #sparse representation is the most critical as the ratio of \n        #local processing (by 1 \xc3\x97 1 convolutions) is increased compared \n        #to the spatial aggregation.""""""\n        x = self.Mixed_7b(x)\n        x = self.Mixed_7c(x)\n\n        #6 -> 1\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n        return x\n\n\ndef inceptionv3():\n    return InceptionV3()\n\n\n\n'"
models/inceptionv4.py,16,"b'# -*- coding: UTF-8 -*-\n"""""" inceptionv4 in pytorch\n\n\n[1] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n\n    Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\n    https://arxiv.org/abs/1602.07261\n""""""\n\nimport torch\nimport torch.nn as nn\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, input_channels, output_channels, **kwargs):\n        super().__init__()\n        self.conv = nn.Conv2d(input_channels, output_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n\n        return x\n\nclass Inception_Stem(nn.Module):\n\n    #""""""Figure 3. The schema for stem of the pure Inception-v4 and \n    #Inception-ResNet-v2 networks. This is the input part of those \n    #networks.""""""\n    def __init__(self, input_channels):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            BasicConv2d(input_channels, 32, kernel_size=3),\n            BasicConv2d(32, 32, kernel_size=3, padding=1),\n            BasicConv2d(32, 64, kernel_size=3, padding=1)\n        )\n\n        self.branch3x3_conv = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n\n        self.branch7x7a = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1),\n            BasicConv2d(64, 64, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(64, 64, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(64, 96, kernel_size=3, padding=1)\n        )\n\n        self.branch7x7b = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1),\n            BasicConv2d(64, 96, kernel_size=3, padding=1)\n        )\n\n        self.branchpoola = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.branchpoolb = BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n\n        x = [\n            self.branch3x3_conv(x),\n            self.branch3x3_pool(x)\n        ]\n        x = torch.cat(x, 1)\n\n        x = [\n            self.branch7x7a(x),\n            self.branch7x7b(x)\n        ]\n        x = torch.cat(x, 1)\n\n        x = [\n            self.branchpoola(x),\n            self.branchpoolb(x)\n        ]\n\n        x = torch.cat(x, 1)\n\n        return x\n\nclass InceptionA(nn.Module):\n\n    #""""""Figure 4. The schema for 35 \xc3\x97 35 grid modules of the pure \n    #Inception-v4 network. This is the Inception-A block of Figure 9.""""""\n    def __init__(self, input_channels):\n        super().__init__()\n\n        self.branch3x3stack = nn.Sequential(\n            BasicConv2d(input_channels, 64, kernel_size=1),\n            BasicConv2d(64, 96, kernel_size=3, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, padding=1)\n        )\n\n        self.branch3x3 = nn.Sequential(\n            BasicConv2d(input_channels, 64, kernel_size=1),\n            BasicConv2d(64, 96, kernel_size=3, padding=1)\n        )\n\n        self.branch1x1 = BasicConv2d(input_channels, 96, kernel_size=1)\n\n        self.branchpool = nn.Sequential(\n            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n            BasicConv2d(input_channels, 96, kernel_size=1)\n        )\n\n    def forward(self, x):\n\n        x = [\n            self.branch3x3stack(x),\n            self.branch3x3(x),\n            self.branch1x1(x),\n            self.branchpool(x)\n        ]\n\n        return torch.cat(x, 1)\n\nclass ReductionA(nn.Module):\n\n    #""""""Figure 7. The schema for 35 \xc3\x97 35 to 17 \xc3\x97 17 reduction module. \n    #Different variants of this blocks (with various number of filters) \n    #are used in Figure 9, and 15 in each of the new Inception(-v4, - ResNet-v1,\n    #-ResNet-v2) variants presented in this paper. The k, l, m, n numbers \n    #represent filter bank sizes which can be looked up in Table 1.\n    def __init__(self, input_channels, k, l, m, n):\n\n        super().__init__()\n        self.branch3x3stack = nn.Sequential(\n            BasicConv2d(input_channels, k, kernel_size=1),\n            BasicConv2d(k, l, kernel_size=3, padding=1),\n            BasicConv2d(l, m, kernel_size=3, stride=2)\n        )\n\n        self.branch3x3 = BasicConv2d(input_channels, n, kernel_size=3, stride=2)\n        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.output_channels = input_channels + n + m\n\n    def forward(self, x):\n\n        x = [\n            self.branch3x3stack(x),\n            self.branch3x3(x),\n            self.branchpool(x)\n        ]\n\n        return torch.cat(x, 1)\n\nclass InceptionB(nn.Module):\n\n    #""""""Figure 5. The schema for 17 \xc3\x97 17 grid modules of the pure Inception-v4 network. \n    #This is the Inception-B block of Figure 9.""""""\n    def __init__(self, input_channels):\n        super().__init__()\n        \n        self.branch7x7stack = nn.Sequential(\n            BasicConv2d(input_channels, 192, kernel_size=1),\n            BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(192, 224, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(224, 224, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n        )\n\n        self.branch7x7 = nn.Sequential(\n            BasicConv2d(input_channels, 192, kernel_size=1),\n            BasicConv2d(192, 224, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n        )\n\n        self.branch1x1 = BasicConv2d(input_channels, 384, kernel_size=1) \n\n        self.branchpool = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1),\n            BasicConv2d(input_channels, 128, kernel_size=1)\n        )\n    \n    def forward(self, x):\n        x = [\n            self.branch1x1(x),\n            self.branch7x7(x),\n            self.branch7x7stack(x),\n            self.branchpool(x)\n        ]\n\n        return torch.cat(x, 1)\n\nclass ReductionB(nn.Module):\n\n    #""""""Figure 8. The schema for 17 \xc3\x97 17 to 8 \xc3\x97 8 grid-reduction mod- ule. \n    #This is the reduction module used by the pure Inception-v4 network in \n    #Figure 9.""""""\n    def __init__(self, input_channels):\n\n        super().__init__()\n        self.branch7x7 = nn.Sequential(\n            BasicConv2d(input_channels, 256, kernel_size=1),\n            BasicConv2d(256, 256, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(256, 320, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(320, 320, kernel_size=3, stride=2, padding=1)\n        )\n\n        self.branch3x3 = nn.Sequential(\n            BasicConv2d(input_channels, 192, kernel_size=1),\n            BasicConv2d(192, 192, kernel_size=3, stride=2, padding=1)\n        )\n\n        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n\n        x = [\n            self.branch3x3(x),\n            self.branch7x7(x),\n            self.branchpool(x)\n        ]\n\n        return torch.cat(x, 1)\n\nclass InceptionC(nn.Module):\n\n    def __init__(self, input_channels):\n        #""""""Figure 6. The schema for 8\xc3\x978 grid modules of the pure \n        #Inceptionv4 network. This is the Inception-C block of Figure 9.""""""\n    \n        super().__init__()\n\n        self.branch3x3stack = nn.Sequential(\n            BasicConv2d(input_channels, 384, kernel_size=1),\n            BasicConv2d(384, 448, kernel_size=(1, 3), padding=(0, 1)),\n            BasicConv2d(448, 512, kernel_size=(3, 1), padding=(1, 0)),\n        )\n        self.branch3x3stacka = BasicConv2d(512, 256, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3stackb = BasicConv2d(512, 256, kernel_size=(3, 1), padding=(1, 0))\n    \n        self.branch3x3 = BasicConv2d(input_channels, 384, kernel_size=1)\n        self.branch3x3a = BasicConv2d(384, 256, kernel_size=(3, 1), padding=(1, 0))\n        self.branch3x3b = BasicConv2d(384, 256, kernel_size=(1, 3), padding=(0, 1))\n\n        self.branch1x1 = BasicConv2d(input_channels, 256, kernel_size=1)\n\n        self.branchpool = nn.Sequential(\n            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n            BasicConv2d(input_channels, 256, kernel_size=1)\n        )\n\n    def forward(self, x):\n        branch3x3stack_output = self.branch3x3stack(x)\n        branch3x3stack_output = [\n            self.branch3x3stacka(branch3x3stack_output),\n            self.branch3x3stackb(branch3x3stack_output)\n        ]\n        branch3x3stack_output = torch.cat(branch3x3stack_output, 1)\n\n        branch3x3_output = self.branch3x3(x)\n        branch3x3_output = [\n            self.branch3x3a(branch3x3_output),\n            self.branch3x3b(branch3x3_output)\n        ]\n        branch3x3_output = torch.cat(branch3x3_output, 1)\n\n        branch1x1_output = self.branch1x1(x)\n\n        branchpool = self.branchpool(x)\n\n        output = [\n            branch1x1_output,\n            branch3x3_output,\n            branch3x3stack_output,\n            branchpool\n        ]\n\n        return torch.cat(output, 1)\n        \nclass InceptionV4(nn.Module):\n\n    def __init__(self, A, B, C, k=192, l=224, m=256, n=384, class_nums=100):\n\n        super().__init__()\n        self.stem = Inception_Stem(3)\n        self.inception_a = self._generate_inception_module(384, 384, A, InceptionA)\n        self.reduction_a = ReductionA(384, k, l, m, n)\n        output_channels = self.reduction_a.output_channels\n        self.inception_b = self._generate_inception_module(output_channels, 1024, B, InceptionB)\n        self.reduction_b = ReductionB(1024)\n        self.inception_c = self._generate_inception_module(1536, 1536, C, InceptionC)\n        self.avgpool = nn.AvgPool2d(7)\n\n        #""""""Dropout (keep 0.8)""""""\n        self.dropout = nn.Dropout2d(1 - 0.8)\n        self.linear = nn.Linear(1536, class_nums)\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.inception_a(x)\n        x = self.reduction_a(x)\n        x = self.inception_b(x)\n        x = self.reduction_b(x)\n        x = self.inception_c(x)\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        x = x.view(-1, 1536)\n        x = self.linear(x)\n\n        return x\n\n    @staticmethod    \n    def _generate_inception_module(input_channels, output_channels, block_num, block):\n\n        layers = nn.Sequential()\n        for l in range(block_num):\n            layers.add_module(""{}_{}"".format(block.__name__, l), block(input_channels))\n            input_channels = output_channels\n        \n        return layers\n\nclass InceptionResNetA(nn.Module):\n\n    #""""""Figure 16. The schema for 35 \xc3\x97 35 grid (Inception-ResNet-A) \n    #module of the Inception-ResNet-v2 network.""""""\n    def __init__(self, input_channels):\n\n        super().__init__()\n        self.branch3x3stack = nn.Sequential(\n            BasicConv2d(input_channels, 32, kernel_size=1),\n            BasicConv2d(32, 48, kernel_size=3, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, padding=1)\n        )\n\n        self.branch3x3 = nn.Sequential(\n            BasicConv2d(input_channels, 32, kernel_size=1),\n            BasicConv2d(32, 32, kernel_size=3, padding=1)\n        )\n\n        self.branch1x1 = BasicConv2d(input_channels, 32, kernel_size=1)\n\n        self.reduction1x1 = nn.Conv2d(128, 384, kernel_size=1)\n        self.shortcut = nn.Conv2d(input_channels, 384, kernel_size=1)\n        self.bn = nn.BatchNorm2d(384)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n\n        residual = [\n            self.branch1x1(x),\n            self.branch3x3(x),\n            self.branch3x3stack(x)\n        ]\n\n        residual = torch.cat(residual, 1)\n        residual = self.reduction1x1(residual)\n        shortcut = self.shortcut(x)\n\n        output = self.bn(shortcut + residual)\n        output = self.relu(output)\n\n        return output\n\nclass InceptionResNetB(nn.Module):\n\n    #""""""Figure 17. The schema for 17 \xc3\x97 17 grid (Inception-ResNet-B) module of \n    #the Inception-ResNet-v2 network.""""""\n    def __init__(self, input_channels):\n\n        super().__init__()\n        self.branch7x7 = nn.Sequential(\n            BasicConv2d(input_channels, 128, kernel_size=1),\n            BasicConv2d(128, 160, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(160, 192, kernel_size=(7, 1), padding=(3, 0))\n        )\n\n        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n\n        self.reduction1x1 = nn.Conv2d(384, 1154, kernel_size=1)\n        self.shortcut = nn.Conv2d(input_channels, 1154, kernel_size=1)\n\n        self.bn = nn.BatchNorm2d(1154)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        residual = [\n            self.branch1x1(x),\n            self.branch7x7(x)\n        ]\n\n        residual = torch.cat(residual, 1)\n\n        #""""""In general we picked some scaling factors between 0.1 and 0.3 to scale the residuals \n        #before their being added to the accumulated layer activations (cf. Figure 20).""""""\n        residual = self.reduction1x1(residual) * 0.1\n\n        shortcut = self.shortcut(x)\n\n        output = self.bn(residual + shortcut)\n        output = self.relu(output)\n\n        return output\n\n\nclass InceptionResNetC(nn.Module):\n\n    def __init__(self, input_channels):\n        \n        #Figure 19. The schema for 8\xc3\x978 grid (Inception-ResNet-C)\n        #module of the Inception-ResNet-v2 network.""""""\n        super().__init__()\n        self.branch3x3 = nn.Sequential(\n            BasicConv2d(input_channels, 192, kernel_size=1),\n            BasicConv2d(192, 224, kernel_size=(1, 3), padding=(0, 1)),\n            BasicConv2d(224, 256, kernel_size=(3, 1), padding=(1, 0))\n        )\n\n        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n        self.reduction1x1 = nn.Conv2d(448, 2048, kernel_size=1)\n        self.shorcut = nn.Conv2d(input_channels, 2048, kernel_size=1)\n        self.bn = nn.BatchNorm2d(2048)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        residual = [\n            self.branch1x1(x),\n            self.branch3x3(x)\n        ]\n\n        residual = torch.cat(residual, 1)\n        residual = self.reduction1x1(residual) * 0.1\n\n        shorcut = self.shorcut(x)\n\n        output = self.bn(shorcut + residual)\n        output = self.relu(output)\n\n        return output\n\nclass InceptionResNetReductionA(nn.Module):\n\n    #""""""Figure 7. The schema for 35 \xc3\x97 35 to 17 \xc3\x97 17 reduction module. \n    #Different variants of this blocks (with various number of filters) \n    #are used in Figure 9, and 15 in each of the new Inception(-v4, - ResNet-v1,\n    #-ResNet-v2) variants presented in this paper. The k, l, m, n numbers \n    #represent filter bank sizes which can be looked up in Table 1.\n    def __init__(self, input_channels, k, l, m, n):\n\n        super().__init__()\n        self.branch3x3stack = nn.Sequential(\n            BasicConv2d(input_channels, k, kernel_size=1),\n            BasicConv2d(k, l, kernel_size=3, padding=1),\n            BasicConv2d(l, m, kernel_size=3, stride=2)\n        )\n\n        self.branch3x3 = BasicConv2d(input_channels, n, kernel_size=3, stride=2)\n        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.output_channels = input_channels + n + m\n\n    def forward(self, x):\n\n        x = [\n            self.branch3x3stack(x),\n            self.branch3x3(x),\n            self.branchpool(x)\n        ]\n\n        return torch.cat(x, 1)\n\nclass InceptionResNetReductionB(nn.Module):\n\n    #""""""Figure 18. The schema for 17 \xc3\x97 17 to 8 \xc3\x97 8 grid-reduction module. \n    #Reduction-B module used by the wider Inception-ResNet-v1 network in\n    #Figure 15.""""""\n    #I believe it was a typo(Inception-ResNet-v1 should be Inception-ResNet-v2)\n    def __init__(self, input_channels):\n\n        super().__init__()\n        self.branchpool = nn.MaxPool2d(3, stride=2)\n\n        self.branch3x3a = nn.Sequential(\n            BasicConv2d(input_channels, 256, kernel_size=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch3x3b = nn.Sequential(\n            BasicConv2d(input_channels, 256, kernel_size=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch3x3stack = nn.Sequential(\n            BasicConv2d(input_channels, 256, kernel_size=1),\n            BasicConv2d(256, 288, kernel_size=3, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n    def forward(self, x):\n        x = [\n            self.branch3x3a(x),\n            self.branch3x3b(x),\n            self.branch3x3stack(x),\n            self.branchpool(x)\n        ]\n\n        x = torch.cat(x, 1)\n        return x\n\nclass InceptionResNetV2(nn.Module):\n\n    def __init__(self, A, B, C, k=256, l=256, m=384, n=384, class_nums=100):\n        super().__init__()\n        self.stem = Inception_Stem(3)\n        self.inception_resnet_a = self._generate_inception_module(384, 384, A, InceptionResNetA)\n        self.reduction_a = InceptionResNetReductionA(384, k, l, m, n)\n        output_channels = self.reduction_a.output_channels\n        self.inception_resnet_b = self._generate_inception_module(output_channels, 1154, B, InceptionResNetB)\n        self.reduction_b = InceptionResNetReductionB(1154)\n        self.inception_resnet_c = self._generate_inception_module(2146, 2048, C, InceptionResNetC)\n\n        #6x6 featuresize\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        #""""""Dropout (keep 0.8)""""""\n        self.dropout = nn.Dropout2d(1 - 0.8)\n        self.linear = nn.Linear(2048, class_nums)\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.inception_resnet_a(x)\n        x = self.reduction_a(x)\n        x = self.inception_resnet_b(x)\n        x = self.reduction_b(x)\n        x = self.inception_resnet_c(x)\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        x = x.view(-1, 2048)\n        x = self.linear(x)\n\n        return x\n\n    @staticmethod\n    def _generate_inception_module(input_channels, output_channels, block_num, block):\n\n        layers = nn.Sequential()\n        for l in range(block_num):\n            layers.add_module(""{}_{}"".format(block.__name__, l), block(input_channels))\n            input_channels = output_channels\n        \n        return layers\n\ndef inceptionv4():\n    return InceptionV4(4, 7, 3)\n\ndef inception_resnet_v2():\n    return InceptionResNetV2(5, 10, 5)\n'"
models/mobilenet.py,1,"b'""""""mobilenet in pytorch\n\n\n\n[1] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam\n\n    MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\n    https://arxiv.org/abs/1704.04861\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass DepthSeperabelConv2d(nn.Module):\n\n    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n        super().__init__()\n        self.depthwise = nn.Sequential(\n            nn.Conv2d(\n                input_channels,\n                input_channels,\n                kernel_size,\n                groups=input_channels,\n                **kwargs),\n            nn.BatchNorm2d(input_channels),\n            nn.ReLU(inplace=True)\n        )\n\n        self.pointwise = nn.Sequential(\n            nn.Conv2d(input_channels, output_channels, 1),\n            nn.BatchNorm2d(output_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n\n        return x\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n\n        super().__init__()\n        self.conv = nn.Conv2d(\n            input_channels, output_channels, kernel_size, **kwargs)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n\n        return x\n\n\nclass MobileNet(nn.Module):\n\n    """"""\n    Args:\n        width multipler: The role of the width multiplier \xce\xb1 is to thin \n                         a network uniformly at each layer. For a given \n                         layer and width multiplier \xce\xb1, the number of \n                         input channels M becomes \xce\xb1M and the number of \n                         output channels N becomes \xce\xb1N.\n    """"""\n\n    def __init__(self, width_multiplier=1, class_num=100):\n       super().__init__()\n\n       alpha = width_multiplier\n       self.stem = nn.Sequential(\n           BasicConv2d(3, int(32 * alpha), 3, padding=1, bias=False),\n           DepthSeperabelConv2d(\n               int(32 * alpha),\n               int(64 * alpha),\n               3,\n               padding=1,\n               bias=False\n           )\n       )\n\n       #downsample\n       self.conv1 = nn.Sequential(\n           DepthSeperabelConv2d(\n               int(64 * alpha),\n               int(128 * alpha),\n               3,\n               stride=2,\n               padding=1,\n               bias=False\n           ),\n           DepthSeperabelConv2d(\n               int(128 * alpha),\n               int(128 * alpha),\n               3,\n               padding=1,\n               bias=False\n           )\n       )\n\n       #downsample\n       self.conv2 = nn.Sequential(\n           DepthSeperabelConv2d(\n               int(128 * alpha),\n               int(256 * alpha),\n               3,\n               stride=2,\n               padding=1,\n               bias=False\n           ),\n           DepthSeperabelConv2d(\n               int(256 * alpha),\n               int(256 * alpha),\n               3,\n               padding=1,\n               bias=False\n           )\n       )\n\n       #downsample\n       self.conv3 = nn.Sequential(\n           DepthSeperabelConv2d(\n               int(256 * alpha),\n               int(512 * alpha),\n               3,\n               stride=2,\n               padding=1,\n               bias=False\n           ),\n\n           DepthSeperabelConv2d(\n               int(512 * alpha),\n               int(512 * alpha),\n               3,\n               padding=1,\n               bias=False\n           ),\n           DepthSeperabelConv2d(\n               int(512 * alpha),\n               int(512 * alpha),\n               3,\n               padding=1,\n               bias=False\n           ),\n           DepthSeperabelConv2d(\n               int(512 * alpha),\n               int(512 * alpha),\n               3,\n               padding=1,\n               bias=False\n           ),\n           DepthSeperabelConv2d(\n               int(512 * alpha),\n               int(512 * alpha),\n               3,\n               padding=1,\n               bias=False\n           ),\n           DepthSeperabelConv2d(\n               int(512 * alpha),\n               int(512 * alpha),\n               3,\n               padding=1,\n               bias=False\n           )\n       )\n\n       #downsample\n       self.conv4 = nn.Sequential(\n           DepthSeperabelConv2d(\n               int(512 * alpha),\n               int(1024 * alpha),\n               3,\n               stride=2,\n               padding=1,\n               bias=False\n           ),\n           DepthSeperabelConv2d(\n               int(1024 * alpha),\n               int(1024 * alpha),\n               3,\n               padding=1,\n               bias=False\n           )\n       )\n\n       self.fc = nn.Linear(int(1024 * alpha), class_num)\n       self.avg = nn.AdaptiveAvgPool2d(1)\n\n    def forward(self, x):\n        x = self.stem(x)\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        x = self.avg(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n\ndef mobilenet(alpha=1, class_num=100):\n    return MobileNet(alpha, class_num)\n\n'"
models/mobilenetv2.py,2,"b'""""""mobilenetv2 in pytorch\n\n\n\n[1] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen\n\n    MobileNetV2: Inverted Residuals and Linear Bottlenecks\n    https://arxiv.org/abs/1801.04381\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LinearBottleNeck(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride, t=6, class_num=100):\n        super().__init__()\n\n        self.residual = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * t, 1),\n            nn.BatchNorm2d(in_channels * t),\n            nn.ReLU6(inplace=True),\n\n            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n            nn.BatchNorm2d(in_channels * t),\n            nn.ReLU6(inplace=True),\n\n            nn.Conv2d(in_channels * t, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n\n        self.stride = stride\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n    \n    def forward(self, x):\n\n        residual = self.residual(x)\n\n        if self.stride == 1 and self.in_channels == self.out_channels:\n            residual += x\n        \n        return residual\n\nclass MobileNetV2(nn.Module):\n\n    def __init__(self, class_num=100):\n        super().__init__()\n\n        self.pre = nn.Sequential(\n            nn.Conv2d(3, 32, 1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU6(inplace=True)\n        )\n\n        self.stage1 = LinearBottleNeck(32, 16, 1, 1)\n        self.stage2 = self._make_stage(2, 16, 24, 2, 6)\n        self.stage3 = self._make_stage(3, 24, 32, 2, 6)\n        self.stage4 = self._make_stage(4, 32, 64, 2, 6)\n        self.stage5 = self._make_stage(3, 64, 96, 1, 6)\n        self.stage6 = self._make_stage(3, 96, 160, 1, 6)\n        self.stage7 = LinearBottleNeck(160, 320, 1, 6)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(320, 1280, 1),\n            nn.BatchNorm2d(1280),\n            nn.ReLU6(inplace=True)\n        )\n\n        self.conv2 = nn.Conv2d(1280, class_num, 1)\n            \n    def forward(self, x):\n        x = self.pre(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.stage5(x)\n        x = self.stage6(x)\n        x = self.stage7(x)\n        x = self.conv1(x)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = self.conv2(x)\n        x = x.view(x.size(0), -1)\n\n        return x\n    \n    def _make_stage(self, repeat, in_channels, out_channels, stride, t):\n\n        layers = []\n        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))\n        \n        while repeat - 1:\n            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t))\n            repeat -= 1\n        \n        return nn.Sequential(*layers)\n\ndef mobilenetv2():\n    return MobileNetV2()'"
models/nasnet.py,4,"b'""""""nasnet in pytorch\n\n\n\n[1] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le\n\n    Learning Transferable Architectures for Scalable Image Recognition\n    https://arxiv.org/abs/1707.07012\n""""""\n\nimport torch\nimport torch.nn as nn\n\nclass SeperableConv2d(nn.Module):\n\n    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n\n        super().__init__()\n        self.depthwise = nn.Conv2d(\n            input_channels,\n            input_channels,\n            kernel_size,\n            groups=input_channels,\n            **kwargs\n        )\n\n        self.pointwise = nn.Conv2d(\n            input_channels,\n            output_channels,\n            1\n        )\n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n\n        return x\n\nclass SeperableBranch(nn.Module):\n\n    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n        """"""Adds 2 blocks of [relu-separable conv-batchnorm].""""""\n        super().__init__()\n        self.block1 = nn.Sequential(\n            nn.ReLU(),\n            SeperableConv2d(input_channels, output_channels, kernel_size, **kwargs),\n            nn.BatchNorm2d(output_channels)\n        )\n\n        self.block2 = nn.Sequential(\n            nn.ReLU(),\n            SeperableConv2d(output_channels, output_channels, kernel_size, stride=1, padding=int(kernel_size / 2)),\n            nn.BatchNorm2d(output_channels)\n        )\n    \n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n\n        return x\n\nclass Fit(nn.Module):\n    """"""Make the cell outputs compatible\n\n    Args:\n        prev_filters: filter number of tensor prev, needs to be modified\n        filters: filter number of normal cell branch output filters\n    """"""\n\n    def __init__(self, prev_filters, filters):\n        super().__init__()\n        self.relu = nn.ReLU()\n\n        self.p1 = nn.Sequential(\n            nn.AvgPool2d(1, stride=2),\n            nn.Conv2d(prev_filters, int(filters / 2), 1)\n        )\n\n        #make sure there is no information loss\n        self.p2 = nn.Sequential(\n            nn.ConstantPad2d((0, 1, 0, 1), 0),\n            nn.ConstantPad2d((-1, 0, -1, 0), 0),   #cropping\n            nn.AvgPool2d(1, stride=2),\n            nn.Conv2d(prev_filters, int(filters / 2), 1)\n        )\n\n        self.bn = nn.BatchNorm2d(filters)\n\n        self.dim_reduce = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(prev_filters, filters, 1),\n            nn.BatchNorm2d(filters)\n        )\n\n        self.filters = filters\n    \n    def forward(self, inputs):\n        x, prev = inputs\n        if prev is None:\n            return x\n\n        #image size does not match\n        elif x.size(2) != prev.size(2):\n            prev = self.relu(prev)\n            p1 = self.p1(prev)\n            p2 = self.p2(prev)\n            prev = torch.cat([p1, p2], 1)\n            prev = self.bn(prev)\n\n        elif prev.size(1) != self.filters:\n            prev = self.dim_reduce(prev)\n\n        return prev\n\n\nclass NormalCell(nn.Module):\n\n    def __init__(self, x_in, prev_in, output_channels):\n        super().__init__()\n\n        self.dem_reduce = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(x_in, output_channels, 1, bias=False),\n            nn.BatchNorm2d(output_channels)\n        )\n\n        self.block1_left = SeperableBranch(\n            output_channels, \n            output_channels,\n            kernel_size=3,\n            padding=1,\n            bias=False\n        )\n        self.block1_right = nn.Sequential()\n\n        self.block2_left = SeperableBranch(\n            output_channels,\n            output_channels,\n            kernel_size=3,\n            padding=1,\n            bias=False\n        )\n        self.block2_right = SeperableBranch(\n            output_channels,\n            output_channels,\n            kernel_size=5,\n            padding=2,\n            bias=False\n        )\n\n        self.block3_left = nn.AvgPool2d(3, stride=1, padding=1)\n        self.block3_right = nn.Sequential()\n\n        self.block4_left = nn.AvgPool2d(3, stride=1, padding=1)\n        self.block4_right = nn.AvgPool2d(3, stride=1, padding=1)\n\n        self.block5_left = SeperableBranch(\n            output_channels,\n            output_channels,\n            kernel_size=5,\n            padding=2,\n            bias=False\n        )\n        self.block5_right = SeperableBranch(\n            output_channels,\n            output_channels,\n            kernel_size=3,\n            padding=1,\n            bias=False\n        )\n\n        self.fit = Fit(prev_in, output_channels)\n    \n    def forward(self, x):\n        x, prev = x\n\n        #return transformed x as new x, and original x as prev \n        #only prev tensor needs to be modified\n        prev = self.fit((x, prev)) \n\n        h = self.dem_reduce(x)\n\n        x1 = self.block1_left(h) + self.block1_right(h)\n        x2 = self.block2_left(prev) + self.block2_right(h)\n        x3 = self.block3_left(h) + self.block3_right(h)\n        x4 = self.block4_left(prev) + self.block4_right(prev)\n        x5 = self.block5_left(prev) + self.block5_right(prev)\n\n        return torch.cat([prev, x1, x2, x3, x4, x5], 1), x\n\nclass ReductionCell(nn.Module):\n\n    def __init__(self, x_in, prev_in, output_channels):\n        super().__init__()\n\n        self.dim_reduce = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(x_in, output_channels, 1),\n            nn.BatchNorm2d(output_channels)\n        )\n\n        #block1\n        self.layer1block1_left = SeperableBranch(output_channels, output_channels, 7, stride=2, padding=3)\n        self.layer1block1_right = SeperableBranch(output_channels, output_channels, 5, stride=2, padding=2)\n\n        #block2\n        self.layer1block2_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.layer1block2_right = SeperableBranch(output_channels, output_channels, 7, stride=2, padding=3)\n\n        #block3\n        self.layer1block3_left = nn.AvgPool2d(3, 2, 1)\n        self.layer1block3_right = SeperableBranch(output_channels, output_channels, 5, stride=2, padding=2)\n\n        #block5\n        self.layer2block1_left = nn.MaxPool2d(3, 2, 1)\n        self.layer2block1_right = SeperableBranch(output_channels, output_channels, 3, stride=1, padding=1)\n\n        #block4\n        self.layer2block2_left = nn.AvgPool2d(3, 1, 1)\n        self.layer2block2_right = nn.Sequential()\n    \n        self.fit = Fit(prev_in, output_channels)\n    \n    def forward(self, x):\n        x, prev = x\n        prev = self.fit((x, prev))\n\n        h = self.dim_reduce(x)\n\n        layer1block1 = self.layer1block1_left(prev) + self.layer1block1_right(h)\n        layer1block2 = self.layer1block2_left(h) + self.layer1block2_right(prev)\n        layer1block3 = self.layer1block3_left(h) + self.layer1block3_right(prev)\n        layer2block1 = self.layer2block1_left(h) + self.layer2block1_right(layer1block1)\n        layer2block2 = self.layer2block2_left(layer1block1) + self.layer2block2_right(layer1block2)\n\n        return torch.cat([\n            layer1block2, #https://github.com/keras-team/keras-applications/blob/master/keras_applications/nasnet.py line 739\n            layer1block3,\n            layer2block1,\n            layer2block2\n        ], 1), x\n\n\nclass NasNetA(nn.Module):\n\n    def __init__(self, repeat_cell_num, reduction_num, filters, stemfilter, class_num=100):\n        super().__init__()\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, stemfilter, 3, padding=1, bias=False),\n            nn.BatchNorm2d(stemfilter)\n        )\n\n        self.prev_filters = stemfilter\n        self.x_filters = stemfilter\n        self.filters = filters\n\n        self.cell_layers = self._make_layers(repeat_cell_num, reduction_num)\n\n        self.relu = nn.ReLU()\n        self.avg = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(self.filters * 6, class_num)\n    \n    \n    def _make_normal(self, block, repeat, output):\n        """"""make normal cell\n        Args:\n            block: cell type\n            repeat: number of repeated normal cell\n            output: output filters for each branch in normal cell\n        Returns:\n            stacked normal cells\n        """"""\n\n        layers = [] \n        for r in range(repeat):\n            layers.append(block(self.x_filters, self.prev_filters, output))\n            self.prev_filters = self.x_filters\n            self.x_filters = output * 6 #concatenate 6 branches\n        \n        return layers\n\n    def _make_reduction(self, block, output):\n        """"""make normal cell\n        Args:\n            block: cell type\n            output: output filters for each branch in reduction cell\n        Returns:\n            reduction cell\n        """"""\n\n        reduction = block(self.x_filters, self.prev_filters, output)\n        self.prev_filters = self.x_filters\n        self.x_filters = output * 4 #stack for 4 branches\n\n        return reduction\n    \n    def _make_layers(self, repeat_cell_num, reduction_num):\n\n        layers = []\n        for i in range(reduction_num):\n\n            layers.extend(self._make_normal(NormalCell, repeat_cell_num, self.filters))\n            self.filters *= 2\n            layers.append(self._make_reduction(ReductionCell, self.filters))\n        \n        layers.extend(self._make_normal(NormalCell, repeat_cell_num, self.filters))\n\n        return nn.Sequential(*layers)\n\n\n    def forward(self, x):\n\n        x = self.stem(x)\n        prev = None\n        x, prev = self.cell_layers((x, prev))\n        x = self.relu(x)\n        x = self.avg(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n        \n        \ndef nasnet():\n\n    #stem filters must be 44, it\'s a pytorch workaround, cant change to other number\n    return NasNetA(4, 2, 44, 44)    \n\n'"
models/preactresnet.py,2,"b'""""""preactresnet in pytorch\n\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun \n\n    Identity Mappings in Deep Residual Networks\n    https://arxiv.org/abs/1603.05027\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PreActBasic(nn.Module):\n\n    expansion = 1\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n        self.residual = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels * PreActBasic.expansion, kernel_size=3, padding=1)\n        )\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels * PreActBasic.expansion:\n            self.shortcut = nn.Conv2d(in_channels, out_channels * PreActBasic.expansion, 1, stride=stride)\n        \n    def forward(self, x):\n\n        res = self.residual(x)\n        shortcut = self.shortcut(x)\n\n        return res + shortcut\n\n\nclass PreActBottleNeck(nn.Module):\n\n    expansion = 4\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.residual = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels * PreActBottleNeck.expansion, 1)\n        )\n\n        self.shortcut = nn.Sequential()\n\n        if stride != 1 or in_channels != out_channels * PreActBottleNeck.expansion:\n            self.shortcut = nn.Conv2d(in_channels, out_channels * PreActBottleNeck.expansion, 1, stride=stride)\n    \n    def forward(self, x):\n\n        res = self.residual(x)\n        shortcut = self.shortcut(x)\n\n        return res + shortcut\n\nclass PreActResNet(nn.Module):\n\n    def __init__(self, block, num_block, class_num=100):\n        super().__init__()\n        self.input_channels = 64\n\n        self.pre = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n\n        self.stage1 = self._make_layers(block, num_block[0], 64,  1)\n        self.stage2 = self._make_layers(block, num_block[1], 128, 2)\n        self.stage3 = self._make_layers(block, num_block[2], 256, 2)\n        self.stage4 = self._make_layers(block, num_block[3], 512, 2)\n\n        self.linear = nn.Linear(self.input_channels, class_num)\n    \n    def _make_layers(self, block, block_num, out_channels, stride):\n        layers = []\n\n        layers.append(block(self.input_channels, out_channels, stride))\n        self.input_channels = out_channels * block.expansion\n\n        while block_num - 1:\n            layers.append(block(self.input_channels, out_channels, 1))\n            self.input_channels = out_channels * block.expansion\n            block_num -= 1\n        \n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.pre(x)\n\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n\n        return x\n\ndef preactresnet18():\n    return PreActResNet(PreActBasic, [2, 2, 2, 2])\n    \ndef preactresnet34():\n    return PreActResNet(PreActBasic, [3, 4, 6, 3])\n\ndef preactresnet50():\n    return PreActResNet(PreActBottleNeck, [3, 4, 6, 3])\n\ndef preactresnet101():\n    return PreActResNet(PreActBottleNeck, [3, 4, 23, 3])\n\ndef preactresnet152():\n    return PreActResNet(PreActBottleNeck, [3, 8, 36, 3])\n\n'"
models/resnet.py,1,"b'""""""resnet in pytorch\n\n\n\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n\n    Deep Residual Learning for Image Recognition\n    https://arxiv.org/abs/1512.03385v1\n""""""\n\nimport torch\nimport torch.nn as nn\n\nclass BasicBlock(nn.Module):\n    """"""Basic Block for resnet 18 and resnet 34\n\n    """"""\n\n    #BasicBlock and BottleNeck block \n    #have different output size\n    #we use class attribute expansion\n    #to distinct\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n\n        #residual function\n        self.residual_function = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n        )\n\n        #shortcut\n        self.shortcut = nn.Sequential()\n\n        #the shortcut output dimension is not the same with residual function\n        #use 1*1 convolution to match the dimension\n        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n            )\n        \n    def forward(self, x):\n        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n\nclass BottleNeck(nn.Module):\n    """"""Residual block for resnet over 50 layers\n\n    """"""\n    expansion = 4\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.residual_function = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n        )\n\n        self.shortcut = nn.Sequential()\n\n        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n            )\n        \n    def forward(self, x):\n        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n    \nclass ResNet(nn.Module):\n\n    def __init__(self, block, num_block, num_classes=100):\n        super().__init__()\n\n        self.in_channels = 64\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True))\n        #we use a different inputsize than the original paper\n        #so conv2_x\'s stride is 1\n        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, out_channels, num_blocks, stride):\n        """"""make resnet layers(by layer i didnt mean this \'layer\' was the \n        same as a neuron netowork layer, ex. conv layer), one layer may \n        contain more than one residual block \n\n        Args:\n            block: block type, basic block or bottle neck block\n            out_channels: output depth channel number of this layer\n            num_blocks: how many blocks per layer\n            stride: the stride of the first block of this layer\n        \n        Return:\n            return a resnet layer\n        """"""\n\n        # we have num_block blocks per layer, the first block \n        # could be 1 or 2, other blocks would always be 1\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels * block.expansion\n        \n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        output = self.conv1(x)\n        output = self.conv2_x(output)\n        output = self.conv3_x(output)\n        output = self.conv4_x(output)\n        output = self.conv5_x(output)\n        output = self.avg_pool(output)\n        output = output.view(output.size(0), -1)\n        output = self.fc(output)\n\n        return output \n\ndef resnet18():\n    """""" return a ResNet 18 object\n    """"""\n    return ResNet(BasicBlock, [2, 2, 2, 2])\n\ndef resnet34():\n    """""" return a ResNet 34 object\n    """"""\n    return ResNet(BasicBlock, [3, 4, 6, 3])\n\ndef resnet50():\n    """""" return a ResNet 50 object\n    """"""\n    return ResNet(BottleNeck, [3, 4, 6, 3])\n\ndef resnet101():\n    """""" return a ResNet 101 object\n    """"""\n    return ResNet(BottleNeck, [3, 4, 23, 3])\n\ndef resnet152():\n    """""" return a ResNet 152 object\n    """"""\n    return ResNet(BottleNeck, [3, 8, 36, 3])\n\n\n\n'"
models/resnext.py,2,"b'""""""resnext in pytorch\n\n\n\n[1] Saining Xie, Ross Girshick, Piotr Doll\xc3\xa1r, Zhuowen Tu, Kaiming He.\n\n    Aggregated Residual Transformations for Deep Neural Networks\n    https://arxiv.org/abs/1611.05431\n""""""\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#only implements ResNext bottleneck c\n\n\n#""""""This strategy exposes a new dimension, which we call \xe2\x80\x9ccardinality\xe2\x80\x9d \n#(the size of the set of transformations), as an essential factor \n#in addition to the dimensions of depth and width.""""""\nCARDINALITY = 32\nDEPTH = 4\nBASEWIDTH = 64\n\n#""""""The grouped convolutional layer in Fig. 3(c) performs 32 groups \n#of convolutions whose input and output channels are 4-dimensional. \n#The grouped convolutional layer concatenates them as the outputs \n#of the layer.""""""\n\nclass ResNextBottleNeckC(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        C = CARDINALITY #How many groups a feature map was splitted into\n\n        #""""""We note that the input/output width of the template is fixed as \n        #256-d (Fig. 3), We note that the input/output width of the template \n        #is fixed as 256-d (Fig. 3), and all widths are dou- bled each time \n        #when the feature map is subsampled (see Table 1).""""""\n        D = int(DEPTH * out_channels / BASEWIDTH) #number of channels per group\n        self.split_transforms = nn.Sequential(\n            nn.Conv2d(in_channels, C * D, kernel_size=1, groups=C, bias=False),\n            nn.BatchNorm2d(C * D),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(C * D, C * D, kernel_size=3, stride=stride, groups=C, padding=1, bias=False),\n            nn.BatchNorm2d(C * D),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(C * D, out_channels * 4, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels * 4),\n        )\n\n        self.shortcut = nn.Sequential()\n\n        if stride != 1 or in_channels != out_channels * 4:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels * 4, stride=stride, kernel_size=1, bias=False),\n                nn.BatchNorm2d(out_channels * 4)\n            )\n\n    def forward(self, x):\n        return F.relu(self.split_transforms(x) + self.shortcut(x))\n\nclass ResNext(nn.Module):\n\n    def __init__(self, block, num_blocks, class_names=100):\n        super().__init__()\n        self.in_channels = 64\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n\n        self.conv2 = self._make_layer(block, num_blocks[0], 64, 1)\n        self.conv3 = self._make_layer(block, num_blocks[1], 128, 2)\n        self.conv4 = self._make_layer(block, num_blocks[2], 256, 2)\n        self.conv5 = self._make_layer(block, num_blocks[3], 512, 2)\n        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * 4, 100)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.avg(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n        \n    def _make_layer(self, block, num_block, out_channels, stride):\n        """"""Building resnext block\n        Args:\n            block: block type(default resnext bottleneck c)\n            num_block: number of blocks per layer\n            out_channels: output channels per block\n            stride: block stride\n        \n        Returns:\n            a resnext layer\n        """"""\n        strides = [stride] + [1] * (num_block - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels * 4\n\n        return nn.Sequential(*layers)\n\ndef resnext50():\n    """""" return a resnext50(c32x4d) network\n    """"""\n    return ResNext(ResNextBottleNeckC, [3, 4, 6, 3])\n\ndef resnext101():\n    """""" return a resnext101(c32x4d) network\n    """"""\n    return ResNext(ResNextBottleNeckC, [3, 4, 23, 3])\n\ndef resnext152():\n    """""" return a resnext101(c32x4d) network\n    """"""\n    return ResNext(ResNextBottleNeckC, [3, 4, 36, 3])\n\n\n\n'"
models/rir.py,5,"b'""""""resnet in resnet in pytorch\n\n\n\n[1] Sasha Targ, Diogo Almeida, Kevin Lyman.\n\n    Resnet in Resnet: Generalizing Residual Architectures\n    https://arxiv.org/abs/1603.08029v1\n""""""\n\nimport torch\nimport torch.nn as nn\n\n#geralized  \nclass ResnetInit(nn.Module):\n    def __init__(self, in_channel, out_channel, stride):\n        super().__init__()\n\n        #""""""The modular unit of the generalized residual network architecture is a \n        #generalized residual block consisting of parallel states for a residual stream, \n        #r, which contains identity shortcut connections and is similar to the structure \n        #of a residual block from the original ResNet with a single convolutional layer \n        #(parameters W l,r\xe2\x86\x92r )\n        self.residual_stream_conv = nn.Conv2d(in_channel, out_channel, 3, padding=1, stride=stride)\n\n        #""""""and a transient stream, t, which is a standard convolutional layer\n        #(W l,t\xe2\x86\x92t ).""""""\n        self.transient_stream_conv = nn.Conv2d(in_channel, out_channel, 3, padding=1, stride=stride)\n\n        #""""""Two additional sets of convolutional filters in each block (W l,r\xe2\x86\x92t , W l,t\xe2\x86\x92r )\n        #also transfer information across streams.""""""\n        self.residual_stream_conv_across = nn.Conv2d(in_channel, out_channel, 3, padding=1, stride=stride)\n\n        #""""""We use equal numbers of filters for the residual and transient streams of the \n        #generalized residual network, but optimizing this hyperparameter could lead to \n        #further potential improvements.""""""\n        self.transient_stream_conv_across = nn.Conv2d(in_channel, out_channel, 3, padding=1, stride=stride)\n\n        self.residual_bn_relu = nn.Sequential(\n            nn.BatchNorm2d(out_channel),\n            nn.ReLU(inplace=True)\n        )\n\n        self.transient_bn_relu = nn.Sequential(\n            nn.BatchNorm2d(out_channel),\n            nn.ReLU(inplace=True)\n        )\n\n        #""""""The form of the shortcut connection can be an identity function with\n        #the appropriate padding or a projection as in He et al. (2015b).""""""\n        self.short_cut = nn.Sequential()\n        if in_channel != out_channel or stride != 1:\n            self.short_cut = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride)\n            )\n        \n\n    def forward(self, x):\n        x_residual, x_transient = x\n        residual_r_r = self.residual_stream_conv(x_residual)\n        residual_r_t = self.residual_stream_conv_across(x_residual)\n        residual_shortcut = self.short_cut(x_residual)\n\n        transient_t_t = self.transient_stream_conv(x_transient)\n        transient_t_r = self.transient_stream_conv_across(x_transient)\n\n        #transient_t_t = self.transient_stream_conv(x_residual)\n        #transient_t_r = self.transient_stream_conv_across(x_residual)\n        #""""""Same-stream and cross-stream activations are summed (along with the \n        #shortcut connection for the residual stream) before applying batch \n        #normalization and ReLU nonlinearities (together \xcf\x83) to get the output \n        #states of the block (Equation 1) (Ioffe & Szegedy, 2015).""""""\n        x_residual = self.residual_bn_relu(residual_r_r + transient_t_r + residual_shortcut)\n        x_transient = self.transient_bn_relu(residual_r_t + transient_t_t)\n\n        return x_residual, x_transient\n    \n\n\nclass RiRBlock(nn.Module):\n    def __init__(self, in_channel, out_channel, layer_num, stride, layer=ResnetInit):\n        super().__init__()\n        self.resnetinit = self._make_layers(in_channel, out_channel, layer_num, stride)\n\n        #self.short_cut = nn.Sequential()\n        #if stride != 1 or in_channel != out_channel:\n        #    self.short_cut = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride) \n\n    def forward(self, x):\n        x_residual, x_transient = self.resnetinit(x)\n        #x_residual = x_residual + self.short_cut(x[0])\n        #x_transient = x_transient + self.short_cut(x[1])\n\n        return (x_residual, x_transient)\n\n    #""""""Replacing each of the convolutional layers within a residual\n    #block from the original ResNet (Figure 1a) with a generalized residual block \n    #(Figure 1b) leads us to a new architecture we call ResNet in ResNet (RiR) \n    #(Figure 1d).""""""\n    def _make_layers(self, in_channel, out_channel, layer_num, stride, layer=ResnetInit):\n        strides = [stride] + [1] * (layer_num - 1)\n        layers = nn.Sequential()\n        for index, s in enumerate(strides):\n            layers.add_module(""generalized layers{}"".format(index), layer(in_channel, out_channel, s))\n            in_channel = out_channel\n\n        return layers\n\nclass ResnetInResneet(nn.Module):\n    def __init__(self, num_classes=100):\n        super().__init__()\n        base = int(96 / 2)\n        self.residual_pre_conv = nn.Sequential(\n            nn.Conv2d(3, base, 3, padding=1),\n            nn.BatchNorm2d(base),\n            nn.ReLU(inplace=True)\n        )\n        self.transient_pre_conv = nn.Sequential(\n            nn.Conv2d(3, base, 3, padding=1),\n            nn.BatchNorm2d(base),\n            nn.ReLU(inplace=True)\n        )\n\n        self.rir1 = RiRBlock(base, base, 2, 1)\n        self.rir2 = RiRBlock(base, base, 2, 1)\n        self.rir3 = RiRBlock(base, base * 2, 2, 2)\n        self.rir4 = RiRBlock(base * 2, base * 2, 2, 1)\n        self.rir5 = RiRBlock(base * 2, base * 2, 2, 1)\n        self.rir6 = RiRBlock(base * 2, base * 4, 2, 2)\n        self.rir7 = RiRBlock(base * 4, base * 4, 2, 1)\n        self.rir8 = RiRBlock(base * 4, base * 4, 2, 1)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(384, num_classes, kernel_size=3, stride=2), #without this convolution, loss will soon be nan\n            nn.BatchNorm2d(num_classes),\n            nn.ReLU(inplace=True),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(900, 450),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(450, 100),\n        )\n\n        self._weight_init()\n    \n    def forward(self, x):\n        x_residual = self.residual_pre_conv(x)\n        x_transient = self.transient_pre_conv(x)\n\n        x_residual, x_transient = self.rir1((x_residual, x_transient))\n        x_residual, x_transient = self.rir2((x_residual, x_transient))\n        x_residual, x_transient = self.rir3((x_residual, x_transient))\n        x_residual, x_transient = self.rir4((x_residual, x_transient))\n        x_residual, x_transient = self.rir5((x_residual, x_transient))\n        x_residual, x_transient = self.rir6((x_residual, x_transient))\n        x_residual, x_transient = self.rir7((x_residual, x_transient))\n        x_residual, x_transient = self.rir8((x_residual, x_transient))\n        h = torch.cat([x_residual, x_transient], 1)\n        h = self.conv1(h)\n        h = h.view(h.size()[0], -1)\n        h = self.classifier(h)\n\n        return h\n\n    def _weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal(m.weight)\n                m.bias.data.fill_(0.01)    \n    \n\ndef resnet_in_resnet():\n    return ResnetInResneet()\n\n#from torch.autograd import Variable\n#\n#net = resnet_in_resnet()\n#print(net(Variable(torch.randn(3, 3, 32, 32))).shape)\n'"
models/senet.py,2,"b'""""""senet in pytorch\n\n\n\n[1] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu\n\n    Squeeze-and-Excitation Networks\n    https://arxiv.org/abs/1709.01507\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BasicResidualSEBlock(nn.Module):\n\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, stride, r=16):\n        super().__init__()\n\n        self.residual = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            \n            nn.Conv2d(out_channels, out_channels * self.expansion, 3, padding=1),\n            nn.BatchNorm2d(out_channels * self.expansion),\n            nn.ReLU(inplace=True)\n        )\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels * self.expansion:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels * self.expansion, 1, stride=stride),\n                nn.BatchNorm2d(out_channels * self.expansion)\n            )\n        \n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        self.excitation = nn.Sequential(\n            nn.Linear(out_channels * self.expansion, out_channels * self.expansion // r),\n            nn.ReLU(inplace=True),\n            nn.Linear(out_channels * self.expansion // r, out_channels * self.expansion),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        shortcut = self.shortcut(x)\n        residual = self.residual(x)\n\n        squeeze = self.squeeze(residual)\n        squeeze = squeeze.view(squeeze.size(0), -1)\n        excitation = self.excitation(squeeze)\n        excitation = excitation.view(residual.size(0), residual.size(1), 1, 1)\n\n        x = residual * excitation.expand_as(residual) + shortcut\n\n        return F.relu(x)\n\nclass BottleneckResidualSEBlock(nn.Module):\n\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride, r=16):\n        super().__init__()\n\n        self.residual = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(out_channels, out_channels, 3, stride=stride, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(out_channels, out_channels * self.expansion, 1),\n            nn.BatchNorm2d(out_channels * self.expansion),\n            nn.ReLU(inplace=True)\n        )\n\n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        self.excitation = nn.Sequential(\n            nn.Linear(out_channels * self.expansion, out_channels * self.expansion // r),\n            nn.ReLU(inplace=True),\n            nn.Linear(out_channels * self.expansion // r, out_channels * self.expansion),\n            nn.Sigmoid()\n        )\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels * self.expansion:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels * self.expansion, 1, stride=stride),\n                nn.BatchNorm2d(out_channels * self.expansion)\n            )\n\n    def forward(self, x):\n\n        shortcut = self.shortcut(x)\n\n        residual = self.residual(x)\n        squeeze = self.squeeze(residual)\n        squeeze = squeeze.view(squeeze.size(0), -1)\n        excitation = self.excitation(squeeze)\n        excitation = excitation.view(residual.size(0), residual.size(1), 1, 1)\n\n        x = residual * excitation.expand_as(residual) + shortcut\n\n        return F.relu(x)\n\nclass SEResNet(nn.Module):\n\n    def __init__(self, block, block_num, class_num=100):\n        super().__init__()\n\n        self.in_channels = 64\n\n        self.pre = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n\n        self.stage1 = self._make_stage(block, block_num[0], 64, 1)\n        self.stage2 = self._make_stage(block, block_num[1], 128, 2)\n        self.stage3 = self._make_stage(block, block_num[2], 256, 2)\n        self.stage4 = self._make_stage(block, block_num[3], 516, 2)\n\n        self.linear = nn.Linear(self.in_channels, class_num)\n    \n    def forward(self, x):\n        x = self.pre(x)\n\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = x.view(x.size(0), -1)\n\n        x = self.linear(x)\n\n        return x\n\n    \n    def _make_stage(self, block, num, out_channels, stride):\n\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride))\n        self.in_channels = out_channels * block.expansion\n\n        while num - 1:\n            layers.append(block(self.in_channels, out_channels, 1))\n            num -= 1\n        \n        return nn.Sequential(*layers)\n        \ndef seresnet18():\n    return SEResNet(BasicResidualSEBlock, [2, 2, 2, 2])\n\ndef seresnet34():\n    return SEResNet(BasicResidualSEBlock, [3, 4, 6, 3])\n\ndef seresnet50():\n    return SEResNet(BottleneckResidualSEBlock, [3, 4, 6, 3])\n\ndef seresnet101():\n    return SEResNet(BottleneckResidualSEBlock, [3, 4, 23, 3])\n\ndef seresnet152():\n    return SEResNet(BottleneckResidualSEBlock, [3, 8, 36, 3])'"
models/shufflenet.py,3,"b'""""""shufflenet in pytorch\n\n\n\n[1] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun.\n\n    ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\n    https://arxiv.org/abs/1707.01083v2\n""""""\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n        super().__init__()\n        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size, **kwargs)\n        self.bn = nn.BatchNorm2d(output_channels)\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\nclass ChannelShuffle(nn.Module):\n\n    def __init__(self, groups):\n        super().__init__()\n        self.groups = groups\n    \n    def forward(self, x):\n        batchsize, channels, height, width = x.data.size()\n        channels_per_group = int(channels / self.groups)\n\n        #""""""suppose a convolutional layer with g groups whose output has\n        #g x n channels; we first reshape the output channel dimension\n        #into (g, n)""""""\n        x = x.view(batchsize, self.groups, channels_per_group, height, width)\n\n        #""""""transposing and then flattening it back as the input of next layer.""""""\n        x = x.transpose(1, 2).contiguous()\n        x = x.view(batchsize, -1, height, width)\n\n        return x\n\nclass DepthwiseConv2d(nn.Module):\n\n    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n        super().__init__()\n        self.depthwise = nn.Sequential(\n            nn.Conv2d(input_channels, output_channels, kernel_size, **kwargs),\n            nn.BatchNorm2d(output_channels)\n        )\n\n    def forward(self, x):\n        return self.depthwise(x)\n\nclass PointwiseConv2d(nn.Module):\n    def __init__(self, input_channels, output_channels, **kwargs):\n        super().__init__()\n        self.pointwise = nn.Sequential(\n            nn.Conv2d(input_channels, output_channels, 1, **kwargs),\n            nn.BatchNorm2d(output_channels)\n        )\n    \n    def forward(self, x):\n        return self.pointwise(x)\n\nclass ShuffleNetUnit(nn.Module):\n\n    def __init__(self, input_channels, output_channels, stage, stride, groups):\n        super().__init__()\n\n        #""""""Similar to [9], we set the number of bottleneck channels to 1/4 \n        #of the output channels for each ShuffleNet unit.""""""\n        self.bottlneck = nn.Sequential(\n            PointwiseConv2d(\n                input_channels, \n                int(output_channels / 4), \n                groups=groups\n            ),\n            nn.ReLU(inplace=True)\n        )\n\n        #""""""Note that for Stage 2, we do not apply group convolution on the first pointwise \n        #layer because the number of input channels is relatively small.""""""\n        if stage == 2:\n            self.bottlneck = nn.Sequential(\n                PointwiseConv2d(\n                    input_channels, \n                    int(output_channels / 4),\n                    groups=groups\n                ),\n                nn.ReLU(inplace=True)\n            )\n        \n        self.channel_shuffle = ChannelShuffle(groups)\n\n        self.depthwise = DepthwiseConv2d(\n            int(output_channels / 4), \n            int(output_channels / 4), \n            3, \n            groups=int(output_channels / 4), \n            stride=stride,\n            padding=1\n        )\n\n        self.expand = PointwiseConv2d(\n            int(output_channels / 4),\n            output_channels,\n            groups=groups\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n        self.fusion = self._add\n        self.shortcut = nn.Sequential()\n\n        #""""""As for the case where ShuffleNet is applied with stride, \n        #we simply make two modifications (see Fig 2 (c)): \n        #(i) add a 3 \xc3\x97 3 average pooling on the shortcut path; \n        #(ii) replace the element-wise addition with channel concatenation, \n        #which makes it easy to enlarge channel dimension with little extra \n        #computation cost.\n        if stride != 1 or input_channels != output_channels:\n            self.shortcut = nn.AvgPool2d(3, stride=2, padding=1)\n\n            self.expand = PointwiseConv2d(\n                int(output_channels / 4),\n                output_channels - input_channels,\n                groups=groups\n            )\n\n            self.fusion = self._cat\n    \n    def _add(self, x, y):\n        return torch.add(x, y)\n    \n    def _cat(self, x, y):\n        return torch.cat([x, y], dim=1)\n\n    def forward(self, x):\n        shortcut = self.shortcut(x)\n\n        shuffled = self.bottlneck(x)\n        shuffled = self.channel_shuffle(shuffled)\n        shuffled = self.depthwise(shuffled)\n        shuffled = self.expand(shuffled)\n\n        output = self.fusion(shortcut, shuffled)\n        output = self.relu(output)\n\n        return output\n\nclass ShuffleNet(nn.Module):\n\n    def __init__(self, num_blocks, num_classes=100, groups=3):\n        super().__init__()\n\n        if groups == 1:\n            out_channels = [24, 144, 288, 567]\n        elif groups == 2:\n            out_channels = [24, 200, 400, 800]\n        elif groups == 3:\n            out_channels = [24, 240, 480, 960]\n        elif groups == 4:\n            out_channels = [24, 272, 544, 1088]\n        elif groups == 8:\n            out_channels = [24, 384, 768, 1536]\n\n        self.conv1 = BasicConv2d(3, out_channels[0], 3, padding=1, stride=1)\n        self.input_channels = out_channels[0]\n\n        self.stage2 = self._make_stage(\n            ShuffleNetUnit, \n            num_blocks[0], \n            out_channels[1], \n            stride=2, \n            stage=2,\n            groups=groups\n        )\n\n        self.stage3 = self._make_stage(\n            ShuffleNetUnit, \n            num_blocks[1], \n            out_channels[2], \n            stride=2,\n            stage=3, \n            groups=groups\n        )\n\n        self.stage4 = self._make_stage(\n            ShuffleNetUnit,\n            num_blocks[2],\n            out_channels[3],\n            stride=2,\n            stage=4,\n            groups=groups\n        )\n\n        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(out_channels[3], num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.avg(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n    def _make_stage(self, block, num_blocks, output_channels, stride, stage, groups):\n        """"""make shufflenet stage \n\n        Args:\n            block: block type, shuffle unit\n            out_channels: output depth channel number of this stage\n            num_blocks: how many blocks per stage\n            stride: the stride of the first block of this stage\n            stage: stage index\n            groups: group number of group convolution \n        Return:\n            return a shuffle net stage\n        """"""\n        strides = [stride] + [1] * (num_blocks - 1)\n\n        stage = []\n\n        for stride in strides:\n            stage.append(\n                block(\n                    self.input_channels, \n                    output_channels, \n                    stride=stride, \n                    stage=stage, \n                    groups=groups\n                )\n            )\n            self.input_channels = output_channels\n\n        return nn.Sequential(*stage)\n\ndef shufflenet():\n    return ShuffleNet([4, 8, 4])\n                              \n\n\n\n'"
models/shufflenetv2.py,4,"b'""""""shufflenetv2 in pytorch\n\n\n\n[1] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun\n\n    ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\n    https://arxiv.org/abs/1807.11164\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef channel_split(x, split):\n    """"""split a tensor into two pieces along channel dimension\n    Args:\n        x: input tensor\n        split:(int) channel size for each pieces\n    """"""\n    assert x.size(1) == split * 2\n    return torch.split(x, split, dim=1)\n    \ndef channel_shuffle(x, groups):\n    """"""channel shuffle operation\n    Args:\n        x: input tensor\n        groups: input branch number\n    """"""\n\n    batch_size, channels, height, width = x.size()\n    channels_per_group = int(channels / groups)\n\n    x = x.view(batch_size, groups, channels_per_group, height, width)\n    x = x.transpose(1, 2).contiguous()\n    x = x.view(batch_size, -1, height, width)\n\n    return x\n\nclass ShuffleUnit(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.stride = stride\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        if stride != 1 or in_channels != out_channels:\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                nn.BatchNorm2d(in_channels),\n                nn.Conv2d(in_channels, int(out_channels / 2), 1),\n                nn.BatchNorm2d(int(out_channels / 2)),\n                nn.ReLU(inplace=True)\n            )\n\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                nn.BatchNorm2d(in_channels),\n                nn.Conv2d(in_channels, int(out_channels / 2), 1),\n                nn.BatchNorm2d(int(out_channels / 2)),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.shortcut = nn.Sequential()\n\n            in_channels = int(in_channels / 2)\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, 1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels),\n                nn.BatchNorm2d(in_channels),\n                nn.Conv2d(in_channels, in_channels, 1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(inplace=True) \n            )\n\n    \n    def forward(self, x):\n\n        if self.stride == 1 and self.out_channels == self.in_channels:\n            shortcut, residual = channel_split(x, int(self.in_channels / 2))\n        else:\n            shortcut = x\n            residual = x\n        \n        shortcut = self.shortcut(shortcut)\n        residual = self.residual(residual)\n        x = torch.cat([shortcut, residual], dim=1)\n        x = channel_shuffle(x, 2)\n        \n        return x\n\nclass ShuffleNetV2(nn.Module):\n\n    def __init__(self, ratio=1, class_num=100):\n        super().__init__()\n        if ratio == 0.5:\n            out_channels = [48, 96, 192, 1024]\n        elif ratio == 1:\n            out_channels = [116, 232, 464, 1024]\n        elif ratio == 1.5:\n            out_channels = [176, 352, 704, 1024]\n        elif ratio == 2:\n            out_channels = [244, 488, 976, 2048]\n        else:\n            ValueError(\'unsupported ratio number\')\n        \n        self.pre = nn.Sequential(\n            nn.Conv2d(3, 24, 3, padding=1),\n            nn.BatchNorm2d(24)\n        )\n\n        self.stage2 = self._make_stage(24, out_channels[0], 3)\n        self.stage3 = self._make_stage(out_channels[0], out_channels[1], 7)\n        self.stage4 = self._make_stage(out_channels[1], out_channels[2], 3)\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(out_channels[2], out_channels[3], 1),\n            nn.BatchNorm2d(out_channels[3]),\n            nn.ReLU(inplace=True)\n        )\n\n        self.fc = nn.Linear(out_channels[3], class_num)\n\n    def forward(self, x):\n        x = self.pre(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.conv5(x)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n    def _make_stage(self, in_channels, out_channels, repeat):\n        layers = []\n        layers.append(ShuffleUnit(in_channels, out_channels, 2))\n\n        while repeat:\n            layers.append(ShuffleUnit(out_channels, out_channels, 1))\n            repeat -= 1\n        \n        return nn.Sequential(*layers)\n\ndef shufflenetv2():\n    return ShuffleNetV2()\n\n\n\n\n\n'"
models/squeezenet.py,2,"b'""""""squeezenet in pytorch\n\n\n\n[1] Song Han, Jeff Pool, John Tran, William J. Dally\n\n    squeezenet: Learning both Weights and Connections for Efficient Neural Networks\n    https://arxiv.org/abs/1506.02626\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass Fire(nn.Module):\n\n    def __init__(self, in_channel, out_channel, squzee_channel):\n\n        super().__init__()\n        self.squeeze = nn.Sequential(\n            nn.Conv2d(in_channel, squzee_channel, 1),\n            nn.BatchNorm2d(squzee_channel),\n            nn.ReLU(inplace=True)\n        )\n\n        self.expand_1x1 = nn.Sequential(\n            nn.Conv2d(squzee_channel, int(out_channel / 2), 1),\n            nn.BatchNorm2d(int(out_channel / 2)),\n            nn.ReLU(inplace=True)\n        )\n\n        self.expand_3x3 = nn.Sequential(\n            nn.Conv2d(squzee_channel, int(out_channel / 2), 3, padding=1),\n            nn.BatchNorm2d(int(out_channel / 2)),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n\n        x = self.squeeze(x)\n        x = torch.cat([\n            self.expand_1x1(x),\n            self.expand_3x3(x)\n        ], 1)\n\n        return x\n\nclass SqueezeNet(nn.Module):\n\n    """"""mobile net with simple bypass""""""\n    def __init__(self, class_num=100):\n\n        super().__init__()\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 96, 3, padding=1),\n            nn.BatchNorm2d(96),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2)\n        )\n\n        self.fire2 = Fire(96, 128, 16)\n        self.fire3 = Fire(128, 128, 16)\n        self.fire4 = Fire(128, 256, 32)\n        self.fire5 = Fire(256, 256, 32)\n        self.fire6 = Fire(256, 384, 48)\n        self.fire7 = Fire(384, 384, 48)\n        self.fire8 = Fire(384, 512, 64)\n        self.fire9 = Fire(512, 512, 64)\n\n        self.conv10 = nn.Conv2d(512, class_num, 1)\n        self.avg = nn.AdaptiveAvgPool2d(1)\n        self.maxpool = nn.MaxPool2d(2, 2)\n            \n    def forward(self, x):\n        x = self.stem(x)\n\n        f2 = self.fire2(x)\n        f3 = self.fire3(f2) + f2\n        f4 = self.fire4(f3)\n        f4 = self.maxpool(f4)\n\n        f5 = self.fire5(f4) + f4\n        f6 = self.fire6(f5)\n        f7 = self.fire7(f6) + f6\n        f8 = self.fire8(f7)\n        f8 = self.maxpool(f8)\n\n        f9 = self.fire9(f8)\n        c10 = self.conv10(f9)\n\n        x = self.avg(c10)\n        x = x.view(x.size(0), -1)\n\n        return x\n\ndef squeezenet(class_num=100):\n    return SqueezeNet(class_num=class_num)\n'"
models/vgg.py,2,"b'""""""vgg in pytorch\n\n\n[1] Karen Simonyan, Andrew Zisserman\n\n    Very Deep Convolutional Networks for Large-Scale Image Recognition.\n    https://arxiv.org/abs/1409.1556v6\n""""""\n\'\'\'VGG11/13/16/19 in Pytorch.\'\'\'\n\nimport torch\nimport torch.nn as nn\n\ncfg = {\n    \'A\' : [64,     \'M\', 128,      \'M\', 256, 256,           \'M\', 512, 512,           \'M\', 512, 512,           \'M\'],\n    \'B\' : [64, 64, \'M\', 128, 128, \'M\', 256, 256,           \'M\', 512, 512,           \'M\', 512, 512,           \'M\'],\n    \'D\' : [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256,      \'M\', 512, 512, 512,      \'M\', 512, 512, 512,      \'M\'],\n    \'E\' : [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\']\n}\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_class=100):\n        super().__init__()\n        self.features = features\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, num_class)\n        )\n\n    def forward(self, x):\n        output = self.features(x)\n        output = output.view(output.size()[0], -1)\n        output = self.classifier(output)\n    \n        return output\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n\n    input_channel = 3\n    for l in cfg:\n        if l == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            continue\n\n        layers += [nn.Conv2d(input_channel, l, kernel_size=3, padding=1)]\n\n        if batch_norm:\n            layers += [nn.BatchNorm2d(l)]\n        \n        layers += [nn.ReLU(inplace=True)]\n        input_channel = l\n    \n    return nn.Sequential(*layers)\n\ndef vgg11_bn():\n    return VGG(make_layers(cfg[\'A\'], batch_norm=True))\n\ndef vgg13_bn():\n    return VGG(make_layers(cfg[\'B\'], batch_norm=True))\n\ndef vgg16_bn():\n    return VGG(make_layers(cfg[\'D\'], batch_norm=True))\n\ndef vgg19_bn():\n    return VGG(make_layers(cfg[\'E\'], batch_norm=True))\n\n\n'"
models/xception.py,1,"b'""""""xception in pytorch\n\n\n[1] Fran\xc3\xa7ois Chollet\n\n    Xception: Deep Learning with Depthwise Separable Convolutions\n    https://arxiv.org/abs/1610.02357\n""""""\n\nimport torch\nimport torch.nn as nn\n\nclass SeperableConv2d(nn.Module):\n\n    #***Figure 4. An \xe2\x80\x9cextreme\xe2\x80\x9d version of our Inception module, \n    #with one spatial convolution per output channel of the 1x1 \n    #convolution.""""""\n    def __init__(self, input_channels, output_channels, kernel_size, **kwargs):\n\n        super().__init__()\n        self.depthwise = nn.Conv2d(\n            input_channels, \n            input_channels, \n            kernel_size, \n            groups=input_channels,\n            bias=False,\n            **kwargs\n        )\n\n        self.pointwise = nn.Conv2d(input_channels, output_channels, 1, bias=False)\n\n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n\n        return x\n\nclass EntryFlow(nn.Module):\n\n    def __init__(self):\n\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True)\n        )\n\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n\n        self.conv3_residual = nn.Sequential(\n            SeperableConv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            SeperableConv2d(128, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(3, stride=2, padding=1),\n        )\n\n        self.conv3_shortcut = nn.Sequential(\n            nn.Conv2d(64, 128, 1, stride=2),\n            nn.BatchNorm2d(128),\n        )\n\n        self.conv4_residual = nn.Sequential(\n            nn.ReLU(inplace=True),\n            SeperableConv2d(128, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            SeperableConv2d(256, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(3, stride=2, padding=1)\n        )\n\n        self.conv4_shortcut = nn.Sequential(\n            nn.Conv2d(128, 256, 1, stride=2),\n            nn.BatchNorm2d(256),\n        )\n\n        #no downsampling\n        self.conv5_residual = nn.Sequential(\n            nn.ReLU(inplace=True),\n            SeperableConv2d(256, 728, 3, padding=1),\n            nn.BatchNorm2d(728),\n            nn.ReLU(inplace=True),\n            SeperableConv2d(728, 728, 3, padding=1),\n            nn.BatchNorm2d(728),\n            nn.MaxPool2d(3, 1, padding=1)\n        )\n\n        #no downsampling\n        self.conv5_shortcut = nn.Sequential(\n            nn.Conv2d(256, 728, 1),\n            nn.BatchNorm2d(728)\n        )\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        residual = self.conv3_residual(x)\n        shortcut = self.conv3_shortcut(x)\n        x = residual + shortcut\n        residual = self.conv4_residual(x)\n        shortcut = self.conv4_shortcut(x)\n        x = residual + shortcut\n        residual = self.conv5_residual(x)\n        shortcut = self.conv5_shortcut(x)\n        x = residual + shortcut\n\n        return x\n\nclass MiddleFLowBlock(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.shortcut = nn.Sequential()\n        self.conv1 = nn.Sequential(\n            nn.ReLU(inplace=True),\n            SeperableConv2d(728, 728, 3, padding=1),\n            nn.BatchNorm2d(728)\n        )\n        self.conv2 = nn.Sequential(\n            nn.ReLU(inplace=True),\n            SeperableConv2d(728, 728, 3, padding=1),\n            nn.BatchNorm2d(728)\n        )\n        self.conv3 = nn.Sequential(\n            nn.ReLU(inplace=True),\n            SeperableConv2d(728, 728, 3, padding=1),\n            nn.BatchNorm2d(728)\n        )\n    \n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.conv2(residual)\n        residual = self.conv3(residual)\n\n        shortcut = self.shortcut(x)\n\n        return shortcut + residual\n\nclass MiddleFlow(nn.Module):\n    def __init__(self, block):\n        super().__init__()\n\n        #""""""then through the middle flow which is repeated eight times""""""\n        self.middel_block = self._make_flow(block, 8)\n    \n    def forward(self, x):\n        x = self.middel_block(x)\n        return x\n\n    def _make_flow(self, block, times):\n        flows = []\n        for i in range(times):\n            flows.append(block())\n        \n        return nn.Sequential(*flows)\n\n\nclass ExitFLow(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.residual = nn.Sequential(\n            nn.ReLU(),\n            SeperableConv2d(728, 728, 3, padding=1),\n            nn.BatchNorm2d(728),\n            nn.ReLU(),\n            SeperableConv2d(728, 1024, 3, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.MaxPool2d(3, stride=2, padding=1)\n        )\n\n        self.shortcut = nn.Sequential(\n            nn.Conv2d(728, 1024, 1, stride=2),\n            nn.BatchNorm2d(1024)\n        )\n\n        self.conv = nn.Sequential(\n            SeperableConv2d(1024, 1536, 3, padding=1),\n            nn.BatchNorm2d(1536),\n            nn.ReLU(inplace=True),\n            SeperableConv2d(1536, 2048, 3, padding=1),\n            nn.BatchNorm2d(2048),\n            nn.ReLU(inplace=True)\n        )\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n    def forward(self, x):\n        shortcut = self.shortcut(x) \n        residual = self.residual(x)\n        output = shortcut + residual\n        output = self.conv(output) \n        output = self.avgpool(output)\n       \n        return output\n\nclass Xception(nn.Module):\n\n    def __init__(self, block, num_class=100):\n        super().__init__()\n        self.entry_flow = EntryFlow()\n        self.middel_flow = MiddleFlow(block)\n        self.exit_flow = ExitFLow()\n\n        self.fc = nn.Linear(2048, num_class)\n    \n    def forward(self, x):\n        x = self.entry_flow(x)\n        x = self.middel_flow(x)\n        x = self.exit_flow(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef xception():\n    return Xception(MiddleFLowBlock)\n\n\n'"
