file_path,api_count,code
models.py,49,"b'import math\nfrom abc import ABC, abstractmethod\n\nimport torch\n\n\nclass MuZeroNetwork:\n    def __new__(cls, config):\n        if config.network == ""fullyconnected"":\n            return MuZeroFullyConnectedNetwork(\n                config.observation_shape,\n                config.stacked_observations,\n                len(config.action_space),\n                config.encoding_size,\n                config.fc_reward_layers,\n                config.fc_value_layers,\n                config.fc_policy_layers,\n                config.fc_representation_layers,\n                config.fc_dynamics_layers,\n                config.support_size,\n            )\n        elif config.network == ""resnet"":\n            return MuZeroResidualNetwork(\n                config.observation_shape,\n                config.stacked_observations,\n                len(config.action_space),\n                config.blocks,\n                config.channels,\n                config.reduced_channels_reward,\n                config.reduced_channels_value,\n                config.reduced_channels_policy,\n                config.resnet_fc_reward_layers,\n                config.resnet_fc_value_layers,\n                config.resnet_fc_policy_layers,\n                config.support_size,\n                config.downsample,\n            )\n        else:\n            raise ValueError(\n                \'The network parameter should be ""fullyconnected"" or ""resnet"".\'\n            )\n\n\nclass AbstractNetwork(ABC, torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        pass\n\n    @abstractmethod\n    def initial_inference(self, observation):\n        pass\n\n    @abstractmethod\n    def recurrent_inference(self, encoded_state, action):\n        pass\n\n    def get_weights(self):\n        return {key: value.cpu() for key, value in self.state_dict().items()}\n\n    def set_weights(self, weights):\n        self.load_state_dict(weights)\n\n\n##################################\n######## Fully Connected #########\n\n\nclass MuZeroFullyConnectedNetwork(AbstractNetwork):\n    def __init__(\n        self,\n        observation_shape,\n        stacked_observations,\n        action_space_size,\n        encoding_size,\n        fc_reward_layers,\n        fc_value_layers,\n        fc_policy_layers,\n        fc_representation_layers,\n        fc_dynamics_layers,\n        support_size,\n    ):\n        super().__init__()\n        self.action_space_size = action_space_size\n        self.full_support_size = 2 * support_size + 1\n\n        self.representation_network = FullyConnectedNetwork(\n            observation_shape[0]\n            * observation_shape[1]\n            * observation_shape[2]\n            * (stacked_observations + 1)\n            + stacked_observations * observation_shape[1] * observation_shape[2],\n            fc_representation_layers,\n            encoding_size,\n        )\n\n        self.dynamics_encoded_state_network = FullyConnectedNetwork(\n            encoding_size + self.action_space_size, fc_dynamics_layers, encoding_size\n        )\n        self.dynamics_reward_network = FullyConnectedNetwork(\n            encoding_size, fc_reward_layers, self.full_support_size,\n        )\n\n        self.prediction_policy_network = FullyConnectedNetwork(\n            encoding_size, fc_policy_layers, self.action_space_size\n        )\n        self.prediction_value_network = FullyConnectedNetwork(\n            encoding_size, fc_value_layers, self.full_support_size,\n        )\n\n    def prediction(self, encoded_state):\n        policy_logits = self.prediction_policy_network(encoded_state)\n        value = self.prediction_value_network(encoded_state)\n        return policy_logits, value\n\n    def representation(self, observation):\n        encoded_state = self.representation_network(\n            observation.view(observation.shape[0], -1)\n        )\n        # Scale encoded state between [0, 1] (See appendix paper Training)\n        min_encoded_state = encoded_state.min(1, keepdim=True)[0]\n        max_encoded_state = encoded_state.max(1, keepdim=True)[0]\n        scale_encoded_state = max_encoded_state - min_encoded_state\n        scale_encoded_state[scale_encoded_state < 1e-5] += 1e-5\n        encoded_state_normalized = (\n            encoded_state - min_encoded_state\n        ) / scale_encoded_state\n        return encoded_state_normalized\n\n    def dynamics(self, encoded_state, action):\n        # Stack encoded_state with a game specific one hot encoded action (See paper appendix Network Architecture)\n        action_one_hot = (\n            torch.zeros((action.shape[0], self.action_space_size))\n            .to(action.device)\n            .float()\n        )\n        action_one_hot.scatter_(1, action.long(), 1.0)\n        x = torch.cat((encoded_state, action_one_hot), dim=1)\n\n        next_encoded_state = self.dynamics_encoded_state_network(x)\n\n        reward = self.dynamics_reward_network(next_encoded_state)\n\n        # Scale encoded state between [0, 1] (See paper appendix Training)\n        min_next_encoded_state = next_encoded_state.min(1, keepdim=True)[0]\n        max_next_encoded_state = next_encoded_state.max(1, keepdim=True)[0]\n        scale_next_encoded_state = max_next_encoded_state - min_next_encoded_state\n        scale_next_encoded_state[scale_next_encoded_state < 1e-5] += 1e-5\n        next_encoded_state_normalized = (\n            next_encoded_state - min_next_encoded_state\n        ) / scale_next_encoded_state\n\n        return next_encoded_state_normalized, reward\n\n    def initial_inference(self, observation):\n        encoded_state = self.representation(observation)\n        policy_logits, value = self.prediction(encoded_state)\n        # reward equal to 0 for consistency\n        reward = (\n            torch.zeros(1, self.full_support_size)\n            .scatter(1, torch.tensor([[self.full_support_size // 2]]).long(), 1.0)\n            .repeat(len(observation), 1)\n            .to(observation.device)\n        )\n\n        return (\n            value,\n            reward,\n            policy_logits,\n            encoded_state,\n        )\n\n    def recurrent_inference(self, encoded_state, action):\n        next_encoded_state, reward = self.dynamics(encoded_state, action)\n        policy_logits, value = self.prediction(next_encoded_state)\n        return value, reward, policy_logits, next_encoded_state\n\n\n###### End Fully Connected #######\n##################################\n\n\n##################################\n############# ResNet #############\n\n\ndef conv3x3(in_channels, out_channels, stride=1):\n    return torch.nn.Conv2d(\n        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n    )\n\n\n# Residual block\nclass ResidualBlock(torch.nn.Module):\n    def __init__(self, num_channels, stride=1):\n        super().__init__()\n        self.conv1 = conv3x3(num_channels, num_channels, stride)\n        self.bn1 = torch.nn.BatchNorm2d(num_channels)\n        self.conv2 = conv3x3(num_channels, num_channels)\n        self.bn2 = torch.nn.BatchNorm2d(num_channels)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = torch.nn.functional.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += x\n        out = torch.nn.functional.relu(out)\n        return out\n\n\n# Downsample observations before representation network (See paper appendix Network Architecture)\nclass DownSample(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(\n            in_channels,\n            out_channels // 2,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False,\n        )\n        self.resblocks1 = torch.nn.ModuleList(\n            [ResidualBlock(out_channels // 2) for _ in range(2)]\n        )\n        self.conv2 = torch.nn.Conv2d(\n            out_channels // 2,\n            out_channels,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False,\n        )\n        self.resblocks2 = torch.nn.ModuleList(\n            [ResidualBlock(out_channels) for _ in range(3)]\n        )\n        self.pooling1 = torch.nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n        self.resblocks3 = torch.nn.ModuleList(\n            [ResidualBlock(out_channels) for _ in range(3)]\n        )\n        self.pooling2 = torch.nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        for block in self.resblocks1:\n            out = block(out)\n        out = self.conv2(out)\n        for block in self.resblocks2:\n            out = block(out)\n        out = self.pooling1(out)\n        for block in self.resblocks3:\n            out = block(out)\n        out = self.pooling2(out)\n        return out\n\n\nclass RepresentationNetwork(torch.nn.Module):\n    def __init__(\n        self,\n        observation_shape,\n        stacked_observations,\n        num_blocks,\n        num_channels,\n        downsample,\n    ):\n        super().__init__()\n        self.use_downsample = downsample\n        if self.use_downsample:\n            self.downsample = DownSample(\n                observation_shape[0] * (stacked_observations + 1)\n                + stacked_observations,\n                num_channels,\n            )\n        self.conv = conv3x3(\n            observation_shape[0] * (stacked_observations + 1) + stacked_observations,\n            num_channels,\n        )\n        self.bn = torch.nn.BatchNorm2d(num_channels)\n        self.resblocks = torch.nn.ModuleList(\n            [ResidualBlock(num_channels) for _ in range(num_blocks)]\n        )\n\n    def forward(self, x):\n        if self.use_downsample:\n            out = self.downsample(x)\n        else:\n            out = self.conv(x)\n            out = self.bn(out)\n            out = torch.nn.functional.relu(out)\n\n        for block in self.resblocks:\n            out = block(out)\n        return out\n\n\nclass DynamicsNetwork(torch.nn.Module):\n    def __init__(\n        self,\n        num_blocks,\n        num_channels,\n        reduced_channels_reward,\n        fc_reward_layers,\n        full_support_size,\n        block_output_size_reward,\n    ):\n        super().__init__()\n        self.conv = conv3x3(num_channels, num_channels - 1)\n        self.bn = torch.nn.BatchNorm2d(num_channels - 1)\n        self.resblocks = torch.nn.ModuleList(\n            [ResidualBlock(num_channels - 1) for _ in range(num_blocks)]\n        )\n\n        self.conv1x1_reward = torch.nn.Conv2d(num_channels - 1, reduced_channels_reward, 1)\n        self.block_output_size_reward = block_output_size_reward\n        self.fc = FullyConnectedNetwork(\n            self.block_output_size_reward,\n            fc_reward_layers,\n            full_support_size,\n        )\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = torch.nn.functional.relu(out)\n        for block in self.resblocks:\n            out = block(out)\n        state = out\n        out = self.conv1x1_reward(out)\n        out = out.view(-1, self.block_output_size_reward)\n        reward = self.fc(out)\n        return state, reward\n\n\nclass PredictionNetwork(torch.nn.Module):\n    def __init__(\n        self,\n        action_space_size,\n        num_blocks,\n        num_channels,\n        reduced_channels_value,\n        reduced_channels_policy,\n        fc_value_layers,\n        fc_policy_layers,\n        full_support_size,\n        block_output_size_value,\n        block_output_size_policy\n    ):\n        super().__init__()\n        self.resblocks = torch.nn.ModuleList(\n            [ResidualBlock(num_channels) for _ in range(num_blocks)]\n        )\n\n        self.conv1x1_value = torch.nn.Conv2d(num_channels, reduced_channels_value, 1)\n        self.conv1x1_policy = torch.nn.Conv2d(num_channels, reduced_channels_policy, 1)\n        self.block_output_size_value = block_output_size_value\n        self.block_output_size_policy = block_output_size_policy\n        self.fc_value = FullyConnectedNetwork(\n            self.block_output_size_value, fc_value_layers, full_support_size\n        )\n        self.fc_policy = FullyConnectedNetwork(\n            self.block_output_size_policy,\n            fc_policy_layers,\n            action_space_size,\n        )\n\n    def forward(self, x):\n        out = x\n        for block in self.resblocks:\n            out = block(out)\n        value = self.conv1x1_value(out)\n        policy = self.conv1x1_policy(out)\n        value = value.view(-1, self.block_output_size_value)\n        policy = policy.view(-1, self.block_output_size_policy)\n        value = self.fc_value(value)\n        policy = self.fc_policy(policy)\n        return policy, value\n\n\nclass MuZeroResidualNetwork(AbstractNetwork):\n    def __init__(\n        self,\n        observation_shape,\n        stacked_observations,\n        action_space_size,\n        num_blocks,\n        num_channels,\n        reduced_channels_reward,\n        reduced_channels_value,\n        reduced_channels_policy,\n        fc_reward_layers,\n        fc_value_layers,\n        fc_policy_layers,\n        support_size,\n        downsample,\n    ):\n        super().__init__()\n        self.action_space_size = action_space_size\n        self.full_support_size = 2 * support_size + 1\n        block_output_size_reward = (\n            (\n                reduced_channels_reward\n                * (observation_shape[1] // 16)\n                * (observation_shape[2] // 16)\n            )\n            if downsample\n            else (reduced_channels_reward * observation_shape[1] * observation_shape[2])\n        )\n\n        block_output_size_value = (\n            (\n                reduced_channels_value\n                * (observation_shape[1] // 16)\n                * (observation_shape[2] // 16)\n            )\n            if downsample\n            else (reduced_channels_value * observation_shape[1] * observation_shape[2])\n        )\n\n        block_output_size_policy = (\n            (\n                reduced_channels_policy\n                * (observation_shape[1] // 16)\n                * (observation_shape[2] // 16)\n            )\n            if downsample\n            else (reduced_channels_policy * observation_shape[1] * observation_shape[2])\n        )\n\n        self.representation_network = RepresentationNetwork(\n            observation_shape,\n            stacked_observations,\n            num_blocks,\n            num_channels,\n            downsample,\n        )\n\n        self.dynamics_network = DynamicsNetwork(\n            num_blocks,\n            num_channels + 1,\n            reduced_channels_reward,\n            fc_reward_layers,\n            self.full_support_size,\n            block_output_size_reward,\n        )\n\n        self.prediction_network = PredictionNetwork(\n            action_space_size,\n            num_blocks,\n            num_channels,\n            reduced_channels_value,\n            reduced_channels_policy,\n            fc_value_layers,\n            fc_policy_layers,\n            self.full_support_size,\n            block_output_size_value,\n            block_output_size_policy,\n        )\n\n    def prediction(self, encoded_state):\n        policy, value = self.prediction_network(encoded_state)\n        return policy, value\n\n    def representation(self, observation):\n        encoded_state = self.representation_network(observation)\n\n        # Scale encoded state between [0, 1] (See appendix paper Training)\n        min_encoded_state = (\n            encoded_state.view(\n                -1,\n                encoded_state.shape[1],\n                encoded_state.shape[2] * encoded_state.shape[3],\n            )\n            .min(2, keepdim=True)[0]\n            .unsqueeze(-1)\n        )\n        max_encoded_state = (\n            encoded_state.view(\n                -1,\n                encoded_state.shape[1],\n                encoded_state.shape[2] * encoded_state.shape[3],\n            )\n            .max(2, keepdim=True)[0]\n            .unsqueeze(-1)\n        )\n        scale_encoded_state = max_encoded_state - min_encoded_state\n        scale_encoded_state[scale_encoded_state < 1e-5] += 1e-5\n        encoded_state_normalized = (\n            encoded_state - min_encoded_state\n        ) / scale_encoded_state\n        return encoded_state_normalized\n\n    def dynamics(self, encoded_state, action):\n        # Stack encoded_state with a game specific one hot encoded action (See paper appendix Network Architecture)\n        action_one_hot = (\n            torch.ones(\n                (\n                    encoded_state.shape[0],\n                    1,\n                    encoded_state.shape[2],\n                    encoded_state.shape[3],\n                )\n            )\n            .to(action.device)\n            .float()\n        )\n        action_one_hot = (\n            action[:, :, None, None] * action_one_hot / self.action_space_size\n        )\n        x = torch.cat((encoded_state, action_one_hot), dim=1)\n        next_encoded_state, reward = self.dynamics_network(x)\n\n        # Scale encoded state between [0, 1] (See paper appendix Training)\n        min_next_encoded_state = (\n            next_encoded_state.view(\n                -1,\n                next_encoded_state.shape[1],\n                next_encoded_state.shape[2] * next_encoded_state.shape[3],\n            )\n            .min(2, keepdim=True)[0]\n            .unsqueeze(-1)\n        )\n        max_next_encoded_state = (\n            next_encoded_state.view(\n                -1,\n                next_encoded_state.shape[1],\n                next_encoded_state.shape[2] * next_encoded_state.shape[3],\n            )\n            .max(2, keepdim=True)[0]\n            .unsqueeze(-1)\n        )\n        scale_next_encoded_state = max_next_encoded_state - min_next_encoded_state\n        scale_next_encoded_state[scale_next_encoded_state < 1e-5] += 1e-5\n        next_encoded_state_normalized = (\n            next_encoded_state - min_next_encoded_state\n        ) / scale_next_encoded_state\n        return next_encoded_state_normalized, reward\n\n    def initial_inference(self, observation):\n        encoded_state = self.representation(observation)\n        policy_logits, value = self.prediction(encoded_state)\n        # reward equal to 0 for consistency\n        reward = (\n            torch.zeros(1, self.full_support_size)\n            .scatter(1, torch.tensor([[self.full_support_size // 2]]).long(), 1.0)\n            .repeat(len(observation), 1)\n            .to(observation.device)\n        )\n        return (\n            value,\n            reward,\n            policy_logits,\n            encoded_state,\n        )\n\n    def recurrent_inference(self, encoded_state, action):\n        next_encoded_state, reward = self.dynamics(encoded_state, action)\n        policy_logits, value = self.prediction(next_encoded_state)\n        return value, reward, policy_logits, next_encoded_state\n\n\n########### End ResNet ###########\n##################################\n\n\nclass FullyConnectedNetwork(torch.nn.Module):\n    def __init__(self, input_size, layer_sizes, output_size, activation=None):\n        super().__init__()\n        size_list = [input_size] + layer_sizes\n        layers = []\n        if 1 < len(size_list):\n            for i in range(len(size_list) - 1):\n                layers.extend(\n                    [\n                        torch.nn.Linear(size_list[i], size_list[i + 1]),\n                        torch.nn.LeakyReLU(),\n                    ]\n                )\n        layers.append(torch.nn.Linear(size_list[-1], output_size))\n        if activation:\n            layers.append(activation)\n        self.layers = torch.nn.ModuleList(layers)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\ndef support_to_scalar(logits, support_size):\n    """"""\n    Transform a categorical representation to a scalar\n    See paper appendix Network Architecture\n    """"""\n    # Decode to a scalar\n    probabilities = torch.softmax(logits, dim=1)\n    support = (\n        torch.tensor([x for x in range(-support_size, support_size + 1)])\n        .expand(probabilities.shape)\n        .float()\n        .to(device=probabilities.device)\n    )\n    x = torch.sum(support * probabilities, dim=1, keepdim=True)\n\n    # Invert the scaling (defined in https://arxiv.org/abs/1805.11593)\n    x = torch.sign(x) * (\n        ((torch.sqrt(1 + 4 * 0.001 * (torch.abs(x) + 1 + 0.001)) - 1) / (2 * 0.001))\n        ** 2\n        - 1\n    )\n    return x\n\n\ndef scalar_to_support(x, support_size):\n    """"""\n    Transform a scalar to a categorical representation with (2 * support_size + 1) categories\n    See paper appendix Network Architecture\n    """"""\n    # Reduce the scale (defined in https://arxiv.org/abs/1805.11593)\n    x = torch.sign(x) * (torch.sqrt(torch.abs(x) + 1) - 1) + 0.001 * x\n\n    # Encode on a vector\n    x = torch.clamp(x, -support_size, support_size)\n    floor = x.floor()\n    prob = x - floor\n    logits = torch.zeros(x.shape[0], x.shape[1], 2 * support_size + 1).to(x.device)\n    logits.scatter_(\n        2, (floor + support_size).long().unsqueeze(-1), (1 - prob).unsqueeze(-1)\n    )\n    indexes = floor + support_size + 1\n    prob = prob.masked_fill_(2 * support_size < indexes, 0.0)\n    indexes = indexes.masked_fill_(2 * support_size < indexes, 0.0)\n    logits.scatter_(2, indexes.long().unsqueeze(-1), prob.unsqueeze(-1))\n    return logits\n'"
muzero.py,3,"b'import copy\nimport importlib\nimport os\nimport pickle\nimport time\n\nimport numpy\nimport ray\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport models\nimport replay_buffer\nimport self_play\nimport shared_storage\nimport trainer\n\n\nclass MuZero:\n    """"""\n    Main class to manage MuZero.\n\n    Args:\n        game_name (str): Name of the game module, it should match the name of a .py file\n        in the ""./games"" directory.\n\n    Example:\n        >>> muzero = MuZero(""cartpole"")\n        >>> muzero.train()\n        >>> muzero.test(render=True, opponent=""self"", muzero_player=None)\n    """"""\n\n    def __init__(self, game_name):\n        self.game_name = game_name\n\n        # Load the game and the config from the module with the game name\n        try:\n            game_module = importlib.import_module(""games."" + self.game_name)\n            self.config = game_module.MuZeroConfig()\n            self.Game = game_module.Game\n        except Exception as err:\n            print(\n                \'{} is not a supported game name, try ""cartpole"" or refer to the documentation for adding a new game.\'.format(\n                    self.game_name\n                )\n            )\n            raise err\n\n        # Fix random generator seed\n        numpy.random.seed(self.config.seed)\n        torch.manual_seed(self.config.seed)\n\n        # Weights and replay buffer used to initialize workers\n        self.muzero_weights = models.MuZeroNetwork(self.config).get_weights()\n        self.replay_buffer = None\n\n    def train(self):\n        ray.init()\n        os.makedirs(self.config.results_path, exist_ok=True)\n\n        # Initialize workers\n        training_worker = trainer.Trainer.options(\n            num_gpus=1 if ""cuda"" in self.config.training_device else 0\n        ).remote(copy.deepcopy(self.muzero_weights), self.config)\n        shared_storage_worker = shared_storage.SharedStorage.remote(\n            copy.deepcopy(self.muzero_weights), self.game_name, self.config,\n        )\n        replay_buffer_worker = replay_buffer.ReplayBuffer.remote(self.config)\n        # Pre-load buffer if pulling from persistent storage\n        if self.replay_buffer:\n            for game_history_id in self.replay_buffer:\n                replay_buffer_worker.save_game.remote(\n                    self.replay_buffer[game_history_id]\n                )\n            print(\n                ""\\nLoaded {} games from replay buffer."".format(len(self.replay_buffer))\n            )\n        self_play_workers = [\n            self_play.SelfPlay.remote(\n                copy.deepcopy(self.muzero_weights),\n                self.Game(self.config.seed + seed),\n                self.config,\n            )\n            for seed in range(self.config.num_actors)\n        ]\n\n        # Launch workers\n        [\n            self_play_worker.continuous_self_play.remote(\n                shared_storage_worker, replay_buffer_worker\n            )\n            for self_play_worker in self_play_workers\n        ]\n        training_worker.continuous_update_weights.remote(\n            replay_buffer_worker, shared_storage_worker\n        )\n\n        # Save performance in TensorBoard\n        self._logging_loop(shared_storage_worker, replay_buffer_worker)\n\n        self.muzero_weights = ray.get(shared_storage_worker.get_weights.remote())\n        self.replay_buffer = ray.get(replay_buffer_worker.get_buffer.remote())\n        # Persist replay buffer to disk\n        print(""\\n\\nPersisting replay buffer games to disk..."")\n        pickle.dump(\n            self.replay_buffer,\n            open(os.path.join(self.config.results_path, ""replay_buffer.pkl""), ""wb""),\n        )\n        # End running actors\n        ray.shutdown()\n\n    def _logging_loop(self, shared_storage_worker, replay_buffer_worker):\n        """"""\n        Keep track of the training performance\n        """"""\n        # Launch the test worker to get performance metrics\n        test_worker = self_play.SelfPlay.remote(\n            copy.deepcopy(self.muzero_weights),\n            self.Game(self.config.seed + self.config.num_actors),\n            self.config,\n        )\n        test_worker.continuous_self_play.remote(shared_storage_worker, None, True)\n\n        # Write everything in TensorBoard\n        writer = SummaryWriter(self.config.results_path)\n\n        print(\n            ""\\nTraining...\\nRun tensorboard --logdir ./results and go to http://localhost:6006/ to see in real time the training performance.\\n""\n        )\n\n        # Save hyperparameters to TensorBoard\n        hp_table = [\n            ""| {} | {} |"".format(key, value)\n            for key, value in self.config.__dict__.items()\n        ]\n        writer.add_text(\n            ""Hyperparameters"",\n            ""| Parameter | Value |\\n|-------|-------|\\n"" + ""\\n"".join(hp_table),\n        )\n        # Save model representation\n        writer.add_text(\n            ""Model summary"",\n            str(models.MuZeroNetwork(self.config)).replace(""\\n"", "" \\n\\n""),\n        )\n        # Loop for updating the training performance\n        counter = 0\n        infos = ray.get(shared_storage_worker.get_infos.remote())\n        try:\n            while infos[""training_step""] < self.config.training_steps:\n                infos = ray.get(shared_storage_worker.get_infos.remote())\n                writer.add_scalar(\n                    ""1.Total reward/1.Total reward"", infos[""total_reward""], counter,\n                )\n                writer.add_scalar(\n                    ""1.Total reward/2.Mean value"", infos[""mean_value""], counter,\n                )\n                writer.add_scalar(\n                    ""1.Total reward/3.Episode length"", infos[""episode_length""], counter,\n                )\n                writer.add_scalar(\n                    ""1.Total reward/4.MuZero reward"", infos[""muzero_reward""], counter,\n                )\n                writer.add_scalar(\n                    ""1.Total reward/5.Opponent reward"",\n                    infos[""opponent_reward""],\n                    counter,\n                )\n                writer.add_scalar(\n                    ""2.Workers/1.Self played games"",\n                    ray.get(replay_buffer_worker.get_self_play_count.remote()),\n                    counter,\n                )\n                writer.add_scalar(\n                    ""2.Workers/2.Training steps"", infos[""training_step""], counter\n                )\n                writer.add_scalar(\n                    ""2.Workers/3.Self played games per training step ratio"",\n                    ray.get(replay_buffer_worker.get_self_play_count.remote())\n                    / max(1, infos[""training_step""]),\n                    counter,\n                )\n                writer.add_scalar(""2.Workers/4.Learning rate"", infos[""lr""], counter)\n                writer.add_scalar(\n                    ""3.Loss/1.Total weighted loss"", infos[""total_loss""], counter\n                )\n                writer.add_scalar(""3.Loss/Value loss"", infos[""value_loss""], counter)\n                writer.add_scalar(""3.Loss/Reward loss"", infos[""reward_loss""], counter)\n                writer.add_scalar(""3.Loss/Policy loss"", infos[""policy_loss""], counter)\n                print(\n                    ""Last test reward: {0:.2f}. Training step: {1}/{2}. Played games: {3}. Loss: {4:.2f}"".format(\n                        infos[""total_reward""],\n                        infos[""training_step""],\n                        self.config.training_steps,\n                        ray.get(replay_buffer_worker.get_self_play_count.remote()),\n                        infos[""total_loss""],\n                    ),\n                    end=""\\r"",\n                )\n                counter += 1\n                time.sleep(0.5)\n        except KeyboardInterrupt as err:\n            # Comment the line below to be able to stop the training but keep running\n            # raise err\n            pass\n\n    def test(self, render, opponent, muzero_player):\n        """"""\n        Test the model in a dedicated thread.\n\n        Args:\n            render: Boolean to display or not the environment.\n\n            opponent: ""self"" for self-play, ""human"" for playing against MuZero and ""random""\n            for a random agent.\n\n            muzero_player: Integer with the player number of MuZero in case of multiplayer\n            games, None let MuZero play all players turn by turn.\n        """"""\n        print(""\\nTesting..."")\n        ray.init()\n        self_play_workers = self_play.SelfPlay.remote(\n            copy.deepcopy(self.muzero_weights),\n            self.Game(numpy.random.randint(1000)),\n            self.config,\n        )\n        history = ray.get(\n            self_play_workers.play_game.remote(0, 0, render, opponent, muzero_player)\n        )\n        ray.shutdown()\n        return sum(history.reward_history)\n\n    def load_model(self, weights_path=None, replay_buffer_path=None):\n        # Load weights\n        if weights_path:\n            if os.path.exists(weights_path):\n                self.muzero_weights = torch.load(weights_path)\n                print(""\\nUsing weights from {}"".format(weights_path))\n            else:\n                print(""\\nThere is no model saved in {}."".format(weights_path))\n\n        # Load replay buffer\n        if replay_buffer_path:\n            if os.path.exists(replay_buffer_path):\n                self.replay_buffer = pickle.load(open(replay_buffer_path, ""rb""))\n                print(""\\nInitializing replay buffer with {}"".format(replay_buffer_path))\n            else:\n                print(\n                    ""Warning: Replay buffer path \'{}\' doesn\'t exist.  Using empty buffer."".format(\n                        replay_buffer_path\n                    )\n                )\n\n\nif __name__ == ""__main__"":\n    print(""\\nWelcome to MuZero! Here\'s a list of games:"")\n    # Let user pick a game\n    games = [\n        filename[:-3]\n        for filename in sorted(os.listdir(""./games""))\n        if filename.endswith("".py"") and filename != ""abstract_game.py""\n    ]\n    for i in range(len(games)):\n        print(""{}. {}"".format(i, games[i]))\n    choice = input(""Enter a number to choose the game: "")\n    valid_inputs = [str(i) for i in range(len(games))]\n    while choice not in valid_inputs:\n        choice = input(""Invalid input, enter a number listed above: "")\n\n    # Initialize MuZero\n    choice = int(choice)\n    muzero = MuZero(games[choice])\n\n    while True:\n        # Configure running options\n        options = [\n            ""Train"",\n            ""Load pretrained model"",\n            ""Render some self play games"",\n            ""Play against MuZero"",\n            ""Test the game manually"",\n            ""Exit"",\n        ]\n        print()\n        for i in range(len(options)):\n            print(""{}. {}"".format(i, options[i]))\n\n        choice = input(""Enter a number to choose an action: "")\n        valid_inputs = [str(i) for i in range(len(options))]\n        while choice not in valid_inputs:\n            choice = input(""Invalid input, enter a number listed above: "")\n        choice = int(choice)\n        if choice == 0:\n            muzero.train()\n        elif choice == 1:\n            weights_path = input(\n                ""Enter a path to the model.weights, or ENTER if none: ""\n            )\n            while weights_path and not os.path.isfile(weights_path):\n                weights_path = input(""Invalid weights path. Try again: "")\n            replay_buffer_path = input(\n                ""Enter path for existing replay buffer, or ENTER if none: ""\n            )\n            while replay_buffer_path and not os.path.isfile(replay_buffer_path):\n                replay_buffer_path = input(""Invalid replay buffer path. Try again: "")\n            muzero.load_model(\n                weights_path=weights_path, replay_buffer_path=replay_buffer_path\n            )\n        elif choice == 2:\n            muzero.test(render=True, opponent=""self"", muzero_player=None)\n        elif choice == 3:\n            muzero.test(render=True, opponent=""human"", muzero_player=0)\n        elif choice == 4:\n            env = muzero.Game()\n            env.reset()\n            env.render()\n\n            done = False\n            while not done:\n                action = env.human_to_action()\n                observation, reward, done = env.step(action)\n                print(\n                    ""\\nAction: {}\\nReward: {}"".format(\n                        env.action_to_string(action), reward\n                    )\n                )\n                env.render()\n        else:\n            break\n        print(""\\nDone"")\n\n    ## Successive training, create a new config file for each experiment\n    # experiments = [""cartpole"", ""tictactoe""]\n    # for experiment in experiments:\n    #     print(""\\nStarting experiment {}"".format(experiment))\n    #     try:\n    #         muzero = MuZero(experiment)\n    #         muzero.train()\n    #     except:\n    #         print(""Skipping {}, an error has occurred."".format(experiment))\n'"
replay_buffer.py,3,"b'import collections\nimport copy\n\nimport numpy\nimport ray\nimport torch\n\nimport models\n\n\n@ray.remote\nclass ReplayBuffer:\n    """"""\n    Class which run in a dedicated thread to store played games and generate batch.\n    """"""\n\n    def __init__(self, config):\n        self.config = config\n        self.buffer = {}\n        self.game_priorities = collections.deque(maxlen=self.config.window_size)\n        self.max_recorded_game_priority = 1.0\n        self.self_play_count = 0\n        self.total_samples = 0\n\n        # Used only for the Reanalyze options\n        self.model = None\n        if self.config.use_last_model_value:\n            self.model = models.MuZeroNetwork(self.config)\n            self.model.to(torch.device(""cpu""))\n            self.model.eval()\n\n        # Fix random generator seed\n        numpy.random.seed(self.config.seed)\n        torch.manual_seed(self.config.seed)\n\n    def save_game(self, game_history):\n        if game_history.priorities is not None:\n            # Avoid read only array when loading replay buffer from pickle\n            game_history.priorities = game_history.priorities.copy()\n        else:\n            if self.config.use_max_priority:\n                game_history.priorities = numpy.full(\n                    len(game_history.root_values), self.max_recorded_game_priority\n                )\n            else:\n                # Initial priorities for the prioritized replay (See paper appendix Training)\n                priorities = []\n                for i, root_value in enumerate(game_history.root_values):\n                    priority = (\n                        numpy.abs(\n                            root_value - self.compute_target_value(game_history, i)\n                        )\n                        ** self.config.PER_alpha\n                    )\n                    priorities.append(priority)\n\n                game_history.priorities = numpy.array(priorities, dtype=numpy.float32)\n\n        self.buffer[self.self_play_count] = game_history\n        self.total_samples += len(game_history.priorities)\n        self.game_priorities.append(numpy.max(game_history.priorities))\n\n        self.self_play_count += 1\n\n        if self.config.window_size < len(self.buffer):\n            del_id = self.self_play_count - len(self.buffer)\n            self.total_samples -= len(self.buffer[del_id].priorities)\n            del self.buffer[del_id]\n\n    def get_self_play_count(self):\n        return self.self_play_count\n\n    def get_buffer(self):\n        return self.buffer\n\n    def get_batch(self, model_weights):\n        (\n            index_batch,\n            observation_batch,\n            action_batch,\n            reward_batch,\n            value_batch,\n            policy_batch,\n            weight_batch,\n            gradient_scale_batch,\n        ) = ([], [], [], [], [], [], [], [])\n\n        if self.config.use_last_model_value:\n            self.model.set_weights(model_weights)\n\n        for _ in range(self.config.batch_size):\n            game_id, game_history, game_prob = self.sample_game(self.buffer)\n            game_pos, pos_prob = self.sample_position(game_history)\n\n            values, rewards, policies, actions = self.make_target(\n                game_history, game_pos\n            )\n\n            index_batch.append([game_id, game_pos])\n            observation_batch.append(\n                game_history.get_stacked_observations(\n                    game_pos, self.config.stacked_observations\n                )\n            )\n            action_batch.append(actions)\n            value_batch.append(values)\n            reward_batch.append(rewards)\n            policy_batch.append(policies)\n            weight_batch.append(\n                (self.total_samples * game_prob * pos_prob) ** (-self.config.PER_beta)\n            )\n            gradient_scale_batch.append(\n                [\n                    min(\n                        self.config.num_unroll_steps,\n                        len(game_history.action_history) - game_pos,\n                    )\n                ]\n                * len(actions)\n            )\n\n        weight_batch = numpy.array(weight_batch, dtype=numpy.float32) / max(\n            weight_batch\n        )\n\n        # observation_batch: batch, channels, height, width\n        # action_batch: batch, num_unroll_steps+1\n        # value_batch: batch, num_unroll_steps+1\n        # reward_batch: batch, num_unroll_steps+1\n        # policy_batch: batch, num_unroll_steps+1, len(action_space)\n        # weight_batch: batch\n        # gradient_scale_batch: batch, num_unroll_steps+1\n        return (\n            index_batch,\n            (\n                observation_batch,\n                action_batch,\n                value_batch,\n                reward_batch,\n                policy_batch,\n                weight_batch,\n                gradient_scale_batch,\n            ),\n        )\n\n    def sample_game(self, buffer):\n        """"""\n        Sample game from buffer either uniformly or according to some priority.\n        See paper appendix Training.\n        """"""\n        game_probs = numpy.array(self.game_priorities, dtype=numpy.float32)\n        game_probs /= numpy.sum(game_probs)\n        game_index = numpy.random.choice(len(self.buffer), p=game_probs)\n        game_prob = game_probs[game_index]\n        game_id = self.self_play_count - len(self.buffer) + game_index\n\n        return game_id, self.buffer[game_id], game_prob\n\n    def sample_position(self, game_history):\n        """"""\n        Sample position from game either uniformly or according to some priority.\n        See paper appendix Training.\n        """"""\n        position_probs = game_history.priorities / sum(game_history.priorities)\n        position_index = numpy.random.choice(len(position_probs), p=position_probs)\n        position_prob = position_probs[position_index]\n\n        return position_index, position_prob\n\n    def update_priorities(self, priorities, index_info):\n        """"""\n        Update game and position priorities with priorities calculated during the training.\n        See Distributed Prioritized Experience Replay https://arxiv.org/abs/1803.00933\n        """"""\n        min_priorities = numpy.min(priorities)\n        if not min_priorities or numpy.isnan(min_priorities) or min_priorities < 1e-5:\n            print(\n                ""Warning : Extreme values ({}) in game priorities. Could be underfitting or overfitting."".format(\n                    min_priorities\n                )\n            )\n        else:\n            for i in range(len(index_info)):\n                game_id, game_pos = index_info[i]\n\n                # The element could be removed since its selection and training\n                if game_id in self.buffer:\n                    # Update position priorities\n                    priority = priorities[i, :]\n                    start_index = game_pos\n                    end_index = min(\n                        game_pos + len(priority), len(self.buffer[game_id].priorities)\n                    )\n                    self.buffer[game_id].priorities[start_index:end_index] = priority[\n                        : end_index - start_index\n                    ]\n\n                    # Update game priorities\n                    game_index = game_id - (self.self_play_count - len(self.buffer))\n                    self.game_priorities[game_index] = numpy.max(\n                        self.buffer[game_id].priorities\n                    )  # option: mean, sum, max\n\n                    self.max_recorded_game_priority = numpy.max(self.game_priorities)\n\n    def compute_target_value(self, game_history, index):\n        # The value target is the discounted root value of the search tree td_steps into the\n        # future, plus the discounted sum of all rewards until then.\n        bootstrap_index = index + self.config.td_steps\n        if bootstrap_index < len(game_history.root_values):\n            if self.config.use_last_model_value:\n                # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n                observation = (\n                    torch.tensor(\n                        game_history.get_stacked_observations(\n                            bootstrap_index, self.config.stacked_observations\n                        )\n                    )\n                    .float()\n                    .unsqueeze(0)\n                )\n                last_step_value = models.support_to_scalar(\n                    self.model.initial_inference(observation)[0],\n                    self.config.support_size,\n                ).item()\n            else:\n                last_step_value = (\n                    game_history.root_values[bootstrap_index]\n                    if game_history.to_play_history[bootstrap_index]\n                    == game_history.to_play_history[index]\n                    else -game_history.root_values[bootstrap_index]\n                )\n\n            value = last_step_value * self.config.discount ** self.config.td_steps\n        else:\n            value = 0\n\n        for i, reward in enumerate(\n            game_history.reward_history[index + 1 : bootstrap_index + 1]\n        ):\n            value += (\n                reward\n                if game_history.to_play_history[index]\n                == game_history.to_play_history[index + 1 + i]\n                else -reward\n            ) * self.config.discount ** i\n\n        return value\n\n    def make_target(self, game_history, state_index):\n        """"""\n        Generate targets for every unroll steps.\n        """"""\n        target_values, target_rewards, target_policies, actions = [], [], [], []\n        for current_index in range(\n            state_index, state_index + self.config.num_unroll_steps + 1\n        ):\n            value = self.compute_target_value(game_history, current_index)\n\n            if current_index < len(game_history.root_values):\n                target_values.append(value)\n                target_rewards.append(game_history.reward_history[current_index])\n                target_policies.append(game_history.child_visits[current_index])\n                actions.append(game_history.action_history[current_index])\n            elif current_index == len(game_history.root_values):\n                target_values.append(0)\n                target_rewards.append(game_history.reward_history[current_index])\n                # Uniform policy\n                target_policies.append(\n                    [\n                        1 / len(game_history.child_visits[0])\n                        for _ in range(len(game_history.child_visits[0]))\n                    ]\n                )\n                actions.append(game_history.action_history[current_index])\n            else:\n                # States past the end of games are treated as absorbing states\n                target_values.append(0)\n                target_rewards.append(0)\n                # Uniform policy\n                target_policies.append(\n                    [\n                        1 / len(game_history.child_visits[0])\n                        for _ in range(len(game_history.child_visits[0]))\n                    ]\n                )\n                actions.append(numpy.random.choice(game_history.action_history))\n\n        return target_values, target_rewards, target_policies, actions\n'"
self_play.py,6,"b'import copy\nimport math\nimport time\n\nimport numpy\nimport ray\nimport torch\n\nimport models\n\n\n@ray.remote\nclass SelfPlay:\n    """"""\n    Class which run in a dedicated thread to play games and save them to the replay-buffer.\n    """"""\n\n    def __init__(self, initial_weights, game, config):\n        self.config = config\n        self.game = game\n\n        # Fix random generator seed\n        numpy.random.seed(self.config.seed)\n        torch.manual_seed(self.config.seed)\n\n        # Initialize the network\n        self.model = models.MuZeroNetwork(self.config)\n        self.model.set_weights(initial_weights)\n        self.model.to(torch.device(""cpu""))\n        self.model.eval()\n\n    def continuous_self_play(self, shared_storage, replay_buffer, test_mode=False):\n        while True:\n            self.model.set_weights(\n                copy.deepcopy(ray.get(shared_storage.get_weights.remote()))\n            )\n\n            if not test_mode:\n                game_history = self.play_game(\n                    self.config.visit_softmax_temperature_fn(\n                        trained_steps=ray.get(shared_storage.get_infos.remote())[\n                            ""training_step""\n                        ]\n                    ),\n                    self.config.temperature_threshold,\n                    False,\n                    ""self"",\n                    0,\n                )\n\n                replay_buffer.save_game.remote(game_history)\n\n            else:\n                # Take the best action (no exploration) in test mode\n                game_history = self.play_game(\n                    0,\n                    self.config.temperature_threshold,\n                    False,\n                    ""self"" if len(self.config.players) == 1 else self.config.opponent,\n                    self.config.muzero_player,\n                )\n\n                # Save to the shared storage\n                shared_storage.set_infos.remote(\n                    ""total_reward"", sum(game_history.reward_history)\n                )\n                shared_storage.set_infos.remote(\n                    ""episode_length"", len(game_history.action_history) - 1\n                )\n                shared_storage.set_infos.remote(\n                    ""mean_value"",\n                    numpy.mean([value for value in game_history.root_values if value]),\n                )\n                if 1 < len(self.config.players):\n                    shared_storage.set_infos.remote(\n                        ""muzero_reward"",\n                        sum(\n                            [\n                                reward\n                                for i, reward in enumerate(game_history.reward_history)\n                                if game_history.to_play_history[i - 1]\n                                == self.config.muzero_player\n                            ]\n                        ),\n                    )\n                    shared_storage.set_infos.remote(\n                        ""opponent_reward"",\n                        sum(\n                            [\n                                reward\n                                for i, reward in enumerate(game_history.reward_history)\n                                if game_history.to_play_history[i - 1]\n                                != self.config.muzero_player\n                            ]\n                        ),\n                    )\n\n            # Managing the self-play / training ratio\n            if not test_mode and self.config.self_play_delay:\n                time.sleep(self.config.self_play_delay)\n            if not test_mode and self.config.ratio:\n                while (\n                    ray.get(replay_buffer.get_self_play_count.remote())\n                    / max(\n                        1, ray.get(shared_storage.get_infos.remote())[""training_step""]\n                    )\n                    > self.config.ratio\n                ):\n                    time.sleep(0.5)\n\n    def play_game(\n        self, temperature, temperature_threshold, render, opponent, muzero_player\n    ):\n        """"""\n        Play one game with actions based on the Monte Carlo tree search at each moves.\n        """"""\n        game_history = GameHistory()\n        observation = self.game.reset()\n        game_history.action_history.append(0)\n        game_history.observation_history.append(observation)\n        game_history.reward_history.append(0)\n        game_history.to_play_history.append(self.game.to_play())\n\n        done = False\n\n        if render:\n            self.game.render()\n\n        with torch.no_grad():\n            while (\n                not done and len(game_history.action_history) <= self.config.max_moves\n            ):\n                stacked_observations = game_history.get_stacked_observations(\n                    -1, self.config.stacked_observations,\n                )\n\n                # Choose the action\n                if opponent == ""self"" or muzero_player == self.game.to_play():\n                    root, tree_depth = MCTS(self.config).run(\n                        self.model,\n                        stacked_observations,\n                        self.game.legal_actions(),\n                        self.game.to_play(),\n                        False if temperature == 0 else True,\n                    )\n                    action = self.select_action(\n                        root,\n                        temperature\n                        if not temperature_threshold\n                        or len(game_history.action_history) < temperature_threshold\n                        else 0,\n                    )\n\n                    if render:\n                        print(""Tree depth: {}"".format(tree_depth))\n                        print(\n                            ""Root value for player {0}: {1:.2f}"".format(\n                                self.game.to_play(), root.value()\n                            )\n                        )\n                else:\n                    action, root, tree_depth = self.select_opponent_action(\n                        opponent, stacked_observations\n                    )\n\n                observation, reward, done = self.game.step(action)\n\n                if render:\n                    print(\n                        ""Played action: {}"".format(self.game.action_to_string(action))\n                    )\n                    self.game.render()\n\n                game_history.store_search_statistics(root, self.config.action_space)\n\n                # Next batch\n                game_history.action_history.append(action)\n                game_history.observation_history.append(observation)\n                game_history.reward_history.append(reward)\n                game_history.to_play_history.append(self.game.to_play())\n\n        self.game.close()\n        return game_history\n\n    def select_opponent_action(self, opponent, stacked_observations):\n        """"""\n        Select opponent action for evaluating MuZero level.\n        """"""\n        if opponent == ""human"":\n            root, tree_depth = MCTS(self.config).run(\n                self.model,\n                stacked_observations,\n                self.game.legal_actions(),\n                self.game.to_play(),\n                0,\n            )\n            print(""Tree depth: {}"".format(tree_depth))\n            print(\n                ""Root value for player {0}: {1:.2f}"".format(\n                    self.game.to_play(), root.value()\n                )\n            )\n            print(\n                ""Player {} turn. MuZero suggests {}"".format(\n                    self.game.to_play(),\n                    self.game.action_to_string(self.select_action(root, 0)),\n                )\n            )\n            return self.game.human_to_action(), root, tree_depth\n        elif opponent == ""expert"":\n            return self.game.expert_agent(), None, None\n        elif opponent == ""random"":\n            return numpy.random.choice(self.game.legal_actions()), None, None\n        else:\n            raise ValueError(\n                \'Wrong argument: ""opponent"" argument should be ""self"", ""human"", ""expert"" or ""random""\'\n            )\n\n    @staticmethod\n    def select_action(node, temperature):\n        """"""\n        Select action according to the visit count distribution and the temperature.\n        The temperature is changed dynamically with the visit_softmax_temperature function \n        in the config.\n        """"""\n        visit_counts = numpy.array(\n            [child.visit_count for child in node.children.values()]\n        )\n        actions = [action for action in node.children.keys()]\n        if temperature == 0:\n            action = actions[numpy.argmax(visit_counts)]\n        elif temperature == float(""inf""):\n            action = numpy.random.choice(actions)\n        else:\n            # See paper appendix Data Generation\n            visit_count_distribution = visit_counts ** (1 / temperature)\n            visit_count_distribution = visit_count_distribution / sum(\n                visit_count_distribution\n            )\n            action = numpy.random.choice(actions, p=visit_count_distribution)\n\n        return action\n\n\n# Game independent\nclass MCTS:\n    """"""\n    Core Monte Carlo Tree Search algorithm.\n    To decide on an action, we run N simulations, always starting at the root of\n    the search tree and traversing the tree according to the UCB formula until we\n    reach a leaf node.\n    """"""\n\n    def __init__(self, config):\n        self.config = config\n\n    def run(self, model, observation, legal_actions, to_play, add_exploration_noise):\n        """"""\n        At the root of the search tree we use the representation function to obtain a\n        hidden state given the current observation.\n        We then run a Monte Carlo Tree Search using only action sequences and the model\n        learned by the network.\n        """"""\n        root = Node(0)\n        observation = (\n            torch.tensor(observation)\n            .float()\n            .unsqueeze(0)\n            .to(next(model.parameters()).device)\n        )\n        (\n            root_predicted_value,\n            reward,\n            policy_logits,\n            hidden_state,\n        ) = model.initial_inference(observation)\n        root_predicted_value = models.support_to_scalar(\n            root_predicted_value, self.config.support_size\n        ).item()\n        reward = models.support_to_scalar(reward, self.config.support_size).item()\n        root.expand(\n            legal_actions, to_play, reward, policy_logits, hidden_state,\n        )\n        if add_exploration_noise:\n            root.add_exploration_noise(\n                dirichlet_alpha=self.config.root_dirichlet_alpha,\n                exploration_fraction=self.config.root_exploration_fraction,\n            )\n\n        min_max_stats = MinMaxStats()\n\n        max_tree_depth = 0\n        for _ in range(self.config.num_simulations):\n            virtual_to_play = to_play\n            node = root\n            search_path = [node]\n            current_tree_depth = 0\n\n            while node.expanded():\n                current_tree_depth += 1\n                action, node = self.select_child(node, min_max_stats)\n                search_path.append(node)\n\n                # Players play turn by turn\n                if virtual_to_play + 1 < len(self.config.players):\n                    virtual_to_play = self.config.players[virtual_to_play + 1]\n                else:\n                    virtual_to_play = self.config.players[0]\n\n            # Inside the search tree we use the dynamics function to obtain the next hidden\n            # state given an action and the previous hidden state\n            parent = search_path[-2]\n            value, reward, policy_logits, hidden_state = model.recurrent_inference(\n                parent.hidden_state,\n                torch.tensor([[action]]).to(parent.hidden_state.device),\n            )\n            value = models.support_to_scalar(value, self.config.support_size).item()\n            reward = models.support_to_scalar(reward, self.config.support_size).item()\n            node.expand(\n                self.config.action_space,\n                virtual_to_play,\n                reward,\n                policy_logits,\n                hidden_state,\n            )\n\n            self.backpropagate(search_path, value, virtual_to_play, min_max_stats)\n\n            max_tree_depth = max(max_tree_depth, current_tree_depth)\n\n        return root, max_tree_depth\n\n    def select_child(self, node, min_max_stats):\n        """"""\n        Select the child with the highest UCB score.\n        """"""\n        max_ucb = max(\n            self.ucb_score(node, child, min_max_stats)\n            for action, child in node.children.items()\n        )\n        action = numpy.random.choice(\n            [\n                action\n                for action, child in node.children.items()\n                if self.ucb_score(node, child, min_max_stats) == max_ucb\n            ]\n        )\n        return action, node.children[action]\n\n    def ucb_score(self, parent, child, min_max_stats):\n        """"""\n        The score for a node is based on its value, plus an exploration bonus based on the prior.\n        """"""\n        pb_c = (\n            math.log(\n                (parent.visit_count + self.config.pb_c_base + 1) / self.config.pb_c_base\n            )\n            + self.config.pb_c_init\n        )\n        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n\n        prior_score = pb_c * child.prior\n\n        if child.visit_count > 0:\n            # Mean value Q\n            value_score = min_max_stats.normalize(\n                child.reward\n                + self.config.discount\n                * (child.value() if len(self.config.players) == 1 else -child.value())\n            )\n        else:\n            value_score = 0\n\n        return prior_score + value_score\n\n    def backpropagate(self, search_path, value, to_play, min_max_stats):\n        """"""\n        At the end of a simulation, we propagate the evaluation all the way up the tree\n        to the root.\n        """"""\n        for node in reversed(search_path):\n            node.value_sum += value if node.to_play == to_play else -value\n            node.visit_count += 1\n            min_max_stats.update(\n                node.reward\n                + self.config.discount\n                * (node.value() if len(self.config.players) == 1 else -node.value())\n            )\n\n            value = (\n                -node.reward if node.to_play == to_play else node.reward\n            ) + self.config.discount * value\n\n\nclass Node:\n    def __init__(self, prior):\n        self.visit_count = 0\n        self.to_play = -1\n        self.prior = prior\n        self.value_sum = 0\n        self.children = {}\n        self.hidden_state = None\n        self.reward = 0\n\n    def expanded(self):\n        return len(self.children) > 0\n\n    def value(self):\n        if self.visit_count == 0:\n            return 0\n        return self.value_sum / self.visit_count\n\n    def expand(self, actions, to_play, reward, policy_logits, hidden_state):\n        """"""\n        We expand a node using the value, reward and policy prediction obtained from the\n        neural network.\n        """"""\n        self.to_play = to_play\n        self.reward = reward\n        self.hidden_state = hidden_state\n\n        policy = {}\n        for a in actions:\n            try:\n                policy[a] = 1 / sum(torch.exp(policy_logits[0] - policy_logits[0][a]))\n            except OverflowError:\n                print(""Warning: prior has been approximated"")\n                policy[a] = 0.0\n        for action, p in policy.items():\n            self.children[action] = Node(p)\n\n    def add_exploration_noise(self, dirichlet_alpha, exploration_fraction):\n        """"""\n        At the start of each search, we add dirichlet noise to the prior of the root to\n        encourage the search to explore new actions.\n        """"""\n        actions = list(self.children.keys())\n        noise = numpy.random.dirichlet([dirichlet_alpha] * len(actions))\n        frac = exploration_fraction\n        for a, n in zip(actions, noise):\n            self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac\n\n\nclass GameHistory:\n    """"""\n    Store only usefull information of a self-play game.\n    """"""\n\n    def __init__(self):\n        self.observation_history = []\n        self.action_history = []\n        self.reward_history = []\n        self.to_play_history = []\n        self.child_visits = []\n        self.root_values = []\n        self.priorities = None\n\n    def store_search_statistics(self, root, action_space):\n        # Turn visit count from root into a policy\n        if root is not None:\n            sum_visits = sum(child.visit_count for child in root.children.values())\n            self.child_visits.append(\n                [\n                    root.children[a].visit_count / sum_visits\n                    if a in root.children\n                    else 0\n                    for a in action_space\n                ]\n            )\n\n            self.root_values.append(root.value())\n        else:\n            self.root_values.append(None)\n\n    def get_stacked_observations(self, index, num_stacked_observations):\n        """"""\n        Generate a new observation with the observation at the index position\n        and num_stacked_observations past observations and actions stacked.\n        """"""\n        # Convert to positive index\n        index = index % len(self.observation_history)\n\n        stacked_observations = self.observation_history[index].copy()\n        for past_observation_index in reversed(\n            range(index - num_stacked_observations, index)\n        ):\n            if 0 <= past_observation_index:\n                previous_observation = numpy.concatenate(\n                    (\n                        self.observation_history[past_observation_index],\n                        [\n                            numpy.ones_like(stacked_observations[0])\n                            * self.action_history[past_observation_index + 1]\n                        ],\n                    )\n                )\n            else:\n                previous_observation = numpy.concatenate(\n                    (\n                        numpy.zeros_like(self.observation_history[index]),\n                        [numpy.zeros_like(stacked_observations[0])],\n                    )\n                )\n\n            stacked_observations = numpy.concatenate(\n                (stacked_observations, previous_observation)\n            )\n\n        return stacked_observations\n\n\nclass MinMaxStats:\n    """"""\n    A class that holds the min-max values of the tree.\n    """"""\n\n    def __init__(self):\n        self.maximum = -float(""inf"")\n        self.minimum = float(""inf"")\n\n    def update(self, value):\n        self.maximum = max(self.maximum, value)\n        self.minimum = min(self.minimum, value)\n\n    def normalize(self, value):\n        if self.maximum > self.minimum:\n            # We normalize only when we have set the maximum and minimum values\n            return (value - self.minimum) / (self.maximum - self.minimum)\n        return value\n'"
shared_storage.py,1,"b'import ray\nimport torch\nimport os\n\n\n@ray.remote\nclass SharedStorage:\n    """"""\n    Class which run in a dedicated thread to store the network weights and some information.\n    """"""\n\n    def __init__(self, weights, game_name, config):\n        self.config = config\n        self.game_name = game_name\n        self.weights = weights\n        self.infos = {\n            ""total_reward"": 0,\n            ""muzero_reward"": 0,\n            ""opponent_reward"": 0,\n            ""episode_length"": 0,\n            ""mean_value"": 0,\n            ""training_step"": 0,\n            ""lr"": 0,\n            ""total_loss"": 0,\n            ""value_loss"": 0,\n            ""reward_loss"": 0,\n            ""policy_loss"": 0,\n        }\n\n    def get_weights(self):\n        return self.weights\n\n    def set_weights(self, weights, path=None):\n        self.weights = weights\n        if not path:\n            path = os.path.join(self.config.results_path, ""model.weights"")\n\n        torch.save(self.weights, path)\n\n    def get_infos(self):\n        return self.infos\n\n    def set_infos(self, key, value):\n        self.infos[key] = value\n'"
trainer.py,14,"b'import time\n\nimport numpy\nimport ray\nimport torch\n\nimport models\n\n\n@ray.remote\nclass Trainer:\n    """"""\n    Class which run in a dedicated thread to train a neural network and save it\n    in the shared storage.\n    """"""\n\n    def __init__(self, initial_weights, config):\n        self.config = config\n        self.training_step = 0\n\n        # Fix random generator seed\n        numpy.random.seed(self.config.seed)\n        torch.manual_seed(self.config.seed)\n\n        # Initialize the network\n        self.model = models.MuZeroNetwork(self.config)\n        self.model.set_weights(initial_weights)\n        self.model.to(torch.device(config.training_device))\n        self.model.train()\n\n        if self.config.optimizer == ""SGD"":\n            self.optimizer = torch.optim.SGD(\n                self.model.parameters(),\n                lr=self.config.lr_init,\n                momentum=self.config.momentum,\n                weight_decay=self.config.weight_decay,\n            )\n        elif self.config.optimizer == ""Adam"":\n            self.optimizer = torch.optim.Adam(\n                self.model.parameters(),\n                lr=self.config.lr_init,\n                weight_decay=self.config.weight_decay,\n            )\n        else:\n            raise ValueError(\n                ""{} is not implemented. You can change the optimizer manually in trainer.py.""\n            )\n\n    def continuous_update_weights(self, replay_buffer, shared_storage_worker):\n        # Wait for the replay buffer to be filled\n        while ray.get(replay_buffer.get_self_play_count.remote()) < 1:\n            time.sleep(0.1)\n\n        # Training loop\n        while True:\n            index_batch, batch = ray.get(\n                replay_buffer.get_batch.remote(self.model.get_weights())\n            )\n            self.update_lr()\n            (\n                priorities,\n                total_loss,\n                value_loss,\n                reward_loss,\n                policy_loss,\n            ) = self.update_weights(batch)\n\n            if self.config.PER:\n                # Save new priorities in the replay buffer (See https://arxiv.org/abs/1803.00933)\n                replay_buffer.update_priorities.remote(priorities, index_batch)\n\n            # Save to the shared storage\n            if self.training_step % self.config.checkpoint_interval == 0:\n                shared_storage_worker.set_weights.remote(self.model.get_weights())\n            shared_storage_worker.set_infos.remote(""training_step"", self.training_step)\n            shared_storage_worker.set_infos.remote(\n                ""lr"", self.optimizer.param_groups[0][""lr""]\n            )\n            shared_storage_worker.set_infos.remote(""total_loss"", total_loss)\n            shared_storage_worker.set_infos.remote(""value_loss"", value_loss)\n            shared_storage_worker.set_infos.remote(""reward_loss"", reward_loss)\n            shared_storage_worker.set_infos.remote(""policy_loss"", policy_loss)\n\n            # Managing the self-play / training ratio\n            if self.config.training_delay:\n                time.sleep(self.config.training_delay)\n            if self.config.ratio:\n                while (\n                    ray.get(replay_buffer.get_self_play_count.remote())\n                    / max(1, self.training_step)\n                    < self.config.ratio\n                ):\n                    time.sleep(0.5)\n\n    def update_weights(self, batch):\n        """"""\n        Perform one training step.\n        """"""\n\n        (\n            observation_batch,\n            action_batch,\n            target_value,\n            target_reward,\n            target_policy,\n            weight_batch,\n            gradient_scale_batch,\n        ) = batch\n\n        # Keep values as scalars for calculating the priorities for the prioritized replay\n        target_value_scalar = numpy.array(target_value, dtype=numpy.float32)\n        priorities = numpy.zeros_like(target_value_scalar)\n\n        device = next(self.model.parameters()).device\n        weight_batch = torch.tensor(weight_batch.copy()).float().to(device)\n        observation_batch = torch.tensor(observation_batch).float().to(device)\n        action_batch = torch.tensor(action_batch).float().to(device).unsqueeze(-1)\n        target_value = torch.tensor(target_value).float().to(device)\n        target_reward = torch.tensor(target_reward).float().to(device)\n        target_policy = torch.tensor(target_policy).float().to(device)\n        gradient_scale_batch = torch.tensor(gradient_scale_batch).float().to(device)\n        # observation_batch: batch, channels, height, width\n        # action_batch: batch, num_unroll_steps+1, 1 (unsqueeze)\n        # target_value: batch, num_unroll_steps+1\n        # target_reward: batch, num_unroll_steps+1\n        # target_policy: batch, num_unroll_steps+1, len(action_space)\n        # gradient_scale_batch: batch, num_unroll_steps+1\n\n        target_value = models.scalar_to_support(target_value, self.config.support_size)\n        target_reward = models.scalar_to_support(\n            target_reward, self.config.support_size\n        )\n        # target_value: batch, num_unroll_steps+1, 2*support_size+1\n        # target_reward: batch, num_unroll_steps+1, 2*support_size+1\n\n        ## Generate predictions\n        value, reward, policy_logits, hidden_state = self.model.initial_inference(\n            observation_batch\n        )\n        predictions = [(value, reward, policy_logits)]\n        for i in range(1, action_batch.shape[1]):\n            value, reward, policy_logits, hidden_state = self.model.recurrent_inference(\n                hidden_state, action_batch[:, i]\n            )\n            # Scale the gradient at the start of the dynamics function (See paper appendix Training)\n            hidden_state.register_hook(lambda grad: grad * 0.5)\n            predictions.append((value, reward, policy_logits))\n        # predictions: num_unroll_steps+1, 3, batch, 2*support_size+1 | 2*support_size+1 | 9 (according to the 2nd dim)\n\n        ## Compute losses\n        value_loss, reward_loss, policy_loss = (0, 0, 0)\n        value, reward, policy_logits = predictions[0]\n        # Ignore reward loss for the first batch step\n        current_value_loss, _, current_policy_loss = self.loss_function(\n            value.squeeze(-1),\n            reward.squeeze(-1),\n            policy_logits,\n            target_value[:, 0],\n            target_reward[:, 0],\n            target_policy[:, 0],\n        )\n        value_loss += current_value_loss\n        policy_loss += current_policy_loss\n        # Compute priorities for the prioritized replay (See paper appendix Training)\n        pred_value_scalar = (\n            models.support_to_scalar(value, self.config.support_size)\n            .detach()\n            .cpu()\n            .numpy()\n            .squeeze()\n        )\n        priorities[:, 0] = (\n            numpy.abs(pred_value_scalar - target_value_scalar[:, 0])\n            ** self.config.PER_alpha\n        )\n\n        for i in range(1, len(predictions)):\n            value, reward, policy_logits = predictions[i]\n            (\n                current_value_loss,\n                current_reward_loss,\n                current_policy_loss,\n            ) = self.loss_function(\n                value.squeeze(-1),\n                reward.squeeze(-1),\n                policy_logits,\n                target_value[:, i],\n                target_reward[:, i],\n                target_policy[:, i],\n            )\n\n            # Scale gradient by the number of unroll steps (See paper appendix Training)\n            current_value_loss.register_hook(\n                lambda grad: grad / gradient_scale_batch[:, i]\n            )\n            current_reward_loss.register_hook(\n                lambda grad: grad / gradient_scale_batch[:, i]\n            )\n            current_policy_loss.register_hook(\n                lambda grad: grad / gradient_scale_batch[:, i]\n            )\n\n            value_loss += current_value_loss\n            reward_loss += current_reward_loss\n            policy_loss += current_policy_loss\n\n            # Compute priorities for the prioritized replay (See paper appendix Training)\n            pred_value_scalar = (\n                models.support_to_scalar(value, self.config.support_size)\n                .detach()\n                .cpu()\n                .numpy()\n                .squeeze()\n            )\n            priorities[:, i] = (\n                numpy.abs(pred_value_scalar - target_value_scalar[:, i])\n                ** self.config.PER_alpha\n            )\n\n        # Scale the value loss, paper recommends by 0.25 (See paper appendix Reanalyze)\n        loss = value_loss * self.config.value_loss_weight + reward_loss + policy_loss\n        if self.config.PER:\n            # Correct PER bias by using importance-sampling (IS) weights\n            loss *= weight_batch\n        # Mean over batch dimension (pseudocode do a sum)\n        loss = loss.mean()\n\n        # Optimize\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        self.training_step += 1\n\n        return (\n            priorities,\n            # For log purpose\n            loss.item(),\n            value_loss.mean().item(),\n            reward_loss.mean().item(),\n            policy_loss.mean().item(),\n        )\n\n    def update_lr(self):\n        """"""\n        Update learning rate\n        """"""\n        lr = self.config.lr_init * self.config.lr_decay_rate ** (\n            self.training_step / self.config.lr_decay_steps\n        )\n        for param_group in self.optimizer.param_groups:\n            param_group[""lr""] = lr\n\n    @staticmethod\n    def loss_function(\n        value, reward, policy_logits, target_value, target_reward, target_policy,\n    ):\n        # Cross-entropy seems to have a better convergence than MSE\n        value_loss = (-target_value * torch.nn.LogSoftmax(dim=1)(value)).sum(1)\n        reward_loss = (-target_reward * torch.nn.LogSoftmax(dim=1)(reward)).sum(1)\n        policy_loss = (-target_policy * torch.nn.LogSoftmax(dim=1)(policy_logits)).sum(\n            1\n        )\n        return value_loss, reward_loss, policy_loss\n'"
games/abstract_game.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass AbstractGame(ABC):\n    """"""\n    Inherit this class for muzero to play\n    """"""\n\n    @abstractmethod\n    def __init__(self, seed=None):\n        pass\n\n    @abstractmethod\n    def step(self, action):\n        """"""\n        Apply action to the game.\n        \n        Args:\n            action : action of the action_space to take.\n\n        Returns:\n            The new observation, the reward and a boolean if the game has ended.\n        """"""\n        pass\n\n    def to_play(self):\n        """"""\n        Return the current player.\n\n        Returns:\n            The current player, it should be an element of the players list in the config. \n        """"""\n        return 0\n\n    @abstractmethod\n    def legal_actions(self):\n        """"""\n        Should return the legal actions at each turn, if it is not available, it can return\n        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n        \n        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n        equal to the action space but to return a negative reward if the action is illegal.\n\n        Returns:\n            An array of integers, subset of the action space.\n        """"""\n        pass\n\n    @abstractmethod\n    def reset(self):\n        """"""\n        Reset the game for a new game.\n        \n        Returns:\n            Initial observation of the game.\n        """"""\n        pass\n\n    def close(self):\n        """"""\n        Properly close the game.\n        """"""\n        pass\n\n    @abstractmethod\n    def render(self):\n        """"""\n        Display the game observation.\n        """"""\n        pass\n\n    def human_to_action(self):\n        """"""\n        For multiplayer games, ask the user for a legal action\n        and return the corresponding action number.\n\n        Returns:\n            An integer from the action space.\n        """"""\n        choice = input(\n            ""Enter the action to play for the player {}: "".format(self.to_play())\n        )\n        while choice not in [str(action) for action in self.legal_actions()]:\n            choice = input(""Enter another action : "")\n        return int(choice)\n\n    def expert_agent(self):\n        """"""\n        Hard coded agent that MuZero faces to assess his progress in multiplayer games.\n        It doesn\'t influence training\n\n        Returns:\n            Action as an integer to take in the current game state\n        """"""\n        raise NotImplementedError\n    \n    def action_to_string(self, action_number):\n        """"""\n        Convert an action number to a string representing the action.\n\n        Args:\n            action_number: an integer from the action space.\n\n        Returns:\n            String representing the action.\n        """"""\n        return str(action_number)\n'"
games/breakout.py,1,"b'import datetime\nimport os\n\nimport gym\nimport numpy\nimport torch\n\nfrom .abstract_game import AbstractGame\n\ntry:\n    import cv2\nexcept ModuleNotFoundError:\n    raise ModuleNotFoundError(\'Please run ""pip install gym[atari]""\')\n\n\nclass MuZeroConfig:\n    def __init__(self):\n        # More help is available here: https://github.com/werner-duvaud/muzero-general/wiki/Hyperparameter-Optimization\n\n        self.seed = 0  # Seed for numpy, torch and the game\n\n\n\n        ### Game\n        self.observation_shape = (3, 96, 96)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n        self.action_space = [i for i in range(4)]  # Fixed list of all possible actions. You should only edit the length\n        self.players = [i for i in range(1)]  # List of players. You should only edit the length\n        self.stacked_observations = 32  # Number of previous observations and previous actions to add to the current observation\n\n        # Evaluate\n        self.muzero_player = 0  # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)\n        self.opponent = None  # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn\'t influence training. None, ""random"" or ""expert"" if implemented in the Game class\n\n\n\n        ### Self-Play\n        self.num_actors = 350  # Number of simultaneous threads self-playing to feed the replay buffer\n        self.max_moves = 27000  # Maximum number of moves if game is not finished before\n        self.num_simulations = 50  # Number of future moves self-simulated\n        self.discount = 0.997  # Chronological discount of the reward\n        self.temperature_threshold = None  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n\n        # Root prior exploration noise\n        self.root_dirichlet_alpha = 0.25\n        self.root_exploration_fraction = 0.25\n\n        # UCB formula\n        self.pb_c_base = 19652\n        self.pb_c_init = 1.25\n\n\n\n        ### Network\n        self.network = ""resnet""  # ""resnet"" / ""fullyconnected""\n        self.support_size = 300  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n\n        # Residual Network\n        self.downsample = True  # Downsample observations before representation network (See paper appendix Network Architecture)\n        self.blocks = 16  # Number of blocks in the ResNet\n        self.channels = 256  # Number of channels in the ResNet\n        self.reduced_channels_reward = 256  # Number of channels in reward head\n        self.reduced_channels_value = 256  # Number of channels in value head\n        self.reduced_channels_policy = 256  # Number of channels in policy head\n        self.resnet_fc_reward_layers = [256, 256]  # Define the hidden layers in the reward head of the dynamic network\n        self.resnet_fc_value_layers = [256, 256]  # Define the hidden layers in the value head of the prediction network\n        self.resnet_fc_policy_layers = [256, 256]  # Define the hidden layers in the policy head of the prediction network\n\n        # Fully Connected Network\n        self.encoding_size = 10\n        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n        self.fc_value_layers = []  # Define the hidden layers in the value network\n        self.fc_policy_layers = []  # Define the hidden layers in the policy network\n\n\n\n        ### Training\n        self.results_path = os.path.join(os.path.dirname(__file__), ""../results"", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(""%Y-%m-%d--%H-%M-%S""))  # Path to store the model weights and TensorBoard logs\n        self.training_steps = int(1000e3)  # Total number of training steps (ie weights update according to a batch)\n        self.batch_size = 1024  # Number of parts of games to train on at each training step\n        self.checkpoint_interval = int(1e3)  # Number of training steps before using the model for self-playing\n        self.value_loss_weight = 0.25  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n        self.training_device = ""cuda"" if torch.cuda.is_available() else ""cpu""  # Train on GPU if available\n\n        self.optimizer = ""SGD""  # ""Adam"" or ""SGD"". Paper uses SGD\n        self.weight_decay = 1e-4  # L2 weights regularization\n        self.momentum = 0.9  # Used only if optimizer is SGD\n\n        # Exponential learning rate schedule\n        self.lr_init = 0.05  # Initial learning rate\n        self.lr_decay_rate = 0.1  # Set it to 1 to use a constant learning rate\n        self.lr_decay_steps = 350e3\n\n\n\n        ### Replay Buffer\n        self.window_size = int(1e6)  # Number of self-play games to keep in the replay buffer\n        self.num_unroll_steps = 5  # Number of game moves to keep for every batch element\n        self.td_steps = 10  # Number of steps in the future to take into account for calculating the target value\n        self.use_last_model_value = True  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n\n        # Prioritized Replay (See paper appendix Training)\n        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n        self.use_max_priority = False  # If False, use the n-step TD error as initial priority. Better for large replay buffer\n        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n        self.PER_beta = 1.0\n\n\n\n        ### Adjust the self play / training ratio to avoid over/underfitting\n        self.self_play_delay = 0  # Number of seconds to wait after each played game\n        self.training_delay = 0  # Number of seconds to wait after each training step\n        self.ratio = None  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n\n\n    def visit_softmax_temperature_fn(self, trained_steps):\n        """"""\n        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n\n        Returns:\n            Positive float.\n        """"""\n        if trained_steps < 500e3:\n            return 1.0\n        elif trained_steps < 750e3:\n            return 0.5\n        else:\n            return 0.25\n\n\nclass Game(AbstractGame):\n    """"""\n    Game wrapper.\n    """"""\n\n    def __init__(self, seed=None):\n        self.env = gym.make(""Breakout-v4"")\n        if seed is not None:\n            self.env.seed(seed)\n\n    def step(self, action):\n        """"""\n        Apply action to the game.\n        \n        Args:\n            action : action of the action_space to take.\n\n        Returns:\n            The new observation, the reward and a boolean if the game has ended.\n        """"""\n        observation, reward, done, _ = self.env.step(action)\n        observation = cv2.resize(observation, (96, 96), interpolation=cv2.INTER_AREA)\n        observation = numpy.asarray(observation, dtype=numpy.float32) / 255.0\n        observation = numpy.moveaxis(observation, -1, 0)\n        return observation, reward, done\n\n    def legal_actions(self):\n        """"""\n        Should return the legal actions at each turn, if it is not available, it can return\n        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n        \n        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n        equal to the action space but to return a negative reward if the action is illegal.        \n\n        Returns:\n            An array of integers, subset of the action space.\n        """"""\n        return [i for i in range(4)]\n\n    def reset(self):\n        """"""\n        Reset the game for a new game.\n        \n        Returns:\n            Initial observation of the game.\n        """"""\n        observation = self.env.reset()\n        observation = cv2.resize(observation, (96, 96), interpolation=cv2.INTER_AREA)\n        observation = numpy.asarray(observation, dtype=numpy.float32) / 255.0\n        observation = numpy.moveaxis(observation, -1, 0)\n        return observation\n\n    def close(self):\n        """"""\n        Properly close the game.\n        """"""\n        self.env.close()\n\n    def render(self):\n        """"""\n        Display the game observation.\n        """"""\n        self.env.render()\n        input(""Press enter to take a step "")\n\n'"
games/cartpole.py,1,"b'import datetime\nimport os\n\nimport gym\nimport numpy\nimport torch\n\nfrom .abstract_game import AbstractGame\n\n\nclass MuZeroConfig:\n    def __init__(self):\n        # More help is available here: https://github.com/werner-duvaud/muzero-general/wiki/Hyperparameter-Optimization\n\n        self.seed = 0  # Seed for numpy, torch and the game\n\n\n\n        ### Game\n        self.observation_shape = (1, 1, 4)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n        self.action_space = [i for i in range(2)]  # Fixed list of all possible actions. You should only edit the length\n        self.players = [i for i in range(1)]  # List of players. You should only edit the length\n        self.stacked_observations = 0  # Number of previous observations and previous actions to add to the current observation\n\n        # Evaluate\n        self.muzero_player = 0  # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)\n        self.opponent = None  # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn\'t influence training. None, ""random"" or ""expert"" if implemented in the Game class\n\n\n\n        ### Self-Play\n        self.num_actors = 1  # Number of simultaneous threads self-playing to feed the replay buffer\n        self.max_moves = 500  # Maximum number of moves if game is not finished before\n        self.num_simulations = 50  # Number of future moves self-simulated\n        self.discount = 0.997  # Chronological discount of the reward\n        self.temperature_threshold = None  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n\n        # Root prior exploration noise\n        self.root_dirichlet_alpha = 0.25\n        self.root_exploration_fraction = 0.25\n\n        # UCB formula\n        self.pb_c_base = 19652\n        self.pb_c_init = 1.25\n\n\n\n        ### Network\n        self.network = ""fullyconnected""  # ""resnet"" / ""fullyconnected""\n        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n        \n        # Residual Network\n        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n        self.blocks = 1  # Number of blocks in the ResNet\n        self.channels = 2  # Number of channels in the ResNet\n        self.reduced_channels_reward = 2  # Number of channels in reward head\n        self.reduced_channels_value = 2  # Number of channels in value head\n        self.reduced_channels_policy = 2  # Number of channels in policy head\n        self.resnet_fc_reward_layers = []  # Define the hidden layers in the reward head of the dynamic network\n        self.resnet_fc_value_layers = []  # Define the hidden layers in the value head of the prediction network\n        self.resnet_fc_policy_layers = []  # Define the hidden layers in the policy head of the prediction network\n\n        # Fully Connected Network\n        self.encoding_size = 8\n        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n        self.fc_value_layers = []  # Define the hidden layers in the value network\n        self.fc_policy_layers = []  # Define the hidden layers in the policy network\n\n\n\n        ### Training\n        self.results_path = os.path.join(os.path.dirname(__file__), ""../results"", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(""%Y-%m-%d--%H-%M-%S""))  # Path to store the model weights and TensorBoard logs\n        self.training_steps = 5000  # Total number of training steps (ie weights update according to a batch)\n        self.batch_size = 128  # Number of parts of games to train on at each training step\n        self.checkpoint_interval = 10  # Number of training steps before using the model for self-playing\n        self.value_loss_weight = 1  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n        self.training_device = ""cuda"" if torch.cuda.is_available() else ""cpu""  # Train on GPU if available\n\n        self.optimizer = ""Adam""  # ""Adam"" or ""SGD"". Paper uses SGD\n        self.weight_decay = 1e-4  # L2 weights regularization\n        self.momentum = 0.9  # Used only if optimizer is SGD\n\n        # Exponential learning rate schedule\n        self.lr_init = 0.05  # Initial learning rate\n        self.lr_decay_rate = 0.9  # Set it to 1 to use a constant learning rate\n        self.lr_decay_steps = 1000\n\n\n\n        ### Replay Buffer\n        self.window_size = 100  # Number of self-play games to keep in the replay buffer\n        self.num_unroll_steps = 10  # Number of game moves to keep for every batch element\n        self.td_steps = 50  # Number of steps in the future to take into account for calculating the target value\n        self.use_last_model_value = True  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n\n        # Prioritized Replay (See paper appendix Training)\n        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n        self.use_max_priority = False  # If False, use the n-step TD error as initial priority. Better for large replay buffer\n        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n        self.PER_beta = 1.0\n\n\n\n        ### Adjust the self play / training ratio to avoid over/underfitting\n        self.self_play_delay = 0  # Number of seconds to wait after each played game\n        self.training_delay = 0  # Number of seconds to wait after each training step\n        self.ratio = None  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n\n\n    def visit_softmax_temperature_fn(self, trained_steps):\n        """"""\n        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n\n        Returns:\n            Positive float.\n        """"""\n        if trained_steps < 0.5 * self.training_steps:\n            return 1.0\n        elif trained_steps < 0.75 * self.training_steps:\n            return 0.5\n        else:\n            return 0.25\n\n\nclass Game(AbstractGame):\n    """"""\n    Game wrapper.\n    """"""\n\n    def __init__(self, seed=None):\n        self.env = gym.make(""CartPole-v1"")\n        if seed is not None:\n            self.env.seed(seed)\n\n    def step(self, action):\n        """"""\n        Apply action to the game.\n        \n        Args:\n            action : action of the action_space to take.\n\n        Returns:\n            The new observation, the reward and a boolean if the game has ended.\n        """"""\n        observation, reward, done, _ = self.env.step(action)\n        return numpy.array([[observation]]), reward, done\n\n    def legal_actions(self):\n        """"""\n        Should return the legal actions at each turn, if it is not available, it can return\n        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n        \n        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n        equal to the action space but to return a negative reward if the action is illegal.        \n\n        Returns:\n            An array of integers, subset of the action space.\n        """"""\n        return [i for i in range(2)]\n\n    def reset(self):\n        """"""\n        Reset the game for a new game.\n        \n        Returns:\n            Initial observation of the game.\n        """"""\n        return numpy.array([[self.env.reset()]])\n\n    def close(self):\n        """"""\n        Properly close the game.\n        """"""\n        self.env.close()\n\n    def render(self):\n        """"""\n        Display the game observation.\n        """"""\n        self.env.render()\n        input(""Press enter to take a step "")\n\n    def action_to_string(self, action_number):\n        """"""\n        Convert an action number to a string representing the action.\n\n        Args:\n            action_number: an integer from the action space.\n\n        Returns:\n            String representing the action.\n        """"""\n        actions = {\n            0: ""Push cart to the left"",\n            1: ""Push cart to the right"",\n        }\n        return ""{}. {}"".format(action_number, actions[action_number])\n'"
games/connect4.py,1,"b'import datetime\nimport os\n\nimport gym\nimport numpy\nimport torch\n\nfrom .abstract_game import AbstractGame\n\n\nclass MuZeroConfig:\n    def __init__(self):\n        # More help is available here: https://github.com/werner-duvaud/muzero-general/wiki/Hyperparameter-Optimization\n\n        self.seed = 0  # Seed for numpy, torch and the game\n\n\n\n        ### Game\n        self.observation_shape = (3, 6, 7)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n        self.action_space = [i for i in range(7)]  # Fixed list of all possible actions. You should only edit the length\n        self.players = [i for i in range(2)]  # List of players. You should only edit the length\n        self.stacked_observations = 0  # Number of previous observations and previous actions to add to the current observation\n\n        # Evaluate\n        self.muzero_player = 0  # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)\n        self.opponent = ""expert""  # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn\'t influence training. None, ""random"" or ""expert"" if implemented in the Game class\n\n\n\n        ### Self-Play\n        self.num_actors = 1  # Number of simultaneous threads self-playing to feed the replay buffer\n        self.max_moves = 42  # Maximum number of moves if game is not finished before\n        self.num_simulations = 200  # Number of future moves self-simulated\n        self.discount = 1  # Chronological discount of the reward\n        self.temperature_threshold = None  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n\n        # Root prior exploration noise\n        self.root_dirichlet_alpha = 0.3\n        self.root_exploration_fraction = 0.25\n\n        # UCB formula\n        self.pb_c_base = 19652\n        self.pb_c_init = 1.25\n\n\n\n        ### Network\n        self.network = ""resnet""  # ""resnet"" / ""fullyconnected""\n        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n        \n        # Residual Network\n        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n        self.blocks = 6  # Number of blocks in the ResNet\n        self.channels = 64  # Number of channels in the ResNet\n        self.reduced_channels_reward = 2  # Number of channels in reward head\n        self.reduced_channels_value = 2  # Number of channels in value head\n        self.reduced_channels_policy = 4  # Number of channels in policy head\n        self.resnet_fc_reward_layers = [32]  # Define the hidden layers in the reward head of the dynamic network\n        self.resnet_fc_value_layers = [32]  # Define the hidden layers in the value head of the prediction network\n        self.resnet_fc_policy_layers = [32]  # Define the hidden layers in the policy head of the prediction network\n        \n        # Fully Connected Network\n        self.encoding_size = 32\n        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n        self.fc_dynamics_layers = [64]  # Define the hidden layers in the dynamics network\n        self.fc_reward_layers = [64]  # Define the hidden layers in the reward network\n        self.fc_value_layers = []  # Define the hidden layers in the value network\n        self.fc_policy_layers = []  # Define the hidden layers in the policy network\n\n\n\n        ### Training\n        self.results_path = os.path.join(os.path.dirname(__file__), ""../results"", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(""%Y-%m-%d--%H-%M-%S""))  # Path to store the model weights and TensorBoard logs\n        self.training_steps = 100000  # Total number of training steps (ie weights update according to a batch)\n        self.batch_size = 256  # Number of parts of games to train on at each training step\n        self.checkpoint_interval = 10  # Number of training steps before using the model for self-playing\n        self.value_loss_weight = 0.25  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n        self.training_device = ""cuda"" if torch.cuda.is_available() else ""cpu""  # Train on GPU if available\n\n        self.optimizer = ""SGD""  # ""Adam"" or ""SGD"". Paper uses SGD\n        self.weight_decay = 1e-4  # L2 weights regularization\n        self.momentum = 0.9  # Used only if optimizer is SGD\n\n        # Exponential learning rate schedule\n        self.lr_init = 0.02  # Initial learning rate\n        self.lr_decay_rate = 1  # Set it to 1 to use a constant learning rate\n        self.lr_decay_steps = 10000\n\n\n\n        ### Replay Buffer\n        self.window_size = 10000  # Number of self-play games to keep in the replay buffer\n        self.num_unroll_steps = 42  # Number of game moves to keep for every batch element\n        self.td_steps = 42  # Number of steps in the future to take into account for calculating the target value\n        self.use_last_model_value = True  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n\n        # Prioritized Replay (See paper appendix Training)\n        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n        self.use_max_priority = False  # If False, use the n-step TD error as initial priority. Better for large replay buffer\n        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n        self.PER_beta = 1.0\n\n\n\n        ### Adjust the self play / training ratio to avoid over/underfitting\n        self.self_play_delay = 0  # Number of seconds to wait after each played game\n        self.training_delay = 0  # Number of seconds to wait after each training step\n        self.ratio = None  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n\n\n    def visit_softmax_temperature_fn(self, trained_steps):\n        """"""\n        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n\n        Returns:\n            Positive float.\n        """"""\n        return 1\n\n\nclass Game(AbstractGame):\n    """"""\n    Game wrapper.\n    """"""\n\n    def __init__(self, seed=None):\n        self.env = Connect4()\n\n    def step(self, action):\n        """"""\n        Apply action to the game.\n        \n        Args:\n            action : action of the action_space to take.\n\n        Returns:\n            The new observation, the reward and a boolean if the game has ended.\n        """"""\n        observation, reward, done = self.env.step(action)\n        return observation, reward * 10, done\n\n    def to_play(self):\n        """"""\n        Return the current player.\n\n        Returns:\n            The current player, it should be an element of the players list in the config. \n        """"""\n        return self.env.to_play()\n\n    def legal_actions(self):\n        """"""\n        Should return the legal actions at each turn, if it is not available, it can return\n        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n        \n        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n        equal to the action space but to return a negative reward if the action is illegal.        \n\n        Returns:\n            An array of integers, subset of the action space.\n        """"""\n        return self.env.legal_actions()\n\n    def reset(self):\n        """"""\n        Reset the game for a new game.\n        \n        Returns:\n            Initial observation of the game.\n        """"""\n        return self.env.reset()\n\n    def render(self):\n        """"""\n        Display the game observation.\n        """"""\n        self.env.render()\n        input(""Press enter to take a step "")\n\n    def encode_board(self):\n        return self.env.encode_board()\n\n    def human_to_action(self):\n        """"""\n        For multiplayer games, ask the user for a legal action\n        and return the corresponding action number.\n\n        Returns:\n            An integer from the action space.\n        """"""\n        choice = input(""Enter the column to play for the player {}: "".format(self.to_play()))\n        while choice not in [str(action) for action in self.legal_actions()]:\n            choice = input(""Enter another column : "")\n        return int(choice)\n\n    def expert_agent(self):\n        """"""\n        Hard coded agent that MuZero faces to assess his progress in multiplayer games.\n        It doesn\'t influence training\n\n        Returns:\n            Action as an integer to take in the current game state\n        """"""\n        return self.env.expert_action()\n\n    def action_to_string(self, action_number):\n        """"""\n        Convert an action number to a string representing the action.\n\n        Args:\n            action_number: an integer from the action space.\n\n        Returns:\n            String representing the action.\n        """"""\n        return ""Play column {}"".format(action_number + 1)\n\n\nclass Connect4:\n    def __init__(self):\n        self.board = numpy.zeros((6, 7)).astype(int)\n        self.player = 1\n\n    def to_play(self):\n        return 0 if self.player == 1 else 1\n\n    def reset(self):\n        self.board = numpy.zeros((6, 7)).astype(int)\n        self.player = 1\n        return self.get_observation()\n\n    def step(self, action):\n        for i in range(6):\n            if self.board[i][action] == 0:\n                self.board[i][action] = self.player\n                break\n\n        done = self.have_winner() or len(self.legal_actions()) == 0\n\n        reward = 1 if self.have_winner() else 0\n\n        self.player *= -1\n\n        return self.get_observation(), reward, done\n\n    def get_observation(self):\n        board_player1 = numpy.where(self.board == 1, 1.0, 0.0)\n        board_player2 = numpy.where(self.board == -1, 1.0, 0.0)\n        board_to_play = numpy.full((6, 7), self.player).astype(float)\n        return numpy.array([board_player1, board_player2, board_to_play])\n\n    def legal_actions(self):\n        legal = []\n        for i in range(7):\n            if self.board[5][i] == 0:\n                legal.append(i)\n        return legal\n\n    def have_winner(self):\n        # Horizontal check\n        for i in range(4):\n            for j in range(6):\n                if (\n                    self.board[j][i] == self.player\n                    and self.board[j][i + 1] == self.player\n                    and self.board[j][i + 2] == self.player\n                    and self.board[j][i + 3] == self.player\n                ):\n                    return True\n\n        # Vertical check\n        for i in range(7):\n            for j in range(3):\n                if (\n                    self.board[j][i] == self.player\n                    and self.board[j + 1][i] == self.player\n                    and self.board[j + 2][i] == self.player\n                    and self.board[j + 3][i] == self.player\n                ):\n                    return True\n\n        # Positive diagonal check\n        for i in range(4):\n            for j in range(3):\n                if (\n                    self.board[j][i] == self.player\n                    and self.board[j + 1][i + 1] == self.player\n                    and self.board[j + 2][i + 2] == self.player\n                    and self.board[j + 3][i + 3] == self.player\n                ):\n                    return True\n\n        # Negative diagonal check\n        for i in range(4):\n            for j in range(3, 6):\n                if (\n                    self.board[j][i] == self.player\n                    and self.board[j - 1][i + 1] == self.player\n                    and self.board[j - 2][i + 2] == self.player\n                    and self.board[j - 3][i + 3] == self.player\n                ):\n                    return True\n\n        return False\n\n    def expert_action(self):\n        board = self.board\n        action = numpy.random.choice(self.legal_actions())\n        for k in range(3):\n            for l in range(4):\n                sub_board = board[k:k+4, l:l+4]\n                # Horizontal and vertical checks\n                for i in range(4):\n                    if abs(sum(sub_board[i, :])) == 3:\n                        ind = numpy.where(sub_board[i, :] == 0)[0][0]\n                        if numpy.count_nonzero(board[:, ind+l]) == i+k:\n                            action = ind + l\n                            if self.player * sum(sub_board[i, :]) > 0:\n                                return action\n\n                    if abs(sum(sub_board[:, i])) == 3:\n                        action = i + l\n                        if self.player * sum(sub_board[:, i]) > 0:\n                            return action\n                # Diagonal checks\n                diag = sub_board.diagonal()\n                anti_diag = numpy.fliplr(sub_board).diagonal()\n                if abs(sum(diag)) == 3:\n                    ind = numpy.where(diag == 0)[0][0]\n                    if numpy.count_nonzero(board[:, ind+l]) == ind+k:\n                        action = ind + l \n                        if self.player * sum(diag) > 0:\n                            return action\n\n                if abs(sum(anti_diag)) == 3:\n                    ind = numpy.where(anti_diag == 0)[0][0]\n                    if numpy.count_nonzero(board[:, 3-ind+l]) == ind+k:\n                        action = 3-ind+l\n                        if self.player * sum(anti_diag) > 0:\n                            return action\n\n        return action\n        \n    def render(self):\n        print(self.board[::-1])\n'"
games/gomoku.py,1,"b'import datetime\nimport math\nimport os\n\nimport gym\nimport numpy\nimport torch\n\nfrom .abstract_game import AbstractGame\n\n\nclass MuZeroConfig:\n    def __init__(self):\n        # More help is available here: https://github.com/werner-duvaud/muzero-general/wiki/Hyperparameter-Optimization\n\n        self.seed = 0  # Seed for numpy, torch and the game\n\n\n\n        ### Game\n        self.observation_shape = (3, 11, 11)  # Dimensions of the game observation, must be 3 (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n        self.action_space = [i for i in range(11 * 11)]  # Fixed list of all possible actions. You should only edit the length\n        self.players = [i for i in range(2)]  # List of players. You should only edit the length\n        self.stacked_observations = 0  # Number of previous observations and previous actions to add to the current observation\n\n        # Evaluate\n        self.muzero_player = 0  # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)\n        self.opponent = ""random""  # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn\'t influence training. None, ""random"" or ""expert"" if implemented in the Game class\n\n\n\n        ### Self-Play\n        self.num_actors = 2  # Number of simultaneous threads self-playing to feed the replay buffer\n        self.max_moves = 121  # Maximum number of moves if game is not finished before\n        self.num_simulations = 400  # Number of future moves self-simulated\n        self.discount = 1  # Chronological discount of the reward\n        self.temperature_threshold = 40  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n\n        # Root prior exploration noise\n        self.root_dirichlet_alpha = 0.3\n        self.root_exploration_fraction = 0.25\n\n        # UCB formula\n        self.pb_c_base = 19652\n        self.pb_c_init = 1.25\n\n\n\n        ### Network\n        self.network = ""resnet""  # ""resnet"" / ""fullyconnected""\n        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n        \n        # Residual Network\n        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n        self.blocks = 6  # Number of blocks in the ResNet\n        self.channels = 128  # Number of channels in the ResNet\n        self.reduced_channels_reward = 2  # Number of channels in reward head\n        self.reduced_channels_value = 2  # Number of channels in value head\n        self.reduced_channels_policy = 4  # Number of channels in policy head\n        self.resnet_fc_reward_layers = [64]  # Define the hidden layers in the reward head of the dynamic network\n        self.resnet_fc_value_layers = [64]  # Define the hidden layers in the value head of the prediction network\n        self.resnet_fc_policy_layers = [64]  # Define the hidden layers in the policy head of the prediction network\n        \n        # Fully Connected Network\n        self.encoding_size = 32\n        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n        self.fc_dynamics_layers = [64]  # Define the hidden layers in the dynamics network\n        self.fc_reward_layers = [64]  # Define the hidden layers in the reward network\n        self.fc_value_layers = []  # Define the hidden layers in the value network\n        self.fc_policy_layers = []  # Define the hidden layers in the policy network\n\n\n\n        ### Training\n        self.results_path = os.path.join(os.path.dirname(__file__), ""../results"", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(""%Y-%m-%d--%H-%M-%S""))  # Path to store the model weights and TensorBoard logs\n        self.training_steps = 10000  # Total number of training steps (ie weights update according to a batch)\n        self.batch_size = 512  # Number of parts of games to train on at each training step\n        self.checkpoint_interval = 50  # Number of training steps before using the model for self-playing\n        self.value_loss_weight = 1  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n        self.training_device = ""cuda"" if torch.cuda.is_available() else ""cpu""  # Train on GPU if available\n\n        self.optimizer = ""Adam""  # ""Adam"" or ""SGD"". Paper uses SGD\n        self.weight_decay = 1e-4  # L2 weights regularization\n        self.momentum = 0.9  # Used only if optimizer is SGD\n\n        # Exponential learning rate schedule\n        self.lr_init = 0.002  # Initial learning rate\n        self.lr_decay_rate = 0.9  # Set it to 1 to use a constant learning rate\n        self.lr_decay_steps = 10000\n\n\n\n        ### Replay Buffer\n        self.window_size = 10000  # Number of self-play games to keep in the replay buffer\n        self.num_unroll_steps = 121  # Number of game moves to keep for every batch element\n        self.td_steps = 121  # Number of steps in the future to take into account for calculating the target value\n        self.use_last_model_value = False  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n\n        # Prioritized Replay (See paper appendix Training)\n        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n        self.use_max_priority = False  # If False, use the n-step TD error as initial priority. Better for large replay buffer\n        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n        self.PER_beta = 1.0\n\n\n\n        ### Adjust the self play / training ratio to avoid over/underfitting\n        self.self_play_delay = 0  # Number of seconds to wait after each played game\n        self.training_delay = 0  # Number of seconds to wait after each training step\n        self.ratio = 8  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n\n\n    def visit_softmax_temperature_fn(self, trained_steps):\n        """"""\n        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n        Returns:\n            Positive float.\n        """"""\n        if trained_steps < 0.5 * self.training_steps:\n            return 1.0\n        elif trained_steps < 0.75 * self.training_steps:\n            return 0.5\n        else:\n            return 0.25\n\n\nclass Game(AbstractGame):\n    """"""\n    Game wrapper.\n    """"""\n\n    def __init__(self, seed=None):\n        self.env = Gomoku()\n\n    def step(self, action):\n        """"""\n        Apply action to the game.\n        \n        Args:\n            action : action of the action_space to take.\n\n        Returns:\n            The new observation, the reward and a boolean if the game has ended.\n        """"""\n        observation, reward, done = self.env.step(action)\n        return observation, reward, done\n\n    def to_play(self):\n        """"""\n        Return the current player.\n\n        Returns:\n            The current player, it should be an element of the players list in the config. \n        """"""\n        return self.env.to_play()\n\n    def legal_actions(self):\n        """"""\n        Should return the legal actions at each turn, if it is not available, it can return\n        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n        \n        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n        equal to the action space but to return a negative reward if the action is illegal.\n\n        Returns:\n            An array of integers, subset of the action space.\n        """"""\n        return self.env.legal_actions()\n\n    def reset(self):\n        """"""\n        Reset the game for a new game.\n        \n        Returns:\n            Initial observation of the game.\n        """"""\n        return self.env.reset()\n\n    def close(self):\n        """"""\n        Properly close the game.\n        """"""\n        pass\n\n    def render(self):\n        """"""\n        Display the game observation.\n        """"""\n        self.env.render()\n        input(""Press enter to take a step "")\n\n    def human_to_action(self):\n        """"""\n        For multiplayer games, ask the user for a legal action\n        and return the corresponding action number.\n\n        Returns:\n            An integer from the action space.\n        """"""\n        valid = False\n        while not valid:\n            valid, action = self.env.human_input_to_action()\n        return action\n    \n    def action_to_string(self, action):\n        """"""\n        Convert an action number to a string representing the action.\n        Args:\n            action_number: an integer from the action space.\n        Returns:\n            String representing the action.\n        """"""\n        return self.env.action_to_human_input(action)\n\n\nclass Gomoku:\n    def __init__(self):\n        self.board_size = 11\n        self.board = numpy.zeros((self.board_size, self.board_size)).astype(int)\n        self.player = 1\n        self.board_markers = [\n            chr(x) for x in range(ord(""A""), ord(""A"") + self.board_size)\n        ]\n\n    def to_play(self):\n        return 0 if self.player == 1 else 1\n\n    def reset(self):\n        self.board = numpy.zeros((self.board_size, self.board_size)).astype(int)\n        self.player = 1\n        return self.get_observation()\n\n    def step(self, action):\n        x = math.floor(action / self.board_size)\n        y = action % self.board_size\n        self.board[x][y] = self.player\n\n        done = self.is_finished()\n\n        reward = 1 if done else 0\n\n        self.player *= -1\n\n        return self.get_observation(), reward, done\n\n    def get_observation(self):\n        board_player1 = numpy.where(self.board == 1, 1.0, 0.0)\n        board_player2 = numpy.where(self.board == -1, 1.0, 0.0)\n        board_to_play = numpy.full((11, 11), self.player).astype(float)\n        return numpy.array([board_player1, board_player2, board_to_play])\n\n    def legal_actions(self):\n        legal = []\n        for i in range(self.board_size):\n            for j in range(self.board_size):\n                if self.board[i][j] == 0:\n                    legal.append(i * self.board_size + j)\n        return legal\n\n    def is_finished(self):\n        has_legal_actions = False\n        directions = ((1, -1), (1, 0), (1, 1), (0, 1))\n        for i in range(self.board_size):\n            for j in range(self.board_size):\n                # if no stone is on the position, don\'t need to consider this position\n                if self.board[i][j] == 0:\n                    has_legal_actions = True\n                    continue\n                # value-value at a coord, i-row, j-col\n                player = self.board[i][j]\n                # check if there exist 5 in a line\n                for d in directions:\n                    x, y = i, j\n                    count = 0\n                    for _ in range(5):\n                        if (x not in range(self.board_size)) or (\n                            y not in range(self.board_size)\n                        ):\n                            break\n                        if self.board[x][y] != player:\n                            break\n                        x += d[0]\n                        y += d[1]\n                        count += 1\n                        # if 5 in a line, store positions of all stones, return value\n                        if count == 5:\n                            return True\n        return not has_legal_actions\n\n    def render(self):\n        marker = ""  ""\n        for i in range(self.board_size):\n            marker = marker + self.board_markers[i] + "" ""\n        print(marker)\n        for row in range(self.board_size):\n            print(chr(ord(""A"") + row), end="" "")\n            for col in range(self.board_size):\n                ch = self.board[row][col]\n                if ch == 0:\n                    print(""."", end="" "")\n                elif ch == 1:\n                    print(""X"", end="" "")\n                elif ch == -1:\n                    print(""O"", end="" "")\n            print()\n\n    def human_input_to_action(self):\n        human_input = input(""Enter an action: "")\n        if (\n            len(human_input) == 2\n            and human_input[0] in self.board_markers\n            and human_input[1] in self.board_markers\n        ):\n            x = ord(human_input[0]) - 65\n            y = ord(human_input[1]) - 65\n            if self.board[x][y] == 0:\n                return True, x * self.board_size + y\n        return False, -1\n\n    def action_to_human_input(self, action):\n        x = math.floor(action / self.board_size)\n        y = action % self.board_size\n        x = chr(x + 65)\n        y = chr(y + 65)\n        return x + y\n'"
games/gridworld.py,1,"b'import datetime\nimport os\n\nimport gym\nimport numpy\nimport torch\n\nfrom .abstract_game import AbstractGame\n\ntry:\n    import gym_minigrid\nexcept ModuleNotFoundError:\n    raise ModuleNotFoundError(\'Please run ""pip install gym_minigrid""\')\n\n\nclass MuZeroConfig:\n    def __init__(self):\n        # More help is available here: https://github.com/werner-duvaud/muzero-general/wiki/Hyperparameter-Optimization\n\n        self.seed = 0  # Seed for numpy, torch and the game\n\n\n\n        ### Game\n        self.observation_shape = (7, 7, 3)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n        self.action_space = [i for i in range(3)]  # Fixed list of all possible actions. You should only edit the length\n        self.players = [i for i in range(1)]  # List of players. You should only edit the length\n        self.stacked_observations = 0  # Number of previous observations and previous actions to add to the current observation\n\n        # Evaluate\n        self.muzero_player = 0  # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)\n        self.opponent = None  # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn\'t influence training. None, ""random"" or ""expert"" if implemented in the Game class\n\n\n\n        ### Self-Play\n        self.num_actors = 4  # Number of simultaneous threads self-playing to feed the replay buffer\n        self.max_moves = 15  # Maximum number of moves if game is not finished before\n        self.num_simulations = 20  # Number of future moves self-simulated\n        self.discount = 0.997  # Chronological discount of the reward\n        self.temperature_threshold = None  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n\n        # Root prior exploration noise\n        self.root_dirichlet_alpha = 0.25\n        self.root_exploration_fraction = 0.25\n\n        # UCB formula\n        self.pb_c_base = 19652\n        self.pb_c_init = 1.25\n\n\n\n        ### Network\n        self.network = ""fullyconnected""  # ""resnet"" / ""fullyconnected""\n        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n        \n        # Residual Network\n        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n        self.blocks = 1  # Number of blocks in the ResNet\n        self.channels = 2  # Number of channels in the ResNet\n        self.reduced_channels_reward = 2  # Number of channels in reward head\n        self.reduced_channels_value = 2  # Number of channels in value head\n        self.reduced_channels_policy = 2  # Number of channels in policy head\n        self.resnet_fc_reward_layers = []  # Define the hidden layers in the reward head of the dynamic network\n        self.resnet_fc_value_layers = []  # Define the hidden layers in the value head of the prediction network\n        self.resnet_fc_policy_layers = []  # Define the hidden layers in the policy head of the prediction network\n\n        # Fully Connected Network\n        self.encoding_size = 8\n        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n        self.fc_value_layers = [16]  # Define the hidden layers in the value network\n        self.fc_policy_layers = [16]  # Define the hidden layers in the policy network\n\n\n\n        ### Training\n        self.results_path = os.path.join(os.path.dirname(__file__), ""../results"", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(""%Y-%m-%d--%H-%M-%S""))  # Path to store the model weights and TensorBoard logs\n        self.training_steps = 30000  # Total number of training steps (ie weights update according to a batch)\n        self.batch_size = 128  # Number of parts of games to train on at each training step\n        self.checkpoint_interval = 10  # Number of training steps before using the model for self-playing\n        self.value_loss_weight = 1  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n        self.training_device = ""cuda"" if torch.cuda.is_available() else ""cpu""  # Train on GPU if available\n\n        self.optimizer = ""Adam""  # ""Adam"" or ""SGD"". Paper uses SGD\n        self.weight_decay = 1e-4  # L2 weights regularization\n        self.momentum = 0.9  # Used only if optimizer is SGD\n\n        # Exponential learning rate schedule\n        self.lr_init = 0.005  # Initial learning rate\n        self.lr_decay_rate = 1  # Set it to 1 to use a constant learning rate\n        self.lr_decay_steps = 1000\n\n\n\n        ### Replay Buffer\n        self.window_size = 5000  # Number of self-play games to keep in the replay buffer\n        self.num_unroll_steps = 10  # Number of game moves to keep for every batch element\n        self.td_steps = 20  # Number of steps in the future to take into account for calculating the target value\n        self.use_last_model_value = False  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n\n        # Prioritized Replay (See paper appendix Training)\n        self.PER = False  # Select in priority the elements in the replay buffer which are unexpected for the network\n        self.use_max_priority = False  # If False, use the n-step TD error as initial priority. Better for large replay buffer\n        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n        self.PER_beta = 1.0\n\n\n\n        ### Adjust the self play / training ratio to avoid over/underfitting\n        self.self_play_delay = 0  # Number of seconds to wait after each played game\n        self.training_delay = 0  # Number of seconds to wait after each training step\n        self.ratio = None  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n\n\n    def visit_softmax_temperature_fn(self, trained_steps):\n        """"""\n        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n\n        Returns:\n            Positive float.\n        """"""\n        if trained_steps < 0.5 * self.training_steps:\n            return 1.0\n        elif trained_steps < 0.75 * self.training_steps:\n            return 0.5\n        else:\n            return 0.25\n\n\nclass Game(AbstractGame):\n    """"""\n    Game wrapper.\n    """"""\n\n    def __init__(self, seed=None):\n        self.env = gym.make(\'MiniGrid-Empty-Random-6x6-v0\')\n        self.env = gym_minigrid.wrappers.ImgObsWrapper(self.env)\n        if seed is not None:\n            self.env.seed(seed)\n\n    def step(self, action):\n        """"""\n        Apply action to the game.\n        \n        Args:\n            action : action of the action_space to take.\n\n        Returns:\n            The new observation, the reward and a boolean if the game has ended.\n        """"""\n        observation, reward, done, _ = self.env.step(action)\n        return numpy.array(observation), reward, done\n\n    def legal_actions(self):\n        """"""\n        Should return the legal actions at each turn, if it is not available, it can return\n        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n        \n        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n        equal to the action space but to return a negative reward if the action is illegal.        \n\n        Returns:\n            An array of integers, subset of the action space.\n        """"""\n        return [i for i in range(3)]\n\n    def reset(self):\n        """"""\n        Reset the game for a new game.\n        \n        Returns:\n            Initial observation of the game.\n        """"""\n        return numpy.array(self.env.reset())\n\n    def close(self):\n        """"""\n        Properly close the game.\n        """"""\n        self.env.close()\n\n    def render(self):\n        """"""\n        Display the game observation.\n        """"""\n        self.env.render()\n        input(""Press enter to take a step "")\n\n    def action_to_string(self, action_number):\n        """"""\n        Convert an action number to a string representing the action.\n\n        Args:\n            action_number: an integer from the action space.\n\n        Returns:\n            String representing the action.\n        """"""\n        actions = {\n            0: ""Turn left"",\n            1: ""Turn right"",\n            2: ""Move forward"",\n            3: ""Pick up an object"",\n            4: ""Drop the object being carried"",\n            5: ""Toggle (open doors, interact with objects)"",\n        }\n        return ""{}. {}"".format(action_number, actions[action_number])\n'"
games/lunarlander.py,1,"b'import datetime\nimport os\n\nimport gym\nimport numpy\nimport torch\n\nfrom .abstract_game import AbstractGame\n\n\nclass MuZeroConfig:\n    def __init__(self):\n        # More help is available here: https://github.com/werner-duvaud/muzero-general/wiki/Hyperparameter-Optimization\n\n        self.seed = 0  # Seed for numpy, torch and the game\n\n\n\n        ### Game\n        self.observation_shape = (1, 1, 8)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n        self.action_space = [i for i in range(4)]  # Fixed list of all possible actions. You should only edit the length\n        self.players = [i for i in range(1)]  # List of players. You should only edit the length\n        self.stacked_observations = 0  # Number of previous observations and previous actions to add to the current observation\n\n        # Evaluate\n        self.muzero_player = 0  # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)\n        self.opponent = None  # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn\'t influence training. None, ""random"" or ""expert"" if implemented in the Game class\n\n\n\n        ### Self-Play\n        self.num_actors = 2  # Number of simultaneous threads self-playing to feed the replay buffer\n        self.max_moves = 700  # Maximum number of moves if game is not finished before\n        self.num_simulations = 50  # Number of future moves self-simulated\n        self.discount = 0.997  # Chronological discount of the reward\n        self.temperature_threshold = None  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n\n        # Root prior exploration noise\n        self.root_dirichlet_alpha = 0.25\n        self.root_exploration_fraction = 0.25\n\n        # UCB formula\n        self.pb_c_base = 19652\n        self.pb_c_init = 1.25\n\n\n\n        ### Network\n        self.network = ""fullyconnected""  # ""resnet"" / ""fullyconnected""\n        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n        \n        # Residual Network\n        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n        self.blocks = 2  # Number of blocks in the ResNet\n        self.channels = 16  # Number of channels in the ResNet\n        self.reduced_channels_reward = 16  # Number of channels in reward head\n        self.reduced_channels_value = 16  # Number of channels in value head\n        self.reduced_channels_policy = 16  # Number of channels in policy head\n        self.resnet_fc_reward_layers = []  # Define the hidden layers in the reward head of the dynamic network\n        self.resnet_fc_value_layers = []  # Define the hidden layers in the value head of the prediction network\n        self.resnet_fc_policy_layers = []  # Define the hidden layers in the policy head of the prediction network\n\n        # Fully Connected Network\n        self.encoding_size = 10\n        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n        self.fc_value_layers = [16]  # Define the hidden layers in the value network\n        self.fc_policy_layers = [8]  # Define the hidden layers in the policy network\n        \n\n\n        ### Training\n        self.results_path = os.path.join(os.path.dirname(__file__), ""../results"", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(""%Y-%m-%d--%H-%M-%S""))  # Path to store the model weights and TensorBoard logs\n        self.training_steps = 200000  # Total number of training steps (ie weights update according to a batch)\n        self.batch_size = 32  # Number of parts of games to train on at each training step\n        self.checkpoint_interval = 10  # Number of training steps before using the model for self-playing\n        self.value_loss_weight = 0.5  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n        self.training_device = ""cuda"" if torch.cuda.is_available() else ""cpu""  # Train on GPU if available\n\n        self.optimizer = ""SGD""  # ""Adam"" or ""SGD"". Paper uses SGD\n        self.weight_decay = 1e-4  # L2 weights regularization\n        self.momentum = 0.9  # Used only if optimizer is SGD\n\n        # Exponential learning rate schedule\n        self.lr_init = 0.1  # Initial learning rate\n        self.lr_decay_rate = 0.95  # Set it to 1 to use a constant learning rate\n        self.lr_decay_steps = 1000\n\n\n\n        ### Replay Buffer\n        self.window_size = 1000  # Number of self-play games to keep in the replay buffer\n        self.num_unroll_steps = 10  # Number of game moves to keep for every batch element\n        self.td_steps = 50  # Number of steps in the future to take into account for calculating the target value\n        self.use_last_model_value = True  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n\n        # Prioritized Replay (See paper appendix Training)\n        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n        self.use_max_priority = True  # If False, use the n-step TD error as initial priority. Better for large replay buffer\n        self.PER_alpha = 1.0  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n        self.PER_beta = 1.0\n\n\n\n        ### Adjust the self play / training ratio to avoid over/underfitting\n        self.self_play_delay = 0  # Number of seconds to wait after each played game\n        self.training_delay = 0  # Number of seconds to wait after each training step\n        self.ratio = 1/50  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n\n\n    def visit_softmax_temperature_fn(self, trained_steps):\n        """"""\n        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n\n        Returns:\n            Positive float.\n        """"""\n        if trained_steps < 0.5 * self.training_steps:\n            return 1.0\n        elif trained_steps < 0.75 * self.training_steps:\n            return 0.5\n        else:\n            return 0.25\n\n\nclass Game(AbstractGame):\n    """"""\n    Game wrapper.\n    """"""\n\n    def __init__(self, seed=None):\n        self.env = gym.make(""LunarLander-v2"")\n        if seed is not None:\n            self.env.seed(seed)\n\n    def step(self, action):\n        """"""\n        Apply action to the game.\n        \n        Args:\n            action : action of the action_space to take.\n\n        Returns:\n            The new observation, the reward and a boolean if the game has ended.\n        """"""\n        observation, reward, done, _ = self.env.step(action)\n        return numpy.array([[observation]]), reward/3, done\n\n    def legal_actions(self):\n        """"""\n        Should return the legal actions at each turn, if it is not available, it can return\n        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n        \n        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n        equal to the action space but to return a negative reward if the action is illegal.        \n\n        Returns:\n            An array of integers, subset of the action space.\n        """"""\n        return [i for i in range(4)]\n\n    def reset(self):\n        """"""\n        Reset the game for a new game.\n        \n        Returns:\n            Initial observation of the game.\n        """"""\n        return numpy.array([[self.env.reset()]])\n\n    def close(self):\n        """"""\n        Properly close the game.\n        """"""\n        self.env.close()\n\n    def render(self):\n        """"""\n        Display the game observation.\n        """"""\n        self.env.render()\n        input(""Press enter to take a step "")\n\n    def action_to_string(self, action_number):\n        """"""\n        Convert an action number to a string representing the action.\n\n        Args:\n            action_number: an integer from the action space.\n\n        Returns:\n            String representing the action.\n        """"""\n        actions = {\n            0: ""Do nothing"", \n            1: ""Fire left orientation engine"",\n            2: ""Fire main engine"",\n            3: ""Fire right orientation engine"",\n        }\n        return ""{}. {}"".format(action_number, actions[action_number])\n'"
games/tictactoe.py,1,"b'import datetime\nimport os\n\nimport gym\nimport numpy\nimport torch\n\nfrom .abstract_game import AbstractGame\n\n\nclass MuZeroConfig:\n    def __init__(self):\n        # More help is available here: https://github.com/werner-duvaud/muzero-general/wiki/Hyperparameter-Optimization\n\n        self.seed = 0  # Seed for numpy, torch and the game\n\n\n\n        ### Game\n        self.observation_shape = (3, 3, 3)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n        self.action_space = [i for i in range(9)]  # Fixed list of all possible actions. You should only edit the length\n        self.players = [i for i in range(2)]  # List of players. You should only edit the length\n        self.stacked_observations = 0  # Number of previous observations and previous actions to add to the current observation\n\n        # Evaluate\n        self.muzero_player = 0  # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)\n        self.opponent = ""expert""  # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn\'t influence training. None, ""random"" or ""expert"" if implemented in the Game class\n\n\n\n        ### Self-Play\n        self.num_actors = 1  # Number of simultaneous threads self-playing to feed the replay buffer\n        self.max_moves = 9  # Maximum number of moves if game is not finished before\n        self.num_simulations = 25  # Number of future moves self-simulated\n        self.discount = 1  # Chronological discount of the reward\n        self.temperature_threshold = 6  # Number of moves before dropping temperature to 0 (ie playing according to the max)\n\n        # Root prior exploration noise\n        self.root_dirichlet_alpha = 0.1\n        self.root_exploration_fraction = 0.25\n\n        # UCB formula\n        self.pb_c_base = 19652\n        self.pb_c_init = 1.25\n\n\n\n        ### Network\n        self.network = ""resnet""  # ""resnet"" / ""fullyconnected""\n        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n\n        # Residual Network\n        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n        self.blocks = 1  # Number of blocks in the ResNet\n        self.channels = 16  # Number of channels in the ResNet\n        self.reduced_channels_reward = 16  # Number of channels in reward head\n        self.reduced_channels_value = 16  # Number of channels in value head\n        self.reduced_channels_policy = 16  # Number of channels in policy head\n        self.resnet_fc_reward_layers = [8]  # Define the hidden layers in the reward head of the dynamic network\n        self.resnet_fc_value_layers = [8]  # Define the hidden layers in the value head of the prediction network\n        self.resnet_fc_policy_layers = [8]  # Define the hidden layers in the policy head of the prediction network\n\n        # Fully Connected Network\n        self.encoding_size = 32\n        self.fc_representation_layers = []  # Define the hidden layers in the representation network\n        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n        self.fc_value_layers = []  # Define the hidden layers in the value network\n        self.fc_policy_layers = []  # Define the hidden layers in the policy network\n\n\n\n        ### Training\n        self.results_path = os.path.join(os.path.dirname(__file__), ""../results"", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(""%Y-%m-%d--%H-%M-%S""))  # Path to store the model weights and TensorBoard logs\n        self.training_steps = 1000000  # Total number of training steps (ie weights update according to a batch)\n        self.batch_size = 64  # Number of parts of games to train on at each training step\n        self.checkpoint_interval = 10  # Number of training steps before using the model for self-playing\n        self.value_loss_weight = 0.25  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n        self.training_device = ""cuda"" if torch.cuda.is_available() else ""cpu""  # Train on GPU if available\n\n        self.optimizer = ""Adam""  # ""Adam"" or ""SGD"". Paper uses SGD\n        self.weight_decay = 1e-4  # L2 weights regularization\n        self.momentum = 0.9  # Used only if optimizer is SGD\n\n        # Exponential learning rate schedule\n        self.lr_init = 0.01  # Initial learning rate\n        self.lr_decay_rate = 1  # Set it to 1 to use a constant learning rate\n        self.lr_decay_steps = 10000\n\n\n\n        ### Replay Buffer\n        self.window_size = 3000  # Number of self-play games to keep in the replay buffer\n        self.num_unroll_steps = 20  # Number of game moves to keep for every batch element\n        self.td_steps = 20  # Number of steps in the future to take into account for calculating the target value\n        self.use_last_model_value = True  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n\n        # Prioritized Replay (See paper appendix Training)\n        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n        self.use_max_priority = False  # If False, use the n-step TD error as initial priority. Better for large replay buffer\n        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n        self.PER_beta = 1.0\n\n\n\n        ### Adjust the self play / training ratio to avoid over/underfitting\n        self.self_play_delay = 0  # Number of seconds to wait after each played game\n        self.training_delay = 0  # Number of seconds to wait after each training step\n        self.ratio = None  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n\n\n    def visit_softmax_temperature_fn(self, trained_steps):\n        """"""\n        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n\n        Returns:\n            Positive float.\n        """"""\n        return 1\n\n\nclass Game(AbstractGame):\n    """"""\n    Game wrapper.\n    """"""\n\n    def __init__(self, seed=None):\n        self.env = TicTacToe()\n\n    def step(self, action):\n        """"""\n        Apply action to the game.\n        \n        Args:\n            action : action of the action_space to take.\n\n        Returns:\n            The new observation, the reward and a boolean if the game has ended.\n        """"""\n        observation, reward, done = self.env.step(action)\n        return observation, reward * 20, done\n\n    def to_play(self):\n        """"""\n        Return the current player.\n\n        Returns:\n            The current player, it should be an element of the players list in the config. \n        """"""\n        return self.env.to_play()\n\n    def legal_actions(self):\n        """"""\n        Should return the legal actions at each turn, if it is not available, it can return\n        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n        \n        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n        equal to the action space but to return a negative reward if the action is illegal.\n    \n        Returns:\n            An array of integers, subset of the action space.\n        """"""\n        return self.env.legal_actions()\n\n    def reset(self):\n        """"""\n        Reset the game for a new game.\n        \n        Returns:\n            Initial observation of the game.\n        """"""\n        return self.env.reset()\n\n    def render(self):\n        """"""\n        Display the game observation.\n        """"""\n        self.env.render()\n        input(""Press enter to take a step "")\n\n    def encode_board(self):\n        return self.env.encode_board()\n\n    def human_to_action(self):\n        """"""\n        For multiplayer games, ask the user for a legal action\n        and return the corresponding action number.\n\n        Returns:\n            An integer from the action space.\n        """"""\n        while True:\n            try:\n                row = int(\n                    input(\n                        ""Enter the row (1, 2 or 3) to play for the player {}: "".format(\n                            self.to_play()\n                        )\n                    )\n                )\n                col = int(\n                    input(\n                        ""Enter the column (1, 2 or 3) to play for the player {}: "".format(\n                            self.to_play()\n                        )\n                    )\n                )\n                choice = (row - 1) * 3 + (col - 1)\n                if (\n                    choice in self.legal_actions()\n                    and 1 <= row\n                    and 1 <= col\n                    and row <= 3\n                    and col <= 3\n                ):\n                    break\n            except:\n                pass\n            print(""Wrong input, try again"")\n        return choice\n\n    def expert_agent(self):\n        """"""\n        Hard coded agent that MuZero faces to assess his progress in multiplayer games.\n        It doesn\'t influence training\n\n        Returns:\n            Action as an integer to take in the current game state\n        """"""\n        return self.env.expert_action()\n        \n    def action_to_string(self, action_number):\n        """"""\n        Convert an action number to a string representing the action.\n        \n        Args:\n            action_number: an integer from the action space.\n\n        Returns:\n            String representing the action.\n        """"""\n        row = 3 - action_number // 3\n        col = action_number % 3 + 1\n        return ""Play row {}, column {}"".format(row, col)\n\n\nclass TicTacToe:\n    def __init__(self):\n        self.board = numpy.zeros((3, 3)).astype(int)\n        self.player = 1\n\n    def to_play(self):\n        return 0 if self.player == 1 else 1\n\n    def reset(self):\n        self.board = numpy.zeros((3, 3)).astype(int)\n        self.player = 1\n        return self.get_observation()\n\n    def step(self, action):\n        row = action // 3\n        col = action % 3\n        self.board[row, col] = self.player\n\n        done = self.have_winner() or len(self.legal_actions()) == 0\n\n        reward = 1 if self.have_winner() else 0\n\n        self.player *= -1\n\n        return self.get_observation(), reward, done\n\n    def get_observation(self):\n        board_player1 = numpy.where(self.board == 1, 1.0, 0.0)\n        board_player2 = numpy.where(self.board == -1, 1.0, 0.0)\n        board_to_play = numpy.full((3, 3), self.player).astype(float)\n        return numpy.array([board_player1, board_player2, board_to_play])\n\n    def legal_actions(self):\n        legal = []\n        for i in range(9):\n            row = i // 3\n            col = i % 3\n            if self.board[row, col] == 0:\n                legal.append(i)\n        return legal\n\n    def have_winner(self):\n        # Horizontal and vertical checks\n        for i in range(3):\n            if (self.board[i, :] == self.player * numpy.ones(3).astype(int)).all():\n                return True\n            if (self.board[:, i] == self.player * numpy.ones(3).astype(int)).all():\n                return True\n\n        # Diagonal checks\n        if (\n            self.board[0, 0] == self.player\n            and self.board[1, 1] == self.player\n            and self.board[2, 2] == self.player\n        ):\n            return True\n        if (\n            self.board[2, 0] == self.player\n            and self.board[1, 1] == self.player\n            and self.board[0, 2] == self.player\n        ):\n            return True\n\n        return False\n\n    def expert_action(self):\n        board = self.board\n        action = numpy.random.choice(self.legal_actions())\n        # Horizontal and vertical checks\n        for i in range(3):\n            if abs(sum(board[i, :])) == 2:\n                ind = numpy.where(board[i, :] == 0)[0][0]\n                action = numpy.ravel_multi_index((numpy.array([i]), numpy.array([ind])), (3, 3))[0]\n                if self.player * sum(board[i, :]) > 0:\n                    return action \n\n            if abs(sum(board[:, i])) == 2:\n                ind = numpy.where(board[:, i] == 0)[0][0]\n                action = numpy.ravel_multi_index((numpy.array([ind]), numpy.array([i])), (3, 3))[0]\n                if self.player * sum(board[:, i]) > 0:\n                    return action\n\n        # Diagonal checks\n        diag = board.diagonal()\n        anti_diag = numpy.fliplr(board).diagonal()\n        if abs(sum(diag)) == 2:\n            ind = numpy.where(diag == 0)[0][0]\n            action = numpy.ravel_multi_index((numpy.array([ind]), numpy.array([ind])), (3, 3))[0]\n            if self.player * sum(diag) > 0:\n                return action\n\n        if abs(sum(anti_diag)) == 2:\n            ind = numpy.where(anti_diag == 0)[0][0]\n            action = numpy.ravel_multi_index((numpy.array([ind]), numpy.array([2 - ind])), (3, 3))[0]\n            if self.player * sum(anti_diag) > 0:\n                return action\n\n        return action\n\n    def render(self):\n        print(self.board[::-1])\n'"
games/twentyone.py,1,"b'""""""\nThis is a very simple form of twenty one. Ace only counts as value 1 not 1 or\n11 for simplicity. This means that there is no such thing as a natural or two\ncard 21. This is a good example of showing how it can provide a good solution\nto even luck based games.\n""""""\n\nimport datetime\nimport os\nfrom random import randint\n\nimport gym\nimport numpy\nimport torch\n\nfrom .abstract_game import AbstractGame\n\n\nclass MuZeroConfig:\n    def __init__(self):\n        # More help is available here: https://github.com/werner-duvaud/muzero-general/wiki/Hyperparameter-Optimization\n\n        self.seed = 0  # Seed for numpy, torch and the game\n\n\n\n        ### Game\n        self.observation_shape = (3,3,3) # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\n        self.action_space = [i for i in range(2)] # Fixed list of all possible actions. You should only edit the length\n        self.players = [i for i in range(1)] # List of players. You should only edit the length\n        self.stacked_observations = 0 # Number of previous observations and previous actions to add to the current observation\n\n        # Evaluate\n        self.muzero_player = 0 # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)\n        self.opponent = None # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn\'t influence training. None, ""random"" or ""expert"" if implemented in the Game class\n\n\n\n        ### Self-Play\n        self.num_actors = 4 # Number of simultaneous threads self-playing to feed the replay buffer\n        self.max_moves = 21 # Maximum number of moves if game is not finished before\n        self.num_simulations = 21 # Number of future moves self-simulated\n        self.discount = 1 # Chronological discount of the reward\n        self.temperature_threshold = None # Number of moves before dropping temperature to 0 (ie playing according to the max)\n\n        # Root prior exploration noise\n        self.root_dirichlet_alpha = 0.25\n        self.root_exploration_fraction = 0.25\n\n        # UCB formula\n        self.pb_c_base = 19652\n        self.pb_c_init = 1.25\n\n\n\n        ### Network\n        self.network = ""resnet""  # ""resnet"" / ""fullyconnected""\n        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size\n\n        # Residual Network\n        self.downsample = False  # Downsample observations before representation network (See paper appendix Network Architecture)\n        self.blocks = 2  # Number of blocks in the ResNet\n        self.channels = 32  # Number of channels in the ResNet\n        self.reduced_channels_reward = 32  # Number of channels in reward head\n        self.reduced_channels_value = 32  # Number of channels in value head\n        self.reduced_channels_policy = 32  # Number of channels in policy head\n        self.resnet_fc_reward_layers = [16]  # Define the hidden layers in the reward head of the dynamic network\n        self.resnet_fc_value_layers = [16]  # Define the hidden layers in the value head of the prediction network\n        self.resnet_fc_policy_layers = [16]  # Define the hidden layers in the policy head of the prediction network\n\n        # Fully Connected Network\n        self.encoding_size = 32\n        self.fc_representation_layers = [16]  # Define the hidden layers in the representation network\n        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\n        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\n        self.fc_value_layers = [16]  # Define the hidden layers in the value network\n        self.fc_policy_layers = [16]  # Define the hidden layers in the policy network\n\n\n\n        ### Training\n        self.results_path = os.path.join(os.path.dirname(__file__), ""../results"", os.path.basename(__file__)[:-3], datetime.datetime.now().strftime(""%Y-%m-%d--%H-%M-%S""))  # Path to store the model weights and TensorBoard logs\n        self.training_steps = 15000  # Total number of training steps (ie weights update according to a batch)\n        self.batch_size = 64  # Number of parts of games to train on at each training step\n        self.checkpoint_interval = 10  # Number of training steps before using the model for self-playing\n        self.value_loss_weight = 0.25  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\n        self.training_device = ""cuda"" if torch.cuda.is_available() else ""cpu""  # Train on GPU if available\n\n        self.optimizer = ""SGD""  # ""Adam"" or ""SGD"". Paper uses SGD\n        self.weight_decay = 1e-4  # L2 weights regularization\n        self.momentum = 0.9  # Used only if optimizer is SGD\n\n        # Exponential learning rate schedule\n        self.lr_init = 0.03  # Initial learning rate\n        self.lr_decay_rate = 0.75  # Set it to 1 to use a constant learning rate\n        self.lr_decay_steps = 150000\n\n\n\n        ### Replay Buffer\n        self.window_size = 10000  # Number of self-play games to keep in the replay buffer\n        self.num_unroll_steps = 20  # Number of game moves to keep for every batch element\n        self.td_steps = 50  # Number of steps in the future to take into account for calculating the target value\n        self.use_last_model_value = True  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\n\n        # Prioritized Replay (See paper appendix Training)\n        self.PER = True  # Select in priority the elements in the replay buffer which are unexpected for the network\n        self.use_max_priority = False  # If False, use the n-step TD error as initial priority. Better for large replay buffer\n        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\n        self.PER_beta = 1.0\n\n\n\n        ### Adjust the self play / training ratio to avoid over/underfitting\n        self.self_play_delay = 0  # Number of seconds to wait after each played game\n        self.training_delay = 0  # Number of seconds to wait after each training step\n        self.ratio = None  # Desired self played games per training step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\n\n\n    def visit_softmax_temperature_fn(self, trained_steps):\n        """"""\n        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\n        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\n\n        Returns:\n            Positive float.\n        """"""\n        if trained_steps < 500e3:\n            return 1.0\n        elif trained_steps < 750e3:\n            return 0.5\n        else:\n            return 0.25\n\nclass Game(AbstractGame):\n    """"""\n    Game wrapper.\n    """"""\n\n    def __init__(self, seed=None):\n        self.env = TwentyOne()\n\n    def step(self, action):\n        """"""\n        Apply action to the game.\n        \n        Args:\n            action : action of the action_space to take.\n\n        Returns:\n            The new observation, the reward and a boolean if the game has ended.\n        """"""\n        observation, reward, done = self.env.step(action)\n        return observation, reward * 10, done\n\n    def to_play(self):\n        """"""\n        Return the current player.\n\n        Returns:\n            The current player, it should be an element of the players list in the config. \n        """"""\n        return self.env.to_play()\n\n    def legal_actions(self):\n        """"""\n        Should return the legal actions at each turn, if it is not available, it can return\n        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n        \n        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n        equal to the action space but to return a negative reward if the action is illegal.\n\n        Returns:\n            An array of integers, subset of the action space.\n        """"""\n        return self.env.legal_actions()\n\n    def reset(self):\n        """"""\n        Reset the game for a new game.\n        \n        Returns:\n            Initial observation of the game.\n        """"""\n        return self.env.reset()\n\n    def render(self):\n        """"""\n        Display the game observation.\n        """"""\n        self.env.render()\n        input(""Press enter to take a step "")\n\n    def human_to_action(self):\n        """"""\n        For multiplayer games, ask the user for a legal action\n        and return the corresponding action number.\n\n        Returns:\n            An integer from the action space.\n        """"""\n        choice = input(""Enter the action (0) Hit, or (1) Stand for the player {}: "".format(self.to_play()))\n        while choice not in [str(action) for action in self.legal_actions()]:\n            choice = input(""Enter either (0) Hit or (1) Stand : "")\n        return int(choice)\n\n    def action_to_string(self, action_number):\n        """"""\n        Convert an action number to a string representing the action.\n\n        Args:\n            action_number: an integer from the action space.\n\n        Returns:\n            String representing the action.\n        """"""\n        actions = {\n            0: ""Hit"",\n            1: ""Stand"",\n        }\n        return ""{}. {}"".format(action_number, actions[action_number])\n\nclass TwentyOne:\n    def __init__(self):\n        self.player_hand = self.deal_card_value()\n        self.dealer_hand = self.deal_card_value()\n\n        self.player = 1\n\n    def to_play(self):\n        return 0 if self.player == 1 else 1\n\n    def reset(self):\n        self.player_hand = self.deal_card_value()\n        self.dealer_hand = self.deal_card_value()\n        self.player = 1\n        return self.get_observation()\n\n    """"""\n    Action: 0 = Hit\n    Action: 1 = Stand\n    """"""\n    def step(self, action):\n\n        if action == 0:\n            self.player_hand += self.deal_card_value()\n\n        done = self.is_busted() or action == 1 or self.player_hand == 21\n\n        reward = 0\n\n        if done:\n            self.dealer_plays()    \n            reward = self.get_reward(True)\n\n        return self.get_observation(), self.get_reward(done), done\n\n    def get_observation(self):\n        return [\n            numpy.array(numpy.full((3, 3), self.player_hand).astype(float)),\n            numpy.array(numpy.full((3, 3), self.dealer_hand).astype(float)),\n            numpy.array(numpy.full((3, 3), 0))\n        ]\n\n    def legal_actions(self):\n        # 0 = hit\n        # 1 = stand\n        return [0, 1]\n\n    def get_reward(self, done):\n        if not done:\n            return 0\n        if self.player_hand <= 21 and self.dealer_hand < self.player_hand:\n            return 1\n        if self.player_hand <= 21 and self.dealer_hand > 21:\n            return 1\n        if self.player_hand > 21:\n            return -1\n        if self.player_hand == self.dealer_hand:\n            return 0\n        return -1\n\n\n    def deal_card_value(self):\n        card = randint(1,13)\n        if card >= 10:\n            value = 10\n        else:\n            value = card\n        return value\n\n    def dealer_plays(self):\n        if self.player_hand > 21:\n            return\n        while self.dealer_hand<=16:\n            self.dealer_hand += self.deal_card_value()\n\n    def is_busted(self):\n        if self.player_hand > 21:\n            return True\n\n    def render(self):\n        print(""Dealer hand: "" + str(self.dealer_hand))\n        print(""Player hand: "" + str(self.player_hand))\n'"
