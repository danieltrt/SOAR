file_path,api_count,code
config.py,3,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : config.py\n# @Time         : Created at 2019-03-18\n# @Blog         : http://zhiweil.ml/\n# @Description  :\n# Copyrights (C) 2018. All Rights Reserved.\nimport os\nimport re\nimport time\nimport torch\nfrom time import strftime, localtime\n\n# ===Program===\nif_test = False\nCUDA = True\nif_save = True\ndata_shuffle = False  # False\noracle_pretrain = True  # True\ngen_pretrain = False\ndis_pretrain = False\nclas_pretrain = False\n\nrun_model = \'seqgan\'  # seqgan, leakgan, maligan, jsdgan, relgan, sentigan\nk_label = 2  # num of labels, >=2\ngen_init = \'truncated_normal\'  # normal, uniform, truncated_normal\ndis_init = \'uniform\'  # normal, uniform, truncated_normal\n\n# ===Oracle or Real, type===\nif_real_data = False  # if use real data\ndataset = \'oracle\'  # oracle, image_coco, emnlp_news, amazon_app_book, mr15\nmodel_type = \'vanilla\'  # vanilla, RMC (custom)\nloss_type = \'rsgan\'  # standard, JS, KL, hinge, tv, LS, rsgan (for RelGAN)\nvocab_size = 5000  # oracle: 5000, coco: 6613, emnlp: 5255, amazon_app_book: 6418, mr15: 6289\nmax_seq_len = 20  # oracle: 20, coco: 37, emnlp: 51, amazon_app_book: 40\nADV_train_epoch = 2000  # SeqGAN, LeakGAN-200, RelGAN-3000\nextend_vocab_size = 0  # plus test data, only used for Classifier\n\ntemp_adpt = \'exp\'  # no, lin, exp, log, sigmoid, quad, sqrt\ntemperature = 1\n\n# ===Basic Train===\nsamples_num = 10000  # 10000, mr15: 2000,\nMLE_train_epoch = 150  # SeqGAN-80, LeakGAN-8, RelGAN-150\nPRE_clas_epoch = 10\ninter_epoch = 15  # LeakGAN-10\nbatch_size = 64  # 64\nstart_letter = 1\npadding_idx = 0\nstart_token = \'BOS\'\npadding_token = \'EOS\'\ngen_lr = 0.01  # 0.01\ngen_adv_lr = 1e-4  # RelGAN-1e-4\ndis_lr = 1e-4  # SeqGAN,LeakGAN-1e-2, RelGAN-1e-4\nclas_lr = 1e-3\nclip_norm = 5.0\n\npre_log_step = 10\nadv_log_step = 20\n\ntrain_data = \'dataset/\' + dataset + \'.txt\'\ntest_data = \'dataset/testdata/\' + dataset + \'_test.txt\'\ncat_train_data = \'dataset/\' + dataset + \'_cat{}.txt\'\ncat_test_data = \'dataset/testdata/\' + dataset + \'_cat{}_test.txt\'\n\n# ===Metrics===\nuse_nll_oracle = True\nuse_nll_gen = True\nuse_nll_div = True\nuse_bleu = True\nuse_self_bleu = True\nuse_clas_acc = True\nuse_ppl = False\n\n# ===Generator===\nADV_g_step = 1  # 1\nrollout_num = 16  # 4\ngen_embed_dim = 32  # 32\ngen_hidden_dim = 32  # 32\ngoal_size = 16  # LeakGAN-16\nstep_size = 4  # LeakGAN-4\n\nmem_slots = 1  # RelGAN-1\nnum_heads = 2  # RelGAN-2\nhead_size = 256  # RelGAN-256\n\n# ===Discriminator===\nd_step = 5  # SeqGAN-50, LeakGAN-5\nd_epoch = 3  # SeqGAN,LeakGAN-3\nADV_d_step = 5  # SeqGAN,LeakGAN,RelGAN-5\nADV_d_epoch = 3  # SeqGAN,LeakGAN-3\n\ndis_embed_dim = 64\ndis_hidden_dim = 64\nnum_rep = 64  # RelGAN\n\n# ===log===\nlog_time_str = strftime(""%m%d_%H%M_%S"", localtime())\nlog_filename = strftime(""log/log_%s"" % log_time_str)\nif os.path.exists(log_filename + \'.txt\'):\n    i = 2\n    while True:\n        if not os.path.exists(log_filename + \'_%d\' % i + \'.txt\'):\n            log_filename = log_filename + \'_%d\' % i\n            break\n        i += 1\nlog_filename = log_filename + \'.txt\'\n\n# Automatically choose GPU or CPU\nif torch.cuda.is_available() and torch.cuda.device_count() > 0:\n    os.system(\'nvidia-smi -q -d Utilization > gpu\')\n    with open(\'gpu\', \'r\') as _tmpfile:\n        util_gpu = list(map(int, re.findall(r\'Gpu\\s+:\\s*(\\d+)\\s*%\', _tmpfile.read())))\n    os.remove(\'gpu\')\n    if len(util_gpu):\n        device = util_gpu.index(min(util_gpu))\n    else:\n        device = 0\nelse:\n    device = -1\n# device=1\n# print(\'device: \', device)\ntorch.cuda.set_device(device)\n\n# ===Save Model and samples===\nsave_root = \'save/{}/{}/{}_{}_lt-{}_sl{}_temp{}_T{}/\'.format(time.strftime(""%Y%m%d""),\n                                                             dataset, run_model, model_type,\n                                                             loss_type, max_seq_len,\n                                                             temperature,\n                                                             log_time_str)\nsave_samples_root = save_root + \'samples/\'\nsave_model_root = save_root + \'models/\'\n\noracle_state_dict_path = \'pretrain/oracle_data/oracle_lstm.pt\'\noracle_samples_path = \'pretrain/oracle_data/oracle_lstm_samples_{}.pt\'\nmulti_oracle_state_dict_path = \'pretrain/oracle_data/oracle{}_lstm.pt\'\nmulti_oracle_samples_path = \'pretrain/oracle_data/oracle{}_lstm_samples_{}.pt\'\n\npretrain_root = \'pretrain/{}/\'.format(dataset if if_real_data else \'oracle_data\')\npretrained_gen_path = pretrain_root + \'gen_MLE_pretrain_{}_{}_sl{}_sn{}.pt\'.format(run_model, model_type, max_seq_len,\n                                                                                   samples_num)\npretrained_dis_path = pretrain_root + \'dis_pretrain_{}_{}_sl{}_sn{}.pt\'.format(run_model, model_type, max_seq_len,\n                                                                               samples_num)\npretrained_clas_path = pretrain_root + \'clas_pretrain_{}_{}_sl{}_sn{}.pt\'.format(run_model, model_type, max_seq_len,\n                                                                                 samples_num)\nsignal_file = \'run_signal.txt\'\n\ntips = \'\'\n\n\n# Init settings according to parser\ndef init_param(opt):\n    global run_model, model_type, loss_type, CUDA, device, data_shuffle, samples_num, vocab_size, \\\n        MLE_train_epoch, ADV_train_epoch, inter_epoch, batch_size, max_seq_len, start_letter, padding_idx, \\\n        gen_lr, gen_adv_lr, dis_lr, clip_norm, pre_log_step, adv_log_step, train_data, test_data, temp_adpt, \\\n        temperature, oracle_pretrain, gen_pretrain, dis_pretrain, ADV_g_step, rollout_num, gen_embed_dim, \\\n        gen_hidden_dim, goal_size, step_size, mem_slots, num_heads, head_size, d_step, d_epoch, \\\n        ADV_d_step, ADV_d_epoch, dis_embed_dim, dis_hidden_dim, num_rep, log_filename, save_root, \\\n        signal_file, tips, save_samples_root, save_model_root, if_real_data, pretrained_gen_path, \\\n        pretrained_dis_path, pretrain_root, if_test, dataset, PRE_clas_epoch, oracle_samples_path, \\\n        pretrained_clas_path, gen_init, dis_init, multi_oracle_samples_path, k_label, cat_train_data, cat_test_data, \\\n        use_nll_oracle, use_nll_gen, use_nll_div, use_bleu, use_self_bleu, use_clas_acc, use_ppl\n\n    if_test = True if opt.if_test == 1 else False\n    run_model = opt.run_model\n    k_label = opt.k_label\n    dataset = opt.dataset\n    model_type = opt.model_type\n    loss_type = opt.loss_type\n    if_real_data = True if opt.if_real_data == 1 else False\n    CUDA = True if opt.cuda == 1 else False\n    device = opt.device\n    data_shuffle = opt.shuffle\n    gen_init = opt.gen_init\n    dis_init = opt.dis_init\n\n    samples_num = opt.samples_num\n    vocab_size = opt.vocab_size\n    MLE_train_epoch = opt.mle_epoch\n    PRE_clas_epoch = opt.clas_pre_epoch\n    ADV_train_epoch = opt.adv_epoch\n    inter_epoch = opt.inter_epoch\n    batch_size = opt.batch_size\n    max_seq_len = opt.max_seq_len\n    start_letter = opt.start_letter\n    padding_idx = opt.padding_idx\n    gen_lr = opt.gen_lr\n    gen_adv_lr = opt.gen_adv_lr\n    dis_lr = opt.dis_lr\n    clip_norm = opt.clip_norm\n    pre_log_step = opt.pre_log_step\n    adv_log_step = opt.adv_log_step\n    temp_adpt = opt.temp_adpt\n    temperature = opt.temperature\n    oracle_pretrain = True if opt.ora_pretrain == 1 else False\n    gen_pretrain = True if opt.gen_pretrain == 1 else False\n    dis_pretrain = True if opt.dis_pretrain == 1 else False\n\n    ADV_g_step = opt.adv_g_step\n    rollout_num = opt.rollout_num\n    gen_embed_dim = opt.gen_embed_dim\n    gen_hidden_dim = opt.gen_hidden_dim\n    goal_size = opt.goal_size\n    step_size = opt.step_size\n    mem_slots = opt.mem_slots\n    num_heads = opt.num_heads\n    head_size = opt.head_size\n\n    d_step = opt.d_step\n    d_epoch = opt.d_epoch\n    ADV_d_step = opt.adv_d_step\n    ADV_d_epoch = opt.adv_d_epoch\n    dis_embed_dim = opt.dis_embed_dim\n    dis_hidden_dim = opt.dis_hidden_dim\n    num_rep = opt.num_rep\n\n    use_nll_oracle = True if opt.use_nll_oracle == 1 else False\n    use_nll_gen = True if opt.use_nll_gen == 1 else False\n    use_nll_div = True if opt.use_nll_div == 1 else False\n    use_bleu = True if opt.use_bleu == 1 else False\n    use_self_bleu = True if opt.use_self_bleu == 1 else False\n    use_clas_acc = True if opt.use_clas_acc == 1 else False\n    use_ppl = True if opt.use_ppl == 1 else False\n\n    log_filename = opt.log_file\n    signal_file = opt.signal_file\n    tips = opt.tips\n\n    # CUDA device\n    torch.cuda.set_device(device)\n\n    # Save path\n    save_root = \'save/{}/{}/{}_{}_lt-{}_sl{}_temp{}_T{}/\'.format(time.strftime(""%Y%m%d""),\n                                                                 dataset, run_model, model_type,\n                                                                 loss_type, max_seq_len,\n                                                                 temperature,\n                                                                 log_time_str)\n    save_samples_root = save_root + \'samples/\'\n    save_model_root = save_root + \'models/\'\n\n    train_data = \'dataset/\' + dataset + \'.txt\'\n    test_data = \'dataset/testdata/\' + dataset + \'_test.txt\'\n    cat_train_data = \'dataset/\' + dataset + \'_cat{}.txt\'\n    cat_test_data = \'dataset/testdata/\' + dataset + \'_cat{}_test.txt\'\n\n    if max_seq_len == 40:\n        oracle_samples_path = \'pretrain/oracle_data/oracle_lstm_samples_{}_sl40.pt\'\n        multi_oracle_samples_path = \'pretrain/oracle_data/oracle{}_lstm_samples_{}_sl40.pt\'\n\n    pretrain_root = \'pretrain/{}/\'.format(dataset if if_real_data else \'oracle_data\')\n    pretrained_gen_path = pretrain_root + \'gen_MLE_pretrain_{}_{}_sl{}_sn{}.pt\'.format(run_model, model_type,\n                                                                                       max_seq_len, samples_num)\n    pretrained_dis_path = pretrain_root + \'dis_pretrain_{}_{}_sl{}_sn{}.pt\'.format(run_model, model_type, max_seq_len,\n                                                                                   samples_num)\n    pretrained_clas_path = pretrain_root + \'clas_pretrain_{}_{}_sl{}_sn{}.pt\'.format(run_model, model_type, max_seq_len,\n                                                                                     samples_num)\n\n    # Assertion\n    assert k_label >= 2, \'Error: k_label = {}, which should be >=2!\'.format(k_label)\n\n    # Create Directory\n    dir_list = [\'save\', \'savefig\', \'log\', \'pretrain\', \'dataset\',\n                \'pretrain/{}\'.format(dataset if if_real_data else \'oracle_data\')]\n    if not if_test:\n        dir_list.extend([save_root, save_samples_root, save_model_root])\n    for d in dir_list:\n        if not os.path.exists(d):\n            os.makedirs(d)\n'"
main.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : config.py\n# @Time         : Created at 2019-03-18\n# @Blog         : http://zhiweil.ml/\n# @Description  :\n# Copyrights (C) 2018. All Rights Reserved.\nfrom __future__ import print_function\n\nimport argparse\n\nimport config as cfg\nfrom utils.text_process import load_test_dict, text_process\n\n\ndef program_config(parser):\n    # Program\n    parser.add_argument('--if_test', default=cfg.if_test, type=int)\n    parser.add_argument('--run_model', default=cfg.run_model, type=str)\n    parser.add_argument('--k_label', default=cfg.k_label, type=int)\n    parser.add_argument('--dataset', default=cfg.dataset, type=str)\n    parser.add_argument('--model_type', default=cfg.model_type, type=str)\n    parser.add_argument('--loss_type', default=cfg.loss_type, type=str)\n    parser.add_argument('--if_real_data', default=cfg.if_real_data, type=int)\n    parser.add_argument('--cuda', default=cfg.CUDA, type=int)\n    parser.add_argument('--device', default=cfg.device, type=int)\n    parser.add_argument('--shuffle', default=cfg.data_shuffle, type=int)\n    parser.add_argument('--gen_init', default=cfg.gen_init, type=str)\n    parser.add_argument('--dis_init', default=cfg.dis_init, type=str)\n\n    # Basic Train\n    parser.add_argument('--samples_num', default=cfg.samples_num, type=int)\n    parser.add_argument('--vocab_size', default=cfg.vocab_size, type=int)\n    parser.add_argument('--mle_epoch', default=cfg.MLE_train_epoch, type=int)\n    parser.add_argument('--clas_pre_epoch', default=cfg.PRE_clas_epoch, type=int)\n    parser.add_argument('--adv_epoch', default=cfg.ADV_train_epoch, type=int)\n    parser.add_argument('--inter_epoch', default=cfg.inter_epoch, type=int)\n    parser.add_argument('--batch_size', default=cfg.batch_size, type=int)\n    parser.add_argument('--max_seq_len', default=cfg.max_seq_len, type=int)\n    parser.add_argument('--start_letter', default=cfg.start_letter, type=int)\n    parser.add_argument('--padding_idx', default=cfg.padding_idx, type=int)\n    parser.add_argument('--gen_lr', default=cfg.gen_lr, type=float)\n    parser.add_argument('--gen_adv_lr', default=cfg.gen_adv_lr, type=float)\n    parser.add_argument('--dis_lr', default=cfg.dis_lr, type=float)\n    parser.add_argument('--clip_norm', default=cfg.clip_norm, type=float)\n    parser.add_argument('--pre_log_step', default=cfg.pre_log_step, type=int)\n    parser.add_argument('--adv_log_step', default=cfg.adv_log_step, type=int)\n    parser.add_argument('--train_data', default=cfg.train_data, type=str)\n    parser.add_argument('--test_data', default=cfg.test_data, type=str)\n    parser.add_argument('--temp_adpt', default=cfg.temp_adpt, type=str)\n    parser.add_argument('--temperature', default=cfg.temperature, type=int)\n    parser.add_argument('--ora_pretrain', default=cfg.oracle_pretrain, type=int)\n    parser.add_argument('--gen_pretrain', default=cfg.gen_pretrain, type=int)\n    parser.add_argument('--dis_pretrain', default=cfg.dis_pretrain, type=int)\n\n    # Generator\n    parser.add_argument('--adv_g_step', default=cfg.ADV_g_step, type=int)\n    parser.add_argument('--rollout_num', default=cfg.rollout_num, type=int)\n    parser.add_argument('--gen_embed_dim', default=cfg.gen_embed_dim, type=int)\n    parser.add_argument('--gen_hidden_dim', default=cfg.gen_hidden_dim, type=int)\n    parser.add_argument('--goal_size', default=cfg.goal_size, type=int)\n    parser.add_argument('--step_size', default=cfg.step_size, type=int)\n    parser.add_argument('--mem_slots', default=cfg.mem_slots, type=int)\n    parser.add_argument('--num_heads', default=cfg.num_heads, type=int)\n    parser.add_argument('--head_size', default=cfg.head_size, type=int)\n\n    # Discriminator\n    parser.add_argument('--d_step', default=cfg.d_step, type=int)\n    parser.add_argument('--d_epoch', default=cfg.d_epoch, type=int)\n    parser.add_argument('--adv_d_step', default=cfg.ADV_d_step, type=int)\n    parser.add_argument('--adv_d_epoch', default=cfg.ADV_d_epoch, type=int)\n    parser.add_argument('--dis_embed_dim', default=cfg.dis_embed_dim, type=int)\n    parser.add_argument('--dis_hidden_dim', default=cfg.dis_hidden_dim, type=int)\n    parser.add_argument('--num_rep', default=cfg.num_rep, type=int)\n\n    # Metrics\n    parser.add_argument('--use_nll_oracle', default=cfg.use_nll_oracle, type=int)\n    parser.add_argument('--use_nll_gen', default=cfg.use_nll_gen, type=int)\n    parser.add_argument('--use_nll_div', default=cfg.use_nll_div, type=int)\n    parser.add_argument('--use_bleu', default=cfg.use_bleu, type=int)\n    parser.add_argument('--use_self_bleu', default=cfg.use_self_bleu, type=int)\n    parser.add_argument('--use_clas_acc', default=cfg.use_clas_acc, type=int)\n    parser.add_argument('--use_ppl', default=cfg.use_ppl, type=int)\n\n    # Log\n    parser.add_argument('--log_file', default=cfg.log_filename, type=str)\n    parser.add_argument('--save_root', default=cfg.save_root, type=str)\n    parser.add_argument('--signal_file', default=cfg.signal_file, type=str)\n    parser.add_argument('--tips', default=cfg.tips, type=str)\n\n    return parser\n\n\n# MAIN\nif __name__ == '__main__':\n    # Hyper Parameters\n    parser = argparse.ArgumentParser()\n    parser = program_config(parser)\n    opt = parser.parse_args()\n\n    if opt.if_real_data:\n        opt.max_seq_len, opt.vocab_size = text_process('dataset/' + opt.dataset + '.txt')\n        cfg.extend_vocab_size = len(load_test_dict(opt.dataset)[0])  # init classifier vocab_size\n    cfg.init_param(opt)\n    opt.save_root = cfg.save_root\n    opt.train_data = cfg.train_data\n    opt.test_data = cfg.test_data\n\n    # ===Dict===\n    if cfg.if_real_data:\n        from instructor.real_data.seqgan_instructor import SeqGANInstructor\n        from instructor.real_data.leakgan_instructor import LeakGANInstructor\n        from instructor.real_data.maligan_instructor import MaliGANInstructor\n        from instructor.real_data.jsdgan_instructor import JSDGANInstructor\n        from instructor.real_data.relgan_instructor import RelGANInstructor\n        from instructor.real_data.sentigan_instructor import SentiGANInstructor\n\n    else:\n        from instructor.oracle_data.seqgan_instructor import SeqGANInstructor\n        from instructor.oracle_data.leakgan_instructor import LeakGANInstructor\n        from instructor.oracle_data.maligan_instructor import MaliGANInstructor\n        from instructor.oracle_data.jsdgan_instructor import JSDGANInstructor\n        from instructor.oracle_data.relgan_instructor import RelGANInstructor\n        from instructor.oracle_data.sentigan_instructor import SentiGANInstructor\n\n    instruction_dict = {\n        'seqgan': SeqGANInstructor,\n        'leakgan': LeakGANInstructor,\n        'maligan': MaliGANInstructor,\n        'jsdgan': JSDGANInstructor,\n        'relgan': RelGANInstructor,\n        'sentigan': SentiGANInstructor,\n    }\n\n    inst = instruction_dict[cfg.run_model](opt)\n    if not cfg.if_test:\n        inst._run()\n    else:\n        inst._test()\n"""
metrics/basic.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : basic.py\n# @Time         : Created at 2019-05-14\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nfrom abc import abstractmethod\n\n\nclass Metrics:\n    def __init__(self, name='Metric'):\n        self.name = name\n\n    def get_name(self):\n        return self.name\n\n    def set_name(self, name):\n        self.name = name\n\n    @abstractmethod\n    def get_score(self):\n        pass\n\n    @abstractmethod\n    def reset(self):\n        pass\n"""
metrics/bleu.py,0,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : bleu.py\n# @Time         : Created at 2019-05-31\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\nfrom multiprocessing import Pool\n\nimport nltk\nimport os\nimport random\nfrom nltk.translate.bleu_score import SmoothingFunction\n\nfrom metrics.basic import Metrics\n\n\nclass BLEU(Metrics):\n    def __init__(self, name=None, test_text=None, real_text=None, gram=3, portion=1, if_use=False):\n        assert type(gram) == int or type(gram) == list, \'Gram format error!\'\n        super(BLEU, self).__init__(\'%s-%s\' % (name, gram))\n\n        self.if_use = if_use\n        self.test_text = test_text\n        self.real_text = real_text\n        self.gram = [gram] if type(gram) == int else gram\n        self.sample_size = 200  # BLEU scores remain nearly unchanged for self.sample_size >= 200\n        self.reference = None\n        self.is_first = True\n        self.portion = portion  # how many portions to use in the evaluation, default to use the whole test dataset\n\n    def get_score(self, is_fast=True, given_gram=None):\n        """"""\n        Get BLEU scores.\n        :param is_fast: Fast mode\n        :param given_gram: Calculate specific n-gram BLEU score\n        """"""\n        if not self.if_use:\n            return 0\n        if self.is_first:\n            self.get_reference()\n            self.is_first = False\n        if is_fast:\n            return self.get_bleu_fast(given_gram)\n        return self.get_bleu(given_gram)\n\n    def reset(self, test_text=None, real_text=None):\n        self.test_text = test_text if test_text else self.test_text\n        self.real_text = real_text if real_text else self.real_text\n\n    def get_reference(self):\n        reference = self.real_text.copy()\n\n        # randomly choose a portion of test data\n        # In-place shuffle\n        random.shuffle(reference)\n        len_ref = len(reference)\n        reference = reference[:int(self.portion * len_ref)]\n        self.reference = reference\n        return reference\n\n    def get_bleu(self, given_gram=None):\n        if given_gram is not None:  # for single gram\n            bleu = list()\n            reference = self.get_reference()\n            weight = tuple((1. / given_gram for _ in range(given_gram)))\n            for idx, hypothesis in enumerate(self.test_text[:self.sample_size]):\n                bleu.append(self.cal_bleu(reference, hypothesis, weight))\n            return round(sum(bleu) / len(bleu), 3)\n        else:  # for multiple gram\n            all_bleu = []\n            for ngram in self.gram:\n                bleu = list()\n                reference = self.get_reference()\n                weight = tuple((1. / ngram for _ in range(ngram)))\n                for idx, hypothesis in enumerate(self.test_text[:self.sample_size]):\n                    bleu.append(self.cal_bleu(reference, hypothesis, weight))\n                all_bleu.append(round(sum(bleu) / len(bleu), 3))\n            return all_bleu\n\n    @staticmethod\n    def cal_bleu(reference, hypothesis, weight):\n        return nltk.translate.bleu_score.sentence_bleu(reference, hypothesis, weight,\n                                                       smoothing_function=SmoothingFunction().method1)\n\n    def get_bleu_fast(self, given_gram=None):\n        reference = self.get_reference()\n        if given_gram is not None:  # for single gram\n            return self.get_bleu_parallel(ngram=given_gram, reference=reference)\n        else:  # for multiple gram\n            all_bleu = []\n            for ngram in self.gram:\n                all_bleu.append(self.get_bleu_parallel(ngram=ngram, reference=reference))\n            return all_bleu\n\n    def get_bleu_parallel(self, ngram, reference):\n        weight = tuple((1. / ngram for _ in range(ngram)))\n        pool = Pool(os.cpu_count())\n        result = list()\n        for idx, hypothesis in enumerate(self.test_text[:self.sample_size]):\n            result.append(pool.apply_async(self.cal_bleu, args=(reference, hypothesis, weight)))\n        score = 0.0\n        cnt = 0\n        for i in result:\n            score += i.get()\n            cnt += 1\n        pool.close()\n        pool.join()\n        return round(score / cnt, 3)\n'"
metrics/clas_acc.py,2,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : clas_acc.py\n# @Time         : Created at 2019/12/4\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\n\nfrom metrics.basic import Metrics\n\n\nclass ACC(Metrics):\n    def __init__(self, if_use=True, gpu=True):\n        super(ACC, self).__init__('clas_acc')\n\n        self.if_use = if_use\n        self.model = None\n        self.data_loader = None\n        self.gpu = gpu\n\n    def get_score(self):\n        if not self.if_use:\n            return 0\n        assert self.model and self.data_loader, 'Need to reset() before get_score()!'\n\n        return self.cal_acc(self.model, self.data_loader)\n\n    def reset(self, model=None, data_loader=None):\n        self.model = model\n        self.data_loader = data_loader\n\n    def cal_acc(self, model, data_loader):\n        total_acc = 0\n        total_num = 0\n        with torch.no_grad():\n            for i, data in enumerate(data_loader):\n                inp, target = data['input'], data['target']\n                if self.gpu:\n                    inp, target = inp.cuda(), target.cuda()\n\n                pred = model.forward(inp)\n                total_acc += torch.sum((pred.argmax(dim=-1) == target)).item()\n                total_num += inp.size(0)\n        return round(total_acc / total_num, 4)\n"""
metrics/nll.py,5,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : nll.py\n# @Time         : Created at 2019-05-31\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.nn as nn\n\nimport config as cfg\nfrom metrics.basic import Metrics\n\n\nclass NLL(Metrics):\n    def __init__(self, name, if_use=False, gpu=False):\n        super(NLL, self).__init__(name)\n\n        self.if_use = if_use\n        self.model = None\n        self.data_loader = None\n        self.label_i = None\n        self.leak_dis = None\n        self.gpu = gpu\n        self.criterion = nn.NLLLoss()\n\n    def get_score(self):\n        """"""note that NLL score need the updated model and data loader each time, use reset() before get_score()""""""\n        if not self.if_use:\n            return 0\n        assert self.model and self.data_loader, \'Need to reset() before get_score()!\'\n\n        if self.leak_dis is not None:  # For LeakGAN\n            return self.cal_nll_with_leak_dis(self.model, self.data_loader, self.leak_dis, self.gpu)\n        elif self.label_i is not None:  # For category text generation\n            return self.cal_nll_with_label(self.model, self.data_loader, self.label_i,\n                                           self.criterion, self.gpu)\n        else:\n            return self.cal_nll(self.model, self.data_loader, self.criterion, self.gpu)\n\n    def reset(self, model=None, data_loader=None, label_i=None, leak_dis=None):\n        self.model = model\n        self.data_loader = data_loader\n        self.label_i = label_i\n        self.leak_dis = leak_dis\n\n    @staticmethod\n    def cal_nll(model, data_loader, criterion, gpu=cfg.CUDA):\n        """"""NLL score for general text generation model.""""""\n        total_loss = 0\n        with torch.no_grad():\n            for i, data in enumerate(data_loader):\n                inp, target = data[\'input\'], data[\'target\']\n                if gpu:\n                    inp, target = inp.cuda(), target.cuda()\n\n                hidden = model.init_hidden(data_loader.batch_size)\n                pred = model.forward(inp, hidden)\n                loss = criterion(pred, target.view(-1))\n                total_loss += loss.item()\n        return round(total_loss / len(data_loader), 4)\n\n    @staticmethod\n    def cal_nll_with_label(model, data_loader, label_i, criterion, gpu=cfg.CUDA):\n        """"""NLL score for category text generation model.""""""\n        assert type(label_i) == int, \'missing label\'\n        total_loss = 0\n        with torch.no_grad():\n            for i, data in enumerate(data_loader):\n                inp, target = data[\'input\'], data[\'target\']\n                label = torch.LongTensor([label_i] * data_loader.batch_size)\n                if gpu:\n                    inp, target, label = inp.cuda(), target.cuda(), label.cuda()\n\n                hidden = model.init_hidden(data_loader.batch_size)\n                if model.name == \'oracle\':\n                    pred = model.forward(inp, hidden)\n                else:\n                    pred = model.forward(inp, hidden, label)\n                loss = criterion(pred, target.view(-1))\n                total_loss += loss.item()\n        return round(total_loss / len(data_loader), 4)\n\n    @staticmethod\n    def cal_nll_with_leak_dis(model, data_loader, leak_dis, gpu=cfg.CUDA):\n        """"""NLL score for LeakGAN.""""""\n        total_loss = 0\n        with torch.no_grad():\n            for i, data in enumerate(data_loader):\n                inp, target = data[\'input\'], data[\'target\']\n                if gpu:\n                    inp, target = inp.cuda(), target.cuda()\n\n                loss = model.batchNLLLoss(target, leak_dis)\n                total_loss += loss.item()\n        return round(total_loss / len(data_loader), 4)\n'"
metrics/ppl.py,0,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : ppl.py\n# @Time         : Created at 2019/12/5\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\nimport string\n\nimport math\nimport numpy as np\nimport os\nimport random\n\nimport config as cfg\nfrom metrics.basic import Metrics\nfrom utils.text_process import write_tokens\n\nkenlm_path = \'/home/zhiwei/kenlm\'  # specify the kenlm path\n\n\nclass PPL(Metrics):\n    def __init__(self, train_data, test_data, n_gram=5, if_use=False):\n        """"""\n        Calculate Perplexity scores, including forward and reverse.\n        PPL-F: PPL_forward, PPL-R: PPL_reverse\n        @param train_data: train_data (GenDataIter)\n        @param test_data: test_data (GenDataIter)\n        @param n_gram: calculate with n-gram\n        @param if_use: if use\n        """"""\n        super(PPL, self).__init__(\'[PPL-F, PPL-R]\')\n\n        self.n_gram = n_gram\n        self.if_use = if_use\n\n        self.gen_tokens = None\n        self.train_data = train_data\n        self.test_data = test_data\n\n    def get_score(self):\n        if not self.if_use:\n            return 0\n        return self.cal_ppl()\n\n    def reset(self, gen_tokens=None):\n        self.gen_tokens = gen_tokens\n\n    def cal_ppl(self):\n        save_path = os.path.join(""/tmp"", \'\'.join(random.choice(\n            string.ascii_uppercase + string.digits) for _ in range(6)))\n        output_path = save_path + "".arpa""\n\n        write_tokens(save_path, self.gen_tokens)  # save to file\n\n        # forward ppl\n        for_lm = self.train_ngram_lm(kenlm_path=kenlm_path, data_path=cfg.test_data,\n                                     output_path=output_path, n_gram=self.n_gram)\n        for_ppl = self.get_ppl(for_lm, self.gen_tokens)\n\n        # reverse ppl\n        try:\n            rev_lm = self.train_ngram_lm(kenlm_path=kenlm_path, data_path=save_path,\n                                         output_path=output_path, n_gram=self.n_gram)\n\n            rev_ppl = self.get_ppl(rev_lm, self.test_data.tokens)\n        except:\n            # Note: Only after the generator is trained few epochs, the reverse ppl can be calculated.\n            rev_ppl = 0\n\n        return [for_ppl, rev_ppl]\n\n    def train_ngram_lm(self, kenlm_path, data_path, output_path, n_gram):\n        """"""\n        Trains a modified Kneser-Ney n-gram KenLM from a text file.\n        Creates a .arpa file to store n-grams.\n        """"""\n        import kenlm\n        import subprocess\n\n        # create .arpa and .bin file of n-grams\n        curdir = os.path.abspath(os.path.curdir)\n        cd_command = ""cd "" + os.path.join(kenlm_path, \'build\')\n        command_1 = ""bin/lmplz -o {} <{} >{} --discount_fallback &"".format(str(n_gram), os.path.join(curdir, data_path),\n                                                                           output_path)\n        command_2 = ""bin/build_binary -s {} {} &"".format(output_path, output_path + "".bin"")\n\n        while True:\n            subprocess.getstatusoutput(cd_command + "" && "" + command_1)  # call without logging output\n            subprocess.getstatusoutput(cd_command + "" && "" + command_2)  # call without logging output\n            if os.path.exists(output_path + "".bin""):\n                break\n\n        # create language model\n        model = kenlm.Model(output_path + "".bin"")\n\n        return model\n\n    def get_ppl(self, lm, tokens):\n        """"""\n        Assume sentences is a list of strings (space delimited sentences)\n        """"""\n        total_nll = 0\n        total_wc = 0\n        for words in tokens:\n            nll = np.sum([-math.log(math.pow(10.0, score))\n                          for score, _, _ in lm.full_scores(\' \'.join(words), bos=True, eos=False)])\n            total_wc += len(words)\n            total_nll += nll\n        ppl = np.exp(total_nll / total_wc)\n        return round(ppl, 4)\n'"
models/JSDGAN_G.py,9,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : JSDGAN_G.py\n# @Time         : Created at 2019/11/17\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\n\nimport torch\nimport torch.nn.functional as F\n\nfrom models.generator import LSTMGenerator\n\n\nclass JSDGAN_G(LSTMGenerator):\n    def __init__(self, mem_slots, num_heads, head_size, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx,\n                 gpu=False):\n        super(JSDGAN_G, self).__init__(embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu)\n        self.name = \'jsdgan\'\n\n        # RMC\n\n    #     self.hidden_dim = mem_slots * num_heads * head_size\n    #     self.lstm = RelationalMemory(mem_slots=mem_slots, head_size=head_size, input_size=embedding_dim,\n    #                                  num_heads=num_heads, return_all_outputs=True)\n    #     self.lstm2out = nn.Linear(self.hidden_dim, vocab_size)\n    #\n    # def init_hidden(self, batch_size=cfg.batch_size):\n    #     """"""init RMC memory""""""\n    #     memory = self.lstm.initial_state(batch_size)\n    #     memory = self.lstm.repackage_hidden(memory)  # detch memory at first\n    #     return memory.cuda() if self.gpu else memory\n\n    def JSD_loss(self, inp, target):\n        """"""\n        Returns a JSDGAN loss\n\n        :param inp: batch_size x seq_len, inp should be target with <s> (start letter) prepended\n        :param target: batch_size x seq_len\n        :return loss: loss to optimize\n        """"""\n        batch_size, seq_len = inp.size()\n        hidden = self.init_hidden(batch_size)\n        pred = self.forward(inp, hidden).view(batch_size, self.max_seq_len, self.vocab_size)\n        target_onehot = F.one_hot(target, self.vocab_size).float()  # batch_size * seq_len * vocab_size\n        pred = torch.sum(pred * target_onehot, dim=-1)  # batch_size * seq_len\n\n        # calculate probabilities of sentences\n        prob_gen = torch.exp(torch.sum(pred, dim=-1).double())  # sum of log prob\n        prob_gen = self.min_max_normal(prob_gen).clamp(min=1e-10)\n        prob_data = torch.DoubleTensor([1 / batch_size] * prob_gen.size(0))\n        if self.gpu:\n            prob_data = prob_data.cuda()\n\n        # calculate the reward\n        reward = torch.log(1. - torch.div(prob_data, prob_data + prob_gen))  # batch_size\n\n        # check if nan\n        if torch.isnan(reward).sum() > 0:\n            print(\'Reward is nan!!!\')\n            exit(1)\n\n        loss = torch.sum((prob_gen * reward).detach() * torch.sum(pred.double(), dim=-1))\n\n        return loss\n\n    def min_max_normal(self, prob):\n        return torch.div(prob - torch.min(prob), torch.clamp(torch.max(prob) - torch.min(prob), min=1e-78))\n\n    def sigmoid_normal(self, prob):\n        """"""push prob either close to 0 or 1""""""\n        return torch.sigmoid((prob - 0.5) * 20)\n'"
models/LeakGAN_D.py,0,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : LeakGAN_D.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nfrom models.discriminator import CNNDiscriminator\n\ndis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\ndis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n\n\nclass LeakGAN_D(CNNDiscriminator):\n    def __init__(self, embed_dim, vocab_size, padding_idx, gpu=False, dropout=0.2):\n        super(LeakGAN_D, self).__init__(embed_dim, vocab_size, dis_filter_sizes, dis_num_filters, padding_idx,\n                                        gpu, dropout)\n'"
models/LeakGAN_G.py,32,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : LeakGAN_G.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\nimport math\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport config as cfg\nfrom utils.helpers import truncated_normal_\n\ndis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\ngoal_out_size = sum(dis_num_filters)\n\n\nclass LeakGAN_G(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, goal_size,\n                 step_size, gpu=False):\n        super(LeakGAN_G, self).__init__()\n        self.name = \'leakgan\'\n\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.max_seq_len = max_seq_len\n        self.vocab_size = vocab_size\n        self.padding_idx = padding_idx\n        self.goal_size = goal_size\n        self.goal_out_size = goal_out_size  # equals to total_num_filters\n        self.step_size = step_size\n        self.gpu = gpu\n        self.temperature = 1.5\n\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n        self.worker = nn.LSTM(embedding_dim, hidden_dim)\n        self.manager = nn.LSTM(goal_out_size, hidden_dim)\n\n        self.work2goal = nn.Linear(hidden_dim, vocab_size * goal_size)\n        self.mana2goal = nn.Linear(hidden_dim, goal_out_size)\n        self.goal2goal = nn.Linear(goal_out_size, goal_size, bias=False)\n\n        self.goal_init = nn.Parameter(torch.rand((cfg.batch_size, goal_out_size)))\n\n        self.init_params()\n\n    def forward(self, idx, inp, work_hidden, mana_hidden, feature, real_goal, no_log=False, train=False):\n        """"""\n        Embeds input and sample on token at a time (seq_len = 1)\n\n        :param idx: index of current token in sentence\n        :param inp: [batch_size]\n        :param work_hidden: 1 * batch_size * hidden_dim\n        :param mana_hidden: 1 * batch_size * hidden_dim\n        :param feature: 1 * batch_size * total_num_filters, feature of current sentence\n        :param real_goal: batch_size * goal_out_size, real_goal in LeakGAN source code\n        :param no_log: no log operation\n        :param train: if train\n\n        :return: out, cur_goal, work_hidden, mana_hidden\n            - out: batch_size * vocab_size\n            - cur_goal: batch_size * 1 * goal_out_size\n        """"""\n        emb = self.embeddings(inp).unsqueeze(0)  # 1 * batch_size * embed_dim\n\n        # Manager\n        mana_out, mana_hidden = self.manager(feature, mana_hidden)  # mana_out: 1 * batch_size * hidden_dim\n        mana_out = self.mana2goal(mana_out.permute([1, 0, 2]))  # batch_size * 1 * goal_out_size\n        cur_goal = F.normalize(mana_out, dim=-1)\n        _real_goal = self.goal2goal(real_goal)  # batch_size * goal_size\n        _real_goal = F.normalize(_real_goal, p=2, dim=-1).unsqueeze(-1)  # batch_size * goal_size * 1\n\n        # Worker\n        work_out, work_hidden = self.worker(emb, work_hidden)  # work_out: 1 * batch_size * hidden_dim\n        work_out = self.work2goal(work_out).view(-1, self.vocab_size,\n                                                 self.goal_size)  # batch_size * vocab_size * goal_size\n\n        # Sample token\n        out = torch.matmul(work_out, _real_goal).squeeze(-1)  # batch_size * vocab_size\n\n        # Temperature control\n        if idx > 1:\n            if train:\n                temperature = 1.0\n            else:\n                temperature = self.temperature\n        else:\n            temperature = self.temperature\n\n        out = temperature * out\n\n        if no_log:\n            out = F.softmax(out, dim=-1)\n        else:\n            out = F.log_softmax(out, dim=-1)\n\n        return out, cur_goal, work_hidden, mana_hidden\n\n    def sample(self, num_samples, batch_size, dis, start_letter=cfg.start_letter, train=False):\n        """"""\n        Samples the network and returns num_samples samples of length max_seq_len.\n        :return: samples: batch_size * max_seq_len\n        """"""\n        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n        samples = torch.zeros(num_batch * batch_size, self.max_seq_len).long()  # larger than num_samples\n        fake_sentences = torch.zeros((batch_size, self.max_seq_len))\n\n        for b in range(num_batch):\n            leak_sample, _, _, _ = self.forward_leakgan(fake_sentences, dis, if_sample=True, no_log=False\n                                                        , start_letter=start_letter, train=False)\n\n            assert leak_sample.shape == (batch_size, self.max_seq_len)\n            samples[b * batch_size:(b + 1) * batch_size, :] = leak_sample\n\n        samples = samples[:num_samples, :]\n\n        return samples  # cut to num_samples\n\n    def pretrain_loss(self, target, dis, start_letter=cfg.start_letter):\n        """"""\n        Returns the pretrain_generator Loss for predicting target sequence.\n\n        Inputs: target, dis, start_letter\n            - target: batch_size * seq_len\n\n        """"""\n        batch_size, seq_len = target.size()\n        _, feature_array, goal_array, leak_out_array = self.forward_leakgan(target, dis, if_sample=False, no_log=False,\n                                                                            start_letter=start_letter)\n\n        # Manager loss\n        mana_cos_loss = self.manager_cos_loss(batch_size, feature_array,\n                                              goal_array)  # batch_size * (seq_len / step_size)\n        manager_loss = -torch.sum(mana_cos_loss) / (batch_size * (seq_len // self.step_size))\n\n        # Worker loss\n        work_nll_loss = self.worker_nll_loss(target, leak_out_array)  # batch_size * seq_len\n        work_loss = torch.sum(work_nll_loss) / (batch_size * seq_len)\n\n        return manager_loss, work_loss\n\n    def adversarial_loss(self, target, rewards, dis, start_letter=cfg.start_letter):\n        """"""\n        Returns a pseudo-loss that gives corresponding policy gradients (on calling .backward()).\n        Inspired by the example in http://karpathy.github.io/2016/05/31/rl/\n\n        Inputs: target, rewards, dis, start_letter\n            - target: batch_size * seq_len\n            - rewards: batch_size * seq_len (discriminator rewards for each token)\n        """"""\n        batch_size, seq_len = target.size()\n        _, feature_array, goal_array, leak_out_array = self.forward_leakgan(target, dis, if_sample=False, no_log=False,\n                                                                            start_letter=start_letter, train=True)\n\n        # Manager Loss\n        t0 = time.time()\n        mana_cos_loss = self.manager_cos_loss(batch_size, feature_array,\n                                              goal_array)  # batch_size * (seq_len / step_size)\n        mana_loss = -torch.sum(rewards * mana_cos_loss) / (batch_size * (seq_len // self.step_size))\n\n        # Worker Loss\n        work_nll_loss = self.worker_nll_loss(target, leak_out_array)  # batch_size * seq_len\n        work_cos_reward = self.worker_cos_reward(feature_array, goal_array)  # batch_size * seq_len\n        work_loss = -torch.sum(work_nll_loss * work_cos_reward) / (batch_size * seq_len)\n\n        return mana_loss, work_loss\n\n    def manager_cos_loss(self, batch_size, feature_array, goal_array):\n        """"""\n        Get manager cosine distance loss\n\n        :return cos_loss: batch_size * (seq_len / step_size)\n        """"""\n        # ===My implements===\n        # offset_feature = feature_array[:, 4:, :]\n        # # \xe4\xb8\x8d\xe8\xae\xb0\xe5\xbd\x95\xe6\x9c\x80\xe5\x90\x8e\xe5\x9b\x9b\xe4\xb8\xaafeature\xe7\x9a\x84\xe5\x8f\x98\xe5\x8c\x96\n        # all_feature = feature_array[:, :-4, :]\n        # all_goal = goal_array[:, :-4, :]\n        # sub_feature = offset_feature - all_feature\n        #\n        # # L2 normalization\n        # sub_feature = F.normalize(sub_feature, p=2, dim=-1)\n        # all_goal = F.normalize(all_goal, p=2, dim=-1)\n        #\n        # cos_loss = F.cosine_similarity(sub_feature, all_goal, dim=-1)  # batch_size * (seq_len - 4)\n        #\n        # return cos_loss\n\n        # ===LeakGAN origin===\n        # get sub_feature and real_goal\n        # batch_size, seq_len = sentences.size()\n        sub_feature = torch.zeros(batch_size, self.max_seq_len // self.step_size, self.goal_out_size)\n        real_goal = torch.zeros(batch_size, self.max_seq_len // self.step_size, self.goal_out_size)\n        for i in range(self.max_seq_len // self.step_size):\n            idx = i * self.step_size\n            sub_feature[:, i, :] = feature_array[:, idx + self.step_size, :] - feature_array[:, idx, :]\n\n            if i == 0:\n                real_goal[:, i, :] = self.goal_init[:batch_size, :]\n            else:\n                idx = (i - 1) * self.step_size + 1\n                real_goal[:, i, :] = torch.sum(goal_array[:, idx:idx + 4, :], dim=1)\n\n        # L2 noramlization\n        sub_feature = F.normalize(sub_feature, p=2, dim=-1)\n        real_goal = F.normalize(real_goal, p=2, dim=-1)\n\n        cos_loss = F.cosine_similarity(sub_feature, real_goal, dim=-1)\n\n        return cos_loss\n\n    def worker_nll_loss(self, target, leak_out_array):\n        """"""\n        Get NLL loss for worker\n\n        :return loss: batch_size * seq_len\n        """"""\n        loss_fn = nn.NLLLoss(reduction=\'none\')\n        loss = loss_fn(leak_out_array.permute([0, 2, 1]), target)\n\n        return loss\n\n    def worker_cos_reward(self, feature_array, goal_array):\n        """"""\n        Get reward for worker (cosine distance)\n\n        :return: cos_loss: batch_size * seq_len\n        """"""\n        for i in range(int(self.max_seq_len / self.step_size)):\n            real_feature = feature_array[:, i * self.step_size, :].unsqueeze(1).expand((-1, self.step_size, -1))\n            feature_array[:, i * self.step_size:(i + 1) * self.step_size, :] = real_feature\n            if i > 0:\n                sum_goal = torch.sum(goal_array[:, (i - 1) * self.step_size:i * self.step_size, :], dim=1, keepdim=True)\n            else:\n                sum_goal = goal_array[:, 0, :].unsqueeze(1)\n            goal_array[:, i * self.step_size:(i + 1) * self.step_size, :] = sum_goal.expand((-1, self.step_size, -1))\n\n        offset_feature = feature_array[:, 1:, :]  # f_{t+1}, batch_size * seq_len * goal_out_size\n        goal_array = goal_array[:, :self.max_seq_len, :]  # batch_size * seq_len * goal_out_size\n        sub_feature = offset_feature - goal_array\n\n        # L2 normalization\n        sub_feature = F.normalize(sub_feature, p=2, dim=-1)\n        all_goal = F.normalize(goal_array, p=2, dim=-1)\n\n        cos_loss = F.cosine_similarity(sub_feature, all_goal, dim=-1)  # batch_size * seq_len\n        return cos_loss\n\n    def forward_leakgan(self, sentences, dis, if_sample, no_log=False, start_letter=cfg.start_letter, train=False):\n        """"""\n        Get all feature and goals according to given sentences\n        :param sentences: batch_size * max_seq_len, not include start token\n        :param dis: discriminator model\n        :param if_sample: if use to sample token\n        :param no_log: if use log operation\n        :param start_letter:\n        :param train: if use temperature parameter\n        :return samples, feature_array, goal_array, leak_out_array:\n            - samples: batch_size * max_seq_len\n            - feature_array: batch_size * (max_seq_len + 1) * total_num_filter\n            - goal_array: batch_size * (max_seq_len + 1) * goal_out_size\n            - leak_out_array: batch_size * max_seq_len * vocab_size\n        """"""\n        batch_size, seq_len = sentences.size()\n\n        feature_array = torch.zeros((batch_size, seq_len + 1, self.goal_out_size))\n        goal_array = torch.zeros((batch_size, seq_len + 1, self.goal_out_size))\n        leak_out_array = torch.zeros((batch_size, seq_len + 1, self.vocab_size))\n\n        samples = torch.zeros(batch_size, seq_len + 1).long()\n        work_hidden = self.init_hidden(batch_size)\n        mana_hidden = self.init_hidden(batch_size)\n        leak_inp = torch.LongTensor([start_letter] * batch_size)\n        # dis_inp = torch.LongTensor([start_letter] * batch_size)\n        real_goal = self.goal_init[:batch_size, :]\n\n        if self.gpu:\n            feature_array = feature_array.cuda()\n            goal_array = goal_array.cuda()\n            leak_out_array = leak_out_array.cuda()\n\n        goal_array[:, 0, :] = real_goal  # g0 = goal_init\n        for i in range(seq_len + 1):\n            # Get feature\n            if if_sample:\n                dis_inp = samples[:, :seq_len]\n            else:  # to get feature and goal\n                dis_inp = torch.zeros(batch_size, seq_len).long()\n                if i > 0:\n                    dis_inp[:, :i] = sentences[:, :i]  # cut sentences\n                    leak_inp = sentences[:, i - 1]\n\n            if self.gpu:\n                dis_inp = dis_inp.cuda()\n                leak_inp = leak_inp.cuda()\n            feature = dis.get_feature(dis_inp).unsqueeze(0)  # !!!note: 1 * batch_size * total_num_filters\n\n            feature_array[:, i, :] = feature.squeeze(0)\n\n            # Get output of one token\n            # cur_goal: batch_size * 1 * goal_out_size\n            out, cur_goal, work_hidden, mana_hidden = self.forward(i, leak_inp, work_hidden, mana_hidden, feature,\n                                                                   real_goal, no_log=no_log, train=train)\n            leak_out_array[:, i, :] = out\n\n            # ===My implement according to paper===\n            # Update real_goal and save goal\n            # if 0 < i < 4:  # not update when i=0\n            #     real_goal = torch.sum(goal_array, dim=1)  # num_samples * goal_out_size\n            # elif i >= 4:\n            #     real_goal = torch.sum(goal_array[:, i - 4:i, :], dim=1)\n            # if i > 0:\n            #     goal_array[:, i, :] = cur_goal.squeeze(1)  # !!!note: save goal after update last_goal\n            # ===LeakGAN origin===\n            # Save goal and update real_goal\n            goal_array[:, i, :] = cur_goal.squeeze(1)\n            if i > 0 and i % self.step_size == 0:\n                real_goal = torch.sum(goal_array[:, i - 3:i + 1, :], dim=1)\n                if i / self.step_size == 1:\n                    real_goal += self.goal_init[:batch_size, :]\n\n            # Sample one token\n            if not no_log:\n                out = torch.exp(out)\n            out = torch.multinomial(out, 1).view(-1)  # [batch_size] (sampling from each row)\n            samples[:, i] = out.data\n            leak_inp = out\n\n        # cut to seq_len\n        samples = samples[:, :seq_len]\n        leak_out_array = leak_out_array[:, :seq_len, :]\n        return samples, feature_array, goal_array, leak_out_array\n\n    def batchNLLLoss(self, target, dis, start_letter=cfg.start_letter):\n        # loss_fn = nn.NLLLoss()\n        # batch_size, seq_len = target.size()\n        _, _, _, leak_out_array = self.forward_leakgan(target, dis, if_sample=False, no_log=False,\n                                                       start_letter=start_letter)\n\n        nll_loss = torch.mean(self.worker_nll_loss(target, leak_out_array))\n\n        return nll_loss\n\n    def init_hidden(self, batch_size=1):\n        h = torch.zeros(1, batch_size, self.hidden_dim)\n        c = torch.zeros(1, batch_size, self.hidden_dim)\n\n        if self.gpu:\n            return h.cuda(), c.cuda()\n        else:\n            return h, c\n\n    def init_goal(self, batch_size):\n        goal = torch.rand((batch_size, self.goal_out_size)).normal_(std=0.1)\n        goal = nn.Parameter(goal)\n\n        if self.gpu:\n            return goal.cuda()\n        else:\n            return goal\n\n    def split_params(self):\n        mana_params = list()\n        work_params = list()\n\n        mana_params += list(self.manager.parameters())\n        mana_params += list(self.mana2goal.parameters())\n        mana_params.append(self.goal_init)\n\n        work_params += list(self.embeddings.parameters())\n        work_params += list(self.worker.parameters())\n        work_params += list(self.work2goal.parameters())\n        work_params += list(self.goal2goal.parameters())\n\n        return mana_params, work_params\n\n    def init_params(self):\n        for param in self.parameters():\n            if param.requires_grad and len(param.shape) > 0:\n                stddev = 1 / math.sqrt(param.shape[0])\n                if cfg.gen_init == \'uniform\':\n                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n                elif cfg.gen_init == \'normal\':\n                    torch.nn.init.normal_(param, std=stddev)\n                elif cfg.gen_init == \'truncated_normal\':\n                    truncated_normal_(param, std=stddev)\n'"
models/MaliGAN_D.py,0,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : MaliGAN_D.py\n# @Time         : Created at 2019/10/17\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nfrom models.discriminator import CNNDiscriminator\n\ndis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\ndis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n\n\nclass MaliGAN_D(CNNDiscriminator):\n    def __init__(self, embed_dim, vocab_size, padding_idx, gpu=False, dropout=0.25):\n        super(MaliGAN_D, self).__init__(embed_dim, vocab_size, dis_filter_sizes, dis_num_filters, padding_idx, gpu,\n                                        dropout)\n'"
models/MaliGAN_G.py,3,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : MaliGAN_G.py\n# @Time         : Created at 2019/10/17\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.nn.functional as F\n\nfrom models.generator import LSTMGenerator\n\n\nclass MaliGAN_G(LSTMGenerator):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n        super(MaliGAN_G, self).__init__(embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu)\n        self.name = \'maligan\'\n\n\n    def adv_loss(self, inp, target, reward):\n        """"""\n        Returns a MaliGAN loss\n\n        :param inp: batch_size x seq_len, inp should be target with <s> (start letter) prepended\n        :param target: batch_size x seq_len\n        :param reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding sentence)\n        :return loss: policy loss\n        """"""\n\n        batch_size, seq_len = inp.size()\n        hidden = self.init_hidden(batch_size)\n\n        out = self.forward(inp, hidden).view(batch_size, self.max_seq_len, self.vocab_size)\n        target_onehot = F.one_hot(target, self.vocab_size).float()  # batch_size * seq_len * vocab_size\n        pred = torch.sum(out * target_onehot, dim=-1)  # batch_size * seq_len\n        loss = -torch.sum(pred * reward)\n\n        return loss\n'"
models/Oracle.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : Oracle.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nfrom models.generator import LSTMGenerator\n\n\nclass Oracle(LSTMGenerator):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n        super(Oracle, self).__init__(embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu)\n        self.name = 'oracle'\n\n        # initialise oracle network with N(0,1)\n        # otherwise variance of initialisation is very small => high NLL for loader sampled from the same model\n        self.init_oracle()\n"""
models/RelGAN_D.py,4,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : RelGAN_D.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom models.discriminator import CNNDiscriminator\n\ndis_filter_sizes = [2, 3, 4, 5]\ndis_num_filters = [300, 300, 300, 300]\n\n\nclass RelGAN_D(CNNDiscriminator):\n    def __init__(self, embed_dim, max_seq_len, num_rep, vocab_size, padding_idx, gpu=False, dropout=0.25):\n        super(RelGAN_D, self).__init__(embed_dim, vocab_size, dis_filter_sizes, dis_num_filters, padding_idx,\n                                       gpu, dropout)\n\n        self.embed_dim = embed_dim\n        self.max_seq_len = max_seq_len\n        self.feature_dim = sum(dis_num_filters)\n        self.emb_dim_single = int(embed_dim / num_rep)\n\n        self.embeddings = nn.Linear(vocab_size, embed_dim, bias=False)\n\n        self.convs = nn.ModuleList([\n            nn.Conv2d(1, n, (f, self.emb_dim_single), stride=(1, self.emb_dim_single)) for (n, f) in\n            zip(dis_num_filters, dis_filter_sizes)\n        ])\n\n        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n        self.feature2out = nn.Linear(self.feature_dim, 100)\n        self.out2logits = nn.Linear(100, 1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.init_params()\n\n    def forward(self, inp):\n        """"""\n        Get logits of discriminator\n        :param inp: batch_size * seq_len * vocab_size\n        :return logits: [batch_size * num_rep] (1-D tensor)\n        """"""\n        emb = self.embeddings(inp).unsqueeze(1)  # batch_size * 1 * max_seq_len * embed_dim\n\n        cons = [F.relu(conv(emb)) for conv in self.convs]  # [batch_size * num_filter * (seq_len-k_h+1) * num_rep]\n        pools = [F.max_pool2d(con, (con.size(2), 1)).squeeze(2) for con in cons]  # [batch_size * num_filter * num_rep]\n        pred = torch.cat(pools, 1)\n        pred = pred.permute(0, 2, 1).contiguous().view(-1, self.feature_dim)  # (batch_size * num_rep) * feature_dim\n        highway = self.highway(pred)\n        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n\n        pred = self.feature2out(self.dropout(pred))\n        logits = self.out2logits(pred).squeeze(1)  # [batch_size * num_rep]\n\n        return logits\n'"
models/RelGAN_G.py,11,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : RelGAN_G.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport config as cfg\nfrom models.generator import LSTMGenerator\nfrom models.relational_rnn_general import RelationalMemory\n\n\nclass RelGAN_G(LSTMGenerator):\n    def __init__(self, mem_slots, num_heads, head_size, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx,\n                 gpu=False):\n        super(RelGAN_G, self).__init__(embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu)\n        self.name = \'relgan\'\n\n        self.temperature = 1.0  # init value is 1.0\n\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n        if cfg.model_type == \'LSTM\':\n            # LSTM\n            self.hidden_dim = hidden_dim\n            self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, batch_first=True)\n            self.lstm2out = nn.Linear(self.hidden_dim, vocab_size)\n        else:\n            # RMC\n            self.hidden_dim = mem_slots * num_heads * head_size\n            self.lstm = RelationalMemory(mem_slots=mem_slots, head_size=head_size, input_size=embedding_dim,\n                                         num_heads=num_heads, return_all_outputs=True)\n            self.lstm2out = nn.Linear(self.hidden_dim, vocab_size)\n\n        self.init_params()\n        pass\n\n    def init_hidden(self, batch_size=cfg.batch_size):\n        if cfg.model_type == \'LSTM\':\n            h = torch.zeros(1, batch_size, self.hidden_dim)\n            c = torch.zeros(1, batch_size, self.hidden_dim)\n\n            if self.gpu:\n                return h.cuda(), c.cuda()\n            else:\n                return h, c\n        else:\n            """"""init RMC memory""""""\n            memory = self.lstm.initial_state(batch_size)\n            memory = self.lstm.repackage_hidden(memory)  # detch memory at first\n            return memory.cuda() if self.gpu else memory\n\n    def step(self, inp, hidden):\n        """"""\n        RelGAN step forward\n        :param inp: [batch_size]\n        :param hidden: memory size\n        :return: pred, hidden, next_token, next_token_onehot, next_o\n            - pred: batch_size * vocab_size, use for adversarial training backward\n            - hidden: next hidden\n            - next_token: [batch_size], next sentence token\n            - next_token_onehot: batch_size * vocab_size, not used yet\n            - next_o: batch_size * vocab_size, not used yet\n        """"""\n        emb = self.embeddings(inp).unsqueeze(1)\n        out, hidden = self.lstm(emb, hidden)\n        gumbel_t = self.add_gumbel(self.lstm2out(out.squeeze(1)))\n        next_token = torch.argmax(gumbel_t, dim=1).detach()\n        # next_token_onehot = F.one_hot(next_token, cfg.vocab_size).float()  # not used yet\n        next_token_onehot = None\n\n        pred = F.softmax(gumbel_t * self.temperature, dim=-1)  # batch_size * vocab_size\n        # next_o = torch.sum(next_token_onehot * pred, dim=1)  # not used yet\n        next_o = None\n\n        return pred, hidden, next_token, next_token_onehot, next_o\n\n    def sample(self, num_samples, batch_size, one_hot=False, start_letter=cfg.start_letter):\n        """"""\n        Sample from RelGAN Generator\n        - one_hot: if return pred of RelGAN, used for adversarial training\n        :return:\n            - all_preds: batch_size * seq_len * vocab_size, only use for a batch\n            - samples: all samples\n        """"""\n        global all_preds\n        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n        samples = torch.zeros(num_batch * batch_size, self.max_seq_len).long()\n        if one_hot:\n            all_preds = torch.zeros(batch_size, self.max_seq_len, self.vocab_size)\n            if self.gpu:\n                all_preds = all_preds.cuda()\n\n        for b in range(num_batch):\n            hidden = self.init_hidden(batch_size)\n            inp = torch.LongTensor([start_letter] * batch_size)\n            if self.gpu:\n                inp = inp.cuda()\n\n            for i in range(self.max_seq_len):\n                pred, hidden, next_token, _, _ = self.step(inp, hidden)\n                samples[b * batch_size:(b + 1) * batch_size, i] = next_token\n                if one_hot:\n                    all_preds[:, i] = pred\n                inp = next_token\n        samples = samples[:num_samples]  # num_samples * seq_len\n\n        if one_hot:\n            return all_preds  # batch_size * seq_len * vocab_size\n        return samples\n\n    @staticmethod\n    def add_gumbel(o_t, eps=1e-10, gpu=cfg.CUDA):\n        """"""Add o_t by a vector sampled from Gumbel(0,1)""""""\n        u = torch.zeros(o_t.size())\n        if gpu:\n            u = u.cuda()\n\n        u.uniform_(0, 1)\n        g_t = -torch.log(-torch.log(u + eps) + eps)\n        gumbel_t = o_t + g_t\n        return gumbel_t\n'"
models/SentiGAN_D.py,1,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : SentiGAN_D.py\n# @Time         : Created at 2019-07-26\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch.nn as nn\n\nfrom models.discriminator import CNNDiscriminator, CNNClassifier\n\ndis_filter_sizes = [2, 3, 4, 5]\ndis_num_filters = [200, 200, 200, 200]\n\nclas_filter_sizes = [2, 3, 4, 5]\nclas_num_filters = [200]\n\n\nclass SentiGAN_D(CNNDiscriminator):\n    def __init__(self, k_label, embed_dim, vocab_size, padding_idx, gpu=False, dropout=0.2):\n        super(SentiGAN_D, self).__init__(embed_dim, vocab_size, dis_filter_sizes, dis_num_filters, padding_idx, gpu,\n                                         dropout)\n\n        self.feature2out = nn.Linear(self.feature_dim, k_label + 1)\n\n        self.init_params()\n\n\n# Classifier\nclass SentiGAN_C(CNNClassifier):\n    def __init__(self, k_label, embed_dim, max_seq_len, num_rep, vocab_size, padding_idx, gpu=False, dropout=0.25):\n        super(SentiGAN_C, self).__init__(k_label, embed_dim, max_seq_len, num_rep, vocab_size, clas_filter_sizes,\n                                         clas_num_filters, padding_idx, gpu, dropout)\n\n        # Use Glove\n        # self.embeddings.from_pretrained(build_embedding_matrix(cfg.dataset))\n'"
models/SentiGAN_G.py,3,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : SentiGAN_G.py\n# @Time         : Created at 2019-07-26\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\n\nimport torch\nimport torch.nn.functional as F\n\nfrom models.generator import LSTMGenerator\n\n\nclass SentiGAN_G(LSTMGenerator):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n        super(SentiGAN_G, self).__init__(embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu)\n        self.name = \'sentigan\'\n\n    def forward(self, inp, hidden, need_hidden=False, use_log=True):\n        """"""\n        Embeds input and applies LSTM\n        :param inp: batch_size * seq_len\n        :param hidden: (h, c)\n        :param need_hidden: if return hidden, use for sampling\n        """"""\n        emb = self.embeddings(inp)  # batch_size * len * embedding_dim\n        if len(inp.size()) == 1:\n            emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim\n\n        out, hidden = self.lstm(emb, hidden)  # out: batch_size * seq_len * hidden_dim\n        out = out.contiguous().view(-1, self.hidden_dim)  # out: (batch_size * len) * hidden_dim\n        out = self.lstm2out(out)  # batch_size * seq_len * vocab_size\n        # out = self.temperature * out  # temperature\n        if use_log:\n            pred = F.log_softmax(out, dim=-1)\n        else:\n            pred = F.softmax(out, dim=-1)\n\n        if need_hidden:\n            return pred, hidden\n        else:\n            return pred\n\n    def batchPGLoss(self, inp, target, reward):\n        """"""\n        Returns a policy gradient loss\n\n        :param inp: batch_size x seq_len, inp should be target with <s> (start letter) prepended\n        :param target: batch_size x seq_len\n        :param reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding sentence)\n        :return loss: policy loss\n        """"""\n\n        batch_size, seq_len = inp.size()\n        hidden = self.init_hidden(batch_size)\n\n        out = self.forward(inp, hidden, use_log=False).view(batch_size, self.max_seq_len, self.vocab_size)\n        target_onehot = F.one_hot(target, self.vocab_size).float()  # batch_size * seq_len * vocab_size\n        pred = torch.sum(out * target_onehot, dim=-1)  # batch_size * seq_len\n        loss = -torch.sum(pred * (1 - reward))\n\n        return loss\n'"
models/SeqGAN_D.py,0,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : SeqGAN_D.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nfrom models.discriminator import CNNDiscriminator\n\ndis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\ndis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n\n\nclass SeqGAN_D(CNNDiscriminator):\n    def __init__(self, embed_dim, vocab_size, padding_idx, gpu=False, dropout=0.25):\n        super(SeqGAN_D, self).__init__(embed_dim, vocab_size, dis_filter_sizes, dis_num_filters, padding_idx, gpu,\n                                       dropout)\n'"
models/SeqGAN_G.py,3,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : SeqGAN_G.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.nn.functional as F\n\nfrom models.generator import LSTMGenerator\n\n\nclass SeqGAN_G(LSTMGenerator):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n        super(SeqGAN_G, self).__init__(embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu)\n        self.name = \'seqgan\'\n\n    def batchPGLoss(self, inp, target, reward):\n        """"""\n        Returns a policy gradient loss\n\n        :param inp: batch_size x seq_len, inp should be target with <s> (start letter) prepended\n        :param target: batch_size x seq_len\n        :param reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding sentence)\n        :return loss: policy loss\n        """"""\n\n        batch_size, seq_len = inp.size()\n        hidden = self.init_hidden(batch_size)\n\n        out = self.forward(inp, hidden).view(batch_size, self.max_seq_len, self.vocab_size)\n        target_onehot = F.one_hot(target, self.vocab_size).float()  # batch_size * seq_len * vocab_size\n        pred = torch.sum(out * target_onehot, dim=-1)  # batch_size * seq_len\n        loss = -torch.sum(pred * reward)\n\n        return loss\n'"
models/discriminator.py,13,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : config.py\n# @Time         : Created at 2019-03-18\n# @Blog         : http://zhiweil.ml/\n# @Description  :\n# Copyrights (C) 2018. All Rights Reserved.\nimport math\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport config as cfg\nfrom utils.helpers import truncated_normal_\n\n\nclass CNNDiscriminator(nn.Module):\n    def __init__(self, embed_dim, vocab_size, filter_sizes, num_filters, padding_idx, gpu=False,\n                 dropout=0.2):\n        super(CNNDiscriminator, self).__init__()\n        self.embedding_dim = embed_dim\n        self.vocab_size = vocab_size\n        self.padding_idx = padding_idx\n        self.feature_dim = sum(num_filters)\n        self.gpu = gpu\n\n        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n        self.convs = nn.ModuleList([\n            nn.Conv2d(1, n, (f, embed_dim)) for (n, f) in zip(num_filters, filter_sizes)\n        ])\n        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n        self.feature2out = nn.Linear(self.feature_dim, 2)\n        self.dropout = nn.Dropout(dropout)\n\n        self.init_params()\n\n    def forward(self, inp):\n        """"""\n        Get final predictions of discriminator\n        :param inp: batch_size * seq_len\n        :return: pred: batch_size * 2\n        """"""\n        feature = self.get_feature(inp)\n        pred = self.feature2out(self.dropout(feature))\n\n        return pred\n\n    def get_feature(self, inp):\n        """"""\n        Get feature vector of given sentences\n        :param inp: batch_size * max_seq_len\n        :return: batch_size * feature_dim\n        """"""\n        emb = self.embeddings(inp).unsqueeze(1)  # batch_size * 1 * max_seq_len * embed_dim\n        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]  # [batch_size * num_filter]\n        pred = torch.cat(pools, 1)  # tensor: batch_size * feature_dim\n        highway = self.highway(pred)\n        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n\n        return pred\n\n    def init_params(self):\n        for param in self.parameters():\n            if param.requires_grad and len(param.shape) > 0:\n                stddev = 1 / math.sqrt(param.shape[0])\n                if cfg.dis_init == \'uniform\':\n                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n                elif cfg.dis_init == \'normal\':\n                    torch.nn.init.normal_(param, std=stddev)\n                elif cfg.dis_init == \'truncated_normal\':\n                    truncated_normal_(param, std=stddev)\n\n\nclass GRUDiscriminator(nn.Module):\n\n    def __init__(self, embedding_dim, vocab_size, hidden_dim, feature_dim, max_seq_len, padding_idx,\n                 gpu=False, dropout=0.2):\n        super(GRUDiscriminator, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.max_seq_len = max_seq_len\n        self.padding_idx = padding_idx\n        self.gpu = gpu\n\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=dropout)\n        self.gru2hidden = nn.Linear(2 * 2 * hidden_dim, feature_dim)\n        self.feature2out = nn.Linear(feature_dim, 2)\n        self.dropout = nn.Dropout(dropout)\n\n        self.init_params()\n\n    def init_hidden(self, batch_size):\n        h = autograd.Variable(torch.zeros(2 * 2 * 1, batch_size, self.hidden_dim))\n\n        if self.gpu:\n            return h.cuda()\n        else:\n            return h\n\n    def forward(self, inp):\n        """"""\n        Get final feature of discriminator\n        :param inp: batch_size * seq_len\n        :return pred: batch_size * 2\n        """"""\n        feature = self.get_feature(inp)\n        pred = self.feature2out(self.dropout(feature))\n\n        return pred\n\n    def get_feature(self, inp):\n        """"""\n        Get feature vector of given sentences\n        :param inp: batch_size * max_seq_len\n        :return: batch_size * feature_dim\n        """"""\n        hidden = self.init_hidden(inp.size(0))\n\n        emb = self.embeddings(input)  # batch_size * seq_len * embedding_dim\n        emb = emb.permute(1, 0, 2)  # seq_len * batch_size * embedding_dim\n        _, hidden = self.gru(emb, hidden)  # 4 * batch_size * hidden_dim\n        hidden = hidden.permute(1, 0, 2).contiguous()  # batch_size * 4 * hidden_dim\n        out = self.gru2hidden(hidden.view(-1, 4 * self.hidden_dim))  # batch_size * 4 * hidden_dim\n        feature = torch.tanh(out)  # batch_size * feature_dim\n\n        return feature\n\n    def init_params(self):\n        for param in self.parameters():\n            if param.requires_grad and len(param.shape) > 0:\n                stddev = 1 / math.sqrt(param.shape[0])\n                if cfg.dis_init == \'uniform\':\n                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n                elif cfg.dis_init == \'normal\':\n                    torch.nn.init.normal_(param, std=stddev)\n                elif cfg.dis_init == \'truncated_normal\':\n                    truncated_normal_(param, std=stddev)\n\n\n# Classifier\nclass CNNClassifier(CNNDiscriminator):\n    def __init__(self, k_label, embed_dim, max_seq_len, num_rep, vocab_size, filter_sizes, num_filters, padding_idx,\n                 gpu=False, dropout=0.25):\n        super(CNNClassifier, self).__init__(embed_dim, vocab_size, filter_sizes, num_filters, padding_idx,\n                                            gpu, dropout)\n\n        self.k_label = k_label\n        self.embed_dim = embed_dim\n        self.max_seq_len = max_seq_len\n        self.feature_dim = sum(num_filters)\n        self.emb_dim_single = int(embed_dim / num_rep)\n\n        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n\n        self.convs = nn.ModuleList([\n            nn.Conv2d(1, n, (f, embed_dim)) for (n, f) in zip(num_filters, filter_sizes)\n        ])  # vanilla\n        # self.convs = nn.ModuleList([\n        #     nn.Conv2d(1, n, (f, self.emb_dim_single), stride=(1, self.emb_dim_single)) for (n, f) in\n        #     zip(num_filters, filter_sizes)\n        # ])  # RelGAN\n\n        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n        self.feature2out = nn.Linear(self.feature_dim, 100)\n        self.out2logits = nn.Linear(100, k_label)  # vanilla\n        # self.out2logits = nn.Linear(num_rep * 100, k_label) # RelGAN\n        self.dropout = nn.Dropout(dropout)\n\n        self.init_params()\n\n    def forward(self, inp):\n        """"""\n        Get logits of discriminator\n        :param inp: batch_size * seq_len * vocab_size\n        :return logits: [batch_size * num_rep] (1-D tensor)\n        """"""\n        emb = self.embeddings(inp).unsqueeze(1)  # batch_size * 1 * max_seq_len * embed_dim\n\n        # vanilla\n        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]  # [batch_size * num_filter]\n        # RelGAN\n        # cons = [F.relu(conv(emb)) for conv in self.convs]  # [batch_size * num_filter * (seq_len-k_h+1) * num_rep]\n        # pools = [F.max_pool2d(con, (con.size(2), 1)).squeeze(2) for con in cons]  # [batch_size * num_filter * num_rep]\n\n        pred = torch.cat(pools, 1)  # batch_size * feature_dim\n        # pred = pred.permute(0, 2, 1).contiguous().view(-1, self.feature_dim)    # RelGAN\n        highway = self.highway(pred)\n        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway, same dim\n\n        pred = self.feature2out(self.dropout(pred))\n        logits = self.out2logits(self.dropout(pred)).squeeze(1)  # vanilla, batch_size * k_label\n        # logits = self.out2logits(self.dropout(pred.view(inp.size(0), -1))).squeeze(1)  # RelGAN, batch_size * k_label\n\n        return logits\n'"
models/generator.py,9,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : config.py\n# @Time         : Created at 2019-03-18\n# @Blog         : http://zhiweil.ml/\n# @Description  :\n# Copyrights (C) 2018. All Rights Reserved.\nimport math\nimport torch\nimport torch.nn as nn\n\nimport config as cfg\nfrom utils.helpers import truncated_normal_\n\n\nclass LSTMGenerator(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n        super(LSTMGenerator, self).__init__()\n        self.name = \'vanilla\'\n\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.max_seq_len = max_seq_len\n        self.vocab_size = vocab_size\n        self.padding_idx = padding_idx\n        self.gpu = gpu\n\n        self.temperature = 1.0\n\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.lstm2out = nn.Linear(hidden_dim, vocab_size)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n        self.init_params()\n\n    def forward(self, inp, hidden, need_hidden=False):\n        """"""\n        Embeds input and applies LSTM\n        :param inp: batch_size * seq_len\n        :param hidden: (h, c)\n        :param need_hidden: if return hidden, use for sampling\n        """"""\n        emb = self.embeddings(inp)  # batch_size * len * embedding_dim\n        if len(inp.size()) == 1:\n            emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim\n\n        out, hidden = self.lstm(emb, hidden)  # out: batch_size * seq_len * hidden_dim\n        out = out.contiguous().view(-1, self.hidden_dim)  # out: (batch_size * len) * hidden_dim\n        out = self.lstm2out(out)  # (batch_size * seq_len) * vocab_size\n        # out = self.temperature * out  # temperature\n        pred = self.softmax(out)\n\n        if need_hidden:\n            return pred, hidden\n        else:\n            return pred\n\n    def sample(self, num_samples, batch_size, start_letter=cfg.start_letter):\n        """"""\n        Samples the network and returns num_samples samples of length max_seq_len.\n        :return samples: num_samples * max_seq_length (a sampled sequence in each row)\n        """"""\n        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n        samples = torch.zeros(num_batch * batch_size, self.max_seq_len).long()\n\n        # Generate sentences with multinomial sampling strategy\n        for b in range(num_batch):\n            hidden = self.init_hidden(batch_size)\n            inp = torch.LongTensor([start_letter] * batch_size)\n            if self.gpu:\n                inp = inp.cuda()\n\n            for i in range(self.max_seq_len):\n                out, hidden = self.forward(inp, hidden, need_hidden=True)  # out: batch_size * vocab_size\n                next_token = torch.multinomial(torch.exp(out), 1)  # batch_size * 1 (sampling from each row)\n                samples[b * batch_size:(b + 1) * batch_size, i] = next_token.view(-1)\n                inp = next_token.view(-1)\n        samples = samples[:num_samples]\n\n        return samples\n\n    def init_params(self):\n        for param in self.parameters():\n            if param.requires_grad and len(param.shape) > 0:\n                stddev = 1 / math.sqrt(param.shape[0])\n                if cfg.gen_init == \'uniform\':\n                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n                elif cfg.gen_init == \'normal\':\n                    torch.nn.init.normal_(param, std=stddev)\n                elif cfg.gen_init == \'truncated_normal\':\n                    truncated_normal_(param, std=stddev)\n\n    def init_oracle(self):\n        for param in self.parameters():\n            if param.requires_grad:\n                torch.nn.init.normal_(param, mean=0, std=1)\n\n    def init_hidden(self, batch_size=cfg.batch_size):\n        h = torch.zeros(1, batch_size, self.hidden_dim)\n        c = torch.zeros(1, batch_size, self.hidden_dim)\n\n        if self.gpu:\n            return h.cuda(), c.cuda()\n        else:\n            return h, c\n'"
models/relational_rnn_general.py,20,"b'import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\n# this class largely follows the official sonnet implementation\n# https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py\n\n\nclass RelationalMemory(nn.Module):\n    """"""\n    Constructs a `RelationalMemory` object.\n    This class is same as the RMC from relational_rnn_models.py, but without language modeling-specific variables.\n    Args:\n      mem_slots: The total number of memory slots to use.\n      head_size: The size of an attention head.\n      input_size: The size of input per step. i.e. the dimension of each input vector\n      num_heads: The number of attention heads to use. Defaults to 1.\n      num_blocks: Number of times to compute attention per time step. Defaults\n        to 1.\n      forget_bias: Bias to use for the forget gate, assuming we are using\n        some form of gating. Defaults to 1.\n      input_bias: Bias to use for the input gate, assuming we are using\n        some form of gating. Defaults to 0.\n      gate_style: Whether to use per-element gating (\'unit\'),\n        per-memory slot gating (\'memory\'), or no gating at all (None).\n        Defaults to `unit`.\n      attention_mlp_layers: Number of layers to use in the post-attention\n        MLP. Defaults to 2.\n      key_size: Size of vector to use for key & query vectors in the attention\n        computation. Defaults to None, in which case we use `head_size`.\n\n      # NEW flag for this class\n      return_all_outputs: Whether the model returns outputs for each step (like seq2seq) or only the final output.\n    Raises:\n      ValueError: gate_style not one of [None, \'memory\', \'unit\'].\n      ValueError: num_blocks is < 1.\n      ValueError: attention_mlp_layers is < 1.\n    """"""\n\n    def __init__(self, mem_slots, head_size, input_size, num_heads=1, num_blocks=1, forget_bias=1., input_bias=0.,\n                 gate_style=\'unit\', attention_mlp_layers=2, key_size=None, return_all_outputs=False):\n        super(RelationalMemory, self).__init__()\n\n        ########## generic parameters for RMC ##########\n        self.mem_slots = mem_slots\n        self.head_size = head_size\n        self.num_heads = num_heads\n        self.mem_size = self.head_size * self.num_heads\n\n        # a new fixed params needed for pytorch port of RMC\n        # +1 is the concatenated input per time step : we do self-attention with the concatenated memory & input\n        # so if the mem_slots = 1, this value is 2\n        self.mem_slots_plus_input = self.mem_slots + 1\n\n        if num_blocks < 1:\n            raise ValueError(\'num_blocks must be >=1. Got: {}.\'.format(num_blocks))\n        self.num_blocks = num_blocks\n\n        if gate_style not in [\'unit\', \'memory\', None]:\n            raise ValueError(\n                \'gate_style must be one of [\\\'unit\\\', \\\'memory\\\', None]. got: \'\n                \'{}.\'.format(gate_style))\n        self.gate_style = gate_style\n\n        if attention_mlp_layers < 1:\n            raise ValueError(\'attention_mlp_layers must be >= 1. Got: {}.\'.format(\n                attention_mlp_layers))\n        self.attention_mlp_layers = attention_mlp_layers\n\n        self.key_size = key_size if key_size else self.head_size\n\n        ########## parameters for multihead attention ##########\n        # value_size is same as head_size\n        self.value_size = self.head_size\n        # total size for query-key-value\n        self.qkv_size = 2 * self.key_size + self.value_size\n        self.total_qkv_size = self.qkv_size * self.num_heads  # denoted as F\n\n        # each head has qkv_sized linear projector\n        # just using one big param is more efficient, rather than this line\n        # self.qkv_projector = [nn.Parameter(torch.randn((self.qkv_size, self.qkv_size))) for _ in range(self.num_heads)]\n        self.qkv_projector = nn.Linear(self.mem_size, self.total_qkv_size)\n        self.qkv_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.total_qkv_size])\n\n        # used for attend_over_memory function\n        self.attention_mlp = nn.ModuleList([nn.Linear(self.mem_size, self.mem_size)] * self.attention_mlp_layers)\n        self.attended_memory_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n        self.attended_memory_layernorm2 = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n\n        ########## parameters for initial embedded input projection ##########\n        self.input_size = input_size\n        self.input_projector = nn.Linear(self.input_size, self.mem_size)\n\n        ########## parameters for gating ##########\n        self.num_gates = 2 * self.calculate_gate_size()\n        self.input_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n        self.memory_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n        # trainable scalar gate bias tensors\n        self.forget_bias = nn.Parameter(torch.tensor(forget_bias, dtype=torch.float32))\n        self.input_bias = nn.Parameter(torch.tensor(input_bias, dtype=torch.float32))\n\n        ########## number of outputs returned #####\n        self.return_all_outputs = return_all_outputs\n\n    def repackage_hidden(self, h):\n        """"""Wraps hidden states in new Tensors, to detach them from their history.""""""\n        # needed for truncated BPTT, called at every batch forward pass\n        if isinstance(h, torch.Tensor):\n            return h.detach()\n        else:\n            return tuple(self.repackage_hidden(v) for v in h)\n\n    def initial_state(self, batch_size, trainable=False):\n        """"""\n        Creates the initial memory.\n        We should ensure each row of the memory is initialized to be unique,\n        so initialize the matrix to be the identity. We then pad or truncate\n        as necessary so that init_state is of size\n        (batch_size, self.mem_slots, self.mem_size).\n        Args:\n          batch_size: The size of the batch.\n          trainable: Whether the initial state is trainable. This is always True.\n        Returns:\n          init_state: A truncated or padded matrix of size\n            (batch_size, self.mem_slots, self.mem_size).\n        """"""\n        init_state = torch.stack([torch.eye(self.mem_slots) for _ in range(batch_size)])\n\n        # pad the matrix with zeros\n        if self.mem_size > self.mem_slots:\n            difference = self.mem_size - self.mem_slots\n            pad = torch.zeros((batch_size, self.mem_slots, difference))\n            init_state = torch.cat([init_state, pad], -1)\n\n        # truncation. take the first \'self.mem_size\' components\n        elif self.mem_size < self.mem_slots:\n            init_state = init_state[:, :, :self.mem_size]\n\n        return init_state\n\n    def multihead_attention(self, memory):\n        """"""\n        Perform multi-head attention from \'Attention is All You Need\'.\n        Implementation of the attention mechanism from\n        https://arxiv.org/abs/1706.03762.\n        Args:\n          memory: Memory tensor to perform attention on.\n        Returns:\n          new_memory: New memory tensor.\n        """"""\n\n        # First, a simple linear projection is used to construct queries\n        qkv = self.qkv_projector(memory)\n        # apply layernorm for every dim except the batch dim\n        qkv = self.qkv_layernorm(qkv)\n\n        # mem_slots needs to be dynamically computed since mem_slots got concatenated with inputs\n        # example: self.mem_slots=10 and seq_length is 3, and then mem_slots is 10 + 1 = 11 for each 3 step forward pass\n        # this is the same as self.mem_slots_plus_input, but defined to keep the sonnet implementation code style\n        mem_slots = memory.shape[1]  # denoted as N\n\n        # split the qkv to multiple heads H\n        # [B, N, F] => [B, N, H, F/H]\n        qkv_reshape = qkv.view(qkv.shape[0], mem_slots, self.num_heads, self.qkv_size)\n\n        # [B, N, H, F/H] => [B, H, N, F/H]\n        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n\n        # [B, H, N, key_size], [B, H, N, key_size], [B, H, N, value_size]\n        q, k, v = torch.split(qkv_transpose, [self.key_size, self.key_size, self.value_size], -1)\n\n        # scale q with d_k, the dimensionality of the key vectors\n        q *= (self.key_size ** -0.5)\n\n        # make it [B, H, N, N]\n        dot_product = torch.matmul(q, k.permute(0, 1, 3, 2))\n        weights = F.softmax(dot_product, dim=-1)\n\n        # output is [B, H, N, V]\n        output = torch.matmul(weights, v)\n\n        # [B, H, N, V] => [B, N, H, V] => [B, N, H*V]\n        output_transpose = output.permute(0, 2, 1, 3).contiguous()\n        new_memory = output_transpose.view((output_transpose.shape[0], output_transpose.shape[1], -1))\n\n        return new_memory\n\n    @property\n    def state_size(self):\n        return [self.mem_slots, self.mem_size]\n\n    @property\n    def output_size(self):\n        return self.mem_slots * self.mem_size\n\n    def calculate_gate_size(self):\n        """"""\n        Calculate the gate size from the gate_style.\n        Returns:\n          The per sample, per head parameter size of each gate.\n        """"""\n        if self.gate_style == \'unit\':\n            return self.mem_size\n        elif self.gate_style == \'memory\':\n            return 1\n        else:  # self.gate_style == None\n            return 0\n\n    def create_gates(self, inputs, memory):\n        """"""\n        Create input and forget gates for this step using `inputs` and `memory`.\n        Args:\n          inputs: Tensor input.\n          memory: The current state of memory.\n        Returns:\n          input_gate: A LSTM-like insert gate.\n          forget_gate: A LSTM-like forget gate.\n        """"""\n        # We\'ll create the input and forget gates at once. Hence, calculate double\n        # the gate size.\n\n        # equation 8: since there is no output gate, h is just a tanh\'ed m\n        memory = torch.tanh(memory)\n\n        # sonnet uses this, but i think it assumes time step of 1 for all cases\n        # if inputs is (B, T, features) where T > 1, this gets incorrect\n        # inputs = inputs.view(inputs.shape[0], -1)\n\n        # fixed implementation\n        if len(inputs.shape) == 3:\n            if inputs.shape[1] > 1:\n                raise ValueError(\n                    ""input seq length is larger than 1. create_gate function is meant to be called for each step, with input seq length of 1"")\n            inputs = inputs.view(inputs.shape[0], -1)\n            # matmul for equation 4 and 5\n            # there is no output gate, so equation 6 is not implemented\n            gate_inputs = self.input_gate_projector(inputs)\n            gate_inputs = gate_inputs.unsqueeze(dim=1)\n            gate_memory = self.memory_gate_projector(memory)\n        else:\n            raise ValueError(""input shape of create_gate function is 2, expects 3"")\n\n        # this completes the equation 4 and 5\n        gates = gate_memory + gate_inputs\n        gates = torch.split(gates, split_size_or_sections=int(gates.shape[2] / 2), dim=2)\n        input_gate, forget_gate = gates\n        assert input_gate.shape[2] == forget_gate.shape[2]\n\n        # to be used for equation 7\n        input_gate = torch.sigmoid(input_gate + self.input_bias)\n        forget_gate = torch.sigmoid(forget_gate + self.forget_bias)\n\n        return input_gate, forget_gate\n\n    def attend_over_memory(self, memory):\n        """"""\n        Perform multiheaded attention over `memory`.\n            Args:\n              memory: Current relational memory.\n            Returns:\n              The attended-over memory.\n        """"""\n        for _ in range(self.num_blocks):\n            attended_memory = self.multihead_attention(memory)\n\n            # Add a skip connection to the multiheaded attention\'s input.\n            memory = self.attended_memory_layernorm(memory + attended_memory)\n\n            # add a skip connection to the attention_mlp\'s input.\n            attention_mlp = memory\n            for i, l in enumerate(self.attention_mlp):\n                attention_mlp = self.attention_mlp[i](attention_mlp)\n                attention_mlp = F.relu(attention_mlp)\n            memory = self.attended_memory_layernorm2(memory + attention_mlp)\n\n        return memory\n\n    def forward_step(self, inputs, memory, treat_input_as_matrix=False):\n        """"""\n        Forward step of the relational memory core.\n        Args:\n          inputs: Tensor input.\n          memory: Memory output from the previous time step.\n          treat_input_as_matrix: Optional, whether to treat `input` as a sequence\n            of matrices. Default to False, in which case the input is flattened\n            into a vector.\n        Returns:\n          output: This time step\'s output.\n          next_memory: The next version of memory to use.\n        """"""\n\n        if treat_input_as_matrix:\n            # keep (Batch, Seq, ...) dim (0, 1), flatten starting from dim 2\n            inputs = inputs.view(inputs.shape[0], inputs.shape[1], -1)\n            # apply linear layer for dim 2\n            inputs_reshape = self.input_projector(inputs)\n        else:\n            # keep (Batch, ...) dim (0), flatten starting from dim 1\n            inputs = inputs.view(inputs.shape[0], -1)\n            # apply linear layer for dim 1\n            inputs = self.input_projector(inputs)\n            # unsqueeze the time step to dim 1\n            inputs_reshape = inputs.unsqueeze(dim=1)\n\n        memory_plus_input = torch.cat([memory, inputs_reshape], dim=1)\n        next_memory = self.attend_over_memory(memory_plus_input)\n\n        # cut out the concatenated input vectors from the original memory slots\n        n = inputs_reshape.shape[1]\n        next_memory = next_memory[:, :-n, :]\n\n        if self.gate_style == \'unit\' or self.gate_style == \'memory\':\n            # these gates are sigmoid-applied ones for equation 7\n            input_gate, forget_gate = self.create_gates(inputs_reshape, memory)\n            # equation 7 calculation\n            next_memory = input_gate * torch.tanh(next_memory)\n            next_memory += forget_gate * memory\n\n        output = next_memory.view(next_memory.shape[0], -1)\n\n        return output, next_memory\n\n    def forward(self, inputs, memory, treat_input_as_matrix=False):\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn\'t, the model would try backpropagating all the way to start of the dataset.\n\n        # for loop implementation of (entire) recurrent forward pass of the model\n        # inputs is batch first [batch, seq], and output logit per step is [batch, vocab]\n        # so the concatenated logits are [seq * batch, vocab]\n\n        # targets are flattened [seq, batch] => [seq * batch], so the dimension is correct\n\n        # memory = self.repackage_hidden(memory)\n        logit = 0\n        logits = []\n        # shape[1] is seq_lenth T\n        for idx_step in range(inputs.shape[1]):\n            logit, memory = self.forward_step(inputs[:, idx_step], memory)\n            logits.append(logit.unsqueeze(1))\n        logits = torch.cat(logits, dim=1)\n\n        if self.return_all_outputs:\n            return logits, memory\n        else:\n            return logit.unsqueeze(1), memory\n\n# ########## DEBUG: unit test code ##########\n# input_size = 32\n# seq_length = 20\n# batch_size = 32\n# num_tokens = 5000\n# model = RelationalMemory(mem_slots=1, head_size=512, input_size=input_size, num_heads=2)\n# model_memory = model.initial_state(batch_size=batch_size)\n#\n# # random input\n# random_input = torch.randn((32, seq_length, input_size))\n# # random targets\n# random_targets = torch.randn((32, seq_length, input_size))\n#\n# # take a one step forward\n# logit, next_memory = model(random_input, model_memory)\n# print(next_memory.shape)\n# print(logit.shape)\n'"
run/run_jsdgan.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : run_jsdgan.py\n# @Time         : Created at 2019/11/29\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport sys\nfrom subprocess import call\n\nimport os\n\n# Job id and gpu_id\nif len(sys.argv) > 2:\n    job_id = int(sys.argv[1])\n    gpu_id = str(sys.argv[2])\n    print('job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\nelif len(sys.argv) > 1:\n    job_id = int(sys.argv[1])\n    gpu_id = 0\n    print('job_id: {}, missing gpu_id (use default {})'.format(job_id, gpu_id))\nelse:\n    job_id = 0\n    gpu_id = 0\n    print('Missing argument: job_id and gpu_id. Use default job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\n\n# Executables\nexecutable = 'python'  # specify your own python interpreter path here\nrootdir = '../'\nscriptname = 'main.py'\n\n# ===Program===\nif_test = int(False)\nrun_model = 'jsdgan'\nCUDA = int(True)\noracle_pretrain = int(True)\ngen_pretrain = int(False)\nMLE_train_epoch = 0  # no pre-training\nADV_train_epoch = 500\ntips = 'JSDGAN experiments'\n\n# ===Oracle  or Real===\nif_real_data = [int(False), int(True), int(True)]\ndataset = ['oracle', 'image_coco', 'emnlp_news']\nvocab_size = [5000, 0, 0]\n\n# ===Basic Param===\ndata_shuffle = int(False)\nmodel_type = 'vanilla'\ngen_init = 'normal'\nsamples_num = 10000\nbatch_size = 64\nmax_seq_len = 20\ngen_lr = 0.01\npre_log_step = 20\nadv_log_step = 5\n\n# ===Generator===\nADV_g_step = 1\ngen_embed_dim = 32\ngen_hidden_dim = 32\n\n# ===Metrics===\nuse_nll_oracle = int(True)\nuse_nll_gen = int(True)\nuse_nll_div = int(True)\nuse_bleu = int(True)\nuse_self_bleu = int(True)\nuse_ppl = int(False)\n\nargs = [\n    # Program\n    '--if_test', if_test,\n    '--run_model', run_model,\n    '--cuda', CUDA,\n    # '--device', gpu_id,  # comment for auto GPU\n    '--ora_pretrain', oracle_pretrain,\n    '--gen_pretrain', gen_pretrain,\n    '--mle_epoch', MLE_train_epoch,\n    '--adv_epoch', ADV_train_epoch,\n    '--tips', tips,\n\n    # Oracle or Real\n    '--if_real_data', if_real_data[job_id],\n    '--dataset', dataset[job_id],\n    '--vocab_size', vocab_size[job_id],\n\n    # Basic Param\n    '--shuffle', data_shuffle,\n    '--model_type', model_type,\n    '--gen_init', gen_init,\n    '--samples_num', samples_num,\n    '--batch_size', batch_size,\n    '--max_seq_len', max_seq_len,\n    '--gen_lr', gen_lr,\n    '--pre_log_step', pre_log_step,\n    '--adv_log_step', adv_log_step,\n\n    # Generator\n    '--adv_g_step', ADV_g_step,\n    '--gen_embed_dim', gen_embed_dim,\n    '--gen_hidden_dim', gen_hidden_dim,\n\n    # Metrics\n    '--use_nll_oracle', use_nll_oracle,\n    '--use_nll_gen', use_nll_gen,\n    '--use_nll_div', use_nll_div,\n    '--use_bleu', use_bleu,\n    '--use_self_bleu', use_self_bleu,\n    '--use_ppl', use_ppl,\n]\n\nargs = list(map(str, args))\nmy_env = os.environ.copy()\ncall([executable, scriptname] + args, env=my_env, cwd=rootdir)\n"""
run/run_leakgan.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : run_leakgan.py\n# @Time         : Created at 2019-05-27\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport sys\nfrom subprocess import call\n\nimport os\n\n# Job id and gpu_id\nif len(sys.argv) > 2:\n    job_id = int(sys.argv[1])\n    gpu_id = str(sys.argv[2])\n    print('job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\nelif len(sys.argv) > 1:\n    job_id = int(sys.argv[1])\n    gpu_id = 0\n    print('job_id: {}, missing gpu_id (use default {})'.format(job_id, gpu_id))\nelse:\n    job_id = 0\n    gpu_id = 0\n    print('Missing argument: job_id and gpu_id. Use default job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\n\n# Executables\nexecutable = 'python'  # specify your own python interpreter path here\nrootdir = '../'\nscriptname = 'main.py'\n\n# ===Program===\nif_test = int(False)\nrun_model = 'leakgan'\nCUDA = int(True)\noracle_pretrain = int(True)\ngen_pretrain = int(False)\ndis_pretrain = int(False)\nMLE_train_epoch = 8\nADV_train_epoch = 200\ninter_epoch = 10\ntips = 'LeakGAN experiments'\n\n# ===Oracle  or Real===\nif_real_data = [int(False), int(True), int(True)]\ndataset = ['oracle', 'image_coco', 'emnlp_news']\nvocab_size = [5000, 0, 0]\n\n# ===Basic Param===\ndata_shuffle = int(False)\nmodel_type = 'vanilla'\ngen_init = 'normal'\ndis_init = 'uniform'\nsamples_num = 10000\nbatch_size = 64\nmax_seq_len = 20\ngen_lr = 0.0015\ndis_lr = 5e-5\npre_log_step = 1\nadv_log_step = 1\n\n# ===Generator===\nADV_g_step = 1\nrollout_num = 4\ngen_embed_dim = 32\ngen_hidden_dim = 32\ngoal_size = 16\nstep_size = 4\n\n# ===Discriminator===\nd_step = 5\nd_epoch = 3\nADV_d_step = 5\nADV_d_epoch = 3\ndis_embed_dim = 64\ndis_hidden_dim = 64\n\n# ===Metrics===\nuse_nll_oracle = int(True)\nuse_nll_gen = int(True)\nuse_nll_div = int(True)\nuse_bleu = int(True)\nuse_self_bleu = int(True)\nuse_ppl = int(False)\n\nargs = [\n    # Program\n    '--if_test', if_test,\n    '--run_model', run_model,\n    '--cuda', CUDA,\n    # '--device', gpu_id,  # comment for auto GPU\n    '--ora_pretrain', oracle_pretrain,\n    '--gen_pretrain', gen_pretrain,\n    '--dis_pretrain', dis_pretrain,\n    '--mle_epoch', MLE_train_epoch,\n    '--adv_epoch', ADV_train_epoch,\n    '--inter_epoch', inter_epoch,\n    '--tips', tips,\n\n    # Oracle or Real\n    '--if_real_data', if_real_data[job_id],\n    '--dataset', dataset[job_id],\n    '--vocab_size', vocab_size[job_id],\n\n    # Basic Param\n    '--shuffle', data_shuffle,\n    '--model_type', model_type,\n    '--gen_init', gen_init,\n    '--dis_init', dis_init,\n    '--samples_num', samples_num,\n    '--batch_size', batch_size,\n    '--max_seq_len', max_seq_len,\n    '--gen_lr', gen_lr,\n    '--dis_lr', dis_lr,\n    '--pre_log_step', pre_log_step,\n    '--adv_log_step', adv_log_step,\n\n    # Generator\n    '--adv_g_step', ADV_g_step,\n    '--rollout_num', rollout_num,\n    '--gen_embed_dim', gen_embed_dim,\n    '--gen_hidden_dim', gen_hidden_dim,\n    '--goal_size', goal_size,\n    '--step_size', step_size,\n\n    # Discriminator\n    '--d_step', d_step,\n    '--d_epoch', d_epoch,\n    '--adv_d_step', ADV_d_step,\n    '--adv_d_epoch', ADV_d_epoch,\n    '--dis_embed_dim', dis_embed_dim,\n    '--dis_hidden_dim', dis_hidden_dim,\n\n    # Metrics\n    '--use_nll_oracle', use_nll_oracle,\n    '--use_nll_gen', use_nll_gen,\n    '--use_nll_div', use_nll_div,\n    '--use_bleu', use_bleu,\n    '--use_self_bleu', use_self_bleu,\n    '--use_ppl', use_ppl,\n]\n\nargs = list(map(str, args))\nmy_env = os.environ.copy()\ncall([executable, scriptname] + args, env=my_env, cwd=rootdir)\n"""
run/run_maligan.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : run_maligan.py\n# @Time         : Created at 2019/11/29\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nfrom subprocess import call\n\nimport os\nimport sys\n\n# Job id and gpu_id\nif len(sys.argv) > 2:\n    job_id = int(sys.argv[1])\n    gpu_id = str(sys.argv[2])\n    print('job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\nelif len(sys.argv) > 1:\n    job_id = int(sys.argv[1])\n    gpu_id = 0\n    print('job_id: {}, missing gpu_id (use default {})'.format(job_id, gpu_id))\nelse:\n    job_id = 0\n    gpu_id = 0\n    print('Missing argument: job_id and gpu_id. Use default job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\n\n# Executables\nexecutable = 'python'  # specify your own python interpreter path here\nrootdir = '../'\nscriptname = 'main.py'\n\n# ===Program===\nif_test = int(False)\nrun_model = 'maligan'\nCUDA = int(True)\noracle_pretrain = int(True)\ngen_pretrain = int(False)\ndis_pretrain = int(False)\nMLE_train_epoch = 80\nADV_train_epoch = 200\ntips = 'MaliGAN experiments'\n\n# ===Oracle  or Real===\nif_real_data = [int(False), int(True), int(True)]\ndataset = ['oracle', 'image_coco', 'emnlp_news']\nvocab_size = [5000, 0, 0]\n\n# ===Basic Param===\ndata_shuffle = int(False)\nmodel_type = 'vanilla'\ngen_init = 'normal'\ndis_init = 'uniform'\nsamples_num = 10000\nbatch_size = 64\nmax_seq_len = 20\ngen_lr = 0.01\ndis_lr = 1e-4\npre_log_step = 10\nadv_log_step = 1\n\n# ===Generator===\nADV_g_step = [50, 1, 1]\nrollout_num = 16\ngen_embed_dim = 32\ngen_hidden_dim = 32\n\n# ===Discriminator===\nd_step = 4\nd_epoch = 2\nADV_d_step = 1\nADV_d_epoch = 3\ndis_embed_dim = 64\ndis_hidden_dim = 64\n\n# ===Metrics===\nuse_nll_oracle = int(True)\nuse_nll_gen = int(True)\nuse_nll_div = int(True)\nuse_bleu = int(True)\nuse_self_bleu = int(True)\nuse_ppl = int(False)\n\nargs = [\n    # Program\n    '--if_test', if_test,\n    '--run_model', run_model,\n    '--cuda', CUDA,\n    # '--device', gpu_id,  # comment for auto GPU\n    '--ora_pretrain', oracle_pretrain,\n    '--gen_pretrain', gen_pretrain,\n    '--dis_pretrain', dis_pretrain,\n    '--mle_epoch', MLE_train_epoch,\n    '--adv_epoch', ADV_train_epoch,\n    '--tips', tips,\n\n    # Oracle or Real\n    '--if_real_data', if_real_data[job_id],\n    '--dataset', dataset[job_id],\n    '--vocab_size', vocab_size[job_id],\n\n    # Basic Param\n    '--shuffle', data_shuffle,\n    '--model_type', model_type,\n    '--gen_init', gen_init,\n    '--dis_init', dis_init,\n    '--samples_num', samples_num,\n    '--batch_size', batch_size,\n    '--max_seq_len', max_seq_len,\n    '--gen_lr', gen_lr,\n    '--dis_lr', dis_lr,\n    '--pre_log_step', pre_log_step,\n    '--adv_log_step', adv_log_step,\n\n    # Generator\n    '--adv_g_step', ADV_g_step[job_id],\n    '--rollout_num', rollout_num,\n    '--gen_embed_dim', gen_embed_dim,\n    '--gen_hidden_dim', gen_hidden_dim,\n\n    # Discriminator\n    '--d_step', d_step,\n    '--d_epoch', d_epoch,\n    '--adv_d_step', ADV_d_step,\n    '--adv_d_epoch', ADV_d_epoch,\n    '--dis_embed_dim', dis_embed_dim,\n    '--dis_hidden_dim', dis_hidden_dim,\n\n    # Metrics\n    '--use_nll_oracle', use_nll_oracle,\n    '--use_nll_gen', use_nll_gen,\n    '--use_nll_div', use_nll_div,\n    '--use_bleu', use_bleu,\n    '--use_self_bleu', use_self_bleu,\n    '--use_ppl', use_ppl,\n]\n\nargs = list(map(str, args))\nmy_env = os.environ.copy()\ncall([executable, scriptname] + args, env=my_env, cwd=rootdir)\n"""
run/run_relgan.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : run_relgan.py\n# @Time         : Created at 2019-05-28\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport sys\nfrom subprocess import call\n\nimport os\n\n# Job id and gpu_id\nif len(sys.argv) > 2:\n    job_id = int(sys.argv[1])\n    gpu_id = str(sys.argv[2])\n    print('job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\nelif len(sys.argv) > 1:\n    job_id = int(sys.argv[1])\n    gpu_id = 0\n    print('job_id: {}, missing gpu_id (use default {})'.format(job_id, gpu_id))\nelse:\n    job_id = 0\n    gpu_id = 0\n    print('Missing argument: job_id and gpu_id. Use default job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\n\n# Executables\nexecutable = 'python'  # specify your own python interpreter path here\nrootdir = '../'\nscriptname = 'main.py'\n\n# ===Program===\nif_test = int(False)\nrun_model = 'relgan'\nCUDA = int(True)\noracle_pretrain = int(True)\ngen_pretrain = int(False)\ndis_pretrain = int(False)\nMLE_train_epoch = 150\nADV_train_epoch = 3000\ntips = 'RelGAN experiments'\n\n# ===Oracle or Real===\nif_real_data = [int(False), int(True), int(True)]\ndataset = ['oracle', 'image_coco', 'emnlp_news']\nloss_type = 'rsgan'\nvocab_size = [5000, 0, 0]\ntemp_adpt = 'exp'\ntemperature = [1, 100, 100]\n\n# ===Basic Param===\ndata_shuffle = int(False)\nmodel_type = 'vanilla'\ngen_init = 'truncated_normal'\ndis_init = 'uniform'\nsamples_num = 10000\nbatch_size = 64\nmax_seq_len = 20\ngen_lr = 0.01\ngen_adv_lr = 1e-4\ndis_lr = 1e-4\npre_log_step = 10\nadv_log_step = 20\n\n# ===Generator===\nADV_g_step = 1\ngen_embed_dim = 32\ngen_hidden_dim = 32\nmem_slots = 1\nnum_heads = 2\nhead_size = 256\n\n# ===Discriminator===\nADV_d_step = 5\ndis_embed_dim = 64\ndis_hidden_dim = 64\nnum_rep = 64\n\n# ===Metrics===\nuse_nll_oracle = int(True)\nuse_nll_gen = int(True)\nuse_nll_div = int(True)\nuse_bleu = int(True)\nuse_self_bleu = int(True)\nuse_ppl = int(False)\n\nargs = [\n    # Program\n    '--if_test', if_test,\n    '--run_model', run_model,\n    '--cuda', CUDA,\n    # '--device', gpu_id,   # comment for auto GPU\n    '--ora_pretrain', oracle_pretrain,\n    '--gen_pretrain', gen_pretrain,\n    '--dis_pretrain', dis_pretrain,\n    '--mle_epoch', MLE_train_epoch,\n    '--adv_epoch', ADV_train_epoch,\n    '--tips', tips,\n\n    # Oracle or Real\n    '--if_real_data', if_real_data[job_id],\n    '--dataset', dataset[job_id],\n    '--loss_type', loss_type,\n    '--vocab_size', vocab_size[job_id],\n    '--temp_adpt', temp_adpt,\n    '--temperature', temperature[job_id],\n\n    # Basic Param\n    '--shuffle', data_shuffle,\n    '--model_type', model_type,\n    '--gen_init', gen_init,\n    '--dis_init', dis_init,\n    '--samples_num', samples_num,\n    '--batch_size', batch_size,\n    '--max_seq_len', max_seq_len,\n    '--gen_lr', gen_lr,\n    '--gen_adv_lr', gen_adv_lr,\n    '--dis_lr', dis_lr,\n    '--pre_log_step', pre_log_step,\n    '--adv_log_step', adv_log_step,\n\n    # Generator\n    '--adv_g_step', ADV_g_step,\n    '--gen_embed_dim', gen_embed_dim,\n    '--gen_hidden_dim', gen_hidden_dim,\n    '--mem_slots', mem_slots,\n    '--num_heads', num_heads,\n    '--head_size', head_size,\n\n    # Discriminator\n    '--adv_d_step', ADV_d_step,\n    '--dis_embed_dim', dis_embed_dim,\n    '--dis_hidden_dim', dis_hidden_dim,\n    '--num_rep', num_rep,\n\n    # Metrics\n    '--use_nll_oracle', use_nll_oracle,\n    '--use_nll_gen', use_nll_gen,\n    '--use_nll_div', use_nll_div,\n    '--use_bleu', use_bleu,\n    '--use_self_bleu', use_self_bleu,\n    '--use_ppl', use_ppl,\n]\n\nargs = list(map(str, args))\nmy_env = os.environ.copy()\ncall([executable, scriptname] + args, env=my_env, cwd=rootdir)\n"""
run/run_sentigan.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : run_sentigan.py\n# @Time         : Created at 2019-05-27\n# @Blog         : http://zhiweil.ml/\n# @Description  :\n# Copyrights (C) 2018. All Rights Reserved.\n\n\nimport sys\nfrom subprocess import call\n\nimport os\n\n# Job id and gpu_id\nif len(sys.argv) > 2:\n    job_id = int(sys.argv[1])\n    gpu_id = str(sys.argv[2])\n    print('job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\nelif len(sys.argv) > 1:\n    job_id = int(sys.argv[1])\n    gpu_id = 0\n    print('job_id: {}, missing gpu_id (use default {})'.format(job_id, gpu_id))\nelse:\n    job_id = 0\n    gpu_id = 0\n    print('Missing argument: job_id and gpu_id. Use default job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\n\n# Executables\nexecutable = 'python'  # specify your own python interpreter path here\nrootdir = '../'\nscriptname = 'main.py'\n\n# ===Program===\nif_test = int(False)\nrun_model = 'sentigan'\nk_label = 2\nCUDA = int(True)\noracle_pretrain = int(True)\ngen_pretrain = int(False)\ndis_pretrain = int(False)\nMLE_train_epoch = 120\nclas_pre_epoch = 5\nADV_train_epoch = 100\ntips = 'SentiGAN experiments'\n\n# ===Oracle or Real===\nif_real_data = [int(False), int(True), int(True)]\ndataset = ['oracle', 'mr15', 'amazon_app_book']\nvocab_size = [5000, 0, 0]\n\n# ===Basic Param===\ndata_shuffle = int(False)\nmodel_type = 'vanilla'\ngen_init = 'normal'\ndis_init = 'uniform'\nsamples_num = 10000\nbatch_size = 64\nmax_seq_len = 20\ngen_lr = 0.01\ndis_lr = 1e-4\npre_log_step = 10\nadv_log_step = 1\n\n# ===Generator===\nADV_g_step = 1\nrollout_num = 16\ngen_embed_dim = 32\ngen_hidden_dim = 32\n\n# ===Discriminator===\nd_step = 5\nd_epoch = 3\nADV_d_step = 4\nADV_d_epoch = 2\ndis_embed_dim = 64\ndis_hidden_dim = 64\n\n# ===Metrics===\nuse_nll_oracle = int(True)\nuse_nll_gen = int(True)\nuse_nll_div = int(True)\nuse_bleu = int(True)\nuse_self_bleu = int(True)\nuse_clas_acc = int(True)\nuse_ppl = int(False)\n\nargs = [\n    # Program\n    '--if_test', if_test,\n    '--run_model', run_model,\n    '--k_label', k_label,\n    '--cuda', CUDA,\n    # '--device', gpu_id,  # comment for auto GPU\n    '--ora_pretrain', oracle_pretrain,\n    '--gen_pretrain', gen_pretrain,\n    '--dis_pretrain', dis_pretrain,\n    '--mle_epoch', MLE_train_epoch,\n    '--clas_pre_epoch', clas_pre_epoch,\n    '--adv_epoch', ADV_train_epoch,\n    '--tips', tips,\n\n    # Oracle or Real\n    '--if_real_data', if_real_data[job_id],\n    '--dataset', dataset[job_id],\n    '--vocab_size', vocab_size[job_id],\n\n    # Basic Param\n    '--shuffle', data_shuffle,\n    '--model_type', model_type,\n    '--gen_init', gen_init,\n    '--dis_init', dis_init,\n    '--samples_num', samples_num,\n    '--batch_size', batch_size,\n    '--max_seq_len', max_seq_len,\n    '--gen_lr', gen_lr,\n    '--dis_lr', dis_lr,\n    '--pre_log_step', pre_log_step,\n    '--adv_log_step', adv_log_step,\n\n    # Generator\n    '--adv_g_step', ADV_g_step,\n    '--rollout_num', rollout_num,\n    '--gen_embed_dim', gen_embed_dim,\n    '--gen_hidden_dim', gen_hidden_dim,\n\n    # Discriminator\n    '--d_step', d_step,\n    '--d_epoch', d_epoch,\n    '--adv_d_step', ADV_d_step,\n    '--adv_d_epoch', ADV_d_epoch,\n    '--dis_embed_dim', dis_embed_dim,\n    '--dis_hidden_dim', dis_hidden_dim,\n\n    # Metrics\n    '--use_nll_oracle', use_nll_oracle,\n    '--use_nll_gen', use_nll_gen,\n    '--use_nll_div', use_nll_div,\n    '--use_bleu', use_bleu,\n    '--use_self_bleu', use_self_bleu,\n    '--use_clas_acc', use_clas_acc,\n    '--use_ppl', use_ppl,\n]\n\nargs = list(map(str, args))\nmy_env = os.environ.copy()\ncall([executable, scriptname] + args, env=my_env, cwd=rootdir)\n"""
run/run_seqgan.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : run_seqgan.py\n# @Time         : Created at 2019-05-27\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport sys\nfrom subprocess import call\n\nimport os\n\n# Job id and gpu_id\nif len(sys.argv) > 2:\n    job_id = int(sys.argv[1])\n    gpu_id = str(sys.argv[2])\n    print('job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\nelif len(sys.argv) > 1:\n    job_id = int(sys.argv[1])\n    gpu_id = 0\n    print('job_id: {}, missing gpu_id (use default {})'.format(job_id, gpu_id))\nelse:\n    job_id = 0\n    gpu_id = 0\n    print('Missing argument: job_id and gpu_id. Use default job_id: {}, gpu_id: {}'.format(job_id, gpu_id))\n\n# Executables\nexecutable = 'python'  # specify your own python interpreter path here\nrootdir = '../'\nscriptname = 'main.py'\n\n# ===Program===\nif_test = int(False)\nrun_model = 'seqgan'\nCUDA = int(True)\noracle_pretrain = int(True)\ngen_pretrain = int(False)\ndis_pretrain = int(False)\nMLE_train_epoch = 120\nADV_train_epoch = 200\ntips = 'SeqGAN experiments'\n\n# ===Oracle  or Real===\nif_real_data = [int(False), int(True), int(True)]\ndataset = ['oracle', 'image_coco', 'emnlp_news']\nvocab_size = [5000, 0, 0]\n\n# ===Basic Param===\ndata_shuffle = int(False)\nmodel_type = 'vanilla'\ngen_init = 'normal'\ndis_init = 'uniform'\nsamples_num = 10000\nbatch_size = 64\nmax_seq_len = 20\ngen_lr = 0.01\ndis_lr = 1e-4\npre_log_step = 10\nadv_log_step = 1\n\n# ===Generator===\nADV_g_step = 1\nrollout_num = 16\ngen_embed_dim = 32\ngen_hidden_dim = 32\n\n# ===Discriminator===\nd_step = 5\nd_epoch = 3\nADV_d_step = 4\nADV_d_epoch = 2\ndis_embed_dim = 64\ndis_hidden_dim = 64\n\n# ===Metrics===\nuse_nll_oracle = int(True)\nuse_nll_gen = int(True)\nuse_nll_div = int(True)\nuse_bleu = int(True)\nuse_self_bleu = int(True)\nuse_ppl = int(False)\n\nargs = [\n    # Program\n    '--if_test', if_test,\n    '--run_model', run_model,\n    '--cuda', CUDA,\n    # '--device', gpu_id,  # comment for auto GPU\n    '--ora_pretrain', oracle_pretrain,\n    '--gen_pretrain', gen_pretrain,\n    '--dis_pretrain', dis_pretrain,\n    '--mle_epoch', MLE_train_epoch,\n    '--adv_epoch', ADV_train_epoch,\n    '--tips', tips,\n\n    # Oracle or Real\n    '--if_real_data', if_real_data[job_id],\n    '--dataset', dataset[job_id],\n    '--vocab_size', vocab_size[job_id],\n\n    # Basic Param\n    '--shuffle', data_shuffle,\n    '--model_type', model_type,\n    '--gen_init', gen_init,\n    '--dis_init', dis_init,\n    '--samples_num', samples_num,\n    '--batch_size', batch_size,\n    '--max_seq_len', max_seq_len,\n    '--gen_lr', gen_lr,\n    '--dis_lr', dis_lr,\n    '--pre_log_step', pre_log_step,\n    '--adv_log_step', adv_log_step,\n\n    # Generator\n    '--adv_g_step', ADV_g_step,\n    '--rollout_num', rollout_num,\n    '--gen_embed_dim', gen_embed_dim,\n    '--gen_hidden_dim', gen_hidden_dim,\n\n    # Discriminator\n    '--d_step', d_step,\n    '--d_epoch', d_epoch,\n    '--adv_d_step', ADV_d_step,\n    '--adv_d_epoch', ADV_d_epoch,\n    '--dis_embed_dim', dis_embed_dim,\n    '--dis_hidden_dim', dis_hidden_dim,\n\n    # Metrics\n    '--use_nll_oracle', use_nll_oracle,\n    '--use_nll_gen', use_nll_gen,\n    '--use_nll_div', use_nll_div,\n    '--use_bleu', use_bleu,\n    '--use_self_bleu', use_self_bleu,\n    '--use_ppl', use_ppl,\n]\n\nargs = list(map(str, args))\nmy_env = os.environ.copy()\ncall([executable, scriptname] + args, env=my_env, cwd=rootdir)\n"""
utils/cat_data_loader.py,11,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : cat_data_loader.py\n# @Time         : Created at 2019-05-31\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport random\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom utils.text_process import *\n\n\nclass GANDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return len(self.data)\n\n\nclass CatGenDataIter:\n    def __init__(self, samples_list, shuffle=None):\n        self.batch_size = cfg.batch_size\n        self.max_seq_len = cfg.max_seq_len\n        self.start_letter = cfg.start_letter\n        self.shuffle = cfg.data_shuffle if not shuffle else shuffle\n        if cfg.if_real_data:\n            self.word2idx_dict, self.idx2word_dict = load_dict(cfg.dataset)\n\n        self.loader = DataLoader(\n            dataset=GANDataset(self.__read_data__(samples_list)),\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            drop_last=True)\n\n        self.input = self._all_data_(\'input\')\n        self.target = self._all_data_(\'target\')\n        self.label = self._all_data_(\'label\')  # from 0 to k-1, different from Discriminator label\n\n    def __read_data__(self, samples_list):\n        """"""\n        input: same as target, but start with start_letter.\n        """"""\n        inp, target, label = self.prepare(samples_list)\n        all_data = [{\'input\': i, \'target\': t, \'label\': l} for (i, t, l) in zip(inp, target, label)]\n        return all_data\n\n    def random_batch(self):\n        """"""Randomly choose a batch from loader, please note that the data should not be shuffled.""""""\n        idx = random.randint(0, len(self.loader) - 1)\n        return list(self.loader)[idx]\n\n    def _all_data_(self, col):\n        return torch.cat([data[col].unsqueeze(0) for data in self.loader.dataset.data], 0)\n\n    def prepare(self, samples_list, gpu=False):\n        """"""Add start_letter to samples as inp, target same as samples""""""\n        all_samples = torch.cat(samples_list, dim=0).long()\n        target = all_samples\n        inp = torch.zeros(all_samples.size()).long()\n        inp[:, 0] = self.start_letter\n        inp[:, 1:] = target[:, :self.max_seq_len - 1]\n\n        label = torch.zeros(all_samples.size(0)).long()\n        for idx in range(len(samples_list)):\n            start = sum([samples_list[i].size(0) for i in range(idx)])\n            label[start: start + samples_list[idx].size(0)] = idx\n\n        # shuffle\n        perm = torch.randperm(inp.size(0))\n        inp = inp[perm].detach()\n        target = target[perm].detach()\n        label = label[perm].detach()\n\n        if gpu:\n            return inp.cuda(), target.cuda(), label.cuda()\n        return inp, target, label\n\n    def load_data(self, filename):\n        """"""Load real data from local file""""""\n        self.tokens = get_tokenlized(filename)\n        samples_index = tokens_to_tensor(self.tokens, self.word2idx_dict)\n        return self.prepare(samples_index)\n\n\nclass CatClasDataIter:\n    """"""Classifier data loader, handle for multi label data""""""\n\n    def __init__(self, samples_list, given_target=None, shuffle=None):\n        """"""\n        - samples_list:  list of tensors, [label_0, label_1, ..., label_k]\n        """"""\n        self.batch_size = cfg.batch_size\n        self.max_seq_len = cfg.max_seq_len\n        self.start_letter = cfg.start_letter\n        self.shuffle = cfg.data_shuffle if not shuffle else shuffle\n\n        self.loader = DataLoader(\n            dataset=GANDataset(self.__read_data__(samples_list, given_target)),\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            drop_last=True)\n\n        self.input = self._all_data_(\'input\')\n        self.target = self._all_data_(\'target\')\n\n    def __read_data__(self, samples_list, given_target=None):\n        inp, target = self.prepare(samples_list, given_target)\n        all_data = [{\'input\': i, \'target\': t} for (i, t) in zip(inp, target)]\n        return all_data\n\n    def random_batch(self):\n        idx = random.randint(0, len(self.loader) - 1)\n        return list(self.loader)[idx]\n        # return next(iter(self.loader))\n\n    def _all_data_(self, col):\n        return torch.cat([data[col].unsqueeze(0) for data in self.loader.dataset.data], 0)\n\n    @staticmethod\n    def prepare(samples_list, given_target=None, detach=True, gpu=False):\n        """"""\n        Build inp and target\n        :param samples_list: list of tensors, [label_0, label_1, ..., label_k]\n        :param given_target: given a target, len(samples_list) = 1\n        :param detach: if detach input\n        :param gpu: if use cuda\n        :returns inp, target:\n            - inp: sentences\n            - target: label index, 0-label_0, 1-label_1, ..., k-label_k\n        """"""\n        if len(samples_list) == 1 and given_target is not None:\n            inp = samples_list[0]\n            if detach:\n                inp = inp.detach()\n            target = torch.LongTensor([given_target] * inp.size(0))\n            if len(inp.size()) == 2:  # samples token, else samples onehot\n                inp = inp.long()\n        else:\n            inp = torch.cat(samples_list, dim=0)  # !!!need .detach()\n            if detach:\n                inp = inp.detach()\n            target = torch.zeros(inp.size(0)).long()\n            if len(inp.size()) == 2:  # samples token, else samples onehot\n                inp = inp.long()\n            for idx in range(1, len(samples_list)):\n                start = sum([samples_list[i].size(0) for i in range(idx)])\n                target[start: start + samples_list[idx].size(0)] = idx\n\n        # shuffle\n        perm = torch.randperm(inp.size(0))\n        inp = inp[perm]\n        target = target[perm]\n\n        if gpu:\n            return inp.cuda(), target.cuda()\n        return inp, target\n'"
utils/data_loader.py,7,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : data_loader.py\n# @Time         : Created at 2019-05-31\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport random\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom utils.text_process import *\n\n\nclass GANDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return len(self.data)\n\n\nclass GenDataIter:\n    def __init__(self, samples, if_test_data=False, shuffle=None):\n        self.batch_size = cfg.batch_size\n        self.max_seq_len = cfg.max_seq_len\n        self.start_letter = cfg.start_letter\n        self.shuffle = cfg.data_shuffle if not shuffle else shuffle\n        if cfg.if_real_data:\n            self.word2idx_dict, self.idx2word_dict = load_dict(cfg.dataset)\n        if if_test_data:  # used for the classifier\n            self.word2idx_dict, self.idx2word_dict = load_test_dict(cfg.dataset)\n\n        self.loader = DataLoader(\n            dataset=GANDataset(self.__read_data__(samples)),\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            drop_last=True)\n\n        self.input = self._all_data_(\'input\')\n        self.target = self._all_data_(\'target\')\n\n    def __read_data__(self, samples):\n        """"""\n        input: same as target, but start with start_letter.\n        """"""\n        # global all_data\n        if isinstance(samples, torch.Tensor):  # Tensor\n            inp, target = self.prepare(samples)\n            all_data = [{\'input\': i, \'target\': t} for (i, t) in zip(inp, target)]\n        elif isinstance(samples, str):  # filename\n            inp, target = self.load_data(samples)\n            all_data = [{\'input\': i, \'target\': t} for (i, t) in zip(inp, target)]\n        else:\n            all_data = None\n        return all_data\n\n    def random_batch(self):\n        """"""Randomly choose a batch from loader, please note that the data should not be shuffled.""""""\n        idx = random.randint(0, len(self.loader) - 1)\n        return list(self.loader)[idx]\n\n    def _all_data_(self, col):\n        return torch.cat([data[col].unsqueeze(0) for data in self.loader.dataset.data], 0)\n\n    @staticmethod\n    def prepare(samples, gpu=False):\n        """"""Add start_letter to samples as inp, target same as samples""""""\n        inp = torch.zeros(samples.size()).long()\n        target = samples\n        inp[:, 0] = cfg.start_letter\n        inp[:, 1:] = target[:, :cfg.max_seq_len - 1]\n\n        if gpu:\n            return inp.cuda(), target.cuda()\n        return inp, target\n\n    def load_data(self, filename):\n        """"""Load real data from local file""""""\n        self.tokens = get_tokenlized(filename)\n        samples_index = tokens_to_tensor(self.tokens, self.word2idx_dict)\n        return self.prepare(samples_index)\n\n\nclass DisDataIter:\n    def __init__(self, pos_samples, neg_samples, shuffle=None):\n        self.batch_size = cfg.batch_size\n        self.max_seq_len = cfg.max_seq_len\n        self.start_letter = cfg.start_letter\n        self.shuffle = cfg.data_shuffle if not shuffle else shuffle\n\n        self.loader = DataLoader(\n            dataset=GANDataset(self.__read_data__(pos_samples, neg_samples)),\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            drop_last=True)\n\n    def __read_data__(self, pos_samples, neg_samples):\n        """"""\n        input: same as target, but start with start_letter.\n        """"""\n        inp, target = self.prepare(pos_samples, neg_samples)\n        all_data = [{\'input\': i, \'target\': t} for (i, t) in zip(inp, target)]\n        return all_data\n\n    def random_batch(self):\n        idx = random.randint(0, len(self.loader) - 1)\n        return list(self.loader)[idx]\n\n    def prepare(self, pos_samples, neg_samples, gpu=False):\n        """"""Build inp and target""""""\n        inp = torch.cat((pos_samples, neg_samples), dim=0).long().detach()  # !!!need .detach()\n        target = torch.ones(inp.size(0)).long()\n        target[pos_samples.size(0):] = 0\n\n        # shuffle\n        perm = torch.randperm(inp.size(0))\n        inp = inp[perm]\n        target = target[perm]\n\n        if gpu:\n            return inp.cuda(), target.cuda()\n        return inp, target\n'"
utils/data_utils.py,10,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : data_utils.py\n# @Time         : Created at 2019-03-16\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\nfrom time import strftime, localtime\n\nimport torch.nn as nn\n\nfrom metrics.nll import NLL\nfrom models.Oracle import Oracle\nfrom utils.data_loader import GenDataIter\nfrom utils.text_process import *\n\n\ndef create_multi_oracle(number):\n    for i in range(number):\n        print(\'Creating Oracle %d...\' % i)\n        oracle = Oracle(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size,\n                        cfg.max_seq_len, cfg.padding_idx, gpu=cfg.CUDA)\n        if cfg.CUDA:\n            oracle = oracle.cuda()\n        large_samples = oracle.sample(cfg.samples_num, 4 * cfg.batch_size)\n        small_samples = oracle.sample(cfg.samples_num // 2, 4 * cfg.batch_size)\n\n        torch.save(oracle.state_dict(), cfg.multi_oracle_state_dict_path.format(i))\n        torch.save(large_samples, cfg.multi_oracle_samples_path.format(i, cfg.samples_num))\n        torch.save(small_samples, cfg.multi_oracle_samples_path.format(i, cfg.samples_num // 2))\n\n        oracle_data = GenDataIter(large_samples)\n        mle_criterion = nn.NLLLoss()\n        groud_truth = NLL.cal_nll(oracle, oracle_data.loader, mle_criterion)\n        print(\'Oracle %d Groud Truth: %.4f\' % (i, groud_truth))\n\n\ndef create_specific_oracle(from_a, to_b, num=1, save_path=\'../pretrain/\'):\n    for i in range(num):\n        while True:\n            oracle = Oracle(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size,\n                            cfg.max_seq_len, cfg.padding_idx, gpu=cfg.CUDA)\n            if cfg.CUDA:\n                oracle = oracle.cuda()\n\n            big_samples = oracle.sample(cfg.samples_num, 8 * cfg.batch_size)\n            small_samples = oracle.sample(cfg.samples_num // 2, 8 * cfg.batch_size)\n\n            oracle_data = GenDataIter(big_samples)\n            mle_criterion = nn.NLLLoss()\n            groud_truth = NLL.cal_nll(oracle, oracle_data.loader, mle_criterion)\n\n            if from_a <= groud_truth <= to_b:\n                dir_path = save_path + \'oracle_data_gt{:.2f}_{}\'.format(groud_truth,\n                                                                        strftime(""%m%d_%H%M%S"", localtime()))\n                if not os.path.exists(dir_path):\n                    os.mkdir(dir_path)\n                print(\'save ground truth: \', groud_truth)\n                # prefix = \'oracle{}_lstm_gt{:.2f}_{}\'.format(i, groud_truth, strftime(""%m%d"", localtime()))\n                prefix = dir_path + \'/oracle_lstm\'\n                torch.save(oracle.state_dict(), \'{}.pt\'.format(prefix))\n                torch.save(big_samples, \'{}_samples_{}.pt\'.format(prefix, cfg.samples_num))\n                torch.save(small_samples, \'{}_samples_{}.pt\'.format(prefix, cfg.samples_num // 2))\n                break\n\n\ndef create_many_oracle(from_a, to_b, num=1, save_path=\'../pretrain/\'):\n    for i in range(num):\n        while True:\n            oracle = Oracle(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size,\n                            cfg.max_seq_len, cfg.padding_idx, gpu=cfg.CUDA)\n            if cfg.CUDA:\n                oracle = oracle.cuda()\n\n            big_samples = oracle.sample(cfg.samples_num, 8 * cfg.batch_size)\n            small_samples = oracle.sample(cfg.samples_num // 2, 8 * cfg.batch_size)\n\n            oracle_data = GenDataIter(big_samples)\n            mle_criterion = nn.NLLLoss()\n            groud_truth = NLL.cal_nll(oracle, oracle_data.loader, mle_criterion)\n\n            if from_a <= groud_truth <= to_b:\n                print(\'save ground truth: \', groud_truth)\n                prefix = \'oracle_lstm\'\n                torch.save(oracle.state_dict(), save_path + \'{}.pt\'.format(prefix))\n                torch.save(big_samples, save_path + \'{}_samples_{}.pt\'.format(prefix, cfg.samples_num))\n                torch.save(small_samples, save_path + \'{}_samples_{}.pt\'.format(prefix, cfg.samples_num // 2))\n                break\n\n\ndef _save(data, filename):\n    with open(filename, \'w\') as fout:\n        for d in data:\n            fout.write(d[\'reviewText\'] + \'\\n\')\n            fout.write(str(d[\'overall\']) + \'\\n\')\n\n\ndef _count(filename):\n    with open(filename, \'r\') as fin:\n        data = fin.read().strip().split(\'\\n\')\n        return len(data) / 2\n\n\ndef clean_amazon_long_sentence():\n    data_root = \'/home/sysu2018/Documents/william/amazon_dataset/\'\n    all_files = os.listdir(data_root)\n\n    print(\'|\\ttype\\t|\\torigin\\t|\\tclean_40\\t|\\tclean_20\\t|\\tfinal_40\\t|\\tfinal_20\\t|\')\n    print(\'|----------|----------|----------|----------|----------|----------|\')\n    for file in all_files:\n        filename = data_root + file\n        if os.path.isdir(filename):\n            continue\n\n        clean_save_40 = []\n        clean_save_20 = []\n        final_save_40 = []\n        final_save_20 = []\n        with open(filename, \'r\') as fin:\n            raw_data = fin.read().strip().split(\'\\n\')\n            for line in raw_data:\n                review = eval(line)[\'reviewText\']\n                if len(review.split()) <= 40:\n                    clean_save_40.append(eval(line))\n                    if len(review.split(\'.\')) <= 2:  # one sentence\n                        final_save_40.append(eval(line))\n\n                if len(review.split()) <= 20:\n                    clean_save_20.append(eval(line))\n                    if len(review.split(\'.\')) <= 2:  # one sentence\n                        final_save_20.append(eval(line))\n\n        save_filename = data_root + \'clean_40/\' + file.lower().split(\'_5\')[0] + \'.txt\'\n        _save(clean_save_40, save_filename)\n        # a = _count(save_filename)\n        save_filename = data_root + \'clean_20/\' + file.lower().split(\'_5\')[0] + \'.txt\'\n        _save(clean_save_20, save_filename)\n        # b = _count(save_filename)\n        save_filename = data_root + \'final_40/\' + file.lower().split(\'_5\')[0] + \'.txt\'\n        _save(final_save_40, save_filename)\n        # c = _count(save_filename)\n        save_filename = data_root + \'final_20/\' + file.lower().split(\'_5\')[0] + \'.txt\'\n        _save(final_save_20, save_filename)\n        # d = _count(save_filename)\n\n        print(\'|\\t%s\\t|\\t%d\\t|\\t%d\\t|\\t%d\\t|\\t%d\\t|\\t%d\\t|\' % (\n            file.lower().split(\'_5\')[0], len(raw_data),\n            len(clean_save_40), len(clean_save_20),\n            len(final_save_40), len(final_save_20)))\n        # print(\'|\\t%s\\t|\\t%d\\t|\\t%d\\t|\\t%d\\t|\\t%d\\t|\\t%d\\t|\' % (\n        #     file.lower().split(\'_5\')[0], len(raw_data), a, b, c, d))\n\n\ndef mean(x, y):\n    return round((2 * x * y) / (x + y), 3)\n\n\ndef mean_list(x, y):\n    res = []\n    for i, j in zip(x, y):\n        res.append(round(mean(i, j), 3))\n    return res\n\n\nif __name__ == \'__main__\':\n    pass\n'"
utils/helpers.py,20,"b'import logging\nimport sys\nfrom time import strftime, gmtime\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom metrics.nll import NLL\nfrom utils.data_loader import GenDataIter\n\n\nclass Signal:\n    """"""Running signal to control training process""""""\n\n    def __init__(self, signal_file):\n        self.signal_file = signal_file\n        self.pre_sig = True\n        self.adv_sig = True\n\n        self.update()\n\n    def update(self):\n        signal_dict = self.read_signal()\n        self.pre_sig = signal_dict[\'pre_sig\']\n        self.adv_sig = signal_dict[\'adv_sig\']\n\n    def read_signal(self):\n        with open(self.signal_file, \'r\') as fin:\n            return eval(fin.read())\n\n\ndef create_logger(name, silent=False, to_disk=False, log_file=None):\n    """"""Create a new logger""""""\n    # setup logger\n    log = logging.getLogger(name)\n    log.setLevel(logging.DEBUG)\n    log.propagate = False\n    formatter = logging.Formatter(fmt=\'%(message)s\', datefmt=\'%Y/%m/%d %I:%M:%S\')\n    if not silent:\n        ch = logging.StreamHandler(sys.stdout)\n        ch.setLevel(logging.DEBUG)\n        ch.setFormatter(formatter)\n        log.addHandler(ch)\n    if to_disk:\n        log_file = log_file if log_file is not None else strftime(""log/log_%m%d_%H%M.txt"", gmtime())\n        if type(log_file) == list:\n            for filename in log_file:\n                fh = logging.FileHandler(filename, mode=\'w\')\n                fh.setLevel(logging.INFO)\n                fh.setFormatter(formatter)\n                log.addHandler(fh)\n        if type(log_file) == str:\n            fh = logging.FileHandler(log_file, mode=\'w\')\n            fh.setLevel(logging.INFO)\n            fh.setFormatter(formatter)\n            log.addHandler(fh)\n    return log\n\n\ndef create_oracle():\n    """"""Create a new Oracle model and Oracle\'s samples""""""\n    import config as cfg\n    from models.Oracle import Oracle\n\n    print(\'Creating Oracle...\')\n    oracle = Oracle(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size,\n                    cfg.max_seq_len, cfg.padding_idx, gpu=cfg.CUDA)\n    if cfg.CUDA:\n        oracle = oracle.cuda()\n\n    torch.save(oracle.state_dict(), cfg.oracle_state_dict_path)\n\n    big_samples = oracle.sample(cfg.samples_num, 4 * cfg.batch_size)\n    # large\n    torch.save(big_samples, cfg.oracle_samples_path.format(cfg.samples_num))\n    # small\n    torch.save(oracle.sample(cfg.samples_num // 2, 4 * cfg.batch_size),\n               cfg.oracle_samples_path.format(cfg.samples_num // 2))\n\n    oracle_data = GenDataIter(big_samples)\n    mle_criterion = nn.NLLLoss()\n    groud_truth = NLL.cal_nll(oracle, oracle_data.loader, mle_criterion)\n    print(\'NLL_Oracle Groud Truth: %.4f\' % groud_truth)\n\n\ndef get_fixed_temperature(temper, i, N, adapt):\n    """"""A function to set up different temperature control policies""""""\n    N = 5000\n\n    if adapt == \'no\':\n        temper_var_np = 1.0  # no increase, origin: temper\n    elif adapt == \'lin\':\n        temper_var_np = 1 + i / (N - 1) * (temper - 1)  # linear increase\n    elif adapt == \'exp\':\n        temper_var_np = temper ** (i / N)  # exponential increase\n    elif adapt == \'log\':\n        temper_var_np = 1 + (temper - 1) / np.log(N) * np.log(i + 1)  # logarithm increase\n    elif adapt == \'sigmoid\':\n        temper_var_np = (temper - 1) * 1 / (1 + np.exp((N / 2 - i) * 20 / N)) + 1  # sigmoid increase\n    elif adapt == \'quad\':\n        temper_var_np = (temper - 1) / (N - 1) ** 2 * i ** 2 + 1\n    elif adapt == \'sqrt\':\n        temper_var_np = (temper - 1) / np.sqrt(N - 1) * np.sqrt(i) + 1\n    else:\n        raise Exception(""Unknown adapt type!"")\n\n    return temper_var_np\n\n\ndef get_losses(d_out_real, d_out_fake, loss_type=\'JS\'):\n    """"""Get different adversarial losses according to given loss_type""""""\n    bce_loss = nn.BCEWithLogitsLoss()\n\n    if loss_type == \'standard\':  # the non-satuating GAN loss\n        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n        d_loss = d_loss_real + d_loss_fake\n\n        g_loss = bce_loss(d_out_fake, torch.ones_like(d_out_fake))\n\n    elif loss_type == \'JS\':  # the vanilla GAN loss\n        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n        d_loss = d_loss_real + d_loss_fake\n\n        g_loss = -d_loss_fake\n\n    elif loss_type == \'KL\':  # the GAN loss implicitly minimizing KL-divergence\n        d_loss_real = bce_loss(d_out_real, torch.ones_like(d_out_real))\n        d_loss_fake = bce_loss(d_out_fake, torch.zeros_like(d_out_fake))\n        d_loss = d_loss_real + d_loss_fake\n\n        g_loss = torch.mean(-d_out_fake)\n\n    elif loss_type == \'hinge\':  # the hinge loss\n        d_loss_real = torch.mean(nn.ReLU(1.0 - d_out_real))\n        d_loss_fake = torch.mean(nn.ReLU(1.0 + d_out_fake))\n        d_loss = d_loss_real + d_loss_fake\n\n        g_loss = -torch.mean(d_out_fake)\n\n    elif loss_type == \'tv\':  # the total variation distance\n        d_loss = torch.mean(nn.Tanh(d_out_fake) - nn.Tanh(d_out_real))\n        g_loss = torch.mean(-nn.Tanh(d_out_fake))\n\n    elif loss_type == \'rsgan\':  # relativistic standard GAN\n        d_loss = bce_loss(d_out_real - d_out_fake, torch.ones_like(d_out_real))\n        g_loss = bce_loss(d_out_fake - d_out_real, torch.ones_like(d_out_fake))\n\n    else:\n        raise NotImplementedError(""Divergence \'%s\' is not implemented"" % loss_type)\n\n    return g_loss, d_loss\n\n\ndef truncated_normal_(tensor, mean=0, std=1):\n    """"""\n    Implemented by @ruotianluo\n    See https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/15\n    """"""\n    size = tensor.shape\n    tmp = tensor.new_empty(size + (4,)).normal_()\n    valid = (tmp < 2) & (tmp > -2)\n    ind = valid.max(-1, keepdim=True)[1]\n    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n    tensor.data.mul_(std).add_(mean)\n    return tensor\n'"
utils/rollout.py,20,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : rollout.py\n# @Time         : Created at 2019-03-15\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport copy\nimport torch\nimport torch.nn.functional as F\n\n\nclass ROLLOUT:\n    def __init__(self, gen, gpu=True):\n        self.gen = gen\n        self.old_model = copy.deepcopy(gen)\n        self.max_seq_len = gen.max_seq_len\n        self.vocab_size = gen.vocab_size\n        self.step_size = gen.step_size if gen.name == \'leakgan\' else 0\n        self.goal_out_size = gen.goal_out_size if gen.name == \'leakgan\' else 0\n        self.gpu = gpu\n\n    def rollout_mc_search(self, sentences, given_num):\n        """"""\n        fill up remain tokens with MC search\n        :param sentences: size of batch_size * max_seq_len\n        :param given_num:\n        :return:\n        """"""\n        batch_size = sentences.size(0)\n\n        # get current state\n        hidden = self.gen.init_hidden(batch_size)\n        # for i in range(given_num):\n        inp = sentences[:, :given_num]\n        out, hidden = self.gen.forward(inp, hidden, need_hidden=True)\n        out = out.view(batch_size, -1, self.vocab_size)[:, -1]\n\n        samples = torch.zeros(batch_size, self.max_seq_len).long()\n        samples[:, :given_num] = sentences[:, :given_num]\n\n        if self.gpu:\n            samples = samples.cuda()\n\n        # MC search\n        for i in range(given_num, self.max_seq_len):\n            out = torch.multinomial(torch.exp(out), 1)\n            samples[:, i] = out.view(-1).data\n            inp = out.view(-1)\n\n            out, hidden = self.gen.forward(inp, hidden, need_hidden=True)\n\n        return samples\n\n    def rollout_mc_search_leakgan(self, sentences, dis, given_num):\n\n        batch_size, seq_len = sentences.size()\n\n        goal_array = torch.zeros((batch_size, seq_len + 1, self.goal_out_size))\n\n        work_hidden = self.gen.init_hidden(batch_size)\n        mana_hidden = self.gen.init_hidden(batch_size)\n        real_goal = self.gen.goal_init[:batch_size, :]\n        out = 0\n\n        if self.gpu:\n            goal_array = goal_array.cuda()\n            real_goal = real_goal.cuda()\n\n        # get current state\n        for i in range(given_num):\n            # Get feature.\n            dis_inp = torch.zeros(batch_size, seq_len).long()\n            dis_inp[:, :i + 1] = sentences[:, :i + 1]  # cut sentences\n            leak_inp = sentences[:, i]\n            if self.gpu:\n                dis_inp = dis_inp.cuda()\n                leak_inp = leak_inp.cuda()\n            feature = dis.get_feature(dis_inp).unsqueeze(0)\n\n            # Get output of one token\n            # cur_goal: batch_size * 1 * goal_out_size\n            out, cur_goal, work_hidden, mana_hidden = self.gen(i, leak_inp, work_hidden, mana_hidden,\n                                                               feature, real_goal, train=True)\n\n            # Save goal and update last_goal\n            goal_array[:, i, :] = cur_goal.squeeze(1)\n            if i > 0 and i % self.step_size == 0:\n                real_goal = torch.sum(goal_array[:, i - 3:i + 1, :], dim=1)\n                if i / self.step_size == 1:\n                    real_goal += self.gen.goal_init[:batch_size, :]\n\n        samples = torch.zeros(batch_size, self.max_seq_len).long()\n        samples[:, :given_num] = sentences[:, :given_num]\n\n        # MC search\n        for i in range(given_num, self.max_seq_len):\n            # Sample one token\n            out = torch.multinomial(torch.exp(out), 1).view(-1)  # [num_samples] (sampling from each row)\n            samples[:, i] = out.data\n\n            # Get feature\n            dis_inp = samples\n            if self.gpu:\n                dis_inp = dis_inp.cuda()\n            feature = dis.get_feature(dis_inp).unsqueeze(0)\n            leak_inp = out\n\n            # Get output of one token\n            # cur_goal: batch_size * 1 * goal_out_size\n            out, cur_goal, work_hidden, mana_hidden = self.gen(i, leak_inp, work_hidden, mana_hidden,\n                                                               feature, real_goal, train=True)\n\n            # Save goal and update last_goal\n            goal_array[:, i, :] = cur_goal.squeeze(1)\n            if i > 0 and i % self.step_size == 0:\n                real_goal = torch.sum(goal_array[:, i - 3:i + 1, :], dim=1)\n                if i / self.step_size == 1:\n                    real_goal += self.gen.goal_init[:batch_size, :]\n\n        if self.gpu:\n            samples = samples.cuda()\n\n        return samples\n\n    def get_reward(self, sentences, rollout_num, dis, current_k=0):\n        """"""\n        get reward via Monte Carlo search\n        :param sentences: size of batch_size * max_seq_len\n        :param rollout_num:\n        :param dis:\n        :param current_k: current training gen\n        :return: reward: [batch_size]\n        """"""\n        with torch.no_grad():\n            batch_size = sentences.size(0)\n            rewards = torch.zeros([rollout_num * self.max_seq_len, batch_size]).float()\n            if self.gpu:\n                rewards = rewards.cuda()\n            idx = 0\n            for i in range(rollout_num):\n                for given_num in range(1, self.max_seq_len + 1):\n                    samples = self.rollout_mc_search(sentences, given_num)\n                    out = dis.forward(samples)\n                    out = F.softmax(out, dim=-1)\n                    reward = out[:, current_k + 1]\n                    rewards[idx] = reward\n                    idx += 1\n\n        # rewards = torch.mean(rewards, dim=0)\n        rewards = torch.mean(rewards.view(batch_size, self.max_seq_len, rollout_num), dim=-1)\n        return rewards\n\n    def get_reward_leakgan(self, sentences, rollout_num, dis, current_k):\n        """"""\n        get reward via Monte Carlo search for LeakGAN\n        :param sentences: size of batch_size * max_seq_len\n        :param rollout_num:\n        :param dis:\n        :param current_k: current training gen\n\n        :return: reward: batch_size * (max_seq_len / step_size)\n        """"""\n        with torch.no_grad():\n            batch_size = sentences.size(0)\n            rewards = torch.zeros([rollout_num * (self.max_seq_len // self.step_size), batch_size]).float()\n            if self.gpu:\n                rewards = rewards.cuda()\n            idx = 0\n            for i in range(rollout_num):\n                for t in range(self.max_seq_len // self.step_size):\n                    given_num = t * self.step_size + 1  # 1, 5, 9, ..\n                    samples = self.rollout_mc_search_leakgan(sentences, dis, given_num)\n                    out = dis(samples)\n                    out = F.softmax(out, dim=-1)\n                    reward = out[:, current_k + 1]\n                    rewards[idx] = reward\n                    idx += 1\n\n        rewards = rewards.view(batch_size, self.max_seq_len // self.step_size, rollout_num)\n        rewards = torch.mean(rewards, dim=-1)\n        return rewards\n\n    def get_token_reward(self, sentences, rollout_num, dis, current_k, given_num):\n        """"""\n        get reward of each token in sequence via Monte Carlo search\n        """"""\n        with torch.no_grad():\n            batch_size = sentences.size(0)\n            rewards = torch.zeros([rollout_num, batch_size]).float()\n            idx = 0\n            for i in range(rollout_num):\n                samples = self.rollout_mc_search(sentences, given_num)\n                out = dis(samples)\n                out = F.softmax(out, dim=-1)\n                reward = out[:, current_k + 1]\n                rewards[idx] = reward\n                idx += 1\n\n        rewards = torch.Tensor(rewards).cuda()\n        rewards = torch.sum(rewards, dim=0) / rollout_num\n        return rewards\n'"
utils/text_process.py,4,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : text_process.py\n# @Time         : Created at 2019-05-14\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport nltk\nimport numpy as np\nimport os\nimport torch\n\nimport config as cfg\n\n\ndef get_tokenlized(file):\n    """"""tokenlize the file""""""\n    tokenlized = list()\n    with open(file) as raw:\n        for text in raw:\n            text = nltk.word_tokenize(text.lower())\n            tokenlized.append(text)\n    return tokenlized\n\n\ndef get_word_list(tokens):\n    """"""get word set""""""\n    word_set = list()\n    for sentence in tokens:\n        for word in sentence:\n            word_set.append(word)\n    return list(set(word_set))\n\n\ndef get_dict(word_set):\n    """"""get word2idx_dict and idx2word_dict""""""\n    word2idx_dict = dict()\n    idx2word_dict = dict()\n\n    index = 2\n    word2idx_dict[cfg.padding_token] = str(cfg.padding_idx)  # padding token\n    idx2word_dict[str(cfg.padding_idx)] = cfg.padding_token\n    word2idx_dict[cfg.start_token] = str(cfg.start_letter)  # start token\n    idx2word_dict[str(cfg.start_letter)] = cfg.start_token\n\n    for word in word_set:\n        word2idx_dict[word] = str(index)\n        idx2word_dict[str(index)] = word\n        index += 1\n    return word2idx_dict, idx2word_dict\n\n\ndef text_process(train_text_loc, test_text_loc=None):\n    """"""get sequence length and dict size""""""\n    train_tokens = get_tokenlized(train_text_loc)\n    if test_text_loc is None:\n        test_tokens = list()\n    else:\n        test_tokens = get_tokenlized(test_text_loc)\n    word_set = get_word_list(train_tokens + test_tokens)\n    word2idx_dict, idx2word_dict = get_dict(word_set)\n\n    if test_text_loc is None:\n        sequence_len = len(max(train_tokens, key=len))\n    else:\n        sequence_len = max(len(max(train_tokens, key=len)), len(max(test_tokens, key=len)))\n\n    return sequence_len, len(word2idx_dict)\n\n\n# ============================================\ndef init_dict(dataset):\n    """"""\n    Initialize dictionaries of dataset, please note that \'0\': padding_idx, \'1\': start_letter.\n    Finally save dictionary files locally.\n    """"""\n    tokens = get_tokenlized(\'dataset/{}.txt\'.format(dataset))\n    word_set = get_word_list(tokens)\n    word2idx_dict, idx2word_dict = get_dict(word_set)\n\n    with open(\'dataset/{}_wi_dict.txt\'.format(dataset), \'w\') as dictout:\n        dictout.write(str(word2idx_dict))\n    with open(\'dataset/{}_iw_dict.txt\'.format(dataset), \'w\') as dictout:\n        dictout.write(str(idx2word_dict))\n\n    print(\'total tokens: \', len(word2idx_dict))\n\n\ndef load_dict(dataset):\n    """"""Load dictionary from local files""""""\n    iw_path = \'dataset/{}_iw_dict.txt\'.format(dataset)\n    wi_path = \'dataset/{}_wi_dict.txt\'.format(dataset)\n\n    if not os.path.exists(iw_path) or not os.path.exists(iw_path):  # initialize dictionaries\n        init_dict(dataset)\n\n    with open(iw_path, \'r\') as dictin:\n        idx2word_dict = eval(dictin.read().strip())\n    with open(wi_path, \'r\') as dictin:\n        word2idx_dict = eval(dictin.read().strip())\n\n    return word2idx_dict, idx2word_dict\n\n\ndef load_test_dict(dataset):\n    """"""Build test data dictionary, extend from train data. For the classifier.""""""\n    word2idx_dict, idx2word_dict = load_dict(dataset)  # train dict\n    # tokens = get_tokenlized(\'dataset/testdata/{}_clas_test.txt\'.format(dataset))\n    tokens = get_tokenlized(\'dataset/testdata/{}_test.txt\'.format(dataset))\n    word_set = get_word_list(tokens)\n    index = len(word2idx_dict)  # current index\n\n    # extend dict with test data\n    for word in word_set:\n        if word not in word2idx_dict:\n            word2idx_dict[word] = str(index)\n            idx2word_dict[str(index)] = word\n            index += 1\n    return word2idx_dict, idx2word_dict\n\n\ndef tensor_to_tokens(tensor, dictionary):\n    """"""transform Tensor to word tokens""""""\n    tokens = []\n    for sent in tensor:\n        sent_token = []\n        for word in sent.tolist():\n            if word == cfg.padding_idx:\n                break\n            sent_token.append(dictionary[str(word)])\n        tokens.append(sent_token)\n    return tokens\n\n\ndef tokens_to_tensor(tokens, dictionary):\n    """"""transform word tokens to Tensor""""""\n    global i\n    tensor = []\n    for sent in tokens:\n        sent_ten = []\n        for i, word in enumerate(sent):\n            if word == cfg.padding_token:\n                break\n            sent_ten.append(int(dictionary[str(word)]))\n        while i < cfg.max_seq_len - 1:\n            sent_ten.append(cfg.padding_idx)\n            i += 1\n        tensor.append(sent_ten[:cfg.max_seq_len])\n    return torch.LongTensor(tensor)\n\n\ndef padding_token(tokens):\n    """"""pad sentences with padding_token""""""\n    global i\n    pad_tokens = []\n    for sent in tokens:\n        sent_token = []\n        for i, word in enumerate(sent):\n            if word == cfg.padding_token:\n                break\n            sent_token.append(word)\n        while i < cfg.max_seq_len - 1:\n            sent_token.append(cfg.padding_token)\n            i += 1\n        pad_tokens.append(sent_token)\n    return pad_tokens\n\n\ndef write_tokens(filename, tokens):\n    """"""Write word tokens to a local file (For Real data)""""""\n    with open(filename, \'w\') as fout:\n        for sent in tokens:\n            fout.write(\' \'.join(sent))\n            fout.write(\'\\n\')\n\n\ndef write_tensor(filename, tensor):\n    """"""Write Tensor to a local file (For Oracle data)""""""\n    with open(filename, \'w\') as fout:\n        for sent in tensor:\n            fout.write(\' \'.join([str(i) for i in sent.tolist()]))\n            fout.write(\'\\n\')\n\n\ndef process_cat_text():\n    import random\n\n    dataset = \'mr\'\n\n    test_ratio = 0.3\n    seq_len = 15\n\n    pos_file = \'dataset/{}/{}{}_cat1.txt\'.format(dataset, dataset, seq_len)\n    neg_file = \'dataset/{}/{}{}_cat0.txt\'.format(dataset, dataset, seq_len)\n    pos_sent = open(pos_file, \'r\').readlines()\n    neg_sent = open(neg_file, \'r\').readlines()\n\n    pos_len = int(test_ratio * len(pos_sent))\n    neg_len = int(test_ratio * len(neg_sent))\n\n    random.shuffle(pos_sent)\n    random.shuffle(neg_sent)\n\n    all_sent_test = pos_sent[:pos_len] + neg_sent[:neg_len]\n    all_sent_train = pos_sent[pos_len:] + neg_sent[neg_len:]\n    random.shuffle(all_sent_test)\n    random.shuffle(all_sent_train)\n\n    f_pos_train = open(\'dataset/{}{}_cat1.txt\'.format(dataset, seq_len), \'w\')\n    f_neg_train = open(\'dataset/{}{}_cat0.txt\'.format(dataset, seq_len), \'w\')\n    f_pos_test = open(\'dataset/testdata/{}{}_cat1_test.txt\'.format(dataset, seq_len), \'w\')\n    f_neg_test = open(\'dataset/testdata/{}{}_cat0_test.txt\'.format(dataset, seq_len), \'w\')\n\n    for p_s in pos_sent[:pos_len]:\n        f_pos_test.write(p_s)\n    for n_s in neg_sent[:neg_len]:\n        f_neg_test.write(n_s)\n    for p_s in pos_sent[pos_len:]:\n        f_pos_train.write(p_s)\n    for n_s in neg_sent[neg_len:]:\n        f_neg_train.write(n_s)\n\n    with open(\'dataset/testdata/{}{}_test.txt\'.format(dataset, seq_len), \'w\') as fout:\n        for sent in all_sent_test:\n            fout.write(sent)\n    with open(\'dataset/{}{}.txt\'.format(dataset, seq_len), \'w\') as fout:\n        for sent in all_sent_train:\n            fout.write(sent)\n\n    f_pos_train.close()\n    f_neg_train.close()\n    f_pos_test.close()\n    f_neg_test.close()\n\n\ndef combine_amazon_text():\n    cat0_name = \'app\'\n    cat1_name = \'book\'\n    root_path = \'dataset/\'\n    cat0_train = open(root_path + cat0_name + \'.txt\', \'r\').readlines()\n    cat0_test = open(root_path + cat0_name + \'_test.txt\', \'r\').readlines()\n    cat1_train = open(root_path + cat1_name + \'.txt\', \'r\').readlines()\n    cat1_test = open(root_path + cat1_name + \'_test.txt\', \'r\').readlines()\n\n    with open(root_path + \'amazon_{}_{}.txt\'.format(cat0_name, cat1_name), \'w\') as fout:\n        for sent in cat0_train:\n            fout.write(sent)\n        for sent in cat1_train:\n            fout.write(sent)\n    with open(root_path + \'testdata/amazon_{}_{}_test.txt\'.format(cat0_name, cat1_name), \'w\') as fout:\n        for sent in cat0_test:\n            fout.write(sent)\n        for sent in cat1_test:\n            fout.write(sent)\n\n\ndef extend_clas_train_data():\n    data_name = \'mr\'\n    dataset = \'mr20\'\n    neg_filter_file = \'dataset/{}/{}_cat0.txt\'.format(data_name, dataset)  # include train and test for generator\n    pos_filter_file = \'dataset/{}/{}_cat1.txt\'.format(data_name, dataset)\n    neg_test_file = \'dataset/testdata/{}_cat0_test.txt\'.format(dataset)\n    pos_test_file = \'dataset/testdata/{}_cat1_test.txt\'.format(dataset)\n    neg_all_file = \'dataset/{}/{}_cat0.txt\'.format(data_name, data_name)\n    pos_all_file = \'dataset/{}/{}_cat1.txt\'.format(data_name, data_name)\n\n    neg_filter = open(neg_filter_file, \'r\').readlines()\n    pos_filter = open(pos_filter_file, \'r\').readlines()\n    neg_test = open(neg_test_file, \'r\').readlines()\n    pos_test = open(pos_test_file, \'r\').readlines()\n    neg_all = open(neg_all_file, \'r\').readlines()\n    pos_all = open(pos_all_file, \'r\').readlines()\n\n    # print(\'neg filter:\', len(neg_filter))\n    # print(\'neg test:\', len(neg_test))\n    # print(\'neg all:\', len(neg_all))\n    # print(\'pos filter:\', len(pos_filter))\n    # print(\'pos test:\', len(pos_test))\n    # print(\'pos all:\', len(pos_all))\n\n    print(\'neg before:\', len(neg_test))\n    for line in neg_all:\n        if line not in neg_filter:\n            neg_test.append(line)\n    print(\'neg after:\', len(neg_test))\n\n    print(\'pos before:\', len(pos_test))\n    for line in pos_all:\n        if line not in pos_filter:\n            pos_test.append(line)\n    print(\'pos after:\', len(pos_test))\n\n    with open(\'dataset/testdata/{}_cat0_clas_test.txt\'.format(dataset), \'w\') as fout:\n        for line in neg_test:\n            fout.write(line)\n    with open(\'dataset/testdata/{}_cat1_clas_test.txt\'.format(dataset), \'w\') as fout:\n        for line in pos_test:\n            fout.write(line)\n    with open(\'dataset/testdata/{}_clas_test.txt\'.format(dataset), \'w\') as fout:\n        for line in neg_test:\n            fout.write(line)\n        for line in pos_test:\n            fout.write(line)\n\n\ndef load_word_vec(path, word2idx_dict=None, type=\'glove\'):\n    """"""Load word embedding from local file""""""\n    fin = open(path, \'r\', encoding=\'utf-8\', newline=\'\\n\', errors=\'ignore\')\n    if type == \'glove\':\n        word2vec_dict = {}\n        for line in fin:\n            tokens = line.rstrip().split()\n            if word2idx_dict is None or tokens[0] in word2idx_dict.keys():\n                word2vec_dict[tokens[0]] = np.asarray(tokens[1:], dtype=\'float32\')\n    elif type == \'word2vec\':\n        import gensim\n        word2vec_dict = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n    else:\n        raise NotImplementedError(\'No such type: %s\' % type)\n    return word2vec_dict\n\n\ndef build_embedding_matrix(dataset):\n    """"""Load or build Glove embedding matrix.""""""\n    embed_filename = \'dataset/glove_embedding_300d_{}.pt\'.format(dataset)\n    if os.path.exists(embed_filename):\n        print(\'Loading embedding:\', embed_filename)\n        embedding_matrix = torch.load(embed_filename)\n    else:\n        print(\'Loading Glove word vectors...\')\n        word2idx_dict, _ = load_dict(dataset)\n        embedding_matrix = np.random.random((len(word2idx_dict) + 2, 300))  # 2 for padding token and start token\n        fname = \'../glove.42B.300d.txt\'  # Glove file\n        # fname = \'../GoogleNews-vectors-negative300.bin\' # Google Word2Vec file\n        word2vec_dict = load_word_vec(fname, word2idx_dict=word2idx_dict, type=\'glove\')\n        print(\'Building embedding matrix:\', embed_filename)\n        for word, i in word2idx_dict.items():\n            if word in word2vec_dict:\n                # words not found in embedding index will be randomly initialized.\n                embedding_matrix[int(i)] = word2vec_dict[word]\n        embedding_matrix = torch.FloatTensor(embedding_matrix)\n        torch.save(embedding_matrix, embed_filename)\n    return embedding_matrix\n\n\nif __name__ == \'__main__\':\n    os.chdir(\'../\')\n    # process_cat_text()\n    # load_test_dict(\'mr15\')\n    # extend_clas_train_data()\n    pass\n'"
utils/visualization.py,0,"b""# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : visualization.py\n# @Time         : Created at 2019-03-19\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport matplotlib.pyplot as plt\n\ntitle_dict = {\n    'gen_pre_loss': 'pre_loss',\n    'gen_adv_loss': 'g_loss',\n    'gen_mana_loss': 'mana_loss',\n    'gen_work_loss': 'work_loss',\n    'dis_loss': 'd_loss',\n    'dis_train_acc': 'train_acc',\n    'dis_eval_acc': 'eval_acc',\n    'NLL_oracle': 'NLL_oracle',\n    'NLL_gen': 'NLL_gen',\n    'BLEU-3': 'BLEU-3',\n}\n\ncolor_list = ['#e74c3c', '#e67e22', '#f1c40f', '#8e44ad', '#2980b9', '#27ae60', '#16a085']\n\n\ndef plt_data(data, step, title, c_id, savefig=False):\n    x = [i for i in range(step)]\n    plt.plot(x, data, color=color_list[c_id], label=title)\n    if savefig:\n        plt.savefig('savefig/' + title + '.png')\n\n\ndef get_log_data(filename):\n    with open(filename, 'r') as fin:\n        all_lines = fin.read().strip().split('\\n')\n        data_dict = {'pre_loss': [], 'g_loss': [], 'mana_loss': [], 'work_loss': [],\n                     'd_loss': [], 'train_acc': [], 'eval_acc': [], 'NLL_oracle': [],\n                     'NLL_gen': [], 'BLEU-3': []}\n\n        for line in all_lines:\n            items = line.split()\n            try:\n                for key in data_dict.keys():\n                    if key in items:\n                        data_dict[key].append(float(items[items.index(key) + 2][:-1]))\n            except:\n                break\n\n    return data_dict\n\n\nif __name__ == '__main__':\n    log_file_root = '../log/'\n    # Custom your log files in lists, no more than len(color_list)\n    log_file_list = ['log_0604_2233', 'log_0605_0120', 'log_0531_1507']\n    legend_text = ['SeqGAN', 'LeakGAN', 'RelGAN']\n\n    color_id = 0\n    data_name = 'NLL_oracle'\n    if_save = False\n    # legend_text = log_file_list\n\n    assert data_name in title_dict.keys(), 'Error data name'\n    plt.clf()\n    plt.title(data_name)\n    all_data_list = []\n    for idx, item in enumerate(log_file_list):\n        log_file = log_file_root + item + '.txt'\n\n        # save log file\n        all_data = get_log_data(log_file)\n        plt_data(all_data[title_dict[data_name]], len(all_data[title_dict[data_name]]),\n                 legend_text[idx], color_id, if_save)\n        color_id += 1\n\n    plt.legend()\n    plt.show()\n"""
instructor/oracle_data/instructor.py,16,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : instructor.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\n\nimport config as cfg\nfrom metrics.nll import NLL\nfrom models.Oracle import Oracle\nfrom utils.data_loader import GenDataIter\nfrom utils.data_utils import create_multi_oracle\nfrom utils.helpers import Signal, create_logger, create_oracle, get_fixed_temperature\nfrom utils.text_process import write_tensor\n\n\nclass BasicInstructor:\n    def __init__(self, opt):\n        self.log = create_logger(__name__, silent=False, to_disk=True,\n                                 log_file=cfg.log_filename if cfg.if_test\n                                 else [cfg.log_filename, cfg.save_root + \'log.txt\'])\n        self.sig = Signal(cfg.signal_file)\n        self.opt = opt\n\n        # oracle, generator, discriminator\n        self.oracle = Oracle(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                             cfg.padding_idx, gpu=cfg.CUDA)\n        self.oracle_list = [Oracle(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                                   cfg.padding_idx, gpu=cfg.CUDA) for _ in range(cfg.k_label)]\n\n        self.dis = None\n        self.clas = None\n\n        self.show_config()\n        self.check_oracle()  # Create Oracle models if not exist\n        # DataLoader\n        self.oracle_samples = torch.load(cfg.oracle_samples_path.format(cfg.samples_num))\n        self.oracle_samples_list = [torch.load(cfg.multi_oracle_samples_path.format(i, cfg.samples_num))\n                                    for i in range(cfg.k_label)]\n\n        self.oracle_data = GenDataIter(self.oracle_samples)\n        self.oracle_data_list = [GenDataIter(self.oracle_samples_list[i]) for i in range(cfg.k_label)]\n\n        # Criterion\n        self.mle_criterion = nn.NLLLoss()\n        self.dis_criterion = nn.CrossEntropyLoss()\n\n        # Metrics\n        self.nll_oracle = NLL(\'NLL_oracle\', if_use=cfg.use_nll_oracle, gpu=cfg.CUDA)\n        self.nll_gen = NLL(\'NLL_gen\', if_use=cfg.use_nll_gen, gpu=cfg.CUDA)\n        self.nll_div = NLL(\'NLL_div\', if_use=cfg.use_nll_div, gpu=cfg.CUDA)\n        self.all_metrics = [self.nll_oracle, self.nll_gen, self.nll_div]\n\n    def _run(self):\n        print(\'Nothing to run in Basic Instructor!\')\n        pass\n\n    def _test(self):\n        pass\n\n    def init_model(self):\n        if cfg.oracle_pretrain:\n            if not os.path.exists(cfg.oracle_state_dict_path):\n                create_oracle()\n            self.oracle.load_state_dict(torch.load(cfg.oracle_state_dict_path))\n\n        if cfg.dis_pretrain:\n            self.log.info(\n                \'Load pretrained discriminator: {}\'.format(cfg.pretrained_dis_path))\n            self.dis.load_state_dict(torch.load(cfg.pretrained_dis_path))\n        if cfg.gen_pretrain:\n            self.log.info(\'Load MLE pretrained generator gen: {}\'.format(cfg.pretrained_gen_path))\n            self.gen.load_state_dict(torch.load(cfg.pretrained_gen_path, map_location=\'cuda:{}\'.format(cfg.device)))\n\n        if cfg.CUDA:\n            self.oracle = self.oracle.cuda()\n            self.gen = self.gen.cuda()\n            self.dis = self.dis.cuda()\n\n    def train_gen_epoch(self, model, data_loader, criterion, optimizer):\n        total_loss = 0\n        for i, data in enumerate(data_loader):\n            inp, target = data[\'input\'], data[\'target\']\n            if cfg.CUDA:\n                inp, target = inp.cuda(), target.cuda()\n\n            hidden = model.init_hidden(data_loader.batch_size)\n            pred = model.forward(inp, hidden)\n            loss = criterion(pred, target.view(-1))\n            self.optimize(optimizer, loss, model)\n            total_loss += loss.item()\n        return total_loss / len(data_loader)\n\n    def train_dis_epoch(self, model, data_loader, criterion, optimizer):\n        total_loss = 0\n        total_acc = 0\n        total_num = 0\n        for i, data in enumerate(data_loader):\n            inp, target = data[\'input\'], data[\'target\']\n            if cfg.CUDA:\n                inp, target = inp.cuda(), target.cuda()\n\n            pred = model.forward(inp)\n            loss = criterion(pred, target)\n            self.optimize(optimizer, loss, model)\n\n            total_loss += loss.item()\n            total_acc += torch.sum((pred.argmax(dim=-1) == target)).item()\n            total_num += inp.size(0)\n\n        total_loss /= len(data_loader)\n        total_acc /= total_num\n        return total_loss, total_acc\n\n    @staticmethod\n    def eval_dis(model, data_loader, criterion):\n        total_loss = 0\n        total_acc = 0\n        total_num = 0\n        with torch.no_grad():\n            for i, data in enumerate(data_loader):\n                inp, target = data[\'input\'], data[\'target\']\n                if cfg.CUDA:\n                    inp, target = inp.cuda(), target.cuda()\n\n                pred = model.forward(inp)\n                loss = criterion(pred, target)\n                total_loss += loss.item()\n                total_acc += torch.sum((pred.argmax(dim=-1) == target)).item()\n                total_num += inp.size(0)\n            total_loss /= len(data_loader)\n            total_acc /= total_num\n        return total_loss, total_acc\n\n    @staticmethod\n    def optimize_multi(opts, losses):\n        for i, (opt, loss) in enumerate(zip(opts, losses)):\n            opt.zero_grad()\n            loss.backward(retain_graph=True if i < len(opts) - 1 else False)\n            opt.step()\n\n    @staticmethod\n    def optimize(opt, loss, model=None, retain_graph=False):\n        opt.zero_grad()\n        loss.backward(retain_graph=retain_graph)\n        if model is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_norm)\n        opt.step()\n\n    def show_config(self):\n        """"""Show parser parameters settings""""""\n        self.log.info(100 * \'=\')\n        self.log.info(\'> training arguments:\')\n        for arg in vars(self.opt):\n            self.log.info(\'>>> {0}: {1}\'.format(arg, getattr(self.opt, arg)))\n        self.log.info(100 * \'=\')\n\n    def cal_metrics(self, fmt_str=False):\n        """"""\n        Calculate metrics\n        :param fmt_str: if return format string for logging\n        """"""\n        with torch.no_grad():\n            # Prepare data for evaluation\n            gen_data = GenDataIter(self.gen.sample(cfg.samples_num, 4 * cfg.batch_size))\n\n            # Reset metrics\n            self.nll_oracle.reset(self.oracle, gen_data.loader)\n            self.nll_gen.reset(self.gen, self.oracle_data.loader)\n            self.nll_div.reset(self.gen, gen_data.loader)\n\n        if fmt_str:\n            return \', \'.join([\'%s = %s\' % (metric.get_name(), metric.get_score()) for metric in self.all_metrics])\n        else:\n            return [metric.get_score() for metric in self.all_metrics]\n\n    def cal_metrics_with_label(self, label_i):\n        assert type(label_i) == int, \'missing label\'\n        with torch.no_grad():\n            # Prepare data for evaluation\n            eval_samples = self.gen.sample(cfg.samples_num, 8 * cfg.batch_size, label_i=label_i)\n            gen_data = GenDataIter(eval_samples)\n\n            # Reset metrics\n            self.nll_oracle.reset(self.oracle_list[label_i], gen_data.loader, label_i)\n            self.nll_gen.reset(self.gen, self.oracle_data_list[label_i].loader, label_i)\n            self.nll_div.reset(self.gen, gen_data.loader, label_i)\n\n        return [metric.get_score() for metric in self.all_metrics]\n\n    def comb_metrics(self, fmt_str=False):\n        all_scores = [self.cal_metrics_with_label(label_i) for label_i in range(cfg.k_label)]\n        all_scores = np.array(all_scores).T.tolist()  # each row for each metric\n\n        if fmt_str:\n            return \', \'.join([\'%s = %s\' % (metric.get_name(), score)\n                              for (metric, score) in zip(self.all_metrics, all_scores)])\n        return all_scores\n\n    def _save(self, phase, epoch):\n        """"""Save model state dict and generator\'s samples""""""\n        if phase != \'ADV\':\n            torch.save(self.gen.state_dict(), cfg.save_model_root + \'gen_{}_{:05d}.pt\'.format(phase, epoch))\n        save_sample_path = cfg.save_samples_root + \'samples_{}_{:05d}.txt\'.format(phase, epoch)\n        samples = self.gen.sample(cfg.batch_size, cfg.batch_size)\n        write_tensor(save_sample_path, samples)\n\n    def update_temperature(self, i, N):\n        self.gen.temperature.data = torch.Tensor([get_fixed_temperature(cfg.temperature, i, N, cfg.temp_adpt)])\n        if cfg.CUDA:\n            self.gen.temperature.data = self.gen.temperature.data.cuda()\n\n    def check_oracle(self):\n        if not cfg.oracle_pretrain:\n            create_oracle()\n            create_multi_oracle(cfg.k_label)\n\n        # General text generation Oracle model\n        if not os.path.exists(cfg.oracle_samples_path.format(cfg.samples_num)) or not cfg.oracle_pretrain:\n            create_oracle()\n\n        # Category text generation Oracle models\n        for i in range(cfg.k_label):\n            if not os.path.exists(cfg.multi_oracle_samples_path.format(i, cfg.samples_num)):\n                create_multi_oracle(cfg.k_label)\n                break\n\n        # Load Oracle state dict\n        self.oracle.load_state_dict(torch.load(cfg.oracle_state_dict_path))\n        for i in range(cfg.k_label):\n            oracle_path = cfg.multi_oracle_state_dict_path.format(i)\n            self.oracle_list[i].load_state_dict(torch.load(oracle_path))\n'"
instructor/oracle_data/jsdgan_instructor.py,3,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : JSDGAN_instructor.py\n# @Time         : Created at 2019/11/16\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\nimport os\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.oracle_data.instructor import BasicInstructor\nfrom models.JSDGAN_G import JSDGAN_G\nfrom utils.helpers import create_oracle\n\n\nclass JSDGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(JSDGANInstructor, self).__init__(opt)\n\n        # generator\n        self.gen = JSDGAN_G(cfg.mem_slots, cfg.num_heads, cfg.head_size, cfg.gen_embed_dim, cfg.gen_hidden_dim,\n                            cfg.vocab_size, cfg.max_seq_len, cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n\n    def init_model(self):\n        if cfg.oracle_pretrain:\n            if not os.path.exists(cfg.oracle_state_dict_path):\n                create_oracle()\n            self.oracle.load_state_dict(torch.load(cfg.oracle_state_dict_path))\n\n        if cfg.gen_pretrain:\n            self.log.info(\'Load MLE pretrained generator gen: {}\'.format(cfg.pretrained_gen_path))\n            self.gen.load_state_dict(torch.load(cfg.pretrained_gen_path, map_location=\'cuda:{}\'.format(cfg.device)))\n\n        if cfg.CUDA:\n            self.oracle = self.oracle.cuda()\n            self.gen = self.gen.cuda()\n\n    def _run(self):\n        # ===PRE-TRAINING===\n        self.log.info(\'Starting Generator MLE Training...\')\n        self.pretrain_generator(cfg.MLE_train_epoch)\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            g_loss = self.adv_train_generator(cfg.ADV_g_step)  # Generator\n\n            if adv_epoch % cfg.adv_log_step == 0:\n                self.log.info(\'[ADV] epoch %d: g_loss = %.4f, %s\' % (adv_epoch, g_loss, self.cal_metrics(fmt_str=True)))\n\n                if cfg.if_save and not cfg.if_test:\n                    self._save(\'ADV\', adv_epoch)\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                pre_loss = self.train_gen_epoch(self.gen, self.oracle_data.loader, self.mle_criterion, self.gen_opt)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\n                        \'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (epoch, pre_loss, self.cal_metrics(fmt_str=True)))\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        """"""\n        The gen is trained using policy gradients, using the reward from the discriminator.\n        Training is done for num_batches batches.\n        """"""\n        global inp, target\n        total_loss = 0\n        for step in range(g_step):\n            for i, data in enumerate(self.oracle_data.loader):\n                inp, target = data[\'input\'], data[\'target\']\n                if cfg.CUDA:\n                    inp, target = inp.cuda(), target.cuda()\n\n                # ===Train===\n                adv_loss = self.gen.JSD_loss(inp, target)\n                self.optimize(self.gen_opt, adv_loss, self.gen)\n                total_loss += adv_loss.item()\n\n        return total_loss\n'"
instructor/oracle_data/leakgan_instructor.py,5,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : leakgan_instructor.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.oracle_data.instructor import BasicInstructor\nfrom models.LeakGAN_D import LeakGAN_D\nfrom models.LeakGAN_G import LeakGAN_G\nfrom utils import rollout\nfrom utils.data_loader import GenDataIter, DisDataIter\nfrom utils.text_process import write_tensor\n\n\nclass LeakGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(LeakGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.gen = LeakGAN_G(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                             cfg.padding_idx, cfg.goal_size, cfg.step_size, cfg.CUDA)\n        self.dis = LeakGAN_D(cfg.dis_embed_dim, cfg.vocab_size, cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # optimizer\n        mana_params, work_params = self.gen.split_params()\n        mana_opt = optim.Adam(mana_params, lr=cfg.gen_lr)\n        work_opt = optim.Adam(work_params, lr=cfg.gen_lr)\n\n        self.gen_opt = [mana_opt, work_opt]\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n\n    def _run(self):\n        for inter_num in range(cfg.inter_epoch):\n            self.log.info(\'>>> Interleaved Round %d...\' % inter_num)\n            self.sig.update()  # update signal\n            if self.sig.pre_sig:\n                # ===DISCRIMINATOR PRE-TRAINING===\n                if not cfg.dis_pretrain:\n                    self.log.info(\'Starting Discriminator Training...\')\n                    self.train_discriminator(cfg.d_step, cfg.d_epoch)\n                    if cfg.if_save and not cfg.if_test:\n                        torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n                        print(\'Save pre-trained discriminator: {}\'.format(cfg.pretrained_dis_path))\n\n                # ===GENERATOR MLE TRAINING===\n                if not cfg.gen_pretrain:\n                    self.log.info(\'Starting Generator MLE Training...\')\n                    self.pretrain_generator(cfg.MLE_train_epoch)\n                    if cfg.if_save and not cfg.if_test:\n                        torch.save(self.gen.state_dict(), cfg.pretrained_gen_path)\n                        print(\'Save pre-trained generator: {}\'.format(cfg.pretrained_gen_path))\n            else:\n                self.log.info(\'>>> Stop by pre_signal! Skip to adversarial training...\')\n                break\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        self.log.info(\'Initial generator: %s\' % (str(self.cal_metrics(fmt_str=True))))\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            self.log.info(\'-----\\nADV EPOCH %d\\n-----\' % adv_epoch)\n            self.sig.update()\n            if self.sig.adv_sig:\n                self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                self.train_discriminator(cfg.ADV_d_step, cfg.ADV_d_epoch, \'ADV\')  # Discriminator\n\n                if adv_epoch % cfg.adv_log_step == 0:\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pretraining for the gen\n\n        - gen_opt: [mana_opt, work_opt]\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                pre_mana_loss = 0\n                pre_work_loss = 0\n\n                # ===Train===\n                for i, data in enumerate(self.oracle_data.loader):\n                    inp, target = data[\'input\'], data[\'target\']\n                    if cfg.CUDA:\n                        inp, target = inp.cuda(), target.cuda()\n\n                    mana_loss, work_loss = self.gen.pretrain_loss(target, self.dis)\n                    self.optimize_multi(self.gen_opt, [mana_loss, work_loss])\n                    pre_mana_loss += mana_loss.data.item()\n                    pre_work_loss += work_loss.data.item()\n                pre_mana_loss = pre_mana_loss / len(self.oracle_data.loader)\n                pre_work_loss = pre_work_loss / len(self.oracle_data.loader)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\'[MLE-GEN] epoch %d : pre_mana_loss = %.4f, pre_work_loss = %.4f, %s\' % (\n                        epoch, pre_mana_loss, pre_work_loss, self.cal_metrics(fmt_str=True)))\n\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step, current_k=0):\n        """"""\n        The gen is trained using policy gradients, using the reward from the discriminator.\n        Training is done for num_batches batches.\n        """"""\n\n        rollout_func = rollout.ROLLOUT(self.gen, cfg.CUDA)\n        adv_mana_loss = 0\n        adv_work_loss = 0\n        for step in range(g_step):\n            with torch.no_grad():\n                gen_samples = self.gen.sample(cfg.batch_size, cfg.batch_size, self.dis,\n                                              train=True)  # !!! train=True, the only place\n                inp, target = GenDataIter.prepare(gen_samples, gpu=cfg.CUDA)\n\n            # ===Train===\n            rewards = rollout_func.get_reward_leakgan(target, cfg.rollout_num, self.dis,\n                                                      current_k).cpu()  # reward with MC search\n            mana_loss, work_loss = self.gen.adversarial_loss(target, rewards, self.dis)\n\n            # update parameters\n            self.optimize_multi(self.gen_opt, [mana_loss, work_loss])\n            adv_mana_loss += mana_loss.data.item()\n            adv_work_loss += work_loss.data.item()\n        # ===Test===\n        self.log.info(\'[ADV-GEN] adv_mana_loss = %.4f, adv_work_loss = %.4f, %s\' % (\n            adv_mana_loss / g_step, adv_work_loss / g_step, self.cal_metrics(fmt_str=True)))\n\n    def train_discriminator(self, d_step, d_epoch, phase=\'MLE\'):\n        """"""\n        Training the discriminator on real_data_samples (positive) and generated samples from gen (negative).\n        Samples are drawn d_step times, and the discriminator is trained for d_epoch d_epoch.\n        """"""\n        # prepare loader for validate\n        global d_loss, train_acc\n        pos_val = self.oracle.sample(8 * cfg.batch_size, cfg.batch_size)\n        neg_val = self.gen.sample(8 * cfg.batch_size, cfg.batch_size, self.dis)\n        dis_eval_data = DisDataIter(pos_val, neg_val)\n\n        for step in range(d_step):\n            # prepare loader for training\n            pos_samples = self.oracle.sample(cfg.samples_num, cfg.batch_size)  # re-sample the Oracle Data\n            neg_samples = self.gen.sample(cfg.samples_num, cfg.batch_size, self.dis)\n            dis_data = DisDataIter(pos_samples, neg_samples)\n\n            for epoch in range(d_epoch):\n                # ===Train===\n                d_loss, train_acc = self.train_dis_epoch(self.dis, dis_data.loader, self.dis_criterion,\n                                                         self.dis_opt)\n\n            # ===Test===\n            _, eval_acc = self.eval_dis(self.dis, dis_eval_data.loader, self.dis_criterion)\n            self.log.info(\'[%s-DIS] d_step %d: d_loss = %.4f, train_acc = %.4f, eval_acc = %.4f,\' % (\n                phase, step, d_loss, train_acc, eval_acc))\n\n    def cal_metrics(self, fmt_str=False):\n        # Prepare data for evaluation\n        gen_data = GenDataIter(self.gen.sample(cfg.samples_num, cfg.batch_size, self.dis))\n\n        # Reset metrics\n        self.nll_oracle.reset(self.oracle, gen_data.loader)\n        self.nll_gen.reset(self.gen, self.oracle_data.loader, leak_dis=self.dis)\n        self.nll_div.reset(self.gen, gen_data.loader, leak_dis=self.dis)\n\n        if fmt_str:\n            return \', \'.join([\'%s = %s\' % (metric.get_name(), metric.get_score()) for metric in self.all_metrics])\n        else:\n            return [metric.get_score() for metric in self.all_metrics]\n\n    def _save(self, phase, epoch):\n        torch.save(self.gen.state_dict(), cfg.save_model_root + \'gen_{}_{:05d}.pt\'.format(phase, epoch))\n        save_sample_path = cfg.save_samples_root + \'samples_{}_{:05d}.txt\'.format(phase, epoch)\n        samples = self.gen.sample(cfg.batch_size, cfg.batch_size, self.dis)\n        write_tensor(save_sample_path, samples)\n'"
instructor/oracle_data/maligan_instructor.py,8,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : maligan_instructor.py\n# @Time         : Created at 2019/10/17\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\n\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.oracle_data.instructor import BasicInstructor\nfrom models.MaliGAN_D import MaliGAN_D\nfrom models.MaliGAN_G import MaliGAN_G\nfrom utils.data_loader import GenDataIter, DisDataIter\n\n\nclass MaliGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(MaliGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.gen = MaliGAN_G(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                             cfg.padding_idx, gpu=cfg.CUDA)\n        self.dis = MaliGAN_D(cfg.dis_embed_dim, cfg.vocab_size, cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.gen_adv_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n\n    def _run(self):\n        # ===PRE-TRAINING===\n        # TRAIN GENERATOR\n        if not cfg.gen_pretrain:\n            self.log.info(\'Starting Generator MLE Training...\')\n            self.pretrain_generator(cfg.MLE_train_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.gen.state_dict(), cfg.pretrained_gen_path)\n                print(\'Save pre-trained generator: {}\'.format(cfg.pretrained_gen_path))\n\n        # ===TRAIN DISCRIMINATOR====\n        if not cfg.dis_pretrain:\n            self.log.info(\'Starting Discriminator Training...\')\n            self.train_discriminator(cfg.d_step, cfg.d_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n                print(\'Save pre-trained discriminator: {}\'.format(cfg.pretrained_dis_path))\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        self.log.info(\'Initial generator: %s\' % (self.cal_metrics(fmt_str=True)))\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            self.log.info(\'-----\\nADV EPOCH %d\\n-----\' % adv_epoch)\n            self.sig.update()\n            if self.sig.adv_sig:\n                self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                self.train_discriminator(cfg.ADV_d_step, cfg.ADV_d_epoch, \'ADV\')  # Discriminator\n\n                if adv_epoch % cfg.adv_log_step == 0:\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                pre_loss = self.train_gen_epoch(self.gen, self.oracle_data.loader, self.mle_criterion, self.gen_opt)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\n                        \'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (epoch, pre_loss, self.cal_metrics(fmt_str=True)))\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        """"""\n        The gen is trained by MLE-like objective.\n        """"""\n        total_g_loss = 0\n        for step in range(g_step):\n            inp, target = GenDataIter.prepare(self.gen.sample(cfg.batch_size, cfg.batch_size), gpu=cfg.CUDA)\n\n            # ===Train===\n            rewards = self.get_mali_reward(target)\n            adv_loss = self.gen.adv_loss(inp, target, rewards)\n            self.optimize(self.gen_adv_opt, adv_loss)\n            total_g_loss += adv_loss.item()\n\n        # ===Test===\n        self.log.info(\'[ADV-GEN]: g_loss = %.4f, %s\' % (total_g_loss, self.cal_metrics(fmt_str=True)))\n\n    def train_discriminator(self, d_step, d_epoch, phase=\'MLE\'):\n        """"""\n        Training the discriminator on real_data_samples (positive) and generated samples from gen (negative).\n        Samples are drawn d_step times, and the discriminator is trained for d_epoch d_epoch.\n        """"""\n        # prepare loader for validate\n        global d_loss, train_acc\n        pos_val = self.oracle.sample(8 * cfg.batch_size, 4 * cfg.batch_size)\n        neg_val = self.gen.sample(8 * cfg.batch_size, 4 * cfg.batch_size)\n        dis_eval_data = DisDataIter(pos_val, neg_val)\n\n        for step in range(d_step):\n            # prepare loader for training\n            pos_samples = self.oracle_samples  # not re-sample the Oracle data\n            neg_samples = self.gen.sample(cfg.samples_num, 4 * cfg.batch_size)\n            dis_data = DisDataIter(pos_samples, neg_samples)\n\n            for epoch in range(d_epoch):\n                # ===Train===\n                d_loss, train_acc = self.train_dis_epoch(self.dis, dis_data.loader, self.dis_criterion,\n                                                         self.dis_opt)\n\n            # ===Test===\n            _, eval_acc = self.eval_dis(self.dis, dis_eval_data.loader, self.dis_criterion)\n            self.log.info(\'[%s-DIS] d_step %d: d_loss = %.4f, train_acc = %.4f, eval_acc = %.4f,\' % (\n                phase, step, d_loss, train_acc, eval_acc))\n\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n\n    def get_mali_reward(self, samples):\n        rewards = []\n        for _ in range(cfg.rollout_num):\n            dis_out = self.dis(samples)[:, 1]\n            rewards.append(dis_out)\n\n        rewards = torch.mean(torch.stack(rewards, dim=0), dim=0)  # batch_size\n        rewards = torch.div(rewards, 1 - rewards)\n        rewards = torch.div(rewards, torch.sum(rewards))\n        rewards -= torch.mean(rewards)\n        rewards = rewards.unsqueeze(1).expand(samples.size())  # batch_size * seq_len\n\n        return rewards\n'"
instructor/oracle_data/relgan_instructor.py,5,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : relgan_instructor.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nimport config as cfg\nfrom instructor.oracle_data.instructor import BasicInstructor\nfrom models.RelGAN_D import RelGAN_D\nfrom models.RelGAN_G import RelGAN_G\nfrom utils.helpers import get_fixed_temperature, get_losses\n\n\nclass RelGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(RelGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.gen = RelGAN_G(cfg.mem_slots, cfg.num_heads, cfg.head_size, cfg.gen_embed_dim, cfg.gen_hidden_dim,\n                            cfg.vocab_size, cfg.max_seq_len, cfg.padding_idx, gpu=cfg.CUDA)\n        self.dis = RelGAN_D(cfg.dis_embed_dim, cfg.max_seq_len, cfg.num_rep, cfg.vocab_size, cfg.padding_idx,\n                            gpu=cfg.CUDA)\n\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.gen_adv_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_adv_lr)\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n\n    def _run(self):\n        # ===PRE-TRAINING (GENERATOR)===\n        if not cfg.gen_pretrain:\n            self.log.info(\'Starting Generator MLE Training...\')\n            self.pretrain_generator(cfg.MLE_train_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.gen.state_dict(), cfg.pretrained_gen_path)\n                print(\'Save pre-trained generator: {}\'.format(cfg.pretrained_gen_path))\n\n        # # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        progress = tqdm(range(cfg.ADV_train_epoch))\n        for adv_epoch in progress:\n            self.sig.update()\n            if self.sig.adv_sig:\n                g_loss = self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                d_loss = self.adv_train_discriminator(cfg.ADV_d_step)  # Discriminator\n                self.update_temperature(adv_epoch, cfg.ADV_train_epoch)  # update temperature\n\n                progress.set_description(\n                    \'g_loss: %.4f, d_loss: %.4f, temperature: %.4f\' % (g_loss, d_loss, self.gen.temperature))\n\n                # TEST\n                if adv_epoch % cfg.adv_log_step == 0:\n                    self.log.info(\'[ADV] epoch %d: g_loss: %.4f, d_loss: %.4f, %s\' % (\n                        adv_epoch, g_loss, d_loss, self.cal_metrics(fmt_str=True)))\n\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                progress.close()\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                # ===Train===\n                pre_loss = self.train_gen_epoch(self.gen, self.oracle_data.loader, self.mle_criterion, self.gen_opt)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\n                        \'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (epoch, pre_loss, self.cal_metrics(fmt_str=True)))\n\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        total_loss = 0\n        for step in range(g_step):\n            real_samples = F.one_hot(self.oracle_data.random_batch()[\'target\'], cfg.vocab_size).float()\n            gen_samples = self.gen.sample(cfg.batch_size, cfg.batch_size, one_hot=True)\n            if cfg.CUDA:\n                real_samples, gen_samples = real_samples.cuda(), gen_samples.cuda()\n\n            # ===Train===\n            d_out_real = self.dis(real_samples)\n            d_out_fake = self.dis(gen_samples)\n            g_loss, _ = get_losses(d_out_real, d_out_fake, cfg.loss_type)\n\n            self.optimize(self.gen_adv_opt, g_loss, self.gen)\n            total_loss += g_loss.item()\n\n        return total_loss / g_step if g_step != 0 else 0\n\n    def adv_train_discriminator(self, d_step):\n        total_loss = 0\n        for step in range(d_step):\n            real_samples = F.one_hot(self.oracle_data.random_batch()[\'target\'], cfg.vocab_size).float()\n            gen_samples = self.gen.sample(cfg.batch_size, cfg.batch_size, one_hot=True)\n            if cfg.CUDA:\n                real_samples, gen_samples = real_samples.cuda(), gen_samples.cuda()\n\n            # ===Train===\n            d_out_real = self.dis(real_samples)\n            d_out_fake = self.dis(gen_samples)\n            _, d_loss = get_losses(d_out_real, d_out_fake, cfg.loss_type)\n\n            self.optimize(self.dis_opt, d_loss, self.dis)\n            total_loss += d_loss.item()\n\n        return total_loss / d_step if d_step != 0 else 0\n\n    def update_temperature(self, i, N):\n        self.gen.temperature = get_fixed_temperature(cfg.temperature, i, N, cfg.temp_adpt)\n\n    @staticmethod\n    def optimize(opt, loss, model=None, retain_graph=False):\n        """"""Add clip_grad_norm_""""""\n        opt.zero_grad()\n        loss.backward(retain_graph=retain_graph)\n        if model is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_norm)\n        opt.step()\n'"
instructor/oracle_data/sentigan_instructor.py,9,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : sentigan_instructor.py\n# @Time         : Created at 2019-07-26\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport os\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.oracle_data.instructor import BasicInstructor\nfrom models.Oracle import Oracle\nfrom models.SentiGAN_D import SentiGAN_D\nfrom models.SentiGAN_G import SentiGAN_G\nfrom utils import rollout\nfrom utils.cat_data_loader import CatClasDataIter\nfrom utils.data_loader import GenDataIter\nfrom utils.data_utils import create_multi_oracle\nfrom utils.text_process import write_tensor\n\n\nclass SentiGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(SentiGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.oracle_list = [Oracle(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                                   cfg.padding_idx, gpu=cfg.CUDA) for _ in range(cfg.k_label)]\n\n        self.gen_list = [SentiGAN_G(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                                    cfg.padding_idx, gpu=cfg.CUDA) for _ in range(cfg.k_label)]\n        self.dis = SentiGAN_D(cfg.k_label, cfg.dis_embed_dim, cfg.vocab_size, cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt_list = [optim.Adam(gen.parameters(), lr=cfg.gen_lr) for gen in self.gen_list]\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n\n    def init_model(self):\n        if cfg.oracle_pretrain:\n            for i in range(cfg.k_label):\n                oracle_path = cfg.multi_oracle_state_dict_path.format(i)\n                if not os.path.exists(oracle_path):\n                    create_multi_oracle(cfg.k_label)\n                self.oracle_list[i].load_state_dict(torch.load(oracle_path))\n\n        if cfg.dis_pretrain:\n            self.log.info(\n                \'Load pretrained discriminator: {}\'.format(cfg.pretrained_dis_path))\n            self.dis.load_state_dict(torch.load(cfg.pretrained_dis_path))\n        if cfg.gen_pretrain:\n            for i in range(cfg.k_label):\n                self.log.info(\'Load MLE pretrained generator gen: {}\'.format(cfg.pretrained_gen_path + \'%d\' % i))\n                self.gen_list[i].load_state_dict(torch.load(cfg.pretrained_gen_path + \'%d\' % i))\n\n        if cfg.CUDA:\n            for i in range(cfg.k_label):\n                self.oracle_list[i] = self.oracle_list[i].cuda()\n                self.gen_list[i] = self.gen_list[i].cuda()\n            self.dis = self.dis.cuda()\n\n    def _run(self):\n        # ===PRE-TRAIN GENERATOR===\n        if not cfg.gen_pretrain:\n            self.log.info(\'Starting Generator MLE Training...\')\n            self.pretrain_generator(cfg.MLE_train_epoch)\n            if cfg.if_save and not cfg.if_test:\n                for i in range(cfg.k_label):\n                    torch.save(self.gen_list[i].state_dict(), cfg.pretrained_gen_path + \'%d\' % i)\n                    print(\'Save pre-trained generator: {}\'.format(cfg.pretrained_gen_path + \'%d\' % i))\n\n        # ===TRAIN DISCRIMINATOR====\n        if not cfg.dis_pretrain:\n            self.log.info(\'Starting Discriminator Training...\')\n            self.train_discriminator(cfg.d_step, cfg.d_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n                print(\'Save pre-trained discriminator: {}\'.format(cfg.pretrained_dis_path))\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        self.log.info(\'Initial generator: %s\', self.comb_metrics(fmt_str=True))\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            self.log.info(\'-----\\nADV EPOCH %d\\n-----\' % adv_epoch)\n            self.sig.update()\n            if self.sig.adv_sig:\n                self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                self.train_discriminator(cfg.ADV_d_step, cfg.ADV_d_epoch, \'ADV\')  # Discriminator\n\n                if adv_epoch % cfg.adv_log_step == 0:\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                for i in range(cfg.k_label):\n                    pre_loss = self.train_gen_epoch(self.gen_list[i], self.oracle_data_list[i].loader,\n                                                    self.mle_criterion, self.gen_opt_list[i])\n\n                    # ===Test===\n                    if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                        if i == cfg.k_label - 1:\n                            self.log.info(\'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (\n                                epoch, pre_loss, self.comb_metrics(fmt_str=True)))\n                            if cfg.if_save and not cfg.if_test:\n                                self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        """"""\n        The gen is trained using policy gradients, using the reward from the discriminator.\n        Training is done for num_batches batches.\n        """"""\n        for i in range(cfg.k_label):\n            rollout_func = rollout.ROLLOUT(self.gen_list[i], cfg.CUDA)\n            total_g_loss = 0\n            for step in range(g_step):\n                inp, target = GenDataIter.prepare(self.gen_list[i].sample(cfg.batch_size, cfg.batch_size), gpu=cfg.CUDA)\n\n                # ===Train===\n                rewards = rollout_func.get_reward(target, cfg.rollout_num, self.dis)\n                adv_loss = self.gen_list[i].batchPGLoss(inp, target, rewards)\n                self.optimize(self.gen_opt_list[i], adv_loss)\n                total_g_loss += adv_loss.item()\n\n        # ===Test===\n        self.log.info(\'[ADV-GEN]: %s\', self.comb_metrics(fmt_str=True))\n\n    def train_discriminator(self, d_step, d_epoch, phase=\'MLE\'):\n        """"""\n        Training the discriminator on real_data_samples (positive) and generated samples from gen (negative).\n        Samples are drawn d_step times, and the discriminator is trained for d_epoch d_epoch.\n        """"""\n        # prepare loader for validate\n        global d_loss, train_acc\n\n        for step in range(d_step):\n            # prepare loader for training\n            real_samples = []\n            fake_samples = []\n            for i in range(cfg.k_label):\n                real_samples.append(self.oracle_samples_list[i])\n                fake_samples.append(self.gen_list[i].sample(cfg.samples_num // cfg.k_label, 8 * cfg.batch_size))\n\n            dis_samples_list = [torch.cat(fake_samples, dim=0)] + real_samples\n            dis_data = CatClasDataIter(dis_samples_list)\n\n            for epoch in range(d_epoch):\n                # ===Train===\n                d_loss, train_acc = self.train_dis_epoch(self.dis, dis_data.loader, self.dis_criterion,\n                                                         self.dis_opt)\n\n            # ===Test===\n            self.log.info(\'[%s-DIS] d_step %d: d_loss = %.4f, train_acc = %.4f\' % (\n                phase, step, d_loss, train_acc))\n\n            if cfg.if_save and not cfg.if_test and phase == \'MLE\':\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n\n    def cal_metrics_with_label(self, label_i):\n        assert type(label_i) == int, \'missing label\'\n        # Prepare data for evaluation\n        eval_samples = self.gen_list[label_i].sample(cfg.samples_num, 8 * cfg.batch_size)\n        gen_data = GenDataIter(eval_samples)\n\n        # Reset metrics\n        self.nll_oracle.reset(self.oracle_list[label_i], gen_data.loader)\n        self.nll_gen.reset(self.gen_list[label_i], self.oracle_data_list[label_i].loader)\n        self.nll_div.reset(self.gen_list[label_i], gen_data.loader)\n\n        return [metric.get_score() for metric in self.all_metrics]\n\n    def _save(self, phase, epoch):\n        """"""Save model state dict and generator\'s samples""""""\n        for i in range(cfg.k_label):\n            torch.save(self.gen_list[i].state_dict(),\n                       cfg.save_model_root + \'gen{}_{}_{:05d}.pt\'.format(i, phase, epoch))\n            save_sample_path = cfg.save_samples_root + \'samples_d{}_{}_{:05d}.txt\'.format(i, phase, epoch)\n            samples = self.gen_list[i].sample(cfg.batch_size, cfg.batch_size)\n            write_tensor(save_sample_path, samples)\n'"
instructor/oracle_data/seqgan_instructor.py,4,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : seqgan_instructor.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.oracle_data.instructor import BasicInstructor\nfrom models.SeqGAN_D import SeqGAN_D\nfrom models.SeqGAN_G import SeqGAN_G\nfrom utils import rollout\nfrom utils.data_loader import GenDataIter, DisDataIter\n\n\nclass SeqGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(SeqGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.gen = SeqGAN_G(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                            cfg.padding_idx, gpu=cfg.CUDA)\n        self.dis = SeqGAN_D(cfg.dis_embed_dim, cfg.vocab_size, cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.gen_adv_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n\n    def _run(self):\n        # ===PRE-TRAINING===\n        # TRAIN GENERATOR\n        if not cfg.gen_pretrain:\n            self.log.info(\'Starting Generator MLE Training...\')\n            self.pretrain_generator(cfg.MLE_train_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.gen.state_dict(), cfg.pretrained_gen_path)\n                print(\'Save pre-trained generator: {}\'.format(cfg.pretrained_gen_path))\n\n        # ===TRAIN DISCRIMINATOR====\n        if not cfg.dis_pretrain:\n            self.log.info(\'Starting Discriminator Training...\')\n            self.train_discriminator(cfg.d_step, cfg.d_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n                print(\'Save pre-trained discriminator: {}\'.format(cfg.pretrained_dis_path))\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        self.log.info(\'Initial generator: %s\' % (self.cal_metrics(fmt_str=True)))\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            self.log.info(\'-----\\nADV EPOCH %d\\n-----\' % adv_epoch)\n            self.sig.update()\n            if self.sig.adv_sig:\n                self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                self.train_discriminator(cfg.ADV_d_step, cfg.ADV_d_epoch, \'ADV\')  # Discriminator\n\n                if adv_epoch % cfg.adv_log_step == 0:\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                pre_loss = self.train_gen_epoch(self.gen, self.oracle_data.loader, self.mle_criterion, self.gen_opt)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\n                        \'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (epoch, pre_loss, self.cal_metrics(fmt_str=True)))\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        """"""\n        The gen is trained using policy gradients, using the reward from the discriminator.\n        Training is done for num_batches batches.\n        """"""\n        rollout_func = rollout.ROLLOUT(self.gen, cfg.CUDA)\n        total_g_loss = 0\n        for step in range(g_step):\n            inp, target = GenDataIter.prepare(self.gen.sample(cfg.batch_size, cfg.batch_size), gpu=cfg.CUDA)\n\n            # ===Train===\n            rewards = rollout_func.get_reward(target, cfg.rollout_num, self.dis)\n            adv_loss = self.gen.batchPGLoss(inp, target, rewards)\n            self.optimize(self.gen_adv_opt, adv_loss)\n            total_g_loss += adv_loss.item()\n\n        # ===Test===\n        self.log.info(\'[ADV-GEN]: g_loss = %.4f, %s\' % (total_g_loss, self.cal_metrics(fmt_str=True)))\n\n    def train_discriminator(self, d_step, d_epoch, phase=\'MLE\'):\n        """"""\n        Training the discriminator on real_data_samples (positive) and generated samples from gen (negative).\n        Samples are drawn d_step times, and the discriminator is trained for d_epoch d_epoch.\n        """"""\n        # prepare loader for validate\n        global d_loss, train_acc\n        pos_val = self.oracle.sample(8 * cfg.batch_size, 4 * cfg.batch_size)\n        neg_val = self.gen.sample(8 * cfg.batch_size, 4 * cfg.batch_size)\n        dis_eval_data = DisDataIter(pos_val, neg_val)\n\n        for step in range(d_step):\n            # prepare loader for training\n            pos_samples = self.oracle_samples  # not re-sample the Oracle data\n            neg_samples = self.gen.sample(cfg.samples_num, 4 * cfg.batch_size)\n            dis_data = DisDataIter(pos_samples, neg_samples)\n\n            for epoch in range(d_epoch):\n                # ===Train===\n                d_loss, train_acc = self.train_dis_epoch(self.dis, dis_data.loader, self.dis_criterion,\n                                                         self.dis_opt)\n\n            # ===Test===\n            _, eval_acc = self.eval_dis(self.dis, dis_eval_data.loader, self.dis_criterion)\n            self.log.info(\'[%s-DIS] d_step %d: d_loss = %.4f, train_acc = %.4f, eval_acc = %.4f,\' % (\n                phase, step, d_loss, train_acc, eval_acc))\n\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n'"
instructor/real_data/instructor.py,11,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : instructor.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport config as cfg\nfrom metrics.bleu import BLEU\nfrom metrics.clas_acc import ACC\nfrom metrics.nll import NLL\nfrom metrics.ppl import PPL\nfrom utils.cat_data_loader import CatClasDataIter\nfrom utils.data_loader import GenDataIter\nfrom utils.helpers import Signal, create_logger, get_fixed_temperature\nfrom utils.text_process import load_dict, write_tokens, tensor_to_tokens\n\n\nclass BasicInstructor:\n    def __init__(self, opt):\n        self.log = create_logger(__name__, silent=False, to_disk=True,\n                                 log_file=cfg.log_filename if cfg.if_test\n                                 else [cfg.log_filename, cfg.save_root + \'log.txt\'])\n        self.sig = Signal(cfg.signal_file)\n        self.opt = opt\n        self.show_config()\n\n        self.clas = None\n\n        # load dictionary\n        self.word2idx_dict, self.idx2word_dict = load_dict(cfg.dataset)\n\n        # Dataloader\n        try:\n            self.train_data = GenDataIter(cfg.train_data)\n            self.test_data = GenDataIter(cfg.test_data, if_test_data=True)\n        except:\n            pass\n\n        try:\n            self.train_data_list = [GenDataIter(cfg.cat_train_data.format(i)) for i in range(cfg.k_label)]\n            self.test_data_list = [GenDataIter(cfg.cat_test_data.format(i), if_test_data=True) for i in\n                                   range(cfg.k_label)]\n            self.clas_data_list = [GenDataIter(cfg.cat_test_data.format(str(i)), if_test_data=True) for i in\n                                   range(cfg.k_label)]\n\n            self.train_samples_list = [self.train_data_list[i].target for i in range(cfg.k_label)]\n            self.clas_samples_list = [self.clas_data_list[i].target for i in range(cfg.k_label)]\n        except:\n            pass\n\n        # Criterion\n        self.mle_criterion = nn.NLLLoss()\n        self.dis_criterion = nn.CrossEntropyLoss()\n        self.clas_criterion = nn.CrossEntropyLoss()\n\n        # Optimizer\n        self.clas_opt = None\n\n        # Metrics\n        self.bleu = BLEU(\'BLEU\', gram=[2, 3, 4, 5], if_use=cfg.use_bleu)\n        self.nll_gen = NLL(\'NLL_gen\', if_use=cfg.use_nll_gen, gpu=cfg.CUDA)\n        self.nll_div = NLL(\'NLL_div\', if_use=cfg.use_nll_div, gpu=cfg.CUDA)\n        self.self_bleu = BLEU(\'Self-BLEU\', gram=[2, 3, 4], if_use=cfg.use_self_bleu)\n        self.clas_acc = ACC(if_use=cfg.use_clas_acc)\n        self.ppl = PPL(self.train_data, self.test_data, n_gram=5, if_use=cfg.use_ppl)\n        self.all_metrics = [self.bleu, self.nll_gen, self.nll_div, self.self_bleu, self.ppl]\n\n    def _run(self):\n        print(\'Nothing to run in Basic Instructor!\')\n        pass\n\n    def _test(self):\n        pass\n\n    def init_model(self):\n        if cfg.dis_pretrain:\n            self.log.info(\n                \'Load pre-trained discriminator: {}\'.format(cfg.pretrained_dis_path))\n            self.dis.load_state_dict(torch.load(cfg.pretrained_dis_path))\n        if cfg.gen_pretrain:\n            self.log.info(\'Load MLE pre-trained generator: {}\'.format(cfg.pretrained_gen_path))\n            self.gen.load_state_dict(torch.load(cfg.pretrained_gen_path))\n\n        if cfg.CUDA:\n            self.gen = self.gen.cuda()\n            self.dis = self.dis.cuda()\n\n    def train_gen_epoch(self, model, data_loader, criterion, optimizer):\n        total_loss = 0\n        for i, data in enumerate(data_loader):\n            inp, target = data[\'input\'], data[\'target\']\n            if cfg.CUDA:\n                inp, target = inp.cuda(), target.cuda()\n\n            hidden = model.init_hidden(data_loader.batch_size)\n            pred = model.forward(inp, hidden)\n            loss = criterion(pred, target.view(-1))\n            self.optimize(optimizer, loss, model)\n            total_loss += loss.item()\n        return total_loss / len(data_loader)\n\n    def train_dis_epoch(self, model, data_loader, criterion, optimizer):\n        total_loss = 0\n        total_acc = 0\n        total_num = 0\n        for i, data in enumerate(data_loader):\n            inp, target = data[\'input\'], data[\'target\']\n            if cfg.CUDA:\n                inp, target = inp.cuda(), target.cuda()\n\n            pred = model.forward(inp)\n            loss = criterion(pred, target)\n            self.optimize(optimizer, loss, model)\n\n            total_loss += loss.item()\n            total_acc += torch.sum((pred.argmax(dim=-1) == target)).item()\n            total_num += inp.size(0)\n\n        total_loss /= len(data_loader)\n        total_acc /= total_num\n        return total_loss, total_acc\n\n    def train_classifier(self, epochs):\n        """"""\n        Classifier for calculating the classification accuracy metric of category text generation.\n\n        Note: the train and test data for the classifier is opposite to the generator.\n        Because the classifier is to calculate the classification accuracy of the generated samples\n        where are trained on self.train_samples_list.\n\n        Since there\'s no test data in synthetic data (oracle data), the synthetic data experiments\n        doesn\'t need a classifier.\n        """"""\n        import copy\n\n        # Prepare data for Classifier\n        clas_data = CatClasDataIter(self.clas_samples_list)\n        eval_clas_data = CatClasDataIter(self.train_samples_list)\n\n        max_acc = 0\n        best_clas = None\n        for epoch in range(epochs):\n            c_loss, c_acc = self.train_dis_epoch(self.clas, clas_data.loader, self.clas_criterion,\n                                                 self.clas_opt)\n            _, eval_acc = self.eval_dis(self.clas, eval_clas_data.loader, self.clas_criterion)\n            if eval_acc > max_acc:\n                best_clas = copy.deepcopy(self.clas.state_dict())  # save the best classifier\n                max_acc = eval_acc\n            self.log.info(\'[PRE-CLAS] epoch %d: c_loss = %.4f, c_acc = %.4f, eval_acc = %.4f, max_eval_acc = %.4f\',\n                          epoch, c_loss, c_acc, eval_acc, max_acc)\n        self.clas.load_state_dict(copy.deepcopy(best_clas))  # Reload the best classifier\n\n    @staticmethod\n    def eval_dis(model, data_loader, criterion):\n        total_loss = 0\n        total_acc = 0\n        total_num = 0\n        with torch.no_grad():\n            for i, data in enumerate(data_loader):\n                inp, target = data[\'input\'], data[\'target\']\n                if cfg.CUDA:\n                    inp, target = inp.cuda(), target.cuda()\n\n                pred = model.forward(inp)\n                loss = criterion(pred, target)\n                total_loss += loss.item()\n                total_acc += torch.sum((pred.argmax(dim=-1) == target)).item()\n                total_num += inp.size(0)\n            total_loss /= len(data_loader)\n            total_acc /= total_num\n        return total_loss, total_acc\n\n    @staticmethod\n    def optimize_multi(opts, losses):\n        for i, (opt, loss) in enumerate(zip(opts, losses)):\n            opt.zero_grad()\n            loss.backward(retain_graph=True if i < len(opts) - 1 else False)\n            opt.step()\n\n    @staticmethod\n    def optimize(opt, loss, model=None, retain_graph=False):\n        opt.zero_grad()\n        loss.backward(retain_graph=retain_graph)\n        if model is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_norm)\n        opt.step()\n\n    def show_config(self):\n        self.log.info(100 * \'=\')\n        self.log.info(\'> training arguments:\')\n        for arg in vars(self.opt):\n            self.log.info(\'>>> {0}: {1}\'.format(arg, getattr(self.opt, arg)))\n        self.log.info(100 * \'=\')\n\n    def cal_metrics(self, fmt_str=False):\n        """"""\n        Calculate metrics\n        :param fmt_str: if return format string for logging\n        """"""\n        with torch.no_grad():\n            # Prepare data for evaluation\n            eval_samples = self.gen.sample(cfg.samples_num, 4 * cfg.batch_size)\n            gen_data = GenDataIter(eval_samples)\n            gen_tokens = tensor_to_tokens(eval_samples, self.idx2word_dict)\n            gen_tokens_s = tensor_to_tokens(self.gen.sample(200, 200), self.idx2word_dict)\n\n            # Reset metrics\n            self.bleu.reset(test_text=gen_tokens, real_text=self.test_data.tokens)\n            self.nll_gen.reset(self.gen, self.train_data.loader)\n            self.nll_div.reset(self.gen, gen_data.loader)\n            self.self_bleu.reset(test_text=gen_tokens_s, real_text=gen_tokens)\n            self.ppl.reset(gen_tokens)\n\n        if fmt_str:\n            return \', \'.join([\'%s = %s\' % (metric.get_name(), metric.get_score()) for metric in self.all_metrics])\n        else:\n            return [metric.get_score() for metric in self.all_metrics]\n\n    def cal_metrics_with_label(self, label_i):\n        assert type(label_i) == int, \'missing label\'\n\n        with torch.no_grad():\n            # Prepare data for evaluation\n            eval_samples = self.gen.sample(cfg.samples_num, 8 * cfg.batch_size, label_i=label_i)\n            gen_data = GenDataIter(eval_samples)\n            gen_tokens = tensor_to_tokens(eval_samples, self.idx2word_dict)\n            gen_tokens_s = tensor_to_tokens(self.gen.sample(200, 200, label_i=label_i), self.idx2word_dict)\n            clas_data = CatClasDataIter([eval_samples], label_i)\n\n            # Reset metrics\n            self.bleu.reset(test_text=gen_tokens, real_text=self.test_data_list[label_i].tokens)\n            self.nll_gen.reset(self.gen, self.train_data_list[label_i].loader, label_i)\n            self.nll_div.reset(self.gen, gen_data.loader, label_i)\n            self.self_bleu.reset(test_text=gen_tokens_s, real_text=gen_tokens)\n            self.clas_acc.reset(self.clas, clas_data.loader)\n            self.ppl.reset(gen_tokens)\n\n        return [metric.get_score() for metric in self.all_metrics]\n\n    def comb_metrics(self, fmt_str=False):\n        all_scores = [self.cal_metrics_with_label(label_i) for label_i in range(cfg.k_label)]\n        all_scores = np.array(all_scores).T.tolist()  # each row for each metric\n\n        if fmt_str:\n            return \', \'.join([\'%s = %s\' % (metric.get_name(), score)\n                              for (metric, score) in zip(self.all_metrics, all_scores)])\n        return all_scores\n\n    def _save(self, phase, epoch):\n        """"""Save model state dict and generator\'s samples""""""\n        if phase != \'ADV\':\n            torch.save(self.gen.state_dict(), cfg.save_model_root + \'gen_{}_{:05d}.pt\'.format(phase, epoch))\n        save_sample_path = cfg.save_samples_root + \'samples_{}_{:05d}.txt\'.format(phase, epoch)\n        samples = self.gen.sample(cfg.batch_size, cfg.batch_size)\n        write_tokens(save_sample_path, tensor_to_tokens(samples, self.idx2word_dict))\n\n    def update_temperature(self, i, N):\n        self.gen.temperature.data = torch.Tensor([get_fixed_temperature(cfg.temperature, i, N, cfg.temp_adpt)])\n        if cfg.CUDA:\n            self.gen.temperature.data = self.gen.temperature.data.cuda()\n'"
instructor/real_data/jsdgan_instructor.py,2,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : JSDGAN_instructor.py\n# @Time         : Created at 2019/11/25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.real_data.instructor import BasicInstructor\nfrom models.JSDGAN_G import JSDGAN_G\n\n\nclass JSDGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(JSDGANInstructor, self).__init__(opt)\n\n        # generator\n        self.gen = JSDGAN_G(cfg.mem_slots, cfg.num_heads, cfg.head_size, cfg.gen_embed_dim, cfg.gen_hidden_dim,\n                            cfg.vocab_size, cfg.max_seq_len, cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n\n    def init_model(self):\n        if cfg.gen_pretrain:\n            self.log.info(\'Load MLE pretrained generator gen: {}\'.format(cfg.pretrained_gen_path))\n            self.gen.load_state_dict(torch.load(cfg.pretrained_gen_path, map_location=\'cuda:{}\'.format(cfg.device)))\n\n        if cfg.CUDA:\n            self.gen = self.gen.cuda()\n\n    def _run(self):\n        # ===PRE-TRAINING===\n        # TRAIN GENERATOR\n        self.log.info(\'Starting Generator MLE Training...\')\n        self.pretrain_generator(cfg.MLE_train_epoch)\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            g_loss = self.adv_train_generator(cfg.ADV_g_step)  # Generator\n\n            if adv_epoch % cfg.adv_log_step == 0:\n                self.log.info(\'[ADV] epoch %d: g_loss = %.4f, %s\' % (adv_epoch, g_loss, self.cal_metrics(fmt_str=True)))\n\n                if cfg.if_save and not cfg.if_test:\n                    self._save(\'ADV\', adv_epoch)\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                pre_loss = self.train_gen_epoch(self.gen, self.train_data.loader, self.mle_criterion, self.gen_opt)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\n                        \'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (epoch, pre_loss, self.cal_metrics(fmt_str=True)))\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        """"""\n        The gen is trained using policy gradients, using the reward from the discriminator.\n        Training is done for num_batches batches.\n        """"""\n        global inp, target\n        total_loss = 0\n        for step in range(g_step):\n            for i, data in enumerate(self.train_data.loader):\n                inp, target = data[\'input\'], data[\'target\']\n                if cfg.CUDA:\n                    inp, target = inp.cuda(), target.cuda()\n\n                # ===Train===\n                adv_loss = self.gen.JSD_loss(inp, target)\n                self.optimize(self.gen_opt, adv_loss, self.gen)\n                total_loss += adv_loss.item()\n\n        return total_loss\n'"
instructor/real_data/leakgan_instructor.py,6,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : leakgan_instructor.py\n# @Time         : Created at 2019-06-05\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.real_data.instructor import BasicInstructor\nfrom models.LeakGAN_D import LeakGAN_D\nfrom models.LeakGAN_G import LeakGAN_G\nfrom utils import rollout\nfrom utils.data_loader import GenDataIter, DisDataIter\nfrom utils.text_process import tensor_to_tokens, write_tokens\n\n\nclass LeakGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(LeakGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.gen = LeakGAN_G(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                             cfg.padding_idx, cfg.goal_size, cfg.step_size, cfg.CUDA)\n        self.dis = LeakGAN_D(cfg.dis_embed_dim, cfg.vocab_size, cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # optimizer\n        mana_params, work_params = self.gen.split_params()\n        mana_opt = optim.Adam(mana_params, lr=cfg.gen_lr)\n        work_opt = optim.Adam(work_params, lr=cfg.gen_lr)\n\n        self.gen_opt = [mana_opt, work_opt]\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n\n    def _run(self):\n        for inter_num in range(cfg.inter_epoch):\n            self.log.info(\'>>> Interleaved Round %d...\' % inter_num)\n            self.sig.update()  # update signal\n            if self.sig.pre_sig:\n                # ===DISCRIMINATOR PRE-TRAINING===\n                if not cfg.dis_pretrain:\n                    self.log.info(\'Starting Discriminator Training...\')\n                    self.train_discriminator(cfg.d_step, cfg.d_epoch)\n                    if cfg.if_save and not cfg.if_test:\n                        torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n                        print(\'Save pre-trained discriminator: {}\'.format(cfg.pretrained_dis_path))\n\n                # ===GENERATOR MLE TRAINING===\n                if not cfg.gen_pretrain:\n                    self.log.info(\'Starting Generator MLE Training...\')\n                    self.pretrain_generator(cfg.MLE_train_epoch)\n                    if cfg.if_save and not cfg.if_test:\n                        torch.save(self.gen.state_dict(), cfg.pretrained_gen_path)\n                        print(\'Save pre-trained generator: {}\'.format(cfg.pretrained_gen_path))\n            else:\n                self.log.info(\'>>> Stop by pre_signal! Skip to adversarial training...\')\n                break\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        self.log.info(\'Initial generator: %s\' % (str(self.cal_metrics(fmt_str=True))))\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            self.log.info(\'-----\\nADV EPOCH %d\\n-----\' % adv_epoch)\n            self.sig.update()\n            if self.sig.adv_sig:\n                self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                self.train_discriminator(cfg.ADV_d_step, cfg.ADV_d_epoch, \'ADV\')  # Discriminator\n\n                if adv_epoch % cfg.adv_log_step == 0:\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pretraining for the gen\n\n        - gen_opt: [mana_opt, work_opt]\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                pre_mana_loss = 0\n                pre_work_loss = 0\n\n                # ===Train===\n                for i, data in enumerate(self.train_data.loader):\n                    inp, target = data[\'input\'], data[\'target\']\n                    if cfg.CUDA:\n                        inp, target = inp.cuda(), target.cuda()\n\n                    mana_loss, work_loss = self.gen.pretrain_loss(target, self.dis)\n                    self.optimize_multi(self.gen_opt, [mana_loss, work_loss])\n                    pre_mana_loss += mana_loss.data.item()\n                    pre_work_loss += work_loss.data.item()\n                pre_mana_loss = pre_mana_loss / len(self.train_data.loader)\n                pre_work_loss = pre_work_loss / len(self.train_data.loader)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\'[MLE-GEN] epoch %d : pre_mana_loss = %.4f, pre_work_loss = %.4f, %s\' % (\n                        epoch, pre_mana_loss, pre_work_loss, self.cal_metrics(fmt_str=True)))\n\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step, current_k=0):\n        """"""\n        The gen is trained using policy gradients, using the reward from the discriminator.\n        Training is done for num_batches batches.\n        """"""\n\n        rollout_func = rollout.ROLLOUT(self.gen, cfg.CUDA)\n        adv_mana_loss = 0\n        adv_work_loss = 0\n        for step in range(g_step):\n            with torch.no_grad():\n                gen_samples = self.gen.sample(cfg.batch_size, cfg.batch_size, self.dis,\n                                              train=True)  # !!! train=True, the only place\n                inp, target = GenDataIter.prepare(gen_samples, gpu=cfg.CUDA)\n\n            # ===Train===\n            rewards = rollout_func.get_reward_leakgan(target, cfg.rollout_num, self.dis,\n                                                      current_k).cpu()  # reward with MC search\n            mana_loss, work_loss = self.gen.adversarial_loss(target, rewards, self.dis)\n\n            # update parameters\n            self.optimize_multi(self.gen_opt, [mana_loss, work_loss])\n            adv_mana_loss += mana_loss.data.item()\n            adv_work_loss += work_loss.data.item()\n        # ===Test===\n        self.log.info(\'[ADV-GEN] adv_mana_loss = %.4f, adv_work_loss = %.4f, %s\' % (\n            adv_mana_loss / g_step, adv_work_loss / g_step, self.cal_metrics(fmt_str=True)))\n\n    def train_discriminator(self, d_step, d_epoch, phase=\'MLE\'):\n        """"""\n        Training the discriminator on real_data_samples (positive) and generated samples from gen (negative).\n        Samples are drawn d_step times, and the discriminator is trained for d_epoch d_epoch.\n        """"""\n        d_loss, train_acc = 0, 0\n        for step in range(d_step):\n            # prepare loader for training\n            pos_samples = self.train_data.target\n            neg_samples = self.gen.sample(cfg.samples_num, cfg.batch_size, self.dis)\n            dis_data = DisDataIter(pos_samples, neg_samples)\n\n            for epoch in range(d_epoch):\n                # ===Train===\n                d_loss, train_acc = self.train_dis_epoch(self.dis, dis_data.loader, self.dis_criterion,\n                                                         self.dis_opt)\n\n            # ===Test===\n            self.log.info(\'[%s-DIS] d_step %d: d_loss = %.4f, train_acc = %.4f,\' % (\n                phase, step, d_loss, train_acc))\n\n    def cal_metrics(self, fmt_str=False):\n        with torch.no_grad():\n            # Prepare data for evaluation\n            eval_samples = self.gen.sample(cfg.samples_num, cfg.batch_size, self.dis)\n            gen_data = GenDataIter(eval_samples)\n            gen_tokens = tensor_to_tokens(eval_samples, self.idx2word_dict)\n            gen_tokens_s = tensor_to_tokens(self.gen.sample(200, cfg.batch_size, self.dis), self.idx2word_dict)\n\n            # Reset metrics\n            self.bleu.reset(test_text=gen_tokens, real_text=self.test_data.tokens)\n            self.nll_gen.reset(self.gen, self.train_data.loader, leak_dis=self.dis)\n            self.nll_div.reset(self.gen, gen_data.loader, leak_dis=self.dis)\n            self.self_bleu.reset(test_text=gen_tokens_s, real_text=gen_tokens)\n            self.ppl.reset(gen_tokens)\n\n        if fmt_str:\n            return \', \'.join([\'%s = %s\' % (metric.get_name(), metric.get_score()) for metric in self.all_metrics])\n        else:\n            return [metric.get_score() for metric in self.all_metrics]\n\n    def _save(self, phase, epoch):\n        torch.save(self.gen.state_dict(), cfg.save_model_root + \'gen_{}_{:05d}.pt\'.format(phase, epoch))\n        save_sample_path = cfg.save_samples_root + \'samples_{}_{:05d}.txt\'.format(phase, epoch)\n        samples = self.gen.sample(cfg.batch_size, cfg.batch_size, self.dis)\n        write_tokens(save_sample_path, tensor_to_tokens(samples, self.idx2word_dict))\n'"
instructor/real_data/maligan_instructor.py,8,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : maligan_instructor.py\n# @Time         : Created at 2019/11/29\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\n\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.real_data.instructor import BasicInstructor\nfrom models.MaliGAN_D import MaliGAN_D\nfrom models.MaliGAN_G import MaliGAN_G\nfrom utils.data_loader import GenDataIter, DisDataIter\n\n\n# noinspection PyUnresolvedReferences\nclass MaliGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(MaliGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.gen = MaliGAN_G(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                             cfg.padding_idx, gpu=cfg.CUDA)\n        self.dis = MaliGAN_D(cfg.dis_embed_dim, cfg.vocab_size, cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.gen_adv_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n\n    def _run(self):\n        # ===PRE-TRAINING===\n        # TRAIN GENERATOR\n        if not cfg.gen_pretrain:\n            self.log.info(\'Starting Generator MLE Training...\')\n            self.pretrain_generator(cfg.MLE_train_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.gen.state_dict(), cfg.pretrained_gen_path)\n                print(\'Save pre-trained generator: {}\'.format(cfg.pretrained_gen_path))\n\n        # ===TRAIN DISCRIMINATOR====\n        if not cfg.dis_pretrain:\n            self.log.info(\'Starting Discriminator Training...\')\n            self.train_discriminator(cfg.d_step, cfg.d_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n                print(\'Save pre-trained discriminator: {}\'.format(cfg.pretrained_dis_path))\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        self.log.info(\'Initial generator: %s\' % (self.cal_metrics(fmt_str=True)))\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            self.log.info(\'-----\\nADV EPOCH %d\\n-----\' % adv_epoch)\n            self.sig.update()\n            if self.sig.adv_sig:\n                self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                self.train_discriminator(cfg.ADV_d_step, cfg.ADV_d_epoch, \'ADV\')  # Discriminator\n\n                if adv_epoch % cfg.adv_log_step == 0:\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                pre_loss = self.train_gen_epoch(self.gen, self.train_data.loader, self.mle_criterion, self.gen_opt)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\n                        \'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (epoch, pre_loss, self.cal_metrics(fmt_str=True)))\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        """"""\n        The gen is trained by MLE-like objective.\n        """"""\n        total_g_loss = 0\n        for step in range(g_step):\n            inp, target = GenDataIter.prepare(self.gen.sample(cfg.batch_size, cfg.batch_size), gpu=cfg.CUDA)\n\n            # ===Train===\n            rewards = self.get_mali_reward(target)\n            adv_loss = self.gen.adv_loss(inp, target, rewards)\n            self.optimize(self.gen_adv_opt, adv_loss)\n            total_g_loss += adv_loss.item()\n\n        # ===Test===\n        self.log.info(\'[ADV-GEN]: g_loss = %.4f, %s\' % (total_g_loss, self.cal_metrics(fmt_str=True)))\n\n    def train_discriminator(self, d_step, d_epoch, phase=\'MLE\'):\n        """"""\n        Training the discriminator on real_data_samples (positive) and generated samples from gen (negative).\n        Samples are drawn d_step times, and the discriminator is trained for d_epoch d_epoch.\n        """"""\n        # prepare loader for validate\n        global d_loss, train_acc\n\n        for step in range(d_step):\n            # prepare loader for training\n            pos_samples = self.train_data.target  # not re-sample the Oracle data\n            neg_samples = self.gen.sample(cfg.samples_num, 4 * cfg.batch_size)\n            dis_data = DisDataIter(pos_samples, neg_samples)\n\n            for epoch in range(d_epoch):\n                # ===Train===\n                d_loss, train_acc = self.train_dis_epoch(self.dis, dis_data.loader, self.dis_criterion,\n                                                         self.dis_opt)\n\n            # ===Test===\n            self.log.info(\'[%s-DIS] d_step %d: d_loss = %.4f, train_acc = %.4f,\' % (\n                phase, step, d_loss, train_acc))\n\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n\n    def get_mali_reward(self, samples):\n        rewards = []\n        for _ in range(cfg.rollout_num):\n            dis_out = self.dis(samples)[:, 1]\n            rewards.append(dis_out)\n\n        rewards = torch.mean(torch.stack(rewards, dim=0), dim=0)  # batch_size\n        rewards = torch.div(rewards, 1 - rewards)\n        rewards = torch.div(rewards, torch.sum(rewards))\n        rewards -= torch.mean(rewards)\n        rewards = rewards.unsqueeze(1).expand(samples.size())  # batch_size * seq_len\n\n        return rewards\n'"
instructor/real_data/relgan_instructor.py,4,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : relgan_instructor.py\n# @Time         : Created at 2019-04-25\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nimport config as cfg\nfrom instructor.real_data.instructor import BasicInstructor\nfrom models.RelGAN_D import RelGAN_D\nfrom models.RelGAN_G import RelGAN_G\nfrom utils.helpers import get_fixed_temperature, get_losses\n\n\nclass RelGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(RelGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.gen = RelGAN_G(cfg.mem_slots, cfg.num_heads, cfg.head_size, cfg.gen_embed_dim, cfg.gen_hidden_dim,\n                            cfg.vocab_size, cfg.max_seq_len, cfg.padding_idx, gpu=cfg.CUDA)\n        self.dis = RelGAN_D(cfg.dis_embed_dim, cfg.max_seq_len, cfg.num_rep, cfg.vocab_size, cfg.padding_idx,\n                            gpu=cfg.CUDA)\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.gen_adv_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_adv_lr)\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n\n    def _run(self):\n        # ===PRE-TRAINING (GENERATOR)===\n        if not cfg.gen_pretrain:\n            self.log.info(\'Starting Generator MLE Training...\')\n            self.pretrain_generator(cfg.MLE_train_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.gen.state_dict(), cfg.pretrained_gen_path)\n                print(\'Save pretrain_generator: {}\'.format(cfg.pretrained_gen_path))\n\n        # # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        progress = tqdm(range(cfg.ADV_train_epoch))\n        for adv_epoch in progress:\n            self.sig.update()\n            if self.sig.adv_sig:\n                g_loss = self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                d_loss = self.adv_train_discriminator(cfg.ADV_d_step)  # Discriminator\n                self.update_temperature(adv_epoch, cfg.ADV_train_epoch)  # update temperature\n\n                progress.set_description(\n                    \'g_loss: %.4f, d_loss: %.4f, temperature: %.4f\' % (g_loss, d_loss, self.gen.temperature))\n\n                # TEST\n                if adv_epoch % cfg.adv_log_step == 0:\n                    self.log.info(\'[ADV] epoch %d: g_loss: %.4f, d_loss: %.4f, %s\' % (\n                        adv_epoch, g_loss, d_loss, self.cal_metrics(fmt_str=True)))\n\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                progress.close()\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                # ===Train===\n                pre_loss = self.train_gen_epoch(self.gen, self.train_data.loader, self.mle_criterion, self.gen_opt)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (\n                        epoch, pre_loss, self.cal_metrics(fmt_str=True)))\n\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        total_loss = 0\n        for step in range(g_step):\n            real_samples = self.train_data.random_batch()[\'target\']\n            gen_samples = self.gen.sample(cfg.batch_size, cfg.batch_size, one_hot=True)\n            if cfg.CUDA:\n                real_samples, gen_samples = real_samples.cuda(), gen_samples.cuda()\n            real_samples = F.one_hot(real_samples, cfg.vocab_size).float()\n\n            # ===Train===\n            d_out_real = self.dis(real_samples)\n            d_out_fake = self.dis(gen_samples)\n            g_loss, _ = get_losses(d_out_real, d_out_fake, cfg.loss_type)\n\n            self.optimize(self.gen_adv_opt, g_loss, self.gen)\n            total_loss += g_loss.item()\n\n        return total_loss / g_step if g_step != 0 else 0\n\n    def adv_train_discriminator(self, d_step):\n        total_loss = 0\n        for step in range(d_step):\n            real_samples = self.train_data.random_batch()[\'target\']\n            gen_samples = self.gen.sample(cfg.batch_size, cfg.batch_size, one_hot=True)\n            if cfg.CUDA:\n                real_samples, gen_samples = real_samples.cuda(), gen_samples.cuda()\n            real_samples = F.one_hot(real_samples, cfg.vocab_size).float()\n\n            # ===Train===\n            d_out_real = self.dis(real_samples)\n            d_out_fake = self.dis(gen_samples)\n            _, d_loss = get_losses(d_out_real, d_out_fake, cfg.loss_type)\n\n            self.optimize(self.dis_opt, d_loss, self.dis)\n            total_loss += d_loss.item()\n\n        return total_loss / d_step if d_step != 0 else 0\n\n    def update_temperature(self, i, N):\n        self.gen.temperature = get_fixed_temperature(cfg.temperature, i, N, cfg.temp_adpt)\n\n    @staticmethod\n    def optimize(opt, loss, model=None, retain_graph=False):\n        opt.zero_grad()\n        loss.backward(retain_graph=retain_graph)\n        if model is not None:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_norm)\n        opt.step()\n'"
instructor/real_data/sentigan_instructor.py,10,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : sentigan_instructor.py\n# @Time         : Created at 2019-07-09\n# @Blog         : http://zhiweil.ml/\n# @Description  :\n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.real_data.instructor import BasicInstructor\nfrom models.SentiGAN_D import SentiGAN_D, SentiGAN_C\nfrom models.SentiGAN_G import SentiGAN_G\nfrom utils import rollout\nfrom utils.cat_data_loader import CatClasDataIter\nfrom utils.data_loader import GenDataIter\nfrom utils.text_process import tensor_to_tokens, write_tokens\n\n\nclass SentiGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(SentiGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.gen_list = [SentiGAN_G(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                                    cfg.padding_idx, gpu=cfg.CUDA) for _ in range(cfg.k_label)]\n        self.dis = SentiGAN_D(cfg.k_label, cfg.dis_embed_dim, cfg.vocab_size, cfg.padding_idx, gpu=cfg.CUDA)\n        self.clas = SentiGAN_C(cfg.k_label, cfg.dis_embed_dim, cfg.max_seq_len, cfg.num_rep, cfg.extend_vocab_size,\n                               cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt_list = [optim.Adam(gen.parameters(), lr=cfg.gen_lr) for gen in self.gen_list]\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n        self.clas_opt = optim.Adam(self.clas.parameters(), lr=cfg.clas_lr)\n\n        # Metrics\n        self.all_metrics.append(self.clas_acc)\n\n    def init_model(self):\n        if cfg.dis_pretrain:\n            self.log.info(\n                \'Load pretrained discriminator: {}\'.format(cfg.pretrained_dis_path))\n            self.dis.load_state_dict(torch.load(cfg.pretrained_dis_path))\n        if cfg.gen_pretrain:\n            for i in range(cfg.k_label):\n                self.log.info(\'Load MLE pretrained generator gen: {}\'.format(cfg.pretrained_gen_path + \'%d\' % i))\n                self.gen_list[i].load_state_dict(torch.load(cfg.pretrained_gen_path + \'%d\' % i))\n        if cfg.clas_pretrain:\n            self.log.info(\'Load  pretrained classifier: {}\'.format(cfg.pretrained_clas_path))\n            self.clas.load_state_dict(torch.load(cfg.pretrained_clas_path, map_location=\'cuda:%d\' % cfg.device))\n\n        if cfg.CUDA:\n            for i in range(cfg.k_label):\n                self.gen_list[i] = self.gen_list[i].cuda()\n            self.dis = self.dis.cuda()\n            self.clas = self.clas.cuda()\n\n    def _run(self):\n        # ===Pre-train Classifier with real data===\n        if cfg.use_clas_acc:\n            self.log.info(\'Start training Classifier...\')\n            self.train_classifier(cfg.PRE_clas_epoch)\n\n        # ===PRE-TRAIN GENERATOR===\n        if not cfg.gen_pretrain:\n            self.log.info(\'Starting Generator MLE Training...\')\n            self.pretrain_generator(cfg.MLE_train_epoch)\n            if cfg.if_save and not cfg.if_test:\n                for i in range(cfg.k_label):\n                    torch.save(self.gen_list[i].state_dict(), cfg.pretrained_gen_path + \'%d\' % i)\n                    print(\'Save pre-trained generator: {}\'.format(cfg.pretrained_gen_path + \'%d\' % i))\n\n        # ===TRAIN DISCRIMINATOR====\n        if not cfg.dis_pretrain:\n            self.log.info(\'Starting Discriminator Training...\')\n            self.train_discriminator(cfg.d_step, cfg.d_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n                print(\'Save pre-trained discriminator: {}\'.format(cfg.pretrained_dis_path))\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        self.log.info(\'Initial generator: %s\', self.comb_metrics(fmt_str=True))\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            self.log.info(\'-----\\nADV EPOCH %d\\n-----\' % adv_epoch)\n            self.sig.update()\n            if self.sig.adv_sig:\n                self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                self.train_discriminator(cfg.ADV_d_step, cfg.ADV_d_epoch, \'ADV\')  # Discriminator\n\n                if adv_epoch % cfg.adv_log_step == 0:\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                for i in range(cfg.k_label):\n                    pre_loss = self.train_gen_epoch(self.gen_list[i], self.train_data_list[i].loader,\n                                                    self.mle_criterion, self.gen_opt_list[i])\n\n                    # ===Test===\n                    if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                        if i == cfg.k_label - 1:\n                            self.log.info(\'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (\n                                epoch, pre_loss, self.comb_metrics(fmt_str=True)))\n                            if cfg.if_save and not cfg.if_test:\n                                self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        """"""\n        The gen is trained using policy gradients, using the reward from the discriminator.\n        Training is done for num_batches batches.\n        """"""\n        for i in range(cfg.k_label):\n            rollout_func = rollout.ROLLOUT(self.gen_list[i], cfg.CUDA)\n            total_g_loss = 0\n            for step in range(g_step):\n                inp, target = GenDataIter.prepare(self.gen_list[i].sample(cfg.batch_size, cfg.batch_size), gpu=cfg.CUDA)\n\n                # ===Train===\n                rewards = rollout_func.get_reward(target, cfg.rollout_num, self.dis, current_k=i)\n                adv_loss = self.gen_list[i].batchPGLoss(inp, target, rewards)\n                self.optimize(self.gen_opt_list[i], adv_loss)\n                total_g_loss += adv_loss.item()\n\n        # ===Test===\n        self.log.info(\'[ADV-GEN]: %s\', self.comb_metrics(fmt_str=True))\n\n    def train_discriminator(self, d_step, d_epoch, phase=\'MLE\'):\n        """"""\n        Training the discriminator on real_data_samples (positive) and generated samples from gen (negative).\n        Samples are drawn d_step times, and the discriminator is trained for d_epoch d_epoch.\n        """"""\n        # prepare loader for validate\n        global d_loss, train_acc\n\n        for step in range(d_step):\n            # prepare loader for training\n            real_samples = []\n            fake_samples = []\n            for i in range(cfg.k_label):\n                real_samples.append(self.train_samples_list[i])\n                fake_samples.append(self.gen_list[i].sample(cfg.samples_num // cfg.k_label, 8 * cfg.batch_size))\n\n            dis_samples_list = [torch.cat(fake_samples, dim=0)] + real_samples\n            dis_data = CatClasDataIter(dis_samples_list)\n\n            for epoch in range(d_epoch):\n                # ===Train===\n                d_loss, train_acc = self.train_dis_epoch(self.dis, dis_data.loader, self.dis_criterion,\n                                                         self.dis_opt)\n\n            # ===Test===\n            self.log.info(\'[%s-DIS] d_step %d: d_loss = %.4f, train_acc = %.4f\' % (\n                phase, step, d_loss, train_acc))\n\n            if cfg.if_save and not cfg.if_test and phase == \'MLE\':\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n\n    def cal_metrics_with_label(self, label_i):\n        assert type(label_i) == int, \'missing label\'\n\n        with torch.no_grad():\n            # Prepare data for evaluation\n            eval_samples = self.gen_list[label_i].sample(cfg.samples_num, 8 * cfg.batch_size)\n            gen_data = GenDataIter(eval_samples)\n            gen_tokens = tensor_to_tokens(eval_samples, self.idx2word_dict)\n            gen_tokens_s = tensor_to_tokens(self.gen_list[label_i].sample(200, 200), self.idx2word_dict)\n            clas_data = CatClasDataIter([eval_samples], label_i)\n\n            # Reset metrics\n            self.bleu.reset(test_text=gen_tokens, real_text=self.test_data_list[label_i].tokens)\n            self.nll_gen.reset(self.gen_list[label_i], self.train_data_list[label_i].loader)\n            self.nll_div.reset(self.gen_list[label_i], gen_data.loader)\n            self.self_bleu.reset(test_text=gen_tokens_s, real_text=gen_tokens)\n            self.clas_acc.reset(self.clas, clas_data.loader)\n            self.ppl.reset(gen_tokens)\n\n        return [metric.get_score() for metric in self.all_metrics]\n\n    def _save(self, phase, epoch):\n        """"""Save model state dict and generator\'s samples""""""\n        for i in range(cfg.k_label):\n            if phase != \'ADV\':\n                torch.save(self.gen_list[i].state_dict(),\n                           cfg.save_model_root + \'gen{}_{}_{:05d}.pt\'.format(i, phase, epoch))\n            save_sample_path = cfg.save_samples_root + \'samples_d{}_{}_{:05d}.txt\'.format(i, phase, epoch)\n            samples = self.gen_list[i].sample(cfg.batch_size, cfg.batch_size)\n            write_tokens(save_sample_path, tensor_to_tokens(samples, self.idx2word_dict))\n'"
instructor/real_data/seqgan_instructor.py,4,"b'# -*- coding: utf-8 -*-\n# @Author       : William\n# @Project      : TextGAN-william\n# @FileName     : seqgan_instructor.py\n# @Time         : Created at 2019-06-05\n# @Blog         : http://zhiweil.ml/\n# @Description  : \n# Copyrights (C) 2018. All Rights Reserved.\n\nimport torch\nimport torch.optim as optim\n\nimport config as cfg\nfrom instructor.real_data.instructor import BasicInstructor\nfrom models.SeqGAN_D import SeqGAN_D\nfrom models.SeqGAN_G import SeqGAN_G\nfrom utils import rollout\nfrom utils.data_loader import GenDataIter, DisDataIter\n\n\nclass SeqGANInstructor(BasicInstructor):\n    def __init__(self, opt):\n        super(SeqGANInstructor, self).__init__(opt)\n\n        # generator, discriminator\n        self.gen = SeqGAN_G(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n                            cfg.padding_idx, gpu=cfg.CUDA)\n        self.dis = SeqGAN_D(cfg.dis_embed_dim, cfg.vocab_size, cfg.padding_idx, gpu=cfg.CUDA)\n        self.init_model()\n\n        # Optimizer\n        self.gen_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.gen_adv_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n\n    def _run(self):\n        # ===PRE-TRAINING===\n        # TRAIN GENERATOR\n        if not cfg.gen_pretrain:\n            self.log.info(\'Starting Generator MLE Training...\')\n            self.pretrain_generator(cfg.MLE_train_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.gen.state_dict(), cfg.pretrained_gen_path)\n                print(\'Save pre-trained generator: {}\'.format(cfg.pretrained_gen_path))\n\n        # ===TRAIN DISCRIMINATOR====\n        if not cfg.dis_pretrain:\n            self.log.info(\'Starting Discriminator Training...\')\n            self.train_discriminator(cfg.d_step, cfg.d_epoch)\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n                print(\'Save pre-trained discriminator: {}\'.format(cfg.pretrained_dis_path))\n\n        # ===ADVERSARIAL TRAINING===\n        self.log.info(\'Starting Adversarial Training...\')\n        self.log.info(\'Initial generator: %s\' % (self.cal_metrics(fmt_str=True)))\n\n        for adv_epoch in range(cfg.ADV_train_epoch):\n            self.log.info(\'-----\\nADV EPOCH %d\\n-----\' % adv_epoch)\n            self.sig.update()\n            if self.sig.adv_sig:\n                self.adv_train_generator(cfg.ADV_g_step)  # Generator\n                self.train_discriminator(cfg.ADV_d_step, cfg.ADV_d_epoch, \'ADV\')  # Discriminator\n\n                if adv_epoch % cfg.adv_log_step == 0:\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'ADV\', adv_epoch)\n            else:\n                self.log.info(\'>>> Stop by adv_signal! Finishing adversarial training...\')\n                break\n\n    def _test(self):\n        print(\'>>> Begin test...\')\n\n        self._run()\n        pass\n\n    def pretrain_generator(self, epochs):\n        """"""\n        Max Likelihood Pre-training for the generator\n        """"""\n        for epoch in range(epochs):\n            self.sig.update()\n            if self.sig.pre_sig:\n                pre_loss = self.train_gen_epoch(self.gen, self.train_data.loader, self.mle_criterion, self.gen_opt)\n\n                # ===Test===\n                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n                    self.log.info(\n                        \'[MLE-GEN] epoch %d : pre_loss = %.4f, %s\' % (epoch, pre_loss, self.cal_metrics(fmt_str=True)))\n                    if cfg.if_save and not cfg.if_test:\n                        self._save(\'MLE\', epoch)\n            else:\n                self.log.info(\'>>> Stop by pre signal, skip to adversarial training...\')\n                break\n\n    def adv_train_generator(self, g_step):\n        """"""\n        The gen is trained using policy gradients, using the reward from the discriminator.\n        Training is done for num_batches batches.\n        """"""\n        rollout_func = rollout.ROLLOUT(self.gen, cfg.CUDA)\n        total_g_loss = 0\n        for step in range(g_step):\n            inp, target = GenDataIter.prepare(self.gen.sample(cfg.batch_size, cfg.batch_size), gpu=cfg.CUDA)\n\n            # ===Train===\n            rewards = rollout_func.get_reward(target, cfg.rollout_num, self.dis)\n            adv_loss = self.gen.batchPGLoss(inp, target, rewards)\n            self.optimize(self.gen_adv_opt, adv_loss)\n            total_g_loss += adv_loss.item()\n\n        # ===Test===\n        self.log.info(\'[ADV-GEN]: g_loss = %.4f, %s\' % (total_g_loss, self.cal_metrics(fmt_str=True)))\n\n    def train_discriminator(self, d_step, d_epoch, phase=\'MLE\'):\n        """"""\n        Training the discriminator on real_data_samples (positive) and generated samples from gen (negative).\n        Samples are drawn d_step times, and the discriminator is trained for d_epoch d_epoch.\n        """"""\n        # prepare loader for validate\n        global d_loss, train_acc\n        for step in range(d_step):\n            # prepare loader for training\n            pos_samples = self.train_data.target\n            neg_samples = self.gen.sample(cfg.samples_num, 4 * cfg.batch_size)\n            dis_data = DisDataIter(pos_samples, neg_samples)\n\n            for epoch in range(d_epoch):\n                # ===Train===\n                d_loss, train_acc = self.train_dis_epoch(self.dis, dis_data.loader, self.dis_criterion,\n                                                         self.dis_opt)\n\n            # ===Test===\n            self.log.info(\'[%s-DIS] d_step %d: d_loss = %.4f, train_acc = %.4f,\' % (\n                phase, step, d_loss, train_acc))\n\n            if cfg.if_save and not cfg.if_test:\n                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n'"
