file_path,api_count,code
setup.py,0,"b""#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport distutils.spawn\nimport shlex\nimport subprocess\nimport sys\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\nversion = '1.9.6'\n\n\nif sys.argv[1] == 'release':\n    if not distutils.spawn.find_executable('twine'):\n        print(\n            'Please install twine:\\n\\n\\tpip install twine\\n',\n            file=sys.stderr,\n        )\n        sys.exit(1)\n\n    commands = [\n        'git pull origin master',\n        'git tag v{:s}'.format(version),\n        'git push origin master --tag',\n        'python setup.py sdist',\n        'twine upload dist/torchfcn-{:s}.tar.gz'.format(version),\n    ]\n    for cmd in commands:\n        print('+ {}'.format(cmd))\n        subprocess.check_call(shlex.split(cmd))\n    sys.exit(0)\n\n\ndef get_long_description():\n    with open('README.md') as f:\n        long_description = f.read()\n\n    try:\n        import github2pypi\n\n        return github2pypi.replace_url(\n            slug='wkentaro/pytorch-fcn', content=long_description\n        )\n    except Exception:\n        return long_description\n\n\ndef get_install_requires():\n    with open('requirements.txt') as f:\n        return [req.strip() for req in f]\n\n\nsetup(\n    name='torchfcn',\n    version=version,\n    packages=find_packages(exclude=['github2pypi']),\n    install_requires=get_install_requires(),\n    description='PyTorch Implementation of Fully Convolutional Networks.',\n    long_description=get_long_description(),\n    long_description_content_type='text/markdown',\n    package_data={'torchfcn': ['ext/*']},\n    include_package_data=True,\n    author='Kentaro Wada',\n    author_email='www.kentaro.wada@gmail.com',\n    license='MIT',\n    url='https://github.com/wkentaro/pytorch-fcn',\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'Natural Language :: English',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: Implementation :: CPython',\n    ],\n)\n"""
torchfcn/__init__.py,0,b'# flake8: noqa\nfrom . import datasets\nfrom . import models\nfrom . import utils\nfrom .trainer import Trainer\n'
torchfcn/trainer.py,5,"b""import datetime\nfrom distutils.version import LooseVersion\nimport math\nimport os\nimport os.path as osp\nimport shutil\n\nimport fcn\nimport numpy as np\nimport pytz\nimport skimage.io\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport tqdm\n\nimport torchfcn\n\n\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    # input: (n, c, h, w), target: (n, h, w)\n    n, c, h, w = input.size()\n    # log_p: (n, c, h, w)\n    if LooseVersion(torch.__version__) < LooseVersion('0.3'):\n        # ==0.2.X\n        log_p = F.log_softmax(input)\n    else:\n        # >=0.3\n        log_p = F.log_softmax(input, dim=1)\n    # log_p: (n*h*w, c)\n    log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous()\n    log_p = log_p[target.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0]\n    log_p = log_p.view(-1, c)\n    # target: (n*h*w,)\n    mask = target >= 0\n    target = target[mask]\n    loss = F.nll_loss(log_p, target, weight=weight, reduction='sum')\n    if size_average:\n        loss /= mask.data.sum()\n    return loss\n\n\nclass Trainer(object):\n\n    def __init__(self, cuda, model, optimizer,\n                 train_loader, val_loader, out, max_iter,\n                 size_average=False, interval_validate=None):\n        self.cuda = cuda\n\n        self.model = model\n        self.optim = optimizer\n\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n\n        self.timestamp_start = \\\n            datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n        self.size_average = size_average\n\n        if interval_validate is None:\n            self.interval_validate = len(self.train_loader)\n        else:\n            self.interval_validate = interval_validate\n\n        self.out = out\n        if not osp.exists(self.out):\n            os.makedirs(self.out)\n\n        self.log_headers = [\n            'epoch',\n            'iteration',\n            'train/loss',\n            'train/acc',\n            'train/acc_cls',\n            'train/mean_iu',\n            'train/fwavacc',\n            'valid/loss',\n            'valid/acc',\n            'valid/acc_cls',\n            'valid/mean_iu',\n            'valid/fwavacc',\n            'elapsed_time',\n        ]\n        if not osp.exists(osp.join(self.out, 'log.csv')):\n            with open(osp.join(self.out, 'log.csv'), 'w') as f:\n                f.write(','.join(self.log_headers) + '\\n')\n\n        self.epoch = 0\n        self.iteration = 0\n        self.max_iter = max_iter\n        self.best_mean_iu = 0\n\n    def validate(self):\n        training = self.model.training\n        self.model.eval()\n\n        n_class = len(self.val_loader.dataset.class_names)\n\n        val_loss = 0\n        visualizations = []\n        label_trues, label_preds = [], []\n        for batch_idx, (data, target) in tqdm.tqdm(\n                enumerate(self.val_loader), total=len(self.val_loader),\n                desc='Valid iteration=%d' % self.iteration, ncols=80,\n                leave=False):\n            if self.cuda:\n                data, target = data.cuda(), target.cuda()\n            data, target = Variable(data), Variable(target)\n            with torch.no_grad():\n                score = self.model(data)\n\n            loss = cross_entropy2d(score, target,\n                                   size_average=self.size_average)\n            loss_data = loss.data.item()\n            if np.isnan(loss_data):\n                raise ValueError('loss is nan while validating')\n            val_loss += loss_data / len(data)\n\n            imgs = data.data.cpu()\n            lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n            lbl_true = target.data.cpu()\n            for img, lt, lp in zip(imgs, lbl_true, lbl_pred):\n                img, lt = self.val_loader.dataset.untransform(img, lt)\n                label_trues.append(lt)\n                label_preds.append(lp)\n                if len(visualizations) < 9:\n                    viz = fcn.utils.visualize_segmentation(\n                        lbl_pred=lp, lbl_true=lt, img=img, n_class=n_class)\n                    visualizations.append(viz)\n        metrics = torchfcn.utils.label_accuracy_score(\n            label_trues, label_preds, n_class)\n\n        out = osp.join(self.out, 'visualization_viz')\n        if not osp.exists(out):\n            os.makedirs(out)\n        out_file = osp.join(out, 'iter%012d.jpg' % self.iteration)\n        skimage.io.imsave(out_file, fcn.utils.get_tile_image(visualizations))\n\n        val_loss /= len(self.val_loader)\n\n        with open(osp.join(self.out, 'log.csv'), 'a') as f:\n            elapsed_time = (\n                datetime.datetime.now(pytz.timezone('Asia/Tokyo')) -\n                self.timestamp_start).total_seconds()\n            log = [self.epoch, self.iteration] + [''] * 5 + \\\n                  [val_loss] + list(metrics) + [elapsed_time]\n            log = map(str, log)\n            f.write(','.join(log) + '\\n')\n\n        mean_iu = metrics[2]\n        is_best = mean_iu > self.best_mean_iu\n        if is_best:\n            self.best_mean_iu = mean_iu\n        torch.save({\n            'epoch': self.epoch,\n            'iteration': self.iteration,\n            'arch': self.model.__class__.__name__,\n            'optim_state_dict': self.optim.state_dict(),\n            'model_state_dict': self.model.state_dict(),\n            'best_mean_iu': self.best_mean_iu,\n        }, osp.join(self.out, 'checkpoint.pth.tar'))\n        if is_best:\n            shutil.copy(osp.join(self.out, 'checkpoint.pth.tar'),\n                        osp.join(self.out, 'model_best.pth.tar'))\n\n        if training:\n            self.model.train()\n\n    def train_epoch(self):\n        self.model.train()\n\n        n_class = len(self.train_loader.dataset.class_names)\n\n        for batch_idx, (data, target) in tqdm.tqdm(\n                enumerate(self.train_loader), total=len(self.train_loader),\n                desc='Train epoch=%d' % self.epoch, ncols=80, leave=False):\n            iteration = batch_idx + self.epoch * len(self.train_loader)\n            if self.iteration != 0 and (iteration - 1) != self.iteration:\n                continue  # for resuming\n            self.iteration = iteration\n\n            if self.iteration % self.interval_validate == 0:\n                self.validate()\n\n            assert self.model.training\n\n            if self.cuda:\n                data, target = data.cuda(), target.cuda()\n            data, target = Variable(data), Variable(target)\n            self.optim.zero_grad()\n            score = self.model(data)\n\n            loss = cross_entropy2d(score, target,\n                                   size_average=self.size_average)\n            loss /= len(data)\n            loss_data = loss.data.item()\n            if np.isnan(loss_data):\n                raise ValueError('loss is nan while training')\n            loss.backward()\n            self.optim.step()\n\n            metrics = []\n            lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n            lbl_true = target.data.cpu().numpy()\n            acc, acc_cls, mean_iu, fwavacc = \\\n                torchfcn.utils.label_accuracy_score(\n                    lbl_true, lbl_pred, n_class=n_class)\n            metrics.append((acc, acc_cls, mean_iu, fwavacc))\n            metrics = np.mean(metrics, axis=0)\n\n            with open(osp.join(self.out, 'log.csv'), 'a') as f:\n                elapsed_time = (\n                    datetime.datetime.now(pytz.timezone('Asia/Tokyo')) -\n                    self.timestamp_start).total_seconds()\n                log = [self.epoch, self.iteration] + [loss_data] + \\\n                    metrics.tolist() + [''] * 5 + [elapsed_time]\n                log = map(str, log)\n                f.write(','.join(log) + '\\n')\n\n            if self.iteration >= self.max_iter:\n                break\n\n    def train(self):\n        max_epoch = int(math.ceil(1. * self.max_iter / len(self.train_loader)))\n        for epoch in tqdm.trange(self.epoch, max_epoch,\n                                 desc='Train', ncols=80):\n            self.epoch = epoch\n            self.train_epoch()\n            if self.iteration >= self.max_iter:\n                break\n"""
torchfcn/utils.py,0,"b'import numpy as np\n\n\ndef _fast_hist(label_true, label_pred, n_class):\n    mask = (label_true >= 0) & (label_true < n_class)\n    hist = np.bincount(\n        n_class * label_true[mask].astype(int) +\n        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n    return hist\n\n\ndef label_accuracy_score(label_trues, label_preds, n_class):\n    """"""Returns accuracy score evaluation result.\n\n      - overall accuracy\n      - mean accuracy\n      - mean IU\n      - fwavacc\n    """"""\n    hist = np.zeros((n_class, n_class))\n    for lt, lp in zip(label_trues, label_preds):\n        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n    acc = np.diag(hist).sum() / hist.sum()\n    with np.errstate(divide=\'ignore\', invalid=\'ignore\'):\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n    with np.errstate(divide=\'ignore\', invalid=\'ignore\'):\n        iu = np.diag(hist) / (\n            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n        )\n    mean_iu = np.nanmean(iu)\n    freq = hist.sum(axis=1) / hist.sum()\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n    return acc, acc_cls, mean_iu, fwavacc\n'"
examples/voc/evaluate.py,5,"b""#!/usr/bin/env python\n\nimport argparse\nimport os\nimport os.path as osp\n\nimport fcn\nimport numpy as np\nimport skimage.io\nimport torch\nfrom torch.autograd import Variable\nimport torchfcn\nimport tqdm\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('model_file', help='Model path')\n    parser.add_argument('-g', '--gpu', type=int, default=0)\n    args = parser.parse_args()\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n    model_file = args.model_file\n\n    root = osp.expanduser('~/data/datasets')\n    val_loader = torch.utils.data.DataLoader(\n        torchfcn.datasets.VOC2011ClassSeg(\n            root, split='seg11valid', transform=True),\n        batch_size=1, shuffle=False,\n        num_workers=4, pin_memory=True)\n\n    n_class = len(val_loader.dataset.class_names)\n\n    if osp.basename(model_file).startswith('fcn32s'):\n        model = torchfcn.models.FCN32s(n_class=21)\n    elif osp.basename(model_file).startswith('fcn16s'):\n        model = torchfcn.models.FCN16s(n_class=21)\n    elif osp.basename(model_file).startswith('fcn8s'):\n        if osp.basename(model_file).startswith('fcn8s-atonce'):\n            model = torchfcn.models.FCN8sAtOnce(n_class=21)\n        else:\n            model = torchfcn.models.FCN8s(n_class=21)\n    else:\n        raise ValueError\n    if torch.cuda.is_available():\n        model = model.cuda()\n    print('==> Loading %s model file: %s' %\n          (model.__class__.__name__, model_file))\n    model_data = torch.load(model_file)\n    try:\n        model.load_state_dict(model_data)\n    except Exception:\n        model.load_state_dict(model_data['model_state_dict'])\n    model.eval()\n\n    print('==> Evaluating with VOC2011ClassSeg seg11valid')\n    visualizations = []\n    label_trues, label_preds = [], []\n    for batch_idx, (data, target) in tqdm.tqdm(enumerate(val_loader),\n                                               total=len(val_loader),\n                                               ncols=80, leave=False):\n        if torch.cuda.is_available():\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        score = model(data)\n\n        imgs = data.data.cpu()\n        lbl_pred = score.data.max(1)[1].cpu().numpy()[:, :, :]\n        lbl_true = target.data.cpu()\n        for img, lt, lp in zip(imgs, lbl_true, lbl_pred):\n            img, lt = val_loader.dataset.untransform(img, lt)\n            label_trues.append(lt)\n            label_preds.append(lp)\n            if len(visualizations) < 9:\n                viz = fcn.utils.visualize_segmentation(\n                    lbl_pred=lp, lbl_true=lt, img=img, n_class=n_class,\n                    label_names=val_loader.dataset.class_names)\n                visualizations.append(viz)\n    metrics = torchfcn.utils.label_accuracy_score(\n        label_trues, label_preds, n_class=n_class)\n    metrics = np.array(metrics)\n    metrics *= 100\n    print('''\\\nAccuracy: {0}\nAccuracy Class: {1}\nMean IU: {2}\nFWAV Accuracy: {3}'''.format(*metrics))\n\n    viz = fcn.utils.get_tile_image(visualizations)\n    skimage.io.imsave('viz_evaluate.png', viz)\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/voc/learning_curve.py,0,"b""#!/usr/bin/env python\n\nfrom __future__ import division\n\nimport argparse\nimport os.path as osp\n\nimport matplotlib\nmatplotlib.use('Agg')  # NOQA\nimport matplotlib.pyplot as plt\nimport pandas\nimport seaborn\n\n\ndef learning_curve(log_file):\n    print('==> Plotting log file: %s' % log_file)\n\n    df = pandas.read_csv(log_file)\n\n    colors = ['red', 'green', 'blue', 'purple', 'orange']\n    colors = seaborn.xkcd_palette(colors)\n\n    plt.figure(figsize=(20, 6), dpi=300)\n\n    row_min = df.min()\n    row_max = df.max()\n\n    # initialize DataFrame for train\n    columns = [\n        'epoch',\n        'iteration',\n        'train/loss',\n        'train/acc',\n        'train/acc_cls',\n        'train/mean_iu',\n        'train/fwavacc',\n    ]\n    df_train = df[columns]\n    if hasattr(df_train, 'rolling'):\n        df_train = df_train.rolling(window=10).mean()\n    else:\n        df_train = pandas.rolling_mean(df_train, window=10)\n    df_train = df_train.dropna()\n    iter_per_epoch = df_train[df_train['epoch'] == 1]['iteration'].values[0]\n    df_train['epoch_detail'] = df_train['iteration'] / iter_per_epoch\n\n    # initialize DataFrame for val\n    columns = [\n        'epoch',\n        'iteration',\n        'valid/loss',\n        'valid/acc',\n        'valid/acc_cls',\n        'valid/mean_iu',\n        'valid/fwavacc',\n    ]\n    df_valid = df[columns]\n    df_valid = df_valid.dropna()\n    df_valid['epoch_detail'] = df_valid['iteration'] / iter_per_epoch\n\n    data_frames = {'train': df_train, 'valid': df_valid}\n\n    n_row = 2\n    n_col = 3\n    for i, split in enumerate(['train', 'valid']):\n        df_split = data_frames[split]\n\n        # loss\n        plt.subplot(n_row, n_col, i * n_col + 1)\n        plt.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))\n        plt.plot(df_split['epoch_detail'], df_split['%s/loss' % split], '-',\n                 markersize=1, color=colors[0], alpha=.5,\n                 label='%s loss' % split)\n        plt.xlim((0, row_max['epoch']))\n        plt.ylim((min(row_min['train/loss'], row_min['valid/loss']),\n                  max(row_max['train/loss'], row_max['valid/loss'])))\n        plt.xlabel('epoch')\n        plt.ylabel('%s loss' % split)\n\n        # loss (log)\n        plt.subplot(n_row, n_col, i * n_col + 2)\n        plt.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))\n        plt.semilogy(df_split['epoch_detail'], df_split['%s/loss' % split],\n                     '-', markersize=1, color=colors[0], alpha=.5,\n                     label='%s loss' % split)\n        plt.xlim((0, row_max['epoch']))\n        plt.ylim((min(row_min['train/loss'], row_min['valid/loss']),\n                  max(row_max['train/loss'], row_max['valid/loss'])))\n        plt.xlabel('epoch')\n        plt.ylabel('%s loss (log)' % split)\n\n        # lbl accuracy\n        plt.subplot(n_row, n_col, i * n_col + 3)\n        plt.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))\n        plt.plot(df_split['epoch_detail'], df_split['%s/acc' % split],\n                 '-', markersize=1, color=colors[1], alpha=.5,\n                 label='%s accuracy' % split)\n        plt.plot(df_split['epoch_detail'], df_split['%s/acc_cls' % split],\n                 '-', markersize=1, color=colors[2], alpha=.5,\n                 label='%s accuracy class' % split)\n        plt.plot(df_split['epoch_detail'], df_split['%s/mean_iu' % split],\n                 '-', markersize=1, color=colors[3], alpha=.5,\n                 label='%s mean IU' % split)\n        plt.plot(df_split['epoch_detail'], df_split['%s/fwavacc' % split],\n                 '-', markersize=1, color=colors[4], alpha=.5,\n                 label='%s fwav accuracy' % split)\n        plt.legend()\n        plt.xlim((0, row_max['epoch']))\n        plt.ylim((0, 1))\n        plt.xlabel('epoch')\n        plt.ylabel('%s label accuracy' % split)\n\n    out_file = osp.splitext(log_file)[0] + '.png'\n    plt.savefig(out_file)\n    print('==> Wrote figure to: %s' % out_file)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('log_file')\n    args = parser.parse_args()\n\n    log_file = args.log_file\n\n    learning_curve(log_file)\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/voc/model_caffe_to_pytorch.py,3,"b""#!/usr/bin/env python\n\nimport os.path as osp\nimport pkg_resources\nimport sys\n\nimport torch\n\n# FIXME: must be after import torch\nimport caffe\n\nimport torchfcn\n\n\nmodels = [\n    ('fcn32s', 'FCN32s', []),\n    ('fcn16s', 'FCN16s', []),\n    ('fcn8s', 'FCN8s', []),\n    ('fcn8s-atonce', 'FCN8sAtOnce', ['scale_pool4', 'scale_pool3']),\n]\n\n\nfor name_lower, name_upper, blacklists in models:\n    print('==> Loading caffe model of %s' % name_upper)\n    pkg_root = pkg_resources.get_distribution('torchfcn').location\n    sys.path.insert(\n        0, osp.join(pkg_root, 'torchfcn/ext/fcn.berkeleyvision.org'))\n    caffe_prototxt = osp.join(\n        pkg_root,\n        'torchfcn/ext/fcn.berkeleyvision.org/voc-%s/deploy.prototxt' %\n        name_lower)\n    caffe_model_path = osp.expanduser(\n        '~/data/models/caffe/%s-heavy-pascal.caffemodel' % name_lower)\n    caffe_model = caffe.Net(caffe_prototxt, caffe_model_path, caffe.TEST)\n\n    torch_model = getattr(torchfcn.models, name_upper)()\n\n    torch_model_params = torch_model.parameters()\n    for name, p1 in caffe_model.params.iteritems():\n        if name in blacklists:\n            continue\n        l2 = getattr(torch_model, name)\n        p2 = l2.weight\n        assert p1[0].data.shape == tuple(p2.data.size())\n        print('%s: %s -> %s' % (name, p1[0].data.shape, p2.data.size()))\n        p2.data = torch.from_numpy(p1[0].data)\n        if len(p1) == 2:\n            p2 = l2.bias\n            assert p1[1].data.shape == tuple(p2.data.size())\n            print('%s: %s -> %s' % (name, p1[1].data.shape, p2.data.size()))\n            p2.data = torch.from_numpy(p1[1].data)\n\n    torch_model_path = osp.expanduser(\n        '~/data/models/pytorch/%s-heavy-pascal.pth' % name_lower)\n    torch.save(torch_model.state_dict(), torch_model_path)\n    print('==> Saved pytorch model: %s' % torch_model_path)\n"""
examples/voc/speedtest.py,7,"b""#!/usr/bin/env python\n\nimport argparse\nimport time\n\nimport numpy as np\nimport six\n\n\ndef bench_chainer(gpu, times, dynamic_input=False):\n    import chainer\n    import fcn\n    print('==> Testing FCN32s with Chainer')\n    chainer.cuda.get_device(gpu).use()\n\n    chainer.config.train = False\n    chainer.config.enable_backprop = False\n\n    if dynamic_input:\n        x_data = np.random.random((1, 3, 480, 640)).astype(np.float32)\n        x_data = chainer.cuda.to_gpu(x_data)\n        x1 = chainer.Variable(x_data)\n        x_data = np.random.random((1, 3, 640, 480)).astype(np.float32)\n        x_data = chainer.cuda.to_gpu(x_data)\n        x2 = chainer.Variable(x_data)\n    else:\n        x_data = np.random.random((1, 3, 480, 640)).astype(np.float32)\n        x_data = chainer.cuda.to_gpu(x_data)\n        x1 = chainer.Variable(x_data)\n\n    model = fcn.models.FCN32s()\n    model.train = False\n    model.to_gpu()\n\n    for i in six.moves.range(5):\n        model(x1)\n    chainer.cuda.Stream().synchronize()\n    t_start = time.time()\n    for i in six.moves.range(times):\n        if dynamic_input:\n            if i % 2 == 1:\n                model(x1)\n            else:\n                model(x2)\n        else:\n            model(x1)\n    chainer.cuda.Stream().synchronize()\n    elapsed_time = time.time() - t_start\n\n    print('Elapsed time: %.2f [s / %d evals]' % (elapsed_time, times))\n    print('Hz: %.2f [hz]' % (times / elapsed_time))\n\n\ndef bench_pytorch(gpu, times, dynamic_input=False):\n    import torch\n    import torchfcn.models\n    print('==> Testing FCN32s with PyTorch')\n    torch.cuda.set_device(gpu)\n    torch.backends.cudnn.benchmark = not dynamic_input\n\n    model = torchfcn.models.FCN32s()\n    model.eval()\n    model = model.cuda()\n\n    if dynamic_input:\n        x_data = np.random.random((1, 3, 480, 640))\n        x1 = torch.autograd.Variable(torch.from_numpy(x_data).float(),\n                                     volatile=True).cuda()\n        x_data = np.random.random((1, 3, 640, 480))\n        x2 = torch.autograd.Variable(torch.from_numpy(x_data).float(),\n                                     volatile=True).cuda()\n    else:\n        x_data = np.random.random((1, 3, 480, 640))\n        x1 = torch.autograd.Variable(torch.from_numpy(x_data).float(),\n                                     volatile=True).cuda()\n\n    for i in six.moves.range(5):\n        model(x1)\n    torch.cuda.synchronize()\n    t_start = time.time()\n    for i in six.moves.range(times):\n        if dynamic_input:\n            if i % 2 == 1:\n                model(x1)\n            else:\n                model(x2)\n        else:\n            model(x1)\n    torch.cuda.synchronize()\n    elapsed_time = time.time() - t_start\n\n    print('Elapsed time: %.2f [s / %d evals]' % (elapsed_time, times))\n    print('Hz: %.2f [hz]' % (times / elapsed_time))\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', type=int, default=0)\n    parser.add_argument('--times', type=int, default=1000)\n    parser.add_argument('--dynamic-input', action='store_true')\n    args = parser.parse_args()\n\n    print('==> Benchmark: gpu=%d, times=%d, dynamic_input=%s' %\n          (args.gpu, args.times, args.dynamic_input))\n    bench_chainer(args.gpu, args.times, args.dynamic_input)\n    bench_pytorch(args.gpu, args.times, args.dynamic_input)\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/voc/summarize_logs.py,0,"b""#!/usr/bin/env python\n\nimport os\nimport os.path as osp\n\nimport pandas as pd\nimport tabulate\nimport yaml\n\n\ndef main():\n    logs_dir = 'logs'\n\n    headers = [\n        'name',\n        'model',\n        'git_hash',\n        'pretrained_model',\n        'epoch',\n        'iteration',\n        'valid/mean_iu',\n    ]\n    rows = []\n    for log in os.listdir(logs_dir):\n        log_dir = osp.join(logs_dir, log)\n        if not osp.isdir(log_dir):\n            continue\n        try:\n            log_file = osp.join(log_dir, 'log.csv')\n            df = pd.read_csv(log_file)\n            columns = [c for c in df.columns if not c.startswith('train')]\n            df = df[columns]\n            df = df.set_index(['epoch', 'iteration'])\n            index_best = df['valid/mean_iu'].idxmax()\n            row_best = df.loc[index_best].dropna()\n\n            with open(osp.join(log_dir, 'config.yaml')) as f:\n                config = yaml.load(f)\n        except Exception:\n            continue\n        rows.append([\n            osp.join(logs_dir, log),\n            config['model'],\n            config['git_hash'],\n            config.get('pretrained_model', None),\n            row_best.index[0][0],\n            row_best.index[0][1],\n            100 * row_best['valid/mean_iu'].values[0],\n        ])\n    rows.sort(key=lambda x: x[-1], reverse=True)\n    print(tabulate.tabulate(rows, headers=headers))\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/voc/train_fcn16s.py,8,"b""#!/usr/bin/env python\n\nimport argparse\nimport datetime\nimport os\nimport os.path as osp\n\nimport torch\nimport yaml\n\nimport torchfcn\n\nfrom train_fcn32s import get_parameters\nfrom train_fcn32s import git_hash\n\n\nhere = osp.dirname(osp.abspath(__file__))\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument('-g', '--gpu', type=int, required=True, help='gpu id')\n    parser.add_argument('--resume', help='checkpoint path')\n    # configurations (same configuration as original work)\n    # https://github.com/shelhamer/fcn.berkeleyvision.org\n    parser.add_argument(\n        '--max-iteration', type=int, default=100000, help='max iteration'\n    )\n    parser.add_argument(\n        '--lr', type=float, default=1.0e-12, help='learning rate',\n    )\n    parser.add_argument(\n        '--weight-decay', type=float, default=0.0005, help='weight decay',\n    )\n    parser.add_argument(\n        '--momentum', type=float, default=0.99, help='momentum',\n    )\n    parser.add_argument(\n        '--pretrained-model',\n        default=torchfcn.models.FCN32s.download(),\n        help='pretrained model of FCN32s',\n    )\n    args = parser.parse_args()\n\n    args.model = 'FCN16s'\n    args.git_hash = git_hash()\n\n    now = datetime.datetime.now()\n    args.out = osp.join(here, 'logs', now.strftime('%Y%m%d_%H%M%S.%f'))\n\n    os.makedirs(args.out)\n    with open(osp.join(args.out, 'config.yaml'), 'w') as f:\n        yaml.safe_dump(args.__dict__, f, default_flow_style=False)\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n    cuda = torch.cuda.is_available()\n\n    torch.manual_seed(1337)\n    if cuda:\n        torch.cuda.manual_seed(1337)\n\n    # 1. dataset\n\n    root = osp.expanduser('~/data/datasets')\n    kwargs = {'num_workers': 4, 'pin_memory': True} if cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        torchfcn.datasets.SBDClassSeg(root, split='train', transform=True),\n        batch_size=1, shuffle=True, **kwargs)\n    val_loader = torch.utils.data.DataLoader(\n        torchfcn.datasets.VOC2011ClassSeg(\n            root, split='seg11valid', transform=True),\n        batch_size=1, shuffle=False, **kwargs)\n\n    # 2. model\n\n    model = torchfcn.models.FCN16s(n_class=21)\n    start_epoch = 0\n    start_iteration = 0\n    if args.resume:\n        checkpoint = torch.load(args.resume)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        start_epoch = checkpoint['epoch']\n        start_iteration = checkpoint['iteration']\n    else:\n        fcn32s = torchfcn.models.FCN32s()\n        state_dict = torch.load(args.pretrained_model)\n        try:\n            fcn32s.load_state_dict(state_dict)\n        except RuntimeError:\n            fcn32s.load_state_dict(state_dict['model_state_dict'])\n        model.copy_params_from_fcn32s(fcn32s)\n    if cuda:\n        model = model.cuda()\n\n    # 3. optimizer\n\n    optim = torch.optim.SGD(\n        [\n            {'params': get_parameters(model, bias=False)},\n            {'params': get_parameters(model, bias=True),\n             'lr': args.lr * 2, 'weight_decay': 0},\n        ],\n        lr=args.lr,\n        momentum=args.momentum,\n        weight_decay=args.weight_decay)\n    if args.resume:\n        optim.load_state_dict(checkpoint['optim_state_dict'])\n\n    trainer = torchfcn.Trainer(\n        cuda=cuda,\n        model=model,\n        optimizer=optim,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        out=args.out,\n        max_iter=args.max_iteration,\n        interval_validate=4000,\n    )\n    trainer.epoch = start_epoch\n    trainer.iteration = start_iteration\n    trainer.train()\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/voc/train_fcn32s.py,8,"b'#!/usr/bin/env python\n\nimport argparse\nimport datetime\nimport os\nimport os.path as osp\nimport shlex\nimport subprocess\n\nimport torch\nimport yaml\n\nimport torchfcn\n\n\ndef git_hash():\n    cmd = \'git log -n 1 --pretty=""%h""\'\n    ret = subprocess.check_output(shlex.split(cmd)).strip()\n    if isinstance(ret, bytes):\n        ret = ret.decode()\n    return ret\n\n\ndef get_parameters(model, bias=False):\n    import torch.nn as nn\n    modules_skipped = (\n        nn.ReLU,\n        nn.MaxPool2d,\n        nn.Dropout2d,\n        nn.Sequential,\n        torchfcn.models.FCN32s,\n        torchfcn.models.FCN16s,\n        torchfcn.models.FCN8s,\n    )\n    for m in model.modules():\n        if isinstance(m, nn.Conv2d):\n            if bias:\n                yield m.bias\n            else:\n                yield m.weight\n        elif isinstance(m, nn.ConvTranspose2d):\n            # weight is frozen because it is just a bilinear upsampling\n            if bias:\n                assert m.bias is None\n        elif isinstance(m, modules_skipped):\n            continue\n        else:\n            raise ValueError(\'Unexpected module: %s\' % str(m))\n\n\nhere = osp.dirname(osp.abspath(__file__))\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\'-g\', \'--gpu\', type=int, required=True, help=\'gpu id\')\n    parser.add_argument(\'--resume\', help=\'checkpoint path\')\n    # configurations (same configuration as original work)\n    # https://github.com/shelhamer/fcn.berkeleyvision.org\n    parser.add_argument(\n        \'--max-iteration\', type=int, default=100000, help=\'max iteration\'\n    )\n    parser.add_argument(\n        \'--lr\', type=float, default=1.0e-10, help=\'learning rate\',\n    )\n    parser.add_argument(\n        \'--weight-decay\', type=float, default=0.0005, help=\'weight decay\',\n    )\n    parser.add_argument(\n        \'--momentum\', type=float, default=0.99, help=\'momentum\',\n    )\n    args = parser.parse_args()\n\n    args.model = \'FCN32s\'\n    args.git_hash = git_hash()\n\n    now = datetime.datetime.now()\n    args.out = osp.join(here, \'logs\', now.strftime(\'%Y%m%d_%H%M%S.%f\'))\n\n    os.makedirs(args.out)\n    with open(osp.join(args.out, \'config.yaml\'), \'w\') as f:\n        yaml.safe_dump(args.__dict__, f, default_flow_style=False)\n\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(args.gpu)\n    cuda = torch.cuda.is_available()\n\n    torch.manual_seed(1337)\n    if cuda:\n        torch.cuda.manual_seed(1337)\n\n    # 1. dataset\n\n    root = osp.expanduser(\'~/data/datasets\')\n    kwargs = {\'num_workers\': 4, \'pin_memory\': True} if cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        torchfcn.datasets.SBDClassSeg(root, split=\'train\', transform=True),\n        batch_size=1, shuffle=True, **kwargs)\n    val_loader = torch.utils.data.DataLoader(\n        torchfcn.datasets.VOC2011ClassSeg(\n            root, split=\'seg11valid\', transform=True),\n        batch_size=1, shuffle=False, **kwargs)\n\n    # 2. model\n\n    model = torchfcn.models.FCN32s(n_class=21)\n    start_epoch = 0\n    start_iteration = 0\n    if args.resume:\n        checkpoint = torch.load(args.resume)\n        model.load_state_dict(checkpoint[\'model_state_dict\'])\n        start_epoch = checkpoint[\'epoch\']\n        start_iteration = checkpoint[\'iteration\']\n    else:\n        vgg16 = torchfcn.models.VGG16(pretrained=True)\n        model.copy_params_from_vgg16(vgg16)\n    if cuda:\n        model = model.cuda()\n\n    # 3. optimizer\n\n    optim = torch.optim.SGD(\n        [\n            {\'params\': get_parameters(model, bias=False)},\n            {\'params\': get_parameters(model, bias=True),\n             \'lr\': args.lr * 2, \'weight_decay\': 0},\n        ],\n        lr=args.lr,\n        momentum=args.momentum,\n        weight_decay=args.weight_decay)\n    if args.resume:\n        optim.load_state_dict(checkpoint[\'optim_state_dict\'])\n\n    trainer = torchfcn.Trainer(\n        cuda=cuda,\n        model=model,\n        optimizer=optim,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        out=args.out,\n        max_iter=args.max_iteration,\n        interval_validate=4000,\n    )\n    trainer.epoch = start_epoch\n    trainer.iteration = start_iteration\n    trainer.train()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/voc/train_fcn8s.py,8,"b""#!/usr/bin/env python\n\nimport argparse\nimport datetime\nimport os\nimport os.path as osp\n\nimport torch\nimport yaml\n\nimport torchfcn\n\nfrom train_fcn32s import get_parameters\nfrom train_fcn32s import git_hash\n\n\nhere = osp.dirname(osp.abspath(__file__))\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument('-g', '--gpu', type=int, required=True, help='gpu id')\n    parser.add_argument('--resume', help='checkpoint path')\n    # configurations (same configuration as original work)\n    # https://github.com/shelhamer/fcn.berkeleyvision.org\n    parser.add_argument(\n        '--max-iteration', type=int, default=100000, help='max iteration'\n    )\n    parser.add_argument(\n        '--lr', type=float, default=1.0e-14, help='learning rate',\n    )\n    parser.add_argument(\n        '--weight-decay', type=float, default=0.0005, help='weight decay',\n    )\n    parser.add_argument(\n        '--momentum', type=float, default=0.99, help='momentum',\n    )\n    parser.add_argument(\n        '--pretrained-model',\n        default=torchfcn.models.FCN16s.download(),\n        help='pretrained model of FCN16s',\n    )\n    args = parser.parse_args()\n\n    args.model = 'FCN8s'\n    args.git_hash = git_hash()\n\n    now = datetime.datetime.now()\n    args.out = osp.join(here, 'logs', now.strftime('%Y%m%d_%H%M%S.%f'))\n\n    os.makedirs(args.out)\n    with open(osp.join(args.out, 'config.yaml'), 'w') as f:\n        yaml.safe_dump(args.__dict__, f, default_flow_style=False)\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n    cuda = torch.cuda.is_available()\n\n    torch.manual_seed(1337)\n    if cuda:\n        torch.cuda.manual_seed(1337)\n\n    # 1. dataset\n\n    root = osp.expanduser('~/data/datasets')\n    kwargs = {'num_workers': 4, 'pin_memory': True} if cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        torchfcn.datasets.SBDClassSeg(root, split='train', transform=True),\n        batch_size=1, shuffle=True, **kwargs)\n    val_loader = torch.utils.data.DataLoader(\n        torchfcn.datasets.VOC2011ClassSeg(\n            root, split='seg11valid', transform=True),\n        batch_size=1, shuffle=False, **kwargs)\n\n    # 2. model\n\n    model = torchfcn.models.FCN8s(n_class=21)\n    start_epoch = 0\n    start_iteration = 0\n    if args.resume:\n        checkpoint = torch.load(args.resume)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        start_epoch = checkpoint['epoch']\n        start_iteration = checkpoint['iteration']\n    else:\n        fcn16s = torchfcn.models.FCN16s()\n        state_dict = torch.load(args.pretrained_model)\n        try:\n            fcn16s.load_state_dict(state_dict)\n        except RuntimeError:\n            fcn16s.load_state_dict(state_dict['model_state_dict'])\n        model.copy_params_from_fcn16s(fcn16s)\n    if cuda:\n        model = model.cuda()\n\n    # 3. optimizer\n\n    optim = torch.optim.SGD(\n        [\n            {'params': get_parameters(model, bias=False)},\n            {'params': get_parameters(model, bias=True),\n             'lr': args.lr * 2, 'weight_decay': 0},\n        ],\n        lr=args.lr,\n        momentum=args.momentum,\n        weight_decay=args.weight_decay)\n    if args.resume:\n        optim.load_state_dict(checkpoint['optim_state_dict'])\n\n    trainer = torchfcn.Trainer(\n        cuda=cuda,\n        model=model,\n        optimizer=optim,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        out=args.out,\n        max_iter=args.max_iteration,\n        interval_validate=4000,\n    )\n    trainer.epoch = start_epoch\n    trainer.iteration = start_iteration\n    trainer.train()\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/voc/train_fcn8s_atonce.py,7,"b""#!/usr/bin/env python\n\nimport argparse\nimport datetime\nimport os\nimport os.path as osp\n\nimport torch\nimport yaml\n\nimport torchfcn\n\nfrom train_fcn32s import get_parameters\nfrom train_fcn32s import git_hash\n\n\nhere = osp.dirname(osp.abspath(__file__))\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument('-g', '--gpu', type=int, required=True, help='gpu id')\n    parser.add_argument('--resume', help='checkpoint path')\n    # configurations (same configuration as original work)\n    # https://github.com/shelhamer/fcn.berkeleyvision.org\n    parser.add_argument(\n        '--max-iteration', type=int, default=100000, help='max iteration'\n    )\n    parser.add_argument(\n        '--lr', type=float, default=1.0e-10, help='learning rate',\n    )\n    parser.add_argument(\n        '--weight-decay', type=float, default=0.0005, help='weight decay',\n    )\n    parser.add_argument(\n        '--momentum', type=float, default=0.99, help='momentum',\n    )\n    args = parser.parse_args()\n\n    args.model = 'FCN8sAtOnce'\n    args.git_hash = git_hash()\n\n    now = datetime.datetime.now()\n    args.out = osp.join(here, 'logs', now.strftime('%Y%m%d_%H%M%S.%f'))\n\n    os.makedirs(args.out)\n    with open(osp.join(args.out, 'config.yaml'), 'w') as f:\n        yaml.safe_dump(args.__dict__, f, default_flow_style=False)\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n    cuda = torch.cuda.is_available()\n\n    torch.manual_seed(1337)\n    if cuda:\n        torch.cuda.manual_seed(1337)\n\n    # 1. dataset\n\n    root = osp.expanduser('~/data/datasets')\n    kwargs = {'num_workers': 4, 'pin_memory': True} if cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        torchfcn.datasets.SBDClassSeg(root, split='train', transform=True),\n        batch_size=1, shuffle=True, **kwargs)\n    val_loader = torch.utils.data.DataLoader(\n        torchfcn.datasets.VOC2011ClassSeg(\n            root, split='seg11valid', transform=True),\n        batch_size=1, shuffle=False, **kwargs)\n\n    # 2. model\n\n    model = torchfcn.models.FCN8sAtOnce(n_class=21)\n    start_epoch = 0\n    start_iteration = 0\n    if args.resume:\n        checkpoint = torch.load(args.resume)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        start_epoch = checkpoint['epoch']\n        start_iteration = checkpoint['iteration']\n    else:\n        vgg16 = torchfcn.models.VGG16(pretrained=True)\n        model.copy_params_from_vgg16(vgg16)\n    if cuda:\n        model = model.cuda()\n\n    # 3. optimizer\n\n    optim = torch.optim.SGD(\n        [\n            {'params': get_parameters(model, bias=False)},\n            {'params': get_parameters(model, bias=True),\n             'lr': args.lr * 2, 'weight_decay': 0},\n        ],\n        lr=args.lr,\n        momentum=args.momentum,\n        weight_decay=args.weight_decay)\n    if args.resume:\n        optim.load_state_dict(checkpoint['optim_state_dict'])\n\n    trainer = torchfcn.Trainer(\n        cuda=cuda,\n        model=model,\n        optimizer=optim,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        out=args.out,\n        max_iter=args.max_iteration,\n        interval_validate=4000,\n    )\n    trainer.epoch = start_epoch\n    trainer.iteration = start_iteration\n    trainer.train()\n\n\nif __name__ == '__main__':\n    main()\n"""
tests/models_tests/test_fcn32s.py,3,"b""# FIXME: Import order causes error:\n# ImportError: dlopen: cannot load any more object with static TL\n# https://github.com/pytorch/pytorch/issues/2083\nimport torch\n\nimport numpy as np\nimport skimage.data\n\nfrom torchfcn.models.fcn32s import get_upsampling_weight\n\n\ndef test_get_upsampling_weight():\n    src = skimage.data.coffee()\n    x = src.transpose(2, 0, 1)\n    x = x[np.newaxis, :, :, :]\n    x = torch.from_numpy(x).float()\n    x = torch.autograd.Variable(x)\n\n    in_channels = 3\n    out_channels = 3\n    kernel_size = 4\n\n    m = torch.nn.ConvTranspose2d(\n        in_channels, out_channels, kernel_size, stride=2, bias=False)\n    m.weight.data = get_upsampling_weight(\n        in_channels, out_channels, kernel_size)\n\n    y = m(x)\n\n    y = y.data.numpy()\n    y = y[0]\n    y = y.transpose(1, 2, 0)\n    dst = y.astype(np.uint8)\n\n    assert abs(src.shape[0] * 2 - dst.shape[0]) <= 2\n    assert abs(src.shape[1] * 2 - dst.shape[1]) <= 2\n\n    return src, dst\n\n\nif __name__ == '__main__':\n    import matplotlib.pyplot as plt\n\n    src, dst = test_get_upsampling_weight()\n    plt.subplot(121)\n    plt.imshow(src)\n    plt.title('x1: {}'.format(src.shape))\n    plt.subplot(122)\n    plt.imshow(dst)\n    plt.title('x2: {}'.format(dst.shape))\n    plt.show()\n"""
torchfcn/datasets/__init__.py,0,b'from .voc import SBDClassSeg  # NOQA\nfrom .voc import VOC2011ClassSeg  # NOQA\nfrom .voc import VOC2012ClassSeg  # NOQA\n'
torchfcn/datasets/voc.py,3,"b""#!/usr/bin/env python\n\nimport collections\nimport os.path as osp\n\nimport numpy as np\nimport PIL.Image\nimport scipy.io\nimport torch\nfrom torch.utils import data\n\n\nclass VOCClassSegBase(data.Dataset):\n\n    class_names = np.array([\n        'background',\n        'aeroplane',\n        'bicycle',\n        'bird',\n        'boat',\n        'bottle',\n        'bus',\n        'car',\n        'cat',\n        'chair',\n        'cow',\n        'diningtable',\n        'dog',\n        'horse',\n        'motorbike',\n        'person',\n        'potted plant',\n        'sheep',\n        'sofa',\n        'train',\n        'tv/monitor',\n    ])\n    mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\n\n    def __init__(self, root, split='train', transform=False):\n        self.root = root\n        self.split = split\n        self._transform = transform\n\n        # VOC2011 and others are subset of VOC2012\n        dataset_dir = osp.join(self.root, 'VOC/VOCdevkit/VOC2012')\n        self.files = collections.defaultdict(list)\n        for split in ['train', 'val']:\n            imgsets_file = osp.join(\n                dataset_dir, 'ImageSets/Segmentation/%s.txt' % split)\n            for did in open(imgsets_file):\n                did = did.strip()\n                img_file = osp.join(dataset_dir, 'JPEGImages/%s.jpg' % did)\n                lbl_file = osp.join(\n                    dataset_dir, 'SegmentationClass/%s.png' % did)\n                self.files[split].append({\n                    'img': img_file,\n                    'lbl': lbl_file,\n                })\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        data_file = self.files[self.split][index]\n        # load image\n        img_file = data_file['img']\n        img = PIL.Image.open(img_file)\n        img = np.array(img, dtype=np.uint8)\n        # load label\n        lbl_file = data_file['lbl']\n        lbl = PIL.Image.open(lbl_file)\n        lbl = np.array(lbl, dtype=np.int32)\n        lbl[lbl == 255] = -1\n        if self._transform:\n            return self.transform(img, lbl)\n        else:\n            return img, lbl\n\n    def transform(self, img, lbl):\n        img = img[:, :, ::-1]  # RGB -> BGR\n        img = img.astype(np.float64)\n        img -= self.mean_bgr\n        img = img.transpose(2, 0, 1)\n        img = torch.from_numpy(img).float()\n        lbl = torch.from_numpy(lbl).long()\n        return img, lbl\n\n    def untransform(self, img, lbl):\n        img = img.numpy()\n        img = img.transpose(1, 2, 0)\n        img += self.mean_bgr\n        img = img.astype(np.uint8)\n        img = img[:, :, ::-1]\n        lbl = lbl.numpy()\n        return img, lbl\n\n\nclass VOC2011ClassSeg(VOCClassSegBase):\n\n    def __init__(self, root, split='train', transform=False):\n        super(VOC2011ClassSeg, self).__init__(\n            root, split=split, transform=transform)\n        pkg_root = osp.join(osp.dirname(osp.realpath(__file__)), '..')\n        imgsets_file = osp.join(\n            pkg_root, 'ext/fcn.berkeleyvision.org',\n            'data/pascal/seg11valid.txt')\n        dataset_dir = osp.join(self.root, 'VOC/VOCdevkit/VOC2012')\n        for did in open(imgsets_file):\n            did = did.strip()\n            img_file = osp.join(dataset_dir, 'JPEGImages/%s.jpg' % did)\n            lbl_file = osp.join(dataset_dir, 'SegmentationClass/%s.png' % did)\n            self.files['seg11valid'].append({'img': img_file, 'lbl': lbl_file})\n\n\nclass VOC2012ClassSeg(VOCClassSegBase):\n\n    url = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar'  # NOQA\n\n    def __init__(self, root, split='train', transform=False):\n        super(VOC2012ClassSeg, self).__init__(\n            root, split=split, transform=transform)\n\n\nclass SBDClassSeg(VOCClassSegBase):\n\n    # XXX: It must be renamed to benchmark.tar to be extracted.\n    url = 'http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz'  # NOQA\n\n    def __init__(self, root, split='train', transform=False):\n        self.root = root\n        self.split = split\n        self._transform = transform\n\n        dataset_dir = osp.join(self.root, 'VOC/benchmark_RELEASE/dataset')\n        self.files = collections.defaultdict(list)\n        for split in ['train', 'val']:\n            imgsets_file = osp.join(dataset_dir, '%s.txt' % split)\n            for did in open(imgsets_file):\n                did = did.strip()\n                img_file = osp.join(dataset_dir, 'img/%s.jpg' % did)\n                lbl_file = osp.join(dataset_dir, 'cls/%s.mat' % did)\n                self.files[split].append({\n                    'img': img_file,\n                    'lbl': lbl_file,\n                })\n\n    def __getitem__(self, index):\n        data_file = self.files[self.split][index]\n        # load image\n        img_file = data_file['img']\n        img = PIL.Image.open(img_file)\n        img = np.array(img, dtype=np.uint8)\n        # load label\n        lbl_file = data_file['lbl']\n        mat = scipy.io.loadmat(lbl_file)\n        lbl = mat['GTcls'][0]['Segmentation'][0].astype(np.int32)\n        lbl[lbl == 255] = -1\n        if self._transform:\n            return self.transform(img, lbl)\n        else:\n            return img, lbl\n"""
torchfcn/models/__init__.py,0,b'# flake8: noqa\nfrom .fcn32s import FCN32s\nfrom .fcn16s import FCN16s\nfrom .fcn8s import FCN8s\nfrom .fcn8s import FCN8sAtOnce\nfrom .vgg import VGG16\n'
torchfcn/models/fcn16s.py,1,"b""import os.path as osp\n\nimport fcn\nimport torch.nn as nn\n\nfrom .fcn32s import get_upsampling_weight\n\n\nclass FCN16s(nn.Module):\n\n    pretrained_model = \\\n        osp.expanduser('~/data/models/pytorch/fcn16s_from_caffe.pth')\n\n    @classmethod\n    def download(cls):\n        return fcn.data.cached_download(\n            url='http://drive.google.com/uc?id=0B9P1L--7Wd2vVGE3TkRMbWlNRms',\n            path=cls.pretrained_model,\n            md5='991ea45d30d632a01e5ec48002cac617',\n        )\n\n    def __init__(self, n_class=21):\n        super(FCN16s, self).__init__()\n        # conv1\n        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=100)\n        self.relu1_1 = nn.ReLU(inplace=True)\n        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.relu1_2 = nn.ReLU(inplace=True)\n        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/2\n\n        # conv2\n        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n        self.relu2_1 = nn.ReLU(inplace=True)\n        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n        self.relu2_2 = nn.ReLU(inplace=True)\n        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/4\n\n        # conv3\n        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n        self.relu3_1 = nn.ReLU(inplace=True)\n        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n        self.relu3_2 = nn.ReLU(inplace=True)\n        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n        self.relu3_3 = nn.ReLU(inplace=True)\n        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/8\n\n        # conv4\n        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n        self.relu4_1 = nn.ReLU(inplace=True)\n        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu4_2 = nn.ReLU(inplace=True)\n        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu4_3 = nn.ReLU(inplace=True)\n        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/16\n\n        # conv5\n        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_1 = nn.ReLU(inplace=True)\n        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_2 = nn.ReLU(inplace=True)\n        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_3 = nn.ReLU(inplace=True)\n        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/32\n\n        # fc6\n        self.fc6 = nn.Conv2d(512, 4096, 7)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.drop6 = nn.Dropout2d()\n\n        # fc7\n        self.fc7 = nn.Conv2d(4096, 4096, 1)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.drop7 = nn.Dropout2d()\n\n        self.score_fr = nn.Conv2d(4096, n_class, 1)\n        self.score_pool4 = nn.Conv2d(512, n_class, 1)\n\n        self.upscore2 = nn.ConvTranspose2d(\n            n_class, n_class, 4, stride=2, bias=False)\n        self.upscore16 = nn.ConvTranspose2d(\n            n_class, n_class, 32, stride=16, bias=False)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.zero_()\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            if isinstance(m, nn.ConvTranspose2d):\n                assert m.kernel_size[0] == m.kernel_size[1]\n                initial_weight = get_upsampling_weight(\n                    m.in_channels, m.out_channels, m.kernel_size[0])\n                m.weight.data.copy_(initial_weight)\n\n    def forward(self, x):\n        h = x\n        h = self.relu1_1(self.conv1_1(h))\n        h = self.relu1_2(self.conv1_2(h))\n        h = self.pool1(h)\n\n        h = self.relu2_1(self.conv2_1(h))\n        h = self.relu2_2(self.conv2_2(h))\n        h = self.pool2(h)\n\n        h = self.relu3_1(self.conv3_1(h))\n        h = self.relu3_2(self.conv3_2(h))\n        h = self.relu3_3(self.conv3_3(h))\n        h = self.pool3(h)\n\n        h = self.relu4_1(self.conv4_1(h))\n        h = self.relu4_2(self.conv4_2(h))\n        h = self.relu4_3(self.conv4_3(h))\n        h = self.pool4(h)\n        pool4 = h  # 1/16\n\n        h = self.relu5_1(self.conv5_1(h))\n        h = self.relu5_2(self.conv5_2(h))\n        h = self.relu5_3(self.conv5_3(h))\n        h = self.pool5(h)\n\n        h = self.relu6(self.fc6(h))\n        h = self.drop6(h)\n\n        h = self.relu7(self.fc7(h))\n        h = self.drop7(h)\n\n        h = self.score_fr(h)\n        h = self.upscore2(h)\n        upscore2 = h  # 1/16\n\n        h = self.score_pool4(pool4)\n        h = h[:, :, 5:5 + upscore2.size()[2], 5:5 + upscore2.size()[3]]\n        score_pool4c = h  # 1/16\n\n        h = upscore2 + score_pool4c\n\n        h = self.upscore16(h)\n        h = h[:, :, 27:27 + x.size()[2], 27:27 + x.size()[3]].contiguous()\n\n        return h\n\n    def copy_params_from_fcn32s(self, fcn32s):\n        for name, l1 in fcn32s.named_children():\n            try:\n                l2 = getattr(self, name)\n                l2.weight  # skip ReLU / Dropout\n            except Exception:\n                continue\n            assert l1.weight.size() == l2.weight.size()\n            assert l1.bias.size() == l2.bias.size()\n            l2.weight.data.copy_(l1.weight.data)\n            l2.bias.data.copy_(l1.bias.data)\n"""
torchfcn/models/fcn32s.py,2,"b'import os.path as osp\n\nimport fcn\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\n# https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py\ndef get_upsampling_weight(in_channels, out_channels, kernel_size):\n    """"""Make a 2D bilinear kernel suitable for upsampling""""""\n    factor = (kernel_size + 1) // 2\n    if kernel_size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:kernel_size, :kernel_size]\n    filt = (1 - abs(og[0] - center) / factor) * \\\n           (1 - abs(og[1] - center) / factor)\n    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n                      dtype=np.float64)\n    weight[range(in_channels), range(out_channels), :, :] = filt\n    return torch.from_numpy(weight).float()\n\n\nclass FCN32s(nn.Module):\n\n    pretrained_model = \\\n        osp.expanduser(\'~/data/models/pytorch/fcn32s_from_caffe.pth\')\n\n    @classmethod\n    def download(cls):\n        return fcn.data.cached_download(\n            url=\'http://drive.google.com/uc?id=0B9P1L--7Wd2vM2oya3k0Zlgtekk\',\n            path=cls.pretrained_model,\n            md5=\'8acf386d722dc3484625964cbe2aba49\',\n        )\n\n    def __init__(self, n_class=21):\n        super(FCN32s, self).__init__()\n        # conv1\n        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=100)\n        self.relu1_1 = nn.ReLU(inplace=True)\n        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.relu1_2 = nn.ReLU(inplace=True)\n        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/2\n\n        # conv2\n        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n        self.relu2_1 = nn.ReLU(inplace=True)\n        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n        self.relu2_2 = nn.ReLU(inplace=True)\n        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/4\n\n        # conv3\n        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n        self.relu3_1 = nn.ReLU(inplace=True)\n        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n        self.relu3_2 = nn.ReLU(inplace=True)\n        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n        self.relu3_3 = nn.ReLU(inplace=True)\n        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/8\n\n        # conv4\n        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n        self.relu4_1 = nn.ReLU(inplace=True)\n        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu4_2 = nn.ReLU(inplace=True)\n        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu4_3 = nn.ReLU(inplace=True)\n        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/16\n\n        # conv5\n        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_1 = nn.ReLU(inplace=True)\n        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_2 = nn.ReLU(inplace=True)\n        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_3 = nn.ReLU(inplace=True)\n        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/32\n\n        # fc6\n        self.fc6 = nn.Conv2d(512, 4096, 7)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.drop6 = nn.Dropout2d()\n\n        # fc7\n        self.fc7 = nn.Conv2d(4096, 4096, 1)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.drop7 = nn.Dropout2d()\n\n        self.score_fr = nn.Conv2d(4096, n_class, 1)\n        self.upscore = nn.ConvTranspose2d(n_class, n_class, 64, stride=32,\n                                          bias=False)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.zero_()\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            if isinstance(m, nn.ConvTranspose2d):\n                assert m.kernel_size[0] == m.kernel_size[1]\n                initial_weight = get_upsampling_weight(\n                    m.in_channels, m.out_channels, m.kernel_size[0])\n                m.weight.data.copy_(initial_weight)\n\n    def forward(self, x):\n        h = x\n        h = self.relu1_1(self.conv1_1(h))\n        h = self.relu1_2(self.conv1_2(h))\n        h = self.pool1(h)\n\n        h = self.relu2_1(self.conv2_1(h))\n        h = self.relu2_2(self.conv2_2(h))\n        h = self.pool2(h)\n\n        h = self.relu3_1(self.conv3_1(h))\n        h = self.relu3_2(self.conv3_2(h))\n        h = self.relu3_3(self.conv3_3(h))\n        h = self.pool3(h)\n\n        h = self.relu4_1(self.conv4_1(h))\n        h = self.relu4_2(self.conv4_2(h))\n        h = self.relu4_3(self.conv4_3(h))\n        h = self.pool4(h)\n\n        h = self.relu5_1(self.conv5_1(h))\n        h = self.relu5_2(self.conv5_2(h))\n        h = self.relu5_3(self.conv5_3(h))\n        h = self.pool5(h)\n\n        h = self.relu6(self.fc6(h))\n        h = self.drop6(h)\n\n        h = self.relu7(self.fc7(h))\n        h = self.drop7(h)\n\n        h = self.score_fr(h)\n\n        h = self.upscore(h)\n        h = h[:, :, 19:19 + x.size()[2], 19:19 + x.size()[3]].contiguous()\n\n        return h\n\n    def copy_params_from_vgg16(self, vgg16):\n        features = [\n            self.conv1_1, self.relu1_1,\n            self.conv1_2, self.relu1_2,\n            self.pool1,\n            self.conv2_1, self.relu2_1,\n            self.conv2_2, self.relu2_2,\n            self.pool2,\n            self.conv3_1, self.relu3_1,\n            self.conv3_2, self.relu3_2,\n            self.conv3_3, self.relu3_3,\n            self.pool3,\n            self.conv4_1, self.relu4_1,\n            self.conv4_2, self.relu4_2,\n            self.conv4_3, self.relu4_3,\n            self.pool4,\n            self.conv5_1, self.relu5_1,\n            self.conv5_2, self.relu5_2,\n            self.conv5_3, self.relu5_3,\n            self.pool5,\n        ]\n        for l1, l2 in zip(vgg16.features, features):\n            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n                assert l1.weight.size() == l2.weight.size()\n                assert l1.bias.size() == l2.bias.size()\n                l2.weight.data = l1.weight.data\n                l2.bias.data = l1.bias.data\n        for i, name in zip([0, 3], [\'fc6\', \'fc7\']):\n            l1 = vgg16.classifier[i]\n            l2 = getattr(self, name)\n            l2.weight.data = l1.weight.data.view(l2.weight.size())\n            l2.bias.data = l1.bias.data.view(l2.bias.size())\n'"
torchfcn/models/fcn8s.py,1,"b""import os.path as osp\n\nimport fcn\nimport torch.nn as nn\n\nfrom .fcn32s import get_upsampling_weight\n\n\nclass FCN8s(nn.Module):\n\n    pretrained_model = \\\n        osp.expanduser('~/data/models/pytorch/fcn8s_from_caffe.pth')\n\n    @classmethod\n    def download(cls):\n        return fcn.data.cached_download(\n            url='http://drive.google.com/uc?id=0B9P1L--7Wd2vT0FtdThWREhjNkU',\n            path=cls.pretrained_model,\n            md5='dbd9bbb3829a3184913bccc74373afbb',\n        )\n\n    def __init__(self, n_class=21):\n        super(FCN8s, self).__init__()\n        # conv1\n        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=100)\n        self.relu1_1 = nn.ReLU(inplace=True)\n        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.relu1_2 = nn.ReLU(inplace=True)\n        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/2\n\n        # conv2\n        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n        self.relu2_1 = nn.ReLU(inplace=True)\n        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n        self.relu2_2 = nn.ReLU(inplace=True)\n        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/4\n\n        # conv3\n        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n        self.relu3_1 = nn.ReLU(inplace=True)\n        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n        self.relu3_2 = nn.ReLU(inplace=True)\n        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n        self.relu3_3 = nn.ReLU(inplace=True)\n        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/8\n\n        # conv4\n        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n        self.relu4_1 = nn.ReLU(inplace=True)\n        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu4_2 = nn.ReLU(inplace=True)\n        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu4_3 = nn.ReLU(inplace=True)\n        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/16\n\n        # conv5\n        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_1 = nn.ReLU(inplace=True)\n        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_2 = nn.ReLU(inplace=True)\n        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n        self.relu5_3 = nn.ReLU(inplace=True)\n        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)  # 1/32\n\n        # fc6\n        self.fc6 = nn.Conv2d(512, 4096, 7)\n        self.relu6 = nn.ReLU(inplace=True)\n        self.drop6 = nn.Dropout2d()\n\n        # fc7\n        self.fc7 = nn.Conv2d(4096, 4096, 1)\n        self.relu7 = nn.ReLU(inplace=True)\n        self.drop7 = nn.Dropout2d()\n\n        self.score_fr = nn.Conv2d(4096, n_class, 1)\n        self.score_pool3 = nn.Conv2d(256, n_class, 1)\n        self.score_pool4 = nn.Conv2d(512, n_class, 1)\n\n        self.upscore2 = nn.ConvTranspose2d(\n            n_class, n_class, 4, stride=2, bias=False)\n        self.upscore8 = nn.ConvTranspose2d(\n            n_class, n_class, 16, stride=8, bias=False)\n        self.upscore_pool4 = nn.ConvTranspose2d(\n            n_class, n_class, 4, stride=2, bias=False)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.zero_()\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            if isinstance(m, nn.ConvTranspose2d):\n                assert m.kernel_size[0] == m.kernel_size[1]\n                initial_weight = get_upsampling_weight(\n                    m.in_channels, m.out_channels, m.kernel_size[0])\n                m.weight.data.copy_(initial_weight)\n\n    def forward(self, x):\n        h = x\n        h = self.relu1_1(self.conv1_1(h))\n        h = self.relu1_2(self.conv1_2(h))\n        h = self.pool1(h)\n\n        h = self.relu2_1(self.conv2_1(h))\n        h = self.relu2_2(self.conv2_2(h))\n        h = self.pool2(h)\n\n        h = self.relu3_1(self.conv3_1(h))\n        h = self.relu3_2(self.conv3_2(h))\n        h = self.relu3_3(self.conv3_3(h))\n        h = self.pool3(h)\n        pool3 = h  # 1/8\n\n        h = self.relu4_1(self.conv4_1(h))\n        h = self.relu4_2(self.conv4_2(h))\n        h = self.relu4_3(self.conv4_3(h))\n        h = self.pool4(h)\n        pool4 = h  # 1/16\n\n        h = self.relu5_1(self.conv5_1(h))\n        h = self.relu5_2(self.conv5_2(h))\n        h = self.relu5_3(self.conv5_3(h))\n        h = self.pool5(h)\n\n        h = self.relu6(self.fc6(h))\n        h = self.drop6(h)\n\n        h = self.relu7(self.fc7(h))\n        h = self.drop7(h)\n\n        h = self.score_fr(h)\n        h = self.upscore2(h)\n        upscore2 = h  # 1/16\n\n        h = self.score_pool4(pool4)\n        h = h[:, :, 5:5 + upscore2.size()[2], 5:5 + upscore2.size()[3]]\n        score_pool4c = h  # 1/16\n\n        h = upscore2 + score_pool4c  # 1/16\n        h = self.upscore_pool4(h)\n        upscore_pool4 = h  # 1/8\n\n        h = self.score_pool3(pool3)\n        h = h[:, :,\n              9:9 + upscore_pool4.size()[2],\n              9:9 + upscore_pool4.size()[3]]\n        score_pool3c = h  # 1/8\n\n        h = upscore_pool4 + score_pool3c  # 1/8\n\n        h = self.upscore8(h)\n        h = h[:, :, 31:31 + x.size()[2], 31:31 + x.size()[3]].contiguous()\n\n        return h\n\n    def copy_params_from_fcn16s(self, fcn16s):\n        for name, l1 in fcn16s.named_children():\n            try:\n                l2 = getattr(self, name)\n                l2.weight  # skip ReLU / Dropout\n            except Exception:\n                continue\n            assert l1.weight.size() == l2.weight.size()\n            l2.weight.data.copy_(l1.weight.data)\n            if l1.bias is not None:\n                assert l1.bias.size() == l2.bias.size()\n                l2.bias.data.copy_(l1.bias.data)\n\n\nclass FCN8sAtOnce(FCN8s):\n\n    pretrained_model = \\\n        osp.expanduser('~/data/models/pytorch/fcn8s-atonce_from_caffe.pth')\n\n    @classmethod\n    def download(cls):\n        return fcn.data.cached_download(\n            url='http://drive.google.com/uc?id=0B9P1L--7Wd2vblE1VUIxV1o2d2M',\n            path=cls.pretrained_model,\n            md5='bfed4437e941fef58932891217fe6464',\n        )\n\n    def forward(self, x):\n        h = x\n        h = self.relu1_1(self.conv1_1(h))\n        h = self.relu1_2(self.conv1_2(h))\n        h = self.pool1(h)\n\n        h = self.relu2_1(self.conv2_1(h))\n        h = self.relu2_2(self.conv2_2(h))\n        h = self.pool2(h)\n\n        h = self.relu3_1(self.conv3_1(h))\n        h = self.relu3_2(self.conv3_2(h))\n        h = self.relu3_3(self.conv3_3(h))\n        h = self.pool3(h)\n        pool3 = h  # 1/8\n\n        h = self.relu4_1(self.conv4_1(h))\n        h = self.relu4_2(self.conv4_2(h))\n        h = self.relu4_3(self.conv4_3(h))\n        h = self.pool4(h)\n        pool4 = h  # 1/16\n\n        h = self.relu5_1(self.conv5_1(h))\n        h = self.relu5_2(self.conv5_2(h))\n        h = self.relu5_3(self.conv5_3(h))\n        h = self.pool5(h)\n\n        h = self.relu6(self.fc6(h))\n        h = self.drop6(h)\n\n        h = self.relu7(self.fc7(h))\n        h = self.drop7(h)\n\n        h = self.score_fr(h)\n        h = self.upscore2(h)\n        upscore2 = h  # 1/16\n\n        h = self.score_pool4(pool4 * 0.01)  # XXX: scaling to train at once\n        h = h[:, :, 5:5 + upscore2.size()[2], 5:5 + upscore2.size()[3]]\n        score_pool4c = h  # 1/16\n\n        h = upscore2 + score_pool4c  # 1/16\n        h = self.upscore_pool4(h)\n        upscore_pool4 = h  # 1/8\n\n        h = self.score_pool3(pool3 * 0.0001)  # XXX: scaling to train at once\n        h = h[:, :,\n              9:9 + upscore_pool4.size()[2],\n              9:9 + upscore_pool4.size()[3]]\n        score_pool3c = h  # 1/8\n\n        h = upscore_pool4 + score_pool3c  # 1/8\n\n        h = self.upscore8(h)\n        h = h[:, :, 31:31 + x.size()[2], 31:31 + x.size()[3]].contiguous()\n\n        return h\n\n    def copy_params_from_vgg16(self, vgg16):\n        features = [\n            self.conv1_1, self.relu1_1,\n            self.conv1_2, self.relu1_2,\n            self.pool1,\n            self.conv2_1, self.relu2_1,\n            self.conv2_2, self.relu2_2,\n            self.pool2,\n            self.conv3_1, self.relu3_1,\n            self.conv3_2, self.relu3_2,\n            self.conv3_3, self.relu3_3,\n            self.pool3,\n            self.conv4_1, self.relu4_1,\n            self.conv4_2, self.relu4_2,\n            self.conv4_3, self.relu4_3,\n            self.pool4,\n            self.conv5_1, self.relu5_1,\n            self.conv5_2, self.relu5_2,\n            self.conv5_3, self.relu5_3,\n            self.pool5,\n        ]\n        for l1, l2 in zip(vgg16.features, features):\n            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n                assert l1.weight.size() == l2.weight.size()\n                assert l1.bias.size() == l2.bias.size()\n                l2.weight.data.copy_(l1.weight.data)\n                l2.bias.data.copy_(l1.bias.data)\n        for i, name in zip([0, 3], ['fc6', 'fc7']):\n            l1 = vgg16.classifier[i]\n            l2 = getattr(self, name)\n            l2.weight.data.copy_(l1.weight.data.view(l2.weight.size()))\n            l2.bias.data.copy_(l1.bias.data.view(l2.bias.size()))\n"""
torchfcn/models/vgg.py,1,"b""import os.path as osp\n\nimport fcn\n\nimport torch\nimport torchvision\n\n\ndef VGG16(pretrained=False):\n    model = torchvision.models.vgg16(pretrained=False)\n    if not pretrained:\n        return model\n    model_file = _get_vgg16_pretrained_model()\n    state_dict = torch.load(model_file)\n    model.load_state_dict(state_dict)\n    return model\n\n\ndef _get_vgg16_pretrained_model():\n    return fcn.data.cached_download(\n        url='http://drive.google.com/uc?id=0B9P1L--7Wd2vLTJZMXpIRkVVRFk',\n        path=osp.expanduser('~/data/models/pytorch/vgg16_from_caffe.pth'),\n        md5='aa75b158f4181e7f6230029eb96c1b13',\n    )\n"""
torchfcn/ext/fcn.berkeleyvision.org/infer.py,0,"b""import numpy as np\nfrom PIL import Image\n\nimport caffe\n\n# load image, switch to BGR, subtract mean, and make dims C x H x W for Caffe\nim = Image.open('pascal/VOC2010/JPEGImages/2007_000129.jpg')\nin_ = np.array(im, dtype=np.float32)\nin_ = in_[:,:,::-1]\nin_ -= np.array((104.00698793,116.66876762,122.67891434))\nin_ = in_.transpose((2,0,1))\n\n# load net\nnet = caffe.Net('voc-fcn8s/deploy.prototxt', 'voc-fcn8s/fcn8s-heavy-pascal.caffemodel', caffe.TEST)\n# shape for input (data blob is N x C x H x W), set data\nnet.blobs['data'].reshape(1, *in_.shape)\nnet.blobs['data'].data[...] = in_\n# run net and take argmax for prediction\nnet.forward()\nout = net.blobs['score'].data[0].argmax(axis=0)\n"""
torchfcn/ext/fcn.berkeleyvision.org/nyud_layers.py,0,"b'import caffe\n\nimport numpy as np\nfrom PIL import Image\nimport scipy.io\n\nimport random\n\nclass NYUDSegDataLayer(caffe.Layer):\n    """"""\n    Load (input image, label image) pairs from NYUDv2\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    The labels follow the 40 class task defined by\n\n        S. Gupta, R. Girshick, p. Arbelaez, and J. Malik. Learning rich features\n        from RGB-D images for object detection and segmentation. ECCV 2014.\n\n    with 0 as the void label and 1-40 the classes.\n\n    Use this to feed data to a fully convolutional network.\n    """"""\n\n    def setup(self, bottom, top):\n        """"""\n        Setup data layer according to parameters:\n\n        - nyud_dir: path to NYUDv2 dir\n        - split: train / val / test\n        - tops: list of tops to output from {color, depth, hha, label}\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for NYUDv2 semantic segmentation.\n\n        example: params = dict(nyud_dir=""/path/to/NYUDVOC2011"", split=""val"",\n                               tops=[\'color\', \'hha\', \'label\'])\n        """"""\n        # config\n        params = eval(self.param_str)\n        self.nyud_dir = params[\'nyud_dir\']\n        self.split = params[\'split\']\n        self.tops = params[\'tops\']\n        self.random = params.get(\'randomize\', True)\n        self.seed = params.get(\'seed\', None)\n\n        # store top data for reshape + forward\n        self.data = {}\n\n        # means\n        self.mean_bgr = np.array((116.190, 97.203, 92.318), dtype=np.float32)\n        self.mean_hha = np.array((132.431, 94.076, 118.477), dtype=np.float32)\n        self.mean_logd = np.array((7.844,), dtype=np.float32)\n\n        # tops: check configuration\n        if len(top) != len(self.tops):\n            raise Exception(""Need to define {} tops for all outputs."")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(""Do not define a bottom."")\n\n        # load indices for images and labels\n        split_f  = \'{}/{}.txt\'.format(self.nyud_dir, self.split)\n        self.indices = open(split_f, \'r\').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if \'train\' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n    def reshape(self, bottom, top):\n        # load data for tops and  reshape tops to fit (1 is the batch dim)\n        for i, t in enumerate(self.tops):\n            self.data[t] = self.load(t, self.indices[self.idx])\n            top[i].reshape(1, *self.data[t].shape)\n\n    def forward(self, bottom, top):\n        # assign output\n        for i, t in enumerate(self.tops):\n            top[i].data[...] = self.data[t]\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n    def load(self, top, idx):\n        if top == \'color\':\n            return self.load_image(idx)\n        elif top == \'label\':\n            return self.load_label(idx)\n        elif top == \'depth\':\n            return self.load_depth(idx)\n        elif top == \'hha\':\n            return self.load_hha(idx)\n        else:\n            raise Exception(""Unknown output type: {}"".format(top))\n\n    def load_image(self, idx):\n        """"""\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        """"""\n        im = Image.open(\'{}/data/images/img_{}.png\'.format(self.nyud_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean_bgr\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n    def load_label(self, idx):\n        """"""\n        Load label image as 1 x height x width integer array of label indices.\n        Shift labels so that classes are 0-39 and void is 255 (to ignore it).\n        The leading singleton dimension is required by the loss.\n        """"""\n        label = scipy.io.loadmat(\'{}/segmentation/img_{}.mat\'.format(self.nyud_dir, idx))[\'segmentation\'].astype(np.uint8)\n        label -= 1  # rotate labels\n        label = label[np.newaxis, ...]\n        return label\n\n    def load_depth(self, idx):\n        """"""\n        Load pre-processed depth for NYUDv2 segmentation set.\n        """"""\n        im = Image.open(\'{}/data/depth/img_{}.png\'.format(self.nyud_dir, idx))\n        d = np.array(im, dtype=np.float32)\n        d = np.log(d)\n        d -= self.mean_logd\n        d = d[np.newaxis, ...]\n        return d\n\n    def load_hha(self, idx):\n        """"""\n        Load HHA features from Gupta et al. ECCV14.\n        See https://github.com/s-gupta/rcnn-depth/blob/master/rcnn/saveHHA.m\n        """"""\n        im = Image.open(\'{}/data/hha/img_{}.png\'.format(self.nyud_dir, idx))\n        hha = np.array(im, dtype=np.float32)\n        hha -= self.mean_hha\n        hha = hha.transpose((2,0,1))\n        return hha\n'"
torchfcn/ext/fcn.berkeleyvision.org/pascalcontext_layers.py,0,"b'import caffe\n\nimport numpy as np\nfrom PIL import Image\nimport scipy.io\n\nimport random\n\nclass PASCALContextSegDataLayer(caffe.Layer):\n    """"""\n    Load (input image, label image) pairs from PASCAL-Context\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    The labels follow the 59 class task defined by\n\n        R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R.\n        Urtasun, and A. Yuille.  The Role of Context for Object Detection and\n        Semantic Segmentation in the Wild.  CVPR 2014.\n\n    Use this to feed data to a fully convolutional network.\n    """"""\n\n    def setup(self, bottom, top):\n        """"""\n        Setup data layer according to parameters:\n\n        - voc_dir: path to PASCAL VOC dir (must contain 2010)\n        - context_dir: path to PASCAL-Context annotations\n        - split: train / val / test\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for PASCAL-Context semantic segmentation.\n\n        example: params = dict(voc_dir=""/path/to/PASCAL"", split=""val"")\n        """"""\n        # config\n        params = eval(self.param_str)\n        self.voc_dir = params[\'voc_dir\'] + \'/VOC2010\'\n        self.context_dir = params[\'context_dir\']\n        self.split = params[\'split\']\n        self.mean = np.array((104.007, 116.669, 122.679), dtype=np.float32)\n        self.random = params.get(\'randomize\', True)\n        self.seed = params.get(\'seed\', None)\n\n        # load labels and resolve inconsistencies by mapping to full 400 labels\n        self.labels_400 = [label.replace(\' \',\'\') for idx, label in np.genfromtxt(self.context_dir + \'/labels.txt\', delimiter=\':\', dtype=None)]\n        self.labels_59 = [label.replace(\' \',\'\') for idx, label in np.genfromtxt(self.context_dir + \'/59_labels.txt\', delimiter=\':\', dtype=None)]\n        for main_label, task_label in zip((\'table\', \'bedclothes\', \'cloth\'), (\'diningtable\', \'bedcloth\', \'clothes\')):\n            self.labels_59[self.labels_59.index(task_label)] = main_label\n\n        # two tops: data and label\n        if len(top) != 2:\n            raise Exception(""Need to define two tops: data and label."")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(""Do not define a bottom."")\n\n        # load indices for images and labels\n        split_f  = \'{}/ImageSets/Main/{}.txt\'.format(self.voc_dir,\n                self.split)\n        self.indices = open(split_f, \'r\').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if \'train\' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n    def reshape(self, bottom, top):\n        # load image + label image pair\n        self.data = self.load_image(self.indices[self.idx])\n        self.label = self.load_label(self.indices[self.idx])\n        # reshape tops to fit (leading 1 is for batch dimension)\n        top[0].reshape(1, *self.data.shape)\n        top[1].reshape(1, *self.label.shape)\n\n    def forward(self, bottom, top):\n        # assign output\n        top[0].data[...] = self.data\n        top[1].data[...] = self.label\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n    def load_image(self, idx):\n        """"""\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        """"""\n        im = Image.open(\'{}/JPEGImages/{}.jpg\'.format(self.voc_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n    def load_label(self, idx):\n        """"""\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        The full 400 labels are translated to the 59 class task labels.\n        """"""\n        label_400 = scipy.io.loadmat(\'{}/trainval/{}.mat\'.format(self.context_dir, idx))[\'LabelMap\']\n        label = np.zeros_like(label_400, dtype=np.uint8)\n        for idx, l in enumerate(self.labels_59):\n            idx_400 = self.labels_400.index(l) + 1\n            label[label_400 == idx_400] = idx + 1\n        label = label[np.newaxis, ...]\n        return label\n'"
torchfcn/ext/fcn.berkeleyvision.org/score.py,0,"b""from __future__ import division\nimport caffe\nimport numpy as np\nimport os\nimport sys\nfrom datetime import datetime\nfrom PIL import Image\n\ndef fast_hist(a, b, n):\n    k = (a >= 0) & (a < n)\n    return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)\n\ndef compute_hist(net, save_dir, dataset, layer='score', gt='label'):\n    n_cl = net.blobs[layer].channels\n    if save_dir:\n        os.mkdir(save_dir)\n    hist = np.zeros((n_cl, n_cl))\n    loss = 0\n    for idx in dataset:\n        net.forward()\n        hist += fast_hist(net.blobs[gt].data[0, 0].flatten(),\n                                net.blobs[layer].data[0].argmax(0).flatten(),\n                                n_cl)\n\n        if save_dir:\n            im = Image.fromarray(net.blobs[layer].data[0].argmax(0).astype(np.uint8), mode='P')\n            im.save(os.path.join(save_dir, idx + '.png'))\n        # compute the loss as well\n        loss += net.blobs['loss'].data.flat[0]\n    return hist, loss / len(dataset)\n\ndef seg_tests(solver, save_format, dataset, layer='score', gt='label'):\n    print '>>>', datetime.now(), 'Begin seg tests'\n    solver.test_nets[0].share_with(solver.net)\n    do_seg_tests(solver.test_nets[0], solver.iter, save_format, dataset, layer, gt)\n\ndef do_seg_tests(net, iter, save_format, dataset, layer='score', gt='label'):\n    n_cl = net.blobs[layer].channels\n    if save_format:\n        save_format = save_format.format(iter)\n    hist, loss = compute_hist(net, save_format, dataset, layer, gt)\n    # mean loss\n    print '>>>', datetime.now(), 'Iteration', iter, 'loss', loss\n    # overall accuracy\n    acc = np.diag(hist).sum() / hist.sum()\n    print '>>>', datetime.now(), 'Iteration', iter, 'overall accuracy', acc\n    # per-class accuracy\n    acc = np.diag(hist) / hist.sum(1)\n    print '>>>', datetime.now(), 'Iteration', iter, 'mean accuracy', np.nanmean(acc)\n    # per-class IU\n    iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n    print '>>>', datetime.now(), 'Iteration', iter, 'mean IU', np.nanmean(iu)\n    freq = hist.sum(1) / hist.sum()\n    print '>>>', datetime.now(), 'Iteration', iter, 'fwavacc', \\\n            (freq[freq > 0] * iu[freq > 0]).sum()\n    return hist\n"""
torchfcn/ext/fcn.berkeleyvision.org/siftflow_layers.py,0,"b'import caffe\n\nimport numpy as np\nfrom PIL import Image\nimport scipy.io\n\nimport random\n\nclass SIFTFlowSegDataLayer(caffe.Layer):\n    """"""\n    Load (input image, label image) pairs from SIFT Flow\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    This data layer has three tops:\n\n    1. the data, pre-processed\n    2. the semantic labels 0-32 and void 255\n    3. the geometric labels 0-2 and void 255\n\n    Use this to feed data to a fully convolutional network.\n    """"""\n\n    def setup(self, bottom, top):\n        """"""\n        Setup data layer according to parameters:\n\n        - siftflow_dir: path to SIFT Flow dir\n        - split: train / val / test\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for semantic segmentation of object and geometric classes.\n\n        example: params = dict(siftflow_dir=""/path/to/siftflow"", split=""val"")\n        """"""\n        # config\n        params = eval(self.param_str)\n        self.siftflow_dir = params[\'siftflow_dir\']\n        self.split = params[\'split\']\n        self.mean = np.array((114.578, 115.294, 108.353), dtype=np.float32)\n        self.random = params.get(\'randomize\', True)\n        self.seed = params.get(\'seed\', None)\n\n        # three tops: data, semantic, geometric\n        if len(top) != 3:\n            raise Exception(""Need to define three tops: data, semantic label, and geometric label."")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(""Do not define a bottom."")\n\n        # load indices for images and labels\n        split_f  = \'{}/{}.txt\'.format(self.siftflow_dir, self.split)\n        self.indices = open(split_f, \'r\').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if \'train\' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n    def reshape(self, bottom, top):\n        # load image + label image pair\n        self.data = self.load_image(self.indices[self.idx])\n        self.label_semantic = self.load_label(self.indices[self.idx], label_type=\'semantic\')\n        self.label_geometric = self.load_label(self.indices[self.idx], label_type=\'geometric\')\n        # reshape tops to fit (leading 1 is for batch dimension)\n        top[0].reshape(1, *self.data.shape)\n        top[1].reshape(1, *self.label_semantic.shape)\n        top[2].reshape(1, *self.label_geometric.shape)\n\n    def forward(self, bottom, top):\n        # assign output\n        top[0].data[...] = self.data\n        top[1].data[...] = self.label_semantic\n        top[2].data[...] = self.label_geometric\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n    def load_image(self, idx):\n        """"""\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        """"""\n        im = Image.open(\'{}/Images/spatial_envelope_256x256_static_8outdoorcategories/{}.jpg\'.format(self.siftflow_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n    def load_label(self, idx, label_type=None):\n        """"""\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        """"""\n        if label_type == \'semantic\':\n            label = scipy.io.loadmat(\'{}/SemanticLabels/spatial_envelope_256x256_static_8outdoorcategories/{}.mat\'.format(self.siftflow_dir, idx))[\'S\']\n        elif label_type == \'geometric\':\n            label = scipy.io.loadmat(\'{}/GeoLabels/spatial_envelope_256x256_static_8outdoorcategories/{}.mat\'.format(self.siftflow_dir, idx))[\'S\']\n            label[label == -1] = 0\n        else:\n            raise Exception(""Unknown label type: {}. Pick semantic or geometric."".format(label_type))\n        label = label.astype(np.uint8)\n        label -= 1  # rotate labels so classes start at 0, void is 255\n        label = label[np.newaxis, ...]\n        return label.copy()\n'"
torchfcn/ext/fcn.berkeleyvision.org/surgery.py,0,"b'from __future__ import division\nimport caffe\nimport numpy as np\n\ndef transplant(new_net, net, suffix=\'\'):\n    """"""\n    Transfer weights by copying matching parameters, coercing parameters of\n    incompatible shape, and dropping unmatched parameters.\n\n    The coercion is useful to convert fully connected layers to their\n    equivalent convolutional layers, since the weights are the same and only\n    the shapes are different.  In particular, equivalent fully connected and\n    convolution layers have shapes O x I and O x I x H x W respectively for O\n    outputs channels, I input channels, H kernel height, and W kernel width.\n\n    Both  `net` to `new_net` arguments must be instantiated `caffe.Net`s.\n    """"""\n    for p in net.params:\n        p_new = p + suffix\n        if p_new not in new_net.params:\n            print \'dropping\', p\n            continue\n        for i in range(len(net.params[p])):\n            if i > (len(new_net.params[p_new]) - 1):\n                print \'dropping\', p, i\n                break\n            if net.params[p][i].data.shape != new_net.params[p_new][i].data.shape:\n                print \'coercing\', p, i, \'from\', net.params[p][i].data.shape, \'to\', new_net.params[p_new][i].data.shape\n            else:\n                print \'copying\', p, \' -> \', p_new, i\n            new_net.params[p_new][i].data.flat = net.params[p][i].data.flat\n\ndef upsample_filt(size):\n    """"""\n    Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n    """"""\n    factor = (size + 1) // 2\n    if size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:size, :size]\n    return (1 - abs(og[0] - center) / factor) * \\\n           (1 - abs(og[1] - center) / factor)\n\ndef interp(net, layers):\n    """"""\n    Set weights of each layer in layers to bilinear kernels for interpolation.\n    """"""\n    for l in layers:\n        m, k, h, w = net.params[l][0].data.shape\n        if m != k and k != 1:\n            print \'input + output channels need to be the same or |output| == 1\'\n            raise\n        if h != w:\n            print \'filters need to be square\'\n            raise\n        filt = upsample_filt(h)\n        net.params[l][0].data[range(m), range(k), :, :] = filt\n\ndef expand_score(new_net, new_layer, net, layer):\n    """"""\n    Transplant an old score layer\'s parameters, with k < k\' classes, into a new\n    score layer with k classes s.t. the first k\' are the old classes.\n    """"""\n    old_cl = net.params[layer][0].num\n    new_net.params[new_layer][0].data[:old_cl][...] = net.params[layer][0].data\n    new_net.params[new_layer][1].data[0,0,0,:old_cl][...] = net.params[layer][1].data\n'"
torchfcn/ext/fcn.berkeleyvision.org/voc_helper.py,0,"b'import os\nimport copy\nimport glob\nimport numpy as np\n\nfrom PIL import Image\n\n\nclass voc:\n    def __init__(self, data_path):\n        # data_path is /path/to/PASCAL/VOC2011\n        self.dir = data_path\n        self.classes = [\'background\', \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                        \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\',\n                        \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\',\n                        \'pottedplant\', \'sheep\', \'sofa\', \'train\', \'tvmonitor\']\n        # for paletting\n        reference_idx = \'2008_000666\'\n        palette_im = Image.open(\'{}/SegmentationClass/{}.png\'.format(\n            self.dir, reference_idx))\n        self.palette = palette_im.palette\n\n    def load_image(self, idx):\n        im = Image.open(\'{}/JPEGImages/{}.jpg\'.format(self.dir, idx))\n        return im\n\n    def load_label(self, idx):\n        """"""\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        """"""\n        label = Image.open(\'{}/SegmentationClass/{}.png\'.format(self.dir, idx))\n        label = np.array(label, dtype=np.uint8)\n        label = label[np.newaxis, ...]\n        return label\n\n    def palette(self, label_im):\n        \'\'\'\n        Transfer the VOC color palette to an output mask for visualization.\n        \'\'\'\n        if label_im.ndim == 3:\n            label_im = label_im[0]\n        label = Image.fromarray(label_im, mode=\'P\')\n        label.palette = copy.copy(self.palette)\n        return label\n'"
torchfcn/ext/fcn.berkeleyvision.org/voc_layers.py,0,"b'import caffe\n\nimport numpy as np\nfrom PIL import Image\n\nimport random\n\nclass VOCSegDataLayer(caffe.Layer):\n    """"""\n    Load (input image, label image) pairs from PASCAL VOC\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    Use this to feed data to a fully convolutional network.\n    """"""\n\n    def setup(self, bottom, top):\n        """"""\n        Setup data layer according to parameters:\n\n        - voc_dir: path to PASCAL VOC year dir\n        - split: train / val / test\n        - mean: tuple of mean values to subtract\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for PASCAL VOC semantic segmentation.\n\n        example\n\n        params = dict(voc_dir=""/path/to/PASCAL/VOC2011"",\n            mean=(104.00698793, 116.66876762, 122.67891434),\n            split=""val"")\n        """"""\n        # config\n        params = eval(self.param_str)\n        self.voc_dir = params[\'voc_dir\']\n        self.split = params[\'split\']\n        self.mean = np.array(params[\'mean\'])\n        self.random = params.get(\'randomize\', True)\n        self.seed = params.get(\'seed\', None)\n\n        # two tops: data and label\n        if len(top) != 2:\n            raise Exception(""Need to define two tops: data and label."")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(""Do not define a bottom."")\n\n        # load indices for images and labels\n        split_f  = \'{}/ImageSets/Segmentation/{}.txt\'.format(self.voc_dir,\n                self.split)\n        self.indices = open(split_f, \'r\').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if \'train\' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n\n    def reshape(self, bottom, top):\n        # load image + label image pair\n        self.data = self.load_image(self.indices[self.idx])\n        self.label = self.load_label(self.indices[self.idx])\n        # reshape tops to fit (leading 1 is for batch dimension)\n        top[0].reshape(1, *self.data.shape)\n        top[1].reshape(1, *self.label.shape)\n\n\n    def forward(self, bottom, top):\n        # assign output\n        top[0].data[...] = self.data\n        top[1].data[...] = self.label\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n\n    def load_image(self, idx):\n        """"""\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        """"""\n        im = Image.open(\'{}/JPEGImages/{}.jpg\'.format(self.voc_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n\n    def load_label(self, idx):\n        """"""\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        """"""\n        im = Image.open(\'{}/SegmentationClass/{}.png\'.format(self.voc_dir, idx))\n        label = np.array(im, dtype=np.uint8)\n        label = label[np.newaxis, ...]\n        return label\n\n\nclass SBDDSegDataLayer(caffe.Layer):\n    """"""\n    Load (input image, label image) pairs from the SBDD extended labeling\n    of PASCAL VOC for semantic segmentation\n    one-at-a-time while reshaping the net to preserve dimensions.\n\n    Use this to feed data to a fully convolutional network.\n    """"""\n\n    def setup(self, bottom, top):\n        """"""\n        Setup data layer according to parameters:\n\n        - sbdd_dir: path to SBDD `dataset` dir\n        - split: train / seg11valid\n        - mean: tuple of mean values to subtract\n        - randomize: load in random order (default: True)\n        - seed: seed for randomization (default: None / current time)\n\n        for SBDD semantic segmentation.\n\n        N.B.segv11alid is the set of segval11 that does not intersect with SBDD.\n        Find it here: https://gist.github.com/shelhamer/edb330760338892d511e.\n\n        example\n\n        params = dict(sbdd_dir=""/path/to/SBDD/dataset"",\n            mean=(104.00698793, 116.66876762, 122.67891434),\n            split=""valid"")\n        """"""\n        # config\n        params = eval(self.param_str)\n        self.sbdd_dir = params[\'sbdd_dir\']\n        self.split = params[\'split\']\n        self.mean = np.array(params[\'mean\'])\n        self.random = params.get(\'randomize\', True)\n        self.seed = params.get(\'seed\', None)\n\n        # two tops: data and label\n        if len(top) != 2:\n            raise Exception(""Need to define two tops: data and label."")\n        # data layers have no bottoms\n        if len(bottom) != 0:\n            raise Exception(""Do not define a bottom."")\n\n        # load indices for images and labels\n        split_f  = \'{}/{}.txt\'.format(self.sbdd_dir,\n                self.split)\n        self.indices = open(split_f, \'r\').read().splitlines()\n        self.idx = 0\n\n        # make eval deterministic\n        if \'train\' not in self.split:\n            self.random = False\n\n        # randomization: seed and pick\n        if self.random:\n            random.seed(self.seed)\n            self.idx = random.randint(0, len(self.indices)-1)\n\n\n    def reshape(self, bottom, top):\n        # load image + label image pair\n        self.data = self.load_image(self.indices[self.idx])\n        self.label = self.load_label(self.indices[self.idx])\n        # reshape tops to fit (leading 1 is for batch dimension)\n        top[0].reshape(1, *self.data.shape)\n        top[1].reshape(1, *self.label.shape)\n\n\n    def forward(self, bottom, top):\n        # assign output\n        top[0].data[...] = self.data\n        top[1].data[...] = self.label\n\n        # pick next input\n        if self.random:\n            self.idx = random.randint(0, len(self.indices)-1)\n        else:\n            self.idx += 1\n            if self.idx == len(self.indices):\n                self.idx = 0\n\n\n    def backward(self, top, propagate_down, bottom):\n        pass\n\n\n    def load_image(self, idx):\n        """"""\n        Load input image and preprocess for Caffe:\n        - cast to float\n        - switch channels RGB -> BGR\n        - subtract mean\n        - transpose to channel x height x width order\n        """"""\n        im = Image.open(\'{}/img/{}.jpg\'.format(self.sbdd_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean\n        in_ = in_.transpose((2,0,1))\n        return in_\n\n\n    def load_label(self, idx):\n        """"""\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        """"""\n        import scipy.io\n        mat = scipy.io.loadmat(\'{}/cls/{}.mat\'.format(self.sbdd_dir, idx))\n        label = mat[\'GTcls\'][0][\'Segmentation\'][0].astype(np.uint8)\n        label = label[np.newaxis, ...]\n        return label\n'"
torchfcn/ext/fcn.berkeleyvision.org/nyud-fcn32s-color-d/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split, tops):\n    n = caffe.NetSpec()\n    n.color, n.depth, n.label = L.Python(module='nyud_layers',\n            layer='NYUDSegDataLayer', ntop=3,\n            param_str=str(dict(nyud_dir='../data/nyud', split=split,\n                tops=tops, seed=1337)))\n    n.data = L.Concat(n.color, n.depth)\n\n    # the base net\n    n.conv1_1_bgrd, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr = L.Convolution(n.drop7, num_output=40, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=40, kernel_size=64, stride=32,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n    n.score = crop(n.upscore, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    tops = ['color', 'depth', 'label']\n    with open('trainval.prototxt', 'w') as f:\n        f.write(str(fcn('trainval', tops)))\n\n    with open('test.prototxt', 'w') as f:\n        f.write(str(fcn('test', tops)))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/nyud-fcn32s-color-d/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../ilsvrc-nets/vgg16-fcn.caffemodel'\nbase_net = caffe.Net('../ilsvrc-nets/vgg16fcn.prototxt', '../vgg16fc.caffemodel',\n        caffe.TEST)\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsurgery.transplant(solver.net, base_net)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\nsolver.net.params['conv1_1_bgrd'][0].data[:, :3] = base_net.params['conv1_1'][0].data\nsolver.net.params['conv1_1_bgrd'][0].data[:, 3] = np.mean(base_net.params['conv1_1'][0].data, axis=1)\nsolver.net.params['conv1_1_bgrd'][1].data[...] = base_net.params['conv1_1'][1].data\n\ndel base_net\n\n# scoring\ntest = np.loadtxt('../data/nyud/test.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(2000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/nyud-fcn32s-color-hha/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef modality_fcn(net_spec, data, modality):\n    n = net_spec\n    # the base net\n    n['conv1_1' + modality], n['relu1_1' + modality] = conv_relu(n[data], 64,\n                                                                 pad=100)\n    n['conv1_2' + modality], n['relu1_2' + modality] = conv_relu(n['relu1_1' +\n        modality], 64)\n    n['pool1' + modality] = max_pool(n['relu1_2' + modality])\n\n    n['conv2_1' + modality], n['relu2_1' + modality] = conv_relu(n['pool1' +\n        modality], 128)\n    n['conv2_2' + modality], n['relu2_2' + modality] = conv_relu(n['relu2_1' +\n        modality], 128)\n    n['pool2' + modality] = max_pool(n['relu2_2' + modality])\n\n    n['conv3_1' + modality], n['relu3_1' + modality] = conv_relu(n['pool2' +\n        modality], 256)\n    n['conv3_2' + modality], n['relu3_2' + modality] = conv_relu(n['relu3_1' +\n        modality], 256)\n    n['conv3_3' + modality], n['relu3_3' + modality] = conv_relu(n['relu3_2' +\n        modality], 256)\n    n['pool3' + modality] = max_pool(n['relu3_3' + modality])\n\n    n['conv4_1' + modality], n['relu4_1' + modality] = conv_relu(n['pool3' +\n        modality], 512)\n    n['conv4_2' + modality], n['relu4_2' + modality] = conv_relu(n['relu4_1' +\n        modality], 512)\n    n['conv4_3' + modality], n['relu4_3' + modality] = conv_relu(n['relu4_2' +\n        modality], 512)\n    n['pool4' + modality] = max_pool(n['relu4_3' + modality])\n\n    n['conv5_1' + modality], n['relu5_1' + modality] = conv_relu(n['pool4' +\n        modality], 512)\n    n['conv5_2' + modality], n['relu5_2' + modality] = conv_relu(n['relu5_1' +\n        modality], 512)\n    n['conv5_3' + modality], n['relu5_3' + modality] = conv_relu(n['relu5_2' +\n        modality], 512)\n    n['pool5' + modality] = max_pool(n['relu5_3' + modality])\n\n    # fully conv\n    n['fc6' + modality], n['relu6' + modality] = conv_relu(\n        n['pool5' + modality], 4096, ks=7, pad=0)\n    n['drop6' + modality] = L.Dropout(\n        n['relu6' + modality], dropout_ratio=0.5, in_place=True)\n    n['fc7' + modality], n['relu7' + modality] = conv_relu(\n        n['drop6' + modality], 4096, ks=1, pad=0)\n    n['drop7' + modality] = L.Dropout(\n        n['relu7' + modality], dropout_ratio=0.5, in_place=True)\n    n['score_fr' + modality] = L.Convolution(\n        n['drop7' + modality], num_output=40, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return n\n\ndef fcn(split, tops):\n    n = caffe.NetSpec()\n    n.color, n.hha, n.label = L.Python(module='nyud_layers',\n            layer='NYUDSegDataLayer', ntop=3,\n            param_str=str(dict(nyud_dir='../data/nyud', split=split,\n                tops=tops, seed=1337)))\n    n = modality_fcn(n, 'color', 'color')\n    n = modality_fcn(n, 'hha', 'hha')\n    n.score_fused = L.Eltwise(n.score_frcolor, n.score_frhha,\n            operation=P.Eltwise.SUM, coeff=[0.5, 0.5])\n    n.upscore = L.Deconvolution(n.score_fused,\n        convolution_param=dict(num_output=40, kernel_size=64, stride=32,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n    n.score = crop(n.upscore, n.color)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n    return n.to_proto()\n\ndef make_net():\n    tops = ['color', 'hha', 'label']\n    with open('trainval.prototxt', 'w') as f:\n        f.write(str(fcn('trainval', tops)))\n\n    with open('test.prototxt', 'w') as f:\n        f.write(str(fcn('test', tops)))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/nyud-fcn32s-color-hha/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\ncolor_proto = '../nyud-rgb-32s/trainval.prototxt'\ncolor_weights = '../nyud-rgb-32s/nyud-rgb-32s-28k.caffemodel'\nhha_proto = '../nyud-hha-32s/trainval.prototxt'\nhha_weights = '../nyud-hha-32s/nyud-hha-32s-60k.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\n\n# surgeries\ncolor_net = caffe.Net(color_proto, color_weights, caffe.TEST)\nsurgery.transplant(solver.net, color_net, suffix='color')\ndel color_net\n\nhha_net = caffe.Net(hha_proto, hha_weights, caffe.TEST)\nsurgery.transplant(solver.net, hha_net, suffix='hha')\ndel hha_net\n\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\ntest = np.loadtxt('../data/nyud/test.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(2000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/nyud-fcn32s-color/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split, tops):\n    n = caffe.NetSpec()\n    n.data, n.label = L.Python(module='nyud_layers',\n            layer='NYUDSegDataLayer', ntop=2,\n            param_str=str(dict(nyud_dir='../data/nyud', split=split,\n                tops=tops, seed=1337)))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr = L.Convolution(n.drop7, num_output=40, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=40, kernel_size=64, stride=32,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n    n.score = crop(n.upscore, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    tops = ['color', 'label']\n    with open('trainval.prototxt', 'w') as f:\n        f.write(str(fcn('trainval', tops)))\n\n    with open('test.prototxt', 'w') as f:\n        f.write(str(fcn('test', tops)))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/nyud-fcn32s-color/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../ilsvrc-nets/vgg16-fcn.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\ntest = np.loadtxt('../data/nyud/test.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(2000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/nyud-fcn32s-hha/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split, tops):\n    n = caffe.NetSpec()\n    n.data, n.label = L.Python(module='nyud_layers',\n            layer='NYUDSegDataLayer', ntop=2,\n            param_str=str(dict(nyud_dir='../data/nyud', split=split,\n                tops=tops, seed=1337)))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr = L.Convolution(n.drop7, num_output=40, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=40, kernel_size=64, stride=32,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n    n.score = crop(n.upscore, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    tops = ['hha', 'label']\n    with open('trainval.prototxt', 'w') as f:\n        f.write(str(fcn('trainval', tops)))\n\n    with open('test.prototxt', 'w') as f:\n        f.write(str(fcn('test', tops)))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/nyud-fcn32s-hha/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../ilsvrc-nets/vgg16-fcn.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\ntest = np.loadtxt('../data/nyud/test.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(2000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/pascalcontext-fcn16s/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    n.data, n.label = L.Python(module='pascalcontext_layers',\n            layer='PASCALContextSegDataLayer', ntop=2,\n            param_str=str(dict(voc_dir='../../data/pascal',\n                context_dir='../../data/pascal-context', split=split,\n                seed=1337)))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr = L.Convolution(n.drop7, num_output=60, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore2 = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=60, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool4 = L.Convolution(n.pool4, num_output=60, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool4c = crop(n.score_pool4, n.upscore2)\n    n.fuse_pool4 = L.Eltwise(n.upscore2, n.score_pool4c,\n            operation=P.Eltwise.SUM)\n    n.upscore16 = L.Deconvolution(n.fuse_pool4,\n        convolution_param=dict(num_output=60, kernel_size=32, stride=16,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score = crop(n.upscore16, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('train.prototxt', 'w') as f:\n        f.write(str(fcn('train')))\n\n    with open('val.prototxt', 'w') as f:\n        f.write(str(fcn('val')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/pascalcontext-fcn16s/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../pascalcontext-fcn32s/pascalcontext-fcn32s.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\nval = np.loadtxt('../data/pascal/VOC2010/ImageSets/Main/val.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(8000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/pascalcontext-fcn32s/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    n.data, n.label = L.Python(module='pascalcontext_layers',\n            layer='PASCALContextSegDataLayer', ntop=2,\n            param_str=str(dict(voc_dir='../../data/pascal',\n                context_dir='../../data/pascal-context', split=split,\n                seed=1337)))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr = L.Convolution(n.drop7, num_output=60, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=60, kernel_size=64, stride=32,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n    n.score = crop(n.upscore, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n\n    return n.to_proto()\n\ndef make_net():\n    with open('train.prototxt', 'w') as f:\n        f.write(str(fcn('train')))\n\n    with open('val.prototxt', 'w') as f:\n        f.write(str(fcn('val')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/pascalcontext-fcn32s/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../ilsvrc-nets/vgg16-fcn.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\nval = np.loadtxt('../data/pascal/VOC2010/ImageSets/Main/val.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(8000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/pascalcontext-fcn8s/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    n.data, n.label = L.Python(module='pascalcontext_layers',\n            layer='PASCALContextSegDataLayer', ntop=2,\n            param_str=str(dict(voc_dir='../../data/pascal',\n                context_dir='../../data/pascal-context', split=split,\n                seed=1337)))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr = L.Convolution(n.drop7, num_output=60, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore2 = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=60, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool4 = L.Convolution(n.pool4, num_output=60, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool4c = crop(n.score_pool4, n.upscore2)\n    n.fuse_pool4 = L.Eltwise(n.upscore2, n.score_pool4c,\n            operation=P.Eltwise.SUM)\n    n.upscore_pool4 = L.Deconvolution(n.fuse_pool4,\n        convolution_param=dict(num_output=60, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool3 = L.Convolution(n.pool3, num_output=60, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool3c = crop(n.score_pool3, n.upscore_pool4)\n    n.fuse_pool3 = L.Eltwise(n.upscore_pool4, n.score_pool3c,\n            operation=P.Eltwise.SUM)\n    n.upscore8 = L.Deconvolution(n.fuse_pool3,\n        convolution_param=dict(num_output=60, kernel_size=16, stride=8,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score = crop(n.upscore8, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('train.prototxt', 'w') as f:\n        f.write(str(fcn('train')))\n\n    with open('val.prototxt', 'w') as f:\n        f.write(str(fcn('val')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/pascalcontext-fcn8s/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../pascalcontext-fcn16s/pascalcontext-fcn16s.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\nval = np.loadtxt('../data/pascal/VOC2010/ImageSets/Main/val.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(8000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/siftflow-fcn16s/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    n.data, n.sem, n.geo = L.Python(module='siftflow_layers',\n            layer='SIFTFlowSegDataLayer', ntop=3,\n            param_str=str(dict(siftflow_dir='../data/sift-flow',\n                split=split, seed=1337)))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr_sem = L.Convolution(n.drop7, num_output=33, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore2_sem = L.Deconvolution(n.score_fr_sem,\n        convolution_param=dict(num_output=33, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool4_sem = L.Convolution(n.pool4, num_output=33, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool4_semc = crop(n.score_pool4_sem, n.upscore2_sem)\n    n.fuse_pool4_sem = L.Eltwise(n.upscore2_sem, n.score_pool4_semc,\n            operation=P.Eltwise.SUM)\n    n.upscore16_sem = L.Deconvolution(n.fuse_pool4_sem,\n        convolution_param=dict(num_output=33, kernel_size=32, stride=16,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_sem = crop(n.upscore16_sem, n.data)\n    # loss to make score happy (o.w. loss_sem)\n    n.loss = L.SoftmaxWithLoss(n.score_sem, n.sem,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    n.score_fr_geo = L.Convolution(n.drop7, num_output=3, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n\n    n.upscore2_geo = L.Deconvolution(n.score_fr_geo,\n        convolution_param=dict(num_output=3, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool4_geo = L.Convolution(n.pool4, num_output=3, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool4_geoc = crop(n.score_pool4_geo, n.upscore2_geo)\n    n.fuse_pool4_geo = L.Eltwise(n.upscore2_geo, n.score_pool4_geoc,\n            operation=P.Eltwise.SUM)\n    n.upscore16_geo = L.Deconvolution(n.fuse_pool4_geo,\n        convolution_param=dict(num_output=3, kernel_size=32, stride=16,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_geo = crop(n.upscore16_geo, n.data)\n    n.loss_geo = L.SoftmaxWithLoss(n.score_geo, n.geo,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('trainval.prototxt', 'w') as f:\n        f.write(str(fcn('trainval')))\n\n    with open('test.prototxt', 'w') as f:\n        f.write(str(fcn('test')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/siftflow-fcn16s/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../siftflow-fcn32s/siftflow-fcn32s.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\ntest = np.loadtxt('../data/sift-flow/test.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(2000)\n    # N.B. metrics on the semantic labels are off b.c. of missing classes;\n    # score manually from the histogram instead for proper evaluation\n    score.seg_tests(solver, False, test, layer='score_sem', gt='sem')\n    score.seg_tests(solver, False, test, layer='score_geo', gt='geo')\n"""
torchfcn/ext/fcn.berkeleyvision.org/siftflow-fcn32s/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    n.data, n.sem, n.geo = L.Python(module='siftflow_layers',\n            layer='SIFTFlowSegDataLayer', ntop=3,\n            param_str=str(dict(siftflow_dir='../data/sift-flow',\n                split=split, seed=1337)))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr_sem = L.Convolution(n.drop7, num_output=33, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore_sem = L.Deconvolution(n.score_fr_sem,\n        convolution_param=dict(num_output=33, kernel_size=64, stride=32,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n    n.score_sem = crop(n.upscore_sem, n.data)\n    # loss to make score happy (o.w. loss_sem)\n    n.loss = L.SoftmaxWithLoss(n.score_sem, n.sem,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    n.score_fr_geo = L.Convolution(n.drop7, num_output=3, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore_geo = L.Deconvolution(n.score_fr_geo,\n        convolution_param=dict(num_output=3, kernel_size=64, stride=32,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n    n.score_geo = crop(n.upscore_geo, n.data)\n    n.loss_geo = L.SoftmaxWithLoss(n.score_geo, n.geo,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('trainval.prototxt', 'w') as f:\n        f.write(str(fcn('trainval')))\n\n    with open('test.prototxt', 'w') as f:\n        f.write(str(fcn('test')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/siftflow-fcn32s/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../ilsvrc-nets/vgg16-fcn.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\ntest = np.loadtxt('../data/sift-flow/test.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(2000)\n    # N.B. metrics on the semantic labels are off b.c. of missing classes;\n    # score manually from the histogram instead for proper evaluation\n    score.seg_tests(solver, False, test, layer='score_sem', gt='sem')\n    score.seg_tests(solver, False, test, layer='score_geo', gt='geo')\n"""
torchfcn/ext/fcn.berkeleyvision.org/siftflow-fcn8s/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    n.data, n.sem, n.geo = L.Python(module='siftflow_layers',\n            layer='SIFTFlowSegDataLayer', ntop=3,\n            param_str=str(dict(siftflow_dir='../data/sift-flow',\n                split=split, seed=1337)))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr_sem = L.Convolution(n.drop7, num_output=33, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore2_sem = L.Deconvolution(n.score_fr_sem,\n        convolution_param=dict(num_output=33, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool4_sem = L.Convolution(n.pool4, num_output=33, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool4_semc = crop(n.score_pool4_sem, n.upscore2_sem)\n    n.fuse_pool4_sem = L.Eltwise(n.upscore2_sem, n.score_pool4_semc,\n            operation=P.Eltwise.SUM)\n    n.upscore_pool4_sem  = L.Deconvolution(n.fuse_pool4_sem,\n        convolution_param=dict(num_output=33, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool3_sem = L.Convolution(n.pool3, num_output=33, kernel_size=1,\n            pad=0, param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2,\n                decay_mult=0)])\n    n.score_pool3_semc = crop(n.score_pool3_sem, n.upscore_pool4_sem)\n    n.fuse_pool3_sem = L.Eltwise(n.upscore_pool4_sem, n.score_pool3_semc,\n            operation=P.Eltwise.SUM)\n    n.upscore8_sem = L.Deconvolution(n.fuse_pool3_sem,\n        convolution_param=dict(num_output=33, kernel_size=16, stride=8,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_sem = crop(n.upscore8_sem, n.data)\n    # loss to make score happy (o.w. loss_sem)\n    n.loss = L.SoftmaxWithLoss(n.score_sem, n.sem,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    n.score_fr_geo = L.Convolution(n.drop7, num_output=3, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n\n    n.upscore2_geo = L.Deconvolution(n.score_fr_geo,\n        convolution_param=dict(num_output=3, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool4_geo = L.Convolution(n.pool4, num_output=3, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool4_geoc = crop(n.score_pool4_geo, n.upscore2_geo)\n    n.fuse_pool4_geo = L.Eltwise(n.upscore2_geo, n.score_pool4_geoc,\n            operation=P.Eltwise.SUM)\n    n.upscore_pool4_geo  = L.Deconvolution(n.fuse_pool4_geo,\n        convolution_param=dict(num_output=3, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool3_geo = L.Convolution(n.pool3, num_output=3, kernel_size=1,\n            pad=0, param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2,\n                decay_mult=0)])\n    n.score_pool3_geoc = crop(n.score_pool3_geo, n.upscore_pool4_geo)\n    n.fuse_pool3_geo = L.Eltwise(n.upscore_pool4_geo, n.score_pool3_geoc,\n            operation=P.Eltwise.SUM)\n    n.upscore8_geo = L.Deconvolution(n.fuse_pool3_geo,\n        convolution_param=dict(num_output=3, kernel_size=16, stride=8,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_geo = crop(n.upscore8_geo, n.data)\n    n.loss_geo = L.SoftmaxWithLoss(n.score_geo, n.geo,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('trainval.prototxt', 'w') as f:\n        f.write(str(fcn('trainval')))\n\n    with open('test.prototxt', 'w') as f:\n        f.write(str(fcn('test')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/siftflow-fcn8s/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../siftflow-fcn16s/siftflow-fcn16s.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\ntest = np.loadtxt('../data/sift-flow/test.txt', dtype=str)\n\nfor _ in range(50):\n    solver.step(2000)\n    # N.B. metrics on the semantic labels are off b.c. of missing classes;\n    # score manually from the histogram instead for proper evaluation\n    score.seg_tests(solver, False, test, layer='score_sem', gt='sem')\n    score.seg_tests(solver, False, test, layer='score_geo', gt='geo')\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn-alexnet/net.py,0,"b""import sys\nsys.path.append('../../python')\n\nimport caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, ks, nout, stride=1, pad=0, group=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n                                num_output=nout, pad=pad, group=group)\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks, stride=1):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    pydata_params = dict(split=split, mean=(104.00699, 116.66877, 122.67892),\n            seed=1337)\n    if split == 'train':\n        pydata_params['sbdd_dir'] = '../data/sbdd/dataset'\n        pylayer = 'SBDDSegDataLayer'\n    else:\n        pydata_params['voc_dir'] = '../data/pascal/VOC2011'\n        pylayer = 'VOCSegDataLayer'\n    n.data, n.label = L.Python(module='voc_layers', layer=pylayer,\n            ntop=2, param_str=str(pydata_params))\n\n    # the base net\n    n.conv1, n.relu1 = conv_relu(n.data, 11, 96, stride=4, pad=100)\n    n.pool1 = max_pool(n.relu1, 3, stride=2)\n    n.norm1 = L.LRN(n.pool1, local_size=5, alpha=1e-4, beta=0.75)\n    n.conv2, n.relu2 = conv_relu(n.norm1, 5, 256, pad=2, group=2)\n    n.pool2 = max_pool(n.relu2, 3, stride=2)\n    n.norm2 = L.LRN(n.pool2, local_size=5, alpha=1e-4, beta=0.75)\n    n.conv3, n.relu3 = conv_relu(n.norm2, 3, 384, pad=1)\n    n.conv4, n.relu4 = conv_relu(n.relu3, 3, 384, pad=1, group=2)\n    n.conv5, n.relu5 = conv_relu(n.relu4, 3, 256, pad=1, group=2)\n    n.pool5 = max_pool(n.relu5, 3, stride=2)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 6, 4096)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 1, 4096)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr = L.Convolution(n.drop7, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=21, kernel_size=63, stride=32,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n    n.score = crop(n.upscore, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=True, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('train.prototxt', 'w') as f:\n        f.write(str(fcn('train')))\n\n    with open('val.prototxt', 'w') as f:\n        f.write(str(fcn('seg11valid')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn-alexnet/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../ilsvrc-nets/alexnet-fcn.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\nval = np.loadtxt('../data/segvalid11.txt', dtype=str)\n\nfor _ in range(25):\n    solver.step(4000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn16s/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    pydata_params = dict(split=split, mean=(104.00699, 116.66877, 122.67892),\n            seed=1337)\n    if split == 'train':\n        pydata_params['sbdd_dir'] = '../../data/sbdd/dataset'\n        pylayer = 'SBDDSegDataLayer'\n    else:\n        pydata_params['voc_dir'] = '../../data/pascal/VOC2011'\n        pylayer = 'VOCSegDataLayer'\n    n.data, n.label = L.Python(module='voc_layers', layer=pylayer,\n            ntop=2, param_str=str(pydata_params))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n    n.score_fr = L.Convolution(n.drop7, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore2 = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=21, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool4 = L.Convolution(n.pool4, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool4c = crop(n.score_pool4, n.upscore2)\n    n.fuse_pool4 = L.Eltwise(n.upscore2, n.score_pool4c,\n            operation=P.Eltwise.SUM)\n    n.upscore16 = L.Deconvolution(n.fuse_pool4,\n        convolution_param=dict(num_output=21, kernel_size=32, stride=16,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score = crop(n.upscore16, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('train.prototxt', 'w') as f:\n        f.write(str(fcn('train')))\n\n    with open('val.prototxt', 'w') as f:\n        f.write(str(fcn('seg11valid')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn16s/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../voc-fcn32s/voc-fcn32s.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\nval = np.loadtxt('../data/segvalid11.txt', dtype=str)\n\nfor _ in range(25):\n    solver.step(4000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn32s/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    pydata_params = dict(split=split, mean=(104.00699, 116.66877, 122.67892),\n            seed=1337)\n    if split == 'train':\n        pydata_params['sbdd_dir'] = '../data/sbdd/dataset'\n        pylayer = 'SBDDSegDataLayer'\n    else:\n        pydata_params['voc_dir'] = '../data/pascal/VOC2011'\n        pylayer = 'VOCSegDataLayer'\n    n.data, n.label = L.Python(module='voc_layers', layer=pylayer,\n            ntop=2, param_str=str(pydata_params))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n    n.score_fr = L.Convolution(n.drop7, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=21, kernel_size=64, stride=32,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n    n.score = crop(n.upscore, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('train.prototxt', 'w') as f:\n        f.write(str(fcn('train')))\n\n    with open('val.prototxt', 'w') as f:\n        f.write(str(fcn('seg11valid')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn32s/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../ilsvrc-nets/vgg16-fcn.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\nval = np.loadtxt('../data/segvalid11.txt', dtype=str)\n\nfor _ in range(25):\n    solver.step(4000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn8s-atonce/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    pydata_params = dict(split=split, mean=(104.00699, 116.66877, 122.67892),\n            seed=1337)\n    if split == 'train':\n        pydata_params['sbdd_dir'] = '../data/sbdd/dataset'\n        pylayer = 'SBDDSegDataLayer'\n    else:\n        pydata_params['voc_dir'] = '../data/pascal/VOC2011'\n        pylayer = 'VOCSegDataLayer'\n    n.data, n.label = L.Python(module='voc_layers', layer=pylayer,\n            ntop=2, param_str=str(pydata_params))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n\n    n.score_fr = L.Convolution(n.drop7, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore2 = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=21, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    # scale pool4 skip for compatibility\n    n.scale_pool4 = L.Scale(n.pool4, filler=dict(type='constant',\n        value=0.01), param=[dict(lr_mult=0)])\n    n.score_pool4 = L.Convolution(n.scale_pool4, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool4c = crop(n.score_pool4, n.upscore2)\n    n.fuse_pool4 = L.Eltwise(n.upscore2, n.score_pool4c,\n            operation=P.Eltwise.SUM)\n    n.upscore_pool4 = L.Deconvolution(n.fuse_pool4,\n        convolution_param=dict(num_output=21, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    # scale pool3 skip for compatibility\n    n.scale_pool3 = L.Scale(n.pool3, filler=dict(type='constant',\n        value=0.0001), param=[dict(lr_mult=0)])\n    n.score_pool3 = L.Convolution(n.scale_pool3, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool3c = crop(n.score_pool3, n.upscore_pool4)\n    n.fuse_pool3 = L.Eltwise(n.upscore_pool4, n.score_pool3c,\n            operation=P.Eltwise.SUM)\n    n.upscore8 = L.Deconvolution(n.fuse_pool3,\n        convolution_param=dict(num_output=21, kernel_size=16, stride=8,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score = crop(n.upscore8, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('train.prototxt', 'w') as f:\n        f.write(str(fcn('train')))\n\n    with open('val.prototxt', 'w') as f:\n        f.write(str(fcn('seg11valid')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn8s-atonce/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../ilsvrc-nets/vgg16-fcn.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\nval = np.loadtxt('../data/segvalid11.txt', dtype=str)\n\nfor _ in range(75):\n    solver.step(4000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn8s/net.py,0,"b""import caffe\nfrom caffe import layers as L, params as P\nfrom caffe.coord_map import crop\n\ndef conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n        num_output=nout, pad=pad,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    return conv, L.ReLU(conv, in_place=True)\n\ndef max_pool(bottom, ks=2, stride=2):\n    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n\ndef fcn(split):\n    n = caffe.NetSpec()\n    pydata_params = dict(split=split, mean=(104.00699, 116.66877, 122.67892),\n            seed=1337)\n    if split == 'train':\n        pydata_params['sbdd_dir'] = '../data/sbdd/dataset'\n        pylayer = 'SBDDSegDataLayer'\n    else:\n        pydata_params['voc_dir'] = '../data/pascal/VOC2011'\n        pylayer = 'VOCSegDataLayer'\n    n.data, n.label = L.Python(module='voc_layers', layer=pylayer,\n            ntop=2, param_str=str(pydata_params))\n\n    # the base net\n    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n    n.pool1 = max_pool(n.relu1_2)\n\n    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n    n.pool2 = max_pool(n.relu2_2)\n\n    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n    n.pool3 = max_pool(n.relu3_3)\n\n    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n    n.pool4 = max_pool(n.relu4_3)\n\n    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n    n.pool5 = max_pool(n.relu5_3)\n\n    # fully conv\n    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n    n.score_fr = L.Convolution(n.drop7, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.upscore2 = L.Deconvolution(n.score_fr,\n        convolution_param=dict(num_output=21, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool4 = L.Convolution(n.pool4, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool4c = crop(n.score_pool4, n.upscore2)\n    n.fuse_pool4 = L.Eltwise(n.upscore2, n.score_pool4c,\n            operation=P.Eltwise.SUM)\n    n.upscore_pool4 = L.Deconvolution(n.fuse_pool4,\n        convolution_param=dict(num_output=21, kernel_size=4, stride=2,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score_pool3 = L.Convolution(n.pool3, num_output=21, kernel_size=1, pad=0,\n        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n    n.score_pool3c = crop(n.score_pool3, n.upscore_pool4)\n    n.fuse_pool3 = L.Eltwise(n.upscore_pool4, n.score_pool3c,\n            operation=P.Eltwise.SUM)\n    n.upscore8 = L.Deconvolution(n.fuse_pool3,\n        convolution_param=dict(num_output=21, kernel_size=16, stride=8,\n            bias_term=False),\n        param=[dict(lr_mult=0)])\n\n    n.score = crop(n.upscore8, n.data)\n    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n            loss_param=dict(normalize=False, ignore_label=255))\n\n    return n.to_proto()\n\ndef make_net():\n    with open('train.prototxt', 'w') as f:\n        f.write(str(fcn('train')))\n\n    with open('val.prototxt', 'w') as f:\n        f.write(str(fcn('seg11valid')))\n\nif __name__ == '__main__':\n    make_net()\n"""
torchfcn/ext/fcn.berkeleyvision.org/voc-fcn8s/solve.py,0,"b""import caffe\nimport surgery, score\n\nimport numpy as np\nimport os\nimport sys\n\ntry:\n    import setproctitle\n    setproctitle.setproctitle(os.path.basename(os.getcwd()))\nexcept:\n    pass\n\nweights = '../voc-fcn16s/voc-fcn16s.caffemodel'\n\n# init\ncaffe.set_device(int(sys.argv[1]))\ncaffe.set_mode_gpu()\n\nsolver = caffe.SGDSolver('solver.prototxt')\nsolver.net.copy_from(weights)\n\n# surgeries\ninterp_layers = [k for k in solver.net.params.keys() if 'up' in k]\nsurgery.interp(solver.net, interp_layers)\n\n# scoring\nval = np.loadtxt('../data/segvalid11.txt', dtype=str)\n\nfor _ in range(25):\n    solver.step(4000)\n    score.seg_tests(solver, False, val, layer='score')\n"""
