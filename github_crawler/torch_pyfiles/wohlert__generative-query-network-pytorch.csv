file_path,api_count,code
placeholder.py,3,"b'import torch\nfrom torch.utils.data import Dataset\n\n\nclass PlaceholderData(Dataset):\n    """"""\n    Random placeholder dataset for testing\n    training loop without loading actual data.\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(PlaceholderData, self).__init__()\n\n    def __len__(self):\n        return 2000\n\n    def __getitem__(self, idx):\n        # (b, m, c, h, w)\n        images = torch.randn(64, 15, 3, 64, 64)\n\n        # (b, m, 5)\n        viewpoints = torch.randn(64, 15, 7)\n\n        return images, viewpoints\n'"
run-convdraw.py,14,"b'import argparse\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import SVHN, MNIST\nfrom torchvision.utils import save_image\n\nfrom draw import ConvolutionalDRAW\ncuda = torch.cuda.is_available()\ndevice = torch.device(""cuda:0"" if cuda else ""cpu"")\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'ConvolutionalDRAW with MNIST/SVHN Example\')\n    parser.add_argument(\'--epochs\', type=int, default=100, help=\'number of epochs to train (default: 100)\')\n    parser.add_argument(\'--data_dir\', type=str, help=\'location of training data\', default=""./train"")\n    parser.add_argument(\'--batch_size\', type=int, default=128, help=\'size of batch (default: 128)\')\n    parser.add_argument(\'--dataset\', type=str, default=""MNIST"", help=\'dataset to use (default: MNIST)\')\n    parser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=2)\n    parser.add_argument(\'--data_parallel\', type=bool, help=\'whether to parallelise based on data (default: False)\', default=False)\n\n    args = parser.parse_args()\n\n    if args.dataset == ""MNIST"":\n        mean, std = 0, 1\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            #transforms.Normalize(mean=(0.1307,), std=(0.3081,)\n            transforms.Lambda(lambda x: torch.bernoulli(x))\n        ])\n        dataset = MNIST(root=args.data_dir, train=True, download=True, transform=transform)\n        loss = nn.BCELoss(reduce=False)\n        output_activation = torch.sigmoid\n        x_dim, x_shape = 1, (28, 28)\n\n    elif args.dataset == ""SVHN"":\n        mean, std = (0.4376, 0.4437, 0.4728), (0.198, 0.201, 0.197)\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n        dataset = SVHN(root=args.data_dir, split=""train"", download=True, transform=transform)\n        loss = nn.MSELoss(reduce=False)\n        output_activation = lambda x: x\n        x_dim, x_shape = 3, (32, 32)\n\n    # Create model and optimizer\n    model = ConvolutionalDRAW(x_dim=x_dim, x_shape=x_shape, h_dim=160, z_dim=12, T=16).to(device)\n    model = nn.DataParallel(model) if args.data_parallel else model\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999))\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, 0.5)\n\n    # Load the dataset\n    kwargs = {\'num_workers\': args.workers, \'pin_memory\': True} if cuda else {}\n    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n    \n    for epoch in range(args.epochs):\n        for x, _ in tqdm(loader):\n            batch_size = x.size(0)\n            x = x.to(device)\n\n            x_hat, kl = model(x)\n            x_hat = output_activation(x_hat)\n            \n            reconstruction = torch.sum(loss(x_hat, x).view(batch_size, -1), dim=1)\n            kl_divergence = torch.sum(kl.view(batch_size, -1), dim=1)\n            elbo = torch.mean(reconstruction + kl_divergence)\n\n            elbo.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        with torch.no_grad():\n            scheduler.step()\n\n            if epoch % 1 == 0:\n                print(""Loss at step {}: {}"".format(epoch, elbo.item()))\n\n                # Not sustainable if not dataparallel\n                x_sample = model.module.sample(args.batch_size)\n\n                # Renormalize to visualise\n                x_sample = (x_sample - mean)/std\n                x_hat = (x_hat - mean)/std\n\n                save_image(x_hat, ""reconstruction-{}.jpg"".format(epoch))\n                save_image(x_sample, ""sample-{}.jpg"".format(epoch))\n\n            if epoch % 10 == 0:\n                torch.save(model, ""model-{}.pt"".format(epoch))'"
run-draw.py,12,"b'from tqdm import tqdm\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import save_image\n\nfrom draw import DRAW\ncuda = torch.cuda.is_available()\ndevice = torch.device(""cuda:0"" if cuda else ""cpu"")\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'DRAW with MNIST Example\')\n    parser.add_argument(\'--epochs\', type=int, default=100, help=\'number of epochs to train (default: 100)\')\n    parser.add_argument(\'--batch_size\', type=int, default=64, help=\'size of batch (default: 64)\')\n    parser.add_argument(\'--data_dir\', type=str, help=\'location of training data\', default=""./train"")\n    parser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=2)\n    parser.add_argument(\'--data_parallel\', type=bool, help=\'whether to parallelise based on data (default: False)\', default=False)\n\n    args = parser.parse_args()\n\n    # Define dataset\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: torch.bernoulli(x))\n    ])\n    dataset = MNIST(root=args.data_dir, train=True, download=True, transform=transform)\n\n    # Create model and optimizer\n    model = DRAW(x_dim=784, h_dim=256, z_dim=16, T=10).to(device)\n    model = nn.DataParallel(model) if args.data_parallel else model\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.5, 0.999))\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, 0.5)\n\n    # Load the dataset\n    kwargs = {\'num_workers\': args.workers, \'pin_memory\': True} if cuda else {}\n    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n    loss = nn.BCELoss(reduce=False).to(device)\n\n    for epoch in range(args.epochs):\n        for x, _ in tqdm(loader):\n            batch_size = x.size(0)\n\n            x = x.view(batch_size, -1).to(device)\n\n            x_hat, kl_divergence = model(x)\n            x_hat = torch.sigmoid(x_hat)\n\n            reconstruction = loss(x_hat, x).sum(1)\n            kl = kl_divergence.sum(1)\n            elbo = torch.mean(reconstruction + kl)\n\n            elbo.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        with torch.no_grad():\n            scheduler.step()\n\n            if epoch % 1 == 0:\n                print(""Loss at step {}: {}"".format(epoch, elbo.item()))\n\n                # Not sustainable if not dataparallel\n                if type(model) is nn.DataParallel:\n                    x_sample = model.module.sample(args.batch_size)\n                else:\n                    x_sample = model.sample(args.batch_size)\n\n                save_image(x_hat, ""reconstruction-{}.jpg"".format(epoch))\n                save_image(x_sample, ""sample-{}.jpg"".format(epoch))\n\n            if epoch % 10 == 0:\n                torch.save(model, ""model-{}.pt"".format(epoch))\n'"
run-gqn.py,17,"b'""""""\nrun-gqn.py\n\nScript to train the a GQN on the Shepard-Metzler dataset\nin accordance to the hyperparameter settings described in\nthe supplementary materials of the paper.\n""""""\nimport random\nimport math\nfrom argparse import ArgumentParser\n\n# Torch\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import Normal\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\n\n# TensorboardX\nfrom tensorboardX import SummaryWriter\n\n# Ignite\nfrom ignite.contrib.handlers import ProgressBar\nfrom ignite.engine import Engine, Events\nfrom ignite.handlers import ModelCheckpoint, Timer\nfrom ignite.metrics import RunningAverage\n\nfrom gqn import GenerativeQueryNetwork, partition, Annealer\nfrom shepardmetzler import ShepardMetzler\n#from placeholder import PlaceholderData as ShepardMetzler\n\ncuda = torch.cuda.is_available()\ndevice = torch.device(""cuda:0"" if cuda else ""cpu"")\n\n# Random seeding\nrandom.seed(99)\ntorch.manual_seed(99)\nif cuda: torch.cuda.manual_seed(99)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser(description=\'Generative Query Network on Shepard Metzler Example\')\n    parser.add_argument(\'--n_epochs\', type=int, default=200, help=\'number of epochs run (default: 200)\')\n    parser.add_argument(\'--batch_size\', type=int, default=1, help=\'multiple of batch size (default: 1)\')\n    parser.add_argument(\'--data_dir\', type=str, help=\'location of data\', default=""train"")\n    parser.add_argument(\'--log_dir\', type=str, help=\'location of logging\', default=""log"")\n    parser.add_argument(\'--fraction\', type=float, help=\'how much of the data to use\', default=1.0)\n    parser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=4)\n    parser.add_argument(\'--data_parallel\', type=bool, help=\'whether to parallelise based on data (default: False)\', default=False)\n    args = parser.parse_args()\n\n    # Create model and optimizer\n    model = GenerativeQueryNetwork(x_dim=3, v_dim=7, r_dim=256, h_dim=128, z_dim=64, L=8).to(device)\n    model = nn.DataParallel(model) if args.data_parallel else model\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=5 * 10 ** (-5))\n\n    # Rate annealing schemes\n    sigma_scheme = Annealer(2.0, 0.7, 80000)\n    mu_scheme = Annealer(5 * 10 ** (-6), 5 * 10 ** (-6), 1.6 * 10 ** 5)\n\n    # Load the dataset\n    train_dataset = ShepardMetzler(root_dir=args.data_dir, fraction=args.fraction)\n    valid_dataset = ShepardMetzler(root_dir=args.data_dir, fraction=args.fraction, train=False)\n\n    kwargs = {\'num_workers\': args.workers, \'pin_memory\': True} if cuda else {}\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n    valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n    def step(engine, batch):\n        model.train()\n\n        x, v = batch\n        x, v = x.to(device), v.to(device)\n        x, v, x_q, v_q = partition(x, v)\n\n        # Reconstruction, representation and divergence\n        x_mu, _, kl = model(x, v, x_q, v_q)\n\n        # Log likelihood\n        sigma = next(sigma_scheme)\n        ll = Normal(x_mu, sigma).log_prob(x_q)\n\n        likelihood     = torch.mean(torch.sum(ll, dim=[1, 2, 3]))\n        kl_divergence  = torch.mean(torch.sum(kl, dim=[1, 2, 3]))\n\n        # Evidence lower bound\n        elbo = likelihood - kl_divergence\n        loss = -elbo\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        with torch.no_grad():\n            # Anneal learning rate\n            mu = next(mu_scheme)\n            i = engine.state.iteration\n            for group in optimizer.param_groups:\n                group[""lr""] = mu * math.sqrt(1 - 0.999 ** i) / (1 - 0.9 ** i)\n\n        return {""elbo"": elbo.item(), ""kl"": kl_divergence.item(), ""sigma"": sigma, ""mu"": mu}\n\n    # Trainer and metrics\n    trainer = Engine(step)\n    metric_names = [""elbo"", ""kl"", ""sigma"", ""mu""]\n    RunningAverage(output_transform=lambda x: x[""elbo""]).attach(trainer, ""elbo"")\n    RunningAverage(output_transform=lambda x: x[""kl""]).attach(trainer, ""kl"")\n    RunningAverage(output_transform=lambda x: x[""sigma""]).attach(trainer, ""sigma"")\n    RunningAverage(output_transform=lambda x: x[""mu""]).attach(trainer, ""mu"")\n    ProgressBar().attach(trainer, metric_names=metric_names)\n\n    # Model checkpointing\n    checkpoint_handler = ModelCheckpoint(""./"", ""checkpoint"", save_interval=1, n_saved=3,\n                                         require_empty=False)\n    trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler,\n                              to_save={\'model\': model.state_dict, \'optimizer\': optimizer.state_dict,\n                                       \'annealers\': (sigma_scheme.data, mu_scheme.data)})\n\n    timer = Timer(average=True).attach(trainer, start=Events.EPOCH_STARTED, resume=Events.ITERATION_STARTED,\n                 pause=Events.ITERATION_COMPLETED, step=Events.ITERATION_COMPLETED)\n\n    # Tensorbard writer\n    writer = SummaryWriter(log_dir=args.log_dir)\n\n    @trainer.on(Events.ITERATION_COMPLETED)\n    def log_metrics(engine):\n        for key, value in engine.state.metrics.items():\n            writer.add_scalar(""training/{}"".format(key), value, engine.state.iteration)\n\n    @trainer.on(Events.EPOCH_COMPLETED)\n    def save_images(engine):\n        with torch.no_grad():\n            x, v = engine.state.batch\n            x, v = x.to(device), v.to(device)\n            x, v, x_q, v_q = partition(x, v)\n\n            x_mu, r, _ = model(x, v, x_q, v_q)\n\n            r = r.view(-1, 1, 16, 16)\n\n            # Send to CPU\n            x_mu = x_mu.detach().cpu().float()\n            r = r.detach().cpu().float()\n\n            writer.add_image(""representation"", make_grid(r), engine.state.epoch)\n            writer.add_image(""reconstruction"", make_grid(x_mu), engine.state.epoch)\n\n    @trainer.on(Events.EPOCH_COMPLETED)\n    def validate(engine):\n        model.eval()\n        with torch.no_grad():\n            x, v = next(iter(valid_loader))\n            x, v = x.to(device), v.to(device)\n            x, v, x_q, v_q = partition(x, v)\n\n            # Reconstruction, representation and divergence\n            x_mu, _, kl = model(x, v, x_q, v_q)\n\n            # Validate at last sigma\n            ll = Normal(x_mu, sigma_scheme.recent).log_prob(x_q)\n\n            likelihood = torch.mean(torch.sum(ll, dim=[1, 2, 3]))\n            kl_divergence = torch.mean(torch.sum(kl, dim=[1, 2, 3]))\n\n            # Evidence lower bound\n            elbo = likelihood - kl_divergence\n\n            writer.add_scalar(""validation/elbo"", elbo.item(), engine.state.epoch)\n            writer.add_scalar(""validation/kl"", kl_divergence.item(), engine.state.epoch)\n\n    @trainer.on(Events.EXCEPTION_RAISED)\n    def handle_exception(engine, e):\n        writer.close()\n        engine.terminate()\n        if isinstance(e, KeyboardInterrupt) and (engine.state.iteration > 1):\n            import warnings\n            warnings.warn(\'KeyboardInterrupt caught. Exiting gracefully.\')\n            checkpoint_handler(engine, { \'model_exception\': model })\n        else: raise e\n\n    trainer.run(train_loader, args.n_epochs)\n    writer.close()\n'"
shepardmetzler.py,8,"b'import os, gzip\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef transform_viewpoint(v):\n    """"""\n    Transforms the viewpoint vector into a consistent\n    representation\n    """"""\n    w, z = torch.split(v, 3, dim=-1)\n    y, p = torch.split(z, 1, dim=-1)\n\n    # position, [yaw, pitch]\n    view_vector = [w, torch.cos(y), torch.sin(y), torch.cos(p), torch.sin(p)]\n    v_hat = torch.cat(view_vector, dim=-1)\n\n    return v_hat\n\n\nclass ShepardMetzler(Dataset):\n    """"""\n    Shepart Metzler mental rotation task\n    dataset. Based on the dataset provided\n    in the GQN paper. Either 5-parts or\n    7-parts.\n    :param root_dir: location of data on disc\n    :param train: whether to use train of test set\n    :param transform: transform on images\n    :param fraction: fraction of dataset to use\n    :param target_transform: transform on viewpoints\n    """"""\n    def __init__(self, root_dir, train=True, transform=None, fraction=1.0, target_transform=transform_viewpoint):\n        super(ShepardMetzler, self).__init__()\n        assert fraction > 0.0 and fraction <= 1.0\n        prefix = ""train"" if train else ""test""\n        self.root_dir = os.path.join(root_dir, prefix)\n        self.records = sorted([p for p in os.listdir(self.root_dir) if ""pt"" in p])\n        self.records = self.records[:int(len(self.records)*fraction)]\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        scene_path = os.path.join(self.root_dir, self.records[idx])\n        with gzip.open(scene_path, ""r"") as f:\n            data = torch.load(f)\n            images, viewpoints = list(zip(*data))\n\n        images = np.stack(images)\n        viewpoints = np.stack(viewpoints)\n\n        # uint8 -> float32\n        images = images.transpose(0, 1, 4, 2, 3)\n        images = torch.FloatTensor(images)/255\n\n        if self.transform:\n            images = self.transform(images)\n\n        viewpoints = torch.FloatTensor(viewpoints)\n        if self.target_transform:\n            viewpoints = self.target_transform(viewpoints)\n\n        return images, viewpoints\n\n'"
draw/__init__.py,0,"b'from .draw import DRAW, ConvolutionalDRAW\n'"
draw/draw.py,22,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Normal, kl_divergence\n\n\nclass BaseAttention(nn.Module):\n    """"""\n    No attention module.\n    """"""\n    def __init__(self, h_dim, x_dim):\n        super(BaseAttention, self).__init__()\n        self.h_dim = h_dim\n        self.x_dim = x_dim\n        self.write_head = nn.Linear(h_dim, x_dim)\n\n    def read(self, x, x_hat, h):\n        return torch.cat([x, x_hat], dim=1)\n\n    def write(self, x):\n        return self.write_head(x)\n\n\nclass FilterBankAttention(BaseAttention):\n    def __init__(self, h_dim, x_dim):\n        """"""\n        Filter bank attention mechanism described in the paper.\n        """"""\n        super(FilterBankAttention, self).__init__(h_dim, x_dim)\n\n    def read(self, x, error, h):\n       return NotImplementedError\n\n    def write(self, x):\n        return NotImplementedError\n\n\nclass DRAW(nn.Module):\n    """"""\n    Deep Recurrent Attentive Writer (DRAW) [Gregor 2015].\n\n    :param x_dim: size of input\n    :param h_dim: number of hidden neurons\n    :param z_dim: number of latent neurons\n    :param T: number of recurrent layers\n    """"""\n    def __init__(self, x_dim, h_dim=256, z_dim=10, T=10, attention_module=BaseAttention):\n        super(DRAW, self).__init__()\n        self.x_dim = x_dim\n        self.z_dim = z_dim\n        self.h_dim = h_dim\n        self.T = T\n\n        # Returns the distribution parameters\n        self.variational = nn.Linear(h_dim, 2*z_dim)\n        self.observation = nn.Linear(x_dim, x_dim)\n\n        # Recurrent encoder/decoder models\n        self.encoder = nn.LSTMCell(2*x_dim + h_dim, h_dim)\n        self.decoder = nn.LSTMCell(z_dim, h_dim)\n\n        # Attention module\n        self.attention = attention_module(h_dim, x_dim)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        # Hidden states (allocate on same device as input)\n        h_enc = x.new_zeros((batch_size, self.h_dim))\n        h_dec = x.new_zeros((batch_size, self.h_dim))\n\n        # Cell states\n        c_enc = x.new_zeros((batch_size, self.h_dim))\n        c_dec = x.new_zeros((batch_size, self.h_dim))\n\n        # Prior distribution\n        p_mu = x.new_zeros((batch_size, self.z_dim))\n        p_std = x.new_ones((batch_size, self.z_dim))\n        self.prior = Normal(p_mu, p_std)\n\n        canvas = x.new_zeros((batch_size, self.x_dim))\n        kl = 0\n\n        for _ in range(self.T):\n            x_hat = x - torch.sigmoid(canvas)\n            att = self.attention.read(x, x_hat, h_dec)\n\n            # Infer posterior density from hidden state\n            h_enc, c_enc = self.encoder(torch.cat([att, h_dec], dim=1), [h_enc, c_enc])\n\n            # Posterior distribution\n            q_mu, q_log_std = torch.split(self.variational(h_enc), self.z_dim, dim=1)\n            q_std = torch.exp(q_log_std)\n            posterior = Normal(q_mu, q_std)\n\n            # Sample from posterior\n            z = posterior.rsample()\n\n            # Send representation through decoder\n            h_dec, c_dec = self.decoder(z, [h_dec, c_dec])\n\n            # Gather representation\n            canvas += self.attention.write(h_dec)\n\n            kl += kl_divergence(posterior, self.prior)\n\n        # Return the reconstruction\n        x_mu = self.observation(canvas)\n        return [x_mu, kl]\n\n    def sample(self, z=None):\n        """"""\n        Generate a sample from the data distribution.\n\n        :param z: latent code, otherwise sample from prior\n        """"""\n        z = self.prior.sample() if z is None else z\n        batch_size = z.size(0)\n\n        canvas = z.new_zeros((batch_size, self.x_dim))\n        h_dec = z.new_zeros((batch_size, self.h_dim))\n        c_dec = z.new_zeros((batch_size, self.h_dim))\n\n        for _ in range(self.T):\n            h_dec, c_dec = self.decoder(z, [h_dec, c_dec])\n            canvas = canvas + self.attention.write(h_dec)\n\n        x_mu = self.observation(canvas)\n        return x_mu\n\n\nclass Conv2dLSTMCell(nn.Module):\n    """"""\n    2d convolutional long short-term memory (LSTM) cell.\n    Functionally equivalent to nn.LSTMCell with the\n    difference being that nn.Kinear layers are replaced\n    by nn.Conv2D layers.\n\n    :param in_channels: number of input channels\n    :param out_channels: number of output channels\n    :param kernel_size: size of image kernel\n    :param stride: length of kernel stride\n    :param padding: number of pixels to pad with\n    """"""\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super(Conv2dLSTMCell, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        kwargs = dict(kernel_size=kernel_size, stride=stride, padding=padding)\n\n        self.forget = nn.Conv2d(in_channels, out_channels, **kwargs)\n        self.input  = nn.Conv2d(in_channels, out_channels, **kwargs)\n        self.output = nn.Conv2d(in_channels, out_channels, **kwargs)\n        self.state  = nn.Conv2d(in_channels, out_channels, **kwargs)\n\n        self.transform = nn.Conv2d(out_channels, in_channels, **kwargs)\n\n    def forward(self, input, states):\n        """"""\n        Send input through the cell.\n\n        :param input: input to send through\n        :param states: (hidden, cell) pair of internal state\n        :return new (hidden, cell) pair\n        """"""\n        (hidden, cell) = states\n\n        input = input + self.transform(hidden)\n\n        forget_gate = torch.sigmoid(self.forget(input))\n        input_gate  = torch.sigmoid(self.input(input))\n        output_gate = torch.sigmoid(self.output(input))\n        state_gate  = torch.tanh(self.state(input))\n\n        # Update internal cell state\n        cell = forget_gate * cell + input_gate * state_gate\n        hidden = output_gate * torch.tanh(cell)\n\n        return hidden, cell\n\n\nclass ConvolutionalDRAW(nn.Module):\n    """"""\n    Convolutional DRAW model described in\n    ""Towards Conceptual Compression"" [Gregor 2016].\n    The model consists of a autoregressive density\n    estimator using a recurrent convolutional network.\n\n    :param x_dim: number of channels in input\n    :param x_shape: tuple representing input image shape\n    :param h_dim: number of hidden channels\n    :param z_dim: number of channels in latent variable\n    :param T: number of recurrent layers\n    """"""\n    def __init__(self, x_dim, x_shape=(32, 32), h_dim=256, z_dim=10, T=10):\n        super(ConvolutionalDRAW, self).__init__()\n        self.x_dim = x_dim\n        self.x_shape = x_shape\n        self.z_dim = z_dim\n        self.h_dim = h_dim\n        self.T = T\n\n        # Outputs parameters of distributions\n        self.variational = nn.Conv2d(h_dim, 2*z_dim, kernel_size=5, stride=1, padding=2)\n        self.prior = nn.Conv2d(h_dim, 2*z_dim, kernel_size=5, stride=1, padding=2)\n\n        # Analogous to original DRAW model\n        self.write_head = nn.Conv2d(h_dim, x_dim*4, kernel_size=1, stride=1, padding=0)\n        self.read_head  = nn.Conv2d(x_dim, x_dim, kernel_size=3, stride=2, padding=1)\n\n        # Recurrent encoder/decoder models\n        self.encoder = Conv2dLSTMCell(2*x_dim, h_dim, kernel_size=5, stride=2, padding=2)\n        self.decoder = Conv2dLSTMCell(z_dim + x_dim, h_dim, kernel_size=5, stride=1, padding=2)\n\n    def forward(self, x):\n        h, w = self.x_shape\n        batch_size = x.size(0)\n\n        # Hidden states (allocate on same device as input)\n        h_enc = x.new_zeros((batch_size, self.h_dim, h//2, w//2))\n        h_dec = x.new_zeros((batch_size, self.h_dim, h//2, w//2))\n\n        # Cell states\n        c_enc = x.new_zeros((batch_size, self.h_dim, h//2, w//2))\n        c_dec = x.new_zeros((batch_size, self.h_dim, h//2, w//2))\n\n        canvas = x.new_zeros((batch_size, self.x_dim, h, w))\n        kl = 0\n\n        for _ in range(self.T):\n            # Reconstruction error\n            epsilon = x - canvas\n\n            # Infer posterior density from hidden state\n            h_enc, c_enc = self.encoder(torch.cat([x, epsilon], dim=1), [h_enc, c_enc])\n\n            # Prior distribution\n            p_mu, p_log_std = torch.split(self.prior(h_dec), self.z_dim, dim=1)\n            p_std = torch.exp(p_log_std)\n            prior = Normal(p_mu, p_std)\n\n            # Posterior distribution\n            q_mu, q_log_std = torch.split(self.variational(h_enc), self.z_dim, dim=1)\n            q_std = torch.exp(q_log_std)\n            posterior = Normal(q_mu, q_std)\n\n            # Sample from posterior\n            z = posterior.rsample()\n\n            canvas_next = self.read_head(canvas)\n\n            # Send representation through decoder\n            h_dec, c_dec = self.decoder(torch.cat([z, canvas_next], dim=1), [h_dec, c_dec])\n\n            # Refine representation\n            canvas = canvas + F.pixel_shuffle(self.write_head(h_dec), 2)\n            kl += kl_divergence(posterior, prior)\n\n        # Return the reconstruction and kl\n        return [canvas, kl]\n\n    def sample(self, x):\n        """"""\n        Sample from the prior to generate a new\n        datapoint.\n\n        :param x: tensor representing shape of sample\n        """"""\n        h, w = self.x_shape\n        batch_size = x.size(0)\n\n        h_dec = x.new_zeros((batch_size, self.h_dim, h//2, w//2))\n        c_dec = x.new_zeros((batch_size, self.h_dim, h//2, w//2))\n\n        canvas = x.new_zeros((batch_size, self.x_dim, h, w))\n\n        for _ in range(self.T):\n            p_mu, p_log_std = torch.split(self.prior(h_dec), self.z_dim, dim=1)\n            p_std = torch.exp(p_log_std)\n            z = Normal(p_mu, p_std).sample()\n\n            canvas_next  = self.read_head(canvas)\n            h_dec, c_dec = self.decoder(torch.cat([z, canvas_next], dim=1), [h_dec, c_dec])\n            canvas = canvas + F.pixel_shuffle(self.write_head(h_dec), 2)\n\n        return canvas\n'"
gqn/__init__.py,0,"b'from .generator import GeneratorNetwork\nfrom .representation import TowerRepresentation, PyramidRepresentation\nfrom .gqn import GenerativeQueryNetwork\nfrom .training import partition, Annealer'"
gqn/generator.py,16,"b'""""""\nThe inference-generator architecture is conceptually\nsimilar to the encoder-decoder pair seen in variational\nautoencoders. The difference here is that the model\nmust infer latents from a cascade of time-dependent inputs\nusing convolutional and recurrent networks.\n\nAdditionally, a representation vector is shared between\nthe networks.\n""""""\nSCALE = 4 # Scale of image generation process\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Normal, kl_divergence\n\n\nclass Conv2dLSTMCell(nn.Module):\n    """"""\n    2d convolutional long short-term memory (LSTM) cell.\n    Functionally equivalent to nn.LSTMCell with the\n    difference being that nn.Kinear layers are replaced\n    by nn.Conv2D layers.\n\n    :param in_channels: number of input channels\n    :param out_channels: number of output channels\n    :param kernel_size: size of image kernel\n    :param stride: length of kernel stride\n    :param padding: number of pixels to pad with\n    """"""\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super(Conv2dLSTMCell, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        kwargs = dict(kernel_size=kernel_size, stride=stride, padding=padding)\n\n        self.forget = nn.Conv2d(in_channels, out_channels, **kwargs)\n        self.input  = nn.Conv2d(in_channels, out_channels, **kwargs)\n        self.output = nn.Conv2d(in_channels, out_channels, **kwargs)\n        self.state  = nn.Conv2d(in_channels, out_channels, **kwargs)\n\n        self.transform = nn.Conv2d(out_channels, in_channels, **kwargs)\n\n    def forward(self, input, states):\n        """"""\n        Send input through the cell.\n\n        :param input: input to send through\n        :param states: (hidden, cell) pair of internal state\n        :return new (hidden, cell) pair\n        """"""\n        (hidden, cell) = states\n\n        input = input + self.transform(hidden)\n\n        forget_gate = torch.sigmoid(self.forget(input))\n        input_gate  = torch.sigmoid(self.input(input))\n        output_gate = torch.sigmoid(self.output(input))\n        state_gate  = torch.tanh(self.state(input))\n\n        # Update internal cell state\n        cell = forget_gate * cell + input_gate * state_gate\n        hidden = output_gate * torch.tanh(cell)\n\n        return hidden, cell\n\n\nclass GeneratorNetwork(nn.Module):\n    """"""\n    Network similar to a convolutional variational\n    autoencoder that refines the generated image\n    over a number of iterations.\n\n    :param x_dim: number of channels in input\n    :param v_dim: dimensions of viewpoint\n    :param r_dim: dimensions of representation\n    :param z_dim: latent channels\n    :param h_dim: hidden channels in LSTM\n    :param L: number of density refinements\n    :param share: whether to share cores across refinements\n    """"""\n    def __init__(self, x_dim, v_dim, r_dim, z_dim=64, h_dim=128, L=12, share=True):\n        super(GeneratorNetwork, self).__init__()\n        self.L = L\n        self.z_dim = z_dim\n        self.h_dim = h_dim\n        self.share = share\n\n        # Core computational units\n        kwargs = dict(kernel_size=5, stride=1, padding=2)\n        inference_args = dict(in_channels=v_dim + r_dim + x_dim + h_dim, out_channels=h_dim, **kwargs)\n        generator_args = dict(in_channels=v_dim + r_dim + z_dim, out_channels=h_dim, **kwargs)\n        if self.share:\n            self.inference_core = Conv2dLSTMCell(**inference_args)\n            self.generator_core = Conv2dLSTMCell(**generator_args)\n        else:\n            self.inference_core = nn.ModuleList([Conv2dLSTMCell(**inference_args) for _ in range(L)])\n            self.generator_core = nn.ModuleList([Conv2dLSTMCell(**generator_args) for _ in range(L)])\n\n        # Inference, prior\n        self.posterior_density = nn.Conv2d(h_dim, 2*z_dim, **kwargs)\n        self.prior_density     = nn.Conv2d(h_dim, 2*z_dim, **kwargs)\n\n        # Generative density\n        self.observation_density = nn.Conv2d(h_dim, x_dim, kernel_size=1, stride=1, padding=0)\n\n        # Up/down-sampling primitives\n        self.upsample   = nn.ConvTranspose2d(h_dim, h_dim, kernel_size=SCALE, stride=SCALE, padding=0, bias=False)\n        self.downsample = nn.Conv2d(x_dim, x_dim, kernel_size=SCALE, stride=SCALE, padding=0, bias=False)\n\n    def forward(self, x, v, r):\n        """"""\n        Attempt to reconstruct x with corresponding\n        viewpoint v and context representation r.\n\n        :param x: image to send through\n        :param v: viewpoint of image\n        :param r: representation for image\n        :return reconstruction of x and kl-divergence\n        """"""\n        batch_size, _, h, w = x.shape\n        kl = 0\n\n        # Downsample x, upsample v and r\n        x = self.downsample(x)\n        v = v.view(batch_size, -1, 1, 1).repeat(1, 1, h // SCALE, w // SCALE)\n        if r.size(2) != h // SCALE:\n            r = r.repeat(1, 1, h // SCALE, w // SCALE)\n\n        # Reset hidden and cell state\n        hidden_i = x.new_zeros((batch_size, self.h_dim, h // SCALE, w // SCALE))\n        cell_i   = x.new_zeros((batch_size, self.h_dim, h // SCALE, w // SCALE))\n\n        hidden_g = x.new_zeros((batch_size, self.h_dim, h // SCALE, w // SCALE))\n        cell_g   = x.new_zeros((batch_size, self.h_dim, h // SCALE, w // SCALE))\n\n        # Canvas for updating\n        u = x.new_zeros((batch_size, self.h_dim, h, w))\n\n        for l in range(self.L):\n            # Prior factor (eta \xcf\x80 network)\n            p_mu, p_std = torch.chunk(self.prior_density(hidden_g), 2, dim=1)\n            prior_distribution = Normal(p_mu, F.softplus(p_std))\n\n            # Inference state update\n            inference = self.inference_core if self.share else self.inference_core[l]\n            hidden_i, cell_i = inference(torch.cat([hidden_g, x, v, r], dim=1), [hidden_i, cell_i])\n\n            # Posterior factor (eta e network)\n            q_mu, q_std = torch.chunk(self.posterior_density(hidden_i), 2, dim=1)\n            posterior_distribution = Normal(q_mu, F.softplus(q_std))\n\n            # Posterior sample\n            z = posterior_distribution.rsample()\n\n            # Generator state update\n            generator = self.generator_core if self.share else self.generator_core[l]\n            hidden_g, cell_g = generator(torch.cat([z, v, r], dim=1), [hidden_g, cell_g])\n\n            # Calculate u\n            u = self.upsample(hidden_g) + u\n\n            # Calculate KL-divergence\n            kl += kl_divergence(posterior_distribution, prior_distribution)\n\n        x_mu = self.observation_density(u)\n\n        return torch.sigmoid(x_mu), kl\n\n    def sample(self, x_shape, v, r):\n        """"""\n        Sample from the prior distribution to generate\n        a new image given a viewpoint and representation\n\n        :param x_shape: (height, width) of image\n        :param v: viewpoint\n        :param r: representation (context)\n        """"""\n        h, w = x_shape\n        batch_size = v.size(0)\n\n        # Increase dimensions\n        v = v.view(batch_size, -1, 1, 1).repeat(1, 1, h // SCALE, w // SCALE)\n        if r.size(2) != h // SCALE:\n            r = r.repeat(1, 1, h // SCALE, w // SCALE)\n\n        # Reset hidden and cell state for generator\n        hidden_g = v.new_zeros((batch_size, self.h_dim, h // SCALE, w // SCALE))\n        cell_g = v.new_zeros((batch_size, self.h_dim, h // SCALE, w // SCALE))\n\n        u = v.new_zeros((batch_size, self.h_dim, h, w))\n\n        for _ in range(self.L):\n            p_mu, p_log_std = torch.chunk(self.prior_density(hidden_g), 2, dim=1)\n            prior_distribution = Normal(p_mu, F.softplus(p_log_std))\n\n            # Prior sample\n            z = prior_distribution.sample()\n\n            # Calculate u\n            hidden_g, cell_g = self.generator_core(torch.cat([z, v, r], dim=1), [hidden_g, cell_g])\n            u = self.upsample(hidden_g) + u\n\n        x_mu = self.observation_density(u)\n        \n        return torch.sigmoid(x_mu)\n'"
gqn/gqn.py,4,"b'import torch\nimport torch.nn as nn\nfrom torch.distributions import Normal\n\nfrom .representation import TowerRepresentation\nfrom .generator import GeneratorNetwork\n\n\nclass GenerativeQueryNetwork(nn.Module):\n    """"""\n    Generative Query Network (GQN) as described\n    in ""Neural scene representation and rendering""\n    [Eslami 2018].\n\n    :param x_dim: number of channels in input\n    :param v_dim: dimensions of viewpoint\n    :param r_dim: dimensions of representation\n    :param z_dim: latent channels\n    :param h_dim: hidden channels in LSTM\n    :param L: Number of refinements of density\n    """"""\n    def __init__(self, x_dim, v_dim, r_dim, h_dim, z_dim, L=12):\n        super(GenerativeQueryNetwork, self).__init__()\n        self.r_dim = r_dim\n\n        self.generator = GeneratorNetwork(x_dim, v_dim, r_dim, z_dim, h_dim, L)\n        self.representation = TowerRepresentation(x_dim, v_dim, r_dim, pool=True)\n\n    def forward(self, context_x, context_v, query_x, query_v):\n        """"""\n        Forward through the GQN.\n\n        :param x: batch of context images [b, m, c, h, w]\n        :param v: batch of context viewpoints for image [b, m, k]\n        :param x_q: batch of query images [b, c, h, w]\n        :param v_q: batch of query viewpoints [b, k]\n        """"""\n        # Merge batch and view dimensions.\n        b, m, *x_dims = context_x.shape\n        _, _, *v_dims = context_v.shape\n\n        x = context_x.view((-1, *x_dims))\n        v = context_v.view((-1, *v_dims))\n\n        # representation generated from input images\n        # and corresponding viewpoints\n        phi = self.representation(x, v)\n\n        # Seperate batch and view dimensions\n        _, *phi_dims = phi.shape\n        phi = phi.view((b, m, *phi_dims))\n\n        # sum over view representations\n        r = torch.sum(phi, dim=1)\n\n        # Use random (image, viewpoint) pair in batch as query\n        x_mu, kl = self.generator(query_x, query_v, r)\n\n        # Return reconstruction and query viewpoint\n        # for computing error\n        return (x_mu, r, kl)\n\n    def sample(self, context_x, context_v, query_v, sigma):\n        """"""\n        Sample from the network given some context and viewpoint.\n\n        :param context_x: set of context images to generate representation\n        :param context_v: viewpoints of `context_x`\n        :param viewpoint: viewpoint to generate image from\n        :param sigma: pixel variance\n        """"""\n        batch_size, n_views, _, h, w = context_x.shape\n\n        _, _, *x_dims = context_x.shape\n        _, _, *v_dims = context_v.shape\n\n        x = context_x.view((-1, *x_dims))\n        v = context_v.view((-1, *v_dims))\n\n        phi = self.representation(x, v)\n\n        _, *phi_dims = phi.shape\n        phi = phi.view((batch_size, n_views, *phi_dims))\n\n        r = torch.sum(phi, dim=1)\n\n        x_mu = self.generator.sample((h, w), query_v, r)\n        return x_mu\n'"
gqn/representation.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass TowerRepresentation(nn.Module):\n    def __init__(self, n_channels, v_dim, r_dim=256, pool=True):\n        """"""\n        Network that generates a condensed representation\n        vector from a joint input of image and viewpoint.\n\n        Employs the tower/pool architecture described in the paper.\n\n        :param n_channels: number of color channels in input image\n        :param v_dim: dimensions of the viewpoint vector\n        :param r_dim: dimensions of representation\n        :param pool: whether to pool representation\n        """"""\n        super(TowerRepresentation, self).__init__()\n        # Final representation size\n        self.r_dim = k = r_dim\n        self.pool = pool\n\n        self.conv1 = nn.Conv2d(n_channels, k, kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(k, k, kernel_size=2, stride=2)\n        self.conv3 = nn.Conv2d(k, k//2, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(k//2, k, kernel_size=2, stride=2)\n\n        self.conv5 = nn.Conv2d(k + v_dim, k, kernel_size=3, stride=1, padding=1)\n        self.conv6 = nn.Conv2d(k + v_dim, k//2, kernel_size=3, stride=1, padding=1)\n        self.conv7 = nn.Conv2d(k//2, k, kernel_size=3, stride=1, padding=1)\n        self.conv8 = nn.Conv2d(k, k, kernel_size=1, stride=1)\n\n        self.avgpool  = nn.AvgPool2d(k//16)\n\n    def forward(self, x, v):\n        """"""\n        Send an (image, viewpoint) pair into the\n        network to generate a representation\n        :param x: image\n        :param v: viewpoint (x, y, z, cos(yaw), sin(yaw), cos(pitch), sin(pitch))\n        :return: representation\n        """"""\n        # Increase dimensions\n        v = v.view(v.size(0), -1, 1, 1)\n        v = v.repeat(1, 1, self.r_dim // 16, self.r_dim // 16)\n\n        # First skip-connected conv block\n        skip_in  = F.relu(self.conv1(x))\n        skip_out = F.relu(self.conv2(skip_in))\n\n        x = F.relu(self.conv3(skip_in))\n        x = F.relu(self.conv4(x)) + skip_out\n\n        # Second skip-connected conv block (merged)\n        skip_in = torch.cat([x, v], dim=1)\n        skip_out  = F.relu(self.conv5(skip_in))\n\n        x = F.relu(self.conv6(skip_in))\n        x = F.relu(self.conv7(x)) + skip_out\n\n        r = F.relu(self.conv8(x))\n\n        if self.pool:\n            r = self.avgpool(r)\n\n        return r\n\n\nclass PyramidRepresentation(nn.Module):\n    def __init__(self, n_channels, v_dim, r_dim=256):\n        """"""\n        Network that generates a condensed representation\n        vector from a joint input of image and viewpoint.\n\n        Employs the pyramid architecture described in the paper.\n\n        :param n_channels: number of color channels in input image\n        :param v_dim: dimensions of the viewpoint vector\n        :param r_dim: dimensions of representation\n        """"""\n        super(PyramidRepresentation, self).__init__()\n        # Final representation size\n        self.r_dim = k = r_dim\n\n        self.conv1 = nn.Conv2d(n_channels + v_dim, k//8, kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(k//8, k//4, kernel_size=2, stride=2)\n        self.conv3 = nn.Conv2d(k//4, k//2, kernel_size=2, stride=2)\n        self.conv4 = nn.Conv2d(k//2, k, kernel_size=8, stride=8)\n\n    def forward(self, x, v):\n        """"""\n        Send an (image, viewpoint) pair into the\n        network to generate a representation\n        :param x: image\n        :param v: viewpoint (x, y, z, cos(yaw), sin(yaw), cos(pitch), sin(pitch))\n        :return: representation\n        """"""\n        # Increase dimensions\n        batch_size, _, h, w = x.shape\n\n        v = v.view(batch_size, -1, 1, 1)\n        v = v.repeat(1, 1, h, w)\n\n        # Merge representation\n        r = torch.cat([x, v], dim=1)\n\n        r  = F.relu(self.conv1(r))\n        r  = F.relu(self.conv2(r))\n        r  = F.relu(self.conv3(r))\n        r  = F.relu(self.conv4(r))\n\n        return r'"
gqn/training.py,0,"b'import random\n\n\nclass Annealer(object):\n    def __init__(self, init, delta, steps):\n        self.init = init\n        self.delta = delta\n        self.steps = steps\n        self.s = 0\n        self.data = self.__repr__()\n        self.recent = init\n\n    def __repr__(self):\n        return {""init"": self.init, ""delta"": self.delta, ""steps"": self.steps, ""s"": self.s}\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        self.s += 1\n        value = max(self.delta + (self.init - self.delta) * (1 - self.s / self.steps), self.delta)\n        self.recent = value\n        return value\n\n\ndef partition(images, viewpoints):\n    """"""\n    Partition batch into context and query sets.\n    :param images\n    :param viewpoints\n    :return: context images, context viewpoint, query image, query viewpoint\n    """"""\n    # Maximum number of context points to use\n    _, b, m, *x_dims = images.shape\n    _, b, m, *v_dims = viewpoints.shape\n\n    # ""Squeeze"" the batch dimension\n    images = images.view((-1, m, *x_dims))\n    viewpoints = viewpoints.view((-1, m, *v_dims))\n\n    # Sample random number of views\n    n_context = random.randint(2, m - 1)\n    indices = random.sample([i for i in range(m)], n_context)\n\n    # Partition into context and query sets\n    context_idx, query_idx = indices[:-1], indices[-1]\n\n    x, v = images[:, context_idx], viewpoints[:, context_idx]\n    x_q, v_q = images[:, query_idx], viewpoints[:, query_idx]\n\n    return x, v, x_q, v_q'"
scripts/tfrecord-converter.py,1,"b'""""""\ntfrecord-converter\n\nTakes a directory of tf-records with Shepard-Metzler data\nand converts it into a number of gzipped PyTorch records\nwith a fixed batch size.\n\nThanks to l3robot and versatran01 for providing initial\nscripts.\n""""""\nimport os, gzip, torch\nimport tensorflow as tf, numpy as np, multiprocessing as mp\nfrom functools import partial\nfrom itertools import islice, chain\nfrom argparse import ArgumentParser\n\n# disable logging and gpu\ntf.logging.set_verbosity(tf.logging.ERROR)\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\n\nPOSE_DIM, IMG_DIM, SEQ_DIM = 5, 64, 15\n\ndef chunk(iterable, size=10):\n    """"""\n    Chunks an iterator into subsets of\n    a given size.\n    """"""\n    iterator = iter(iterable)\n    for first in iterator:\n        yield chain([first], islice(iterator, size - 1))\n\ndef process(record):\n    """"""\n    Processes a tf-record into a numpy (image, pose) tuple.\n    """"""\n    kwargs = dict(dtype=tf.uint8, back_prop=False)\n    for data in tf.python_io.tf_record_iterator(record):\n        instance = tf.parse_single_example(data, {\n            \'frames\': tf.FixedLenFeature(shape=SEQ_DIM, dtype=tf.string),\n            \'cameras\': tf.FixedLenFeature(shape=SEQ_DIM * POSE_DIM, dtype=tf.float32)\n        })\n\n        # Get data\n        images = tf.concat(instance[\'frames\'], axis=0)\n        poses  = instance[\'cameras\']\n\n        # Convert\n        images = tf.map_fn(tf.image.decode_jpeg, tf.reshape(images, [-1]), **kwargs)\n        images = tf.reshape(images, (-1, SEQ_DIM, IMG_DIM, IMG_DIM, 3))\n        poses  = tf.reshape(poses,  (-1, SEQ_DIM, POSE_DIM))\n\n        # Numpy conversion\n        images, poses = images.numpy(), poses.numpy()\n        yield np.squeeze(images), np.squeeze(poses)\n\ndef convert(record, batch_size):\n    """"""\n    Processes and saves a tf-record.\n    """"""\n    path, filename = os.path.split(record)\n    basename, *_ = os.path.splitext(filename)\n    print(basename)\n\n    batch_process = lambda r: chunk(process(r), batch_size)\n\n    for i, batch in enumerate(batch_process(record)):\n        p = os.path.join(path, ""{0:}-{1:02}.pt.gz"".format(basename, i))\n        with gzip.open(p, \'wb\') as f:\n            torch.save(list(batch), f)\n\nif __name__ == \'__main__\':\n    tf.enable_eager_execution()\n    parser = ArgumentParser(description=\'Convert gqn tfrecords to gzip files.\')\n    parser.add_argument(\'base_dir\', nargs=1,\n                        help=\'base directory of gqn dataset\')\n    parser.add_argument(\'dataset\', type=str, default=""shepard_metzler_5_parts"",\n                        help=\'datasets to convert, eg. shepard_metzler_5_parts\')\n    parser.add_argument(\'-b\', \'--batch-size\', type=int, default=64,\n                        help=\'number of sequences in each output file\')\n    parser.add_argument(\'-m\', \'--mode\', type=str, default=\'train\',\n                        help=\'whether to convert train or test\')\n    args = parser.parse_args()\n\n    # Find path\n    base_dir = os.path.expanduser(args.base_dir[0])\n    data_dir = os.path.join(base_dir, args.dataset, args.mode)\n\n    # Find all records\n    records = [os.path.join(data_dir, f) for f in sorted(os.listdir(data_dir))]\n    records = [f for f in records if ""tfrecord"" in f]\n\n    with mp.Pool(processes=mp.cpu_count()) as pool:\n        f = partial(convert, batch_size=args.batch_size)\n        pool.map(f, records)\n'"
