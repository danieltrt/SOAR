file_path,api_count,code
examples/betavae.py,9,"b'from urllib import request\n\nimport torch\nimport numpy as np\nimport sys\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nsys.path.append(""../semi-supervised"")\n\ntorch.manual_seed(1337)\nnp.random.seed(1337)\n\ncuda = torch.cuda.is_available()\nprint(""CUDA: {}"".format(cuda))\n\ndef binary_cross_entropy(r, x):\n    ""Drop in replacement until PyTorch adds `reduce` keyword.""\n    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n\n\nclass SpriteDataset(Dataset):\n    def __init__(self, transform=None, download=False):\n        self.transform = transform\n        url = ""https://github.com/deepmind/dsprites-dataset/raw/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz""\n\n        if download:\n            request.urlretrieve(url, ""./dsprites.npz"")\n\n        try:\n            self.dset = np.load(""./dsprites.npz"", encoding=""bytes"")[""imgs""]\n        except FileNotFoundError:\n            print(""Dataset not found, have you set download=True?"")\n\n    def __len__(self):\n        return len(self.dset)\n\n    def __getitem__(self, idx):\n        sample = self.dset[idx]\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n\nif __name__ == ""__main__"":\n    from itertools import repeat\n    from torch.autograd import Variable\n\n    dset = SpriteDataset(transform=lambda x: x.reshape(-1), download=True)\n    unlabelled = DataLoader(dset, batch_size=16, shuffle=True, sampler=SubsetRandomSampler(np.arange(len(dset)//3)))\n\n    models = []\n\n    from models import VariationalAutoencoder\n    model = VariationalAutoencoder([64**2, 10, [1200, 1200]])\n    model.decoder = nn.Sequential(\n        nn.Linear(10, 1200),\n        nn.Tanh(),\n        nn.Linear(1200, 1200),\n        nn.Tanh(),\n        nn.Linear(1200, 1200),\n        nn.Tanh(),\n        nn.Linear(10, 64**2),\n        nn.Sigmoid(),\n    )\n\n    if cuda: model = model.cuda()\n\n    beta = repeat(4.0)\n    optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-2)\n\n    epochs = 251\n    best = np.inf\n\n    file = open(model.__class__.__name__ + "".log"", \'w+\')\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for u in unlabelled:\n            u = Variable(u.float())\n\n            if cuda:\n                u = u.cuda(device=0)\n\n            reconstruction = model(u)\n\n            likelihood = -binary_cross_entropy(reconstruction, u)\n            elbo = likelihood - next(beta) * model.kl_divergence\n\n            L = -torch.mean(elbo)\n\n            L.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            total_loss += L.data[0]\n\n        m = len(unlabelled)\n        print(total_loss / m, sep=""\\t"")\n\n        if total_loss < best:\n            best = total_loss\n            torch.save(model, \'{}.pt\'.format(model.__class__.__name__))\n'"
examples/mnist_sslvae.py,19,"b'import torch\nimport numpy as np\nimport sys\nsys.path.append(""../semi-supervised"")\n\ntorch.manual_seed(1337)\nnp.random.seed(1337)\n\ncuda = torch.cuda.is_available()\nprint(""CUDA: {}"".format(cuda))\n\ndef binary_cross_entropy(r, x):\n    ""Drop in replacement until PyTorch adds `reduce` keyword.""\n    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n\nn_labels = 10\ndef get_mnist(location=""./"", batch_size=64, labels_per_class=100):\n    from functools import reduce\n    from operator import __or__\n    from torch.utils.data.sampler import SubsetRandomSampler\n    from torchvision.datasets import MNIST\n    import torchvision.transforms as transforms\n    from utils import onehot\n\n    flatten_bernoulli = lambda x: transforms.ToTensor()(x).view(-1).bernoulli()\n\n    mnist_train = MNIST(location, train=True, download=True,\n                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n    mnist_valid = MNIST(location, train=False, download=True,\n                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n\n    def get_sampler(labels, n=None):\n        # Only choose digits in n_labels\n        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n\n        # Ensure uniform distribution of labels\n        np.random.shuffle(indices)\n        indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in range(n_labels)])\n\n        indices = torch.from_numpy(indices)\n        sampler = SubsetRandomSampler(indices)\n        return sampler\n\n    # Dataloaders for MNIST\n    labelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n                                           sampler=get_sampler(mnist_train.train_labels.numpy(), labels_per_class))\n    unlabelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n                                             sampler=get_sampler(mnist_train.train_labels.numpy()))\n    validation = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n                                             sampler=get_sampler(mnist_valid.test_labels.numpy()))\n\n    return labelled, unlabelled, validation\n\nif __name__ == ""__main__"":\n    from itertools import repeat, cycle\n    from torch.autograd import Variable\n    from inference import SVI, DeterministicWarmup, ImportanceWeightedSampler\n\n    labelled, unlabelled, validation = get_mnist(location=""./"", batch_size=100, labels_per_class=10)\n    alpha = 0.1 * len(unlabelled) / len(labelled)\n\n    models = []\n\n    # Kingma 2014, M2 model. Reported: 88%, achieved: ??%\n    # from models import DeepGenerativeModel\n    # models += [DeepGenerativeModel([784, n_labels, 50, [600, 600]])]\n\n    # Maal\xc3\xb8e 2016, ADGM model. Reported: 99.4%, achieved: ??%\n    # from models import AuxiliaryDeepGenerativeModel\n    # models += [AuxiliaryDeepGenerativeModel([784, n_labels, 100, 100, [500, 500]])]\n\n    from models import LadderDeepGenerativeModel\n    models += [LadderDeepGenerativeModel([784, n_labels, [32, 16, 8], [128, 128, 128]])]\n\n    for model in models:\n        if cuda: model = model.cuda()\n\n        beta = DeterministicWarmup(n=4*len(unlabelled)*100)\n        sampler = ImportanceWeightedSampler(mc=1, iw=1)\n\n        elbo = SVI(model, likelihood=binary_cross_entropy, beta=beta, sampler=sampler)\n        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n\n        epochs = 251\n        best = 0.0\n\n        file = open(model.__class__.__name__ + "".log"", \'w+\')\n\n        for epoch in range(epochs):\n            model.train()\n            total_loss, labelled_loss, unlabelled_loss, accuracy = (0, 0, 0, 0)\n            for (x, y), (u, _) in zip(cycle(labelled), unlabelled):\n                # Wrap in variables\n                x, y, u = Variable(x), Variable(y), Variable(u)\n\n                if cuda:\n                    # They need to be on the same device and be synchronized.\n                    x, y = x.cuda(device=0), y.cuda(device=0)\n                    u = u.cuda(device=0)\n\n                L = -elbo(x, y)\n                U = -elbo(u)\n\n                # Add auxiliary classification loss q(y|x)\n                logits = model.classify(x)\n                classication_loss = torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n\n                J_alpha = L - alpha * classication_loss + U\n\n                J_alpha.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                total_loss += J_alpha.data[0]\n                labelled_loss += L.data[0]\n                unlabelled_loss += U.data[0]\n\n                _, pred_idx = torch.max(logits, 1)\n                _, lab_idx = torch.max(y, 1)\n                accuracy += torch.mean((pred_idx.data == lab_idx.data).float())\n\n            m = len(unlabelled)\n            print(*(total_loss / m, labelled_loss / m, unlabelled_loss / m, accuracy / m), sep=""\\t"", file=file)\n\n            if epoch % 1 == 0:\n                model.eval()\n                print(""Epoch: {}"".format(epoch))\n                print(""[Train]\\t\\t J_a: {:.2f}, L: {:.2f}, U: {:.2f}, accuracy: {:.2f}"".format(total_loss / m,\n                                                                                              labelled_loss / m,\n                                                                                              unlabelled_loss / m,\n                                                                                              accuracy / m))\n\n                total_loss, labelled_loss, unlabelled_loss, accuracy = (0, 0, 0, 0)\n                for x, y in validation:\n                    x, y = Variable(x), Variable(y)\n\n                    if cuda:\n                        x, y = x.cuda(device=0), y.cuda(device=0)\n\n                    L = -elbo(x, y)\n                    U = -elbo(x)\n\n                    logits = model.classify(x)\n                    classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n\n                    J_alpha = L + alpha * classication_loss + U\n\n                    total_loss += J_alpha.data[0]\n                    labelled_loss += L.data[0]\n                    unlabelled_loss += U.data[0]\n\n                    _, pred_idx = torch.max(logits, 1)\n                    _, lab_idx = torch.max(y, 1)\n                    accuracy += torch.mean((pred_idx.data == lab_idx.data).float())\n\n                m = len(validation)\n                print(*(total_loss / m, labelled_loss / m, unlabelled_loss / m, accuracy / m), sep=""\\t"", file=file)\n                print(""[Validation]\\t J_a: {:.2f}, L: {:.2f}, U: {:.2f}, accuracy: {:.2f}"".format(total_loss / m,\n                                                                                              labelled_loss / m,\n                                                                                              unlabelled_loss / m,\n                                                                                              accuracy / m))\n\n            if accuracy > best:\n                best = accuracy\n                torch.save(model, \'{}.pt\'.format(model.__class__.__name__))'"
semi-supervised/utils.py,12,"b'import torch\nfrom torch.autograd import Variable\n\n\ndef enumerate_discrete(x, y_dim):\n    """"""\n    Generates a `torch.Tensor` of size batch_size x n_labels of\n    the given label.\n\n    Example: generate_label(2, 1, 3) #=> torch.Tensor([[0, 1, 0],\n                                                       [0, 1, 0]])\n    :param x: tensor with batch size to mimic\n    :param y_dim: number of total labels\n    :return variable\n    """"""\n    def batch(batch_size, label):\n        labels = (torch.ones(batch_size, 1) * label).type(torch.LongTensor)\n        y = torch.zeros((batch_size, y_dim))\n        y.scatter_(1, labels, 1)\n        return y.type(torch.LongTensor)\n\n    batch_size = x.size(0)\n    generated = torch.cat([batch(batch_size, i) for i in range(y_dim)])\n\n    if x.is_cuda:\n        generated = generated.cuda()\n\n    return Variable(generated.float())\n\n\ndef onehot(k):\n    """"""\n    Converts a number to its one-hot or 1-of-k representation\n    vector.\n    :param k: (int) length of vector\n    :return: onehot function\n    """"""\n    def encode(label):\n        y = torch.zeros(k)\n        if label < k:\n            y[label] = 1\n        return y\n    return encode\n\n\ndef log_sum_exp(tensor, dim=-1, sum_op=torch.sum):\n    """"""\n    Uses the LogSumExp (LSE) as an approximation for the sum in a log-domain.\n    :param tensor: Tensor to compute LSE over\n    :param dim: dimension to perform operation over\n    :param sum_op: reductive operation to be applied, e.g. torch.sum or torch.mean\n    :return: LSE\n    """"""\n    max, _ = torch.max(tensor, dim=dim, keepdim=True)\n    return torch.log(sum_op(torch.exp(tensor - max), dim=dim, keepdim=True) + 1e-8) + max'"
examples/notebooks/datautils.py,7,"b'import torch\nimport numpy as np\nimport sys\nfrom urllib import request\nfrom torch.utils.data import Dataset\nsys.path.append(""../semi-supervised"")\nn_labels = 10\ncuda = torch.cuda.is_available()\n\n\nclass SpriteDataset(Dataset):\n    """"""\n    A PyTorch wrapper for the dSprites dataset by\n    Matthey et al. 2017. The dataset provides a 2D scene\n    with a sprite under different transformations:\n    * color\n    * shape\n    * scale\n    * orientation\n    * x-position\n    * y-position\n    """"""\n    def __init__(self, transform=None):\n        self.transform = transform\n        url = ""https://github.com/deepmind/dsprites-dataset/raw/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz""\n        \n        try:\n            self.dset = np.load(""./dsprites.npz"", encoding=""bytes"")[""imgs""]\n        except FileNotFoundError:\n            request.urlretrieve(url, ""./dsprites.npz"")\n            self.dset = np.load(""./dsprites.npz"", encoding=""bytes"")[""imgs""]\n\n    def __len__(self):\n        return len(self.dset)\n\n    def __getitem__(self, idx):\n        sample = self.dset[idx]\n                \n        if self.transform:\n            sample = self.transform(sample)\n            \n        return sample\n\n\ndef get_mnist(location=""./"", batch_size=64, labels_per_class=100):\n    from functools import reduce\n    from operator import __or__\n    from torch.utils.data.sampler import SubsetRandomSampler\n    from torchvision.datasets import MNIST\n    import torchvision.transforms as transforms\n    from utils import onehot\n\n    flatten_bernoulli = lambda x: transforms.ToTensor()(x).view(-1).bernoulli()\n\n    mnist_train = MNIST(location, train=True, download=True,\n                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n    mnist_valid = MNIST(location, train=False, download=True,\n                        transform=flatten_bernoulli, target_transform=onehot(n_labels))\n\n    def get_sampler(labels, n=None):\n        # Only choose digits in n_labels\n        (indices,) = np.where(reduce(__or__, [labels == i for i in np.arange(n_labels)]))\n\n        # Ensure uniform distribution of labels\n        np.random.shuffle(indices)\n        indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in range(n_labels)])\n\n        indices = torch.from_numpy(indices)\n        sampler = SubsetRandomSampler(indices)\n        return sampler\n\n    # Dataloaders for MNIST\n    labelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n                                           sampler=get_sampler(mnist_train.train_labels.numpy(), labels_per_class))\n    unlabelled = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n                                             sampler=get_sampler(mnist_train.train_labels.numpy()))\n    validation = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, num_workers=2, pin_memory=cuda,\n                                             sampler=get_sampler(mnist_valid.test_labels.numpy()))\n\n    return labelled, unlabelled, validation\n'"
semi-supervised/inference/__init__.py,0,"b'from .distributions import log_standard_gaussian, log_gaussian, log_standard_categorical\nfrom .variational import SVI, DeterministicWarmup, ImportanceWeightedSampler'"
semi-supervised/inference/distributions.py,6,"b'import math\nimport torch\nimport torch.nn.functional as F\n\n\ndef log_standard_gaussian(x):\n    """"""\n    Evaluates the log pdf of a standard normal distribution at x.\n\n    :param x: point to evaluate\n    :return: log N(x|0,I)\n    """"""\n    return torch.sum(-0.5 * math.log(2 * math.pi) - x ** 2 / 2, dim=-1)\n\n\ndef log_gaussian(x, mu, log_var):\n    """"""\n    Returns the log pdf of a normal distribution parametrised\n    by mu and log_var evaluated at x.\n\n    :param x: point to evaluate\n    :param mu: mean of distribution\n    :param log_var: log variance of distribution\n    :return: log N(x|\xc2\xb5,\xcf\x83)\n    """"""\n    log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n    return torch.sum(log_pdf, dim=-1)\n\n\ndef log_standard_categorical(p):\n    """"""\n    Calculates the cross entropy between a (one-hot) categorical vector\n    and a standard (uniform) categorical distribution.\n\n    :param p: one-hot categorical distribution\n    :return: H(p, u)\n    """"""\n    # Uniform prior over y\n    prior = F.softmax(torch.ones_like(p), dim=1)\n    prior.requires_grad = False\n\n    cross_entropy = -torch.sum(p * torch.log(prior + 1e-8), dim=1)\n\n    return cross_entropy'"
semi-supervised/inference/variational.py,6,"b'from itertools import repeat\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom utils import log_sum_exp, enumerate_discrete\nfrom .distributions import log_standard_categorical\n\nclass ImportanceWeightedSampler(object):\n    """"""\n    Importance weighted sampler [Burda 2015] to\n    be used in conjunction with SVI.\n    """"""\n    def __init__(self, mc=1, iw=1):\n        """"""\n        Initialise a new sampler.\n        :param mc: number of Monte Carlo samples\n        :param iw: number of Importance Weighted samples\n        """"""\n        self.mc = mc\n        self.iw = iw\n\n    def resample(self, x):\n        return x.repeat(self.mc * self.iw, 1)\n\n    def __call__(self, elbo):\n        elbo = elbo.view(self.mc, self.iw, -1)\n        elbo = torch.mean(log_sum_exp(elbo, dim=1, sum_op=torch.mean), dim=0)\n        return elbo.view(-1)\n\n\nclass DeterministicWarmup(object):\n    """"""\n    Linear deterministic warm-up as described in\n    [S\xc3\xb8nderby 2016].\n    """"""\n    def __init__(self, n=100, t_max=1):\n        self.t = 0\n        self.t_max = t_max\n        self.inc = 1/n\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        t = self.t + self.inc\n\n        self.t = self.t_max if t > self.t_max else t\n        return self.t\n\n\nclass SVI(nn.Module):\n    """"""\n    Stochastic variational inference (SVI).\n    """"""\n    base_sampler = ImportanceWeightedSampler(mc=1, iw=1)\n    def __init__(self, model, likelihood=F.binary_cross_entropy, beta=repeat(1), sampler=base_sampler):\n        """"""\n        Initialises a new SVI optimizer for semi-\n        supervised learning.\n        :param model: semi-supervised model to evaluate\n        :param likelihood: p(x|y,z) for example BCE or MSE\n        :param sampler: sampler for x and y, e.g. for Monte Carlo\n        :param beta: warm-up/scaling of KL-term\n        """"""\n        super(SVI, self).__init__()\n        self.model = model\n        self.likelihood = likelihood\n        self.sampler = sampler\n        self.beta = beta\n\n    def forward(self, x, y=None):\n        is_labelled = False if y is None else True\n\n        # Prepare for sampling\n        xs, ys = (x, y)\n\n        # Enumerate choices of label\n        if not is_labelled:\n            ys = enumerate_discrete(xs, self.model.y_dim)\n            xs = xs.repeat(self.model.y_dim, 1)\n\n        # Increase sampling dimension\n        xs = self.sampler.resample(xs)\n        ys = self.sampler.resample(ys)\n\n        reconstruction = self.model(xs, ys)\n\n        # p(x|y,z)\n        likelihood = -self.likelihood(reconstruction, xs)\n\n        # p(y)\n        prior = -log_standard_categorical(ys)\n\n        # Equivalent to -L(x, y)\n        elbo = likelihood + prior - next(self.beta) * self.model.kl_divergence\n        L = self.sampler(elbo)\n\n        if is_labelled:\n            return torch.mean(L)\n\n        logits = self.model.classify(x)\n\n        L = L.view_as(logits.t()).t()\n\n        # Calculate entropy H(q(y|x)) and sum over all labels\n        H = -torch.sum(torch.mul(logits, torch.log(logits + 1e-8)), dim=-1)\n        L = torch.sum(torch.mul(logits, L), dim=-1)\n\n        # Equivalent to -U(x)\n        U = L + H\n        return torch.mean(U)'"
semi-supervised/layers/__init__.py,0,"b'from .stochastic import GaussianSample, GaussianMerge, GumbelSoftmax\nfrom .flow import NormalizingFlows, PlanarNormalizingFlow'"
semi-supervised/layers/flow.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PlanarNormalizingFlow(nn.Module):\n    """"""\n    Planar normalizing flow [Rezende & Mohamed 2015].\n    Provides a tighter bound on the ELBO by giving more expressive\n    power to the approximate distribution, such as by introducing\n    covariance between terms.\n    """"""\n    def __init__(self, in_features):\n        super(PlanarNormalizingFlow, self).__init__()\n        self.u = nn.Parameter(torch.randn(in_features))\n        self.w = nn.Parameter(torch.randn(in_features))\n        self.b = nn.Parameter(torch.ones(1))\n\n    def forward(self, z):\n        # Create uhat such that it is parallel to w\n        uw = torch.dot(self.u, self.w)\n        muw = -1 + F.softplus(uw)\n        uhat = self.u + (muw - uw) * torch.transpose(self.w, 0, -1) / torch.sum(self.w ** 2)\n\n        # Equation 21 - Transform z\n        zwb = torch.mv(z, self.w) + self.b\n\n        f_z = z + (uhat.view(1, -1) * F.tanh(zwb).view(-1, 1))\n\n        # Compute the Jacobian using the fact that\n        # tanh(x) dx = 1 - tanh(x)**2\n        psi = (1 - F.tanh(zwb)**2).view(-1, 1) * self.w.view(1, -1)\n        psi_u = torch.mv(psi, uhat)\n\n        # Return the transformed output along\n        # with log determninant of J\n        logdet_jacobian = torch.log(torch.abs(1 + psi_u) + 1e-8)\n\n        return f_z, logdet_jacobian\n\n\nclass NormalizingFlows(nn.Module):\n    """"""\n    Presents a sequence of normalizing flows as a torch.nn.Module.\n    """"""\n    def __init__(self, in_features, flow_type=PlanarNormalizingFlow, n_flows=1):\n        super(NormalizingFlows, self).__init__()\n        self.flows = nn.ModuleList([flow_type(in_features) for _ in range(n_flows)])\n\n    def forward(self, z):\n        log_det_jacobian = []\n\n        for flow in self.flows:\n            z, j = flow(z)\n            log_det_jacobian.append(j)\n\n        return z, sum(log_det_jacobian)'"
semi-supervised/layers/stochastic.py,9,"b'import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Stochastic(nn.Module):\n    """"""\n    Base stochastic layer that uses the\n    reparametrization trick [Kingma 2013]\n    to draw a sample from a distribution\n    parametrised by mu and log_var.\n    """"""\n    def reparametrize(self, mu, log_var):\n        epsilon = Variable(torch.randn(mu.size()), requires_grad=False)\n\n        if mu.is_cuda:\n            epsilon = epsilon.cuda()\n\n        # log_std = 0.5 * log_var\n        # std = exp(log_std)\n        std = log_var.mul(0.5).exp_()\n\n        # z = std * epsilon + mu\n        z = mu.addcmul(std, epsilon)\n\n        return z\n\nclass GaussianSample(Stochastic):\n    """"""\n    Layer that represents a sample from a\n    Gaussian distribution.\n    """"""\n    def __init__(self, in_features, out_features):\n        super(GaussianSample, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n\n        self.mu = nn.Linear(in_features, out_features)\n        self.log_var = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        mu = self.mu(x)\n        log_var = F.softplus(self.log_var(x))\n\n        return self.reparametrize(mu, log_var), mu, log_var\n\n\nclass GaussianMerge(GaussianSample):\n    """"""\n    Precision weighted merging of two Gaussian\n    distributions.\n    Merges information from z into the given\n    mean and log variance and produces\n    a sample from this new distribution.\n    """"""\n    def __init__(self, in_features, out_features):\n        super(GaussianMerge, self).__init__(in_features, out_features)\n\n    def forward(self, z, mu1, log_var1):\n        # Calculate precision of each distribution\n        # (inverse variance)\n        mu2 = self.mu(z)\n        log_var2 = F.softplus(self.log_var(z))\n        precision1, precision2 = (1/torch.exp(log_var1), 1/torch.exp(log_var2))\n\n        # Merge distributions into a single new\n        # distribution\n        mu = ((mu1 * precision1) + (mu2 * precision2)) / (precision1 + precision2)\n\n        var = 1 / (precision1 + precision2)\n        log_var = torch.log(var + 1e-8)\n\n        return self.reparametrize(mu, log_var), mu, log_var\n\n\nclass GumbelSoftmax(Stochastic):\n    """"""\n    Layer that represents a sample from a categorical\n    distribution. Enables sampling and stochastic\n    backpropagation using the Gumbel-Softmax trick.\n    """"""\n    def __init__(self, in_features, out_features, n_distributions):\n        super(GumbelSoftmax, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_distributions = n_distributions\n\n        self.logits = nn.Linear(in_features, n_distributions*out_features)\n\n    def forward(self, x, tau=1.0):\n        logits = self.logits(x).view(-1, self.n_distributions)\n\n        # variational distribution over categories\n        softmax = F.softmax(logits, dim=-1) #q_y\n        sample = self.reparametrize(logits, tau).view(-1, self.n_distributions, self.out_features)\n        sample = torch.mean(sample, dim=1)\n\n        return sample, softmax\n\n    def reparametrize(self, logits, tau=1.0):\n        epsilon = Variable(torch.rand(logits.size()), requires_grad=False)\n\n        if logits.is_cuda:\n            epsilon = epsilon.cuda()\n\n        # Gumbel distributed noise\n        gumbel = -torch.log(-torch.log(epsilon+1e-8)+1e-8)\n        # Softmax as a continuous approximation of argmax\n        y = F.softmax((logits + gumbel)/tau, dim=1)\n        return y\n'"
semi-supervised/models/__init__.py,0,"b'from .vae import VariationalAutoencoder, LadderVariationalAutoencoder, GumbelAutoencoder\nfrom .dgm import DeepGenerativeModel, StackedDeepGenerativeModel, AuxiliaryDeepGenerativeModel,\\\n    LadderDeepGenerativeModel'"
semi-supervised/models/dgm.py,13,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\nfrom .vae import VariationalAutoencoder\nfrom .vae import Encoder, Decoder, LadderEncoder, LadderDecoder\n\n\nclass Classifier(nn.Module):\n    def __init__(self, dims):\n        """"""\n        Single hidden layer classifier\n        with softmax output.\n        """"""\n        super(Classifier, self).__init__()\n        [x_dim, h_dim, y_dim] = dims\n        self.dense = nn.Linear(x_dim, h_dim)\n        self.logits = nn.Linear(h_dim, y_dim)\n\n    def forward(self, x):\n        x = F.relu(self.dense(x))\n        x = F.softmax(self.logits(x), dim=-1)\n        return x\n\n\nclass DeepGenerativeModel(VariationalAutoencoder):\n    def __init__(self, dims):\n        """"""\n        M2 code replication from the paper\n        \'Semi-Supervised Learning with Deep Generative Models\'\n        (Kingma 2014) in PyTorch.\n\n        The ""Generative semi-supervised model"" is a probabilistic\n        model that incorporates label information in both\n        inference and generation.\n\n        Initialise a new generative model\n        :param dims: dimensions of x, y, z and hidden layers.\n        """"""\n        [x_dim, self.y_dim, z_dim, h_dim] = dims\n        super(DeepGenerativeModel, self).__init__([x_dim, z_dim, h_dim])\n\n        self.encoder = Encoder([x_dim + self.y_dim, h_dim, z_dim])\n        self.decoder = Decoder([z_dim + self.y_dim, list(reversed(h_dim)), x_dim])\n        self.classifier = Classifier([x_dim, h_dim[0], self.y_dim])\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.xavier_normal(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    def forward(self, x, y):\n        # Add label and data and generate latent variable\n        z, z_mu, z_log_var = self.encoder(torch.cat([x, y], dim=1))\n\n        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n\n        # Reconstruct data point from latent data and label\n        x_mu = self.decoder(torch.cat([z, y], dim=1))\n\n        return x_mu\n\n    def classify(self, x):\n        logits = self.classifier(x)\n        return logits\n\n    def sample(self, z, y):\n        """"""\n        Samples from the Decoder to generate an x.\n        :param z: latent normal variable\n        :param y: label (one-hot encoded)\n        :return: x\n        """"""\n        y = y.float()\n        x = self.decoder(torch.cat([z, y], dim=1))\n        return x\n\n\nclass StackedDeepGenerativeModel(DeepGenerativeModel):\n    def __init__(self, dims, features):\n        """"""\n        M1+M2 model as described in [Kingma 2014].\n\n        Initialise a new stacked generative model\n        :param dims: dimensions of x, y, z and hidden layers\n        :param features: a pretrained M1 model of class `VariationalAutoencoder`\n            trained on the same dataset.\n        """"""\n        [x_dim, y_dim, z_dim, h_dim] = dims\n        super(StackedDeepGenerativeModel, self).__init__([features.z_dim, y_dim, z_dim, h_dim])\n\n        # Be sure to reconstruct with the same dimensions\n        in_features = self.decoder.reconstruction.in_features\n        self.decoder.reconstruction = nn.Linear(in_features, x_dim)\n\n        # Make vae feature model untrainable by freezing parameters\n        self.features = features\n        self.features.train(False)\n\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x, y):\n        # Sample a new latent x from the M1 model\n        x_sample, _, _ = self.features.encoder(x)\n\n        # Use the sample as new input to M2\n        return super(StackedDeepGenerativeModel, self).forward(x_sample, y)\n\n    def classify(self, x):\n        _, x, _ = self.features.encoder(x)\n        logits = self.classifier(x)\n        return logits\n\nclass AuxiliaryDeepGenerativeModel(DeepGenerativeModel):\n    def __init__(self, dims):\n        """"""\n        Auxiliary Deep Generative Models [Maal\xc3\xb8e 2016]\n        code replication. The ADGM introduces an additional\n        latent variable \'a\', which enables the model to fit\n        more complex variational distributions.\n\n        :param dims: dimensions of x, y, z, a and hidden layers.\n        """"""\n        [x_dim, y_dim, z_dim, a_dim, h_dim] = dims\n        super(AuxiliaryDeepGenerativeModel, self).__init__([x_dim, y_dim, z_dim, h_dim])\n\n        self.aux_encoder = Encoder([x_dim, h_dim, a_dim])\n        self.aux_decoder = Encoder([x_dim + z_dim + y_dim, list(reversed(h_dim)), a_dim])\n\n        self.classifier = Classifier([x_dim + a_dim, h_dim[0], y_dim])\n\n        self.encoder = Encoder([a_dim + y_dim + x_dim, h_dim, z_dim])\n        self.decoder = Decoder([y_dim + z_dim, list(reversed(h_dim)), x_dim])\n\n    def classify(self, x):\n        # Auxiliary inference q(a|x)\n        a, a_mu, a_log_var = self.aux_encoder(x)\n\n        # Classification q(y|a,x)\n        logits = self.classifier(torch.cat([x, a], dim=1))\n        return logits\n\n    def forward(self, x, y):\n        """"""\n        Forward through the model\n        :param x: features\n        :param y: labels\n        :return: reconstruction\n        """"""\n        # Auxiliary inference q(a|x)\n        q_a, q_a_mu, q_a_log_var = self.aux_encoder(x)\n\n        # Latent inference q(z|a,y,x)\n        z, z_mu, z_log_var = self.encoder(torch.cat([x, y, q_a], dim=1))\n\n        # Generative p(x|z,y)\n        x_mu = self.decoder(torch.cat([z, y], dim=1))\n\n        # Generative p(a|z,y,x)\n        p_a, p_a_mu, p_a_log_var = self.aux_decoder(torch.cat([x, y, z], dim=1))\n\n        a_kl = self._kld(q_a, (q_a_mu, q_a_log_var), (p_a_mu, p_a_log_var))\n        z_kl = self._kld(z, (z_mu, z_log_var))\n\n        self.kl_divergence = a_kl + z_kl\n\n        return x_mu\n\n\nclass LadderDeepGenerativeModel(DeepGenerativeModel):\n    def __init__(self, dims):\n        """"""\n        Ladder version of the Deep Generative Model.\n        Uses a hierarchical representation that is\n        trained end-to-end to give very nice disentangled\n        representations.\n\n        :param dims: dimensions of x, y, z layers and h layers\n            note that len(z) == len(h).\n        """"""\n        [x_dim, y_dim, z_dim, h_dim] = dims\n        super(LadderDeepGenerativeModel, self).__init__([x_dim, y_dim, z_dim[0], h_dim])\n\n        neurons = [x_dim, *h_dim]\n        encoder_layers = [LadderEncoder([neurons[i - 1], neurons[i], z_dim[i - 1]]) for i in range(1, len(neurons))]\n\n        e = encoder_layers[-1]\n        encoder_layers[-1] = LadderEncoder([e.in_features + y_dim, e.out_features, e.z_dim])\n\n        decoder_layers = [LadderDecoder([z_dim[i - 1], h_dim[i - 1], z_dim[i]]) for i in range(1, len(h_dim))][::-1]\n\n        self.classifier = Classifier([x_dim, h_dim[0], y_dim])\n\n        self.encoder = nn.ModuleList(encoder_layers)\n        self.decoder = nn.ModuleList(decoder_layers)\n        self.reconstruction = Decoder([z_dim[0]+y_dim, h_dim, x_dim])\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.xavier_normal(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    def forward(self, x, y):\n        # Gather latent representation\n        # from encoders along with final z.\n        latents = []\n        for i, encoder in enumerate(self.encoder):\n            if i == len(self.encoder)-1:\n                x, (z, mu, log_var) = encoder(torch.cat([x, y], dim=1))\n            else:\n                x, (z, mu, log_var) = encoder(x)\n            latents.append((mu, log_var))\n\n        latents = list(reversed(latents))\n\n        self.kl_divergence = 0\n        for i, decoder in enumerate([-1, *self.decoder]):\n            # If at top, encoder == decoder,\n            # use prior for KL.\n            l_mu, l_log_var = latents[i]\n            if i == 0:\n                self.kl_divergence += self._kld(z, (l_mu, l_log_var))\n\n            # Perform downword merge of information.\n            else:\n                z, kl = decoder(z, l_mu, l_log_var)\n                self.kl_divergence += self._kld(*kl)\n\n        x_mu = self.reconstruction(torch.cat([z, y], dim=1))\n        return x_mu\n\n    def sample(self, z, y):\n        for i, decoder in enumerate(self.decoder):\n            z = decoder(z)\n        return self.reconstruction(torch.cat([z, y], dim=1))'"
semi-supervised/models/vae.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn import init\n\nfrom layers import GaussianSample, GaussianMerge, GumbelSoftmax\nfrom inference import log_gaussian, log_standard_gaussian\n\n\nclass Perceptron(nn.Module):\n    def __init__(self, dims, activation_fn=F.relu, output_activation=None):\n        super(Perceptron, self).__init__()\n        self.dims = dims\n        self.activation_fn = activation_fn\n        self.output_activation = output_activation\n\n        self.layers = nn.ModuleList(list(map(lambda d: nn.Linear(*d), list(zip(dims, dims[1:])))))\n\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = layer(x)\n            if i == len(self.layers)-1 and self.output_activation is not None:\n                x = self.output_activation(x)\n            else:\n                x = self.activation_fn(x)\n\n        return x\n\n\n\nclass Encoder(nn.Module):\n    def __init__(self, dims, sample_layer=GaussianSample):\n        """"""\n        Inference network\n\n        Attempts to infer the probability distribution\n        p(z|x) from the data by fitting a variational\n        distribution q_\xcf\x86(z|x). Returns the two parameters\n        of the distribution (\xc2\xb5, log \xcf\x83\xc2\xb2).\n\n        :param dims: dimensions of the networks\n           given by the number of neurons on the form\n           [input_dim, [hidden_dims], latent_dim].\n        """"""\n        super(Encoder, self).__init__()\n\n        [x_dim, h_dim, z_dim] = dims\n        neurons = [x_dim, *h_dim]\n        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n\n        self.hidden = nn.ModuleList(linear_layers)\n        self.sample = sample_layer(h_dim[-1], z_dim)\n\n    def forward(self, x):\n        for layer in self.hidden:\n            x = F.relu(layer(x))\n        return self.sample(x)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, dims):\n        """"""\n        Generative network\n\n        Generates samples from the original distribution\n        p(x) by transforming a latent representation, e.g.\n        by finding p_\xce\xb8(x|z).\n\n        :param dims: dimensions of the networks\n            given by the number of neurons on the form\n            [latent_dim, [hidden_dims], input_dim].\n        """"""\n        super(Decoder, self).__init__()\n\n        [z_dim, h_dim, x_dim] = dims\n\n        neurons = [z_dim, *h_dim]\n        linear_layers = [nn.Linear(neurons[i-1], neurons[i]) for i in range(1, len(neurons))]\n        self.hidden = nn.ModuleList(linear_layers)\n\n        self.reconstruction = nn.Linear(h_dim[-1], x_dim)\n\n        self.output_activation = nn.Sigmoid()\n\n    def forward(self, x):\n        for layer in self.hidden:\n            x = F.relu(layer(x))\n        return self.output_activation(self.reconstruction(x))\n\n\nclass VariationalAutoencoder(nn.Module):\n    def __init__(self, dims):\n        """"""\n        Variational Autoencoder [Kingma 2013] model\n        consisting of an encoder/decoder pair for which\n        a variational distribution is fitted to the\n        encoder. Also known as the M1 model in [Kingma 2014].\n\n        :param dims: x, z and hidden dimensions of the networks\n        """"""\n        super(VariationalAutoencoder, self).__init__()\n\n        [x_dim, z_dim, h_dim] = dims\n        self.z_dim = z_dim\n        self.flow = None\n\n        self.encoder = Encoder([x_dim, h_dim, z_dim])\n        self.decoder = Decoder([z_dim, list(reversed(h_dim)), x_dim])\n        self.kl_divergence = 0\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.xavier_normal(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    def _kld(self, z, q_param, p_param=None):\n        """"""\n        Computes the KL-divergence of\n        some element z.\n\n        KL(q||p) = -\xe2\x88\xab q(z) log [ p(z) / q(z) ]\n                 = -E[log p(z) - log q(z)]\n\n        :param z: sample from q-distribuion\n        :param q_param: (mu, log_var) of the q-distribution\n        :param p_param: (mu, log_var) of the p-distribution\n        :return: KL(q||p)\n        """"""\n        (mu, log_var) = q_param\n\n        if self.flow is not None:\n            f_z, log_det_z = self.flow(z)\n            qz = log_gaussian(z, mu, log_var) - sum(log_det_z)\n            z = f_z\n        else:\n            qz = log_gaussian(z, mu, log_var)\n\n        if p_param is None:\n            pz = log_standard_gaussian(z)\n        else:\n            (mu, log_var) = p_param\n            pz = log_gaussian(z, mu, log_var)\n\n        kl = qz - pz\n\n        return kl\n\n    def add_flow(self, flow):\n        self.flow = flow\n\n    def forward(self, x, y=None):\n        """"""\n        Runs a data point through the model in order\n        to provide its reconstruction and q distribution\n        parameters.\n\n        :param x: input data\n        :return: reconstructed input\n        """"""\n        z, z_mu, z_log_var = self.encoder(x)\n\n        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n\n        x_mu = self.decoder(z)\n\n        return x_mu\n\n    def sample(self, z):\n        """"""\n        Given z ~ N(0, I) generates a sample from\n        the learned distribution based on p_\xce\xb8(x|z).\n        :param z: (torch.autograd.Variable) Random normal variable\n        :return: (torch.autograd.Variable) generated sample\n        """"""\n        return self.decoder(z)\n\n\nclass GumbelAutoencoder(nn.Module):\n    def __init__(self, dims, n_samples=100):\n        super(GumbelAutoencoder, self).__init__()\n\n        [x_dim, z_dim, h_dim] = dims\n        self.z_dim = z_dim\n        self.n_samples = n_samples\n\n        self.encoder = Perceptron([x_dim, *h_dim])\n        self.sampler = GumbelSoftmax(h_dim[-1], z_dim, n_samples)\n        self.decoder = Perceptron([z_dim, *reversed(h_dim), x_dim], output_activation=F.sigmoid)\n\n        self.kl_divergence = 0\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.xavier_normal(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    def _kld(self, qz):\n        k = Variable(torch.FloatTensor([self.z_dim]), requires_grad=False)\n        kl = qz * (torch.log(qz + 1e-8) - torch.log(1.0/k))\n        kl = kl.view(-1, self.n_samples, self.z_dim)\n        return torch.sum(torch.sum(kl, dim=1), dim=1)\n\n    def forward(self, x, y=None, tau=1):\n        x = self.encoder(x)\n\n        sample, qz = self.sampler(x, tau)\n        self.kl_divergence = self._kld(qz)\n\n        x_mu = self.decoder(sample)\n\n        return x_mu\n\n    def sample(self, z):\n        return self.decoder(z)\n\n\nclass LadderEncoder(nn.Module):\n    def __init__(self, dims):\n        """"""\n        The ladder encoder differs from the standard encoder\n        by using batch-normalization and LReLU activation.\n        Additionally, it also returns the transformation x.\n\n        :param dims: dimensions [input_dim, [hidden_dims], [latent_dims]].\n        """"""\n        super(LadderEncoder, self).__init__()\n        [x_dim, h_dim, self.z_dim] = dims\n        self.in_features = x_dim\n        self.out_features = h_dim\n\n        self.linear = nn.Linear(x_dim, h_dim)\n        self.batchnorm = nn.BatchNorm1d(h_dim)\n        self.sample = GaussianSample(h_dim, self.z_dim)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = F.leaky_relu(self.batchnorm(x), 0.1)\n        return x, self.sample(x)\n\n\nclass LadderDecoder(nn.Module):\n    def __init__(self, dims):\n        """"""\n        The ladder dencoder differs from the standard encoder\n        by using batch-normalization and LReLU activation.\n        Additionally, it also returns the transformation x.\n\n        :param dims: dimensions of the networks\n            given by the number of neurons on the form\n            [latent_dim, [hidden_dims], input_dim].\n        """"""\n        super(LadderDecoder, self).__init__()\n\n        [self.z_dim, h_dim, x_dim] = dims\n\n        self.linear1 = nn.Linear(x_dim, h_dim)\n        self.batchnorm1 = nn.BatchNorm1d(h_dim)\n        self.merge = GaussianMerge(h_dim, self.z_dim)\n\n        self.linear2 = nn.Linear(x_dim, h_dim)\n        self.batchnorm2 = nn.BatchNorm1d(h_dim)\n        self.sample = GaussianSample(h_dim, self.z_dim)\n\n    def forward(self, x, l_mu=None, l_log_var=None):\n        if l_mu is not None:\n            # Sample from this encoder layer and merge\n            z = self.linear1(x)\n            z = F.leaky_relu(self.batchnorm1(z), 0.1)\n            q_z, q_mu, q_log_var = self.merge(z, l_mu, l_log_var)\n\n        # Sample from the decoder and send forward\n        z = self.linear2(x)\n        z = F.leaky_relu(self.batchnorm2(z), 0.1)\n        z, p_mu, p_log_var = self.sample(z)\n\n        if l_mu is None:\n            return z\n\n        return z, (q_z, (q_mu, q_log_var), (p_mu, p_log_var))\n\n\nclass LadderVariationalAutoencoder(VariationalAutoencoder):\n    def __init__(self, dims):\n        """"""\n        Ladder Variational Autoencoder as described by\n        [S\xc3\xb8nderby 2016]. Adds several stochastic\n        layers to improve the log-likelihood estimate.\n\n        :param dims: x, z and hidden dimensions of the networks\n        """"""\n        [x_dim, z_dim, h_dim] = dims\n        super(LadderVariationalAutoencoder, self).__init__([x_dim, z_dim[0], h_dim])\n\n        neurons = [x_dim, *h_dim]\n        encoder_layers = [LadderEncoder([neurons[i - 1], neurons[i], z_dim[i - 1]]) for i in range(1, len(neurons))]\n        decoder_layers = [LadderDecoder([z_dim[i - 1], h_dim[i - 1], z_dim[i]]) for i in range(1, len(h_dim))][::-1]\n\n        self.encoder = nn.ModuleList(encoder_layers)\n        self.decoder = nn.ModuleList(decoder_layers)\n        self.reconstruction = Decoder([z_dim[0], h_dim, x_dim])\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.xavier_normal(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    def forward(self, x):\n        # Gather latent representation\n        # from encoders along with final z.\n        latents = []\n        for encoder in self.encoder:\n            x, (z, mu, log_var) = encoder(x)\n            latents.append((mu, log_var))\n\n        latents = list(reversed(latents))\n\n        self.kl_divergence = 0\n        for i, decoder in enumerate([-1, *self.decoder]):\n            # If at top, encoder == decoder,\n            # use prior for KL.\n            l_mu, l_log_var = latents[i]\n            if i == 0:\n                self.kl_divergence += self._kld(z, (l_mu, l_log_var))\n\n            # Perform downword merge of information.\n            else:\n                z, kl = decoder(z, l_mu, l_log_var)\n                self.kl_divergence += self._kld(*kl)\n\n        x_mu = self.reconstruction(z)\n        return x_mu\n\n    def sample(self, z):\n        for decoder in self.decoder:\n            z = decoder(z)\n        return self.reconstruction(z)'"
