file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nexec(open(\'pytorch3dunet/__version__.py\').read())\nsetup(\n    name=""pytorch3dunet"",\n    packages=find_packages(exclude=[""tests""]),\n    version=__version__,\n    author=""Adrian Wolny, Lorenzo Cerrone"",\n    url=""https://github.com/wolny/pytorch-3dunet"",\n    license=""MIT"",\n    python_requires=\'>=3.7\'\n)\n'"
pytorch3dunet/__init__.py,0,b'from .__version__ import __version__\n'
pytorch3dunet/__version__.py,0,"b""__version__ = '1.2.10'\n"""
pytorch3dunet/predict.py,3,"b'import importlib\nimport os\n\nimport torch\nimport torch.nn as nn\n\nfrom pytorch3dunet.datasets.utils import get_test_loaders\nfrom pytorch3dunet.unet3d import utils\nfrom pytorch3dunet.unet3d.config import load_config\nfrom pytorch3dunet.unet3d.model import get_model\n\nlogger = utils.get_logger(\'UNet3DPredict\')\n\n\ndef _get_output_file(dataset, suffix=\'_predictions\', output_dir=None):\n    input_dir, file_name = os.path.split(dataset.file_path)\n    if output_dir is None:\n        output_dir = input_dir\n    output_file = os.path.join(output_dir, os.path.splitext(file_name)[0] + suffix + \'.h5\')\n    return output_file\n\n\ndef _get_dataset_names(config, number_of_datasets, prefix=\'predictions\'):\n    dataset_names = config.get(\'dest_dataset_name\')\n    if dataset_names is not None:\n        if isinstance(dataset_names, str):\n            return [dataset_names]\n        else:\n            return dataset_names\n    else:\n        if number_of_datasets == 1:\n            return [prefix]\n        else:\n            return [f\'{prefix}{i}\' for i in range(number_of_datasets)]\n\n\ndef _get_predictor(model, loader, output_file, config):\n    predictor_config = config.get(\'predictor\', {})\n    class_name = predictor_config.get(\'name\', \'StandardPredictor\')\n\n    m = importlib.import_module(\'pytorch3dunet.unet3d.predictor\')\n    predictor_class = getattr(m, class_name)\n\n    return predictor_class(model, loader, output_file, config, **predictor_config)\n\n\ndef main():\n    # Load configuration\n    config = load_config()\n\n    # Create the model\n    model = get_model(config)\n\n    # Load model state\n    model_path = config[\'model_path\']\n    logger.info(f\'Loading model from {model_path}...\')\n    utils.load_checkpoint(model_path, model)\n    # use DataParallel if more than 1 GPU available\n    device = config[\'device\']\n    if torch.cuda.device_count() > 1 and not device.type == \'cpu\':\n        model = nn.DataParallel(model)\n        logger.info(f\'Using {torch.cuda.device_count()} GPUs for prediction\')\n\n    logger.info(f""Sending the model to \'{device}\'"")\n    model = model.to(device)\n\n    output_dir = config[\'loaders\'].get(\'output_dir\', None)\n    if output_dir is not None:\n        os.makedirs(output_dir, exist_ok=True)\n        logger.info(f\'Saving predictions to: {output_dir}\')\n\n    for test_loader in get_test_loaders(config):\n        logger.info(f""Processing \'{test_loader.dataset.file_path}\'..."")\n\n        output_file = _get_output_file(dataset=test_loader.dataset, output_dir=output_dir)\n\n        predictor = _get_predictor(model, test_loader, output_file, config)\n        # run the model prediction on the entire dataset and save to the \'output_file\' H5\n        predictor.predict()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch3dunet/train.py,10,"b'import importlib\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom pytorch3dunet.datasets.utils import get_train_loaders\nfrom pytorch3dunet.unet3d.config import load_config\nfrom pytorch3dunet.unet3d.losses import get_loss_criterion\nfrom pytorch3dunet.unet3d.metrics import get_evaluation_metric\nfrom pytorch3dunet.unet3d.model import get_model\nfrom pytorch3dunet.unet3d.trainer import UNet3DTrainer\nfrom pytorch3dunet.unet3d.utils import get_logger, get_tensorboard_formatter\nfrom pytorch3dunet.unet3d.utils import get_number_of_learnable_parameters\n\nlogger = get_logger(\'UNet3DTrain\')\n\n\ndef _create_trainer(config, model, optimizer, lr_scheduler, loss_criterion, eval_criterion, loaders):\n    assert \'trainer\' in config, \'Could not find trainer configuration\'\n    trainer_config = config[\'trainer\']\n\n    resume = trainer_config.get(\'resume\', None)\n    pre_trained = trainer_config.get(\'pre_trained\', None)\n    skip_train_validation = trainer_config.get(\'skip_train_validation\', False)\n\n    # get tensorboard formatter\n    tensorboard_formatter = get_tensorboard_formatter(trainer_config.get(\'tensorboard_formatter\', None))\n\n    if resume is not None:\n        # continue training from a given checkpoint\n        return UNet3DTrainer.from_checkpoint(resume, model,\n                                             optimizer, lr_scheduler, loss_criterion,\n                                             eval_criterion, loaders, tensorboard_formatter=tensorboard_formatter)\n    elif pre_trained is not None:\n        # fine-tune a given pre-trained model\n        return UNet3DTrainer.from_pretrained(pre_trained, model, optimizer, lr_scheduler, loss_criterion,\n                                             eval_criterion, device=config[\'device\'], loaders=loaders,\n                                             max_num_epochs=trainer_config[\'epochs\'],\n                                             max_num_iterations=trainer_config[\'iters\'],\n                                             validate_after_iters=trainer_config[\'validate_after_iters\'],\n                                             log_after_iters=trainer_config[\'log_after_iters\'],\n                                             eval_score_higher_is_better=trainer_config[\'eval_score_higher_is_better\'],\n                                             tensorboard_formatter=tensorboard_formatter,\n                                             skip_train_validation=skip_train_validation)\n    else:\n        # start training from scratch\n        return UNet3DTrainer(model, optimizer, lr_scheduler, loss_criterion, eval_criterion,\n                             config[\'device\'], loaders, trainer_config[\'checkpoint_dir\'],\n                             max_num_epochs=trainer_config[\'epochs\'],\n                             max_num_iterations=trainer_config[\'iters\'],\n                             validate_after_iters=trainer_config[\'validate_after_iters\'],\n                             log_after_iters=trainer_config[\'log_after_iters\'],\n                             eval_score_higher_is_better=trainer_config[\'eval_score_higher_is_better\'],\n                             tensorboard_formatter=tensorboard_formatter,\n                             skip_train_validation=skip_train_validation)\n\n\ndef _create_optimizer(config, model):\n    assert \'optimizer\' in config, \'Cannot find optimizer configuration\'\n    optimizer_config = config[\'optimizer\']\n    learning_rate = optimizer_config[\'learning_rate\']\n    weight_decay = optimizer_config[\'weight_decay\']\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    return optimizer\n\n\ndef _create_lr_scheduler(config, optimizer):\n    lr_config = config.get(\'lr_scheduler\', None)\n    if lr_config is None:\n        # use ReduceLROnPlateau as a default scheduler\n        return ReduceLROnPlateau(optimizer, mode=\'max\', factor=0.5, patience=20, verbose=True)\n    else:\n        class_name = lr_config.pop(\'name\')\n        m = importlib.import_module(\'torch.optim.lr_scheduler\')\n        clazz = getattr(m, class_name)\n        # add optimizer to the config\n        lr_config[\'optimizer\'] = optimizer\n        return clazz(**lr_config)\n\n\ndef main():\n    # Load and log experiment configuration\n    config = load_config()\n    logger.info(config)\n\n    manual_seed = config.get(\'manual_seed\', None)\n    if manual_seed is not None:\n        logger.info(f\'Seed the RNG for all devices with {manual_seed}\')\n        torch.manual_seed(manual_seed)\n        # see https://pytorch.org/docs/stable/notes/randomness.html\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    # Create the model\n    model = get_model(config)\n    # use DataParallel if more than 1 GPU available\n    device = config[\'device\']\n    if torch.cuda.device_count() > 1 and not device.type == \'cpu\':\n        model = nn.DataParallel(model)\n        logger.info(f\'Using {torch.cuda.device_count()} GPUs for training\')\n\n    # put the model on GPUs\n    logger.info(f""Sending the model to \'{config[\'device\']}\'"")\n    model = model.to(device)\n\n    # Log the number of learnable parameters\n    logger.info(f\'Number of learnable params {get_number_of_learnable_parameters(model)}\')\n\n    # Create loss criterion\n    loss_criterion = get_loss_criterion(config)\n    # Create evaluation metric\n    eval_criterion = get_evaluation_metric(config)\n\n    # Create data loaders\n    loaders = get_train_loaders(config)\n\n    # Create the optimizer\n    optimizer = _create_optimizer(config, model)\n\n    # Create learning rate adjustment strategy\n    lr_scheduler = _create_lr_scheduler(config, optimizer)\n\n    # Create model trainer\n    trainer = _create_trainer(config, model=model, optimizer=optimizer, lr_scheduler=lr_scheduler,\n                              loss_criterion=loss_criterion, eval_criterion=eval_criterion, loaders=loaders)\n    # Start training\n    trainer.fit()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tests/__init__.py,0,b''
tests/conftest.py,0,"b""import os\n\nimport h5py\nimport numpy as np\nimport pytest\nimport yaml\n\nTEST_FILES = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)),\n    'resources',\n)\n\n\n@pytest.fixture\ndef ovule_label():\n    path = os.path.join(TEST_FILES, 'sample_ovule.h5')\n    with h5py.File(path, 'r') as f:\n        return f['label'][...]\n\n\n@pytest.fixture\ndef transformer_config():\n    config_path = os.path.join(TEST_FILES, 'transformer_config.yml')\n    return yaml.load(open(config_path, 'r'))\n\n\n@pytest.fixture\ndef train_config():\n    config_path = os.path.join(TEST_FILES, 'config_train.yml')\n    return yaml.load(open(config_path, 'r'))\n\n@pytest.fixture\ndef test_config():\n    config_path = os.path.join(TEST_FILES, 'config_test.yml')\n    return yaml.load(open(config_path, 'r'))\n\n@pytest.fixture\ndef train_config_2d():\n    config_path = os.path.join(TEST_FILES, 'config_train_2d.yml')\n    return yaml.load(open(config_path, 'r'))\n\n\n@pytest.fixture\ndef random_input(tmpdir):\n    shape = (32, 128, 128)\n    return _create_random_input(tmpdir, shape, min_label=0)\n\n\n@pytest.fixture\ndef random_input_with_ignore(tmpdir):\n    shape = (32, 128, 128)\n    return _create_random_input(tmpdir, shape, min_label=-1)\n\n\ndef _create_random_input(tmpdir, shape, min_label):\n    path = os.path.join(tmpdir, 'test.h5')\n    with h5py.File(path, 'w') as f:\n        f.create_dataset('raw', data=np.random.rand(*shape))\n        f.create_dataset('label', data=np.random.randint(min_label, 2, shape))\n    return path\n"""
tests/test_criterion.py,35,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom skimage import measure\n\nfrom pytorch3dunet.augment.transforms import LabelToAffinities, StandardLabelToBoundary\nfrom pytorch3dunet.embeddings.contrastive_loss import ContrastiveLoss\nfrom pytorch3dunet.unet3d.losses import GeneralizedDiceLoss, WeightedCrossEntropyLoss, \\\n    DiceLoss, TagsAngularLoss, WeightedSmoothL1Loss, _MaskingLossWrapper, SkipLastTargetChannelWrapper, BCEDiceLoss\nfrom pytorch3dunet.unet3d.metrics import DiceCoefficient, MeanIoU, BoundaryAveragePrecision, AdaptedRandError, \\\n    BoundaryAdaptedRandError, EmbeddingsAdaptedRandError\n\n\ndef _compute_criterion(criterion, n_times=100):\n    shape = [1, 0, 30, 30, 30]\n    # channel size varies between 1 and 4\n    results = []\n    for C in range(1, 5):\n        batch_shape = list(shape)\n        batch_shape[1] = C\n        batch_shape = tuple(batch_shape)\n        results.append(_eval_criterion(criterion, batch_shape, n_times))\n\n    return results\n\n\ndef _eval_criterion(criterion, batch_shape, n_times=100):\n    results = []\n    # compute criterion n_times\n    for i in range(n_times):\n        input = torch.rand(batch_shape, requires_grad=True)\n        target = torch.zeros(batch_shape).random_(0, 2)\n        output = criterion(input, target)\n        results.append(output)\n\n    return results\n\n\nclass TestCriterion:\n    def test_dice_coefficient(self):\n        results = _compute_criterion(DiceCoefficient())\n        # check that all of the coefficients belong to [0, 1]\n        results = np.array(results)\n        assert np.all(results > 0)\n        assert np.all(results < 1)\n\n    def test_mean_iou_simple(self):\n        results = _compute_criterion(MeanIoU())\n        # check that all of the coefficients belong to [0, 1]\n        results = np.array(results)\n        assert np.all(results > 0)\n        assert np.all(results < 1)\n\n    def test_mean_iou(self):\n        criterion = MeanIoU()\n        x = torch.randn(3, 3, 3, 3)\n        _, index = torch.max(x, dim=0, keepdim=True)\n        # create target tensor\n        target = torch.zeros_like(x, dtype=torch.long).scatter_(0, index, 1)\n        pred = torch.zeros_like(target, dtype=torch.float)\n        mask = target == 1\n        # create prediction tensor\n        pred[mask] = torch.rand(1)\n        # make sure the dimensions are right\n        target = torch.unsqueeze(target, dim=0)\n        pred = torch.unsqueeze(pred, dim=0)\n        assert criterion(pred, target) == 1\n\n    def test_mean_iou_one_channel(self):\n        criterion = MeanIoU()\n        pred = torch.rand(1, 1, 3, 3, 3)\n        target = pred > 0.5\n        target = target.long()\n        assert criterion(pred, target) == 1\n\n    def test_average_precision_synthethic_data(self):\n        input = np.zeros((64, 200, 200), dtype=np.int)\n        for i in range(40, 200, 40):\n            input[:, :, i:i + 2] = 1\n        for i in range(40, 200, 40):\n            input[:, i:i + 2, :] = 1\n        for i in range(40, 64, 40):\n            input[i:i + 2, :, :] = 1\n\n        target = measure.label(np.logical_not(input).astype(np.int), background=0)\n        input = torch.tensor(input.reshape((1, 1) + input.shape))\n        target = torch.tensor(target.reshape((1, 1) + target.shape))\n        ap = BoundaryAveragePrecision()\n        # expect perfect AP\n        assert ap(input, target) == 1.0\n\n    def test_average_precision_real_data(self, ovule_label):\n        label = ovule_label[64:128, 64:128, 64:128]\n        ltb = LabelToAffinities((1, 2, 4, 6), aggregate_affinities=True)\n        pred = ltb(label)\n        label = torch.tensor(label.reshape((1, 1) + label.shape).astype('int64'))\n        pred = torch.tensor(np.expand_dims(pred, 0))\n        ap = BoundaryAveragePrecision()\n        assert ap(pred, label) > 0.5\n\n    def test_adapted_rand_error(self, ovule_label):\n        label = ovule_label[64:128, 64:128, 64:128].astype('int64')\n        input = torch.tensor(label.reshape((1, 1) + label.shape))\n        label = torch.tensor(label.reshape((1, 1) + label.shape))\n        arand = AdaptedRandError()\n        assert arand(input, label) == 0\n\n    def test_adapted_rand_error_on_real_data(self, ovule_label):\n        label = ovule_label[64:128, 64:128, 64:128].astype('int64')\n        ltb = StandardLabelToBoundary()\n        pred = ltb(label)\n        label = torch.tensor(label.reshape((1, 1) + label.shape))\n        pred = torch.tensor(np.expand_dims(pred, 0))\n        arand = BoundaryAdaptedRandError(use_last_target=True)\n        assert arand(pred, label) < 0.2\n\n    def test_adapted_rand_from_embeddings(self, ovule_label):\n        label = ovule_label[64:128, 64:128, 64:128].astype('int64')\n        pred = np.random.rand(*label.shape).reshape((1, 1) + label.shape)\n        label = torch.tensor(label.reshape((1, 1) + label.shape))\n        pred = torch.tensor(pred)\n        arand = EmbeddingsAdaptedRandError(min_cluster_size=50)\n        assert arand(pred, label) <= 1.0\n\n    def test_generalized_dice_loss(self):\n        results = _compute_criterion(GeneralizedDiceLoss())\n        # check that all of the coefficients belong to [0, 1]\n        results = np.array(results)\n        assert np.all(results > 0)\n        assert np.all(results < 1)\n\n    def test_dice_loss(self):\n        results = _compute_criterion(DiceLoss())\n        # check that all of the coefficients belong to [0, 1]\n        results = np.array(results)\n        assert np.all(results > 0)\n        assert np.all(results < 1)\n\n    def test_bce_dice_loss(self):\n        results = _compute_criterion(BCEDiceLoss(1., 1.))\n        results = np.array(results)\n        assert np.all(results > 0)\n\n    def test_weighted_ce(self):\n        criterion = WeightedCrossEntropyLoss()\n        shape = [1, 0, 30, 30, 30]\n        target_shape = [1, 30, 30, 30]\n        results = []\n        for C in range(1, 5):\n            input_shape = list(shape)\n            input_shape[1] = C\n            input_shape = tuple(input_shape)\n            for i in range(100):\n                input = torch.rand(input_shape, requires_grad=True)\n                target = torch.zeros(target_shape, dtype=torch.long).random_(0, C)\n                output = criterion(input, target)\n                output.backward()\n                results.append(output)\n\n        results = np.array(results)\n        assert np.all(results >= 0)\n\n    def test_ignore_index_loss(self):\n        loss = _MaskingLossWrapper(nn.BCEWithLogitsLoss(), ignore_index=-1)\n        input = torch.rand((3, 3))\n        input[1, 1] = 1.\n        input.requires_grad = True\n        target = -1. * torch.ones((3, 3))\n        target[1, 1] = 1.\n        output = loss(input, target)\n        output.backward()\n\n    def test_skip_last_target_channel(self):\n        loss = SkipLastTargetChannelWrapper(nn.BCEWithLogitsLoss())\n        input = torch.rand(1, 1, 3, 3, 3, requires_grad=True)\n        target = torch.empty(1, 2, 3, 3, 3).random_(2)\n        output = loss(input, target)\n        output.backward()\n        assert output.item() > 0\n\n    def test_tags_angular_loss(self):\n        coeff = [1.0, 0.8, 0.5]\n        loss_criterion = TagsAngularLoss(coeff)\n        inputs = [torch.rand((1, 3, 4, 4, 4)) for _ in range(len(coeff))]\n        inputs = [i / torch.norm(i, p=2, dim=1).clamp(min=1e-8) for i in inputs]\n        targets = [torch.rand((1, 3, 4, 4, 4)) for _ in range(len(coeff))]\n        targets = [i / torch.norm(i, p=2, dim=1).clamp(min=1e-8) for i in targets]\n\n        loss = loss_criterion(inputs, targets, None)\n        assert loss > 0\n\n    def test_contrastive_loss(self):\n        loss_criterion = ContrastiveLoss(0.5, 1.5)\n        C = 10\n        input = torch.randn(3, 16, 64, 64, 64, requires_grad=True)\n        target = torch.randint(C, (3, 64, 64, 64))\n\n        loss = loss_criterion(input, target)\n        loss.backward()\n        assert loss > 0\n\n    def test_weighted_smooth_l1loss(self):\n        loss_criterion = WeightedSmoothL1Loss(threshold=0., initial_weight=0.1)\n        input = torch.randn(3, 16, 64, 64, 64, requires_grad=True)\n        target = torch.randn(3, 16, 64, 64, 64)\n        loss = loss_criterion(input, target)\n        loss.backward()\n        assert loss > 0\n"""
tests/test_dataset.py,1,"b""import os\nfrom tempfile import NamedTemporaryFile\n\nimport h5py\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\nfrom pytorch3dunet.datasets.hdf5 import StandardHDF5Dataset, AbstractHDF5Dataset\n\n\nclass TestHDF5Dataset:\n    def test_hdf5_dataset(self, transformer_config):\n        path = create_random_dataset((128, 128, 128))\n\n        patch_shapes = [(127, 127, 127), (69, 70, 70), (32, 64, 64)]\n        stride_shapes = [(1, 1, 1), (17, 23, 23), (32, 64, 64)]\n\n        phase = 'test'\n\n        for patch_shape, stride_shape in zip(patch_shapes, stride_shapes):\n            with h5py.File(path, 'r') as f:\n                raw = f['raw'][...]\n                label = f['label'][...]\n\n                dataset = StandardHDF5Dataset(path, phase=phase,\n                                              slice_builder_config=_slice_builder_conf(patch_shape, stride_shape),\n                                              transformer_config=transformer_config[phase]['transformer'],\n                                              mirror_padding=None,\n                                              raw_internal_path='raw',\n                                              label_internal_path='label')\n\n                # create zero-arrays of the same shape as the original dataset in order to verify if every element\n                # was visited during the iteration\n                visit_raw = np.zeros_like(raw)\n                visit_label = np.zeros_like(label)\n\n                for (_, idx) in dataset:\n                    visit_raw[idx] = 1\n                    visit_label[idx] = 1\n\n                # verify that every element was visited at least once\n                assert np.all(visit_raw)\n                assert np.all(visit_label)\n\n    def test_hdf5_with_multiple_label_datasets(self, transformer_config):\n        path = create_random_dataset((128, 128, 128), label_datasets=['label1', 'label2'])\n        patch_shape = (32, 64, 64)\n        stride_shape = (32, 64, 64)\n        phase = 'train'\n        dataset = StandardHDF5Dataset(path, phase=phase,\n                                      slice_builder_config=_slice_builder_conf(patch_shape, stride_shape),\n                                      transformer_config=transformer_config[phase]['transformer'],\n                                      raw_internal_path='raw',\n                                      label_internal_path=['label1', 'label2'])\n\n        for raw, labels in dataset:\n            assert len(labels) == 2\n\n    def test_hdf5_with_multiple_raw_and_label_datasets(self, transformer_config):\n        path = create_random_dataset((128, 128, 128), raw_datasets=['raw1', 'raw2'],\n                                     label_datasets=['label1', 'label2'])\n        patch_shape = (32, 64, 64)\n        stride_shape = (32, 64, 64)\n        phase = 'train'\n        dataset = StandardHDF5Dataset(path, phase=phase,\n                                      slice_builder_config=_slice_builder_conf(patch_shape, stride_shape),\n                                      transformer_config=transformer_config[phase]['transformer'],\n                                      raw_internal_path=['raw1', 'raw2'], label_internal_path=['label1', 'label2'])\n\n        for raws, labels in dataset:\n            assert len(raws) == 2\n            assert len(labels) == 2\n\n    def test_augmentation(self, transformer_config):\n        raw = np.random.rand(32, 96, 96)\n        # assign raw to label's channels for ease of comparison\n        label = np.stack(raw for _ in range(3))\n        # create temporary h5 file\n        tmp_file = NamedTemporaryFile()\n        tmp_path = tmp_file.name\n        with h5py.File(tmp_path, 'w') as f:\n            f.create_dataset('raw', data=raw)\n            f.create_dataset('label', data=label)\n\n        # set phase='train' in order to execute the train transformers\n        phase = 'train'\n        dataset = StandardHDF5Dataset(tmp_path, phase=phase,\n                                      slice_builder_config=_slice_builder_conf((16, 64, 64), (8, 32, 32)),\n                                      transformer_config=transformer_config[phase]['transformer'])\n\n        # test augmentations using DataLoader with 4 worker threads\n        data_loader = DataLoader(dataset, batch_size=1, num_workers=4, shuffle=True)\n        for (img, label) in data_loader:\n            for i in range(label.shape[0]):\n                assert np.allclose(img, label[i])\n\n    def test_traverse_file_paths(self, tmpdir):\n        test_tmp_dir = os.path.join(tmpdir, 'test')\n        os.mkdir(test_tmp_dir)\n\n        expected_files = [\n            os.path.join(tmpdir, 'f1.h5'),\n            os.path.join(test_tmp_dir, 'f2.h5'),\n            os.path.join(test_tmp_dir, 'f3.hdf'),\n            os.path.join(test_tmp_dir, 'f4.hdf5'),\n            os.path.join(test_tmp_dir, 'f5.hd5')\n        ]\n        # create expected files\n        for ef in expected_files:\n            with h5py.File(ef, 'w') as f:\n                f.create_dataset('raw', data=np.random.randn(4, 4, 4))\n\n        # make sure that traverse_file_paths runs correctly\n        file_paths = [os.path.join(tmpdir, 'f1.h5'), test_tmp_dir]\n        actual_files = AbstractHDF5Dataset.traverse_h5_paths(file_paths)\n\n        assert expected_files == actual_files\n\n\ndef create_random_dataset(shape, ignore_index=False, raw_datasets=None, label_datasets=None):\n    if label_datasets is None:\n        label_datasets = ['label']\n    if raw_datasets is None:\n        raw_datasets = ['raw']\n\n    tmp_file = NamedTemporaryFile(delete=False)\n\n    with h5py.File(tmp_file.name, 'w') as f:\n        for raw_dataset in raw_datasets:\n            f.create_dataset(raw_dataset, data=np.random.rand(*shape))\n\n        for label_dataset in label_datasets:\n            if ignore_index:\n                f.create_dataset(label_dataset, data=np.random.randint(-1, 2, shape))\n            else:\n                f.create_dataset(label_dataset, data=np.random.randint(0, 2, shape))\n\n    return tmp_file.name\n\n\ndef _slice_builder_conf(patch_shape, stride_shape):\n    return {\n        'name': 'SliceBuilder',\n        'patch_shape': patch_shape,\n        'stride_shape': stride_shape\n    }\n"""
tests/test_predictor.py,3,"b'import os\nfrom tempfile import NamedTemporaryFile\n\nimport h5py\nimport numpy as np\nimport torch\nfrom skimage.metrics import adapted_rand_error\nfrom torch.utils.data import DataLoader\n\nfrom pytorch3dunet.datasets.hdf5 import StandardHDF5Dataset\nfrom pytorch3dunet.datasets.utils import prediction_collate, get_test_loaders\nfrom pytorch3dunet.predict import _get_output_file, _get_predictor\nfrom pytorch3dunet.unet3d.model import get_model\nfrom pytorch3dunet.unet3d.predictor import EmbeddingsPredictor\nfrom pytorch3dunet.unet3d.utils import remove_halo\n\n\nclass FakePredictor(EmbeddingsPredictor):\n    def __init__(self, model, loader, output_file, config, clustering, iou_threshold=0.7, **kwargs):\n        super().__init__(model, loader, output_file, config, clustering, iou_threshold=iou_threshold, **kwargs)\n\n    def _embeddings_to_segmentation(self, embeddings):\n        return embeddings\n\n\nclass FakeModel:\n    def __call__(self, input):\n        return input\n\n    def eval(self):\n        pass\n\n\nclass TestPredictor:\n    def test_stanard_predictor(self, tmpdir, test_config):\n        # Add output dir\n        test_config[\'loaders\'][\'output_dir\'] = tmpdir\n\n        # create random dataset\n        tmp = NamedTemporaryFile(delete=False)\n\n        with h5py.File(tmp.name, \'w\') as f:\n            shape = (32, 64, 64)\n            f.create_dataset(\'raw\', data=np.random.rand(*shape))\n\n        # Add input file\n        test_config[\'loaders\'][\'test\'][\'file_paths\'] = [tmp.name]\n\n        # Create the model with random weights\n        model = get_model(test_config)\n        # Create device and update config\n        device = torch.device(""cuda:0"" if torch.cuda.is_available() else \'cpu\')\n        test_config[\'device\'] = device\n        model = model.to(device)\n\n        for test_loader in get_test_loaders(test_config):\n\n            output_file = _get_output_file(dataset=test_loader.dataset, output_dir=tmpdir)\n\n            predictor = _get_predictor(model, test_loader, output_file, test_config)\n            # run the model prediction on the entire dataset and save to the \'output_file\' H5\n            predictor.predict()\n\n\n\n\n    def test_embeddings_predictor(self, tmpdir):\n        config = {\n            \'model\': {\'output_heads\': 1},\n            \'device\': torch.device(\'cpu\')\n        }\n\n        slice_builder_config = {\n            \'name\': \'SliceBuilder\',\n            \'patch_shape\': (64, 200, 200),\n            \'stride_shape\': (40, 150, 150)\n        }\n\n        transformer_config = {\n            \'raw\': [\n                {\'name\': \'ToTensor\', \'expand_dims\': False, \'dtype\': \'long\'}\n            ]\n        }\n\n        gt_file = \'resources/sample_ovule.h5\'\n        output_file = os.path.join(tmpdir, \'output_segmentation.h5\')\n\n        dataset = StandardHDF5Dataset(gt_file, phase=\'test\',\n                                      slice_builder_config=slice_builder_config,\n                                      transformer_config=transformer_config,\n                                      mirror_padding=None,\n                                      raw_internal_path=\'label\')\n\n        loader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False, collate_fn=prediction_collate)\n\n        predictor = FakePredictor(FakeModel(), loader, output_file, config, clustering=\'meanshift\', bandwidth=0.5)\n\n        predictor.predict()\n\n        with h5py.File(gt_file, \'r\') as f:\n            with h5py.File(output_file, \'r\') as g:\n                gt = f[\'label\'][...]\n                segm = g[\'segmentation/meanshift\'][...]\n                arand_error = adapted_rand_error(gt, segm)[0]\n\n                assert arand_error < 0.1\n\n    def test_remove_halo(self):\n        patch_halo = (4, 4, 4)\n        shape = (128, 128, 128)\n        input = np.random.randint(0, 10, size=(1, 16, 16, 16))\n\n        index = (slice(0, 1), slice(12, 28), slice(16, 32), slice(16, 32))\n        u_patch, u_index = remove_halo(input, index, shape, patch_halo)\n\n        assert np.array_equal(input[:, 4:12, 4:12, 4:12], u_patch)\n        assert u_index == (slice(0, 1), slice(16, 24), slice(20, 28), slice(20, 28))\n\n        index = (slice(0, 1), slice(112, 128), slice(112, 128), slice(112, 128))\n        u_patch, u_index = remove_halo(input, index, shape, patch_halo)\n\n        assert np.array_equal(input[:, 4:16, 4:16, 4:16], u_patch)\n        assert u_index == (slice(0, 1), slice(116, 128), slice(116, 128), slice(116, 128))\n'"
tests/test_trainer.py,1,"b'import os\nfrom tempfile import NamedTemporaryFile\n\nimport h5py\nimport numpy as np\nimport torch\n\nfrom pytorch3dunet.datasets.utils import get_train_loaders\nfrom pytorch3dunet.train import _create_optimizer, _create_lr_scheduler\nfrom pytorch3dunet.unet3d.losses import get_loss_criterion\nfrom pytorch3dunet.unet3d.metrics import get_evaluation_metric\nfrom pytorch3dunet.unet3d.model import get_model\nfrom pytorch3dunet.unet3d.trainer import UNet3DTrainer\nfrom pytorch3dunet.unet3d.utils import DefaultTensorboardFormatter\n\n\nclass TestUNet3DTrainer:\n    def test_ce_loss(self, tmpdir, capsys, train_config):\n        with capsys.disabled():\n            assert_train_save_load(tmpdir, train_config, \'CrossEntropyLoss\', \'MeanIoU\', \'UNet3D\')\n\n    def test_wce_loss(self, tmpdir, capsys, train_config):\n        with capsys.disabled():\n            assert_train_save_load(tmpdir, train_config, \'WeightedCrossEntropyLoss\', \'MeanIoU\', \'UNet3D\')\n\n    def test_bce_loss(self, tmpdir, capsys, train_config):\n        with capsys.disabled():\n            assert_train_save_load(tmpdir, train_config, \'BCEWithLogitsLoss\', \'DiceCoefficient\', \'UNet3D\')\n\n    def test_dice_loss(self, tmpdir, capsys, train_config):\n        with capsys.disabled():\n            assert_train_save_load(tmpdir, train_config, \'DiceLoss\', \'MeanIoU\', \'UNet3D\')\n\n    def test_pce_loss(self, tmpdir, capsys, train_config):\n        with capsys.disabled():\n            assert_train_save_load(tmpdir, train_config, \'PixelWiseCrossEntropyLoss\', \'MeanIoU\', \'UNet3D\',\n                                   weight_map=True)\n\n    def test_residual_unet(self, tmpdir, capsys, train_config):\n        with capsys.disabled():\n            assert_train_save_load(tmpdir, train_config, \'CrossEntropyLoss\', \'MeanIoU\', \'ResidualUNet3D\')\n\n    def test_2d_unet(self, tmpdir, capsys, train_config_2d):\n        with capsys.disabled():\n            assert_train_save_load(tmpdir, train_config_2d, \'CrossEntropyLoss\', \'MeanIoU\', \'UNet2D\',\n                                   shape=(3, 1, 128, 128))\n\n\ndef assert_train_save_load(tmpdir, train_config, loss, val_metric, model, weight_map=False, shape=(3, 64, 64, 64)):\n    max_num_epochs = train_config[\'trainer\'][\'epochs\']\n    log_after_iters = train_config[\'trainer\'][\'log_after_iters\']\n    validate_after_iters = train_config[\'trainer\'][\'validate_after_iters\']\n    max_num_iterations = train_config[\'trainer\'][\'iters\']\n\n    trainer = _train_save_load(tmpdir, train_config, loss, val_metric, model, weight_map, shape)\n\n    assert trainer.num_iterations == max_num_iterations\n    assert trainer.max_num_epochs == max_num_epochs\n    assert trainer.log_after_iters == log_after_iters\n    assert trainer.validate_after_iters == validate_after_iters\n    assert trainer.max_num_iterations == max_num_iterations\n\n\ndef _train_save_load(tmpdir, train_config, loss, val_metric, model, weight_map, shape):\n    binary_loss = loss in [\'BCEWithLogitsLoss\', \'DiceLoss\', \'BCEDiceLoss\', \'GeneralizedDiceLoss\']\n\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else \'cpu\')\n\n    train_config[\'model\'][\'name\'] = model\n    train_config.update({\n        # get device to train on\n        \'device\': device,\n        \'loss\': {\'name\': loss, \'weight\': np.random.rand(2).astype(np.float32), \'pos_weight\': 3.},\n        \'eval_metric\': {\'name\': val_metric}\n    })\n    train_config[\'model\'][\'final_sigmoid\'] = binary_loss\n\n    if weight_map:\n        train_config[\'loaders\'][\'weight_internal_path\'] = \'weight_map\'\n\n    loss_criterion = get_loss_criterion(train_config)\n    eval_criterion = get_evaluation_metric(train_config)\n    model = get_model(train_config)\n    model = model.to(device)\n\n    if loss in [\'BCEWithLogitsLoss\']:\n        label_dtype = \'float32\'\n        train_config[\'loaders\'][\'train\'][\'transformer\'][\'label\'][0][\'dtype\'] = label_dtype\n        train_config[\'loaders\'][\'val\'][\'transformer\'][\'label\'][0][\'dtype\'] = label_dtype\n\n    train = _create_random_dataset(shape, binary_loss)\n    val = _create_random_dataset(shape, binary_loss)\n    train_config[\'loaders\'][\'train\'][\'file_paths\'] = [train]\n    train_config[\'loaders\'][\'val\'][\'file_paths\'] = [val]\n\n    loaders = get_train_loaders(train_config)\n\n    optimizer = _create_optimizer(train_config, model)\n    lr_scheduler = _create_lr_scheduler(train_config, optimizer)\n\n    formatter = DefaultTensorboardFormatter()\n    trainer = UNet3DTrainer(model, optimizer, lr_scheduler,\n                            loss_criterion, eval_criterion,\n                            device, loaders, tmpdir,\n                            max_num_epochs=train_config[\'trainer\'][\'epochs\'],\n                            log_after_iters=train_config[\'trainer\'][\'log_after_iters\'],\n                            validate_after_iters=train_config[\'trainer\'][\'log_after_iters\'],\n                            max_num_iterations=train_config[\'trainer\'][\'iters\'],\n                            tensorboard_formatter=formatter)\n    trainer.fit()\n    # test loading the trainer from the checkpoint\n    trainer = UNet3DTrainer.from_checkpoint(os.path.join(tmpdir, \'last_checkpoint.pytorch\'),\n                                            model, optimizer, lr_scheduler,\n                                            loss_criterion, eval_criterion,\n                                            loaders, tensorboard_formatter=formatter)\n    return trainer\n\n\ndef _create_random_dataset(shape, channel_per_class):\n    tmp = NamedTemporaryFile(delete=False)\n\n    with h5py.File(tmp.name, \'w\') as f:\n        l_shape = w_shape = shape\n        # make sure that label and weight tensors are 3D\n        if len(shape) == 4:\n            l_shape = shape[1:]\n            w_shape = shape[1:]\n\n        if channel_per_class:\n            l_shape = (2,) + l_shape\n\n        f.create_dataset(\'raw\', data=np.random.rand(*shape))\n        f.create_dataset(\'label\', data=np.random.randint(0, 2, l_shape))\n        f.create_dataset(\'weight_map\', data=np.random.rand(*w_shape))\n\n    return tmp.name\n'"
tests/test_transforms.py,0,"b""import numpy as np\n\nfrom pytorch3dunet.augment.transforms import RandomLabelToAffinities, LabelToAffinities, Transformer, Relabel, \\\n    CropToFixed\n\n\nclass TestTransforms:\n    config = {'dtype': 'long'}\n\n    def test_random_label_to_boundary(self):\n        size = 20\n        label = _diagonal_label_volume(size)\n\n        transform = RandomLabelToAffinities(np.random.RandomState())\n        result = transform(label)\n        assert result.shape == (1,) + label.shape\n\n    def test_random_label_to_boundary_with_ignore(self):\n        size = 20\n        label = _diagonal_label_volume(size, init=-1)\n\n        transform = RandomLabelToAffinities(np.random.RandomState(), ignore_index=-1)\n        result = transform(label)\n        assert result.shape == (1,) + label.shape\n        assert -1 in np.unique(result)\n\n    def test_label_to_boundary(self):\n        size = 20\n        label = _diagonal_label_volume(size)\n\n        # this transform will produce 2 channels\n        transform = LabelToAffinities(offsets=(2, 4), aggregate_affinities=True)\n        result = transform(label)\n        assert result.shape == (2,) + label.shape\n        assert np.array_equal(np.unique(result), [0, 1])\n\n    def test_label_to_boundary_with_ignore(self):\n        size = 20\n        label = _diagonal_label_volume(size, init=-1)\n\n        transform = LabelToAffinities(offsets=(2, 4), ignore_index=-1, aggregate_affinities=True)\n        result = transform(label)\n        assert result.shape == (2,) + label.shape\n        assert np.array_equal(np.unique(result), [-1, 0, 1])\n\n    def test_label_to_boundary_no_aggregate(self):\n        size = 20\n        label = _diagonal_label_volume(size)\n\n        # this transform will produce 6 channels\n        transform = LabelToAffinities(offsets=(2, 4), aggregate_affinities=False)\n        result = transform(label)\n        assert result.shape == (6,) + label.shape\n        assert np.array_equal(np.unique(result), [0, 1])\n\n    def test_relabel(self):\n        label = np.array([[10, 10, 10], [0, 0, 0], [5, 5, 5]])\n        r = Relabel()\n        result = r(label)\n        assert np.array_equal(result, np.array([[2, 2, 2], [0, 0, 0], [1, 1, 1]]))\n\n    def test_BaseTransformer(self):\n        config = {\n            'raw': [{'name': 'Standardize'}, {'name': 'ToTensor', 'expand_dims': True}],\n            'label': [{'name': 'ToTensor', 'expand_dims': False, 'dtype': 'long'}],\n            'weight': [{'name': 'ToTensor', 'expand_dims': False}]\n        }\n        base_config = {'mean': 0, 'std': 1}\n        transformer = Transformer(config, base_config)\n        raw_transforms = transformer.raw_transform().transforms\n        assert raw_transforms[0].mean == 0\n        assert raw_transforms[0].std == 1\n        assert raw_transforms[1].expand_dims\n        label_transforms = transformer.label_transform().transforms\n        assert not label_transforms[0].expand_dims\n        assert label_transforms[0].dtype == 'long'\n        weight_transforms = transformer.weight_transform().transforms\n        assert not weight_transforms[0].expand_dims\n\n    def test_StandardTransformer(self):\n        config = {\n            'raw': [\n                {'name': 'Standardize'},\n                {'name': 'RandomContrast', 'execution_probability': 0.5},\n                {'name': 'RandomFlip'},\n                {'name': 'RandomRotate90'},\n                {'name': 'ToTensor', 'expand_dims': True}\n            ],\n            'label': [\n                {'name': 'RandomFlip'},\n                {'name': 'RandomRotate90'},\n                {'name': 'ToTensor', 'expand_dims': False, 'dtype': 'long'}\n            ]\n        }\n        base_config = {'mean': 0, 'std': 1}\n        transformer = Transformer(config, base_config)\n        raw_transforms = transformer.raw_transform().transforms\n        assert raw_transforms[0].mean == 0\n        assert raw_transforms[0].std == 1\n        assert raw_transforms[1].execution_probability == 0.5\n        assert raw_transforms[4].expand_dims\n        label_transforms = transformer.label_transform().transforms\n        assert len(label_transforms) == 3\n\n    def test_AnisotropicRotationTransformer(self):\n        config = {\n            'raw': [\n                {'name': 'Standardize'},\n                {'name': 'RandomContrast', 'execution_probability': 0.5},\n                {'name': 'RandomFlip'},\n                {'name': 'RandomRotate90'},\n                {'name': 'RandomRotate', 'angle_spectrum': 17, 'axes': [[2, 1]]},\n                {'name': 'ToTensor', 'expand_dims': True}\n            ],\n            'label': [\n                {'name': 'RandomFlip'},\n                {'name': 'RandomRotate90'},\n                {'name': 'RandomRotate', 'angle_spectrum': 17, 'axes': [[2, 1]]},\n                {'name': 'ToTensor', 'expand_dims': False, 'dtype': 'long'}\n            ]\n        }\n        base_config = {'mean': 0, 'std': 1}\n        transformer = Transformer(config, base_config)\n        raw_transforms = transformer.raw_transform().transforms\n        assert raw_transforms[0].mean == 0\n        assert raw_transforms[0].std == 1\n        assert raw_transforms[1].execution_probability == 0.5\n        assert raw_transforms[4].angle_spectrum == 17\n        assert raw_transforms[4].axes == [[2, 1]]\n        label_transforms = transformer.label_transform().transforms\n        assert len(label_transforms) == 4\n\n    def test_LabelToBoundaryTransformer(self):\n        config = {\n            'raw': [\n                {'name': 'Standardize'},\n                {'name': 'RandomContrast', 'execution_probability': 0.5},\n                {'name': 'RandomFlip'},\n                {'name': 'RandomRotate90'},\n                {'name': 'RandomRotate', 'angle_spectrum': 17, 'axes': [[2, 1]], 'mode': 'reflect'},\n                {'name': 'ToTensor', 'expand_dims': True}\n            ],\n            'label': [\n                {'name': 'RandomFlip'},\n                {'name': 'RandomRotate90'},\n                {'name': 'RandomRotate', 'angle_spectrum': 17, 'axes': [[2, 1]], 'mode': 'reflect'},\n                {'name': 'LabelToAffinities', 'offsets': [2, 4, 6, 8]},\n                {'name': 'ToTensor', 'expand_dims': False, 'dtype': 'long'}\n            ]\n        }\n        base_config = {'mean': 0, 'std': 1}\n        transformer = Transformer(config, base_config)\n        raw_transforms = transformer.raw_transform().transforms\n        assert raw_transforms[0].mean == 0\n        assert raw_transforms[0].std == 1\n        assert raw_transforms[1].execution_probability == 0.5\n        assert raw_transforms[4].angle_spectrum == 17\n        assert raw_transforms[4].axes == [[2, 1]]\n        assert raw_transforms[4].mode == 'reflect'\n        label_transforms = transformer.label_transform().transforms\n        assert label_transforms[2].angle_spectrum == 17\n        assert label_transforms[2].axes == [[2, 1]]\n        assert label_transforms[2].mode == 'reflect'\n        # 3 conv kernels per offset\n        assert len(label_transforms[3].kernels) == 12\n\n    def test_RandomLabelToBoundaryTransformer(self):\n        config = {\n            'raw': [\n                {'name': 'Normalize'},\n                {'name': 'RandomContrast', 'execution_probability': 0.5},\n                {'name': 'RandomFlip'},\n                {'name': 'RandomRotate90'},\n                {'name': 'RandomRotate', 'angle_spectrum': 17, 'axes': [[2, 1]], 'mode': 'reflect'},\n                {'name': 'ToTensor', 'expand_dims': True}\n            ],\n            'label': [\n                {'name': 'RandomFlip'},\n                {'name': 'RandomRotate90'},\n                {'name': 'RandomRotate', 'angle_spectrum': 17, 'axes': [[2, 1]], 'mode': 'reflect'},\n                {'name': 'RandomLabelToAffinities', 'max_offset': 4},\n                {'name': 'ToTensor', 'expand_dims': False, 'dtype': 'long'}\n            ]\n        }\n        base_config = {'mean': 0, 'std': 1}\n        transformer = Transformer(config, base_config)\n        label_transforms = transformer.label_transform().transforms\n        assert label_transforms[3].offsets == (1, 2, 3, 4)\n\n    def test_crop_to_fixed_when_crop_bigger_than_volume(self):\n        m = np.random.rand(200, 200, 200)\n\n        rs1 = np.random.RandomState(13)\n\n        t = CropToFixed(rs1, size=(256, 256))\n\n        expected = np.pad(m, ((0, 0), (28, 28), (28, 28)), mode='reflect')\n\n        assert np.array_equal(expected, t(m))\n\n    def test_crop_to_fixed(self):\n        m = np.random.rand(200, 200, 200)\n\n        rs1 = np.random.RandomState(13)\n        rs2 = np.random.RandomState(13)\n\n        t = CropToFixed(rs1, size=(128, 128))\n\n        r = 200 - 128\n        y_start = rs2.randint(r)\n        x_start = rs2.randint(r)\n\n        m_crop = m[:, y_start:y_start + 128, x_start:x_start + 128]\n\n        assert np.array_equal(m_crop, t(m))\n\n\ndef _diagonal_label_volume(size, init=1):\n    label = init * np.ones((size, size, size), dtype=np.int)\n    for i in range(size):\n        for j in range(size):\n            for k in range(size):\n                if i + j > 2 * k:\n                    label[i, j, k] = 3\n    return label\n"""
pytorch3dunet/augment/__init__.py,0,b''
pytorch3dunet/augment/transforms.py,2,"b'import importlib\n\nimport numpy as np\nimport torch\nfrom scipy.ndimage import rotate, map_coordinates, gaussian_filter\nfrom scipy.ndimage.filters import convolve\nfrom skimage.filters import gaussian\nfrom skimage.segmentation import find_boundaries\nfrom torchvision.transforms import Compose\n\n# WARN: use fixed random state for reproducibility; if you want to randomize on each run seed with `time.time()` e.g.\nGLOBAL_RANDOM_STATE = np.random.RandomState(47)\n\n\nclass RandomFlip:\n    """"""\n    Randomly flips the image across the given axes. Image can be either 3D (DxHxW) or 4D (CxDxHxW).\n\n    When creating make sure that the provided RandomStates are consistent between raw and labeled datasets,\n    otherwise the models won\'t converge.\n    """"""\n\n    def __init__(self, random_state, axis_prob=0.5, **kwargs):\n        assert random_state is not None, \'RandomState cannot be None\'\n        self.random_state = random_state\n        self.axes = (0, 1, 2)\n        self.axis_prob = axis_prob\n\n    def __call__(self, m):\n        assert m.ndim in [3, 4], \'Supports only 3D (DxHxW) or 4D (CxDxHxW) images\'\n\n        for axis in self.axes:\n            if self.random_state.uniform() > self.axis_prob:\n                if m.ndim == 3:\n                    m = np.flip(m, axis)\n                else:\n                    channels = [np.flip(m[c], axis) for c in range(m.shape[0])]\n                    m = np.stack(channels, axis=0)\n\n        return m\n\n\nclass RandomRotate90:\n    """"""\n    Rotate an array by 90 degrees around a randomly chosen plane. Image can be either 3D (DxHxW) or 4D (CxDxHxW).\n\n    When creating make sure that the provided RandomStates are consistent between raw and labeled datasets,\n    otherwise the models won\'t converge.\n\n    IMPORTANT: assumes DHW axis order (that\'s why rotation is performed across (1,2) axis)\n    """"""\n\n    def __init__(self, random_state, **kwargs):\n        self.random_state = random_state\n        # always rotate around z-axis\n        self.axis = (1, 2)\n\n    def __call__(self, m):\n        assert m.ndim in [3, 4], \'Supports only 3D (DxHxW) or 4D (CxDxHxW) images\'\n\n        # pick number of rotations at random\n        k = self.random_state.randint(0, 4)\n        # rotate k times around a given plane\n        if m.ndim == 3:\n            m = np.rot90(m, k, self.axis)\n        else:\n            channels = [np.rot90(m[c], k, self.axis) for c in range(m.shape[0])]\n            m = np.stack(channels, axis=0)\n\n        return m\n\n\nclass RandomRotate:\n    """"""\n    Rotate an array by a random degrees from taken from (-angle_spectrum, angle_spectrum) interval.\n    Rotation axis is picked at random from the list of provided axes.\n    """"""\n\n    def __init__(self, random_state, angle_spectrum=30, axes=None, mode=\'reflect\', order=0, **kwargs):\n        if axes is None:\n            axes = [(1, 0), (2, 1), (2, 0)]\n        else:\n            assert isinstance(axes, list) and len(axes) > 0\n\n        self.random_state = random_state\n        self.angle_spectrum = angle_spectrum\n        self.axes = axes\n        self.mode = mode\n        self.order = order\n\n    def __call__(self, m):\n        axis = self.axes[self.random_state.randint(len(self.axes))]\n        angle = self.random_state.randint(-self.angle_spectrum, self.angle_spectrum)\n\n        if m.ndim == 3:\n            m = rotate(m, angle, axes=axis, reshape=False, order=self.order, mode=self.mode, cval=-1)\n        else:\n            channels = [rotate(m[c], angle, axes=axis, reshape=False, order=self.order, mode=self.mode, cval=-1) for c\n                        in range(m.shape[0])]\n            m = np.stack(channels, axis=0)\n\n        return m\n\n\nclass RandomContrast:\n    """"""\n    Adjust contrast by scaling each voxel to `mean + alpha * (v - mean)`.\n    """"""\n\n    def __init__(self, random_state, alpha=(0.5, 1.5), mean=0.0, execution_probability=0.1, **kwargs):\n        self.random_state = random_state\n        assert len(alpha) == 2\n        self.alpha = alpha\n        self.mean = mean\n        self.execution_probability = execution_probability\n\n    def __call__(self, m):\n        if self.random_state.uniform() < self.execution_probability:\n            alpha = self.random_state.uniform(self.alpha[0], self.alpha[1])\n            result = self.mean + alpha * (m - self.mean)\n            return np.clip(result, -1, 1)\n\n        return m\n\n\n# it\'s relatively slow, i.e. ~1s per patch of size 64x200x200, so use multiple workers in the DataLoader\n# remember to use spline_order=0 when transforming the labels\nclass ElasticDeformation:\n    """"""\n    Apply elasitc deformations of 3D patches on a per-voxel mesh. Assumes ZYX axis order (or CZYX if the data is 4D).\n    Based on: https://github.com/fcalvet/image_tools/blob/master/image_augmentation.py#L62\n    """"""\n\n    def __init__(self, random_state, spline_order, alpha=2000, sigma=50, execution_probability=0.1, apply_3d=True,\n                 **kwargs):\n        """"""\n        :param spline_order: the order of spline interpolation (use 0 for labeled images)\n        :param alpha: scaling factor for deformations\n        :param sigma: smoothing factor for Gaussian filter\n        :param execution_probability: probability of executing this transform\n        :param apply_3d: if True apply deformations in each axis\n        """"""\n        self.random_state = random_state\n        self.spline_order = spline_order\n        self.alpha = alpha\n        self.sigma = sigma\n        self.execution_probability = execution_probability\n        self.apply_3d = apply_3d\n\n    def __call__(self, m):\n        if self.random_state.uniform() < self.execution_probability:\n            assert m.ndim in [3, 4]\n\n            if m.ndim == 3:\n                volume_shape = m.shape\n            else:\n                volume_shape = m[0].shape\n\n            if self.apply_3d:\n                dz = gaussian_filter(self.random_state.randn(*volume_shape), self.sigma, mode=""reflect"") * self.alpha\n            else:\n                dz = np.zeros_like(m)\n\n            dy, dx = [\n                gaussian_filter(\n                    self.random_state.randn(*volume_shape),\n                    self.sigma, mode=""reflect""\n                ) * self.alpha for _ in range(2)\n            ]\n\n            z_dim, y_dim, x_dim = volume_shape\n            z, y, x = np.meshgrid(np.arange(z_dim), np.arange(y_dim), np.arange(x_dim), indexing=\'ij\')\n            indices = z + dz, y + dy, x + dx\n\n            if m.ndim == 3:\n                return map_coordinates(m, indices, order=self.spline_order, mode=\'reflect\')\n            else:\n                channels = [map_coordinates(c, indices, order=self.spline_order, mode=\'reflect\') for c in m]\n                return np.stack(channels, axis=0)\n\n        return m\n\n\ndef blur_boundary(boundary, sigma):\n    boundary = gaussian(boundary, sigma=sigma)\n    boundary[boundary >= 0.5] = 1\n    boundary[boundary < 0.5] = 0\n    return boundary\n\n\nclass CropToFixed:\n    def __init__(self, random_state, size=(256, 256), centered=False, **kwargs):\n        self.random_state = random_state\n        self.crop_y, self.crop_x = size\n        self.centered = centered\n\n    def __call__(self, m):\n        def _padding(pad_total):\n            half_total = pad_total // 2\n            return (half_total, pad_total - half_total)\n\n        def _rand_range_and_pad(crop_size, max_size):\n            """"""\n            Returns a tuple:\n                max_value (int) for the corner dimension. The corner dimension is chosen as `self.random_state(max_value)`\n                pad (int): padding in both directions; if crop_size is lt max_size the pad is 0\n            """"""\n            if crop_size < max_size:\n                return max_size - crop_size, (0, 0)\n            else:\n                return 1, _padding(crop_size - max_size)\n\n        def _start_and_pad(crop_size, max_size):\n            if crop_size < max_size:\n                return (max_size - crop_size) // 2, (0, 0)\n            else:\n                return 0, _padding(crop_size - max_size)\n\n        _, y, x = m.shape\n\n        if not self.centered:\n            y_range, y_pad = _rand_range_and_pad(self.crop_y, y)\n            x_range, x_pad = _rand_range_and_pad(self.crop_x, x)\n\n            y_start = self.random_state.randint(y_range)\n            x_start = self.random_state.randint(x_range)\n\n        else:\n            y_start, y_pad = _start_and_pad(self.crop_y, y)\n            x_start, x_pad = _start_and_pad(self.crop_x, x)\n\n        result = m[:, y_start:y_start + self.crop_y, x_start:x_start + self.crop_x]\n        return np.pad(result, pad_width=((0, 0), y_pad, x_pad), mode=\'reflect\')\n\n\nclass AbstractLabelToBoundary:\n    AXES_TRANSPOSE = [\n        (0, 1, 2),  # X\n        (0, 2, 1),  # Y\n        (2, 0, 1)  # Z\n    ]\n\n    def __init__(self, ignore_index=None, aggregate_affinities=False, append_label=False, **kwargs):\n        """"""\n        :param ignore_index: label to be ignored in the output, i.e. after computing the boundary the label ignore_index\n            will be restored where is was in the patch originally\n        :param aggregate_affinities: aggregate affinities with the same offset across Z,Y,X axes\n        :param append_label: if True append the orignal ground truth labels to the last channel\n        :param blur: Gaussian blur the boundaries\n        :param sigma: standard deviation for Gaussian kernel\n        """"""\n        self.ignore_index = ignore_index\n        self.aggregate_affinities = aggregate_affinities\n        self.append_label = append_label\n\n    def __call__(self, m):\n        """"""\n        Extract boundaries from a given 3D label tensor.\n        :param m: input 3D tensor\n        :return: binary mask, with 1-label corresponding to the boundary and 0-label corresponding to the background\n        """"""\n        assert m.ndim == 3\n\n        kernels = self.get_kernels()\n        boundary_arr = [np.where(np.abs(convolve(m, kernel)) > 0, 1, 0) for kernel in kernels]\n        channels = np.stack(boundary_arr)\n        results = []\n        if self.aggregate_affinities:\n            assert len(kernels) % 3 == 0, ""Number of kernels must be divided by 3 (one kernel per offset per Z,Y,X axes""\n            # aggregate affinities with the same offset\n            for i in range(0, len(kernels), 3):\n                # merge across X,Y,Z axes (logical OR)\n                xyz_aggregated_affinities = np.logical_or.reduce(channels[i:i + 3, ...]).astype(np.int)\n                # recover ignore index\n                xyz_aggregated_affinities = _recover_ignore_index(xyz_aggregated_affinities, m, self.ignore_index)\n                results.append(xyz_aggregated_affinities)\n        else:\n            results = [_recover_ignore_index(channels[i], m, self.ignore_index) for i in range(channels.shape[0])]\n\n        if self.append_label:\n            # append original input data\n            results.append(m)\n\n        # stack across channel dim\n        return np.stack(results, axis=0)\n\n    @staticmethod\n    def create_kernel(axis, offset):\n        # create conv kernel\n        k_size = offset + 1\n        k = np.zeros((1, 1, k_size), dtype=np.int)\n        k[0, 0, 0] = 1\n        k[0, 0, offset] = -1\n        return np.transpose(k, axis)\n\n    def get_kernels(self):\n        raise NotImplementedError\n\n\nclass StandardLabelToBoundary:\n    def __init__(self, ignore_index=None, append_label=False, blur=False, sigma=1, mode=\'thick\', foreground=False,\n                 **kwargs):\n        self.ignore_index = ignore_index\n        self.append_label = append_label\n        self.blur = blur\n        self.sigma = sigma\n        self.mode = mode\n        self.foreground = foreground\n\n    def __call__(self, m):\n        assert m.ndim == 3\n\n        boundaries = find_boundaries(m, connectivity=2, mode=self.mode)\n        if self.blur:\n            boundaries = blur_boundary(boundaries, self.sigma)\n\n        results = []\n        if self.foreground:\n            foreground = (m > 0).astype(\'uint8\')\n            results.append(_recover_ignore_index(foreground, m, self.ignore_index))\n\n        results.append(_recover_ignore_index(boundaries, m, self.ignore_index))\n\n        if self.append_label:\n            # append original input data\n            results.append(m)\n\n        return np.stack(results, axis=0)\n\n\nclass BlobsWithBoundary:\n    def __init__(self, mode=None, append_label=False, blur=False, sigma=1, **kwargs):\n        if mode is None:\n            mode = [\'thick\', \'inner\', \'outer\']\n        self.mode = mode\n        self.append_label = append_label\n        self.blur = blur\n        self.sigma = sigma\n\n    def __call__(self, m):\n        assert m.ndim == 3\n\n        # get the segmentation mask\n        results = [(m > 0).astype(\'uint8\')]\n\n        for bm in self.mode:\n            boundary = find_boundaries(m, connectivity=2, mode=bm)\n            if self.blur:\n                boundary = blur_boundary(boundary, self.sigma)\n            results.append(boundary)\n\n        if self.append_label:\n            results.append(m)\n\n        return np.stack(results, axis=0)\n\n\nclass BlobsToMask:\n    """"""\n    Returns binary mask from labeled image, i.e. every label greater than 0 is treated as foreground.\n\n    """"""\n\n    def __init__(self, append_label=False, boundary=False, cross_entropy=False, **kwargs):\n        self.cross_entropy = cross_entropy\n        self.boundary = boundary\n        self.append_label = append_label\n\n    def __call__(self, m):\n        assert m.ndim == 3\n\n        # get the segmentation mask\n        mask = (m > 0).astype(\'uint8\')\n        results = [mask]\n\n        if self.boundary:\n            outer = find_boundaries(m, connectivity=2, mode=\'outer\')\n            if self.cross_entropy:\n                # boundary is class 2\n                mask[outer > 0] = 2\n                results = [mask]\n            else:\n                results.append(outer)\n\n        if self.append_label:\n            results.append(m)\n\n        return np.stack(results, axis=0)\n\n\nclass RandomLabelToAffinities(AbstractLabelToBoundary):\n    """"""\n    Converts a given volumetric label array to binary mask corresponding to borders between labels.\n    One specify the max_offset (thickness) of the border. Then the offset is picked at random every time you call\n    the transformer (offset is picked form the range 1:max_offset) for each axis and the boundary computed.\n    One may use this scheme in order to make the network more robust against various thickness of borders in the ground\n    truth  (think of it as a boundary denoising scheme).\n    """"""\n\n    def __init__(self, random_state, max_offset=10, ignore_index=None, append_label=False, z_offset_scale=2, **kwargs):\n        super().__init__(ignore_index=ignore_index, append_label=append_label, aggregate_affinities=False)\n        self.random_state = random_state\n        self.offsets = tuple(range(1, max_offset + 1))\n        self.z_offset_scale = z_offset_scale\n\n    def get_kernels(self):\n        rand_offset = self.random_state.choice(self.offsets)\n        axis_ind = self.random_state.randint(3)\n        # scale down z-affinities due to anisotropy\n        if axis_ind == 2:\n            rand_offset = max(1, rand_offset // self.z_offset_scale)\n\n        rand_axis = self.AXES_TRANSPOSE[axis_ind]\n        # return a single kernel\n        return [self.create_kernel(rand_axis, rand_offset)]\n\n\nclass LabelToAffinities(AbstractLabelToBoundary):\n    """"""\n    Converts a given volumetric label array to binary mask corresponding to borders between labels (which can be seen\n    as an affinity graph: https://arxiv.org/pdf/1706.00120.pdf)\n    One specify the offsets (thickness) of the border. The boundary will be computed via the convolution operator.\n    """"""\n\n    def __init__(self, offsets, ignore_index=None, append_label=False, aggregate_affinities=False, z_offsets=None,\n                 **kwargs):\n        super().__init__(ignore_index=ignore_index, append_label=append_label,\n                         aggregate_affinities=aggregate_affinities)\n\n        assert isinstance(offsets, list) or isinstance(offsets, tuple), \'offsets must be a list or a tuple\'\n        assert all(a > 0 for a in offsets), ""\'offsets must be positive""\n        assert len(set(offsets)) == len(offsets), ""\'offsets\' must be unique""\n        if z_offsets is not None:\n            assert len(offsets) == len(z_offsets), \'z_offsets length must be the same as the length of offsets\'\n        else:\n            # if z_offsets is None just use the offsets for z-affinities\n            z_offsets = list(offsets)\n        self.z_offsets = z_offsets\n\n        self.kernels = []\n        # create kernel for every axis-offset pair\n        for xy_offset, z_offset in zip(offsets, z_offsets):\n            for axis_ind, axis in enumerate(self.AXES_TRANSPOSE):\n                final_offset = xy_offset\n                if axis_ind == 2:\n                    final_offset = z_offset\n                # create kernels for a given offset in every direction\n                self.kernels.append(self.create_kernel(axis, final_offset))\n\n    def get_kernels(self):\n        return self.kernels\n\n\nclass LabelToZAffinities(AbstractLabelToBoundary):\n    """"""\n    Converts a given volumetric label array to binary mask corresponding to borders between labels (which can be seen\n    as an affinity graph: https://arxiv.org/pdf/1706.00120.pdf)\n    One specify the offsets (thickness) of the border. The boundary will be computed via the convolution operator.\n    """"""\n\n    def __init__(self, offsets, ignore_index=None, append_label=False, **kwargs):\n        super().__init__(ignore_index=ignore_index, append_label=append_label)\n\n        assert isinstance(offsets, list) or isinstance(offsets, tuple), \'offsets must be a list or a tuple\'\n        assert all(a > 0 for a in offsets), ""\'offsets must be positive""\n        assert len(set(offsets)) == len(offsets), ""\'offsets\' must be unique""\n\n        self.kernels = []\n        z_axis = self.AXES_TRANSPOSE[2]\n        # create kernels\n        for z_offset in offsets:\n            self.kernels.append(self.create_kernel(z_axis, z_offset))\n\n    def get_kernels(self):\n        return self.kernels\n\n\nclass LabelToBoundaryAndAffinities:\n    """"""\n    Combines the StandardLabelToBoundary and LabelToAffinities in the hope\n    that that training the network to predict both would improve the main task: boundary prediction.\n    """"""\n\n    def __init__(self, xy_offsets, z_offsets, append_label=False, blur=False, sigma=1, ignore_index=None, mode=\'thick\',\n                 foreground=False, **kwargs):\n        # blur only StandardLabelToBoundary results; we don\'t want to blur the affinities\n        self.l2b = StandardLabelToBoundary(blur=blur, sigma=sigma, ignore_index=ignore_index, mode=mode,\n                                           foreground=foreground)\n        self.l2a = LabelToAffinities(offsets=xy_offsets, z_offsets=z_offsets, append_label=append_label,\n                                     ignore_index=ignore_index)\n\n    def __call__(self, m):\n        boundary = self.l2b(m)\n        affinities = self.l2a(m)\n        return np.concatenate((boundary, affinities), axis=0)\n\n\nclass FlyWingBoundary:\n    """"""\n    Use if the volume contains a single pixel boundaries between labels. Gives the single pixel boundary in the 1st\n    channel and the \'thick\' boundary in the 2nd channel and optional z-affinities\n    """"""\n\n    def __init__(self, append_label=False, thick_boundary=True, ignore_index=None, z_offsets=None, **kwargs):\n        self.append_label = append_label\n        self.thick_boundary = thick_boundary\n        self.ignore_index = ignore_index\n        self.lta = None\n        if z_offsets is not None:\n            self.lta = LabelToZAffinities(z_offsets, ignore_index=ignore_index)\n\n    def __call__(self, m):\n        boundary = (m == 0).astype(\'uint8\')\n        results = [boundary]\n\n        if self.thick_boundary:\n            t_boundary = find_boundaries(m, connectivity=1, mode=\'outer\', background=0)\n            results.append(t_boundary)\n\n        if self.lta is not None:\n            z_affs = self.lta(m)\n            for z_aff in z_affs:\n                results.append(z_aff)\n\n        if self.ignore_index is not None:\n            for b in results:\n                b[m == self.ignore_index] = self.ignore_index\n\n        if self.append_label:\n            # append original input data\n            results.append(m)\n\n        return np.stack(results, axis=0)\n\n\nclass LabelToMaskAndAffinities:\n    def __init__(self, xy_offsets, z_offsets, append_label=False, background=0, ignore_index=None, **kwargs):\n        self.background = background\n        self.l2a = LabelToAffinities(offsets=xy_offsets, z_offsets=z_offsets, append_label=append_label,\n                                     ignore_index=ignore_index)\n\n    def __call__(self, m):\n        mask = m > self.background\n        mask = np.expand_dims(mask.astype(np.uint8), axis=0)\n        affinities = self.l2a(m)\n        return np.concatenate((mask, affinities), axis=0)\n\n\nclass Standardize:\n    """"""\n    Apply Z-score normalization to a given input tensor, i.e. re-scaling the values to be 0-mean and 1-std.\n    Mean and std parameter have to be provided explicitly.\n    """"""\n\n    def __init__(self, mean, std, eps=1e-6, **kwargs):\n        self.mean = mean\n        self.std = std\n        self.eps = eps\n\n    def __call__(self, m):\n        return (m - self.mean) / np.clip(self.std, a_min=self.eps, a_max=None)\n\n\nclass Normalize:\n    """"""\n    Apply simple min-max scaling to a given input tensor, i.e. shrinks the range of the data in a fixed range of [-1, 1].\n    """"""\n\n    def __init__(self, min_value, max_value, **kwargs):\n        assert max_value > min_value\n        self.min_value = min_value\n        self.value_range = max_value - min_value\n\n    def __call__(self, m):\n        norm_0_1 = (m - self.min_value) / self.value_range\n        return np.clip(2 * norm_0_1 - 1, -1, 1)\n\n\nclass AdditiveGaussianNoise:\n    def __init__(self, random_state, scale=(0.0, 1.0), execution_probability=0.1, **kwargs):\n        self.execution_probability = execution_probability\n        self.random_state = random_state\n        self.scale = scale\n\n    def __call__(self, m):\n        if self.random_state.uniform() < self.execution_probability:\n            std = self.random_state.uniform(self.scale[0], self.scale[1])\n            gaussian_noise = self.random_state.normal(0, std, size=m.shape)\n            return m + gaussian_noise\n        return m\n\n\nclass AdditivePoissonNoise:\n    def __init__(self, random_state, lam=(0.0, 1.0), execution_probability=0.1, **kwargs):\n        self.execution_probability = execution_probability\n        self.random_state = random_state\n        self.lam = lam\n\n    def __call__(self, m):\n        if self.random_state.uniform() < self.execution_probability:\n            lam = self.random_state.uniform(self.lam[0], self.lam[1])\n            poisson_noise = self.random_state.poisson(lam, size=m.shape)\n            return m + poisson_noise\n        return m\n\n\nclass ToTensor:\n    """"""\n    Converts a given input numpy.ndarray into torch.Tensor. Adds additional \'channel\' axis when the input is 3D\n    and expand_dims=True (use for raw data of the shape (D, H, W)).\n    """"""\n\n    def __init__(self, expand_dims, dtype=np.float32, **kwargs):\n        self.expand_dims = expand_dims\n        self.dtype = dtype\n\n    def __call__(self, m):\n        assert m.ndim in [3, 4], \'Supports only 3D (DxHxW) or 4D (CxDxHxW) images\'\n        # add channel dimension\n        if self.expand_dims and m.ndim == 3:\n            m = np.expand_dims(m, axis=0)\n\n        return torch.from_numpy(m.astype(dtype=self.dtype))\n\n\nclass Relabel:\n    """"""\n    Relabel a numpy array of labels into a consecutive numbers, e.g.\n    [10,10, 0, 6, 6] -> [2, 2, 0, 1, 1]. Useful when one has an instance segmentation volume\n    at hand and would like to create a one-hot-encoding for it. Without a consecutive labeling the task would be harder.\n    """"""\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, m):\n        _, unique_labels = np.unique(m, return_inverse=True)\n        m = unique_labels.reshape(m.shape)\n        return m\n\n\nclass Identity:\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, m):\n        return m\n\n\ndef get_transformer(config, min_value, max_value, mean, std):\n    base_config = {\'min_value\': min_value, \'max_value\': max_value, \'mean\': mean, \'std\': std}\n    return Transformer(config, base_config)\n\n\nclass Transformer:\n    def __init__(self, phase_config, base_config):\n        self.phase_config = phase_config\n        self.config_base = base_config\n        self.seed = GLOBAL_RANDOM_STATE.randint(10000000)\n\n    def raw_transform(self):\n        return self._create_transform(\'raw\')\n\n    def label_transform(self):\n        return self._create_transform(\'label\')\n\n    def weight_transform(self):\n        return self._create_transform(\'weight\')\n\n    @staticmethod\n    def _transformer_class(class_name):\n        m = importlib.import_module(\'pytorch3dunet.augment.transforms\')\n        clazz = getattr(m, class_name)\n        return clazz\n\n    def _create_transform(self, name):\n        assert name in self.phase_config, f\'Could not find {name} transform\'\n        return Compose([\n            self._create_augmentation(c) for c in self.phase_config[name]\n        ])\n\n    def _create_augmentation(self, c):\n        config = dict(self.config_base)\n        config.update(c)\n        config[\'random_state\'] = np.random.RandomState(self.seed)\n        aug_class = self._transformer_class(config[\'name\'])\n        return aug_class(**config)\n\n\ndef _recover_ignore_index(input, orig, ignore_index):\n    if ignore_index is not None:\n        mask = orig == ignore_index\n        input[mask] = ignore_index\n\n    return input\n'"
pytorch3dunet/datasets/__init__.py,0,b''
pytorch3dunet/datasets/dsb.py,0,"b'import os\n\nimport imageio\nimport numpy as np\n\nfrom pytorch3dunet.augment import transforms\nfrom pytorch3dunet.datasets.utils import ConfigDataset, calculate_stats\nfrom pytorch3dunet.unet3d.utils import get_logger\n\nlogger = get_logger(\'DSB2018Dataset\')\n\n\nclass DSB2018Dataset(ConfigDataset):\n    def __init__(self, root_dir, phase, transformer_config, mirror_padding=(0, 32, 32), expand_dims=True):\n        assert os.path.isdir(root_dir), \'root_dir is not a directory\'\n        assert phase in [\'train\', \'val\', \'test\']\n\n        # use mirror padding only during the \'test\' phase\n        if phase in [\'train\', \'val\']:\n            mirror_padding = None\n        if mirror_padding is not None:\n            assert len(mirror_padding) == 3, f""Invalid mirror_padding: {mirror_padding}""\n        self.mirror_padding = mirror_padding\n\n        self.phase = phase\n\n        # load raw images\n        images_dir = os.path.join(root_dir, \'images\')\n        assert os.path.isdir(images_dir)\n        self.images = self._load_files(images_dir, expand_dims)\n\n        min_value, max_value, mean, std = calculate_stats(self.images)\n        logger.info(f\'Input stats: min={min_value}, max={max_value}, mean={mean}, std={std}\')\n\n        transformer = transforms.get_transformer(transformer_config, min_value=min_value, max_value=max_value,\n                                                 mean=mean, std=std)\n\n        # load raw images transformer\n        self.raw_transform = transformer.raw_transform()\n\n        if phase != \'test\':\n            # load labeled images\n            masks_dir = os.path.join(root_dir, \'masks\')\n            assert os.path.isdir(masks_dir)\n            self.masks = self._load_files(masks_dir, expand_dims)\n            assert len(self.images) == len(self.masks)\n            # load label images transformer\n            self.masks_transform = transformer.label_transform()\n        else:\n            self.masks = None\n            self.masks_transform = None\n\n            # add mirror padding if needed\n            if self.mirror_padding is not None:\n                z, y, x = self.mirror_padding\n                pad_width = ((z, z), (y, y), (x, x))\n                padded_imgs = []\n                for img in self.images:\n                    padded_img = np.pad(img, pad_width=pad_width, mode=\'reflect\')\n                    padded_imgs.append(padded_img)\n\n                self.images = padded_imgs\n\n    def __getitem__(self, idx):\n        if idx >= len(self):\n            raise StopIteration\n\n        img = self.images[idx]\n        if self.phase != \'test\':\n            mask = self.masks[idx]\n            return self.raw_transform(img), self.masks_transform(mask)\n        else:\n            raise NotImplementedError\n\n    def __len__(self):\n        return len(self.images)\n\n    @classmethod\n    def create_datasets(cls, dataset_config, phase):\n        phase_config = dataset_config[phase]\n        # load data augmentation configuration\n        transformer_config = phase_config[\'transformer\']\n        # load files to process\n        file_paths = phase_config[\'file_paths\']\n        # mirror padding conf\n        mirror_padding = dataset_config.get(\'mirror_padding\', None)\n\n        if phase != \'test\':\n            return [cls(file_paths[0], phase, transformer_config, mirror_padding)]\n        else:\n            raise NotImplementedError\n\n    @staticmethod\n    def _load_files(dir, expand_dims):\n        files_data = []\n        for file in os.listdir(dir):\n            path = os.path.join(dir, file)\n            img = np.asarray(imageio.imread(path))\n            if expand_dims:\n                img = np.expand_dims(img, axis=0)\n\n            files_data.append(img)\n\n        return files_data\n'"
pytorch3dunet/datasets/hdf5.py,1,"b'import glob\nimport os\nfrom itertools import chain\nfrom multiprocessing import Lock\n\nimport h5py\nimport numpy as np\n\nimport pytorch3dunet.augment.transforms as transforms\nfrom pytorch3dunet.datasets.utils import get_slice_builder, ConfigDataset, calculate_stats\nfrom pytorch3dunet.unet3d.utils import get_logger\n\nlogger = get_logger(\'HDF5Dataset\')\nlock = Lock()\n\n\nclass AbstractHDF5Dataset(ConfigDataset):\n    """"""\n    Implementation of torch.utils.data.Dataset backed by the HDF5 files, which iterates over the raw and label datasets\n    patch by patch with a given stride.\n    """"""\n\n    def __init__(self, file_path,\n                 phase,\n                 slice_builder_config,\n                 transformer_config,\n                 mirror_padding=(16, 32, 32),\n                 raw_internal_path=\'raw\',\n                 label_internal_path=\'label\',\n                 weight_internal_path=None):\n        """"""\n        :param file_path: path to H5 file containing raw data as well as labels and per pixel weights (optional)\n        :param phase: \'train\' for training, \'val\' for validation, \'test\' for testing; data augmentation is performed\n            only during the \'train\' phase\n        :para\'/home/adrian/workspace/ilastik-datasets/VolkerDeconv/train\'m slice_builder_config: configuration of the SliceBuilder\n        :param transformer_config: data augmentation configuration\n        :param mirror_padding (int or tuple): number of voxels padded to each axis\n        :param raw_internal_path (str or list): H5 internal path to the raw dataset\n        :param label_internal_path (str or list): H5 internal path to the label dataset\n        :param weight_internal_path (str or list): H5 internal path to the per pixel weights\n        """"""\n        assert phase in [\'train\', \'val\', \'test\']\n        if phase in [\'train\', \'val\']:\n            mirror_padding = None\n\n        if mirror_padding is not None:\n            if isinstance(mirror_padding, int):\n                mirror_padding = (mirror_padding,) * 3\n            else:\n                assert len(mirror_padding) == 3, f""Invalid mirror_padding: {mirror_padding}""\n\n        self.mirror_padding = mirror_padding\n        self.phase = phase\n        self.file_path = file_path\n\n        # convert raw_internal_path, label_internal_path and weight_internal_path to list for ease of computation\n        if isinstance(raw_internal_path, str):\n            raw_internal_path = [raw_internal_path]\n        if isinstance(label_internal_path, str):\n            label_internal_path = [label_internal_path]\n        if isinstance(weight_internal_path, str):\n            weight_internal_path = [weight_internal_path]\n\n        internal_paths = list(raw_internal_path)\n        if label_internal_path is not None:\n            internal_paths.extend(label_internal_path)\n        if weight_internal_path is not None:\n            internal_paths.extend(weight_internal_path)\n\n        input_file = self.create_h5_file(file_path, internal_paths)\n\n        self.raws = self.fetch_and_check(input_file, raw_internal_path)\n\n        # calculate global min, max, mean and std for normalization\n        min_value, max_value, mean, std = calculate_stats(self.raws)\n        logger.info(f\'Input stats: min={min_value}, max={max_value}, mean={mean}, std={std}\')\n\n        self.transformer = transforms.get_transformer(transformer_config, min_value=min_value, max_value=max_value,\n                                                      mean=mean, std=std)\n        self.raw_transform = self.transformer.raw_transform()\n\n        if phase != \'test\':\n            # create label/weight transform only in train/val phase\n            self.label_transform = self.transformer.label_transform()\n            self.labels = self.fetch_and_check(input_file, label_internal_path)\n\n            if weight_internal_path is not None:\n                # look for the weight map in the raw file\n                self.weight_maps = self.fetch_and_check(input_file, weight_internal_path)\n                self.weight_transform = self.transformer.weight_transform()\n            else:\n                self.weight_maps = None\n\n            self._check_dimensionality(self.raws, self.labels)\n        else:\n            # \'test\' phase used only for predictions so ignore the label dataset\n            self.labels = None\n            self.weight_maps = None\n\n            # add mirror padding if needed\n            if self.mirror_padding is not None:\n                z, y, x = self.mirror_padding\n                pad_width = ((z, z), (y, y), (x, x))\n                padded_volumes = []\n                for raw in self.raws:\n                    if raw.ndim == 4:\n                        channels = [np.pad(r, pad_width=pad_width, mode=\'reflect\') for r in raw]\n                        padded_volume = np.stack(channels)\n                    else:\n                        padded_volume = np.pad(raw, pad_width=pad_width, mode=\'reflect\')\n\n                    padded_volumes.append(padded_volume)\n\n                self.raws = padded_volumes\n\n        # build slice indices for raw and label data sets\n        slice_builder = get_slice_builder(self.raws, self.labels, self.weight_maps, slice_builder_config)\n        self.raw_slices = slice_builder.raw_slices\n        self.label_slices = slice_builder.label_slices\n        self.weight_slices = slice_builder.weight_slices\n\n        self.patch_count = len(self.raw_slices)\n        logger.info(f\'Number of patches: {self.patch_count}\')\n\n    @staticmethod\n    def create_h5_file(file_path, internal_paths):\n        raise NotImplementedError\n\n    @staticmethod\n    def fetch_datasets(input_file_h5, internal_paths):\n        raise NotImplementedError\n\n    def fetch_and_check(self, input_file_h5, internal_paths):\n        datasets = self.fetch_datasets(input_file_h5, internal_paths)\n        # expand dims if 2d\n        fn = lambda ds: np.expand_dims(ds, axis=0) if ds.ndim == 2 else ds\n        datasets = list(map(fn, datasets))\n        return datasets\n\n    def __getitem__(self, idx):\n        if idx >= len(self):\n            raise StopIteration\n\n        # get the slice for a given index \'idx\'\n        raw_idx = self.raw_slices[idx]\n        # get the raw data patch for a given slice\n        raw_patch_transformed = self._transform_patches(self.raws, raw_idx, self.raw_transform)\n\n        if self.phase == \'test\':\n            # discard the channel dimension in the slices: predictor requires only the spatial dimensions of the volume\n            if len(raw_idx) == 4:\n                raw_idx = raw_idx[1:]\n            return raw_patch_transformed, raw_idx\n        else:\n            # get the slice for a given index \'idx\'\n            label_idx = self.label_slices[idx]\n            label_patch_transformed = self._transform_patches(self.labels, label_idx, self.label_transform)\n            if self.weight_maps is not None:\n                weight_idx = self.weight_slices[idx]\n                # return the transformed weight map for a given patch together with raw and label data\n                weight_patch_transformed = self._transform_patches(self.weight_maps, weight_idx, self.weight_transform)\n                return raw_patch_transformed, label_patch_transformed, weight_patch_transformed\n            # return the transformed raw and label patches\n            return raw_patch_transformed, label_patch_transformed\n\n    @staticmethod\n    def _transform_patches(datasets, label_idx, transformer):\n        transformed_patches = []\n        for dataset in datasets:\n            # get the label data and apply the label transformer\n            transformed_patch = transformer(dataset[label_idx])\n            transformed_patches.append(transformed_patch)\n\n        # if transformed_patches is a singleton list return the first element only\n        if len(transformed_patches) == 1:\n            return transformed_patches[0]\n        else:\n            return transformed_patches\n\n    def __len__(self):\n        return self.patch_count\n\n    @staticmethod\n    def _check_dimensionality(raws, labels):\n        def _volume_shape(volume):\n            if volume.ndim == 3:\n                return volume.shape\n            return volume.shape[1:]\n\n        for raw, label in zip(raws, labels):\n            assert raw.ndim in [3, 4], \'Raw dataset must be 3D (DxHxW) or 4D (CxDxHxW)\'\n            assert label.ndim in [3, 4], \'Label dataset must be 3D (DxHxW) or 4D (CxDxHxW)\'\n\n            assert _volume_shape(raw) == _volume_shape(label), \'Raw and labels have to be of the same size\'\n\n    @classmethod\n    def create_datasets(cls, dataset_config, phase):\n        phase_config = dataset_config[phase]\n\n        # load data augmentation configuration\n        transformer_config = phase_config[\'transformer\']\n        # load slice builder config\n        slice_builder_config = phase_config[\'slice_builder\']\n        # load files to process\n        file_paths = phase_config[\'file_paths\']\n        # file_paths may contain both files and directories; if the file_path is a directory all H5 files inside\n        # are going to be included in the final file_paths\n        file_paths = cls.traverse_h5_paths(file_paths)\n\n        datasets = []\n        for file_path in file_paths:\n            try:\n                logger.info(f\'Loading {phase} set from: {file_path}...\')\n                dataset = cls(file_path=file_path,\n                              phase=phase,\n                              slice_builder_config=slice_builder_config,\n                              transformer_config=transformer_config,\n                              mirror_padding=dataset_config.get(\'mirror_padding\', None),\n                              raw_internal_path=dataset_config.get(\'raw_internal_path\', \'raw\'),\n                              label_internal_path=dataset_config.get(\'label_internal_path\', \'label\'),\n                              weight_internal_path=dataset_config.get(\'weight_internal_path\', None))\n                datasets.append(dataset)\n            except Exception:\n                logger.error(f\'Skipping {phase} set: {file_path}\', exc_info=True)\n        return datasets\n\n    @staticmethod\n    def traverse_h5_paths(file_paths):\n        assert isinstance(file_paths, list)\n        results = []\n        for file_path in file_paths:\n            if os.path.isdir(file_path):\n                # if file path is a directory take all H5 files in that directory\n                iters = [glob.glob(os.path.join(file_path, ext)) for ext in [\'*.h5\', \'*.hdf\', \'*.hdf5\', \'*.hd5\']]\n                for fp in chain(*iters):\n                    results.append(fp)\n            else:\n                results.append(file_path)\n        return results\n\n\nclass StandardHDF5Dataset(AbstractHDF5Dataset):\n    """"""\n    Implementation of the HDF5 dataset which loads the data from all of the H5 files into the memory.\n    Fast but might consume a lot of memory.\n    """"""\n\n    def __init__(self, file_path, phase, slice_builder_config, transformer_config, mirror_padding=(16, 32, 32),\n                 raw_internal_path=\'raw\', label_internal_path=\'label\', weight_internal_path=None):\n        super().__init__(file_path=file_path,\n                         phase=phase,\n                         slice_builder_config=slice_builder_config,\n                         transformer_config=transformer_config,\n                         mirror_padding=mirror_padding,\n                         raw_internal_path=raw_internal_path,\n                         label_internal_path=label_internal_path,\n                         weight_internal_path=weight_internal_path)\n\n    @staticmethod\n    def create_h5_file(file_path, internal_paths):\n        return h5py.File(file_path, \'r\')\n\n    @staticmethod\n    def fetch_datasets(input_file_h5, internal_paths):\n        return [input_file_h5[internal_path][...] for internal_path in internal_paths]\n\n\nclass LazyHDF5Dataset(AbstractHDF5Dataset):\n    """"""\n    Implementation of the HDF5 dataset which loads the data lazily. It\'s slower, but has a low memory footprint.\n\n    The problem of loading h5 dataset from multiple loader workers results in an error:\n\n        # WARN: we load everything into memory due to hdf5 bug when reading H5 from multiple subprocesses, i.e.\n        # File ""h5py/_proxy.pyx"", line 84, in h5py._proxy.H5PY_H5Dread\n        # OSError: Can\'t read data (inflate() failed)\n\n    this happens when the H5 dataset is compressed. The workaround is to create the uncompressed datasets\n    from a single worker (synchronization is necessary) and use them instead. Assuming the user specified internal\n    dataset path as PATH, this will create a corresponding `_uncompressed_PATH` dataset inside the same H5 file.\n\n    Unfortunately even after fixing the above error, reading the H5 from multiple worker threads sometimes\n    returns corrupted data and as a result. e.g. cross-entropy loss fails with: RuntimeError: CUDA error: device-side assert triggered.\n\n    This can be workaround by using only a single worker thread, i.e. set `num_workers: 1` in the config.\n    """"""\n\n    def __init__(self, file_path, phase, slice_builder_config, transformer_config, mirror_padding=(16, 32, 32),\n                 raw_internal_path=\'raw\', label_internal_path=\'label\', weight_internal_path=None):\n        super().__init__(file_path=file_path,\n                         phase=phase,\n                         slice_builder_config=slice_builder_config,\n                         transformer_config=transformer_config,\n                         mirror_padding=mirror_padding,\n                         raw_internal_path=raw_internal_path,\n                         label_internal_path=label_internal_path,\n                         weight_internal_path=weight_internal_path)\n\n    @staticmethod\n    def create_h5_file(file_path, internal_paths):\n        # make this part mutually exclusive\n        lock.acquire()\n\n        uncompressed_paths = {}\n        for internal_path in internal_paths:\n            if internal_path is not None:\n                assert \'_uncompressed\' not in internal_path\n                uncompressed_paths[internal_path] = f\'_uncompressed_{internal_path}\'\n\n        with h5py.File(file_path, \'r+\') as f:\n            for k, v in uncompressed_paths.items():\n                if v not in f:\n                    # create uncompressed dataset\n                    data = f[k][...]\n                    f.create_dataset(v, data=data)\n\n        lock.release()\n\n        # finally return the H5\n        return h5py.File(file_path, \'r\')\n\n    @staticmethod\n    def fetch_datasets(input_file_h5, internal_paths):\n        # convert to uncompressed\n        internal_paths = [f\'_uncompressed_{internal_path}\' for internal_path in internal_paths]\n        return [input_file_h5[internal_path] for internal_path in internal_paths]\n'"
pytorch3dunet/datasets/utils.py,10,"b'import collections\nimport importlib\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, ConcatDataset, Dataset\n\nfrom pytorch3dunet.unet3d.utils import get_logger\n\nlogger = get_logger(\'Dataset\')\n\n\nclass ConfigDataset(Dataset):\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n    @classmethod\n    def create_datasets(cls, dataset_config, phase):\n        """"""\n        Factory method for creating a list of datasets based on the provided config.\n\n        Args:\n            dataset_config (dict): dataset configuration\n            phase (str): one of [\'train\', \'val\', \'test\']\n\n        Returns:\n            list of `Dataset` instances\n        """"""\n        raise NotImplementedError\n\n\nclass SliceBuilder:\n    """"""\n    Builds the position of the patches in a given raw/label/weight ndarray based on the the patch and stride shape\n    """"""\n\n    def __init__(self, raw_datasets, label_datasets, weight_dataset, patch_shape, stride_shape, **kwargs):\n        """"""\n        :param raw_datasets: ndarray of raw data\n        :param label_datasets: ndarray of ground truth labels\n        :param weight_dataset: ndarray of weights for the labels\n        :param patch_shape: the shape of the patch DxHxW\n        :param stride_shape: the shape of the stride DxHxW\n        :param kwargs: additional metadata\n        """"""\n\n        patch_shape = tuple(patch_shape)\n        stride_shape = tuple(stride_shape)\n        skip_shape_check = kwargs.get(\'skip_shape_check\', False)\n        if not skip_shape_check:\n            self._check_patch_shape(patch_shape)\n\n        self._raw_slices = self._build_slices(raw_datasets[0], patch_shape, stride_shape)\n        if label_datasets is None:\n            self._label_slices = None\n        else:\n            # take the first element in the label_datasets to build slices\n            self._label_slices = self._build_slices(label_datasets[0], patch_shape, stride_shape)\n            assert len(self._raw_slices) == len(self._label_slices)\n        if weight_dataset is None:\n            self._weight_slices = None\n        else:\n            self._weight_slices = self._build_slices(weight_dataset[0], patch_shape, stride_shape)\n            assert len(self.raw_slices) == len(self._weight_slices)\n\n    @property\n    def raw_slices(self):\n        return self._raw_slices\n\n    @property\n    def label_slices(self):\n        return self._label_slices\n\n    @property\n    def weight_slices(self):\n        return self._weight_slices\n\n    @staticmethod\n    def _build_slices(dataset, patch_shape, stride_shape):\n        """"""Iterates over a given n-dim dataset patch-by-patch with a given stride\n        and builds an array of slice positions.\n\n        Returns:\n            list of slices, i.e.\n            [(slice, slice, slice, slice), ...] if len(shape) == 4\n            [(slice, slice, slice), ...] if len(shape) == 3\n        """"""\n        slices = []\n        if dataset.ndim == 4:\n            in_channels, i_z, i_y, i_x = dataset.shape\n        else:\n            i_z, i_y, i_x = dataset.shape\n\n        k_z, k_y, k_x = patch_shape\n        s_z, s_y, s_x = stride_shape\n        z_steps = SliceBuilder._gen_indices(i_z, k_z, s_z)\n        for z in z_steps:\n            y_steps = SliceBuilder._gen_indices(i_y, k_y, s_y)\n            for y in y_steps:\n                x_steps = SliceBuilder._gen_indices(i_x, k_x, s_x)\n                for x in x_steps:\n                    slice_idx = (\n                        slice(z, z + k_z),\n                        slice(y, y + k_y),\n                        slice(x, x + k_x)\n                    )\n                    if dataset.ndim == 4:\n                        slice_idx = (slice(0, in_channels),) + slice_idx\n                    slices.append(slice_idx)\n        return slices\n\n    @staticmethod\n    def _gen_indices(i, k, s):\n        assert i >= k, \'Sample size has to be bigger than the patch size\'\n        for j in range(0, i - k + 1, s):\n            yield j\n        if j + k < i:\n            yield i - k\n\n    @staticmethod\n    def _check_patch_shape(patch_shape):\n        assert len(patch_shape) == 3, \'patch_shape must be a 3D tuple\'\n        assert patch_shape[1] >= 64 and patch_shape[2] >= 64, \'Height and Width must be greater or equal 64\'\n        assert patch_shape[0] >= 16, \'Depth must be greater or equal 16\'\n\n\nclass FilterSliceBuilder(SliceBuilder):\n    """"""\n    Filter patches containing more than `1 - threshold` of ignore_index label\n    """"""\n\n    def __init__(self, raw_datasets, label_datasets, weight_datasets, patch_shape, stride_shape, ignore_index=(0,),\n                 threshold=0.6, slack_acceptance=0.01, **kwargs):\n        super().__init__(raw_datasets, label_datasets, weight_datasets, patch_shape, stride_shape, **kwargs)\n        if label_datasets is None:\n            return\n\n        rand_state = np.random.RandomState(47)\n\n        def ignore_predicate(raw_label_idx):\n            label_idx = raw_label_idx[1]\n            patch = label_datasets[0][label_idx]\n            non_ignore_counts = np.array([np.count_nonzero(patch != ii) for ii in ignore_index])\n            non_ignore_counts = non_ignore_counts / patch.size\n            return np.any(non_ignore_counts > threshold) or rand_state.rand() < slack_acceptance\n\n        zipped_slices = zip(self.raw_slices, self.label_slices)\n        # ignore slices containing too much ignore_index\n        filtered_slices = list(filter(ignore_predicate, zipped_slices))\n        # unzip and save slices\n        raw_slices, label_slices = zip(*filtered_slices)\n        self._raw_slices = list(raw_slices)\n        self._label_slices = list(label_slices)\n\n\nclass EmbeddingsSliceBuilder(FilterSliceBuilder):\n    """"""\n    Filter patches containing more than `1 - threshold` of ignore_index label and patches containing more than\n    `patch_max_instances` labels\n    """"""\n\n    def __init__(self, raw_datasets, label_datasets, weight_datasets, patch_shape, stride_shape, ignore_index=(0,),\n                 threshold=0.8, slack_acceptance=0.01, patch_max_instances=48, patch_min_instances=5, **kwargs):\n        super().__init__(raw_datasets, label_datasets, weight_datasets, patch_shape, stride_shape, ignore_index,\n                         threshold, slack_acceptance, **kwargs)\n\n        if label_datasets is None:\n            return\n\n        rand_state = np.random.RandomState(47)\n\n        def ignore_predicate(raw_label_idx):\n            label_idx = raw_label_idx[1]\n            patch = label_datasets[0][label_idx]\n            num_instances = np.unique(patch).size\n\n            # patch_max_instances is a hard constraint\n            if num_instances <= patch_max_instances:\n                # make sure that we have at least patch_min_instances in the batch and allow some slack\n                return num_instances >= patch_min_instances or rand_state.rand() < slack_acceptance\n\n            return False\n\n        zipped_slices = zip(self.raw_slices, self.label_slices)\n        # ignore slices containing too much ignore_index\n        filtered_slices = list(filter(ignore_predicate, zipped_slices))\n        # unzip and save slices\n        raw_slices, label_slices = zip(*filtered_slices)\n        self._raw_slices = list(raw_slices)\n        self._label_slices = list(label_slices)\n\n\nclass RandomFilterSliceBuilder(EmbeddingsSliceBuilder):\n    """"""\n    Filter patches containing more than `1 - threshold` of ignore_index label and return only random sample of those.\n    """"""\n\n    def __init__(self, raw_datasets, label_datasets, weight_datasets, patch_shape, stride_shape, ignore_index=(0,),\n                 threshold=0.8, slack_acceptance=0.01, patch_max_instances=48, patch_acceptance_probab=0.1,\n                 max_num_patches=25, **kwargs):\n        super().__init__(raw_datasets, label_datasets, weight_datasets, patch_shape, stride_shape,\n                         ignore_index=ignore_index, threshold=threshold, slack_acceptance=slack_acceptance,\n                         patch_max_instances=patch_max_instances, **kwargs)\n\n        self.max_num_patches = max_num_patches\n\n        if label_datasets is None:\n            return\n\n        rand_state = np.random.RandomState(47)\n\n        def ignore_predicate(raw_label_idx):\n            result = rand_state.rand() < patch_acceptance_probab\n            if result:\n                self.max_num_patches -= 1\n\n            return result and self.max_num_patches > 0\n\n        zipped_slices = zip(self.raw_slices, self.label_slices)\n        # ignore slices containing too much ignore_index\n        filtered_slices = list(filter(ignore_predicate, zipped_slices))\n        # unzip and save slices\n        raw_slices, label_slices = zip(*filtered_slices)\n        self._raw_slices = list(raw_slices)\n        self._label_slices = list(label_slices)\n\n\ndef _get_cls(class_name):\n    modules = [\'pytorch3dunet.datasets.hdf5\', \'pytorch3dunet.datasets.dsb\', \'pytorch3dunet.datasets.utils\']\n    for module in modules:\n        m = importlib.import_module(module)\n        clazz = getattr(m, class_name, None)\n        if clazz is not None:\n            return clazz\n    raise RuntimeError(f\'Unsupported dataset class: {class_name}\')\n\n\ndef get_slice_builder(raws, labels, weight_maps, config):\n    assert \'name\' in config\n    logger.info(f""Slice builder config: {config}"")\n    slice_builder_cls = _get_cls(config[\'name\'])\n    return slice_builder_cls(raws, labels, weight_maps, **config)\n\n\ndef get_train_loaders(config):\n    """"""\n    Returns dictionary containing the training and validation loaders (torch.utils.data.DataLoader).\n\n    :param config: a top level configuration object containing the \'loaders\' key\n    :return: dict {\n        \'train\': <train_loader>\n        \'val\': <val_loader>\n    }\n    """"""\n    assert \'loaders\' in config, \'Could not find data loaders configuration\'\n    loaders_config = config[\'loaders\']\n\n    logger.info(\'Creating training and validation set loaders...\')\n\n    # get dataset class\n    dataset_cls_str = loaders_config.get(\'dataset\', None)\n    if dataset_cls_str is None:\n        dataset_cls_str = \'StandardHDF5Dataset\'\n        logger.warn(f""Cannot find dataset class in the config. Using default \'{dataset_cls_str}\'."")\n    dataset_class = _get_cls(dataset_cls_str)\n\n    assert set(loaders_config[\'train\'][\'file_paths\']).isdisjoint(loaders_config[\'val\'][\'file_paths\']), \\\n        ""Train and validation \'file_paths\' overlap. One cannot use validation data for training!""\n\n    train_datasets = dataset_class.create_datasets(loaders_config, phase=\'train\')\n\n    val_datasets = dataset_class.create_datasets(loaders_config, phase=\'val\')\n\n    num_workers = loaders_config.get(\'num_workers\', 1)\n    logger.info(f\'Number of workers for train/val dataloader: {num_workers}\')\n    batch_size = loaders_config.get(\'batch_size\', 1)\n    if torch.cuda.device_count() > 1 and not config[\'device\'].type == \'cpu\':\n        logger.info(\n            f\'{torch.cuda.device_count()} GPUs available. Using batch_size = {torch.cuda.device_count()} * {batch_size}\')\n        batch_size = batch_size * torch.cuda.device_count()\n\n    logger.info(f\'Batch size for train/val loader: {batch_size}\')\n    # when training with volumetric data use batch_size of 1 due to GPU memory constraints\n    return {\n        \'train\': DataLoader(ConcatDataset(train_datasets), batch_size=batch_size, shuffle=True,\n                            num_workers=num_workers),\n        \'val\': DataLoader(ConcatDataset(val_datasets), batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    }\n\n\ndef get_test_loaders(config):\n    """"""\n    Returns test DataLoader.\n\n    :return: generator of DataLoader objects\n    """"""\n\n    assert \'loaders\' in config, \'Could not find data loaders configuration\'\n    loaders_config = config[\'loaders\']\n\n    logger.info(\'Creating test set loaders...\')\n\n    # get dataset class\n    dataset_cls_str = loaders_config.get(\'dataset\', None)\n    if dataset_cls_str is None:\n        dataset_cls_str = \'StandardHDF5Dataset\'\n        logger.warn(f""Cannot find dataset class in the config. Using default \'{dataset_cls_str}\'."")\n    dataset_class = _get_cls(dataset_cls_str)\n\n    test_datasets = dataset_class.create_datasets(loaders_config, phase=\'test\')\n\n    num_workers = loaders_config.get(\'num_workers\', 1)\n    logger.info(f\'Number of workers for the dataloader: {num_workers}\')\n\n    batch_size = loaders_config.get(\'batch_size\', 1)\n    if torch.cuda.device_count() > 1 and not config[\'device\'].type == \'cpu\':\n        logger.info(\n            f\'{torch.cuda.device_count()} GPUs available. Using batch_size = {torch.cuda.device_count()} * {batch_size}\')\n        batch_size = batch_size * torch.cuda.device_count()\n\n    logger.info(f\'Batch size for dataloader: {batch_size}\')\n\n    # use generator in order to create data loaders lazily one by one\n    for test_dataset in test_datasets:\n        logger.info(f\'Loading test set from: {test_dataset.file_path}...\')\n        yield DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=prediction_collate)\n\n\ndef prediction_collate(batch):\n    error_msg = ""batch must contain tensors or slice; found {}""\n    if isinstance(batch[0], torch.Tensor):\n        return torch.stack(batch, 0)\n    elif isinstance(batch[0], tuple) and isinstance(batch[0][0], slice):\n        return batch\n    elif isinstance(batch[0], collections.Sequence):\n        transposed = zip(*batch)\n        return [prediction_collate(samples) for samples in transposed]\n\n    raise TypeError((error_msg.format(type(batch[0]))))\n\n\ndef calculate_stats(images):\n    """"""\n    Calculates min, max, mean, std given a list of ndarrays\n    """"""\n    # flatten first since the images might not be the same size\n    flat = np.concatenate(\n        [img.ravel() for img in images]\n    )\n    return np.min(flat), np.max(flat), np.mean(flat), np.std(flat)\n'"
pytorch3dunet/embeddings/__init__.py,0,b''
pytorch3dunet/embeddings/contrastive_loss.py,17,"b'import torch\nfrom torch import nn\n\nfrom pytorch3dunet.unet3d.utils import expand_as_one_hot\n\n\nclass ContrastiveLoss(nn.Module):\n    """"""\n    Implementation of contrastive loss defined in https://arxiv.org/pdf/1708.02551.pdf\n    \'Semantic Instance Segmentation with a Discriminative Loss Function\'\n    """"""\n\n    def __init__(self, delta_var, delta_dist, norm=\'fro\', alpha=1., beta=1., gamma=0.001):\n        super(ContrastiveLoss, self).__init__()\n        self.delta_var = delta_var\n        self.delta_dist = delta_dist\n        self.norm = norm\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n\n    def _compute_cluster_means(self, input, target):\n        embedding_dims = input.size()[1]\n        # expand target: NxCxDxHxW -> NxCxExDxHxW\n        # NxCx1xDxHxW\n        target = target.unsqueeze(2)\n        # save target\'s copy in order to compute the average embeddings later\n        target_copy = target.clone()\n        shape = list(target.size())\n        shape[2] = embedding_dims\n        target = target.expand(shape)\n\n        # expand input: NxExDxHxW -> Nx1xExDxHxW\n        input = input.unsqueeze(1)\n\n        # sum embeddings in each instance (multiply first via broadcasting) output: NxCxEx1x1x1\n        embeddings_per_instance = input * target\n        num = torch.sum(embeddings_per_instance, dim=(3, 4, 5), keepdim=True)\n        # get number of voxels in each cluster output: NxCx1x1x1x1\n        num_voxels_per_instance = torch.sum(target_copy, dim=(3, 4, 5), keepdim=True)\n        # compute mean embeddings NxCxEx1x1x1\n        mean_embeddings = num / num_voxels_per_instance\n        # return mean embeddings and additional tensors needed for further computations\n        return mean_embeddings, embeddings_per_instance\n\n    def _compute_variance_term(self, cluster_means, embeddings_per_instance, target):\n        # compute the distance to cluster means, result:(NxCxDxHxW)\n        embedding_norms = torch.norm(embeddings_per_instance - cluster_means, self.norm, dim=2)\n        # get per instance distances (apply instance mask)\n        embedding_norms = embedding_norms * target\n        # zero out distances less than delta_var and sum to get the variance (NxC)\n        embedding_variance = torch.clamp(embedding_norms - self.delta_var, min=0) ** 2\n        embedding_variance = torch.sum(embedding_variance, dim=(2, 3, 4))\n        # get number of voxels per instance (NxC)\n        num_voxels_per_instance = torch.sum(target, dim=(2, 3, 4))\n        # normalize the variance term\n        C = target.size()[1]\n        variance_term = torch.sum(embedding_variance / num_voxels_per_instance, dim=1) / C\n        return variance_term\n\n    def _compute_distance_term(self, cluster_means, C):\n        if C == 1:\n            # just one cluster in the batch, so distance term does not contribute to the loss\n            return 0.\n        # squeeze space dims\n        for _ in range(3):\n            cluster_means = cluster_means.squeeze(-1)\n        # expand cluster_means tensor in order to compute the pair-wise distance between cluster means\n        cluster_means = cluster_means.unsqueeze(1)\n        shape = list(cluster_means.size())\n        shape[1] = C\n        # NxCxCxEx1x1x1\n        cm_matrix1 = cluster_means.expand(shape)\n        # transpose the cluster_means matrix in order to compute pair-wise distances\n        cm_matrix2 = cm_matrix1.permute(0, 2, 1, 3)\n        # compute pair-wise distances (NxCxC)\n        dist_matrix = torch.norm(cm_matrix1 - cm_matrix2, p=self.norm, dim=3)\n        # create matrix for the repulsion distance (i.e. cluster centers further apart than 2 * delta_dist\n        # are not longer repulsed)\n        repulsion_dist = 2 * self.delta_dist * (1 - torch.eye(C))\n        # 1xCxC\n        repulsion_dist = repulsion_dist.unsqueeze(0).to(cluster_means.device)\n        # zero out distances grater than 2*delta_dist (NxCxC)\n        hinged_dist = torch.clamp(repulsion_dist - dist_matrix, min=0) ** 2\n        # sum all of the hinged pair-wise distances\n        hinged_dist = torch.sum(hinged_dist, dim=(1, 2))\n        # normalized by the number of paris and return\n        return hinged_dist / (C * (C - 1))\n\n    def _compute_regularizer_term(self, cluster_means, C):\n        # squeeze space dims\n        for _ in range(3):\n            cluster_means = cluster_means.squeeze(-1)\n        norms = torch.norm(cluster_means, p=self.norm, dim=2)\n        assert norms.size()[1] == C\n        # return the average norm per batch\n        return torch.sum(norms, dim=1).div(C)\n\n    def forward(self, input, target):\n        """"""\n        Args:\n             input (torch.tensor): embeddings predicted by the network (NxExDxHxW) (E - embedding dims)\n             target (torch.tensor): ground truth instance segmentation (NxDxHxW)\n\n        Returns:\n            Combined loss defined as: alpha * variance_term + beta * distance_term + gamma * regularization_term\n        """"""\n        # get number of instances in the batch\n        C = torch.unique(target).size()[0]\n        # expand each label as a one-hot vector: N x D x H x W -> N x C x D x H x W\n        target = expand_as_one_hot(target, C)\n        # compare spatial dimensions\n        assert input.dim() == target.dim() == 5\n        assert input.size()[2:] == target.size()[2:]\n\n        # compute mean embeddings and assign embeddings to instances\n        cluster_means, embeddings_per_instance = self._compute_cluster_means(input, target)\n        variance_term = self._compute_variance_term(cluster_means, embeddings_per_instance, target)\n        distance_term = self._compute_distance_term(cluster_means, C)\n        regularization_term = self._compute_regularizer_term(cluster_means, C)\n        # total loss\n        loss = self.alpha * variance_term + self.beta * distance_term + self.gamma * regularization_term\n        # reduce batch dimension\n        return torch.mean(loss)\n'"
pytorch3dunet/unet3d/__init__.py,0,b''
pytorch3dunet/unet3d/buildingblocks.py,2,"b'from functools import partial\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\ndef conv3d(in_channels, out_channels, kernel_size, bias, padding):\n    return nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)\n\n\ndef create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding):\n    """"""\n    Create a list of modules with together constitute a single conv layer with non-linearity\n    and optional batchnorm/groupnorm.\n\n    Args:\n        in_channels (int): number of input channels\n        out_channels (int): number of output channels\n        kernel_size(int or tuple): size of the convolving kernel\n        order (string): order of things, e.g.\n            \'cr\' -> conv + ReLU\n            \'gcr\' -> groupnorm + conv + ReLU\n            \'cl\' -> conv + LeakyReLU\n            \'ce\' -> conv + ELU\n            \'bcr\' -> batchnorm + conv + ReLU\n        num_groups (int): number of groups for the GroupNorm\n        padding (int or tuple): add zero-padding added to all three sides of the input\n\n    Return:\n        list of tuple (name, module)\n    """"""\n    assert \'c\' in order, ""Conv layer MUST be present""\n    assert order[0] not in \'rle\', \'Non-linearity cannot be the first operation in the layer\'\n\n    modules = []\n    for i, char in enumerate(order):\n        if char == \'r\':\n            modules.append((\'ReLU\', nn.ReLU(inplace=True)))\n        elif char == \'l\':\n            modules.append((\'LeakyReLU\', nn.LeakyReLU(negative_slope=0.1, inplace=True)))\n        elif char == \'e\':\n            modules.append((\'ELU\', nn.ELU(inplace=True)))\n        elif char == \'c\':\n            # add learnable bias only in the absence of batchnorm/groupnorm\n            bias = not (\'g\' in order or \'b\' in order)\n            modules.append((\'conv\', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))\n        elif char == \'g\':\n            is_before_conv = i < order.index(\'c\')\n            if is_before_conv:\n                num_channels = in_channels\n            else:\n                num_channels = out_channels\n\n            # use only one group if the given number of groups is greater than the number of channels\n            if num_channels < num_groups:\n                num_groups = 1\n\n            assert num_channels % num_groups == 0, f\'Expected number of channels in input to be divisible by num_groups. num_channels={num_channels}, num_groups={num_groups}\'\n            modules.append((\'groupnorm\', nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)))\n        elif char == \'b\':\n            is_before_conv = i < order.index(\'c\')\n            if is_before_conv:\n                modules.append((\'batchnorm\', nn.BatchNorm3d(in_channels)))\n            else:\n                modules.append((\'batchnorm\', nn.BatchNorm3d(out_channels)))\n        else:\n            raise ValueError(f""Unsupported layer type \'{char}\'. MUST be one of [\'b\', \'g\', \'r\', \'l\', \'e\', \'c\']"")\n\n    return modules\n\n\nclass SingleConv(nn.Sequential):\n    """"""\n    Basic convolutional module consisting of a Conv3d, non-linearity and optional batchnorm/groupnorm. The order\n    of operations can be specified via the `order` parameter\n\n    Args:\n        in_channels (int): number of input channels\n        out_channels (int): number of output channels\n        kernel_size (int or tuple): size of the convolving kernel\n        order (string): determines the order of layers, e.g.\n            \'cr\' -> conv + ReLU\n            \'crg\' -> conv + ReLU + groupnorm\n            \'cl\' -> conv + LeakyReLU\n            \'ce\' -> conv + ELU\n        num_groups (int): number of groups for the GroupNorm\n        padding (int or tuple):\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, order=\'gcr\', num_groups=8, padding=1):\n        super(SingleConv, self).__init__()\n\n        for name, module in create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=padding):\n            self.add_module(name, module)\n\n\nclass DoubleConv(nn.Sequential):\n    """"""\n    A module consisting of two consecutive convolution layers (e.g. BatchNorm3d+ReLU+Conv3d).\n    We use (Conv3d+ReLU+GroupNorm3d) by default.\n    This can be changed however by providing the \'order\' argument, e.g. in order\n    to change to Conv3d+BatchNorm3d+ELU use order=\'cbe\'.\n    Use padded convolutions to make sure that the output (H_out, W_out) is the same\n    as (H_in, W_in), so that you don\'t have to crop in the decoder path.\n\n    Args:\n        in_channels (int): number of input channels\n        out_channels (int): number of output channels\n        encoder (bool): if True we\'re in the encoder path, otherwise we\'re in the decoder\n        kernel_size (int or tuple): size of the convolving kernel\n        order (string): determines the order of layers, e.g.\n            \'cr\' -> conv + ReLU\n            \'crg\' -> conv + ReLU + groupnorm\n            \'cl\' -> conv + LeakyReLU\n            \'ce\' -> conv + ELU\n        num_groups (int): number of groups for the GroupNorm\n        padding (int or tuple): add zero-padding added to all three sides of the input\n    """"""\n\n    def __init__(self, in_channels, out_channels, encoder, kernel_size=3, order=\'gcr\', num_groups=8, padding=1):\n        super(DoubleConv, self).__init__()\n        if encoder:\n            # we\'re in the encoder path\n            conv1_in_channels = in_channels\n            conv1_out_channels = out_channels // 2\n            if conv1_out_channels < in_channels:\n                conv1_out_channels = in_channels\n            conv2_in_channels, conv2_out_channels = conv1_out_channels, out_channels\n        else:\n            # we\'re in the decoder path, decrease the number of channels in the 1st convolution\n            conv1_in_channels, conv1_out_channels = in_channels, out_channels\n            conv2_in_channels, conv2_out_channels = out_channels, out_channels\n\n        # conv1\n        self.add_module(\'SingleConv1\',\n                        SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups,\n                                   padding=padding))\n        # conv2\n        self.add_module(\'SingleConv2\',\n                        SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups,\n                                   padding=padding))\n\n\nclass ExtResNetBlock(nn.Module):\n    """"""\n    Basic UNet block consisting of a SingleConv followed by the residual block.\n    The SingleConv takes care of increasing/decreasing the number of channels and also ensures that the number\n    of output channels is compatible with the residual block that follows.\n    This block can be used instead of standard DoubleConv in the Encoder module.\n    Motivated by: https://arxiv.org/pdf/1706.00120.pdf\n\n    Notice we use ELU instead of ReLU (order=\'cge\') and put non-linearity after the groupnorm.\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, order=\'cge\', num_groups=8, **kwargs):\n        super(ExtResNetBlock, self).__init__()\n\n        # first convolution\n        self.conv1 = SingleConv(in_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n        # residual block\n        self.conv2 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n        # remove non-linearity from the 3rd convolution since it\'s going to be applied after adding the residual\n        n_order = order\n        for c in \'rel\':\n            n_order = n_order.replace(c, \'\')\n        self.conv3 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=n_order,\n                                num_groups=num_groups)\n\n        # create non-linearity separately\n        if \'l\' in order:\n            self.non_linearity = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        elif \'e\' in order:\n            self.non_linearity = nn.ELU(inplace=True)\n        else:\n            self.non_linearity = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        # apply first convolution and save the output as a residual\n        out = self.conv1(x)\n        residual = out\n\n        # residual block\n        out = self.conv2(out)\n        out = self.conv3(out)\n\n        out += residual\n        out = self.non_linearity(out)\n\n        return out\n\n\nclass Encoder(nn.Module):\n    """"""\n    A single module from the encoder path consisting of the optional max\n    pooling layer (one may specify the MaxPool kernel_size to be different\n    than the standard (2,2,2), e.g. if the volumetric data is anisotropic\n    (make sure to use complementary scale_factor in the decoder path) followed by\n    a DoubleConv module.\n    Args:\n        in_channels (int): number of input channels\n        out_channels (int): number of output channels\n        conv_kernel_size (int or tuple): size of the convolving kernel\n        apply_pooling (bool): if True use MaxPool3d before DoubleConv\n        pool_kernel_size (int or tuple): the size of the window\n        pool_type (str): pooling layer: \'max\' or \'avg\'\n        basic_module(nn.Module): either ResNetBlock or DoubleConv\n        conv_layer_order (string): determines the order of layers\n            in `DoubleConv` module. See `DoubleConv` for more info.\n        num_groups (int): number of groups for the GroupNorm\n        padding (int or tuple): add zero-padding added to all three sides of the input\n    """"""\n\n    def __init__(self, in_channels, out_channels, conv_kernel_size=3, apply_pooling=True,\n                 pool_kernel_size=2, pool_type=\'max\', basic_module=DoubleConv, conv_layer_order=\'gcr\',\n                 num_groups=8, padding=1):\n        super(Encoder, self).__init__()\n        assert pool_type in [\'max\', \'avg\']\n        if apply_pooling:\n            if pool_type == \'max\':\n                self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n            else:\n                self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)\n        else:\n            self.pooling = None\n\n        self.basic_module = basic_module(in_channels, out_channels,\n                                         encoder=True,\n                                         kernel_size=conv_kernel_size,\n                                         order=conv_layer_order,\n                                         num_groups=num_groups,\n                                         padding=padding)\n\n    def forward(self, x):\n        if self.pooling is not None:\n            x = self.pooling(x)\n        x = self.basic_module(x)\n        return x\n\n\nclass Decoder(nn.Module):\n    """"""\n    A single module for decoder path consisting of the upsampling layer\n    (either learned ConvTranspose3d or nearest neighbor interpolation) followed by a basic module (DoubleConv or ExtResNetBlock).\n    Args:\n        in_channels (int): number of input channels\n        out_channels (int): number of output channels\n        conv_kernel_size (int or tuple): size of the convolving kernel\n        scale_factor (tuple): used as the multiplier for the image H/W/D in\n            case of nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation\n            from the corresponding encoder\n        basic_module(nn.Module): either ResNetBlock or DoubleConv\n        conv_layer_order (string): determines the order of layers\n            in `DoubleConv` module. See `DoubleConv` for more info.\n        num_groups (int): number of groups for the GroupNorm\n        padding (int or tuple): add zero-padding added to all three sides of the input\n    """"""\n\n    def __init__(self, in_channels, out_channels, conv_kernel_size=3, scale_factor=(2, 2, 2), basic_module=DoubleConv,\n                 conv_layer_order=\'gcr\', num_groups=8, mode=\'nearest\', padding=1):\n        super(Decoder, self).__init__()\n        if basic_module == DoubleConv:\n            # if DoubleConv is the basic_module use interpolation for upsampling and concatenation joining\n            self.upsampling = Upsampling(transposed_conv=False, in_channels=in_channels, out_channels=out_channels,\n                                         kernel_size=conv_kernel_size, scale_factor=scale_factor, mode=mode)\n            # concat joining\n            self.joining = partial(self._joining, concat=True)\n        else:\n            # if basic_module=ExtResNetBlock use transposed convolution upsampling and summation joining\n            self.upsampling = Upsampling(transposed_conv=True, in_channels=in_channels, out_channels=out_channels,\n                                         kernel_size=conv_kernel_size, scale_factor=scale_factor, mode=mode)\n            # sum joining\n            self.joining = partial(self._joining, concat=False)\n            # adapt the number of in_channels for the ExtResNetBlock\n            in_channels = out_channels\n\n        self.basic_module = basic_module(in_channels, out_channels,\n                                         encoder=False,\n                                         kernel_size=conv_kernel_size,\n                                         order=conv_layer_order,\n                                         num_groups=num_groups,\n                                         padding=padding)\n\n    def forward(self, encoder_features, x):\n        x = self.upsampling(encoder_features=encoder_features, x=x)\n        x = self.joining(encoder_features, x)\n        x = self.basic_module(x)\n        return x\n\n    @staticmethod\n    def _joining(encoder_features, x, concat):\n        if concat:\n            return torch.cat((encoder_features, x), dim=1)\n        else:\n            return encoder_features + x\n\n\nclass Upsampling(nn.Module):\n    """"""\n    Upsamples a given multi-channel 3D data using either interpolation or learned transposed convolution.\n\n    Args:\n        transposed_conv (bool): if True uses ConvTranspose3d for upsampling, otherwise uses interpolation\n        in_channels (int): number of input channels for transposed conv\n            used only if transposed_conv is True\n        out_channels (int): number of output channels for transpose conv\n            used only if transposed_conv is True\n        kernel_size (int or tuple): size of the convolving kernel\n            used only if transposed_conv is True\n        scale_factor (int or tuple): stride of the convolution\n            used only if transposed_conv is True\n        mode (str): algorithm used for upsampling:\n            \'nearest\' | \'linear\' | \'bilinear\' | \'trilinear\' | \'area\'. Default: \'nearest\'\n            used only if transposed_conv is False\n    """"""\n\n    def __init__(self, transposed_conv, in_channels=None, out_channels=None, kernel_size=3,\n                 scale_factor=(2, 2, 2), mode=\'nearest\'):\n        super(Upsampling, self).__init__()\n\n        if transposed_conv:\n            # make sure that the output size reverses the MaxPool3d from the corresponding encoder\n            # (D_out\xe2\x80\x85=\xe2\x80\x85(D_in\xe2\x80\x85\xe2\x88\x92\xe2\x80\x851)\xe2\x80\x85\xc3\x97\xe2\x80\x85\xe2\x80\x85stride[0]\xe2\x80\x85\xe2\x88\x92\xe2\x80\x852\xe2\x80\x85\xc3\x97\xe2\x80\x85\xe2\x80\x85padding[0]\xe2\x80\x85+\xe2\x80\x85\xe2\x80\x85kernel_size[0]\xe2\x80\x85+\xe2\x80\x85\xe2\x80\x85output_padding[0])\n            self.upsample = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size, stride=scale_factor,\n                                               padding=1)\n        else:\n            self.upsample = partial(self._interpolate, mode=mode)\n\n    def forward(self, encoder_features, x):\n        output_size = encoder_features.size()[2:]\n        return self.upsample(x, output_size)\n\n    @staticmethod\n    def _interpolate(x, size, mode):\n        return F.interpolate(x, size=size, mode=mode)\n'"
pytorch3dunet/unet3d/config.py,3,"b'import argparse\n\nimport torch\nimport yaml\n\nfrom pytorch3dunet.unet3d import utils\n\nlogger = utils.get_logger(\'ConfigLoader\')\n\n\ndef load_config():\n    parser = argparse.ArgumentParser(description=\'UNet3D\')\n    parser.add_argument(\'--config\', type=str, help=\'Path to the YAML config file\', required=True)\n    args = parser.parse_args()\n    config = _load_config_yaml(args.config)\n    # Get a device to train on\n    device_str = config.get(\'device\', None)\n    if device_str is not None:\n        logger.info(f""Device specified in config: \'{device_str}\'"")\n        if device_str.startswith(\'cuda\') and not torch.cuda.is_available():\n            logger.warn(\'CUDA not available, using CPU\')\n            device_str = \'cpu\'\n    else:\n        device_str = ""cuda:0"" if torch.cuda.is_available() else \'cpu\'\n        logger.info(f""Using \'{device_str}\' device"")\n\n    device = torch.device(device_str)\n    config[\'device\'] = device\n    return config\n\n\ndef _load_config_yaml(config_file):\n    return yaml.safe_load(open(config_file, \'r\'))\n'"
pytorch3dunet/unet3d/losses.py,18,"b'import torch\nimport torch.nn.functional as F\nfrom torch import nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn import MSELoss, SmoothL1Loss, L1Loss\n\nfrom pytorch3dunet.embeddings.contrastive_loss import ContrastiveLoss\nfrom pytorch3dunet.unet3d.utils import expand_as_one_hot\n\n\ndef compute_per_channel_dice(input, target, epsilon=1e-6, weight=None):\n    """"""\n    Computes DiceCoefficient as defined in https://arxiv.org/abs/1606.04797 given  a multi channel input and target.\n    Assumes the input is a normalized probability, e.g. a result of Sigmoid or Softmax function.\n\n    Args:\n         input (torch.Tensor): NxCxSpatial input tensor\n         target (torch.Tensor): NxCxSpatial target tensor\n         epsilon (float): prevents division by zero\n         weight (torch.Tensor): Cx1 tensor of weight per channel/class\n    """"""\n\n    # input and target shapes must match\n    assert input.size() == target.size(), ""\'input\' and \'target\' must have the same shape""\n\n    input = flatten(input)\n    target = flatten(target)\n    target = target.float()\n\n    # compute per channel Dice Coefficient\n    intersect = (input * target).sum(-1)\n    if weight is not None:\n        intersect = weight * intersect\n\n    # here we can use standard dice (input + target).sum(-1) or extension (see V-Net) (input^2 + target^2).sum(-1)\n    denominator = (input * input).sum(-1) + (target * target).sum(-1)\n    return 2 * (intersect / denominator.clamp(min=epsilon))\n\n\nclass _MaskingLossWrapper(nn.Module):\n    """"""\n    Loss wrapper which prevents the gradient of the loss to be computed where target is equal to `ignore_index`.\n    """"""\n\n    def __init__(self, loss, ignore_index):\n        super(_MaskingLossWrapper, self).__init__()\n        assert ignore_index is not None, \'ignore_index cannot be None\'\n        self.loss = loss\n        self.ignore_index = ignore_index\n\n    def forward(self, input, target):\n        mask = target.clone().ne_(self.ignore_index)\n        mask.requires_grad = False\n\n        # mask out input/target so that the gradient is zero where on the mask\n        input = input * mask\n        target = target * mask\n\n        # forward masked input and target to the loss\n        return self.loss(input, target)\n\n\nclass SkipLastTargetChannelWrapper(nn.Module):\n    """"""\n    Loss wrapper which removes additional target channel\n    """"""\n\n    def __init__(self, loss, squeeze_channel=False):\n        super(SkipLastTargetChannelWrapper, self).__init__()\n        self.loss = loss\n        self.squeeze_channel = squeeze_channel\n\n    def forward(self, input, target):\n        assert target.size(1) > 1, \'Target tensor has a singleton channel dimension, cannot remove channel\'\n\n        # skips last target channel if needed\n        target = target[:, :-1, ...]\n\n        if self.squeeze_channel:\n            # squeeze channel dimension if singleton\n            target = torch.squeeze(target, dim=1)\n        return self.loss(input, target)\n\n\nclass _AbstractDiceLoss(nn.Module):\n    """"""\n    Base class for different implementations of Dice loss.\n    """"""\n\n    def __init__(self, weight=None, sigmoid_normalization=True):\n        super(_AbstractDiceLoss, self).__init__()\n        self.register_buffer(\'weight\', weight)\n        # The output from the network during training is assumed to be un-normalized probabilities and we would\n        # like to normalize the logits. Since Dice (or soft Dice in this case) is usually used for binary data,\n        # normalizing the channels with Sigmoid is the default choice even for multi-class segmentation problems.\n        # However if one would like to apply Softmax in order to get the proper probability distribution from the\n        # output, just specify sigmoid_normalization=False.\n        if sigmoid_normalization:\n            self.normalization = nn.Sigmoid()\n        else:\n            self.normalization = nn.Softmax(dim=1)\n\n    def dice(self, input, target, weight):\n        # actual Dice score computation; to be implemented by the subclass\n        raise NotImplementedError\n\n    def forward(self, input, target):\n        # get probabilities from logits\n        input = self.normalization(input)\n\n        # compute per channel Dice coefficient\n        per_channel_dice = self.dice(input, target, weight=self.weight)\n\n        # average Dice score across all channels/classes\n        return 1. - torch.mean(per_channel_dice)\n\n\nclass DiceLoss(_AbstractDiceLoss):\n    """"""Computes Dice Loss according to https://arxiv.org/abs/1606.04797.\n    For multi-class segmentation `weight` parameter can be used to assign different weights per class.\n    """"""\n\n    def __init__(self, weight=None, sigmoid_normalization=True):\n        super().__init__(weight, sigmoid_normalization)\n\n    def dice(self, input, target, weight):\n        return compute_per_channel_dice(input, target, weight=self.weight)\n\n\nclass GeneralizedDiceLoss(_AbstractDiceLoss):\n    """"""Computes Generalized Dice Loss (GDL) as described in https://arxiv.org/pdf/1707.03237.pdf.\n    """"""\n\n    def __init__(self, sigmoid_normalization=True, epsilon=1e-6):\n        super().__init__(weight=None, sigmoid_normalization=sigmoid_normalization)\n        self.epsilon = epsilon\n\n    def dice(self, input, target, weight):\n        assert input.size() == target.size(), ""\'input\' and \'target\' must have the same shape""\n\n        input = flatten(input)\n        target = flatten(target)\n        target = target.float()\n\n        if input.size(0) == 1:\n            # for GDL to make sense we need at least 2 channels (see https://arxiv.org/pdf/1707.03237.pdf)\n            # put foreground and background voxels in separate channels\n            input = torch.cat((input, 1 - input), dim=0)\n            target = torch.cat((target, 1 - target), dim=0)\n\n        # GDL weighting: the contribution of each label is corrected by the inverse of its volume\n        w_l = target.sum(-1)\n        w_l = 1 / (w_l * w_l).clamp(min=self.epsilon)\n        w_l.requires_grad = False\n\n        intersect = (input * target).sum(-1)\n        intersect = intersect * w_l\n\n        denominator = (input + target).sum(-1)\n        denominator = (denominator * w_l).clamp(min=self.epsilon)\n\n        return 2 * (intersect.sum() / denominator.sum())\n\n\nclass BCEDiceLoss(nn.Module):\n    """"""Linear combination of BCE and Dice losses""""""\n\n    def __init__(self, alpha, beta):\n        super(BCEDiceLoss, self).__init__()\n        self.alpha = alpha\n        self.bce = nn.BCEWithLogitsLoss()\n        self.beta = beta\n        self.dice = DiceLoss()\n\n    def forward(self, input, target):\n        return self.alpha * self.bce(input, target) + self.beta * self.dice(input, target)\n\n\nclass WeightedCrossEntropyLoss(nn.Module):\n    """"""WeightedCrossEntropyLoss (WCE) as described in https://arxiv.org/pdf/1707.03237.pdf\n    """"""\n\n    def __init__(self, ignore_index=-1):\n        super(WeightedCrossEntropyLoss, self).__init__()\n        self.ignore_index = ignore_index\n\n    def forward(self, input, target):\n        weight = self._class_weights(input)\n        return F.cross_entropy(input, target, weight=weight, ignore_index=self.ignore_index)\n\n    @staticmethod\n    def _class_weights(input):\n        # normalize the input first\n        input = F.softmax(input, dim=1)\n        flattened = flatten(input)\n        nominator = (1. - flattened).sum(-1)\n        denominator = flattened.sum(-1)\n        class_weights = Variable(nominator / denominator, requires_grad=False)\n        return class_weights\n\n\nclass PixelWiseCrossEntropyLoss(nn.Module):\n    def __init__(self, class_weights=None, ignore_index=None):\n        super(PixelWiseCrossEntropyLoss, self).__init__()\n        self.register_buffer(\'class_weights\', class_weights)\n        self.ignore_index = ignore_index\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, target, weights):\n        assert target.size() == weights.size()\n        # normalize the input\n        log_probabilities = self.log_softmax(input)\n        # standard CrossEntropyLoss requires the target to be (NxDxHxW), so we need to expand it to (NxCxDxHxW)\n        target = expand_as_one_hot(target, C=input.size()[1], ignore_index=self.ignore_index)\n        # expand weights\n        weights = weights.unsqueeze(0)\n        weights = weights.expand_as(input)\n\n        # create default class_weights if None\n        if self.class_weights is None:\n            class_weights = torch.ones(input.size()[1]).float().to(input.device)\n        else:\n            class_weights = self.class_weights\n\n        # resize class_weights to be broadcastable into the weights\n        class_weights = class_weights.view(1, -1, 1, 1, 1)\n\n        # multiply weights tensor by class weights\n        weights = class_weights * weights\n\n        # compute the losses\n        result = -weights * target * log_probabilities\n        # average the losses\n        return result.mean()\n\n\nclass TagsAngularLoss(nn.Module):\n    def __init__(self, tags_coefficients):\n        super(TagsAngularLoss, self).__init__()\n        self.tags_coefficients = tags_coefficients\n\n    def forward(self, inputs, targets, weight):\n        assert isinstance(inputs, list)\n        # if there is just one output head the \'inputs\' is going to be a singleton list [tensor]\n        # and \'targets\' is just going to be a tensor (that\'s how the HDF5Dataloader works)\n        # so wrap targets in a list in this case\n        if len(inputs) == 1:\n            targets = [targets]\n        assert len(inputs) == len(targets) == len(self.tags_coefficients)\n        loss = 0\n        for input, target, alpha in zip(inputs, targets, self.tags_coefficients):\n            loss += alpha * square_angular_loss(input, target, weight)\n\n        return loss\n\n\nclass WeightedSmoothL1Loss(nn.SmoothL1Loss):\n    def __init__(self, threshold, initial_weight, apply_below_threshold=True):\n        super().__init__(reduction=""none"")\n        self.threshold = threshold\n        self.apply_below_threshold = apply_below_threshold\n        self.weight = initial_weight\n\n    def forward(self, input, target):\n        l1 = super().forward(input, target)\n\n        if self.apply_below_threshold:\n            mask = target < self.threshold\n        else:\n            mask = target >= self.threshold\n\n        l1[mask] = l1[mask] * self.weight\n\n        return l1.mean()\n\n\ndef square_angular_loss(input, target, weights=None):\n    """"""\n    Computes square angular loss between input and target directions.\n    Makes sure that the input and target directions are normalized so that torch.acos would not produce NaNs.\n\n    :param input: 5D input tensor (NCDHW)\n    :param target: 5D target tensor (NCDHW)\n    :param weights: 3D weight tensor in order to balance different instance sizes\n    :return: per pixel weighted sum of squared angular losses\n    """"""\n    assert input.size() == target.size()\n    # normalize and multiply by the stability_coeff in order to prevent NaN results from torch.acos\n    stability_coeff = 0.999999\n    input = input / torch.norm(input, p=2, dim=1).detach().clamp(min=1e-8) * stability_coeff\n    target = target / torch.norm(target, p=2, dim=1).detach().clamp(min=1e-8) * stability_coeff\n    # compute cosine map\n    cosines = (input * target).sum(dim=1)\n    error_radians = torch.acos(cosines)\n    if weights is not None:\n        return (error_radians * error_radians * weights).sum()\n    else:\n        return (error_radians * error_radians).sum()\n\n\ndef flatten(tensor):\n    """"""Flattens a given tensor such that the channel axis is first.\n    The shapes are transformed as follows:\n       (N, C, D, H, W) -> (C, N * D * H * W)\n    """"""\n    # number of channels\n    C = tensor.size(1)\n    # new axis order\n    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n    transposed = tensor.permute(axis_order)\n    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n    return transposed.contiguous().view(C, -1)\n\n\ndef get_loss_criterion(config):\n    """"""\n    Returns the loss function based on provided configuration\n    :param config: (dict) a top level configuration object containing the \'loss\' key\n    :return: an instance of the loss function\n    """"""\n    assert \'loss\' in config, \'Could not find loss function configuration\'\n    loss_config = config[\'loss\']\n    name = loss_config.pop(\'name\')\n\n    ignore_index = loss_config.pop(\'ignore_index\', None)\n    skip_last_target = loss_config.pop(\'skip_last_target\', False)\n    weight = loss_config.pop(\'weight\', None)\n\n    if weight is not None:\n        # convert to cuda tensor if necessary\n        weight = torch.tensor(weight).to(config[\'device\'])\n\n    pos_weight = loss_config.pop(\'pos_weight\', None)\n    if pos_weight is not None:\n        # convert to cuda tensor if necessary\n        pos_weight = torch.tensor(pos_weight).to(config[\'device\'])\n\n    loss = _create_loss(name, loss_config, weight, ignore_index, pos_weight)\n\n    if not (ignore_index is None or name in [\'CrossEntropyLoss\', \'WeightedCrossEntropyLoss\']):\n        # use MaskingLossWrapper only for non-cross-entropy losses, since CE losses allow specifying \'ignore_index\' directly\n        loss = _MaskingLossWrapper(loss, ignore_index)\n\n    if skip_last_target:\n        loss = SkipLastTargetChannelWrapper(loss, loss_config.get(\'squeeze_channel\', False))\n\n    return loss\n\n\nSUPPORTED_LOSSES = [\'BCEWithLogitsLoss\', \'BCEDiceLoss\', \'CrossEntropyLoss\', \'WeightedCrossEntropyLoss\',\n                    \'PixelWiseCrossEntropyLoss\', \'GeneralizedDiceLoss\', \'DiceLoss\', \'TagsAngularLoss\', \'MSELoss\',\n                    \'SmoothL1Loss\', \'L1Loss\', \'WeightedSmoothL1Loss\']\n\n\ndef _create_loss(name, loss_config, weight, ignore_index, pos_weight):\n    if name == \'BCEWithLogitsLoss\':\n        return nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    elif name == \'BCEDiceLoss\':\n        alpha = loss_config.get(\'alphs\', 1.)\n        beta = loss_config.get(\'beta\', 1.)\n        return BCEDiceLoss(alpha, beta)\n    elif name == \'CrossEntropyLoss\':\n        if ignore_index is None:\n            ignore_index = -100  # use the default \'ignore_index\' as defined in the CrossEntropyLoss\n        return nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index)\n    elif name == \'WeightedCrossEntropyLoss\':\n        if ignore_index is None:\n            ignore_index = -100  # use the default \'ignore_index\' as defined in the CrossEntropyLoss\n        return WeightedCrossEntropyLoss(ignore_index=ignore_index)\n    elif name == \'PixelWiseCrossEntropyLoss\':\n        return PixelWiseCrossEntropyLoss(class_weights=weight, ignore_index=ignore_index)\n    elif name == \'GeneralizedDiceLoss\':\n        sigmoid_normalization = loss_config.get(\'sigmoid_normalization\', True)\n        return GeneralizedDiceLoss(sigmoid_normalization=sigmoid_normalization)\n    elif name == \'DiceLoss\':\n        sigmoid_normalization = loss_config.get(\'sigmoid_normalization\', True)\n        return DiceLoss(weight=weight, sigmoid_normalization=sigmoid_normalization)\n    elif name == \'TagsAngularLoss\':\n        tags_coefficients = loss_config[\'tags_coefficients\']\n        return TagsAngularLoss(tags_coefficients)\n    elif name == \'MSELoss\':\n        return MSELoss()\n    elif name == \'SmoothL1Loss\':\n        return SmoothL1Loss()\n    elif name == \'L1Loss\':\n        return L1Loss()\n    elif name == \'ContrastiveLoss\':\n        return ContrastiveLoss(loss_config[\'delta_var\'], loss_config[\'delta_dist\'], loss_config[\'norm\'],\n                               loss_config[\'alpha\'], loss_config[\'beta\'], loss_config[\'gamma\'])\n    elif name == \'WeightedSmoothL1Loss\':\n        return WeightedSmoothL1Loss(threshold=loss_config[\'threshold\'], initial_weight=loss_config[\'initial_weight\'],\n                                    apply_below_threshold=loss_config.get(\'apply_below_threshold\', True))\n    else:\n        raise RuntimeError(f""Unsupported loss function: \'{name}\'. Supported losses: {SUPPORTED_LOSSES}"")\n'"
pytorch3dunet/unet3d/metrics.py,21,"b'import importlib\nimport os\nimport time\n\nimport hdbscan\nimport numpy as np\nimport torch\nfrom skimage import measure\nfrom skimage.metrics import adapted_rand_error, peak_signal_noise_ratio\nfrom sklearn.cluster import MeanShift\n\nfrom pytorch3dunet.unet3d.losses import compute_per_channel_dice\nfrom pytorch3dunet.unet3d.seg_metrics import AveragePrecision, Accuracy\nfrom pytorch3dunet.unet3d.utils import get_logger, expand_as_one_hot, plot_segm, convert_to_numpy\n\nlogger = get_logger(\'EvalMetric\')\n\n\nclass DiceCoefficient:\n    """"""Computes Dice Coefficient.\n    Generalized to multiple channels by computing per-channel Dice Score\n    (as described in https://arxiv.org/pdf/1707.03237.pdf) and theTn simply taking the average.\n    Input is expected to be probabilities instead of logits.\n    This metric is mostly useful when channels contain the same semantic class (e.g. affinities computed with different offsets).\n    DO NOT USE this metric when training with DiceLoss, otherwise the results will be biased towards the loss.\n    """"""\n\n    def __init__(self, epsilon=1e-6, **kwargs):\n        self.epsilon = epsilon\n\n    def __call__(self, input, target):\n        # Average across channels in order to get the final score\n        return torch.mean(compute_per_channel_dice(input, target, epsilon=self.epsilon))\n\n\nclass MeanIoU:\n    """"""\n    Computes IoU for each class separately and then averages over all classes.\n    """"""\n\n    def __init__(self, skip_channels=(), ignore_index=None, **kwargs):\n        """"""\n        :param skip_channels: list/tuple of channels to be ignored from the IoU computation\n        :param ignore_index: id of the label to be ignored from IoU computation\n        """"""\n        self.ignore_index = ignore_index\n        self.skip_channels = skip_channels\n\n    def __call__(self, input, target):\n        """"""\n        :param input: 5D probability maps torch float tensor (NxCxDxHxW)\n        :param target: 4D or 5D ground truth torch tensor. 4D (NxDxHxW) tensor will be expanded to 5D as one-hot\n        :return: intersection over union averaged over all channels\n        """"""\n        assert input.dim() == 5\n\n        n_classes = input.size()[1]\n\n        if target.dim() == 4:\n            target = expand_as_one_hot(target, C=n_classes, ignore_index=self.ignore_index)\n\n        assert input.size() == target.size()\n\n        per_batch_iou = []\n        for _input, _target in zip(input, target):\n            binary_prediction = self._binarize_predictions(_input, n_classes)\n\n            if self.ignore_index is not None:\n                # zero out ignore_index\n                mask = _target == self.ignore_index\n                binary_prediction[mask] = 0\n                _target[mask] = 0\n\n            # convert to uint8 just in case\n            binary_prediction = binary_prediction.byte()\n            _target = _target.byte()\n\n            per_channel_iou = []\n            for c in range(n_classes):\n                if c in self.skip_channels:\n                    continue\n\n                per_channel_iou.append(self._jaccard_index(binary_prediction[c], _target[c]))\n\n            assert per_channel_iou, ""All channels were ignored from the computation""\n            mean_iou = torch.mean(torch.tensor(per_channel_iou))\n            per_batch_iou.append(mean_iou)\n\n        return torch.mean(torch.tensor(per_batch_iou))\n\n    def _binarize_predictions(self, input, n_classes):\n        """"""\n        Puts 1 for the class/channel with the highest probability and 0 in other channels. Returns byte tensor of the\n        same size as the input tensor.\n        """"""\n        if n_classes == 1:\n            # for single channel input just threshold the probability map\n            result = input > 0.5\n            return result.long()\n\n        _, max_index = torch.max(input, dim=0, keepdim=True)\n        return torch.zeros_like(input, dtype=torch.uint8).scatter_(0, max_index, 1)\n\n    def _jaccard_index(self, prediction, target):\n        """"""\n        Computes IoU for a given target and prediction tensors\n        """"""\n        return torch.sum(prediction & target).float() / torch.clamp(torch.sum(prediction | target).float(), min=1e-8)\n\n\nclass AdaptedRandError:\n    """"""\n    A functor which computes an Adapted Rand error as defined by the SNEMI3D contest\n    (http://brainiac2.mit.edu/SNEMI3D/evaluation).\n\n    This is a generic implementation which takes the input, converts it to the segmentation image (see `input_to_segm()`)\n    and then computes the ARand between the segmentation and the ground truth target. Depending on one\'s use case\n    it\'s enough to extend this class and implement the `input_to_segm` method.\n\n    Args:\n        use_last_target (bool): use only the last channel from the target to compute the ARand\n        save_plots (bool): save predicted segmentation (result from `input_to_segm`) together with GT segmentation as a PNG\n        plots_dir (string): directory where the plots are to be saved\n    """"""\n\n    def __init__(self, use_last_target=False, save_plots=False, plots_dir=\'.\', **kwargs):\n        self.use_last_target = use_last_target\n        self.save_plots = save_plots\n        self.plots_dir = plots_dir\n        if not os.path.exists(plots_dir) and save_plots:\n            os.makedirs(plots_dir)\n\n    def __call__(self, input, target):\n        """"""\n        Compute ARand Error for each input, target pair in the batch and return the mean value.\n\n        Args:\n            input (torch.tensor): 5D (NCDHW) output from the network\n            target (torch.tensor): 4D (NDHW) ground truth segmentation\n\n        Returns:\n            average ARand Error across the batch\n        """"""\n        def _arand_err(gt, seg):\n            n_seg = len(np.unique(seg))\n            if n_seg == 1:\n                return 0.\n            return adapted_rand_error(gt, seg)[0]\n\n        # converts input and target to numpy arrays\n        input, target = convert_to_numpy(input, target)\n        if self.use_last_target:\n            target = target[:, -1, ...]  # 4D\n        else:\n            # use 1st target channel\n            target = target[:, 0, ...]  # 4D\n\n        # ensure target is of integer type\n        target = target.astype(np.int)\n\n        per_batch_arand = []\n        for _input, _target in zip(input, target):\n            n_clusters = len(np.unique(_target))\n            # skip ARand eval if there is only one label in the patch due to the zero-division error in Arand impl\n            # xxx/skimage/metrics/_adapted_rand_error.py:70: RuntimeWarning: invalid value encountered in double_scalars\n            # precision = sum_p_ij2 / sum_a2\n            logger.info(f\'Number of ground truth clusters: {n_clusters}\')\n            if n_clusters == 1:\n                logger.info(\'Skipping ARandError computation: only 1 label present in the ground truth\')\n                per_batch_arand.append(0.)\n                continue\n\n            # convert _input to segmentation CDHW\n            segm = self.input_to_segm(_input)\n            assert segm.ndim == 4\n\n            if self.save_plots:\n                # save predicted and ground truth segmentation\n                plot_segm(segm, _target, self.plots_dir)\n\n            # compute per channel arand and return the minimum value\n            per_channel_arand = [_arand_err(_target, channel_segm) for channel_segm in segm]\n            logger.info(f\'Min ARand for channel: {np.argmin(per_channel_arand)}\')\n            per_batch_arand.append(np.min(per_channel_arand))\n\n        # return mean arand error\n        mean_arand = torch.mean(torch.tensor(per_batch_arand))\n        logger.info(f\'ARand: {mean_arand.item()}\')\n        return mean_arand\n\n    def input_to_segm(self, input):\n        """"""\n        Converts input tensor (output from the network) to the segmentation image. E.g. if the input is the boundary\n        pmaps then one option would be to threshold it and run connected components in order to return the segmentation.\n\n        :param input: 4D tensor (CDHW)\n        :return: segmentation volume either 4D (segmentation per channel)\n        """"""\n        # by deafult assume that input is a segmentation volume itself\n        return input\n\n\nclass BoundaryAdaptedRandError(AdaptedRandError):\n    """"""\n    Compute ARand between the input boundary map and target segmentation.\n    Boundary map is thresholded, and connected components is run to get the predicted segmentation\n    """"""\n\n    def __init__(self, thresholds=None, use_last_target=True, input_channel=None, invert_pmaps=True,\n                 save_plots=False, plots_dir=\'.\', **kwargs):\n        super().__init__(use_last_target=use_last_target, save_plots=save_plots, plots_dir=plots_dir, **kwargs)\n        if thresholds is None:\n            thresholds = [0.3, 0.4, 0.5, 0.6]\n        assert isinstance(thresholds, list)\n        self.thresholds = thresholds\n        self.input_channel = input_channel\n        self.invert_pmaps = invert_pmaps\n\n    def input_to_segm(self, input):\n        if self.input_channel is not None:\n            input = np.expand_dims(input[self.input_channel], axis=0)\n\n        segs = []\n        for predictions in input:\n            for th in self.thresholds:\n                # threshold probability maps\n                predictions = predictions > th\n\n                if self.invert_pmaps:\n                    # for connected component analysis we need to treat boundary signal as background\n                    # assign 0-label to boundary mask\n                    predictions = np.logical_not(predictions)\n\n                predictions = predictions.astype(np.uint8)\n                # run connected components on the predicted mask; consider only 1-connectivity\n                seg = measure.label(predictions, background=0, connectivity=1)\n                segs.append(seg)\n\n        return np.stack(segs)\n\n\nclass GenericAdaptedRandError(AdaptedRandError):\n    def __init__(self, input_channels, thresholds=None, use_last_target=True, invert_channels=None,\n                 save_plots=False, plots_dir=\'.\', **kwargs):\n\n        super().__init__(use_last_target=use_last_target, save_plots=save_plots, plots_dir=plots_dir, **kwargs)\n        assert isinstance(input_channels, list) or isinstance(input_channels, tuple)\n        self.input_channels = input_channels\n        if thresholds is None:\n            thresholds = [0.3, 0.4, 0.5, 0.6]\n        assert isinstance(thresholds, list)\n        self.thresholds = thresholds\n        if invert_channels is None:\n            invert_channels = []\n        self.invert_channels = invert_channels\n\n    def input_to_segm(self, input):\n        # pick only the channels specified in the input_channels\n        results = []\n        for i in self.input_channels:\n            c = input[i]\n            # invert channel if necessary\n            if i in self.invert_channels:\n                c = 1 - c\n            results.append(c)\n\n        input = np.stack(results)\n\n        segs = []\n        for predictions in input:\n            for th in self.thresholds:\n                # run connected components on the predicted mask; consider only 1-connectivity\n                seg = measure.label((predictions > th).astype(np.uint8), background=0, connectivity=1)\n                segs.append(seg)\n\n        return np.stack(segs)\n\n\nclass EmbeddingsAdaptedRandError(AdaptedRandError):\n    def __init__(self, min_cluster_size=100, min_samples=None, metric=\'euclidean\', cluster_selection_method=\'eom\',\n                 save_plots=False, plots_dir=\'.\', **kwargs):\n        super().__init__(save_plots=save_plots, plots_dir=plots_dir, **kwargs)\n\n        logger.info(f\'HDBSCAN params: min_cluster_size: {min_cluster_size}, min_samples: {min_samples}\')\n        self.clustering = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric=metric,\n                                          cluster_selection_method=cluster_selection_method)\n\n    def input_to_segm(self, embeddings):\n        logger.info(""Computing clusters with HDBSCAN..."")\n\n        # shape of the output segmentation\n        output_shape = embeddings.shape[1:]\n        # reshape (C, D, H, W) -> (C, D * H * W) and transpose\n        flattened_embeddings = embeddings.reshape(embeddings.shape[0], -1).transpose()\n\n        # perform clustering and reshape in order to get the segmentation volume\n        start = time.time()\n        segm = self.clustering.fit_predict(flattened_embeddings).reshape(output_shape)\n        logger.info(f\'Number of clusters found by HDBSCAN: {np.max(segm)}. Duration: {time.time() - start} sec.\')\n\n        # assign noise to new cluster (by default hdbscan gives -1 label to outliers)\n        noise_label = np.max(segm) + 1\n        segm[segm == -1] = noise_label\n\n        return np.expand_dims(segm, axis=0)\n\n\n# Just for completeness, however sklean MeanShift implementation is just too slow for clustering embeddings\nclass EmbeddingsMeanShiftAdaptedRandError(AdaptedRandError):\n    def __init__(self, bandwidth, save_plots=False, plots_dir=\'.\', **kwargs):\n        super().__init__(save_plots=save_plots, plots_dir=plots_dir, **kwargs)\n        logger.info(f\'MeanShift params: bandwidth: {bandwidth}\')\n        # use bin_seeding to speedup the mean-shift significantly\n        self.clustering = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n\n    def input_to_segm(self, embeddings):\n        logger.info(""Computing clusters with MeanShift..."")\n\n        # shape of the output segmentation\n        output_shape = embeddings.shape[1:]\n        # reshape (C, D, H, W) -> (C, D * H * W) and transpose\n        flattened_embeddings = embeddings.reshape(embeddings.shape[0], -1).transpose()\n\n        # perform clustering and reshape in order to get the segmentation volume\n        start = time.time()\n        segm = self.clustering.fit_predict(flattened_embeddings).reshape(output_shape)\n        logger.info(f\'Number of clusters found by MeanShift: {np.max(segm)}. Duration: {time.time() - start} sec.\')\n        return np.expand_dims(segm, axis=0)\n\n\nclass GenericAveragePrecision:\n    def __init__(self, min_instance_size=None, use_last_target=False, metric=\'ap\', **kwargs):\n        self.min_instance_size = min_instance_size\n        self.use_last_target = use_last_target\n        assert metric in [\'ap\', \'acc\']\n        if metric == \'ap\':\n            # use AveragePrecision\n            self.metric = AveragePrecision()\n        else:\n            # use Accuracy at 0.5 IoU\n            self.metric = Accuracy(iou_threshold=0.5)\n\n    def __call__(self, input, target):\n        assert isinstance(input, torch.Tensor) and isinstance(target, torch.Tensor)\n        assert input.dim() == 5\n        assert target.dim() == 5\n\n        input, target = convert_to_numpy(input, target)\n        if self.use_last_target:\n            target = target[:, -1, ...]  # 4D\n        else:\n            # use 1st target channel\n            target = target[:, 0, ...]  # 4D\n\n        batch_aps = []\n        # iterate over the batch\n        for inp, tar in zip(input, target):\n            segs = self.input_to_seg(inp)  # 4D\n            # convert target to seg\n            tar = self.target_to_seg(tar)\n            # filter small instances if necessary\n            tar = self._filter_instances(tar)\n\n            # compute average precision per channel\n            segs_aps = [self.metric(self._filter_instances(seg), tar) for seg in segs]\n\n            logger.info(f\'Max Average Precision for channel: {np.argmax(segs_aps)}\')\n            # save max AP\n            batch_aps.append(np.max(segs_aps))\n\n        return torch.tensor(batch_aps).mean()\n\n    def _filter_instances(self, input):\n        """"""\n        Filters instances smaller than \'min_instance_size\' by overriding them with 0-index\n        :param input: input instance segmentation\n        """"""\n        if self.min_instance_size is not None:\n            labels, counts = np.unique(input, return_counts=True)\n            for label, count in zip(labels, counts):\n                if count < self.min_instance_size:\n                    input[input == label] = 0\n        return input\n\n    def input_to_seg(self, input):\n        raise NotImplementedError\n\n    def target_to_seg(self, target):\n        return target\n\n\nclass BlobsAveragePrecision(GenericAveragePrecision):\n    """"""\n    Computes Average Precision given foreground prediction and ground truth instance segmentation.\n    """"""\n\n    def __init__(self, thresholds=None, metric=\'ap\', min_instance_size=None, input_channel=0, **kwargs):\n        super().__init__(min_instance_size=min_instance_size, use_last_target=True, metric=metric)\n        if thresholds is None:\n            thresholds = [0.4, 0.5, 0.6, 0.7, 0.8]\n        assert isinstance(thresholds, list)\n        self.thresholds = thresholds\n        self.input_channel = input_channel\n\n    def input_to_seg(self, input):\n        input = input[self.input_channel]\n        segs = []\n        for th in self.thresholds:\n            # threshold and run connected components\n            mask = (input > th).astype(np.uint8)\n            seg = measure.label(mask, background=0, connectivity=1)\n            segs.append(seg)\n        return np.stack(segs)\n\n\nclass BlobsBoundaryAveragePrecision(GenericAveragePrecision):\n    """"""\n    Computes Average Precision given foreground prediction, boundary prediction and ground truth instance segmentation.\n    Segmentation mask is computed as (P_mask - P_boundary) > th followed by a connected component\n    """"""\n    def __init__(self, thresholds=None, metric=\'ap\', min_instance_size=None, **kwargs):\n        super().__init__(min_instance_size=min_instance_size, use_last_target=True, metric=metric)\n        if thresholds is None:\n            thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n        assert isinstance(thresholds, list)\n        self.thresholds = thresholds\n\n    def input_to_seg(self, input):\n        # input = P_mask - P_boundary\n        input = input[0] - input[1]\n        segs = []\n        for th in self.thresholds:\n            # threshold and run connected components\n            mask = (input > th).astype(np.uint8)\n            seg = measure.label(mask, background=0, connectivity=1)\n            segs.append(seg)\n        return np.stack(segs)\n\n\nclass BoundaryAveragePrecision(GenericAveragePrecision):\n    """"""\n    Computes Average Precision given boundary prediction and ground truth instance segmentation.\n    """"""\n\n    def __init__(self, thresholds=None, min_instance_size=None, input_channel=0, **kwargs):\n        super().__init__(min_instance_size=min_instance_size, use_last_target=True)\n        if thresholds is None:\n            thresholds = [0.3, 0.4, 0.5, 0.6]\n        assert isinstance(thresholds, list)\n        self.thresholds = thresholds\n        self.input_channel = input_channel\n\n    def input_to_seg(self, input):\n        input = input[self.input_channel]\n        segs = []\n        for th in self.thresholds:\n            seg = measure.label(np.logical_not(input > th).astype(np.uint8), background=0, connectivity=1)\n            segs.append(seg)\n        return np.stack(segs)\n\n\nclass PSNR:\n    """"""\n    Computes Peak Signal to Noise Ratio. Use e.g. as an eval metric for denoising task\n    """"""\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, input, target):\n        input, target = convert_to_numpy(input, target)\n        return peak_signal_noise_ratio(target, input)\n\n\nclass WithinAngleThreshold:\n    """"""\n    Returns the percentage of predicted directions which are more than \'angle_threshold\' apart from the ground\n    truth directions. \'angle_threshold\' is expected to be given in degrees not radians.\n    """"""\n\n    def __init__(self, angle_threshold, **kwargs):\n        self.threshold_radians = angle_threshold / 360 * np.pi\n\n    def __call__(self, inputs, targets):\n        assert isinstance(inputs, list)\n        if len(inputs) == 1:\n            targets = [targets]\n        assert len(inputs) == len(targets)\n\n        within_count = 0\n        total_count = 0\n        for input, target in zip(inputs, targets):\n            # normalize and multiply by the stability_coeff in order to prevent NaN results from torch.acos\n            stability_coeff = 0.999999\n            input = input / torch.norm(input, p=2, dim=1).detach().clamp(min=1e-8) * stability_coeff\n            target = target / torch.norm(target, p=2, dim=1).detach().clamp(min=1e-8) * stability_coeff\n            # compute cosine map\n            cosines = (input * target).sum(dim=1)\n            error_radians = torch.acos(cosines)\n            # increase by the number of directions within the threshold\n            within_count += error_radians[error_radians < self.threshold_radians].numel()\n            # increase by the number of all directions\n            total_count += error_radians.numel()\n\n        return torch.tensor(within_count / total_count)\n\n\nclass InverseAngularError:\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, inputs, targets, **kwargs):\n        assert isinstance(inputs, list)\n        if len(inputs) == 1:\n            targets = [targets]\n        assert len(inputs) == len(targets)\n\n        total_error = 0\n        for input, target in zip(inputs, targets):\n            # normalize and multiply by the stability_coeff in order to prevent NaN results from torch.acos\n            stability_coeff = 0.999999\n            input = input / torch.norm(input, p=2, dim=1).detach().clamp(min=1e-8) * stability_coeff\n            target = target / torch.norm(target, p=2, dim=1).detach().clamp(min=1e-8) * stability_coeff\n            # compute cosine map\n            cosines = (input * target).sum(dim=1)\n            error_radians = torch.acos(cosines)\n            total_error += error_radians.sum()\n\n        return torch.tensor(1. / total_error)\n\n\ndef get_evaluation_metric(config):\n    """"""\n    Returns the evaluation metric function based on provided configuration\n    :param config: (dict) a top level configuration object containing the \'eval_metric\' key\n    :return: an instance of the evaluation metric\n    """"""\n\n    def _metric_class(class_name):\n        m = importlib.import_module(\'pytorch3dunet.unet3d.metrics\')\n        clazz = getattr(m, class_name)\n        return clazz\n\n    assert \'eval_metric\' in config, \'Could not find evaluation metric configuration\'\n    metric_config = config[\'eval_metric\']\n    metric_class = _metric_class(metric_config[\'name\'])\n    return metric_class(**metric_config)\n'"
pytorch3dunet/unet3d/model.py,1,"b'import importlib\n\nimport torch.nn as nn\n\nfrom pytorch3dunet.unet3d.buildingblocks import Encoder, Decoder, DoubleConv, ExtResNetBlock\nfrom pytorch3dunet.unet3d.utils import number_of_features_per_level\n\n\nclass Abstract3DUNet(nn.Module):\n    """"""\n    Base class for standard and residual UNet.\n\n    Args:\n        in_channels (int): number of input channels\n        out_channels (int): number of output segmentation masks;\n            Note that that the of out_channels might correspond to either\n            different semantic classes or to different binary segmentation mask.\n            It\'s up to the user of the class to interpret the out_channels and\n            use the proper loss criterion during training (i.e. CrossEntropyLoss (multi-class)\n            or BCEWithLogitsLoss (two-class) respectively)\n        f_maps (int, tuple): number of feature maps at each level of the encoder; if it\'s an integer the number\n            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the\n            final 1x1 convolution, otherwise apply nn.Softmax. MUST be True if nn.BCELoss (two-class) is used\n            to train the model. MUST be False if nn.CrossEntropyLoss (multi-class) is used to train the model.\n        basic_module: basic model for the encoder/decoder (DoubleConv, ExtResNetBlock, ....)\n        layer_order (string): determines the order of layers\n            in `SingleConv` module. e.g. \'crg\' stands for Conv3d+ReLU+GroupNorm3d.\n            See `SingleConv` for more info\n        f_maps (int, tuple): if int: number of feature maps in the first conv layer of the encoder (default: 64);\n            if tuple: number of feature maps at each level\n        num_groups (int): number of groups for the GroupNorm\n        num_levels (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)\n        is_segmentation (bool): if True (semantic segmentation problem) Sigmoid/Softmax normalization is applied\n            after the final convolution; if False (regression problem) the normalization layer is skipped at the end\n        testing (bool): if True (testing mode) the `final_activation` (if present, i.e. `is_segmentation=true`)\n            will be applied as the last operation during the forward pass; if False the model is in training mode\n            and the `final_activation` (even if present) won\'t be applied; default: False\n        conv_kernel_size (int or tuple): size of the convolving kernel in the basic_module\n        pool_kernel_size (int or tuple): the size of the window\n        conv_padding (int or tuple): add zero-padding added to all three sides of the input\n    """"""\n\n    def __init__(self, in_channels, out_channels, final_sigmoid, basic_module, f_maps=64, layer_order=\'gcr\',\n                 num_groups=8, num_levels=4, is_segmentation=True, testing=False,\n                 conv_kernel_size=3, pool_kernel_size=2, conv_padding=1, **kwargs):\n        super(Abstract3DUNet, self).__init__()\n\n        self.testing = testing\n\n        if isinstance(f_maps, int):\n            f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)\n\n        # create encoder path consisting of Encoder modules. Depth of the encoder is equal to `len(f_maps)`\n        encoders = []\n        for i, out_feature_num in enumerate(f_maps):\n            if i == 0:\n                encoder = Encoder(in_channels, out_feature_num,\n                                  apply_pooling=False,  # skip pooling in the firs encoder\n                                  basic_module=basic_module,\n                                  conv_layer_order=layer_order,\n                                  conv_kernel_size=conv_kernel_size,\n                                  num_groups=num_groups,\n                                  padding=conv_padding)\n            else:\n                # TODO: adapt for anisotropy in the data, i.e. use proper pooling kernel to make the data isotropic after 1-2 pooling operations\n                encoder = Encoder(f_maps[i - 1], out_feature_num,\n                                  basic_module=basic_module,\n                                  conv_layer_order=layer_order,\n                                  conv_kernel_size=conv_kernel_size,\n                                  num_groups=num_groups,\n                                  pool_kernel_size=pool_kernel_size,\n                                  padding=conv_padding)\n\n            encoders.append(encoder)\n\n        self.encoders = nn.ModuleList(encoders)\n\n        # create decoder path consisting of the Decoder modules. The length of the decoder is equal to `len(f_maps) - 1`\n        decoders = []\n        reversed_f_maps = list(reversed(f_maps))\n        for i in range(len(reversed_f_maps) - 1):\n            if basic_module == DoubleConv:\n                in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n            else:\n                in_feature_num = reversed_f_maps[i]\n\n            out_feature_num = reversed_f_maps[i + 1]\n            # TODO: if non-standard pooling was used, make sure to use correct striding for transpose conv\n            # currently strides with a constant stride: (2, 2, 2)\n            decoder = Decoder(in_feature_num, out_feature_num,\n                              basic_module=basic_module,\n                              conv_layer_order=layer_order,\n                              conv_kernel_size=conv_kernel_size,\n                              num_groups=num_groups,\n                              padding=conv_padding)\n            decoders.append(decoder)\n\n        self.decoders = nn.ModuleList(decoders)\n\n        # in the last layer a 1\xc3\x971 convolution reduces the number of output\n        # channels to the number of labels\n        self.final_conv = nn.Conv3d(f_maps[0], out_channels, 1)\n\n        if is_segmentation:\n            # semantic segmentation problem\n            if final_sigmoid:\n                self.final_activation = nn.Sigmoid()\n            else:\n                self.final_activation = nn.Softmax(dim=1)\n        else:\n            # regression problem\n            self.final_activation = None\n\n    def forward(self, x):\n        # encoder part\n        encoders_features = []\n        for encoder in self.encoders:\n            x = encoder(x)\n            # reverse the encoder outputs to be aligned with the decoder\n            encoders_features.insert(0, x)\n\n        # remove the last encoder\'s output from the list\n        # !!remember: it\'s the 1st in the list\n        encoders_features = encoders_features[1:]\n\n        # decoder part\n        for decoder, encoder_features in zip(self.decoders, encoders_features):\n            # pass the output from the corresponding encoder and the output\n            # of the previous decoder\n            x = decoder(encoder_features, x)\n\n        x = self.final_conv(x)\n\n        # apply final_activation (i.e. Sigmoid or Softmax) only during prediction. During training the network outputs\n        # logits and it\'s up to the user to normalize it before visualising with tensorboard or computing validation metric\n        if self.testing and self.final_activation is not None:\n            x = self.final_activation(x)\n\n        return x\n\n\nclass UNet3D(Abstract3DUNet):\n    """"""\n    3DUnet model from\n    `""3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation""\n        <https://arxiv.org/pdf/1606.06650.pdf>`.\n\n    Uses `DoubleConv` as a basic_module and nearest neighbor upsampling in the decoder\n    """"""\n\n    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order=\'gcr\',\n                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n        super(UNet3D, self).__init__(in_channels=in_channels, out_channels=out_channels, final_sigmoid=final_sigmoid,\n                                     basic_module=DoubleConv, f_maps=f_maps, layer_order=layer_order,\n                                     num_groups=num_groups, num_levels=num_levels, is_segmentation=is_segmentation,\n                                     conv_padding=conv_padding, **kwargs)\n\n\nclass ResidualUNet3D(Abstract3DUNet):\n    """"""\n    Residual 3DUnet model implementation based on https://arxiv.org/pdf/1706.00120.pdf.\n    Uses ExtResNetBlock as a basic building block, summation joining instead\n    of concatenation joining and transposed convolutions for upsampling (watch out for block artifacts).\n    Since the model effectively becomes a residual net, in theory it allows for deeper UNet.\n    """"""\n\n    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order=\'gcr\',\n                 num_groups=8, num_levels=5, is_segmentation=True, conv_padding=1, **kwargs):\n        super(ResidualUNet3D, self).__init__(in_channels=in_channels, out_channels=out_channels,\n                                             final_sigmoid=final_sigmoid,\n                                             basic_module=ExtResNetBlock, f_maps=f_maps, layer_order=layer_order,\n                                             num_groups=num_groups, num_levels=num_levels,\n                                             is_segmentation=is_segmentation, conv_padding=conv_padding,\n                                             **kwargs)\n\n\nclass UNet2D(Abstract3DUNet):\n    """"""\n    Just a standard 2D Unet. Arises naturally by specifying conv_kernel_size=(1, 3, 3), pool_kernel_size=(1, 2, 2).\n    """"""\n\n    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order=\'gcr\',\n                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n        if conv_padding == 1:\n            conv_padding = (0, 1, 1)\n        super(UNet2D, self).__init__(in_channels=in_channels,\n                                     out_channels=out_channels,\n                                     final_sigmoid=final_sigmoid,\n                                     basic_module=DoubleConv,\n                                     f_maps=f_maps,\n                                     layer_order=layer_order,\n                                     num_groups=num_groups,\n                                     num_levels=num_levels,\n                                     is_segmentation=is_segmentation,\n                                     conv_kernel_size=(1, 3, 3),\n                                     pool_kernel_size=(1, 2, 2),\n                                     conv_padding=conv_padding,\n                                     **kwargs)\n\n\ndef get_model(config):\n    def _model_class(class_name):\n        m = importlib.import_module(\'pytorch3dunet.unet3d.model\')\n        clazz = getattr(m, class_name)\n        return clazz\n\n    assert \'model\' in config, \'Could not find model configuration\'\n    model_config = config[\'model\']\n    model_class = _model_class(model_config[\'name\'])\n    return model_class(**model_config)\n'"
pytorch3dunet/unet3d/predictor.py,4,"b'import time\n\nimport h5py\nimport hdbscan\nimport numpy as np\nimport torch\nfrom sklearn.cluster import MeanShift\n\nfrom pytorch3dunet.datasets.utils import SliceBuilder\nfrom pytorch3dunet.unet3d.utils import get_logger\nfrom pytorch3dunet.unet3d.utils import remove_halo\n\nlogger = get_logger(\'UNet3DPredictor\')\n\n\nclass _AbstractPredictor:\n    def __init__(self, model, loader, output_file, config, **kwargs):\n        self.model = model\n        self.loader = loader\n        self.output_file = output_file\n        self.config = config\n        self.predictor_config = kwargs\n\n    @staticmethod\n    def _volume_shape(dataset):\n        # TODO: support multiple internal datasets\n        raw = dataset.raws[0]\n        if raw.ndim == 3:\n            return raw.shape\n        else:\n            return raw.shape[1:]\n\n    @staticmethod\n    def _get_output_dataset_names(number_of_datasets, prefix=\'predictions\'):\n        if number_of_datasets == 1:\n            return [prefix]\n        else:\n            return [f\'{prefix}{i}\' for i in range(number_of_datasets)]\n\n    def predict(self):\n        raise NotImplementedError\n\n\nclass StandardPredictor(_AbstractPredictor):\n    """"""\n    Applies the model on the given dataset and saves the result in the `output_file` in the H5 format.\n    Predictions from the network are kept in memory. If the results from the network don\'t fit in into RAM\n    use `LazyPredictor` instead.\n\n    The output dataset names inside the H5 is given by `des_dataset_name` config argument. If the argument is\n    not present in the config \'predictions{n}\' is used as a default dataset name, where `n` denotes the number\n    of the output head from the network.\n\n    Args:\n        model (Unet3D): trained 3D UNet model used for prediction\n        data_loader (torch.utils.data.DataLoader): input data loader\n        output_file (str): path to the output H5 file\n        config (dict): global config dict\n    """"""\n\n    def __init__(self, model, loader, output_file, config, **kwargs):\n        super().__init__(model, loader, output_file, config, **kwargs)\n\n    def predict(self):\n        out_channels = self.config[\'model\'].get(\'out_channels\')\n        if out_channels is None:\n            out_channels = self.config[\'model\'][\'dt_out_channels\']\n\n        prediction_channel = self.config.get(\'prediction_channel\', None)\n        if prediction_channel is not None:\n            logger.info(f""Using only channel \'{prediction_channel}\' from the network output"")\n\n        device = self.config[\'device\']\n        output_heads = self.config[\'model\'].get(\'output_heads\', 1)\n\n        logger.info(f\'Running prediction on {len(self.loader)} batches...\')\n\n        # dimensionality of the the output predictions\n        volume_shape = self._volume_shape(self.loader.dataset)\n        if prediction_channel is None:\n            prediction_maps_shape = (out_channels,) + volume_shape\n        else:\n            # single channel prediction map\n            prediction_maps_shape = (1,) + volume_shape\n\n        logger.info(f\'The shape of the output prediction maps (CDHW): {prediction_maps_shape}\')\n\n        patch_halo = self.predictor_config.get(\'patch_halo\', (4, 8, 8))\n        self._validate_halo(patch_halo, self.config[\'loaders\'][\'test\'][\'slice_builder\'])\n        logger.info(f\'Using patch_halo: {patch_halo}\')\n\n        # create destination H5 file\n        h5_output_file = h5py.File(self.output_file, \'w\')\n        # allocate prediction and normalization arrays\n        logger.info(\'Allocating prediction and normalization arrays...\')\n        prediction_maps, normalization_masks = self._allocate_prediction_maps(prediction_maps_shape,\n                                                                              output_heads, h5_output_file)\n\n        # Sets the module in evaluation mode explicitly (necessary for batchnorm/dropout layers if present)\n        self.model.eval()\n        # Set the `testing=true` flag otherwise the final Softmax/Sigmoid won\'t be applied!\n        self.model.testing = True\n        # Run predictions on the entire input dataset\n        with torch.no_grad():\n            for batch, indices in self.loader:\n                # send batch to device\n                batch = batch.to(device)\n\n                # forward pass\n                predictions = self.model(batch)\n\n                # wrap predictions into a list if there is only one output head from the network\n                if output_heads == 1:\n                    predictions = [predictions]\n\n                # for each output head\n                for prediction, prediction_map, normalization_mask in zip(predictions, prediction_maps,\n                                                                          normalization_masks):\n\n                    # convert to numpy array\n                    prediction = prediction.cpu().numpy()\n\n                    # for each batch sample\n                    for pred, index in zip(prediction, indices):\n                        # save patch index: (C,D,H,W)\n                        if prediction_channel is None:\n                            channel_slice = slice(0, out_channels)\n                        else:\n                            channel_slice = slice(0, 1)\n                        index = (channel_slice,) + index\n\n                        if prediction_channel is not None:\n                            # use only the \'prediction_channel\'\n                            logger.info(f""Using channel \'{prediction_channel}\'..."")\n                            pred = np.expand_dims(pred[prediction_channel], axis=0)\n\n                        logger.info(f\'Saving predictions for slice:{index}...\')\n\n                        # remove halo in order to avoid block artifacts in the output probability maps\n                        u_prediction, u_index = remove_halo(pred, index, volume_shape, patch_halo)\n                        # accumulate probabilities into the output prediction array\n                        prediction_map[u_index] += u_prediction\n                        # count voxel visits for normalization\n                        normalization_mask[u_index] += 1\n\n        # save results to\n        self._save_results(prediction_maps, normalization_masks, output_heads, h5_output_file, self.loader.dataset)\n        # close the output H5 file\n        h5_output_file.close()\n\n    def _allocate_prediction_maps(self, output_shape, output_heads, output_file):\n        # initialize the output prediction arrays\n        prediction_maps = [np.zeros(output_shape, dtype=\'float32\') for _ in range(output_heads)]\n        # initialize normalization mask in order to average out probabilities of overlapping patches\n        normalization_masks = [np.zeros(output_shape, dtype=\'uint8\') for _ in range(output_heads)]\n        return prediction_maps, normalization_masks\n\n    def _save_results(self, prediction_maps, normalization_masks, output_heads, output_file, dataset):\n        def _slice_from_pad(pad):\n            if pad == 0:\n                return slice(None, None)\n            else:\n                return slice(pad, -pad)\n\n        # save probability maps\n        prediction_datasets = self._get_output_dataset_names(output_heads, prefix=\'predictions\')\n        for prediction_map, normalization_mask, prediction_dataset in zip(prediction_maps, normalization_masks,\n                                                                          prediction_datasets):\n            prediction_map = prediction_map / normalization_mask\n\n            if dataset.mirror_padding is not None:\n                z_s, y_s, x_s = [_slice_from_pad(p) for p in dataset.mirror_padding]\n\n                logger.info(f\'Dataset loaded with mirror padding: {dataset.mirror_padding}. Cropping before saving...\')\n\n                prediction_map = prediction_map[:, z_s, y_s, x_s]\n\n            logger.info(f\'Saving predictions to: {self.output_file}/{prediction_dataset}...\')\n            output_file.create_dataset(prediction_dataset, data=prediction_map, compression=""gzip"")\n\n    @staticmethod\n    def _validate_halo(patch_halo, slice_builder_config):\n        patch = slice_builder_config[\'patch_shape\']\n        stride = slice_builder_config[\'stride_shape\']\n\n        patch_overlap = np.subtract(patch, stride)\n\n        assert np.all(\n            patch_overlap - patch_halo >= 0), f""Not enough patch overlap for stride: {stride} and halo: {patch_halo}""\n\n\nclass LazyPredictor(StandardPredictor):\n    """"""\n        Applies the model on the given dataset and saves the result in the `output_file` in the H5 format.\n        Predicted patches are directly saved into the H5 and they won\'t be stored in memory. Since this predictor\n        is slower than the `StandardPredictor` it should only be used when the predicted volume does not fit into RAM.\n\n        The output dataset names inside the H5 is given by `des_dataset_name` config argument. If the argument is\n        not present in the config \'predictions{n}\' is used as a default dataset name, where `n` denotes the number\n        of the output head from the network.\n\n        Args:\n            model (Unet3D): trained 3D UNet model used for prediction\n            data_loader (torch.utils.data.DataLoader): input data loader\n            output_file (str): path to the output H5 file\n            config (dict): global config dict\n        """"""\n\n    def __init__(self, model, loader, output_file, config, **kwargs):\n        super().__init__(model, loader, output_file, config, **kwargs)\n\n    def _allocate_prediction_maps(self, output_shape, output_heads, output_file):\n        # allocate datasets for probability maps\n        prediction_datasets = self._get_output_dataset_names(output_heads, prefix=\'predictions\')\n        prediction_maps = [\n            output_file.create_dataset(dataset_name, shape=output_shape, dtype=\'float32\', chunks=True,\n                                       compression=\'gzip\')\n            for dataset_name in prediction_datasets]\n\n        # allocate datasets for normalization masks\n        normalization_datasets = self._get_output_dataset_names(output_heads, prefix=\'normalization\')\n        normalization_masks = [\n            output_file.create_dataset(dataset_name, shape=output_shape, dtype=\'uint8\', chunks=True,\n                                       compression=\'gzip\')\n            for dataset_name in normalization_datasets]\n\n        return prediction_maps, normalization_masks\n\n    def _save_results(self, prediction_maps, normalization_masks, output_heads, output_file, dataset):\n        if dataset.mirror_padding:\n            logger.warn(\n                f\'Mirror padding unsupported in LazyPredictor. Output predictions will be padded with pad_width: {dataset.pad_width}\')\n\n        prediction_datasets = self._get_output_dataset_names(output_heads, prefix=\'predictions\')\n        normalization_datasets = self._get_output_dataset_names(output_heads, prefix=\'normalization\')\n\n        # normalize the prediction_maps inside the H5\n        for prediction_map, normalization_mask, prediction_dataset, normalization_dataset in zip(prediction_maps,\n                                                                                                 normalization_masks,\n                                                                                                 prediction_datasets,\n                                                                                                 normalization_datasets):\n            # split the volume into 4 parts and load each into the memory separately\n            logger.info(f\'Normalizing {prediction_dataset}...\')\n\n            z, y, x = prediction_map.shape[1:]\n            # take slices which are 1/27 of the original volume\n            patch_shape = (z // 3, y // 3, x // 3)\n            for index in SliceBuilder._build_slices(prediction_map, patch_shape=patch_shape, stride_shape=patch_shape):\n                logger.info(f\'Normalizing slice: {index}\')\n                prediction_map[index] /= normalization_mask[index]\n                # make sure to reset the slice that has been visited already in order to avoid \'double\' normalization\n                # when the patches overlap with each other\n                normalization_mask[index] = 1\n\n            logger.info(f\'Deleting {normalization_dataset}...\')\n            del output_file[normalization_dataset]\n\n\nclass EmbeddingsPredictor(_AbstractPredictor):\n    """"""\n    Applies the embedding model on the given dataset and saves the result in the `output_file` in the H5 format.\n\n    The resulting volume is the segmentation itself (not the embedding vectors) obtained by clustering embeddings\n    with HDBSCAN or MeanShift algorithm patch by patch and then stitching the patches together.\n    """"""\n\n    def __init__(self, model, loader, output_file, config, clustering, iou_threshold=0.7, noise_label=-1, **kwargs):\n        super().__init__(model, loader, output_file, config, **kwargs)\n\n        self.iou_threshold = iou_threshold\n        self.noise_label = noise_label\n        self.clustering = clustering\n\n        assert clustering in [\'hdbscan\', \'meanshift\'], \'Only HDBSCAN and MeanShift are supported\'\n        logger.info(f\'IoU threshold: {iou_threshold}\')\n\n        self.clustering_name = clustering\n        self.clustering = self._get_clustering(clustering, kwargs)\n\n    def predict(self):\n        device = self.config[\'device\']\n        output_heads = self.config[\'model\'].get(\'output_heads\', 1)\n\n        logger.info(f\'Running prediction on {len(self.loader)} patches...\')\n\n        # dimensionality of the the output segmentation\n        volume_shape = self._volume_shape(self.loader.dataset)\n\n        logger.info(f\'The shape of the output segmentation (DHW): {volume_shape}\')\n\n        logger.info(\'Allocating segmentation array...\')\n        # initialize the output prediction arrays\n        output_segmentations = [np.zeros(volume_shape, dtype=\'int32\') for _ in range(output_heads)]\n        # initialize visited_voxels arrays\n        visited_voxels_arrays = [np.zeros(volume_shape, dtype=\'uint8\') for _ in range(output_heads)]\n\n        # Sets the module in evaluation mode explicitly\n        self.model.eval()\n        self.model.testing = True\n        # Run predictions on the entire input dataset\n        with torch.no_grad():\n            for batch, indices in self.loader:\n                # logger.info(f\'Predicting embeddings for slice:{index}\')\n\n                # send batch to device\n                batch = batch.to(device)\n                # forward pass\n                embeddings = self.model(batch)\n\n                # wrap predictions into a list if there is only one output head from the network\n                if output_heads == 1:\n                    embeddings = [embeddings]\n\n                for prediction, output_segmentation, visited_voxels_array in zip(embeddings, output_segmentations,\n                                                                                 visited_voxels_arrays):\n\n                    # convert to numpy array\n                    prediction = prediction.cpu().numpy()\n\n                    # iterate sequentially because of the current simple stitching that we\'re using\n                    for pred, index in zip(prediction, indices):\n                        # convert embeddings to segmentation with hdbscan clustering\n                        segmentation = self._embeddings_to_segmentation(pred)\n                        # stitch patches\n                        self._merge_segmentation(segmentation, index, output_segmentation, visited_voxels_array)\n\n        # save results\n        with h5py.File(self.output_file, \'w\') as output_file:\n            prediction_datasets = self._get_output_dataset_names(output_heads,\n                                                                 prefix=f\'segmentation/{self.clustering_name}\')\n            for output_segmentation, prediction_dataset in zip(output_segmentations, prediction_datasets):\n                logger.info(f\'Saving predictions to: {output_file}/{prediction_dataset}...\')\n                output_file.create_dataset(prediction_dataset, data=output_segmentation, compression=""gzip"")\n\n    def _embeddings_to_segmentation(self, embeddings):\n        """"""\n        Cluster embeddings vectors with HDBSCAN and return the segmented volume.\n\n        Args:\n            embeddings (ndarray): 4D (CDHW) embeddings tensor\n        Returns:\n            3D (DHW) segmentation\n        """"""\n        # shape of the output segmentation\n        output_shape = embeddings.shape[1:]\n        # reshape (C, D, H, W) -> (C, D * H * W) and transpose -> (D * H * W, C)\n        flattened_embeddings = embeddings.reshape(embeddings.shape[0], -1).transpose()\n\n        logger.info(\'Clustering embeddings...\')\n        # perform clustering and reshape in order to get the segmentation volume\n        start = time.time()\n        clusters = self.clustering.fit_predict(flattened_embeddings).reshape(output_shape)\n        logger.info(\n            f\'Number of clusters found by {self.clustering}: {np.max(clusters)}. Duration: {time.time() - start} sec.\')\n        return clusters\n\n    def _merge_segmentation(self, segmentation, index, output_segmentation, visited_voxels_array):\n        """"""\n        Given the `segmentation` patch, its `index` in the `output_segmentation` array and the array visited voxels\n        merge the segmented patch (`segmentation`) into the `output_segmentation`\n\n        Args:\n            segmentation (ndarray): segmented patch\n            index (tuple): position of the patch inside `output_segmentation` volume\n            output_segmentation (ndarray): current state of the output segmentation\n            visited_voxels_array (ndarray): array of voxels visited so far (same size as `output_segmentation`); visited\n                voxels will be marked by a number greater than 0\n        """"""\n        index = tuple(index)\n        # get new unassigned label\n        max_label = np.max(output_segmentation) + 1\n        # make sure there are no clashes between current segmentation patch and the output_segmentation\n        # but keep the noise label\n        noise_mask = segmentation == self.noise_label\n        segmentation += int(max_label)\n        segmentation[noise_mask] = self.noise_label\n        # get the overlap mask in the current patch\n        overlap_mask = visited_voxels_array[index] > 0\n        # get the new labels inside the overlap_mask\n        new_labels = np.unique(segmentation[overlap_mask])\n        merged_labels = self._merge_labels(output_segmentation[index], new_labels, segmentation)\n        # relabel new segmentation with the merged labels\n        for current_label, new_label in merged_labels:\n            segmentation[segmentation == new_label] = current_label\n        # update the output_segmentation\n        output_segmentation[index] = segmentation\n        # visit the patch\n        visited_voxels_array[index] += 1\n\n    def _merge_labels(self, current_segmentation, new_labels, new_segmentation):\n        def _most_frequent_label(labels):\n            unique, counts = np.unique(labels, return_counts=True)\n            ind = np.argmax(counts)\n            return unique[ind]\n\n        result = []\n        # iterate over new_labels and merge regions if the IoU exceeds a given threshold\n        for new_label in new_labels:\n            # skip \'noise\' label assigned by hdbscan\n            if new_label == self.noise_label:\n                continue\n            new_label_mask = new_segmentation == new_label\n            # get only the most frequent overlapping label\n            most_frequent_label = _most_frequent_label(current_segmentation[new_label_mask])\n            # skip \'noise\' label\n            if most_frequent_label == self.noise_label:\n                continue\n            current_label_mask = current_segmentation == most_frequent_label\n            # compute Jaccard index\n            iou = np.bitwise_and(new_label_mask, current_label_mask).sum() / np.bitwise_or(new_label_mask,\n                                                                                           current_label_mask).sum()\n            if iou > self.iou_threshold:\n                # merge labels\n                result.append((most_frequent_label, new_label))\n\n        return result\n\n    def _get_clustering(self, clustering_alg, kwargs):\n        logger.info(f\'Using {clustering_alg} for clustering\')\n\n        if clustering_alg == \'hdbscan\':\n            min_cluster_size = kwargs.get(\'min_cluster_size\', 50)\n            min_samples = kwargs.get(\'min_samples\', None),\n            metric = kwargs.get(\'metric\', \'euclidean\')\n            cluster_selection_method = kwargs.get(\'cluster_selection_method\', \'eom\')\n\n            logger.info(f\'HDBSCAN params: min_cluster_size: {min_cluster_size}, min_samples: {min_samples}\')\n            return hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric=metric,\n                                   cluster_selection_method=cluster_selection_method)\n        else:\n            bandwidth = kwargs[\'bandwidth\']\n            logger.info(f\'MeanShift params: bandwidth: {bandwidth}, bin_seeding: True\')\n            # use fast MeanShift with bin seeding\n            return MeanShift(bandwidth=bandwidth, bin_seeding=True)\n'"
pytorch3dunet/unet3d/seg_metrics.py,0,"b'import numpy as np\nfrom skimage.metrics import contingency_table\n\n\ndef precision(tp, fp, fn):\n    return tp / (tp + fp) if tp > 0 else 0\n\n\ndef recall(tp, fp, fn):\n    return tp / (tp + fn) if tp > 0 else 0\n\n\ndef accuracy(tp, fp, fn):\n    return tp / (tp + fp + fn) if tp > 0 else 0\n\n\ndef f1(tp, fp, fn):\n    return (2 * tp) / (2 * tp + fp + fn) if tp > 0 else 0\n\n\ndef _relabel(input):\n    _, unique_labels = np.unique(input, return_inverse=True)\n    return unique_labels.reshape(input.shape)\n\n\ndef _iou_matrix(gt, seg):\n    # relabel gt and seg for smaller memory footprint of contingency table\n    gt = _relabel(gt)\n    seg = _relabel(seg)\n\n    # get number of overlapping pixels between GT and SEG\n    n_inter = contingency_table(gt, seg).A\n\n    # number of pixels for GT instances\n    n_gt = n_inter.sum(axis=1, keepdims=True)\n    # number of pixels for SEG instances\n    n_seg = n_inter.sum(axis=0, keepdims=True)\n\n    # number of pixels in the union between GT and SEG instances\n    n_union = n_gt + n_seg - n_inter\n\n    iou_matrix = n_inter / n_union\n    # make sure that the values are within [0,1] range\n    assert 0 <= np.min(iou_matrix) <= np.max(iou_matrix) <= 1\n\n    return iou_matrix\n\n\nclass SegmentationMetrics:\n    """"""\n    Computes precision, recall, accuracy, f1 score for a given ground truth and predicted segmentation.\n    Contingency table for a given ground truth and predicted segmentation is computed eagerly upon construction\n    of the instance of `SegmentationMetrics`.\n\n    Args:\n        gt (ndarray): ground truth segmentation\n        seg (ndarray): predicted segmentation\n    """"""\n\n    def __init__(self, gt, seg):\n        self.iou_matrix = _iou_matrix(gt, seg)\n\n    def metrics(self, iou_threshold):\n        """"""\n        Computes precision, recall, accuracy, f1 score at a given IoU threshold\n        """"""\n        # ignore background\n        iou_matrix = self.iou_matrix[1:, 1:]\n        detection_matrix = (iou_matrix > iou_threshold).astype(np.uint8)\n        n_gt, n_seg = detection_matrix.shape\n\n        # if the iou_matrix is empty or all values are 0\n        trivial = min(n_gt, n_seg) == 0 or np.all(detection_matrix == 0)\n        if trivial:\n            tp = fp = fn = 0\n        else:\n            # count non-zero rows to get the number of TP\n            tp = np.count_nonzero(detection_matrix.sum(axis=1))\n            # count zero rows to get the number of FN\n            fn = n_gt - tp\n            # count zero columns to get the number of FP\n            fp = n_seg - np.count_nonzero(detection_matrix.sum(axis=0))\n\n        return {\n            \'precision\': precision(tp, fp, fn),\n            \'recall\': recall(tp, fp, fn),\n            \'accuracy\': accuracy(tp, fp, fn),\n            \'f1\': f1(tp, fp, fn)\n        }\n\n\nclass Accuracy:\n    """"""\n    Computes accuracy between ground truth and predicted segmentation a a given threshold value.\n    Defined as: AC = TP / (TP + FP + FN).\n    Kaggle DSB2018 calls it Precision, see:\n    https://www.kaggle.com/stkbailey/step-by-step-explanation-of-scoring-metric.\n    """"""\n\n    def __init__(self, iou_threshold):\n        self.iou_threshold = iou_threshold\n\n    def __call__(self, input_seg, gt_seg):\n        metrics = SegmentationMetrics(gt_seg, input_seg).metrics(self.iou_threshold)\n        return metrics[\'accuracy\']\n\n\nclass AveragePrecision:\n    """"""\n    Average precision taken for the IoU range (0.5, 0.95) with a step of 0.05 as defined in:\n    https://www.kaggle.com/stkbailey/step-by-step-explanation-of-scoring-metric\n    """"""\n\n    def __init__(self):\n        self.iou_range = np.linspace(0.50, 0.95, 10)\n\n    def __call__(self, input_seg, gt_seg):\n        # compute contingency_table\n        sm = SegmentationMetrics(gt_seg, input_seg)\n        # compute accuracy for each threshold\n        acc = [sm.metrics(iou)[\'accuracy\'] for iou in self.iou_range]\n        # return the average\n        return np.mean(acc)\n'"
pytorch3dunet/unet3d/trainer.py,8,"b'import os\n\nimport torch\nimport torch.nn as nn\nfrom tensorboardX import SummaryWriter\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom pytorch3dunet.unet3d.utils import get_logger\nfrom . import utils\n\nlogger = get_logger(\'UNet3DTrainer\')\n\n\nclass UNet3DTrainer:\n    """"""3D UNet trainer.\n\n    Args:\n        model (Unet3D): UNet 3D model to be trained\n        optimizer (nn.optim.Optimizer): optimizer used for training\n        lr_scheduler (torch.optim.lr_scheduler._LRScheduler): learning rate scheduler\n            WARN: bear in mind that lr_scheduler.step() is invoked after every validation step\n            (i.e. validate_after_iters) not after every epoch. So e.g. if one uses StepLR with step_size=30\n            the learning rate will be adjusted after every 30 * validate_after_iters iterations.\n        loss_criterion (callable): loss function\n        eval_criterion (callable): used to compute training/validation metric (such as Dice, IoU, AP or Rand score)\n            saving the best checkpoint is based on the result of this function on the validation set\n        device (torch.device): device to train on\n        loaders (dict): \'train\' and \'val\' loaders\n        checkpoint_dir (string): dir for saving checkpoints and tensorboard logs\n        max_num_epochs (int): maximum number of epochs\n        max_num_iterations (int): maximum number of iterations\n        validate_after_iters (int): validate after that many iterations\n        log_after_iters (int): number of iterations before logging to tensorboard\n        validate_iters (int): number of validation iterations, if None validate\n            on the whole validation set\n        eval_score_higher_is_better (bool): if True higher eval scores are considered better\n        best_eval_score (float): best validation score so far (higher better)\n        num_iterations (int): useful when loading the model from the checkpoint\n        num_epoch (int): useful when loading the model from the checkpoint\n        tensorboard_formatter (callable): converts a given batch of input/output/target image to a series of images\n            that can be displayed in tensorboard\n        skip_train_validation (bool): if True eval_criterion is not evaluated on the training set (used mostly when\n            evaluation is expensive)\n    """"""\n\n    def __init__(self, model, optimizer, lr_scheduler, loss_criterion,\n                 eval_criterion, device, loaders, checkpoint_dir,\n                 max_num_epochs=100, max_num_iterations=1e5,\n                 validate_after_iters=100, log_after_iters=100,\n                 validate_iters=None, num_iterations=1, num_epoch=0,\n                 eval_score_higher_is_better=True, best_eval_score=None,\n                 tensorboard_formatter=None, skip_train_validation=False):\n\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = lr_scheduler\n        self.loss_criterion = loss_criterion\n        self.eval_criterion = eval_criterion\n        self.device = device\n        self.loaders = loaders\n        self.checkpoint_dir = checkpoint_dir\n        self.max_num_epochs = max_num_epochs\n        self.max_num_iterations = max_num_iterations\n        self.validate_after_iters = validate_after_iters\n        self.log_after_iters = log_after_iters\n        self.validate_iters = validate_iters\n        self.eval_score_higher_is_better = eval_score_higher_is_better\n\n        logger.info(model)\n        logger.info(f\'eval_score_higher_is_better: {eval_score_higher_is_better}\')\n\n        if best_eval_score is not None:\n            self.best_eval_score = best_eval_score\n        else:\n            # initialize the best_eval_score\n            if eval_score_higher_is_better:\n                self.best_eval_score = float(\'-inf\')\n            else:\n                self.best_eval_score = float(\'+inf\')\n\n        self.writer = SummaryWriter(log_dir=os.path.join(checkpoint_dir, \'logs\'))\n\n        assert tensorboard_formatter is not None, \'TensorboardFormatter must be provided\'\n        self.tensorboard_formatter = tensorboard_formatter\n\n        self.num_iterations = num_iterations\n        self.num_epoch = num_epoch\n        self.skip_train_validation = skip_train_validation\n\n    @classmethod\n    def from_checkpoint(cls, checkpoint_path, model, optimizer, lr_scheduler, loss_criterion, eval_criterion, loaders,\n                        tensorboard_formatter=None, skip_train_validation=False):\n        logger.info(f""Loading checkpoint \'{checkpoint_path}\'..."")\n        state = utils.load_checkpoint(checkpoint_path, model, optimizer)\n        logger.info(\n            f""Checkpoint loaded. Epoch: {state[\'epoch\']}. Best val score: {state[\'best_eval_score\']}. Num_iterations: {state[\'num_iterations\']}"")\n        checkpoint_dir = os.path.split(checkpoint_path)[0]\n        return cls(model, optimizer, lr_scheduler,\n                   loss_criterion, eval_criterion,\n                   torch.device(state[\'device\']),\n                   loaders, checkpoint_dir,\n                   eval_score_higher_is_better=state[\'eval_score_higher_is_better\'],\n                   best_eval_score=state[\'best_eval_score\'],\n                   num_iterations=state[\'num_iterations\'],\n                   num_epoch=state[\'epoch\'],\n                   max_num_epochs=state[\'max_num_epochs\'],\n                   max_num_iterations=state[\'max_num_iterations\'],\n                   validate_after_iters=state[\'validate_after_iters\'],\n                   log_after_iters=state[\'log_after_iters\'],\n                   validate_iters=state[\'validate_iters\'],\n                   tensorboard_formatter=tensorboard_formatter,\n                   skip_train_validation=skip_train_validation)\n\n    @classmethod\n    def from_pretrained(cls, pre_trained, model, optimizer, lr_scheduler, loss_criterion, eval_criterion,\n                        device, loaders,\n                        max_num_epochs=100, max_num_iterations=1e5,\n                        validate_after_iters=100, log_after_iters=100,\n                        validate_iters=None, num_iterations=1, num_epoch=0,\n                        eval_score_higher_is_better=True, best_eval_score=None,\n                        tensorboard_formatter=None, skip_train_validation=False):\n        logger.info(f""Logging pre-trained model from \'{pre_trained}\'..."")\n        utils.load_checkpoint(pre_trained, model, None)\n        checkpoint_dir = os.path.split(pre_trained)[0]\n        return cls(model, optimizer, lr_scheduler,\n                   loss_criterion, eval_criterion,\n                   device, loaders, checkpoint_dir,\n                   eval_score_higher_is_better=eval_score_higher_is_better,\n                   best_eval_score=best_eval_score,\n                   num_iterations=num_iterations,\n                   num_epoch=num_epoch,\n                   max_num_epochs=max_num_epochs,\n                   max_num_iterations=max_num_iterations,\n                   validate_after_iters=validate_after_iters,\n                   log_after_iters=log_after_iters,\n                   validate_iters=validate_iters,\n                   tensorboard_formatter=tensorboard_formatter,\n                   skip_train_validation=skip_train_validation)\n\n    def fit(self):\n        for _ in range(self.num_epoch, self.max_num_epochs):\n            # train for one epoch\n            should_terminate = self.train(self.loaders[\'train\'])\n\n            if should_terminate:\n                logger.info(\'Stopping criterion is satisfied. Finishing training\')\n                return\n\n            self.num_epoch += 1\n        logger.info(f""Reached maximum number of epochs: {self.max_num_epochs}. Finishing training..."")\n\n    def train(self, train_loader):\n        """"""Trains the model for 1 epoch.\n\n        Args:\n            train_loader (torch.utils.data.DataLoader): training data loader\n\n        Returns:\n            True if the training should be terminated immediately, False otherwise\n        """"""\n        train_losses = utils.RunningAverage()\n        train_eval_scores = utils.RunningAverage()\n\n        # sets the model in training mode\n        self.model.train()\n\n        for i, t in enumerate(train_loader):\n            logger.info(\n                f\'Training iteration {self.num_iterations}. Batch {i}. Epoch [{self.num_epoch}/{self.max_num_epochs - 1}]\')\n\n            input, target, weight = self._split_training_batch(t)\n\n            output, loss = self._forward_pass(input, target, weight)\n\n            train_losses.update(loss.item(), self._batch_size(input))\n\n            # compute gradients and update parameters\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            if self.num_iterations % self.validate_after_iters == 0:\n                # set the model in eval mode\n                self.model.eval()\n                # evaluate on validation set\n                eval_score = self.validate(self.loaders[\'val\'])\n                # set the model back to training mode\n                self.model.train()\n\n                # adjust learning rate if necessary\n                if isinstance(self.scheduler, ReduceLROnPlateau):\n                    self.scheduler.step(eval_score)\n                else:\n                    self.scheduler.step()\n                # log current learning rate in tensorboard\n                self._log_lr()\n                # remember best validation metric\n                is_best = self._is_best_eval_score(eval_score)\n\n                # save checkpoint\n                self._save_checkpoint(is_best)\n\n            if self.num_iterations % self.log_after_iters == 0:\n                # if model contains final_activation layer for normalizing logits apply it, otherwise both\n                # the evaluation metric as well as images in tensorboard will be incorrectly computed\n                if hasattr(self.model, \'final_activation\') and self.model.final_activation is not None:\n                    output = self.model.final_activation(output)\n\n                # compute eval criterion\n                if not self.skip_train_validation:\n                    eval_score = self.eval_criterion(output, target)\n                    train_eval_scores.update(eval_score.item(), self._batch_size(input))\n\n                # log stats, params and images\n                logger.info(\n                    f\'Training stats. Loss: {train_losses.avg}. Evaluation score: {train_eval_scores.avg}\')\n                self._log_stats(\'train\', train_losses.avg, train_eval_scores.avg)\n                self._log_params()\n                self._log_images(input, target, output, \'train_\')\n\n            if self.should_stop():\n                return True\n\n            self.num_iterations += 1\n\n        return False\n\n    def should_stop(self):\n        """"""\n        Training will terminate if maximum number of iterations is exceeded or the learning rate drops below\n        some predefined threshold (1e-6 in our case)\n        """"""\n        if self.max_num_iterations < self.num_iterations:\n            logger.info(f\'Maximum number of iterations {self.max_num_iterations} exceeded.\')\n            return True\n\n        min_lr = 1e-6\n        lr = self.optimizer.param_groups[0][\'lr\']\n        if lr < min_lr:\n            logger.info(f\'Learning rate below the minimum {min_lr}.\')\n            return True\n\n        return False\n\n    def validate(self, val_loader):\n        logger.info(\'Validating...\')\n\n        val_losses = utils.RunningAverage()\n        val_scores = utils.RunningAverage()\n\n        with torch.no_grad():\n            for i, t in enumerate(val_loader):\n                logger.info(f\'Validation iteration {i}\')\n\n                input, target, weight = self._split_training_batch(t)\n\n                output, loss = self._forward_pass(input, target, weight)\n                val_losses.update(loss.item(), self._batch_size(input))\n\n                # if model contains final_activation layer for normalizing logits apply it, otherwise\n                # the evaluation metric will be incorrectly computed\n                if hasattr(self.model, \'final_activation\') and self.model.final_activation is not None:\n                    output = self.model.final_activation(output)\n\n                if i % 100 == 0:\n                    self._log_images(input, target, output, \'val_\')\n\n                eval_score = self.eval_criterion(output, target)\n                val_scores.update(eval_score.item(), self._batch_size(input))\n\n                if self.validate_iters is not None and self.validate_iters <= i:\n                    # stop validation\n                    break\n\n            self._log_stats(\'val\', val_losses.avg, val_scores.avg)\n            logger.info(f\'Validation finished. Loss: {val_losses.avg}. Evaluation score: {val_scores.avg}\')\n            return val_scores.avg\n\n    def _split_training_batch(self, t):\n        def _move_to_device(input):\n            if isinstance(input, tuple) or isinstance(input, list):\n                return tuple([_move_to_device(x) for x in input])\n            else:\n                return input.to(self.device)\n\n        t = _move_to_device(t)\n        weight = None\n        if len(t) == 2:\n            input, target = t\n        else:\n            input, target, weight = t\n        return input, target, weight\n\n    def _forward_pass(self, input, target, weight=None):\n        # forward pass\n        output = self.model(input)\n\n        # compute the loss\n        if weight is None:\n            loss = self.loss_criterion(output, target)\n        else:\n            loss = self.loss_criterion(output, target, weight)\n\n        return output, loss\n\n    def _is_best_eval_score(self, eval_score):\n        if self.eval_score_higher_is_better:\n            is_best = eval_score > self.best_eval_score\n        else:\n            is_best = eval_score < self.best_eval_score\n\n        if is_best:\n            logger.info(f\'Saving new best evaluation metric: {eval_score}\')\n            self.best_eval_score = eval_score\n\n        return is_best\n\n    def _save_checkpoint(self, is_best):\n        # remove `module` prefix from layer names when using `nn.DataParallel`\n        # see: https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/20\n        if isinstance(self.model, nn.DataParallel):\n            state_dict = self.model.module.state_dict()\n        else:\n            state_dict = self.model.state_dict()\n\n        utils.save_checkpoint({\n            \'epoch\': self.num_epoch + 1,\n            \'num_iterations\': self.num_iterations,\n            \'model_state_dict\': state_dict,\n            \'best_eval_score\': self.best_eval_score,\n            \'eval_score_higher_is_better\': self.eval_score_higher_is_better,\n            \'optimizer_state_dict\': self.optimizer.state_dict(),\n            \'device\': str(self.device),\n            \'max_num_epochs\': self.max_num_epochs,\n            \'max_num_iterations\': self.max_num_iterations,\n            \'validate_after_iters\': self.validate_after_iters,\n            \'log_after_iters\': self.log_after_iters,\n            \'validate_iters\': self.validate_iters\n        }, is_best, checkpoint_dir=self.checkpoint_dir,\n            logger=logger)\n\n    def _log_lr(self):\n        lr = self.optimizer.param_groups[0][\'lr\']\n        self.writer.add_scalar(\'learning_rate\', lr, self.num_iterations)\n\n    def _log_stats(self, phase, loss_avg, eval_score_avg):\n        tag_value = {\n            f\'{phase}_loss_avg\': loss_avg,\n            f\'{phase}_eval_score_avg\': eval_score_avg\n        }\n\n        for tag, value in tag_value.items():\n            self.writer.add_scalar(tag, value, self.num_iterations)\n\n    def _log_params(self):\n        logger.info(\'Logging model parameters and gradients\')\n        for name, value in self.model.named_parameters():\n            self.writer.add_histogram(name, value.data.cpu().numpy(), self.num_iterations)\n            self.writer.add_histogram(name + \'/grad\', value.grad.data.cpu().numpy(), self.num_iterations)\n\n    def _log_images(self, input, target, prediction, prefix=\'\'):\n        inputs_map = {\n            \'inputs\': input,\n            \'targets\': target,\n            \'predictions\': prediction\n        }\n        img_sources = {}\n        for name, batch in inputs_map.items():\n            if isinstance(batch, list) or isinstance(batch, tuple):\n                for i, b in enumerate(batch):\n                    img_sources[f\'{name}{i}\'] = b.data.cpu().numpy()\n            else:\n                img_sources[name] = batch.data.cpu().numpy()\n\n        for name, batch in img_sources.items():\n            for tag, image in self.tensorboard_formatter(name, batch):\n                self.writer.add_image(prefix + tag, image, self.num_iterations, dataformats=\'CHW\')\n\n    @staticmethod\n    def _batch_size(input):\n        if isinstance(input, list) or isinstance(input, tuple):\n            return input[0].size(0)\n        else:\n            return input.size(0)\n'"
pytorch3dunet/unet3d/utils.py,11,"b'import importlib\nimport io\nimport logging\nimport os\nimport shutil\nimport sys\nimport uuid\n\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom sklearn.decomposition import PCA\n\nplt.ioff()\nplt.switch_backend(\'agg\')\n\n\ndef save_checkpoint(state, is_best, checkpoint_dir, logger=None):\n    """"""Saves model and training parameters at \'{checkpoint_dir}/last_checkpoint.pytorch\'.\n    If is_best==True saves \'{checkpoint_dir}/best_checkpoint.pytorch\' as well.\n\n    Args:\n        state (dict): contains model\'s state_dict, optimizer\'s state_dict, epoch\n            and best evaluation metric value so far\n        is_best (bool): if True state contains the best model seen so far\n        checkpoint_dir (string): directory where the checkpoint are to be saved\n    """"""\n\n    def log_info(message):\n        if logger is not None:\n            logger.info(message)\n\n    if not os.path.exists(checkpoint_dir):\n        log_info(\n            f""Checkpoint directory does not exists. Creating {checkpoint_dir}"")\n        os.mkdir(checkpoint_dir)\n\n    last_file_path = os.path.join(checkpoint_dir, \'last_checkpoint.pytorch\')\n    log_info(f""Saving last checkpoint to \'{last_file_path}\'"")\n    torch.save(state, last_file_path)\n    if is_best:\n        best_file_path = os.path.join(checkpoint_dir, \'best_checkpoint.pytorch\')\n        log_info(f""Saving best checkpoint to \'{best_file_path}\'"")\n        shutil.copyfile(last_file_path, best_file_path)\n\n\ndef load_checkpoint(checkpoint_path, model, optimizer=None):\n    """"""Loads model and training parameters from a given checkpoint_path\n    If optimizer is provided, loads optimizer\'s state_dict of as well.\n\n    Args:\n        checkpoint_path (string): path to the checkpoint to be loaded\n        model (torch.nn.Module): model into which the parameters are to be copied\n        optimizer (torch.optim.Optimizer) optional: optimizer instance into\n            which the parameters are to be copied\n\n    Returns:\n        state\n    """"""\n    if not os.path.exists(checkpoint_path):\n        raise IOError(f""Checkpoint \'{checkpoint_path}\' does not exist"")\n\n    state = torch.load(checkpoint_path, map_location=\'cpu\')\n    model.load_state_dict(state[\'model_state_dict\'])\n\n    if optimizer is not None:\n        optimizer.load_state_dict(state[\'optimizer_state_dict\'])\n\n    return state\n\n\ndef save_network_output(output_path, output, logger=None):\n    if logger is not None:\n        logger.info(f\'Saving network output to: {output_path}...\')\n    output = output.detach().cpu()[0]\n    with h5py.File(output_path, \'w\') as f:\n        f.create_dataset(\'predictions\', data=output, compression=\'gzip\')\n\n\nloggers = {}\n\n\ndef get_logger(name, level=logging.INFO):\n    global loggers\n    if loggers.get(name) is not None:\n        return loggers[name]\n    else:\n        logger = logging.getLogger(name)\n        logger.setLevel(level)\n        # Logging to console\n        stream_handler = logging.StreamHandler(sys.stdout)\n        formatter = logging.Formatter(\n            \'%(asctime)s [%(threadName)s] %(levelname)s %(name)s - %(message)s\')\n        stream_handler.setFormatter(formatter)\n        logger.addHandler(stream_handler)\n\n        loggers[name] = logger\n\n        return logger\n\n\ndef get_number_of_learnable_parameters(model):\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    return sum([np.prod(p.size()) for p in model_parameters])\n\n\nclass RunningAverage:\n    """"""Computes and stores the average\n    """"""\n\n    def __init__(self):\n        self.count = 0\n        self.sum = 0\n        self.avg = 0\n\n    def update(self, value, n=1):\n        self.count += n\n        self.sum += value * n\n        self.avg = self.sum / self.count\n\n\ndef find_maximum_patch_size(model, device):\n    """"""Tries to find the biggest patch size that can be send to GPU for inference\n    without throwing CUDA out of memory""""""\n    logger = get_logger(\'PatchFinder\')\n    in_channels = model.in_channels\n\n    patch_shapes = [(64, 128, 128), (96, 128, 128),\n                    (64, 160, 160), (96, 160, 160),\n                    (64, 192, 192), (96, 192, 192)]\n\n    for shape in patch_shapes:\n        # generate random patch of a given size\n        patch = np.random.randn(*shape).astype(\'float32\')\n\n        patch = torch \\\n            .from_numpy(patch) \\\n            .view((1, in_channels) + patch.shape) \\\n            .to(device)\n\n        logger.info(f""Current patch size: {shape}"")\n        model(patch)\n\n\ndef remove_halo(patch, index, shape, patch_halo):\n    """"""\n    Remove `pad_width` voxels around the edges of a given patch.\n    """"""\n    assert len(patch_halo) == 3\n\n    def _new_slices(slicing, max_size, pad):\n        if slicing.start == 0:\n            p_start = 0\n            i_start = 0\n        else:\n            p_start = pad\n            i_start = slicing.start + pad\n\n        if slicing.stop == max_size:\n            p_stop = None\n            i_stop = max_size\n        else:\n            p_stop = -pad if pad != 0 else 1\n            i_stop = slicing.stop - pad\n\n        return slice(p_start, p_stop), slice(i_start, i_stop)\n\n    D, H, W = shape\n\n    i_c, i_z, i_y, i_x = index\n    p_c = slice(0, patch.shape[0])\n\n    p_z, i_z = _new_slices(i_z, D, patch_halo[0])\n    p_y, i_y = _new_slices(i_y, H, patch_halo[1])\n    p_x, i_x = _new_slices(i_x, W, patch_halo[2])\n\n    patch_index = (p_c, p_z, p_y, p_x)\n    index = (i_c, i_z, i_y, i_x)\n    return patch[patch_index], index\n\n\ndef number_of_features_per_level(init_channel_number, num_levels):\n    return [init_channel_number * 2 ** k for k in range(num_levels)]\n\n\nclass _TensorboardFormatter:\n    """"""\n    Tensorboard formatters converts a given batch of images (be it input/output to the network or the target segmentation\n    image) to a series of images that can be displayed in tensorboard. This is the parent class for all tensorboard\n    formatters which ensures that returned images are in the \'CHW\' format.\n    """"""\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, name, batch):\n        """"""\n        Transform a batch to a series of tuples of the form (tag, img), where `tag` corresponds to the image tag\n        and `img` is the image itself.\n\n        Args:\n             name (str): one of \'inputs\'/\'targets\'/\'predictions\'\n             batch (torch.tensor): 4D or 5D torch tensor\n        """"""\n\n        def _check_img(tag_img):\n            tag, img = tag_img\n\n            assert img.ndim == 2 or img.ndim == 3, \'Only 2D (HW) and 3D (CHW) images are accepted for display\'\n\n            if img.ndim == 2:\n                img = np.expand_dims(img, axis=0)\n            else:\n                C = img.shape[0]\n                assert C == 1 or C == 3, \'Only (1, H, W) or (3, H, W) images are supported\'\n\n            return tag, img\n\n        assert name in [\'inputs\', \'targets\', \'predictions\']\n\n        tagged_images = self.process_batch(name, batch)\n\n        return list(map(_check_img, tagged_images))\n\n    def process_batch(self, name, batch):\n        raise NotImplementedError\n\n\nclass DefaultTensorboardFormatter(_TensorboardFormatter):\n    def __init__(self, skip_last_target=False, **kwargs):\n        super().__init__(**kwargs)\n        self.skip_last_target = skip_last_target\n\n    def process_batch(self, name, batch):\n        if name == \'targets\' and self.skip_last_target:\n            batch = batch[:, :-1, ...]\n\n        tag_template = \'{}/batch_{}/channel_{}/slice_{}\'\n\n        tagged_images = []\n\n        if batch.ndim == 5:\n            # NCDHW\n            slice_idx = batch.shape[2] // 2  # get the middle slice\n            for batch_idx in range(batch.shape[0]):\n                for channel_idx in range(batch.shape[1]):\n                    tag = tag_template.format(name, batch_idx, channel_idx, slice_idx)\n                    img = batch[batch_idx, channel_idx, slice_idx, ...]\n                    tagged_images.append((tag, self._normalize_img(img)))\n        else:\n            # batch hafrom sklearn.decomposition import PCAs no channel dim: NDHW\n            slice_idx = batch.shape[1] // 2  # get the middle slice\n            for batch_idx in range(batch.shape[0]):\n                tag = tag_template.format(name, batch_idx, 0, slice_idx)\n                img = batch[batch_idx, slice_idx, ...]\n                tagged_images.append((tag, self._normalize_img(img)))\n\n        return tagged_images\n\n    @staticmethod\n    def _normalize_img(img):\n        return np.nan_to_num((img - np.min(img)) / np.ptp(img))\n\n\nclass EmbeddingsTensorboardFormatter(DefaultTensorboardFormatter):\n    def __init__(self, plot_variance=False, **kwargs):\n        super().__init__(**kwargs)\n        self.plot_variance = plot_variance\n\n    def process_batch(self, name, batch):\n        if name == \'inputs\':\n            assert batch.ndim == 5\n            # skip coordinate channels and take only the first \'raw\' channel\n            batch = batch[:, 0, ...]\n            return super().process_batch(name, batch)\n        elif name == \'predictions\':\n            return self._embeddings_to_rgb(batch)\n        else:\n            return super().process_batch(name, batch)\n\n    def _embeddings_to_rgb(self, batch):\n        assert batch.ndim == 5\n\n        tag_template = \'embeddings/batch_{}/slice_{}\'\n        tagged_images = []\n\n        slice_idx = batch.shape[2] // 2  # get the middle slice\n        for batch_idx in range(batch.shape[0]):\n            tag = tag_template.format(batch_idx, slice_idx)\n            img = batch[batch_idx, :, slice_idx, ...]  # CHW\n            rgb_img = self._pca_project(img)\n            tagged_images.append((tag, rgb_img))\n            if self.plot_variance:\n                cum_explained_variance_img = self._plot_cum_explained_variance(img)\n                tagged_images.append((f\'cumulative_explained_variance/batch_{batch_idx}\', cum_explained_variance_img))\n\n        return tagged_images\n\n    def _pca_project(self, embeddings):\n        assert embeddings.ndim == 3\n        # reshape (C, H, W) -> (C, H * W) and transpose\n        flattened_embeddings = embeddings.reshape(embeddings.shape[0], -1).transpose()\n        # init PCA with 3 principal components: one for each RGB channel\n        pca = PCA(n_components=3)\n        # fit the model with embeddings and apply the dimensionality reduction\n        flattened_embeddings = pca.fit_transform(flattened_embeddings)\n        # reshape back to original\n        shape = list(embeddings.shape)\n        shape[0] = 3\n        img = flattened_embeddings.transpose().reshape(shape)\n        # normalize to [0, 255]\n        img = 255 * (img - np.min(img)) / np.ptp(img)\n        return img.astype(\'uint8\')\n\n    def _plot_cum_explained_variance(self, embeddings):\n        # reshape (C, H, W) -> (C, H * W) and transpose\n        flattened_embeddings = embeddings.reshape(embeddings.shape[0], -1).transpose()\n        # fit PCA to the data\n        pca = PCA().fit(flattened_embeddings)\n\n        plt.figure()\n        # plot cumulative explained variance ratio\n        plt.plot(np.cumsum(pca.explained_variance_ratio_))\n        plt.xlabel(\'number of components\')\n        plt.ylabel(\'cumulative explained variance\');\n        buf = io.BytesIO()\n        plt.savefig(buf, format=\'jpeg\')\n        buf.seek(0)\n        img = np.asarray(Image.open(buf)).transpose(2, 0, 1)\n        plt.close(\'all\')\n        return img\n\n\ndef get_tensorboard_formatter(config):\n    if config is None:\n        return DefaultTensorboardFormatter()\n\n    class_name = config[\'name\']\n    m = importlib.import_module(\'pytorch3dunet.unet3d.utils\')\n    clazz = getattr(m, class_name)\n    return clazz(**config)\n\n\ndef expand_as_one_hot(input, C, ignore_index=None):\n    """"""\n    Converts NxDxHxW label image to NxCxDxHxW, where each label gets converted to its corresponding one-hot vector\n    :param input: 4D input image (NxDxHxW)\n    :param C: number of channels/labels\n    :param ignore_index: ignore index to be kept during the expansion\n    :return: 5D output image (NxCxDxHxW)\n    """"""\n    assert input.dim() == 4\n\n    # expand the input tensor to Nx1xDxHxW before scattering\n    input = input.unsqueeze(1)\n    # create result tensor shape (NxCxDxHxW)\n    shape = list(input.size())\n    shape[1] = C\n\n    if ignore_index is not None:\n        # create ignore_index mask for the result\n        mask = input.expand(shape) == ignore_index\n        # clone the src tensor and zero out ignore_index in the input\n        input = input.clone()\n        input[input == ignore_index] = 0\n        # scatter to get the one-hot tensor\n        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n        # bring back the ignore_index in the result\n        result[mask] = ignore_index\n        return result\n    else:\n        # scatter to get the one-hot tensor\n        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n\n\ndef plot_segm(segm, ground_truth, plots_dir=\'.\'):\n    """"""\n    Saves predicted and ground truth segmentation into a PNG files (one per channel).\n\n    :param segm: 4D ndarray (CDHW)\n    :param ground_truth: 4D ndarray (CDHW)\n    :param plots_dir: directory where to save the plots\n    """"""\n    assert segm.ndim == 4\n    if ground_truth.ndim == 3:\n        stacked = [ground_truth for _ in range(segm.shape[0])]\n        ground_truth = np.stack(stacked)\n\n    assert ground_truth.ndim == 4\n\n    f, axarr = plt.subplots(1, 2)\n\n    for seg, gt in zip(segm, ground_truth):\n        mid_z = seg.shape[0] // 2\n\n        axarr[0].imshow(seg[mid_z], cmap=\'prism\')\n        axarr[0].set_title(\'Predicted segmentation\')\n\n        axarr[1].imshow(gt[mid_z], cmap=\'prism\')\n        axarr[1].set_title(\'Ground truth segmentation\')\n\n        file_name = f\'segm_{str(uuid.uuid4())[:8]}.png\'\n        plt.savefig(os.path.join(plots_dir, file_name))\n\n\ndef convert_to_numpy(input, target):\n    """"""\n    Coverts input and target torch tensors to numpy ndarrays\n\n    Args:\n        input (torch.Tensor): 5D torch tensor\n        target (torch.Tensor): 5D torch tensor\n\n    Returns:\n        tuple (input, target) tensors\n    """"""\n    assert isinstance(input, torch.Tensor), ""Expected input to be torch.Tensor""\n    assert isinstance(target, torch.Tensor), ""Expected target to be torch.Tensor""\n\n    input = input.detach().cpu().numpy()  # 5D\n    target = target.detach().cpu().numpy()  # 5D\n\n    return input, target\n'"
tests/test_bioimage-io/conftest.py,0,"b'import os\nfrom pathlib import Path\n\nimport pytest\n\n\n@pytest.fixture\ndef cache_path(tmp_path):\n    return Path(os.getenv(""PYBIO_CACHE_PATH"", tmp_path))\n'"
tests/test_bioimage-io/test_UNet3DArabidopsisOvules.py,2,"b'from io import BytesIO\nfrom pathlib import Path\n\nimport h5py\nimport imageio\nimport numpy\nimport pytest\nimport torch\n\nfrom pybio.core.transformations import apply_transformations\nfrom pybio.spec import load_model\nfrom pybio.spec.utils import get_instance\nfrom pytorch3dunet.unet3d.model import UNet3D\n\n\n@pytest.fixture\ndef dummy_input():\n    return [numpy.random.uniform(-2, 2, [1, 1, 80, 160, 160]).astype(numpy.float32)]\n\n\ndef test_dummy_input(cache_path, dummy_input):\n    spec_path = (\n        Path(__file__).parent / ""../../bioimage-io/UNet3DArabidopsisOvules.model/UNet3DArabidopsisOvules.model.yaml""\n    )\n    assert spec_path.exists()\n\n    pybio_model = load_model(str(spec_path), cache_path=cache_path)\n    for dummy, spec in zip(dummy_input, pybio_model.spec.inputs):\n        assert str(dummy.dtype) == spec.data_type\n        assert dummy.shape == spec.shape\n\n\ndef test_Net3DArabidopsisOvules_forward(cache_path):\n    spec_path = (\n        Path(__file__).parent / ""../../bioimage-io/UNet3DArabidopsisOvules.model/UNet3DArabidopsisOvules.model.yaml""\n    ).resolve()\n    assert spec_path.exists(), spec_path\n    pybio_model = load_model(str(spec_path), cache_path=cache_path)\n    assert pybio_model.spec.outputs[0].shape.reference_input == ""raw""\n    assert pybio_model.spec.outputs[0].shape.scale == (1, 1, 1, 1, 1)\n    assert pybio_model.spec.outputs[0].shape.offset == (0, 0, 0, 0, 0)\n\n    assert isinstance(pybio_model.spec.prediction.weights.source, BytesIO)\n    assert pybio_model.spec.test_input is not None\n    assert pybio_model.spec.test_input.suffix == "".npz"", pybio_model.spec.test_input.suffix\n    assert pybio_model.spec.test_output is not None\n    assert pybio_model.spec.test_output.suffix == "".npz"", pybio_model.spec.test_output.suffix\n\n\n    model: torch.nn.Module = get_instance(pybio_model)\n    assert isinstance(model, UNet3D)\n    assert hasattr(model, ""forward"")\n    model_weights = torch.load(pybio_model.spec.prediction.weights.source, map_location=torch.device(""cpu""))\n    model.load_state_dict(model_weights)\n    pre_transformations = [get_instance(trf) for trf in pybio_model.spec.prediction.preprocess]\n    post_transformations = [get_instance(trf) for trf in pybio_model.spec.prediction.postprocess]\n\n    ipt = imageio.imread(pybio_model.spec.test_input)\n    assert len(ipt.shape) == 5\n    assert ipt.shape == pybio_model.spec.inputs[0].shape\n\n    expected = imageio.imread(pybio_model.spec.test_output)\n    assert pybio_model.spec.outputs[0].shape.reference_input == pybio_model.spec.inputs[0].name\n    assert all([s == 1 for s in pybio_model.spec.outputs[0].shape.scale])\n    assert all([off == 0 for off in pybio_model.spec.outputs[0].shape.offset])\n    assert expected.shape == pybio_model.spec.inputs[0].shape\n\n    test_roi = (slice(0, 1), slice(0, 1), slice(0, 32), slice(0, 32), slice(0, 32))  # to lower test mem consumption\n    ipt = ipt[test_roi]\n    expected = expected[test_roi]\n    ipt = apply_transformations(pre_transformations, ipt)\n    assert isinstance(ipt, list)\n    assert len(ipt) == 1\n    ipt = ipt[0]\n    out = model.forward(ipt)\n    out = apply_transformations(post_transformations, out)\n    assert isinstance(out, list)\n    assert len(out) == 1\n    out = out[0]\n    # assert out.shape == pybio_model.spec.inputs[0].shape  # test_roi makes test invalid\n    assert str(out.dtype).split(""."")[-1] == pybio_model.spec.outputs[0].data_type\n    assert numpy.allclose(expected, out, atol=0.1)  # test_roi requires bigger atol\n'"
tests/test_bioimage-io/test_manifest.py,0,"b'from pathlib import Path\n\nimport pytest\nimport yaml\n\nfrom pybio.spec import load_spec_and_kwargs, utils\n\nMANIFEST_PATH = Path(__file__).parent / ""../../manifest.yaml""\n\n\ndef pytest_generate_tests(metafunc):\n    if ""category"" in metafunc.fixturenames and ""spec_path"" in metafunc.fixturenames:\n        with MANIFEST_PATH.open() as f:\n            manifest = yaml.safe_load(f)\n\n        categories_and_spec_paths = [\n            (category, spec_path) for category, spec_paths in manifest.items() for spec_path in spec_paths\n        ]\n        metafunc.parametrize(""category,spec_path"", categories_and_spec_paths)\n\n\n@pytest.fixture\ndef required_spec_kwargs():\n    repo_root = Path(__file__).parent.parent\n    kwargs = {}\n    # yaml.safe_load(\n    #     f""""""\n    # """"""\n    # )\n\n    # testing the test data...\n    for spec_path in kwargs:\n        if not (MANIFEST_PATH.parent / spec_path).exists():\n            raise FileNotFoundError(MANIFEST_PATH.parent / spec_path)\n\n    return kwargs\n\n\ndef test_load_specs_from_manifest(cache_path, category, spec_path, required_spec_kwargs):\n    kwargs = required_spec_kwargs.get(spec_path, {})\n\n    spec_path = MANIFEST_PATH.parent / spec_path\n    assert spec_path.exists()\n\n    loaded_spec = load_spec_and_kwargs(str(spec_path), **kwargs, cache_path=cache_path)\n    instance = utils.get_instance(loaded_spec)\n    assert instance\n'"
tests/test_bioimage-io/test_packages/test_UNet3DArabidopsisOvules_model.py,0,"b'from pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom typing import List, Optional\nfrom zipfile import ZipFile\n\nimport pytest\n\nfrom pybio.spec.utils import cache_uri, get_instance, load_model\n\nMODEL_EXTENSIONS = ("".model.yaml"", "".model.yml"")\nPACKAGE_URL = ""https://github.com/wolny/pytorch-3dunet/releases/download/1.2.6/UNet3DArabidopsisOvules.model.zip""\n\n\ndef guess_model_path(file_names: List[str]) -> Optional[str]:\n    for file_name in file_names:\n        if file_name.endswith(MODEL_EXTENSIONS):\n            return file_name\n\n    return None\n\n\ndef eval_model_zip(model_zip: ZipFile, cache_path: Path):\n    with TemporaryDirectory() as tempdir:\n        temp_path = Path(tempdir)\n        if cache_path is None:\n            cache_path = temp_path / ""cache""\n\n        model_zip.extractall(temp_path)\n        spec_file_str = guess_model_path([str(file_name) for file_name in temp_path.glob(""*"")])\n        pybio_model = load_model(spec_file_str, root_path=temp_path, cache_path=cache_path)\n\n        return get_instance(pybio_model)\n\n\n@pytest.fixture\ndef package_bytes(cache_path):\n    return cache_uri(uri_str=PACKAGE_URL, hash={}, cache_path=cache_path)\n\n\ndef test_eval_model_zip(package_bytes, cache_path):\n    with ZipFile(package_bytes) as zf:\n        model = eval_model_zip(zf, cache_path=cache_path)\n\n    assert model  # todo: improve test_eval_model_zip\n'"
