file_path,api_count,code
eval_agedb30.py,6,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: eval_agedb30.py\n@time: 2018/12/25 19:05\n@desc: The AgeDB-30 test protocol is same with LFW, so I just copy the code from eval_lfw.py\n'''\n\n\nimport numpy as np\nimport scipy.io\nimport os\nimport torch.utils.data\nfrom backbone import mobilefacenet, resnet, arcfacenet, cbam\nfrom dataset.agedb import AgeDB30\nimport torchvision.transforms as transforms\nfrom torch.nn import DataParallel\nimport argparse\n\ndef getAccuracy(scores, flags, threshold):\n    p = np.sum(scores[flags == 1] > threshold)\n    n = np.sum(scores[flags == -1] < threshold)\n    return 1.0 * (p + n) / len(scores)\n\ndef getThreshold(scores, flags, thrNum):\n    accuracys = np.zeros((2 * thrNum + 1, 1))\n    thresholds = np.arange(-thrNum, thrNum + 1) * 1.0 / thrNum\n    for i in range(2 * thrNum + 1):\n        accuracys[i] = getAccuracy(scores, flags, thresholds[i])\n    max_index = np.squeeze(accuracys == np.max(accuracys))\n    bestThreshold = np.mean(thresholds[max_index])\n    return bestThreshold\n\ndef evaluation_10_fold(feature_path='./result/cur_epoch_agedb_result.mat'):\n    ACCs = np.zeros(10)\n    result = scipy.io.loadmat(feature_path)\n    for i in range(10):\n        fold = result['fold']\n        flags = result['flag']\n        featureLs = result['fl']\n        featureRs = result['fr']\n\n        valFold = fold != i\n        testFold = fold == i\n        flags = np.squeeze(flags)\n\n        mu = np.mean(np.concatenate((featureLs[valFold[0], :], featureRs[valFold[0], :]), 0), 0)\n        mu = np.expand_dims(mu, 0)\n        featureLs = featureLs - mu\n        featureRs = featureRs - mu\n        featureLs = featureLs / np.expand_dims(np.sqrt(np.sum(np.power(featureLs, 2), 1)), 1)\n        featureRs = featureRs / np.expand_dims(np.sqrt(np.sum(np.power(featureRs, 2), 1)), 1)\n\n        scores = np.sum(np.multiply(featureLs, featureRs), 1)\n        threshold = getThreshold(scores[valFold[0]], flags[valFold[0]], 10000)\n        ACCs[i] = getAccuracy(scores[testFold[0]], flags[testFold[0]], threshold)\n\n    return ACCs\n\ndef loadModel(data_root, file_list, backbone_net, gpus='0', resume=None):\n\n    if backbone_net == 'MobileFace':\n        net = mobilefacenet.MobileFaceNet()\n    elif backbone_net == 'CBAM_50':\n        net = cbam.CBAMResNet(50, feature_dim=args.feature_dim, mode='ir')\n    elif backbone_net == 'CBAM_50_SE':\n        net = cbam.CBAMResNet(50, feature_dim=args.feature_dim, mode='ir_se')\n    elif backbone_net == 'CBAM_100':\n        net = cbam.CBAMResNet(100, feature_dim=args.feature_dim, mode='ir')\n    elif backbone_net == 'CBAM_100_SE':\n        net = cbam.CBAMResNet(100, feature_dim=args.feature_dim, mode='ir_se')\n    else:\n        print(backbone_net, ' is not available!')\n\n    # gpu init\n    multi_gpus = False\n    if len(gpus.split(',')) > 1:\n        multi_gpus = True\n    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    net.load_state_dict(torch.load(resume)['net_state_dict'])\n\n    if multi_gpus:\n        net = DataParallel(net).to(device)\n    else:\n        net = net.to(device)\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n    agedb_dataset = AgeDB30(data_root, file_list, transform=transform)\n    agedb_loader = torch.utils.data.DataLoader(agedb_dataset, batch_size=128,\n                                             shuffle=False, num_workers=2, drop_last=False)\n\n    return net.eval(), device, agedb_dataset, agedb_loader\n\ndef getFeatureFromTorch(feature_save_dir, net, device, data_set, data_loader):\n    featureLs = None\n    featureRs = None\n    count = 0\n    for data in data_loader:\n        for i in range(len(data)):\n            data[i] = data[i].to(device)\n        count += data[0].size(0)\n        #print('extracing deep features from the face pair {}...'.format(count))\n        with torch.no_grad():\n            res = [net(d).data.cpu().numpy() for d in data]\n        featureL = np.concatenate((res[0], res[1]), 1)\n        featureR = np.concatenate((res[2], res[3]), 1)\n        # print(featureL.shape, featureR.shape)\n        if featureLs is None:\n            featureLs = featureL\n        else:\n            featureLs = np.concatenate((featureLs, featureL), 0)\n        if featureRs is None:\n            featureRs = featureR\n        else:\n            featureRs = np.concatenate((featureRs, featureR), 0)\n        # print(featureLs.shape, featureRs.shape)\n\n    result = {'fl': featureLs, 'fr': featureRs, 'fold': data_set.folds, 'flag': data_set.flags}\n    scipy.io.savemat(feature_save_dir, result)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Testing')\n    parser.add_argument('--root', type=str, default='/media/sda/AgeDB-30/agedb30_align_112', help='The path of lfw data')\n    parser.add_argument('--file_list', type=str, default='/media/sda/AgeDB-30/agedb_30_pair.txt', help='The path of lfw data')\n    parser.add_argument('--resume', type=str, default='./model/SERES100_SERES100_IR_20190528_132635/Iter_342000_net.ckpt', help='The path pf save model')\n    parser.add_argument('--backbone_net', type=str, default='CBAM_100_SE', help='MobileFace, CBAM_50, CBAM_50_SE, CBAM_100, CBAM_100_SE')\n    parser.add_argument('--feature_dim', type=int, default=512, help='feature dimension')\n    parser.add_argument('--feature_save_path', type=str, default='./result/cur_epoch_agedb_result.mat',\n                        help='The path of the extract features save, must be .mat file')\n    parser.add_argument('--gpus', type=str, default='2,3', help='gpu list')\n    args = parser.parse_args()\n\n    net, device, agedb_dataset, agedb_loader = loadModel(args.root, args.file_list, args.backbone_net, args.gpus, args.resume)\n    getFeatureFromTorch(args.feature_save_path, net, device, agedb_dataset, agedb_loader)\n    ACCs = evaluation_10_fold(args.feature_save_path)\n    for i in range(len(ACCs)):\n        print('{}    {:.2f}'.format(i + 1, ACCs[i] * 100))\n    print('--------')\n    print('AVE    {:.4f}'.format(np.mean(ACCs) * 100))\n\n"""
eval_cfp.py,6,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: eval_cfp.py\n@time: 2018/12/26 16:23\n@desc: this code is very similar with eval_lfw.py and eval_agedb30.py\n'''\n\n\nimport numpy as np\nimport scipy.io\nimport os\nimport torch.utils.data\nfrom backbone import mobilefacenet, resnet, arcfacenet, cbam\nfrom dataset.cfp import CFP_FP\nimport torchvision.transforms as transforms\nfrom torch.nn import DataParallel\nimport argparse\n\ndef getAccuracy(scores, flags, threshold):\n    p = np.sum(scores[flags == 1] > threshold)\n    n = np.sum(scores[flags == -1] < threshold)\n    return 1.0 * (p + n) / len(scores)\n\ndef getThreshold(scores, flags, thrNum):\n    accuracys = np.zeros((2 * thrNum + 1, 1))\n    thresholds = np.arange(-thrNum, thrNum + 1) * 1.0 / thrNum\n    for i in range(2 * thrNum + 1):\n        accuracys[i] = getAccuracy(scores, flags, thresholds[i])\n    max_index = np.squeeze(accuracys == np.max(accuracys))\n    bestThreshold = np.mean(thresholds[max_index])\n    return bestThreshold\n\ndef evaluation_10_fold(feature_path='./result/cur_epoch_cfp_result.mat'):\n    ACCs = np.zeros(10)\n    result = scipy.io.loadmat(feature_path)\n    for i in range(10):\n        fold = result['fold']\n        flags = result['flag']\n        featureLs = result['fl']\n        featureRs = result['fr']\n\n        valFold = fold != i\n        testFold = fold == i\n        flags = np.squeeze(flags)\n\n        mu = np.mean(np.concatenate((featureLs[valFold[0], :], featureRs[valFold[0], :]), 0), 0)\n        mu = np.expand_dims(mu, 0)\n        featureLs = featureLs - mu\n        featureRs = featureRs - mu\n        featureLs = featureLs / np.expand_dims(np.sqrt(np.sum(np.power(featureLs, 2), 1)), 1)\n        featureRs = featureRs / np.expand_dims(np.sqrt(np.sum(np.power(featureRs, 2), 1)), 1)\n\n        scores = np.sum(np.multiply(featureLs, featureRs), 1)\n        threshold = getThreshold(scores[valFold[0]], flags[valFold[0]], 10000)\n        ACCs[i] = getAccuracy(scores[testFold[0]], flags[testFold[0]], threshold)\n\n    return ACCs\n\ndef loadModel(data_root, file_list, backbone_net, gpus='0', resume=None):\n\n    if backbone_net == 'MobileFace':\n        net = mobilefacenet.MobileFaceNet()\n    elif backbone_net == 'CBAM_50':\n        net = cbam.CBAMResNet(50, feature_dim=args.feature_dim, mode='ir')\n    elif backbone_net == 'CBAM_50_SE':\n        net = cbam.CBAMResNet(50, feature_dim=args.feature_dim, mode='ir_se')\n    elif backbone_net == 'CBAM_100':\n        net = cbam.CBAMResNet(100, feature_dim=args.feature_dim, mode='ir')\n    elif backbone_net == 'CBAM_100_SE':\n        net = cbam.CBAMResNet(100, feature_dim=args.feature_dim, mode='ir_se')\n    else:\n        print(backbone_net, ' is not available!')\n\n    # gpu init\n    multi_gpus = False\n    if len(gpus.split(',')) > 1:\n        multi_gpus = True\n    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    net.load_state_dict(torch.load(resume)['net_state_dict'])\n\n    if multi_gpus:\n        net = DataParallel(net).to(device)\n    else:\n        net = net.to(device)\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n    cfp_dataset = CFP_FP(data_root, file_list, transform=transform)\n    cfp_loader = torch.utils.data.DataLoader(cfp_dataset, batch_size=128,\n                                             shuffle=False, num_workers=4, drop_last=False)\n\n    return net.eval(), device, cfp_dataset, cfp_loader\n\ndef getFeatureFromTorch(feature_save_dir, net, device, data_set, data_loader):\n    featureLs = None\n    featureRs = None\n    count = 0\n    for data in data_loader:\n        for i in range(len(data)):\n            data[i] = data[i].to(device)\n        count += data[0].size(0)\n        #print('extracing deep features from the face pair {}...'.format(count))\n        with torch.no_grad():\n            res = [net(d).data.cpu().numpy() for d in data]\n        featureL = np.concatenate((res[0], res[1]), 1)\n        featureR = np.concatenate((res[2], res[3]), 1)\n        # print(featureL.shape, featureR.shape)\n        if featureLs is None:\n            featureLs = featureL\n        else:\n            featureLs = np.concatenate((featureLs, featureL), 0)\n        if featureRs is None:\n            featureRs = featureR\n        else:\n            featureRs = np.concatenate((featureRs, featureR), 0)\n        # print(featureLs.shape, featureRs.shape)\n\n    result = {'fl': featureLs, 'fr': featureRs, 'fold': data_set.folds, 'flag': data_set.flags}\n    scipy.io.savemat(feature_save_dir, result)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Testing')\n    parser.add_argument('--root', type=str, default='/media/sda/CFP-FP/cfp_fp_aligned_112', help='The path of lfw data')\n    parser.add_argument('--file_list', type=str, default='/media/sda/CFP-FP/cfp_fp_pair.txt', help='The path of lfw data')\n    parser.add_argument('--resume', type=str, default='./model/SERES100_SERES100_IR_20190528_132635/Iter_342000_net.ckpt', help='The path pf save model')\n    parser.add_argument('--backbone_net', type=str, default='CBAM_100_SE', help='MobileFace, CBAM_50, CBAM_50_SE, CBAM_100, CBAM_100_SE')\n    parser.add_argument('--feature_dim', type=int, default=512, help='feature dimension')\n    parser.add_argument('--feature_save_path', type=str, default='./result/cur_epoch_cfp_result.mat',\n                        help='The path of the extract features save, must be .mat file')\n    parser.add_argument('--gpus', type=str, default='2,3', help='gpu list')\n    args = parser.parse_args()\n\n    net, device, agedb_dataset, agedb_loader = loadModel(args.root, args.file_list, args.backbone_net, args.gpus, args.resume)\n    getFeatureFromTorch(args.feature_save_path, net, device, agedb_dataset, agedb_loader)\n    ACCs = evaluation_10_fold(args.feature_save_path)\n    for i in range(len(ACCs)):\n        print('{}    {:.2f}'.format(i + 1, ACCs[i] * 100))\n    print('--------')\n    print('AVE    {:.4f}'.format(np.mean(ACCs) * 100))"""
eval_deepglint_merge.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\'\'\'\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: eval_deepglint_merge.py.py\n@time: 2019/3/21 11:09\n@desc: merge the feature of deepglint test data to one file. original deepglint feature is generated by the protocol of megaface.\n\'\'\'\n\n""""""\nWe use the same format as Megaface(http://megaface.cs.washington.edu) \nexcept that we merge all files into a single binary file.\n\nfor examples:\n\nwhen megaface: N * (512, 1) \nwhile deepglint:(N, 512)\n\n""""""\nimport struct\nimport numpy as np\nimport sys, os\nimport argparse\n\ncv_type_to_dtype = {\n    5: np.dtype(\'float32\')\n}\n\ndtype_to_cv_type = {v: k for k, v in cv_type_to_dtype.items()}\n\n\ndef write_mat(f, m):\n    """"""Write mat m to file f""""""\n    if len(m.shape) == 1:\n        rows = m.shape[0]\n        cols = 1\n    else:\n        rows, cols = m.shape\n    header = struct.pack(\'iiii\', rows, cols, cols * 4, dtype_to_cv_type[m.dtype])\n    f.write(header)\n    f.write(m.data)\n\n\ndef read_mat(f):\n    """"""\n    Reads an OpenCV mat from the given file opened in binary mode\n    """"""\n    rows, cols, stride, type_ = struct.unpack(\'iiii\', f.read(4 * 4))\n    mat = np.fromstring(f.read(rows * stride), dtype=cv_type_to_dtype[type_])\n    return mat.reshape(rows, cols)\n\n\ndef load_mat(filename):\n    """"""\n    Reads a OpenCV Mat from the given filename\n    """"""\n    return read_mat(open(filename, \'rb\'))\n\n\ndef save_mat(filename, m):\n    """"""Saves mat m to the given filename""""""\n    return write_mat(open(filename, \'wb\'), m)\n\n\n\ndef main(args):\n\n    deepglint_features = args.deepglint_features_path\n    # merge all features into one file\n    total_feature = []\n    total_files = []\n    for root, dirs, files in os.walk(deepglint_features):\n        for file in files:\n            filename = os.path.join(root, file)\n            ext = os.path.splitext(filename)[1]\n            ext = ext.lower()\n            if ext in (\'.feat\'):\n                total_files.append(filename)\n\n    assert len(total_files) == 1862120\n    total_files.sort()  # important\n\n    for i in range(len(total_files)):\n        filename = total_files[i]\n        tmp_feature = load_mat(filename)\n        # print(filename)\n        # print(tmp_feature.shape)\n        tmp_feature = tmp_feature.T\n        total_feature.append(tmp_feature)\n        print(i + 1, tmp_feature.shape)\n        # write_mat(feature_path_out, feature_fusion)\n\n    print(\'total feature number: \', len(total_feature))\n    total_feature = np.array(total_feature).squeeze()\n    print(total_feature.shape, total_feature.dtype, type(total_feature))\n    save_mat(\'deepglint_test_feature.bin\', total_feature)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--deepglint_features_path"", type=str, default=""/home/wujiyang/deepglint/deepglint_feature_ir+ws/"")\n    args = parser.parse_args()\n\n    main(args)\n'"
eval_lfw.py,6,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: eval_lfw.py\n@time: 2018/12/22 9:47\n@desc:\n'''\n\nimport numpy as np\nimport scipy.io\nimport os\nimport json\nimport torch.utils.data\nfrom backbone import mobilefacenet, resnet, arcfacenet, cbam\nfrom dataset.lfw import LFW\nimport torchvision.transforms as transforms\nfrom torch.nn import DataParallel\nimport argparse\n\ndef getAccuracy(scores, flags, threshold):\n    p = np.sum(scores[flags == 1] > threshold)\n    n = np.sum(scores[flags == -1] < threshold)\n    return 1.0 * (p + n) / len(scores)\n\ndef getThreshold(scores, flags, thrNum):\n    accuracys = np.zeros((2 * thrNum + 1, 1))\n    thresholds = np.arange(-thrNum, thrNum + 1) * 1.0 / thrNum\n    for i in range(2 * thrNum + 1):\n        accuracys[i] = getAccuracy(scores, flags, thresholds[i])\n    max_index = np.squeeze(accuracys == np.max(accuracys))\n    bestThreshold = np.mean(thresholds[max_index])\n    return bestThreshold\n\ndef evaluation_10_fold(feature_path='./result/cur_epoch_result.mat'):\n    ACCs = np.zeros(10)\n    result = scipy.io.loadmat(feature_path)\n    for i in range(10):\n        fold = result['fold']\n        flags = result['flag']\n        featureLs = result['fl']\n        featureRs = result['fr']\n\n        valFold = fold != i\n        testFold = fold == i\n        flags = np.squeeze(flags)\n\n        mu = np.mean(np.concatenate((featureLs[valFold[0], :], featureRs[valFold[0], :]), 0), 0)\n        mu = np.expand_dims(mu, 0)\n        featureLs = featureLs - mu\n        featureRs = featureRs - mu\n        featureLs = featureLs / np.expand_dims(np.sqrt(np.sum(np.power(featureLs, 2), 1)), 1)\n        featureRs = featureRs / np.expand_dims(np.sqrt(np.sum(np.power(featureRs, 2), 1)), 1)\n\n        scores = np.sum(np.multiply(featureLs, featureRs), 1)\n        threshold = getThreshold(scores[valFold[0]], flags[valFold[0]], 10000)\n        ACCs[i] = getAccuracy(scores[testFold[0]], flags[testFold[0]], threshold)\n\n    return ACCs\n\ndef loadModel(data_root, file_list, backbone_net, gpus='0', resume=None):\n\n    if backbone_net == 'MobileFace':\n        net = mobilefacenet.MobileFaceNet()\n    elif backbone_net == 'CBAM_50':\n        net = cbam.CBAMResNet(50, feature_dim=args.feature_dim, mode='ir')\n    elif backbone_net == 'CBAM_50_SE':\n        net = cbam.CBAMResNet(50, feature_dim=args.feature_dim, mode='ir_se')\n    elif backbone_net == 'CBAM_100':\n        net = cbam.CBAMResNet(100, feature_dim=args.feature_dim, mode='ir')\n    elif backbone_net == 'CBAM_100_SE':\n        net = cbam.CBAMResNet(100, feature_dim=args.feature_dim, mode='ir_se')\n    else:\n        print(backbone_net, ' is not available!')\n\n    # gpu init\n    multi_gpus = False\n    if len(gpus.split(',')) > 1:\n        multi_gpus = True\n    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    net.load_state_dict(torch.load(resume)['net_state_dict'])\n\n    if multi_gpus:\n        net = DataParallel(net).to(device)\n    else:\n        net = net.to(device)\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n    lfw_dataset = LFW(data_root, file_list, transform=transform)\n    lfw_loader = torch.utils.data.DataLoader(lfw_dataset, batch_size=128,\n                                             shuffle=False, num_workers=2, drop_last=False)\n\n    return net.eval(), device, lfw_dataset, lfw_loader\n\ndef getFeatureFromTorch(feature_save_dir, net, device, data_set, data_loader):\n    featureLs = None\n    featureRs = None\n    count = 0\n    for data in data_loader:\n        for i in range(len(data)):\n            data[i] = data[i].to(device)\n        count += data[0].size(0)\n        #print('extracing deep features from the face pair {}...'.format(count))\n        with torch.no_grad():\n            res = [net(d).data.cpu().numpy() for d in data]\n        featureL = np.concatenate((res[0], res[1]), 1)\n        featureR = np.concatenate((res[2], res[3]), 1)\n        # print(featureL.shape, featureR.shape)\n        if featureLs is None:\n            featureLs = featureL\n        else:\n            featureLs = np.concatenate((featureLs, featureL), 0)\n        if featureRs is None:\n            featureRs = featureR\n        else:\n            featureRs = np.concatenate((featureRs, featureR), 0)\n        # print(featureLs.shape, featureRs.shape)\n\n    result = {'fl': featureLs, 'fr': featureRs, 'fold': data_set.folds, 'flag': data_set.flags}\n    scipy.io.savemat(feature_save_dir, result)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Testing')\n    parser.add_argument('--root', type=str, default='/media/sda/lfw/lfw_align_112', help='The path of lfw data')\n    parser.add_argument('--file_list', type=str, default='/media/sda/lfw/pairs.txt', help='The path of lfw data')\n    parser.add_argument('--backbone_net', type=str, default='CBAM_100_SE', help='MobileFace, CBAM_50, CBAM_50_SE, CBAM_100, CBAM_100_SE')\n    parser.add_argument('--feature_dim', type=int, default=512, help='feature dimension')\n    parser.add_argument('--resume', type=str, default='./model/SERES100_SERES100_IR_20190528_132635/Iter_342000_net.ckpt',\n                        help='The path pf save model')\n    parser.add_argument('--feature_save_path', type=str, default='./result/cur_epoch_lfw_result.mat',\n                        help='The path of the extract features save, must be .mat file')\n    parser.add_argument('--gpus', type=str, default='1,3', help='gpu list')\n    args = parser.parse_args()\n\n    net, device, lfw_dataset, lfw_loader = loadModel(args.root, args.file_list, args.backbone_net, args.gpus, args.resume)\n    getFeatureFromTorch(args.feature_save_path, net, device, lfw_dataset, lfw_loader)\n    ACCs = evaluation_10_fold(args.feature_save_path)\n    for i in range(len(ACCs)):\n        print('{}    {:.2f}'.format(i+1, ACCs[i] * 100))\n    print('--------')\n    print('AVE    {:.4f}'.format(np.mean(ACCs) * 100))\n"""
eval_lfw_blufr.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: eval_lfw_blufr.py\n@time: 2019/1/17 15:52\n@desc: test lfw accuracy on blufr protocol\n'''\n'''\nLFW BLUFR TEST PROTOCOL\n\nOfficial Website: http://www.cbsr.ia.ac.cn/users/scliao/projects/blufr/\n\nWhen I try to do this, I find that the blufr_lfw_config.mat file provided by above site is too old.\nSome image files listed in the mat have been removed in lfw pairs.txt\nSo this work is suspended for now...\n'''\n\nimport scipy.io as sio\nimport argparse\n\ndef readName(file='pairs.txt'):\n    name_list = []\n    f = open(file, 'r')\n    lines = f.readlines()\n\n    for line in lines[1:]:\n        line_split = line.rstrip().split()\n        if len(line_split) == 3:\n            name_list.append(line_split[0])\n        elif len(line_split) == 4:\n            name_list.append(line_split[0])\n            name_list.append(line_split[2])\n        else:\n            print('wrong file, please check again')\n\n    return list(set(name_list))\n\n\ndef main(args):\n    blufr_info = sio.loadmat(args.lfw_blufr_file)\n    #print(blufr_info)\n    name_list = readName()\n\n    image = blufr_info['imageList']\n    missing_files = []\n    for i in range(image.shape[0]):\n        name = image[i][0][0]\n        index = name.rfind('_')\n        name = name[0:index]\n        if name not in name_list:\n            print(name)\n            missing_files.append(name)\n    print('lfw pairs.txt total persons: ', len(name_list))\n    print('blufr_mat_missing persons: ', len(missing_files))\n\n    '''\n    Some of the missing file:\n    Zdravko_Mucic\n    Zelma_Novelo\n    Zeng_Qinghong\n    Zumrati_Juma\n    lfw pairs.txt total persons:  4281\n    blufr_mat_missing persons:  1549\n    \n    '''\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='lfw blufr test')\n    parser.add_argument('--lfw_blufr_file', type=str, default='./blufr_lfw_config.mat', help='feature dimension')\n    parser.add_argument('--lfw_pairs.txt', type=str, default='./pairs.txt', help='feature dimension')\n    parser.add_argument('--gpus', type=str, default='2,3', help='gpu list')\n    args = parser.parse_args()\n\n    main(args)"""
eval_megaface.py,6,"b'#!/usr/bin/env python\n# encoding: utf-8\n\'\'\'\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: eval_megaface.py\n@time: 2018/12/24 16:28\n@desc: megaface feature extractor\n\'\'\'\nimport numpy as np\nimport struct\nimport os\nimport torch.utils.data\nfrom backbone import mobilefacenet, cbam, self_attention\nfrom dataset.megaface import MegaFace\nimport torchvision.transforms as transforms\nimport argparse\nfrom torch.nn import DataParallel\n\n\ncv_type_to_dtype = {5: np.dtype(\'float32\'), 6: np.dtype(\'float64\')}\ndtype_to_cv_type = {v: k for k, v in cv_type_to_dtype.items()}\n\ndef write_mat(filename, m):\n    """"""Write mat m to file f""""""\n    if len(m.shape) == 1:\n        rows = m.shape[0]\n        cols = 1\n    else:\n        rows, cols = m.shape\n    header = struct.pack(\'iiii\', rows, cols, cols * 4, dtype_to_cv_type[m.dtype])\n\n    with open(filename, \'wb\') as outfile:\n        outfile.write(header)\n        outfile.write(m.data)\n\n\ndef read_mat(filename):\n    """"""\n    Reads an OpenCV mat from the given file opened in binary mode\n    """"""\n    with open(filename, \'rb\') as fin:\n        rows, cols, stride, type_ = struct.unpack(\'iiii\', fin.read(4 * 4))\n        mat = np.fromstring(str(fin.read(rows * stride)), dtype=cv_type_to_dtype[type_])\n        return mat.reshape(rows, cols)\n\n\ndef extract_feature(model_path, backbone_net, face_scrub_path, megaface_path, batch_size=32, gpus=\'0\', do_norm=False):\n\n    if backbone_net == \'MobileFace\':\n        net = mobilefacenet.MobileFaceNet()\n    elif backbone_net == \'CBAM_50\':\n        net = cbam.CBAMResNet(50, feature_dim=args.feature_dim, mode=\'ir\')\n    elif backbone_net == \'CBAM_50_SE\':\n        net = cbam.CBAMResNet(50, feature_dim=args.feature_dim, mode=\'ir_se\')\n    elif backbone_net == \'CBAM_100\':\n        net = cbam.CBAMResNet(100, feature_dim=args.feature_dim, mode=\'ir\')\n    elif backbone_net == \'CBAM_100_SE\':\n        net = cbam.CBAMResNet(100, feature_dim=args.feature_dim, mode=\'ir_se\')\n    else:\n        print(args.backbone, \' is not available!\')\n\n    multi_gpus = False\n    if len(gpus.split(\',\')) > 1:\n        multi_gpus = True\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpus\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    net.load_state_dict(torch.load(model_path)[\'net_state_dict\'])\n    if multi_gpus:\n        net = DataParallel(net).to(device)\n    else:\n        net = net.to(device)\n    net.eval()\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n    megaface_dataset = MegaFace(face_scrub_path, megaface_path, transform=transform)\n    megaface_loader = torch.utils.data.DataLoader(megaface_dataset, batch_size=batch_size,\n                                             shuffle=False, num_workers=12, drop_last=False)\n\n    for data in megaface_loader:\n        img, img_path= data[0].to(device), data[1]\n        with torch.no_grad():\n            output = net(img).data.cpu().numpy()\n\n        if do_norm is False:\n            for i in range(len(img_path)):\n                abs_path = img_path[i] + \'.feat\'\n                write_mat(abs_path, output[i])\n            print(\'extract 1 batch...without feature normalization\')\n        else:\n            for i in range(len(img_path)):\n                abs_path = img_path[i] + \'.feat\'\n                feat = output[i]\n                feat = feat / np.sqrt((np.dot(feat, feat)))\n                write_mat(abs_path, feat)\n            print(\'extract 1 batch...with feature normalization\')\n    print(\'all images have been processed!\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Testing\')\n    parser.add_argument(\'--model_path\', type=str, default=\'./model/RES100_RES100_IR_20190423_100728/Iter_333000_net.ckpt\', help=\'The path of trained model\')\n    parser.add_argument(\'--backbone_net\', type=str, default=\'CBAM_100\', help=\'MobileFace, CBAM_50, CBAM_50_SE, CBAM_100, CBAM_100_SE\')\n    parser.add_argument(\'--facescrub_dir\', type=str, default=\'/media/sda/megaface_test_kit/facescrub_align_112/\', help=\'facescrub data\')\n    parser.add_argument(\'--megaface_dir\', type=str, default=\'/media/sda/megaface_test_kit/megaface_align_112/\', help=\'megaface data\')\n    parser.add_argument(\'--batch_size\', type=int, default=1024, help=\'batch size\')\n    parser.add_argument(\'--feature_dim\', type=int, default=512, help=\'feature dimension\')\n    parser.add_argument(\'--gpus\', type=str, default=\'0,1,2,3\', help=\'gpu list\')\n    parser.add_argument(""--do_norm"", type=int, default=1, help=""1 if normalize feature, 0 do nothing(Default case)"")\n    args = parser.parse_args()\n\n    extract_feature(args.model_path, args.backbone_net, args.facescrub_dir, args.megaface_dir, args.batch_size, args.gpus, args.do_norm)'"
train.py,15,"b'#!/usr/bin/env python\n# encoding: utf-8\n\'\'\'\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: train.py.py\n@time: 2018/12/21 17:37\n@desc: train script for deep face recognition\n\'\'\'\n\nimport os\nimport torch.utils.data\nfrom torch.nn import DataParallel\nfrom datetime import datetime\nfrom backbone.mobilefacenet import MobileFaceNet\nfrom backbone.cbam import CBAMResNet\nfrom backbone.attention import ResidualAttentionNet_56, ResidualAttentionNet_92\nfrom margin.ArcMarginProduct import ArcMarginProduct\nfrom margin.MultiMarginProduct import MultiMarginProduct\nfrom margin.CosineMarginProduct import CosineMarginProduct\nfrom margin.InnerProduct import InnerProduct\nfrom utils.visualize import Visualizer\nfrom utils.logging import init_log\nfrom dataset.casia_webface import CASIAWebFace\nfrom dataset.lfw import LFW\nfrom dataset.agedb import AgeDB30\nfrom dataset.cfp import CFP_FP\nfrom torch.optim import lr_scheduler\nimport torch.optim as optim\nimport time\nfrom eval_lfw import evaluation_10_fold, getFeatureFromTorch\nimport numpy as np\nimport torchvision.transforms as transforms\nimport argparse\n\n\ndef train(args):\n    # gpu init\n    multi_gpus = False\n    if len(args.gpus.split(\',\')) > 1:\n        multi_gpus = True\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpus\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # log init\n    save_dir = os.path.join(args.save_dir, args.model_pre + args.backbone.upper() + \'_\' + datetime.now().strftime(\'%Y%m%d_%H%M%S\'))\n    if os.path.exists(save_dir):\n        raise NameError(\'model dir exists!\')\n    os.makedirs(save_dir)\n    logging = init_log(save_dir)\n    _print = logging.info\n\n    # dataset loader\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n    # validation dataset\n    trainset = CASIAWebFace(args.train_root, args.train_file_list, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n                                              shuffle=True, num_workers=8, drop_last=False)\n    # test dataset\n    lfwdataset = LFW(args.lfw_test_root, args.lfw_file_list, transform=transform)\n    lfwloader = torch.utils.data.DataLoader(lfwdataset, batch_size=128,\n                                             shuffle=False, num_workers=4, drop_last=False)\n    agedbdataset = AgeDB30(args.agedb_test_root, args.agedb_file_list, transform=transform)\n    agedbloader = torch.utils.data.DataLoader(agedbdataset, batch_size=128,\n                                            shuffle=False, num_workers=4, drop_last=False)\n    cfpfpdataset = CFP_FP(args.cfpfp_test_root, args.cfpfp_file_list, transform=transform)\n    cfpfploader = torch.utils.data.DataLoader(cfpfpdataset, batch_size=128,\n                                              shuffle=False, num_workers=4, drop_last=False)\n\n    # define backbone and margin layer\n    if args.backbone == \'MobileFace\':\n        net = MobileFaceNet()\n    elif args.backbone == \'Res50_IR\':\n        net = CBAMResNet(50, feature_dim=args.feature_dim, mode=\'ir\')\n    elif args.backbone == \'SERes50_IR\':\n        net = CBAMResNet(50, feature_dim=args.feature_dim, mode=\'ir_se\')\n    elif args.backbone == \'Res100_IR\':\n        net = CBAMResNet(100, feature_dim=args.feature_dim, mode=\'ir\')\n    elif args.backbone == \'SERes100_IR\':\n        net = CBAMResNet(100, feature_dim=args.feature_dim, mode=\'ir_se\')\n    elif args.backbone == \'Attention_56\':\n        net = ResidualAttentionNet_56(feature_dim=args.feature_dim)\n    elif args.backbone == \'Attention_92\':\n        net = ResidualAttentionNet_92(feature_dim=args.feature_dim)\n    else:\n        print(args.backbone, \' is not available!\')\n\n    if args.margin_type == \'ArcFace\':\n        margin = ArcMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)\n    elif args.margin_type == \'MultiMargin\':\n        margin = MultiMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)\n    elif args.margin_type == \'CosFace\':\n        margin = CosineMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)\n    elif args.margin_type == \'Softmax\':\n        margin = InnerProduct(args.feature_dim, trainset.class_nums)\n    elif args.margin_type == \'SphereFace\':\n        pass\n    else:\n        print(args.margin_type, \'is not available!\')\n\n    if args.resume:\n        print(\'resume the model parameters from: \', args.net_path, args.margin_path)\n        net.load_state_dict(torch.load(args.net_path)[\'net_state_dict\'])\n        margin.load_state_dict(torch.load(args.margin_path)[\'net_state_dict\'])\n\n    # define optimizers for different layer\n    criterion = torch.nn.CrossEntropyLoss().to(device)\n    optimizer_ft = optim.SGD([\n        {\'params\': net.parameters(), \'weight_decay\': 5e-4},\n        {\'params\': margin.parameters(), \'weight_decay\': 5e-4}\n    ], lr=0.1, momentum=0.9, nesterov=True)\n    exp_lr_scheduler = lr_scheduler.MultiStepLR(optimizer_ft, milestones=[6, 11, 16], gamma=0.1)\n\n    if multi_gpus:\n        net = DataParallel(net).to(device)\n        margin = DataParallel(margin).to(device)\n    else:\n        net = net.to(device)\n        margin = margin.to(device)\n\n\n    best_lfw_acc = 0.0\n    best_lfw_iters = 0\n    best_agedb30_acc = 0.0\n    best_agedb30_iters = 0\n    best_cfp_fp_acc = 0.0\n    best_cfp_fp_iters = 0\n    total_iters = 0\n    vis = Visualizer(env=args.model_pre + args.backbone)\n    for epoch in range(1, args.total_epoch + 1):\n        exp_lr_scheduler.step()\n        # train model\n        _print(\'Train Epoch: {}/{} ...\'.format(epoch, args.total_epoch))\n        net.train()\n\n        since = time.time()\n        for data in trainloader:\n            img, label = data[0].to(device), data[1].to(device)\n            optimizer_ft.zero_grad()\n\n            raw_logits = net(img)\n            output = margin(raw_logits, label)\n            total_loss = criterion(output, label)\n            total_loss.backward()\n            optimizer_ft.step()\n\n            total_iters += 1\n            # print train information\n            if total_iters % 100 == 0:\n                # current training accuracy\n                _, predict = torch.max(output.data, 1)\n                total = label.size(0)\n                correct = (np.array(predict.cpu()) == np.array(label.data.cpu())).sum()\n                time_cur = (time.time() - since) / 100\n                since = time.time()\n                vis.plot_curves({\'softmax loss\': total_loss.item()}, iters=total_iters, title=\'train loss\',\n                                xlabel=\'iters\', ylabel=\'train loss\')\n                vis.plot_curves({\'train accuracy\': correct / total}, iters=total_iters, title=\'train accuracy\', xlabel=\'iters\',\n                                ylabel=\'train accuracy\')\n\n                _print(""Iters: {:0>6d}/[{:0>2d}], loss: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}"".format(total_iters, epoch, total_loss.item(), correct/total, time_cur, exp_lr_scheduler.get_lr()[0]))\n\n            # save model\n            if total_iters % args.save_freq == 0:\n                msg = \'Saving checkpoint: {}\'.format(total_iters)\n                _print(msg)\n                if multi_gpus:\n                    net_state_dict = net.module.state_dict()\n                    margin_state_dict = margin.module.state_dict()\n                else:\n                    net_state_dict = net.state_dict()\n                    margin_state_dict = margin.state_dict()\n                if not os.path.exists(save_dir):\n                    os.mkdir(save_dir)\n                torch.save({\n                    \'iters\': total_iters,\n                    \'net_state_dict\': net_state_dict},\n                    os.path.join(save_dir, \'Iter_%06d_net.ckpt\' % total_iters))\n                torch.save({\n                    \'iters\': total_iters,\n                    \'net_state_dict\': margin_state_dict},\n                    os.path.join(save_dir, \'Iter_%06d_margin.ckpt\' % total_iters))\n\n            # test accuracy\n            if total_iters % args.test_freq == 0:\n\n                # test model on lfw\n                net.eval()\n                getFeatureFromTorch(\'./result/cur_lfw_result.mat\', net, device, lfwdataset, lfwloader)\n                lfw_accs = evaluation_10_fold(\'./result/cur_lfw_result.mat\')\n                _print(\'LFW Ave Accuracy: {:.4f}\'.format(np.mean(lfw_accs) * 100))\n                if best_lfw_acc <= np.mean(lfw_accs) * 100:\n                    best_lfw_acc = np.mean(lfw_accs) * 100\n                    best_lfw_iters = total_iters\n\n                # test model on AgeDB30\n                getFeatureFromTorch(\'./result/cur_agedb30_result.mat\', net, device, agedbdataset, agedbloader)\n                age_accs = evaluation_10_fold(\'./result/cur_agedb30_result.mat\')\n                _print(\'AgeDB-30 Ave Accuracy: {:.4f}\'.format(np.mean(age_accs) * 100))\n                if best_agedb30_acc <= np.mean(age_accs) * 100:\n                    best_agedb30_acc = np.mean(age_accs) * 100\n                    best_agedb30_iters = total_iters\n\n                # test model on CFP-FP\n                getFeatureFromTorch(\'./result/cur_cfpfp_result.mat\', net, device, cfpfpdataset, cfpfploader)\n                cfp_accs = evaluation_10_fold(\'./result/cur_cfpfp_result.mat\')\n                _print(\'CFP-FP Ave Accuracy: {:.4f}\'.format(np.mean(cfp_accs) * 100))\n                if best_cfp_fp_acc <= np.mean(cfp_accs) * 100:\n                    best_cfp_fp_acc = np.mean(cfp_accs) * 100\n                    best_cfp_fp_iters = total_iters\n                _print(\'Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}\'.format(\n                    best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))\n\n                vis.plot_curves({\'lfw\': np.mean(lfw_accs), \'agedb-30\': np.mean(age_accs), \'cfp-fp\': np.mean(cfp_accs)}, iters=total_iters,\n                                title=\'test accuracy\', xlabel=\'iters\', ylabel=\'test accuracy\')\n                net.train()\n\n    _print(\'Finally Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}\'.format(\n        best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))\n    print(\'finishing training\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch for deep face recognition\')\n    parser.add_argument(\'--train_root\', type=str, default=\'/media/ramdisk/msra_align_112\', help=\'train image root\')\n    parser.add_argument(\'--train_file_list\', type=str, default=\'/media/ramdisk/msra_align_train.list\', help=\'train list\')\n    parser.add_argument(\'--lfw_test_root\', type=str, default=\'/media/sda/lfw/lfw_align_112\', help=\'lfw image root\')\n    parser.add_argument(\'--lfw_file_list\', type=str, default=\'/media/sda/lfw/pairs.txt\', help=\'lfw pair file list\')\n    parser.add_argument(\'--agedb_test_root\', type=str, default=\'/media/sda/AgeDB-30/agedb30_align_112\', help=\'agedb image root\')\n    parser.add_argument(\'--agedb_file_list\', type=str, default=\'/media/sda/AgeDB-30/agedb_30_pair.txt\', help=\'agedb pair file list\')\n    parser.add_argument(\'--cfpfp_test_root\', type=str, default=\'/media/sda/CFP-FP/cfp_fp_aligned_112\', help=\'agedb image root\')\n    parser.add_argument(\'--cfpfp_file_list\', type=str, default=\'/media/sda/CFP-FP/cfp_fp_pair.txt\', help=\'agedb pair file list\')\n\n    parser.add_argument(\'--backbone\', type=str, default=\'SERes100_IR\', help=\'MobileFace, Res50_IR, SERes50_IR, Res100_IR, SERes100_IR, Attention_56, Attention_92\')\n    parser.add_argument(\'--margin_type\', type=str, default=\'ArcFace\', help=\'ArcFace, CosFace, SphereFace, MultiMargin, Softmax\')\n    parser.add_argument(\'--feature_dim\', type=int, default=512, help=\'feature dimension, 128 or 512\')\n    parser.add_argument(\'--scale_size\', type=float, default=32.0, help=\'scale size\')\n    parser.add_argument(\'--batch_size\', type=int, default=200, help=\'batch size\')\n    parser.add_argument(\'--total_epoch\', type=int, default=18, help=\'total epochs\')\n\n    parser.add_argument(\'--save_freq\', type=int, default=3000, help=\'save frequency\')\n    parser.add_argument(\'--test_freq\', type=int, default=3000, help=\'test frequency\')\n    parser.add_argument(\'--resume\', type=int, default=False, help=\'resume model\')\n    parser.add_argument(\'--net_path\', type=str, default=\'\', help=\'resume model\')\n    parser.add_argument(\'--margin_path\', type=str, default=\'\', help=\'resume model\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'./model\', help=\'model save dir\')\n    parser.add_argument(\'--model_pre\', type=str, default=\'SERES100_\', help=\'model prefix\')\n    parser.add_argument(\'--gpus\', type=str, default=\'0,1,2,3\', help=\'model prefix\')\n\n    args = parser.parse_args()\n\n    train(args)\n\n\n'"
train_center.py,16,"b'#!/usr/bin/env python\n# encoding: utf-8\n\'\'\'\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: train_center.py\n@time: 2019/1/3 11:12\n@desc: train script for my attention net and center loss\n\'\'\'\n\n\'\'\'\nPleause use the train.py for your training process.\n\'\'\'\n\nimport os\nimport torch.utils.data\nfrom torch.nn import DataParallel\nfrom datetime import datetime\nfrom backbone.mobilefacenet import MobileFaceNet\nfrom backbone.resnet import ResNet50, ResNet101\nfrom backbone.arcfacenet import SEResNet_IR\nfrom backbone.spherenet import SphereNet\nfrom margin.ArcMarginProduct import ArcMarginProduct\nfrom margin.InnerProduct import InnerProduct\nfrom lossfunctions.centerloss import CenterLoss\nfrom utils.logging import init_log\nfrom dataset.casia_webface import CASIAWebFace\nfrom dataset.lfw import LFW\nfrom dataset.agedb import AgeDB30\nfrom dataset.cfp import CFP_FP\nfrom utils.visualize import Visualizer\nfrom torch.optim import lr_scheduler\nimport torch.optim as optim\nimport time\nfrom eval_lfw import evaluation_10_fold, getFeatureFromTorch\nimport numpy as np\nimport torchvision.transforms as transforms\nimport argparse\n\n\ndef train(args):\n    # gpu init\n    multi_gpus = False\n    if len(args.gpus.split(\',\')) > 1:\n        multi_gpus = True\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpus\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # log init\n    save_dir = os.path.join(args.save_dir, args.model_pre + args.backbone.upper() + \'_\' + datetime.now().strftime(\'%Y%m%d_%H%M%S\'))\n    if os.path.exists(save_dir):\n        raise NameError(\'model dir exists!\')\n    os.makedirs(save_dir)\n    logging = init_log(save_dir)\n    _print = logging.info\n\n    # dataset loader\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n    # validation dataset\n    trainset = CASIAWebFace(args.train_root, args.train_file_list, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n                                              shuffle=True, num_workers=8, drop_last=False)\n    # test dataset\n    lfwdataset = LFW(args.lfw_test_root, args.lfw_file_list, transform=transform)\n    lfwloader = torch.utils.data.DataLoader(lfwdataset, batch_size=128,\n                                             shuffle=False, num_workers=4, drop_last=False)\n    agedbdataset = AgeDB30(args.agedb_test_root, args.agedb_file_list, transform=transform)\n    agedbloader = torch.utils.data.DataLoader(agedbdataset, batch_size=128,\n                                            shuffle=False, num_workers=4, drop_last=False)\n    cfpfpdataset = CFP_FP(args.cfpfp_test_root, args.cfpfp_file_list, transform=transform)\n    cfpfploader = torch.utils.data.DataLoader(cfpfpdataset, batch_size=128,\n                                              shuffle=False, num_workers=4, drop_last=False)\n\n    # define backbone and margin layer\n    if args.backbone == \'MobileFace\':\n        net = MobileFaceNet()\n    elif args.backbone == \'Res50\':\n        net = ResNet50()\n    elif args.backbone == \'Res101\':\n        net = ResNet101()\n    elif args.backbone == \'Res50_IR\':\n        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode=\'ir\')\n    elif args.backbone == \'SERes50_IR\':\n        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode=\'se_ir\')\n    elif args.backbone == \'SphereNet\':\n        net = SphereNet(num_layers=64, feature_dim=args.feature_dim)\n    else:\n        print(args.backbone, \' is not available!\')\n\n    if args.margin_type == \'ArcFace\':\n        margin = ArcMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)\n    elif args.margin_type == \'CosFace\':\n        pass\n    elif args.margin_type == \'SphereFace\':\n        pass\n    elif args.margin_type == \'InnerProduct\':\n        margin = InnerProduct(args.feature_dim, trainset.class_nums)\n    else:\n        print(args.margin_type, \'is not available!\')\n\n    if args.resume:\n        print(\'resume the model parameters from: \', args.net_path, args.margin_path)\n        net.load_state_dict(torch.load(args.net_path)[\'net_state_dict\'])\n        margin.load_state_dict(torch.load(args.margin_path)[\'net_state_dict\'])\n\n    # define optimizers for different layers\n    criterion_classi = torch.nn.CrossEntropyLoss().to(device)\n    optimizer_classi = optim.SGD([\n        {\'params\': net.parameters(), \'weight_decay\': 5e-4},\n        {\'params\': margin.parameters(), \'weight_decay\': 5e-4}\n    ], lr=0.1, momentum=0.9, nesterov=True)\n\n    #criterion_center = CenterLoss(trainset.class_nums, args.feature_dim).to(device)\n    #optimizer_center = optim.SGD(criterion_center.parameters(), lr=0.5)\n\n    scheduler_classi = lr_scheduler.MultiStepLR(optimizer_classi, milestones=[25, 50, 65], gamma=0.1)\n\n    if multi_gpus:\n        net = DataParallel(net).to(device)\n        margin = DataParallel(margin).to(device)\n    else:\n        net = net.to(device)\n        margin = margin.to(device)\n\n    best_lfw_acc = 0.0\n    best_lfw_iters = 0\n    best_agedb30_acc = 0.0\n    best_agedb30_iters = 0\n    best_cfp_fp_acc = 0.0\n    best_cfp_fp_iters = 0\n    total_iters = 0\n    #vis = Visualizer(env=\'softmax_center_xavier\')\n    for epoch in range(1, args.total_epoch + 1):\n        scheduler_classi.step()\n        # train model\n        _print(\'Train Epoch: {}/{} ...\'.format(epoch, args.total_epoch))\n        net.train()\n\n        since = time.time()\n        for data in trainloader:\n            img, label = data[0].to(device), data[1].to(device)\n            feature = net(img)\n            output = margin(feature)\n            loss_classi = criterion_classi(output, label)\n            #loss_center = criterion_center(feature, label)\n            total_loss = loss_classi #+ loss_center * args.weight_center\n\n            optimizer_classi.zero_grad()\n            #optimizer_center.zero_grad()\n            total_loss.backward()\n            optimizer_classi.step()\n            #optimizer_center.step()\n\n            total_iters += 1\n            # print train information\n            if total_iters % 100 == 0:\n                # current training accuracy\n                _, predict = torch.max(output.data, 1)\n                total = label.size(0)\n                correct = (np.array(predict) == np.array(label.data)).sum()\n                time_cur = (time.time() - since) / 100\n                since = time.time()\n                #vis.plot_curves({\'softmax loss\': loss_classi.item(), \'center loss\': loss_center.item()}, iters=total_iters, title=\'train loss\', xlabel=\'iters\', ylabel=\'train loss\')\n                #vis.plot_curves({\'train accuracy\': correct / total}, iters=total_iters, title=\'train accuracy\', xlabel=\'iters\', ylabel=\'train accuracy\')\n                print(""Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, loss_center: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}"".format(total_iters,\n                                                                                                                                          epoch,\n                                                                                                                                          loss_classi.item(),\n                                                                                                                                          loss_center.item(),\n                                                                                                                                          correct/total,\n                                                                                                                                          time_cur,\n                                                                                                                                          scheduler_classi.get_lr()[\n                                                                                                                                              0]))\n            # save model\n            if total_iters % args.save_freq == 0:\n                msg = \'Saving checkpoint: {}\'.format(total_iters)\n                _print(msg)\n                if multi_gpus:\n                    net_state_dict = net.module.state_dict()\n                    margin_state_dict = margin.module.state_dict()\n                else:\n                    net_state_dict = net.state_dict()\n                    margin_state_dict = margin.state_dict()\n\n                if not os.path.exists(save_dir):\n                    os.mkdir(save_dir)\n                torch.save({\n                    \'iters\': total_iters,\n                    \'net_state_dict\': net_state_dict},\n                    os.path.join(save_dir, \'Iter_%06d_net.ckpt\' % total_iters))\n                torch.save({\n                    \'iters\': total_iters,\n                    \'net_state_dict\': margin_state_dict},\n                    os.path.join(save_dir, \'Iter_%06d_margin.ckpt\' % total_iters))\n                #torch.save({\n                #    \'iters\': total_iters,\n                #    \'net_state_dict\': criterion_center.state_dict()},\n                #    os.path.join(save_dir, \'Iter_%06d_center.ckpt\' % total_iters))\n\n            # test accuracy\n            if total_iters % args.test_freq == 0:\n\n                # test model on lfw\n                net.eval()\n                getFeatureFromTorch(\'./result/cur_lfw_result.mat\', net, device, lfwdataset, lfwloader)\n                lfw_accs = evaluation_10_fold(\'./result/cur_lfw_result.mat\')\n                _print(\'LFW Ave Accuracy: {:.4f}\'.format(np.mean(lfw_accs) * 100))\n                if best_lfw_acc < np.mean(lfw_accs) * 100:\n                    best_lfw_acc = np.mean(lfw_accs) * 100\n                    best_lfw_iters = total_iters\n\n                # test model on AgeDB30\n                getFeatureFromTorch(\'./result/cur_agedb30_result.mat\', net, device, agedbdataset, agedbloader)\n                age_accs = evaluation_10_fold(\'./result/cur_agedb30_result.mat\')\n                _print(\'AgeDB-30 Ave Accuracy: {:.4f}\'.format(np.mean(age_accs) * 100))\n                if best_agedb30_acc < np.mean(age_accs) * 100:\n                    best_agedb30_acc = np.mean(age_accs) * 100\n                    best_agedb30_iters = total_iters\n\n                # test model on CFP-FP\n                getFeatureFromTorch(\'./result/cur_cfpfp_result.mat\', net, device, cfpfpdataset, cfpfploader)\n                cfp_accs = evaluation_10_fold(\'./result/cur_cfpfp_result.mat\')\n                _print(\'CFP-FP Ave Accuracy: {:.4f}\'.format(np.mean(cfp_accs) * 100))\n                if best_cfp_fp_acc < np.mean(cfp_accs) * 100:\n                    best_cfp_fp_acc = np.mean(cfp_accs) * 100\n                    best_cfp_fp_iters = total_iters\n                _print(\'Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}\'.format(\n                    best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))\n\n                #vis.plot_curves({\'lfw\': np.mean(lfw_accs), \'agedb-30\': np.mean(age_accs), \'cfp-fp\': np.mean(cfp_accs)}, iters=total_iters,\n                #                title=\'test accuracy\', xlabel=\'iters\', ylabel=\'test accuracy\')\n                net.train()\n\n    _print(\'Finally Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}\'.format(\n        best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))\n    print(\'finishing training\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch for deep face recognition\')\n    parser.add_argument(\'--train_root\', type=str, default=\'/media/ramdisk/webface_align_112\', help=\'train image root\')\n    parser.add_argument(\'--train_file_list\', type=str, default=\'/media/ramdisk/webface_align_train.list\', help=\'train list\')\n    parser.add_argument(\'--lfw_test_root\', type=str, default=\'/media/ramdisk/lfw_align_112\', help=\'lfw image root\')\n    parser.add_argument(\'--lfw_file_list\', type=str, default=\'/media/ramdisk/pairs.txt\', help=\'lfw pair file list\')\n    parser.add_argument(\'--agedb_test_root\', type=str, default=\'/media/sda/AgeDB-30/agedb30_align_112\', help=\'agedb image root\')\n    parser.add_argument(\'--agedb_file_list\', type=str, default=\'/media/sda/AgeDB-30/agedb_30_pair.txt\', help=\'agedb pair file list\')\n    parser.add_argument(\'--cfpfp_test_root\', type=str, default=\'/media/sda/CFP-FP/cfp_fp_aligned_112\', help=\'agedb image root\')\n    parser.add_argument(\'--cfpfp_file_list\', type=str, default=\'/media/sda/CFP-FP/cfp_fp_pair.txt\', help=\'agedb pair file list\')\n\n    parser.add_argument(\'--backbone\', type=str, default=\'MobileFace\', help=\'MobileFace, Res50, Res101, Res50_IR, SERes50_IR, SphereNet\')\n    parser.add_argument(\'--margin_type\', type=str, default=\'InnerProduct\', help=\'InnerProduct, ArcFace, CosFace, SphereFace\')\n    parser.add_argument(\'--feature_dim\', type=int, default=128, help=\'feature dimension, 128 or 512\')\n    parser.add_argument(\'--scale_size\', type=float, default=32.0, help=\'scale size\')\n    parser.add_argument(\'--batch_size\', type=int, default=256, help=\'batch size\')\n    parser.add_argument(\'--total_epoch\', type=int, default=80, help=\'total epochs\')\n    parser.add_argument(\'--weight_center\', type=float, default=0.01, help=\'center loss weight\')\n\n    parser.add_argument(\'--save_freq\', type=int, default=2000, help=\'save frequency\')\n    parser.add_argument(\'--test_freq\', type=int, default=2000, help=\'test frequency\')\n    parser.add_argument(\'--resume\', type=int, default=False, help=\'resume model\')\n    parser.add_argument(\'--net_path\', type=str, default=\'\', help=\'resume model\')\n    parser.add_argument(\'--margin_path\', type=str, default=\'\', help=\'resume model\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'./model\', help=\'model save dir\')\n    parser.add_argument(\'--model_pre\', type=str, default=\'Softmax_Center_\', help=\'model prefix\')\n    parser.add_argument(\'--gpus\', type=str, default=\'0,1\', help=\'model prefix\')\n\n    args = parser.parse_args()\n\n    train(args)\n\n\n'"
train_softmax.py,15,"b'#!/usr/bin/env python\n# encoding: utf-8\n\'\'\'\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: train_softmax.py\n@time: 2019/1/7 8:33\n@desc: original softmax training with Casia-Webface\n\'\'\'\n\'\'\'\nPleause use the train.py for your training process.\n\'\'\'\n\nimport os\nimport torch.utils.data\nfrom torch.nn import DataParallel\nfrom datetime import datetime\nfrom backbone.mobilefacenet import MobileFaceNet\nfrom backbone.resnet import ResNet50, ResNet101\nfrom backbone.arcfacenet import SEResNet_IR\nfrom backbone.spherenet import SphereNet\nfrom margin.ArcMarginProduct import ArcMarginProduct\nfrom margin.InnerProduct import InnerProduct\nfrom utils.visualize import Visualizer\nfrom utils.logging import init_log\nfrom dataset.casia_webface import CASIAWebFace\nfrom dataset.lfw import LFW\nfrom dataset.agedb import AgeDB30\nfrom dataset.cfp import CFP_FP\nfrom torch.optim import lr_scheduler\nimport torch.optim as optim\nimport time\nfrom eval_lfw import evaluation_10_fold, getFeatureFromTorch\nimport numpy as np\nimport torchvision.transforms as transforms\nimport argparse\n\ndef train(args):\n    # gpu init\n    multi_gpus = False\n    if len(args.gpus.split(\',\')) > 1:\n        multi_gpus = True\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpus\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # log init\n    save_dir = os.path.join(args.save_dir, args.model_pre + args.backbone.upper() + \'_\' + datetime.now().strftime(\'%Y%m%d_%H%M%S\'))\n    if os.path.exists(save_dir):\n        raise NameError(\'model dir exists!\')\n    os.makedirs(save_dir)\n    logging = init_log(save_dir)\n    _print = logging.info\n\n    # dataset loader\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n    # validation dataset\n    trainset = CASIAWebFace(args.train_root, args.train_file_list, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n                                              shuffle=True, num_workers=8, drop_last=False)\n    # test dataset\n    lfwdataset = LFW(args.lfw_test_root, args.lfw_file_list, transform=transform)\n    lfwloader = torch.utils.data.DataLoader(lfwdataset, batch_size=128,\n                                             shuffle=False, num_workers=4, drop_last=False)\n    agedbdataset = AgeDB30(args.agedb_test_root, args.agedb_file_list, transform=transform)\n    agedbloader = torch.utils.data.DataLoader(agedbdataset, batch_size=128,\n                                            shuffle=False, num_workers=4, drop_last=False)\n    cfpfpdataset = CFP_FP(args.cfpfp_test_root, args.cfpfp_file_list, transform=transform)\n    cfpfploader = torch.utils.data.DataLoader(cfpfpdataset, batch_size=128,\n                                              shuffle=False, num_workers=4, drop_last=False)\n\n    # define backbone and margin layer\n    if args.backbone == \'MobileFace\':\n        net = MobileFaceNet()\n    elif args.backbone == \'Res50\':\n        net = ResNet50()\n    elif args.backbone == \'Res101\':\n        net = ResNet101()\n    elif args.backbone == \'Res50_IR\':\n        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode=\'ir\')\n    elif args.backbone == \'SERes50_IR\':\n        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode=\'se_ir\')\n    elif args.backbone == \'SphereNet\':\n        net = SphereNet(num_layers=64, feature_dim=args.feature_dim)\n    else:\n        print(args.backbone, \' is not available!\')\n\n    if args.margin_type == \'ArcFace\':\n        margin = ArcMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)\n    elif args.margin_type == \'CosFace\':\n        pass\n    elif args.margin_type == \'SphereFace\':\n        pass\n    elif args.margin_type == \'InnerProduct\':\n        margin = InnerProduct(args.feature_dim, trainset.class_nums)\n    else:\n        print(args.margin_type, \'is not available!\')\n\n    if args.resume:\n        print(\'resume the model parameters from: \', args.net_path, args.margin_path)\n        net.load_state_dict(torch.load(args.net_path)[\'net_state_dict\'])\n        margin.load_state_dict(torch.load(args.margin_path)[\'net_state_dict\'])\n\n    # define optimizers for different layer\n\n    criterion_classi = torch.nn.CrossEntropyLoss().to(device)\n    optimizer_classi = optim.SGD([\n        {\'params\': net.parameters(), \'weight_decay\': 5e-4},\n        {\'params\': margin.parameters(), \'weight_decay\': 5e-4}\n    ], lr=0.1, momentum=0.9, nesterov=True)\n    scheduler_classi = lr_scheduler.MultiStepLR(optimizer_classi, milestones=[20, 35, 45], gamma=0.1)\n\n    if multi_gpus:\n        net = DataParallel(net).to(device)\n        margin = DataParallel(margin).to(device)\n    else:\n        net = net.to(device)\n        margin = margin.to(device)\n\n    best_lfw_acc = 0.0\n    best_lfw_iters = 0\n    best_agedb30_acc = 0.0\n    best_agedb30_iters = 0\n    best_cfp_fp_acc = 0.0\n    best_cfp_fp_iters = 0\n    total_iters = 0\n    vis = Visualizer(env=\'softmax_train\')\n    for epoch in range(1, args.total_epoch + 1):\n        scheduler_classi.step()\n        # train model\n        _print(\'Train Epoch: {}/{} ...\'.format(epoch, args.total_epoch))\n        net.train()\n\n        since = time.time()\n        for data in trainloader:\n            img, label = data[0].to(device), data[1].to(device)\n            feature = net(img)\n            output = margin(feature)\n            loss_classi = criterion_classi(output, label)\n            total_loss = loss_classi\n\n            optimizer_classi.zero_grad()\n            total_loss.backward()\n            optimizer_classi.step()\n\n            total_iters += 1\n            # print train information\n            if total_iters % 100 == 0:\n                #current training accuracy\n                _, predict = torch.max(output.data, 1)\n                total = label.size(0)\n                correct = (np.array(predict) == np.array(label.data)).sum()\n                time_cur = (time.time() - since) / 100\n                since = time.time()\n                vis.plot_curves({\'train loss\': loss_classi.item()}, iters=total_iters, title=\'train loss\', xlabel=\'iters\', ylabel=\'train loss\')\n                vis.plot_curves({\'train accuracy\': correct/total}, iters=total_iters, title=\'train accuracy\', xlabel=\'iters\', ylabel=\'train accuracy\')\n                print(""Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}"".format(total_iters,\n                                                                                                                                          epoch,\n                                                                                                                                          loss_classi.item(),\n                                                                                                                                          correct/total,\n                                                                                                                                          time_cur,\n                                                                                                                                          scheduler_classi.get_lr()[\n                                                                                                                                              0]))\n            # save model\n            if total_iters % args.save_freq == 0:\n                msg = \'Saving checkpoint: {}\'.format(total_iters)\n                _print(msg)\n                if multi_gpus:\n                    net_state_dict = net.module.state_dict()\n                    margin_state_dict = margin.module.state_dict()\n                else:\n                    net_state_dict = net.state_dict()\n                    margin_state_dict = margin.state_dict()\n\n                if not os.path.exists(save_dir):\n                    os.mkdir(save_dir)\n                torch.save({\n                    \'iters\': total_iters,\n                    \'net_state_dict\': net_state_dict},\n                    os.path.join(save_dir, \'Iter_%06d_net.ckpt\' % total_iters))\n                torch.save({\n                    \'iters\': total_iters,\n                    \'net_state_dict\': margin_state_dict},\n                    os.path.join(save_dir, \'Iter_%06d_margin.ckpt\' % total_iters))\n\n            # test accuracy\n            if total_iters % args.test_freq == 0:\n                # test model on lfw\n                net.eval()\n                getFeatureFromTorch(\'./result/cur_lfw_result.mat\', net, device, lfwdataset, lfwloader)\n                lfw_accs = evaluation_10_fold(\'./result/cur_lfw_result.mat\')\n                _print(\'LFW Ave Accuracy: {:.4f}\'.format(np.mean(lfw_accs) * 100))\n                if best_lfw_acc < np.mean(lfw_accs) * 100:\n                    best_lfw_acc = np.mean(lfw_accs) * 100\n                    best_lfw_iters = total_iters\n                # test model on AgeDB30\n                getFeatureFromTorch(\'./result/cur_agedb30_result.mat\', net, device, agedbdataset, agedbloader)\n                age_accs = evaluation_10_fold(\'./result/cur_agedb30_result.mat\')\n                _print(\'AgeDB-30 Ave Accuracy: {:.4f}\'.format(np.mean(age_accs) * 100))\n                if best_agedb30_acc < np.mean(age_accs) * 100:\n                    best_agedb30_acc = np.mean(age_accs) * 100\n                    best_agedb30_iters = total_iters\n                # test model on CFP-FP\n                getFeatureFromTorch(\'./result/cur_cfpfp_result.mat\', net, device, cfpfpdataset, cfpfploader)\n                cfp_accs = evaluation_10_fold(\'./result/cur_cfpfp_result.mat\')\n                _print(\'CFP-FP Ave Accuracy: {:.4f}\'.format(np.mean(cfp_accs) * 100))\n                if best_cfp_fp_acc < np.mean(cfp_accs) * 100:\n                    best_cfp_fp_acc = np.mean(cfp_accs) * 100\n                    best_cfp_fp_iters = total_iters\n                _print(\'Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}\'.format(\n                    best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))\n                vis.plot_curves({\'lfw\': np.mean(lfw_accs), \'agedb-30\': np.mean(age_accs), \'cfp-fp\': np.mean(cfp_accs)}, iters=total_iters, title=\'test accuracy\', xlabel=\'iters\', ylabel=\'test accuracy\')\n                net.train()\n\n    _print(\'Finally Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}\'.format(\n        best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))\n    print(\'finishing training\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch for deep face recognition\')\n    parser.add_argument(\'--train_root\', type=str, default=\'/media/ramdisk/webface_align_112\', help=\'train image root\')\n    parser.add_argument(\'--train_file_list\', type=str, default=\'/media/ramdisk/webface_align_train.list\', help=\'train list\')\n    parser.add_argument(\'--lfw_test_root\', type=str, default=\'/media/ramdisk/lfw_align_112\', help=\'lfw image root\')\n    parser.add_argument(\'--lfw_file_list\', type=str, default=\'/media/ramdisk/pairs.txt\', help=\'lfw pair file list\')\n    parser.add_argument(\'--agedb_test_root\', type=str, default=\'/media/sda/AgeDB-30/agedb30_align_112\', help=\'agedb image root\')\n    parser.add_argument(\'--agedb_file_list\', type=str, default=\'/media/sda/AgeDB-30/agedb_30_pair.txt\', help=\'agedb pair file list\')\n    parser.add_argument(\'--cfpfp_test_root\', type=str, default=\'/media/sda/CFP-FP/cfp_fp_aligned_112\', help=\'agedb image root\')\n    parser.add_argument(\'--cfpfp_file_list\', type=str, default=\'/media/sda/CFP-FP/cfp_fp_pair.txt\', help=\'agedb pair file list\')\n\n    parser.add_argument(\'--backbone\', type=str, default=\'MobileFace\', help=\'MobileFace, Res50, Res101, Res50_IR, SERes50_IR, SphereNet\')\n    parser.add_argument(\'--margin_type\', type=str, default=\'InnerProduct\', help=\'InnerProduct, ArcFace, CosFace, SphereFace\')\n    parser.add_argument(\'--feature_dim\', type=int, default=128, help=\'feature dimension, 128 or 512\')\n    parser.add_argument(\'--scale_size\', type=float, default=32.0, help=\'scale size\')\n    parser.add_argument(\'--batch_size\', type=int, default=256, help=\'batch size\')\n    parser.add_argument(\'--total_epoch\', type=int, default=50, help=\'total epochs\')\n    parser.add_argument(\'--weight_center\', type=float, default=1.0, help=\'center loss weight\')\n\n    parser.add_argument(\'--save_freq\', type=int, default=2000, help=\'save frequency\')\n    parser.add_argument(\'--test_freq\', type=int, default=2000, help=\'test frequency\')\n    parser.add_argument(\'--resume\', type=int, default=False, help=\'resume model\')\n    parser.add_argument(\'--net_path\', type=str, default=\'\', help=\'resume model\')\n    parser.add_argument(\'--margin_path\', type=str, default=\'\', help=\'resume model\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'./model\', help=\'model save dir\')\n    parser.add_argument(\'--model_pre\', type=str, default=\'Softmax_\', help=\'model prefix\')\n    parser.add_argument(\'--gpus\', type=str, default=\'2,3\', help=\'model prefix\')\n\n    args = parser.parse_args()\n\n    train(args)\n\n\n'"
backbone/__init__.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: __init__.py.py\n@time: 2018/12/21 15:30\n@desc:\n'''"""
backbone/arcfacenet.py,1,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: arcfacenet.py\n@time: 2018/12/26 10:15\n@desc: Network structures used in the arcface paper, including ResNet50-IR, ResNet101-IR, SEResNet50-IR, SEResNet101-IR\n\n''''''\nUpdate: This file has been deprecated, all the models build in this class have been rebuild in cbam.py\n        Yet the code in this file still works.\n'''\n\n\nimport torch\nfrom torch import nn\nfrom collections import namedtuple\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n\nclass SEModule(nn.Module):\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n\n        return input * x\n\n\nclass BottleNeck_IR(nn.Module):\n    def __init__(self, in_channel, out_channel, stride):\n        super(BottleNeck_IR, self).__init__()\n        if in_channel == out_channel:\n            self.shortcut_layer = nn.MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=(1, 1), stride=stride, bias=False),\n                nn.BatchNorm2d(out_channel)\n            )\n\n        self.res_layer = nn.Sequential(nn.BatchNorm2d(in_channel),\n                                       nn.Conv2d(in_channel, out_channel, (3, 3), 1, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       nn.PReLU(out_channel),\n                                       nn.Conv2d(out_channel, out_channel, (3, 3), stride, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel))\n\n    def forward(self, x):\n        shortcut = self.shortcut_layer(x)\n        res = self.res_layer(x)\n\n        return shortcut + res\n\nclass BottleNeck_IR_SE(nn.Module):\n    def __init__(self, in_channel, out_channel, stride):\n        super(BottleNeck_IR_SE, self).__init__()\n        if in_channel == out_channel:\n            self.shortcut_layer = nn.MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=(1, 1), stride=stride, bias=False),\n                nn.BatchNorm2d(out_channel)\n            )\n\n        self.res_layer = nn.Sequential(nn.BatchNorm2d(in_channel),\n                                       nn.Conv2d(in_channel, out_channel, (3, 3), 1, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       nn.PReLU(out_channel),\n                                       nn.Conv2d(out_channel, out_channel, (3, 3), stride, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       SEModule(out_channel, 16))\n\n    def forward(self, x):\n        shortcut = self.shortcut_layer(x)\n        res = self.res_layer(x)\n\n        return shortcut + res\n\n\nclass Bottleneck(namedtuple('Block', ['in_channel', 'out_channel', 'stride'])):\n    '''A named tuple describing a ResNet block.'''\n\n\ndef get_block(in_channel, out_channel, num_units, stride=2):\n    return [Bottleneck(in_channel, out_channel, stride)] + [Bottleneck(out_channel, out_channel, 1) for i in range(num_units - 1)]\n\n\ndef get_blocks(num_layers):\n    if num_layers == 50:\n        blocks = [\n            get_block(in_channel=64, out_channel=64, num_units=3),\n            get_block(in_channel=64, out_channel=128, num_units=4),\n            get_block(in_channel=128, out_channel=256, num_units=14),\n            get_block(in_channel=256, out_channel=512, num_units=3)\n        ]\n    elif num_layers == 100:\n        blocks = [\n            get_block(in_channel=64, out_channel=64, num_units=3),\n            get_block(in_channel=64, out_channel=128, num_units=13),\n            get_block(in_channel=128, out_channel=256, num_units=30),\n            get_block(in_channel=256, out_channel=512, num_units=3)\n        ]\n    elif num_layers == 152:\n        blocks = [\n            get_block(in_channel=64, out_channel=64, num_units=3),\n            get_block(in_channel=64, out_channel=128, num_units=8),\n            get_block(in_channel=128, out_channel=256, num_units=36),\n            get_block(in_channel=256, out_channel=512, num_units=3)\n        ]\n    return blocks\n\n\nclass SEResNet_IR(nn.Module):\n    def __init__(self, num_layers, feature_dim=512, drop_ratio=0.4, mode = 'ir'):\n        super(SEResNet_IR, self).__init__()\n        assert num_layers in [50, 100, 152], 'num_layers should be 50, 100 or 152'\n        assert mode in ['ir', 'se_ir'], 'mode should be ir or se_ir'\n        blocks = get_blocks(num_layers)\n        if mode == 'ir':\n            unit_module = BottleNeck_IR\n        elif mode == 'se_ir':\n            unit_module = BottleNeck_IR_SE\n        self.input_layer = nn.Sequential(nn.Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n                                         nn.BatchNorm2d(64),\n                                         nn.PReLU(64))\n\n        self.output_layer = nn.Sequential(nn.BatchNorm2d(512),\n                                          nn.Dropout(drop_ratio),\n                                          Flatten(),\n                                          nn.Linear(512 * 7 * 7, feature_dim),\n                                          nn.BatchNorm1d(feature_dim))\n        modules = []\n        for block in blocks:\n            for bottleneck in block:\n                modules.append(\n                    unit_module(bottleneck.in_channel,\n                                bottleneck.out_channel,\n                                bottleneck.stride))\n        self.body = nn.Sequential(*modules)\n\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.body(x)\n        x = self.output_layer(x)\n\n        return x\n\n\nif __name__ == '__main__':\n    input = torch.Tensor(2, 3, 112, 112)\n    net = SEResNet_IR(100, mode='se_ir')\n    print(net)\n\n    x = net(input)\n    print(x.shape)"""
backbone/attention.py,2,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: attention.py\n@time: 2019/2/14 14:12\n@desc: Residual Attention Network for Image Classification, CVPR 2017.\n       Attention 56 and Attention 92.\n'''\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\nclass ResidualBlock(nn.Module):\n\n    def __init__(self, in_channel, out_channel, stride=1):\n        super(ResidualBlock, self).__init__()\n        self.in_channel = in_channel\n        self.out_channel = out_channel\n        self.stride = stride\n\n        self.res_bottleneck = nn.Sequential(nn.BatchNorm2d(in_channel),\n                                            nn.ReLU(inplace=True),\n                                            nn.Conv2d(in_channel, out_channel//4, 1, 1, bias=False),\n                                            nn.BatchNorm2d(out_channel//4),\n                                            nn.ReLU(inplace=True),\n                                            nn.Conv2d(out_channel//4, out_channel//4, 3, stride, padding=1, bias=False),\n                                            nn.BatchNorm2d(out_channel//4),\n                                            nn.ReLU(inplace=True),\n                                            nn.Conv2d(out_channel//4, out_channel, 1, 1, bias=False))\n        self.shortcut = nn.Conv2d(in_channel, out_channel, 1, stride, bias=False)\n\n    def forward(self, x):\n        res = x\n        out = self.res_bottleneck(x)\n        if self.in_channel != self.out_channel or self.stride != 1:\n            res = self.shortcut(x)\n\n        out += res\n        return out\n\nclass AttentionModule_stage1(nn.Module):\n\n    # input size is 56*56\n    def __init__(self, in_channel, out_channel, size1=(56, 56), size2=(28, 28), size3=(14, 14)):\n        super(AttentionModule_stage1, self).__init__()\n        self.share_residual_block = ResidualBlock(in_channel, out_channel)\n        self.trunk_branches = nn.Sequential(ResidualBlock(in_channel, out_channel),\n                                            ResidualBlock(in_channel, out_channel))\n\n        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.mask_block1 = ResidualBlock(in_channel, out_channel)\n        self.skip_connect1 = ResidualBlock(in_channel, out_channel)\n\n        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.mask_block2 = ResidualBlock(in_channel, out_channel)\n        self.skip_connect2 = ResidualBlock(in_channel, out_channel)\n\n        self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.mask_block3 = nn.Sequential(ResidualBlock(in_channel, out_channel),\n                                         ResidualBlock(in_channel, out_channel))\n\n        self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)\n        self.mask_block4 = ResidualBlock(in_channel, out_channel)\n\n        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n        self.mask_block5 = ResidualBlock(in_channel, out_channel)\n\n        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n        self.mask_block6 = nn.Sequential(nn.BatchNorm2d(out_channel),\n                                         nn.ReLU(inplace=True),\n                                         nn.Conv2d(out_channel, out_channel, 1, 1, bias=False),\n                                         nn.BatchNorm2d(out_channel),\n                                         nn.ReLU(inplace=True),\n                                         nn.Conv2d(out_channel, out_channel, 1, 1, bias=False),\n                                         nn.Sigmoid())\n\n        self.last_block = ResidualBlock(in_channel, out_channel)\n\n    def forward(self, x):\n        x = self.share_residual_block(x)\n        out_trunk = self.trunk_branches(x)\n\n        out_pool1 = self.mpool1(x)\n        out_block1 = self.mask_block1(out_pool1)\n        out_skip_connect1 = self.skip_connect1(out_block1)\n\n        out_pool2 = self.mpool2(out_block1)\n        out_block2 = self.mask_block2(out_pool2)\n        out_skip_connect2 = self.skip_connect2(out_block2)\n\n        out_pool3 = self.mpool3(out_block2)\n        out_block3 = self.mask_block3(out_pool3)\n        #\n        out_inter3 = self.interpolation3(out_block3) + out_block2\n        out = out_inter3 + out_skip_connect2\n        out_block4 = self.mask_block4(out)\n\n        out_inter2 = self.interpolation2(out_block4) + out_block1\n        out = out_inter2 + out_skip_connect1\n        out_block5 = self.mask_block5(out)\n\n        out_inter1 = self.interpolation1(out_block5) + out_trunk\n        out_block6 = self.mask_block6(out_inter1)\n\n        out = (1 + out_block6) + out_trunk\n        out_last = self.last_block(out)\n\n        return out_last\n\nclass AttentionModule_stage2(nn.Module):\n\n    # input image size is 28*28\n    def __init__(self, in_channels, out_channels, size1=(28, 28), size2=(14, 14)):\n        super(AttentionModule_stage2, self).__init__()\n        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n\n        self.trunk_branches = nn.Sequential(\n            ResidualBlock(in_channels, out_channels),\n            ResidualBlock(in_channels, out_channels)\n         )\n\n        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n\n        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.softmax2_blocks = nn.Sequential(\n            ResidualBlock(in_channels, out_channels),\n            ResidualBlock(in_channels, out_channels)\n        )\n\n        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n        self.softmax3_blocks = ResidualBlock(in_channels, out_channels)\n        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n        self.softmax4_blocks = nn.Sequential(\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n            nn.Sigmoid()\n        )\n        self.last_blocks = ResidualBlock(in_channels, out_channels)\n\n    def forward(self, x):\n        x = self.first_residual_blocks(x)\n        out_trunk = self.trunk_branches(x)\n        out_mpool1 = self.mpool1(x)\n        out_softmax1 = self.softmax1_blocks(out_mpool1)\n        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n\n        out_mpool2 = self.mpool2(out_softmax1)\n        out_softmax2 = self.softmax2_blocks(out_mpool2)\n\n        out_interp2 = self.interpolation2(out_softmax2) + out_softmax1\n        out = out_interp2 + out_skip1_connection\n\n        out_softmax3 = self.softmax3_blocks(out)\n        out_interp1 = self.interpolation1(out_softmax3) + out_trunk\n        out_softmax4 = self.softmax4_blocks(out_interp1)\n        out = (1 + out_softmax4) * out_trunk\n        out_last = self.last_blocks(out)\n\n        return out_last\n\nclass AttentionModule_stage3(nn.Module):\n\n    # input image size is 14*14\n    def __init__(self, in_channels, out_channels, size1=(14, 14)):\n        super(AttentionModule_stage3, self).__init__()\n        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n\n        self.trunk_branches = nn.Sequential(\n            ResidualBlock(in_channels, out_channels),\n            ResidualBlock(in_channels, out_channels)\n         )\n\n        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.softmax1_blocks = nn.Sequential(\n            ResidualBlock(in_channels, out_channels),\n            ResidualBlock(in_channels, out_channels)\n        )\n\n        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n\n        self.softmax2_blocks = nn.Sequential(\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n            nn.Sigmoid()\n        )\n\n        self.last_blocks = ResidualBlock(in_channels, out_channels)\n\n    def forward(self, x):\n        x = self.first_residual_blocks(x)\n        out_trunk = self.trunk_branches(x)\n        out_mpool1 = self.mpool1(x)\n        out_softmax1 = self.softmax1_blocks(out_mpool1)\n\n        out_interp1 = self.interpolation1(out_softmax1) + out_trunk\n        out_softmax2 = self.softmax2_blocks(out_interp1)\n        out = (1 + out_softmax2) * out_trunk\n        out_last = self.last_blocks(out)\n\n        return out_last\n\nclass ResidualAttentionNet_56(nn.Module):\n\n    # for input size 112\n    def __init__(self, feature_dim=512, drop_ratio=0.4):\n        super(ResidualAttentionNet_56, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.residual_block1 = ResidualBlock(64, 256)\n        self.attention_module1 = AttentionModule_stage1(256, 256)\n        self.residual_block2 = ResidualBlock(256, 512, 2)\n        self.attention_module2 = AttentionModule_stage2(512, 512)\n        self.residual_block3 = ResidualBlock(512, 512, 2)\n        self.attention_module3 = AttentionModule_stage3(512, 512)\n        self.residual_block4 = ResidualBlock(512, 512, 2)\n        self.residual_block5 = ResidualBlock(512, 512)\n        self.residual_block6 = ResidualBlock(512, 512)\n        self.output_layer = nn.Sequential(nn.BatchNorm2d(512),\n                                          nn.Dropout(drop_ratio),\n                                          Flatten(),\n                                          nn.Linear(512 * 7 * 7, feature_dim),\n                                          nn.BatchNorm1d(feature_dim))\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.mpool1(out)\n        # print(out.data)\n        out = self.residual_block1(out)\n        out = self.attention_module1(out)\n        out = self.residual_block2(out)\n        out = self.attention_module2(out)\n        out = self.residual_block3(out)\n        # print(out.data)\n        out = self.attention_module3(out)\n        out = self.residual_block4(out)\n        out = self.residual_block5(out)\n        out = self.residual_block6(out)\n        out = self.output_layer(out)\n\n        return out\n\nclass ResidualAttentionNet_92(nn.Module):\n\n    # for input size 112\n    def __init__(self, feature_dim=512, drop_ratio=0.4):\n        super(ResidualAttentionNet_92, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.residual_block1 = ResidualBlock(64, 256)\n        self.attention_module1 = AttentionModule_stage1(256, 256)\n        self.residual_block2 = ResidualBlock(256, 512, 2)\n        self.attention_module2 = AttentionModule_stage2(512, 512)\n        self.attention_module2_2 = AttentionModule_stage2(512, 512)  # tbq add\n        self.residual_block3 = ResidualBlock(512, 1024, 2)\n        self.attention_module3 = AttentionModule_stage3(1024, 1024)\n        self.attention_module3_2 = AttentionModule_stage3(1024, 1024)  # tbq add\n        self.attention_module3_3 = AttentionModule_stage3(1024, 1024)  # tbq add\n        self.residual_block4 = ResidualBlock(1024, 2048, 2)\n        self.residual_block5 = ResidualBlock(2048, 2048)\n        self.residual_block6 = ResidualBlock(2048, 2048)\n        self.output_layer = nn.Sequential(nn.BatchNorm2d(2048),\n                                          nn.Dropout(drop_ratio),\n                                          Flatten(),\n                                          nn.Linear(2048 * 7 * 7, feature_dim),\n                                          nn.BatchNorm1d(feature_dim))\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.mpool1(out)\n        # print(out.data)\n        out = self.residual_block1(out)\n        out = self.attention_module1(out)\n        out = self.residual_block2(out)\n        out = self.attention_module2(out)\n        out = self.attention_module2_2(out)\n        out = self.residual_block3(out)\n        # print(out.data)\n        out = self.attention_module3(out)\n        out = self.attention_module3_2(out)\n        out = self.attention_module3_3(out)\n        out = self.residual_block4(out)\n        out = self.residual_block5(out)\n        out = self.residual_block6(out)\n        out = self.output_layer(out)\n\n        return out\n\n\nif __name__ == '__main__':\n    input = torch.Tensor(2, 3, 112, 112)\n    net = ResidualAttentionNet_56()\n    print(net)\n\n    x = net(input)\n    print(x.shape)"""
backbone/cbam.py,4,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: cbam.py\n@time: 2019/1/14 15:33\n@desc: Convolutional Block Attention Module in ECCV 2018, including channel attention module and spatial attention module.\n'''\n\nimport torch\nfrom torch import nn\nimport time\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\nclass SEModule(nn.Module):\n    '''Squeeze and Excitation Module'''\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n\n        return input * x\n\nclass CAModule(nn.Module):\n    '''Channel Attention Module'''\n    def __init__(self, channels, reduction):\n        super(CAModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.shared_mlp = nn.Sequential(nn.Conv2d(channels, channels // reduction, kernel_size=1, padding=0, bias=False),\n                                        nn.ReLU(inplace=True),\n                                        nn.Conv2d(channels // reduction, channels, kernel_size=1, padding=0, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        input = x\n        avg_pool = self.avg_pool(x)\n        max_pool = self.max_pool(x)\n        x = self.shared_mlp(avg_pool) + self.shared_mlp(max_pool)\n        x = self.sigmoid(x)\n\n        return input * x\n\nclass SAModule(nn.Module):\n    '''Spatial Attention Module'''\n    def __init__(self):\n        super(SAModule, self).__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        input = x\n        avg_c = torch.mean(x, 1, True)\n        max_c, _ = torch.max(x, 1, True)\n        x = torch.cat((avg_c, max_c), 1)\n        x = self.conv(x)\n        x = self.sigmoid(x)\n        return input * x\n\nclass BottleNeck_IR(nn.Module):\n    '''Improved Residual Bottlenecks'''\n    def __init__(self, in_channel, out_channel, stride, dim_match):\n        super(BottleNeck_IR, self).__init__()\n        self.res_layer = nn.Sequential(nn.BatchNorm2d(in_channel),\n                                       nn.Conv2d(in_channel, out_channel, (3, 3), 1, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       nn.PReLU(out_channel),\n                                       nn.Conv2d(out_channel, out_channel, (3, 3), stride, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel))\n        if dim_match:\n            self.shortcut_layer = None\n        else:\n            self.shortcut_layer = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=(1, 1), stride=stride, bias=False),\n                nn.BatchNorm2d(out_channel)\n            )\n\n    def forward(self, x):\n        shortcut = x\n        res = self.res_layer(x)\n\n        if self.shortcut_layer is not None:\n            shortcut = self.shortcut_layer(x)\n\n        return shortcut + res\n\nclass BottleNeck_IR_SE(nn.Module):\n    '''Improved Residual Bottlenecks with Squeeze and Excitation Module'''\n    def __init__(self, in_channel, out_channel, stride, dim_match):\n        super(BottleNeck_IR_SE, self).__init__()\n        self.res_layer = nn.Sequential(nn.BatchNorm2d(in_channel),\n                                       nn.Conv2d(in_channel, out_channel, (3, 3), 1, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       nn.PReLU(out_channel),\n                                       nn.Conv2d(out_channel, out_channel, (3, 3), stride, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       SEModule(out_channel, 16))\n        if dim_match:\n            self.shortcut_layer = None\n        else:\n            self.shortcut_layer = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=(1, 1), stride=stride, bias=False),\n                nn.BatchNorm2d(out_channel)\n            )\n\n    def forward(self, x):\n        shortcut = x\n        res = self.res_layer(x)\n\n        if self.shortcut_layer is not None:\n            shortcut = self.shortcut_layer(x)\n\n        return shortcut + res\n\nclass BottleNeck_IR_CAM(nn.Module):\n    '''Improved Residual Bottlenecks with Channel Attention Module'''\n    def __init__(self, in_channel, out_channel, stride, dim_match):\n        super(BottleNeck_IR_CAM, self).__init__()\n        self.res_layer = nn.Sequential(nn.BatchNorm2d(in_channel),\n                                       nn.Conv2d(in_channel, out_channel, (3, 3), 1, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       nn.PReLU(out_channel),\n                                       nn.Conv2d(out_channel, out_channel, (3, 3), stride, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       CAModule(out_channel, 16))\n        if dim_match:\n            self.shortcut_layer = None\n        else:\n            self.shortcut_layer = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=(1, 1), stride=stride, bias=False),\n                nn.BatchNorm2d(out_channel)\n            )\n\n    def forward(self, x):\n        shortcut = x\n        res = self.res_layer(x)\n\n        if self.shortcut_layer is not None:\n            shortcut = self.shortcut_layer(x)\n\n        return shortcut + res\n\nclass BottleNeck_IR_SAM(nn.Module):\n    '''Improved Residual Bottlenecks with Spatial Attention Module'''\n    def __init__(self, in_channel, out_channel, stride, dim_match):\n        super(BottleNeck_IR_SAM, self).__init__()\n        self.res_layer = nn.Sequential(nn.BatchNorm2d(in_channel),\n                                       nn.Conv2d(in_channel, out_channel, (3, 3), 1, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       nn.PReLU(out_channel),\n                                       nn.Conv2d(out_channel, out_channel, (3, 3), stride, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       SAModule())\n        if dim_match:\n            self.shortcut_layer = None\n        else:\n            self.shortcut_layer = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=(1, 1), stride=stride, bias=False),\n                nn.BatchNorm2d(out_channel)\n            )\n\n    def forward(self, x):\n        shortcut = x\n        res = self.res_layer(x)\n\n        if self.shortcut_layer is not None:\n            shortcut = self.shortcut_layer(x)\n\n        return shortcut + res\n\nclass BottleNeck_IR_CBAM(nn.Module):\n    '''Improved Residual Bottleneck with Channel Attention Module and Spatial Attention Module'''\n    def __init__(self, in_channel, out_channel, stride, dim_match):\n        super(BottleNeck_IR_CBAM, self).__init__()\n        self.res_layer = nn.Sequential(nn.BatchNorm2d(in_channel),\n                                       nn.Conv2d(in_channel, out_channel, (3, 3), 1, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       nn.PReLU(out_channel),\n                                       nn.Conv2d(out_channel, out_channel, (3, 3), stride, 1, bias=False),\n                                       nn.BatchNorm2d(out_channel),\n                                       CAModule(out_channel, 16),\n                                       SAModule()\n                                       )\n        if dim_match:\n            self.shortcut_layer = None\n        else:\n            self.shortcut_layer = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=(1, 1), stride=stride, bias=False),\n                nn.BatchNorm2d(out_channel)\n            )\n\n    def forward(self, x):\n        shortcut = x\n        res = self.res_layer(x)\n\n        if self.shortcut_layer is not None:\n            shortcut = self.shortcut_layer(x)\n\n        return shortcut + res\n\n\nfilter_list = [64, 64, 128, 256, 512]\ndef get_layers(num_layers):\n    if num_layers == 50:\n        return [3, 4, 14, 3]\n    elif num_layers == 100:\n        return [3, 13, 30, 3]\n    elif num_layers == 152:\n        return [3, 8, 36, 3]\n\nclass CBAMResNet(nn.Module):\n    def __init__(self, num_layers, feature_dim=512, drop_ratio=0.4, mode='ir',filter_list=filter_list):\n        super(CBAMResNet, self).__init__()\n        assert num_layers in [50, 100, 152], 'num_layers should be 50, 100 or 152'\n        assert mode in ['ir', 'ir_se', 'ir_cam', 'ir_sam', 'ir_cbam'], 'mode should be ir, ir_se, ir_cam, ir_sam or ir_cbam'\n        layers = get_layers(num_layers)\n        if mode == 'ir':\n            block = BottleNeck_IR\n        elif mode == 'ir_se':\n            block = BottleNeck_IR_SE\n        elif mode == 'ir_cam':\n            block = BottleNeck_IR_CAM\n        elif mode == 'ir_sam':\n            block = BottleNeck_IR_SAM\n        elif mode == 'ir_cbam':\n            block = BottleNeck_IR_CBAM\n\n        self.input_layer = nn.Sequential(nn.Conv2d(3, 64, (3, 3), stride=1, padding=1, bias=False),\n                                         nn.BatchNorm2d(64),\n                                         nn.PReLU(64))\n        self.layer1 = self._make_layer(block, filter_list[0], filter_list[1], layers[0], stride=2)\n        self.layer2 = self._make_layer(block, filter_list[1], filter_list[2], layers[1], stride=2)\n        self.layer3 = self._make_layer(block, filter_list[2], filter_list[3], layers[2], stride=2)\n        self.layer4 = self._make_layer(block, filter_list[3], filter_list[4], layers[3], stride=2)\n\n        self.output_layer = nn.Sequential(nn.BatchNorm2d(512),\n                                          nn.Dropout(drop_ratio),\n                                          Flatten(),\n                                          nn.Linear(512 * 7 * 7, feature_dim),\n                                          nn.BatchNorm1d(feature_dim))\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, in_channel, out_channel, blocks, stride):\n        layers = []\n        layers.append(block(in_channel, out_channel, stride, False))\n        for i in range(1, blocks):\n            layers.append(block(out_channel, out_channel, 1, True))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.output_layer(x)\n\n        return x\n\nif __name__ == '__main__':\n    input = torch.Tensor(2, 3, 112, 112)\n    net = CBAMResNet(50, mode='ir')\n\n    out = net(input)\n    print(out.shape)\n"""
backbone/mobilefacenet.py,1,"b'#!/usr/bin/env python\n# encoding: utf-8\n\'\'\'\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: mobilefacenet.py\n@time: 2018/12/21 15:45\n@desc: mobilefacenet backbone\n\'\'\'\n\nimport torch\nfrom torch import nn\nimport math\n\nMobileFaceNet_BottleNeck_Setting = [\n    # t, c , n ,s\n    [2, 64, 5, 2],\n    [4, 128, 1, 2],\n    [2, 128, 6, 1],\n    [4, 128, 1, 2],\n    [2, 128, 2, 1]\n]\n\nclass BottleNeck(nn.Module):\n    def __init__(self, inp, oup, stride, expansion):\n        super(BottleNeck, self).__init__()\n        self.connect = stride == 1 and inp == oup\n\n        self.conv = nn.Sequential(\n            # 1*1 conv\n            nn.Conv2d(inp, inp * expansion, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(inp * expansion),\n            nn.PReLU(inp * expansion),\n\n            # 3*3 depth wise conv\n            nn.Conv2d(inp * expansion, inp * expansion, 3, stride, 1, groups=inp * expansion, bias=False),\n            nn.BatchNorm2d(inp * expansion),\n            nn.PReLU(inp * expansion),\n\n            # 1*1 conv\n            nn.Conv2d(inp * expansion, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        )\n\n    def forward(self, x):\n        if self.connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, inp, oup, k, s, p, dw=False, linear=False):\n        super(ConvBlock, self).__init__()\n        self.linear = linear\n        if dw:\n            self.conv = nn.Conv2d(inp, oup, k, s, p, groups=inp, bias=False)\n        else:\n            self.conv = nn.Conv2d(inp, oup, k, s, p, bias=False)\n\n        self.bn = nn.BatchNorm2d(oup)\n        if not linear:\n            self.prelu = nn.PReLU(oup)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.linear:\n            return x\n        else:\n            return self.prelu(x)\n\n\nclass MobileFaceNet(nn.Module):\n    def __init__(self, feature_dim=128, bottleneck_setting=MobileFaceNet_BottleNeck_Setting):\n        super(MobileFaceNet, self).__init__()\n        self.conv1 = ConvBlock(3, 64, 3, 2, 1)\n        self.dw_conv1 = ConvBlock(64, 64, 3, 1, 1, dw=True)\n\n        self.cur_channel = 64\n        block = BottleNeck\n        self.blocks = self._make_layer(block, bottleneck_setting)\n\n        self.conv2 = ConvBlock(128, 512, 1, 1, 0)\n        self.linear7 = ConvBlock(512, 512, 7, 1, 0, dw=True, linear=True)\n        self.linear1 = ConvBlock(512, feature_dim, 1, 1, 0, linear=True)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, setting):\n        layers = []\n        for t, c, n, s in setting:\n            for i in range(n):\n                if i == 0:\n                    layers.append(block(self.cur_channel, c, s, t))\n                else:\n                    layers.append(block(self.cur_channel, c, 1, t))\n                self.cur_channel = c\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.dw_conv1(x)\n        x = self.blocks(x)\n        x = self.conv2(x)\n        x = self.linear7(x)\n        x = self.linear1(x)\n        x = x.view(x.size(0), -1)\n\n        return x\n\n\nif __name__ == ""__main__"":\n    input = torch.Tensor(2, 3, 112, 112)\n    net = MobileFaceNet()\n    print(net)\n\n    x = net(input)\n    print(x.shape)'"
backbone/resnet.py,2,"b'#!/usr/bin/env python\n# encoding: utf-8\n\'\'\'\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: resnet.py\n@time: 2018/12/24 14:40\n@desc: Original ResNet backbone, including ResNet18, ResNet34, ResNet50, ResNet101 and ResNet152, we removed the last global average pooling layer\n       and replaced it with a fully connected layer with dimension of 512. BN is used for fast convergence.\n\'\'\'\nimport torch\nimport torch.nn as nn\n\ndef ResNet18():\n    model = ResNet(BasicBlock, [2, 2, 2, 2])\n    return model\n\ndef ResNet34():\n    model = ResNet(BasicBlock, [3, 4, 6, 3])\n    return model\n\ndef ResNet50():\n    model = ResNet(Bottleneck, [3, 4, 6, 3])\n    return model\n\ndef ResNet101():\n    model = ResNet(Bottleneck, [3, 4, 23, 3])\n    return model\n\ndef ResNet152():\n    model = ResNet(Bottleneck, [3, 8, 36, 3])\n    return model\n\n__all__ = [\'ResNet\', \'ResNet18\', \'ResNet34\', \'ResNet50\', \'ResNet101\', \'ResNet152\']\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, stride=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, feature_dim=512, drop_ratio=0.4, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        self.output_layer = nn.Sequential(nn.BatchNorm2d(512 * block.expansion),\n                                          nn.Dropout(drop_ratio),\n                                          Flatten(),\n                                          nn.Linear(512 * block.expansion * 7 * 7, feature_dim),\n                                          nn.BatchNorm1d(feature_dim))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.output_layer(x)\n\n        return x\n\n\nif __name__ == ""__main__"":\n    input = torch.Tensor(2, 3, 112, 112)\n    net = ResNet50()\n    print(net)\n\n    x = net(input)\n    print(x.shape)'"
backbone/spherenet.py,2,"b'#!/usr/bin/env python\n# encoding: utf-8\n\'\'\'\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: spherenet.py\n@time: 2018/12/26 10:14\n@desc: A 64 layer residual network struture used in sphereface and cosface, for fast convergence, I add BN after every Conv layer.\n\'\'\'\n\nimport torch\nimport torch.nn as nn\n\nclass Block(nn.Module):\n    def __init__(self, channels):\n        super(Block, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.prelu1 = nn.PReLU(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n        self.prelu2 = nn.PReLU(channels)\n\n    def forward(self, x):\n        short_cut = x\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.prelu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.prelu2(x)\n\n        return x + short_cut\n\n\nclass SphereNet(nn.Module):\n    def __init__(self, num_layers = 20, feature_dim=512):\n        super(SphereNet, self).__init__()\n        assert num_layers in [20, 64], \'SphereNet num_layers should be 20 or 64\'\n        if num_layers == 20:\n            layers = [1, 2, 4, 1]\n        elif num_layers == 64:\n            layers = [3, 7, 16, 3]\n        else:\n            raise ValueError(\'sphere\' + str(num_layers) + "" IS NOT SUPPORTED! (sphere20 or sphere64)"")\n\n        filter_list = [3, 64, 128, 256, 512]\n        block = Block\n        self.layer1 = self._make_layer(block, filter_list[0], filter_list[1], layers[0], stride=2)\n        self.layer2 = self._make_layer(block, filter_list[1], filter_list[2], layers[1], stride=2)\n        self.layer3 = self._make_layer(block, filter_list[2], filter_list[3], layers[2], stride=2)\n        self.layer4 = self._make_layer(block, filter_list[3], filter_list[4], layers[3], stride=2)\n        self.fc = nn.Linear(512 * 7 * 7, feature_dim)\n        self.last_bn = nn.BatchNorm1d(feature_dim)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                if m.bias is not None:\n                    nn.init.xavier_uniform_(m.weight)\n                    nn.init.constant_(m.bias, 0)\n                else:\n                    nn.init.normal_(m.weight, 0, 0.01)\n\n    def _make_layer(self, block, inplanes, planes, num_units, stride):\n        layers = []\n        layers.append(nn.Conv2d(inplanes, planes, 3, stride, 1))\n        layers.append(nn.BatchNorm2d(planes))\n        layers.append(nn.PReLU(planes))\n        for i in range(num_units):\n            layers.append(block(planes))\n\n        return nn.Sequential(*layers)\n\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = self.last_bn(x)\n\n        return x\n\n\nif __name__ == \'__main__\':\n    input = torch.Tensor(2, 3, 112, 112)\n    net = SphereNet(num_layers=64, feature_dim=512)\n\n    out = net(input)\n    print(out.shape)\n\n'"
cppapi/pytorch2torchscript.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: pytorch2torchscript.py\n@time: 2019/2/18 17:45\n@desc: convert your pytorch model to torch script and save to file\n'''\n\n"""
dataset/__init__.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: __init__.py.py\n@time: 2018/12/21 15:31\n@desc:\n'''"""
dataset/agedb.py,2,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: agedb.py.py\n@time: 2018/12/25 18:43\n@desc: AgeDB-30 test data loader, agedb test protocol is the same with lfw\n'''\n\nimport numpy as np\nimport cv2\nimport os\nimport torch.utils.data as data\n\nimport torch\nimport torchvision.transforms as transforms\n\ndef img_loader(path):\n    try:\n        with open(path, 'rb') as f:\n            img = cv2.imread(path)\n            if len(img.shape) == 2:\n                img = np.stack([img] * 3, 2)\n            return img\n    except IOError:\n        print('Cannot load image ' + path)\n\nclass AgeDB30(data.Dataset):\n    def __init__(self, root, file_list, transform=None, loader=img_loader):\n\n        self.root = root\n        self.file_list = file_list\n        self.transform = transform\n        self.loader = loader\n        self.nameLs = []\n        self.nameRs = []\n        self.folds = []\n        self.flags = []\n\n        with open(file_list) as f:\n            pairs = f.read().splitlines()\n        for i, p in enumerate(pairs):\n            p = p.split(' ')\n            nameL = p[0]\n            nameR = p[1]\n            fold = i // 600\n            flag = int(p[2])\n\n            self.nameLs.append(nameL)\n            self.nameRs.append(nameR)\n            self.folds.append(fold)\n            self.flags.append(flag)\n\n    def __getitem__(self, index):\n\n        img_l = self.loader(os.path.join(self.root, self.nameLs[index]))\n        img_r = self.loader(os.path.join(self.root, self.nameRs[index]))\n        imglist = [img_l, cv2.flip(img_l, 1), img_r, cv2.flip(img_r, 1)]\n\n        if self.transform is not None:\n            for i in range(len(imglist)):\n                imglist[i] = self.transform(imglist[i])\n\n            imgs = imglist\n            return imgs\n        else:\n            imgs = [torch.from_numpy(i) for i in imglist]\n            return imgs\n\n    def __len__(self):\n        return len(self.nameLs)\n\n\nif __name__ == '__main__':\n    root = '/media/sda/AgeDB-30/agedb30_align_112'\n    file_list = '/media/sda/AgeDB-30/agedb_30_pair.txt'\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n\n    dataset = AgeDB30(root, file_list, transform=transform)\n    trainloader = data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2, drop_last=False)\n    for data in trainloader:\n        for d in data:\n            print(d[0].shape)"""
dataset/casia_webface.py,2,"b'#!/usr/bin/env python\n# encoding: utf-8\n\'\'\'\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: casia_webface.py\n@time: 2018/12/21 19:09\n@desc: CASIA-WebFace dataset loader\n\'\'\'\n\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nimport numpy as np\nimport cv2\nimport os\nimport torch\n\n\ndef img_loader(path):\n    try:\n        with open(path, \'rb\') as f:\n            img = cv2.imread(path)\n            if len(img.shape) == 2:\n                img = np.stack([img] * 3, 2)\n            return img\n    except IOError:\n        print(\'Cannot load image \' + path)\n\n\nclass CASIAWebFace(data.Dataset):\n    def __init__(self, root, file_list, transform=None, loader=img_loader):\n\n        self.root = root\n        self.transform = transform\n        self.loader = loader\n\n        image_list = []\n        label_list = []\n        with open(file_list) as f:\n            img_label_list = f.read().splitlines()\n        for info in img_label_list:\n            image_path, label_name = info.split(\' \')\n            image_list.append(image_path)\n            label_list.append(int(label_name))\n\n        self.image_list = image_list\n        self.label_list = label_list\n        self.class_nums = len(np.unique(self.label_list))\n        print(""dataset size: "", len(self.image_list), \'/\', self.class_nums)\n\n    def __getitem__(self, index):\n        img_path = self.image_list[index]\n        label = self.label_list[index]\n\n        img = self.loader(os.path.join(self.root, img_path))\n\n        # random flip with ratio of 0.5\n        flip = np.random.choice(2) * 2 - 1\n        if flip == 1:\n            img = cv2.flip(img, 1)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img)\n\n        return img, label\n\n    def __len__(self):\n        return len(self.image_list)\n\n\nif __name__ == \'__main__\':\n    root = \'D:/data/webface_align_112\'\n    file_list = \'D:/data/webface_align_train.list\'\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n    dataset = CASIAWebFace(root, file_list, transform=transform)\n    trainloader = data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2, drop_last=False)\n    print(len(dataset))\n    for data in trainloader:\n        print(data[0].shape)'"
dataset/cfp.py,2,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: cfp.py\n@time: 2018/12/26 16:19\n@desc: the CFP-FP test dataset loader, it's similar with lfw and adedb, except that it has 700 pairs every fold\n'''\n\n\nimport numpy as np\nimport cv2\nimport os\nimport torch.utils.data as data\n\nimport torch\nimport torchvision.transforms as transforms\n\ndef img_loader(path):\n    try:\n        with open(path, 'rb') as f:\n            img = cv2.imread(path)\n            if len(img.shape) == 2:\n                img = np.stack([img] * 3, 2)\n            return img\n    except IOError:\n        print('Cannot load image ' + path)\n\nclass CFP_FP(data.Dataset):\n    def __init__(self, root, file_list, transform=None, loader=img_loader):\n\n        self.root = root\n        self.file_list = file_list\n        self.transform = transform\n        self.loader = loader\n        self.nameLs = []\n        self.nameRs = []\n        self.folds = []\n        self.flags = []\n\n        with open(file_list) as f:\n            pairs = f.read().splitlines()\n        for i, p in enumerate(pairs):\n            p = p.split(' ')\n            nameL = p[0]\n            nameR = p[1]\n            fold = i // 700\n            flag = int(p[2])\n\n            self.nameLs.append(nameL)\n            self.nameRs.append(nameR)\n            self.folds.append(fold)\n            self.flags.append(flag)\n\n    def __getitem__(self, index):\n\n        img_l = self.loader(os.path.join(self.root, self.nameLs[index]))\n        img_r = self.loader(os.path.join(self.root, self.nameRs[index]))\n        imglist = [img_l, cv2.flip(img_l, 1), img_r, cv2.flip(img_r, 1)]\n\n        if self.transform is not None:\n            for i in range(len(imglist)):\n                imglist[i] = self.transform(imglist[i])\n\n            imgs = imglist\n            return imgs\n        else:\n            imgs = [torch.from_numpy(i) for i in imglist]\n            return imgs\n\n    def __len__(self):\n        return len(self.nameLs)\n\n\nif __name__ == '__main__':\n    root = '/media/sda/CFP-FP/CFP_FP_aligned_112'\n    file_list = '/media/sda/CFP-FP/cfp-fp-pair.txt'\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n\n    dataset = CFP_FP(root, file_list, transform=transform)\n    trainloader = data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2, drop_last=False)\n    for data in trainloader:\n        for d in data:\n            print(d[0].shape)"""
dataset/lfw.py,2,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: lfw.py.py\n@time: 2018/12/22 10:00\n@desc: lfw dataset loader\n'''\n\nimport numpy as np\nimport cv2\nimport os\nimport torch.utils.data as data\n\nimport torch\nimport torchvision.transforms as transforms\n\ndef img_loader(path):\n    try:\n        with open(path, 'rb') as f:\n            img = cv2.imread(path)\n            if len(img.shape) == 2:\n                img = np.stack([img] * 3, 2)\n            return img\n    except IOError:\n        print('Cannot load image ' + path)\n\nclass LFW(data.Dataset):\n    def __init__(self, root, file_list, transform=None, loader=img_loader):\n\n        self.root = root\n        self.file_list = file_list\n        self.transform = transform\n        self.loader = loader\n        self.nameLs = []\n        self.nameRs = []\n        self.folds = []\n        self.flags = []\n\n        with open(file_list) as f:\n            pairs = f.read().splitlines()[1:]\n        for i, p in enumerate(pairs):\n            p = p.split('\\t')\n            if len(p) == 3:\n                nameL = p[0] + '/' + p[0] + '_' + '{:04}.jpg'.format(int(p[1]))\n                nameR = p[0] + '/' + p[0] + '_' + '{:04}.jpg'.format(int(p[2]))\n                fold = i // 600\n                flag = 1\n            elif len(p) == 4:\n                nameL = p[0] + '/' + p[0] + '_' + '{:04}.jpg'.format(int(p[1]))\n                nameR = p[2] + '/' + p[2] + '_' + '{:04}.jpg'.format(int(p[3]))\n                fold = i // 600\n                flag = -1\n            self.nameLs.append(nameL)\n            self.nameRs.append(nameR)\n            self.folds.append(fold)\n            self.flags.append(flag)\n\n    def __getitem__(self, index):\n\n        img_l = self.loader(os.path.join(self.root, self.nameLs[index]))\n        img_r = self.loader(os.path.join(self.root, self.nameRs[index]))\n        imglist = [img_l, cv2.flip(img_l, 1), img_r, cv2.flip(img_r, 1)]\n\n        if self.transform is not None:\n            for i in range(len(imglist)):\n                imglist[i] = self.transform(imglist[i])\n\n            imgs = imglist\n            return imgs\n        else:\n            imgs = [torch.from_numpy(i) for i in imglist]\n            return imgs\n\n    def __len__(self):\n        return len(self.nameLs)\n\n\nif __name__ == '__main__':\n    root = 'D:/data/lfw_align_112'\n    file_list = 'D:/data/pairs.txt'\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n\n    dataset = LFW(root, file_list, transform=transform)\n    #dataset = LFW(root, file_list)\n    trainloader = data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2, drop_last=False)\n    print(len(dataset))\n    for data in trainloader:\n        for d in data:\n            print(d[0].shape)"""
dataset/lfw_2.py,2,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: lfw_2.py\n@time: 2019/2/19 16:59\n@desc:  lfw dataset from insightface ,just like agedb and cfp-fp\n'''\n\n\nimport numpy as np\nimport cv2\nimport os\nimport torch.utils.data as data\n\nimport torch\nimport torchvision.transforms as transforms\n\ndef img_loader(path):\n    try:\n        with open(path, 'rb') as f:\n            img = cv2.imread(path)\n            if len(img.shape) == 2:\n                img = np.stack([img] * 3, 2)\n            return img\n    except IOError:\n        print('Cannot load image ' + path)\n\nclass LFW_2(data.Dataset):\n    def __init__(self, root, file_list, transform=None, loader=img_loader):\n\n        self.root = root\n        self.file_list = file_list\n        self.transform = transform\n        self.loader = loader\n        self.nameLs = []\n        self.nameRs = []\n        self.folds = []\n        self.flags = []\n\n        with open(file_list) as f:\n            pairs = f.read().splitlines()\n        for i, p in enumerate(pairs):\n            p = p.split(' ')\n            nameL = p[0]\n            nameR = p[1]\n            fold = i // 600\n            flag = int(p[2])\n\n            self.nameLs.append(nameL)\n            self.nameRs.append(nameR)\n            self.folds.append(fold)\n            self.flags.append(flag)\n\n    def __getitem__(self, index):\n\n        img_l = self.loader(os.path.join(self.root, self.nameLs[index]))\n        img_r = self.loader(os.path.join(self.root, self.nameRs[index]))\n        imglist = [img_l, cv2.flip(img_l, 1), img_r, cv2.flip(img_r, 1)]\n\n        if self.transform is not None:\n            for i in range(len(imglist)):\n                imglist[i] = self.transform(imglist[i])\n\n            imgs = imglist\n            return imgs\n        else:\n            imgs = [torch.from_numpy(i) for i in imglist]\n            return imgs\n\n    def __len__(self):\n        return len(self.nameLs)\n\n\nif __name__ == '__main__':\n    root = '/media/sda/insightface_emore/lfw'\n    file_list = '/media/sda/insightface_emore/pair_lfw.txt'\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n\n    dataset = LFW_2(root, file_list, transform=transform)\n    trainloader = data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2, drop_last=False)\n    for data in trainloader:\n        for d in data:\n            print(d[0].shape)"""
dataset/megaface.py,2,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: megaface.py\n@time: 2018/12/24 16:29\n@desc:\n'''\n\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nimport numpy as np\nimport cv2\nimport os\nimport torch\n\ndef img_loader(path):\n    try:\n        with open(path, 'rb') as f:\n            img = cv2.imread(path)\n            if len(img.shape) == 2:\n                img = np.stack([img] * 3, 2)\n            return img\n    except IOError:\n        print('Cannot load image ' + path)\n\n\nclass MegaFace(data.Dataset):\n    def __init__(self, facescrub_dir, megaface_dir, transform=None, loader=img_loader):\n\n        self.transform = transform\n        self.loader = loader\n\n        test_image_file_list = []\n        print('Scanning files under facescrub and megaface...')\n        for root, dirs, files in os.walk(facescrub_dir):\n            for e in files:\n                filename = os.path.join(root, e)\n                ext = os.path.splitext(filename)[1].lower()\n                if ext in ('.png', '.bmp', '.jpg', '.jpeg'):\n                    test_image_file_list.append(filename)\n        for root, dirs, files in os.walk(megaface_dir):\n            for e in files:\n                filename = os.path.join(root, e)\n                ext = os.path.splitext(filename)[1].lower()\n                if ext in ('.png', '.bmp', '.jpg', '.jpeg'):\n                    test_image_file_list.append(filename)\n\n        self.image_list = test_image_file_list\n\n    def __getitem__(self, index):\n        img_path = self.image_list[index]\n        img = self.loader(img_path)\n\n        #\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\xe5\x9b\xbe\xe5\x83\x8f\n        #img = cv2.flip(img, 1)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        else:\n            img = torch.from_numpy(img)\n\n        return img, img_path\n\n    def __len__(self):\n        return len(self.image_list)\n\n\nif __name__ == '__main__':\n    facescrub = '/media/sda/megaface_test_kit/facescrub_align_112/'\n    megaface = '/media/sda/megaface_test_kit/megaface_align_112/'\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # range [0, 255] -> [0.0,1.0]\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # range [0.0, 1.0] -> [-1.0,1.0]\n    ])\n    dataset = MegaFace(facescrub, megaface, transform=transform)\n    trainloader = data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2, drop_last=False)\n    print(len(dataset))\n    for data in trainloader:\n        print(data.shape)"""
lossfunctions/__init__.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: __init__.py.py\n@time: 2019/1/4 15:24\n@desc:\n'''"""
lossfunctions/agentcenterloss.py,5,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: agentcenterloss.py\n@time: 2019/1/7 10:53\n@desc: the variety of center loss, which use the class weight as the class center and normalize both the weight and feature,\n       in this way, the cos distance of weight and feature can be used as the supervised signal.\n       It's similar with torch.nn.CosineEmbeddingLoss, x_1 means weight_i, x_2 means feature_i.\n'''\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AgentCenterLoss(nn.Module):\n\n    def __init__(self, num_classes, feat_dim, scale):\n        super(AgentCenterLoss, self).__init__()\n        self.num_classes = num_classes\n        self.feat_dim = feat_dim\n        self.scale = scale\n\n        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n\n    def forward(self, x, labels):\n        '''\n        Parameters:\n            x: input tensor with shape (batch_size, feat_dim)\n            labels: ground truth label with shape (batch_size)\n        Return:\n            loss of centers\n        '''\n        cos_dis = F.linear(F.normalize(x), F.normalize(self.centers)) * self.scale\n\n        one_hot = torch.zeros_like(cos_dis)\n        one_hot.scatter_(1, labels.view(-1, 1), 1.0)\n\n        # loss = 1 - cosine(i)\n        loss = one_hot * self.scale - (one_hot * cos_dis)\n\n        return loss.mean()"""
lossfunctions/centerloss.py,7,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: centerloss.py\n@time: 2019/1/4 15:24\n@desc: the implementation of center loss\n'''\n\nimport torch\nimport torch.nn as nn\n\nclass CenterLoss(nn.Module):\n\n    def __init__(self, num_classes, feat_dim):\n        super(CenterLoss, self).__init__()\n        self.num_classes = num_classes\n        self.feat_dim = feat_dim\n\n        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n\n    def forward(self, x, labels):\n        '''\n        Parameters:\n            x: input tensor with shape (batch_size, feat_dim)\n            labels: ground truth label with shape (batch_size)\n        Return:\n            loss of centers\n        '''\n        # compute the distance of (x-center)^2\n        batch_size = x.size(0)\n        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n        distmat.addmm_(1, -2, x, self.centers.t())\n\n        # get one_hot matrix\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        classes = torch.arange(self.num_classes).long().to(device)\n        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n\n        dist = []\n        for i in range(batch_size):\n            value = distmat[i][mask[i]]\n            value = value.clamp(min=1e-12, max=1e+12)  # for numerical stability\n            dist.append(value)\n        dist = torch.cat(dist)\n        loss = dist.mean()\n\n        return loss"""
margin/ArcMarginProduct.py,8,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: ArcMarginProduct.py\n@time: 2018/12/25 9:13\n@desc: additive angular margin for arcface/insightface\n'''\n\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_feature=128, out_feature=10575, s=32.0, m=0.50, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_feature = in_feature\n        self.out_feature = out_feature\n        self.s = s\n        self.m = m\n        self.weight = Parameter(torch.Tensor(out_feature, in_feature))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n\n        # make the function cos(theta+m) monotonic decreasing while theta in [0\xc2\xb0,180\xc2\xb0]\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, x, label):\n        # cos(theta)\n        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n        # cos(theta + m)\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where((cosine - self.th) > 0, phi, cosine - self.mm)\n\n        #one_hot = torch.zeros(cosine.size(), device='cuda' if torch.cuda.is_available() else 'cpu')\n        one_hot = torch.zeros_like(cosine)\n        one_hot.scatter_(1, label.view(-1, 1), 1)\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output = output * self.s\n\n        return output\n\n\nif __name__ == '__main__':\n    pass"""
margin/CosineMarginProduct.py,6,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: CosineMarginProduct.py\n@time: 2018/12/25 9:13\n@desc: additive cosine margin for cosface\n'''\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\n\nclass CosineMarginProduct(nn.Module):\n    def __init__(self, in_feature=128, out_feature=10575, s=30.0, m=0.35):\n        super(CosineMarginProduct, self).__init__()\n        self.in_feature = in_feature\n        self.out_feature = out_feature\n        self.s = s\n        self.m = m\n        self.weight = Parameter(torch.Tensor(out_feature, in_feature))\n        nn.init.xavier_uniform_(self.weight)\n\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        # one_hot = torch.zeros(cosine.size(), device='cuda' if torch.cuda.is_available() else 'cpu')\n        one_hot = torch.zeros_like(cosine)\n        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n\n        output = self.s * (cosine - one_hot * self.m)\n        return output\n\n\nif __name__ == '__main__':\n    pass"""
margin/InnerProduct.py,4,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: InnerProduct.py\n@time: 2019/1/4 16:54\n@desc: just normal inner product as fully connected layer do.\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\nclass InnerProduct(nn.Module):\n    def __init__(self, in_feature=128, out_feature=10575):\n        super(InnerProduct, self).__init__()\n        self.in_feature = in_feature\n        self.out_feature = out_feature\n\n        self.weight = Parameter(torch.Tensor(out_feature, in_feature))\n        nn.init.xavier_uniform_(self.weight)\n\n\n    def forward(self, input, label):\n        # label not used\n        output = F.linear(input, self.weight)\n        return output\n\n\nif __name__ == '__main__':\n    pass"""
margin/MultiMarginProduct.py,7,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: MultiMarginProduct.py\n@time: 2019/3/30 10:09\n@desc: Combination of additive angular margin and additive cosine margin\n'''\n\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\n\nclass MultiMarginProduct(nn.Module):\n    def __init__(self, in_feature=128, out_feature=10575, s=32.0, m1=0.20, m2=0.35, easy_margin=False):\n        super(MultiMarginProduct, self).__init__()\n        self.in_feature = in_feature\n        self.out_feature = out_feature\n        self.s = s\n        self.m1 = m1\n        self.m2 = m2\n        self.weight = Parameter(torch.Tensor(out_feature, in_feature))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m1 = math.cos(m1)\n        self.sin_m1 = math.sin(m1)\n\n        # make the function cos(theta+m) monotonic decreasing while theta in [0\xc2\xb0,180\xc2\xb0]\n        self.th = math.cos(math.pi - m1)\n        self.mm = math.sin(math.pi - m1) * m1\n\n    def forward(self, x, label):\n        # cos(theta)\n        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n        # cos(theta + m1)\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m1 - sine * self.sin_m1\n\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where((cosine - self.th) > 0, phi, cosine - self.mm)\n\n\n        one_hot = torch.zeros_like(cosine)\n        one_hot.scatter_(1, label.view(-1, 1), 1)\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine) # additive angular margin\n        output = output - one_hot * self.m2 # additive cosine margin\n        output = output * self.s\n\n        return output\n\n\nif __name__ == '__main__':\n    pass"""
margin/SphereMarginProduct.py,6,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: SphereMarginProduct.py\n@time: 2018/12/25 9:19\n@desc: multiplicative angular margin for sphereface\n'''\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport math\n\nclass SphereMarginProduct(nn.Module):\n    def __init__(self, in_feature, out_feature, m=4, base=1000.0, gamma=0.0001, power=2, lambda_min=5.0, iter=0):\n        assert m in [1, 2, 3, 4], 'margin should be 1, 2, 3 or 4'\n        self.in_feature = in_feature\n        self.out_feature = out_feature\n        self.m = m\n        self.base = base\n        self.gamma = gamma\n        self.power = power\n        self.lambda_min = lambda_min\n        self.iter = 0\n        self.weight = Parameter(torch.Tensor(out_feature, in_feature))\n        nn.init.xavier_uniform_(self.weight)\n\n        # duplication formula\n        self.margin_formula = [\n            lambda x : x ** 0,\n            lambda x : x ** 1,\n            lambda x : 2 * x ** 2 - 1,\n            lambda x : 4 * x ** 3 - 3 * x,\n            lambda x : 8 * x ** 4 - 8 * x ** 2 + 1,\n            lambda x : 16 * x ** 5 - 20 * x ** 3 + 5 * x\n        ]\n\n    def forward(self, input, label):\n        self.iter += 1\n        self.cur_lambda = max(self.lambda_min, self.base * (1 + self.gamma * self.iter) ** (-1 * self.power))\n\n        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))\n        cos_theta = cos_theta(-1, 1)\n\n        cos_m_theta = self.margin_formula(self.m)(cos_theta)\n        theta = cos_theta.data.acos()\n        k = ((self.m * theta) / math.pi).floor()\n        phi_theta = ((-1.0) ** k) * cos_m_theta - 2 * k\n        phi_theta_ = (self.cur_lambda * cos_theta + phi_theta) / (1 + self.cur_lambda)\n        norm_of_feature = torch.norm(input, 2, 1)\n\n        one_hot = torch.zeros_like(cos_theta)\n        one_hot.scatter_(1, label.view(-1, 1), 1)\n\n        output = one_hot * phi_theta_ + (1 - one_hot) * cos_theta\n        output *= norm_of_feature.view(-1, 1)\n\n        return output\n\n\nif __name__ == '__main__':\n    pass"""
margin/__init__.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: __init__.py.py\n@time: 2018/12/25 9:12\n@desc:\n'''"""
utils/__init__.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: __init__.py.py\n@time: 2018/12/22 9:41\n@desc:\n'''"""
utils/load_images_from_bin.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: load_images_from_bin.py\n@time: 2018/12/25 19:21\n@desc: For AgeDB-30 and CFP-FP test dataset, we use the mxnet binary file provided by insightface, this is the tool to restore\n       the aligned images from mxnet binary file.\n       You should install a mxnet-cpu first, just do 'pip install mxnet==1.2.1' is ok.\n'''\n\nfrom PIL import Image\nimport cv2\nimport os\nimport pickle\nimport mxnet as mx\nfrom tqdm import tqdm\n\n'''\nFor train dataset, insightface provide a mxnet .rec file, just install a mxnet-cpu for extract images\n'''\n\ndef load_mx_rec(rec_path):\n    save_path = os.path.join(rec_path, 'emore_images_2')\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    imgrec = mx.recordio.MXIndexedRecordIO(os.path.join(rec_path, 'train.idx'), os.path.join(rec_path, 'train.rec'), 'r')\n    img_info = imgrec.read_idx(0)\n    header,_ = mx.recordio.unpack(img_info)\n    max_idx = int(header.label[0])\n    for idx in tqdm(range(1,max_idx)):\n        img_info = imgrec.read_idx(idx)\n        header, img = mx.recordio.unpack_img(img_info)\n        label = int(header.label)\n        #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        #img = Image.fromarray(img)\n        label_path = os.path.join(save_path, str(label).zfill(6))\n        if not os.path.exists(label_path):\n            os.makedirs(label_path)\n        #img.save(os.path.join(label_path, str(idx).zfill(8) + '.jpg'), quality=95)\n        cv2.imwrite(os.path.join(label_path, str(idx).zfill(8) + '.jpg'), img)\n\n\ndef load_image_from_bin(bin_path, save_dir):\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    file = open(os.path.join(save_dir, '../', 'lfw_pair.txt'), 'w')\n    bins, issame_list = pickle.load(open(bin_path, 'rb'), encoding='bytes')\n    for idx in tqdm(range(len(bins))):\n        _bin = bins[idx]\n        img = mx.image.imdecode(_bin).asnumpy()\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        cv2.imwrite(os.path.join(save_dir, str(idx+1).zfill(5)+'.jpg'), img)\n        if idx % 2 == 0:\n            label = 1 if issame_list[idx//2] == True else -1\n            file.write(str(idx+1).zfill(5) + '.jpg' + ' ' + str(idx+2).zfill(5) +'.jpg' + ' ' + str(label) + '\\n')\n\n\nif __name__ == '__main__':\n    #bin_path = 'D:/face_data_emore/faces_webface_112x112/lfw.bin'\n    #save_dir = 'D:/face_data_emore/faces_webface_112x112/lfw'\n    rec_path = 'D:/face_data_emore/faces_emore'\n    load_mx_rec(rec_path)\n    #load_image_from_bin(bin_path, save_dir)\n"""
utils/logging.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: logging.py\n@time: 2018/12/22 9:42\n@desc: logging tools\n'''\n\nfrom __future__ import print_function\nimport os\nimport logging\n\n\ndef init_log(output_dir):\n    logging.basicConfig(level=logging.DEBUG,\n                        format='%(asctime)s %(message)s',\n                        datefmt='%Y%m%d-%H:%M:%S',\n                        filename=os.path.join(output_dir, 'log.log'),\n                        filemode='w')\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    logging.getLogger('').addHandler(console)\n    return logging\n\n\nif __name__ == '__main__':\n    pass\n"""
utils/plot_logit.py,6,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: plot_logit.py\n@time: 2019/3/29 14:21\n@desc: plot the logit corresponding to shpereface, cosface, arcface and so on.\n'''\n\nimport math\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef softmax(theta):\n    return torch.cos(theta)\n\ndef sphereface(theta, m=4):\n    return (torch.cos(m * theta) + 20 * torch.cos(theta)) / (20 + 1)\n\ndef cosface(theta, m):\n    return torch.cos(theta) - m\n\ndef arcface(theta, m):\n    return torch.cos(theta + m)\n\ndef multimargin(theta, m1, m2):\n    return torch.cos(theta + m1) - m2\n\n\ntheta = torch.arange(0, math.pi, 0.001)\nprint(theta.type)\n\nx = theta.numpy()\ny_softmax = softmax(theta).numpy()\ny_cosface = cosface(theta, 0.35).numpy()\ny_arcface = arcface(theta, 0.5).numpy()\n\ny_multimargin_1 = multimargin(theta, 0.2, 0.3).numpy()\ny_multimargin_2 = multimargin(theta, 0.2, 0.4).numpy()\ny_multimargin_3 = multimargin(theta, 0.3, 0.2).numpy()\ny_multimargin_4 = multimargin(theta, 0.3, 0.3).numpy()\ny_multimargin_5 = multimargin(theta, 0.4, 0.2).numpy()\ny_multimargin_6 = multimargin(theta, 0.4, 0.3).numpy()\n\nplt.plot(x, y_softmax, x, y_cosface, x, y_arcface, x, y_multimargin_1, x, y_multimargin_2, x, y_multimargin_3, x, y_multimargin_4, x, y_multimargin_5, x, y_multimargin_6)\nplt.legend(['Softmax(0.00, 0.00)', 'CosFace(0.00, 0.35)', 'ArcFace(0.50, 0.00)', 'MultiMargin(0.20, 0.30)', 'MultiMargin(0.20, 0.40)', 'MultiMargin(0.30, 0.20)', 'MultiMargin(0.30, 0.30)', 'MultiMargin(0.40, 0.20)', 'MultiMargin(0.40, 0.30)'])\nplt.grid(False)\nplt.xlim((0, 3/4*math.pi))\nplt.ylim((-1.2, 1.2))\n\nplt.xticks(np.arange(0, 2.4, 0.3))\nplt.yticks(np.arange(-1.2, 1.2, 0.2))\nplt.xlabel('Angular between the Feature and Target Center (Radian: 0 - 3/4 Pi)')\nplt.ylabel('Target Logit')\n\nplt.savefig('target logits')"""
utils/plot_theta.py,6,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: plot_theta.py\n@time: 2019/1/2 19:08\n@desc: plot theta distribution between weight and feature vector\n'''\n\nfrom matplotlib import pyplot as plt\nplt.switch_backend('agg')\n\nimport argparse\nfrom backbone.mobilefacenet import MobileFaceNet\nfrom margin.ArcMarginProduct import ArcMarginProduct\nfrom torch.utils.data import DataLoader\nimport torch\n\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport os\nimport numpy as np\nfrom dataset.casia_webface import CASIAWebFace\n\n\ndef get_train_loader(img_folder, filelist):\n    print('Loading dataset...')\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n    ])\n    trainset = CASIAWebFace(img_folder, filelist, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n                                              shuffle=False, num_workers=8, drop_last=False)\n    return trainloader\n\ndef load_model(backbone_state_dict, margin_state_dict, device):\n\n    # load model\n    net = MobileFaceNet()\n    net.load_state_dict(torch.load(backbone_state_dict)['net_state_dict'])\n    margin = ArcMarginProduct(in_feature=128, out_feature=10575)\n    margin.load_state_dict(torch.load(margin_state_dict)['net_state_dict'])\n\n    net = net.to(device)\n    margin = margin.to(device)\n\n    return net.eval(), margin.eval()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='plot theta distribution of trained model')\n    parser.add_argument('--img_root', type=str, default='/media/ramdisk/webface_align_112', help='train image root')\n    parser.add_argument('--file_list', type=str, default='/media/ramdisk/webface_align_train.list', help='train list')\n    parser.add_argument('--backbone_file', type=str, default='../model/Paper_MOBILEFACE_20190103_111830/Iter_088000_net.ckpt', help='backbone state dict file')\n    parser.add_argument('--margin_file', type=str, default='../model/Paper_MOBILEFACE_20190103_111830/Iter_088000_margin.ckpt', help='backbone state dict file')\n    parser.add_argument('--gpus', type=str, default='0', help='model prefix, single gpu only')\n    args = parser.parse_args()\n\n    # gpu init\n    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # load pretrain model\n    trained_net, trained_margin = load_model(args.backbone_file, args.margin_file, device)\n\n    # initial model\n    initial_net = MobileFaceNet()\n    initial_margin = ArcMarginProduct()\n    initial_net = initial_net.to(device).eval()\n    initial_margin = initial_margin.to(device).eval()\n\n    # image dataloader\n    image_loader = get_train_loader(args.img_root, args.file_list)\n    theta_trained = []\n    theta_initial = []\n    for data in image_loader:\n        img, label = data[0].to(device), data[1].to(device)\n        # pretrained\n        embedding = trained_net(img)\n        cos_theta = F.linear(F.normalize(embedding), F.normalize(trained_margin.weight))\n        cos_theta = cos_theta.clamp(-1, 1).detach().cpu().numpy()\n        for i in range(img.shape[0]):\n            cos_trget = cos_theta[i][label[i]]\n            theta_trained.append(np.arccos(cos_trget) / np.pi * 180)\n        # initial\n        embedding = initial_net(img)\n        cos_theta = F.linear(F.normalize(embedding), F.normalize(initial_margin.weight))\n        cos_theta = cos_theta.clamp(-1, 1).detach().cpu().numpy()\n        for i in range(img.shape[0]):\n            cos_trget = cos_theta[i][label[i]]\n            theta_initial.append(np.arccos(cos_trget) / np.pi * 180)\n    '''\n    # write theta list to txt file\n    trained_theta_file = open('arcface_theta.txt', 'w')\n    initial_theta_file = open('initial_theta.txt', 'w')\n    for item in theta_trained:\n        trained_theta_file.write(str(item))\n        trained_theta_file.write('\\n')\n    for item in theta_initial:\n        initial_theta_file.write(str(item))\n        initial_theta_file.write('\\n')\n\n    # plot the theta, read theta from txt first\n    theta_trained = []\n    theta_initial = []\n    trained_theta_file = open('arcface_theta.txt', 'r')\n    initial_theta_file = open('initial_theta.txt', 'r')\n    lines = trained_theta_file.readlines()\n    for line in lines:\n        theta_trained.append(float(line.strip('\\n')[0]))\n    lines = initial_theta_file.readlines()\n    for line in lines:\n        theta_initial.append(float(line.split('\\n')[0]))\n    '''\n    print(len(theta_trained), len(theta_initial))\n    plt.figure()\n    plt.xlabel('Theta')\n    plt.ylabel('Numbers')\n    plt.title('Theta Distribution')\n    plt.hist(theta_trained, bins=180, normed=0)\n    plt.hist(theta_initial, bins=180, normed=0)\n    plt.legend(['trained theta distribution', 'initial theta distribution'])\n    plt.savefig('theta_distribution_hist.jpg')\n"""
utils/visualize.py,0,"b""#!/usr/bin/env python\n# encoding: utf-8\n'''\n@author: wujiyang\n@contact: wujiyang@hust.edu.cn\n@file: visualize.py\n@time: 2019/1/7 16:07\n@desc: visualize tools\n'''\n\nimport visdom\nimport numpy as np\nimport time\n\nclass Visualizer():\n    def __init__(self, env='default', **kwargs):\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        self.index = 1\n\n    def plot_curves(self, d, iters, title='loss', xlabel='iters', ylabel='accuracy'):\n        name = list(d.keys())\n        val = list(d.values())\n        if len(val) == 1:\n            y = np.array(val)\n        else:\n            y = np.array(val).reshape(-1, len(val))\n        self.vis.line(Y=y,\n                      X=np.array([self.index]),\n                      win=title,\n                      opts=dict(legend=name, title = title, xlabel=xlabel, ylabel=ylabel),\n                      update=None if self.index == 0 else 'append')\n        self.index = iters\n\n\nif __name__ == '__main__':\n    vis = Visualizer(env='test')\n    for i in range(10):\n        x = i\n        y = 2 * i\n        z = 4 * i\n        vis.plot_curves({'train': x, 'test': y}, iters=i, title='train')\n        vis.plot_curves({'train': z, 'test': y, 'val': i}, iters=i, title='test')\n        time.sleep(1)"""
