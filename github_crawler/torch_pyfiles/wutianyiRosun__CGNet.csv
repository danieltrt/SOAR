file_path,api_count,code
camvid_test.py,9,"b'###########################################################################\n# Created by: Tianyi Wu\n# Email: wutianyi@ict.ac.cn \n# Copyright (c) 2018\n###########################################################################\nimport os\nimport time\nimport torch\nimport pickle\nimport timeit\nimport random\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils import data\nfrom argparse import ArgumentParser\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\n#user\nfrom model import CGNet\nfrom utils.metric import get_iou\nfrom utils.modeltools import netParams\nfrom utils.loss import CrossEntropyLoss2d\nfrom utils.convert_state import convert_state_dict\nfrom  dataset.camvid import CamVidDataSet,CamVidValDataSet, CamVidTrainInform\n\ndef test(args, test_loader, model, criterion):\n    """"""\n    args:\n      val_loader: loaded for validation dataset\n      model: model\n      criterion: loss function\n    return: class IoU and mean IoU\n    """"""\n    #evaluation or test mode\n    model.eval()\n    total_batches = len(test_loader)\n   \n    data_list=[]\n    for i, (input, label, size, name) in enumerate(test_loader):\n        input_var = Variable(input, volatile=True).cuda()\n        output = model(input_var)\n        output= output.cpu().data[0].numpy()\n        gt = np.asarray(label[0].numpy(), dtype = np.uint8)\n        output= output.transpose(1,2,0)\n        output= np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n        data_list.append( [gt.flatten(), output.flatten()])\n    meanIoU, per_class_iu= get_iou(data_list, args.classes)\n    return meanIoU, per_class_iu\n\ndef test_model(args):\n    """"""\n    main function for testing \n    args:\n       args: global arguments\n    """"""\n    print(""=====> Check if the cached file exists "")\n    if not os.path.isfile(args.inform_data_file):\n        print(""%s is not found"" %(args.inform_data_file))\n        dataCollect = CamVidTrainInform(args.data_dir, args.classes, train_set_file= args.dataset_list, \n                                        inform_data_file = args.inform_data_file) #collect mean std, weigth_class information\n        datas = dataCollect.collectDataAndSave()\n        if datas is None:\n            print(\'Error while pickling data. Please check.\')\n            exit(-1)\n    else:\n        print(""%s exists"" %(args.inform_data_file))\n        datas = pickle.load(open(args.inform_data_file, ""rb""))\n    \n    print(args)\n    global network_type\n     \n    if args.cuda:\n        print(""=====> Use gpu id: \'{}\'"".format(args.gpus))\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpus\n        if not torch.cuda.is_available():\n            raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\n    \n    args.seed = random.randint(1, 10000)\n    print(""Random Seed: "", args.seed)\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed) \n    cudnn.enabled = True\n\n    M = args.M\n    N = args.N\n    model = CGNet.Context_Guided_Network(classes= args.classes, M= M, N= N)\n    network_type=""CGNet""\n    print(""=====> current architeture:  CGNet_M%sN%s""%(M, N))\n    total_paramters = netParams(model)\n    print(""the number of parameters: "" + str(total_paramters))\n    print(""data[\'classWeights\']: "", datas[\'classWeights\'])\n    weight = torch.from_numpy(datas[\'classWeights\'])\n    print(""=====> Dataset statistics"")\n    print(""mean and std: "", datas[\'mean\'], datas[\'std\'])\n    \n    # define optimization criteria\n    criteria = CrossEntropyLoss2d(weight, args.ignore_label)\n    if args.cuda:\n        model = model.cuda()\n        criteria = criteria.cuda()\n    \n    #load test set\n    train_transform= transforms.Compose([\n        transforms.ToTensor()])\n    testLoader = data.DataLoader(CamVidValDataSet(args.data_dir, args.test_data_list,f_scale=1,  mean= datas[\'mean\']),\n                                  batch_size = args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=====> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            #model.load_state_dict(convert_state_dict(checkpoint[\'model\']))\n            model.load_state_dict(checkpoint[\'model\'])\n        else:\n            print(""=====> no checkpoint found at \'{}\'"".format(args.resume))\n    \n    cudnn.benchmark= True\n\n    print(""=====> beginning test"")\n    print(""length of test set:"", len(testLoader))\n    mIOU_val, per_class_iu = test(args, testLoader, model, criteria)\n    print(mIOU_val)\n    print(per_class_iu)\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', type = str, default = ""CGNet"", help = ""model name: Context Guided Network"")\n    parser.add_argument(\'--dataset\', type = str, default = ""camvid"", help = ""camvid or cityscapes"")\n    parser.add_argument(\'--ignore_label\', type = int, default = 11, help = ""nClass"")\n    parser.add_argument(\'--data_dir\', default = ""/home/wty/AllDataSet/CamVid"", help = ""data directory"")\n    parser.add_argument(\'--test_data_list\', default = ""./dataset/list/CamVid/camvid_test_list.txt"", help= ""data directory"")\n    parser.add_argument(\'--scaleIn\', type = int, default = 1, help = ""for input image, default is 1, keep fixed size"")  \n    parser.add_argument(\'--num_workers\', type = int, default = 1, help = ""the number of parallel threads"") \n    parser.add_argument(\'--batch_size\', type = int, default = 1, help = ""the batch size is set to 1 when testing"")\n    parser.add_argument(\'--resume\', type = str, default = \'./checkpoint/camvid/CGNet_M3N21bs8gpu1_ontrainval/model_800.pth\', \n                         help = ""use this file to load last checkpoint for testing"")\n    parser.add_argument(\'--classes\', type = int, default = 11, \n                         help = ""the number of classes in the dataset. 19 and 11 for cityscapes and camvid, respectively"")\n    parser.add_argument(\'--inform_data_file\', default = ""./dataset/wtfile/camvid_inform.pkl"", \n                         help = ""storing classes weights, mean and std"")\n    parser.add_argument(\'--M\', type = int, default = 3,  help = ""the number of block in stage 2"")\n    parser.add_argument(\'--N\', type = int, default = 21, help = ""the number of block in stage 3"")\n    parser.add_argument(\'--cuda\', type = bool, default = True, help = ""running on CPU or GPU"")\n    parser.add_argument(""--gpus"", type = str, default = ""0"",  help = ""gpu ids (default: 0)"")\n    parser.add_argument(""--gpu_nums"",  type = int, default=1 , help=""the number of gpu"")\n    \n    args = parser.parse_args()\n    test_model(args)\n\n\n'"
camvid_train.py,16,"b'###########################################################################\n# Created by: Tianyi Wu\n# Email: wutianyi@ict.ac.cn \n# Copyright (c) 2018\n###########################################################################\nimport os\nimport time\nimport torch\nimport timeit\nimport pickle\nimport random\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils import data\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\nfrom argparse import ArgumentParser\n#user\nfrom model import CGNet  # network\nfrom utils.metric import get_iou\nfrom utils.modeltools import netParams\nfrom utils.loss import CrossEntropyLoss2d  # loss function\nfrom utils.convert_state import convert_state_dict\nfrom dataset.camvid import CamVidDataSet,CamVidValDataSet, CamVidTrainInform  #dataset\n\ndef val(args, val_loader, model, criterion):\n    """"""\n    args:\n      val_loader: loaded for validation dataset\n      model: model\n      criterion: loss function\n    return: IoU class, and mean IoU\n    """"""\n    #evaluation mode\n    model.eval()\n    total_batches = len(val_loader)\n   \n    data_list=[]\n    for i, (input, label, size, name) in enumerate(val_loader):\n        start_time = time.time()\n        input_var = Variable(input, volatile=True).cuda()\n        output = model(input_var)\n        time_taken = time.time() - start_time\n        print(""[%d/%d]  time: %.2f"" % (i, total_batches, time_taken))\n        output= output.cpu().data[0].numpy()\n        gt = np.asarray(label[0].numpy(), dtype = np.uint8)\n        output= output.transpose(1,2,0)\n        output= np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n        data_list.append( [gt.flatten(), output.flatten()])\n\n    meanIoU, per_class_iu= get_iou(data_list, args.classes)\n    return meanIoU, per_class_iu\n\ndef adjust_learning_rate( args, cur_epoch, max_epoch, curEpoch_iter, perEpoch_iter, baselr):\n    """"""\n    poly learning stategyt\n    lr = baselr*(1-iter/max_iter)^power\n    """"""\n    cur_iter = cur_epoch*perEpoch_iter + curEpoch_iter\n    max_iter=max_epoch*perEpoch_iter\n    lr = baselr*pow( (1 - 1.0*cur_iter/max_iter), 0.9)\n\n    return lr\n\n\ndef train(args, train_loader, model, criterion, optimizer, epoch):\n    """"""\n    args:\n       train_loader: loaded for training dataset\n       model: model\n       criterion: loss function\n       optimizer: optimization algorithm, such as ADAM or SGD\n       epoch: epoch number\n    return: average loss, per class IoU, and mean IoU\n    """"""\n    model.train()\n    epoch_loss = []\n\n    data_list=[]\n    total_batches = len(train_loader)\n    print(""=====> the number of iterations per epoch: "", total_batches)\n    for iteration, batch in enumerate( train_loader, 0 ):\n        lr= adjust_learning_rate( args, cur_epoch = epoch, max_epoch = args.max_epochs, \n                                  curEpoch_iter = iteration, perEpoch_iter = total_batches, baselr = args.lr )\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr;\n        \n        start_time = time.time()\n        images, labels, _, _ = batch\n        images = Variable( images ).cuda()\n        labels = Variable( labels.long() ).cuda()\n        output = model( images )\n        loss = criterion(output, labels)\n        optimizer.zero_grad()  #set the grad to zero\n        loss.backward()\n        optimizer.step()\n        epoch_loss.append( loss.item() )\n        time_taken = time.time() - start_time\n        \n        gt = np.asarray( labels.cpu().data[0].numpy(), dtype = np.uint8 )\n        output = output.cpu().data[0].numpy()\n        output = output.transpose(1,2,0)\n        output = np.asarray( np.argmax(output, axis=2), dtype=np.uint8 )\n\n        data_list.append( [gt.flatten(), output.flatten()] )\n\n        print(\'=====> epoch[%d/%d] iter: (%d/%d) \\tcur_lr: %.6f loss: %.3f time:%.2f\' % ( epoch, args.max_epochs,\n              iteration, total_batches, lr,loss.item(), time_taken ) )\n\n    average_epoch_loss_train = sum( epoch_loss ) / len( epoch_loss )\n    meanIoU, per_class_iu = get_iou( data_list, args.classes )\n\n    return average_epoch_loss_train, per_class_iu, meanIoU, lr\n\ndef train_model(args):\n    """"""\n    args:\n       args: global arguments\n    """"""\n    h, w = map(int, args.input_size.split(\',\'))\n    input_size = (h, w)\n    print(""=====> checking if inform_data_file exists"")\n    if not os.path.isfile(args.inform_data_file):\n        print(""%s is not found"" %( args.inform_data_file ) )\n        dataCollect = CamVidTrainInform(args.data_dir, args.classes, train_set_file = args.dataset_list, \n                                        inform_data_file = args.inform_data_file) #collect mean std, weigth_class information\n        datas = dataCollect.collectDataAndSave()\n        if datas is None:\n            print(""error while pickling data. Please check."")\n            exit(-1)\n    else:\n        print(""find file: "", str(args.inform_data_file))\n        datas = pickle.load( open( args.inform_data_file, ""rb"") )\n    \n    print(args)\n    global network_type\n     \n    if args.cuda:\n        print(""=====> use gpu id: \'{}\'"".format(args.gpus))\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpus\n        if not torch.cuda.is_available():\n            raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\n    \n    args.seed = random.randint(1, 10000)\n    print(""====> Random Seed: "", args.seed)\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed) \n    \n    cudnn.enabled = True\n    M = args.M\n    N = args.N\n    print(""=====> building network"")\n    model = CGNet.Context_Guided_Network(classes= args.classes, M= M, N= N)\n    network_type=""CGNet""\n    print(""=====> current architeture:  CGNet"")\n    \n    print(""=====> computing network parameters"")\n    total_paramters = netParams(model)\n    print(""the number of parameters: "" + str(total_paramters))\n    \n    print(""data[\'classWeights\']: "", datas[\'classWeights\'])\n    print(\'=====> Dataset statistics\')\n    print(\'mean and std: \', datas[\'mean\'], datas[\'std\'])\n    \n    # define optimization criteria\n    weight = torch.from_numpy(datas[\'classWeights\'])\n    criteria = CrossEntropyLoss2d(weight, args.ignore_label)\n\n    if args.cuda:\n        criteria = criteria.cuda()\n        args.gpu_nums = 1\n        if torch.cuda.device_count()>1:\n            print(""torch.cuda.device_count()="",torch.cuda.device_count())\n            args.gpu_nums = torch.cuda.device_count()\n            model = torch.nn.DataParallel(model).cuda()\n        else:\n            print(""single GPU for training"")\n            model = model.cuda()  \n    \n    args.savedir = ( args.savedir + args.dataset + \'/\'+ network_type +""_M""+ str(M) + \'N\' +str(N) + \'bs\' \n                    + str(args.batch_size)+ \'gpu\' + str(args.gpu_nums)+ ""_""+str(args.train_type)+\'/\')\n    if not os.path.exists(args.savedir):\n        os.makedirs(args.savedir)\n\n    #Data augmentation, compose the data with transforms\n    train_transform= transforms.Compose([\n        transforms.ToTensor()])\n    trainLoader = data.DataLoader( CamVidDataSet( args.data_dir, args.train_data_list, crop_size = input_size, scale = args.random_scale, \n                                                      mirror = args.random_mirror, mean = datas[\'mean\'] ),\n                                   batch_size = args.batch_size, shuffle = True, num_workers = args.num_workers, \n                                   pin_memory = True, drop_last = True )\n    valLoader = data.DataLoader( CamVidValDataSet( args.data_dir, args.val_data_list,f_scale = 1,  mean = datas[\'mean\']),\n                                 batch_size = 1, shuffle = True, num_workers = args.num_workers, pin_memory = True, drop_last = True )\n\n    start_epoch = 0\n    if args.resume:\n        if os.path.isfile(args.resume):\n            checkpoint = torch.load(args.resume)\n            start_epoch = checkpoint[\'epoch\']\n            model.load_state_dict(checkpoint[\'model\'])\n            #model.load_state_dict(convert_state_dict(checkpoint[\'model\']))\n            print(""=====> loading checkpoint \'{}\' (epoch {})"".format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=====> no checkpoint found at \'{}\'"".format(args.resume))\n    \n    model.train()\n    cudnn.benchmark= True\n    \n    logFileLoc = args.savedir + args.logFile\n    if os.path.isfile(logFileLoc):\n        logger = open(logFileLoc, \'a\')\n    else:\n        logger = open(logFileLoc, \'w\')\n        logger.write(""Parameters: %s"" % (str(total_paramters)))\n        logger.write(""\\n%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t"" % (\'Epoch\', \'Loss(Tr)\', \'Loss(val)\', \'mIOU (tr)\', \'mIOU (val)\'))\n    logger.flush()\n\n    optimizer = torch.optim.Adam(model.parameters(), args.lr, (0.9, 0.999), eps=1e-08, weight_decay=5e-4)\n \n    print(\'=====> beginning training\')\n    for epoch in range(start_epoch, args.max_epochs):\n        #training\n        lossTr, per_class_iu_tr, mIOU_tr, lr = train(args, trainLoader, model, criteria, optimizer, epoch)\n        \n        #validation\n        if epoch % 50 ==0:\n            mIOU_val, per_class_iu = val(args, valLoader, model, criteria)\n            logger.write(""\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.7f"" % (epoch, lossTr, mIOU_tr, mIOU_val, lr))\n            logger.flush()\n            print(""epoch: "" + str(epoch) + \' Details\')\n            print(""\\nEpoch No.: %d\\tTrain Loss = %.4f\\t mIOU(tr) = %.4f\\t mIOU(val) = %.4f\\t lr= %.6f"" % (epoch,\n                   lossTr, mIOU_tr, mIOU_val, lr))\n        else:\n            logger.write(""\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.7f"" % (epoch, lossTr, mIOU_tr, lr))\n            logger.flush()\n            print(""Epoch : "" + str(epoch) + \' Details\')\n            print(""\\nEpoch No.: %d\\tTrain Loss = %.4f\\t mIOU(tr) = %.4f\\t lr= %.6f"" % (epoch, lossTr, mIOU_tr, lr))\n        \n        #save the model\n        model_file_name = args.savedir +\'/model_\' + str(epoch + 1) + \'.pth\'\n        state = {""epoch"": epoch+1, ""model"": model.state_dict()}\n        if epoch > args.max_epochs - 10:\n            torch.save(state, model_file_name)\n        elif not epoch % 20:\n            torch.save(state, model_file_name)\n\n    logger.close()\n\nif __name__ == \'__main__\':\n    start = timeit.default_timer()\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', default = ""CGNet"", help = ""model name: Context Guided Network (CGNet)"")\n    parser.add_argument(\'--dataset\', default = ""camvid"", help = ""dataset: cityscapes or camvid"")\n    parser.add_argument(\'--ignore_label\', type = int,  default = 11, help = ""nClass"")\n    parser.add_argument(\'--data_dir\', default = ""/home/wty/AllDataSet/CamVid"", help =\'data directory\')\n    parser.add_argument(\'--dataset_list\', default = ""camvid_trainval_list.txt"",\n                        help = ""train and val data, for computing the ration of all kinds, mean and std"")\n    parser.add_argument(\'--train_data_list\', default = ""./dataset/list/CamVid/camvid_trainval_list.txt"", help = ""train set"")\n    parser.add_argument(\'--train_type\', type = str, default = ""ontrainval"", \n                         help = ""ontrain for training on train set, ontrainval for training on train+val set"")\n    parser.add_argument(\'--max_epochs\', type = int, default = 800, help = ""the number of epochs: 800 for train+val set"")\n    parser.add_argument(\'--val_data_list\', default = ""./dataset/list/CamVid/camvid_val_list.txt"", help = ""val set"")\n    parser.add_argument(\'--scaleIn\', type = int, default = 1, help = ""for input image, default is 1, keep fixed size"")  \n    parser.add_argument(\'--input_size\', type = str, default = ""360,360"", help = ""input size of model"") \n    parser.add_argument(\'--random_mirror\', type = bool, default = True, help = ""input image random mirror"") \n    parser.add_argument(\'--random_scale\', type = bool, default = True, help = ""input image resize 0.5 to 2"") \n    parser.add_argument(\'--num_workers\', type = int, default = 1, help = "" the number of parallel threads"") \n    parser.add_argument(\'--batch_size\', type = int, default = 8, help = ""the batch size is set to 16 for 2 GPUs"")\n\n    parser.add_argument(\'--lr\', type = float, default = 1e-3, help = ""initial learning rate"")\n    parser.add_argument(\'--savedir\', default = ""./checkpoint/"", help = ""directory to save the model snapshot"")\n    parser.add_argument(\'--resume\', type = str, default = ""./checkpoint/camvid/CGNet_M3N21bs8gpu1_ontrainval/model_1.pth"", \n                         help = ""use this file to load last checkpoint for continuing training"")  \n    parser.add_argument(\'--classes\', type = int, default = 11, \n                         help = ""the number of classes in the dataset. 19 and 11 for cityscapes and camvid, respectively"")\n    parser.add_argument(\'--inform_data_file\', default = ""./dataset/wtfile/camvid_inform.pkl"", \n                         help = ""saving statistic information of the dataset (train+val set), classes weigtht, mean and std"")\n    parser.add_argument(\'--M\', type = int, default = 3, help = ""the number of blocks in stage 2"")\n    parser.add_argument(\'--N\', type = int, default = 21, help = ""the number of blocks in stage 3"")\n    parser.add_argument(\'--logFile\', default= ""log.txt"", help = ""storing the training and validation logs"")\n    parser.add_argument(\'--cuda\', type = bool, default = True, help = ""running on CPU or GPU"")\n    parser.add_argument(\'--gpus\', type = str, default = ""0"", help = ""default GPU devices (0)"")\n    args = parser.parse_args()\n    train_model(args)\n    end = timeit.default_timer()\n    print(""training time:"", 1.0*(end-start)/3600)\n\n'"
cityscapes_eval.py,9,"b'###########################################################################\n# Created by: Tianyi Wu\n# Email: wutianyi@ict.ac.cn \n# Copyright (c) 2018\n###########################################################################\nimport os\nimport time\nimport torch\nimport pickle\nimport random\nimport numpy as np\nimport torch.nn as nn\nfrom PIL import Image\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom argparse import ArgumentParser\n#user\nfrom model import CGNet  # network\nfrom utils.metric import get_iou\nfrom utils.loss import CrossEntropyLoss2d  # loss function\nfrom utils.convert_state import convert_state_dict\nfrom utils.colorize_mask import cityscapes_colorize_mask\nfrom  dataset.cityscapes import CityscapesValDataSet, CityscapesTrainInform  # dataset\n\n\ndef val(args, val_loader, model, criterion):\n    """"""\n    args:\n      val_loader: loaded for validation dataset\n      model: model\n      criterion: loss function\n    return: IoU class, and mean IoU\n    """"""\n    #evaluation mode\n    model.eval()\n    total_batches = len(val_loader)\n   \n    data_list=[]\n    for i, (input, label, size, name) in enumerate(val_loader):\n        input_var = Variable(input, volatile=True).cuda()\n        output = model(input_var)\n        # save seg image\n        output= output.cpu().data[0].numpy()  # 1xCxHxW ---> CxHxW\n        gt = np.asarray(label[0].numpy(), dtype = np.uint8)\n        output= output.transpose(1,2,0) # CxHxW --> HxWxC\n        output= np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n        data_list.append( [gt.flatten(), output.flatten()])\n        \n        output_color = cityscapes_colorize_mask(output)\n        output = Image.fromarray(output)\n        output.save(\'%s/%s.png\'%(args.save_seg_dir, name[0]))\n        output_color.save(\'%s/%s_color.png\' % (args.save_seg_dir, name[0]))\n\n    meanIoU, IoUs= get_iou(data_list, args.classes)\n    print(""mean IoU:"", meanIoU)\n    print(IoUs)\n\ndef ValidateSegmentation(args):\n    """"""\n     main function for validation\n     param args: global arguments\n     return: None\n    """"""\n    print(args)\n    global network_type\n\n    if args.cuda:\n        print(""=====> use gpu id: \'{}\'"".format(args.gpus))\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpus\n        if not torch.cuda.is_available():\n            raise Exception(""no GPU found or Wrong gpu id, please run without --cuda"")\n    \n    args.seed = random.randint(1, 10000)\n    print(""Random Seed: "", args.seed)\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed) \n    \n    print(\'=====> checking if processed cached_data_file exists\')\n    if not os.path.isfile(args.inform_data_file):\n        dataCollect = CityscapesTrainInform(args.data_dir, args.classes, train_set_file = args.dataset_list, \n                                            inform_data_file = args.inform_data_file) #collect mean std, weigth_class information\n        data= dataCollect.collectDataAndSave()\n        if data is  None:\n            print(""error while pickling data, please check"")\n            exit(-1)\n    else:\n        data = pickle.load(open(args.inform_data_file, ""rb""))\n    M = args.M\n    N = args.N\n    \n    model = CGNet.Context_Guided_Network(classes= args.classes, M= M, N= N)\n    network_type=""CGNet""\n    print(""Arch:  CGNet"")\n    # define optimization criteria\n    weight = torch.from_numpy(data[\'classWeights\']) # convert the numpy array to torch\n    if args.cuda:\n        weight = weight.cuda()\n    criteria = CrossEntropyLoss2d(weight) #weight\n\n    if args.cuda:\n        model = model.cuda()  # using GPU for inference\n        criteria = criteria.cuda()\n        cudnn.benchmark = True\n\n    print(\'Dataset statistics\')\n    print(\'mean and std: \', data[\'mean\'], data[\'std\'])\n    print(\'classWeights: \', data[\'classWeights\'])\n\n    if args.save_seg_dir:\n        if not os.path.exists(args.save_seg_dir):\n            os.makedirs(args.save_seg_dir)\n\n    # validation set\n    valLoader = torch.utils.data.DataLoader( CityscapesValDataSet(args.data_dir, args.val_data_list, f_scale = 1, mean = data[\'mean\']),\n                                             batch_size = 1, shuffle = False, num_workers = args.num_workers, pin_memory = True)\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=====> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            #model.load_state_dict(checkpoint[\'model\'])\n            model.load_state_dict(convert_state_dict(checkpoint[\'model\']))\n        else:\n            print(""=====> no checkpoint found at \'{}\'"".format(args.resume))\n    \n    print(""=====> beginning validation"")\n    print(""validation set length: "", len(valLoader))\n    val(args, valLoader, model, criteria)\n\nif __name__ == \'__main__\':\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', default = ""CGNet"", help = ""model name: Context Guided Network (CGNet"")\n    parser.add_argument(\'--data_dir\', default = ""/home/wty/AllDataSet/Cityscapes"", help = ""data directory"")\n    parser.add_argument(\'--dataset_list\', default = ""cityscapes_trainval_list.txt"",\n                        help = ""train and val data, for computing the ratio of all classes, mean and std"")\n    parser.add_argument(\'--val_data_list\', default = ""./dataset/list/Cityscapes/cityscapes_val_list.txt"", help = ""val set"")\n    parser.add_argument(\'--scaleIn\', type = int, default = 1, help = ""rescale input image, default is 1, keep fixed size"")  \n    parser.add_argument(\'--num_workers\', type = int, default= 1, help = ""the number of parallel threads"") \n    parser.add_argument(\'--batch_size\', type = int, default = 1, help="" the batch_size is set to 1 when evaluating or testing"") \n    parser.add_argument(\'--resume\', type = str, default = ""./checkpoint/cityscapes/CGNet_M3N21bs16gpu2_ontrain/model_1.pth"", \n                        help = ""use the file to load last checkpoint for evaluating or testing "")\n    parser.add_argument(\'--classes\', type = int, default = 19, \n                        help = ""the number of classes in the dataset. 19 and 11 for cityscapes and camvid, respectively"")\n    parser.add_argument(\'--inform_data_file\', default = ""./dataset/wtfile/cityscapes_inform.pkl"", \n                        help = ""storing the classes weights, mean and std"")\n    parser.add_argument(\'--cuda\', default = True, help = ""run on CPU or GPU"")\n    parser.add_argument(\'--M\', type = int, default = 3, help = ""the number of blocks in stage 2"")\n    parser.add_argument(\'--N\', type = int, default = 21, help = ""the number of blocks in stage 3"")\n    parser.add_argument(\'--save_seg_dir\', type = str, default = ""./result/cityscapes/val/"", help = ""saving path of prediction result"")\n    parser.add_argument(""--gpus"", default = ""5"", type = str, help = ""gpu ids (default: 0)"")\n\n    ValidateSegmentation(parser.parse_args())\n\n'"
cityscapes_test.py,9,"b'###########################################################################\n# Created by: Tianyi Wu\n# Email: wutianyi@ict.ac.cn \n# Copyright (c) 2018\n###########################################################################\nimport os\nimport time\nimport torch\nimport pickle\nimport random\nimport numpy as np\nimport torch.nn as nn\nfrom PIL import Image\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom argparse import ArgumentParser\n#user\nfrom model import CGNet  # network\nfrom utils.metric import get_iou\nfrom utils.loss import CrossEntropyLoss2d  # loss function\nfrom utils.convert_state import convert_state_dict\nfrom utils.colorize_mask import cityscapes_colorize_mask\nfrom  dataset.cityscapes import CityscapesTestDataSet, CityscapesTrainInform  # dataset\n\n\ndef test(args, test_loader, model):\n    """"""\n    args:\n      test_loader: loaded for test set\n      model: model\n      criterion: loss function\n    return: IoU class, and mean IoU\n    """"""\n    #evaluation mode\n    model.eval()\n    total_batches = len(test_loader) \n    for i, (input, size, name) in enumerate(test_loader):\n        input_var = Variable(input, volatile=True).cuda()\n        output = model(input_var)\n        # save seg image\n        output= output.cpu().data[0].numpy()  # 1xCxHxW ---> CxHxW\n        output= output.transpose(1,2,0) # CxHxW --> HxWxC\n        output= np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n\n        output_color = cityscapes_colorize_mask(output)\n        output = Image.fromarray(output)\n        #output.save( ""%s/%s.png "" % (args.save_seg_dir, name[0]) )\n        output_color.save( ""%s/%s_color.png"" % (args.save_seg_dir, name[0]))\n\n\ndef test_func(args):\n    """"""\n     main function for testing\n     param args: global arguments\n     return: None\n    """"""\n    print(args)\n    global network_type\n\n    if args.cuda:\n        print(""=====> use gpu id: \'{}\'"".format(args.gpus))\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpus\n        if not torch.cuda.is_available():\n            raise Exception(""no GPU found or wrong gpu id, please run without --cuda"")\n    \n    args.seed = random.randint(1, 10000)\n    print(""Random Seed: "", args.seed)\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed) \n    \n    print(\'=====> checking if processed cached_data_file exists\')\n    if not os.path.isfile(args.inform_data_file):\n        dataCollect = CityscapesTrainInform(args.data_dir, args.classes, train_set_file = args.dataset_list, \n                                            inform_data_file = args.inform_data_file) #collect mean std, weigth_class information\n        data= dataCollect.collectDataAndSave()\n        if data is  None:\n            print(""error while pickling data, please check"")\n            exit(-1)\n    else:\n        data = pickle.load(open(args.inform_data_file, ""rb""))\n    M = args.M\n    N = args.N\n    \n    model = CGNet.Context_Guided_Network(classes= args.classes, M= M, N= N)\n    network_type=""CGNet""\n    print(""Arch:  CGNet"")\n    # define optimization criteria\n    weight = torch.from_numpy(data[\'classWeights\']) # convert the numpy array to torch\n    if args.cuda:\n        weight = weight.cuda()\n    criteria = CrossEntropyLoss2d(weight) #weight\n\n    if args.cuda:\n        model = model.cuda()  # using GPU for inference\n        criteria = criteria.cuda()\n        cudnn.benchmark = True\n\n    print(\'Dataset statistics\')\n    print(\'mean and std: \', data[\'mean\'], data[\'std\'])\n    print(\'classWeights: \', data[\'classWeights\'])\n\n    if args.save_seg_dir:\n        if not os.path.exists(args.save_seg_dir):\n            os.makedirs(args.save_seg_dir)\n\n    # validation set\n    testLoader = torch.utils.data.DataLoader( CityscapesTestDataSet(args.data_dir, args.test_data_list, mean = data[\'mean\']),\n                                             batch_size = 1, shuffle = False, num_workers = args.num_workers, pin_memory = True)\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=====> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            #model.load_state_dict(checkpoint[\'model\'])\n            model.load_state_dict(convert_state_dict(checkpoint[\'model\']))\n        else:\n            print(""=====> no checkpoint found at \'{}\'"".format(args.resume))\n    \n    print(""=====> beginning testing"")\n    print(""test set length: "", len(testLoader))\n    test(args, testLoader, model)\n\nif __name__ == \'__main__\':\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', default = ""CGNet"", help = ""model name: Context Guided Network (CGNet)"")\n    parser.add_argument(\'--data_dir\', default = ""/home/wty/AllDataSet/Cityscapes"", help = ""data directory"")\n    parser.add_argument(\'--dataset_list\', default = ""cityscapes_trainval_list.txt"",\n                        help = ""train and val data, for computing the ratio of all classes, mean and std"")\n    parser.add_argument(\'--test_data_list\', default = ""./dataset/list/Cityscapes/cityscapes_test_list.txt"", help = ""test set"")\n    parser.add_argument(\'--scaleIn\', type = int, default = 1, help = ""rescale input image, default is 1, keep fixed size"")  \n    parser.add_argument(\'--num_workers\', type = int, default= 1, help = ""the number of parallel threads"") \n    parser.add_argument(\'--batch_size\', type = int, default = 1, help="" the batch_size is set to 1 when evaluating or testing"") \n    parser.add_argument(\'--resume\', type = str, default = ""./checkpoint/cityscapes/CGNet_M3N21bs16gpu2_ontrain/model_1.pth"", \n                        help = ""use the file to load last checkpoint for evaluating or testing "")\n    parser.add_argument(\'--classes\', type = int, default = 19, \n                        help = ""the number of classes in the dataset. 19 and 11 for cityscapes and camvid, respectively"")\n    parser.add_argument(\'--inform_data_file\', default = ""./dataset/wtfile/cityscapes_inform.pkl"", \n                        help = ""storing the classes weights, mean and std"")\n    parser.add_argument(\'--cuda\', default = True, help = ""run on CPU or GPU"")\n    parser.add_argument(\'--M\', type = int, default = 3, help = ""the number of blocks in stage 2"")\n    parser.add_argument(\'--N\', type = int, default = 21, help = ""the number of blocks in stage 3"")\n    parser.add_argument(\'--save_seg_dir\', type = str, default = ""./result/cityscapes/test/"", help = ""saving path of prediction result"")\n    parser.add_argument(""--gpus"", default = ""7"", type = str, help = ""gpu ids (default: 2)"")\n\n    test_func(parser.parse_args())\n\n'"
cityscapes_train.py,16,"b'###########################################################################\n# Created by: Tianyi Wu\n# Email: wutianyi@ict.ac.cn \n# Copyright (c) 2018\n###########################################################################\nimport os\nimport time\nimport torch\nimport timeit\nimport pickle\nimport random\nimport numpy as np\nimport torch.nn as nn\nfrom torch.utils import data\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\nfrom argparse import ArgumentParser\n#user\nfrom model import CGNet  # network\nfrom utils.metric import get_iou\nfrom utils.modeltools import netParams\nfrom utils.loss import CrossEntropyLoss2d  # loss function\nfrom utils.convert_state import convert_state_dict\nfrom  dataset.cityscapes import CityscapesDataSet,CityscapesValDataSet, CityscapesTrainInform  # dataset\n\ndef val(args, val_loader, model, criterion):\n    """"""\n    args:\n      val_loader: loaded for validation dataset\n      model: model\n      criterion: loss function\n    return: IoU class, and mean IoU\n    """"""\n    #evaluation mode\n    model.eval()\n    total_batches = len(val_loader)\n   \n    data_list=[]\n    for i, (input, label, size, name) in enumerate(val_loader):\n        start_time = time.time()\n        input_var = Variable(input, volatile=True).cuda()\n        output = model(input_var)\n        time_taken = time.time() - start_time\n        print(""[%d/%d]  time: %.2f"" % (i, total_batches, time_taken))\n        output= output.cpu().data[0].numpy()\n        gt = np.asarray(label[0].numpy(), dtype = np.uint8)\n        output= output.transpose(1,2,0)\n        output= np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n        data_list.append( [gt.flatten(), output.flatten()])\n\n    meanIoU, per_class_iu= get_iou(data_list, args.classes)\n    return meanIoU, per_class_iu\n\ndef adjust_learning_rate( args, cur_epoch, max_epoch, curEpoch_iter, perEpoch_iter, baselr):\n    """"""\n    poly learning stategyt\n    lr = baselr*(1-iter/max_iter)^power\n    """"""\n    cur_iter = cur_epoch*perEpoch_iter + curEpoch_iter\n    max_iter=max_epoch*perEpoch_iter\n    lr = baselr*pow( (1 - 1.0*cur_iter/max_iter), 0.9)\n\n    return lr\n\n\ndef train(args, train_loader, model, criterion, optimizer, epoch):\n    """"""\n    args:\n       train_loader: loaded for training dataset\n       model: model\n       criterion: loss function\n       optimizer: optimization algorithm, such as ADAM or SGD\n       epoch: epoch number\n    return: average loss, per class IoU, and mean IoU\n    """"""\n    model.train()\n    epoch_loss = []\n\n    data_list=[]\n    total_batches = len(train_loader)\n    print(""=====> the number of iterations per epoch: "", total_batches)\n    for iteration, batch in enumerate( train_loader, 0 ):\n        lr= adjust_learning_rate( args, cur_epoch = epoch, max_epoch = args.max_epochs, \n                                  curEpoch_iter = iteration, perEpoch_iter = total_batches, baselr = args.lr )\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr;\n        \n        start_time = time.time()\n        images, labels, _, _ = batch\n        images = Variable( images ).cuda()\n        labels = Variable( labels.long() ).cuda()\n        output = model( images )\n        loss = criterion(output, labels)\n        optimizer.zero_grad()  #set the grad to zero\n        loss.backward()\n        optimizer.step()\n        epoch_loss.append( loss.item() )\n        time_taken = time.time() - start_time\n        \n        gt = np.asarray( labels.cpu().data[0].numpy(), dtype = np.uint8 )\n        output = output.cpu().data[0].numpy()\n        output = output.transpose(1,2,0)\n        output = np.asarray( np.argmax(output, axis=2), dtype=np.uint8 )\n\n        data_list.append( [gt.flatten(), output.flatten()] )\n\n        print(\'=====> epoch[%d/%d] iter: (%d/%d) \\tcur_lr: %.6f loss: %.3f time:%.2f\' % ( epoch, args.max_epochs,\n              iteration, total_batches, lr,loss.item(), time_taken ) )\n\n    average_epoch_loss_train = sum( epoch_loss ) / len( epoch_loss )\n    meanIoU, per_class_iu = get_iou( data_list, args.classes )\n\n    return average_epoch_loss_train, per_class_iu, meanIoU, lr\n\ndef train_model(args):\n    """"""\n    args:\n       args: global arguments\n    """"""\n    h, w = map(int, args.input_size.split(\',\'))\n    input_size = (h, w)\n    print(""=====> checking if inform_data_file exists"")\n    if not os.path.isfile(args.inform_data_file):\n        print(""%s is not found"" %( args.inform_data_file ) )\n        dataCollect = CityscapesTrainInform(args.data_dir, args.classes, train_set_file = args.dataset_list, \n                                            inform_data_file = args.inform_data_file) #collect mean std, weigth_class information\n        datas = dataCollect.collectDataAndSave()\n        if datas is None:\n            print(""error while pickling data. Please check."")\n            exit(-1)\n    else:\n        print(""find file: "", str(args.inform_data_file))\n        datas = pickle.load( open( args.inform_data_file, ""rb"") )\n    \n    print(args)\n    global network_type\n     \n    if args.cuda:\n        print(""=====> use gpu id: \'{}\'"".format(args.gpus))\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpus\n        if not torch.cuda.is_available():\n            raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\n    \n    args.seed = random.randint(1, 10000)\n    print(""====> Random Seed: "", args.seed)\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed) \n    \n    cudnn.enabled = True\n    M = args.M\n    N = args.N\n    model = CGNet.Context_Guided_Network(classes= args.classes, M= M, N= N)\n    network_type=""CGNet""\n    print(""=====> current architeture:  CGNet"")\n    \n    print(""=====> computing network parameters"")\n    total_paramters = netParams(model)\n    print(""the number of parameters: "" + str(total_paramters))\n    \n    print(""data[\'classWeights\']: "", datas[\'classWeights\'])\n    print(\'=====> Dataset statistics\')\n    print(\'mean and std: \', datas[\'mean\'], datas[\'std\'])\n    \n    # define optimization criteria\n    weight = torch.from_numpy(datas[\'classWeights\'])\n    criteria = CrossEntropyLoss2d(weight)\n\n    if args.cuda:\n        criteria = criteria.cuda()\n        if torch.cuda.device_count()>1:\n            print(""torch.cuda.device_count()="",torch.cuda.device_count())\n            args.gpu_nums = torch.cuda.device_count()\n            model = torch.nn.DataParallel(model).cuda()  #multi-card data parallel\n        else:\n            print(""single GPU for training"")\n            model = model.cuda()  #1-card data parallel\n    \n    args.savedir = ( args.savedir + args.dataset + \'/\'+ network_type +""_M""+ str(M) + \'N\' +str(N) + \'bs\' \n                    + str(args.batch_size)+ \'gpu\' + str(args.gpu_nums)+ ""_""+str(args.train_type)+\'/\')\n    if not os.path.exists(args.savedir):\n        os.makedirs(args.savedir)\n\n    train_transform= transforms.Compose([\n        transforms.ToTensor()])\n    trainLoader = data.DataLoader( CityscapesDataSet( args.data_dir, args.train_data_list, crop_size = input_size, scale = args.random_scale, \n                                                      mirror = args.random_mirror, mean = datas[\'mean\'] ),\n                                   batch_size = args.batch_size, shuffle = True, num_workers = args.num_workers, \n                                   pin_memory = True, drop_last = True )\n    valLoader = data.DataLoader( CityscapesValDataSet( args.data_dir, args.val_data_list,f_scale = 1,  mean = datas[\'mean\']),\n                                 batch_size = 1, shuffle = True, num_workers = args.num_workers, pin_memory = True, drop_last = True )\n\n    start_epoch = 0\n    if args.resume:\n        if os.path.isfile(args.resume):\n            checkpoint = torch.load(args.resume)\n            start_epoch = checkpoint[\'epoch\']\n            model.load_state_dict(checkpoint[\'model\'])\n            #model.load_state_dict(convert_state_dict(checkpoint[\'model\']))\n            print(""=====> loaded checkpoint \'{}\' (epoch {})"".format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=====> no checkpoint found at \'{}\'"".format(args.resume))\n    \n    model.train()\n    cudnn.benchmark= True\n    \n    logFileLoc = args.savedir + args.logFile\n    if os.path.isfile(logFileLoc):\n        logger = open(logFileLoc, \'a\')\n    else:\n        logger = open(logFileLoc, \'w\')\n        logger.write(""Parameters: %s"" % (str(total_paramters)))\n        logger.write(""\\n%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\\t\\t"" % (\'Epoch\', \'Loss(Tr)\', \'Loss(val)\', \'mIOU (tr)\', \'mIOU (val)\'))\n    logger.flush()\n\n    optimizer = torch.optim.Adam(model.parameters(), args.lr, (0.9, 0.999), eps=1e-08, weight_decay=5e-4)\n \n    print(\'=====> beginning training\')\n    for epoch in range(start_epoch, args.max_epochs):\n        #training\n        lossTr, per_class_iu_tr, mIOU_tr, lr = train(args, trainLoader, model, criteria, optimizer, epoch)\n        \n        #validation\n        if epoch % 50 ==0:\n            mIOU_val, per_class_iu = val(args, valLoader, model, criteria)\n            # record train information\n            logger.write(""\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.7f"" % (epoch, lossTr, mIOU_tr, mIOU_val, lr))\n            logger.flush()\n            print(""Epoch : "" + str(epoch) + \' Details\')\n            print(""\\nEpoch No.: %d\\tTrain Loss = %.4f\\t mIOU(tr) = %.4f\\t mIOU(val) = %.4f\\t lr= %.6f"" % (epoch,\n                   lossTr, mIOU_tr, mIOU_val, lr))\n        else:\n            # record train information\n            logger.write(""\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.7f"" % (epoch, lossTr, mIOU_tr, lr))\n            logger.flush()\n            print(""Epoch : "" + str(epoch) + \' Details\')\n            print(""\\nEpoch No.: %d\\tTrain Loss = %.4f\\t mIOU(tr) = %.4f\\t lr= %.6f"" % (epoch, lossTr, mIOU_tr, lr))\n        \n        #save the model\n        model_file_name = args.savedir +\'/model_\' + str(epoch + 1) + \'.pth\'\n        state = {""epoch"": epoch+1, ""model"": model.state_dict()}\n        if epoch > args.max_epochs - 10 :\n            torch.save(state, model_file_name)\n        elif not epoch % 20:\n            torch.save(state, model_file_name)\n\n    logger.close()\n\nif __name__ == \'__main__\':\n    start = timeit.default_timer()\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', default = ""CGNet"", help = ""model name: Context Guided Network (CGNet)"")\n    parser.add_argument(\'--dataset\', default = ""cityscapes"", help = ""dataset: cityscapes or camvid"")\n    parser.add_argument(\'--data_dir\', default = ""/home/wty/AllDataSet/Cityscapes"", help =\'data directory\')\n    parser.add_argument(\'--dataset_list\', default = ""cityscapes_trainval_list.txt"",\n                        help = ""train and val data, for computing the ration of all kinds, mean and std"")\n    parser.add_argument(\'--train_data_list\', default = ""./dataset/list/Cityscapes/cityscapes_trainval_list.txt"", help = ""train set"")\n    parser.add_argument(\'--train_type\', type = str, default = ""ontrainval"", \n                         help = ""ontrain for training on train set, ontrainval for training on train+val set"")\n    parser.add_argument(\'--max_epochs\', type = int, default = 350, help = ""the number of epochs: 300 for train set, 350 for train+val set"")\n    parser.add_argument(\'--val_data_list\', default = ""./dataset/list/Cityscapes/cityscapes_val_list.txt"", help = ""val set"")\n    parser.add_argument(\'--scaleIn\', type = int, default = 1, help = ""for input image, default is 1, keep fixed size"")  \n    parser.add_argument(\'--input_size\', type = str, default = ""680,680"", help = ""input size of model"") \n    parser.add_argument(\'--random_mirror\', type = bool, default = True, help = ""input image random mirror"") \n    parser.add_argument(\'--random_scale\', type = bool, default = True, help = ""input image resize 0.5 to 2"") \n    parser.add_argument(\'--num_workers\', type = int, default = 1, help = "" the number of parallel threads"") \n    parser.add_argument(\'--batch_size\', type = int, default = 16, help = ""the batch size is set to 16 for 2 GPUs"")\n\n    parser.add_argument(\'--lr\', type = float, default = 1e-3, help = ""initial learning rate"")\n    parser.add_argument(\'--savedir\', default = ""./checkpoint/"", help = ""directory to save the model snapshot"")\n    parser.add_argument(\'--resume\', type = str, default = ""./checkpoint/cityscapes/CGNet_M3N21bs16gpu2_ontrainval/model_1.pth"", \n                         help = ""use this file to load last checkpoint for continuing training"")  \n    parser.add_argument(\'--classes\', type = int, default = 19, \n                         help = ""the number of classes in the dataset. 19 and 11 for cityscapes and camvid, respectively"")\n    parser.add_argument(\'--inform_data_file\', default = ""./dataset/wtfile/cityscapes_inform.pkl"", \n                         help = ""saving statistic information of the dataset (train+val set), classes weigtht, mean and std"")\n    parser.add_argument(\'--M\', type = int, default = 3, help = ""the number of blocks in stage 2"")\n    parser.add_argument(\'--N\', type = int, default = 21, help = ""the number of blocks in stage 3"")\n    parser.add_argument(\'--logFile\', default= ""log.txt"", help = ""storing the training and validation logs"")\n    parser.add_argument(\'--cuda\', type = bool, default = True, help = ""running on CPU or GPU"")\n    parser.add_argument(\'--gpus\', type = str, default = ""0,1"", help = ""default GPU devices (0,1)"")\n    args = parser.parse_args()\n    train_model(args)\n    end = timeit.default_timer()\n    print(""training time:"", 1.0*(end-start)/3600)\n\n'"
vis_net.py,0,"b'from utils.summary import summary\nfrom model import CGNet\n\nmodel = CGNet.Context_Guided_Network(19, M=3, N=21)\nmodel.cuda()\nsummary(model,(3,640, 640))\n'"
dataset/__init__.py,0,b'from .camvid import *\nfrom .cityscapes import *\n'
dataset/camvid.py,1,"b'import torch\nimport os\nimport os.path as osp\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport collections\nimport torchvision\nimport cv2\nfrom torch.utils import data\nimport pickle\nfrom PIL import Image\n\n""""""\nCamVid is a road scene understanding dataset with 367 training images and 233 testing images of day and dusk scenes. \nThe challenge is to segment 11 classes such as road, building, cars, pedestrians, signs, poles, side-walk etc. We \nresize images to 360x480 pixels for training and testing.\n""""""\n\nclass CamVidDataSet(data.Dataset):\n    """""" \n       CamVidDataSet is employed to load train set\n       Args:\n        root: the CamVid dataset path, \n        list_path: camvid_train_list.txt, include partial path\n\n    """"""\n    def __init__(self, root= \'/home/wty/AllDataSet/CamVid\', list_path=\'./dataset/list/CamVid/camvid_train_list.txt\', max_iters=None, crop_size=(360, 360), \n            mean=(128, 128, 128), scale=True, mirror=True, ignore_label=11 ):\n        self.root = root\n        self.list_path = list_path\n        self.crop_h, self.crop_w = crop_size\n        self.scale = scale\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.is_mirror = mirror\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        if not max_iters==None:\n            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n        self.files = []\n\n\n        # for split in [""train"", ""trainval"", ""val""]:\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            #print(img_file)\n            label_file = osp.join(self.root, name.split()[1])\n            #print(label_file)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": name\n            })\n\n        print(""length of train set: "",len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n        size = image.shape\n        name = datafiles[""name""]\n        if self.scale:\n            f_scale = 0.5 + random.randint(0, 15) / 10.0  #random resize between 0.5 and 2 \n            image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_LINEAR)\n            label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_NEAREST)\n\n        image = np.asarray(image, np.float32)\n        \n        image = image[:, :, ::-1]  # change to BGR\n        image -= self.mean\n        img_h, img_w = label.shape\n        pad_h = max(self.crop_h - img_h, 0)\n        pad_w = max(self.crop_w - img_w, 0)\n        if pad_h > 0 or pad_w > 0:\n            img_pad = cv2.copyMakeBorder(image, 0, pad_h, 0, \n                pad_w, cv2.BORDER_CONSTANT, \n                value=(0.0, 0.0, 0.0))\n            label_pad = cv2.copyMakeBorder(label, 0, pad_h, 0, \n                pad_w, cv2.BORDER_CONSTANT,\n                value=(self.ignore_label,))\n        else:\n            img_pad, label_pad = image, label\n\n        img_h, img_w = label_pad.shape\n        h_off = random.randint(0, img_h - self.crop_h)\n        w_off = random.randint(0, img_w - self.crop_w)\n        # roi = cv2.Rect(w_off, h_off, self.crop_w, self.crop_h);\n        image = np.asarray(img_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\n        label = np.asarray(label_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\n        \n        \n        image = image.transpose((2, 0, 1)) # NHWC -> NCHW\n        \n        if self.is_mirror:\n            flip = np.random.choice(2) * 2 - 1\n            image = image[:, :, ::flip]\n            label = label[:, ::flip]\n\n\n        return image.copy(), label.copy(), np.array(size), name\n\nclass CamVidValDataSet(data.Dataset):\n    """""" \n       CamVidValDataSet is employed to load val set\n       Args:\n        root: the CamVid dataset path, \n        list_path: camvid_val_list.txt, include partial path\n\n    """"""\n    def __init__(self, root= \'/home/wty/AllDataSet/CamVid\', list_path=\'./dataset/list/CamVid/camvid_val_list.txt\', f_scale=1, mean=(128, 128, 128), ignore_label=11):\n        self.root = root\n        self.list_path = list_path\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.f_scale = f_scale\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        self.files = []\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            #print(img_file)\n            label_file = osp.join(self.root, name.split()[1])\n            #print(label_file)\n            image_name = name.strip().split()[0].strip().split(\'/\',1)[1].split(\'.\')[0]\n            #print(""image_name:  "",image_name)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": image_name\n            })\n\n        print(""length of Validation set: "",len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n        size = image.shape\n        name = datafiles[""name""]\n        if self.f_scale!=1:\n            image = cv2.resize(image, None, fx=self.f_scale, fy=self.f_scale, interpolation = cv2.INTER_LINEAR)\n            #label = cv2.resize(label, None, fx=self.f_scale, fy=self.f_scale, interpolation = cv2.INTER_NEAREST)\n\n        image = np.asarray(image, np.float32)\n\n        image = image[:, :, ::-1]  # change to BGR\n        image -= self.mean\n        image = image.transpose((2, 0, 1)) # HWC -> CHW\n        \n        #print(\'image.shape:\',image.shape)\n        return image.copy(), label.copy(), np.array(size), name\n\nclass CamVidTestDataSet(data.Dataset):\n    """""" \n       CamVidTestDataSet is employed to load test set\n       Args:\n        root: the CamVid dataset path, \n        list_path: camvid_test_list.txt, include partial path\n\n    """"""\n    def __init__(self, root= \'/home/wty/AllDataSet/CamVid\', list_path=\'./dataset/list/CamVid/camvid_test_list.txt\', mean=(128, 128, 128), ignore_label=11):\n        self.root = root\n        self.list_path = list_path\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        self.files = []\n        for name in self.img_ids:\n\n            img_file = osp.join(self.root, name.split()[0])\n            print(img_file)\n            image_name = name.strip().split()[0].strip().split(\'/\',3)[3].split(\'.\')[0]\n            print(image_name)\n            self.files.append({\n                ""img"": img_file,\n                ""name"": image_name\n            })\n        print(""lenth of test set "", len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        name = datafiles[""name""]\n\n      \n\n        image = np.asarray(image, np.float32)\n\n        size = image.shape\n        image = image[:, :, ::-1]  # change to BGR\n        image -= self.mean\n        image = image.transpose((2, 0, 1)) # HWC -> CHW\n\n        return image.copy(), np.array(size), name\n\n\nclass CamVidTrainInform:\n    """""" To get statistical information about the train set, such as mean, std, class distribution.\n        The class is employed for tackle class imbalance.\n    """"""\n    def __init__(self, data_dir= \'/home/wty/AllDataSet/CamVid\', classes= 11, train_set_file=""camvid_trainval_list.txt"", inform_data_file=""cityscapes_inform.pkl"", normVal=1.10):\n        """"""\n        Args:\n           data_dir: directory where the dataset is kept\n           classes: number of classes in the dataset\n           inform_data_file: location where cached file has to be stored\n           normVal: normalization value, as defined in ERFNet paper\n        """"""\n        self.data_dir = data_dir\n        self.classes = classes\n        self.classWeights = np.ones(self.classes, dtype=np.float32)\n        self.normVal = normVal\n        self.mean = np.zeros(3, dtype=np.float32)\n        self.std = np.zeros(3, dtype=np.float32)\n        self.train_set_file= train_set_file\n        self.inform_data_file = inform_data_file\n\n    def compute_class_weights(self, histogram):\n        """"""to compute the class weights\n        Args:\n            histogram: distribution of class samples\n        """"""\n        normHist = histogram / np.sum(histogram)\n        for i in range(self.classes):\n            self.classWeights[i] = 1 / (np.log(self.normVal + normHist[i]))\n\n    def readWholeTrainSet(self, fileName , train_flag=True):\n        """"""to read the whole train set of current dataset.\n        Args:\n        fileName: train set file that stores the image locations\n        trainStg: if processing training or validation data\n        \n        return: 0 if successful\n        """"""\n        global_hist = np.zeros(self.classes, dtype=np.float32)\n\n        no_files = 0\n        min_val_al = 0\n        max_val_al = 0\n        with open(self.data_dir + \'/\' + fileName, \'r\') as textFile:\n        #with open(fileName, \'r\') as textFile:\n            for line in textFile:\n                # we expect the text file to contain the data in following format\n                # <RGB Image> <Label Image>\n                line_arr = line.split()\n                img_file = ((self.data_dir).strip() + \'/\' + line_arr[0].strip()).strip()\n                label_file = ((self.data_dir).strip() + \'/\' + line_arr[1].strip()).strip()\n\n                label_img = cv2.imread(label_file, 0)\n                unique_values = np.unique(label_img)\n                max_val = max(unique_values)\n                min_val = min(unique_values)\n\n                max_val_al = max(max_val, max_val_al)\n                min_val_al = min(min_val, min_val_al)\n\n                if train_flag == True:\n                    hist = np.histogram(label_img, self.classes, [0, self.classes-1])\n                    global_hist += hist[0]\n\n                    rgb_img = cv2.imread(img_file)\n                    self.mean[0] += np.mean(rgb_img[:,:,0])\n                    self.mean[1] += np.mean(rgb_img[:, :, 1])\n                    self.mean[2] += np.mean(rgb_img[:, :, 2])\n\n                    self.std[0] += np.std(rgb_img[:, :, 0])\n                    self.std[1] += np.std(rgb_img[:, :, 1])\n                    self.std[2] += np.std(rgb_img[:, :, 2])\n\n                else:\n                    print(""we can only collect statistical information of train set, please check"")\n\n                if max_val > (self.classes - 1) or min_val < 0:\n                    print(\'Labels can take value between 0 and number of classes.\')\n                    print(\'Some problem with labels. Please check. label_set:\',unique_values)\n                    print(\'Label Image ID: \' + label_file)\n                no_files += 1\n\n        # divide the mean and std values by the sample space size\n        self.mean /= no_files\n        self.std /= no_files\n\n        #compute the class imbalance information\n        self.compute_class_weights(global_hist)\n        return 0\n\n    def collectDataAndSave(self):\n        """""" To collect statistical information of train set and then save it.\n        The file train.txt should be inside the data directory.\n        """"""\n        print(\'Processing training data\')\n        return_val = self.readWholeTrainSet(fileName= self.train_set_file)\n\n        print(\'Pickling data\')\n        if return_val ==0 :\n            data_dict = dict()\n            data_dict[\'mean\'] = self.mean\n            data_dict[\'std\'] = self.std\n            data_dict[\'classWeights\'] = self.classWeights\n            pickle.dump(data_dict, open(self.inform_data_file, ""wb""))\n            return data_dict\n        return None\n\n\nif __name__ == \'__main__\':\n    dataCollect = CamVidTrainInform(""/home/wty/AllDataSet/CamVid"", 11, \'camvid_trainval_list.txt\')\n    data = dataCollect.collect(DataAndSave())\n'"
dataset/cityscapes.py,1,"b'import torch\nimport os\nimport os.path as osp\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport collections\nimport torchvision\nimport cv2\nfrom torch.utils import data\nimport pickle\nfrom PIL import Image\n\nclass CityscapesDataSet(data.Dataset):\n    """""" \n       CityscapesDataSet is employed to load train set\n       Args:\n        root: the Cityscapes dataset path, \n         cityscapes\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 gtFine\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 leftImg8bit\n        list_path: cityscapes_train_list.txt, include partial path\n        mean: bgr_mean (73.15835921, 82.90891754, 72.39239876)\n\n    """"""\n    def __init__(self, root= \'/home/wty/AllDataSet/Cityscapes\', list_path = \'dataset/list/Cityscapes/cityscapes_train_list.txt\', max_iters=None, \n                 crop_size=(512, 1024), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=255 ):\n        self.root = root\n        self.list_path = list_path\n        self.crop_h, self.crop_w = crop_size\n        self.scale = scale\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.is_mirror = mirror\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        if not max_iters==None:\n            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n        self.files = []\n\n\n        # for split in [""train"", ""trainval"", ""val""]:\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            #print(img_file)\n            label_file = osp.join(self.root, name.split()[1])\n            #print(label_file)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": name\n            })\n\n        print(""length of dataset: "",len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n        size = image.shape\n        name = datafiles[""name""]\n        if self.scale:\n            f_scale = 0.5 + random.randint(0, 15) / 10.0  #random resize between 0.5 and 2 \n            image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_LINEAR)\n            label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_NEAREST)\n\n        image = np.asarray(image, np.float32)\n        \n        image = image[:, :, ::-1]  # change to BGR\n        image -= self.mean\n        img_h, img_w = label.shape\n        pad_h = max(self.crop_h - img_h, 0)\n        pad_w = max(self.crop_w - img_w, 0)\n        if pad_h > 0 or pad_w > 0:\n            img_pad = cv2.copyMakeBorder(image, 0, pad_h, 0, \n                pad_w, cv2.BORDER_CONSTANT, \n                value=(0.0, 0.0, 0.0))\n            label_pad = cv2.copyMakeBorder(label, 0, pad_h, 0, \n                pad_w, cv2.BORDER_CONSTANT,\n                value=(self.ignore_label,))\n        else:\n            img_pad, label_pad = image, label\n\n        img_h, img_w = label_pad.shape\n        h_off = random.randint(0, img_h - self.crop_h)\n        w_off = random.randint(0, img_w - self.crop_w)\n        # roi = cv2.Rect(w_off, h_off, self.crop_w, self.crop_h);\n        image = np.asarray(img_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\n        label = np.asarray(label_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\n        \n        \n        image = image.transpose((2, 0, 1)) # NHWC -> NCHW\n        \n        if self.is_mirror:\n            flip = np.random.choice(2) * 2 - 1\n            image = image[:, :, ::flip]\n            label = label[:, ::flip]\n\n\n        return image.copy(), label.copy(), np.array(size), name\n\nclass CityscapesValDataSet(data.Dataset):\n    """""" \n       CityscapesDataSet is employed to load val set\n       Args:\n        root: the Cityscapes dataset path, \n         cityscapes\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 gtFine\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 leftImg8bit\n        list_path: cityscapes_val_list.txt, include partial path\n\n    """"""\n    def __init__(self, root= \'/home/wty/AllDataSet/Cityscapes\', list_path= \'./dataset/list/Cityscapes/cityscapes_val_list.txt\', \n                 f_scale=1, mean=(128, 128, 128), ignore_label=255):\n        self.root = root\n        self.list_path = list_path\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.f_scale = f_scale\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        self.files = []\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            #print(img_file)\n            label_file = osp.join(self.root, name.split()[1])\n            #print(label_file)\n            image_name = name.strip().split()[0].strip().split(\'/\',3)[3].split(\'.\')[0]\n            #print(""image_name:  "",image_name)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": image_name\n            })\n\n        print(""length of dataset: "",len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n        size = image.shape\n        name = datafiles[""name""]\n        if self.f_scale!=1:\n            image = cv2.resize(image, None, fx=self.f_scale, fy=self.f_scale, interpolation = cv2.INTER_LINEAR)\n            label = cv2.resize(label, None, fx=self.f_scale, fy=self.f_scale, interpolation = cv2.INTER_NEAREST)\n\n        image = np.asarray(image, np.float32)\n\n        image = image[:, :, ::-1]  # change to BGR\n        image -= self.mean\n        image = image.transpose((2, 0, 1)) # HWC -> CHW\n        \n        #print(\'image.shape:\',image.shape)\n        return image.copy(), label.copy(), np.array(size), name\n\nclass CityscapesTestDataSet(data.Dataset):\n    """""" \n       CityscapesDataSet is employed to load test set\n       Args:\n        root: the Cityscapes dataset path, \n         cityscapes\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 gtFine\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 leftImg8bit\n        list_path: cityscapes_test_list.txt, include partial path\n\n    """"""\n    def __init__(self, root = \'/home/wty/AllDataSet/Cityscapes\', list_path = \'./dataset/list/Cityscapes/cityscapes_test_list.txt\', mean=(128, 128, 128), ignore_label=255):\n        self.root = root\n        self.list_path = list_path\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        self.files = []\n        for name in self.img_ids:\n\n            img_file = osp.join(self.root, name.split()[0])\n            #print(img_file)\n            image_name = name.strip().split()[0].strip().split(\'/\',3)[3].split(\'.\')[0]\n            #print(image_name)\n            self.files.append({\n                ""img"": img_file,\n                ""name"": image_name\n            })\n        print(""lenth of dataset: "", len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        name = datafiles[""name""]\n        image = np.asarray(image, np.float32)\n        size = image.shape\n        image = image[:, :, ::-1]  # change to BGR\n        image -= self.mean\n        image = image.transpose((2, 0, 1)) # HWC -> CHW\n        return image.copy(), np.array(size), name\n\n\nclass CityscapesTrainInform:\n    """""" To get statistical information about the train set, such as mean, std, class distribution.\n        The class is employed for tackle class imbalance.\n    """"""\n    def __init__(self, data_dir= \'/home/wty/AllDataSet/Cityscapes\', classes= 19, train_set_file=""cityscapes_trainval_list.txt"", inform_data_file=""cityscapes_inform.pkl"", normVal=1.10):\n        """"""\n        Args:\n           data_dir: directory where the dataset is kept\n           classes: number of classes in the dataset\n           inform_data_file: location where cached file has to be stored\n           normVal: normalization value, as defined in ERFNet paper\n        """"""\n        self.data_dir = data_dir\n        self.classes = classes\n        self.classWeights = np.ones(self.classes, dtype=np.float32)\n        self.normVal = normVal\n        self.mean = np.zeros(3, dtype=np.float32)\n        self.std = np.zeros(3, dtype=np.float32)\n        self.train_set_file= train_set_file\n        self.inform_data_file = inform_data_file\n\n    def compute_class_weights(self, histogram):\n        """"""to compute the class weights\n        Args:\n            histogram: distribution of class samples\n        """"""\n        normHist = histogram / np.sum(histogram)\n        for i in range(self.classes):\n            self.classWeights[i] = 1 / (np.log(self.normVal + normHist[i]))\n\n    def readWholeTrainSet(self, fileName , train_flag=True):\n        """"""to read the whole train set of current dataset.\n        Args:\n        fileName: train set file that stores the image locations\n        trainStg: if processing training or validation data\n        \n        return: 0 if successful\n        """"""\n        global_hist = np.zeros(self.classes, dtype=np.float32)\n\n        no_files = 0\n        min_val_al = 0\n        max_val_al = 0\n        with open(self.data_dir + \'/\' + fileName, \'r\') as textFile:\n        #with open(fileName, \'r\') as textFile:\n            for line in textFile:\n                # we expect the text file to contain the data in following format\n                # <RGB Image> <Label Image>\n                line_arr = line.split()\n                img_file = ((self.data_dir).strip() + \'/\' + line_arr[0].strip()).strip()\n                label_file = ((self.data_dir).strip() + \'/\' + line_arr[1].strip()).strip()\n\n                label_img = cv2.imread(label_file, 0)\n                unique_values = np.unique(label_img)\n                max_val = max(unique_values)\n                min_val = min(unique_values)\n\n                max_val_al = max(max_val, max_val_al)\n                min_val_al = min(min_val, min_val_al)\n\n                if train_flag == True:\n                    hist = np.histogram(label_img, self.classes, [0, self.classes-1])\n                    global_hist += hist[0]\n\n                    rgb_img = cv2.imread(img_file)\n                    self.mean[0] += np.mean(rgb_img[:,:,0])\n                    self.mean[1] += np.mean(rgb_img[:, :, 1])\n                    self.mean[2] += np.mean(rgb_img[:, :, 2])\n\n                    self.std[0] += np.std(rgb_img[:, :, 0])\n                    self.std[1] += np.std(rgb_img[:, :, 1])\n                    self.std[2] += np.std(rgb_img[:, :, 2])\n\n                else:\n                    print(""we can only collect statistical information of train set, please check"")\n\n                if max_val > (self.classes - 1) or min_val < 0:\n                    print(\'Labels can take value between 0 and number of classes.\')\n                    print(\'Some problem with labels. Please check. label_set:\',unique_values)\n                    print(\'Label Image ID: \' + label_file)\n                no_files += 1\n\n        # divide the mean and std values by the sample space size\n        self.mean /= no_files\n        self.std /= no_files\n\n        #compute the class imbalance information\n        self.compute_class_weights(global_hist)\n        return 0\n\n    def collectDataAndSave(self):\n        """""" To collect statistical information of train set and then save it.\n        The file train.txt should be inside the data directory.\n        """"""\n        print(\'Processing training data\')\n        return_val = self.readWholeTrainSet(fileName= self.train_set_file)\n\n        print(\'Pickling data\')\n        if return_val ==0 :\n            data_dict = dict()\n            data_dict[\'mean\'] = self.mean\n            data_dict[\'std\'] = self.std\n            data_dict[\'classWeights\'] = self.classWeights\n            pickle.dump(data_dict, open(self.inform_data_file, ""wb""))\n            return data_dict\n        return None\n\n\nif __name__ == \'__main__\':\n    """"""\n    #dst = CityscapesDataSet(""/home/wty/AllDataSet/CityScapes"",\n    #                        \'/home/wty/AllDataSet/CityScapes/cityscapes_train_list.txt\', scale=False)\n    dst = CityscapesValDataSet(""/home/wty/AllDataSet/CityScapes"", \'./list/Cityscapes/cityscapes_val_list.txt\')\n    trainloader = data.DataLoader(dst, batch_size=3)\n    for i, data in enumerate(trainloader):\n        imgs, labels, size, name = data\n        if i == 0:\n            print(name)\n            print(size)\n    """"""\n    \n    dataCollect = CityscapesTrainInform(""/home/wty/AllDataSet/CityScapes"", 19, \'cityscapes_train_list.txt\')\n    data = dataCollect.collect(DataAndSave())\n'"
model/BiSeNet_resnet.py,8,"b'import torch\nfrom torch import nn\nfrom torchvision.models import resnet18\nimport torch.nn.functional as F\n\n\nclass SpatialPath(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.layer1 = self.downsample_block(3, 64)\n        self.layer2 = self.downsample_block(64, 128)\n        self.layer3 = self.downsample_block(128, 256)\n\n    def downsample_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.layer3(x)\n\n\nclass ARM(nn.Module):\n\n    def __init__(self, input_h, input_w, channels):\n        super().__init__()\n        self.pool = nn.AvgPool2d( (input_h, input_w) )\n        self.conv = nn.Conv2d(channels, channels, kernel_size=1, stride=1)\n        self.norm = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        feature_map = x\n        x = self.pool(x)\n        x = self.conv(x)\n        x = self.norm(x)\n        x = torch.sigmoid(x)\n        return x.expand_as(feature_map) * feature_map\n\n\nclass FFM(nn.Module):\n\n    def __init__(self, input_h, input_w, channels):\n        super().__init__()\n        self.feature = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.ReLU()\n        )\n        self.pool = nn.AvgPool2d( ( input_h // 8, input_w // 8 ))\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=1, stride=1)\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=1, stride=1)\n\n    def forward(self, x1, x2):\n        feature = torch.cat([x1, x2], dim=1)\n        feature = self.feature(feature)\n\n        x = self.pool(feature)\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = torch.sigmoid(x)\n        return feature + x.expand_as(feature) * feature\n\n\nclass ContextPath(nn.Module):\n\n    def __init__(self, input_h, input_w):\n        super().__init__()\n        self.input_h = input_h\n        self.input_w = input_w\n        self.backbone = resnet18(pretrained=True)\n        self.x8_arm = ARM(input_h // 8, input_w // 8, 128)\n        self.x16_arm = ARM(input_h // 16, input_w // 16, 256)\n        self.x32_arm = ARM(input_h // 32, input_w // 32,  512)\n        self.global_pool = nn.AvgPool2d((input_h // 32, input_w //32))\n\n    def forward(self, x):\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n        x = self.backbone.layer1(x)\n\n        feature_x8 = self.backbone.layer2(x)\n        feature_x16 = self.backbone.layer3(feature_x8)\n        feature_x32 = self.backbone.layer4(feature_x16)\n        center = self.global_pool(feature_x32)\n\n        feature_x8 = self.x8_arm(feature_x8)\n        feature_x16 = self.x16_arm(feature_x16)\n        feature_x32 = self.x32_arm(feature_x32)\n\n        up_feature_x32 = F.upsample(center, size=(self.input_h // 32, self.input_w //32), mode=\'bilinear\', align_corners=False)\n        ensemble_feature_x32 = feature_x32 + up_feature_x32\n\n        up_feature_x16 = F.upsample(ensemble_feature_x32, scale_factor=2, mode=\'bilinear\', align_corners=False)\n        ensemble_feature_x16 = torch.cat((feature_x16, up_feature_x16), dim=1)\n\n        up_feature_x8 = F.upsample(ensemble_feature_x16, scale_factor=2, mode=\'bilinear\', align_corners=False)\n        ensemble_feature_x8 = torch.cat((feature_x8, up_feature_x8), dim=1)\n\n        return ensemble_feature_x8\n\n\nclass BiSeNet_res18(nn.Module):\n\n    def __init__(self, input_h, input_w, n_classes = 19):\n        super().__init__()\n        self.spatial_path = SpatialPath()\n        self.context_path = ContextPath(input_h, input_w)\n        self.ffm = FFM(input_h, input_w, 1152)\n        self.pred = nn.Conv2d(1152, n_classes, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x1 = self.spatial_path(x)\n        #print(""x1.size:"", x1.size())\n        x2 = self.context_path(x)\n        #print(""x2.size:"", x2.size())\n        feature = self.ffm(x1, x2)\n        seg = self.pred(feature)\n        return F.upsample(seg, x.size()[2:], mode=\'bilinear\', align_corners=False)\n\n\nif __name__ == \'__main__\':\n    #images = torch.rand(2, 3, 224, 224)\n    images = torch.rand(8, 3, 1024, 2048)\n    model = BiSeNet_res18(2048, 1024)\n    print(model(images).size())\n'"
model/BiSeNet_xception.py,8,"b'import torch\nfrom torch import nn\nfrom .xception import xception\nimport torch.nn.functional as F\n\n\nclass SpatialPath(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.layer1 = self.downsample_block(3, 64)\n        self.layer2 = self.downsample_block(64, 128)\n        self.layer3 = self.downsample_block(128, 256)\n\n    def downsample_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return self.layer3(x)\n\n\nclass ARM(nn.Module):\n\n    def __init__(self, input_h, input_w, channels):\n        super().__init__()\n        self.pool = nn.AvgPool2d( (input_h, input_w) )\n        self.conv = nn.Conv2d(channels, channels, kernel_size=1, stride=1)\n        self.norm = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        feature_map = x\n        x = self.pool(x)\n        x = self.conv(x)\n        x = self.norm(x)\n        x = torch.sigmoid(x)\n        return x.expand_as(feature_map) * feature_map\n\n\nclass FFM(nn.Module):\n\n    def __init__(self, input_h, input_w, channels):\n        super().__init__()\n        self.feature = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.ReLU()\n        )\n        self.pool = nn.AvgPool2d( ( input_h // 8, input_w // 8 ))\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=1, stride=1)\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=1, stride=1)\n\n    def forward(self, x1, x2):\n        feature = torch.cat([x1, x2], dim=1)\n        feature = self.feature(feature)\n\n        x = self.pool(feature)\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = torch.sigmoid(x)\n        return feature + x.expand_as(feature) * feature\n\n\nclass ContextPath(nn.Module):\n\n    def __init__(self, input_h, input_w):\n        super().__init__()\n        self.input_h = input_h\n        self.input_w = input_w\n        self.backbone = xception()\n        self.x8_arm = ARM(input_h // 8, input_w // 8, 256)\n        self.x16_arm = ARM(input_h // 16, input_w // 16, 728)\n        self.x32_arm = ARM(input_h // 32, input_w // 32,  2048)\n        self.global_pool = nn.AvgPool2d((input_h // 32, input_w //32))\n\n    def forward(self, x):\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n\n        feature_x8 = self.backbone.layer3(x)\n        feature_x16 = self.backbone.layer4(feature_x8)\n        feature_x32 = self.backbone.layer5(feature_x16)\n        center = self.global_pool(feature_x32)\n\n        feature_x8 = self.x8_arm(feature_x8)\n        feature_x16 = self.x16_arm(feature_x16)\n        feature_x32 = self.x32_arm(feature_x32)\n\n        up_feature_x32 = F.upsample(center, size=(self.input_h // 32, self.input_w //32), mode=\'bilinear\', align_corners=False)\n        ensemble_feature_x32 = feature_x32 + up_feature_x32\n\n        up_feature_x16 = F.upsample(ensemble_feature_x32, scale_factor=2, mode=\'bilinear\', align_corners=False)\n        ensemble_feature_x16 = torch.cat((feature_x16, up_feature_x16), dim=1)\n\n        up_feature_x8 = F.upsample(ensemble_feature_x16, scale_factor=2, mode=\'bilinear\', align_corners=False)\n        ensemble_feature_x8 = torch.cat((feature_x8, up_feature_x8), dim=1)\n\n        return ensemble_feature_x8\n\n\nclass BiSeNet_Xception34(nn.Module):\n\n    def __init__(self, input_h, input_w, n_classes = 19):\n        super().__init__()\n        self.spatial_path = SpatialPath()\n        self.context_path = ContextPath(input_h, input_w)\n        self.ffm = FFM(input_h, input_w, 3288)\n        self.pred = nn.Conv2d(3288, n_classes, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x1 = self.spatial_path(x)\n        #print(""x1.size:"", x1.size())\n        x2 = self.context_path(x)\n        #print(""x2.size:"", x2.size())\n        feature = self.ffm(x1, x2)\n        seg = self.pred(feature)\n        return F.upsample(seg, x.size()[2:], mode=\'bilinear\', align_corners=False)\n\n\nif __name__ == \'__main__\':\n    #images = torch.rand(2, 3, 224, 224)\n    images = torch.rand(2, 3, 1024, 2048)\n    model = BiSeNet_Xception34(1024, 2048)\n    print(model(images).size())\n'"
model/CGNet.py,7,"b'###########################################################################\n# Created by: Tianyi Wu\n# Email: wutianyi@ict.ac.cn \n# Copyright (c) 2018\n###########################################################################\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [""Context_Guided_Network""]  \n#Filter out variables, functions, and classes that other programs don\'t need or don\'t want when running cmd ""from CGNet import *""\n\nclass ConvBNPReLU(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        """"""\n        args:\n            nIn: number of input channels\n            nOut: number of output channels\n            kSize: kernel size\n            stride: stride rate for down-sampling. Default is 1\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BNPReLU(nn.Module):\n    def __init__(self, nOut):\n        """"""\n        args:\n           nOut: channels of output feature maps\n        """"""\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: normalized and thresholded feature map\n        """"""\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\nclass ConvBN(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        """"""\n        args:\n           nIn: number of input channels\n           nOut: number of output channels\n           kSize: kernel size\n           stride: optinal stide for down-sampling\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\nclass Conv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        """"""\n        args:\n            nIn: number of input channels\n            nOut: number of output channels\n            kSize: kernel size\n            stride: optional stride rate for down-sampling\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        return output\n\nclass ChannelWiseConv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        """"""\n        Args:\n            nIn: number of input channels\n            nOut: number of output channels, default (nIn == nOut)\n            kSize: kernel size\n            stride: optional stride rate for down-sampling\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), groups=nIn, bias=False)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        return output\nclass DilatedConv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n        """"""\n        args:\n           nIn: number of input channels\n           nOut: number of output channels\n           kSize: kernel size\n           stride: optional stride rate for down-sampling\n           d: dilation rate\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2) * d\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False, dilation=d)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        return output\n\nclass ChannelWiseDilatedConv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n        """"""\n        args:\n           nIn: number of input channels\n           nOut: number of output channels, default (nIn == nOut)\n           kSize: kernel size\n           stride: optional stride rate for down-sampling\n           d: dilation rate\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2) * d\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), groups= nIn, bias=False, dilation=d)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        return output\n\nclass FGlo(nn.Module):\n    """"""\n    the FGlo class is employed to refine the joint feature of both local feature and surrounding context.\n    """"""\n    def __init__(self, channel, reduction=16):\n        super(FGlo, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, channel // reduction),\n                nn.ReLU(inplace=True),\n                nn.Linear(channel // reduction, channel),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass ContextGuidedBlock_Down(nn.Module):\n    """"""\n    the size of feature map divided 2, (H,W,C)---->(H/2, W/2, 2C)\n    """"""\n    def __init__(self, nIn, nOut, dilation_rate=2, reduction=16):\n        """"""\n        args:\n           nIn: the channel of input feature map\n           nOut: the channel of output feature map, and nOut=2*nIn\n        """"""\n        super().__init__()\n        self.conv1x1 = ConvBNPReLU(nIn, nOut, 3, 2)  #  size/2, channel: nIn--->nOut\n        \n        self.F_loc = ChannelWiseConv(nOut, nOut, 3, 1)\n        self.F_sur = ChannelWiseDilatedConv(nOut, nOut, 3, 1, dilation_rate)\n        \n        self.bn = nn.BatchNorm2d(2*nOut, eps=1e-3)\n        self.act = nn.PReLU(2*nOut)\n        self.reduce = Conv(2*nOut, nOut,1,1)  #reduce dimension: 2*nOut--->nOut\n        \n        self.F_glo = FGlo(nOut, reduction)    \n\n    def forward(self, input):\n        output = self.conv1x1(input)\n        loc = self.F_loc(output)\n        sur = self.F_sur(output)\n\n        joi_feat = torch.cat([loc, sur],1)  #  the joint feature\n        joi_feat = self.bn(joi_feat)\n        joi_feat = self.act(joi_feat)\n        joi_feat = self.reduce(joi_feat)     #channel= nOut\n        \n        output = self.F_glo(joi_feat)  # F_glo is employed to refine the joint feature\n\n        return output\n\n\nclass ContextGuidedBlock(nn.Module):\n    def __init__(self, nIn, nOut, dilation_rate=2, reduction=16, add=True):\n        """"""\n        args:\n           nIn: number of input channels\n           nOut: number of output channels, \n           add: if true, residual learning\n        """"""\n        super().__init__()\n        n= int(nOut/2)\n        self.conv1x1 = ConvBNPReLU(nIn, n, 1, 1)  #1x1 Conv is employed to reduce the computation\n        self.F_loc = ChannelWiseConv(n, n, 3, 1) # local feature\n        self.F_sur = ChannelWiseDilatedConv(n, n, 3, 1, dilation_rate) # surrounding context\n        self.bn_prelu = BNPReLU(nOut)\n        self.add = add\n        self.F_glo= FGlo(nOut, reduction)\n\n    def forward(self, input):\n        output = self.conv1x1(input)\n        loc = self.F_loc(output)\n        sur = self.F_sur(output)\n        \n        joi_feat = torch.cat([loc, sur], 1) \n\n        joi_feat = self.bn_prelu(joi_feat)\n\n        output = self.F_glo(joi_feat)  #F_glo is employed to refine the joint feature\n        # if residual version\n        if self.add:\n            output  = input + output\n        return output\n\nclass InputInjection(nn.Module):\n    def __init__(self, downsamplingRatio):\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, downsamplingRatio):\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n    def forward(self, input):\n        for pool in self.pool:\n            input = pool(input)\n        return input\n\n\nclass Context_Guided_Network(nn.Module):\n    """"""\n    This class defines the proposed Context Guided Network (CGNet) in this work.\n    """"""\n    def __init__(self, classes=19, M= 3, N= 21, dropout_flag = False):\n        """"""\n        args:\n          classes: number of classes in the dataset. Default is 19 for the cityscapes\n          M: the number of blocks in stage 2\n          N: the number of blocks in stage 3\n        """"""\n        super().__init__()\n        self.level1_0 = ConvBNPReLU(3, 32, 3, 2)      # feature map size divided 2, 1/2\n        self.level1_1 = ConvBNPReLU(32, 32, 3, 1)                          \n        self.level1_2 = ConvBNPReLU(32, 32, 3, 1)      \n\n        self.sample1 = InputInjection(1)  #down-sample for Input Injection, factor=2\n        self.sample2 = InputInjection(2)  #down-sample for Input Injiection, factor=4\n\n        self.b1 = BNPReLU(32 + 3)\n        \n        #stage 2\n        self.level2_0 = ContextGuidedBlock_Down(32 +3, 64, dilation_rate=2,reduction=8)  \n        self.level2 = nn.ModuleList()\n        for i in range(0, M-1):\n            self.level2.append(ContextGuidedBlock(64 , 64, dilation_rate=2, reduction=8))  #CG block\n        self.bn_prelu_2 = BNPReLU(128 + 3)\n        \n        #stage 3\n        self.level3_0 = ContextGuidedBlock_Down(128 + 3, 128, dilation_rate=4, reduction=16) \n        self.level3 = nn.ModuleList()\n        for i in range(0, N-1):\n            self.level3.append(ContextGuidedBlock(128 , 128, dilation_rate=4, reduction=16)) # CG block\n        self.bn_prelu_3 = BNPReLU(256)\n\n        if dropout_flag:\n            print(""have droput layer"")\n            self.classifier = nn.Sequential(nn.Dropout2d(0.1, False),Conv(256, classes, 1, 1))\n        else:\n            self.classifier = nn.Sequential(Conv(256, classes, 1, 1))\n\n        #init weights\n        for m in self.modules():\n            classname = m.__class__.__name__\n            if classname.find(\'Conv2d\')!= -1:\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n                elif classname.find(\'ConvTranspose2d\')!= -1:\n                    nn.init.kaiming_normal_(m.weight)\n                    if m.bias is not None:\n                        m.bias.data.zero_()\n\n    def forward(self, input):\n        """"""\n        args:\n            input: Receives the input RGB image\n            return: segmentation map\n        """"""\n        # stage 1\n        output0 = self.level1_0(input)\n        output0 = self.level1_1(output0)\n        output0 = self.level1_2(output0)\n        inp1 = self.sample1(input)\n        inp2 = self.sample2(input)\n\n        # stage 2\n        output0_cat = self.b1(torch.cat([output0, inp1], 1))\n        output1_0 = self.level2_0(output0_cat) # down-sampled\n        \n        for i, layer in enumerate(self.level2):\n            if i==0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.bn_prelu_2(torch.cat([output1,  output1_0, inp2], 1))\n\n        # stage 3\n        output2_0 = self.level3_0(output1_cat) # down-sampled\n        for i, layer in enumerate(self.level3):\n            if i==0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.bn_prelu_3(torch.cat([output2_0, output2], 1))\n       \n        # classifier\n        classifier = self.classifier(output2_cat)\n\n        # upsample segmenation map ---> the input image size\n        out = F.upsample(classifier, input.size()[2:], mode=\'bilinear\',align_corners = False)   #Upsample score map, factor=8\n        return out\n\n\n'"
model/DFN.py,3,"b'#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n# Author: Xiangtai(lxtpku@pku.edu.cn)\n# Implementation of Paper Learning a Discriminative Feature Network for Semantic Segmentation (CVPR2018)(face_plus_plus)\n\n\nimport torch\nimport torch.nn as nn\nfrom model.resnet import resnet101\n#from torchvision.models import resnet101\n\n__all__ = [""DFN""]\n\nclass CAB(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(CAB, self).__init__()\n        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n        self.sigmod = nn.Sigmoid()\n\n    def forward(self, x):\n        x1, x2 = x  # high, low\n        x = torch.cat([x1,x2],dim=1)\n        x = self.global_pooling(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.sigmod(x)\n        x2 = x * x2\n        res = x2 + x1\n        return res\n\nclass RRB(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(RRB, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        res  = self.conv2(x)\n        res = self.bn(res)\n        res = self.relu(res)\n        res = self.conv3(res)\n        return self.relu(x + res)\n\n\nclass DFN(nn.Module):\n    def __init__(self, num_class=19):\n        super(DFN, self).__init__()\n        self.num_class = num_class\n        self.resnet_features = resnet101(pretrained=False)\n        self.layer0 = nn.Sequential(self.resnet_features.conv1, self.resnet_features.bn1,\n                                    self.resnet_features.relu  #self.resnet_features.conv3,\n                                    #self.resnet_features.bn3, self.resnet_features.relu3\n                                    )\n        self.layer1 = nn.Sequential(self.resnet_features.maxpool, self.resnet_features.layer1)\n        self.layer2 = self.resnet_features.layer2\n        self.layer3 = self.resnet_features.layer3\n        self.layer4 = self.resnet_features.layer4\n\n        # this is for smooth network\n        self.out_conv = nn.Conv2d(2048,self.num_class,kernel_size=1,stride=1)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.cab1 = CAB(self.num_class*2,self.num_class)\n        self.cab2 = CAB(self.num_class*2,self.num_class)\n        self.cab3 = CAB(self.num_class*2,self.num_class)\n        self.cab4 = CAB(self.num_class*2,self.num_class)\n\n        self.rrb_d_1 = RRB(256, self.num_class)\n        self.rrb_d_2 = RRB(512, self.num_class)\n        self.rrb_d_3 = RRB(1024, self.num_class)\n        self.rrb_d_4 = RRB(2048, self.num_class)\n\n        self.upsample = nn.Upsample(scale_factor=2,mode=""bilinear"")\n        self.upsample_4 = nn.Upsample(scale_factor=4, mode=""bilinear"")\n        self.upsample_8 = nn.Upsample(scale_factor=8, mode=""bilinear"")\n\n        self.rrb_u_1 = RRB(self.num_class,self.num_class)\n        self.rrb_u_2 = RRB(self.num_class,self.num_class)\n        self.rrb_u_3 = RRB(self.num_class,self.num_class)\n        self.rrb_u_4 = RRB(self.num_class,self.num_class)\n\n\n        ## this is for boarder net work\n        self.rrb_db_1 = RRB(256, self.num_class)\n        self.rrb_db_2 = RRB(512, self.num_class)\n        self.rrb_db_3 = RRB(1024, self.num_class)\n        self.rrb_db_4 = RRB(2048, self.num_class)\n\n        self.rrb_trans_1 = RRB(self.num_class,self.num_class)\n        self.rrb_trans_2 = RRB(self.num_class,self.num_class)\n        self.rrb_trans_3 = RRB(self.num_class,self.num_class)\n\n    def forward(self, x):\n        # suppose input = x , if x 512\n        f0 = self.layer0(x)  # 256\n        f1 = self.layer1(f0)  # 128\n        f2 = self.layer2(f1)  # 64\n        f3 = self.layer3(f2)  # 32\n        f4 = self.layer4(f3)  # 16\n\n        # for border network\n        res1 = self.rrb_db_1(f1)\n        res1 = self.rrb_trans_1(res1 + self.upsample(self.rrb_db_2(f2)))\n        res1 = self.rrb_trans_2(res1 + self.upsample_4(self.rrb_db_3(f3)))\n        res1 = self.rrb_trans_3(res1 + self.upsample_8(self.rrb_db_4(f4)))\n        # print (res1.size())\n        # for smooth network\n        res2 = self.out_conv(f4)\n        res2 = self.global_pool(res2)  #\n        res2 = nn.Upsample(size=f4.size()[2:],mode=""nearest"")(res2)\n\n        f4 = self.rrb_d_4(f4)\n        res2 = self.cab4([res2,f4])\n        res2 = self.rrb_u_1(res2)\n\n        f3 = self.rrb_d_3(f3)\n        res2 = self.cab3([self.upsample(res2),f3])\n        res2 =self.rrb_u_2(res2)\n\n        f2 = self.rrb_d_2(f2)\n        res2 = self.cab2([self.upsample(res2), f2])\n        res2 =self.rrb_u_3(res2)\n\n        f1 = self.rrb_d_1(f1)\n        res2 = self.cab1([self.upsample(res2), f1])\n        res2 = self.rrb_u_4(res2)\n\n        return res1, res2\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\nif __name__ == \'__main__\':\n    model = DFN(19).cuda()\n    model.freeze_bn()\n    model.eval()\n    image = torch.autograd.Variable(torch.randn(1, 3, 512, 512), volatile=True).cuda()\n    res1, res2 = model(image)\n    print (res1.size(), res2.size())\n'"
model/DeepLabV3plus_resnet.py,11,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, rate=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               dilation=rate, padding=rate, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.rate = rate\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n\n    def __init__(self, nInputChannels, block, layers, os=16, pretrained=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        if os == 16:\n            strides = [1, 2, 2, 1]\n            rates = [1, 1, 1, 2]\n            blocks = [1, 2, 4]\n        elif os == 8:\n            strides = [1, 2, 1, 1]\n            rates = [1, 1, 2, 2]\n            blocks = [1, 2, 1]\n        else:\n            raise NotImplementedError\n\n        # Modules\n        self.conv1 = nn.Conv2d(nInputChannels, 64, kernel_size=7, stride=2, padding=3,\n                                bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=strides[0], rate=rates[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=strides[1], rate=rates[1])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=strides[2], rate=rates[2])\n        self.layer4 = self._make_MG_unit(block, 512, blocks=blocks, stride=strides[3], rate=rates[3])\n\n        self._init_weight()\n\n        if pretrained:\n            self._load_pretrained_model()\n\n    def _make_layer(self, block, planes, blocks, stride=1, rate=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, rate, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_MG_unit(self, block, planes, blocks=[1,2,4], stride=1, rate=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, rate=blocks[0]*rate, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, len(blocks)):\n            layers.append(block(self.inplanes, planes, stride=1, rate=blocks[i]*rate))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        low_level_feat = x\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x, low_level_feat\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _load_pretrained_model(self):\n        pretrain_dict = model_zoo.load_url(\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\ndef ResNet101(nInputChannels=3, os=16, pretrained=False):\n    model = ResNet(nInputChannels, Bottleneck, [3, 4, 23, 3], os, pretrained=pretrained)\n    return model\n\n\nclass ASPP_module(nn.Module):\n    def __init__(self, inplanes, planes, rate):\n        super(ASPP_module, self).__init__()\n        if rate == 1:\n            kernel_size = 1\n            padding = 0\n        else:\n            kernel_size = 3\n            padding = rate\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=rate, bias=False)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass DeepLabv3_plus(nn.Module):\n    def __init__(self, nInputChannels=3, n_classes=21, os=16, pretrained=False, _print=True):\n        if _print:\n            print(""Constructing DeepLabv3+ model..."")\n            print(""Number of classes: {}"".format(n_classes))\n            print(""Output stride: {}"".format(os))\n            print(""Number of Input Channels: {}"".format(nInputChannels))\n        super(DeepLabv3_plus, self).__init__()\n\n        # Atrous Conv\n        self.resnet_features = ResNet101(nInputChannels, os, pretrained=pretrained)\n\n        # ASPP\n        if os == 16:\n            rates = [1, 6, 12, 18]\n        elif os == 8:\n            rates = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = ASPP_module(2048, 256, rate=rates[0])\n        self.aspp2 = ASPP_module(2048, 256, rate=rates[1])\n        self.aspp3 = ASPP_module(2048, 256, rate=rates[2])\n        self.aspp4 = ASPP_module(2048, 256, rate=rates[3])\n\n        self.relu = nn.ReLU()\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(256),\n                                             nn.ReLU())\n\n        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(256)\n\n        # adopt [1x1, 48] for channel reduction.\n        self.conv2 = nn.Conv2d(256, 48, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(48)\n\n        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       nn.BatchNorm2d(256),\n                                       nn.ReLU(),\n                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       nn.BatchNorm2d(256),\n                                       nn.ReLU(),\n                                       nn.Conv2d(256, n_classes, kernel_size=1, stride=1))\n\n    def forward(self, input):\n        x, low_level_features = self.resnet_features(input)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.upsample(x5, size=x4.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = F.upsample(x, size=(int(math.ceil(input.size()[-2]/4)),\n                                int(math.ceil(input.size()[-1]/4))), mode=\'bilinear\', align_corners=True)\n\n        low_level_features = self.conv2(low_level_features)\n        low_level_features = self.bn2(low_level_features)\n        low_level_features = self.relu(low_level_features)\n\n\n        x = torch.cat((x, low_level_features), dim=1)\n        x = self.last_conv(x)\n        x = F.upsample(x, size=input.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        return x\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n                #print(""nn.conv2d"")\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\ndef get_1x_lr_params(model):\n    """"""\n    This generator returns all the parameters of the net except for\n    the last classification layer. Note that for each batchnorm layer,\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n    any batchnorm parameter\n    """"""\n    b = [model.resnet_features]\n    for i in range(len(b)):\n        for k in b[i].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef get_10x_lr_params(model):\n    """"""\n    This generator returns all the parameters for the last layer of the net,\n    which does the classification of pixel into classes\n    """"""\n    b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]\n    for j in range(len(b)):\n        for k in b[j].parameters():\n            if k.requires_grad:\n                yield k\n\n\nif __name__ == ""__main__"":\n    model = DeepLabv3_plus(nInputChannels=3, n_classes=21, os=16, pretrained=True, _print=True)\n    model.eval()\n    image = torch.randn(1, 3, 512, 512)\n    with torch.no_grad():\n        output = model.forward(image)\n    print(output.size())\n'"
model/DeepLabV3plus_xception.py,10,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=0, dilation=1, bias=False):\n        super(SeparableConv2d, self).__init__()\n\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation,\n                               groups=inplanes, bias=bias)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\ndef fixed_padding(inputs, kernel_size, rate):\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n    return padded_inputs\n\n\nclass SeparableConv2d_same(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False):\n        super(SeparableConv2d_same, self).__init__()\n\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0, dilation,\n                               groups=inplanes, bias=bias)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = fixed_padding(x, self.conv1.kernel_size[0], rate=self.conv1.dilation[0])\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, inplanes, planes, reps, stride=1, dilation=1, start_with_relu=True, grow_first=True, is_last=False):\n        super(Block, self).__init__()\n\n        if planes != inplanes or stride != 1:\n            self.skip = nn.Conv2d(inplanes, planes, 1, stride=stride, bias=False)\n            self.skipbn = nn.BatchNorm2d(planes)\n        else:\n            self.skip = None\n\n        self.relu = nn.ReLU(inplace=True)\n        rep = []\n\n        filters = inplanes\n        if grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))\n            rep.append(nn.BatchNorm2d(planes))\n            filters = planes\n\n        for i in range(reps - 1):\n            rep.append(self.relu)\n            rep.append(SeparableConv2d_same(filters, filters, 3, stride=1, dilation=dilation))\n            rep.append(nn.BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))\n            rep.append(nn.BatchNorm2d(planes))\n\n        if not start_with_relu:\n            rep = rep[1:]\n\n        if stride != 1:\n            rep.append(SeparableConv2d_same(planes, planes, 3, stride=2))\n\n        if stride == 1 and is_last:\n            rep.append(SeparableConv2d_same(planes, planes, 3, stride=1))\n\n\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x += skip\n\n        return x\n\n\nclass Xception(nn.Module):\n    """"""\n    Modified Alighed Xception\n    """"""\n    def __init__(self, inplanes=3, os=16, pretrained=False):\n        super(Xception, self).__init__()\n\n        if os == 16:\n            entry_block3_stride = 2\n            middle_block_rate = 1\n            exit_block_rates = (1, 2)\n        elif os == 8:\n            entry_block3_stride = 1\n            middle_block_rate = 2\n            exit_block_rates = (2, 4)\n        else:\n            raise NotImplementedError\n\n\n        # Entry flow\n        self.conv1 = nn.Conv2d(inplanes, 32, 3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        self.block1 = Block(64, 128, reps=2, stride=2, start_with_relu=False)\n        self.block2 = Block(128, 256, reps=2, stride=2, start_with_relu=True, grow_first=True)\n        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, start_with_relu=True, grow_first=True,\n                            is_last=True)\n\n        # Middle flow\n        self.block4  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block5  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block6  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block7  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block8  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block9  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block10 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block11 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block12 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block13 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block14 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block15 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block16 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block17 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block18 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n        self.block19 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n\n        # Exit flow\n        self.block20 = Block(728, 1024, reps=2, stride=1, dilation=exit_block_rates[0],\n                             start_with_relu=True, grow_first=False, is_last=True)\n\n        self.conv3 = SeparableConv2d_same(1024, 1536, 3, stride=1, dilation=exit_block_rates[1])\n        self.bn3 = nn.BatchNorm2d(1536)\n\n        self.conv4 = SeparableConv2d_same(1536, 1536, 3, stride=1, dilation=exit_block_rates[1])\n        self.bn4 = nn.BatchNorm2d(1536)\n\n        self.conv5 = SeparableConv2d_same(1536, 2048, 3, stride=1, dilation=exit_block_rates[1])\n        self.bn5 = nn.BatchNorm2d(2048)\n\n        # Init weights\n        self.__init_weight()\n\n        # Load pretrained model\n        if pretrained:\n            self.__load_xception_pretrained()\n\n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        low_level_feat = x\n        x = self.block2(x)\n        x = self.block3(x)\n\n        # Middle flow\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n        x = self.block13(x)\n        x = self.block14(x)\n        x = self.block15(x)\n        x = self.block16(x)\n        x = self.block17(x)\n        x = self.block18(x)\n        x = self.block19(x)\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n\n        return x, low_level_feat\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def __load_xception_pretrained(self):\n        pretrain_dict = model_zoo.load_url(\'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n\n        for k, v in pretrain_dict.items():\n            print(k)\n            if k in state_dict:\n                if \'pointwise\' in k:\n                    v = v.unsqueeze(-1).unsqueeze(-1)\n                if k.startswith(\'block12\'):\n                    model_dict[k.replace(\'block12\', \'block20\')] = v\n                elif k.startswith(\'block11\'):\n                    model_dict[k.replace(\'block11\', \'block12\')] = v\n                elif k.startswith(\'conv3\'):\n                    model_dict[k] = v\n                elif k.startswith(\'bn3\'):\n                    model_dict[k] = v\n                    model_dict[k.replace(\'bn3\', \'bn4\')] = v\n                elif k.startswith(\'conv4\'):\n                    model_dict[k.replace(\'conv4\', \'conv5\')] = v\n                elif k.startswith(\'bn4\'):\n                    model_dict[k.replace(\'bn4\', \'bn5\')] = v\n                else:\n                    model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\nclass ASPP_module(nn.Module):\n    def __init__(self, inplanes, planes, rate):\n        super(ASPP_module, self).__init__()\n        if rate == 1:\n            kernel_size = 1\n            padding = 0\n        else:\n            kernel_size = 3\n            padding = rate\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=rate, bias=False)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        self.__init_weight()\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass DeepLabv3_plus(nn.Module):\n    def __init__(self, nInputChannels=3, n_classes=21, os=16, pretrained=False, _print=True):\n        if _print:\n            print(""Constructing DeepLabv3+ model..."")\n            print(""Number of classes: {}"".format(n_classes))\n            print(""Output stride: {}"".format(os))\n            print(""Number of Input Channels: {}"".format(nInputChannels))\n        super(DeepLabv3_plus, self).__init__()\n\n        # Atrous Conv\n        self.xception_features = Xception(nInputChannels, os, pretrained)\n\n        # ASPP\n        if os == 16:\n            rates = [1, 6, 12, 18]\n        elif os == 8:\n            rates = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = ASPP_module(2048, 256, rate=rates[0])\n        self.aspp2 = ASPP_module(2048, 256, rate=rates[1])\n        self.aspp3 = ASPP_module(2048, 256, rate=rates[2])\n        self.aspp4 = ASPP_module(2048, 256, rate=rates[3])\n\n        self.relu = nn.ReLU()\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(256),\n                                             nn.ReLU())\n\n        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(256)\n\n        # adopt [1x1, 48] for channel reduction.\n        self.conv2 = nn.Conv2d(128, 48, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(48)\n\n        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       nn.BatchNorm2d(256),\n                                       nn.ReLU(),\n                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       nn.BatchNorm2d(256),\n                                       nn.ReLU(),\n                                       nn.Conv2d(256, n_classes, kernel_size=1, stride=1))\n\n    def forward(self, input):\n        x, low_level_features = self.xception_features(input)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.upsample(x5, size=x4.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = F.upsample(x, size=(int(math.ceil(input.size()[-2]/4)),\n                                int(math.ceil(input.size()[-1]/4))), mode=\'bilinear\', align_corners=True)\n\n        low_level_features = self.conv2(low_level_features)\n        low_level_features = self.bn2(low_level_features)\n        low_level_features = self.relu(low_level_features)\n\n\n        x = torch.cat((x, low_level_features), dim=1)\n        x = self.last_conv(x)\n        x = F.upsample(x, size=input.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        return x\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                # torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\ndef get_1x_lr_params(model):\n    """"""\n    This generator returns all the parameters of the net except for\n    the last classification layer. Note that for each batchnorm layer,\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n    any batchnorm parameter\n    """"""\n    b = [model.xception_features]\n    for i in range(len(b)):\n        for k in b[i].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef get_10x_lr_params(model):\n    """"""\n    This generator returns all the parameters for the last layer of the net,\n    which does the classification of pixel into classes\n    """"""\n    b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]\n    for j in range(len(b)):\n        for k in b[j].parameters():\n            if k.requires_grad:\n                yield k\n\n\nif __name__ == ""__main__"":\n    model = DeepLabv3_plus(nInputChannels=3, n_classes=21, os=16, pretrained=True, _print=True)\n    model.eval()\n    image = torch.randn(1, 3, 512, 512)\n    with torch.no_grad():\n        output = model.forward(image)\n    print(output.size())\n'"
model/DenseASPP.py,8,"b'import torch\nimport torch.nn.functional as F\n\nfrom torch import nn\nfrom collections import OrderedDict\nfrom torch.nn import BatchNorm2d as bn\n\n# densenet201\nModel_CFG = {\n    \'bn_size\': 4,\n    \'drop_rate\': 0,\n    \'growth_rate\': 32,\n    \'num_init_features\': 64,\n    \'block_config\': (6, 12, 48, 32),\n\n    \'dropout0\': 0.1,\n    \'dropout1\': 0.1,\n    \'d_feature0\': 480,\n    \'d_feature1\': 240,\n}\n\nclass DenseASPP(nn.Module):\n    """"""\n    * output_scale can only set as 8 or 16\n    """"""\n    def __init__(self,n_class=19, output_stride=8):\n        super(DenseASPP, self).__init__()\n        #bn_size = model_cfg[\'bn_size\']\n        bn_size = 4\n        drop_rate = 0\n        growth_rate = 32\n        num_init_features = 64\n        block_config = (6, 12, 48, 32)\n\n        dropout0 = 0.1\n        dropout1 = 0.1\n        d_feature0 = 480\n        d_feature1 = 240\n\n        feature_size = int(output_stride / 8)\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', bn(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        # block1*****************************************************************************************************\n        block = _DenseBlock(num_layers=block_config[0], num_input_features=num_features,\n                            bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n        self.features.add_module(\'denseblock%d\' % 1, block)\n        num_features = num_features + block_config[0] * growth_rate\n\n        trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n        self.features.add_module(\'transition%d\' % 1, trans)\n        num_features = num_features // 2\n\n        # block2*****************************************************************************************************\n        block = _DenseBlock(num_layers=block_config[1], num_input_features=num_features,\n                            bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n        self.features.add_module(\'denseblock%d\' % 2, block)\n        num_features = num_features + block_config[1] * growth_rate\n\n        trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2, stride=feature_size)\n        self.features.add_module(\'transition%d\' % 2, trans)\n        num_features = num_features // 2\n\n        # block3*****************************************************************************************************\n        block = _DenseBlock(num_layers=block_config[2], num_input_features=num_features, bn_size=bn_size,\n                            growth_rate=growth_rate, drop_rate=drop_rate, dilation_rate=int(2 / feature_size))\n        self.features.add_module(\'denseblock%d\' % 3, block)\n        num_features = num_features + block_config[2] * growth_rate\n\n        trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2, stride=1)\n        self.features.add_module(\'transition%d\' % 3, trans)\n        num_features = num_features // 2\n\n        # block4*****************************************************************************************************\n        block = _DenseBlock(num_layers=block_config[3], num_input_features=num_features, bn_size=bn_size,\n                            growth_rate=growth_rate, drop_rate=drop_rate, dilation_rate=int(4 / feature_size))\n        self.features.add_module(\'denseblock%d\' % 4, block)\n        num_features = num_features + block_config[3] * growth_rate\n\n        trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2, stride=1)\n        self.features.add_module(\'transition%d\' % 4, trans)\n        num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', bn(num_features))\n        if feature_size > 1:\n            self.features.add_module(\'upsample\', nn.Upsample(scale_factor=2, mode=\'bilinear\'))\n\n        self.ASPP_3 = _DenseAsppBlock(input_num=num_features, num1=d_feature0, num2=d_feature1,\n                                      dilation_rate=3, drop_out=dropout0, bn_start=False)\n\n        self.ASPP_6 = _DenseAsppBlock(input_num=num_features + d_feature1 * 1, num1=d_feature0, num2=d_feature1,\n                                      dilation_rate=6, drop_out=dropout0, bn_start=True)\n\n        self.ASPP_12 = _DenseAsppBlock(input_num=num_features + d_feature1 * 2, num1=d_feature0, num2=d_feature1,\n                                       dilation_rate=12, drop_out=dropout0, bn_start=True)\n\n        self.ASPP_18 = _DenseAsppBlock(input_num=num_features + d_feature1 * 3, num1=d_feature0, num2=d_feature1,\n                                       dilation_rate=18, drop_out=dropout0, bn_start=True)\n\n        self.ASPP_24 = _DenseAsppBlock(input_num=num_features + d_feature1 * 4, num1=d_feature0, num2=d_feature1,\n                                       dilation_rate=24, drop_out=dropout0, bn_start=True)\n        num_features = num_features + 5 * d_feature1\n\n        self.classification = nn.Sequential(\n            nn.Dropout2d(p=dropout1),\n            nn.Conv2d(in_channels=num_features, out_channels=n_class, kernel_size=1, padding=0),\n            nn.Upsample(scale_factor=8, mode=\'bilinear\'),\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_uniform(m.weight.data)\n\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, _input):\n        feature = self.features(_input)\n\n        aspp3 = self.ASPP_3(feature)\n        feature = torch.cat((aspp3, feature), dim=1)\n\n        aspp6 = self.ASPP_6(feature)\n        feature = torch.cat((aspp6, feature), dim=1)\n\n        aspp12 = self.ASPP_12(feature)\n        feature = torch.cat((aspp12, feature), dim=1)\n\n        aspp18 = self.ASPP_18(feature)\n        feature = torch.cat((aspp18, feature), dim=1)\n\n        aspp24 = self.ASPP_24(feature)\n        feature = torch.cat((aspp24, feature), dim=1)\n\n        cls = self.classification(feature)\n\n        return cls\n\n\nclass _DenseAsppBlock(nn.Sequential):\n    """""" ConvNet block for building DenseASPP. """"""\n\n    def __init__(self, input_num, num1, num2, dilation_rate, drop_out, bn_start=True):\n        super(_DenseAsppBlock, self).__init__()\n        if bn_start:\n            self.add_module(\'norm1\', bn(input_num, momentum=0.0003)),\n\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)),\n\n        self.add_module(\'norm2\', bn(num1, momentum=0.0003)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n                                            dilation=dilation_rate, padding=dilation_rate)),\n\n        self.drop_rate = drop_out\n\n    def forward(self, _input):\n        feature = super(_DenseAsppBlock, self).forward(_input)\n\n        if self.drop_rate > 0:\n            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)\n\n        return feature\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, dilation_rate=1):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm1\', bn(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm2\', bn(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, dilation=dilation_rate, padding=dilation_rate, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, dilation_rate=1):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate,\n                                bn_size, drop_rate, dilation_rate=dilation_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features, stride=2):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', bn(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))\n        if stride == 2:\n            self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=stride))\n\n\nif __name__ == ""__main__"":\n    model = DenseASPP()\n    print(model)\n'"
model/ENet.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport pdb\n\n__all__ = [""ENet""]\n\n\nclass InitialBlock(nn.Module):\n    \'\'\'\n    The initial block for Enet has 2 branches: The convolution branch and\n    maxpool branch.\n    The conv branch has 13 layers, while the maxpool branch gives 3 layers\n    corresponding to the RBG channels.\n    Both output layers are then concatenated to give an output of 16 layers.\n    INPUTS:\n    - input(Tensor): A 4D tensor of shape [batch_size, channel, height, width]\n    \'\'\'\n\n    def __init__(self):\n        super(InitialBlock, self).__init__()\n        self.conv = nn.Conv2d(3, 13, (3, 3), stride=2, padding=1)\n        self.batch_norm = nn.BatchNorm2d(13, 1e-3)\n        self.prelu = nn.PReLU(13)\n        self.pool = nn.MaxPool2d(2, stride=2)\n\n    def forward(self, input):\n        output = torch.cat([\n            self.prelu(self.batch_norm(self.conv(input))), self.pool(input)\n        ], 1)\n        return output\n\n\nclass BottleNeck(nn.Module):\n    \'\'\'\n    The bottle module has three different kinds of variants:\n    1. A regular convolution which you can decide whether or not to downsample.\n    2. A dilated convolution which requires you to have a dilation factor.\n    3. An asymetric convolution that has a decomposed filter size of 5x1 and\n    1x5 separately.\n    INPUTS:\n    - inputs(Tensor): a 4D Tensor of the previous convolutional block of shape\n    [batch_size, channel, height, widht].\n    - output_channels(int): an integer indicating the output depth of the\n    output convolutional block.\n    - regularlizer_prob(float): the float p that represents the prob of\n    dropping a layer for spatial dropout regularlization.\n    - downsampling(bool): if True, a max-pool2D layer is added to downsample\n    the spatial sizes.\n    - upsampling(bool): if True, the upsampling bottleneck is activated but\n    requires pooling indices to upsample.\n    - dilated(bool): if True, then dilated convolution is done, but requires\n    a dilation rate to be given.\n    - dilation_rate(int): the dilation factor for performing atrous\n    convolution/dilated convolution\n    - asymmetric(bool): if True, then asymmetric convolution is done, and\n    the only filter size used here is 5.\n    - use_relu(bool): if True, then all the prelus become relus according to\n    Enet author.\n    \'\'\'\n\n    def __init__(self,\n                 input_channels=None,\n                 output_channels=None,\n                 regularlizer_prob=0.1,\n                 downsampling=False,\n                 upsampling=False,\n                 dilated=False,\n                 dilation_rate=None,\n                 asymmetric=False,\n                 use_relu=False):\n        super(BottleNeck, self).__init__()\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.downsampling = downsampling\n        self.upsampling = upsampling\n        self.use_relu = use_relu\n\n        internal = output_channels // 4\n        input_stride = 2 if downsampling else 1\n        # First projection with 1x1 kernel (2x2 for downsampling)\n        conv1x1_1 = nn.Conv2d(input_channels, internal,\n                              input_stride, input_stride, bias=False)\n        batch_norm1 = nn.BatchNorm2d(internal, 1e-3)\n        prelu1 = self._prelu(internal, use_relu)\n        self.block1x1_1 = nn.Sequential(conv1x1_1, batch_norm1, prelu1)\n\n        conv = None\n        if downsampling:\n            self.pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n            conv = nn.Conv2d(internal, internal, 3, stride=1, padding=1)\n        elif upsampling:\n            # padding is replaced with spatial convolution without bias.\n            spatial_conv = nn.Conv2d(input_channels, output_channels, 1,\n                                     bias=False)\n            batch_norm = nn.BatchNorm2d(output_channels, 1e-3)\n            self.conv_before_unpool = nn.Sequential(spatial_conv, batch_norm)\n            self.unpool = nn.MaxUnpool2d(2)\n            conv = nn.ConvTranspose2d(internal, internal, 3,\n                                      stride=2, padding=1, output_padding=1)\n        elif dilated:\n            conv = nn.Conv2d(internal, internal, 3, padding=dilation_rate,\n                             dilation=dilation_rate)\n        elif asymmetric:\n            conv1 = nn.Conv2d(internal, internal, [5, 1], padding=(2, 0),\n                              bias=False)\n            conv2 = nn.Conv2d(internal, internal, [1, 5], padding=(0, 2))\n            conv = nn.Sequential(conv1, conv2)\n        else:\n            conv = nn.Conv2d(internal, internal, 3, padding=1)\n\n        batch_norm = nn.BatchNorm2d(internal, 1e-3)\n        prelu = self._prelu(internal, use_relu)\n        self.middle_block = nn.Sequential(conv, batch_norm, prelu)\n\n        # Final projection with 1x1 kernel\n        conv1x1_2 = nn.Conv2d(internal, output_channels, 1, bias=False)\n        batch_norm2 = nn.BatchNorm2d(output_channels, 1e-3)\n        prelu2 = self._prelu(output_channels, use_relu)\n        self.block1x1_2 = nn.Sequential(conv1x1_2, batch_norm2, prelu2)\n\n        # regularlize\n        self.dropout = nn.Dropout2d(regularlizer_prob)\n\n    def _prelu(self, channels, use_relu):\n        return (nn.PReLU(channels) if use_relu is False else nn.ReLU())\n\n    def forward(self, input, pooling_indices=None):\n        main = None\n        input_shape = input.size()\n        if self.downsampling:\n            main, indices = self.pool(input)\n            if (self.output_channels != self.input_channels):\n                pad = Variable(torch.Tensor(input_shape[0],\n                               self.output_channels - self.input_channels,\n                               input_shape[2] // 2,\n                               input_shape[3] // 2).zero_(), requires_grad=False)\n                if (torch.cuda.is_available):\n                    pad = pad.cuda(0)\n                main = torch.cat((main, pad), 1)\n        elif self.upsampling:\n            main = self.unpool(self.conv_before_unpool(input), pooling_indices)\n        else:\n            main = input\n\n        other_net = nn.Sequential(self.block1x1_1, self.middle_block,\n                                  self.block1x1_2)\n        other = other_net(input)\n        output = F.relu(main + other)\n        if (self.downsampling):\n            return output, indices\n        return output\n\nENCODER_LAYER_NAMES = [\'initial\', \'bottleneck_1_0\', \'bottleneck_1_1\',\n                       \'bottleneck_1_2\', \'bottleneck_1_3\', \'bottleneck_1_4\',\n                       \'bottleneck_2_0\', \'bottleneck_2_1\', \'bottleneck_2_2\',\n                       \'bottleneck_2_3\', \'bottleneck_2_4\', \'bottleneck_2_5\',\n                       \'bottleneck_2_6\', \'bottleneck_2_7\', \'bottleneck_2_8\',\n                       \'bottleneck_3_1\', \'bottleneck_3_2\', \'bottleneck_3_3\',\n                       \'bottleneck_3_4\', \'bottleneck_3_5\', \'bottleneck_3_6\',\n                       \'bottleneck_3_7\', \'bottleneck_3_8\', \'classifier\']\nDECODER_LAYER_NAMES = [\'bottleneck_4_0\', \'bottleneck_4_1\', \'bottleneck_4_2\'\n                       \'bottleneck_5_0\', \'bottleneck_5_1\', \'fullconv\']\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_classes, only_encode=True):\n        super(Encoder, self).__init__()\n        self.state = only_encode\n        layers = []\n        layers.append(InitialBlock())\n        layers.append(BottleNeck(16, 64, regularlizer_prob=0.01,\n                                 downsampling=True))\n        for i in range(4):\n            layers.append(BottleNeck(64, 64, regularlizer_prob=0.01))\n        \n        # Section 2 and 3\n        layers.append(BottleNeck(64, 128, downsampling=True))\n        for i in range(2):\n            layers.append(BottleNeck(128, 128))\n            layers.append(BottleNeck(128, 128, dilated=True, dilation_rate=2))\n            layers.append(BottleNeck(128, 128, asymmetric=True))\n            layers.append(BottleNeck(128, 128, dilated=True, dilation_rate=4))\n            layers.append(BottleNeck(128, 128))\n            layers.append(BottleNeck(128, 128, dilated=True, dilation_rate=8))\n            layers.append(BottleNeck(128, 128, asymmetric=True))\n            layers.append(BottleNeck(128, 128, dilated=True, dilation_rate=16))\n        # only training encoder\n        if only_encode:\n            layers.append(nn.Conv2d(128, num_classes, 1))\n\n        for layer, layer_name in zip(layers, ENCODER_LAYER_NAMES):\n            super(Encoder, self).__setattr__(layer_name, layer)\n        self.layers = layers\n\n    \n    def forward(self, input):\n        pooling_stack = []\n        output = input\n        for layer in self.layers:\n            if hasattr(layer, \'downsampling\') and layer.downsampling:\n                output, pooling_indices = layer(output)\n                pooling_stack.append(pooling_indices)\n            else:\n                output = layer(output)\n\n        if self.state:\n            output = F.upsample(output, cfg.TRAIN.IMG_SIZE, None, \'bilinear\')\n\n        return output, pooling_stack\n\n\nclass Decoder(nn.Module):\n    def __init__(self, num_classes):\n        super(Decoder, self).__init__()\n        layers = []\n        # Section 4\n        layers.append(BottleNeck(128, 64, upsampling=True, use_relu=True))\n        layers.append(BottleNeck(64, 64, use_relu=True))\n        layers.append(BottleNeck(64, 64, use_relu=True))\n\n        # Section 5\n        layers.append(BottleNeck(64, 16, upsampling=True, use_relu=True))\n        layers.append(BottleNeck(16, 16, use_relu=True))\n        layers.append(nn.ConvTranspose2d(16, num_classes, 2, stride=2))\n\n        self.layers = nn.ModuleList([layer for layer in layers])\n    \n    def forward(self, input, pooling_stack):\n        output = input\n        for layer in self.layers:\n            if hasattr(layer, \'upsampling\') and layer.upsampling:\n                pooling_indices = pooling_stack.pop()\n                output = layer(output, pooling_indices)\n            else:\n                output = layer(output)\n        return output\n\n\nclass ENet(nn.Module):\n    def __init__(self,n_classes=19, only_encode=False):\n        super(ENet, self).__init__()\n        self.state = only_encode\n        self.encoder = Encoder(n_classes,only_encode=only_encode)\n        self.decoder = Decoder(n_classes)\n\n    def forward(self, input):\n        output, pooling_stack = self.encoder(input)\n        if not self.state:\n            output = self.decoder(output, pooling_stack)\n        return output\n'"
model/ESPNet.py,13,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n__all__ = [""ESPNet_Encoder"", ""ESPNet""]\n\nclass CBR(nn.Module):\n    \'\'\'\n    This class defines the convolution layer with batch normalization and PReLU activation\n    \'\'\'\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BR(nn.Module):\n    \'\'\'\n        This class groups the batch normalization and PReLU activation\n    \'\'\'\n    def __init__(self, nOut):\n        \'\'\'\n        :param nOut: output feature maps\n        \'\'\'\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: normalized and thresholded feature map\n        \'\'\'\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\nclass CB(nn.Module):\n    \'\'\'\n       This class groups the convolution and batch normalization\n    \'\'\'\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optinal stide for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n\n    def forward(self, input):\n        \'\'\'\n\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\nclass C(nn.Module):\n    \'\'\'\n    This class is for a convolutional layer.\n    \'\'\'\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\nclass CDilated(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution, which can maintain feature map size\n    \'\'\'\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1)/2) * d\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False, dilation=d)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\nclass DownSamplerB(nn.Module):\n    def __init__(self, nIn, nOut):\n        super().__init__()\n        n = int(nOut/5)\n        n1 = nOut - 4*n\n        self.c1 = C(nIn, n, 3, 2)\n        self.d1 = CDilated(n, n1, 3, 1, 1)\n        self.d2 = CDilated(n, n, 3, 1, 2)\n        self.d4 = CDilated(n, n, 3, 1, 4)\n        self.d8 = CDilated(n, n, 3, 1, 8)\n        self.d16 = CDilated(n, n, 3, 1, 16)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-3)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        output1 = self.c1(input)\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n         \n        # Using hierarchical feature fusion (HFF) to ease the gridding artifacts which is introduced \n        # by the large effective receptive filed of the ESP module \n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        combine = torch.cat([d1, add1, add2, add3, add4],1)\n        #combine_in_out = input + combine  #shotcut path\n        output = self.bn(combine)\n        output = self.act(output)\n        return output\n#ESP block\nclass DilatedParllelResidualBlockB(nn.Module):\n    \'\'\'\n    This class defines the ESP block, which is based on the following principle\n        Reduce ---> Split ---> Transform --> Merge\n    \'\'\'\n    def __init__(self, nIn, nOut, add=True):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param add: if true, add a residual connection through identity operation. You can use projection too as\n                in ResNet paper, but we avoid to use it if the dimensions are not the same because we do not want to\n                increase the module complexity\n        \'\'\'\n        super().__init__()\n        n = int(nOut/5)  #K=5, \n        n1 = nOut - 4*n  #(N-(K-1)INT(N/K)) for dilation rate of 2^0, for producing an output feature map of channel=nOut\n        self.c1 = C(nIn, n, 1, 1)  #the point-wise convolutions with 1x1 help in reducing the computation, channel=c\n\n        #K=5, dilation rate: 2^{k-1},k={1,2,3,...,K}\n        self.d1 = CDilated(n, n1, 3, 1, 1) # dilation rate of 2^0\n        self.d2 = CDilated(n, n, 3, 1, 2) # dilation rate of 2^1\n        self.d4 = CDilated(n, n, 3, 1, 4) # dilation rate of 2^2\n        self.d8 = CDilated(n, n, 3, 1, 8) # dilation rate of 2^3\n        self.d16 = CDilated(n, n, 3, 1, 16) # dilation rate of 2^4\n        self.bn = BR(nOut)\n        self.add = add\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        # reduce\n        output1 = self.c1(input)\n        # split and transform\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n\n        \n        # Using hierarchical feature fusion (HFF) to ease the gridding artifacts which is introduced \n        # by the large effective receptive filed of the ESP module \n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        #merge\n        combine = torch.cat([d1, add1, add2, add3, add4], 1)\n\n        # if residual version\n        if self.add:\n            combine = input + combine\n        output = self.bn(combine)\n        return output\n\nclass InputProjectionA(nn.Module):\n    \'\'\'\n    This class projects the input image to the same spatial dimensions as the feature map.\n    For example, if the input image is 512 x512 x3 and spatial dimensions of feature map size are 56x56xF, then\n    this class will generate an output of 56x56x3, for input reinforcement, which establishes a direct link between \n    the input image and encoding stage, improving the flow of information.    \n    \'\'\'\n    def __init__(self, samplingTimes):\n        \'\'\'\n        :param samplingTimes: The rate at which you want to down-sample the image\n        \'\'\'\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, samplingTimes):\n            #pyramid-based approach for down-sampling\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, input):\n        \'\'\'\n        :param input: Input RGB Image\n        :return: down-sampled image (pyramid-based approach)\n        \'\'\'\n        for pool in self.pool:\n            input = pool(input)\n        return input\n\n\nclass ESPNet_Encoder(nn.Module):\n    \'\'\'\n    This class defines the ESPNet-C network in the paper\n    \'\'\'\n    def __init__(self, classes=20, p=5, q=3):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        \'\'\'\n        super().__init__()\n        self.level1 = CBR(3, 16, 3, 2)      # feature map size divided 2,                         1/2\n        self.sample1 = InputProjectionA(1)  #down-sample for input reinforcement, factor=2\n        self.sample2 = InputProjectionA(2)  #down-sample for input reinforcement, factor=4\n\n        self.b1 = BR(16 + 3)\n        self.level2_0 = DownSamplerB(16 +3, 64)  # Downsample Block, feature map size divided 2,    1/4\n\n        self.level2 = nn.ModuleList()\n        for i in range(0, p):\n            self.level2.append(DilatedParllelResidualBlockB(64 , 64))  #ESP block\n        self.b2 = BR(128 + 3)\n\n        self.level3_0 = DownSamplerB(128 + 3, 128) #Downsample Block, feature map size divided 2,   1/8\n        self.level3 = nn.ModuleList()\n        for i in range(0, q):\n            self.level3.append(DilatedParllelResidualBlockB(128 , 128)) # ESPblock\n        self.b3 = BR(256)\n\n        self.classifier = C(256, classes, 1, 1)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: Receives the input RGB image\n        :return: the transformed feature map with spatial dimensions 1/8th of the input image\n        \'\'\'\n        output0 = self.level1(input)\n        inp1 = self.sample1(input)\n        inp2 = self.sample2(input)\n\n        output0_cat = self.b1(torch.cat([output0, inp1], 1))\n        output1_0 = self.level2_0(output0_cat) # down-sampled\n        \n        for i, layer in enumerate(self.level2):\n            if i==0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.b2(torch.cat([output1,  output1_0, inp2], 1))\n\n        output2_0 = self.level3_0(output1_cat) # down-sampled\n        for i, layer in enumerate(self.level3):\n            if i==0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.b3(torch.cat([output2_0, output2], 1))\n\n        classifier = self.classifier(output2_cat)\n\n        #return classifier\n        out = F.upsample(classifier, input.size()[2:], mode=\'bilinear\')   #Upsample score map, factor=8\n        return out\n        \nclass ESPNet(nn.Module):\n    \'\'\'\n    This class defines the ESPNet network\n    \'\'\'\n\n    def __init__(self, classes=19, p=2, q=3, encoderFile=None):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        :param encoderFile: pretrained encoder weights. Recall that we first trained the ESPNet-C and then attached the\n                            RUM-based light weight decoder. See paper for more details.\n        \'\'\'\n        super().__init__()\n        self.encoder = ESPNet_Encoder(classes, p, q)\n        if encoderFile != None:\n            self.encoder.load_state_dict(torch.load(encoderFile))\n            print(\'Encoder loaded!\')\n        # load the encoder modules\n        self.modules = []\n        for i, m in enumerate(self.encoder.children()):\n            self.modules.append(m)\n\n        # light-weight decoder\n        self.level3_C = C(128 + 3, classes, 1, 1)\n        self.br = nn.BatchNorm2d(classes, eps=1e-03)\n        self.conv = CBR(19 + classes, classes, 3, 1)\n\n        self.up_l3 = nn.Sequential(nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False))\n        self.combine_l2_l3 = nn.Sequential(BR(2*classes), DilatedParllelResidualBlockB(2*classes , classes, add=False))\n\n        self.up_l2 = nn.Sequential(nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False), BR(classes))\n\n        self.classifier = nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: RGB image\n        :return: transformed feature map\n        \'\'\'\n        output0 = self.modules[0](input)\n        inp1 = self.modules[1](input)\n        inp2 = self.modules[2](input)\n\n        output0_cat = self.modules[3](torch.cat([output0, inp1], 1))\n        output1_0 = self.modules[4](output0_cat)  # down-sampled\n\n        for i, layer in enumerate(self.modules[5]):\n            if i == 0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.modules[6](torch.cat([output1, output1_0, inp2], 1))\n\n        output2_0 = self.modules[7](output1_cat)  # down-sampled\n        for i, layer in enumerate(self.modules[8]):\n            if i == 0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.modules[9](torch.cat([output2_0, output2], 1)) # concatenate for feature map width expansion\n\n        output2_c = self.up_l3(self.br(self.modules[10](output2_cat))) #RUM\n\n        output1_C = self.level3_C(output1_cat) # project to C-dimensional space\n        comb_l2_l3 = self.up_l2(self.combine_l2_l3(torch.cat([output1_C, output2_c], 1))) #RUM\n\n        concat_features = self.conv(torch.cat([comb_l2_l3, output0_cat], 1))\n\n        classifier = self.classifier(concat_features)\n        return classifier\n'"
model/PSPNet.py,4,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport numpy as np\naffine_par = True  #True: BN has learnable affine parameters, False: without learnable affine parameters of BatchNorm Layer\n\n\ndef outS(i):\n    i = int(i)\n    i = (i+1)/2\n    i = int(np.ceil((i+1)/2.0))\n    i = (i+1)/2\n    return i\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, affine = affine_par)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, affine = affine_par)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n        self.bn1 = nn.BatchNorm2d(planes,affine = affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n\n        padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n                               padding=padding, bias=False, dilation = dilation)\n        self.bn2 = nn.BatchNorm2d(planes,affine = affine_par)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4, affine = affine_par)\n        for i in self.bn3.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass Classifier_Module(nn.Module):\n\n    def __init__(self, dilation_series, padding_series, num_classes):\n        super(Classifier_Module, self).__init__()\n        self.conv2d_list = nn.ModuleList()\n        for dilation, padding in zip(dilation_series, padding_series):\n            self.conv2d_list.append(nn.Conv2d(2048, num_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias = True))\n\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list)-1):\n            out += self.conv2d_list[i+1](x)\n        return out\n\nclass Residual_Covolution(nn.Module):\n    def __init__(self, icol, ocol, num_classes):\n        super(Residual_Covolution, self).__init__()\n        self.conv1 = nn.Conv2d(icol, ocol, kernel_size=3, stride=1, padding=12, dilation=12, bias=True)\n        self.conv2 = nn.Conv2d(ocol, num_classes, kernel_size=3, stride=1, padding=12, dilation=12, bias=True)\n        self.conv3 = nn.Conv2d(num_classes, ocol, kernel_size=1, stride=1, padding=0, dilation=1, bias=True)\n        self.conv4 = nn.Conv2d(ocol, icol, kernel_size=1, stride=1, padding=0, dilation=1, bias=True)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        dow1 = self.conv1(x)\n        dow1 = self.relu(dow1)\n        seg = self.conv2(dow1)\n        inc1 = self.conv3(seg)\n        add1 = dow1 + self.relu(inc1)\n        inc2 = self.conv4(add1)\n        out = x + self.relu(inc2)\n        return out, seg\n\nclass PSPModule(nn.Module):\n    """"""Ref: Pyramid Scene Parsing Network,CVPR2017, http://arxiv.org/abs/1612.01105 """"""\n\n    def __init__(self, inChannel, midReduction=4, outChannel=512, sizes=(1, 2, 3, 6)):\n        super(PSPModule, self).__init__()\n        self.midChannel= int(inChannel/midReduction)  #1x1Conv channel num, defalut=512\n        self.stages = []\n        self.stages = nn.ModuleList([self._make_stage(inChannel, self.midChannel, size) for size in sizes])  #pooling->conv1x1\n        self.bottleneck = nn.Conv2d( (inChannel+ self.midChannel*4), outChannel, kernel_size=3)  #channel: 4096->512 1x1\n        self.bn = nn.BatchNorm2d(outChannel)\n        self.prelu = nn.PReLU()\n\n    def _make_stage(self, inChannel, midChannel,  size):\n        pooling = nn.AdaptiveAvgPool2d(output_size=(size, size))\n        Conv = nn.Conv2d(inChannel, midChannel, kernel_size=1, bias=False)\n        return nn.Sequential(pooling, Conv)\n\n    def forward(self, feats):\n        h, w = feats.size(2), feats.size(3)\n        mulBranches = [F.upsample(input=stage(feats), size=(h, w), mode=\'bilinear\') for stage in self.stages] + [feats]  #four parallel baranches\n        out = self.bottleneck( torch.cat( (mulBranches[0], mulBranches[1], mulBranches[2], mulBranches[3],feats) ,1))\n        out = self.bn(out)\n        out = self.prelu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes, midReduction=4):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, affine = affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        \n\n        # PSPModule\n        self.pspmodule = PSPModule(inChannel= 512*block.expansion, midReduction=midReduction, outChannel=512, sizes=(1,2,3,6))\n        self.spatial_drop = nn.Dropout2d(p=0.1)\n\n        \n        # auxiliary classifier followd res4, I think the auxiliary is not neccesary!!!\n        # main classifier\n        self.main_classifier = nn.Conv2d(512, num_classes, kernel_size=1)\n        #self.softmax = nn.Softmax()\n\n\n        # init weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        #        for i in m.parameters():\n        #            i.requires_grad = False\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion or dilation == 2 or dilation == 4:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion,affine = affine_par))\n        for i in downsample._modules[\'1\'].parameters():\n            i.requires_grad = False\n        layers = []\n        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n\n\n    def forward(self, x):\n        input_size = x.size()[2:]\n        x = self.conv1(x)  #7x7Conv\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)  #res2\n        x = self.layer2(x)  #res3\n        x = self.layer3(x)  #res4\n        x = self.layer4(x)  #res5\n        \n        # PSPModule\n        #print(""before PSPModule, tensor size:"", x.size())\n        x = self.pspmodule(x)\n\n        #print(""after PSPModule, tensor size:"", x.size())\n        x = self.spatial_drop(x)\n\n        # classifier \n        x = self.main_classifier(x)\n\n        #print(""before upsample, tensor size:"", x.size())\n        x = F.upsample(x, input_size, mode=\'bilinear\')  #upsample to the size of input image, scale=8\n        #print(""after upsample, tensor size:"", x.size())\n        #x = self.softmax(x)\n        return x  #shape: NxCxHxW, (H,W) is equal to that of input image\n\n\ndef PSPNet(num_classes=19):\n    """""" """"""\n    model = ResNet(Bottleneck,[3, 4, 23, 3], num_classes)\n    return model\n\nif __name__==""__main__"":\n    network = PSPNet()\n    print(network)\n\n'"
model/SegNet.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nclass SegNet(nn.Module):\n    def __init__(self,input_nbr=3 ,label_nbr= 19):\n        super(SegNet, self).__init__()\n\n        batchNorm_momentum = 0.1\n\n        self.conv11 = nn.Conv2d(input_nbr, 64, kernel_size=3, padding=1)\n        self.bn11 = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn12 = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n\n        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn21 = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n        self.conv22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn22 = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n\n        self.conv31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn31 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n        self.conv32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn32 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n        self.conv33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn33 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n\n        self.conv41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.bn41 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn42 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn43 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n\n        self.conv51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn51 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn52 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn53 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n\n        self.conv53d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn53d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv52d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn52d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv51d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn51d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n\n        self.conv43d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn43d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv42d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn42d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv41d = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n        self.bn41d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n\n        self.conv33d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn33d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n        self.conv32d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn32d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n        self.conv31d = nn.Conv2d(256,  128, kernel_size=3, padding=1)\n        self.bn31d = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n\n        self.conv22d = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn22d = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n        self.conv21d = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.bn21d = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n\n        self.conv12d = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn12d = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n        self.conv11d = nn.Conv2d(64, label_nbr, kernel_size=3, padding=1)\n\n\n    def forward(self, x):\n\n        # Stage 1\n        x11 = F.relu(self.bn11(self.conv11(x)))\n        x12 = F.relu(self.bn12(self.conv12(x11)))\n        x1p, id1 = F.max_pool2d(x12,kernel_size=2, stride=2,return_indices=True)\n\n        # Stage 2\n        x21 = F.relu(self.bn21(self.conv21(x1p)))\n        x22 = F.relu(self.bn22(self.conv22(x21)))\n        x2p, id2 = F.max_pool2d(x22,kernel_size=2, stride=2,return_indices=True)\n\n        # Stage 3\n        x31 = F.relu(self.bn31(self.conv31(x2p)))\n        x32 = F.relu(self.bn32(self.conv32(x31)))\n        x33 = F.relu(self.bn33(self.conv33(x32)))\n        x3p, id3 = F.max_pool2d(x33,kernel_size=2, stride=2,return_indices=True)\n\n        # Stage 4\n        x41 = F.relu(self.bn41(self.conv41(x3p)))\n        x42 = F.relu(self.bn42(self.conv42(x41)))\n        x43 = F.relu(self.bn43(self.conv43(x42)))\n        x4p, id4 = F.max_pool2d(x43,kernel_size=2, stride=2,return_indices=True)\n\n        # Stage 5\n        x51 = F.relu(self.bn51(self.conv51(x4p)))\n        x52 = F.relu(self.bn52(self.conv52(x51)))\n        x53 = F.relu(self.bn53(self.conv53(x52)))\n        x5p, id5 = F.max_pool2d(x53,kernel_size=2, stride=2,return_indices=True)\n\n\n        # Stage 5d\n        x5d = F.max_unpool2d(x5p, id5, kernel_size=2, stride=2)\n        x53d = F.relu(self.bn53d(self.conv53d(x5d)))\n        x52d = F.relu(self.bn52d(self.conv52d(x53d)))\n        x51d = F.relu(self.bn51d(self.conv51d(x52d)))\n\n        # Stage 4d\n        x4d = F.max_unpool2d(x51d, id4, kernel_size=2, stride=2)\n        x43d = F.relu(self.bn43d(self.conv43d(x4d)))\n        x42d = F.relu(self.bn42d(self.conv42d(x43d)))\n        x41d = F.relu(self.bn41d(self.conv41d(x42d)))\n\n        # Stage 3d\n        x3d = F.max_unpool2d(x41d, id3, kernel_size=2, stride=2)\n        x33d = F.relu(self.bn33d(self.conv33d(x3d)))\n        x32d = F.relu(self.bn32d(self.conv32d(x33d)))\n        x31d = F.relu(self.bn31d(self.conv31d(x32d)))\n\n        # Stage 2d\n        x2d = F.max_unpool2d(x31d, id2, kernel_size=2, stride=2)\n        x22d = F.relu(self.bn22d(self.conv22d(x2d)))\n        x21d = F.relu(self.bn21d(self.conv21d(x22d)))\n\n        # Stage 1d\n        x1d = F.max_unpool2d(x21d, id1, kernel_size=2, stride=2)\n        x12d = F.relu(self.bn12d(self.conv12d(x1d)))\n        x11d = self.conv11d(x12d)\n\n        return x11d\n\n    def load_from_segnet(self, model_path):\n        s_dict = self.state_dict()# create a copy of the state dict\n        th = torch.load(model_path).state_dict() # load the weigths\n        # for name in th:\n            # s_dict[corresp_name[name]] = th[name]\n        self.load_state_dict(th)\n'"
model/__init__.py,0,b'from .CGNet import *\nfrom .BiSeNet_resnet import *\nfrom .BiSeNet_xception import *\nfrom .DeepLabV3plus_resnet import *\nfrom .DeepLabV3plus_xception import *\nfrom .DenseASPP import *\nfrom .DFN import *\nfrom .ENet import *\nfrom .ESPNet import *\nfrom .PSPNet import *\nfrom .SegNet import *\nfrom .resnet import *\nfrom .xception import *\n'
model/resnet.py,7,"b'""""""Dilated ResNet""""""\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n# Author: Xiangtai(lxtpku@pku.edu.cn)\n# from torchvision\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn as nn\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\', \'BasicBlock\', \'Bottleneck\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    """"""ResNet BasicBlock\n    """"""\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, previous_dilation=1,\n                 norm_layer=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                               padding=dilation, dilation=dilation, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                               padding=previous_dilation, dilation=previous_dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    """"""ResNet Bottleneck\n    """"""\n    # pylint: disable=unused-argument\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, dilation=1,\n                 downsample=None, previous_dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride,\n            padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(\n            planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def _sum_each(self, x, y):\n        assert(len(x) == len(y))\n        z = []\n        for i in range(len(x)):\n            z.append(x[i]+y[i])\n        return z\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    """"""Dilated Pre-trained ResNet Model, which preduces the stride of 8 featuremaps at conv5.\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    classes : int, default 1000\n        Number of classification classes.\n    dilated : bool, default False\n        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n        typically used in Semantic Segmentation.\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n    Reference:\n        - He, Kaiming, et al. ""Deep residual learning for image recognition."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n        - Yu, Fisher, and Vladlen Koltun. ""Multi-scale context aggregation by dilated convolutions.""\n    """"""\n    # pylint: disable=unused-variable\n    def __init__(self, block, layers, num_classes=1000, dilated=True, norm_layer=nn.BatchNorm2d):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                           dilation=2, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=4, norm_layer=norm_layer)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                           norm_layer=norm_layer)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, norm_layer):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(block(self.inplanes, planes, stride, dilation=1,\n                                downsample=downsample, previous_dilation=dilation, norm_layer=norm_layer))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, dilation=2,\n                                downsample=downsample, previous_dilation=dilation, norm_layer=norm_layer))\n        else:\n            raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation, previous_dilation=dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n'"
model/xception.py,4,"b'"""""" \nCreates an Xception Model as defined in:\nFrancois Chollet\nXception: Deep Learning with Depthwise Separable Convolutions\nhttps://arxiv.org/pdf/1610.02357.pdf\nThis weights ported from the Keras implementation. Achieves the following performance on the validation set:\nLoss:0.9173 Prec@1:78.892 Prec@5:94.292\nREMEMBER to set your image size to 3x299x299 for both test and validation\nnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                  std=[0.5, 0.5, 0.5])\nThe resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n""""""\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom torch.nn import init\nimport torch\n\n__all__ = [\'xception\']\n\nmodel_urls = {\n    \'xception\':\'https://www.dropbox.com/s/1hplpzet9d7dv29/xception-c0a72b38.pth.tar?dl=1\'\n}\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n    \n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n        super(Block, self).__init__()\n\n        if out_filters != in_filters or strides!=1:\n            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip=None\n        \n        self.relu = nn.ReLU(inplace=True)\n        rep=[]\n\n        filters=in_filters\n        if grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n            filters = out_filters\n\n        for i in range(reps-1):\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(filters))\n        \n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n\n        if not start_with_relu:\n            rep = rep[1:]\n        else:\n            rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3,strides,1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self,inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x+=skip\n        return x\n\n\n\nclass Xception(nn.Module):\n    """"""\n    Xception optimized for the ImageNet dataset, as specified in\n    https://arxiv.org/pdf/1610.02357.pdf\n    """"""\n    def __init__(self, num_classes=19):\n        """""" Constructor\n        Args:\n            num_classes: number of classes\n        """"""\n        super(Xception, self).__init__()\n\n        \n        self.num_classes = num_classes\n\n        self.layer1 = nn.Sequential( nn.Conv2d(3, 32, 3,2, 0, bias=False),\n                                     nn.BatchNorm2d(32),\n                                     nn.ReLU(inplace=True),\n                                     nn.Conv2d(32,64,3,bias=False),\n                                     nn.BatchNorm2d(64),\n                                     nn.ReLU(inplace=True))  # H/2 x W/2\n        #do relu here\n\n        self.layer2 = Block(64,128,2,2,start_with_relu=False,grow_first=True) # H/4 x W/4     # block1\n        self.layer3 = Block(128,256,2,2,start_with_relu=True,grow_first=True) # H/8 x W/8     # block2\n\n        self.layer4 = nn.Sequential( Block(256,728,2,2,start_with_relu=True,grow_first=True), # block3\n                                     Block(728,728,3,1,start_with_relu=True,grow_first=True),\n                                     Block(728,728,3,1,start_with_relu=True,grow_first=True),\n                                     Block(728,728,3,1,start_with_relu=True,grow_first=True),\n                                     Block(728,728,3,1,start_with_relu=True,grow_first=True),\n                                     Block(728,728,3,1,start_with_relu=True,grow_first=True),\n                                     Block(728,728,3,1,start_with_relu=True,grow_first=True),\n                                     Block(728,728,3,1,start_with_relu=True,grow_first=True),  # block10\n                                     Block(728,728,3,1,start_with_relu=True,grow_first=True))  # block11  H/16 x W/16\n\n        self.layer5 = nn.Sequential( Block(728,1024,2,2,start_with_relu=True,grow_first=False),\n                                     SeparableConv2d(1024,1536,3,1,1),\n                                     nn.BatchNorm2d(1536),\n                                     nn.ReLU(inplace=True),\n                                     SeparableConv2d(1536,2048,3,1,1),\n                                     nn.BatchNorm2d(2048),\n                                     nn.ReLU(inplace=True)) #block12 H/32 x W/32\n\n        self.fc = nn.Linear(2048, num_classes)\n\n\n\n        #------- init weights --------\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        #-----------------------------\n\n\n\n\n\n    def forward(self, x):\n        x = self.layer1(x)  # H/2 x W/2\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.block4(x)  # H/16 x W/16\n        x = self.block5(x)  # H/32 x W/32\n        \n\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n\ndef xception(pretrained=False,**kwargs):\n    """"""\n    Construct Xception.\n    """"""\n\n    model = Xception(**kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'xception\']))\n    return model\n\n\ndef netParams(model):\n    \'\'\'\n    Computing total network parameters\n    Args:\n       model: model\n    return: total network parameters\n    \'\'\'\n    total_paramters = 0\n    for parameter in model.parameters():\n        i = len(parameter.size())\n        p = 1\n        for j in range(i):\n            p *= parameter.size(j)\n        total_paramters += p\n\n    return total_paramters\n\n\nif __name__ == \'__main__\':\n    model = xception()\n    print(""model params:"", netParams(model))\n\n'"
utils/colorize_mask.py,1,"b""from PIL import Image\nimport torch\nimport numpy as np\n\ncityscapes_palette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\n\nzero_pad = 256 * 3 - len(cityscapes_palette)\nfor i in range(zero_pad):\n    cityscapes_palette.append(0)\n\ndef cityscapes_colorize_mask(mask):\n    # mask: numpy array of the mask\n    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n    new_mask.putpalette(cityscapes_palette)\n\n    return new_mask\n\n\nclass VOCColorize(object):\n    def __init__(self, n=22):\n        self.cmap = voc_color_map(22)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.shape\n        color_image = np.zeros((3, size[0], size[1]), dtype=np.uint8)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image)\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        # handle void\n        mask = (255 == gray_image)\n        color_image[0][mask] = color_image[1][mask] = color_image[2][mask] = 255\n\n        return color_image\n\ndef voc_color_map(N=256, normalized=False):\n    def bitget(byteval, idx):\n        return ((byteval & (1 << idx)) != 0)\n\n    dtype = 'float32' if normalized else 'uint8'\n    cmap = np.zeros((N, 3), dtype=dtype)\n    for i in range(N):\n        r = g = b = 0\n        c = i\n        for j in range(8):\n            r = r | (bitget(c, 0) << 7-j)\n            g = g | (bitget(c, 1) << 7-j)\n            b = b | (bitget(c, 2) << 7-j)\n            c = c >> 3\n\n        cmap[i] = np.array([r, g, b])\n\n    cmap = cmap/255 if normalized else cmap\n    return cmap\n"""
utils/compute_iou.py,0,"b'import numpy as np\nimport argparse\nimport json\nfrom PIL import Image\nfrom os.path import join\n\n\ndef fast_hist(a, b, n):\n    k = (a >= 0) & (a < n)\n    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n\n\ndef per_class_iu(hist):\n    return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n\n\ndef label_mapping(input, mapping):\n    output = np.copy(input)\n    for ind in range(len(mapping)):\n        output[input == mapping[ind][0]] = mapping[ind][1]\n    return np.array(output, dtype=np.int64)\n\n\ndef compute_mIoU(gt_dir, pred_dir, devkit_dir=\'\'):\n    """"""\n    Compute IoU given the predicted colorized images and \n    """"""\n    with open(join(devkit_dir, \'info.json\'), \'r\') as fp:\n      info = json.load(fp)\n    num_classes = np.int(info[\'classes\'])\n    print(\'Num classes\', num_classes)\n    name_classes = np.array(info[\'label\'], dtype=np.str)\n    mapping = np.array(info[\'label2train\'], dtype=np.int)\n    hist = np.zeros((num_classes, num_classes))\n\n    image_path_list = join(devkit_dir, \'val.txt\')\n    label_path_list = join(devkit_dir, \'label.txt\')\n    gt_imgs = open(label_path_list, \'r\').read().splitlines()\n    gt_imgs = [join(gt_dir, x) for x in gt_imgs]\n    pred_imgs = open(image_path_list, \'r\').read().splitlines()\n    pred_imgs = [join(pred_dir, x.split(\'/\')[-1]) for x in pred_imgs]\n\n    for ind in range(len(gt_imgs)):\n        pred = np.array(Image.open(pred_imgs[ind]))\n        label = np.array(Image.open(gt_imgs[ind]))\n        label = label_mapping(label, mapping)\n        if len(label.flatten()) != len(pred.flatten()):\n            print(\'Skipping: len(gt) = {:d}, len(pred) = {:d}, {:s}, {:s}\'.format(len(label.flatten()), len(pred.flatten()), gt_imgs[ind], pred_imgs[ind]))\n            continue\n        hist += fast_hist(label.flatten(), pred.flatten(), num_classes)\n        if ind > 0 and ind % 10 == 0:\n            print(\'{:d} / {:d}: {:0.2f}\'.format(ind, len(gt_imgs), 100*np.mean(per_class_iu(hist))))\n    \n    mIoUs = per_class_iu(hist)\n    for ind_class in range(num_classes):\n        print(\'===>\' + name_classes[ind_class] + \':\\t\' + str(round(mIoUs[ind_class] * 100, 2)))\n    print(\'===> mIoU: \' + str(round(np.nanmean(mIoUs) * 100, 2)))\n    return mIoUs\n\n\ndef main(args):\n   compute_mIoU(args.gt_dir, args.pred_dir, args.devkit_dir)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gt_dir\', type=str, default=\'/home/wty/AllDataSet/CityScapes/gtFine/val\',\n                         help=\'directory which stores CityScapes val gt images\')\n    parser.add_argument(\'--pred_dir\', type=str, default=\'result/\',\n                         help=\'directory which stores CityScapes val pred images\')\n    parser.add_argument(\'--devkit_dir\', default=\'dataset/cityscapes_list\', help=\'base directory of cityscapes\')\n    args = parser.parse_args()\n    main(args)\n'"
utils/convert_state.py,0,"b'###################################################################################################################\n#\n#  The scripts is employed to convert a state_dict from a DataParallel module to normal module state_dict inplace, \n#  which is removing the prefix module.\n#  Additionally, you can load the model weigths by employing nn.DataParallel, In this case, you need not employ \n#  convert_state_dict() to remove the prefix module.\n#  Author: Rosun\n#  Date: 2018/03/14\n#\n##################################################################################################################\n\nfrom collections import OrderedDict\nimport os\nimport numpy as np\n\ndef convert_state_dict(state_dict):\n    """"""\n    Converts a state dict saved from a dataParallel module to normal module state_dict inplace\n    Args:   \n        state_dict is the loaded DataParallel model_state\n    """"""\n    state_dict_new = OrderedDict()\n    #print(type(state_dict))\n    for k, v in state_dict.items():\n        #print(k)\n        name = k[7:] # remove the prefix module.\n        # My heart is borken, the pytorch have no ability to do with the problem.\n        state_dict_new[name] = v\n    return state_dict_new\n\n'"
utils/loss.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\n\nclass CrossEntropyLoss2d(nn.Module):\n    '''\n    This file defines a cross entropy loss for 2D images\n    '''\n    def __init__(self, weight=None, ignore_label= 255):\n        '''\n        :param weight: 1D weight vector to deal with the class-imbalance\n        Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. \n        You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.\n        '''\n        super().__init__()\n\n        #self.loss = nn.NLLLoss2d(weight, ignore_index=255)\n        self.loss = nn.NLLLoss(weight, ignore_index= ignore_label)\n    def forward(self, outputs, targets):\n        return self.loss(F.log_softmax(outputs, 1), targets)\n"""
utils/metric.py,0,"b'import os, sys\nimport numpy as np\n\nfrom multiprocessing import Pool \n#import copy_reg\nimport copyreg\nimport types\ndef _pickle_method(m):\n    if m.im_self is None:\n        return getattr, (m.im_class, m.im_func.func_name)\n    else:\n        return getattr, (m.im_self, m.im_func.func_name)\n\ncopyreg.pickle(types.MethodType, _pickle_method)\n\nclass ConfusionMatrix(object):\n\n    def __init__(self, nclass, classes=None, ignore_label= 255):\n        self.nclass = nclass\n        self.classes = classes\n        self.M = np.zeros((nclass, nclass))\n        self.ignore_label= ignore_label\n\n    def add(self, gt, pred):\n        assert(np.max(pred) <= self.nclass)\n        assert(len(gt) == len(pred))\n        for i in range(len(gt)):\n            if not gt[i] == self.ignore_label:\n                self.M[gt[i], pred[i]] += 1.0\n\n    def addM(self, matrix):\n        assert(matrix.shape == self.M.shape)\n        self.M += matrix\n\n    def __str__(self):\n        pass\n\n    def recall(self):\n        recall = 0.0\n        for i in range(self.nclass):\n            recall += self.M[i, i] / np.sum(self.M[:, i])\n\n        return recall/self.nclass\n\n    def accuracy(self):\n        accuracy = 0.0\n        for i in range(self.nclass):\n            accuracy += self.M[i, i] / np.sum(self.M[i, :])\n\n        return accuracy/self.nclass\n\n    def jaccard(self):\n        jaccard = 0.0\n        jaccard_perclass = []\n        for i in range(self.nclass):\n            if not self.M[i, i] == 0:\n                jaccard_perclass.append(self.M[i, i] / (np.sum(self.M[i, :]) + np.sum(self.M[:, i]) - self.M[i, i]))\n\n        return np.sum(jaccard_perclass)/len(jaccard_perclass), jaccard_perclass, self.M\n\n    def generateM(self, item):\n        gt, pred = item\n        m = np.zeros((self.nclass, self.nclass))\n        assert(len(gt) == len(pred))\n        for i in range(len(gt)):\n            if gt[i] < self.nclass: #and pred[i] < self.nclass:\n                m[gt[i], pred[i]] += 1.0\n        return m\n\ndef get_iou(data_list, class_num, save_path=None):\n    """""" \n    Args:\n      data_list: a list, its elements [gt, output]\n      class_num: the number of label\n    """"""\n    from multiprocessing import Pool \n\n    ConfM = ConfusionMatrix(class_num)\n    f = ConfM.generateM\n    pool = Pool() \n    m_list = pool.map(f, data_list)\n    pool.close() \n    pool.join() \n    \n    for m in m_list:\n        ConfM.addM(m)\n\n    aveJ, j_list, M = ConfM.jaccard()\n    #print(j_list)\n    #print(M)\n    #print(\'meanIOU: \' + str(aveJ) + \'\\n\')\n\n    if save_path:\n        with open(save_path, \'w\') as f:\n            f.write(\'meanIOU: \' + str(aveJ) + \'\\n\')\n            f.write(str(j_list)+\'\\n\')\n            f.write(str(M)+\'\\n\')\n    return aveJ, j_list\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    m_list = []\n    data_list = []\n    test_ids = [i.strip() for i in open(args.test_ids) if not i.strip() == \'\']\n    for index, img_id in enumerate(test_ids):\n        if index % 100 == 0:\n            print(\'%d processd\'%(index))\n        pred_img_path = os.path.join(args.pred_dir, img_id+\'.png\')\n        gt_img_path = os.path.join(args.gt_dir, img_id+\'.png\')\n        pred = cv2.imread(pred_img_path, cv2.IMREAD_GRAYSCALE)\n        gt = cv2.imread(gt_img_path, cv2.IMREAD_GRAYSCALE)\n        # show_all(gt, pred)\n        data_list.append([gt.flatten(), pred.flatten()])\n\n    ConfM = ConfusionMatrix(args.class_num)\n    f = ConfM.generateM\n    pool = Pool() \n    m_list = pool.map(f, data_list)\n    pool.close() \n    pool.join() \n    \n    for m in m_list:\n        ConfM.addM(m)\n\n    aveJ, j_list, M = ConfM.jaccard()\n    with open(args.save_path, \'w\') as f:\n        f.write(\'meanIOU: \' + str(aveJ) + \'\\n\')\n        f.write(str(j_list)+\'\\n\')\n        f.write(str(M)+\'\\n\')\n'"
utils/modelsize.py,3,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\nclass SizeEstimator(object):\n\n    def __init__(self, model, input_size=(1,1,32,32), bits=32):\n        '''\n        Estimates the size of PyTorch models in memory\n        for a given input size\n        '''\n        self.model = model\n        self.input_size = input_size\n        self.bits = 32\n        return\n\n    def get_parameter_sizes(self):\n        '''Get sizes of all parameters in `model`'''\n        mods = list(self.model.modules())\n        sizes = []\n        \n        for i in range(1,len(mods)):\n            m = mods[i]\n            p = list(m.parameters())\n            for j in range(len(p)):\n                sizes.append(np.array(p[j].size()))\n\n        self.param_sizes = sizes\n        return\n\n    def get_output_sizes(self):\n        '''Run sample input through each layer to get output sizes'''\n        input_ = Variable(torch.FloatTensor(*self.input_size), volatile=True)\n        mods = list(self.model.modules())\n        out_sizes = []\n        for i in range(1, len(mods)):\n            m = mods[i]\n            out = m(input_)\n            out_sizes.append(np.array(out.size()))\n            input_ = out\n\n        self.out_sizes = out_sizes\n        return\n\n    def calc_param_bits(self):\n        '''Calculate total number of bits to store `model` parameters'''\n        total_bits = 0\n        for i in range(len(self.param_sizes)):\n            s = self.param_sizes[i]\n            bits = np.prod(np.array(s))*self.bits\n            total_bits += bits\n        self.param_bits = total_bits\n        return\n\n    def calc_forward_backward_bits(self):\n        '''Calculate bits to store forward and backward pass'''\n        total_bits = 0\n        for i in range(len(self.out_sizes)):\n            s = self.out_sizes[i]\n            bits = np.prod(np.array(s))*self.bits\n            total_bits += bits\n        # multiply by 2 for both forward AND backward\n        self.forward_backward_bits = (total_bits*2)\n        return\n\n    def calc_input_bits(self):\n        '''Calculate bits to store input'''\n        self.input_bits = np.prod(np.array(self.input_size))*self.bits\n        return\n\n    def estimate_size(self):\n        '''Estimate model size in memory in megabytes and bits'''\n        self.get_parameter_sizes()\n        self.get_output_sizes()\n        self.calc_param_bits()\n        self.calc_forward_backward_bits()\n        self.calc_input_bits()\n        total = self.param_bits + self.forward_backward_bits + self.input_bits\n\n        total_megabytes = (total/8)/(1024**2)\n        return total_megabytes, total\n"""
utils/modeltools.py,8,"b'import numpy as np\nimport os\nimport torch\nimport random\nimport torchvision\nfrom torch.autograd import Variable\n\ndef netParams(model):\n    """"""\n    computing total network parameters\n    args:\n       model: model\n    return: the number of parameters\n    """"""\n    total_paramters = 0\n    for parameter in model.parameters():\n        i = len(parameter.size())\n        p = 1\n        for j in range(i):\n            p *= parameter.size(j)\n        total_paramters += p\n\n    print(""the number of params: %.2f M"" % (total_paramters / 1e6))\n    return total_paramters\n\n\n\ndef print_model_param_nums(model=None):\n    if model == None:\n        model = torchvision.models.alexnet()\n    total = sum([param.nelement() if param.requires_grad else 0 for param in model.parameters()])\n    print(""  + Number of params: %.2fM "" % (total / 1e6))\n\n\ndef print_model_param_flops(model=None, input_w=1024, input_h=2048, multiply_adds=True):\n\n    prods = {}\n    def save_hook(name):\n        def hook_per(self, input, output):\n            prods[name] = np.prod(input[0].shape)\n        return hook_per\n\n    list_1=[]\n    def simple_hook(self, input, output):\n        list_1.append(np.prod(input[0].shape))\n    list_2={}\n    def simple_hook2(self, input, output):\n        list_2[\'names\'] = np.prod(input[0].shape)\n\n\n    list_conv=[]\n    def conv_hook(self, input, output):\n        batch_size, input_channels, input_height, input_width = input[0].size()\n        output_channels, output_height, output_width = output[0].size()\n\n        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\n        bias_ops = 1 if self.bias is not None else 0\n\n        params = output_channels * (kernel_ops + bias_ops)\n        flops = (kernel_ops * (2 if multiply_adds else 1) + bias_ops) * output_channels * output_height * output_width * batch_size\n\n        list_conv.append(flops)\n\n\n    list_linear=[]\n    def linear_hook(self, input, output):\n        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n\n        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n        bias_ops = self.bias.nelement()\n\n        flops = batch_size * (weight_ops + bias_ops)\n        list_linear.append(flops)\n\n    list_bn=[]\n    def bn_hook(self, input, output):\n        list_bn.append(input[0].nelement() * 2)\n\n    list_relu=[]\n    def relu_hook(self, input, output):\n        list_relu.append(input[0].nelement())\n\n    list_pooling=[]\n    def pooling_hook(self, input, output):\n        #print(""input:"", input.size())\n        #print(""input[0]:"", input[0].size())\n        batch_size, input_channels, input_height, input_width = input[0].size()\n        #print(""size:"", output[0].size())\n        #print(""size:"", type(output))\n        #print(output)\n        output_channels, output_height, output_width = output[0].size()\n\n        kernel_ops = self.kernel_size * self.kernel_size\n        bias_ops = 0\n        params = 0\n        flops = (kernel_ops + bias_ops) * output_channels * output_height * output_width * batch_size\n\n        list_pooling.append(flops)\n\n    list_upsample=[]\n    # For bilinear upsample\n    def upsample_hook(self, input, output):\n        batch_size, input_channels, input_height, input_width = input[0].size()\n        output_channels, output_height, output_width = output[0].size()\n\n        flops = output_height * output_width * output_channels * batch_size * 12\n        list_upsample.append(flops)\n\n    def foo(net):\n        childrens = list(net.children())\n        if not childrens:\n            if isinstance(net, torch.nn.Conv2d):\n                net.register_forward_hook(conv_hook)\n            if isinstance(net, torch.nn.Linear):\n                net.register_forward_hook(linear_hook)\n            if isinstance(net, torch.nn.BatchNorm2d):\n                net.register_forward_hook(bn_hook)\n            if isinstance(net, torch.nn.ReLU) or isinstance(net, torch.nn.PReLU):\n                net.register_forward_hook(relu_hook)\n            if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d) or isinstance(net, torch.nn.AdaptiveAvgPool2d):\n                net.register_forward_hook(pooling_hook)\n            if isinstance(net, torch.nn.Upsample):\n                net.register_forward_hook(upsample_hook)\n      \n            return\n        for c in childrens:\n            foo(c)\n\n    if model == None:\n        model = torchvision.models.alexnet()\n    foo(model)\n    input = Variable(torch.rand(3,input_w,input_h).unsqueeze(0), requires_grad = True)\n    out = model(input)\n\n\n    total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling) + sum(list_upsample))\n\n    print(\'  + Number of FLOPs: %.2fG\' % (total_flops / 1e9))\n\n\n\n'"
utils/summary.py,9,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom collections import OrderedDict\n\n\ndef summary(model, input_size):\n        def register_hook(module):\n            def hook(module, input, output):\n                class_name = str(module.__class__).split(\'.\')[-1].split(""\'"")[0]\n                module_idx = len(summary)\n\n                m_key = \'%s-%i\' % (class_name, module_idx+1)\n                summary[m_key] = OrderedDict()\n                summary[m_key][\'input_shape\'] = list(input[0].size())\n                summary[m_key][\'input_shape\'][0] = -1\n                if isinstance(output, (list,tuple)):\n                    summary[m_key][\'output_shape\'] = [[-1] + list(o.size())[1:] for o in output]\n                else:\n                    summary[m_key][\'output_shape\'] = list(output.size())\n                    summary[m_key][\'output_shape\'][0] = -1\n\n                params = 0\n                if hasattr(module, \'weight\'):\n                    params += torch.prod(torch.LongTensor(list(module.weight.size())))\n                    summary[m_key][\'trainable\'] = module.weight.requires_grad\n                if hasattr(module, \'bias\') and hasattr(module.bias, \'size\'):\n                    params +=  torch.prod(torch.LongTensor(list(module.bias.size())))\n                summary[m_key][\'nb_params\'] = params\n                \n            if (not isinstance(module, nn.Sequential) and \n               not isinstance(module, nn.ModuleList) and \n               not (module == model)):\n                hooks.append(module.register_forward_hook(hook))\n                \n        if torch.cuda.is_available():\n            dtype = torch.cuda.FloatTensor\n        else:\n            dtype = torch.FloatTensor\n        \n        # check if there are multiple inputs to the network\n        if isinstance(input_size[0], (list, tuple)):\n            x = [Variable(torch.rand(1,*in_size)).type(dtype) for in_size in input_size]\n        else:\n            x = Variable(torch.rand(1,*input_size)).type(dtype)\n            \n            \n        # print(type(x[0]))\n        # create properties\n        summary = OrderedDict()\n        hooks = []\n        # register hook\n        model.apply(register_hook)\n        # make a forward pass\n        # print(x.shape)\n        model(x)\n        # remove these hooks\n        for h in hooks:\n            h.remove()\n\n        print(\'----------------------------------------------------------------\')\n        line_new = \'{:>25}  {:>30} {:>15}\'.format(\'Layer (type)\', \'Output Shape\', \'Param #\')\n        print(line_new)\n        print(\'================================================================\')\n        total_params = 0\n        trainable_params = 0\n        for layer in summary:\n            # input_shape, output_shape, trainable, nb_params\n            line_new = \'{:>25}  {:>30} {:>15}\'.format(layer, str(summary[layer][\'output_shape\']), summary[layer][\'nb_params\'])\n            total_params += summary[layer][\'nb_params\']\n            if \'trainable\' in summary[layer]:\n                if summary[layer][\'trainable\'] == True:\n                    trainable_params += summary[layer][\'nb_params\']\n            print(line_new)\n        print(\'================================================================\')\n        print(\'Total params: \' + str(total_params))\n        print(\'Trainable params: \' + str(trainable_params))\n        print(\'Non-trainable params: \' + str(total_params - trainable_params))\n        print(\'----------------------------------------------------------------\')\n        # return summary\n'"
utils/trainID2labelID.py,0,"b""# converting trainIDs to labelIDs for evaluating the test set segmenatation results of the cityscapes dataset\n\nimport numpy as np\nimport os\nfrom PIL import Image\n\n#index: trainId from 0 to 18, 19 semantic class   val: labelIDs\ncityscapes_trainIds2labelIds = np.array([7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33], dtype=np.uint8)\n\ndef trainIDs2LabelID(trainID_png_dir, save_dir):\n    print('save_dir:  ',save_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    png_list = os.listdir(trainID_png_dir)\n    for index ,png_filename in enumerate(png_list):\n        #\n        png_path=os.path.join(trainID_png_dir, png_filename)\n        #print(png_path)\n        print( 'processing(', index,'/',len(png_list), ') ....' )\n        image = Image.open(png_path)   # image is a PIL #image \n        pngdata = np.array(image)\n        trainID = pngdata  #model prediction\n        row, col = pngdata.shape \n        labelID = np.zeros( (row , col), dtype=np.uint8 )\n        for i in range(row):\n            for j in range(col):\n                labelID[i][j] =  cityscapes_trainIds2labelIds[ trainID[i][j] ]\n\n        res_path = os.path.join(save_dir, png_filename)\n        new_im = Image.fromarray(labelID)\n        new_im.save(res_path)\n\n\nif __name__ == '__main__':\n     trainID_png_dir  = './result/cityscapes/test/'\n     save_dir =  './result/cityscapes_submit/'\n     trainIDs2LabelID(trainID_png_dir, save_dir)\n"""
