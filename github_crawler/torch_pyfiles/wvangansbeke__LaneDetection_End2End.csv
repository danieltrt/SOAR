file_path,api_count,code
Backprojection_Loss/Loss_crit.py,23,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2\nfrom Networks.utils import get_homography\n\nclass polynomial():\n    """"""\n    Polynomial class with exact integral calculation according to the\n    trapezium rule.\n    """"""\n    def __init__(self, coeffs, a=0, b=0.7, n=100):\n        self.a1, self.b1, self.c1 = torch.chunk(coeffs, 3, 1)\n        self.a1, self.b1, self.c1 = self.a1.squeeze(), self.b1.squeeze(), self.c1.squeeze()\n        self.a = a\n        self.b = b\n        self.n = n\n\n    def calc_pol(self, x):\n        return self.a1*x**2 + self.b1*x + self.c1\n\n    def trapezoidal(self, other):\n        h = float(self.b - self.a) / self.n\n        s = 0.0\n        s += abs(self.calc_pol(self.a)/2.0 - other.calc_pol(other.a)/2.0)\n        for i in range(1, self.n):\n            s += abs(self.calc_pol(self.a + i*h) - other.calc_pol(self.a + i*h))\n        s += abs(self.calc_pol(self.b)/2.0 - other.calc_pol(self.b)/2.0)\n        out = s*h\n        return out\n\n\n# pol1 = polynomial(coeffs=torch.FloatTensor([[0, 1, 0]]), a=-1, b=1)\n# pol2 = polynomial(coeffs=torch.FloatTensor([[0, 0, 0]]), a=-1, b=1)\n# pol1 = polynomial(coeffs=torch.FloatTensor([[0, 1, 0]]), a=0, b=1)\n# pol2 = polynomial(coeffs=torch.FloatTensor([[1, 0, 0]]), a=0, b=1)\n# print(\'Area by trapezium rule is {}\'.format(pol1.trapezoidal(pol2)))\n\n\ndef define_loss_crit(options):\n    \'\'\'\n    Define loss cirterium:\n        -MSE loss on curve parameters in ortho view\n        -MSE loss on points after backprojection to normal view\n        -Area loss\n    \'\'\'\n    if options.loss_policy == \'mse\':\n        loss_crit = MSE_Loss(options)\n    elif options.loss_policy == \'homography_mse\':\n        loss_crit = Homography_MSE_Loss(options)\n    elif options.loss_policy == \'backproject\':\n        loss_crit = backprojection_loss(options)\n    elif options.loss_policy == \'area\':\n        loss_crit = Area_Loss(options.order, options.weight_funct)\n    else:\n        return NotImplementedError(\'The requested loss criterion is not implemented\')\n    weights = (torch.Tensor([1] + [options.weight_seg]*(options.nclasses))).cuda()\n    loss_seg = nn.CrossEntropyLoss(weights)\n    # loss_seg = CrossEntropyLoss2d(seg=True, nclasses=options.nclasses, weight=options.weight_seg)\n    return loss_crit, loss_seg\n\n\nclass CrossEntropyLoss2d(nn.Module):\n    \'\'\'\n    Standard 2d cross entropy loss on all pixels of image\n    My implemetation (but since Pytorch 0.2.0 libs have their\n    owm optimized implementation, consider using theirs)\n    \'\'\'\n    def __init__(self, weight=None, size_average=True, seg=False, nclasses=2):\n        if seg:\n            weights = torch.Tensor([1] + [weight]*(nclasses))\n            weights = weights.cuda()\n        super(CrossEntropyLoss2d, self).__init__()\n        self.nll_loss = nn.NLLLoss2d(weights, size_average)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss(F.log_softmax(inputs, dim=1), targets[:, 0, :, :])\n\n\nclass Area_Loss(nn.Module):\n    \'\'\'\n    Compute area between curves by integrating (x1 - x2)^2 over y\n    *Area:\n        *order 0: int((c1 - c2)**2)dy\n        *order 1: int((b1*y - b2*y + c1 - c2)**2)dy\n        *order 2: int((a1*y**2 - a2*y**2 + b1*y - b2*y + c1 - c2)**2)dy\n\n    *A weight function W can be added:\n        Weighted area: int(W(y)*diff**2)dy\n        with W(y):\n            *1\n            *(1-y)\n            *(1-y**0.5)\n    \'\'\'\n    def __init__(self, order, weight_funct):\n        super(Area_Loss, self).__init__()\n        self.order = order\n        self.weight_funct = weight_funct\n\n    def forward(self, params, gt_params, compute=True):\n        diff = params.squeeze(-1) - gt_params\n        a = diff[:, 0]\n        b = diff[:, 1]\n        t = 0.7 # up to which y location to integrate\n        if self.order == 2:\n            c = diff[:, 2]\n            if self.weight_funct == \'none\':\n                # weight (1)\n                loss_fit = (a**2)*(t**5)/5+2*a*b*(t**4)/4 + \\\n                           (b**2+c*2*a)*(t**3)/3+2*b*c*(t**2)/2+(c**2)*t\n            elif self.weight_funct == \'linear\':\n                # weight (1-y)\n                loss_fit = c**2*t - t**5*((2*a*b)/5 - a**2/5) + \\\n                           t**2*(b*c - c**2/2) - (a**2*t**6)/6 - \\\n                           t**4*(b**2/4 - (a*b)/2 + (a*c)/2) + \\\n                           t**3*(b**2/3 - (2*c*b)/3 + (2*a*c)/3)\n            elif self.weight_funct == \'quadratic\':\n                # weight (1-y**0.5)\n                loss_fit = t**3*(1/3*b**2 + 2/3*a*c) - \\\n                           t**(7/2)*(2/7*b**2 + 4/7*a*c) + \\\n                           c**2*t + 0.2*a**2*t**5 - 2/11*a**2*t**(11/2) - \\\n                           2/3*c**2*t**(3/2) + 0.5*a*b*t**4 - \\\n                           4/9*a*b*t**(9/2) + b*c*t**2 - 0.8*b*c*t**(5/2)\n            else:\n                return NotImplementedError(\'The requested weight function is \\\n                        not implemented, only order 1 or order 2 possible\')\n        elif self.order == 1:\n            loss_fit = (b**2)*t + a*b*(t**2) + ((a**2)*(t**3))/3\n        else:\n            return NotImplementedError(\'The requested order is not implemented, only none, linear or quadratic possible\')\n\n        # Mask select if lane is present\n        mask = torch.prod(gt_params != 0, 1).byte()\n        loss_fit = torch.masked_select(loss_fit, mask)\n        loss_fit = loss_fit.mean(0) if loss_fit.size()[0] != 0 else 0 # mean over the batch\n        return loss_fit\n\n\nclass MSE_Loss(nn.Module):\n    \'\'\'\n    Compute mean square error loss on curve parameters\n    in ortho or normal view\n    \'\'\'\n    def __init__(self, options):\n        super(MSE_Loss, self).__init__()\n        self.loss_crit = nn.MSELoss()\n        if not options.no_cuda:\n            self.loss_crit = self.loss_crit.cuda()\n\n    def forward(self, params, gt_params, compute=True):\n        loss = self.loss_crit(params.squeeze(-1), gt_params)\n        return loss\n\nclass backprojection_loss(nn.Module):\n    \'\'\'\n    Compute mean square error loss on points in normal view\n    instead of parameters in ortho view\n    \'\'\'\n    def __init__(self, options):\n        super(backprojection_loss, self).__init__()\n        M, M_inv = get_homography(options.resize, options.no_mapping)\n        self.M, self.M_inv = torch.from_numpy(M), torch.from_numpy(M_inv)\n        start = 160\n        delta = 10\n        num_heights = (720-start)//delta\n        self.y_d = (torch.arange(start,720,delta)-80).double() / 2.5\n        self.ones = torch.ones(num_heights).double()\n        self.y_prime = (self.M[1,1:2]*self.y_d + self.M[1,2:])/(self.M[2,1:2]*self.y_d+self.M[2,2:])\n        self.y_eval = 255 - self.y_prime\n\n        if options.order == 0:\n            self.Y = self.tensor_ones\n        elif options.order == 1:\n            self.Y = torch.stack((self.y_eval, self.ones), 1)\n        elif options.order == 2:\n            self.Y = torch.stack((self.y_eval**2, self.y_eval, self.ones), 1)\n        elif options.order == 3:\n            self.Y = torch.stack((self.y_eval**3, self.y_eval**2, self.y_eval, self.ones), 1)\n        else:\n            raise NotImplementedError(\n                    \'Requested order {} for polynomial fit is not implemented\'.format(options.order))\n\n        self.Y = self.Y.unsqueeze(0).repeat(options.batch_size, 1, 1)\n        self.ones = torch.ones(options.batch_size, num_heights, 1).double()\n        self.y_prime = self.y_prime.unsqueeze(0).repeat(options.batch_size, 1).unsqueeze(2)\n        self.M_inv = self.M_inv.unsqueeze(0).repeat(options.batch_size, 1, 1)\n\n        if not options.no_cuda:\n            self.M = self.M.cuda()\n            self.M_inv = self.M_inv.cuda()\n            self.y_prime = self.y_prime.cuda()\n            self.Y = self.Y.cuda()\n            self.ones = self.ones.cuda()\n\n    def forward(self, params, x_gt, valid_samples):\n        # Sample at y_d in the homography space\n        bs = params.size(0)\n        x_prime = torch.bmm(self.Y[:bs], params)\n\n        # Transform sampled points back\n        coordinates = torch.stack((x_prime, self.y_prime[:bs], self.ones[:bs]), 2).squeeze(3).permute((0, 2, 1))\n        trans = torch.bmm(self.M_inv[:bs], coordinates)\n        x_cal = trans[:,0,:]/trans[:,2,:]\n        # y_cal = trans[:,1,:]/trans[:,2,:] # sanity check\n\n        # Compute error\n        x_err = (x_gt-x_cal)*valid_samples\n        loss = torch.sum(x_err**2) / (valid_samples.sum())\n        if valid_samples.sum() == 0:\n            loss = 0\n        return loss, x_cal * valid_samples\n'"
Backprojection_Loss/eval_lane.py,0,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport ujson as json\n\nclass LaneEval(object):\n    lr = LinearRegression()\n    pixel_thresh = 20\n    pt_thresh = 0.85\n\n    @staticmethod\n    def get_angle(xs, y_samples):\n        xs, ys = xs[xs >= 0], y_samples[xs >= 0]\n        if len(xs) > 1:\n            LaneEval.lr.fit(ys[:, None], xs)\n            k = LaneEval.lr.coef_[0]\n            theta = np.arctan(k)\n        else:\n            theta = 0\n        return theta\n\n    @staticmethod\n    def line_accuracy(pred, gt, thresh):\n        pred = np.array([p if p >= 0 else -100 for p in pred])\n        gt = np.array([g if g >= 0 else -100 for g in gt])\n        return np.sum(np.where(np.abs(pred - gt) < thresh, 1., 0.)) / len(gt)\n\n    @staticmethod\n    def bench(pred, gt, y_samples, running_time):\n        if any(len(p) != len(y_samples) for p in pred):\n            raise Exception(\'Format of lanes error.\')\n        if running_time > 200 or len(gt) + 2 < len(pred):\n            return 0., 0., 1.\n        angles = [LaneEval.get_angle(np.array(x_gts), np.array(y_samples)) for x_gts in gt]\n        threshs = [LaneEval.pixel_thresh / np.cos(angle) for angle in angles]\n        line_accs = []\n        fp, fn = 0., 0.\n        matched = 0.\n        for x_gts, thresh in zip(gt, threshs):\n            accs = [LaneEval.line_accuracy(np.array(x_preds), np.array(x_gts), thresh) for x_preds in pred]\n            max_acc = np.max(accs) if len(accs) > 0 else 0.\n            if max_acc < LaneEval.pt_thresh:\n                fn += 1\n            else:\n                matched += 1\n            line_accs.append(max_acc)\n        fp = len(pred) - matched\n        if len(gt) > 4 and fn > 0:\n            fn -= 1\n        s = sum(line_accs)\n        if len(gt) > 4:\n            s -= min(line_accs)\n        return s / max(min(4.0, len(gt)), 1.), fp / len(pred) if len(pred) > 0 else 0., fn / max(min(len(gt), 4.) , 1.)\n\n    @staticmethod\n    def bench_one_submit(pred_file, gt_file):\n        try:\n            json_pred = [json.loads(line) for line in open(pred_file).readlines()]\n        except BaseException as e:\n            raise Exception(\'Fail to load json file of the prediction.\')\n        json_gt = [json.loads(line) for line in open(gt_file).readlines()]\n        if len(json_gt) != len(json_pred):\n            raise Exception(\'We do not get the predictions of all the test tasks\')\n        gts = {l[\'raw_file\']: l for l in json_gt}\n        accuracy, fp, fn = 0., 0., 0.\n        for pred in json_pred:\n            if \'raw_file\' not in pred or \'lanes\' not in pred or \'run_time\' not in pred:\n                raise Exception(\'raw_file or lanes or run_time not in some predictions.\')\n            raw_file = pred[\'raw_file\']\n            pred_lanes = pred[\'lanes\']\n            run_time = pred[\'run_time\']\n            if raw_file not in gts:\n                raise Exception(\'Some raw_file from your predictions do not exist in the test tasks.\')\n            gt = gts[raw_file]\n            gt_lanes = gt[\'lanes\']\n            y_samples = gt[\'h_samples\']\n            try:\n                a, p, n = LaneEval.bench(pred_lanes, gt_lanes, y_samples, run_time)\n            except BaseException as e:\n                raise Exception(\'Format of lanes error.\')\n            accuracy += a\n            fp += p\n            fn += n\n        num = len(gts)\n        # the first return parameter is the default ranking parameter\n#        return json.dumps([\n#            {\'name\': \'Accuracy\', \'value\': accuracy / num, \'order\': \'desc\'},\n#            {\'name\': \'FP\', \'value\': fp / num, \'order\': \'asc\'},\n#            {\'name\': \'FN\', \'value\': fn / num, \'order\': \'asc\'}\n#        ])\n        return [accuracy / num, fp / num, fn / num]\n\n\nif __name__ == \'__main__\':\n    import sys\n    try:\n        if len(sys.argv) != 3:\n            raise Exception(\'Invalid input arguments\')\n        print (LaneEval.bench_one_submit(sys.argv[1], sys.argv[2]))\n    except Exception as e:\n        print (e.message)\n        sys.exit(e.message)\n'"
Backprojection_Loss/main.py,17,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nimport torch\nimport torch.optim\nimport torch.nn as nn\n\nimport os\nimport glob\nimport time\nimport sys\nimport shutil\nimport json\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nfrom Dataloader.Load_Data_new import get_loader, get_testloader, \\\n                                     load_valid_set_file_all \n                                    \n# from eval_lane import LaneEval\nfrom Loss_crit import define_loss_crit, polynomial \nfrom Networks.LSQ_layer import Net\nfrom Networks.utils import define_args, save_weightmap, first_run, \\\n                           mkdir_if_missing, Logger, define_init_weights,\\\n                           define_scheduler, define_optim, AverageMeter\nfrom test import test_model\n\n\ndef get_flags(epoch=0):\n    # Set flags for pretraining\n    skip = False\n    if args.pretrained:\n        if (epoch < args.pretrain_epochs):\n            args.end_to_end = False\n            print(""Pretraining so set args.end_to_end to {}"".format(args.end_to_end))\n            if epoch < args.skip_epochs:\n                print(""Skipping LSQ layer"")\n                skip=True\n        else:\n            args.end_to_end = True\n    return skip\n\ndef main():\n    global args\n    global mse_policy\n    parser = define_args()\n    args = parser.parse_args()\n    if not args.end_to_end:\n        assert args.pretrained == False\n    if args.clas:\n        assert args.nclasses == 4\n    if args.val_batch_size is None:\n        args.val_batch_size = args.batch_size\n    \n\n    # Check GPU availability\n    if not args.no_cuda and not torch.cuda.is_available():\n        raise Exception(""No gpu available for usage"")\n    torch.backends.cudnn.benchmark = args.cudnn\n\n    # Define save path\n    save_id = \'Mod_{}_opt_{}_loss_{}_lr_{}_batch_{}_end2end_{}_chol_{}_lanes_{}_pretrain{}_clas{}_mask{}_flip_on{}_activation_{}\' \\\n            .format(args.mod, args.optimizer,\n                    args.loss_policy,\n                    args.learning_rate,\n                    args.batch_size,\n                    args.end_to_end,\n                    args.use_cholesky,\n                    args.nclasses,\n                    args.pretrained,\n                    args.clas,\n                    args.mask_percentage,\n                    args.flip_on,\n                    args.activation_layer)\n    \n\n    train_loader, valid_loader, valid_idx = get_loader(args.num_train,\n                                                       args.json_file, \'Labels/lanes_ordered.json\',\n                                                       args.image_dir, \n                                                       args.gt_dir,\n                                                       args.flip_on, args.batch_size, args.val_batch_size,\n                                                       shuffle=True, num_workers=args.nworkers,\n                                                       end_to_end=args.end_to_end,\n                                                       resize=args.resize,\n                                                       nclasses=args.nclasses,\n                                                       split_percentage=args.split_percentage)\n\n    test_loader = get_testloader(args.test_dir, args.val_batch_size, args.nworkers)\n\n    # Define network\n    model = Net(args)\n    define_init_weights(model, args.weight_init)\n\n    if not args.no_cuda:\n        # Load model on gpu before passing params to optimizer\n        model = model.cuda()\n\n    # Define optimizer and scheduler\n    optimizer = define_optim(args.optimizer, model.parameters(),\n                             args.learning_rate, args.weight_decay)\n    scheduler = define_scheduler(optimizer, args)\n\n\n    # Define loss criteria for multiple tasks\n    criterion, criterion_seg = define_loss_crit(args)\n    criterion_horizon = nn.BCEWithLogitsLoss().cuda()\n    criterion_line_class = nn.BCEWithLogitsLoss().cuda()\n\n    # Name\n    global crit_string\n    if args.loss_policy == \'area\' and args.end_to_end:\n        crit_string = \'AREA**2\' \n    elif args.loss_policy == \'backproject\' and args.end_to_end:\n        crit_string = \'MSE\' \n    else:\n        crit_string = \'ENTROPY\' \n    if args.clas:\n        crit_string = \'TOT LOSS\' \n\n    # Logging setup\n    best_epoch = 0\n    lowest_loss = np.inf\n    losses_valid = np.inf\n    highest_score = 0\n    log_file_name = \'log_train_start_0.txt\'\n    args.save_path = os.path.join(args.save_path, save_id)\n    mkdir_if_missing(args.save_path)\n    mkdir_if_missing(os.path.join(args.save_path, \'example/\'))\n    mkdir_if_missing(os.path.join(args.save_path, \'example/train\'))\n    mkdir_if_missing(os.path.join(args.save_path, \'example/valid\'))\n    mkdir_if_missing(os.path.join(args.save_path, \'example/pretrain\'))\n    mkdir_if_missing(os.path.join(args.save_path, \'example/testset\'))\n\n    # Computes the file with lane data of the validation set\n    validation_set_path = os.path.join(args.save_path , \'validation_set.json\')\n    load_valid_set_file_all(valid_idx, validation_set_path, args.image_dir) \n    global valid_set_labels\n    global val_set_path\n    global ls_result_path\n    valid_set_labels = [json.loads(line) for line in open(validation_set_path).readlines()]\n    val_set_path = os.path.join(args.save_path, \'validation_set_dst.json\')\n    ls_result_path = os.path.join(args.save_path, \'ls_result.json\')\n\n    # Train, evaluate or resume\n    args.resume = first_run(args.save_path)\n    if args.resume and not args.test_mode and not args.evaluate:\n        path = os.path.join(args.save_path, \'checkpoint_model_epoch_{}.pth.tar\'.format(\n            int(args.resume)))\n        if os.path.isfile(path):\n            log_file_name = \'log_train_start_{}.txt\'.format(args.resume)\n            # Redirect stdout\n            sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(path)\n            args.start_epoch = checkpoint[\'epoch\']\n            lowest_loss = checkpoint[\'loss\']\n            best_epoch = checkpoint[\'best epoch\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            log_file_name = \'log_train_start_0.txt\'\n            # Redirect stdout\n            sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n            print(""=> no checkpoint found at \'{}\'"".format(path))\n\n    # Only evaluate\n    elif args.evaluate:\n        skip = get_flags()\n        files = glob.glob(os.path.join(args.save_path, \'model_best*\'))\n        if len(files) == 0:\n            print(\'No checkpoint found!\')\n        else:\n            best_file_name = files[0]\n            if os.path.isfile(best_file_name):\n                sys.stdout = Logger(os.path.join(args.save_path, \'Evaluate.txt\'))\n                print(""=> loading checkpoint \'{}\'"".format(best_file_name))\n                checkpoint = torch.load(best_file_name)\n                model.load_state_dict(checkpoint[\'state_dict\'])\n            else:\n                print(""=> no checkpoint found at \'{}\'"".format(best_file_name))\n\n        # validate(valid_loader, model, criterion, criterion_seg, \n                # criterion_line_class, criterion_horizon)\n\n        if args.clas:\n            test_model(test_loader, model, \n                       criterion, \n                       criterion_seg, \n                       criterion_line_class, \n                       criterion_horizon, args)\n        return\n\n    # Start training from clean slate\n    else:\n        # Redirect stdout\n        sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n\n    # INIT MODEL\n    print(40*""=""+""\\nArgs:{}\\n"".format(args)+40*""="")\n    print(""Init model: \'{}\'"".format(args.mod))\n    print(""Number of parameters in model {} is {:.3f}M"".format(\n        args.mod.upper(), sum(tensor.numel() for tensor in model.parameters())/1e6))\n\n    # Define activation for classification branch\n    if args.clas:\n        Sigm = nn.Sigmoid()\n\n    # Start training and validation for nepochs\n    for epoch in range(args.start_epoch, args.nepochs):\n        print(""\\n => Start train set for EPOCH {}"".format(epoch + 1))\n        print(""Saving to: "", args.save_path)\n        # Adjust learning rate\n        if args.lr_policy is not None and args.lr_policy != \'plateau\':\n            scheduler.step()\n            lr = optimizer.param_groups[0][\'lr\']\n            print(\'lr is set to {}\'.format(lr))\n\n        skip = get_flags(epoch)\n\n        # Define container objects to keep track of multiple losses/metrics\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        rmse_metric = AverageMeter()\n        losses_skip = AverageMeter()\n\n        # Specfiy operation modus\n        model.train()\n\n        # compute timing\n        end = time.time()\n\n        # Start training loop\n        for i, (input, gt, lanes, idx, gt_line, gt_horizon, valid_points) in tqdm(enumerate(train_loader)):\n            # Time dataloader\n            data_time.update(time.time() - end)\n\n            # Reset coordinates\n            x_cal0, x_cal1, x_cal2, x_cal3 = [None]*4\n\n            # Put inputs on gpu if possible\n            if not args.no_cuda:\n                input, lanes = input.cuda(), lanes.cuda()\n                valid_points = valid_points.cuda()\n                gt = gt.cuda().squeeze(1)\n            assert lanes.size(1) == 4\n            gt0, gt1, gt2, gt3 = lanes[:, 0, :], lanes[:, 1, :], lanes[:, 2, :], lanes[:, 3, :]\n\n            # Skip LSQ layer to make sure matrix cannot be singular\n            # TODO check if this is really necessary\n            if skip:\n                output_net = model(input, gt_line, args.end_to_end, early_return=True)\n                loss = criterion_seg(output_net, gt)\n                # Setup backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                losses_skip.update(loss.item(), input.size(0))\n                # Plot\n                if (i + 1) % args.save_freq == 0:\n                    img = input[0].permute(1, 2, 0).data.cpu().numpy()\n                    gt_orig = gt[0].data.cpu().numpy()\n                    _, out = torch.max(output_net[0], dim=0)\n                    out = out.data.cpu().numpy()\n                    img = np.clip(img, 0, 1)\n                    fig = plt.figure()\n                    ax1 = fig.add_subplot(311)\n                    ax2 = fig.add_subplot(312)\n                    ax3 = fig.add_subplot(313)\n                    ax1.imshow(img)\n                    ax2.imshow(gt_orig)\n                    ax3.imshow(out)\n                    fig.savefig(args.save_path + \'/example/pretrain/idx-{}_batch-{}\'.format(0, i))\n                    plt.clf()\n                    plt.close(fig)\n\n                # Skip rest\n                continue\n\n            # Run model\n            try:\n                beta0, beta1, beta2, beta3, weightmap_zeros, \\\n                output_net, outputs_line, outputs_horizon, output_seg = model(input, gt_line, args.end_to_end, gt=gt)\n            except RuntimeError as e:\n                print(""Batch with idx {} skipped due to singular matrix"".format(idx.numpy()))\n                print(e)\n                continue\n\n            # Compute losses on parameters or on segmentation\n            if args.end_to_end:\n                loss_left, x_cal0 = criterion(beta0, gt0, valid_points[:, 0])\n                loss_right, x_cal1 = criterion(beta1, gt1, valid_points[:, 1])\n                if args.nclasses > 3:\n                    # add losses of further lane lines\n                    loss_left1, x_cal2 = criterion(beta2, gt2, valid_points[:, 2])\n                    loss_right1, x_cal3 = criterion(beta3, gt3, valid_points[:, 3])\n                    loss_left += loss_left1\n                    loss_right += loss_right1\n                # average loss over lanes\n                loss = (loss_left + loss_right) / args.nclasses\n            else:\n                loss = criterion_seg(output_net, gt)\n                with torch.no_grad():\n                    loss_left, x_cal0 = criterion(beta0, gt0, valid_points[:, 0])\n                    loss_right, x_cal1 = criterion(beta1, gt1, valid_points[:, 1])\n                    if args.nclasses > 3:\n                        # add losses of further lane lines\n                        loss_left1, x_cal2 = criterion(beta2, gt2, valid_points[:, 2])\n                        loss_right1, x_cal3 = criterion(beta3, gt3, valid_points[:, 3])\n                        loss_left += loss_left1\n                        loss_right += loss_right1\n                    loss_metric = (loss_left + loss_right) / args.nclasses\n                    rmse_metric.update(loss_metric.item(), input.size(0))\n\n            # Horizon task & Line classification task\n            if args.clas:\n                gt_horizon, gt_line = gt_horizon.cuda(), \\\n                                      gt_line.cuda()\n                loss_horizon = criterion_horizon(outputs_horizon, gt_horizon).double()\n                loss_line = criterion_line_class(outputs_line, gt_line).double()\n                loss = loss*args.weight_fit + (loss_line + loss_horizon)*args.weight_class\n            else:\n                line_pred = gt_line\n\n            # Update loss\n            losses.update(loss.item(), input.size(0))\n\n            # Clip gradients (usefull for instabilities or mistakes in ground truth)\n            if args.clip_grad_norm != 0:\n                nn.utils.clip_grad_norm(model.parameters(), args.clip_grad_norm)\n\n            # Setup backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Time trainig iteration\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            # Print info\n            if (i + 1) % args.print_freq == 0:\n                print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.8f} ({loss.avg:.8f})\\t\'\n                      \'rmse_metric {rmse.val:.8f} ({rmse.avg:.8f})\'.format(\n                       epoch+1, i+1, len(train_loader), batch_time=batch_time,\n                       data_time=data_time, loss=losses, rmse=rmse_metric))\n\n            # Plot weightmap and curves\n            if (i + 1) % args.save_freq == 0:\n                save_weightmap(\'train\', weightmap_zeros, x_cal0, x_cal1, x_cal2, x_cal3,\n                               gt0, gt1, gt2, gt3, gt, 0, i, input,\n                               args.no_ortho, args.resize, args.save_path, args.nclasses, args.no_mapping)\n\n        print(""===> Average {}-loss on training set is {:.8f}"".format(crit_string, losses.avg))\n        if not skip:\n            losses_valid, acc_hor_tot, acc_line_tot, rmse_metric_valid = validate(valid_loader,\n                                                                                  model, criterion,\n                                                                                  criterion_seg, \n                                                                                  criterion_line_class,\n                                                                                  criterion_horizon,\n                                                                                  epoch)\n            print(""===> Average {}-loss on validation set is {:.8f}"".format(crit_string, losses_valid))\n        else:\n            print(""===> Average segmentation-loss on training set is {:.8f}"".format(losses_skip.avg))\n        if not args.end_to_end and not skip:\n            print(""===> Average rmse on training set is {:.8f}"".format(rmse_metric.avg))\n            print(""===> Average rmse on validation set is {:.8f}"".format(rmse_metric_valid))\n\n        if args.clas and len(valid_loader) != 0 :\n            print(""===> Average HORIZON ACC on val is {:.8}"".format(acc_hor_tot))\n            print(""===> Average LINE ACC on val is {:.8}"".format(acc_line_tot))\n\n        print(""===> Last best {}-loss was {:.8f} in epoch {}"".format(\n            crit_string, lowest_loss, best_epoch))\n\n        total_score = losses_valid\n\n        # TODO get acc\n        if args.clas:\n            metric = test_model(test_loader, model, \n                       criterion, \n                       criterion_seg, \n                       criterion_line_class, \n                       criterion_horizon, args)\n            total_score = metric\n\n\n        # Adjust learning_rate if loss plateaued\n        if args.lr_policy == \'plateau\':\n            scheduler.step(total_score)\n            lr = optimizer.param_groups[0][\'lr\']\n            print(\'LR plateaued, hence is set to {}\'.format(lr))\n\n        # File to keep latest epoch\n        with open(os.path.join(args.save_path, \'first_run.txt\'), \'w\') as f:\n            f.write(str(epoch))\n        # Save model\n        to_save = False\n        if total_score > highest_score:\n            to_save = True\n            best_epoch = epoch+1\n            highest_score = total_score\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'best epoch\': best_epoch,\n            \'arch\': args.mod,\n            \'state_dict\': model.state_dict(),\n            \'loss\': lowest_loss,\n            \'optimizer\': optimizer.state_dict()}, to_save, epoch)\n\n\ndef validate(loader, model, criterion, criterion_seg, \n            criterion_line_class, criterion_horizon, epoch=0):\n\n    # Define container to keep track of metric and loss\n    losses = AverageMeter()\n    acc_hor_tot = AverageMeter()\n    acc_line_tot = AverageMeter()\n    rmse_metric_valid = AverageMeter()\n\n    # Evaluate model\n    model.eval()\n\n    # Only forward pass, hence no gradients/updates needed\n    with torch.no_grad():\n        \n        # Start validation loop\n        for i, (input, gt, lanes, idx, gt_line, gt_horizon, index, valid_points) in tqdm(enumerate(loader)):\n\n            # Reset coordinates\n            x_cal0, x_cal1, x_cal2, x_cal3 = [None]*4\n\n            # Put inputs on gpu if possible\n            if not args.no_cuda:\n                input, lanes = input.cuda(), lanes.cuda()\n                input = input.float()\n                valid_points = valid_points.cuda()\n                gt = gt.cuda().squeeze(1)\n            assert lanes.size(1) == 4\n            gt0, gt1, gt2, gt3 = lanes[:, 0, :], lanes[:, 1, :], lanes[:, 2, :], lanes[:, 3, :]\n\n            # Run model\n            try:\n                beta0, beta1, beta2, beta3, weightmap_zeros, \\\n                output_net, outputs_line, outputs_horizon, output_seg = model(input, gt_line, args.end_to_end, gt = gt)\n            except RuntimeError as e:\n                print(""Batch with idx {} skipped due to singular matrix"".format(idx.numpy()))\n                print(e)\n                continue\n\n            # Compute losses on parameters or on segmentation\n            if args.end_to_end:\n                loss_left, x_cal0 = criterion(beta0, gt0, valid_points[:, 0])\n                loss_right, x_cal1 = criterion(beta1, gt1, valid_points[:, 1])\n                if args.nclasses > 3:\n                    # add losses of further lane lines\n                    loss_left1, x_cal2 = criterion(beta2, gt2, valid_points[:, 2])\n                    loss_right1, x_cal3 = criterion(beta3, gt3, valid_points[:, 3])\n                    loss_left += loss_left1\n                    loss_right += loss_right1\n                # average loss over lanes\n                loss = (loss_left + loss_right) / args.nclasses\n            else:\n                loss = criterion_seg(output_net, gt)\n                with torch.no_grad():\n                    loss_left, x_cal0 = criterion(beta0, gt0, valid_points[:, 0])\n                    loss_right, x_cal1 = criterion(beta1, gt1, valid_points[:, 1])\n                    if args.nclasses > 3:\n                        # add losses of further lane lines\n                        loss_left1, x_cal2 = criterion(beta2, gt2, valid_points[:, 2])\n                        loss_right1, x_cal3 = criterion(beta3, gt3, valid_points[:, 3])\n                        loss_left += loss_left1\n                        loss_right += loss_right1\n                    loss_metric = (loss_left + loss_right) / args.nclasses\n                    rmse_metric_valid.update(loss_metric.item(), input.size(0))\n\n            # Update losses\n            losses.update(loss.item(), input.size(0))\n\n            # Horizon task & Line classification task\n            if args.clas:\n                gt_horizon, gt_line = gt_horizon.cuda(), \\\n                                      gt_line.cuda()\n                horizon_pred = torch.round(nn.Sigmoid()(outputs_horizon))\n                acc = torch.eq(horizon_pred, gt_horizon)\n                acc_hor = torch.sum(acc).float()/(args.resize*args.val_batch_size)\n                acc_hor_tot.update(acc_hor.item())\n                line_pred = torch.round(nn.Sigmoid()(outputs_line))\n                acc = torch.eq(line_pred, gt_line)\n                acc_line = torch.sum(acc).float()/(args.nclasses*args.val_batch_size)\n                acc_line_tot.update(acc_line.item())\n                loss_horizon = criterion_horizon(outputs_horizon, gt_horizon)\n                loss_line = criterion_line_class(outputs_line, gt_line)\n                loss = loss*args.weight_fit + (loss_line + loss_horizon)*args.weight_class\n            else:\n                line_pred = gt_line\n\n            # Print info\n            if (i + 1) % args.print_freq == 0:\n                    print(\'Test: [{0}/{1}]\\t\'\n                          \'Loss {loss.val:.8f} ({loss.avg:.8f})\\t\'\n                          \'rmse {rmse_metric.val:.8f} ({rmse_metric.avg:.8f})\'.format(\n                           i+1, len(loader), loss=losses, rmse_metric=rmse_metric_valid))\n\n            # Plot weightmap and curves\n            if (i + 1) % 25 == 0:\n                save_weightmap(\'valid\', weightmap_zeros, x_cal0, x_cal1, x_cal2, x_cal3,\n                               gt0, gt1, gt2, gt3, gt, 0, i, input,\n                               args.no_ortho, args.resize, args.save_path, args.nclasses, args.no_mapping)\n\n        # Print statistic about epoch\n        if args.evaluate:\n            print(""===> Average {}-loss on validation set is {:.8}"".format(crit_string, \n                                                                           losses.avg))\n            if args.clas and len(loader) != 0:\n                print(""===> Average HORIZON ACC on val is {:.8}"".format(acc_hor_tot.avg))\n                print(""===> Average LINE ACC on val is {:.8}"".format(acc_hor_tot.avg))\n\n        return losses.avg, acc_hor_tot.avg, acc_line_tot.avg, rmse_metric_valid.avg\n\n\ndef save_checkpoint(state, to_copy, epoch):\n    filepath = os.path.join(args.save_path, \'checkpoint_model_epoch_{}.pth.tar\'.format(epoch))\n    torch.save(state, filepath)\n    if to_copy:\n        if epoch > 0:\n            lst = glob.glob(os.path.join(args.save_path, \'model_best*\'))\n            if len(lst) != 0:\n                os.remove(lst[0])\n        shutil.copyfile(filepath, os.path.join(args.save_path, \n            \'model_best_epoch_{}.pth.tar\'.format(epoch)))\n        print(""Best model copied"")\n    if epoch > 0:\n        prev_checkpoint_filename = os.path.join(args.save_path, \n                \'checkpoint_model_epoch_{}.pth.tar\'.format(epoch-1))\n        if os.path.exists(prev_checkpoint_filename):\n            os.remove(prev_checkpoint_filename)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Backprojection_Loss/test.py,19,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nimport torch\nimport torch.optim\nimport torch.nn as nn\nfrom eval_lane import LaneEval\nfrom tqdm import tqdm\nfrom Networks.utils import get_homography\nimport os\nimport json\nimport cv2\nfrom PIL import Image\nfrom Networks.utils import AverageMeter\nimport time\n\ndef resize_coordinates(array):\n    return array*2.5\n\ndef test_model(loader, model, criterion, criterion_seg, \n               criterion_line_class, criterion_horizon, args, epoch=0):\n    # Init \n    assert args.end_to_end == True\n    params = Projections(args)\n    gt_file = os.path.join(args.test_dir, \'test_label.json\')\n    gt_lanes = [json.loads(line) for line in open(gt_file,\'r\')]\n    test_set_file = os.path.join(args.save_path, \'test_set_predictions.json\')\n    colormap = [(255,0,0), (0,255,0), (255,255,0), (0,0,255), (0, 128, 128)]\n    batch_time = AverageMeter()\n\n    # Evaluate model\n    model.eval()\n\n    # Only forward pass, hence no gradients needed\n    with torch.no_grad():\n        with open(test_set_file, \'w\') as jsonFile:\n            # Start validation loop\n            for i, input in tqdm(enumerate(loader)):\n\n                # Reset coordinates\n                x_cal0, x_cal1, x_cal2, x_cal3 = [None]*4\n\n                # Put inputs on gpu if possible\n                if not args.no_cuda:\n                    input = input.cuda(non_blocking=True).float()\n\n                # Run model\n                torch.cuda.synchronize()\n                a = time.time()\n                beta0, beta1, beta2, beta3, weightmap_zeros, \\\n                    output_net, outputs_line, outputs_horizon, output_seg = model(input, gt_line=np.array([1,1]), \n                                                                                  end_to_end=args.end_to_end, gt=None)\n                torch.cuda.synchronize()\n                b = time.time()\n                batch_time.update(b-a)\n\n                # Horizon task & Line classification task\n                if args.clas:\n                    horizon_pred = nn.Sigmoid()(outputs_horizon).sum(dim=1)\n                    horizon_pred = (torch.round((resize_coordinates(horizon_pred) + 80)/10)*10).int()\n                    line_pred = torch.round(nn.Sigmoid()(outputs_line))\n                else:\n                    assert False\n\n                # Calculate X coordinates\n                x_cal0 = params.compute_coordinates(beta0)\n                x_cal1 = params.compute_coordinates(beta1)\n                x_cal2 = params.compute_coordinates(beta2)\n                x_cal3 = params.compute_coordinates(beta3)\n                lanes_pred = torch.stack((x_cal0, x_cal1, x_cal2, x_cal3), dim=1)\n\n                # Check line type branch\n                line_pred = line_pred[:, [1, 2, 0, 3]]\n                lanes_pred[(1 - line_pred[:, :, None]).byte().expand_as(lanes_pred)] = -2\n\n                # Check horizon branch\n                bounds = ((horizon_pred - 160) / 10)\n                for k, bound in enumerate(bounds):\n                    lanes_pred[k, :, :bound.item()] = -2\n\n                # TODO check intersections\n                lanes_pred[lanes_pred > 1279] = -2\n                lanes_pred[lanes_pred < 0] = -2\n\n                # Write predictions to json file\n                lanes_pred = np.int_(np.round(lanes_pred.data.cpu().numpy())).tolist()\n                num_el = input.size(0)\n\n                for j in range(num_el):\n                    lanes_to_write = lanes_pred[j]\n                    im_id = i*args.val_batch_size + j \n                    json_line = gt_lanes[im_id]\n                    json_line[""lanes""] = lanes_to_write\n                    json_line[""run_time""] = 20\n                    json.dump(json_line, jsonFile)\n                    jsonFile.write(\'\\n\')\n\n                    if args.draw_testset:\n                        test = weightmap_zeros[j]\n                        weight0= test[0]\n                        weight1= test[1]\n                        weight2= test[2]\n                        weight3= test[3]\n                        to_vis = weight0/weight0.max()+weight1/weight1.max()+weight2/weight2.max()+weight3/weight3.max()\n                        to_vis = to_vis.data.cpu().numpy()\n                        to_vis = Image.fromarray(to_vis)\n                        to_vis.save(os.path.join(args.save_path + \'/example/testset\', \'{}_vis.jpg\'.format(im_id))) \n                        im_path = json_line[\'raw_file\']\n                        img_name = os.path.join(args.test_dir, im_path)\n                        with open(img_name, \'rb\') as f:\n                            img = np.array(Image.open(f).convert(\'RGB\'))\n                        for lane_i in range(len(lanes_to_write)):\n                            x_orig = lanes_to_write[lane_i]\n                            pt_or = [(xcord, ycord) for (xcord, ycord) in zip(x_orig, json_line[\'h_samples\']) if xcord!=-2]\n                            for point in pt_or:\n                               img = cv2.circle(img, tuple(np.int32(point)), thickness=-1, color=colormap[lane_i], radius = 3)\n                        img = Image.fromarray(np.uint8(img))\n                        img.save(os.path.join(args.save_path + \'/example/testset\', \'{}.jpg\'.format(im_id))) \n\n\n        # Calculate accuracy\n        if args.clas and args.nclasses > 3:\n            acc_seg = LaneEval.bench_one_submit(test_set_file, gt_file)\n            print(acc_seg)\n            print(""===> Average ACC on TESTSET is {:.8} in {:.6}s for a batch"".format(acc_seg[0], batch_time.avg))\n    return acc_seg[0]\n\n\nclass Projections():\n    \'\'\'\n    Compute coordintes after backprojection to original perspective\n    \'\'\'\n    def __init__(self, options):\n        super(Projections, self).__init__()\n        M, M_inv = get_homography(resize=options.resize, no_mapping=False)\n        self.M, self.M_inv = torch.from_numpy(M), torch.from_numpy(M_inv)\n        start = 160\n        delta = 10\n        num_heights = (720-start)//delta\n        self.y_d = (torch.arange(start,720,delta)-80).double() / 2.5\n        self.ones = torch.ones(num_heights).double()\n        self.y_prime = (self.M[1,1:2]*self.y_d + self.M[1,2:])/(self.M[2,1:2]*self.y_d+self.M[2,2:])\n        self.y_eval = 255 - self.y_prime\n        self.Y = torch.stack((self.y_eval**2, self.y_eval, self.ones), 1)\n\n        if options.order == 0:\n            self.Y = self.tensor_ones\n        elif options.order == 1:\n            self.Y = torch.stack((self.y_eval, self.ones), 1)\n        elif options.order == 2:\n            self.Y = torch.stack((self.y_eval**2, self.y_eval, self.ones), 1)\n        elif options.order == 3:\n            self.Y = torch.stack((self.y_eval**3, self.y_eval**2, self.y_eval, self.ones), 1)\n        else:\n            raise NotImplementedError(\n                    \'Requested order {} for polynomial fit is not implemented\'.format(options.order))\n        self.Y = self.Y.unsqueeze(0).repeat(options.batch_size, 1, 1)\n        self.ones = torch.ones(options.batch_size, num_heights, 1).double()\n        self.y_prime = self.y_prime.unsqueeze(0).repeat(options.batch_size, 1).unsqueeze(2)\n        self.M_inv = self.M_inv.unsqueeze(0).repeat(options.batch_size, 1, 1)\n\n        # use gpu\n        self.M = self.M.cuda()\n        self.M_inv = self.M_inv.cuda()\n        self.y_prime = self.y_prime.cuda()\n        self.Y = self.Y.cuda()\n        self.ones = self.ones.cuda()\n\n    def compute_coordinates(self, params):\n        # Sample at y_d in the homography space\n        bs = params.size(0)\n        x_prime = torch.bmm(self.Y[:bs], params)\n\n        # Transform sampled points back\n        coordinates = torch.stack((x_prime, self.y_prime[:bs], self.ones[:bs]), 2).squeeze(3).permute((0, 2, 1))\n        trans = torch.bmm(self.M_inv[:bs], coordinates)\n        x_cal = trans[:,0,:]/trans[:,2,:]\n        # y_cal = trans[:,1,:]/trans[:,2,:] # sanity check\n\n        # Rezize\n        x_cal = resize_coordinates(x_cal)\n\n        return x_cal\n'"
Birds_Eye_View_Loss/Loss_crit.py,11,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nclass polynomial():\n    """"""\n    Polynomial class with exact integral calculation according to the\n    trapezium rule.\n    """"""\n    def __init__(self, coeffs, a=0, b=0.7, n=100):\n        self.a1, self.b1, self.c1 = torch.chunk(coeffs, 3, 1)\n        self.a1, self.b1, self.c1 = self.a1.squeeze(), self.b1.squeeze(), self.c1.squeeze()\n        self.a = a\n        self.b = b\n        self.n = n\n\n    def calc_pol(self, x):\n        return self.a1*x**2 + self.b1*x + self.c1\n\n    def trapezoidal(self, other):\n        h = float(self.b - self.a) / self.n\n        s = 0.0\n        s += abs(self.calc_pol(self.a)/2.0 - other.calc_pol(other.a)/2.0)\n        for i in range(1, self.n):\n            s += abs(self.calc_pol(self.a + i*h) - other.calc_pol(self.a + i*h))\n        s += abs(self.calc_pol(self.b)/2.0 - other.calc_pol(self.b)/2.0)\n        out = s*h\n        return out\n\n\n# pol1 = polynomial(coeffs=torch.FloatTensor([[0, 1, 0]]), a=-1, b=1)\n# pol2 = polynomial(coeffs=torch.FloatTensor([[0, 0, 0]]), a=-1, b=1)\n# pol1 = polynomial(coeffs=torch.FloatTensor([[0, 1, 0]]), a=0, b=1)\n# pol2 = polynomial(coeffs=torch.FloatTensor([[1, 0, 0]]), a=0, b=1)\n# print(\'Area by trapezium rule is {}\'.format(pol1.trapezoidal(pol2)))\n\n\ndef define_loss_crit(options):\n    \'\'\'\n    Define loss cirterium:\n        -MSE loss on curve parameters in ortho view\n        -MSE loss on points after backprojection to normal view\n        -Area loss\n    \'\'\'\n    if options.loss_policy == \'mse\':\n        loss_crit = MSE_Loss(options)\n    elif options.loss_policy == \'area\':\n        loss_crit = Area_Loss(options.order, options.weight_funct)\n    else:\n        return NotImplementedError(\'The requested loss criterion is not implemented\')\n    return loss_crit, CrossEntropyLoss2d(options.weight_seg, seg=True)\n\n\nclass CrossEntropyLoss2d(nn.Module):\n    \'\'\'\n    Standard 2d cross entropy loss on all pixels of image\n    My implemetation (but since Pytorch 0.2.0 libs have their\n    owm optimized implementation, consider using theirs)\n    \'\'\'\n    def __init__(self, weight=None, size_average=True, seg=False):\n        if seg:\n            weights = torch.Tensor([1] + [weight]*(2))\n            weights = weights.cuda()\n        super(CrossEntropyLoss2d, self).__init__()\n        self.nll_loss = nn.NLLLoss2d(weights, size_average)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss(F.log_softmax(inputs, dim=1), targets[:, 0, :, :])\n\n\nclass Area_Loss(nn.Module):\n    \'\'\'\n    Compute area between curves by integrating (x1 - x2)^2 over y\n    *Area:\n        *order 0: int((c1 - c2)**2)dy\n        *order 1: int((b1*y - b2*y + c1 - c2)**2)dy\n        *order 2: int((a1*y**2 - a2*y**2 + b1*y - b2*y + c1 - c2)**2)dy\n\n    *A weight function W can be added:\n        Weighted area: int(W(y)*diff**2)dy\n        with W(y):\n            *1\n            *(1-y)\n            *(1-y**0.5)\n    \'\'\'\n    def __init__(self, order, weight_funct):\n        super(Area_Loss, self).__init__()\n        self.order = order\n        self.weight_funct = weight_funct\n\n    def forward(self, params, gt_params, compute=True):\n        diff = params.squeeze(-1) - gt_params\n        a = diff[:, 0]\n        b = diff[:, 1]\n        t = 0.7 # up to which y location to integrate\n        if self.order == 2:\n            c = diff[:, 2]\n            if self.weight_funct == \'none\':\n                # weight (1)\n                loss_fit = (a**2)*(t**5)/5+2*a*b*(t**4)/4 + \\\n                           (b**2+c*2*a)*(t**3)/3+2*b*c*(t**2)/2+(c**2)*t\n            elif self.weight_funct == \'linear\':\n                # weight (1-y)\n                loss_fit = c**2*t - t**5*((2*a*b)/5 - a**2/5) + \\\n                           t**2*(b*c - c**2/2) - (a**2*t**6)/6 - \\\n                           t**4*(b**2/4 - (a*b)/2 + (a*c)/2) + \\\n                           t**3*(b**2/3 - (2*c*b)/3 + (2*a*c)/3)\n            elif self.weight_funct == \'quadratic\':\n                # weight (1-y**0.5)\n                loss_fit = t**3*(1/3*b**2 + 2/3*a*c) - \\\n                           t**(7/2)*(2/7*b**2 + 4/7*a*c) + \\\n                           c**2*t + 0.2*a**2*t**5 - 2/11*a**2*t**(11/2) - \\\n                           2/3*c**2*t**(3/2) + 0.5*a*b*t**4 - \\\n                           4/9*a*b*t**(9/2) + b*c*t**2 - 0.8*b*c*t**(5/2)\n            else:\n                return NotImplementedError(\'The requested weight function is \\\n                        not implemented, only order 1 or order 2 possible\')\n        elif self.order == 1:\n            loss_fit = (b**2)*t + a*b*(t**2) + ((a**2)*(t**3))/3\n        else:\n            return NotImplementedError(\'The requested order is not implemented, only none, linear or quadratic possible\')\n\n        # Mask select if lane is present\n        mask = torch.prod(gt_params != 0, 1).byte()\n        loss_fit = torch.masked_select(loss_fit, mask)\n        loss_fit = loss_fit.mean(0) if loss_fit.size()[0] != 0 else 0 # mean over the batch\n        return loss_fit\n\n\nclass MSE_Loss(nn.Module):\n    \'\'\'\n    Compute mean square error loss on curve parameters\n    in ortho or normal view\n    \'\'\'\n    def __init__(self, options):\n        super(MSE_Loss, self).__init__()\n        self.loss_crit = nn.MSELoss()\n        if not options.no_cuda:\n            self.loss_crit = self.loss_crit.cuda()\n\n    def forward(self, params, gt_params, compute=True):\n        loss = self.loss_crit(params.squeeze(-1), gt_params)\n        return loss\n'"
Birds_Eye_View_Loss/eval_lane.py,0,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport ujson as json\n\nclass LaneEval(object):\n    lr = LinearRegression()\n    pixel_thresh = 20\n    pt_thresh = 0.85\n\n    @staticmethod\n    def get_angle(xs, y_samples):\n        xs, ys = xs[xs >= 0], y_samples[xs >= 0]\n        if len(xs) > 1:\n            LaneEval.lr.fit(ys[:, None], xs)\n            k = LaneEval.lr.coef_[0]\n            theta = np.arctan(k)\n        else:\n            theta = 0\n        return theta\n\n    @staticmethod\n    def line_accuracy(pred, gt, thresh):\n        pred = np.array([p if p >= 0 else -100 for p in pred])\n        gt = np.array([g if g >= 0 else -100 for g in gt])\n        return np.sum(np.where(np.abs(pred - gt) < thresh, 1., 0.)) / len(gt)\n\n    @staticmethod\n    def bench(pred, gt, y_samples, running_time):\n        if any(len(p) != len(y_samples) for p in pred):\n            raise Exception(\'Format of lanes error.\')\n        if running_time > 200 or len(gt) + 2 < len(pred):\n            return 0., 0., 1.\n        angles = [LaneEval.get_angle(np.array(x_gts), np.array(y_samples)) for x_gts in gt]\n        threshs = [LaneEval.pixel_thresh / np.cos(angle) for angle in angles]\n        line_accs = []\n        fp, fn = 0., 0.\n        matched = 0.\n        for x_gts, thresh in zip(gt, threshs):\n            accs = [LaneEval.line_accuracy(np.array(x_preds), np.array(x_gts), thresh) for x_preds in pred]\n            max_acc = np.max(accs) if len(accs) > 0 else 0.\n            if max_acc < LaneEval.pt_thresh:\n                fn += 1\n            else:\n                matched += 1\n            line_accs.append(max_acc)\n        fp = len(pred) - matched\n        if len(gt) > 4 and fn > 0:\n            fn -= 1\n        s = sum(line_accs)\n        if len(gt) > 4:\n            s -= min(line_accs)\n        return s / max(min(4.0, len(gt)), 1.), fp / len(pred) if len(pred) > 0 else 0., fn / max(min(len(gt), 4.) , 1.)\n\n    @staticmethod\n    def bench_one_submit(pred_file, gt_file):\n        try:\n            json_pred = [json.loads(line) for line in open(pred_file).readlines()]\n        except BaseException as e:\n            raise Exception(\'Fail to load json file of the prediction.\')\n        json_gt = [json.loads(line) for line in open(gt_file).readlines()]\n        if len(json_gt) != len(json_pred):\n            raise Exception(\'We do not get the predictions of all the test tasks\')\n        gts = {l[\'raw_file\']: l for l in json_gt}\n        accuracy, fp, fn = 0., 0., 0.\n        for pred in json_pred:\n            if \'raw_file\' not in pred or \'lanes\' not in pred or \'run_time\' not in pred:\n                raise Exception(\'raw_file or lanes or run_time not in some predictions.\')\n            raw_file = pred[\'raw_file\']\n            pred_lanes = pred[\'lanes\']\n            run_time = pred[\'run_time\']\n            if raw_file not in gts:\n                raise Exception(\'Some raw_file from your predictions do not exist in the test tasks.\')\n            gt = gts[raw_file]\n            gt_lanes = gt[\'lanes\']\n            y_samples = gt[\'h_samples\']\n            try:\n                a, p, n = LaneEval.bench(pred_lanes, gt_lanes, y_samples, run_time)\n            except BaseException as e:\n                raise Exception(\'Format of lanes error.\')\n            accuracy += a\n            fp += p\n            fn += n\n        num = len(gts)\n        # the first return parameter is the default ranking parameter\n#        return json.dumps([\n#            {\'name\': \'Accuracy\', \'value\': accuracy / num, \'order\': \'desc\'},\n#            {\'name\': \'FP\', \'value\': fp / num, \'order\': \'asc\'},\n#            {\'name\': \'FN\', \'value\': fn / num, \'order\': \'asc\'}\n#        ])\n        return [accuracy / num, fp / num, fn / num]\n\n\nif __name__ == \'__main__\':\n    import sys\n    try:\n        if len(sys.argv) != 3:\n            raise Exception(\'Invalid input arguments\')\n        print (LaneEval.bench_one_submit(sys.argv[1], sys.argv[2]))\n    except Exception as e:\n        print (e.message)\n        sys.exit(e.message)\n'"
Birds_Eye_View_Loss/main.py,27,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nimport torch\nimport torch.optim\nimport torch.nn as nn\n\nimport os\nimport glob\nimport time\nimport sys\nimport shutil\nimport json\nfrom tqdm import tqdm\nfrom tensorboardX import SummaryWriter\n\nfrom Dataloader.Load_Data_new import get_loader, get_homography, \\\n                                     load_valid_set_file_all, write_lsq_results\nfrom eval_lane import LaneEval\nfrom Loss_crit import define_loss_crit, polynomial \nfrom Networks.LSQ_layer import Net\nfrom Networks.utils import define_args, save_weightmap, first_run,\\\n                           mkdir_if_missing, Logger, define_init_weights,\\\n                           define_scheduler, define_optim, AverageMeter \\\n\n\n\ndef main():\n    global args\n    global mse_policy\n    parser = define_args()\n    args = parser.parse_args()\n    if not args.end_to_end:\n        assert args.pretrained == False\n    mse_policy = args.loss_policy == \'homography_mse\'\n    if args.clas:\n        assert args.nclasses == 4\n\n    # Check GPU availability\n    if not args.no_cuda and not torch.cuda.is_available():\n        raise Exception(""No gpu available for usage"")\n    torch.backends.cudnn.benchmark = args.cudnn\n\n    # Define save path\n    save_id = \'Mod_{}_opt_{}_loss_{}_lr_{}_batch_{}_end2end_{}_lanes_{}_resize_{}_pretrain{}_clas{}\' \\\n            .format(args.mod, args.optimizer,\n                    args.loss_policy,\n                    args.learning_rate,\n                    args.batch_size,\n                    args.end_to_end,\n                    args.nclasses,\n                    args.resize,\n                    args.pretrained,\n                    args.clas)\n\n    # Get homography\n    M_inv = get_homography(args.resize)\n\n\n    # Dataloader for training and validation set\n    train_loader, valid_loader, valid_idx = get_loader(args.num_train,\n                                                       args.json_file, args.image_dir, \n                                                       args.gt_dir,\n                                                       args.flip_on, args.batch_size,\n                                                       shuffle=True, num_workers=args.nworkers,\n                                                       end_to_end=args.end_to_end,\n                                                       resize=args.resize,\n                                                       split_percentage=args.split_percentage)\n\n    # Define network\n    model = Net(args)\n    define_init_weights(model, args.weight_init)\n\n    if not args.no_cuda:\n        # Load model on gpu before passing params to optimizer\n        model = model.cuda()\n\n    # Define optimizer and scheduler\n    optimizer = define_optim(args.optimizer, model.parameters(),\n                             args.learning_rate, args.weight_decay)\n    scheduler = define_scheduler(optimizer, args)\n\n    # Define loss criteria for multiple tasks\n    criterion, criterion_seg = define_loss_crit(args)\n    criterion_line_class = nn.CrossEntropyLoss().cuda()\n    criterion_horizon = nn.BCEWithLogitsLoss().cuda()\n\n    # Name\n    global crit_string\n    crit_string = \'AREA**2\' if args.end_to_end else \'ENTROPY\'\n    if args.clas:\n        crit_string = \'TOT LOSS\' \n\n    # Logging setup\n    best_epoch = 0\n    lowest_loss = np.inf\n    log_file_name = \'log_train_start_0.txt\'\n    args.save_path = os.path.join(args.save_path, save_id)\n    mkdir_if_missing(args.save_path)\n    mkdir_if_missing(os.path.join(args.save_path, \'example/\'))\n    mkdir_if_missing(os.path.join(args.save_path, \'example/train\'))\n    mkdir_if_missing(os.path.join(args.save_path, \'example/valid\'))\n\n    # Computes the file with lane data of the validation set\n    validation_set_path = os.path.join(args.save_path , \'validation_set.json\')\n    load_valid_set_file_all(valid_idx, validation_set_path, args.image_dir) \n    global valid_set_labels\n    global val_set_path\n    global ls_result_path\n    valid_set_labels = [json.loads(line) for line in open(validation_set_path).readlines()]\n    val_set_path = os.path.join(args.save_path, \'validation_set_dst.json\')\n    ls_result_path = os.path.join(args.save_path, \'ls_result.json\')\n\n    # Tensorboard writer\n    if not args.no_tb:\n        global writer\n        writer = SummaryWriter(os.path.join(args.save_path, \'Tensorboard/\'))\n    # Train, evaluate or resume\n    args.resume = first_run(args.save_path)\n    if args.resume and not args.test_mode and not args.evaluate:\n        path = os.path.join(args.save_path, \'checkpoint_model_epoch_{}.pth.tar\'.format(\n            int(args.resume)))\n        if os.path.isfile(path):\n            log_file_name = \'log_train_start_{}.txt\'.format(args.resume)\n            # Redirect stdout\n            sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(path)\n            args.start_epoch = checkpoint[\'epoch\']\n            lowest_loss = checkpoint[\'loss\']\n            best_epoch = checkpoint[\'best epoch\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            log_file_name = \'log_train_start_0.txt\'\n            # Redirect stdout\n            sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n            print(""=> no checkpoint found at \'{}\'"".format(path))\n\n    # Only evaluate\n    elif args.evaluate:\n        best_file_name = glob.glob(os.path.join(args.save_path, \'model_best*\'))[0]\n        if os.path.isfile(best_file_name):\n            sys.stdout = Logger(os.path.join(args.save_path, \'Evaluate.txt\'))\n            print(""=> loading checkpoint \'{}\'"".format(best_file_name))\n            checkpoint = torch.load(best_file_name)\n            model.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(best_file_name))\n        validate(valid_loader, model, criterion, criterion_seg, \n                criterion_line_class, criterion_horizon, M_inv)\n        return\n\n    # Start training from clean slate\n    else:\n        # Redirect stdout\n        sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n\n    # INIT MODEL\n    print(40*""=""+""\\nArgs:{}\\n"".format(args)+40*""="")\n    print(""Init model: \'{}\'"".format(args.mod))\n    print(""Number of parameters in model {} is {:.3f}M"".format(\n        args.mod.upper(), sum(tensor.numel() for tensor in model.parameters())/1e6))\n\n    # Start training and validation for nepochs\n    for epoch in range(args.start_epoch, args.nepochs):\n        print(""\\n => Start train set for EPOCH {}"".format(epoch + 1))\n        # Adjust learning rate\n        if args.lr_policy is not None and args.lr_policy != \'plateau\':\n            scheduler.step()\n            lr = optimizer.param_groups[0][\'lr\']\n            print(\'lr is set to {}\'.format(lr))\n\n        if args.pretrained:\n            if (epoch < args.pretrain_epochs):\n                args.end_to_end = False\n                print(""Pretraining so set args.end_to_end to {}"".format(args.end_to_end))\n            else:\n                args.end_to_end = True\n\n        # Define container objects to keep track of multiple losses/metrics\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        avg_area = AverageMeter()\n        exact_area = AverageMeter()\n\n        # Specfiy operation modus\n        model.train()\n\n        # compute timing\n        end = time.time()\n\n        # Start training loop\n        for i, (input, gt, params, idx, gt_line, gt_horizon) in tqdm(enumerate(train_loader)):\n\n            # Time dataloader\n            data_time.update(time.time() - end)\n\n            # Put inputs on gpu if possible\n            if not args.no_cuda:\n                input, params = input.cuda(non_blocking=True), params.cuda(non_blocking=True)\n                input = input.float()\n            assert params.size(1) == 4\n            gt0, gt1, gt2, gt3 = params[:, 0, :], params[:, 1, :], params[:, 2, :], params[:, 3, :]\n\n            # Run model\n            try:\n                beta0, beta1, beta2, beta3, weightmap_zeros, M, \\\n                output_net, outputs_line, outputs_horizon = model(input, args.end_to_end)\n            except RuntimeError as e:\n                print(""Batch with idx {} skipped due to singular matrix"".format(idx.numpy()))\n                print(e)\n                continue\n\n            # Compute losses on parameters or on segmentation\n            if args.end_to_end:\n                loss = criterion(beta0, gt0) + criterion(beta1, gt1)\n                if args.nclasses > 3:\n                    # Masks to create zero in the loss when lane line is not present\n                    mask_llhs = torch.prod(gt2 != 0, 1) \\\n                            .unsqueeze(1).unsqueeze(1).expand_as(beta2).type(torch.FloatTensor)\n                    mask_rrhs = torch.prod(gt3 != 0, 1) \\\n                            .unsqueeze(1).unsqueeze(1).expand_as(beta3).type(torch.FloatTensor)\n                    if not args.no_cuda:\n                        mask_llhs = mask_llhs.cuda()\n                        mask_rrhs = mask_rrhs.cuda()\n                    beta2 = beta2*mask_llhs\n                    beta3 = beta3*mask_rrhs\n\n                    # add losses of further lane lines\n                    loss += criterion(beta2, gt2) + criterion(beta3, gt3)\n\n            else:\n                gt = gt.cuda(non_blocking=True)\n                loss = criterion_seg(output_net, gt)\n                with torch.no_grad():\n                    area = criterion(beta0, gt0) + criterion(beta1, gt1)\n                    avg_area.update(area.item(), input.size(0))\n\n            # Horizon task & Line classification task\n            if args.clas:\n                gt_horizon, gt_line = gt_horizon.cuda(non_blocking=True), \\\n                                      gt_line.cuda(non_blocking=True)\n                _, line_pred = torch.max(outputs_line, 1)\n                loss_horizon = criterion_horizon(outputs_horizon, gt_horizon)\n                loss_line = criterion_line_class(outputs_line, gt_line)\n                loss = loss*args.weight_fit + (loss_line + loss_horizon)*args.weight_class\n            else:\n                line_pred = gt_line\n\n            losses.update(loss.item(), input.size(0))\n\n            # Clip gradients (usefull for instabilities or mistakes in ground truth)\n            if args.clip_grad_norm != 0:\n                nn.utils.clip_grad_norm(model.parameters(), args.clip_grad_norm)\n\n            # Setup backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Time trainig iteration\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            # Exact AREA computation for egolines on cpu\n            with torch.no_grad():\n                gt_left_lines = polynomial(gt0.cpu())\n                gt_right_lines = polynomial(gt1.cpu())\n                pred_left_lines = polynomial(beta0.cpu())\n                pred_right_lines = polynomial(beta1.cpu())\n                trap_left = pred_left_lines.trapezoidal(gt_left_lines)\n                trap_right = pred_right_lines.trapezoidal(gt_right_lines)\n                exact_area.update(((trap_left + trap_right)/2).mean().item(), input.size(0))\n\n            # Print info\n            if (i + 1) % args.print_freq == 0:\n                print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.8f} ({loss.avg:.8f})\'.format(\n                       epoch+1, i+1, len(train_loader), batch_time=batch_time,\n                       data_time=data_time, loss=losses))\n\n            # Plot weightmap and curves\n            if (i + 1) % args.save_freq == 0:\n                save_weightmap(\'train\', M, M_inv,\n                               weightmap_zeros, beta0, beta1, beta2, beta3,\n                               gt0, gt1, gt2, gt3, line_pred, gt, 0, i, input,\n                               args.no_ortho, args.resize, args.save_path)\n\n        losses_valid, avg_area_valid, avg_exact_area, \\\n        acc_hor_tot, acc_line_tot = validate(valid_loader,\n                                             model, criterion,\n                                             criterion_seg, \n                                             criterion_line_class,\n                                             criterion_horizon,\n                                             M_inv,\n                                             epoch)\n        if not args.end_to_end:\n            print(""===> Average AREA**2 from segmentation  on training set is {:.8f}"" \\\n                    .format(avg_area.avg))\n            print(""===> Average AREA**2 from segmetnation validation set is {:.8f}"" \\\n                    .format(avg_area_valid))\n        print(""===> Average {}-loss on training set is {:.8f}"".format(crit_string, losses.avg))\n        print(""===> Average {}-loss on validation set is {:.8f}"".format(crit_string, losses_valid))\n        print(""===> Average exact area on training set is {:.8f}"".format(exact_area.avg))\n        print(""===> Average exact area on validation set is {:.8f}"".format(avg_exact_area))\n\n        if args.clas:\n            print(""===> Average HORIZON ACC on val is {:.8}"".format(acc_hor_tot))\n            print(""===> Average LINE ACC on val is {:.8}"".format(acc_line_tot))\n\n        print(""===> Last best {}-loss was {:.8f} in epoch {}"".format(\n            crit_string, lowest_loss, best_epoch))\n\n        if not args.no_tb:\n            if args.end_to_end:\n                writer.add_scalars(\'Loss/Area**2\', {\'Training\': losses.avg}, epoch)\n                writer.add_scalars(\'Loss/Area**2\', {\'Validation\': losses_valid}, epoch)\n            else:\n                writer.add_scalars(\'Loss/Area**2\', {\'Training\': avg_area.avg}, epoch)\n                writer.add_scalars(\'Loss/Area**2\', {\'Validation\': avg_area_valid}, epoch)\n                writer.add_scalars(\'CROSS-ENTROPY\', {\'Training\': losses.avg}, epoch)\n                writer.add_scalars(\'CROSS-ENTROPY\', {\'Validation\': losses_valid}, epoch)\n            writer.add_scalars(\'Metric\', {\'Training\': exact_area.avg}, epoch)\n            writer.add_scalars(\'Metric\', {\'Validation\': avg_exact_area}, epoch)\n\n        total_score = avg_exact_area\n\n        # Adjust learning_rate if loss plateaued\n        if args.lr_policy == \'plateau\':\n            scheduler.step(total_score)\n            lr = optimizer.param_groups[0][\'lr\']\n            print(\'LR plateaued, hence is set to {}\'.format(lr))\n\n        # File to keep latest epoch\n        with open(os.path.join(args.save_path, \'first_run.txt\'), \'w\') as f:\n            f.write(str(epoch))\n        # Save model\n        to_save = False\n        if total_score < lowest_loss:\n            to_save = True\n            best_epoch = epoch+1\n            lowest_loss = total_score\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'best epoch\': best_epoch,\n            \'arch\': args.mod,\n            \'state_dict\': model.state_dict(),\n            \'loss\': lowest_loss,\n            \'optimizer\': optimizer.state_dict()}, to_save, epoch)\n    if not args.no_tb:\n        writer.close()\n\n\ndef validate(loader, model, criterion, criterion_seg, \n            criterion_line_class, criterion_horizon, M_inv, epoch=0):\n\n    # Define container to keep track of metric and loss\n    losses = AverageMeter()\n    avg_area = AverageMeter()\n    avg_trapezium_rule = AverageMeter()\n    acc_hor_tot = AverageMeter()\n    acc_line_tot = AverageMeter()\n\n    # Evaluate model\n    model.eval()\n\n    # Only forward pass, hence no gradients needed\n    with torch.no_grad():\n        \n        # Start validation loop\n        for i, (input, gt, params, idx, gt_line, gt_horizon, index) in tqdm(enumerate(loader)):\n            if not args.no_cuda:\n                input, params = input.cuda(non_blocking=True), params.cuda(non_blocking=True)\n                input = input.float()\n            gt0, gt1, gt2, gt3 = params[:, 0, :], params[:, 1, :], params[:, 2, :], params[:, 3, :]\n\n            # Evaluate model\n            try:\n                beta0, beta1, beta2, beta3, weightmap_zeros, M, \\\n                output_net, outputs_line, outputs_horizon = model(input, args.end_to_end)\n            except RuntimeError as e:\n                print(""Batch with idx {} skipped due to singular matrix"".format(idx.numpy()))\n                print(e)\n                continue\n\n            # Compute losses on parameters or segmentation\n            if args.end_to_end:\n                loss = criterion(beta0, gt0) + criterion(beta1, gt1)\n                if args.nclasses > 3:\n                    # Masks to create zero in the loss when lane line is not present\n                    mask_llhs = torch.prod(gt2 != 0, 1) \\\n                            .unsqueeze(1).unsqueeze(1).expand_as(beta2).type(torch.FloatTensor)\n                    mask_rrhs = torch.prod(gt3 != 0, 1) \\\n                            .unsqueeze(1).unsqueeze(1).expand_as(beta3).type(torch.FloatTensor)\n                    if not args.no_cuda:\n                        mask_llhs = mask_llhs.cuda()\n                        mask_rrhs = mask_rrhs.cuda()\n                    beta2 = beta2*mask_llhs\n                    beta3 = beta3*mask_rrhs\n\n                    # add losses of further lane lines\n                    loss += criterion(beta2, gt2) + criterion(beta3, gt3)\n            else:\n                gt = gt.cuda(non_blocking=True)\n                loss = criterion_seg(output_net, gt)\n                area = criterion(beta0, gt0) + criterion(beta1, gt1)\n                avg_area.update(area.item(), input.size(0))\n\n            # Horizon task & Line classification task\n            if args.clas:\n                gt_horizon, gt_line = gt_horizon.cuda(non_blocking=True), \\\n                                      gt_line.cuda(non_blocking=True)\n                horizon_pred = torch.round(nn.Sigmoid()(outputs_horizon))\n                acc = torch.eq(horizon_pred, gt_horizon)\n                acc_hor = torch.sum(acc).float()/(args.resize*args.batch_size)\n                acc_hor_tot.update(acc_hor.item())\n                _, line_pred = torch.max(outputs_line, 1)\n                acc = torch.eq(line_pred, gt_line)\n                acc_line = torch.sum(acc).float()/(args.nclasses*args.batch_size)\n                acc_line_tot.update(acc_line.item())\n                loss_horizon = criterion_horizon(outputs_horizon, gt_horizon)\n                loss_line = criterion_line_class(outputs_line, gt_line)\n                loss = loss*args.weight_fit + (loss_line + loss_horizon)*args.weight_class\n            else:\n                line_pred = gt_line\n\n            # Exact area computation\n            gt_left_lines = polynomial(gt0.cpu())\n            gt_right_lines = polynomial(gt1.cpu())\n            pred_left_lines = polynomial(beta0.cpu())\n            pred_right_lines = polynomial(beta1.cpu())\n            trap_left = pred_left_lines.trapezoidal(gt_left_lines)\n            trap_right = pred_right_lines.trapezoidal(gt_right_lines)\n            avg_trapezium_rule.update(((trap_left + trap_right)/2).mean().item(), input.size(0))\n            losses.update(loss.item(), input.size(0))\n\n            #Write predictions to json file\n            if args.clas:\n                num_el = input.size(0)\n                if args.nclasses > 2:\n                    params_batch = torch.cat((beta0, beta1, beta2, beta3),2) \\\n                        .transpose(1, 2).data.tolist()\n                else: \n                    params_batch = torch.cat((beta0, beta1),2).transpose(1, 2).data.tolist()\n                    \n                line_type = line_pred.data.tolist()\n                horizon_pred = horizon_pred.data.tolist()\n                with open(val_set_path, \'w\') as jsonFile:\n                    for j in range(num_el):\n                        im_id = index[j]\n                        json_line = valid_set_labels[im_id]\n                        line_id = line_type[j]\n                        horizon_est = horizon_pred[j]\n                        params = params_batch[j]\n                        json_line[""params""] = params\n                        json_line[""line_id""] = line_id\n                        json_line[""horizon_est""] = horizon_est\n                        json.dump(json_line, jsonFile)\n                        jsonFile.write(\'\\n\')\n\n            # Print info\n            if (i + 1) % args.print_freq == 0:\n                    print(\'Test: [{0}/{1}]\\t\'\n                          \'Loss {loss.val:.8f} ({loss.avg:.8f})\\t\'\n                          \'Area {metric.val:.8f} ({metric.avg:.8f})\'.format(\n                           i+1, len(loader), loss=losses, metric=avg_area))\n\n            # Plot weightmap and curves\n            if (i + 1) % 25 == 0:\n                save_weightmap(\'valid\', M, M_inv,\n                               weightmap_zeros, beta0, beta1, beta2, beta3,\n                               gt0, gt1, gt2, gt3, line_pred, gt, 0, i, input,\n                               args.no_ortho, args.resize, args.save_path)\n\n        # Compute x, y coordinates for accuracy later\n        if args.clas and args.nclasses > 3:\n            write_lsq_results(val_set_path, ls_result_path, args.nclasses, \n                    False, False, args.resize, no_ortho=args.no_ortho)\n            acc_seg = LaneEval.bench_one_submit(ls_result_path, val_set_path)\n            print(""===> Average ACC_SEG on val is {:.8}"".format(acc_seg[0]))\n\n        if args.evaluate:\n            print(""===> Average {}-loss on validation set is {:.8}"".format(crit_string, \n                                                                           losses.avg))\n            print(""===> Average exact area on validation set is {:.8}"".format(\n                avg_trapezium_rule.avg))\n            if not args.end_to_end:\n                print(""===> Average area**2 on validation set is {:.8}"".format(avg_area.avg))\n            if args.clas:\n                print(""===> Average HORIZON ACC on val is {:.8}"".format(acc_hor_tot.avg))\n                print(""===> Average LINE ACC on val is {:.8}"".format(acc_hor_tot.avg))\n\n        return losses.avg, avg_area.avg, avg_trapezium_rule.avg, acc_hor_tot.avg, acc_line_tot.avg\n\n\ndef save_checkpoint(state, to_copy, epoch):\n    filepath = os.path.join(args.save_path, \'checkpoint_model_epoch_{}.pth.tar\'.format(epoch))\n    torch.save(state, filepath)\n    if to_copy:\n        if epoch > 0:\n            lst = glob.glob(os.path.join(args.save_path, \'model_best*\'))\n            if len(lst) != 0:\n                os.remove(lst[0])\n        shutil.copyfile(filepath, os.path.join(args.save_path, \n            \'model_best_epoch_{}.pth.tar\'.format(epoch)))\n        print(""Best model copied"")\n    if epoch > 0:\n        prev_checkpoint_filename = os.path.join(args.save_path, \n                \'checkpoint_model_epoch_{}.pth.tar\'.format(epoch-1))\n        if os.path.exists(prev_checkpoint_filename):\n            os.remove(prev_checkpoint_filename)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Backprojection_Loss/Dataloader/Load_Data_new.py,9,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport os\nimport torch\nfrom torch.autograd import Variable\nfrom PIL import Image, ImageOps\nimport cv2\nimport json\nimport numbers\nimport random\nimport warnings\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\nfrom torch.utils.data.dataloader import default_collate\nwarnings.simplefilter(\'ignore\', np.RankWarning)\nfrom torch.utils.data.sampler import Sampler\n\n\n\n\ndef get_testloader(path, batch_size, num_workers, resize=256):\n    json_file = os.path.join(path, \'test_label.json\')\n    transformed_dataset = LaneTestSet(gt_file=json_file,\n                                      path=path,\n                                      resize=resize)\n\n    data_loader = DataLoader(dataset=transformed_dataset, \n                                          batch_size=batch_size,\n                                          shuffle=False,\n                                          num_workers=num_workers,\n                                          drop_last=False)\n    return data_loader\n\n\nclass LaneTestSet(Dataset):\n    \'\'\'Dataset used as testset\'\'\'\n    def __init__(self, gt_file, resize, path):\n        self.img_info = [json.loads(line) for line in open(gt_file,\'r\')]\n        print(\'size test loader: \', len(self.img_info))\n        self.path = path\n        self.resize = resize\n        self.totensor = transforms.ToTensor()\n\n    def __len__(self):\n        return len(self.img_info)\n\n    def __getitem__(self, idx):\n        im_path = self.img_info[idx][\'raw_file\']\n        img_name = os.path.join(self.path, im_path)\n        with open(img_name, \'rb\') as f:\n            image = Image.open(f).convert(\'RGB\')\n\n        # Crop and resize images\n        w, h = image.size\n        image = F.crop(image, h-640, 0, 640, w)\n        image = F.resize(image, size=(self.resize, 2*self.resize), interpolation=Image.BILINEAR)\n        image = self.totensor(image).float()\n        return image\n\n\nclass LaneDataset(Dataset):\n    """"""Dataset with labeled lanes""""""\n    def __init__(self, end_to_end, valid_idx, json_file, lanes_file, image_dir, gt_dir, flip_on, resize, nclasses):\n        """"""\n        Args: valid_idx (list)  : Indices of validation images\n              json_file (string): File with labels\n              image_dir (string): Path to the images\n              gt_dir    (string): Path to the ground truth images\n              flip on   (bool)  : Boolean to flip images randomly\n              end_to_end (bool) : Boolean to compute lane line params end to end\n              resize    (int)   : Height of resized image\n        """"""\n        line_file = \'Labels/label_new.json\'\n        self.image_dir = image_dir\n        self.gt_dir = gt_dir\n        self.valid_idx = valid_idx\n        self.flip_on = flip_on\n        self.resize = resize\n        self.totensor = transforms.ToTensor()\n        self.params = [json.loads(line) for line in open(json_file).readlines()]\n        self.ordered_lanes = [json.loads(line) for line in open(lanes_file).readlines()]\n        self.line_file = [json.loads(line) for line in open(line_file).readlines()]\n        self.end_to_end = end_to_end\n        self.rgb_lst = sorted(os.listdir(image_dir))\n        self.gt_lst = sorted(os.listdir(gt_dir))\n        self.num_imgs = len(self.rgb_lst)\n        assert len(self.rgb_lst) == len(self.gt_lst) == 3626\n\n        target_idx = [int(i.split(\'.\')[0]) for i in self.rgb_lst]\n        self.valid_idx = [target_idx[i]-1 for i in valid_idx]\n        self.num_points = 56\n        self.nclasses = nclasses\n        print(\'Flipping images randomly:\', self.flip_on)\n        print(\'End to end lane detection:\', self.end_to_end)\n\n    def __len__(self):\n        """"""\n        Conventional len method\n        """"""\n        return self.num_img\n\n    def __getitem__(self, idx):\n        """"""\n        Args: idx (int): Index in list to load image\n        """"""\n        # Load img, gt, x_coordinates, y_coordinates, line_type\n        assert self.rgb_lst[idx].split(\'.\')[0] == self.gt_lst[idx].split(\'.\')[0]\n        img_name = os.path.join(self.image_dir, self.rgb_lst[idx])\n        gt_name = os.path.join(self.gt_dir, self.gt_lst[idx])\n        with open(img_name, \'rb\') as f:\n            image = (Image.open(f).convert(\'RGB\'))\n        with open(gt_name, \'rb\') as f:\n            gt = (Image.open(f).convert(\'P\'))\n        idx = int(self.rgb_lst[idx].split(\'.\')[0]) - 1\n        lanes_lst = self.ordered_lanes[idx][""lanes""]\n        h_samples = self.ordered_lanes[idx][""h_samples""]\n        line_lst = self.line_file[idx][""lines""]\n\n        # Crop and resize images\n        w, h = image.size\n        image, gt = F.crop(image, h-640, 0, 640, w), F.crop(gt, h-640, 0, 640, w)\n        image = F.resize(image, size=(self.resize, 2*self.resize), interpolation=Image.BILINEAR)\n        gt = F.resize(gt, size=(self.resize, 2*self.resize), interpolation=Image.NEAREST)\n\n        # Adjust size of lanes matrix\n        # lanes = np.array(lanes_lst)[:, -self.num_points:]\n        lanes = np.array(lanes_lst)\n        to_add = np.full((4, 56 - lanes.shape[1]), -2)\n        lanes = np.hstack((to_add, lanes))\n\n        # Get valid coordinates from lanes matrix\n        valid_points = np.int32(lanes>0)\n        valid_points[:, :8] = 0 # start from h-samples = 210\n\n        # Resize coordinates\n        lanes = lanes/2.5\n        track = lanes < 0\n        h_samples = np.array(h_samples)/2.5 - 32\n        lanes[track] = -2\n\n        # Compute horizon for resized img\n        horizon_lanes = []\n        for lane in lanes:\n            horizon_lanes.append(min([y_cord for (x_cord, y_cord) in zip(lane, h_samples) if x_cord != -2] or [self.resize]))\n        y_val = min(horizon_lanes)\n        horizon = torch.zeros(gt.size[1])\n        horizon[0:int(np.floor(y_val))] = 1\n\n        # Compute line type in image\n        # line_lst = np.prod(lanes == -2, axis=1) # 1 when line is not present\n\n        gt = np.array(gt)\n        idx3 = np.isin(gt, 3)\n        idx4 = np.isin(gt, 4)\n        if self.nclasses < 3:\n            gt[idx3] = 0\n            gt[idx4] = 0\n        # Flip ground truth ramdomly\n        hflip_input = np.random.uniform(0.0, 1.0) > 0.5 and self.flip_on\n        if idx not in self.valid_idx and hflip_input:\n            image, gt = F.hflip(image), np.flip(gt, axis=1)\n            idx1 = np.isin(gt, 1)\n            idx2 = np.isin(gt, 2)\n            gt[idx1] = 2\n            gt[idx2] = 1\n            gt[idx3] = 4\n            gt[idx4] = 3\n            lanes = (2*self.resize - 1) - lanes\n            lanes[track] = -2\n            lanes = lanes[[1, 0, 3, 2]]\n            # line_lst = np.prod(lanes == -2, axis=1)\n            line_lst = mirror_list(line_lst)\n\n        # Get Tensors\n        gt = Image.fromarray(gt)\n        image, gt = self.totensor(image).float(), (self.totensor(gt)*255).long()\n\n        # Cast to correct types\n        line_lst = np.array(line_lst[3:7])\n        line_lst = torch.from_numpy(np.array(line_lst + 1)).clamp(0, 1).float()\n        valid_points = torch.from_numpy(valid_points).double()\n\n        lanes = torch.from_numpy(lanes).double()\n        horizon = horizon.float()\n\n        if idx in self.valid_idx:\n            index = self.valid_idx.index(idx)\n            return image, gt, lanes, idx, line_lst, horizon, index, valid_points\n        return image, gt, lanes, idx, line_lst, horizon, valid_points\n\n\ndef mirror_list(lst):\n    \'\'\'\n    Mirror lists of lane and line classification ground truth in order to make flipping possible\n    \'\'\'\n    middle = len(lst)//2\n    first = list(reversed(lst[:middle]))\n    second = list(reversed(lst[middle:]))\n    return second + first\n\ndef homogenous_transformation(Matrix, x, y):\n    """"""\n    Helper function to transform coordionates defined by transformation matrix\n\n    Args:\n            Matrix (multi dim - array): Transformation matrix\n            x (array): original x coordinates\n            y (array): original y coordinates\n    """"""\n    ones = np.ones((1,len(y)))\n    coordinates = np.vstack((x, y, ones))\n    trans = np.matmul(Matrix, coordinates)\n\n    x_vals = trans[0,:]/trans[2,:]\n    y_vals = trans[1,:]/trans[2,:]\n    return x_vals, y_vals\n\n\ndef get_homography(resize=320):\n    factor = resize/640\n    y_start = 0.3*(resize-1)\n    y_stop = (resize-1)\n    src = np.float32([[0.45*(2*resize-1),y_start],\n                      [0.55*(2*resize-1), y_start],\n                      [0.1*(2*resize-1),y_stop],\n                      [0.9*(2*resize-1), y_stop]])\n    dst = np.float32([[0.45*(2*resize-1), y_start],\n                      [0.55*(2*resize-1), y_start],\n                      [0.45*(2*resize-1), y_stop],\n                      [0.55*(2*resize-1),y_stop]])\n    M = cv2.getPerspectiveTransform(src, dst)\n    M_inv = cv2.getPerspectiveTransform(dst, src)\n    return M_inv\n\n\n\nclass SequentialIndicesSampler(Sampler):\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return iter(self.indices)\n\n    def __len__(self):\n        return len(self.indices)\n\ndef get_loader(num_train, json_file, lanes_file, image_dir, gt_dir, flip_on, batch_size, val_batch_size,\n               shuffle, num_workers, end_to_end, resize, nclasses, split_percentage=0.2):\n    \'\'\'\n    Splits dataset in training and validation set and creates dataloaders\n    \'\'\'\n    indices = list(range(num_train))\n    split = int(np.floor(split_percentage*num_train))\n\n    if shuffle is True:\n        np.random.seed(num_train)\n        np.random.shuffle(indices)\n    train_idx, valid_idx = indices[split:], indices[:split]\n    print(\'size train loader is\', len(train_idx))\n    print(\'size valid loader is\', len(valid_idx))\n    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n    valid_sampler = SequentialIndicesSampler(valid_idx)\n\n    transformed_dataset = LaneDataset(end_to_end=end_to_end,\n                                      valid_idx=valid_idx,\n                                      json_file=json_file,\n                                      lanes_file=lanes_file,\n                                      image_dir=image_dir,\n                                      gt_dir=gt_dir,\n                                      flip_on=flip_on,\n                                      resize=resize,\n                                      nclasses=nclasses)\n\n    train_loader = DataLoader(transformed_dataset,\n                              batch_size=batch_size, sampler=train_sampler,\n                              num_workers=num_workers, pin_memory=True, drop_last=True) #, collate_fn=my_collate)\n\n    valid_loader = DataLoader(transformed_dataset,\n                              batch_size=val_batch_size, sampler=valid_sampler,\n                              num_workers=num_workers, pin_memory=True, drop_last=True) #collate_fn=my_collate)\n\n    return train_loader, valid_loader, valid_idx\n\n\ndef my_collate(batch):\n    batch = [dict for dict in batch if dict[\'4_lanes\'] is True]\n    return default_collate(batch)\n\n\ndef load_valid_set_file(valid_idx):\n    file1 = ""Labels/label_data_0313.json""\n    file2 = ""Labels/label_data_0531.json""\n    file3 = ""Labels/label_data_0601.json""\n    labels_file1 = [json.loads(line) for line in open(file1).readlines()]\n    labels_file2 = [json.loads(line) for line in open(file2).readlines()]\n    labels_file3 = [json.loads(line) for line in open(file3).readlines()]\n    len_file1 = len(labels_file1)\n    len_file2 = len(labels_file2)\n\n    with open(\'validation_set.json\', \'w\') as jsonFile:\n        for image_id in valid_idx:\n            if image_id < len_file1:\n                labels = labels_file1[image_id]\n            elif image_id < (len_file1 + len_file2):\n                image_id_new = image_id - len_file1\n                labels = labels_file2[image_id_new]\n            else:\n                image_id_new = image_id - len_file1 - len_file2\n                labels = labels_file3[image_id_new]\n\n            json.dump(labels, jsonFile)\n            jsonFile.write(\'\\n\')\n\n\ndef load_valid_set_file_all(valid_idx, target_file, image_dir):\n    # file1 = ""Labels/Curve_parameters.json""\n    file1 = ""Labels/label_data_all.json""\n    labels_file = [json.loads(line) for line in open(file1).readlines()]\n    content = sorted(os.listdir(image_dir))\n    target_idx = [int(i.split(\'.\')[0]) for i in content]\n    new_idx = [target_idx[i]-1 for i in valid_idx]\n    with open(target_file, \'w\') as jsonFile:\n        for image_id in new_idx:\n            labels = labels_file[image_id]\n            json.dump(labels, jsonFile)\n            jsonFile.write(\'\\n\')\n\n\ndef load_0313_valid_set_file(valid_idx, nclasses):\n    file1 = ""Labels/label_data_0313.json""\n    labels_file1 = [json.loads(line) for line in open(file1).readlines()]\n    with open(\'validation_set.json\', \'w\') as jsonFile:\n        for image_id in valid_idx:\n            labels = labels_file1[image_id]\n            labels[\'lanes\'] = labels[\'lanes\'][0:nclasses]\n            json.dump(labels, jsonFile)\n            jsonFile.write(\'\\n\')\n\n\n'"
Backprojection_Loss/Networks/ERFNet.py,4,"b'# ERFNet full model definition for Pytorch\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n    \n\nclass non_bottleneck_1d (nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n        \n\n    def forward(self, input):\n\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n        \n        return F.relu(output+input)    #+input = identity (residual connection)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(in_channels, 16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16,64))\n\n        for x in range(0, 5):    #5 times\n           self.layers.append(non_bottleneck_1d(64, 0.03, 1)) \n\n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 2):    #2 times\n            self.layers.append(non_bottleneck_1d(128, 0.3, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 16))\n\n        #Only in encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass UpsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.bn(output)\n        return F.relu(output)\n\nclass Decoder (nn.Module):\n    def __init__(self, num_classes, pretrain, do_segmentation=False):\n        super().__init__()\n        self.pretrain = pretrain\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(UpsamplerBlock(128,64))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n\n        self.layers.append(UpsamplerBlock(64,16))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n\n        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n        if pretrain:\n            self.output_conv2 = nn.ConvTranspose2d( 16, num_classes + 1 , 2, stride=2, padding=0, output_padding=0, bias=True)\n\n\n        self.do_segmentation = do_segmentation\n        if do_segmentation: \n            self.layers1 = nn.ModuleList()\n            self.layers1.append(UpsamplerBlock(128,64))\n            self.layers1.append(non_bottleneck_1d(64, 0, 1))\n            self.layers1.append(non_bottleneck_1d(64, 0, 1))\n\n            self.layers1.append(UpsamplerBlock(64,16))\n            self.layers1.append(non_bottleneck_1d(16, 0, 1))\n            self.layers1.append(non_bottleneck_1d(16, 0, 1))\n            output_conv3 = nn.ConvTranspose2d(16, num_classes + 1 , 2, stride=2, padding=0, output_padding=0, bias=True)\n            self.layers1.append(output_conv3)\n\n\n    def forward(self, input, flag):\n        output = input\n        output_seg = input\n\n        for layer in self.layers:\n            output = layer(output)\n        if self.pretrain:\n\n            if flag:\n                output = self.output_conv(output)\n            else:\n                output = self.output_conv2(output)\n        else:\n            output = self.output_conv(output)\n\n        if self.do_segmentation:\n            for layer1 in self.layers1:\n                output_seg = layer1(output_seg)\n        return output, output_seg\n\n# ERFNet\nclass Net(nn.Module):\n    def __init__(self, layers=18, in_channels=1, out_channels=1, pretrained=False, pool=False):  #use encoder to pass pretrained encoder\n        super().__init__()\n        self.encoder = Encoder(in_channels, out_channels)\n        self.decoder = Decoder(out_channels, pretrained)\n        \n    def forward(self, input, flag, only_encode=False):\n        if only_encode:\n            return self.encoder.forward(input, predict=True)\n        else:\n            encoder_output = self.encoder(input)\n            decoder_output, output_seg = self.decoder.forward(encoder_output, flag)\n            return encoder_output, decoder_output, output_seg\n'"
Backprojection_Loss/Networks/LSQ_layer.py,52,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nimport torch\nimport torch.optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom math import ceil\nimport cv2\nimport Networks\nfrom Networks.gels import GELS\nfrom Networks.utils import get_homography\n\n\ndef square_tensor(x):\n    return x**2\n\n\ndef return_tensor(x):\n    return x\n\n\ndef activation_layer(activation=\'square\', no_cuda=False):\n    place_cuda = True\n    if activation == \'sigmoid\':\n        layer = nn.Sigmoid()\n    elif activation == \'relu\':\n        layer = nn.ReLU()\n    elif activation == \'softplus\':\n        layer = nn.Softplus()\n    elif activation == \'square\':\n        layer = square_tensor\n        place_cuda = False\n    elif activation == \'abs\':\n        layer = torch.abs\n        place_cuda = False\n    elif activation == \'none\':\n        layer = return_tensor\n    else:\n        raise NotImplementedError(\'Activation type: {} is not implemented\'.format(activation))\n    if not no_cuda and place_cuda:\n        layer = layer.cuda()\n    return layer\n\n\ndef ProjectiveGridGenerator(size, theta, no_cuda):\n    # Compute base grid (in img space)\n    N, C, H, W = size\n    linear_points_W = torch.linspace(0, W - 1, W)\n    linear_points_H = torch.linspace(0, H - 1, H)\n\n    base_grid = theta.new(N, H, W, 3)\n    base_grid[:, :, :, 0] = torch.ger(\n            torch.ones(H), linear_points_W).expand_as(base_grid[:, :, :, 0])\n    base_grid[:, :, :, 1] = torch.ger(\n            linear_points_H, torch.ones(W)).expand_as(base_grid[:, :, :, 1])\n    base_grid[:, :, :, 2] = 1\n\n    # Transform base grid (to homography space)\n    grid = torch.bmm(base_grid.view(N, H * W, 3), theta.transpose(1, 2))\n    grid = torch.div(grid[:, :, 0:2], grid[:, :, 2:])\n    if not no_cuda:\n        grid = grid.cuda()\n    return grid\n\n\n\nclass Weighted_least_squares(nn.Module):\n    def __init__(self, size, nclasses, order, no_cuda, reg_ls=0, use_cholesky=False):\n        super().__init__()\n        N, C, self.H, W = size\n        self.nclasses = nclasses\n        self.tensor_ones = torch.ones(N, self.H*W, 1).float()\n        self.order = order\n        self.reg_ls = reg_ls*torch.eye(order+1)\n        self.use_cholesky = use_cholesky\n        if not no_cuda:\n            self.reg_ls = self.reg_ls.cuda()\n            self.tensor_ones = self.tensor_ones.cuda()\n\n    def forward(self, W, grid):\n        beta2, beta3 = None, None\n\n        # Prepare x-y grid\n        W = W.view(-1, self.nclasses, grid.size(1))\n        bs = W.size(0)\n        grid = grid[:bs]\n        tensor_ones = self.tensor_ones[:bs]\n        x_map = grid[:, :, 0].unsqueeze(2)\n        y_map = (255 - grid[:, :, 1]).unsqueeze(2) # No pixel coordinates here!\n\n        # Compute matrix Y (to solve system W*Y*beta = W*X)\n        if self.order == 0:\n            Y = self.tensor_ones\n        elif self.order == 1:\n            Y = torch.cat((y_map, tensor_ones), 2)\n        elif self.order == 2:\n            Y = torch.cat((y_map**2, y_map, tensor_ones), 2)\n        elif self.order == 3:\n            Y = torch.cat((y_map**3, y_map**2, y_map, tensor_ones), 2)\n        else:\n            raise NotImplementedError(\n                    \'Requested order {} for polynomial fit is not implemented\'.format(self.order))\n\n        # Left egoline\n        W0 = W[:, 0, :].unsqueeze(2)\n        Y0 = torch.mul(W0, Y)\n        if not self.use_cholesky:\n            Z = torch.bmm(Y0.transpose(1, 2), Y0) + self.reg_ls\n            Z_inv = torch.inverse(Z)\n            X = torch.bmm(Y0.transpose(1, 2), torch.mul(W0, x_map))\n            beta0 = torch.bmm(Z_inv, X)\n        else:\n            beta0 = GELS.apply(Y0, torch.mul(W0, x_map))\n\n        # Right egoline\n        W1 = W[:, 1, :].unsqueeze(2)\n        Y1 = torch.mul(W1, Y)\n        if not self.use_cholesky:\n            Z = torch.bmm(Y1.transpose(1, 2), Y1) + self.reg_ls\n            Z_inv = torch.inverse(Z)\n            X = torch.bmm(Y1.transpose(1, 2), torch.mul(W1, x_map))\n            beta1 = torch.bmm(Z_inv, X)\n        else:\n            beta1 = GELS.apply(Y1, torch.mul(W1, x_map))\n\n        # Further lane lines\n        if self.nclasses > 3:\n            W2 = W[:, 2, :].unsqueeze(2)\n            Y2 = torch.mul(W2, Y)\n            if not self.use_cholesky:\n                Z = torch.bmm(Y2.transpose(1, 2), Y2) + self.reg_ls\n                Z_inv = torch.inverse(Z)\n                X = torch.bmm(Y2.transpose(1, 2), torch.mul(W2, x_map))\n                beta2 = torch.bmm(Z_inv, X)\n            else:\n                beta2 = GELS.apply(Y2, torch.mul(W2, x_map))\n            beta2 = beta2.double()\n            W3 = W[:, 3, :].unsqueeze(2)\n            Y3 = torch.mul(W3, Y)\n            if not self.use_cholesky:\n                Z = torch.bmm(Y3.transpose(1, 2), Y3) + self.reg_ls\n                Z_inv = torch.inverse(Z)\n                X = torch.bmm(Y3.transpose(1, 2), torch.mul(W3, x_map))\n                beta3 = torch.bmm(Z_inv, X)\n            else:\n                beta3 = GELS.apply(Y3, torch.mul(W3, x_map))\n            beta3 = beta3.double()\n\n        return beta0.double(), beta1.double(), beta2, beta3\n\n\nclass Classification(nn.Module):\n    def __init__(self, class_type, size, channels_in, resize):\n        super().__init__()\n        self.class_type = class_type\n        filter_size = 1\n        pad = (filter_size-1)//2\n        self.conv1 = nn.Conv2d(channels_in, 128, filter_size,\n           stride=1, padding=pad, bias=True)\n        self.conv1_bn = nn.BatchNorm2d(128)\n        \n        filter_size = 3\n        pad = (filter_size-1)//2\n        self.conv2 = nn.Conv2d(128, 128, filter_size,\n           stride=1, padding=pad, bias=True)\n        self.conv2_bn = nn.BatchNorm2d(128)\n        \n        self.conv3 = nn.Conv2d(128, 64, filter_size,\n           stride=1, padding=pad, bias=True)\n        self.conv3_bn = nn.BatchNorm2d(64)\n        \n        self.conv4 = nn.Conv2d(64, 64, filter_size,\n           stride=1, padding=pad, bias=True)\n        self.conv4_bn = nn.BatchNorm2d(64)\n        \n        rows, cols = size\n        self.avgpool = nn.AvgPool2d((1,cols))\n        self.maxpool = nn.MaxPool2d((2, 2), stride = 2)\n        \n        if class_type == \'line\':\n            self.fully_connected1 = nn.Linear(64*rows*cols//4, 128)\n            self.fully_connected_line1 = nn.Linear(128, 4)\n        else:\n            self.fully_connected_horizon = nn.Linear(64*rows, resize)\n            \n    def forward(self, x):\n        x = F.relu(self.conv1_bn(self.conv1(x)))\n        x = F.relu(self.conv2_bn(self.conv2(x)))\n        x = F.relu(self.conv3_bn(self.conv3(x)))\n        x = F.relu(self.conv4_bn(self.conv4(x)))\n        if self.class_type == \'line\':\n            x = self.maxpool(x)\n        else:\n            x = self.avgpool(x)\n        x = x.view(x.size()[0],-1)\n        batch_size = x.size(0)\n        if self.class_type == \'line\':\n            x = F.relu(self.fully_connected1(x))\n            x = self.fully_connected_line1(x)\n        else:\n            x = self.fully_connected_horizon(x)\n        return x\n\n\nclass Net(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.nclasses = args.nclasses\n\n        # define sizes and perspective transformation\n        resize = args.resize\n        size = torch.Size([args.batch_size, args.nclasses, args.resize, 2*args.resize])\n        M, _ = get_homography(args.resize, args.no_mapping)\n        M = torch.from_numpy(M).unsqueeze_(0).expand([args.batch_size, 3, 3]).float()\n\n        # Define network\n        out_channels = args.nclasses + int(not args.end_to_end)\n\n        self.net = Networks.define_model(mod=args.mod, layers=args.layers, \n                                         in_channels=args.channels_in,\n                                         out_channels=out_channels, \n                                         pretrained=args.pretrained, pool=args.pool)\n        # Init activation\n        self.activation = activation_layer(args.activation_layer, args.no_cuda)\n        # Init grid generator\n        self.grid = ProjectiveGridGenerator(size, M, args.no_cuda)\n        # Init LS layer\n        self.ls_layer = Weighted_least_squares(size, args.nclasses, args.order, \n                args.no_cuda, args.reg_ls, args.use_cholesky)\n\n        # mask configuration\n        zero_rows = ceil(args.resize*args.mask_percentage)\n        self.idx_row = torch.linspace(0, zero_rows-1, zero_rows).long()\n        n_row = 13\n        self.idx_col1 = Variable(torch.linspace(1, n_row, n_row+1).long())\n        self.idx_col2 = Variable(torch.linspace(0, n_row, n_row+1).long())+2*resize-(n_row+1)\n        idx_mask = (np.arange(resize)[:, None] < np.arange(2*resize)-(resize+10))*1\n        idx_mask = np.flip(idx_mask, 1).copy() + idx_mask\n        self.idx_mask = Variable(torch.from_numpy(idx_mask)) \\\n                .type(torch.ByteTensor).expand(\n                        args.batch_size, args.nclasses, resize, 2*resize)\n\n        self.end_to_end = args.end_to_end\n        self.pretrained = args.pretrained\n        self.classification_branch = args.clas\n        if self.classification_branch:\n            size_enc = (32, 64)\n            chan = 128\n            self.line_classification = Classification(\'line\', size=size_enc, \n                    channels_in=chan, resize=resize)\n            self.horizon_estimation = Classification(\'horizon\', size=size_enc, \n                    channels_in=chan, resize=resize)\n\n        # Place on GPU if specified\n        if not args.no_cuda:\n            self.idx_row = self.idx_row.cuda()\n            self.idx_col1 = self.idx_col1.cuda()\n            self.idx_col2 = self.idx_col2.cuda()\n            self.idx_mask = self.idx_mask.cuda()\n            if self.classification_branch:\n                self.line_classification = self.line_classification.cuda()\n                self.horizon_estimation = self.horizon_estimation.cuda()\n\n    def forward(self, input, gt_line, end_to_end, early_return=False, gt=None):\n        # 0. Init variables\n        line, horizon = None, None\n\n        # 1. Push batch trough network\n        shared_encoder, output, output_seg = self.net(input, end_to_end*self.pretrained)\n        if early_return:\n            return output\n\n        # 2. use activation function\n        if not end_to_end:\n            activated = output.detach()\n            _, activated = torch.max(activated, 1)\n\n            activated = activated.float()\n            if self.nclasses < 3:\n                left = activated*(activated == 1).float()\n                right = activated*(activated == 2).float()\n                activated = torch.stack((left, right), 1)\n            else:\n                left1 = activated*(activated == 1).float()\n                right1 = activated*(activated == 2).float()\n                left2 = activated*(activated == 3).float()\n                right2 = activated*(activated == 4).float()\n                activated = torch.stack((left1, right1, left2, right2), 1)\n        else:\n            activated = self.activation(output)\n            if self.classification_branch:\n                line = self.line_classification(shared_encoder)\n                horizon = self.horizon_estimation(shared_encoder)\n\n        # 3. use mask\n        masked = activated.index_fill(2, self.idx_row, 0)\n        # trapezium mask not needed for only two lanes\n        # Makes convergence easier for lane lines further away\n        # output = output.index_fill(3,self.idx_col1,0)\n        # output = output.index_fill(3,self.idx_col2,0)\n        # output = output.masked_fill(self.idx_mask,0)\n\n        # Prevent singular matrix\n        if gt_line.sum() != 0 and end_to_end == False:\n            gt_mask = gt_line[:, :, None, None].byte().expand_as(masked)\n            masked[gt_mask] = masked[0, 0].unsqueeze(0).repeat(gt_line.sum().item(), 1, 1).view(-1)\n\n        # 4. Least squares layer\n        beta0, beta1, beta2, beta3 = self.ls_layer(masked, self.grid)\n        return beta0, beta1, beta2, beta3, masked, output, line, horizon, output_seg\n'"
Backprojection_Loss/Networks/Least_squares_net.py,69,"b'""""""\r\nAuthor: Wouter Van Gansbeke\r\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\r\n""""""\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torch.optim\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.nn import init\r\nfrom torch.optim import lr_scheduler\r\nfrom torch.autograd import Variable\r\nfrom Networks.segnet_long import SegNet\r\nfrom Networks.Enet_vanilla import ENet\r\nfrom Networks.unet import UNet, _DecoderBlock\r\nfrom Networks.vision import ResNet18\r\n\r\nimport cv2\r\nfrom math import ceil\r\n\r\ndef Init_Projective_transform(nclasses, batch_size, resize, grid_sample, sample_factor):\r\n    #M_orig: unnormalized Transformation matrix\r\n    #M: normalized transformation matrix\r\n    #M_inv: Inverted normalized transformation matrix --> Needed for grid sample\r\n    #original aspect ratio: 720x1280 --> after 80 rows cropped: 640x1280 --> after resize: 320x640 (default) or resize x 2*resize (in general)\r\n    if not grid_sample:\r\n        size = torch.Size([batch_size, nclasses, resize, 2*resize])\r\n    else: \r\n        size = torch.Size([batch_size, nclasses, resize//sample_factor+1, 2*resize//sample_factor+1])\r\n    y_start = 0.3\r\n    y_stop = 1\r\n    xd1, xd2, xd3, xd4 = 0.45, 0.55, 0.45, 0.55\r\n    src = np.float32([[0.45,y_start],[0.55, y_start],[0.1,y_stop],[0.9, y_stop]])\r\n    dst = np.float32([[xd3, y_start],[xd4, y_start],[xd1, y_stop],[xd2, y_stop]])\r\n    M = cv2.getPerspectiveTransform(src,dst)\r\n    M_inv = cv2.getPerspectiveTransform(dst,src)\r\n    M = torch.from_numpy(M).unsqueeze_(0).expand([batch_size,3,3]).type(torch.FloatTensor)\r\n    M_inv = torch.from_numpy(M_inv).unsqueeze_(0).expand([batch_size,3,3]).type(torch.FloatTensor)\r\n    return size, M, M_inv\r\n\r\nclass resnet_block(nn.Module):\r\n    def __init__(self, in_channels, out_channels, dilation, encode=True):\r\n        super().__init__()\r\n        self.bn1 = nn.BatchNorm2d(in_channels)\r\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=dilation, dilation=dilation, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(out_channels)\r\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=dilation, dilation=dilation, bias=False)\r\n        self.conv_shortcut = nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False)\r\n            \r\n    def forward(self, x):\r\n        shortcut = self.conv_shortcut(x)\r\n        \r\n        x = F.relu(self.bn1(x))\r\n        x = self.conv1(x)\r\n        x = F.relu(self.bn2(x))\r\n        x = self.conv2(x)\r\n        \r\n        x = x + shortcut\r\n#        x = F.relu(x) #W relu toegevoegd\r\n        \r\n        return x\r\n\r\nclass simple_net(nn.Module):\r\n    def __init__(self, nclasses):\r\n        super().__init__()\r\n#        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3,\r\n#                               bias=False)\r\n#        \r\n        self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(32)\r\n        self.resnet_block1 = resnet_block(32, 32, 1)\r\n        self.pool = nn.MaxPool2d(2, (2,2))\r\n        self.resnet_block2 = resnet_block(32, 64, 1)\r\n        self.resnet_block3 = resnet_block(64, 128, 2) # dilation 2\r\n        self.resnet_block4 = resnet_block(128, 256, 4) # dilation 4\r\n        self.resnet_block5 = resnet_block(256, 128, 2) # dilation 2\r\n        self.resnet_block6 = resnet_block(128, 64, 1)\r\n        self.upsample = nn.ConvTranspose2d(64, 64,kernel_size=2, stride=2)\r\n        self.resnet_block7 = resnet_block(64, 32, 1)\r\n        self.conv_out = nn.Conv2d(32, nclasses, 1, stride=1, padding=0, bias=True)\r\n#        self.conv_out = nn.ConvTranspose2d(32, nclasses,kernel_size=2, stride=2)\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.resnet_block1(x)\r\n#        x = self.pool(x)\r\n        x = self.resnet_block2(x)\r\n        x = self.resnet_block3(x)\r\n        x = self.resnet_block4(x)\r\n        output_encoder = x\r\n        \r\n        x = self.resnet_block5(x)\r\n        x = self.resnet_block6(x)\r\n#        x = self.upsample(x)\r\n        x = self.resnet_block7(x)\r\n        x = self.conv_out(x)\r\n        \r\n        return x, output_encoder\r\n    \r\nclass Classification_old(nn.Module):\r\n    def __init__(self, class_type, size, resize=320):\r\n        super().__init__()\r\n        self.class_type = class_type\r\n        \r\n        self.conv1 = nn.Conv2d(512, 256, 3,\r\n           stride=1, padding=1, bias=True)\r\n        self.conv1_bn = nn.BatchNorm2d(256)\r\n        \r\n        self.conv2 = nn.Conv2d(256, 128, 3,\r\n           stride=1, padding=1, bias=True)\r\n        self.conv2_bn = nn.BatchNorm2d(128)\r\n        \r\n        self.conv3 = nn.Conv2d(128, 64, 3,\r\n           stride=1, padding=1, bias=True)\r\n        self.conv3_bn = nn.BatchNorm2d(64)\r\n        \r\n        self.conv4 = nn.Conv2d(64, 64, 3,\r\n           stride=1, padding=1, bias=True)\r\n        self.conv4_bn = nn.BatchNorm2d(64)\r\n        \r\n        self.conv5 = nn.Conv2d(64, 32, 3,\r\n           stride=1, padding=1, bias=True)\r\n        self.conv5_bn = nn.BatchNorm2d(32)\r\n        \r\n        self.pool1 = nn.MaxPool2d((2, 2), stride = 2)\r\n        self.pool2 = nn.MaxPool2d((2, 2), stride = 2)\r\n        rows, cols = size\r\n        self.fully_connected1 = nn.Linear(32*rows*cols//4, 1024)\r\n#        self.fully_connected1 = nn.Linear(3648, 1024)\r\n        if class_type == \'line\':\r\n            self.fully_connected2 = nn.Linear(1024, 128)\r\n            self.fully_connected_line1 = nn.Linear(128, 3)\r\n            self.fully_connected_line2 = nn.Linear(128, 3)\r\n            self.fully_connected_line3 = nn.Linear(128, 3)\r\n            self.fully_connected_line4 = nn.Linear(128, 3)\r\n        else:\r\n            self.fully_connected_horizon = nn.Linear(1024, resize)\r\n            \r\n    def forward(self, x):\r\n        x = F.relu(self.conv1_bn(self.conv1(x)))\r\n        x = F.relu(self.conv2_bn(self.conv2(x)))\r\n        x = F.relu(self.conv3_bn(self.conv3(x)))\r\n#        x = self.pool1(x)\r\n        x = F.relu(self.conv4_bn(self.conv4(x)))\r\n        x = F.relu(self.conv5_bn(self.conv5(x)))\r\n        x = self.pool1(x)\r\n        x = x.view(x.size()[0],-1)\r\n        x = F.relu(self.fully_connected1(x))\r\n        \r\n        batch_size = x.size(0)\r\n        if self.class_type == \'line\':\r\n            x = F.relu(self.fully_connected2(x))\r\n            x1 = self.fully_connected_line1(x).view(batch_size,3,1,1)\r\n            x2 = self.fully_connected_line2(x).view(batch_size,3,1,1)\r\n            x3 = self.fully_connected_line3(x).view(batch_size,3,1,1)\r\n            x4 = self.fully_connected_line4(x).view(batch_size,3,1,1)\r\n            \r\n            x = torch.cat((x1, x2, x3, x4), 2)\r\n            \r\n        else:\r\n            x = self.fully_connected_horizon(x)\r\n        return x\r\n\r\nclass Classification(nn.Module):\r\n    def __init__(self, class_type, size, channels_in, resize):\r\n        super().__init__()\r\n        self.class_type = class_type\r\n        filter_size = 1\r\n        pad = (filter_size-1)//2\r\n        self.conv1 = nn.Conv2d(channels_in, 128, filter_size,\r\n           stride=1, padding=pad, bias=True)\r\n        self.conv1_bn = nn.BatchNorm2d(128)\r\n        \r\n        filter_size = 3\r\n        pad = (filter_size-1)//2\r\n        self.conv2 = nn.Conv2d(128, 128, filter_size,\r\n           stride=1, padding=pad, bias=True)\r\n        self.conv2_bn = nn.BatchNorm2d(128)\r\n        \r\n        self.conv3 = nn.Conv2d(128, 64, filter_size,\r\n           stride=1, padding=pad, bias=True)\r\n        self.conv3_bn = nn.BatchNorm2d(64)\r\n        \r\n        self.conv4 = nn.Conv2d(64, 64, filter_size,\r\n           stride=1, padding=pad, bias=True)\r\n        self.conv4_bn = nn.BatchNorm2d(64)\r\n        \r\n        rows, cols = size\r\n        self.avgpool = nn.AvgPool2d((1,cols))\r\n        self.maxpool = nn.MaxPool2d((2, 2), stride = 2)\r\n        \r\n        if class_type == \'line\':\r\n            self.fully_connected1 = nn.Linear(64*rows*cols//4, 128)\r\n            self.fully_connected_line1 = nn.Linear(128, 3)\r\n            self.fully_connected_line2 = nn.Linear(128, 3)\r\n            self.fully_connected_line3 = nn.Linear(128, 3)\r\n            self.fully_connected_line4 = nn.Linear(128, 3)\r\n        else:\r\n            self.fully_connected_horizon = nn.Linear(64*rows, resize)\r\n            \r\n    def forward(self, x):\r\n        x = F.relu(self.conv1_bn(self.conv1(x)))\r\n        x = F.relu(self.conv2_bn(self.conv2(x)))\r\n        x = F.relu(self.conv3_bn(self.conv3(x)))\r\n        x = F.relu(self.conv4_bn(self.conv4(x)))\r\n        if self.class_type == \'line\':\r\n            x = self.maxpool(x)\r\n        else:\r\n            x = self.avgpool(x)\r\n        x = x.view(x.size()[0],-1)\r\n        batch_size = x.size(0)\r\n        if self.class_type == \'line\':\r\n            x = F.relu(self.fully_connected1(x))\r\n            x1 = self.fully_connected_line1(x).view(batch_size,3,1,1)\r\n            x2 = self.fully_connected_line2(x).view(batch_size,3,1,1)\r\n            x3 = self.fully_connected_line3(x).view(batch_size,3,1,1)\r\n            x4 = self.fully_connected_line4(x).view(batch_size,3,1,1)\r\n            \r\n            x = torch.cat((x1, x2, x3, x4), 2)\r\n        else:\r\n            x = self.fully_connected_horizon(x)\r\n        return x\r\n\r\n\r\nclass DLT(nn.Module):\r\n    def __init__(self, batch_size, cuda, size, channels_in):\r\n        super().__init__()\r\n        self.activation = nn.Tanh()\r\n        self.spatial_trans = Spatial_transformer_net(size, channels_in)\r\n        self.xs1, self.xs2, self.xs3, self.xs4 = 0.1, 0.9, 0.45, 0.55\r\n        self.ys1, self.ys2 = 1, 0.3\r\n        self.xd1, self.xd2, self.xd3, self.xd4 = 0.45, 0.55, 0.45, 0.55\r\n        self.yd1, self.yd2 = 1, 0.3\r\n        A = torch.FloatTensor([[0,0,0,-self.ys1,-1,self.ys1*self.yd1],\r\n                                        [self.xs1,self.ys1,1,0,0,0],\r\n                                        [self.xs2,self.ys1,1,0,0,0],\r\n                                        [0,0,0,-self.ys2,-1,0],\r\n                                        [self.xs3,self.ys2,1,0,0,0],\r\n                                        [self.xs4,self.ys2,1,0,0,0]])\r\n        \r\n        B = torch.FloatTensor([[-self.yd1],\r\n                               [0],\r\n                               [0],\r\n                               [0],\r\n                               [0],\r\n                               [0]])\r\n        self.A = A.expand(batch_size,6,6)\r\n        self.B = B.expand(batch_size,6,1)\r\n        self.zeros = Variable(torch.zeros(batch_size,1,1))\r\n        self.ones = Variable(torch.ones(batch_size,1,1))\r\n        if cuda: \r\n            self.activation = self.activation.cuda()\r\n            self.A = self.A.cuda()\r\n            self.B = self.B.cuda()\r\n            self.zeros = self.zeros.cuda()\r\n            self.ones = self.ones.cuda()\r\n            self.spatial_trans = self.spatial_trans.cuda()\r\n    def forward(self, output_encoder):\r\n        x = self.spatial_trans(output_encoder)\r\n        x = self.activation(x)/16\r\n        A = Variable(self.A.clone())\r\n        B = Variable(self.B.clone())\r\n        A[:,1,5] = -self.ys1*(self.xd1+x[:,0])\r\n        A[:,2,5] = -self.ys1*(self.xd2+x[:,1])\r\n        A[:,3,5] =  self.ys2*(self.yd2+x[:,2])\r\n        A[:,4,5] = -self.ys2*(self.xd3+x[:,0])\r\n        A[:,5,5] = -self.ys2*(self.xd4+x[:,1])\r\n        \r\n        B[:,1,0] =   self.xd1+x[:,0]\r\n        B[:,2,0] =   self.xd2+x[:,1]\r\n        B[:,3,0] = -(self.yd2+x[:,2])\r\n        B[:,4,0] =   self.xd3+x[:,0]\r\n        B[:,5,0] =   self.xd4+x[:,1]\r\n        \r\n        A_prime = torch.bmm(A.transpose(1,2), A)\r\n        B_prime = torch.bmm(A.transpose(1,2), B)\r\n        \r\n        h = torch.stack([torch.gesv(b, a)[0] for b, a in zip(torch.unbind(B_prime), torch.unbind(A_prime))])\r\n        h = torch.cat((h[:,0:3,:],self.zeros,h[:,3:5,:],self.zeros,h[:,5:6,:],self.ones),1)\r\n        h = h.view(-1,3,3)\r\n#        x.register_hook(save_grad(\'x\'))\r\n        return h, x\r\n    \r\nclass Spatial_transformer_net(nn.Module):\r\n    def __init__(self, size, channels_in):\r\n        super().__init__()\r\n        filter_size = 1\r\n        pad = (filter_size-1)//2\r\n        self.conv1 = nn.Conv2d(channels_in, 128, filter_size,\r\n           stride=1, padding=pad, bias=True)\r\n        self.conv1_bn = nn.BatchNorm2d(128)\r\n        \r\n        filter_size = 3\r\n        pad = (filter_size-1)//2\r\n        self.conv2 = nn.Conv2d(128, 128, filter_size,\r\n           stride=1, padding=pad, bias=True)\r\n        self.conv2_bn = nn.BatchNorm2d(128)\r\n        \r\n        self.conv3 = nn.Conv2d(128, 64, filter_size,\r\n           stride=1, padding=pad, bias=True)\r\n        self.conv3_bn = nn.BatchNorm2d(64)\r\n        \r\n        self.conv4 = nn.Conv2d(64, 64, filter_size,\r\n           stride=1, padding=pad, bias=True)\r\n        self.conv4_bn = nn.BatchNorm2d(64)\r\n        \r\n        rows, cols = size\r\n        self.maxpool = nn.MaxPool2d((2, 2), stride = 2)\r\n        \r\n        self.conv5 = nn.Conv2d(64, 64, filter_size,\r\n           stride=1, padding=pad, bias=True)\r\n        self.conv4_bn = nn.BatchNorm2d(64)\r\n        \r\n        self.fully_connected1 = nn.Linear(64*rows*cols//4, 128)\r\n        self.fully_connected2 = nn.Linear(128, 3)\r\n        \r\n        self.fully_connected2.weight.data.fill_(0)\r\n        self.fully_connected2.bias.data = torch.FloatTensor([0, 0, 0])\r\n   \r\n    def forward(self, x):\r\n        x = F.relu(self.conv1_bn(self.conv1(x)))\r\n        x = F.relu(self.conv2_bn(self.conv2(x)))\r\n        x = F.relu(self.conv3_bn(self.conv3(x)))\r\n        x = self.maxpool(x)\r\n        x = F.relu(self.conv4_bn(self.conv4(x)))\r\n        x = x.view(x.size()[0],-1)\r\n        x = F.relu(self.fully_connected1(x))\r\n        x = self.fully_connected2(x)\r\n        return x\r\n\r\n\r\nclass Proxy_branch_segmentation(nn.Module):\r\n    def __init__(self, channels_in, resize, nclasses):\r\n        super().__init__()\r\n        kernel_size = 3\r\n        padding = (kernel_size-1)//2\r\n        self.dec4 = _DecoderBlock(channels_in, 256, 256, pad=padding)\r\n        self.dec3 = _DecoderBlock(256, 128, 128, pad=padding)\r\n        self.dec2 = _DecoderBlock(128, 64, 64, pad=padding)\r\n        self.final = nn.Conv2d(64, nclasses+1, kernel_size=1)\r\n        self.resize = resize\r\n\r\n    def forward(self, x):\r\n        x = self.dec4(x)\r\n        x = self.dec3(x)\r\n        x = self.dec2(x)\r\n        x = self.final(x)\r\n        x = F.upsample(x, (self.resize, 2*self.resize), mode=\'bilinear\')\r\n        return x\r\n        \r\n    \r\nclass ProjectiveGridGenerator(nn.Module):\r\n    def __init__(self, size, theta, cuda):\r\n        super().__init__()\r\n        self.N, self.C, self.H, self.W = size\r\n        linear_points_W = torch.linspace(0, 1, self.W)\r\n        linear_points_H = torch.linspace(0, 1, self.H)\r\n\r\n        self.base_grid = theta.new(self.N, self.H, self.W, 3)\r\n        self.base_grid[:, :, :, 0] = torch.ger(torch.ones(self.H), linear_points_W).expand_as(self.base_grid[:, :, :, 0])\r\n        self.base_grid[:, :, :, 1] = torch.ger(linear_points_H, torch.ones(self.W)).expand_as(self.base_grid[:, :, :, 1])\r\n        self.base_grid[:, :, :, 2] = 1\r\n        \r\n        self.base_grid = Variable(self.base_grid)\r\n        if cuda:\r\n            self.base_grid = self.base_grid.cuda()\r\n            \r\n    def forward(self, theta, no_ortho_view):\r\n        if no_ortho_view:\r\n            return self.base_grid.view(self.N, -1, 3)\r\n        \r\n        grid = torch.bmm(self.base_grid.view(self.N, self.H * self.W, 3), theta.transpose(1, 2))\r\n        grid = torch.div(grid[:,:,0:2],grid[:,:,2:])\r\n        return grid\r\n\r\n\r\nclass Weighted_least_squares(nn.Module):\r\n    def __init__(self, size, nclasses, order, cuda, reg_ls=0, use_cholesky=False, sample_factor=5):\r\n        super().__init__()\r\n        self.sample_factor = sample_factor\r\n        N, C, self.H, W = size\r\n        self.nclasses = nclasses\r\n        self.tensor_ones = Variable(torch.ones(N,self.H*W,1))\r\n        self.order = order\r\n        self.reg_ls = Variable(reg_ls*torch.eye(order+1))\r\n        self.use_cholesky = use_cholesky\r\n        if cuda:\r\n            self.reg_ls = self.reg_ls.cuda()\r\n            self.tensor_ones = self.tensor_ones.cuda()\r\n            \r\n    def forward(self, W, grid, sample_grid):\r\n        beta1, beta2, beta3 = None, None, None\r\n        if sample_grid:\r\n            W = W[:,:,::self.sample_factor,::self.sample_factor].contiguous()\r\n\r\n        W = W.view(-1,self.nclasses, grid.size(1))\r\n        W0 = W[:,0,:].unsqueeze(2)\r\n        x_map = grid[:,:,0].unsqueeze(2)\r\n        y_map = ((1)-grid[:,:,1]).unsqueeze(2)\r\n        if self.order == 0:\r\n            Y = self.tensor_ones\r\n        elif self.order == 1:\r\n            Y = torch.cat((y_map, self.tensor_ones), 2)\r\n        elif self.order == 2:\r\n            Y = torch.cat(((y_map**2), y_map, self.tensor_ones), 2)\r\n        else:\r\n            raise NotImplementedError(\'Requested order for polynomial fit is not implemented\')\r\n        \r\n        Y = Y[0:W.size(0),:,:]\r\n        x_map = x_map[0:W.size(0),:,:]\r\n        Y0 = torch.mul(W0, Y)\r\n        Z = torch.bmm(Y0.transpose(1,2), Y0) + self.reg_ls\r\n        \r\n        if not self.use_cholesky:\r\n            Z_inv = [torch.inverse(matrix) for matrix in torch.unbind(Z)]\r\n            Z_inv = torch.stack(Z_inv)\r\n            X = torch.bmm(Y0.transpose(1,2), torch.mul(W0, x_map))\r\n            beta0 = torch.bmm(Z_inv, X)\r\n        else:\r\n            #cholesky\r\n            beta0=[]\r\n            X = torch.bmm(Y0.transpose(1,2), torch.mul(W0, x_map))\r\n            for image, rhs in zip(torch.unbind(Z),torch.unbind(X)):\r\n                R = torch.potrf(image)\r\n                opl = torch.trtrs(rhs, R.transpose(0,1))\r\n                beta0.append(torch.trtrs(opl[0],R ,upper=False)[0])\r\n            beta0 = torch.cat((beta0),1).transpose(0,1).unsqueeze(2)\r\n        \r\n        if self.nclasses > 1:\r\n            W1 = W[:,1,:].unsqueeze(2)\r\n            Y1=torch.mul(W1, Y)\r\n            Z = torch.bmm(Y1.transpose(1,2), Y1) + self.reg_ls\r\n            Z_inv = [torch.inverse(matrix) for matrix in torch.unbind(Z)]\r\n            Z_inv = torch.stack(Z_inv)\r\n            X = torch.bmm(Y1.transpose(1,2), torch.mul(W1, x_map))\r\n            beta1 = torch.bmm(Z_inv, X)\r\n            \r\n        if self.nclasses > 2:\r\n            W2 = W[:,2,:].unsqueeze(2)\r\n            Y2=torch.mul(W2, Y)\r\n            Z = torch.bmm(Y2.transpose(1,2), Y2) + self.reg_ls\r\n            Z_inv = [torch.inverse(matrix) for matrix in torch.unbind(Z)]\r\n            Z_inv = torch.stack(Z_inv)\r\n            X = torch.bmm(Y2.transpose(1,2), torch.mul(W2, x_map))\r\n            beta2 = torch.bmm(Z_inv, X)\r\n            \r\n            W3 = W[:,3,:].unsqueeze(2)\r\n            Y3=torch.mul(W3, Y)\r\n            Z = torch.bmm(Y3.transpose(1,2), Y3) + self.reg_ls\r\n            Z_inv = [torch.inverse(matrix) for matrix in torch.unbind(Z)]\r\n            Z_inv = torch.stack(Z_inv)\r\n            X = torch.bmm(Y3.transpose(1,2), torch.mul(W3, x_map))\r\n            beta3 = torch.bmm(Z_inv, X)\r\n            \r\n        return beta0, beta1, beta2, beta3\r\n    \r\nclass Net(nn.Module):\r\n    def __init__(self, options):\r\n        super().__init__()\r\n        \r\n        #define sizes and perspective transformation\r\n        resize = options.resize\r\n        size = torch.Size([options.batch_size, options.nclasses, options.resize, 2*options.resize])\r\n        size, M, M_inv = Init_Projective_transform(options.nclasses, options.batch_size, options.resize, options.grid_sample, options.sample_factor)\r\n        self.M = Variable(M)\r\n        \r\n        #Init net\r\n        self.net = define_network(options)\r\n        #Init activation\r\n        self.activation = activation_layer(options.activation_layer, options.cuda)\r\n        #Init grid generator \r\n        self.project_layer = ProjectiveGridGenerator(size, M, options.cuda)\r\n        #Init LS layer\r\n        self.ls_layer = Weighted_least_squares(size, options.nclasses, options.order, options.cuda, options.reg_ls, options.use_cholesky)\r\n        \r\n        ############Classfication branch configuration#################\r\n        if options.classification_branch:\r\n            factor = options.resize/256\r\n            if options.model_seg == \'unet\' :\r\n                chan = 512\r\n                size_enc = (32,64)\r\n                if options.pad == 1:\r\n                    size_enc = tuple(int(i*factor) for i in size_enc)\r\n                elif options.pad == 0:\r\n                    size_enc = (16,48) if factor==1 else (24,64)\r\n                else:\r\n                    raise NotImplementedError\r\n            elif options.model_seg == \'segnet\':\r\n                chan = 512\r\n                size_enc = (8,16)\r\n                size_enc = tuple(int(i*factor) for i in size_enc)\r\n            elif options.model_seg == \'enet\':\r\n                chan = 128\r\n                size_enc = (32,64)\r\n                size_enc = tuple(int(i*factor) for i in size_enc)\r\n            elif options.model_seg == \'resnet\':\r\n                chan = 256\r\n                num_downsample = 0\r\n                size_enc = tuple(int(i*factor) for i in size_enc)\r\n                size_enc = (options.resize//(2**num_downsample),2*options.resize//(2**num_downsample))\r\n            elif options.model_seg == \'resnet18\':\r\n                chan = 512\r\n                size_enc = (8,16) \r\n                size_enc = tuple(int(i*factor) for i in size_enc)\r\n            else:\r\n                raise NotImplementedError\r\n                \r\n            self.line_classification = Classification(\'line\', size=size_enc, channels_in=chan, resize=resize)\r\n            self.horizon_estimation = Classification(\'horizon\', size=size_enc, channels_in=chan, resize=resize)\r\n        \r\n        #DLT, proxy and classification branch initialization\r\n        self.classification_branch = options.classification_branch\r\n        self.DLT = DLT(options.batch_size, options.cuda, size=size_enc, channels_in=chan)\r\n        self.proxy = Proxy_branch_segmentation(chan, resize, options.nclasses)\r\n        self.DLT_on = options.DLT_on\r\n        self.proxy_branch = options.proxy_branch\r\n        \r\n        #mask configuration\r\n        zero_rows = ceil(options.resize*options.mask_percentage)\r\n        self.idx_row=Variable(torch.linspace(0,zero_rows-1,zero_rows).long())\r\n        n_row = 13\r\n        self.idx_col1=Variable(torch.linspace(0,n_row,n_row+1).long())\r\n        self.idx_col2=Variable(torch.linspace(0,n_row,n_row+1).long())+2*resize-(n_row+1)\r\n        idx_mask = (np.arange(resize)[:,None] < np.arange(2*resize)-(resize+10))*1\r\n        idx_mask = np.flip(idx_mask,1).copy() + idx_mask\r\n        self.idx_mask = Variable(torch.from_numpy(idx_mask)).type(torch.ByteTensor).expand(options.batch_size,options.nclasses,resize,2*resize)\r\n        \r\n        #Place on GPU if specified\r\n        if options.cuda:\r\n            self.M = self.M.cuda()\r\n            self.idx_row = self.idx_row.cuda()\r\n            self.idx_col1 = self.idx_col1.cuda()\r\n            self.idx_col2 = self.idx_col2.cuda()\r\n            self.idx_mask = self.idx_mask.cuda()\r\n            if options.classification_branch:\r\n                self.line_classification = self.line_classification.cuda()\r\n                self.horizon_estimation = self.horizon_estimation.cuda()\r\n                self.DLT = self.DLT.cuda()\r\n            if options.proxy_branch:\r\n                self.proxy = self.proxy.cuda()\r\n                \r\n    def forward(self, input, no_ortho_view, sample_grid):\r\n        output_seg, line, horizon, x = None, None, None, None\r\n        #1. Push bach trough network\r\n        output, output_encoder = self.net(input)\r\n        \r\n        #2. Classfication branch for line and horizon estimation\r\n        if self.classification_branch:\r\n            line = self.line_classification(output_encoder)\r\n            horizon = self.horizon_estimation(output_encoder)\r\n            \r\n        #3. Proxy branch for segmentation\r\n        if self.proxy_branch:\r\n            output_seg = self.proxy(output_encoder)\r\n        \r\n        #4. Use activation function \r\n        output = self.activation(output)\r\n            \r\n        #5. Use mask \r\n        if not no_ortho_view:\r\n            output = output.index_fill(2,self.idx_row,0)\r\n#            output = output.index_fill(3,self.idx_col1,0)\r\n#            output = output.index_fill(3,self.idx_col2,0)\r\n#            output = output.masked_fill(self.idx_mask,0)\r\n\r\n        #6. DLT transform\r\n        if self.DLT_on:\r\n            M, x = self.DLT(output_encoder)\r\n            grid = self.project_layer(M, no_ortho_view)\r\n        else:\r\n            M = self.M\r\n            grid = self.project_layer(self.M, no_ortho_view)\r\n        \r\n        #7. LS layer\r\n        beta0, beta1, beta2, beta3 = self.ls_layer(output, grid, sample_grid)\r\n        return beta0, beta1, beta2, beta3, output**2, output_seg, line, horizon, M, x\r\n\r\n\r\ndef define_network(options, norm=\'batch\'):\r\n    print(\'defining network\')\r\n    if options.model_seg == \'resnet\':\r\n        net = simple_net(options.nclasses)\r\n    elif options.model_seg == \'resnet18\':\r\n        net = ResNet18(options.nclasses)\r\n    elif options.model_seg == \'unet\':\r\n        net = UNet(options.nclasses, options.activation_net, options.pad)\r\n    elif options.model_seg == \'enet\':\r\n        net = ENet(options.nclasses, options.norm, options.no_dropout)\r\n    elif options.model_seg == \'segnet\':\r\n        net = SegNet(options.nclasses)\r\n    else:\r\n        raise NotImplementedError(\'The requested {} is not yet implemented\'.format(options.model_seg))\r\n    if options.cuda:\r\n        net = net.cuda()\r\n    print(net)\r\n    return net\r\n\r\ndef define_optimizer(params, options):\r\n    print(\'define optimizer\')\r\n    if options.optimizer == \'adam\':\r\n        optimizer = torch.optim.Adam(params, lr=options.learning_rate, weight_decay=options.weight_decay)\r\n    elif options.optimizer == \'sgd\':\r\n        optimizer = torch.optim.SGD(params, lr=options.learning_rate, momentum=0.9, weight_decay=options.weight_decay)\r\n    else:\r\n        raise NotImplementedError\r\n    return optimizer\r\n\r\ndef get_scheduler(optimizer, opt):\r\n    if opt.lr_policy == \'lambda\':\r\n        def lambda_rule(epoch):\r\n            lr_l = 1.0 - max(0, epoch + 1 - opt.niter) / float(opt.niter_decay + 1)\r\n            return lr_l\r\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\r\n    elif opt.lr_policy == \'step\':\r\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\r\n    elif opt.lr_policy == \'plateau\':\r\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\', factor=0.2, threshold=0.001, patience=5)\r\n    elif opt.lr_policy == \'none\':\r\n        scheduler = None\r\n    else:\r\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', opt.lr_policy)\r\n    return scheduler\r\n\r\ndef square_tensor(x):\r\n    return x**2\r\n\r\ndef return_tensor(x):\r\n    return x\r\n\r\ndef activation_layer(activation=\'square\', cuda=True):\r\n    place_cuda = True\r\n    if activation == \'sigmoid\':\r\n        layer = nn.Sigmoid()\r\n    elif activation == \'relu\':\r\n        layer = nn.ReLU()\r\n    elif activation == \'softplus\':\r\n        layer = nn.Softplus()\r\n    elif activation ==\'square\':\r\n        layer = square_tensor\r\n        place_cuda = False\r\n    elif activation == \'abs\':\r\n        layer = torch.abs\r\n        place_cuda = False\r\n    elif activation == \'none\':\r\n        layer = return_tensor\r\n    else:\r\n        raise NotImplementedError(\'Activation type: {} is not implemented\'.format(activation))\r\n    if cuda and place_cuda:\r\n        layer = layer.cuda()\r\n    return layer\r\n\r\ndef init_weights(net, init_w=\'normal\', activation=\'relu\'):\r\n    print(\'Init weights in network with [{}]\'.format(init_w))\r\n    if init_w == \'normal\':\r\n        net.apply(weights_init_normal)\r\n    elif init_w == \'xavier\':\r\n        net.apply(weights_init_xavier)\r\n    elif init_w == \'kaiming\':\r\n        net.apply(weights_init_kaiming)\r\n    elif init_w == \'orthogonal\':\r\n        net.apply(weights_init_orthogonal)\r\n    else:\r\n        raise NotImplementedError(\'initialization method [{}] is not implemented\'.format(init))\r\n\r\ndef weights_init_normal(m):\r\n    classname = m.__class__.__name__\r\n#    print(classname)\r\n    if classname.find(\'Conv\') != -1:\r\n        init.normal(m.weight.data, 0.0, 0.02)\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.normal(m.weight.data, 0.0, 0.02)\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal(m.weight.data, 1.0, 0.02)\r\n        init.constant(m.bias.data, 0.0)\r\n\r\ndef weights_init_xavier(m):\r\n    classname = m.__class__.__name__\r\n    # print(classname)\r\n    if classname.find(\'Conv\') != -1:\r\n        init.xavier_normal(m.weight.data, gain=0.02)\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.xavier_normal(m.weight.data, gain=0.02)\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal(m.weight.data, 1.0, 0.02)\r\n        init.constant(m.bias.data, 0.0)\r\n\r\n\r\ndef weights_init_kaiming(m):\r\n    classname = m.__class__.__name__\r\n    # print(classname)\r\n    if classname.find(\'Conv\') != -1:\r\n        init.kaiming_normal(m.weight.data, a=0, mode=\'fan_in\')\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.kaiming_normal(m.weight.data, a=0, mode=\'fan_in\')\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal(m.weight.data, 1.0, 0.02)\r\n        init.constant(m.bias.data, 0.0)\r\n        \r\n\r\ndef weights_init_orthogonal(m):\r\n    classname = m.__class__.__name__\r\n#    print(classname)\r\n    if classname.find(\'Conv\') != -1:\r\n        init.orthogonal(m.weight.data, gain=1)\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.orthogonal(m.weight.data, gain=1)\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal(m.weight.data, 1.0, 0.02)\r\n        init.constant(m.bias.data, 0.0)\r\n\r\n'"
Backprojection_Loss/Networks/__init__.py,0,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nfrom .ERFNet import Net\n\nmodel_dict = {\'erfnet\': Net}\n\n\ndef allowed_models():\n    return model_dict.keys()\n\n\ndef define_model(mod, **kwargs):\n    if mod not in allowed_models():\n        raise KeyError(""The requested model: {} is not implemented"".format(mod))\n    else:\n        return model_dict[mod](**kwargs)\n'"
Backprojection_Loss/Networks/gels.py,7,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport torch\nfrom torch.autograd import Function\n\nclass GELS(Function):\n    @staticmethod\n    def forward(ctx, A, b):\n        u = torch.cholesky(torch.matmul(A.transpose(-1, -2), A), upper=True)\n        ret = torch.cholesky_solve(torch.matmul(A.transpose(-1, -2), b), u, upper=True)\n        ctx.save_for_backward(u, ret, A, b)\n        return ret\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        chol, x, a, b = ctx.saved_tensors\n        z = torch.cholesky_solve(grad_output, chol, upper=True)\n        xzt = torch.matmul(x, z.transpose(-1,-2))\n        zx_sym = xzt + xzt.transpose(-1, -2)\n        grad_A = - torch.matmul(a, zx_sym) + torch.matmul(b, z.transpose(-1, -2))\n        grad_b = torch.matmul(a, z)\n        return grad_A, grad_b\n'"
Backprojection_Loss/Networks/utils.py,7,"b'""""""\r\nAuthor: Wouter Van Gansbeke\r\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\r\n""""""\r\n\r\nimport cv2\r\nimport argparse\r\nimport numpy as np\r\nimport os\r\nimport sys\r\nimport errno\r\nfrom PIL import Image\r\nimport torch\r\nimport torch.optim\r\nfrom torch.optim import lr_scheduler\r\nimport torch.nn.init as init\r\nimport matplotlib\r\nmatplotlib.use(\'Agg\')\r\nimport matplotlib.pyplot as plt\r\n# from scipy.optimize import fsolve\r\nplt.rcParams[\'figure.figsize\'] = (35, 30)\r\n\r\n\r\ndef define_args():\r\n    parser = argparse.ArgumentParser(description=\'Lane_detection_all_objectives\')\r\n    # Segmentation model settings\r\n    parser.add_argument(\'--dataset\', default=\'lane_detection\', help=\'dataset images to train on\')\r\n    parser.add_argument(\'--batch_size\', type=int, default=8, help=\'batch size\')\r\n    parser.add_argument(\'--val_batch_size\', type=int, default=None, help=\'batch size during eval\')\r\n    parser.add_argument(\'--nepochs\', type=int, default=500, help=\'total numbers of epochs\')\r\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-4, help=\'learning rate\')\r\n    parser.add_argument(\'--no_cuda\', action=\'store_true\', help=\'if gpu available\')\r\n    parser.add_argument(\'--nworkers\', type=int, default=8, help=\'num of threads\')\r\n    parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout in network\')\r\n    parser.add_argument(\'--nclasses\', type=int, default=2, choices=[2, 4], help=\'num output channels for segmentation\')\r\n    parser.add_argument(\'--crop_size\', type=int, default=80, help=\'crop from image\')\r\n    parser.add_argument(\'--resize\', type=int, default=256, help=\'resize image to resize x (ratio*resize)\')\r\n    parser.add_argument(\'--mod\', type=str, default=\'erfnet\', help=\'model to train\')\r\n    parser.add_argument(\'--layers\', type=int, default=18, help=\'amount of layers in model\')\r\n    parser.add_argument(""--pool"", type=str2bool, nargs=\'?\', const=True, default=True, help=""use pooling"")\r\n    parser.add_argument(""--draw_testset"", type=str2bool, nargs=\'?\', const=True, default=False, help=""plot testset results"")\r\n    parser.add_argument(""--pretrained"", type=str2bool, nargs=\'?\', const=True, default=False, help=""use pretrained model"")\r\n    parser.add_argument(\'--pretrain_epochs\', type=int, default=20, help=\'Number of epochs to perform segmentation pretraining\')\r\n    parser.add_argument(\'--skip_epochs\', type=int, default=10, help=\'Number of epochs to skip lsq layer\')\r\n    parser.add_argument(\'--channels_in\', type=int, default=3, help=\'num channels of input image\')\r\n    parser.add_argument(\'--norm\', type=str, default=\'batch\', help=\'normalisation layer you want to use\')\r\n    parser.add_argument(""--flip_on"", type=str2bool, nargs=\'?\', const=True, default=False, help=""Random flip input images"")\r\n    parser.add_argument(\'--num_train\', type=int, default=3626, help=\'Train on how many images of trainset\')\r\n    parser.add_argument(\'--split_percentage\', type=float, default=0.2, help=\'where to split dataset in train and validationset\')\r\n    parser.add_argument(\'--test_mode\', action=\'store_true\', help=\'prevents loading latest saved model\')\r\n    parser.add_argument(\'--start_epoch\', type=int, default=0, help=\'prevents loading latest saved model\')\r\n    parser.add_argument(\'--evaluate\', action=\'store_true\', help=\'only perform evaluation\')\r\n    parser.add_argument(\'--resume\', type=str, default=\'\', help=\'resume latest saved run\')\r\n    # Optimizer settings\r\n    parser.add_argument(\'--optimizer\', type=str, default=\'adam\', help=\'adam or sgd\')\r\n    parser.add_argument(\'--weight_init\', type=str, default=\'kaiming\', help=\'normal, xavier, kaiming, orhtogonal weights initialisation\')\r\n    parser.add_argument(\'--weight_decay\', type=float, default=0, help=\'L2 weight decay/regularisation on?\')\r\n    parser.add_argument(\'--lr_decay\', action=\'store_true\', help=\'decay learning rate with rule\')\r\n    parser.add_argument(\'--niter\', type=int, default=50, help=\'# of iter at starting learning rate\')\r\n    parser.add_argument(\'--niter_decay\', type=int, default=400, help=\'# of iter to linearly decay learning rate to zero\')\r\n    parser.add_argument(\'--lr_policy\', default=None, help=\'learning rate policy: lambda|step|plateau\')\r\n    parser.add_argument(\'--lr_decay_iters\', type=int, default=30, help=\'multiply by a gamma every lr_decay_iters iterations\')\r\n    parser.add_argument(\'--clip_grad_norm\', type=int, default=0, help=\'performs gradient clipping\')\r\n    # Fitting layer settings\r\n    parser.add_argument(\'--order\', type=int, default=2, help=\'order of polynomial for curve fitting\')\r\n    parser.add_argument(\'--activation_layer\', type=str, default=\'square\', help=\'Which activation after decoder do you want?\')\r\n    parser.add_argument(\'--reg_ls\', type=float, default=0, help=\'Regularization term for matrix inverse\')\r\n    parser.add_argument(\'--no_ortho\', action=\'store_true\', help=\'if no ortho transformation is desired\')\r\n    parser.add_argument(\'--mask_percentage\', type=float, default=0.3, help=\'mask to apply where birds eye view is not defined\')\r\n    parser.add_argument(""--use_cholesky"", type=str2bool, nargs=\'?\', const=True, default=False, help=""use cholesky decomposition"")\r\n    parser.add_argument(\'--activation_net\', type=str, default=\'relu\', help=\'activation in network used\')\r\n    # Paths settings\r\n    # parser.add_argument(\'--image_dir\', type=str, default=\'/usr/data/tmp/Lane_Detection/DATASET/images/\', help=\'directory to image dir\')\r\n    # parser.add_argument(\'--gt_dir\', type=str, default=\'/usr/data/tmp/Lane_Detection/DATASET/ground_truth/\', help=\'directory to gt\')\r\n    parser.add_argument(\'--image_dir\', type=str, required=True, help=\'directory to image dir\')\r\n    parser.add_argument(\'--gt_dir\', type=str, required=True, help=\'directory to gt\')\r\n    parser.add_argument(\'--test_dir\', type=str, default=\'/usr/data/tmp/Lane_Detection/TESTSET/\', help=\'Path to testset\')\r\n    parser.add_argument(\'--save_path\', type=str, default=\'Saved/\', help=\'directory to gt\')\r\n    parser.add_argument(\'--json_file\', type=str, default=\'Labels/Curve_parameters.json\', help=\'directory to json input\')\r\n    # LOSS settings\r\n    parser.add_argument(\'--weight_seg\', type=int, default=30, help=\'weight in loss criterium for segmentation\')\r\n    parser.add_argument(\'--weight_class\', type=float, default=1, help=\'weight in loss criterium for classification branch\')\r\n    parser.add_argument(\'--weight_fit\', type=float, default=1, help=\'weight in loss criterium for fit\')\r\n    parser.add_argument(\'--loss_policy\', type=str, default=\'area\', help=\'use area_loss, homography_mse or classical mse in birds eye view\')\r\n    parser.add_argument(\'--weight_funct\', type=str, default=\'none\', help=\'apply weight function in birds eye when computing area loss\')\r\n    parser.add_argument(""--end_to_end"", type=str2bool, nargs=\'?\', const=True, default=True, help=""regression towards curve params by network or postprocessing"")\r\n    parser.add_argument(""--no_mapping"", type=str2bool, nargs=\'?\', const=True, default=False, help=""no birds eye view"")\r\n    parser.add_argument(\'--gamma\', type=float, default=0., help=\'factor to decay learning rate every lr_decay_iters with\')\r\n    parser.add_argument(""--clas"", type=str2bool, nargs=\'?\', const=True, default=False, help=""Horizon and line classification tasks"")\r\n    # CUDNN usage\r\n    parser.add_argument(""--cudnn"", type=str2bool, nargs=\'?\', const=True, default=True, help=""cudnn optimization active"")\r\n    # Tensorboard settings\r\n    parser.add_argument(""--no_tb"", type=str2bool, nargs=\'?\', const=True, default=True, help=""Use tensorboard logging by tensorflow"")\r\n    # Print settings\r\n    parser.add_argument(\'--print_freq\', type=int, default=500, help=\'padding\')\r\n    parser.add_argument(\'--save_freq\', type=int, default=100, help=\'padding\')\r\n    # Skip batch\r\n    parser.add_argument(\'--list\', type=int, nargs=\'+\', default=[954, 2789], help=\'Images you want to skip\')\r\n    return parser\r\n\r\n\r\n\r\n\r\ndef get_homography(resize=256, no_mapping=False):\r\n    if no_mapping:\r\n        M = np.identity(3)\r\n        M_inv = np.identity(3)\r\n    else:\r\n        y_start = 0.20*resize\r\n        y_stop = resize-1\r\n        src = np.float32([[0.45*(2*resize),y_start],\r\n                          [0.55*(2*resize), y_start],\r\n                          [0.02*(2*resize),y_stop],\r\n                          [0.97*(2*resize), y_stop]])\r\n        dst = np.float32([[0.45*(2*resize), y_start],\r\n                          [0.55*(2*resize), y_start],\r\n                          [0.45*(2*resize), y_stop],\r\n                          [0.55*(2*resize),y_stop]])\r\n        M = cv2.getPerspectiveTransform(src, dst)\r\n        M_inv = cv2.getPerspectiveTransform(dst, src)\r\n    return M, M_inv\r\n\r\n\r\ncolormap= [(255,0,0), (0,255,0), (255,255,0), (0,0,255)]\r\nh_samples = (np.arange(160,720,10)-80) / 2.5\r\n\r\ndef save_weightmap(train_or_val, weightmap_zeros,\r\n                   x_cal0, x_cal1, x_cal2, x_cal3, \r\n                   gt0, gt1, gt2, gt3,\r\n                   gt, idx, i, images, no_ortho, resize, save_path, nclasses, no_mapping=False):\r\n    # Get homography matrices\r\n    M, M_inv = get_homography(resize, no_mapping=no_mapping)\r\n\r\n    # Sum weight maps\r\n    wm0_zeros = weightmap_zeros.data.cpu()[0, 0].numpy()\r\n    wm1_zeros = weightmap_zeros.data.cpu()[0, 1].numpy()\r\n    wm = wm0_zeros/np.max(wm0_zeros) + wm1_zeros/np.max(wm1_zeros)\r\n    if nclasses > 2:\r\n        wm2_zeros = weightmap_zeros.data.cpu()[0, 2].numpy()\r\n        wm3_zeros = weightmap_zeros.data.cpu()[0, 3].numpy()\r\n        wm += wm2_zeros/np.max(wm2_zeros) + wm3_zeros/np.max(wm3_zeros)\r\n\r\n    # Get imgs and lanes coordinates to plot\r\n    img = images[0].permute(1, 2, 0).data.cpu().numpy()\r\n    img_gt = np.copy(img)\r\n    gt_orig = gt[0].data.cpu().numpy()\r\n    lanes_pred = [x_cal0, x_cal1, x_cal2, x_cal3]\r\n    lanes_gt = [gt0, gt1, gt2, gt3]\r\n\r\n    # Warp input\r\n    warped_img = cv2.warpPerspective(np.asarray(img), M, (2*resize,resize))\r\n\r\n    # Visualize lane line coordinates\r\n    for index in range(nclasses):\r\n        lane_pred = lanes_pred[index].data.cpu().numpy()[0]\r\n        lane_gt = lanes_gt[index].data.cpu().numpy()[0]\r\n        points_pred = [(x,y) for x,y in zip(lane_pred, h_samples) if x!=0]\r\n        points_gt = [(x,y) for x,y in zip(lane_gt, h_samples) if x!=-2]\r\n        for pt in points_pred:\r\n            img = cv2.circle(np.array(img), tuple(np.int32(pt)), color=colormap[index], radius = 2)\r\n        for pt_gt in points_gt:\r\n            img_gt = cv2.circle(np.array(img_gt), tuple(np.int32(pt_gt)), color=colormap[index], radius = 2)\r\n\r\n    img = np.clip(img, 0, 1)\r\n    img_gt = np.clip(img_gt, 0, 1)\r\n    warped_img = np.clip(warped_img, 0, 1)\r\n    fig = plt.figure()\r\n    ax1 = fig.add_subplot(321)\r\n    ax2 = fig.add_subplot(322)\r\n    ax3 = fig.add_subplot(323)\r\n    ax4 = fig.add_subplot(324)\r\n    ax5 = fig.add_subplot(325)\r\n    ax1.imshow(img_gt)\r\n    ax1.set_title(\'Ground Truth\')\r\n    ax2.imshow(img)\r\n    ax2.set_title(\'Prediction LSQ\')\r\n    ax3.imshow(wm)\r\n    ax3.set_title(\'Weight Maps\')\r\n    ax4.imshow(gt_orig)\r\n    ax4.set_title(\'Segmentation ground truth \')\r\n    # plt.subplots_adjust(left=0.2, right=0.68)\r\n    # plt.colorbar(plot4)\r\n    ax5.imshow(warped_img)\r\n    ax5.set_title(\'Warped img\')\r\n    fig.savefig(save_path + \'/example/{}/weight_idx-{}_batch-{}\'.format(train_or_val, idx, i))\r\n    plt.clf()\r\n    plt.close(fig)\r\n\r\n\r\ndef test_projective_transform(input, resize, M):\r\n    # test grid using built in F.grid_sampler method.\r\n    M_scaledup = np.array([[M[0,0],M[0,1]*2,M[0,2]*(2*resize-1)],[0,M[1,1],M[1,2]*(resize-1)],[0,M[2,1]/(resize-1),M[2,2]]])\r\n    inp = cv2.warpPerspective(np.asarray(input), M_scaledup, (2*resize,resize))\r\n#    y_start = 0.3*(resize-1)\r\n#    y_stop = resize-1\r\n#\r\n#    #normalized x and y (720x1280 is original aspect ratio)\r\n#    src = np.float32([[0.45*(2*resize-1),y_start],[0.55*(2*resize-1), y_start],[0.1*(2*resize-1),y_stop],[0.9*(2*resize-1), y_stop]])\r\n#    dst = np.float32([[0.45*(2*resize-1), y_start],[0.55*(2*resize-1), y_start],[0.45*(2*resize-1), y_stop],[0.55*(2*resize-1),y_stop]])\r\n#    M_orig = cv2.getPerspectiveTransform(src, dst)\r\n#    inp = cv2.warpPerspective(np.asarray(input), M_orig, (2*resize,resize))\r\n    return inp, M_scaledup\r\n\r\n\r\ndef draw_fitted_line(img, params, resize, color=(255,0,0)):\r\n    params = params.data.cpu().tolist()\r\n    y_stop = 0.7\r\n    y_prime = np.linspace(0, y_stop, 20)\r\n    params = [0] * (4 - len(params)) + params\r\n    d, a, b, c = [*params]\r\n    x_pred = d*(y_prime**3) + a*(y_prime)**2 + b*(y_prime) + c\r\n    x_pred = x_pred*(2*resize-1)\r\n    y_prime = (1-y_prime)*(resize-1)\r\n    lane = [(xcord, ycord) for (xcord, ycord) in zip(x_pred, y_prime)] \r\n    img = cv2.polylines(img, [np.int32(lane)], isClosed = False, color = color,thickness = 1)\r\n    return img, lane\r\n\r\n\r\ndef draw_horizon(img, horizon, resize=256, color=(255,0,0)):\r\n    x = np.arange(2*resize-1)\r\n    horizon_line = [(x_cord, horizon+1) for x_cord in x]\r\n    img = cv2.polylines(img.copy(), [np.int32(horizon_line)], isClosed = False, color = color,thickness = 1)\r\n    return img\r\n\r\n\r\ndef draw_homography_points(img, x, resize=256, color=(255,0,0)):\r\n    y_start1 = (0.3+x[2])*(resize-1)\r\n    y_start = 0.3*(resize-1)\r\n    y_stop = resize-1\r\n    src = np.float32([[0.45*(2*resize-1),y_start],[0.55*(2*resize-1), y_start],[0.1*(2*resize-1),y_stop],[0.9*(2*resize-1), y_stop]])\r\n    dst = np.float32([[(0.45+x[0])*(2*resize-1), y_start1],[(0.55+x[1])*(2*resize-1), y_start1],[(0.45+x[0])*(2*resize-1), y_stop],[(0.55+x[1])*(2*resize-1),y_stop]])\r\n    dst_ideal = np.float32([[0.45*(2*resize-1), y_start],[0.55*(2*resize-1), y_start],[0.45*(2*resize-1), y_stop],[0.55*(2*resize-1),y_stop]])\r\n    [cv2.circle(np.asarray(img), tuple(idx), radius=5, thickness=-1, color=(255,0,0)) for idx in src]\r\n    [cv2.circle(np.asarray(img), tuple(idx), radius=5, thickness=-1, color=(0,255,0)) for idx in dst_ideal]\r\n    [cv2.circle(np.asarray(img), tuple(idx), radius=5, thickness=-1, color=(0,0,255)) for idx in dst]\r\n    return img\r\n\r\n\r\ndef save_image(output, gt_params, i=1, resize=320):\r\n    outputs_seg = output.permute(0,2,3,1)\r\n    im = np.asarray(outputs_seg.data.cpu()[0])\r\n    im = draw_fitted_line(im,gt_params[0],resize)\r\n    im = Image.fromarray(im.astype(\'uint8\'), \'RGB\')\r\n    im.save(\'simple_net/simple_net_train/{}.png\'.format(i[0]))\r\n\r\n\r\ndef save_output(output,gt_params, i=1):\r\n    output = output*255/(torch.max(output))\r\n    output = output.permute(0,2,3,1)\r\n    im = np.asarray(output.data.cpu()[0]).squeeze(2)\r\n    \r\n#    im = draw_fitted_line(im,gt_params[0],resize)\r\n    im = Image.fromarray(im.astype(\'uint8\'), \'P\')\r\n    im.save(\'simple_net/simple_net_output/{}.png\'.format(i[0]))\r\n\r\n\r\ndef line_right_eq(x):\r\n    y = 0.438/0.7*x + 0.56 #0.7/0.438*(x-0.56)\r\n    return y\r\n\r\n\r\ndef line_left_eq(x):\r\n    y = -x*0.438/0.7 + 0.44#-0.7/0.438*(x-0.44)\r\n    return y\r\n\r\n\r\ndef f(x, *params):\r\n    \'\'\'\r\n    Constructs objective function which will be solved iteratively\r\n    \'\'\'\r\n    a, b, c, left_or_right = params\r\n    if left_or_right == \'left\':\r\n        funct = a*x**2 + b*x + c - line_left_eq(x)\r\n    else:\r\n        funct = a*x**2 + b*x + c - line_right_eq(x)\r\n    return funct\r\n\r\n\r\ndef draw_mask_line(im, beta0, beta1, beta2, beta3, resize):\r\n    beta0 = beta0.data.cpu().tolist()\r\n    beta1 = beta1.data.cpu().tolist()\r\n    beta2 = beta2.data.cpu().tolist()\r\n    beta3 = beta3.data.cpu().tolist()\r\n    params0 = *beta0, \'left\'\r\n    params1 = *beta1, \'right\'\r\n    params2 = *beta2, \'left\'\r\n    params3 = *beta3, \'right\'\r\n    x, y, order = [], [], []\r\n    max_lhs = fsolve(f, 0.05, args=params0)\r\n    if max_lhs > 0:\r\n        x.append(line_left_eq(max_lhs[0]))\r\n        y.append(1-max_lhs[0])\r\n        order.append(0)\r\n    else:\r\n        max_lhs = 0\r\n    max_rhs = fsolve(f, 0.05, args=params1)\r\n    if max_rhs > 0:\r\n        x.append(line_right_eq(max_rhs[0]))\r\n        y.append(1-max_rhs[0])\r\n        order.append(1)\r\n    else:\r\n        max_rhs = 0 \r\n    max_left = fsolve(f, 0.05, args=params2)\r\n    if max_left > 0:\r\n        x.append(line_left_eq(max_left[0]))\r\n        y.append(1-max_left[0])\r\n        order.append(2)\r\n    else:\r\n        max_left = 0\r\n    max_right = fsolve(f, 0.05, args=params3)\r\n    if max_right > 0:\r\n        x.append(line_right_eq(max_right[0]))\r\n        y.append(1-max_right[0])\r\n        order.append(3)\r\n    else:\r\n        max_right = 0 \r\n    y_stop = 1\r\n    y_prime = np.linspace(0, y_stop, 40)\r\n    x_prime_right = line_right_eq(y_prime)\r\n    x_prime_left = line_left_eq(y_prime)\r\n    y_prime, x_prime_lft, x_prime_rght = (1-y_prime)*(resize-1), x_prime_left*(2*resize-1), x_prime_right*(2*resize-1)\r\n    line_right = [(xcord, ycord) for (xcord, ycord) in zip(x_prime_rght, y_prime)] \r\n    line_left = [(xcord, ycord) for (xcord, ycord) in zip(x_prime_lft, y_prime)] \r\n    im = cv2.polylines(im, [np.int32(line_right)], isClosed = False, color = (255,0,0),thickness = 1)\r\n    im = cv2.polylines(im, [np.int32(line_left)], isClosed = False, color = (255,0,0),thickness = 1)\r\n    x = np.array(x)\r\n    y = np.array(y)\r\n    x_left, x_right = line_left_eq(max_left)*(2*resize-1), line_right_eq(max_right)*(2*resize-1)\r\n    y_left, y_right = (1-max_left)*(resize-1), (1-max_right)*(resize-1)\r\n    cv2.circle(np.asarray(im), (x_left,y_left), radius=3, thickness=-1, color=(0,0,255))\r\n    cv2.circle(np.asarray(im), (x_right,y_right), radius=3, thickness=-1, color=(0,0,255))\r\n    x_prime, y_prime = homogenous_transformation(x,y)\r\n    maxima = np.zeros(4)\r\n    for i, idx in enumerate(order):\r\n        maxima[idx] = y_prime[i]\r\n    return im, np.int_(np.round(x_prime*(2*resize-1))), np.int_(np.round(y_prime*(resize-1)))\r\n\r\n\r\ndef homogenous_transformation(x,y):\r\n    """"""\r\n    Helper function to transform coordionates defined by transformation matrix\r\n    \r\n    Args:\r\n            Matrix (multi dim - array): Transformation matrix\r\n            x (array): original x coordinates\r\n            y (array): original y coordinates\r\n    """"""\r\n    y_start = 0.3\r\n    y_stop = 1\r\n    src = np.float32([[0.45,y_start],[0.55, y_start],[0.1,y_stop],[0.9, y_stop]])\r\n    dst = np.float32([[0.45, y_start],[0.55, y_start],[0.45, y_stop],[0.55,y_stop]])\r\n    M_inv = cv2.getPerspectiveTransform(dst,src)\r\n    \r\n    ones = np.ones((1,len(y)))\r\n    coordinates = np.vstack((x, y, ones))\r\n    trans = np.matmul(M_inv, coordinates)\r\n            \r\n    x_vals = trans[0,:]/trans[2,:]\r\n    y_vals = trans[1,:]/trans[2,:]\r\n    return x_vals, y_vals\r\n\r\n\r\ndef first_run(save_path):\r\n    txt_file = os.path.join(save_path,\'first_run.txt\')\r\n    if not os.path.exists(txt_file):\r\n        open(txt_file, \'w\').close()\r\n    else:\r\n        saved_epoch = open(txt_file).read()\r\n        if saved_epoch is None:\r\n            print(\'You forgot to delete [first run file]\')\r\n            return \'\' \r\n        return saved_epoch\r\n    return \'\'\r\n\r\n\r\ndef mkdir_if_missing(directory):\r\n    if not os.path.exists(directory):\r\n        try:\r\n            os.makedirs(directory)\r\n        except OSError as e:\r\n            if e.errno != errno.EEXIST:\r\n                raise\r\n\r\n\r\n# trick from stackoverflow\r\ndef str2bool(argument):\r\n    if argument.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\r\n        return True\r\n    elif argument.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\r\n        return False\r\n    else:\r\n        raise argparse.ArgumentTypeError(\'Wrong argument in argparse, should be a boolean\')\r\n\r\n\r\nclass Logger(object):\r\n    """"""\r\n    Source https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\r\n    """"""\r\n    def __init__(self, fpath=None):\r\n        self.console = sys.stdout\r\n        self.file = None\r\n        self.fpath = fpath\r\n        if fpath is not None:\r\n            mkdir_if_missing(os.path.dirname(fpath))\r\n            self.file = open(fpath, \'w\')\r\n\r\n    def __del__(self):\r\n        self.close()\r\n\r\n    def __enter__(self):\r\n        pass\r\n\r\n    def __exit__(self, *args):\r\n        self.close()\r\n\r\n    def write(self, msg):\r\n        self.console.write(msg)\r\n        if self.file is not None:\r\n            self.file.write(msg)\r\n\r\n    def flush(self):\r\n        self.console.flush()\r\n        if self.file is not None:\r\n            self.file.flush()\r\n            os.fsync(self.file.fileno())\r\n\r\n    def close(self):\r\n        self.console.close()\r\n        if self.file is not None:\r\n            self.file.close()\r\n\r\n\r\nclass AverageMeter(object):\r\n    """"""Computes and stores the average and current value""""""\r\n    def __init__(self):\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.val = 0\r\n        self.avg = 0\r\n        self.sum = 0\r\n        self.count = 0\r\n\r\n    def update(self, val, n=1):\r\n        self.val = val\r\n        self.sum += val * n\r\n        self.count += n\r\n        self.avg = self.sum / self.count\r\n\r\n\r\ndef define_optim(optim, params, lr, weight_decay):\r\n    if optim == \'adam\':\r\n        optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\r\n    elif optim == \'sgd\':\r\n        optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\r\n    elif optim == \'rmsprop\':\r\n        optimizer = torch.optim.RMSprop(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\r\n    else:\r\n        raise KeyError(""The requested optimizer: {} is not implemented"".format(optim))\r\n    return optimizer\r\n\r\n\r\ndef define_scheduler(optimizer, args):\r\n    if args.lr_policy == \'lambda\':\r\n        def lambda_rule(epoch):\r\n            lr_l = 1.0 - max(0, epoch + 1 - args.niter) / float(args.niter_decay + 1)\r\n            return lr_l\r\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\r\n    elif args.lr_policy == \'step\':\r\n        scheduler = lr_scheduler.StepLR(optimizer,\r\n                                        step_size=args.lr_decay_iters, gamma=args.gamma)\r\n    elif args.lr_policy == \'plateau\':\r\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\r\n                                                   factor=args.gamma,\r\n                                                   threshold=0.0001,\r\n                                                   patience=args.lr_decay_iters)\r\n    elif args.lr_policy == \'none\':\r\n        scheduler = None\r\n    else:\r\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', args.lr_policy)\r\n    return scheduler\r\n\r\n\r\ndef define_init_weights(model, init_w=\'normal\', activation=\'relu\'):\r\n    print(\'Init weights in network with [{}]\'.format(init_w))\r\n    if init_w == \'normal\':\r\n        model.apply(weights_init_normal)\r\n    elif init_w == \'xavier\':\r\n        model.apply(weights_init_xavier)\r\n    elif init_w == \'kaiming\':\r\n        model.apply(weights_init_kaiming)\r\n    elif init_w == \'orthogonal\':\r\n        model.apply(weights_init_orthogonal)\r\n    else:\r\n        raise NotImplementedError(\'initialization method [{}] is not implemented\'.format(init_w))\r\n\r\n\r\ndef weights_init_normal(m):\r\n    classname = m.__class__.__name__\r\n#    print(classname)\r\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\r\n        init.normal_(m.weight.data, 0.0, 0.02)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.normal_(m.weight.data, 0.0, 0.02)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal_(m.weight.data, 1.0, 0.02)\r\n        init.constant_(m.bias.data, 0.0)\r\n\r\n\r\ndef weights_init_xavier(m):\r\n    classname = m.__class__.__name__\r\n    # print(classname)\r\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\r\n        init.xavier_normal_(m.weight.data, gain=0.02)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.xavier_normal_(m.weight.data, gain=0.02)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal_(m.weight.data, 1.0, 0.02)\r\n        init.constant_(m.bias.data, 0.0)\r\n\r\n\r\ndef weights_init_kaiming(m):\r\n    classname = m.__class__.__name__\r\n    # print(classname)\r\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\r\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\', nonlinearity=\'relu\')\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\', nonlinearity=\'relu\')\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal_(m.weight.data, 1.0, 0.02)\r\n        init.constant_(m.bias.data, 0.0)\r\n\r\n\r\ndef weights_init_orthogonal(m):\r\n    classname = m.__class__.__name__\r\n#    print(classname)\r\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\r\n        init.orthogonal(m.weight.data, gain=1)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.orthogonal(m.weight.data, gain=1)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal_(m.weight.data, 1.0, 0.02)\r\n        init.constant_(m.bias.data, 0.0)\r\n'"
Birds_Eye_View_Loss/Dataloader/Load_Data_new.py,8,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport os\nimport torch\nfrom torch.autograd import Variable\nfrom PIL import Image, ImageOps\nimport cv2\nimport json\nimport numbers\nimport random\nimport warnings\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\nfrom torch.utils.data.dataloader import default_collate\nwarnings.simplefilter(\'ignore\', np.RankWarning)\n\n\nclass LaneDataset(Dataset):\n    """"""Dataset with labeled lanes""""""\n    def __init__(self, end_to_end, valid_idx, json_file, image_dir, gt_dir, flip_on, resize):\n        """"""\n        Args: valid_idx (list)  : Indices of validation images\n              json_file (string): File with labels\n              image_dir (string): Path to the images\n              gt_dir    (string): Path to the ground truth images\n              flip on   (bool)  : Boolean to flip images randomly\n              end_to_end (bool) : Boolean to compute lane line params end to end\n              resize    (int)   : Height of resized image\n        """"""\n        line_file = \'Labels/label_new.json\'\n        self.image_dir = image_dir\n        self.gt_dir = gt_dir\n        self.valid_idx = valid_idx\n        self.flip_on = flip_on\n        self.resize = resize\n        self.totensor = transforms.ToTensor()\n        self.params = [json.loads(line) for line in open(json_file).readlines()]\n        self.line_file = [json.loads(line) for line in open(line_file).readlines()]\n        self.end_to_end = end_to_end\n        self.rgb_lst = sorted(os.listdir(image_dir))\n        self.gt_lst = sorted(os.listdir(gt_dir))\n        self.num_imgs = len(self.rgb_lst)\n        assert len(self.rgb_lst) == len(self.gt_lst) == 2535\n\n        target_idx = [int(i.split(\'.\')[0]) for i in self.rgb_lst]\n        self.valid_idx = [target_idx[i]-1 for i in valid_idx]\n\n    def __len__(self):\n        """"""\n        Conventional len method\n        """"""\n        return self.num_img\n\n    def __getitem__(self, idx):\n        """"""\n        Args: idx (int): Index in list to load image\n        """"""\n        assert self.rgb_lst[idx].split(\'.\')[0] == self.gt_lst[idx].split(\'.\')[0]\n        img_name = os.path.join(self.image_dir, self.rgb_lst[idx])\n        gt_name = os.path.join(self.gt_dir, self.gt_lst[idx])\n        with open(img_name, \'rb\') as f:\n            image = (Image.open(f).convert(\'RGB\'))\n        with open(gt_name, \'rb\') as f:\n            gt = (Image.open(f).convert(\'P\'))\n        idx = int(self.rgb_lst[idx].split(\'.\')[0]) - 1\n        params = self.params[idx][""poly_params""]\n        line_lst = self.line_file[idx][""lines""]\n\n        w, h = image.size\n        image, gt = F.crop(image, h-640, 0, 640, w), F.crop(gt, h-640, 0, 640, w)\n        image = F.resize(image, size=(self.resize, 2*self.resize), interpolation=Image.BILINEAR)\n        gt = F.resize(gt, size=(self.resize, 2*self.resize), interpolation=Image.NEAREST)\n        gt = np.asarray(gt).copy()\n        idx3 = np.isin(gt, 3)\n        idx4 = np.isin(gt, 4)\n        gt[idx3] = 0\n        gt[idx4] = 0\n        # params = [params[0], params[1]]\n        hflip_input = np.random.uniform(0.0, 1.0) > 0.5 and self.flip_on\n        if idx not in self.valid_idx and hflip_input:\n            image, gt = F.hflip(image), F.hflip(gt)\n            line_lst = mirror_list(line_lst)\n\n            idx1 = np.isin(gt, 1)\n            idx2 = np.isin(gt, 2)\n            gt[idx1] = 2\n            gt[idx2] = 1\n            params = [params[1], params[0], params[3], params[2]]\n            params = np.array(params)\n            params = -params\n            params[:, -1] = 1+params[:, -1]\n\n\n        gt = Image.fromarray(gt)\n        params = torch.from_numpy(np.array(params)).float()\n        image, gt = self.totensor(image).float(), (self.totensor(gt)*255).long()\n\n        y_val = gt.nonzero()[0, 1]\n        horizon = torch.zeros(gt.size(1))\n        horizon[0:y_val] = 1\n        line_lst = np.array(line_lst[3:7])\n        line_lst = torch.from_numpy(np.array(line_lst + 1))\n        line_lst = line_lst.long()\n        horizon = horizon.float()\n\n        if idx in self.valid_idx:\n            index = self.valid_idx.index(idx)\n            return image, gt, params, idx, line_lst, horizon, index\n        return image, gt, params, idx, line_lst, horizon\n\n\ndef mirror_list(lst):\n    \'\'\'\n    Mirror lists of lane and line classification ground truth in order to make flipping possible\n    \'\'\'\n    middle = len(lst)//2\n    first = list(reversed(lst[:middle]))\n    second = list(reversed(lst[middle:]))\n    return second + first\n\n\ndef homogenous_transformation(Matrix, x, y):\n    """"""\n    Helper function to transform coordionates defined by transformation matrix\n\n    Args:\n            Matrix (multi dim - array): Transformation matrix\n            x (array): original x coordinates\n            y (array): original y coordinates\n    """"""\n    ones = np.ones((1,len(y)))\n    coordinates = np.vstack((x, y, ones))\n    trans = np.matmul(Matrix, coordinates)\n\n    x_vals = trans[0,:]/trans[2,:]\n    y_vals = trans[1,:]/trans[2,:]\n    return x_vals, y_vals\n\n\ndef get_homography(resize=320):\n    factor = resize/640\n    y_start = 0.3*(resize-1)\n    y_stop = (resize-1)\n    src = np.float32([[0.45*(2*resize-1),y_start],\n                      [0.55*(2*resize-1), y_start],\n                      [0.1*(2*resize-1),y_stop],\n                      [0.9*(2*resize-1), y_stop]])\n    dst = np.float32([[0.45*(2*resize-1), y_start],\n                      [0.55*(2*resize-1), y_start],\n                      [0.45*(2*resize-1), y_stop],\n                      [0.55*(2*resize-1),y_stop]])\n    M = cv2.getPerspectiveTransform(src, dst)\n    M_inv = cv2.getPerspectiveTransform(dst, src)\n    return M_inv\n\n\nclass Scale(object):\n    """"""Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or tuple): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    """"""\n\n    def __init__(self, output_size, type_dataset, interpolation=Image.BILINEAR):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n        self.interpolation = interpolation\n        self.type = type_dataset\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n\n        h, w = image.size\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n        img = image.resize((new_h, new_w), Image.BILINEAR)\n        if self.type == \'train\':\n            gt = sample[\'ground truth\']\n            gt = gt.resize((new_h, new_w), Image.NEAREST)\n            return {\'image\': (img), \'ground truth\': (gt)}\n        else:\n            return {\'image\': np.asarray(img)}\n\n\nclass Crop(object):\n    """"""Crop the given PIL Image.\n    Args:\n        img (PIL Image): Image to be cropped.\n        i: Upper pixel coordinate.\n        j: Left pixel coordinate.\n        h: Height of the cropped image.\n        w: Width of the cropped image.\n    Returns:\n        PIL Image: Cropped image.\n    """"""\n    def __init__(self, start_y, type_dataset):\n        self.i = start_y\n        self.type = type_dataset\n\n    def __call__(self, sample):\n\n        image = sample[\'image\'] \n        image = image.crop((0, self.i, 1280, 720))\n        if self.type == \'train\':\n            gt = sample[\'ground truth\']\n            gt = gt.crop((0, self.i, 1280, 720))\n            return {\'image\': image, \'ground truth\': gt}\n        else:\n            return {\'image\': image}\n\n\nclass RandomCrop(object):\n    """"""Crop the given PIL Image at a random location.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n        padding (int or sequence, optional): Optional padding on each border\n            of the image. Default is 0, i.e no padding. If a sequence of length\n            4 is provided, it is used to pad left, top, right, bottom borders\n            respectively.\n    """"""\n\n    def __init__(self, size=(280, 560), padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    @staticmethod\n    def get_params(img, output_size):\n        """"""Get parameters for ``crop`` for a random crop.\n        Args:\n            img (PIL Image): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n        """"""\n        w, h = img.size\n        th, tw = output_size\n        if w == tw and h == th:\n            return 0, 0, h, w\n\n        i = random.randint(0, h - th)\n        j = random.randint(0, w - tw)\n        return i, j, th, tw\n\n    def __call__(self, sample):\n        """"""\n        Args:\n            img (PIL Image): Image to be cropped.\n        Returns:\n            PIL Image: Cropped image.\n        """"""\n        img, gt = sample[\'image\'], sample[\'ground truth\']\n        \n        if self.padding > 0:\n            img = ImageOps.expand(img, border=self.padding, fill=0)\n            gt = ImageOps.expand(gt, border=self.padding, fill=0)\n\n        i, j, h, w = self.get_params(img, self.size)\n        img = img.crop((j, i, j + w, i + h))\n        gt = gt.crop((j, i, j + w, i + h))\n\n        return {\'image\': img, \'ground truth\': gt}\n\n\ndef plot_dataloader_batch(batch):\n    images_batch = batch[\'image\']\n    plt.figure()\n    grid_im = utils.make_grid(images_batch)\n    plt.imshow(grid_im.numpy().transpose((1, 2, 0)))\n\n\ndef get_loader(num_train, json_file, image_dir, gt_dir, flip_on, batch_size,\n               shuffle, num_workers, end_to_end, resize, split_percentage=0.2):\n    \'\'\'\n    Splits dataset in training and validation set and creates dataloaders\n    \'\'\'\n    indices = list(range(num_train))\n    split = int(np.floor(split_percentage*num_train))\n\n    if shuffle is True:\n        np.random.seed(num_train)\n        np.random.shuffle(indices)\n    train_idx, valid_idx = indices[split:], indices[:split]\n    train_idx = train_idx[0:len(train_idx)//batch_size*batch_size]\n    valid_idx = valid_idx[0:len(valid_idx)//batch_size*batch_size]\n    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n    valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n\n    transformed_dataset = LaneDataset(end_to_end=end_to_end,\n                                      valid_idx=valid_idx,\n                                      json_file=json_file,\n                                      image_dir=image_dir,\n                                      gt_dir=gt_dir,\n                                      flip_on=flip_on,\n                                      resize=resize)\n\n    train_loader = DataLoader(transformed_dataset,\n                              batch_size=batch_size, sampler=train_sampler,\n                              num_workers=num_workers, pin_memory=True) #, collate_fn=my_collate)\n\n    valid_loader = DataLoader(transformed_dataset,\n                              batch_size=batch_size, sampler=valid_sampler,\n                              num_workers=num_workers, pin_memory=True) #collate_fn=my_collate)\n\n    return train_loader, valid_loader, valid_idx\n\n\ndef my_collate(batch):\n    batch = [dict for dict in batch if dict[\'4_lanes\'] is True]\n    return default_collate(batch)\n\n\ndef write_lsq_results(src_file, dst_file, nclasses, all_branches_ready, \n                      horizon_on, resize, no_ortho, calc_intersection=False, \n                      draw_image=False, path_test_set = \'../../../\', test_phase=False):\n    \'\'\'\n    Computes json file with point coordinates for every lane\n    \'\'\'\n    factor = 640/resize\n    y_start = 0.3\n    y_stop = 1\n    src = np.float32([[0.45,y_start],[0.55, y_start],[0.1,y_stop],[0.9, y_stop]])\n    dst = np.float32([[0.45, y_start],[0.55, y_start],[0.45, y_stop],[0.55,y_stop]])\n    M = cv2.getPerspectiveTransform(src,dst)\n    M_inv = cv2.getPerspectiveTransform(dst,src)\n    lines = [json.loads(line) for line in open(src_file).readlines()]\n    with open(dst_file, \'w\') as jsonFile:\n        for i, line in enumerate(lines):\n            h_samples = line[\'h_samples\']\n            y_orig = np.array(h_samples)\n            y_d = (np.array(h_samples)-80)/639\n            y_prime = ((M[1][1]*y_d + M[1][2])/(M[2][1]*y_d+M[2][2]))\n            y_eval = (1-y_prime)\n            lanes_json = np.full((nclasses,len(h_samples)), -2)\n            lanes = line[""lanes""]\n            params = line[""params""]\n            line_id = line[""line_id""]\n            horizon = line[""horizon_est""]\n            \n            if draw_image and i%50==0:\n                path = path_test_set + \'test_set/\' + line[""raw_file""]\n                img = plt.imread(path)\n                img = np.asarray(img)\n            \n            if calc_intersection:\n                maxima = instersection_points(params, M_inv, resize)\n                \n            no_left_line = True if line_id[0] == 0 else False\n            no_right_line = True if line_id[3] == 0 else False\n            for j in range(len(params)):\n                if test_phase:\n                    lane = lanes\n                else:\n                    lane = lanes[j]\n                if all_branches_ready:\n                    if (j==2 and no_left_line) or (j==3 and no_right_line):\n                        continue\n                else:\n                    zipped_y_vals = [(x,y) for x,y in zip(lane, h_samples) if x!=-2]\n                    if len(zipped_y_vals) == 0:\n                        continue\n                \n                h = [y for x,y in zip(lane, h_samples) if x!=-2]\n                if len(h) == 0:\n                    minimum, maximum = 250, 710\n                else:\n                    minimum, maximum = np.min(h), np.max(h) \n                if all_branches_ready and horizon_on:\n                    minimum = sum(horizon)*factor+80\n                    if calc_intersection:\n                        maximum = maxima[j]*factor+84\n                params_j = [0]*(3-len(params[j])) + params[j]\n                a,b,c = params_j\n                \n                if not no_ortho:\n                    x_new = (a*y_eval**2 + b*y_eval + c)\n                    x_new, y_new = homogenous_transformation(M_inv, x_new, y_prime)\n                else:\n                    y_new = 1-y_d\n                    x_new = (a*y_new**2 + b*y_new + c)\n                x_new, y_new = x_new*1279, y_new*639+80 \n                x_new = np.int_(np.round(x_new))\n                x_new, y_new = zip(*[(x,y) if y >= max(210,minimum) and y <= maximum else (-2,y) \n                                for x,y in zip(x_new, y_orig)])\n                \n                lanes_json[j] = list(x_new)\n                \n                if draw_image and i%50==0:\n                    pt = [(xcord, ycord) for (xcord, ycord) in zip(x_new, y_new) if xcord!=-2]\n                    for idx in pt:\n                        cv2.circle(img, idx, radius=4, thickness=-1, color=(255, 0, 0))\n            if draw_image and i%50==0:\n                img = Image.fromarray(img)\n                img.save(\'../Evaluate/Results/{}.png\'.format(i))\n            json_line = line\n            json_line[""run_time""] = 20\n            json_line[""lanes""] = lanes_json.tolist()\n            json.dump(json_line, jsonFile)\n            jsonFile.write(\'\\n\' )\n\n\ndef load_valid_set_file(valid_idx):\n    file1 = ""Labels/label_data_0313.json""\n    file2 = ""Labels/label_data_0531.json""\n    file3 = ""Labels/label_data_0601.json""\n    labels_file1 = [json.loads(line) for line in open(file1).readlines()]\n    labels_file2 = [json.loads(line) for line in open(file2).readlines()]\n    labels_file3 = [json.loads(line) for line in open(file3).readlines()]\n    len_file1 = len(labels_file1)\n    len_file2 = len(labels_file2)\n\n    with open(\'validation_set.json\', \'w\') as jsonFile:\n        for image_id in valid_idx:\n            if image_id < len_file1:\n                labels = labels_file1[image_id]\n            elif image_id < (len_file1 + len_file2):\n                image_id_new = image_id - len_file1\n                labels = labels_file2[image_id_new]\n            else:\n                image_id_new = image_id - len_file1 - len_file2\n                labels = labels_file3[image_id_new]\n\n            json.dump(labels, jsonFile)\n            jsonFile.write(\'\\n\')\n\n\ndef load_valid_set_file_all(valid_idx, target_file, image_dir):\n    file1 = ""Labels/Curve_parameters.json""\n    labels_file = [json.loads(line) for line in open(file1).readlines()]\n    content = sorted(os.listdir(image_dir))\n    target_idx = [int(i.split(\'.\')[0]) for i in content]\n    new_idx = [target_idx[i]-1 for i in valid_idx]\n    with open(target_file, \'w\') as jsonFile:\n        for image_id in new_idx:\n            labels = labels_file[image_id]\n            json.dump(labels, jsonFile)\n            jsonFile.write(\'\\n\')\n\n\ndef load_0313_valid_set_file(valid_idx, nclasses):\n    file1 = ""Labels/label_data_0313.json""\n    labels_file1 = [json.loads(line) for line in open(file1).readlines()]\n    with open(\'validation_set.json\', \'w\') as jsonFile:\n        for image_id in valid_idx:\n            labels = labels_file1[image_id]\n            labels[\'lanes\'] = labels[\'lanes\'][0:nclasses]\n            json.dump(labels, jsonFile)\n            jsonFile.write(\'\\n\')\n\n\ndef get_testloader(json_file, transform, batch_size, shuffle, num_workers, path):\n    transformed_dataset = LaneTestSet(json_file=json_file,\n                                      path=path,\n                                      transform=transform)\n\n    data_loader = DataLoader(dataset=transformed_dataset, \n                                          batch_size=batch_size,\n                                          shuffle=shuffle,\n                                          num_workers=num_workers)\n    return data_loader\n'"
Birds_Eye_View_Loss/Networks/ERFNet.py,4,"b'# ERFNet full model definition for Pytorch\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n    \n\nclass non_bottleneck_1d (nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n        \n\n    def forward(self, input):\n\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n        \n        return F.relu(output+input)    #+input = identity (residual connection)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(in_channels, 16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16,64))\n\n        for x in range(0, 5):    #5 times\n           self.layers.append(non_bottleneck_1d(64, 0.03, 1)) \n\n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 2):    #2 times\n            self.layers.append(non_bottleneck_1d(128, 0.3, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 16))\n\n        #Only in encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass UpsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.bn(output)\n        return F.relu(output)\n\nclass Decoder (nn.Module):\n    def __init__(self, num_classes, pretrain):\n        super().__init__()\n        self.pretrain = pretrain\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(UpsamplerBlock(128,64))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n\n        self.layers.append(UpsamplerBlock(64,16))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n\n        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n        if pretrain:\n            self.output_conv2 = nn.ConvTranspose2d( 16, num_classes + 1 , 2, stride=2, padding=0, output_padding=0, bias=True)\n\n\n    def forward(self, input, flag):\n        output = input\n\n        for layer in self.layers:\n            output = layer(output)\n        if self.pretrain:\n\n            if flag:\n                output = self.output_conv(output)\n            else:\n                output = self.output_conv2(output)\n        else:\n            output = self.output_conv(output)\n        return output\n\n# ERFNet\nclass Net(nn.Module):\n    def __init__(self, layers=18, in_channels=1, out_channels=1, pretrained=False, pool=False):  #use encoder to pass pretrained encoder\n        super().__init__()\n        self.encoder = Encoder(in_channels, out_channels)\n        self.decoder = Decoder(out_channels, pretrained)\n        \n    def forward(self, input, flag, only_encode=False):\n        if only_encode:\n            return self.encoder.forward(input, predict=True)\n        else:\n            encoder_output = self.encoder(input)\n            decoder_output = self.decoder.forward(encoder_output, flag)\n            return encoder_output, decoder_output\n'"
Birds_Eye_View_Loss/Networks/LSQ_layer.py,61,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nimport torch\nimport torch.optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom math import ceil\nimport cv2\nimport Networks\n\n\ndef Init_Projective_transform(nclasses, batch_size, resize):\n    # M_orig: unnormalized Transformation matrix\n    # M: normalized transformation matrix\n    # M_inv: Inverted normalized transformation matrix --> Needed for grid sample\n    # original aspect ratio: 720x1280 --> after 80 rows cropped: 640x1280 --> after resize: 256x512 (default) or resize x 2*resize (in general)\n    size = torch.Size([batch_size, nclasses, resize, 2*resize])\n    y_start = 0.3\n    y_stop = 1\n    xd1, xd2, xd3, xd4 = 0.45, 0.55, 0.45, 0.55\n    src = np.float32([[0.45, y_start], [0.55, y_start], [0.1, y_stop], [0.9, y_stop]])\n    dst = np.float32([[xd3, y_start], [xd4, y_start], [xd1, y_stop], [xd2, y_stop]])\n    M = cv2.getPerspectiveTransform(src, dst)\n    M_inv = cv2.getPerspectiveTransform(dst, src)\n    M = torch.from_numpy(M).unsqueeze_(0).expand([batch_size, 3, 3]).type(torch.FloatTensor)\n    M_inv = torch.from_numpy(M_inv).unsqueeze_(0).expand([batch_size, 3, 3]).type(torch.FloatTensor)\n    return size, M, M_inv\n\n\ndef square_tensor(x):\n    return x**2\n\n\ndef return_tensor(x):\n    return x\n\n\ndef activation_layer(activation=\'square\', no_cuda=False):\n    place_cuda = True\n    if activation == \'sigmoid\':\n        layer = nn.Sigmoid()\n    elif activation == \'relu\':\n        layer = nn.ReLU()\n    elif activation == \'softplus\':\n        layer = nn.Softplus()\n    elif activation == \'square\':\n        layer = square_tensor\n        place_cuda = False\n    elif activation == \'abs\':\n        layer = torch.abs\n        place_cuda = False\n    elif activation == \'none\':\n        layer = return_tensor\n    else:\n        raise NotImplementedError(\'Activation type: {} is not implemented\'.format(activation))\n    if not no_cuda and place_cuda:\n        layer = layer.cuda()\n    return layer\n\n\nclass ProjectiveGridGenerator(nn.Module):\n    def __init__(self, size, theta, no_cuda):\n        super().__init__()\n        self.N, self.C, self.H, self.W = size\n        linear_points_W = torch.linspace(0, 1 - 1/self.W, self.W)\n        linear_points_H = torch.linspace(0, 1 - 1/self.H, self.H)\n\n        self.base_grid = theta.new(self.N, self.H, self.W, 3)\n        self.base_grid[:, :, :, 0] = torch.ger(\n                torch.ones(self.H), linear_points_W).expand_as(self.base_grid[:, :, :, 0])\n        self.base_grid[:, :, :, 1] = torch.ger(\n                linear_points_H, torch.ones(self.W)).expand_as(self.base_grid[:, :, :, 1])\n        self.base_grid[:, :, :, 2] = 1\n\n        self.base_grid = Variable(self.base_grid)\n        if not no_cuda:\n            self.base_grid = self.base_grid.cuda()\n\n    def forward(self, theta):\n        grid = torch.bmm(self.base_grid.view(self.N, self.H * self.W, 3), theta.transpose(1, 2))\n        grid = torch.div(grid[:, :, 0:2], grid[:, :, 2:])\n        return grid\n\n\nclass Weighted_least_squares(nn.Module):\n    def __init__(self, size, nclasses, order, no_cuda, reg_ls=0, use_cholesky=False):\n        super().__init__()\n        N, C, self.H, W = size\n        self.nclasses = nclasses\n        self.tensor_ones = Variable(torch.ones(N, self.H*W, 1))\n        self.order = order\n        self.reg_ls = Variable(reg_ls*torch.eye(order+1))\n        self.use_cholesky = use_cholesky\n        if not no_cuda:\n            self.reg_ls = self.reg_ls.cuda()\n            self.tensor_ones = self.tensor_ones.cuda()\n\n    def forward(self, W, grid):\n        beta2, beta3 = None, None\n\n        W = W.view(-1, self.nclasses, grid.size(1))\n        W0 = W[:, 0, :].unsqueeze(2)\n        x_map = grid[:, :, 0].unsqueeze(2)\n        y_map = ((1)-grid[:, :, 1]).unsqueeze(2)\n        if self.order == 0:\n            Y = self.tensor_ones\n        elif self.order == 1:\n            Y = torch.cat((y_map, self.tensor_ones), 2)\n        elif self.order == 2:\n            Y = torch.cat(((y_map**2), y_map, self.tensor_ones), 2)\n        else:\n            raise NotImplementedError(\n                    \'Requested order {} for polynomial fit is not implemented\'.format(self.order))\n\n#        Y = Y[0:W.size(0),:,:]\n#        x_map = x_map[0:W.size(0),:,:]\n        Y0 = torch.mul(W0, Y)\n        Z = torch.bmm(Y0.transpose(1, 2), Y0) + self.reg_ls\n\n        if not self.use_cholesky:\n            # Z_inv = [torch.inverse(matrix) for matrix in torch.unbind(Z)]\n            # Z_inv = torch.stack(Z_inv)\n            Z_inv = torch.inverse(Z)\n            X = torch.bmm(Y0.transpose(1, 2), torch.mul(W0, x_map))\n            beta0 = torch.bmm(Z_inv, X)\n        else:\n            # cholesky\n            # TODO check this\n            beta0 = []\n            X = torch.bmm(Y0.transpose(1, 2), torch.mul(W0, x_map))\n            for image, rhs in zip(torch.unbind(Z), torch.unbind(X)):\n                R = torch.potrf(image)\n                opl = torch.trtrs(rhs, R.transpose(0, 1))\n                beta0.append(torch.trtrs(opl[0], R, upper=False)[0])\n            beta0 = torch.cat((beta0), 1).transpose(0, 1).unsqueeze(2)\n\n        W1 = W[:, 1, :].unsqueeze(2)\n        Y1 = torch.mul(W1, Y)\n        Z = torch.bmm(Y1.transpose(1, 2), Y1) + self.reg_ls\n        # TODO : use torch.inverse over batch (is possible since pytorch 1.0.0)\n        # Z_inv = [torch.inverse(matrix) for matrix in torch.unbind(Z)]\n        # Z_inv = torch.stack(Z_inv)\n        Z_inv = torch.inverse(Z)\n        X = torch.bmm(Y1.transpose(1, 2), torch.mul(W1, x_map))\n        beta1 = torch.bmm(Z_inv, X)\n\n        if self.nclasses > 3:\n            W2 = W[:, 2, :].unsqueeze(2)\n            Y2 = torch.mul(W2, Y)\n            Z = torch.bmm(Y2.transpose(1, 2), Y2) + self.reg_ls\n            Z_inv = torch.inverse(Z)\n            X = torch.bmm(Y2.transpose(1, 2), torch.mul(W2, x_map))\n            beta2 = torch.bmm(Z_inv, X)\n\n            W3 = W[:, 3, :].unsqueeze(2)\n            Y3 = torch.mul(W3, Y)\n            Z = torch.bmm(Y3.transpose(1, 2), Y3) + self.reg_ls\n            Z_inv = torch.inverse(Z)\n            X = torch.bmm(Y3.transpose(1, 2), torch.mul(W3, x_map))\n            beta3 = torch.bmm(Z_inv, X)\n\n        return beta0, beta1, beta2, beta3\n\n\nclass Classification(nn.Module):\n    def __init__(self, class_type, size, channels_in, resize):\n        super().__init__()\n        self.class_type = class_type\n        filter_size = 1\n        pad = (filter_size-1)//2\n        self.conv1 = nn.Conv2d(channels_in, 128, filter_size,\n           stride=1, padding=pad, bias=True)\n        self.conv1_bn = nn.BatchNorm2d(128)\n        \n        filter_size = 3\n        pad = (filter_size-1)//2\n        self.conv2 = nn.Conv2d(128, 128, filter_size,\n           stride=1, padding=pad, bias=True)\n        self.conv2_bn = nn.BatchNorm2d(128)\n        \n        self.conv3 = nn.Conv2d(128, 64, filter_size,\n           stride=1, padding=pad, bias=True)\n        self.conv3_bn = nn.BatchNorm2d(64)\n        \n        self.conv4 = nn.Conv2d(64, 64, filter_size,\n           stride=1, padding=pad, bias=True)\n        self.conv4_bn = nn.BatchNorm2d(64)\n        \n        rows, cols = size\n        self.avgpool = nn.AvgPool2d((1,cols))\n        self.maxpool = nn.MaxPool2d((2, 2), stride = 2)\n        \n        if class_type == \'line\':\n            self.fully_connected1 = nn.Linear(64*rows*cols//4, 128)\n            self.fully_connected_line1 = nn.Linear(128, 3)\n            self.fully_connected_line2 = nn.Linear(128, 3)\n            self.fully_connected_line3 = nn.Linear(128, 3)\n            self.fully_connected_line4 = nn.Linear(128, 3)\n        else:\n            self.fully_connected_horizon = nn.Linear(64*rows, resize)\n            \n    def forward(self, x):\n        x = F.relu(self.conv1_bn(self.conv1(x)))\n        x = F.relu(self.conv2_bn(self.conv2(x)))\n        x = F.relu(self.conv3_bn(self.conv3(x)))\n        x = F.relu(self.conv4_bn(self.conv4(x)))\n        if self.class_type == \'line\':\n            x = self.maxpool(x)\n        else:\n            x = self.avgpool(x)\n        x = x.view(x.size()[0],-1)\n        batch_size = x.size(0)\n        if self.class_type == \'line\':\n            x = F.relu(self.fully_connected1(x))\n            x1 = self.fully_connected_line1(x).view(batch_size,3,1,1)\n            x2 = self.fully_connected_line2(x).view(batch_size,3,1,1)\n            x3 = self.fully_connected_line3(x).view(batch_size,3,1,1)\n            x4 = self.fully_connected_line4(x).view(batch_size,3,1,1)\n            \n            x = torch.cat((x1, x2, x3, x4), 2).squeeze(3)\n        else:\n            x = self.fully_connected_horizon(x)\n        return x\n\n\nclass Net(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n\n        # define sizes and perspective transformation\n        resize = args.resize\n        size = torch.Size([args.batch_size, args.nclasses, args.resize, 2*args.resize])\n        size, M, M_inv = Init_Projective_transform(args.nclasses, args.batch_size, args.resize)\n        self.M = M\n\n        # Define network\n        out_channels = args.nclasses + int(not args.end_to_end)\n\n        self.net = Networks.define_model(mod=args.mod, layers=args.layers, \n                                         in_channels=args.channels_in,\n                                         out_channels=out_channels, \n                                         pretrained=args.pretrained, pool=args.pool)\n        # Init activation\n        self.activation = activation_layer(args.activation_layer, args.no_cuda)\n        # Init grid generator\n        self.project_layer = ProjectiveGridGenerator(size, M, args.no_cuda)\n        # Init LS layer\n        self.ls_layer = Weighted_least_squares(size, args.nclasses, args.order, \n                args.no_cuda, args.reg_ls, args.use_cholesky)\n\n        # mask configuration\n        zero_rows = ceil(args.resize*args.mask_percentage)\n        self.idx_row = Variable(torch.linspace(0, zero_rows-1, zero_rows).long())\n        n_row = 13\n        self.idx_col1 = Variable(torch.linspace(1, n_row, n_row+1).long())\n        self.idx_col2 = Variable(torch.linspace(0, n_row, n_row+1).long())+2*resize-(n_row+1)\n        idx_mask = (np.arange(resize)[:, None] < np.arange(2*resize)-(resize+10))*1\n        idx_mask = np.flip(idx_mask, 1).copy() + idx_mask\n        self.idx_mask = Variable(torch.from_numpy(idx_mask)) \\\n                .type(torch.ByteTensor).expand(\n                        args.batch_size, args.nclasses, resize, 2*resize)\n\n        self.end_to_end = args.end_to_end\n        self.pretrained = args.pretrained\n        self.classification_branch = args.clas\n        if self.classification_branch:\n            size_enc = (32, 64)\n            chan = 128\n            self.line_classification = Classification(\'line\', size=size_enc, \n                    channels_in=chan, resize=resize)\n            self.horizon_estimation = Classification(\'horizon\', size=size_enc, \n                    channels_in=chan, resize=resize)\n\n        # Place on GPU if specified\n        if not args.no_cuda:\n            self.M = self.M.cuda()\n            self.idx_row = self.idx_row.cuda()\n            self.idx_col1 = self.idx_col1.cuda()\n            self.idx_col2 = self.idx_col2.cuda()\n            self.idx_mask = self.idx_mask.cuda()\n            if self.classification_branch:\n                self.line_classification = self.line_classification.cuda()\n                self.horizon_estimation = self.horizon_estimation.cuda()\n\n    def forward(self, input, end_to_end):\n        # 0. Init variables\n        line, horizon = None, None\n\n        # 1. Push batch trough network\n        shared_encoder, output = self.net(input, end_to_end*self.pretrained)\n\n        # 2. use activation function\n        # if not self.end_to_end:\n            # output = output.detach()\n        # activated = output[:, 1:, :, :]\n        # activated = torch.clamp(activated, min=0)\n        if not end_to_end:\n            activated = output.detach()\n            _, activated = torch.max(activated, 1)\n            activated = activated.float()\n            left = activated*(activated == 1).float()\n            right = activated*(activated == 2).float()\n            activated = torch.stack((left, right), 1)\n        else:\n            activated = self.activation(output)\n            if self.classification_branch:\n                line = self.line_classification(shared_encoder)\n                horizon = self.horizon_estimation(shared_encoder)\n\n        # 3. use mask\n        masked = activated.index_fill(2, self.idx_row, 0)\n        # trapezium mask not needed for only two lanes\n        # Makes convergence easier for lane lines further away\n        # output = output.index_fill(3,self.idx_col1,0)\n        # output = output.index_fill(3,self.idx_col2,0)\n        # output = output.masked_fill(self.idx_mask,0)\n\n        # 4. Least squares layer\n        grid = self.project_layer(self.M)\n        beta0, beta1, beta2, beta3 = self.ls_layer(masked, grid)\n        return beta0, beta1, beta2, beta3, masked, self.M, output, line, horizon\n'"
Birds_Eye_View_Loss/Networks/__init__.py,0,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nfrom .ERFNet import Net\n\n# Add models if desired\nmodel_dict = {\'erfnet\': Net}\n\n\ndef allowed_models():\n    return model_dict.keys()\n\n\ndef define_model(mod, **kwargs):\n    if mod not in allowed_models():\n        raise KeyError(""The requested model: {} is not implemented"".format(mod))\n    else:\n        return model_dict[mod](**kwargs)\n'"
Birds_Eye_View_Loss/Networks/utils.py,7,"b'""""""\r\nAuthor: Wouter Van Gansbeke\r\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\r\n""""""\r\n\r\nimport cv2\r\nimport argparse\r\nimport numpy as np\r\nimport os\r\nimport sys\r\nimport errno\r\nfrom PIL import Image\r\nimport torch\r\nimport torch.optim\r\nfrom torch.optim import lr_scheduler\r\nimport torch.nn.init as init\r\nimport matplotlib\r\nmatplotlib.use(\'Agg\')\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.optimize import fsolve\r\nplt.rcParams[\'figure.figsize\'] = (35, 30)\r\n\r\n\r\ndef define_args():\r\n    parser = argparse.ArgumentParser(description=\'Lane_detection_all_objectives\')\r\n    # Segmentation model settings\r\n    parser.add_argument(\'--dataset\', default=\'lane_detection\', help=\'dataset images to train on\')\r\n    parser.add_argument(\'--batch_size\', type=int, default=8, help=\'batch size\')\r\n    parser.add_argument(\'--nepochs\', type=int, default=350, help=\'total numbers of epochs\')\r\n    parser.add_argument(\'--learning_rate\', type=float, default=1e-4, help=\'learning rate\')\r\n    parser.add_argument(\'--no_cuda\', action=\'store_true\', help=\'if gpu available\')\r\n    parser.add_argument(\'--nworkers\', type=int, default=8, help=\'num of threads\')\r\n    parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout in network\')\r\n    parser.add_argument(\'--nclasses\', type=int, default=2, choices=[2, 4], help=\'num output channels for segmentation\')\r\n    parser.add_argument(\'--crop_size\', type=int, default=80, help=\'crop from image\')\r\n    parser.add_argument(\'--resize\', type=int, default=256, help=\'resize image to resize x (ratio*resize)\')\r\n    parser.add_argument(\'--mod\', type=str, default=\'erfnet\', help=\'model to train\')\r\n    parser.add_argument(\'--layers\', type=int, default=18, help=\'amount of layers in model\')\r\n    parser.add_argument(""--pool"", type=str2bool, nargs=\'?\', const=True, default=True, help=""use pooling"")\r\n    parser.add_argument(""--pretrained"", type=str2bool, nargs=\'?\', const=True, default=False, help=""use pretrained model"")\r\n    parser.add_argument(\'--pretrain_epochs\', type=int, default=20, help=\'Number of epochs to perform segmentation pretraining\')\r\n    parser.add_argument(\'--channels_in\', type=int, default=3, help=\'num channels of input image\')\r\n    parser.add_argument(\'--norm\', type=str, default=\'batch\', help=\'normalisation layer you want to use\')\r\n    parser.add_argument(\'--flip_on\', action=\'store_true\', help=\'Random flip input images on?\')\r\n    parser.add_argument(\'--num_train\', type=int, default=2535, help=\'Train on how many images of trainset\')\r\n    parser.add_argument(\'--split_percentage\', type=float, default=0.2, help=\'where to split dataset in train and validationset\')\r\n    parser.add_argument(\'--test_mode\', action=\'store_true\', help=\'prevents loading latest saved model\')\r\n    parser.add_argument(\'--start_epoch\', type=int, default=0, help=\'prevents loading latest saved model\')\r\n    parser.add_argument(\'--evaluate\', action=\'store_true\', help=\'only perform evaluation\')\r\n    parser.add_argument(\'--resume\', type=str, default=\'\', help=\'resume latest saved run\')\r\n    # Optimizer settings\r\n    parser.add_argument(\'--optimizer\', type=str, default=\'adam\', help=\'adam or sgd\')\r\n    parser.add_argument(\'--weight_init\', type=str, default=\'kaiming\', help=\'normal, xavier, kaiming, orhtogonal weights initialisation\')\r\n    parser.add_argument(\'--weight_decay\', type=float, default=0, help=\'L2 weight decay/regularisation on?\')\r\n    parser.add_argument(\'--lr_decay\', action=\'store_true\', help=\'decay learning rate with rule\')\r\n    parser.add_argument(\'--niter\', type=int, default=50, help=\'# of iter at starting learning rate\')\r\n    parser.add_argument(\'--niter_decay\', type=int, default=400, help=\'# of iter to linearly decay learning rate to zero\')\r\n    parser.add_argument(\'--lr_policy\', default=None, help=\'learning rate policy: lambda|step|plateau\')\r\n    parser.add_argument(\'--lr_decay_iters\', type=int, default=30, help=\'multiply by a gamma every lr_decay_iters iterations\')\r\n    parser.add_argument(\'--clip_grad_norm\', type=int, default=0, help=\'performs gradient clipping\')\r\n    # Fitting layer settings\r\n    parser.add_argument(\'--order\', type=int, default=2, help=\'order of polynomial for curve fitting\')\r\n    parser.add_argument(\'--activation_layer\', type=str, default=\'square\', help=\'Which activation after decoder do you want?\')\r\n    parser.add_argument(\'--reg_ls\', type=float, default=0, help=\'Regularization term for matrix inverse\')\r\n    parser.add_argument(\'--no_ortho\', action=\'store_true\', help=\'if no ortho transformation is desired\')\r\n    parser.add_argument(\'--mask_percentage\', type=float, default=0.3, help=\'mask to apply where birds eye view is not defined\')\r\n    parser.add_argument(\'--use_cholesky\', action=\'store_true\', help=\'use cholesky decomposition\')\r\n    parser.add_argument(\'--activation_net\', type=str, default=\'relu\', help=\'activation in network used\')\r\n    # Paths settings\r\n    parser.add_argument(\'--image_dir\', type=str, required=True, help=\'directory to image dir\')\r\n    parser.add_argument(\'--gt_dir\', type=str, required=True, help=\'directory to gt\')\r\n    parser.add_argument(\'--save_path\', type=str, default=\'Saved/\', help=\'directory to gt\')\r\n    parser.add_argument(\'--json_file\', type=str, default=\'Labels/Curve_parameters.json\', help=\'directory to json input\')\r\n    # LOSS settings\r\n    parser.add_argument(\'--weight_seg\', type=int, default=30, help=\'weight in loss criterium for segmentation\')\r\n    parser.add_argument(\'--weight_class\', type=float, default=1, help=\'weight in loss criterium for classification branch\')\r\n    parser.add_argument(\'--weight_fit\', type=float, default=1, help=\'weight in loss criterium for fit\')\r\n    parser.add_argument(\'--loss_policy\', type=str, default=\'area\', help=\'use area_loss, homography_mse or classical mse in birds eye view\')\r\n    parser.add_argument(\'--weight_funct\', type=str, default=\'none\', help=\'apply weight function in birds eye when computing area loss\')\r\n    parser.add_argument(""--end_to_end"", type=str2bool, nargs=\'?\', const=True, default=True, help=""regression towards curve params by network or postprocessing"")\r\n    parser.add_argument(\'--gamma\', type=float, default=0., help=\'factor to decay learning rate every lr_decay_iters with\')\r\n    parser.add_argument(""--clas"", type=str2bool, nargs=\'?\', const=True, default=False, help=""Horizon and line classification tasks"")\r\n    # CUDNN usage\r\n    parser.add_argument(""--cudnn"", type=str2bool, nargs=\'?\', const=True, default=True, help=""cudnn optimization active"")\r\n    # Tensorboard settings\r\n    parser.add_argument(""--no_tb"", type=str2bool, nargs=\'?\', const=True, default=True, help=""Use tensorboard logging by tensorflow"")\r\n    # Print settings\r\n    parser.add_argument(\'--print_freq\', type=int, default=500, help=\'padding\')\r\n    parser.add_argument(\'--save_freq\', type=int, default=500, help=\'padding\')\r\n    # Skip batch\r\n    parser.add_argument(\'--list\', type=int, nargs=\'+\', default=[954, 2789], help=\'Images you want to skip\')\r\n    return parser\r\n\r\n\r\ndef save_weightmap(train_or_val, M, M_inv, weightmap_zeros,\r\n                   beta0, beta1, beta2, beta3, gt_params_lhs, \r\n                   gt_params_rhs, gt_params_llhs, gt_params_rrhs, line_class,\r\n                   gt, idx, i, images, no_ortho, resize, save_path):\r\n    M = M.data.cpu().numpy()[0]\r\n    x = np.zeros(3)\r\n\r\n    line_class = line_class[0].cpu().numpy()\r\n    left_lane = True if line_class[0] != 0 else False\r\n    right_lane = True if line_class[3] != 0 else False\r\n\r\n    wm0_zeros = weightmap_zeros.data.cpu()[0, 0].numpy()\r\n    wm1_zeros = weightmap_zeros.data.cpu()[0, 1].numpy()\r\n\r\n    im = images.permute(0, 2, 3, 1).data.cpu().numpy()[0]\r\n    im_orig = np.copy(im)\r\n    gt_orig = gt.permute(0, 2, 3, 1).data.cpu().numpy()[0, :, :, 0]\r\n    im_orig = draw_homography_points(im_orig, x, resize)\r\n\r\n    im, M_scaledup = test_projective_transform(im, resize, M)\r\n\r\n    im, _ = draw_fitted_line(im, gt_params_rhs[0], resize, (0, 255, 0))\r\n    im, _ = draw_fitted_line(im, gt_params_lhs[0], resize, (0, 255, 0))\r\n    im, lane0 = draw_fitted_line(im, beta0[0], resize, (255, 0, 0))\r\n    im, lane1 = draw_fitted_line(im, beta1[0], resize, (0, 0, 255))\r\n    if beta2 is not None:\r\n        im, _ = draw_fitted_line(im, gt_params_llhs[0], resize, (0, 255, 0))\r\n        im, _ = draw_fitted_line(im, gt_params_rrhs[0], resize, (0, 255, 0))\r\n        if left_lane:\r\n            im, lane2 = draw_fitted_line(im, beta2[0], resize, (255, 255, 0))\r\n        if right_lane:\r\n            im, lane3 = draw_fitted_line(im, beta3[0], resize, (255, 128, 0))\r\n\r\n\r\n    if not no_ortho:\r\n        im_inverse = cv2.warpPerspective(im, np.linalg.inv(M_scaledup), (2*resize, resize))\r\n    else:\r\n        im_inverse = im_orig\r\n\r\n    im_orig = np.clip(im_orig, 0, 1)\r\n    im_inverse = np.clip(im_inverse, 0, 1)\r\n    im = np.clip(im, 0, 1)\r\n\r\n    fig = plt.figure()\r\n    ax1 = fig.add_subplot(421)\r\n    ax2 = fig.add_subplot(422)\r\n    ax3 = fig.add_subplot(423)\r\n    ax4 = fig.add_subplot(424)\r\n    ax5 = fig.add_subplot(425)\r\n    ax6 = fig.add_subplot(426)\r\n    ax7 = fig.add_subplot(427)\r\n    ax1.imshow(im)\r\n    ax2.imshow(wm0_zeros)\r\n    ax3.imshow(im_inverse)\r\n    ax4.imshow(wm1_zeros)\r\n    ax5.imshow(wm0_zeros/np.max(wm0_zeros)+wm1_zeros/np.max(wm1_zeros))\r\n    ax6.imshow(im_orig)\r\n    ax7.imshow(gt_orig)\r\n    fig.savefig(save_path + \'/example/{}/weight_idx-{}_batch-{}\'.format(train_or_val, idx, i))\r\n    plt.clf()\r\n    plt.close(fig)\r\n\r\n\r\ndef test_projective_transform(input, resize, M):\r\n    # test grid using built in F.grid_sampler method.\r\n    M_scaledup = np.array([[M[0,0],M[0,1]*2,M[0,2]*(2*resize-1)],[0,M[1,1],M[1,2]*(resize-1)],[0,M[2,1]/(resize-1),M[2,2]]])\r\n    inp = cv2.warpPerspective(np.asarray(input), M_scaledup, (2*resize,resize))\r\n    return inp, M_scaledup\r\n\r\n\r\ndef draw_fitted_line(img, params, resize, color=(255,0,0)):\r\n    params = params.data.cpu().tolist()\r\n    y_stop = 0.7\r\n    y_prime = np.linspace(0, y_stop, 20)\r\n    params = [0] * (4 - len(params)) + params\r\n    d, a, b, c = [*params]\r\n    x_pred = d*(y_prime**3) + a*(y_prime)**2 + b*(y_prime) + c\r\n    x_pred = x_pred*(2*resize-1)\r\n    y_prime = (1-y_prime)*(resize-1)\r\n    lane = [(xcord, ycord) for (xcord, ycord) in zip(x_pred, y_prime)] \r\n    img = cv2.polylines(img, [np.int32(lane)], isClosed = False, color = color,thickness = 1)\r\n    return img, lane\r\n\r\n\r\ndef draw_horizon(img, horizon, resize=256, color=(255,0,0)):\r\n    x = np.arange(2*resize-1)\r\n    horizon_line = [(x_cord, horizon+1) for x_cord in x]\r\n    img = cv2.polylines(img.copy(), [np.int32(horizon_line)], isClosed = False, color = color,thickness = 1)\r\n    return img\r\n\r\n\r\ndef draw_homography_points(img, x, resize=256, color=(255,0,0)):\r\n    y_start1 = (0.3+x[2])*(resize-1)\r\n    y_start = 0.3*(resize-1)\r\n    y_stop = resize-1\r\n    src = np.float32([[0.45*(2*resize-1),y_start],[0.55*(2*resize-1), y_start],[0.1*(2*resize-1),y_stop],[0.9*(2*resize-1), y_stop]])\r\n    dst = np.float32([[(0.45+x[0])*(2*resize-1), y_start1],[(0.55+x[1])*(2*resize-1), y_start1],[(0.45+x[0])*(2*resize-1), y_stop],[(0.55+x[1])*(2*resize-1),y_stop]])\r\n    dst_ideal = np.float32([[0.45*(2*resize-1), y_start],[0.55*(2*resize-1), y_start],[0.45*(2*resize-1), y_stop],[0.55*(2*resize-1),y_stop]])\r\n    [cv2.circle(np.asarray(img), tuple(idx), radius=5, thickness=-1, color=(255,0,0)) for idx in src]\r\n    [cv2.circle(np.asarray(img), tuple(idx), radius=5, thickness=-1, color=(0,255,0)) for idx in dst_ideal]\r\n    [cv2.circle(np.asarray(img), tuple(idx), radius=5, thickness=-1, color=(0,0,255)) for idx in dst]\r\n    return img\r\n\r\n\r\ndef save_image(output, gt_params, i=1, resize=320):\r\n    outputs_seg = output.permute(0,2,3,1)\r\n    im = np.asarray(outputs_seg.data.cpu()[0])\r\n    im = draw_fitted_line(im,gt_params[0],resize)\r\n    im = Image.fromarray(im.astype(\'uint8\'), \'RGB\')\r\n    im.save(\'simple_net/simple_net_train/{}.png\'.format(i[0]))\r\n\r\n\r\ndef save_output(output,gt_params, i=1):\r\n    output = output*255/(torch.max(output))\r\n    output = output.permute(0,2,3,1)\r\n    im = np.asarray(output.data.cpu()[0]).squeeze(2)\r\n    \r\n#    im = draw_fitted_line(im,gt_params[0],resize)\r\n    im = Image.fromarray(im.astype(\'uint8\'), \'P\')\r\n    im.save(\'simple_net/simple_net_output/{}.png\'.format(i[0]))\r\n\r\n\r\ndef line_right_eq(x):\r\n    y = 0.438/0.7*x + 0.56 #0.7/0.438*(x-0.56)\r\n    return y\r\n\r\n\r\ndef line_left_eq(x):\r\n    y = -x*0.438/0.7 + 0.44#-0.7/0.438*(x-0.44)\r\n    return y\r\n\r\n\r\ndef f(x, *params):\r\n    \'\'\'\r\n    Constructs objective function which will be solved iteratively\r\n    \'\'\'\r\n    a, b, c, left_or_right = params\r\n    if left_or_right == \'left\':\r\n        funct = a*x**2 + b*x + c - line_left_eq(x)\r\n    else:\r\n        funct = a*x**2 + b*x + c - line_right_eq(x)\r\n    return funct\r\n\r\n\r\ndef draw_mask_line(im, beta0, beta1, beta2, beta3, resize):\r\n    beta0 = beta0.data.cpu().tolist()\r\n    beta1 = beta1.data.cpu().tolist()\r\n    beta2 = beta2.data.cpu().tolist()\r\n    beta3 = beta3.data.cpu().tolist()\r\n    params0 = *beta0, \'left\'\r\n    params1 = *beta1, \'right\'\r\n    params2 = *beta2, \'left\'\r\n    params3 = *beta3, \'right\'\r\n    x, y, order = [], [], []\r\n    max_lhs = fsolve(f, 0.05, args=params0)\r\n    if max_lhs > 0:\r\n        x.append(line_left_eq(max_lhs[0]))\r\n        y.append(1-max_lhs[0])\r\n        order.append(0)\r\n    else:\r\n        max_lhs = 0\r\n    max_rhs = fsolve(f, 0.05, args=params1)\r\n    if max_rhs > 0:\r\n        x.append(line_right_eq(max_rhs[0]))\r\n        y.append(1-max_rhs[0])\r\n        order.append(1)\r\n    else:\r\n        max_rhs = 0 \r\n    max_left = fsolve(f, 0.05, args=params2)\r\n    if max_left > 0:\r\n        x.append(line_left_eq(max_left[0]))\r\n        y.append(1-max_left[0])\r\n        order.append(2)\r\n    else:\r\n        max_left = 0\r\n    max_right = fsolve(f, 0.05, args=params3)\r\n    if max_right > 0:\r\n        x.append(line_right_eq(max_right[0]))\r\n        y.append(1-max_right[0])\r\n        order.append(3)\r\n    else:\r\n        max_right = 0 \r\n    y_stop = 1\r\n    y_prime = np.linspace(0, y_stop, 40)\r\n    x_prime_right = line_right_eq(y_prime)\r\n    x_prime_left = line_left_eq(y_prime)\r\n    y_prime, x_prime_lft, x_prime_rght = (1-y_prime)*(resize-1), x_prime_left*(2*resize-1), x_prime_right*(2*resize-1)\r\n    line_right = [(xcord, ycord) for (xcord, ycord) in zip(x_prime_rght, y_prime)] \r\n    line_left = [(xcord, ycord) for (xcord, ycord) in zip(x_prime_lft, y_prime)] \r\n    im = cv2.polylines(im, [np.int32(line_right)], isClosed = False, color = (255,0,0),thickness = 1)\r\n    im = cv2.polylines(im, [np.int32(line_left)], isClosed = False, color = (255,0,0),thickness = 1)\r\n    x = np.array(x)\r\n    y = np.array(y)\r\n    x_left, x_right = line_left_eq(max_left)*(2*resize-1), line_right_eq(max_right)*(2*resize-1)\r\n    y_left, y_right = (1-max_left)*(resize-1), (1-max_right)*(resize-1)\r\n    cv2.circle(np.asarray(im), (x_left,y_left), radius=3, thickness=-1, color=(0,0,255))\r\n    cv2.circle(np.asarray(im), (x_right,y_right), radius=3, thickness=-1, color=(0,0,255))\r\n    x_prime, y_prime = homogenous_transformation(x,y)\r\n    maxima = np.zeros(4)\r\n    for i, idx in enumerate(order):\r\n        maxima[idx] = y_prime[i]\r\n    return im, np.int_(np.round(x_prime*(2*resize-1))), np.int_(np.round(y_prime*(resize-1)))\r\n\r\n\r\ndef homogenous_transformation(x,y):\r\n    """"""\r\n    Helper function to transform coordionates defined by transformation matrix\r\n    \r\n    Args:\r\n            Matrix (multi dim - array): Transformation matrix\r\n            x (array): original x coordinates\r\n            y (array): original y coordinates\r\n    """"""\r\n    y_start = 0.3\r\n    y_stop = 1\r\n    src = np.float32([[0.45,y_start],[0.55, y_start],[0.1,y_stop],[0.9, y_stop]])\r\n    dst = np.float32([[0.45, y_start],[0.55, y_start],[0.45, y_stop],[0.55,y_stop]])\r\n    M_inv = cv2.getPerspectiveTransform(dst,src)\r\n    \r\n    ones = np.ones((1,len(y)))\r\n    coordinates = np.vstack((x, y, ones))\r\n    trans = np.matmul(M_inv, coordinates)\r\n            \r\n    x_vals = trans[0,:]/trans[2,:]\r\n    y_vals = trans[1,:]/trans[2,:]\r\n    return x_vals, y_vals\r\n\r\n\r\ndef first_run(save_path):\r\n    txt_file = os.path.join(save_path,\'first_run.txt\')\r\n    if not os.path.exists(txt_file):\r\n        open(txt_file, \'w\').close()\r\n    else:\r\n        saved_epoch = open(txt_file).read()\r\n        if saved_epoch is None:\r\n            print(\'You forgot to delete [first run file]\')\r\n            return \'\' \r\n        return saved_epoch\r\n    return \'\'\r\n\r\n\r\ndef mkdir_if_missing(directory):\r\n    if not os.path.exists(directory):\r\n        try:\r\n            os.makedirs(directory)\r\n        except OSError as e:\r\n            if e.errno != errno.EEXIST:\r\n                raise\r\n\r\n\r\n# trick from stackoverflow\r\ndef str2bool(argument):\r\n    if argument.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\r\n        return True\r\n    elif argument.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\r\n        return False\r\n    else:\r\n        raise argparse.ArgumentTypeError(\'Wrong argument in argparse, should be a boolean\')\r\n\r\n\r\nclass Logger(object):\r\n    """"""\r\n    Source https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\r\n    """"""\r\n    def __init__(self, fpath=None):\r\n        self.console = sys.stdout\r\n        self.file = None\r\n        self.fpath = fpath\r\n        if fpath is not None:\r\n            mkdir_if_missing(os.path.dirname(fpath))\r\n            self.file = open(fpath, \'w\')\r\n\r\n    def __del__(self):\r\n        self.close()\r\n\r\n    def __enter__(self):\r\n        pass\r\n\r\n    def __exit__(self, *args):\r\n        self.close()\r\n\r\n    def write(self, msg):\r\n        self.console.write(msg)\r\n        if self.file is not None:\r\n            self.file.write(msg)\r\n\r\n    def flush(self):\r\n        self.console.flush()\r\n        if self.file is not None:\r\n            self.file.flush()\r\n            os.fsync(self.file.fileno())\r\n\r\n    def close(self):\r\n        self.console.close()\r\n        if self.file is not None:\r\n            self.file.close()\r\n\r\n\r\nclass AverageMeter(object):\r\n    """"""Computes and stores the average and current value""""""\r\n    def __init__(self):\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.val = 0\r\n        self.avg = 0\r\n        self.sum = 0\r\n        self.count = 0\r\n\r\n    def update(self, val, n=1):\r\n        self.val = val\r\n        self.sum += val * n\r\n        self.count += n\r\n        self.avg = self.sum / self.count\r\n\r\n\r\ndef define_optim(optim, params, lr, weight_decay):\r\n    if optim == \'adam\':\r\n        optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\r\n    elif optim == \'sgd\':\r\n        optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\r\n    elif optim == \'rmsprop\':\r\n        optimizer = torch.optim.RMSprop(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\r\n    else:\r\n        raise KeyError(""The requested optimizer: {} is not implemented"".format(optim))\r\n    return optimizer\r\n\r\n\r\ndef define_scheduler(optimizer, args):\r\n    if args.lr_policy == \'lambda\':\r\n        def lambda_rule(epoch):\r\n            lr_l = 1.0 - max(0, epoch + 1 - args.niter) / float(args.niter_decay + 1)\r\n            return lr_l\r\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\r\n    elif args.lr_policy == \'step\':\r\n        scheduler = lr_scheduler.StepLR(optimizer,\r\n                                        step_size=args.lr_decay_iters, gamma=args.gamma)\r\n    elif args.lr_policy == \'plateau\':\r\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\r\n                                                   factor=args.gamma,\r\n                                                   threshold=0.0001,\r\n                                                   patience=args.lr_decay_iters)\r\n    elif args.lr_policy == \'none\':\r\n        scheduler = None\r\n    else:\r\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', args.lr_policy)\r\n    return scheduler\r\n\r\n\r\ndef define_init_weights(model, init_w=\'normal\', activation=\'relu\'):\r\n    print(\'Init weights in network with [{}]\'.format(init_w))\r\n    if init_w == \'normal\':\r\n        model.apply(weights_init_normal)\r\n    elif init_w == \'xavier\':\r\n        model.apply(weights_init_xavier)\r\n    elif init_w == \'kaiming\':\r\n        model.apply(weights_init_kaiming)\r\n    elif init_w == \'orthogonal\':\r\n        model.apply(weights_init_orthogonal)\r\n    else:\r\n        raise NotImplementedError(\'initialization method [{}] is not implemented\'.format(init_w))\r\n\r\n\r\ndef weights_init_normal(m):\r\n    classname = m.__class__.__name__\r\n#    print(classname)\r\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\r\n        init.normal_(m.weight.data, 0.0, 0.02)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.normal_(m.weight.data, 0.0, 0.02)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal_(m.weight.data, 1.0, 0.02)\r\n        init.constant_(m.bias.data, 0.0)\r\n\r\n\r\ndef weights_init_xavier(m):\r\n    classname = m.__class__.__name__\r\n    # print(classname)\r\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\r\n        init.xavier_normal_(m.weight.data, gain=0.02)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.xavier_normal_(m.weight.data, gain=0.02)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal_(m.weight.data, 1.0, 0.02)\r\n        init.constant_(m.bias.data, 0.0)\r\n\r\n\r\ndef weights_init_kaiming(m):\r\n    classname = m.__class__.__name__\r\n    # print(classname)\r\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\r\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\', nonlinearity=\'relu\')\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\', nonlinearity=\'relu\')\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal_(m.weight.data, 1.0, 0.02)\r\n        init.constant_(m.bias.data, 0.0)\r\n\r\n\r\ndef weights_init_orthogonal(m):\r\n    classname = m.__class__.__name__\r\n#    print(classname)\r\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\r\n        init.orthogonal(m.weight.data, gain=1)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'Linear\') != -1:\r\n        init.orthogonal(m.weight.data, gain=1)\r\n        if m.bias is not None:\r\n            m.bias.data.zero_()\r\n    elif classname.find(\'BatchNorm2d\') != -1:\r\n        init.normal_(m.weight.data, 1.0, 0.02)\r\n        init.constant_(m.bias.data, 0.0)\r\n'"
