file_path,api_count,code
main.py,14,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport argparse\nimport numpy as np\nimport os\nimport sys\nimport time\nimport shutil\nimport glob\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim\nimport Models\nimport Datasets\nimport warnings\nimport random\nfrom datetime import datetime\nfrom Loss.loss import define_loss, allowed_losses, MSE_loss\nfrom Loss.benchmark_metrics import Metrics, allowed_metrics\nfrom Datasets.dataloader import get_loader\nsys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\nfrom Utils.utils import str2bool, define_optim, define_scheduler, \\\n                        Logger, AverageMeter, first_run, mkdir_if_missing, \\\n                        define_init_weights, init_distributed_mode\n\n# Training setttings\nparser = argparse.ArgumentParser(description=\'KITTI Depth Completion Task\')\nparser.add_argument(\'--dataset\', type=str, default=\'kitti\', choices=Datasets.allowed_datasets(), help=\'dataset to work with\')\nparser.add_argument(\'--nepochs\', type=int, default=100, help=\'Number of epochs for training\')\nparser.add_argument(\'--thres\', type=int, default=0, help=\'epoch for pretraining\')\nparser.add_argument(\'--start_epoch\', type=int, default=0, help=\'Start epoch number for training\')\nparser.add_argument(\'--mod\', type=str, default=\'mod\', choices=Models.allowed_models(), help=\'Model for use\')\nparser.add_argument(\'--batch_size\', type=int, default=7, help=\'batch size\')\nparser.add_argument(\'--val_batch_size\', default=None, help=\'batch size selection validation set\')\nparser.add_argument(\'--learning_rate\', metavar=\'lr\', type=float, default=1e-3, help=\'learning rate\')\nparser.add_argument(\'--no_cuda\', action=\'store_true\', help=\'no gpu usage\')\n\nparser.add_argument(\'--evaluate\', action=\'store_true\', help=\'only evaluate\')\nparser.add_argument(\'--resume\', type=str, default=\'\', help=\'resume latest saved run\')\nparser.add_argument(\'--nworkers\', type=int, default=8, help=\'num of threads\')\nparser.add_argument(\'--nworkers_val\', type=int, default=0, help=\'num of threads\')\nparser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout in network\')\nparser.add_argument(\'--subset\', type=int, default=None, help=\'Take subset of train set\')\nparser.add_argument(\'--input_type\', type=str, default=\'rgb\', choices=[\'depth\',\'rgb\'], help=\'use rgb for rgbdepth\')\nparser.add_argument(\'--side_selection\', type=str, default=\'\', help=\'train on one specific stereo camera\')\nparser.add_argument(\'--no_tb\', type=str2bool, nargs=\'?\', const=True,\n                    default=True, help=""use mask_gt - mask_input as final mask for loss calculation"")\nparser.add_argument(\'--test_mode\', action=\'store_true\', help=\'Do not use resume\')\nparser.add_argument(\'--pretrained\', type=str2bool, nargs=\'?\', const=True, default=True, help=\'use pretrained model\')\nparser.add_argument(\'--load_external_mod\', type=str2bool, nargs=\'?\', const=True, default=False, help=\'path to external mod\')\n\n# Data augmentation settings\nparser.add_argument(\'--crop_w\', type=int, default=1216, help=\'width of image after cropping\')\nparser.add_argument(\'--crop_h\', type=int, default=256, help=\'height of image after cropping\')\nparser.add_argument(\'--max_depth\', type=float, default=85.0, help=\'maximum depth of LIDAR input\')\nparser.add_argument(\'--sparse_val\', type=float, default=0.0, help=\'value to endode sparsity with\')\nparser.add_argument(""--rotate"", type=str2bool, nargs=\'?\', const=True, default=False, help=""rotate image"")\nparser.add_argument(""--flip"", type=str, default=\'hflip\', help=""flip image: vertical|horizontal"")\nparser.add_argument(""--rescale"", type=str2bool, nargs=\'?\', const=True,\n                    default=False, help=""Rescale values of sparse depth input randomly"")\nparser.add_argument(""--normal"", type=str2bool, nargs=\'?\', const=True, default=False, help=""normalize depth/rgb input"")\nparser.add_argument(""--no_aug"", type=str2bool, nargs=\'?\', const=True, default=False, help=""rotate image"")\n\n# Paths settings\nparser.add_argument(\'--save_path\', default=\'Saved/\', help=\'save path\')\nparser.add_argument(\'--data_path\', required=True, help=\'path to desired dataset\')\n\n# Optimizer settings\nparser.add_argument(\'--optimizer\', type=str, default=\'adam\', help=\'adam or sgd\')\nparser.add_argument(\'--weight_init\', type=str, default=\'kaiming\', help=\'normal, xavier, kaiming, orhtogonal weights initialisation\')\nparser.add_argument(\'--weight_decay\', type=float, default=0, help=\'L2 weight decay/regularisation on?\')\nparser.add_argument(\'--lr_decay\', action=\'store_true\', help=\'decay learning rate with rule\')\nparser.add_argument(\'--niter\', type=int, default=50, help=\'# of iter at starting learning rate\')\nparser.add_argument(\'--niter_decay\', type=int, default=400, help=\'# of iter to linearly decay learning rate to zero\')\nparser.add_argument(\'--lr_policy\', type=str, default=None, help=\'{}learning rate policy: lambda|step|plateau\')\nparser.add_argument(\'--lr_decay_iters\', type=int, default=7, help=\'multiply by a gamma every lr_decay_iters iterations\')\nparser.add_argument(\'--clip_grad_norm\', type=int, default=0, help=\'performs gradient clipping\')\nparser.add_argument(\'--gamma\', type=float, default=0.5, help=\'factor to decay learning rate every lr_decay_iters with\')\n\n# Loss settings\nparser.add_argument(\'--loss_criterion\', type=str, default=\'mse\', choices=allowed_losses(), help=""loss criterion"")\nparser.add_argument(\'--print_freq\', type=int, default=10000, help=""print every x iterations"")\nparser.add_argument(\'--save_freq\', type=int, default=100000, help=""save every x interations"")\nparser.add_argument(\'--metric\', type=str, default=\'rmse\', choices=allowed_metrics(), help=""metric to use during evaluation"")\nparser.add_argument(\'--metric_1\', type=str, default=\'mae\', choices=allowed_metrics(), help=""metric to use during evaluation"")\nparser.add_argument(\'--wlid\', type=float, default=0.1, help=""weight base loss"")\nparser.add_argument(\'--wrgb\', type=float, default=0.1, help=""weight base loss"")\nparser.add_argument(\'--wpred\', type=float, default=1, help=""weight base loss"")\nparser.add_argument(\'--wguide\', type=float, default=0.1, help=""weight base loss"")\n# Cudnn\nparser.add_argument(""--cudnn"", type=str2bool, nargs=\'?\', const=True,\n                    default=True, help=""cudnn optimization active"")\nparser.add_argument(\'--gpu_ids\', default=\'1\', type=str, help=\'gpu device ids for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(""--multi"", type=str2bool, nargs=\'?\', const=True,\n                    default=False, help=""use multiple gpus"")\nparser.add_argument(""--seed"", type=str2bool, nargs=\'?\', const=True,\n                    default=True, help=""use seed"")\nparser.add_argument(""--use_disp"", type=str2bool, nargs=\'?\', const=True,\n                    default=False, help=""regress towards disparities"")\nparser.add_argument(\'--num_samples\', default=0, type=int, help=\'number of samples\')\n# distributed training\nparser.add_argument(\'--world_size\', default=1, type=int,\n                    help=\'number of distributed processes\')\nparser.add_argument(\'--dist_url\', default=\'env://\', help=\'url used to set up distributed training\')\nparser.add_argument(\'--local_rank\', dest=""local_rank"", default=0, type=int)\n\n\ndef main():\n    global args\n    args = parser.parse_args()\n    if args.num_samples == 0:\n        args.num_samples = None\n    if args.val_batch_size is None:\n        args.val_batch_size = args.batch_size\n    if args.seed:\n        random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        # torch.backends.cudnn.deterministic = True\n        # warnings.warn(\'You have chosen to seed training. \'\n                      # \'This will turn on the CUDNN deterministic setting, \'\n                      # \'which can slow down your training considerably! \'\n                      # \'You may see unexpected behavior when restarting from checkpoints.\')\n\n    # For distributed training\n    # init_distributed_mode(args)\n\n    if not args.no_cuda and not torch.cuda.is_available():\n        raise Exception(""No gpu available for usage"")\n    torch.backends.cudnn.benchmark = args.cudnn\n    # Init model\n    channels_in = 1 if args.input_type == \'depth\' else 4\n    model = Models.define_model(mod=args.mod, in_channels=channels_in, thres=args.thres)\n    define_init_weights(model, args.weight_init)\n    # Load on gpu before passing params to optimizer\n    if not args.no_cuda:\n        if not args.multi:\n            model = model.cuda()\n        else:\n            model = torch.nn.DataParallel(model).cuda()\n            # model.cuda()\n            # model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n            # model = model.module\n\n    save_id = \'{}_{}_{}_{}_{}_batch{}_pretrain{}_wlid{}_wrgb{}_wguide{}_wpred{}_patience{}_num_samples{}_multi{}\'.\\\n              format(args.mod, args.optimizer, args.loss_criterion,\n                     args.learning_rate,\n                     args.input_type, \n                     args.batch_size,\n                     args.pretrained, args.wlid, args.wrgb, args.wguide, args.wpred, \n                     args.lr_decay_iters, args.num_samples, args.multi)\n\n\n    # INIT optimizer/scheduler/loss criterion\n    optimizer = define_optim(args.optimizer, model.parameters(), args.learning_rate, args.weight_decay)\n    scheduler = define_scheduler(optimizer, args)\n\n    # Optional to use different losses\n    criterion_local = define_loss(args.loss_criterion)\n    criterion_lidar = define_loss(args.loss_criterion)\n    criterion_rgb = define_loss(args.loss_criterion)\n    criterion_guide = define_loss(args.loss_criterion)\n\n    # INIT dataset\n    dataset = Datasets.define_dataset(args.dataset, args.data_path, args.input_type, args.side_selection)\n    dataset.prepare_dataset()\n    train_loader, valid_loader, valid_selection_loader = get_loader(args, dataset)\n\n    # Resume training\n    best_epoch = 0\n    lowest_loss = np.inf\n    args.save_path = os.path.join(args.save_path, save_id)\n    mkdir_if_missing(args.save_path)\n    log_file_name = \'log_train_start_0.txt\'\n    args.resume = first_run(args.save_path)\n    if args.resume and not args.test_mode and not args.evaluate:\n        path = os.path.join(args.save_path, \'checkpoint_model_epoch_{}.pth.tar\'.format(int(args.resume)))\n        if os.path.isfile(path):\n            log_file_name = \'log_train_start_{}.txt\'.format(args.resume)\n            # stdout\n            sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(path)\n            args.start_epoch = checkpoint[\'epoch\']\n            lowest_loss = checkpoint[\'loss\']\n            best_epoch = checkpoint[\'best epoch\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            log_file_name = \'log_train_start_0.txt\'\n            # stdout\n            sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n            print(""=> no checkpoint found at \'{}\'"".format(path))\n\n    # Only evaluate\n    elif args.evaluate:\n        print(""Evaluate only"")\n        best_file_lst = glob.glob(os.path.join(args.save_path, \'model_best*\'))\n        if len(best_file_lst) != 0:\n            best_file_name = best_file_lst[0]\n            print(best_file_name)\n            if os.path.isfile(best_file_name):\n                sys.stdout = Logger(os.path.join(args.save_path, \'Evaluate.txt\'))\n                print(""=> loading checkpoint \'{}\'"".format(best_file_name))\n                checkpoint = torch.load(best_file_name)\n                model.load_state_dict(checkpoint[\'state_dict\'])\n            else:\n                print(""=> no checkpoint found at \'{}\'"".format(best_file_name))\n        else:\n            print(""=> no checkpoint found at due to empy list in folder {}"".format(args.save_path))\n        validate(valid_selection_loader, model, criterion_lidar, criterion_rgb, criterion_local, criterion_guide)\n        return\n\n    # Start training from clean slate\n    else:\n        # Redirect stdout\n        sys.stdout = Logger(os.path.join(args.save_path, log_file_name))\n\n    # INIT MODEL\n    print(40*""=""+""\\nArgs:{}\\n"".format(args)+40*""="")\n    print(""Init model: \'{}\'"".format(args.mod))\n    print(""Number of parameters in model {} is {:.3f}M"".format(args.mod.upper(), sum(tensor.numel() for tensor in model.parameters())/1e6))\n\n    # Load pretrained state for cityscapes in GLOBAL net\n    if args.pretrained and not args.resume:\n        if not args.load_external_mod:\n            if not args.multi:\n                target_state = model.depthnet.state_dict()\n            else:\n                target_state = model.module.depthnet.state_dict()\n            check = torch.load(\'erfnet_pretrained.pth\')\n            for name, val in check.items():\n                # Exclude multi GPU prefix\n                mono_name = name[7:] \n                if mono_name not in target_state:\n                     continue\n                try:\n                    target_state[mono_name].copy_(val)\n                except RuntimeError:\n                    continue\n            print(\'Successfully loaded pretrained model\')\n        else:\n            check = torch.load(\'external_mod.pth.tar\')\n            lowest_loss_load = check[\'loss\']\n            target_state = model.state_dict()\n            for name, val in check[\'state_dict\'].items():\n                if name not in target_state:\n                    continue\n                try:\n                    target_state[name].copy_(val)\n                except RuntimeError:\n                    continue\n            print(""=> loaded EXTERNAL checkpoint with best rmse {}""\n                    .format(lowest_loss_load))\n\n    # Start training\n    for epoch in range(args.start_epoch, args.nepochs):\n        print(""\\n => Start EPOCH {}"".format(epoch + 1))\n        print(datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'))\n        print(args.save_path)\n        # Adjust learning rate\n        if args.lr_policy is not None and args.lr_policy != \'plateau\':\n            scheduler.step()\n            lr = optimizer.param_groups[0][\'lr\']\n            print(\'lr is set to {}\'.format(lr))\n\n        # Define container objects\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        score_train = AverageMeter()\n        score_train_1 = AverageMeter()\n        metric_train = Metrics(max_depth=args.max_depth, disp=args.use_disp, normal=args.normal)\n\n        # Train model for args.nepochs\n        model.train()\n\n        # compute timing\n        end = time.time()\n\n        # Load dataset\n        for i, (input, gt) in tqdm(enumerate(train_loader)):\n\n            # Time dataloader\n            data_time.update(time.time() - end)\n\n            # Put inputs on gpu if possible\n            if not args.no_cuda:\n                input, gt = input.cuda(), gt.cuda()\n            prediction, lidar_out, precise, guide = model(input, epoch)\n\n            loss = criterion_local(prediction, gt)\n            loss_lidar = criterion_lidar(lidar_out, gt)\n            loss_rgb = criterion_rgb(precise, gt)\n            loss_guide = criterion_guide(guide, gt)\n            loss = args.wpred*loss + args.wlid*loss_lidar + args.wrgb*loss_rgb + args.wguide*loss_guide\n\n            losses.update(loss.item(), input.size(0))\n            metric_train.calculate(prediction[:, 0:1].detach(), gt.detach())\n            score_train.update(metric_train.get_metric(args.metric), metric_train.num)\n            score_train_1.update(metric_train.get_metric(args.metric_1), metric_train.num)\n\n            # Clip gradients (usefull for instabilities or mistakes in ground truth)\n            if args.clip_grad_norm != 0:\n                nn.utils.clip_grad_norm(model.parameters(), args.clip_grad_norm)\n\n            # Setup backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Time trainig iteration\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            # Print info\n            if (i + 1) % args.print_freq == 0:\n                print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Metric {score.val:.4f} ({score.avg:.4f})\'.format(\n                       epoch+1, i+1, len(train_loader), batch_time=batch_time,\n                       loss=losses,\n                       score=score_train))\n\n\n        print(""===> Average RMSE score on training set is {:.4f}"".format(score_train.avg))\n        print(""===> Average MAE score on training set is {:.4f}"".format(score_train_1.avg))\n        # Evaulate model on validation set\n        print(""=> Start validation set"")\n        score_valid, score_valid_1, losses_valid = validate(valid_loader, model, criterion_lidar, criterion_rgb, criterion_local, criterion_guide, epoch)\n        print(""===> Average RMSE score on validation set is {:.4f}"".format(score_valid))\n        print(""===> Average MAE score on validation set is {:.4f}"".format(score_valid_1))\n        # Evaluate model on selected validation set\n        if args.subset is None:\n            print(""=> Start selection validation set"")\n            score_selection, score_selection_1, losses_selection = validate(valid_selection_loader, model, criterion_lidar, criterion_rgb, criterion_local, criterion_guide, epoch)\n            total_score = score_selection\n            print(""===> Average RMSE score on selection set is {:.4f}"".format(score_selection))\n            print(""===> Average MAE score on selection set is {:.4f}"".format(score_selection_1))\n        else:\n            total_score = score_valid\n\n        print(""===> Last best score was RMSE of {:.4f} in epoch {}"".format(lowest_loss,\n                                                                           best_epoch))\n        # Adjust lr if loss plateaued\n        if args.lr_policy == \'plateau\':\n            scheduler.step(total_score)\n            lr = optimizer.param_groups[0][\'lr\']\n            print(\'LR plateaued, hence is set to {}\'.format(lr))\n\n        # File to keep latest epoch\n        with open(os.path.join(args.save_path, \'first_run.txt\'), \'w\') as f:\n            f.write(str(epoch))\n\n        # Save model\n        to_save = False\n        if total_score < lowest_loss:\n\n            to_save = True\n            best_epoch = epoch+1\n            lowest_loss = total_score\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'best epoch\': best_epoch,\n            \'arch\': args.mod,\n            \'state_dict\': model.state_dict(),\n            \'loss\': lowest_loss,\n            \'optimizer\': optimizer.state_dict()}, to_save, epoch)\n    if not args.no_tb:\n        writer.close()\n\n\ndef validate(loader, model, criterion_lidar, criterion_rgb, criterion_local, criterion_guide, epoch=0):\n    # batch_time = AverageMeter()\n    losses = AverageMeter()\n    metric = Metrics(max_depth=args.max_depth, disp=args.use_disp, normal=args.normal)\n    score = AverageMeter()\n    score_1 = AverageMeter()\n    # Evaluate model\n    model.eval()\n    # Only forward pass, hence no grads needed\n    with torch.no_grad():\n        # end = time.time()\n        for i, (input, gt) in tqdm(enumerate(loader)):\n            if not args.no_cuda:\n                input, gt = input.cuda(non_blocking=True), gt.cuda(non_blocking=True)\n            prediction, lidar_out, precise, guide = model(input, epoch)\n\n            loss = criterion_local(prediction, gt, epoch)\n            loss_lidar = criterion_lidar(lidar_out, gt, epoch)\n            loss_rgb = criterion_rgb(precise, gt, epoch)\n            loss_guide = criterion_guide(guide, gt, epoch)\n            loss = args.wpred*loss + args.wlid*loss_lidar + args.wrgb*loss_rgb + args.wguide*loss_guide\n            losses.update(loss.item(), input.size(0))\n\n            metric.calculate(prediction[:, 0:1], gt)\n            score.update(metric.get_metric(args.metric), metric.num)\n            score_1.update(metric.get_metric(args.metric_1), metric.num)\n\n            if (i + 1) % args.print_freq == 0:\n                print(\'Test: [{0}/{1}]\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Metric {score.val:.4f} ({score.avg:.4f})\'.format(\n                       i+1, len(loader), loss=losses,\n                       score=score))\n\n        if args.evaluate:\n            print(""===> Average RMSE score on validation set is {:.4f}"".format(score.avg))\n            print(""===> Average MAE score on validation set is {:.4f}"".format(score_1.avg))\n    return score.avg, score_1.avg, losses.avg\n\n\ndef save_checkpoint(state, to_copy, epoch):\n    filepath = os.path.join(args.save_path, \'checkpoint_model_epoch_{}.pth.tar\'.format(epoch))\n    torch.save(state, filepath)\n    if to_copy:\n        if epoch > 0:\n            lst = glob.glob(os.path.join(args.save_path, \'model_best*\'))\n            if len(lst) != 0:\n                os.remove(lst[0])\n        shutil.copyfile(filepath, os.path.join(args.save_path, \'model_best_epoch_{}.pth.tar\'.format(epoch)))\n        print(""Best model copied"")\n    if epoch > 0:\n        prev_checkpoint_filename = os.path.join(args.save_path, \'checkpoint_model_epoch_{}.pth.tar\'.format(epoch-1))\n        if os.path.exists(prev_checkpoint_filename):\n            os.remove(prev_checkpoint_filename)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Datasets/Kitti_loader.py,0,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport os\nimport sys\nimport re\nimport numpy as np\nfrom PIL import Image\n\nsys.path.insert(1, os.path.join(sys.path[0], \'..\'))\nfrom Utils.utils import write_file, depth_read\n\'\'\'\nattention:\n    There is mistake in 2011_09_26_drive_0009_sync/proj_depth 4 files were\n    left out 177-180 .png. Hence these files were also deleted in rgb\n\'\'\'\n\n\nclass Random_Sampler():\n    ""Class to downsample input lidar points""\n\n    def __init__(self, num_samples):\n        self.num_samples = num_samples\n\n    def sample(self, depth):\n        mask_keep = depth > 0\n        n_keep = np.count_nonzero(mask_keep)\n\n        if n_keep == 0:\n            return mask_keep\n        else:\n            depth_sampled = np.zeros(depth.shape)\n            prob = float(self.num_samples) / n_keep\n            mask_keep =  np.bitwise_and(mask_keep, np.random.uniform(0, 1, depth.shape) < prob)\n            depth_sampled[mask_keep] = depth[mask_keep]\n            return depth_sampled\n\n\nclass Kitti_preprocessing(object):\n    def __init__(self, dataset_path, input_type=\'depth\', side_selection=\'\'):\n        self.train_paths = {\'img\': [], \'lidar_in\': [], \'gt\': []}\n        self.val_paths = {\'img\': [], \'lidar_in\': [], \'gt\': []}\n        self.selected_paths = {\'img\': [], \'lidar_in\': [], \'gt\': []}\n        self.test_files = {\'img\': [], \'lidar_in\': []}\n        self.dataset_path = dataset_path\n        self.side_selection = side_selection\n        self.left_side_selection = \'image_02\'\n        self.right_side_selection = \'image_03\'\n        self.depth_keyword = \'proj_depth\'\n        self.rgb_keyword = \'Rgb\'\n        # self.use_rgb = input_type == \'rgb\'\n        self.use_rgb = True\n        self.date_selection = \'2011_09_26\'\n\n    def get_paths(self):\n        # train and validation dirs\n        for type_set in os.listdir(self.dataset_path):\n            for root, dirs, files in os.walk(os.path.join(self.dataset_path, type_set)):\n                if re.search(self.depth_keyword, root):\n                    self.train_paths[\'lidar_in\'].extend(sorted([os.path.join(root, file) for file in files\n                                                        if re.search(\'velodyne_raw\', root)\n                                                        and re.search(\'train\', root)\n                                                        and re.search(self.side_selection, root)]))\n                    self.val_paths[\'lidar_in\'].extend(sorted([os.path.join(root, file) for file in files\n                                                              if re.search(\'velodyne_raw\', root)\n                                                              and re.search(\'val\', root)\n                                                              and re.search(self.side_selection, root)]))\n                    self.train_paths[\'gt\'].extend(sorted([os.path.join(root, file) for file in files\n                                                          if re.search(\'groundtruth\', root)\n                                                          and re.search(\'train\', root)\n                                                          and re.search(self.side_selection, root)]))\n                    self.val_paths[\'gt\'].extend(sorted([os.path.join(root, file) for file in files\n                                                        if re.search(\'groundtruth\', root)\n                                                        and re.search(\'val\', root)\n                                                        and re.search(self.side_selection, root)]))\n                if self.use_rgb:\n                    if re.search(self.rgb_keyword, root) and re.search(self.side_selection, root):\n                        self.train_paths[\'img\'].extend(sorted([os.path.join(root, file) for file in files\n                                                               if re.search(\'train\', root)]))\n                                                               # and (re.search(\'image_02\', root) or re.search(\'image_03\', root))\n                                                               # and re.search(\'data\', root)]))\n                       # if len(self.train_paths[\'img\']) != 0:\n                           # test = [os.path.join(root, file) for file in files if re.search(\'train\', root)]\n                        self.val_paths[\'img\'].extend(sorted([os.path.join(root, file) for file in files\n                                                            if re.search(\'val\', root)]))\n                                                            # and (re.search(\'image_02\', root) or re.search(\'image_03\', root))\n                                                            # and re.search(\'data\', root)]))\n               # if len(self.train_paths[\'lidar_in\']) != len(self.train_paths[\'img\']):\n                   # print(root)\n\n\n    def downsample(self, lidar_data, destination, num_samples=500):\n        # Define sampler\n        sampler = Random_Sampler(num_samples)\n\n        for i, lidar_set_path in tqdm.tqdm(enumerate(lidar_data)):\n            # Read in lidar data\n            name = os.path.splitext(os.path.basename(lidar_set_path))[0]\n            sparse_depth = Image.open(lidar_set_path)\n\n\n            # Convert to numpy array\n            sparse_depth = np.array(sparse_depth, dtype=int)\n            assert(np.max(sparse_depth) > 255)\n\n            # Downsample per collumn\n            sparse_depth = sampler.sample(sparse_depth)\n\n            # Convert to img\n            sparse_depth_img = Image.fromarray(sparse_depth.astype(np.uint32))\n\n            # Save\n            folder = os.path.join(*str.split(lidar_set_path, os.path.sep)[7:12])\n            os.makedirs(os.path.join(destination, os.path.join(folder)), exist_ok=True)\n            sparse_depth_img.save(os.path.join(destination, os.path.join(folder, name)) + \'.png\')\n\n    def convert_png_to_rgb(self, rgb_images, destination):\n        for i, img_set_path in tqdm.tqdm(enumerate(rgb_images)):\n            name = os.path.splitext(os.path.basename(img_set_path))[0]\n            im = Image.open(img_set_path)\n            rgb_im = im.convert(\'RGB\')\n            folder = os.path.join(*str.split(img_set_path, os.path.sep)[8:12])\n            os.makedirs(os.path.join(destination, os.path.join(folder)), exist_ok=True)\n            rgb_im.save(os.path.join(destination, os.path.join(folder, name)) + \'.jpg\')\n            # rgb_im.save(os.path.join(destination, name) + \'.jpg\')\n\n    def get_selected_paths(self, selection):\n        files = []\n        for file in sorted(os.listdir(os.path.join(self.dataset_path, selection))):\n            files.append(os.path.join(self.dataset_path, os.path.join(selection, file)))\n        return files\n\n    def prepare_dataset(self):\n        path_to_val_sel = \'depth_selection/val_selection_cropped\'\n        path_to_test = \'depth_selection/test_depth_completion_anonymous\'\n        self.get_paths()\n        self.selected_paths[\'lidar_in\'] = self.get_selected_paths(os.path.join(path_to_val_sel, \'velodyne_raw\'))\n        self.selected_paths[\'gt\'] = self.get_selected_paths(os.path.join(path_to_val_sel, \'groundtruth_depth\'))\n        self.selected_paths[\'img\'] = self.get_selected_paths(os.path.join(path_to_val_sel, \'image\'))\n        self.test_files[\'lidar_in\'] = self.get_selected_paths(os.path.join(path_to_test, \'velodyne_raw\'))\n        if self.use_rgb:\n            self.selected_paths[\'img\'] = self.get_selected_paths(os.path.join(path_to_val_sel, \'image\'))\n            self.test_files[\'img\'] = self.get_selected_paths(os.path.join(path_to_test, \'image\'))\n            print(len(self.train_paths[\'lidar_in\']))\n            print(len(self.train_paths[\'img\']))\n            print(len(self.train_paths[\'gt\']))\n            print(len(self.val_paths[\'lidar_in\']))\n            print(len(self.val_paths[\'img\']))\n            print(len(self.val_paths[\'gt\']))\n            print(len(self.test_files[\'lidar_in\']))\n            print(len(self.test_files[\'img\']))\n\n    def compute_mean_std(self):\n        nums = np.array([])\n        means = np.array([])\n        stds = np.array([])\n        max_lst = np.array([])\n        for i, raw_img_path in tqdm.tqdm(enumerate(self.train_paths[\'lidar_in\'])):\n            raw_img = Image.open(raw_img_path)\n            raw_np = depth_read(raw_img)\n            vec = raw_np[raw_np >= 0]\n            # vec = vec/84.0\n            means = np.append(means, np.mean(vec))\n            stds = np.append(stds, np.std(vec))\n            nums = np.append(nums, len(vec))\n            max_lst = np.append(max_lst, np.max(vec))\n        mean = np.dot(nums, means)/np.sum(nums)\n        std = np.sqrt((np.dot(nums, stds**2) + np.dot(nums, (means-mean)**2))/np.sum(nums))\n        return mean, std, max_lst\n\n\nif __name__ == \'__main__\':\n\n    # Imports\n    import tqdm\n    from PIL import Image\n    import os\n    import argparse\n    from Utils.utils import str2bool\n\n    # arguments\n    parser = argparse.ArgumentParser(description=\'Preprocess\')\n    parser.add_argument(""--png2img"", type=str2bool, nargs=\'?\', const=True, default=False)\n    parser.add_argument(""--calc_params"", type=str2bool, nargs=\'?\', const=True, default=False)\n    parser.add_argument(\'--num_samples\', default=0, type=int, help=\'number of samples\')\n    parser.add_argument(\'--datapath\', default=\'/usr/data/tmp/Depth_Completion/data\')\n    parser.add_argument(\'--dest\', default=\'/usr/data/tmp/\')\n    args = parser.parse_args()\n\n    dataset = Kitti_preprocessing(args.datapath, input_type=\'rgb\')\n    dataset.prepare_dataset()\n    if args.png2img:\n        os.makedirs(os.path.join(args.dest, \'Rgb\'), exist_ok=True)\n        destination_train = os.path.join(args.dest, \'Rgb/train\')\n        destination_valid = os.path.join(args.dest, \'Rgb/val\')\n        dataset.convert_png_to_rgb(dataset.train_paths[\'img\'], destination_train)\n        dataset.convert_png_to_rgb(dataset.val_paths[\'img\'], destination_valid)\n    if args.calc_params:\n        import matplotlib.pyplot as plt\n        params = dataset.compute_mean_std()\n        mu_std = params[0:2]\n        max_lst = params[-1]\n        print(\'Means and std equals {} and {}\'.format(*mu_std))\n        plt.hist(max_lst, bins=\'auto\')\n        plt.title(\'Histogram for max depth\')\n        plt.show()\n        # mean, std = 14.969576188369581, 11.149000139428104\n        # Normalized\n        # mean, std = 0.17820924033773314, 0.1327261921360489\n    if args.num_samples != 0:\n        print(""Making downsampled dataset"")\n        os.makedirs(os.path.join(args.dest), exist_ok=True)\n        destination_train = os.path.join(args.dest, \'train\')\n        destination_valid = os.path.join(args.dest, \'val\')\n        dataset.downsample(dataset.train_paths[\'lidar_in\'], destination_train, args.num_samples)\n        dataset.downsample(dataset.val_paths[\'lidar_in\'], destination_valid, args.num_samples)\n'"
Datasets/__init__.py,0,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nfrom .Kitti_loader import *\n\ndataset_dict = {\'kitti\': Kitti_preprocessing}\n\ndef allowed_datasets():\n    return dataset_dict.keys()\n\ndef define_dataset(data, *args):\n    if data not in allowed_datasets():\n        raise KeyError(""The requested dataset is not implemented"")\n    else:\n        return dataset_dict[\'kitti\'](*args)\n    \n'"
Datasets/dataloader.py,4,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport os\nimport torch\nfrom PIL import Image\nimport random\nimport torchvision.transforms.functional as F\nfrom Utils.utils import depth_read\n\n\ndef get_loader(args, dataset):\n    """"""\n    Define the different dataloaders for training and validation\n    """"""\n    crop_size = (args.crop_h, args.crop_w)\n    perform_transformation = not args.no_aug\n\n    train_dataset = Dataset_loader(\n            args.data_path, dataset.train_paths, args.input_type, resize=None,\n            rotate=args.rotate, crop=crop_size, flip=args.flip, rescale=args.rescale,\n            max_depth=args.max_depth, sparse_val=args.sparse_val, normal=args.normal, \n            disp=args.use_disp, train=perform_transformation, num_samples=args.num_samples)\n    val_dataset = Dataset_loader(\n            args.data_path, dataset.val_paths, args.input_type, resize=None,\n            rotate=args.rotate, crop=crop_size, flip=args.flip, rescale=args.rescale,\n            max_depth=args.max_depth, sparse_val=args.sparse_val, normal=args.normal, \n            disp=args.use_disp, train=False, num_samples=args.num_samples)\n    val_select_dataset = Dataset_loader(\n            args.data_path, dataset.selected_paths, args.input_type,\n            resize=None, rotate=args.rotate, crop=crop_size,\n            flip=args.flip, rescale=args.rescale, max_depth=args.max_depth,\n            sparse_val=args.sparse_val, normal=args.normal, \n            disp=args.use_disp, train=False, num_samples=args.num_samples)\n\n    train_sampler = None\n    val_sampler = None\n    if args.subset is not None:\n        random.seed(1)\n        train_idx = [i for i in random.sample(range(len(train_dataset)-1), args.subset)]\n        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n        random.seed(1)\n        val_idx = [i for i in random.sample(range(len(val_dataset)-1), round(args.subset*0.5))]\n        val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_idx)\n\n    train_loader = DataLoader(\n        train_dataset, batch_size=args.batch_size, sampler=train_sampler,\n        shuffle=train_sampler is None, num_workers=args.nworkers,\n        pin_memory=True, drop_last=True)\n    val_loader = DataLoader(\n        val_dataset, batch_size=int(args.val_batch_size),  sampler=val_sampler,\n        shuffle=val_sampler is None, num_workers=args.nworkers_val,\n        pin_memory=True, drop_last=True)\n    val_selection_loader = DataLoader(\n        val_select_dataset, batch_size=int(args.val_batch_size), shuffle=False,\n        num_workers=args.nworkers_val, pin_memory=True, drop_last=True)\n    return train_loader, val_loader, val_selection_loader\n\n\nclass Dataset_loader(Dataset):\n    """"""Dataset with labeled lanes""""""\n\n    def __init__(self, data_path, dataset_type, input_type, resize,\n                 rotate, crop, flip, rescale, max_depth, sparse_val=0.0, \n                 normal=False, disp=False, train=False, num_samples=None):\n\n        # Constants\n        self.use_rgb = input_type == \'rgb\'\n        self.datapath = data_path\n        self.dataset_type = dataset_type\n        self.train = train\n        self.resize = resize\n        self.flip = flip\n        self.crop = crop\n        self.rotate = rotate\n        self.rescale = rescale\n        self.max_depth = max_depth\n        self.sparse_val = sparse_val\n\n        # Transformations\n        self.totensor = transforms.ToTensor()\n        self.center_crop = transforms.CenterCrop(size=crop)\n\n        # Names\n        self.img_name = \'img\'\n        self.lidar_name = \'lidar_in\' \n        self.gt_name = \'gt\' \n\n        # Define random sampler\n        self.num_samples = num_samples\n\n\n    def __len__(self):\n        """"""\n        Conventional len method\n        """"""\n        return len(self.dataset_type[\'lidar_in\'])\n\n\n    def define_transforms(self, input, gt, img=None):\n        # Define random variabels\n        hflip_input = np.random.uniform(0.0, 1.0) > 0.5 and self.flip == \'hflip\'\n\n        if self.train:\n            i, j, h, w = transforms.RandomCrop.get_params(input, output_size=self.crop)\n            input = F.crop(input, i, j, h, w)\n            gt = F.crop(gt, i, j, h, w)\n            if hflip_input:\n                input, gt = F.hflip(input), F.hflip(gt)\n\n            if self.use_rgb:\n                img = F.crop(img, i, j, h, w)\n                if hflip_input:\n                    img = F.hflip(img)\n            input, gt = depth_read(input, self.sparse_val), depth_read(gt, self.sparse_val)\n            \n        else:\n            input, gt = self.center_crop(input), self.center_crop(gt)\n            if self.use_rgb:\n                img = self.center_crop(img)\n            input, gt = depth_read(input, self.sparse_val), depth_read(gt, self.sparse_val)\n            \n\n        return input, gt, img\n\n    def __getitem__(self, idx):\n        """"""\n        Args: idx (int): Index of images to make batch\n        Returns (tuple): Sample of velodyne data and ground truth.\n        """"""\n        sparse_depth_name = os.path.join(self.dataset_type[self.lidar_name][idx])\n        gt_name = os.path.join(self.dataset_type[self.gt_name][idx])\n        with open(sparse_depth_name, \'rb\') as f:\n            sparse_depth = Image.open(f)\n            w, h = sparse_depth.size\n            sparse_depth = F.crop(sparse_depth, h-self.crop[0], 0, self.crop[0], w)\n        with open(gt_name, \'rb\') as f:\n            gt = Image.open(f)\n            gt = F.crop(gt, h-self.crop[0], 0, self.crop[0], w)\n        img = None\n        if self.use_rgb:\n            img_name = self.dataset_type[self.img_name][idx]\n            with open(img_name, \'rb\') as f:\n                img = (Image.open(f).convert(\'RGB\'))\n            img = F.crop(img, h-self.crop[0], 0, self.crop[0], w)\n\n        sparse_depth_np, gt_np, img_pil = self.define_transforms(sparse_depth, gt, img)\n        input, gt = self.totensor(sparse_depth_np).float(), self.totensor(gt_np).float()\n\n        if self.use_rgb:\n            img_tensor = self.totensor(img_pil).float()\n            img_tensor = img_tensor*255.0\n            input = torch.cat((input, img_tensor), dim=0)\n        return input, gt\n\n'"
Loss/benchmark_metrics.py,3,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport torch\n\nclass Metrics(object):\n    def __init__(self, max_depth=85.0, disp=False, normal=False):\n        self.rmse, self.mae = 0, 0\n        self.num = 0\n        self.disp = disp\n        self.max_depth = max_depth\n        self.min_disp = 1.0/max_depth\n        self.normal = normal\n\n    def calculate(self, prediction, gt):\n        valid_mask = (gt > 0).detach()\n\n        self.num = valid_mask.sum().item()\n        prediction = prediction[valid_mask]\n        gt = gt[valid_mask]\n\n        if self.disp:\n            prediction = torch.clamp(prediction, min=self.min_disp)\n            prediction = 1./prediction\n            gt = 1./gt\n        if self.normal:\n            prediction = prediction * self.max_depth\n            gt = gt * self.max_depth\n        prediction = torch.clamp(prediction, min=0, max=self.max_depth)\n\n        abs_diff = (prediction - gt).abs()\n        self.rmse = torch.sqrt(torch.mean(torch.pow(abs_diff, 2))).item()\n        self.mae = abs_diff.mean().item()\n\n    def get_metric(self, metric_name):\n        return self.__dict__[metric_name]\n\n\ndef allowed_metrics():\n    return Metrics().__dict__.keys()\n'"
Loss/loss.py,21,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef allowed_losses():\n    return loss_dict.keys()\n\n\ndef define_loss(loss_name, *args):\n    if loss_name not in allowed_losses():\n        raise NotImplementedError(\'Loss functions {} is not yet implemented\'.format(loss_name))\n    else:\n        return loss_dict[loss_name](*args)\n\n\nclass MAE_loss(nn.Module):\n    def __init__(self):\n        super(MAE_loss, self).__init__()\n\n    def forward(self, prediction, gt, input, epoch=0):\n        prediction = prediction[:, 0:1]\n        abs_err = torch.abs(prediction - gt)\n        mask = (gt > 0).detach()\n        mae_loss = torch.mean(abs_err[mask])\n        return mae_loss\n\n\nclass MAE_log_loss(nn.Module):\n    def __init__(self):\n        super(MAE_log_loss, self).__init__()\n\n    def forward(self, prediction, gt):\n        prediction = torch.clamp(prediction, min=0)\n        abs_err = torch.abs(torch.log(prediction+1e-6) - torch.log(gt+1e-6))\n        mask = (gt > 0).detach()\n        mae_log_loss = torch.mean(abs_err[mask])\n        return mae_log_loss\n\n\nclass MSE_loss(nn.Module):\n    def __init__(self):\n        super(MSE_loss, self).__init__()\n\n    def forward(self, prediction, gt, epoch=0):\n        err = prediction[:,0:1] - gt\n        mask = (gt > 0).detach()\n        mse_loss = torch.mean((err[mask])**2)\n        return mse_loss\n\n\nclass MSE_loss_uncertainty(nn.Module):\n    def __init__(self):\n        super(MSE_loss_uncertainty, self).__init__()\n\n    def forward(self, prediction, gt, epoch=0):\n        mask = (gt > 0).detach()\n        depth = prediction[:, 0:1, :, :]\n        conf = torch.abs(prediction[:, 1:, :, :])\n        err = depth - gt\n        conf_loss = torch.mean(0.5*(err[mask]**2)*torch.exp(-conf[mask]) + 0.5*conf[mask])\n        return conf_loss \n\n\nclass MSE_log_loss(nn.Module):\n    def __init__(self):\n        super(MSE_log_loss, self).__init__()\n\n    def forward(self, prediction, gt):\n        prediction = torch.clamp(prediction, min=0)\n        err = torch.log(prediction+1e-6) - torch.log(gt+1e-6)\n        mask = (gt > 0).detach()\n        mae_log_loss = torch.mean(err[mask]**2)\n        return mae_log_loss\n\n\nclass Huber_loss(nn.Module):\n    def __init__(self, delta=10):\n        super(Huber_loss, self).__init__()\n        self.delta = delta\n\n    def forward(self, outputs, gt, input, epoch=0):\n        outputs = outputs[:, 0:1, :, :]\n        err = torch.abs(outputs - gt)\n        mask = (gt > 0).detach()\n        err = err[mask]\n        squared_err = 0.5*err**2\n        linear_err = err - 0.5*self.delta\n        return torch.mean(torch.where(err < self.delta, squared_err, linear_err))\n\n\n\nclass Berhu_loss(nn.Module):\n    def __init__(self, delta=0.05):\n        super(Berhu_loss, self).__init__()\n        self.delta = delta\n\n    def forward(self, prediction, gt, epoch=0):\n        prediction = prediction[:, 0:1]\n        err = torch.abs(prediction - gt)\n        mask = (gt > 0).detach()\n        err = torch.abs(err[mask])\n        c = self.delta*err.max().item()\n        squared_err = (err**2+c**2)/(2*c)\n        linear_err = err\n        return torch.mean(torch.where(err > c, squared_err, linear_err))\n\n\nclass Huber_delta1_loss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, prediction, gt, input):\n        mask = (gt > 0).detach().float()\n        loss = F.smooth_l1_loss(prediction*mask, gt*mask, reduction=\'none\')\n        return torch.mean(loss)\n\n\nclass Disparity_Loss(nn.Module):\n    def __init__(self, order=2):\n        super(Disparity_Loss, self).__init__()\n        self.order = order\n\n    def forward(self, prediction, gt):\n        mask = (gt > 0).detach()\n        gt = gt[mask]\n        gt = 1./gt\n        prediction = prediction[mask]\n        err = torch.abs(prediction - gt)\n        err = torch.mean(err**self.order)\n        return err\n\n\nloss_dict = {\n    \'mse\': MSE_loss,\n    \'mae\': MAE_loss,\n    \'log_mse\': MSE_log_loss,\n    \'log_mae\': MAE_log_loss,\n    \'huber\': Huber_loss,\n    \'huber1\': Huber_delta1_loss,\n    \'berhu\': Berhu_loss,\n    \'disp\': Disparity_Loss,\n    \'uncert\': MSE_loss_uncertainty}\n'"
Models/ERFNet.py,3,"b'# ERFNet full model definition for Pytorch\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n\n\nclass non_bottleneck_1d (nn.Module):\n    def __init__(self, chann, dropprob, dilated):\n        super().__init__()\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1, 0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation=(dilated, 1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1*dilated), bias=True, dilation=(1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n\n    def forward(self, input):\n\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n\n        return F.relu(output+input)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        chans = 32 if in_channels > 16 else 16\n        self.initial_block = DownsamplerBlock(in_channels, chans)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(chans, 64))\n\n        for x in range(0, 5):\n            self.layers.append(non_bottleneck_1d(64, 0.03, 1)) \n\n        self.layers.append(DownsamplerBlock(64, 128))\n\n        for x in range(0, 2):\n            self.layers.append(non_bottleneck_1d(128, 0.3, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 16))\n\n        #Only in encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass UpsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.bn(output)\n        return F.relu(output)\n\n\nclass Decoder (nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.layer1 = UpsamplerBlock(128, 64)\n        self.layer2 = non_bottleneck_1d(64, 0, 1)\n        self.layer3 = non_bottleneck_1d(64, 0, 1) # 64x64x304\n\n        self.layer4 = UpsamplerBlock(64, 32)\n        self.layer5 = non_bottleneck_1d(32, 0, 1)\n        self.layer6 = non_bottleneck_1d(32, 0, 1) # 32x128x608\n\n        self.output_conv = nn.ConvTranspose2d(32, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n\n    def forward(self, input):\n        output = input\n        output = self.layer1(output)\n        output = self.layer2(output)\n        output = self.layer3(output)\n        em2 = output\n        output = self.layer4(output)\n        output = self.layer5(output)\n        output = self.layer6(output)\n        em1 = output\n\n        output = self.output_conv(output)\n\n        return output, em1, em2\n\n\nclass Net(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):  #use encoder to pass pretrained encoder\n        super().__init__()\n        self.encoder = Encoder(in_channels, out_channels)\n        self.decoder = Decoder(out_channels)\n\n    def forward(self, input, only_encode=False):\n        if only_encode:\n            return self.encoder.forward(input, predict=True)\n        else:\n            output = self.encoder(input)\n            return self.decoder.forward(output)\n'"
Models/__init__.py,0,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nfrom .model import uncertainty_net as model\n\nmodel_dict = {\'mod\': model}\n\ndef allowed_models():\n    return model_dict.keys()\n\n\ndef define_model(mod, **kwargs):\n    if mod not in allowed_models():\n        raise KeyError(""The requested model: {} is not implemented"".format(mod))\n    else:\n        return model_dict[mod](**kwargs)\n'"
Models/model.py,10,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nimport numpy as np\nfrom .ERFNet import Net\n\nclass uncertainty_net(nn.Module):\n    def __init__(self, in_channels, out_channels=1, thres=15):\n        super(uncertainty_net, self).__init__()\n        out_chan = 2\n\n        combine = \'concat\'\n        self.combine = combine\n        self.in_channels = in_channels\n\n        out_channels = 3\n        self.depthnet = Net(in_channels=in_channels, out_channels=out_channels)\n\n        local_channels_in = 2 if self.combine == \'concat\' else 1\n        self.convbnrelu = nn.Sequential(convbn(local_channels_in, 32, 3, 1, 1, 1),\n                                        nn.ReLU(inplace=True))\n        self.hourglass1 = hourglass_1(32)\n        self.hourglass2 = hourglass_2(32)\n        self.fuse = nn.Sequential(convbn(32, 32, 3, 1, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   nn.Conv2d(32, out_chan, kernel_size=3, padding=1, stride=1, bias=True))\n        self.activation = nn.ReLU(inplace=True)\n        self.thres = thres\n        self.softmax = torch.nn.Softmax(dim=1)\n\n    def forward(self, input, epoch=50):\n        if self.in_channels > 1:\n            rgb_in = input[:, 1:, :, :]\n            lidar_in = input[:, 0:1, :, :]\n        else:\n            lidar_in = input\n\n        # 1. GLOBAL NET\n        embedding0, embedding1, embedding2 = self.depthnet(input)\n\n        global_features = embedding0[:, 0:1, :, :]\n        precise_depth = embedding0[:, 1:2, :, :]\n        conf = embedding0[:, 2:, :, :]\n\n        # 2. Fuse \n        if self.combine == \'concat\':\n            input = torch.cat((lidar_in, global_features), 1)\n        elif self.combine == \'add\':\n            input = lidar_in + global_features\n        elif self.combine == \'mul\':\n            input = lidar_in * global_features\n        elif self.combine == \'sigmoid\':\n            input = lidar_in * nn.Sigmoid()(global_features)\n        else:\n            input = lidar_in\n\n        # 3. LOCAL NET\n        out = self.convbnrelu(input)\n        out1, embedding3, embedding4 = self.hourglass1(out, embedding1, embedding2)\n        out1 = out1 + out\n        out2 = self.hourglass2(out1, embedding3, embedding4)\n        out2 = out2 + out\n        out = self.fuse(out2)\n        lidar_out = out\n\n        # 4. Late Fusion\n        lidar_to_depth, lidar_to_conf = torch.chunk(out, 2, dim=1)\n        lidar_to_conf, conf = torch.chunk(self.softmax(torch.cat((lidar_to_conf, conf), 1)), 2, dim=1)\n        out = conf * precise_depth + lidar_to_conf * lidar_to_depth\n\n        return out, lidar_out, precise_depth, global_features\n\n\ndef convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False))\n                         # nn.BatchNorm2d(out_planes))\n\n\nclass hourglass_1(nn.Module):\n    def __init__(self, channels_in):\n        super(hourglass_1, self).__init__()\n\n        self.conv1 = nn.Sequential(convbn(channels_in, channels_in, kernel_size=3, stride=2, pad=1, dilation=1),\n                                   nn.ReLU(inplace=True))\n\n        self.conv2 = convbn(channels_in, channels_in, kernel_size=3, stride=1, pad=1, dilation=1)\n\n        self.conv3 = nn.Sequential(convbn(channels_in*2, channels_in*2, kernel_size=3, stride=2, pad=1, dilation=1),\n                                   nn.ReLU(inplace=True))\n\n        self.conv4 = nn.Sequential(convbn(channels_in*2, channels_in*2, kernel_size=3, stride=1, pad=1, dilation=1))\n\n        self.conv5 = nn.Sequential(nn.ConvTranspose2d(channels_in*4, channels_in*2, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n                                   nn.BatchNorm2d(channels_in*2),\n                                   nn.ReLU(inplace=True))\n\n        self.conv6 = nn.Sequential(nn.ConvTranspose2d(channels_in*2, channels_in, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n                                   nn.BatchNorm2d(channels_in))\n\n    def forward(self, x, em1, em2):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.relu(x, inplace=True)\n        x = torch.cat((x, em1), 1)\n\n        x_prime = self.conv3(x)\n        x_prime = self.conv4(x_prime)\n        x_prime = F.relu(x_prime, inplace=True)\n        x_prime = torch.cat((x_prime, em2), 1)\n\n        out = self.conv5(x_prime)\n        out = self.conv6(out)\n\n        return out, x, x_prime\n\n\nclass hourglass_2(nn.Module):\n    def __init__(self, channels_in):\n        super(hourglass_2, self).__init__()\n\n        self.conv1 = nn.Sequential(convbn(channels_in, channels_in*2, kernel_size=3, stride=2, pad=1, dilation=1),\n                                   nn.BatchNorm2d(channels_in*2),\n                                   nn.ReLU(inplace=True))\n\n        self.conv2 = convbn(channels_in*2, channels_in*2, kernel_size=3, stride=1, pad=1, dilation=1)\n\n        self.conv3 = nn.Sequential(convbn(channels_in*2, channels_in*2, kernel_size=3, stride=2, pad=1, dilation=1),\n                                   nn.BatchNorm2d(channels_in*2),\n                                   nn.ReLU(inplace=True))\n\n        self.conv4 = nn.Sequential(convbn(channels_in*2, channels_in*4, kernel_size=3, stride=1, pad=1, dilation=1))\n\n        self.conv5 = nn.Sequential(nn.ConvTranspose2d(channels_in*4, channels_in*2, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n                                   nn.BatchNorm2d(channels_in*2),\n                                   nn.ReLU(inplace=True))\n\n        self.conv6 = nn.Sequential(nn.ConvTranspose2d(channels_in*2, channels_in, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n                                   nn.BatchNorm2d(channels_in))\n\n    def forward(self, x, em1, em2):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = x + em1\n        x = F.relu(x, inplace=True)\n\n        x_prime = self.conv3(x)\n        x_prime = self.conv4(x_prime)\n        x_prime = x_prime + em2\n        x_prime = F.relu(x_prime, inplace=True)\n\n        out = self.conv5(x_prime)\n        out = self.conv6(out)\n\n        return out \n\n\n\nif __name__ == \'__main__\':\n    batch_size = 4\n    in_channels = 4\n    H, W = 256, 1216\n    model = uncertainty_net(34, in_channels).cuda()\n    print(model)\n    print(""Number of parameters in model is {:.3f}M"".format(sum(tensor.numel() for tensor in model.parameters())/1e6))\n    input = torch.rand((batch_size, in_channels, H, W)).cuda().float()\n    out = model(input)\n    print(out.shape)\n'"
Test/test.py,12,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport argparse\nimport torch\nimport torchvision.transforms as transforms\nimport os, sys\nfrom PIL import Image\nimport glob\nimport tqdm\nsys.path.insert(1, os.path.join(sys.path[0], \'..\'))\ncwd = os.getcwd()\nprint(cwd)\nimport numpy as np\nfrom Utils.utils import str2bool, AverageMeter, depth_read \nimport Models\nimport Datasets\nfrom PIL import ImageOps\nimport matplotlib.pyplot as plt\nimport time\n\n#Training setttings\nparser = argparse.ArgumentParser(description=\'KITTI Depth Completion Task TEST\')\nparser.add_argument(\'--dataset\', type=str, default=\'kitti\', choices = Datasets.allowed_datasets(), help=\'dataset to work with\')\nparser.add_argument(\'--mod\', type=str, default=\'mod\', choices = Models.allowed_models(), help=\'Model for use\')\nparser.add_argument(\'--no_cuda\', action=\'store_true\', help=\'no gpu usage\')\nparser.add_argument(\'--input_type\', type=str, default=\'rgb\', help=\'use rgb for rgbdepth\')\n# Data augmentation settings\nparser.add_argument(\'--crop_w\', type=int, default=1216, help=\'width of image after cropping\')\nparser.add_argument(\'--crop_h\', type=int, default=256, help=\'height of image after cropping\')\n\n# Paths settings\nparser.add_argument(\'--save_path\', type= str, default=\'../Saved/best\', help=\'save path\')\nparser.add_argument(\'--data_path\', type=str, required=True, help=\'path to desired datasets\')\n\n# Cudnn\nparser.add_argument(""--cudnn"", type=str2bool, nargs=\'?\', const=True, default=True, help=""cudnn optimization active"")\nparser.add_argument(\'--multi\', type=str2bool, nargs=\'?\', const=True, default=False, help=""use multiple gpus"")\nparser.add_argument(\'--normal\', type=str2bool, nargs=\'?\', const=True, default=False, help=""Normalize input"")\nparser.add_argument(\'--max_depth\', type=float, default=85.0, help=""maximum depth of input"")\nparser.add_argument(\'--sparse_val\', type=float, default=0.0, help=""encode sparse values with 0"")\nparser.add_argument(\'--num_samples\', default=0, type=int, help=\'number of samples\')\n\n\ndef main():\n    global args\n    global dataset\n    args = parser.parse_args()\n\n    torch.backends.cudnn.benchmark = args.cudnn\n\n    best_file_name = glob.glob(os.path.join(args.save_path, \'model_best*\'))[0]\n\n    save_root = os.path.join(os.path.dirname(best_file_name), \'results\')\n    if not os.path.isdir(save_root):\n        os.makedirs(save_root)\n\n    print(""==========\\nArgs:{}\\n=========="".format(args))\n    # INIT\n    print(""Init model: \'{}\'"".format(args.mod))\n    channels_in = 1 if args.input_type == \'depth\' else 4\n    model = Models.define_model(mod=args.mod, in_channels=channels_in)\n    print(""Number of parameters in model {} is {:.3f}M"".format(args.mod.upper(), sum(tensor.numel() for tensor in model.parameters())/1e6))\n    if not args.no_cuda:\n        # Load on gpu before passing params to optimizer\n        if not args.multi:\n            model = model.cuda()\n        else:\n            model = torch.nn.DataParallel(model).cuda()\n    if os.path.isfile(best_file_name):\n        print(""=> loading checkpoint \'{}\'"".format(best_file_name))\n        checkpoint = torch.load(best_file_name)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        lowest_loss = checkpoint[\'loss\']\n        best_epoch = checkpoint[\'best epoch\']\n        print(\'Lowest RMSE for selection validation set was {:.4f} in epoch {}\'.format(lowest_loss, best_epoch))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(best_file_name))\n        return\n\n    if not args.no_cuda:\n        model = model.cuda()\n    print(""Initializing dataset {}"".format(args.dataset))\n    dataset = Datasets.define_dataset(args.dataset, args.data_path, args.input_type)\n    dataset.prepare_dataset()\n    to_pil = transforms.ToPILImage()\n    to_tensor = transforms.ToTensor()\n    norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    depth_norm = transforms.Normalize(mean=[14.97/args.max_depth], std=[11.15/args.max_depth])\n    model.eval()\n    print(""===> Start testing"")\n    total_time = []\n\n    with torch.no_grad():\n        for i, (img, rgb, gt) in tqdm.tqdm(enumerate(zip(dataset.selected_paths[\'lidar_in\'],\n                                           dataset.selected_paths[\'img\'], dataset.selected_paths[\'gt\']))):\n\n            raw_path = os.path.join(img)\n            raw_pil = Image.open(raw_path)\n            gt_path = os.path.join(gt)\n            gt_pil = Image.open(gt)\n            assert raw_pil.size == (1216, 352)\n\n            crop = 352-args.crop_h\n            raw_pil_crop = raw_pil.crop((0, crop, 1216, 352))\n            gt_pil_crop = gt_pil.crop((0, crop, 1216, 352))\n\n            raw = depth_read(raw_pil_crop, args.sparse_val)\n            raw = to_tensor(raw).float()\n            gt = depth_read(gt_pil_crop, args.sparse_val)\n            gt = to_tensor(gt).float()\n            valid_mask = (raw > 0).detach().float()\n\n            input = torch.unsqueeze(raw, 0).cuda()\n            gt = torch.unsqueeze(gt, 0).cuda()\n\n            if args.normal:\n                # Put in {0-1} range and then normalize\n                input = input/args.max_depth\n                # input = depth_norm(input)\n\n            if args.input_type == \'rgb\':\n                rgb_path = os.path.join(rgb)\n                rgb_pil = Image.open(rgb_path)\n                assert rgb_pil.size == (1216, 352)\n                rgb_pil_crop = rgb_pil.crop((0, crop, 1216, 352))\n                rgb = to_tensor(rgb_pil_crop).float()\n                rgb = torch.unsqueeze(rgb, 0).cuda()\n                if not args.normal:\n                    rgb = rgb*255.0\n\n                input = torch.cat((input, rgb), 1)\n\n            torch.cuda.synchronize()\n            a = time.perf_counter()\n            output, _, _, _ = model(input)\n            torch.cuda.synchronize()\n            b = time.perf_counter()\n            total_time.append(b-a)\n            if args.normal:\n                output = output*args.max_depth\n            output = torch.clamp(output, min=0, max=85)\n\n            output = output * 256.\n            raw = raw * 256.\n            output = output[0][0:1].cpu()\n            data = output[0].numpy()\n    \n            if crop != 0:\n                padding = (0, 0, crop, 0)\n                output = torch.nn.functional.pad(output, padding, ""constant"", 0)\n                output[:, 0:crop] = output[:, crop].repeat(crop, 1)\n\n            pil_img = to_pil(output.int())\n            assert pil_img.size == (1216, 352)\n            pil_img.save(os.path.join(save_root, os.path.basename(img)))\n    print(\'average_time: \', sum(total_time[100:])/(len(total_time[100:])))\n    print(\'num imgs: \', i + 1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Utils/utils.py,14,"b'""""""\nAuthor: Wouter Van Gansbeke\nLicensed under the CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/)\n""""""\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.rcParams[\'figure.figsize\'] = (35, 30)\nfrom PIL import Image\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport argparse\nimport os\nimport torch.optim\nfrom torch.optim import lr_scheduler\nimport errno\nimport sys\nfrom torchvision import transforms\nimport torch.nn.init as init\nimport torch.distributed as dist\n\ndef define_optim(optim, params, lr, weight_decay):\n    if optim == \'adam\':\n        optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n    elif optim == \'sgd\':\n        optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n    elif optim == \'rmsprop\':\n        optimizer = torch.optim.RMSprop(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n    else:\n        raise KeyError(""The requested optimizer: {} is not implemented"".format(optim))\n    return optimizer\n\n\ndef define_scheduler(optimizer, args):\n    if args.lr_policy == \'lambda\':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + 1 - args.niter) / float(args.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif args.lr_policy == \'step\':\n        scheduler = lr_scheduler.StepLR(optimizer,\n                                        step_size=args.lr_decay_iters, gamma=args.gamma)\n    elif args.lr_policy == \'plateau\':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\',\n                                                   factor=args.gamma,\n                                                   threshold=0.0001,\n                                                   patience=args.lr_decay_iters)\n    elif args.lr_policy == \'none\':\n        scheduler = None\n    else:\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', args.lr_policy)\n    return scheduler\n\n\ndef define_init_weights(model, init_w=\'normal\', activation=\'relu\'):\n    print(\'Init weights in network with [{}]\'.format(init_w))\n    if init_w == \'normal\':\n        model.apply(weights_init_normal)\n    elif init_w == \'xavier\':\n        model.apply(weights_init_xavier)\n    elif init_w == \'kaiming\':\n        model.apply(weights_init_kaiming)\n    elif init_w == \'orthogonal\':\n        model.apply(weights_init_orthogonal)\n    else:\n        raise NotImplementedError(\'initialization method [{}] is not implemented\'.format(init_w))\n\n\ndef first_run(save_path):\n    txt_file = os.path.join(save_path, \'first_run.txt\')\n    if not os.path.exists(txt_file):\n        open(txt_file, \'w\').close()\n    else:\n        saved_epoch = open(txt_file).read()\n        if saved_epoch is None:\n            print(\'You forgot to delete [first run file]\')\n            return \'\'\n        return saved_epoch\n    return \'\'\n\n\ndef depth_read(img, sparse_val):\n    # loads depth map D from png file\n    # and returns it as a numpy array,\n    # for details see readme.txt\n    depth_png = np.array(img, dtype=int)\n    depth_png = np.expand_dims(depth_png, axis=2)\n    # make sure we have a proper 16bit depth map here.. not 8bit!\n    assert(np.max(depth_png) > 255)\n    depth = depth_png.astype(np.float) / 256.\n    depth[depth_png == 0] = sparse_val\n    return depth\n\n\nclass show_figs():\n    def __init__(self, input_type, savefig=False):\n        self.input_type = input_type\n        self.savefig = savefig\n\n    def save(self, img, name):\n        img.save(name)\n\n    def transform(self, input, name=\'test.png\'):\n        if isinstance(input, torch.tensor):\n            input = torch.clamp(input, min=0, max=255).int().cpu().numpy()\n            input = input * 256.\n            img = Image.fromarray(input)\n\n        elif isinstance(input, np.array):\n            img = Image.fromarray(input)\n\n        else:\n            raise NotImplementedError(\'Input type not recognized type\')\n\n        if self.savefig:\n            self.save(img, name)\n        else:\n            return img\n\n# trick from stackoverflow\ndef str2bool(argument):\n    if argument.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif argument.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Wrong argument in argparse, should be a boolean\')\n\n\ndef mkdir_if_missing(directory):\n    if not os.path.exists(directory):\n        try:\n            os.makedirs(directory)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef write_file(content, location):\n    file = open(location, \'w\')\n    file.write(str(content))\n    file.close()\n\n\nclass Logger(object):\n    """"""\n    Source https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\n    """"""\n    def __init__(self, fpath=None):\n        self.console = sys.stdout\n        self.file = None\n        self.fpath = fpath\n        if fpath is not None:\n            mkdir_if_missing(os.path.dirname(fpath))\n            self.file = open(fpath, \'w\')\n\n    def __del__(self):\n        self.close()\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *args):\n        self.close()\n\n    def write(self, msg):\n        self.console.write(msg)\n        if self.file is not None:\n            self.file.write(msg)\n\n    def flush(self):\n        self.console.flush()\n        if self.file is not None:\n            self.file.flush()\n            os.fsync(self.file.fileno())\n\n    def close(self):\n        self.console.close()\n        if self.file is not None:\n            self.file.close()\n\ndef save_image(img_merge, filename):\n    img_merge = Image.fromarray(img_merge.astype(\'uint8\'))\n    img_merge.save(filename)\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n#    print(classname)\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\n        init.normal_(m.weight.data, 0.0, 0.02)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif classname.find(\'Linear\') != -1:\n        init.normal_(m.weight.data, 0.0, 0.02)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif classname.find(\'BatchNorm2d\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_xavier(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\n        init.xavier_normal_(m.weight.data, gain=0.02)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif classname.find(\'Linear\') != -1:\n        init.xavier_normal_(m.weight.data, gain=0.02)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif classname.find(\'BatchNorm2d\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\', nonlinearity=\'relu\')\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif classname.find(\'Linear\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\', nonlinearity=\'relu\')\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif classname.find(\'BatchNorm2d\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef weights_init_orthogonal(m):\n    classname = m.__class__.__name__\n#    print(classname)\n    if classname.find(\'Conv\') != -1 or classname.find(\'ConvTranspose\') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif classname.find(\'Linear\') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif classname.find(\'BatchNorm2d\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\n\ndef save_fig(inp, name=\'saved.png\'):\n    if isinstance(inp, torch.Tensor):\n        # inp = inp.permute([2, 0, 1])\n        inp = transforms.ToPILImage()(inp.int())\n        inp.save(name)\n        return\n    pil = Image.fromarray(inp)\n    pil.save(name)\n\ndef setup_for_distributed(is_master):\n    """"""\n    This function disables printing when not in master process\n    """"""\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop(\'force\', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\ndef init_distributed_mode(args):\n    if \'RANK\' in os.environ and \'WORLD_SIZE\' in os.environ:\n        args.rank = int(os.environ[""RANK""])\n        args.world_size = int(os.environ[\'WORLD_SIZE\'])\n        args.gpu = int(os.environ[\'LOCAL_RANK\'])\n    elif \'SLURM_PROCID\' in os.environ:\n        args.rank = int(os.environ[\'SLURM_PROCID\'])\n        args.gpu = args.rank % torch.cuda.device_count()\n    else:\n        print(\'Not using distributed mode\')\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = \'nccl\'\n    print(\'| distributed init (rank {}): {}\'.format(\n        args.rank, args.dist_url), flush=True)\n    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                         world_size=args.world_size, rank=args.rank)\n    # Does not seem to work?\n    torch.distributed.barrier()\n    setup_for_distributed(args.rank == 0)\n'"
