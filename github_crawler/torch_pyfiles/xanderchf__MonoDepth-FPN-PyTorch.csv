file_path,api_count,code
main_fpn.py,47,"b'import numpy as np\nimport os, sys\nfrom constants import *\nfrom model_fpn import I2D\nimport argparse, time\nfrom utils.net_utils import adjust_learning_rate\nimport torch\nfrom torch.autograd import Variable\n# from dataset.dataloader import DepthDataset\nfrom dataset.nyuv2_dataset import NYUv2Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data.sampler import Sampler\nfrom collections import Counter\nimport matplotlib, cv2\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nclass RMSE_log(nn.Module):\n    def __init__(self):\n        super(RMSE_log, self).__init__()\n    \n    def forward(self, fake, real):\n        if not fake.shape == real.shape:\n            _,_,H,W = real.shape\n            fake = F.upsample(fake, size=(H,W), mode=\'bilinear\')\n        loss = torch.sqrt( torch.mean( torch.abs(torch.log(real)-torch.log(fake)) ** 2 ) )\n        return loss\n\nclass L1(nn.Module):\n    def __init__(self):\n        super(L1, self).__init__()\n    \n    def forward(self, fake, real):\n        if not fake.shape == real.shape:\n            _,_,H,W = real.shape\n            fake = F.upsample(fake, size=(H,W), mode=\'bilinear\')\n        loss = torch.mean( torch.abs(10.*real-10.*fake) )\n        return loss\n\nclass L1_log(nn.Module):\n    def __init__(self):\n        super(L1_log, self).__init__()\n    \n    def forward(self, fake, real):\n        if not fake.shape == real.shape:\n            _,_,H,W = real.shape\n            fake = F.upsample(fake, size=(H,W), mode=\'bilinear\')\n        loss = torch.mean( torch.abs(torch.log(real)-torch.log(fake)) )\n        return loss\n    \nclass BerHu(nn.Module):\n    def __init__(self, threshold=0.2):\n        super(BerHu, self).__init__()\n        self.threshold = threshold\n    \n    def forward(real, fake):\n        mask = real>0\n        if not fake.shape == real.shape:\n            _,_,H,W = real.shape\n            fake = F.upsample(fake, size=(H,W), mode=\'bilinear\')\n        fake = fake * mask\n        diff = torch.abs(real-fake)\n        delta = self.threshold * torch.max(diff).data.cpu().numpy()[0]\n\n        part1 = -F.threshold(-diff, -delta, 0.)\n        part2 = F.threshold(diff**2 - delta**2, 0., -delta**2.) + delta**2\n        part2 = part2 / (2.*delta)\n\n        loss = part1 + part2\n        loss = torch.sum(loss)\n        return loss\n    \nclass RMSE(nn.Module):\n    def __init__(self):\n        super(RMSE, self).__init__()\n    \n    def forward(self, fake, real):\n        if not fake.shape == real.shape:\n            _,_,H,W = real.shape\n            fake = F.upsample(fake, size=(H,W), mode=\'bilinear\')\n        loss = torch.sqrt( torch.mean( torch.abs(10.*real-10.*fake) ** 2 ) )\n        return loss\n\nclass GradLoss(nn.Module):\n    def __init__(self):\n        super(GradLoss, self).__init__()\n    \n    # L1 norm\n    def forward(self, grad_fake, grad_real):\n        \n        return torch.sum( torch.mean( torch.abs(grad_real-grad_fake) ) )\n\n    \nclass NormalLoss(nn.Module):\n    def __init__(self):\n        super(NormalLoss, self).__init__()\n    \n    def forward(self, grad_fake, grad_real):\n        prod = ( grad_fake[:,:,None,:] @ grad_real[:,:,:,None] ).squeeze(-1).squeeze(-1)\n        fake_norm = torch.sqrt( torch.sum( grad_fake**2, dim=-1 ) )\n        real_norm = torch.sqrt( torch.sum( grad_real**2, dim=-1 ) )\n        \n        return 1 - torch.mean( prod/(fake_norm*real_norm) )\n            \n# def get_acc(output, target):\n#     # takes in two tensors to compute accuracy\n#     pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n#     correct = pred.eq(target.data.view_as(pred)).cpu().sum()\n#     print(""Target: "", Counter(target.data.cpu().numpy()))\n#     print(""Pred: "", Counter(pred.cpu().numpy().flatten().tolist()))\n#     return float(correct)*100 / target.size(0) \n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Single image depth estimation\')\n    parser.add_argument(\'--dataset\', dest=\'dataset\',\n                      help=\'training dataset\',\n                      default=\'nyuv2\', type=str)\n    parser.add_argument(\'--epochs\', dest=\'max_epochs\',\n                      help=\'number of epochs to train\',\n                      default=NUM_EPOCHS, type=int)\n    parser.add_argument(\'--cuda\', dest=\'cuda\',\n                      help=\'whether use CUDA\',\n                      action=\'store_true\')\n    parser.add_argument(\'--bs\', dest=\'bs\',\n                      help=\'batch_size\',\n                      default=16, type=int)\n    parser.add_argument(\'--num_workers\', dest=\'num_workers\',\n                      help=\'num_workers\',\n                      default=1, type=int)\n    parser.add_argument(\'--disp_interval\', dest=\'disp_interval\',\n                      help=\'display interval\',\n                      default=10, type=int)\n    parser.add_argument(\'--output_dir\', dest=\'output_dir\',\n                      help=\'output directory\',\n                      default=\'saved_models\', type=str)\n\n# config optimization\n    parser.add_argument(\'--o\', dest=\'optimizer\',\n                      help=\'training optimizer\',\n                      default=""sgd"", type=str)\n    parser.add_argument(\'--lr\', dest=\'lr\',\n                      help=\'starting learning rate\',\n                      default=1e-3, type=float)\n    parser.add_argument(\'--lr_decay_step\', dest=\'lr_decay_step\',\n                      help=\'step to do learning rate decay, unit is epoch\',\n                      default=5, type=int)\n    parser.add_argument(\'--lr_decay_gamma\', dest=\'lr_decay_gamma\',\n                      help=\'learning rate decay ratio\',\n                      default=0.1, type=float)\n\n# set training session\n    parser.add_argument(\'--s\', dest=\'session\',\n                      help=\'training session\',\n                      default=1, type=int)\n    parser.add_argument(\'--eval_epoch\', dest=\'eval_epoch\',\n                      help=\'number of epoch to evaluate\',\n                      default=2, type=int)\n\n# resume trained model\n    parser.add_argument(\'--r\', dest=\'resume\',\n                      help=\'resume checkpoint or not\',\n                      default=False, type=bool)\n    parser.add_argument(\'--start_at\', dest=\'start_epoch\',\n                      help=\'epoch to start with\',\n                      default=0, type=int)\n    parser.add_argument(\'--checksession\', dest=\'checksession\',\n                      help=\'checksession to load model\',\n                      default=1, type=int)\n    parser.add_argument(\'--checkepoch\', dest=\'checkepoch\',\n                      help=\'checkepoch to load model\',\n                      default=1, type=int)\n    parser.add_argument(\'--checkpoint\', dest=\'checkpoint\',\n                      help=\'checkpoint to load model\',\n                      default=0, type=int)\n\n# training parameters\n    parser.add_argument(\'--gamma_sup\', dest=\'gamma_sup\',\n                      help=\'factor of supervised loss\',\n                      default=1., type=float)\n    parser.add_argument(\'--gamma_unsup\', dest=\'gamma_unsup\',\n                      help=\'factor of unsupervised loss\',\n                      default=1., type=float)\n    parser.add_argument(\'--gamma_reg\', dest=\'gamma_reg\',\n                      help=\'factor of regularization loss\',\n                      default=10., type=float)\n\n    args = parser.parse_args()\n    return args\n\ndef get_coords(b, h, w):\n    i_range = Variable(torch.arange(0, h).view(1, h, 1).expand(b,1,h,w))  # [B, 1, H, W]\n    j_range = Variable(torch.arange(0, w).view(1, 1, w).expand(b,1,h,w))  # [B, 1, H, W]\n    coords = torch.cat((j_range, i_range), dim=1)\n    norm = Variable(torch.Tensor([w,h]).view(1,2,1,1))\n    coords = coords * 2. / norm - 1.\n    coords = coords.permute(0, 2, 3, 1)\n   \n    return coords\n        \ndef resize_tensor(img, coords):\n    return nn.functional.grid_sample(img, coords, mode=\'bilinear\', padding_mode=\'zeros\')\n\ndef imgrad(img):\n    img = torch.mean(img, 1, True)\n    fx = np.array([[1,0,-1],[2,0,-2],[1,0,-1]])\n    conv1 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)\n    weight = torch.from_numpy(fx).float().unsqueeze(0).unsqueeze(0)\n    if img.is_cuda:\n        weight = weight.cuda()\n    conv1.weight = nn.Parameter(weight)\n    grad_x = conv1(img)\n\n    fy = np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n    conv2 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)\n    weight = torch.from_numpy(fy).float().unsqueeze(0).unsqueeze(0)\n    if img.is_cuda:\n        weight = weight.cuda()\n    conv2.weight = nn.Parameter(weight)\n    grad_y = conv2(img)\n\n#     grad = torch.sqrt(torch.pow(grad_x,2) + torch.pow(grad_y,2))\n    \n    return grad_y, grad_x\n\ndef imgrad_yx(img):\n    N,C,_,_ = img.size()\n    grad_y, grad_x = imgrad(img)\n    return torch.cat((grad_y.view(N,C,-1), grad_x.view(N,C,-1)), dim=1)\n\ndef reg_scalor(grad_yx):\n    return torch.exp(-torch.abs(grad_yx)/255.)\n    \n    \nclass sampler(Sampler):\n  def __init__(self, train_size, batch_size):\n    self.num_data = train_size\n    self.num_per_batch = int(train_size / batch_size)\n    self.batch_size = batch_size\n    self.range = torch.arange(0,batch_size).view(1, batch_size).long()\n    self.leftover_flag = False\n    if train_size % batch_size:\n      self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n      self.leftover_flag = True\n\n  def __iter__(self):\n    rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size\n    self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range\n\n    self.rand_num_view = self.rand_num.view(-1)\n\n    if self.leftover_flag:\n      self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)\n\n    return iter(self.rand_num_view)\n\n  def __len__(self):\n    return self.num_data\n\ndef collate_fn(data):\n    imgs, depths = zip(*data)\n    B = len(imgs)\n    im_batch = torch.ones((B,3,376,1242))\n    d_batch = torch.ones((B,1,376,1242))\n    for ind in range(B):\n        im, depth = imgs[ind], depths[ind]\n        im_batch[ind, :, -im.shape[1]:, :im.shape[2]] = im\n        d_batch[ind, :, -depth.shape[1]:, :depth.shape[2]] = depth\n    return im_batch, d_batch\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n\n    if torch.cuda.is_available() and not args.cuda:\n        print(""WARNING: You might want to run with --cuda"")\n\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n        \n    # dataset\n    if args.dataset == \'kitti\':\n        train_dataset = DepthDataset(root=\'/disk2/depth_data/kitti/train\') # KittiDataset(train=True)\n        eval_dataset = DepthDataset(root=\'/disk2/depth_data/kitti/train\') # KittiDataset(train=False)\n#         train_dataset = DepthDataset(root=\'../data/kitti/train\') # KittiDataset(train=True)\n#         eval_dataset = DepthDataset(root=\'../data/kitti/train\') # KittiDataset(train=False)\n        train_size = len(train_dataset)\n        eval_size = len(eval_dataset)\n        print(train_size, eval_size)\n\n        train_batch_sampler = sampler(train_size, args.bs)\n        eval_batch_sampler = sampler(eval_size, args.bs)\n\n        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs,\n                                shuffle=True, collate_fn=collate_fn, num_workers=args.num_workers)\n        \n        eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=args.bs,\n                                shuffle=True, collate_fn=collate_fn, num_workers=args.num_workers)\n        \n    elif args.dataset == \'nyuv2\':\n        train_dataset = NYUv2Dataset()\n        train_size = len(train_dataset)\n        eval_dataset = NYUv2Dataset(train=False)\n        eval_size = len(eval_dataset)\n        print(train_size)\n\n        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs,\n                                shuffle=True, num_workers=args.num_workers)\n        eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=args.bs,\n                                shuffle=True, num_workers=args.num_workers)\n        \n    elif args.dataset == \'scannet\':\n        pass\n\n    # network initialization\n    print(\'Initializing model...\')\n    i2d = I2D(fixed_feature_weights=False)\n    if args.cuda:\n        i2d = i2d.cuda()\n        \n    print(\'Done!\')\n\n    # hyperparams\n    lr = args.lr\n    bs = args.bs\n    lr_decay_step = args.lr_decay_step\n    lr_decay_gamma = args.lr_decay_gamma\n\n    # params\n    params = []\n    for key, value in dict(i2d.named_parameters()).items():\n      if value.requires_grad:\n        if \'bias\' in key:\n          params += [{\'params\':[value],\'lr\':lr*(DOUBLE_BIAS + 1), \\\n                  \'weight_decay\': 4e-5 and WEIGHT_DECAY or 0}]\n        else:\n          params += [{\'params\':[value],\'lr\':lr, \'weight_decay\': 4e-5}]\n\n    # optimizer\n    if args.optimizer == ""adam"":\n        optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=4e-5)\n    elif args.optimizer == ""sgd"":\n        optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9)\n\n    rmse = RMSE()\n    depth_criterion = RMSE_log()\n    grad_criterion = GradLoss()\n    normal_criterion = NormalLoss()\n    eval_metric = RMSE_log()\n    \n    # resume\n    if args.resume:\n        load_name = os.path.join(args.output_dir,\n          \'i2d_1_{}.pth\'.format(args.checkepoch))\n        print(""loading checkpoint %s"" % (load_name))\n        state = i2d.state_dict()\n        checkpoint = torch.load(load_name)\n        args.start_epoch = checkpoint[\'epoch\']\n        checkpoint = {k: v for k, v in checkpoint[\'model\'].items() if k in state}\n        state.update(checkpoint)\n        i2d.load_state_dict(state)\n#         optimizer.load_state_dict(checkpoint[\'optimizer\'])\n#         lr = optimizer.param_groups[0][\'lr\']\n        if \'pooling_mode\' in checkpoint.keys():\n            POOLING_MODE = checkpoint[\'pooling_mode\']\n        print(""loaded checkpoint %s"" % (load_name))\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    # constants\n    iters_per_epoch = int(train_size / args.bs)\n    \n    grad_factor = 10.\n    normal_factor = 1.\n    \n    for epoch in range(args.start_epoch, args.max_epochs):\n        \n        # setting to train mode\n        i2d.train()\n        start = time.time()\n        if epoch % (args.lr_decay_step + 1) == 0:\n            adjust_learning_rate(optimizer, args.lr_decay_gamma)\n            lr *= args.lr_decay_gamma\n\n        img = Variable(torch.FloatTensor(1))\n        z = Variable(torch.FloatTensor(1))\n        if args.cuda:\n            img = img.cuda()\n            z = z.cuda()\n        \n        train_data_iter = iter(train_dataloader)\n        \n        for step in range(iters_per_epoch):\n            start = time.time()\n            data = train_data_iter.next()\n            \n            img.data.resize_(data[0].size()).copy_(data[0])\n            z.data.resize_(data[1].size()).copy_(data[1])\n\n            optimizer.zero_grad()\n            z_fake = i2d(img)\n            depth_loss = depth_criterion(z_fake, z)\n            \n            grad_real, grad_fake = imgrad_yx(z), imgrad_yx(z_fake)\n            grad_loss = grad_criterion(grad_fake, grad_real)     * grad_factor * (epoch>3)\n            normal_loss = normal_criterion(grad_fake, grad_real) * normal_factor * (epoch>7)\n            \n            loss = depth_loss + grad_loss + normal_loss\n            loss.backward()\n            optimizer.step()\n\n            end = time.time()\n\n            # info\n            if step % args.disp_interval == 0:\n\n                print(""[epoch %2d][iter %4d] loss: %.4f RMSElog: %.4f grad_loss: %.4f normal_loss: %.4f"" \\\n                                % (epoch, step, loss, depth_loss, grad_loss, normal_loss))\n#                 print(""[epoch %2d][iter %4d] loss: %.4f iRMSE: %.4f"" \\\n#                                 % (epoch, step, loss, metric))\n        # save model\n        save_name = os.path.join(args.output_dir, \'i2d_{}_{}.pth\'.format(args.session, epoch))\n\n        torch.save({\'epoch\': epoch+1,\n                    \'model\': i2d.state_dict(), \n#                     \'optimizer\': optimizer.state_dict(),\n                   },\n                   save_name)\n\n        print(\'save model: {}\'.format(save_name))\n        print(\'time elapsed: %fs\' % (end - start))\n            \n        if epoch % 1 == 0:\n            # setting to eval mode\n            i2d.eval()\n\n            img = Variable(torch.FloatTensor(1), volatile=True)\n            z = Variable(torch.FloatTensor(1), volatile=True)\n            if args.cuda:\n                img = img.cuda()\n                z = z.cuda()\n\n            print(\'evaluating...\')\n\n            eval_loss = 0\n            rmse_accum = 0\n            count = 0\n            eval_data_iter = iter(eval_dataloader)\n            for i, data in enumerate(eval_data_iter):\n                print(i,\'/\',len(eval_data_iter)-1)\n\n                img.data.resize_(data[0].size()).copy_(data[0])\n                z.data.resize_(data[1].size()).copy_(data[1])\n\n                z_fake = i2d(img)\n                depth_loss = float(img.size(0)) * rmse(z_fake, z)**2\n                eval_loss += depth_loss\n                rmse_accum += float(img.size(0)) * eval_metric(z_fake, z)**2\n                count += float(img.size(0))\n\n            print(""[epoch %2d] RMSE_log: %.4f RMSE: %.4f"" \\\n                            % (epoch, torch.sqrt(eval_loss/count), torch.sqrt(rmse_accum/count)))\n            with open(\'val.txt\', \'a\') as f:\n                f.write(""[epoch %2d] RMSE_log: %.4f RMSE: %.4f\\n"" \\\n                            % (epoch, torch.sqrt(eval_loss/count), torch.sqrt(rmse_accum/count)))\n\n       \n\n'"
model_fpn.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\nfrom torchvision.models.resnet import resnet101\n\ndef agg_node(in_planes, out_planes):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n    )\n\ndef smooth(in_planes, out_planes):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n    )\n\ndef predict(in_planes, out_planes):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1),\n        nn.Sigmoid(),\n    )\n\ndef upshuffle(in_planes, out_planes, upscale_factor):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes*upscale_factor**2, kernel_size=3, stride=1, padding=1),\n        nn.PixelShuffle(upscale_factor),\n        nn.ReLU()\n    )\n\nclass I2D(nn.Module):\n    def __init__(self, pretrained=True, fixed_feature_weights=False):\n        super(I2D, self).__init__()\n\n        resnet = resnet101(pretrained=pretrained)\n\n        # Freeze those weights\n        if fixed_feature_weights:\n            for p in resnet.parameters():\n                p.requires_grad = False\n\n        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n        self.layer1 = nn.Sequential(resnet.layer1)\n        self.layer2 = nn.Sequential(resnet.layer2)\n        self.layer3 = nn.Sequential(resnet.layer3)\n        self.layer4 = nn.Sequential(resnet.layer4)\n\n        # Top layer\n        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n\n        # Lateral layers\n        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0)\n\n        # Smooth layers\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        # Aggregate layers\n        self.agg1 = agg_node(256, 128)\n        self.agg2 = agg_node(256, 128)\n        self.agg3 = agg_node(256, 128)\n        self.agg4 = agg_node(256, 128)\n        \n        # Upshuffle layers\n        self.up1 = upshuffle(128,128,8)\n        self.up2 = upshuffle(128,128,4)\n        self.up3 = upshuffle(128,128,2)\n        \n        # Depth prediction\n        self.predict1 = smooth(512, 128)\n        self.predict2 = predict(128, 1)\n        \n    def _upsample_add(self, x, y):\n        '''Upsample and add two feature maps.\n        Args:\n          x: (Variable) top feature map to be upsampled.\n          y: (Variable) lateral feature map.\n        Returns:\n          (Variable) added feature map.\n        Note in PyTorch, when input size is odd, the upsampled feature map\n        with `F.upsample(..., scale_factor=2, mode='nearest')`\n        maybe not equal to the lateral feature map size.\n        e.g.\n        original input size: [N,_,15,15] ->\n        conv2d feature map size: [N,_,8,8] ->\n        upsampled feature map size: [N,_,16,16]\n        So we choose bilinear upsample which supports arbitrary output sizes.\n        '''\n        _,_,H,W = y.size()\n        return F.upsample(x, size=(H,W), mode='bilinear') + y\n\n    def forward(self, x):\n        _,_,H,W = x.size()\n        \n        # Bottom-up\n        c1 = self.layer0(x)\n        c2 = self.layer1(c1)\n        c3 = self.layer2(c2)\n        c4 = self.layer3(c3)\n        c5 = self.layer4(c4)\n\n        # Top-down\n        p5 = self.toplayer(c5)\n        p4 = self._upsample_add(p5, self.latlayer1(c4))\n        p4 = self.smooth1(p4)\n        p3 = self._upsample_add(p4, self.latlayer2(c3))\n        p3 = self.smooth2(p3)\n        p2 = self._upsample_add(p3, self.latlayer3(c2))\n        p2 = self.smooth3(p2)\n        \n        # Top-down predict and refine\n        d5, d4, d3, d2 = self.up1(self.agg1(p5)), self.up2(self.agg2(p4)), self.up3(self.agg3(p3)), self.agg4(p2)\n        _,_,H,W = d2.size()\n        vol = torch.cat( [ F.upsample(d, size=(H,W), mode='bilinear') for d in [d5,d4,d3,d2] ], dim=1 )\n        \n        # return self.predict2( self.up4(self.predict1(vol)) )\n        return self.predict2( self.predict1(vol) )     # img : depth = 4 : 1 """
dataset/__init__.py,0,b''
dataset/augment.py,2,"b'import torch.utils.data as data\nimport numpy as np\nfrom PIL import Image\nfrom scipy.misc import imread\nfrom path import Path\nfrom constants import *\nfrom torchvision.transforms import Resize, Compose, ToPILImage, ToTensor, RandomHorizontalFlip, CenterCrop, ColorJitter\nimport torch, time, os\nimport torch.nn.functional as F\nimport random\nimport scipy.ndimage as ndimage\nfrom scipy import misc\n\nclass RandomCrop(object):\n    """"""Crop randomly the image in a sample.\n\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    """"""\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w, :]\n\n        landmarks = landmarks[top: top + new_h,\n                      left: left + new_w]\n\n        return image, landmarks\n\n\nclass CropCenter(object):\n    """"""Crops the given inputs and target arrays at the center to have a region of\n    the given size. size can be a tuple (target_height, target_width)\n    or an integer, in which case the target will be of a square shape (size, size)\n    Careful, img1 and img2 may not be the same size\n    """"""\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, inputs, target):\n        h1, w1, _ = inputs.shape\n        th, tw = self.size\n        x1 = int(round((w1 - tw) / 2.))\n        y1 = int(round((h1 - th) / 2.))\n\n        inputs = inputs[y1 : y1 + th, x1 : x1 + tw]\n        target = target[y1 : y1 + th, x1 : x1 + tw]\n        return inputs,target\n    \nclass RandomCropRotate(object):\n    """"""Random rotation of the image from -angle to angle (in degrees)\n    A crop is done to keep same image ratio, and no black pixels\n    angle: max angle of the rotation, cannot be more than 180 degrees\n    interpolation order: Default: 2 (bilinear)\n    """"""\n    def __init__(self, angle, diff_angle=0, order=2):\n        self.angle = angle\n        self.order = order\n        self.diff_angle = diff_angle\n\n    def __call__(self, sample):\n        inputs,target = sample\n        h,w,_ = inputs.shape\n\n        applied_angle  = random.uniform(-self.angle,self.angle)\n        diff = random.uniform(-self.diff_angle,self.diff_angle)\n        angle1 = applied_angle - diff/2\n\n        angle1_rad = angle1*np.pi/180\n        \n        inputs = ndimage.interpolation.rotate(inputs, angle1, reshape=True, order=self.order)\n        target = ndimage.interpolation.rotate(target, angle1, reshape=True, order=self.order)\n        \n        #keep angle1 and angle2 within [0,pi/2] with a reflection at pi/2: -1rad is 1rad, 2rad is pi - 2 rad\n        angle1_rad = np.pi/2 - np.abs(angle1_rad%np.pi - np.pi/2)\n        \n        c1 = np.cos(angle1_rad)\n        s1 = np.sin(angle1_rad)\n        c_diag = h/np.sqrt(h*h+w*w)\n        s_diag = w/np.sqrt(h*h+w*w)\n\n        ratio = 1./(c1+w/float(h)*s1)\n\n        crop = CropCenter((int(h*ratio),int(w*ratio)))\n        return crop(inputs, target)\n    \nclass RandomHorizontalFlip(object):\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, sample):\n        image, landmarks = sample\n        if np.random.randn() > 0.5:\n            image = image[:,::-1,:]\n            landmarks = landmarks[:,::-1]\n\n        return image, landmarks\n    \nif __name__ == \'__main__\':\n    root = \'/disk2/data/nyuv2/\'\n    rgb_paths = [root+\'train_rgb/\'+d for d in os.listdir(root+\'train_rgb/\')]\n    augmentation = Compose([RandomCrop((420,560)), RandomCropRotate(10)])\n    counter = 3\n    while counter < 6:\n        counter += 1\n        c = 0\n        for path in rgb_paths:\n            c += 1\n            print(\'%d %d/%d\' %(counter, c, len(rgb_paths)))\n            img = Image.open(path)\n            depth = Image.open(path.replace(\'rgb\', \'depth\'))\n            rgb, depth = np.array(img), np.array(depth)[:,:,None]\n            rgb, depth = augmentation((rgb, depth))\n            depth = depth.squeeze(-1)\n            rgb, depth = Image.fromarray(rgb), Image.fromarray(depth)\n            rgb, depth = Resize((420,560))(rgb), Resize((420,560))(depth)\n            rgb = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)(rgb)\n            \n            rgb.save(root+\'aug_rgb/\'+path.split(\'/\')[-1].split(\'.\')[0]+str(counter)+\'.png\')\n            \n            depth.save(root+\'aug_depth/\'+path.replace(\'rgb\', \'depth\').split(\'/\')[-1].split(\'.\')[0]+str(counter)+\'.png\')\n            '"
dataset/constants.py,0,"b'TRAIN_IMG_SIZE = (187,621)\n'"
dataset/dataloader.py,2,"b""import torch.utils.data as data\nimport numpy as np\nfrom PIL import Image\nfrom scipy.misc import imread\n# from path import Path\nimport os\nfrom glob import glob\nfrom constants import *\nfrom torchvision.transforms import Resize, Compose, ToPILImage, ToTensor #, RandomHorizontalFlip\nimport torch, time\nimport torch.nn.functional as F\n\nclass KittiDataset(data.Dataset):\n    def __init__(self, train=True):\n        root = '/disk2/data/eigen'\n#         root = '/media/xander/Essence/kitti'\n        if train:\n            self.root = os.path.join(root, 'train')\n        else:\n#             self.root = os.path.join(root, 'val')\n            self.root = os.path.join(root, 'test')\n        self.path_temp = os.path.join(self.root, '%s', '%s.%s')\n        self.imtype = 'png'\n        if train:\n            self.length = len(glob(self.path_temp%('depth_out', '*', self.imtype)))\n            self.files = glob(self.path_temp%('depth_out', '*', self.imtype))\n        else:\n            self.length = len(glob(self.path_temp%('depth_out', '*', self.imtype)))\n            self.files = glob(self.path_temp%('depth_out', '*', self.imtype))\n        self.train = train\n        self.rgb_transform = Compose([Resize([192,640]), ToTensor()])\n        self.depth_transform = Compose([Resize([48,160]), ToTensor()])\n        \n    def __getitem__(self, index):\n        \n        if self.train:\n            depth = Image.open( self.files[index] )\n            img = Image.open( self.files[index].replace('depth_out', 'img') )\n            rgb, d = self.rgb_transform(img), self.depth_transform(depth)\n        else:\n            depth = Image.open( self.files[index] )\n            img = Image.open( self.files[index].replace('depth_out', 'img') )\n            rgb, d = self.rgb_transform(img), self.depth_transform(depth)\n        rgb, d = rgb[:,64:,:], d[:,16:,:]\n        \n        return rgb, d.float()/65536.\n\n    def __len__(self):\n#         return 16 # for debug purpose\n        return self.length\n\nclass ScannetDataset(data.Dataset):\n    def __init__(self, train=True):\n        if train:\n            self.root = os.path.join('/disk2/data/scannet', 'train')\n        else:\n            self.root = os.path.join('/disk2/data/scannet', 'val')\n        self.path_temp = os.path.join(self.root, '%s', '%s.%s')\n        self.imtype = 'jpg'\n        self.length = len(glob(self.path_temp%('img', '*', self.imtype)))\n        self.train = train\n        self.rgb_transform = Compose([Resize([484, 648]), ToTensor()])\n        self.depth_transform = Compose([Resize([121, 162]), ToTensor()])\n        \n    def __getitem__(self, index):\n        \n        img = Image.open( self.path_temp%('img',str(index).zfill(5),self.imtype) )\n  \n        if self.train:\n            depth = Image.open( self.path_temp%('depth_out',str(index).zfill(5),'png') )\n            img, depth = self.rgb_transform(img), self.depth_transform(depth)\n        else:\n            depth = Image.open( self.path_temp%('depth',str(index).zfill(5),'png') )\n            img, depth = ToTensor()(img), ToTensor()(depth)\n\n        return img, depth.float()/65536.\n\n    def __len__(self):\n#         return 16 # for debug purpose\n        return self.length\n\n\nkitti_train = KittiDataset()\nkitti_val = KittiDataset(train=False)\nscannet_train = ScannetDataset()\nscannet_val = ScannetDataset(train=False)\n\nif __name__ == '__main__':\n    # Testing\n    for item in kitti_train[0]:\n        print(item.size())\n        \n    for item in scannet_train[0]:\n        print(item.size())\n"""
dataset/kitti_dataset.py,3,"b""import torch.utils.data as data\nimport numpy as np\nfrom PIL import Image\nfrom scipy.misc import imread\nfrom path import Path\nfrom constants import *\nfrom torchvision.transforms import Resize, Compose, ToPILImage, ToTensor, RandomHorizontalFlip\nimport torch, time\nimport torch.nn.functional as F\n\ndef load_depth(filename):\n    depth_png = np.asarray(Image.open(filename))\n\n    assert(np.max(depth_png) > 255)\n\n    depth = depth_png.astype(np.float32) / (256. * 256.)\n    \n    depth[depth == 0] = 1.\n    \n    return depth\n\nclass KittiDataset(data.Dataset):\n    def __init__(self, root='/disk2/data/', seed=None, train=True):\n        \n        np.random.seed(seed)\n        self.root = Path(root)\n        img_dir = self.root/'kitti_train_images.txt' if train else self.root/'kitti_val_images.txt'\n        depth_dir = self.root/'kitti_train_depth_maps.txt' if train else self.root/'kitti_val_depth_maps.txt'\n        # intr_dir = self.root/'kitti_train_intrinsics.txt' if train else self.root/'kitti_val_intrinsics.txt'\n        self.img_l_paths = [d[:-1] for d in open(img_dir) if 'image_02' in d]\n        self.depth_l_paths = [d[:-1] for d in open(depth_dir) if 'image_02' in d]\n        \n        # at least 20 frames between 2 examples\n        del_idxs = []\n        cur = 0\n        for i in range(1,len(self.img_l_paths)):\n            idx = int(self.img_l_paths[i][-7:-4])\n            cur_idx = int(self.img_l_paths[cur][-7:-4])\n            if abs(idx-cur_idx) < 3:\n                del_idxs += [i]\n            else:\n                cur = i\n        self.img_l_paths = np.delete(self.img_l_paths, del_idxs)\n        self.depth_l_paths = np.delete(self.depth_l_paths, del_idxs)\n\n        self.length = len(self.img_l_paths)\n            \n    def __getitem__(self, index):\n        depth = torch.FloatTensor( load_depth(self.depth_l_paths[index])[None,:,:] )\n        img = ToTensor()( Image.open(self.img_l_paths[index]) )\n        \n        tpad = 376 - img.size(1) \n        rpad = 1242 - img.size(2)\n        \n        # (padLeft, padRight, padTop, padBottom)\n        img = F.pad(img.unsqueeze(0), pad=(0, rpad, tpad, 0), mode='reflect')\n        depth = F.pad(depth.unsqueeze(0), pad=(0, rpad, tpad, 0), mode='constant', value=1.)\n        return img.data.squeeze(0), depth.data.squeeze(0)\n\n    def __len__(self):\n#         return 16 # for debug purpose\n        return self.length\n\nif __name__ == '__main__':\n    # Testing\n    dataset = KittiDataset()\n    print(len(dataset))\n    for item in dataset[0]:\n        print(item.size())\n"""
dataset/nyuv2_dataset.py,2,"b'import torch.utils.data as data\nimport numpy as np\nfrom PIL import Image\nfrom scipy.misc import imread\nfrom path import Path\nfrom constants import *\nfrom torchvision.transforms import Resize, Compose, ToPILImage, ToTensor, RandomHorizontalFlip, CenterCrop, ColorJitter\nimport torch, time, os\nimport torch.nn.functional as F\nimport random\nimport scipy.ndimage as ndimage\nfrom scipy import misc\n\nclass RandomCrop(object):\n    """"""Crop randomly the image in a sample.\n\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    """"""\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w, :]\n\n        landmarks = landmarks[top: top + new_h,\n                      left: left + new_w]\n\n        return image, landmarks\n\n\nclass CropCenter(object):\n    """"""Crops the given inputs and target arrays at the center to have a region of\n    the given size. size can be a tuple (target_height, target_width)\n    or an integer, in which case the target will be of a square shape (size, size)\n    Careful, img1 and img2 may not be the same size\n    """"""\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, inputs, target):\n        h1, w1, _ = inputs.shape\n        th, tw = self.size\n        x1 = int(round((w1 - tw) / 2.))\n        y1 = int(round((h1 - th) / 2.))\n\n        inputs = inputs[y1 : y1 + th, x1 : x1 + tw]\n        target = target[y1 : y1 + th, x1 : x1 + tw]\n        return inputs,target\n    \nclass RandomCropRotate(object):\n    """"""Random rotation of the image from -angle to angle (in degrees)\n    A crop is done to keep same image ratio, and no black pixels\n    angle: max angle of the rotation, cannot be more than 180 degrees\n    interpolation order: Default: 2 (bilinear)\n    """"""\n    def __init__(self, angle, diff_angle=0, order=2):\n        self.angle = angle\n        self.order = order\n        self.diff_angle = diff_angle\n\n    def __call__(self, sample):\n        inputs,target = sample\n        h,w,_ = inputs.shape\n\n        applied_angle  = random.uniform(-self.angle,self.angle)\n        diff = random.uniform(-self.diff_angle,self.diff_angle)\n        angle1 = applied_angle - diff/2\n\n        angle1_rad = angle1*np.pi/180\n        \n        inputs = ndimage.interpolation.rotate(inputs, angle1, reshape=True, order=self.order)\n        target = ndimage.interpolation.rotate(target, angle1, reshape=True, order=self.order)\n        \n        #keep angle1 and angle2 within [0,pi/2] with a reflection at pi/2: -1rad is 1rad, 2rad is pi - 2 rad\n        angle1_rad = np.pi/2 - np.abs(angle1_rad%np.pi - np.pi/2)\n        \n        c1 = np.cos(angle1_rad)\n        s1 = np.sin(angle1_rad)\n        c_diag = h/np.sqrt(h*h+w*w)\n        s_diag = w/np.sqrt(h*h+w*w)\n\n        ratio = 1./(c1+w/float(h)*s1)\n\n        crop = CropCenter((int(h*ratio),int(w*ratio)))\n        return crop(inputs, target)\n    \nclass RandomHorizontalFlip(object):\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, sample):\n        image, landmarks = sample\n        if np.random.randn() > 0.5:\n            image = image[:,::-1,:]\n            landmarks = landmarks[:,::-1]\n\n        return image, landmarks\n    \nclass NYUv2Dataset(data.Dataset):\n    def __init__(self, root=\'/disk2/data/nyuv2/\', seed=None, train=True):\n        \n        np.random.seed(seed)\n        self.root = Path(root)\n        self.train = train\n        if train:\n            self.rgb_paths = [root+\'train_rgb/\'+d for d in os.listdir(root+\'train_rgb/\')]\n            # Randomly choose 50k images without replacement\n            self.rgb_paths = np.random.choice(self.rgb_paths, 50000, False)\n        else:\n            self.rgb_paths = [root+\'test_rgb/\'+d for d in os.listdir(root+\'test_rgb/\')]\n        \n        if train!=train: # vis\n            self.train = True\n            self.rgb_paths = [root+\'vis_rgb/\'+d for d in os.listdir(root+\'vis_rgb/\')]\n        \n        self.augmentation = Compose([RandomHorizontalFlip()]) # , RandomCropRotate(10)\n        self.rgb_transform = Compose([ToPILImage(), Resize((480,640)), ToTensor()]) # ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), \n        self.depth_transform = Compose([ToPILImage(), Resize((120,160)), ToTensor()])\n        \n        if self.train:\n            self.length = 50000 # len(self.rgb_paths)\n        else:\n            self.length = len(self.rgb_paths)\n            \n    def __getitem__(self, index):\n        path = self.rgb_paths[index]\n        rgb = Image.open(path)\n        depth = Image.open(path.replace(\'rgb\', \'depth\'))\n        \n        if self.train:\n            rgb, depth = np.array(rgb), np.array(depth)[:,:,None]\n            rgb, depth = self.augmentation((rgb, depth))\n            return self.rgb_transform(rgb), self.depth_transform(depth).squeeze(-1)\n        \n        return Compose([Resize((240,320)), ToTensor()])(rgb), ToTensor()(depth)\n        \n        return rgb, depth\n\n    def __len__(self):\n        return self.length\n\nif __name__ == \'__main__\':\n    # Testing\n    dataset = NYUv2Dataset()\n    print(len(dataset))\n    for item in dataset[0]:\n        print(item.size())\n'"
