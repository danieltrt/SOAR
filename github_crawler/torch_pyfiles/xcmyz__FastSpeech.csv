file_path,api_count,code
dataset.py,5,"b'import torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\nimport math\nimport os\n\nimport hparams\nimport audio as Audio\nfrom text import text_to_sequence\nfrom utils import process_text, pad_1D, pad_2D\n\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n\nclass FastSpeechDataset(Dataset):\n    """""" LJSpeech """"""\n\n    def __init__(self):\n        self.text = process_text(os.path.join(""data"", ""train.txt""))\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        mel_gt_name = os.path.join(\n            hparams.mel_ground_truth, ""ljspeech-mel-%05d.npy"" % (idx+1))\n        mel_gt_target = np.load(mel_gt_name)\n        D = np.load(os.path.join(hparams.alignment_path, str(idx)+"".npy""))\n\n        character = self.text[idx][0:len(self.text[idx])-1]\n        character = np.array(text_to_sequence(\n            character, hparams.text_cleaners))\n\n        sample = {""text"": character,\n                  ""mel_target"": mel_gt_target,\n                  ""D"": D}\n\n        return sample\n\n\ndef reprocess(batch, cut_list):\n    texts = [batch[ind][""text""] for ind in cut_list]\n    mel_targets = [batch[ind][""mel_target""] for ind in cut_list]\n    Ds = [batch[ind][""D""] for ind in cut_list]\n\n    length_text = np.array([])\n    for text in texts:\n        length_text = np.append(length_text, text.shape[0])\n\n    src_pos = list()\n    max_len = int(max(length_text))\n    for length_src_row in length_text:\n        src_pos.append(np.pad([i+1 for i in range(int(length_src_row))],\n                              (0, max_len-int(length_src_row)), \'constant\'))\n    src_pos = np.array(src_pos)\n\n    length_mel = np.array(list())\n    for mel in mel_targets:\n        length_mel = np.append(length_mel, mel.shape[0])\n\n    mel_pos = list()\n    max_mel_len = int(max(length_mel))\n    for length_mel_row in length_mel:\n        mel_pos.append(np.pad([i+1 for i in range(int(length_mel_row))],\n                              (0, max_mel_len-int(length_mel_row)), \'constant\'))\n    mel_pos = np.array(mel_pos)\n\n    texts = pad_1D(texts)\n    Ds = pad_1D(Ds)\n    mel_targets = pad_2D(mel_targets)\n\n    out = {""text"": texts,\n           ""mel_target"": mel_targets,\n           ""D"": Ds,\n           ""mel_pos"": mel_pos,\n           ""src_pos"": src_pos,\n           ""mel_max_len"": max_mel_len}\n\n    return out\n\n\ndef collate_fn(batch):\n    len_arr = np.array([d[""text""].shape[0] for d in batch])\n    index_arr = np.argsort(-len_arr)\n    batchsize = len(batch)\n    real_batchsize = int(math.sqrt(batchsize))\n\n    cut_list = list()\n    for i in range(real_batchsize):\n        cut_list.append(index_arr[i*real_batchsize:(i+1)*real_batchsize])\n\n    output = list()\n    for i in range(real_batchsize):\n        output.append(reprocess(batch, cut_list[i]))\n\n    return output\n\n\nif __name__ == ""__main__"":\n    # Test\n    dataset = FastSpeechDataset()\n    training_loader = DataLoader(dataset,\n                                 batch_size=1,\n                                 shuffle=False,\n                                 collate_fn=collate_fn,\n                                 drop_last=True,\n                                 num_workers=0)\n    total_step = hparams.epochs * len(training_loader) * hparams.batch_size\n\n    cnt = 0\n    for i, batchs in enumerate(training_loader):\n        for j, data_of_batch in enumerate(batchs):\n            mel_target = torch.from_numpy(\n                data_of_batch[""mel_target""]).float().to(device)\n            D = torch.from_numpy(data_of_batch[""D""]).int().to(device)\n            # print(mel_target.size())\n            # print(D.sum())\n            print(cnt)\n            if mel_target.size(1) == D.sum().item():\n                cnt += 1\n\n    print(cnt)\n'"
fastspeech.py,2,"b'import torch\r\nimport torch.nn as nn\r\n\r\nfrom transformer.Models import Encoder, Decoder\r\nfrom transformer.Layers import Linear, PostNet\r\nfrom modules import LengthRegulator\r\nimport hparams as hp\r\n\r\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\r\n\r\n\r\nclass FastSpeech(nn.Module):\r\n    """""" FastSpeech """"""\r\n\r\n    def __init__(self):\r\n        super(FastSpeech, self).__init__()\r\n\r\n        self.encoder = Encoder()\r\n        self.length_regulator = LengthRegulator()\r\n        self.decoder = Decoder()\r\n\r\n        self.mel_linear = Linear(hp.decoder_output_size, hp.num_mels)\r\n        self.postnet = PostNet()\r\n\r\n    def forward(self, src_seq, src_pos, mel_pos=None, mel_max_length=None, length_target=None, alpha=1.0):\r\n        encoder_output, _ = self.encoder(src_seq, src_pos)\r\n\r\n        if self.training:\r\n            length_regulator_output, duration_predictor_output = self.length_regulator(encoder_output,\r\n                                                                                       target=length_target,\r\n                                                                                       alpha=alpha,\r\n                                                                                       mel_max_length=mel_max_length)\r\n            decoder_output = self.decoder(length_regulator_output, mel_pos)\r\n\r\n            mel_output = self.mel_linear(decoder_output)\r\n            mel_output_postnet = self.postnet(mel_output) + mel_output\r\n\r\n            return mel_output, mel_output_postnet, duration_predictor_output\r\n        else:\r\n            length_regulator_output, decoder_pos = self.length_regulator(encoder_output,\r\n                                                                         alpha=alpha)\r\n\r\n            decoder_output = self.decoder(length_regulator_output, decoder_pos)\r\n\r\n            mel_output = self.mel_linear(decoder_output)\r\n            mel_output_postnet = self.postnet(mel_output) + mel_output\r\n\r\n            return mel_output, mel_output_postnet\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    # Test\r\n    model = FastSpeech()\r\n    print(sum(param.numel() for param in model.parameters()))\r\n'"
glow.py,49,"b'# *****************************************************************************\n#  Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions are met:\n#      * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n#      * Redistributions in binary form must reproduce the above copyright\n#        notice, this list of conditions and the following disclaimer in the\n#        documentation and/or other materials provided with the distribution.\n#      * Neither the name of the NVIDIA CORPORATION nor the\n#        names of its contributors may be used to endorse or promote products\n#        derived from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND\n#  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n#  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n#  DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n# *****************************************************************************\nimport copy\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a+input_b\n    t_act = torch.nn.functional.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.nn.functional.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act\n    return acts\n\n\nclass WaveGlowLoss(torch.nn.Module):\n    def __init__(self, sigma=1.0):\n        super(WaveGlowLoss, self).__init__()\n        self.sigma = sigma\n\n    def forward(self, model_output):\n        z, log_s_list, log_det_W_list = model_output\n        for i, log_s in enumerate(log_s_list):\n            if i == 0:\n                log_s_total = torch.sum(log_s)\n                log_det_W_total = log_det_W_list[i]\n            else:\n                log_s_total = log_s_total + torch.sum(log_s)\n                log_det_W_total += log_det_W_list[i]\n\n        loss = torch.sum(z*z)/(2*self.sigma*self.sigma) - \\\n            log_s_total - log_det_W_total\n        return loss/(z.size(0)*z.size(1)*z.size(2))\n\n\nclass Invertible1x1Conv(torch.nn.Module):\n    """"""\n    The layer outputs both the convolution, and the log determinant\n    of its weight matrix.  If reverse=True it does convolution with\n    inverse\n    """"""\n\n    def __init__(self, c):\n        super(Invertible1x1Conv, self).__init__()\n        self.conv = torch.nn.Conv1d(c, c, kernel_size=1, stride=1, padding=0,\n                                    bias=False)\n\n        # Sample a random orthonormal matrix to initialize weights\n        W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n\n        # Ensure determinant is 1.0 not -1.0\n        if torch.det(W) < 0:\n            W[:, 0] = -1*W[:, 0]\n        W = W.view(c, c, 1)\n        self.conv.weight.data = W\n\n    def forward(self, z, reverse=False):\n        # shape\n        batch_size, group_size, n_of_groups = z.size()\n\n        W = self.conv.weight.squeeze()\n\n        if reverse:\n            if not hasattr(self, \'W_inverse\'):\n                # Reverse computation\n                W_inverse = W.inverse()\n                W_inverse = Variable(W_inverse[..., None])\n                if z.type() == \'torch.cuda.HalfTensor\':\n                    W_inverse = W_inverse.half()\n                self.W_inverse = W_inverse\n            z = F.conv1d(z, self.W_inverse, bias=None, stride=1, padding=0)\n            return z\n        else:\n            # Forward computation\n            log_det_W = batch_size * n_of_groups * torch.logdet(W)\n            z = self.conv(z)\n            return z, log_det_W\n\n\nclass WN(torch.nn.Module):\n    """"""\n    This is the WaveNet like layer for the affine coupling.  The primary difference\n    from WaveNet is the convolutions need not be causal.  There is also no dilation\n    size reset.  The dilation only doubles on each layer\n    """"""\n\n    def __init__(self, n_in_channels, n_mel_channels, n_layers, n_channels,\n                 kernel_size):\n        super(WN, self).__init__()\n        assert(kernel_size % 2 == 1)\n        assert(n_channels % 2 == 0)\n        self.n_layers = n_layers\n        self.n_channels = n_channels\n        self.in_layers = torch.nn.ModuleList()\n        self.res_skip_layers = torch.nn.ModuleList()\n        self.cond_layers = torch.nn.ModuleList()\n\n        start = torch.nn.Conv1d(n_in_channels, n_channels, 1)\n        start = torch.nn.utils.weight_norm(start, name=\'weight\')\n        self.start = start\n\n        # Initializing last layer to 0 makes the affine coupling layers\n        # do nothing at first.  This helps with training stability\n        end = torch.nn.Conv1d(n_channels, 2*n_in_channels, 1)\n        end.weight.data.zero_()\n        end.bias.data.zero_()\n        self.end = end\n\n        for i in range(n_layers):\n            dilation = 2 ** i\n            padding = int((kernel_size*dilation - dilation)/2)\n            in_layer = torch.nn.Conv1d(n_channels, 2*n_channels, kernel_size,\n                                       dilation=dilation, padding=padding)\n            in_layer = torch.nn.utils.weight_norm(in_layer, name=\'weight\')\n            self.in_layers.append(in_layer)\n\n            cond_layer = torch.nn.Conv1d(n_mel_channels, 2*n_channels, 1)\n            cond_layer = torch.nn.utils.weight_norm(cond_layer, name=\'weight\')\n            self.cond_layers.append(cond_layer)\n\n            # last one is not necessary\n            if i < n_layers - 1:\n                res_skip_channels = 2*n_channels\n            else:\n                res_skip_channels = n_channels\n            res_skip_layer = torch.nn.Conv1d(n_channels, res_skip_channels, 1)\n            res_skip_layer = torch.nn.utils.weight_norm(\n                res_skip_layer, name=\'weight\')\n            self.res_skip_layers.append(res_skip_layer)\n\n    def forward(self, forward_input):\n        audio, spect = forward_input\n        audio = self.start(audio)\n\n        for i in range(self.n_layers):\n            acts = fused_add_tanh_sigmoid_multiply(\n                self.in_layers[i](audio),\n                self.cond_layers[i](spect),\n                torch.IntTensor([self.n_channels]))\n\n            res_skip_acts = self.res_skip_layers[i](acts)\n            if i < self.n_layers - 1:\n                audio = res_skip_acts[:, :self.n_channels, :] + audio\n                skip_acts = res_skip_acts[:, self.n_channels:, :]\n            else:\n                skip_acts = res_skip_acts\n\n            if i == 0:\n                output = skip_acts\n            else:\n                output = skip_acts + output\n        return self.end(output)\n\n\nclass WaveGlow(torch.nn.Module):\n    def __init__(self, n_mel_channels, n_flows, n_group, n_early_every,\n                 n_early_size, WN_config):\n        super(WaveGlow, self).__init__()\n\n        self.upsample = torch.nn.ConvTranspose1d(n_mel_channels,\n                                                 n_mel_channels,\n                                                 1024, stride=256)\n        assert(n_group % 2 == 0)\n        self.n_flows = n_flows\n        self.n_group = n_group\n        self.n_early_every = n_early_every\n        self.n_early_size = n_early_size\n        self.WN = torch.nn.ModuleList()\n        self.convinv = torch.nn.ModuleList()\n\n        n_half = int(n_group/2)\n\n        # Set up layers with the right sizes based on how many dimensions\n        # have been output already\n        n_remaining_channels = n_group\n        for k in range(n_flows):\n            if k % self.n_early_every == 0 and k > 0:\n                n_half = n_half - int(self.n_early_size/2)\n                n_remaining_channels = n_remaining_channels - self.n_early_size\n            self.convinv.append(Invertible1x1Conv(n_remaining_channels))\n            self.WN.append(WN(n_half, n_mel_channels*n_group, **WN_config))\n        self.n_remaining_channels = n_remaining_channels  # Useful during inference\n\n    def forward(self, forward_input):\n        """"""\n        forward_input[0] = mel_spectrogram:  batch x n_mel_channels x frames\n        forward_input[1] = audio: batch x time\n        """"""\n        spect, audio = forward_input\n\n        #  Upsample spectrogram to size of audio\n        spect = self.upsample(spect)\n        assert(spect.size(2) >= audio.size(1))\n        if spect.size(2) > audio.size(1):\n            spect = spect[:, :, :audio.size(1)]\n\n        spect = spect.unfold(2, self.n_group, self.n_group).permute(0, 2, 1, 3)\n        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0, 2, 1)\n\n        audio = audio.unfold(1, self.n_group, self.n_group).permute(0, 2, 1)\n        output_audio = []\n        log_s_list = []\n        log_det_W_list = []\n\n        for k in range(self.n_flows):\n            if k % self.n_early_every == 0 and k > 0:\n                output_audio.append(audio[:, :self.n_early_size, :])\n                audio = audio[:, self.n_early_size:, :]\n\n            audio, log_det_W = self.convinv[k](audio)\n            log_det_W_list.append(log_det_W)\n\n            n_half = int(audio.size(1)/2)\n            audio_0 = audio[:, :n_half, :]\n            audio_1 = audio[:, n_half:, :]\n\n            output = self.WN[k]((audio_0, spect))\n            log_s = output[:, n_half:, :]\n            b = output[:, :n_half, :]\n            audio_1 = torch.exp(log_s)*audio_1 + b\n            log_s_list.append(log_s)\n\n            audio = torch.cat([audio_0, audio_1], 1)\n\n        output_audio.append(audio)\n        return torch.cat(output_audio, 1), log_s_list, log_det_W_list\n\n    def infer(self, spect, sigma=1.0):\n        spect = self.upsample(spect)\n        # trim conv artifacts. maybe pad spec to kernel multiple\n        time_cutoff = self.upsample.kernel_size[0] - self.upsample.stride[0]\n        spect = spect[:, :, :-time_cutoff]\n\n        spect = spect.unfold(2, self.n_group, self.n_group).permute(0, 2, 1, 3)\n        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0, 2, 1)\n\n        if spect.type() == \'torch.cuda.HalfTensor\':\n            audio = torch.cuda.HalfTensor(spect.size(0),\n                                          self.n_remaining_channels,\n                                          spect.size(2)).normal_()\n        else:\n            audio = torch.cuda.FloatTensor(spect.size(0),\n                                           self.n_remaining_channels,\n                                           spect.size(2)).normal_()\n\n        audio = torch.autograd.Variable(sigma*audio)\n\n        for k in reversed(range(self.n_flows)):\n            n_half = int(audio.size(1)/2)\n            audio_0 = audio[:, :n_half, :]\n            audio_1 = audio[:, n_half:, :]\n\n            output = self.WN[k]((audio_0, spect))\n            s = output[:, n_half:, :]\n            b = output[:, :n_half, :]\n            audio_1 = (audio_1 - b)/torch.exp(s)\n            audio = torch.cat([audio_0, audio_1], 1)\n\n            audio = self.convinv[k](audio, reverse=True)\n\n            if k % self.n_early_every == 0 and k > 0:\n                if spect.type() == \'torch.cuda.HalfTensor\':\n                    z = torch.cuda.HalfTensor(spect.size(\n                        0), self.n_early_size, spect.size(2)).normal_()\n                else:\n                    z = torch.cuda.FloatTensor(spect.size(\n                        0), self.n_early_size, spect.size(2)).normal_()\n                audio = torch.cat((sigma*z, audio), 1)\n\n        audio = audio.permute(0, 2, 1).contiguous().view(\n            audio.size(0), -1).data\n        return audio\n\n    @staticmethod\n    def remove_weightnorm(model):\n        waveglow = model\n        for WN in waveglow.WN:\n            WN.start = torch.nn.utils.remove_weight_norm(WN.start)\n            WN.in_layers = remove(WN.in_layers)\n            WN.cond_layers = remove(WN.cond_layers)\n            WN.res_skip_layers = remove(WN.res_skip_layers)\n        return waveglow\n\n\ndef remove(conv_list):\n    new_conv_list = torch.nn.ModuleList()\n    for old_conv in conv_list:\n        old_conv = torch.nn.utils.remove_weight_norm(old_conv)\n        new_conv_list.append(old_conv)\n    return new_conv_list\n'"
hparams.py,0,"b'from text import symbols\n\n# Text\ntext_cleaners = [\'english_cleaners\']\n\n# Mel\nn_mel_channels = 80\nnum_mels = 80\n\n# FastSpeech\nvocab_size = 1024\nN = 6\nHead = 2\nd_model = 384\nduration_predictor_filter_size = 256\nduration_predictor_kernel_size = 3\ndropout = 0.1\n\nword_vec_dim = 384\nencoder_n_layer = 6\nencoder_head = 2\nencoder_conv1d_filter_size = 1536\nmax_sep_len = 2048\nencoder_output_size = 384\ndecoder_n_layer = 6\ndecoder_head = 2\ndecoder_conv1d_filter_size = 1536\ndecoder_output_size = 384\nfft_conv1d_kernel = 3\nfft_conv1d_padding = 1\nduration_predictor_filter_size = 256\nduration_predictor_kernel_size = 3\ndropout = 0.1\n\n# Train\nalignment_path = ""./alignments""\ncheckpoint_path = ""./model_new""\nlogger_path = ""./logger""\nmel_ground_truth = ""./mels""\n\nbatch_size = 64\nepochs = 1000\nn_warm_up_step = 4000\n\nlearning_rate = 1e-3\nweight_decay = 1e-6\ngrad_clip_thresh = 1.0\ndecay_step = [500000, 1000000, 2000000]\n\nsave_step = 1000\nlog_step = 5\nclear_Time = 20\n'"
loss.py,2,"b'import torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass FastSpeechLoss(nn.Module):\r\n    """""" FastSPeech Loss """"""\r\n\r\n    def __init__(self):\r\n        super(FastSpeechLoss, self).__init__()\r\n        self.mse_loss = nn.MSELoss()\r\n        self.l1_loss = nn.L1Loss()\r\n\r\n    def forward(self, mel, mel_postnet, duration_predicted, mel_target, duration_predictor_target):\r\n        mel_target.requires_grad = False\r\n        mel_loss = self.mse_loss(mel, mel_target)\r\n        mel_postnet_loss = self.mse_loss(mel_postnet, mel_target)\r\n\r\n        duration_predictor_target.requires_grad = False\r\n        # duration_predictor_target = duration_predictor_target + 1\r\n        # duration_predictor_target = torch.log(\r\n        #     duration_predictor_target.float())\r\n\r\n        # print(duration_predictor_target)\r\n        # print(duration_predicted)\r\n\r\n        duration_predictor_loss = self.l1_loss(\r\n            duration_predicted, duration_predictor_target.float())\r\n\r\n        return mel_loss, mel_postnet_loss, duration_predictor_loss\r\n'"
modules.py,15,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import OrderedDict\nimport numpy as np\nimport copy\nimport math\n\nimport hparams as hp\nimport utils\n\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n\ndef get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None):\n    \'\'\' Sinusoid position encoding table \'\'\'\n\n    def cal_angle(position, hid_idx):\n        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n\n    def get_posi_angle_vec(position):\n        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n\n    sinusoid_table = np.array([get_posi_angle_vec(pos_i)\n                               for pos_i in range(n_position)])\n\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    if padding_idx is not None:\n        # zero vector for padding dimension\n        sinusoid_table[padding_idx] = 0.\n\n    return torch.FloatTensor(sinusoid_table)\n\n\ndef clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\n\nclass LengthRegulator(nn.Module):\n    """""" Length Regulator """"""\n\n    def __init__(self):\n        super(LengthRegulator, self).__init__()\n        self.duration_predictor = DurationPredictor()\n\n    def LR(self, x, duration_predictor_output, alpha=1.0, mel_max_length=None):\n        output = list()\n\n        for batch, expand_target in zip(x, duration_predictor_output):\n            output.append(self.expand(batch, expand_target, alpha))\n\n        if mel_max_length:\n            output = utils.pad(output, mel_max_length)\n        else:\n            output = utils.pad(output)\n\n        return output\n\n    def expand(self, batch, predicted, alpha):\n        out = list()\n\n        for i, vec in enumerate(batch):\n            expand_size = predicted[i].item()\n            out.append(vec.expand(int(expand_size*alpha), -1))\n        out = torch.cat(out, 0)\n\n        return out\n\n    def rounding(self, num):\n        if num - int(num) >= 0.5:\n            return int(num) + 1\n        else:\n            return int(num)\n\n    def forward(self, x, alpha=1.0, target=None, mel_max_length=None):\n        duration_predictor_output = self.duration_predictor(x)\n\n        if self.training:\n            output = self.LR(x, target, mel_max_length=mel_max_length)\n            return output, duration_predictor_output\n        else:\n            for idx, ele in enumerate(duration_predictor_output[0]):\n                duration_predictor_output[0][idx] = self.rounding(ele)\n            output = self.LR(x, duration_predictor_output, alpha)\n            mel_pos = torch.stack(\n                [torch.Tensor([i+1 for i in range(output.size(1))])]).long().to(device)\n\n            return output, mel_pos\n\n\nclass DurationPredictor(nn.Module):\n    """""" Duration Predictor """"""\n\n    def __init__(self):\n        super(DurationPredictor, self).__init__()\n\n        self.input_size = hp.d_model\n        self.filter_size = hp.duration_predictor_filter_size\n        self.kernel = hp.duration_predictor_kernel_size\n        self.conv_output_size = hp.duration_predictor_filter_size\n        self.dropout = hp.dropout\n\n        self.conv_layer = nn.Sequential(OrderedDict([\n            (""conv1d_1"", Conv(self.input_size,\n                              self.filter_size,\n                              kernel_size=self.kernel,\n                              padding=1)),\n            (""layer_norm_1"", nn.LayerNorm(self.filter_size)),\n            (""relu_1"", nn.ReLU()),\n            (""dropout_1"", nn.Dropout(self.dropout)),\n            (""conv1d_2"", Conv(self.filter_size,\n                              self.filter_size,\n                              kernel_size=self.kernel,\n                              padding=1)),\n            (""layer_norm_2"", nn.LayerNorm(self.filter_size)),\n            (""relu_2"", nn.ReLU()),\n            (""dropout_2"", nn.Dropout(self.dropout))\n        ]))\n\n        self.linear_layer = Linear(self.conv_output_size, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, encoder_output):\n        out = self.conv_layer(encoder_output)\n        out = self.linear_layer(out)\n\n        out = self.relu(out)\n\n        out = out.squeeze()\n\n        if not self.training:\n            out = out.unsqueeze(0)\n\n        return out\n\n\nclass Conv(nn.Module):\n    """"""\n    Convolution Module\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=1,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 bias=True,\n                 w_init=\'linear\'):\n        """"""\n        :param in_channels: dimension of input\n        :param out_channels: dimension of output\n        :param kernel_size: size of kernel\n        :param stride: size of stride\n        :param padding: size of padding\n        :param dilation: dilation rate\n        :param bias: boolean. if True, bias is included.\n        :param w_init: str. weight inits with xavier initialization.\n        """"""\n        super(Conv, self).__init__()\n\n        self.conv = nn.Conv1d(in_channels,\n                              out_channels,\n                              kernel_size=kernel_size,\n                              stride=stride,\n                              padding=padding,\n                              dilation=dilation,\n                              bias=bias)\n\n        nn.init.xavier_uniform_(\n            self.conv.weight, gain=nn.init.calculate_gain(w_init))\n\n    def forward(self, x):\n        x = x.contiguous().transpose(1, 2)\n        x = self.conv(x)\n        x = x.contiguous().transpose(1, 2)\n\n        return x\n\n\nclass Linear(nn.Module):\n    """"""\n    Linear Module\n    """"""\n\n    def __init__(self, in_dim, out_dim, bias=True, w_init=\'linear\'):\n        """"""\n        :param in_dim: dimension of input\n        :param out_dim: dimension of output\n        :param bias: boolean. if True, bias is included.\n        :param w_init: str. weight inits with xavier initialization.\n        """"""\n        super(Linear, self).__init__()\n        self.linear_layer = nn.Linear(in_dim, out_dim, bias=bias)\n\n        nn.init.xavier_uniform_(\n            self.linear_layer.weight,\n            gain=nn.init.calculate_gain(w_init))\n\n    def forward(self, x):\n        return self.linear_layer(x)\n\n\nclass FFN(nn.Module):\n    """"""\n    Positionwise Feed-Forward Network\n    """"""\n\n    def __init__(self, num_hidden):\n        """"""\n        :param num_hidden: dimension of hidden \n        """"""\n        super(FFN, self).__init__()\n        self.w_1 = Conv(num_hidden, num_hidden * 4,\n                        kernel_size=3, padding=1, w_init=\'relu\')\n        self.w_2 = Conv(num_hidden * 4, num_hidden, kernel_size=3, padding=1)\n        self.dropout = nn.Dropout(p=0.1)\n        self.layer_norm = nn.LayerNorm(num_hidden)\n\n    def forward(self, input_):\n        # FFN Network\n        x = input_\n        x = self.w_2(torch.relu(self.w_1(x)))\n\n        # residual connection\n        x = x + input_\n\n        # dropout\n        x = self.dropout(x)\n\n        # layer normalization\n        x = self.layer_norm(x)\n\n        return x\n\n\nclass MultiheadAttention(nn.Module):\n    """"""\n    Multihead attention mechanism (dot attention)\n    """"""\n\n    def __init__(self, num_hidden_k):\n        """"""\n        :param num_hidden_k: dimension of hidden \n        """"""\n        super(MultiheadAttention, self).__init__()\n\n        self.num_hidden_k = num_hidden_k\n        self.attn_dropout = nn.Dropout(p=0.1)\n\n    def forward(self, key, value, query, mask=None, query_mask=None):\n        # Get attention score\n        attn = torch.bmm(query, key.transpose(1, 2))\n        attn = attn / math.sqrt(self.num_hidden_k)\n\n        # Masking to ignore padding (key side)\n        if mask is not None:\n            attn = attn.masked_fill(mask, -2 ** 32 + 1)\n            attn = torch.softmax(attn, dim=-1)\n        else:\n            attn = torch.softmax(attn, dim=-1)\n\n        # Masking to ignore padding (query side)\n        if query_mask is not None:\n            attn = attn * query_mask\n\n        # Dropout\n        attn = self.attn_dropout(attn)\n\n        # Get Context Vector\n        result = torch.bmm(attn, value)\n\n        return result, attn\n\n\nclass Attention(nn.Module):\n    """"""\n    Attention Network\n    """"""\n\n    def __init__(self, num_hidden, h=2):\n        """"""\n        :param num_hidden: dimension of hidden\n        :param h: num of heads \n        """"""\n        super(Attention, self).__init__()\n\n        self.num_hidden = num_hidden\n        self.num_hidden_per_attn = num_hidden // h\n        self.h = h\n\n        self.key = Linear(num_hidden, num_hidden, bias=False)\n        self.value = Linear(num_hidden, num_hidden, bias=False)\n        self.query = Linear(num_hidden, num_hidden, bias=False)\n\n        self.multihead = MultiheadAttention(self.num_hidden_per_attn)\n\n        self.residual_dropout = nn.Dropout(p=0.1)\n\n        self.final_linear = Linear(num_hidden * 2, num_hidden)\n\n        self.layer_norm_1 = nn.LayerNorm(num_hidden)\n\n    def forward(self, memory, decoder_input, mask=None, query_mask=None):\n\n        batch_size = memory.size(0)\n        seq_k = memory.size(1)\n        seq_q = decoder_input.size(1)\n\n        # Repeat masks h times\n        if query_mask is not None:\n            query_mask = query_mask.unsqueeze(-1).repeat(1, 1, seq_k)\n            query_mask = query_mask.repeat(self.h, 1, 1)\n        if mask is not None:\n            mask = mask.repeat(self.h, 1, 1)\n\n        # Make multihead\n        key = self.key(memory).view(batch_size,\n                                    seq_k,\n                                    self.h,\n                                    self.num_hidden_per_attn)\n        value = self.value(memory).view(batch_size,\n                                        seq_k,\n                                        self.h,\n                                        self.num_hidden_per_attn)\n        query = self.query(decoder_input).view(batch_size,\n                                               seq_q,\n                                               self.h,\n                                               self.num_hidden_per_attn)\n\n        key = key.permute(2, 0, 1, 3).contiguous().view(-1,\n                                                        seq_k,\n                                                        self.num_hidden_per_attn)\n        value = value.permute(2, 0, 1, 3).contiguous().view(-1,\n                                                            seq_k,\n                                                            self.num_hidden_per_attn)\n        query = query.permute(2, 0, 1, 3).contiguous().view(-1,\n                                                            seq_q,\n                                                            self.num_hidden_per_attn)\n\n        # Get context vector\n        result, attns = self.multihead(\n            key, value, query, mask=mask, query_mask=query_mask)\n\n        # Concatenate all multihead context vector\n        result = result.view(self.h, batch_size, seq_q,\n                             self.num_hidden_per_attn)\n        result = result.permute(1, 2, 0, 3).contiguous().view(\n            batch_size, seq_q, -1)\n\n        # Concatenate context vector with input (most important)\n        result = torch.cat([decoder_input, result], dim=-1)\n\n        # Final linear\n        result = self.final_linear(result)\n\n        # Residual dropout & connection\n        result = self.residual_dropout(result)\n        result = result + decoder_input\n\n        # Layer normalization\n        result = self.layer_norm_1(result)\n\n        return result, attns\n\n\nclass FFTBlock(torch.nn.Module):\n    """"""FFT Block""""""\n\n    def __init__(self,\n                 d_model,\n                 n_head=hp.Head):\n        super(FFTBlock, self).__init__()\n        self.slf_attn = clones(Attention(d_model), hp.N)\n        self.pos_ffn = clones(FFN(d_model), hp.N)\n\n        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(1024,\n                                                                                d_model,\n                                                                                padding_idx=0), freeze=True)\n\n    def forward(self, x, pos, return_attns=False):\n        # Get character mask\n        if self.training:\n            c_mask = pos.ne(0).type(torch.float)\n            mask = pos.eq(0).unsqueeze(1).repeat(1, x.size(1), 1)\n        else:\n            c_mask, mask = None, None\n\n        # Get positional embedding, apply alpha and add\n        pos = self.pos_emb(pos)\n        x = x + pos\n\n        # Attention encoder-encoder\n        attns = list()\n        for slf_attn, ffn in zip(self.slf_attn, self.pos_ffn):\n            x, attn = slf_attn(x, x, mask=mask, query_mask=c_mask)\n            x = ffn(x)\n            attns.append(attn)\n\n        return x, attns\n'"
optimizer.py,0,"b""import numpy as np\r\n\r\n\r\nclass ScheduledOptim():\r\n    ''' A simple wrapper class for learning rate scheduling '''\r\n\r\n    def __init__(self, optimizer, d_model, n_warmup_steps, current_steps):\r\n        self._optimizer = optimizer\r\n        self.n_warmup_steps = n_warmup_steps\r\n        self.n_current_steps = current_steps\r\n        self.init_lr = np.power(d_model, -0.5)\r\n\r\n    def step_and_update_lr_frozen(self, learning_rate_frozen):\r\n        for param_group in self._optimizer.param_groups:\r\n            param_group['lr'] = learning_rate_frozen\r\n        self._optimizer.step()\r\n\r\n    def step_and_update_lr(self):\r\n        self._update_learning_rate()\r\n        self._optimizer.step()\r\n\r\n    def get_learning_rate(self):\r\n        learning_rate = 0.0\r\n        for param_group in self._optimizer.param_groups:\r\n            learning_rate = param_group['lr']\r\n\r\n        return learning_rate\r\n\r\n    def zero_grad(self):\r\n        # print(self.init_lr)\r\n        self._optimizer.zero_grad()\r\n\r\n    def _get_lr_scale(self):\r\n        return np.min([\r\n            np.power(self.n_current_steps, -0.5),\r\n            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\r\n\r\n    def _update_learning_rate(self):\r\n        ''' Learning rate scheduling per step '''\r\n        self.n_current_steps += 1\r\n        lr = self.init_lr * self._get_lr_scale()\r\n\r\n        for param_group in self._optimizer.param_groups:\r\n            param_group['lr'] = lr\r\n"""
preprocess.py,0,"b'import torch\nimport numpy as np\nimport shutil\nimport os\n\nfrom utils import load_data, get_Tacotron2, get_WaveGlow\nfrom utils import process_text, load_data\nfrom data import ljspeech\nimport hparams as hp\nimport waveglow\nimport audio as Audio\n\n\ndef preprocess_ljspeech(filename):\n    in_dir = filename\n    out_dir = hp.mel_ground_truth\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n    metadata = ljspeech.build_from_path(in_dir, out_dir)\n    write_metadata(metadata, out_dir)\n\n    shutil.move(os.path.join(hp.mel_ground_truth, ""train.txt""),\n                os.path.join(""data"", ""train.txt""))\n\n\ndef write_metadata(metadata, out_dir):\n    with open(os.path.join(out_dir, \'train.txt\'), \'w\', encoding=\'utf-8\') as f:\n        for m in metadata:\n            f.write(m + \'\\n\')\n\n\ndef main():\n    path = os.path.join(""data"", ""LJSpeech-1.1"")\n    preprocess_ljspeech(path)\n\n    text_path = os.path.join(""data"", ""train.txt"")\n    texts = process_text(text_path)\n\n    if not os.path.exists(hp.alignment_path):\n        os.mkdir(hp.alignment_path)\n    else:\n        return\n\n    tacotron2 = get_Tacotron2()\n\n    num = 0\n    for ind, text in enumerate(texts[num:]):\n        print(ind)\n\n        character = text[0:len(text)-1]\n        mel_gt_name = os.path.join(\n            hp.mel_ground_truth, ""ljspeech-mel-%05d.npy"" % (ind+num+1))\n        mel_gt_target = np.load(mel_gt_name)\n        _, _, D = load_data(character, mel_gt_target, tacotron2)\n\n        np.save(os.path.join(hp.alignment_path, str(\n            ind+num) + "".npy""), D, allow_pickle=False)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
synthesis.py,9,"b'import torch\r\nimport torch.nn as nn\r\nimport matplotlib\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport time\r\nimport os\r\n\r\nfrom fastspeech import FastSpeech\r\nfrom text import text_to_sequence\r\nimport hparams as hp\r\nimport utils\r\nimport audio as Audio\r\nimport glow\r\nimport waveglow\r\n\r\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\r\n\r\n\r\ndef get_FastSpeech(num):\r\n    checkpoint_path = ""checkpoint_"" + str(num) + "".pth.tar""\r\n    model = nn.DataParallel(FastSpeech()).to(device)\r\n    model.load_state_dict(torch.load(os.path.join(\r\n        hp.checkpoint_path, checkpoint_path))[\'model\'])\r\n    model.eval()\r\n\r\n    return model\r\n\r\n\r\ndef synthesis(model, text, alpha=1.0):\r\n    text = np.array(text_to_sequence(text, hp.text_cleaners))\r\n    text = np.stack([text])\r\n\r\n    src_pos = np.array([i+1 for i in range(text.shape[1])])\r\n    src_pos = np.stack([src_pos])\r\n    with torch.no_grad():\r\n        sequence = torch.autograd.Variable(\r\n            torch.from_numpy(text)).cuda().long()\r\n        src_pos = torch.autograd.Variable(\r\n            torch.from_numpy(src_pos)).cuda().long()\r\n\r\n        mel, mel_postnet = model.module.forward(sequence, src_pos, alpha=alpha)\r\n\r\n        return mel[0].cpu().transpose(0, 1), \\\r\n            mel_postnet[0].cpu().transpose(0, 1), \\\r\n            mel.transpose(1, 2), \\\r\n            mel_postnet.transpose(1, 2)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    # Test\r\n    num = 112000\r\n    alpha = 1.0\r\n    model = get_FastSpeech(num)\r\n    words = ""Let\xe2\x80\x99s go out to the airport. The plane landed ten minutes ago.""\r\n\r\n    mel, mel_postnet, mel_torch, mel_postnet_torch = synthesis(\r\n        model, words, alpha=alpha)\r\n\r\n    if not os.path.exists(""results""):\r\n        os.mkdir(""results"")\r\n    Audio.tools.inv_mel_spec(mel_postnet, os.path.join(\r\n        ""results"", words + ""_"" + str(num) + ""_griffin_lim.wav""))\r\n\r\n    wave_glow = utils.get_WaveGlow()\r\n    waveglow.inference.inference(mel_postnet_torch, wave_glow, os.path.join(\r\n        ""results"", words + ""_"" + str(num) + ""_waveglow.wav""))\r\n\r\n    tacotron2 = utils.get_Tacotron2()\r\n    mel_tac2, _, _ = utils.load_data_from_tacotron2(words, tacotron2)\r\n    waveglow.inference.inference(torch.stack([torch.from_numpy(\r\n        mel_tac2).cuda()]), wave_glow, os.path.join(""results"", ""tacotron2.wav""))\r\n\r\n    utils.plot_data([mel.numpy(), mel_postnet.numpy(), mel_tac2])\r\n'"
train.py,10,"b'import torch\r\nimport torch.nn as nn\r\n\r\nfrom multiprocessing import cpu_count\r\nimport numpy as np\r\nimport argparse\r\nimport os\r\nimport time\r\nimport math\r\n\r\nfrom fastspeech import FastSpeech\r\nfrom loss import FastSpeechLoss\r\nfrom dataset import FastSpeechDataset, collate_fn, DataLoader\r\nfrom optimizer import ScheduledOptim\r\nimport hparams as hp\r\nimport utils\r\n\r\n\r\ndef main(args):\r\n    # Get device\r\n    device = torch.device(\'cuda\'if torch.cuda.is_available()else \'cpu\')\r\n\r\n    # Define model\r\n    model = nn.DataParallel(FastSpeech()).to(device)\r\n    print(""Model Has Been Defined"")\r\n    num_param = utils.get_param_num(model)\r\n    print(\'Number of FastSpeech Parameters:\', num_param)\r\n\r\n    # Get dataset\r\n    dataset = FastSpeechDataset()\r\n\r\n    # Optimizer and loss\r\n    optimizer = torch.optim.Adam(\r\n        model.parameters(), betas=(0.9, 0.98), eps=1e-9)\r\n    scheduled_optim = ScheduledOptim(optimizer,\r\n                                     hp.d_model,\r\n                                     hp.n_warm_up_step,\r\n                                     args.restore_step)\r\n    fastspeech_loss = FastSpeechLoss().to(device)\r\n    print(""Defined Optimizer and Loss Function."")\r\n\r\n    # Load checkpoint if exists\r\n    try:\r\n        checkpoint = torch.load(os.path.join(\r\n            hp.checkpoint_path, \'checkpoint_%d.pth.tar\' % args.restore_step))\r\n        model.load_state_dict(checkpoint[\'model\'])\r\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\r\n        print(""\\n---Model Restored at Step %d---\\n"" % args.restore_step)\r\n    except:\r\n        print(""\\n---Start New Training---\\n"")\r\n        if not os.path.exists(hp.checkpoint_path):\r\n            os.mkdir(hp.checkpoint_path)\r\n\r\n    # Init logger\r\n    if not os.path.exists(hp.logger_path):\r\n        os.mkdir(hp.logger_path)\r\n\r\n    # Define Some Information\r\n    Time = np.array([])\r\n    Start = time.clock()\r\n\r\n    # Training\r\n    model = model.train()\r\n\r\n    for epoch in range(hp.epochs):\r\n        # Get Training Loader\r\n        training_loader = DataLoader(dataset,\r\n                                     batch_size=hp.batch_size**2,\r\n                                     shuffle=True,\r\n                                     collate_fn=collate_fn,\r\n                                     drop_last=True,\r\n                                     num_workers=0)\r\n        total_step = hp.epochs * len(training_loader) * hp.batch_size\r\n\r\n        for i, batchs in enumerate(training_loader):\r\n            for j, data_of_batch in enumerate(batchs):\r\n                start_time = time.clock()\r\n\r\n                current_step = i * hp.batch_size + j + args.restore_step + \\\r\n                    epoch * len(training_loader)*hp.batch_size + 1\r\n\r\n                # Init\r\n                scheduled_optim.zero_grad()\r\n\r\n                # Get Data\r\n                character = torch.from_numpy(\r\n                    data_of_batch[""text""]).long().to(device)\r\n                mel_target = torch.from_numpy(\r\n                    data_of_batch[""mel_target""]).float().to(device)\r\n                D = torch.from_numpy(data_of_batch[""D""]).int().to(device)\r\n                mel_pos = torch.from_numpy(\r\n                    data_of_batch[""mel_pos""]).long().to(device)\r\n                src_pos = torch.from_numpy(\r\n                    data_of_batch[""src_pos""]).long().to(device)\r\n                max_mel_len = data_of_batch[""mel_max_len""]\r\n\r\n                # Forward\r\n                mel_output, mel_postnet_output, duration_predictor_output = model(character,\r\n                                                                                  src_pos,\r\n                                                                                  mel_pos=mel_pos,\r\n                                                                                  mel_max_length=max_mel_len,\r\n                                                                                  length_target=D)\r\n\r\n                # print(mel_target.size())\r\n                # print(mel_output.size())\r\n\r\n                # Cal Loss\r\n                mel_loss, mel_postnet_loss, duration_loss = fastspeech_loss(mel_output,\r\n                                                                            mel_postnet_output,\r\n                                                                            duration_predictor_output,\r\n                                                                            mel_target,\r\n                                                                            D)\r\n                total_loss = mel_loss + mel_postnet_loss + duration_loss\r\n\r\n                # Logger\r\n                t_l = total_loss.item()\r\n                m_l = mel_loss.item()\r\n                m_p_l = mel_postnet_loss.item()\r\n                d_l = duration_loss.item()\r\n\r\n                with open(os.path.join(""logger"", ""total_loss.txt""), ""a"") as f_total_loss:\r\n                    f_total_loss.write(str(t_l)+""\\n"")\r\n\r\n                with open(os.path.join(""logger"", ""mel_loss.txt""), ""a"") as f_mel_loss:\r\n                    f_mel_loss.write(str(m_l)+""\\n"")\r\n\r\n                with open(os.path.join(""logger"", ""mel_postnet_loss.txt""), ""a"") as f_mel_postnet_loss:\r\n                    f_mel_postnet_loss.write(str(m_p_l)+""\\n"")\r\n\r\n                with open(os.path.join(""logger"", ""duration_loss.txt""), ""a"") as f_d_loss:\r\n                    f_d_loss.write(str(d_l)+""\\n"")\r\n\r\n                # Backward\r\n                total_loss.backward()\r\n\r\n                # Clipping gradients to avoid gradient explosion\r\n                nn.utils.clip_grad_norm_(\r\n                    model.parameters(), hp.grad_clip_thresh)\r\n\r\n                # Update weights\r\n                if args.frozen_learning_rate:\r\n                    scheduled_optim.step_and_update_lr_frozen(\r\n                        args.learning_rate_frozen)\r\n                else:\r\n                    scheduled_optim.step_and_update_lr()\r\n\r\n                # Print\r\n                if current_step % hp.log_step == 0:\r\n                    Now = time.clock()\r\n\r\n                    str1 = ""Epoch [{}/{}], Step [{}/{}]:"".format(\r\n                        epoch+1, hp.epochs, current_step, total_step)\r\n                    str2 = ""Mel Loss: {:.4f}, Mel PostNet Loss: {:.4f}, Duration Loss: {:.4f};"".format(\r\n                        m_l, m_p_l, d_l)\r\n                    str3 = ""Current Learning Rate is {:.6f}."".format(\r\n                        scheduled_optim.get_learning_rate())\r\n                    str4 = ""Time Used: {:.3f}s, Estimated Time Remaining: {:.3f}s."".format(\r\n                        (Now-Start), (total_step-current_step)*np.mean(Time))\r\n\r\n                    print(""\\n"" + str1)\r\n                    print(str2)\r\n                    print(str3)\r\n                    print(str4)\r\n\r\n                    with open(os.path.join(""logger"", ""logger.txt""), ""a"") as f_logger:\r\n                        f_logger.write(str1 + ""\\n"")\r\n                        f_logger.write(str2 + ""\\n"")\r\n                        f_logger.write(str3 + ""\\n"")\r\n                        f_logger.write(str4 + ""\\n"")\r\n                        f_logger.write(""\\n"")\r\n\r\n                if current_step % hp.save_step == 0:\r\n                    torch.save({\'model\': model.state_dict(), \'optimizer\': optimizer.state_dict(\r\n                    )}, os.path.join(hp.checkpoint_path, \'checkpoint_%d.pth.tar\' % current_step))\r\n                    print(""save model at step %d ..."" % current_step)\r\n\r\n                end_time = time.clock()\r\n                Time = np.append(Time, end_time - start_time)\r\n                if len(Time) == hp.clear_Time:\r\n                    temp_value = np.mean(Time)\r\n                    Time = np.delete(\r\n                        Time, [i for i in range(len(Time))], axis=None)\r\n                    Time = np.append(Time, temp_value)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\'--restore_step\', type=int, default=0)\r\n    parser.add_argument(\'--frozen_learning_rate\', type=bool, default=False)\r\n    parser.add_argument(""--learning_rate_frozen"", type=float, default=1e-3)\r\n    args = parser.parse_args()\r\n\r\n    main(args)\r\n'"
utils.py,15,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport os\n\nimport tacotron2 as Tacotron2\nimport text\nimport hparams\n\n\ndef process_text(train_text_path):\n    with open(train_text_path, ""r"", encoding=""utf-8"") as f:\n        txt = []\n        for line in f.readlines():\n            txt.append(line)\n\n        return txt\n\n\ndef get_param_num(model):\n    num_param = sum(param.numel() for param in model.parameters())\n    return num_param\n\n\ndef plot_data(data, figsize=(12, 4)):\n    _, axes = plt.subplots(1, len(data), figsize=figsize)\n    for i in range(len(data)):\n        axes[i].imshow(data[i], aspect=\'auto\',\n                       origin=\'bottom\', interpolation=\'none\')\n\n    if not os.path.exists(""img""):\n        os.mkdir(""img"")\n    plt.savefig(os.path.join(""img"", ""model_test.jpg""))\n\n\ndef get_mask_from_lengths(lengths, max_len=None):\n    if max_len == None:\n        max_len = torch.max(lengths).item()\n\n    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n    mask = (ids < lengths.unsqueeze(1)).byte()\n\n    return mask\n\n\ndef get_WaveGlow():\n    waveglow_path = os.path.join(""waveglow"", ""pretrained_model"")\n    waveglow_path = os.path.join(waveglow_path, ""waveglow_256channels.pt"")\n    wave_glow = torch.load(waveglow_path)[\'model\']\n    wave_glow = wave_glow.remove_weightnorm(wave_glow)\n    wave_glow.cuda().eval()\n    for m in wave_glow.modules():\n        if \'Conv\' in str(type(m)):\n            setattr(m, \'padding_mode\', \'zeros\')\n\n    return wave_glow\n\n\ndef get_Tacotron2():\n    checkpoint_path = ""tacotron2_statedict.pt""\n    checkpoint_path = os.path.join(os.path.join(\n        ""tacotron2"", ""pretrained_model""), checkpoint_path)\n\n    model = Tacotron2.model.Tacotron2(\n        Tacotron2.hparams.create_hparams()).cuda()\n    model.load_state_dict(torch.load(checkpoint_path)[\'state_dict\'])\n    _ = model.cuda().eval()\n\n    return model\n\n\ndef get_D(alignment):\n    D = np.array([0 for _ in range(np.shape(alignment)[1])])\n\n    for i in range(np.shape(alignment)[0]):\n        max_index = alignment[i].tolist().index(alignment[i].max())\n        D[max_index] = D[max_index] + 1\n\n    return D\n\n\ndef pad_1D(inputs, PAD=0):\n\n    def pad_data(x, length, PAD):\n        x_padded = np.pad(x, (0, length - x.shape[0]),\n                          mode=\'constant\',\n                          constant_values=PAD)\n        return x_padded\n\n    max_len = max((len(x) for x in inputs))\n    padded = np.stack([pad_data(x, max_len, PAD) for x in inputs])\n\n    return padded\n\n\ndef pad_2D(inputs, maxlen=None):\n\n    def pad(x, max_len):\n        PAD = 0\n        if np.shape(x)[0] > max_len:\n            raise ValueError(""not max_len"")\n\n        s = np.shape(x)[1]\n        x_padded = np.pad(x, (0, max_len - np.shape(x)[0]),\n                          mode=\'constant\',\n                          constant_values=PAD)\n        return x_padded[:, :s]\n\n    if maxlen:\n        output = np.stack([pad(x, maxlen) for x in inputs])\n    else:\n        max_len = max(np.shape(x)[0] for x in inputs)\n        output = np.stack([pad(x, max_len) for x in inputs])\n\n    return output\n\n\ndef pad(input_ele, mel_max_length=None):\n    if mel_max_length:\n        out_list = list()\n        max_len = mel_max_length\n        for i, batch in enumerate(input_ele):\n            one_batch_padded = F.pad(\n                batch, (0, 0, 0, max_len-batch.size(0)), ""constant"", 0.0)\n            out_list.append(one_batch_padded)\n        out_padded = torch.stack(out_list)\n        return out_padded\n    else:\n        out_list = list()\n        max_len = max([input_ele[i].size(0)for i in range(len(input_ele))])\n\n        for i, batch in enumerate(input_ele):\n            one_batch_padded = F.pad(\n                batch, (0, 0, 0, max_len-batch.size(0)), ""constant"", 0.0)\n            out_list.append(one_batch_padded)\n        out_padded = torch.stack(out_list)\n        return out_padded\n\n\ndef load_data(txt, mel, model):\n    character = text.text_to_sequence(txt, hparams.text_cleaners)\n    character = torch.from_numpy(np.stack([np.array(character)])).long().cuda()\n\n    text_length = torch.Tensor([character.size(1)]).long().cuda()\n    mel = torch.from_numpy(np.stack([mel.T])).float().cuda()\n    max_len = mel.size(2)\n    output_length = torch.Tensor([max_len]).long().cuda()\n\n    inputs = character, text_length, mel, max_len, output_length\n\n    with torch.no_grad():\n        [_, mel_tacotron2, _, alignment], cemb = model.forward(inputs)\n\n    alignment = alignment[0].cpu().numpy()\n    cemb = cemb[0].cpu().numpy()\n\n    D = get_D(alignment)\n    D = np.array(D)\n\n    mel_tacotron2 = mel_tacotron2[0].cpu().numpy()\n\n    return mel_tacotron2, cemb, D\n\n\ndef load_data_from_tacotron2(txt, model):\n    character = text.text_to_sequence(txt, hparams.text_cleaners)\n    character = torch.from_numpy(np.stack([np.array(character)])).long().cuda()\n\n    with torch.no_grad():\n        [_, mel, _, alignment], cemb = model.inference(character)\n\n    alignment = alignment[0].cpu().numpy()\n    cemb = cemb[0].cpu().numpy()\n\n    D = get_D(alignment)\n    D = np.array(D)\n\n    mel = mel[0].cpu().numpy()\n\n    return mel, cemb, D\n'"
audio/__init__.py,0,b'import audio.hparams\nimport audio.tools\nimport audio.stft\nimport audio.audio_processing\n'
audio/audio_processing.py,3,"b'import torch\nimport numpy as np\nfrom scipy.signal import get_window\nimport librosa.util as librosa_util\n\n\ndef window_sumsquare(window, n_frames, hop_length=200, win_length=800,\n                     n_fft=800, dtype=np.float32, norm=None):\n    """"""\n    # from librosa 0.6\n    Compute the sum-square envelope of a window function at a given hop length.\n\n    This is used to estimate modulation effects induced by windowing\n    observations in short-time fourier transforms.\n\n    Parameters\n    ----------\n    window : string, tuple, number, callable, or list-like\n        Window specification, as in `get_window`\n\n    n_frames : int > 0\n        The number of analysis frames\n\n    hop_length : int > 0\n        The number of samples to advance between frames\n\n    win_length : [optional]\n        The length of the window function.  By default, this matches `n_fft`.\n\n    n_fft : int > 0\n        The length of each analysis frame.\n\n    dtype : np.dtype\n        The data type of the output\n\n    Returns\n    -------\n    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`\n        The sum-squared envelope of the window function\n    """"""\n    if win_length is None:\n        win_length = n_fft\n\n    n = n_fft + hop_length * (n_frames - 1)\n    x = np.zeros(n, dtype=dtype)\n\n    # Compute the squared window at the desired length\n    win_sq = get_window(window, win_length, fftbins=True)\n    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n    win_sq = librosa_util.pad_center(win_sq, n_fft)\n\n    # Fill the envelope\n    for i in range(n_frames):\n        sample = i * hop_length\n        x[sample:min(n, sample + n_fft)\n          ] += win_sq[:max(0, min(n_fft, n - sample))]\n    return x\n\n\ndef griffin_lim(magnitudes, stft_fn, n_iters=30):\n    """"""\n    PARAMS\n    ------\n    magnitudes: spectrogram magnitudes\n    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n    """"""\n\n    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n    angles = angles.astype(np.float32)\n    angles = torch.autograd.Variable(torch.from_numpy(angles))\n    signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n\n    for i in range(n_iters):\n        _, angles = stft_fn.transform(signal)\n        signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n    return signal\n\n\ndef dynamic_range_compression(x, C=1, clip_val=1e-5):\n    """"""\n    PARAMS\n    ------\n    C: compression factor\n    """"""\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef dynamic_range_decompression(x, C=1):\n    """"""\n    PARAMS\n    ------\n    C: compression factor used to compress\n    """"""\n    return torch.exp(x) / C\n'"
audio/hparams.py,0,b'max_wav_value = 32768.0\nsampling_rate = 22050\nfilter_length = 1024\nhop_length = 256\nwin_length = 1024\nn_mel_channels = 80\nmel_fmin = 0.0\nmel_fmax = 8000.0\n'
audio/stft.py,21,"b'import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\n\nfrom scipy.signal import get_window\nfrom librosa.util import pad_center, tiny\nfrom librosa.filters import mel as librosa_mel_fn\n\nfrom audio.audio_processing import dynamic_range_compression\nfrom audio.audio_processing import dynamic_range_decompression\nfrom audio.audio_processing import window_sumsquare\n\n\nclass STFT(torch.nn.Module):\n    """"""adapted from Prem Seetharaman\'s https://github.com/pseeth/pytorch-stft""""""\n\n    def __init__(self, filter_length=800, hop_length=200, win_length=800,\n                 window=\'hann\'):\n        super(STFT, self).__init__()\n        self.filter_length = filter_length\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.window = window\n        self.forward_transform = None\n        scale = self.filter_length / self.hop_length\n        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n\n        cutoff = int((self.filter_length / 2 + 1))\n        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n                                   np.imag(fourier_basis[:cutoff, :])])\n\n        forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n        inverse_basis = torch.FloatTensor(\n            np.linalg.pinv(scale * fourier_basis).T[:, None, :])\n\n        if window is not None:\n            assert(filter_length >= win_length)\n            # get window and zero center pad it to filter_length\n            fft_window = get_window(window, win_length, fftbins=True)\n            fft_window = pad_center(fft_window, filter_length)\n            fft_window = torch.from_numpy(fft_window).float()\n\n            # window the bases\n            forward_basis *= fft_window\n            inverse_basis *= fft_window\n\n        self.register_buffer(\'forward_basis\', forward_basis.float())\n        self.register_buffer(\'inverse_basis\', inverse_basis.float())\n\n    def transform(self, input_data):\n        num_batches = input_data.size(0)\n        num_samples = input_data.size(1)\n\n        self.num_samples = num_samples\n\n        # similar to librosa, reflect-pad the input\n        input_data = input_data.view(num_batches, 1, num_samples)\n        input_data = F.pad(\n            input_data.unsqueeze(1),\n            (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0),\n            mode=\'reflect\')\n        input_data = input_data.squeeze(1)\n\n        forward_transform = F.conv1d(\n            input_data.cuda(),\n            Variable(self.forward_basis, requires_grad=False).cuda(),\n            stride=self.hop_length,\n            padding=0).cpu()\n\n        cutoff = int((self.filter_length / 2) + 1)\n        real_part = forward_transform[:, :cutoff, :]\n        imag_part = forward_transform[:, cutoff:, :]\n\n        magnitude = torch.sqrt(real_part**2 + imag_part**2)\n        phase = torch.autograd.Variable(\n            torch.atan2(imag_part.data, real_part.data))\n\n        return magnitude, phase\n\n    def inverse(self, magnitude, phase):\n        recombine_magnitude_phase = torch.cat(\n            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)\n\n        inverse_transform = F.conv_transpose1d(\n            recombine_magnitude_phase,\n            Variable(self.inverse_basis, requires_grad=False),\n            stride=self.hop_length,\n            padding=0)\n\n        if self.window is not None:\n            window_sum = window_sumsquare(\n                self.window, magnitude.size(-1), hop_length=self.hop_length,\n                win_length=self.win_length, n_fft=self.filter_length,\n                dtype=np.float32)\n            # remove modulation effects\n            approx_nonzero_indices = torch.from_numpy(\n                np.where(window_sum > tiny(window_sum))[0])\n            window_sum = torch.autograd.Variable(\n                torch.from_numpy(window_sum), requires_grad=False)\n            window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum\n            inverse_transform[:, :,\n                              approx_nonzero_indices] /= window_sum[approx_nonzero_indices]\n\n            # scale by hop ratio\n            inverse_transform *= float(self.filter_length) / self.hop_length\n\n        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]\n        inverse_transform = inverse_transform[:,\n                                              :, :-int(self.filter_length/2):]\n\n        return inverse_transform\n\n    def forward(self, input_data):\n        self.magnitude, self.phase = self.transform(input_data)\n        reconstruction = self.inverse(self.magnitude, self.phase)\n        return reconstruction\n\n\nclass TacotronSTFT(torch.nn.Module):\n    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n                 mel_fmax=8000.0):\n        super(TacotronSTFT, self).__init__()\n        self.n_mel_channels = n_mel_channels\n        self.sampling_rate = sampling_rate\n        self.stft_fn = STFT(filter_length, hop_length, win_length)\n        mel_basis = librosa_mel_fn(\n            sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)\n        mel_basis = torch.from_numpy(mel_basis).float()\n        self.register_buffer(\'mel_basis\', mel_basis)\n\n    def spectral_normalize(self, magnitudes):\n        output = dynamic_range_compression(magnitudes)\n        return output\n\n    def spectral_de_normalize(self, magnitudes):\n        output = dynamic_range_decompression(magnitudes)\n        return output\n\n    def mel_spectrogram(self, y):\n        """"""Computes mel-spectrograms from a batch of waves\n        PARAMS\n        ------\n        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n\n        RETURNS\n        -------\n        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n        """"""\n        assert(torch.min(y.data) >= -1)\n        assert(torch.max(y.data) <= 1)\n\n        magnitudes, phases = self.stft_fn.transform(y)\n        magnitudes = magnitudes.data\n        mel_output = torch.matmul(self.mel_basis, magnitudes)\n        mel_output = self.spectral_normalize(mel_output)\n        return mel_output\n'"
audio/tools.py,10,"b'import torch\nimport numpy as np\nfrom scipy.io.wavfile import read\nfrom scipy.io.wavfile import write\n\nimport audio.stft as stft\nimport audio.hparams as hparams\nfrom audio.audio_processing import griffin_lim\n\n_stft = stft.TacotronSTFT(\n    hparams.filter_length, hparams.hop_length, hparams.win_length,\n    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n    hparams.mel_fmax)\n\n\ndef load_wav_to_torch(full_path):\n    sampling_rate, data = read(full_path)\n    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n\n\ndef get_mel(filename):\n    audio, sampling_rate = load_wav_to_torch(filename)\n    if sampling_rate != _stft.sampling_rate:\n        raise ValueError(""{} {} SR doesn\'t match target {} SR"".format(\n            sampling_rate, _stft.sampling_rate))\n    audio_norm = audio / hparams.max_wav_value\n    audio_norm = audio_norm.unsqueeze(0)\n    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n    melspec = _stft.mel_spectrogram(audio_norm)\n    melspec = torch.squeeze(melspec, 0)\n    # melspec = torch.from_numpy(_normalize(melspec.numpy()))\n\n    return melspec\n\n\ndef get_mel_from_wav(audio):\n    sampling_rate = hparams.sampling_rate\n    if sampling_rate != _stft.sampling_rate:\n        raise ValueError(""{} {} SR doesn\'t match target {} SR"".format(\n            sampling_rate, _stft.sampling_rate))\n    audio_norm = audio / hparams.max_wav_value\n    audio_norm = audio_norm.unsqueeze(0)\n    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n    melspec = _stft.mel_spectrogram(audio_norm)\n    melspec = torch.squeeze(melspec, 0)\n\n    return melspec\n\n\ndef inv_mel_spec(mel, out_filename, griffin_iters=60):\n    mel = torch.stack([mel])\n    # mel = torch.stack([torch.from_numpy(_denormalize(mel.numpy()))])\n    mel_decompress = _stft.spectral_de_normalize(mel)\n    mel_decompress = mel_decompress.transpose(1, 2).data.cpu()\n    spec_from_mel_scaling = 1000\n    spec_from_mel = torch.mm(mel_decompress[0], _stft.mel_basis)\n    spec_from_mel = spec_from_mel.transpose(0, 1).unsqueeze(0)\n    spec_from_mel = spec_from_mel * spec_from_mel_scaling\n\n    audio = griffin_lim(torch.autograd.Variable(\n        spec_from_mel[:, :, :-1]), _stft.stft_fn, griffin_iters)\n\n    audio = audio.squeeze()\n    audio = audio.cpu().numpy()\n    audio_path = out_filename\n    write(audio_path, hparams.sampling_rate, audio)\n'"
data/ljspeech.py,0,"b'import numpy as np\nimport os\nimport audio as Audio\n\n\ndef build_from_path(in_dir, out_dir):\n    index = 1\n    out = list()\n\n    with open(os.path.join(in_dir, \'metadata.csv\'), encoding=\'utf-8\') as f:\n        for line in f:\n            parts = line.strip().split(\'|\')\n            wav_path = os.path.join(in_dir, \'wavs\', \'%s.wav\' % parts[0])\n            text = parts[2]\n            out.append(_process_utterance(out_dir, index, wav_path, text))\n\n            if index % 100 == 0:\n                print(""Done %d"" % index)\n            index = index + 1\n\n    return out\n\n\ndef _process_utterance(out_dir, index, wav_path, text):\n    # Compute a mel-scale spectrogram from the wav:\n    mel_spectrogram = Audio.tools.get_mel(wav_path).numpy().astype(np.float32)\n    # print(mel_spectrogram)\n\n    # Write the spectrograms to disk:\n    mel_filename = \'ljspeech-mel-%05d.npy\' % index\n    np.save(os.path.join(out_dir, mel_filename),\n            mel_spectrogram.T, allow_pickle=False)\n\n    return text\n'"
tacotron2/__init__.py,0,b'import tacotron2.hparams\r\nimport tacotron2.model\r\nimport tacotron2.layers\r\n'
tacotron2/hparams.py,0,"b'from text import symbols\n\n\nclass Hparams:\n    """""" hyper parameters """"""\n\n    def __init__(self):\n        ################################\n        # Experiment Parameters        #\n        ################################\n        self.epochs = 500\n        self.iters_per_checkpoint = 1000\n        self.seed = 1234\n        self.dynamic_loss_scaling = True\n        self.fp16_run = False\n        self.distributed_run = False\n        self.dist_backend = ""nccl""\n        self.dist_url = ""tcp://localhost:54321""\n        self.cudnn_enabled = True\n        self.cudnn_benchmark = False\n        self.ignore_layers = [\'embedding.weight\']\n\n        ################################\n        # Data Parameters             #\n        ################################\n        self.load_mel_from_disk = False\n        self.training_files = \'filelists/ljs_audio_text_train_filelist.txt\'\n        self.validation_files = \'filelists/ljs_audio_text_val_filelist.txt\'\n        self.text_cleaners = [\'english_cleaners\']\n\n        ################################\n        # Audio Parameters             #\n        ################################\n        self.max_wav_value = 32768.0\n        self.sampling_rate = 22050\n        self.filter_length = 1024\n        self.hop_length = 256\n        self.win_length = 1024\n        self.n_mel_channels = 80\n        self.mel_fmin = 0.0\n        self.mel_fmax = 8000.0\n\n        ################################\n        # Model Parameters             #\n        ################################\n        self.n_symbols = len(symbols)\n        self.symbols_embedding_dim = 512\n\n        # Encoder parameters\n        self.encoder_kernel_size = 5\n        self.encoder_n_convolutions = 3\n        self.encoder_embedding_dim = 512\n\n        # Decoder parameters\n        self.n_frames_per_step = 1  # currently only 1 is supported\n        self.decoder_rnn_dim = 1024\n        self.prenet_dim = 256\n        self.max_decoder_steps = 1000\n        self.gate_threshold = 0.5\n        self.p_attention_dropout = 0.1\n        self.p_decoder_dropout = 0.1\n\n        # Attention parameters\n        self.attention_rnn_dim = 1024\n        self.attention_dim = 128\n\n        # Location Layer parameters\n        self.attention_location_n_filters = 32\n        self.attention_location_kernel_size = 31\n\n        # Mel-post processing network parameters\n        self.postnet_embedding_dim = 512\n        self.postnet_kernel_size = 5\n        self.postnet_n_convolutions = 5\n\n        ################################\n        # Optimization Hyperparameters #\n        ################################\n        self.use_saved_learning_rate = False\n        self.learning_rate = 1e-3\n        self.weight_decay = 1e-6\n        self.grad_clip_thresh = 1.0\n        self.batch_size = 64\n        self.mask_padding = True  # set model\'s padded outputs to padded values\n\n    def return_self(self):\n        return self\n\n\ndef create_hparams():\n    hparams = Hparams()\n    return hparams.return_self()\n'"
tacotron2/layers.py,8,"b""import torch\nfrom librosa.filters import mel as librosa_mel_fn\n\n\nclass LinearNorm(torch.nn.Module):\n    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n        super(LinearNorm, self).__init__()\n        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n\n        torch.nn.init.xavier_uniform_(\n            self.linear_layer.weight,\n            gain=torch.nn.init.calculate_gain(w_init_gain))\n\n    def forward(self, x):\n        return self.linear_layer(x)\n\n\nclass ConvNorm(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n        super(ConvNorm, self).__init__()\n        if padding is None:\n            assert(kernel_size % 2 == 1)\n            padding = int(dilation * (kernel_size - 1) / 2)\n\n        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n                                    kernel_size=kernel_size, stride=stride,\n                                    padding=padding, dilation=dilation,\n                                    bias=bias)\n\n        torch.nn.init.xavier_uniform_(\n            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n\n    def forward(self, signal):\n        conv_signal = self.conv(signal)\n        return conv_signal\n"""
tacotron2/model.py,15,"b'from math import sqrt\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom tacotron2.layers import ConvNorm, LinearNorm\nfrom tacotron2.utils import to_gpu, get_mask_from_lengths\n\n\nclass LocationLayer(nn.Module):\n    def __init__(self, attention_n_filters, attention_kernel_size,\n                 attention_dim):\n        super(LocationLayer, self).__init__()\n        padding = int((attention_kernel_size - 1) / 2)\n        self.location_conv = ConvNorm(2, attention_n_filters,\n                                      kernel_size=attention_kernel_size,\n                                      padding=padding, bias=False, stride=1,\n                                      dilation=1)\n        self.location_dense = LinearNorm(attention_n_filters, attention_dim,\n                                         bias=False, w_init_gain=\'tanh\')\n\n    def forward(self, attention_weights_cat):\n        processed_attention = self.location_conv(attention_weights_cat)\n        processed_attention = processed_attention.transpose(1, 2)\n        processed_attention = self.location_dense(processed_attention)\n        return processed_attention\n\n\nclass Attention(nn.Module):\n    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n                 attention_location_n_filters, attention_location_kernel_size):\n        super(Attention, self).__init__()\n        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n                                      bias=False, w_init_gain=\'tanh\')\n        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n                                       w_init_gain=\'tanh\')\n        self.v = LinearNorm(attention_dim, 1, bias=False)\n        self.location_layer = LocationLayer(attention_location_n_filters,\n                                            attention_location_kernel_size,\n                                            attention_dim)\n        self.score_mask_value = -float(""inf"")\n\n    def get_alignment_energies(self, query, processed_memory,\n                               attention_weights_cat):\n        """"""\n        PARAMS\n        ------\n        query: decoder output (batch, n_mel_channels * n_frames_per_step)\n        processed_memory: processed encoder outputs (B, T_in, attention_dim)\n        attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n\n        RETURNS\n        -------\n        alignment (batch, max_time)\n        """"""\n\n        processed_query = self.query_layer(query.unsqueeze(1))\n        processed_attention_weights = self.location_layer(\n            attention_weights_cat)\n        energies = self.v(torch.tanh(\n            processed_query + processed_attention_weights + processed_memory))\n\n        energies = energies.squeeze(-1)\n        return energies\n\n    def forward(self, attention_hidden_state, memory, processed_memory,\n                attention_weights_cat, mask):\n        """"""\n        PARAMS\n        ------\n        attention_hidden_state: attention rnn last output\n        memory: encoder outputs\n        processed_memory: processed encoder outputs\n        attention_weights_cat: previous and cummulative attention weights\n        mask: binary mask for padded data\n        """"""\n        alignment = self.get_alignment_energies(\n            attention_hidden_state, processed_memory, attention_weights_cat)\n\n        if mask is not None:\n            alignment.data.masked_fill_(mask, self.score_mask_value)\n\n        attention_weights = F.softmax(alignment, dim=1)\n        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n        attention_context = attention_context.squeeze(1)\n\n        return attention_context, attention_weights\n\n\nclass Prenet(nn.Module):\n    def __init__(self, in_dim, sizes):\n        super(Prenet, self).__init__()\n        in_sizes = [in_dim] + sizes[:-1]\n        self.layers = nn.ModuleList(\n            [LinearNorm(in_size, out_size, bias=False)\n             for (in_size, out_size) in zip(in_sizes, sizes)])\n\n    def forward(self, x):\n        for linear in self.layers:\n            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n        return x\n\n\nclass Postnet(nn.Module):\n    """"""Postnet\n        - Five 1-d convolution with 512 channels and kernel size 5\n    """"""\n\n    def __init__(self, hparams):\n        super(Postnet, self).__init__()\n        self.convolutions = nn.ModuleList()\n\n        self.convolutions.append(\n            nn.Sequential(\n                ConvNorm(hparams.n_mel_channels, hparams.postnet_embedding_dim,\n                         kernel_size=hparams.postnet_kernel_size, stride=1,\n                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n                         dilation=1, w_init_gain=\'tanh\'),\n                nn.BatchNorm1d(hparams.postnet_embedding_dim))\n        )\n\n        for i in range(1, hparams.postnet_n_convolutions - 1):\n            self.convolutions.append(\n                nn.Sequential(\n                    ConvNorm(hparams.postnet_embedding_dim,\n                             hparams.postnet_embedding_dim,\n                             kernel_size=hparams.postnet_kernel_size, stride=1,\n                             padding=int(\n                                 (hparams.postnet_kernel_size - 1) / 2),\n                             dilation=1, w_init_gain=\'tanh\'),\n                    nn.BatchNorm1d(hparams.postnet_embedding_dim))\n            )\n\n        self.convolutions.append(\n            nn.Sequential(\n                ConvNorm(hparams.postnet_embedding_dim, hparams.n_mel_channels,\n                         kernel_size=hparams.postnet_kernel_size, stride=1,\n                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n                         dilation=1, w_init_gain=\'linear\'),\n                nn.BatchNorm1d(hparams.n_mel_channels))\n        )\n\n    def forward(self, x):\n        for i in range(len(self.convolutions) - 1):\n            x = F.dropout(torch.tanh(\n                self.convolutions[i](x)), 0.5, self.training)\n        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n\n        return x\n\n\nclass Encoder(nn.Module):\n    """"""Encoder module:\n        - Three 1-d convolution banks\n        - Bidirectional LSTM\n    """"""\n\n    def __init__(self, hparams):\n        super(Encoder, self).__init__()\n\n        convolutions = []\n        for _ in range(hparams.encoder_n_convolutions):\n            conv_layer = nn.Sequential(\n                ConvNorm(hparams.encoder_embedding_dim,\n                         hparams.encoder_embedding_dim,\n                         kernel_size=hparams.encoder_kernel_size, stride=1,\n                         padding=int((hparams.encoder_kernel_size - 1) / 2),\n                         dilation=1, w_init_gain=\'relu\'),\n                nn.BatchNorm1d(hparams.encoder_embedding_dim))\n            convolutions.append(conv_layer)\n        self.convolutions = nn.ModuleList(convolutions)\n\n        self.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n                            int(hparams.encoder_embedding_dim / 2), 1,\n                            batch_first=True, bidirectional=True)\n\n    def forward(self, x, input_lengths):\n        for conv in self.convolutions:\n            x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n\n        x = x.transpose(1, 2)\n\n        # pytorch tensor are not reversible, hence the conversion\n        input_lengths = input_lengths.cpu().numpy()\n        x = nn.utils.rnn.pack_padded_sequence(\n            x, input_lengths, batch_first=True)\n\n        self.lstm.flatten_parameters()\n        outputs, _ = self.lstm(x)\n\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n            outputs, batch_first=True)\n\n        return outputs\n\n    def inference(self, x):\n        for conv in self.convolutions:\n            x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n\n        x = x.transpose(1, 2)\n\n        self.lstm.flatten_parameters()\n        outputs, _ = self.lstm(x)\n\n        return outputs\n\n\nclass Decoder(nn.Module):\n    def __init__(self, hparams):\n        super(Decoder, self).__init__()\n        self.n_mel_channels = hparams.n_mel_channels\n        self.n_frames_per_step = hparams.n_frames_per_step\n        self.encoder_embedding_dim = hparams.encoder_embedding_dim\n        self.attention_rnn_dim = hparams.attention_rnn_dim\n        self.decoder_rnn_dim = hparams.decoder_rnn_dim\n        self.prenet_dim = hparams.prenet_dim\n        self.max_decoder_steps = hparams.max_decoder_steps\n        self.gate_threshold = hparams.gate_threshold\n        self.p_attention_dropout = hparams.p_attention_dropout\n        self.p_decoder_dropout = hparams.p_decoder_dropout\n\n        self.prenet = Prenet(\n            hparams.n_mel_channels * hparams.n_frames_per_step,\n            [hparams.prenet_dim, hparams.prenet_dim])\n\n        self.attention_rnn = nn.LSTMCell(\n            hparams.prenet_dim + hparams.encoder_embedding_dim,\n            hparams.attention_rnn_dim)\n\n        self.attention_layer = Attention(\n            hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n            hparams.attention_dim, hparams.attention_location_n_filters,\n            hparams.attention_location_kernel_size)\n\n        self.decoder_rnn = nn.LSTMCell(\n            hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n            hparams.decoder_rnn_dim, 1)\n\n        self.linear_projection = LinearNorm(\n            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim,\n            hparams.n_mel_channels * hparams.n_frames_per_step)\n\n        self.gate_layer = LinearNorm(\n            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n            bias=True, w_init_gain=\'sigmoid\')\n\n    def get_go_frame(self, memory):\n        """""" Gets all zeros frames to use as first decoder input\n        PARAMS\n        ------\n        memory: decoder outputs\n\n        RETURNS\n        -------\n        decoder_input: all zeros frames\n        """"""\n        B = memory.size(0)\n        decoder_input = Variable(memory.data.new(\n            B, self.n_mel_channels * self.n_frames_per_step).zero_())\n        return decoder_input\n\n    def initialize_decoder_states(self, memory, mask):\n        """""" Initializes attention rnn states, decoder rnn states, attention\n        weights, attention cumulative weights, attention context, stores memory\n        and stores processed memory\n        PARAMS\n        ------\n        memory: Encoder outputs\n        mask: Mask for padded data if training, expects None for inference\n        """"""\n        B = memory.size(0)\n        MAX_TIME = memory.size(1)\n\n        self.attention_hidden = Variable(memory.data.new(\n            B, self.attention_rnn_dim).zero_())\n        self.attention_cell = Variable(memory.data.new(\n            B, self.attention_rnn_dim).zero_())\n\n        self.decoder_hidden = Variable(memory.data.new(\n            B, self.decoder_rnn_dim).zero_())\n        self.decoder_cell = Variable(memory.data.new(\n            B, self.decoder_rnn_dim).zero_())\n\n        self.attention_weights = Variable(memory.data.new(\n            B, MAX_TIME).zero_())\n        self.attention_weights_cum = Variable(memory.data.new(\n            B, MAX_TIME).zero_())\n        self.attention_context = Variable(memory.data.new(\n            B, self.encoder_embedding_dim).zero_())\n\n        self.memory = memory\n        self.processed_memory = self.attention_layer.memory_layer(memory)\n        self.mask = mask\n\n    def parse_decoder_inputs(self, decoder_inputs):\n        """""" Prepares decoder inputs, i.e. mel outputs\n        PARAMS\n        ------\n        decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n\n        RETURNS\n        -------\n        inputs: processed decoder inputs\n\n        """"""\n        # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n        decoder_inputs = decoder_inputs.transpose(1, 2)\n        decoder_inputs = decoder_inputs.view(\n            decoder_inputs.size(0),\n            int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n        # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n        decoder_inputs = decoder_inputs.transpose(0, 1)\n        return decoder_inputs\n\n    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n        """""" Prepares decoder outputs for output\n        PARAMS\n        ------\n        mel_outputs:\n        gate_outputs: gate output energies\n        alignments:\n\n        RETURNS\n        -------\n        mel_outputs:\n        gate_outpust: gate output energies\n        alignments:\n        """"""\n        # (T_out, B) -> (B, T_out)\n        alignments = torch.stack(alignments).transpose(0, 1)\n        # (T_out, B) -> (B, T_out)\n        gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n        gate_outputs = gate_outputs.contiguous()\n        # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n        # decouple frames per step\n        mel_outputs = mel_outputs.view(\n            mel_outputs.size(0), -1, self.n_mel_channels)\n        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n        mel_outputs = mel_outputs.transpose(1, 2)\n\n        return mel_outputs, gate_outputs, alignments\n\n    def decode(self, decoder_input):\n        """""" Decoder step using stored states, attention and memory\n        PARAMS\n        ------\n        decoder_input: previous mel output\n\n        RETURNS\n        -------\n        mel_output:\n        gate_output: gate output energies\n        attention_weights:\n        """"""\n        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n        self.attention_hidden, self.attention_cell = self.attention_rnn(\n            cell_input, (self.attention_hidden, self.attention_cell))\n        self.attention_hidden = F.dropout(\n            self.attention_hidden, self.p_attention_dropout, self.training)\n\n        attention_weights_cat = torch.cat(\n            (self.attention_weights.unsqueeze(1),\n             self.attention_weights_cum.unsqueeze(1)), dim=1)\n        self.attention_context, self.attention_weights = self.attention_layer(\n            self.attention_hidden, self.memory, self.processed_memory,\n            attention_weights_cat, self.mask)\n\n        self.attention_weights_cum += self.attention_weights\n        decoder_input = torch.cat(\n            (self.attention_hidden, self.attention_context), -1)\n        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n            decoder_input, (self.decoder_hidden, self.decoder_cell))\n        self.decoder_hidden = F.dropout(\n            self.decoder_hidden, self.p_decoder_dropout, self.training)\n\n        decoder_hidden_attention_context = torch.cat(\n            (self.decoder_hidden, self.attention_context), dim=1)\n        decoder_output = self.linear_projection(\n            decoder_hidden_attention_context)\n\n        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n        return decoder_output, gate_prediction, self.attention_weights\n\n    def forward(self, memory, decoder_inputs, memory_lengths):\n        """""" Decoder forward pass for training\n        PARAMS\n        ------\n        memory: Encoder outputs\n        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n        memory_lengths: Encoder output lengths for attention masking.\n\n        RETURNS\n        -------\n        mel_outputs: mel outputs from the decoder\n        gate_outputs: gate outputs from the decoder\n        alignments: sequence of attention weights from the decoder\n        """"""\n\n        decoder_input = self.get_go_frame(memory).unsqueeze(0)\n        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n        decoder_inputs = self.prenet(decoder_inputs)\n\n        self.initialize_decoder_states(\n            memory, mask=~get_mask_from_lengths(memory_lengths))\n\n        mel_outputs, gate_outputs, alignments = [], [], []\n        while len(mel_outputs) < decoder_inputs.size(0) - 1:\n            decoder_input = decoder_inputs[len(mel_outputs)]\n            mel_output, gate_output, attention_weights = self.decode(\n                decoder_input)\n            mel_outputs += [mel_output.squeeze(1)]\n            gate_outputs += [gate_output.squeeze().unsqueeze(0)]\n            alignments += [attention_weights]\n\n        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n            mel_outputs, gate_outputs, alignments)\n\n        return mel_outputs, gate_outputs, alignments\n\n    def inference(self, memory):\n        """""" Decoder inference\n        PARAMS\n        ------\n        memory: Encoder outputs\n\n        RETURNS\n        -------\n        mel_outputs: mel outputs from the decoder\n        gate_outputs: gate outputs from the decoder\n        alignments: sequence of attention weights from the decoder\n        """"""\n        decoder_input = self.get_go_frame(memory)\n\n        self.initialize_decoder_states(memory, mask=None)\n\n        mel_outputs, gate_outputs, alignments = [], [], []\n        while True:\n            decoder_input = self.prenet(decoder_input)\n            mel_output, gate_output, alignment = self.decode(decoder_input)\n\n            mel_outputs += [mel_output.squeeze(1)]\n            gate_outputs += [gate_output]\n            alignments += [alignment]\n\n            if torch.sigmoid(gate_output.data) > self.gate_threshold:\n                break\n            elif len(mel_outputs) == self.max_decoder_steps:\n                # print(""Warning! Reached max decoder steps"")\n                break\n\n            decoder_input = mel_output\n\n        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n            mel_outputs, gate_outputs, alignments)\n\n        return mel_outputs, gate_outputs, alignments\n\n\nclass Tacotron2(nn.Module):\n    def __init__(self, hparams):\n        super(Tacotron2, self).__init__()\n        self.mask_padding = hparams.mask_padding\n        self.fp16_run = hparams.fp16_run\n        self.n_mel_channels = hparams.n_mel_channels\n        self.n_frames_per_step = hparams.n_frames_per_step\n        self.embedding = nn.Embedding(\n            hparams.n_symbols, hparams.symbols_embedding_dim)\n        std = sqrt(2.0 / (hparams.n_symbols + hparams.symbols_embedding_dim))\n        val = sqrt(3.0) * std  # uniform bounds for std\n        self.embedding.weight.data.uniform_(-val, val)\n        self.encoder = Encoder(hparams)\n        self.decoder = Decoder(hparams)\n        self.postnet = Postnet(hparams)\n\n    def parse_batch(self, batch):\n        text_padded, input_lengths, mel_padded, gate_padded, \\\n            output_lengths = batch\n        text_padded = to_gpu(text_padded).long()\n        input_lengths = to_gpu(input_lengths).long()\n        max_len = torch.max(input_lengths.data).item()\n        mel_padded = to_gpu(mel_padded).float()\n        gate_padded = to_gpu(gate_padded).float()\n        output_lengths = to_gpu(output_lengths).long()\n\n        return (\n            (text_padded, input_lengths, mel_padded, max_len, output_lengths),\n            (mel_padded, gate_padded))\n\n    def parse_output(self, outputs, output_lengths=None):\n        if self.mask_padding and output_lengths is not None:\n            mask = ~get_mask_from_lengths(output_lengths)\n            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n            mask = mask.permute(1, 0, 2)\n\n            outputs[0].data.masked_fill_(mask, 0.0)\n            outputs[1].data.masked_fill_(mask, 0.0)\n            outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n\n        return outputs\n\n    def forward(self, inputs):\n        text_inputs, text_lengths, mels, max_len, output_lengths = inputs\n        text_lengths, output_lengths = text_lengths.data, output_lengths.data\n\n        embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n\n        encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n\n        mel_outputs, gate_outputs, alignments = self.decoder(\n            encoder_outputs, mels, memory_lengths=text_lengths)\n\n        mel_outputs_postnet = self.postnet(mel_outputs)\n        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n\n        return self.parse_output(\n            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n            output_lengths), encoder_outputs\n\n    def inference(self, inputs):\n        embedded_inputs = self.embedding(inputs).transpose(1, 2)\n        encoder_outputs = self.encoder.inference(embedded_inputs)\n        mel_outputs, gate_outputs, alignments = self.decoder.inference(\n            encoder_outputs)\n\n        mel_outputs_postnet = self.postnet(mel_outputs)\n        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n\n        outputs = self.parse_output(\n            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n\n        return outputs, encoder_outputs\n'"
tacotron2/utils.py,5,"b'import numpy as np\nfrom scipy.io.wavfile import read\nimport torch\n\n\ndef get_mask_from_lengths(lengths):\n    max_len = torch.max(lengths).item()\n    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n    mask = (ids < lengths.unsqueeze(1)).byte()\n    return mask\n\n\ndef load_wav_to_torch(full_path):\n    sampling_rate, data = read(full_path)\n    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n\n\ndef load_filepaths_and_text(filename, split=""|""):\n    with open(filename, encoding=\'utf-8\') as f:\n        filepaths_and_text = [line.strip().split(split) for line in f]\n    return filepaths_and_text\n\n\ndef to_gpu(x):\n    x = x.contiguous()\n\n    if torch.cuda.is_available():\n        x = x.cuda(non_blocking=True)\n    return torch.autograd.Variable(x)\n'"
text/__init__.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\nimport re\nfrom text import cleaners\nfrom text.symbols import symbols\n\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r\'(.*?)\\{(.+?)\\}(.*)\')\n\n\ndef text_to_sequence(text, cleaner_names):\n    \'\'\'Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n      The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n      in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n      Args:\n        text: string to convert to a sequence\n        cleaner_names: names of the cleaner functions to run the text through\n\n      Returns:\n        List of integers corresponding to the symbols in the text\n    \'\'\'\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(\n            _clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    return sequence\n\n\ndef sequence_to_text(sequence):\n    \'\'\'Converts a sequence of IDs back to a string\'\'\'\n    result = \'\'\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \'@\':\n                s = \'{%s}\' % s[1:]\n            result += s\n    return result.replace(\'}{\', \' \')\n\n\ndef _clean_text(text, cleaner_names):\n    for name in cleaner_names:\n        cleaner = getattr(cleaners, name)\n        if not cleaner:\n            raise Exception(\'Unknown cleaner: %s\' % name)\n        text = cleaner(text)\n    return text\n\n\ndef _symbols_to_sequence(symbols):\n    return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n    return _symbols_to_sequence([\'@\' + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n    return s in _symbol_to_id and s is not \'_\' and s is not \'~\'\n'"
text/cleaners.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\n\n\'\'\'\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You\'ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\'\'\'\n\n\n# Regular expression matching whitespace:\nimport re\nfrom unidecode import unidecode\nfrom .numbers import normalize_numbers\n_whitespace_re = re.compile(r\'\\s+\')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\'\\\\b%s\\\\.\' % x[0], re.IGNORECASE), x[1]) for x in [\n    (\'mrs\', \'misess\'),\n    (\'mr\', \'mister\'),\n    (\'dr\', \'doctor\'),\n    (\'st\', \'saint\'),\n    (\'co\', \'company\'),\n    (\'jr\', \'junior\'),\n    (\'maj\', \'major\'),\n    (\'gen\', \'general\'),\n    (\'drs\', \'doctors\'),\n    (\'rev\', \'reverend\'),\n    (\'lt\', \'lieutenant\'),\n    (\'hon\', \'honorable\'),\n    (\'sgt\', \'sergeant\'),\n    (\'capt\', \'captain\'),\n    (\'esq\', \'esquire\'),\n    (\'ltd\', \'limited\'),\n    (\'col\', \'colonel\'),\n    (\'ft\', \'fort\'),\n]]\n\n\ndef expand_abbreviations(text):\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef expand_numbers(text):\n    return normalize_numbers(text)\n\n\ndef lowercase(text):\n    return text.lower()\n\n\ndef collapse_whitespace(text):\n    return re.sub(_whitespace_re, \' \', text)\n\n\ndef convert_to_ascii(text):\n    return unidecode(text)\n\n\ndef basic_cleaners(text):\n    \'\'\'Basic pipeline that lowercases and collapses whitespace without transliteration.\'\'\'\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef transliteration_cleaners(text):\n    \'\'\'Pipeline for non-English text that transliterates to ASCII.\'\'\'\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef english_cleaners(text):\n    \'\'\'Pipeline for English text, including number and abbreviation expansion.\'\'\'\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n'"
text/cmudict.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\n\nimport re\n\n\nvalid_symbols = [\n    \'AA\', \'AA0\', \'AA1\', \'AA2\', \'AE\', \'AE0\', \'AE1\', \'AE2\', \'AH\', \'AH0\', \'AH1\', \'AH2\',\n    \'AO\', \'AO0\', \'AO1\', \'AO2\', \'AW\', \'AW0\', \'AW1\', \'AW2\', \'AY\', \'AY0\', \'AY1\', \'AY2\',\n    \'B\', \'CH\', \'D\', \'DH\', \'EH\', \'EH0\', \'EH1\', \'EH2\', \'ER\', \'ER0\', \'ER1\', \'ER2\', \'EY\',\n    \'EY0\', \'EY1\', \'EY2\', \'F\', \'G\', \'HH\', \'IH\', \'IH0\', \'IH1\', \'IH2\', \'IY\', \'IY0\', \'IY1\',\n    \'IY2\', \'JH\', \'K\', \'L\', \'M\', \'N\', \'NG\', \'OW\', \'OW0\', \'OW1\', \'OW2\', \'OY\', \'OY0\',\n    \'OY1\', \'OY2\', \'P\', \'R\', \'S\', \'SH\', \'T\', \'TH\', \'UH\', \'UH0\', \'UH1\', \'UH2\', \'UW\',\n    \'UW0\', \'UW1\', \'UW2\', \'V\', \'W\', \'Y\', \'Z\', \'ZH\'\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n    \'\'\'Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\'\'\'\n\n    def __init__(self, file_or_path, keep_ambiguous=True):\n        if isinstance(file_or_path, str):\n            with open(file_or_path, encoding=\'latin-1\') as f:\n                entries = _parse_cmudict(f)\n        else:\n            entries = _parse_cmudict(file_or_path)\n        if not keep_ambiguous:\n            entries = {word: pron for word,\n                       pron in entries.items() if len(pron) == 1}\n        self._entries = entries\n\n    def __len__(self):\n        return len(self._entries)\n\n    def lookup(self, word):\n        \'\'\'Returns list of ARPAbet pronunciations of the given word.\'\'\'\n        return self._entries.get(word.upper())\n\n\n_alt_re = re.compile(r\'\\([0-9]+\\)\')\n\n\ndef _parse_cmudict(file):\n    cmudict = {}\n    for line in file:\n        if len(line) and (line[0] >= \'A\' and line[0] <= \'Z\' or line[0] == ""\'""):\n            parts = line.split(\'  \')\n            word = re.sub(_alt_re, \'\', parts[0])\n            pronunciation = _get_pronunciation(parts[1])\n            if pronunciation:\n                if word in cmudict:\n                    cmudict[word].append(pronunciation)\n                else:\n                    cmudict[word] = [pronunciation]\n    return cmudict\n\n\ndef _get_pronunciation(s):\n    parts = s.strip().split(\' \')\n    for part in parts:\n        if part not in _valid_symbol_set:\n            return None\n    return \' \'.join(parts)\n'"
text/numbers.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\n\nimport inflect\nimport re\n\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r\'([0-9][0-9\\,]+[0-9])\')\n_decimal_number_re = re.compile(r\'([0-9]+\\.[0-9]+)\')\n_pounds_re = re.compile(r\'\xc2\xa3([0-9\\,]*[0-9]+)\')\n_dollars_re = re.compile(r\'\\$([0-9\\.\\,]*[0-9]+)\')\n_ordinal_re = re.compile(r\'[0-9]+(st|nd|rd|th)\')\n_number_re = re.compile(r\'[0-9]+\')\n\n\ndef _remove_commas(m):\n    return m.group(1).replace(\',\', \'\')\n\n\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\'.\', \' point \')\n\n\ndef _expand_dollars(m):\n    match = m.group(1)\n    parts = match.split(\'.\')\n    if len(parts) > 2:\n        return match + \' dollars\'  # Unexpected format\n    dollars = int(parts[0]) if parts[0] else 0\n    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n    if dollars and cents:\n        dollar_unit = \'dollar\' if dollars == 1 else \'dollars\'\n        cent_unit = \'cent\' if cents == 1 else \'cents\'\n        return \'%s %s, %s %s\' % (dollars, dollar_unit, cents, cent_unit)\n    elif dollars:\n        dollar_unit = \'dollar\' if dollars == 1 else \'dollars\'\n        return \'%s %s\' % (dollars, dollar_unit)\n    elif cents:\n        cent_unit = \'cent\' if cents == 1 else \'cents\'\n        return \'%s %s\' % (cents, cent_unit)\n    else:\n        return \'zero dollars\'\n\n\ndef _expand_ordinal(m):\n    return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n    num = int(m.group(0))\n    if num > 1000 and num < 3000:\n        if num == 2000:\n            return \'two thousand\'\n        elif num > 2000 and num < 2010:\n            return \'two thousand \' + _inflect.number_to_words(num % 100)\n        elif num % 100 == 0:\n            return _inflect.number_to_words(num // 100) + \' hundred\'\n        else:\n            return _inflect.number_to_words(num, andword=\'\', zero=\'oh\', group=2).replace(\', \', \' \')\n    else:\n        return _inflect.number_to_words(num, andword=\'\')\n\n\ndef normalize_numbers(text):\n    text = re.sub(_comma_number_re, _remove_commas, text)\n    text = re.sub(_pounds_re, r\'\\1 pounds\', text)\n    text = re.sub(_dollars_re, _expand_dollars, text)\n    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n    text = re.sub(_ordinal_re, _expand_ordinal, text)\n    text = re.sub(_number_re, _expand_number, text)\n    return text\n'"
text/symbols.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\n\n\'\'\'\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run through Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details. \'\'\'\n\nfrom text import cmudict\n_pad        = \'_\'\n_punctuation = \'!\\\'(),.:;? \'\n_special = \'-\'\n_letters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\'\n\n# Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\'@\' + s for s in cmudict.valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad] + list(_special) + list(_punctuation) + \\\n    list(_letters) + _arpabet\n'"
transformer/Beam.py,5,"b'import torch\nimport numpy as np\nimport transformer.Constants as Constants\n\n\nclass Beam():\n    \'\'\' Beam search \'\'\'\n\n    def __init__(self, size, device=False):\n\n        self.size = size\n        self._done = False\n\n        # The score for each translation on the beam.\n        self.scores = torch.zeros((size,), dtype=torch.float, device=device)\n        self.all_scores = []\n\n        # The backpointers at each time-step.\n        self.prev_ks = []\n\n        # The outputs at each time-step.\n        self.next_ys = [torch.full(\n            (size,), Constants.PAD, dtype=torch.long, device=device)]\n        self.next_ys[0][0] = Constants.BOS\n\n    def get_current_state(self):\n        ""Get the outputs for the current timestep.""\n        return self.get_tentative_hypothesis()\n\n    def get_current_origin(self):\n        ""Get the backpointers for the current timestep.""\n        return self.prev_ks[-1]\n\n    @property\n    def done(self):\n        return self._done\n\n    def advance(self, word_prob):\n        ""Update beam status and check if finished or not.""\n        num_words = word_prob.size(1)\n\n        # Sum the previous scores.\n        if len(self.prev_ks) > 0:\n            beam_lk = word_prob + self.scores.unsqueeze(1).expand_as(word_prob)\n        else:\n            beam_lk = word_prob[0]\n\n        flat_beam_lk = beam_lk.view(-1)\n\n        best_scores, best_scores_id = flat_beam_lk.topk(\n            self.size, 0, True, True)  # 1st sort\n        best_scores, best_scores_id = flat_beam_lk.topk(\n            self.size, 0, True, True)  # 2nd sort\n\n        self.all_scores.append(self.scores)\n        self.scores = best_scores\n\n        # bestScoresId is flattened as a (beam x word) array,\n        # so we need to calculate which word and beam each score came from\n        prev_k = best_scores_id / num_words\n        self.prev_ks.append(prev_k)\n        self.next_ys.append(best_scores_id - prev_k * num_words)\n\n        # End condition is when top-of-beam is EOS.\n        if self.next_ys[-1][0].item() == Constants.EOS:\n            self._done = True\n            self.all_scores.append(self.scores)\n\n        return self._done\n\n    def sort_scores(self):\n        ""Sort the scores.""\n        return torch.sort(self.scores, 0, True)\n\n    def get_the_best_score_and_idx(self):\n        ""Get the score of the best in the beam.""\n        scores, ids = self.sort_scores()\n        return scores[1], ids[1]\n\n    def get_tentative_hypothesis(self):\n        ""Get the decoded sequence for the current timestep.""\n\n        if len(self.next_ys) == 1:\n            dec_seq = self.next_ys[0].unsqueeze(1)\n        else:\n            _, keys = self.sort_scores()\n            hyps = [self.get_hypothesis(k) for k in keys]\n            hyps = [[Constants.BOS] + h for h in hyps]\n            dec_seq = torch.LongTensor(hyps)\n\n        return dec_seq\n\n    def get_hypothesis(self, k):\n        """""" Walk back to construct the full hypothesis. """"""\n        hyp = []\n        for j in range(len(self.prev_ks) - 1, -1, -1):\n            hyp.append(self.next_ys[j+1][k])\n            k = self.prev_ks[j][k]\n\n        return list(map(lambda x: x.item(), hyp[::-1]))\n'"
transformer/Constants.py,0,"b""PAD = 0\nUNK = 1\nBOS = 2\nEOS = 3\n\nPAD_WORD = '<blank>'\nUNK_WORD = '<unk>'\nBOS_WORD = '<s>'\nEOS_WORD = '</s>'\n"""
transformer/Layers.py,8,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport numpy as np\nfrom collections import OrderedDict\n\nfrom transformer.SubLayers import MultiHeadAttention, PositionwiseFeedForward\nfrom text.symbols import symbols\n\n\nclass Linear(nn.Module):\n    """"""\n    Linear Module\n    """"""\n\n    def __init__(self, in_dim, out_dim, bias=True, w_init=\'linear\'):\n        """"""\n        :param in_dim: dimension of input\n        :param out_dim: dimension of output\n        :param bias: boolean. if True, bias is included.\n        :param w_init: str. weight inits with xavier initialization.\n        """"""\n        super(Linear, self).__init__()\n        self.linear_layer = nn.Linear(in_dim, out_dim, bias=bias)\n\n        nn.init.xavier_uniform_(\n            self.linear_layer.weight,\n            gain=nn.init.calculate_gain(w_init))\n\n    def forward(self, x):\n        return self.linear_layer(x)\n\n\nclass PreNet(nn.Module):\n    """"""\n    Pre Net before passing through the network\n    """"""\n\n    def __init__(self, input_size, hidden_size, output_size, p=0.5):\n        """"""\n        :param input_size: dimension of input\n        :param hidden_size: dimension of hidden unit\n        :param output_size: dimension of output\n        """"""\n        super(PreNet, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.layer = nn.Sequential(OrderedDict([\n            (\'fc1\', Linear(self.input_size, self.hidden_size)),\n            (\'relu1\', nn.ReLU()),\n            (\'dropout1\', nn.Dropout(p)),\n            (\'fc2\', Linear(self.hidden_size, self.output_size)),\n            (\'relu2\', nn.ReLU()),\n            (\'dropout2\', nn.Dropout(p)),\n        ]))\n\n    def forward(self, input_):\n\n        out = self.layer(input_)\n\n        return out\n\n\nclass Conv(nn.Module):\n    """"""\n    Convolution Module\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=1,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 bias=True,\n                 w_init=\'linear\'):\n        """"""\n        :param in_channels: dimension of input\n        :param out_channels: dimension of output\n        :param kernel_size: size of kernel\n        :param stride: size of stride\n        :param padding: size of padding\n        :param dilation: dilation rate\n        :param bias: boolean. if True, bias is included.\n        :param w_init: str. weight inits with xavier initialization.\n        """"""\n        super(Conv, self).__init__()\n\n        self.conv = nn.Conv1d(in_channels,\n                              out_channels,\n                              kernel_size=kernel_size,\n                              stride=stride,\n                              padding=padding,\n                              dilation=dilation,\n                              bias=bias)\n\n        nn.init.xavier_uniform_(\n            self.conv.weight, gain=nn.init.calculate_gain(w_init))\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass FFTBlock(torch.nn.Module):\n    """"""FFT Block""""""\n\n    def __init__(self,\n                 d_model,\n                 d_inner,\n                 n_head,\n                 d_k,\n                 d_v,\n                 dropout=0.1):\n        super(FFTBlock, self).__init__()\n        self.slf_attn = MultiHeadAttention(\n            n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(\n            d_model, d_inner, dropout=dropout)\n\n    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n        enc_output, enc_slf_attn = self.slf_attn(\n            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n        enc_output *= non_pad_mask\n\n        enc_output = self.pos_ffn(enc_output)\n        enc_output *= non_pad_mask\n\n        return enc_output, enc_slf_attn\n\n\nclass ConvNorm(torch.nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=1,\n                 stride=1,\n                 padding=None,\n                 dilation=1,\n                 bias=True,\n                 w_init_gain=\'linear\'):\n        super(ConvNorm, self).__init__()\n\n        if padding is None:\n            assert(kernel_size % 2 == 1)\n            padding = int(dilation * (kernel_size - 1) / 2)\n\n        self.conv = torch.nn.Conv1d(in_channels,\n                                    out_channels,\n                                    kernel_size=kernel_size,\n                                    stride=stride,\n                                    padding=padding,\n                                    dilation=dilation,\n                                    bias=bias)\n\n        torch.nn.init.xavier_uniform_(\n            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n\n    def forward(self, signal):\n        conv_signal = self.conv(signal)\n\n        return conv_signal\n\n\nclass PostNet(nn.Module):\n    """"""\n    PostNet: Five 1-d convolution with 512 channels and kernel size 5\n    """"""\n\n    def __init__(self,\n                 n_mel_channels=80,\n                 postnet_embedding_dim=512,\n                 postnet_kernel_size=5,\n                 postnet_n_convolutions=5):\n\n        super(PostNet, self).__init__()\n        self.convolutions = nn.ModuleList()\n\n        self.convolutions.append(\n            nn.Sequential(\n                ConvNorm(n_mel_channels,\n                         postnet_embedding_dim,\n                         kernel_size=postnet_kernel_size,\n                         stride=1,\n                         padding=int((postnet_kernel_size - 1) / 2),\n                         dilation=1,\n                         w_init_gain=\'tanh\'),\n\n                nn.BatchNorm1d(postnet_embedding_dim))\n        )\n\n        for i in range(1, postnet_n_convolutions - 1):\n            self.convolutions.append(\n                nn.Sequential(\n                    ConvNorm(postnet_embedding_dim,\n                             postnet_embedding_dim,\n                             kernel_size=postnet_kernel_size,\n                             stride=1,\n                             padding=int((postnet_kernel_size - 1) / 2),\n                             dilation=1,\n                             w_init_gain=\'tanh\'),\n\n                    nn.BatchNorm1d(postnet_embedding_dim))\n            )\n\n        self.convolutions.append(\n            nn.Sequential(\n                ConvNorm(postnet_embedding_dim,\n                         n_mel_channels,\n                         kernel_size=postnet_kernel_size,\n                         stride=1,\n                         padding=int((postnet_kernel_size - 1) / 2),\n                         dilation=1,\n                         w_init_gain=\'linear\'),\n\n                nn.BatchNorm1d(n_mel_channels))\n        )\n\n    def forward(self, x):\n        x = x.contiguous().transpose(1, 2)\n\n        for i in range(len(self.convolutions) - 1):\n            x = F.dropout(torch.tanh(\n                self.convolutions[i](x)), 0.5, self.training)\n        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n\n        x = x.contiguous().transpose(1, 2)\n        return x\n'"
transformer/Models.py,3,"b'import torch\nimport torch.nn as nn\nimport numpy as np\n\nimport transformer.Constants as Constants\nfrom transformer.Layers import FFTBlock, PreNet, PostNet, Linear\nfrom text.symbols import symbols\nimport hparams as hp\n\n\ndef get_non_pad_mask(seq):\n    assert seq.dim() == 2\n    return seq.ne(Constants.PAD).type(torch.float).unsqueeze(-1)\n\n\ndef get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None):\n    \'\'\' Sinusoid position encoding table \'\'\'\n\n    def cal_angle(position, hid_idx):\n        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n\n    def get_posi_angle_vec(position):\n        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n\n    sinusoid_table = np.array([get_posi_angle_vec(pos_i)\n                               for pos_i in range(n_position)])\n\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    if padding_idx is not None:\n        # zero vector for padding dimension\n        sinusoid_table[padding_idx] = 0.\n\n    return torch.FloatTensor(sinusoid_table)\n\n\ndef get_attn_key_pad_mask(seq_k, seq_q):\n    \'\'\' For masking out the padding part of key sequence. \'\'\'\n\n    # Expand to fit the shape of key query attention matrix.\n    len_q = seq_q.size(1)\n    padding_mask = seq_k.eq(Constants.PAD)\n    padding_mask = padding_mask.unsqueeze(\n        1).expand(-1, len_q, -1)  # b x lq x lk\n\n    return padding_mask\n\n\nclass Encoder(nn.Module):\n    \'\'\' Encoder \'\'\'\n\n    def __init__(self,\n                 n_src_vocab=len(symbols)+1,\n                 len_max_seq=hp.max_sep_len,\n                 d_word_vec=hp.word_vec_dim,\n                 n_layers=hp.encoder_n_layer,\n                 n_head=hp.encoder_head,\n                 d_k=64,\n                 d_v=64,\n                 d_model=hp.word_vec_dim,\n                 d_inner=hp.encoder_conv1d_filter_size,\n                 dropout=hp.dropout):\n\n        super(Encoder, self).__init__()\n\n        n_position = len_max_seq + 1\n\n        self.src_word_emb = nn.Embedding(\n            n_src_vocab, d_word_vec, padding_idx=Constants.PAD)\n\n        self.position_enc = nn.Embedding.from_pretrained(\n            get_sinusoid_encoding_table(n_position, d_word_vec, padding_idx=0),\n            freeze=True)\n\n        self.layer_stack = nn.ModuleList([FFTBlock(\n            d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers)])\n\n    def forward(self, src_seq, src_pos, return_attns=False):\n\n        enc_slf_attn_list = []\n\n        # -- Prepare masks\n        slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)\n        non_pad_mask = get_non_pad_mask(src_seq)\n\n        # -- Forward\n        enc_output = self.src_word_emb(src_seq) + self.position_enc(src_pos)\n\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(\n                enc_output,\n                non_pad_mask=non_pad_mask,\n                slf_attn_mask=slf_attn_mask)\n            if return_attns:\n                enc_slf_attn_list += [enc_slf_attn]\n\n        return enc_output, non_pad_mask\n\n\nclass Decoder(nn.Module):\n    """""" Decoder """"""\n\n    def __init__(self,\n                 len_max_seq=hp.max_sep_len,\n                 d_word_vec=hp.word_vec_dim,\n                 n_layers=hp.decoder_n_layer,\n                 n_head=hp.decoder_head,\n                 d_k=64,\n                 d_v=64,\n                 d_model=hp.word_vec_dim,\n                 d_inner=hp.decoder_conv1d_filter_size,\n                 dropout=hp.dropout):\n\n        super(Decoder, self).__init__()\n\n        n_position = len_max_seq + 1\n\n        self.position_enc = nn.Embedding.from_pretrained(\n            get_sinusoid_encoding_table(n_position, d_word_vec, padding_idx=0),\n            freeze=True)\n\n        self.layer_stack = nn.ModuleList([FFTBlock(\n            d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers)])\n\n    def forward(self, enc_seq, enc_pos, return_attns=False):\n\n        dec_slf_attn_list = []\n\n        # -- Prepare masks\n        slf_attn_mask = get_attn_key_pad_mask(seq_k=enc_pos, seq_q=enc_pos)\n        non_pad_mask = get_non_pad_mask(enc_pos)\n\n        # -- Forward\n        dec_output = enc_seq + self.position_enc(enc_pos)\n\n        for dec_layer in self.layer_stack:\n            dec_output, dec_slf_attn = dec_layer(\n                dec_output,\n                non_pad_mask=non_pad_mask,\n                slf_attn_mask=slf_attn_mask)\n            if return_attns:\n                dec_slf_attn_list += [dec_slf_attn]\n\n        return dec_output\n'"
transformer/Modules.py,3,"b""import torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass ScaledDotProductAttention(nn.Module):\n    ''' Scaled Dot-Product Attention '''\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.bmm(q, k.transpose(1, 2))\n        attn = attn / self.temperature\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -np.inf)\n\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n\n        return output, attn\n"""
transformer/SubLayers.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom transformer.Modules import ScaledDotProductAttention\nimport hparams as hp\n\n\nclass MultiHeadAttention(nn.Module):\n    ''' Multi-Head Attention module '''\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k)\n        self.w_ks = nn.Linear(d_model, n_head * d_k)\n        self.w_vs = nn.Linear(d_model, n_head * d_v)\n        nn.init.normal_(self.w_qs.weight, mean=0,\n                        std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_ks.weight, mean=0,\n                        std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_vs.weight, mean=0,\n                        std=np.sqrt(2.0 / (d_model + d_v)))\n\n        self.attention = ScaledDotProductAttention(\n            temperature=np.power(d_k, 0.5))\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.fc = nn.Linear(n_head * d_v, d_model)\n        nn.init.xavier_normal_(self.fc.weight)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, q, k, v, mask=None):\n\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n\n        sz_b, len_q, _ = q.size()\n        sz_b, len_k, _ = k.size()\n        sz_b, len_v, _ = v.size()\n\n        residual = q\n\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        q = q.permute(2, 0, 1, 3).contiguous().view(-1,\n                                                    len_q, d_k)  # (n*b) x lq x dk\n        k = k.permute(2, 0, 1, 3).contiguous().view(-1,\n                                                    len_k, d_k)  # (n*b) x lk x dk\n        v = v.permute(2, 0, 1, 3).contiguous().view(-1,\n                                                    len_v, d_v)  # (n*b) x lv x dv\n\n        mask = mask.repeat(n_head, 1, 1)  # (n*b) x .. x ..\n        output, attn = self.attention(q, k, v, mask=mask)\n\n        output = output.view(n_head, sz_b, len_q, d_v)\n        output = output.permute(1, 2, 0, 3).contiguous().view(\n            sz_b, len_q, -1)  # b x lq x (n*dv)\n\n        output = self.dropout(self.fc(output))\n        output = self.layer_norm(output + residual)\n\n        return output, attn\n\n\nclass PositionwiseFeedForward(nn.Module):\n    ''' A two-feed-forward-layer module '''\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n\n        # Use Conv1D\n        # position-wise\n        self.w_1 = nn.Conv1d(\n            d_in, d_hid, kernel_size=hp.fft_conv1d_kernel, padding=hp.fft_conv1d_padding)\n        # position-wise\n        self.w_2 = nn.Conv1d(\n            d_hid, d_in, kernel_size=hp.fft_conv1d_kernel, padding=hp.fft_conv1d_padding)\n\n        self.layer_norm = nn.LayerNorm(d_in)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        residual = x\n        output = x.transpose(1, 2)\n        output = self.w_2(F.relu(self.w_1(output)))\n        output = output.transpose(1, 2)\n        output = self.dropout(output)\n        output = self.layer_norm(output + residual)\n\n        return output\n"""
transformer/__init__.py,0,b'import transformer.Constants\nimport transformer.Modules\nimport transformer.Layers\nimport transformer.SubLayers\nimport transformer.Models\nimport transformer.Beam\n'
waveglow/__init__.py,0,b'import waveglow.inference\nimport waveglow.mel2samp\nimport waveglow.glow\n'
waveglow/convert_model.py,11,"b""import sys\nimport copy\nimport torch\n\ndef _check_model_old_version(model):\n    if hasattr(model.WN[0], 'res_layers'):\n        return True\n    else:\n        return False\n\ndef update_model(old_model):\n    if not _check_model_old_version(old_model):\n        return old_model\n    new_model = copy.deepcopy(old_model)\n    for idx in range(0, len(new_model.WN)):\n        wavenet = new_model.WN[idx]\n        wavenet.res_skip_layers = torch.nn.ModuleList()\n        n_channels = wavenet.n_channels\n        n_layers = wavenet.n_layers\n        for i in range(0, n_layers):\n            if i < n_layers - 1:\n                res_skip_channels = 2*n_channels\n            else:\n                res_skip_channels = n_channels\n            res_skip_layer = torch.nn.Conv1d(n_channels, res_skip_channels, 1)\n            skip_layer = torch.nn.utils.remove_weight_norm(wavenet.skip_layers[i])\n            if i < n_layers - 1:\n                res_layer = torch.nn.utils.remove_weight_norm(wavenet.res_layers[i])\n                res_skip_layer.weight = torch.nn.Parameter(torch.cat([res_layer.weight, skip_layer.weight]))\n                res_skip_layer.bias = torch.nn.Parameter(torch.cat([res_layer.bias, skip_layer.bias]))\n            else:\n                res_skip_layer.weight = torch.nn.Parameter(skip_layer.weight)\n                res_skip_layer.bias = torch.nn.Parameter(skip_layer.bias)\n            res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')\n            wavenet.res_skip_layers.append(res_skip_layer)\n        del wavenet.res_layers\n        del wavenet.skip_layers\n    return new_model\n\nif __name__ == '__main__':\n    old_model_path = sys.argv[1]\n    new_model_path = sys.argv[2]\n    model = torch.load(old_model_path)\n    model['model'] = update_model(model['model'])\n    torch.save(model, new_model_path)\n    \n"""
waveglow/glow.py,49,"b'# *****************************************************************************\n#  Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions are met:\n#      * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n#      * Redistributions in binary form must reproduce the above copyright\n#        notice, this list of conditions and the following disclaimer in the\n#        documentation and/or other materials provided with the distribution.\n#      * Neither the name of the NVIDIA CORPORATION nor the\n#        names of its contributors may be used to endorse or promote products\n#        derived from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND\n#  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n#  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n#  DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n# *****************************************************************************\nimport copy\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a+input_b\n    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act\n    return acts\n\n\nclass WaveGlowLoss(torch.nn.Module):\n    def __init__(self, sigma=1.0):\n        super(WaveGlowLoss, self).__init__()\n        self.sigma = sigma\n\n    def forward(self, model_output):\n        z, log_s_list, log_det_W_list = model_output\n        for i, log_s in enumerate(log_s_list):\n            if i == 0:\n                log_s_total = torch.sum(log_s)\n                log_det_W_total = log_det_W_list[i]\n            else:\n                log_s_total = log_s_total + torch.sum(log_s)\n                log_det_W_total += log_det_W_list[i]\n\n        loss = torch.sum(z*z)/(2*self.sigma*self.sigma) - log_s_total - log_det_W_total\n        return loss/(z.size(0)*z.size(1)*z.size(2))\n\n\nclass Invertible1x1Conv(torch.nn.Module):\n    """"""\n    The layer outputs both the convolution, and the log determinant\n    of its weight matrix.  If reverse=True it does convolution with\n    inverse\n    """"""\n    def __init__(self, c):\n        super(Invertible1x1Conv, self).__init__()\n        self.conv = torch.nn.Conv1d(c, c, kernel_size=1, stride=1, padding=0,\n                                    bias=False)\n\n        # Sample a random orthonormal matrix to initialize weights\n        W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n\n        # Ensure determinant is 1.0 not -1.0\n        if torch.det(W) < 0:\n            W[:,0] = -1*W[:,0]\n        W = W.view(c, c, 1)\n        self.conv.weight.data = W\n\n    def forward(self, z, reverse=False):\n        # shape\n        batch_size, group_size, n_of_groups = z.size()\n\n        W = self.conv.weight.squeeze()\n\n        if reverse:\n            if not hasattr(self, \'W_inverse\'):\n                # Reverse computation\n                W_inverse = W.float().inverse()\n                W_inverse = Variable(W_inverse[..., None])\n                if z.type() == \'torch.cuda.HalfTensor\':\n                    W_inverse = W_inverse.half()\n                self.W_inverse = W_inverse\n            z = F.conv1d(z, self.W_inverse, bias=None, stride=1, padding=0)\n            return z\n        else:\n            # Forward computation\n            log_det_W = batch_size * n_of_groups * torch.logdet(W)\n            z = self.conv(z)\n            return z, log_det_W\n\n\nclass WN(torch.nn.Module):\n    """"""\n    This is the WaveNet like layer for the affine coupling.  The primary difference\n    from WaveNet is the convolutions need not be causal.  There is also no dilation\n    size reset.  The dilation only doubles on each layer\n    """"""\n    def __init__(self, n_in_channels, n_mel_channels, n_layers, n_channels,\n                 kernel_size):\n        super(WN, self).__init__()\n        assert(kernel_size % 2 == 1)\n        assert(n_channels % 2 == 0)\n        self.n_layers = n_layers\n        self.n_channels = n_channels\n        self.in_layers = torch.nn.ModuleList()\n        self.res_skip_layers = torch.nn.ModuleList()\n        self.cond_layers = torch.nn.ModuleList()\n\n        start = torch.nn.Conv1d(n_in_channels, n_channels, 1)\n        start = torch.nn.utils.weight_norm(start, name=\'weight\')\n        self.start = start\n\n        # Initializing last layer to 0 makes the affine coupling layers\n        # do nothing at first.  This helps with training stability\n        end = torch.nn.Conv1d(n_channels, 2*n_in_channels, 1)\n        end.weight.data.zero_()\n        end.bias.data.zero_()\n        self.end = end\n\n        for i in range(n_layers):\n            dilation = 2 ** i\n            padding = int((kernel_size*dilation - dilation)/2)\n            in_layer = torch.nn.Conv1d(n_channels, 2*n_channels, kernel_size,\n                                       dilation=dilation, padding=padding)\n            in_layer = torch.nn.utils.weight_norm(in_layer, name=\'weight\')\n            self.in_layers.append(in_layer)\n\n            cond_layer = torch.nn.Conv1d(n_mel_channels, 2*n_channels, 1)\n            cond_layer = torch.nn.utils.weight_norm(cond_layer, name=\'weight\')\n            self.cond_layers.append(cond_layer)\n\n            # last one is not necessary\n            if i < n_layers - 1:\n                res_skip_channels = 2*n_channels\n            else:\n                res_skip_channels = n_channels\n            res_skip_layer = torch.nn.Conv1d(n_channels, res_skip_channels, 1)\n            res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name=\'weight\')\n            self.res_skip_layers.append(res_skip_layer)\n\n    def forward(self, forward_input):\n        audio, spect = forward_input\n        audio = self.start(audio)\n\n        for i in range(self.n_layers):\n            acts = fused_add_tanh_sigmoid_multiply(\n                self.in_layers[i](audio),\n                self.cond_layers[i](spect),\n                torch.IntTensor([self.n_channels]))\n\n            res_skip_acts = self.res_skip_layers[i](acts)\n            if i < self.n_layers - 1:\n                audio = res_skip_acts[:,:self.n_channels,:] + audio\n                skip_acts = res_skip_acts[:,self.n_channels:,:]\n            else:\n                skip_acts = res_skip_acts\n\n            if i == 0:\n                output = skip_acts\n            else:\n                output = skip_acts + output\n        return self.end(output)\n\n\nclass WaveGlow(torch.nn.Module):\n    def __init__(self, n_mel_channels, n_flows, n_group, n_early_every,\n                 n_early_size, WN_config):\n        super(WaveGlow, self).__init__()\n\n        self.upsample = torch.nn.ConvTranspose1d(n_mel_channels,\n                                                 n_mel_channels,\n                                                 1024, stride=256)\n        assert(n_group % 2 == 0)\n        self.n_flows = n_flows\n        self.n_group = n_group\n        self.n_early_every = n_early_every\n        self.n_early_size = n_early_size\n        self.WN = torch.nn.ModuleList()\n        self.convinv = torch.nn.ModuleList()\n\n        n_half = int(n_group/2)\n\n        # Set up layers with the right sizes based on how many dimensions\n        # have been output already\n        n_remaining_channels = n_group\n        for k in range(n_flows):\n            if k % self.n_early_every == 0 and k > 0:\n                n_half = n_half - int(self.n_early_size/2)\n                n_remaining_channels = n_remaining_channels - self.n_early_size\n            self.convinv.append(Invertible1x1Conv(n_remaining_channels))\n            self.WN.append(WN(n_half, n_mel_channels*n_group, **WN_config))\n        self.n_remaining_channels = n_remaining_channels  # Useful during inference\n\n    def forward(self, forward_input):\n        """"""\n        forward_input[0] = mel_spectrogram:  batch x n_mel_channels x frames\n        forward_input[1] = audio: batch x time\n        """"""\n        spect, audio = forward_input\n\n        #  Upsample spectrogram to size of audio\n        spect = self.upsample(spect)\n        assert(spect.size(2) >= audio.size(1))\n        if spect.size(2) > audio.size(1):\n            spect = spect[:, :, :audio.size(1)]\n\n        spect = spect.unfold(2, self.n_group, self.n_group).permute(0, 2, 1, 3)\n        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0, 2, 1)\n\n        audio = audio.unfold(1, self.n_group, self.n_group).permute(0, 2, 1)\n        output_audio = []\n        log_s_list = []\n        log_det_W_list = []\n\n        for k in range(self.n_flows):\n            if k % self.n_early_every == 0 and k > 0:\n                output_audio.append(audio[:,:self.n_early_size,:])\n                audio = audio[:,self.n_early_size:,:]\n\n            audio, log_det_W = self.convinv[k](audio)\n            log_det_W_list.append(log_det_W)\n\n            n_half = int(audio.size(1)/2)\n            audio_0 = audio[:,:n_half,:]\n            audio_1 = audio[:,n_half:,:]\n\n            output = self.WN[k]((audio_0, spect))\n            log_s = output[:, n_half:, :]\n            b = output[:, :n_half, :]\n            audio_1 = torch.exp(log_s)*audio_1 + b\n            log_s_list.append(log_s)\n\n            audio = torch.cat([audio_0, audio_1],1)\n\n        output_audio.append(audio)\n        return torch.cat(output_audio,1), log_s_list, log_det_W_list\n\n    def infer(self, spect, sigma=1.0):\n        spect = self.upsample(spect)\n        # trim conv artifacts. maybe pad spec to kernel multiple\n        time_cutoff = self.upsample.kernel_size[0] - self.upsample.stride[0]\n        spect = spect[:, :, :-time_cutoff]\n\n        spect = spect.unfold(2, self.n_group, self.n_group).permute(0, 2, 1, 3)\n        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0, 2, 1)\n\n        if spect.type() == \'torch.cuda.HalfTensor\':\n            audio = torch.cuda.HalfTensor(spect.size(0),\n                                          self.n_remaining_channels,\n                                          spect.size(2)).normal_()\n        else:\n            audio = torch.cuda.FloatTensor(spect.size(0),\n                                           self.n_remaining_channels,\n                                           spect.size(2)).normal_()\n\n        audio = torch.autograd.Variable(sigma*audio)\n\n        for k in reversed(range(self.n_flows)):\n            n_half = int(audio.size(1)/2)\n            audio_0 = audio[:,:n_half,:]\n            audio_1 = audio[:,n_half:,:]\n\n            output = self.WN[k]((audio_0, spect))\n            s = output[:, n_half:, :]\n            b = output[:, :n_half, :]\n            audio_1 = (audio_1 - b)/torch.exp(s)\n            audio = torch.cat([audio_0, audio_1],1)\n\n            audio = self.convinv[k](audio, reverse=True)\n\n            if k % self.n_early_every == 0 and k > 0:\n                if spect.type() == \'torch.cuda.HalfTensor\':\n                    z = torch.cuda.HalfTensor(spect.size(0), self.n_early_size, spect.size(2)).normal_()\n                else:\n                    z = torch.cuda.FloatTensor(spect.size(0), self.n_early_size, spect.size(2)).normal_()\n                audio = torch.cat((sigma*z, audio),1)\n\n        audio = audio.permute(0,2,1).contiguous().view(audio.size(0), -1).data\n        return audio\n\n    @staticmethod\n    def remove_weightnorm(model):\n        waveglow = model\n        for WN in waveglow.WN:\n            WN.start = torch.nn.utils.remove_weight_norm(WN.start)\n            WN.in_layers = remove(WN.in_layers)\n            WN.cond_layers = remove(WN.cond_layers)\n            WN.res_skip_layers = remove(WN.res_skip_layers)\n        return waveglow\n\n\ndef remove(conv_list):\n    new_conv_list = torch.nn.ModuleList()\n    for old_conv in conv_list:\n        old_conv = torch.nn.utils.remove_weight_norm(old_conv)\n        new_conv_list.append(old_conv)\n    return new_conv_list\n'"
waveglow/inference.py,3,"b'# *****************************************************************************\n#  Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions are met:\n#      * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n#      * Redistributions in binary form must reproduce the above copyright\n#        notice, this list of conditions and the following disclaimer in the\n#        documentation and/or other materials provided with the distribution.\n#      * Neither the name of the NVIDIA CORPORATION nor the\n#        names of its contributors may be used to endorse or promote products\n#        derived from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n#  ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n# *****************************************************************************\nimport os\nfrom scipy.io.wavfile import write\nimport torch\nfrom waveglow.mel2samp import files_to_list, MAX_WAV_VALUE\n# from denoiser import Denoiser\n\n\ndef inference(mel, waveglow, audio_path, sigma=1.0, sampling_rate=22050):\n    with torch.no_grad():\n        audio = waveglow.infer(mel, sigma=sigma)\n        audio = audio * MAX_WAV_VALUE\n    audio = audio.squeeze()\n    audio = audio.cpu().numpy()\n    audio = audio.astype(\'int16\')\n    write(audio_path, sampling_rate, audio)\n\n\ndef test_speed(mel, waveglow, sigma=1.0, sampling_rate=22050):\n    with torch.no_grad():\n        audio = waveglow.infer(mel, sigma=sigma)\n        audio = audio * MAX_WAV_VALUE\n\n\ndef get_wav(mel, waveglow, sigma=1.0, sampling_rate=22050):\n    with torch.no_grad():\n        audio = waveglow.infer(mel, sigma=sigma)\n        audio = audio * MAX_WAV_VALUE\n    audio = audio.squeeze()\n    audio = audio.cpu()\n\n    return audio\n'"
waveglow/mel2samp.py,7,"b'# *****************************************************************************\n#  Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions are met:\n#      * Redistributions of source code must retain the above copyright\n#        notice, this list of conditions and the following disclaimer.\n#      * Redistributions in binary form must reproduce the above copyright\n#        notice, this list of conditions and the following disclaimer in the\n#        documentation and/or other materials provided with the distribution.\n#      * Neither the name of the NVIDIA CORPORATION nor the\n#        names of its contributors may be used to endorse or promote products\n#        derived from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND\n#  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n#  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n#  DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY\n#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n#  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n#  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n# *****************************************************************************\\\n# from tacotron2.layers import TacotronSTFT\nimport os\nimport random\nimport argparse\nimport json\nimport torch\nimport torch.utils.data\nimport sys\nfrom scipy.io.wavfile import read\n\n# We\'re using the audio processing from TacoTron2 to make sure it matches\nsys.path.insert(0, \'tacotron2\')\n\nMAX_WAV_VALUE = 32768.0\n\n\ndef files_to_list(filename):\n    """"""\n    Takes a text file of filenames and makes a list of filenames\n    """"""\n    with open(filename, encoding=\'utf-8\') as f:\n        files = f.readlines()\n\n    files = [f.rstrip() for f in files]\n    return files\n\n\n# def load_wav_to_torch(full_path):\n#     """"""\n#     Loads wavdata into torch array\n#     """"""\n#     sampling_rate, data = read(full_path)\n#     return torch.from_numpy(data).float(), sampling_rate\n\n\n# class Mel2Samp(torch.utils.data.Dataset):\n#     """"""\n#     This is the main class that calculates the spectrogram and returns the\n#     spectrogram, audio pair.\n#     """"""\n\n#     def __init__(self, training_files, segment_length, filter_length,\n#                  hop_length, win_length, sampling_rate, mel_fmin, mel_fmax):\n#         self.audio_files = files_to_list(training_files)\n#         random.seed(1234)\n#         random.shuffle(self.audio_files)\n#         self.stft = TacotronSTFT(filter_length=filter_length,\n#                                  hop_length=hop_length,\n#                                  win_length=win_length,\n#                                  sampling_rate=sampling_rate,\n#                                  mel_fmin=mel_fmin, mel_fmax=mel_fmax)\n#         self.segment_length = segment_length\n#         self.sampling_rate = sampling_rate\n\n#     def get_mel(self, audio):\n#         audio_norm = audio / MAX_WAV_VALUE\n#         audio_norm = audio_norm.unsqueeze(0)\n#         audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n#         melspec = self.stft.mel_spectrogram(audio_norm)\n#         melspec = torch.squeeze(melspec, 0)\n#         return melspec\n\n#     def __getitem__(self, index):\n#         # Read audio\n#         filename = self.audio_files[index]\n#         audio, sampling_rate = load_wav_to_torch(filename)\n#         if sampling_rate != self.sampling_rate:\n#             raise ValueError(""{} SR doesn\'t match target {} SR"".format(\n#                 sampling_rate, self.sampling_rate))\n\n#         # Take segment\n#         if audio.size(0) >= self.segment_length:\n#             max_audio_start = audio.size(0) - self.segment_length\n#             audio_start = random.randint(0, max_audio_start)\n#             audio = audio[audio_start:audio_start+self.segment_length]\n#         else:\n#             audio = torch.nn.functional.pad(\n#                 audio, (0, self.segment_length - audio.size(0)), \'constant\').data\n\n#         mel = self.get_mel(audio)\n#         audio = audio / MAX_WAV_VALUE\n\n#         return (mel, audio)\n\n#     def __len__(self):\n#         return len(self.audio_files)\n\n\n# # ===================================================================\n# # Takes directory of clean audio and makes directory of spectrograms\n# # Useful for making test sets\n# # ===================================================================\n# if __name__ == ""__main__"":\n#     # Get defaults so it can work with no Sacred\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument(\'-f\', ""--filelist_path"", required=True)\n#     parser.add_argument(\'-c\', \'--config\', type=str,\n#                         help=\'JSON file for configuration\')\n#     parser.add_argument(\'-o\', \'--output_dir\', type=str,\n#                         help=\'Output directory\')\n#     args = parser.parse_args()\n\n#     with open(args.config) as f:\n#         data = f.read()\n#     data_config = json.loads(data)[""data_config""]\n#     mel2samp = Mel2Samp(**data_config)\n\n#     filepaths = files_to_list(args.filelist_path)\n\n#     # Make directory if it doesn\'t exist\n#     if not os.path.isdir(args.output_dir):\n#         os.makedirs(args.output_dir)\n#         os.chmod(args.output_dir, 0o775)\n\n#     for filepath in filepaths:\n#         audio, sr = load_wav_to_torch(filepath)\n#         melspectrogram = mel2samp.get_mel(audio)\n#         filename = os.path.basename(filepath)\n#         new_filepath = args.output_dir + \'/\' + filename + \'.pt\'\n#         print(new_filepath)\n#         torch.save(melspectrogram, new_filepath)\n'"
