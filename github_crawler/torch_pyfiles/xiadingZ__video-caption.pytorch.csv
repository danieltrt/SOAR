file_path,api_count,code
dataloader.py,5,"b'import json\n\nimport random\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass VideoDataset(Dataset):\n\n    def get_vocab_size(self):\n        return len(self.get_vocab())\n\n    def get_vocab(self):\n        return self.ix_to_word\n\n    def get_seq_length(self):\n        return self.seq_length\n\n    def __init__(self, opt, mode):\n        super(VideoDataset, self).__init__()\n        self.mode = mode  # to load train/val/test data\n\n        # load the json file which contains information about the dataset\n        self.captions = json.load(open(opt[""caption_json""]))\n        info = json.load(open(opt[""info_json""]))\n        self.ix_to_word = info[\'ix_to_word\']\n        self.word_to_ix = info[\'word_to_ix\']\n        print(\'vocab size is \', len(self.ix_to_word))\n        self.splits = info[\'videos\']\n        print(\'number of train videos: \', len(self.splits[\'train\']))\n        print(\'number of val videos: \', len(self.splits[\'val\']))\n        print(\'number of test videos: \', len(self.splits[\'test\']))\n\n        self.feats_dir = opt[""feats_dir""]\n        self.c3d_feats_dir = opt[\'c3d_feats_dir\']\n        self.with_c3d = opt[\'with_c3d\']\n        print(\'load feats from %s\' % (self.feats_dir))\n        # load in the sequence data\n        self.max_len = opt[""max_len""]\n        print(\'max sequence length in data is\', self.max_len)\n\n    def __getitem__(self, ix):\n        """"""This function returns a tuple that is further passed to collate_fn\n        """"""\n        # which part of data to load\n        if self.mode == \'val\':\n            ix += len(self.splits[\'train\'])\n        elif self.mode == \'test\':\n            ix = ix + len(self.splits[\'train\']) + len(self.splits[\'val\'])\n        \n        fc_feat = []\n        for dir in self.feats_dir:\n            fc_feat.append(np.load(os.path.join(dir, \'video%i.npy\' % (ix))))\n        fc_feat = np.concatenate(fc_feat, axis=1)\n        if self.with_c3d == 1:\n            c3d_feat = np.load(os.path.join(self.c3d_feats_dir, \'video%i.npy\'%(ix)))\n            c3d_feat = np.mean(c3d_feat, axis=0, keepdims=True)\n            fc_feat = np.concatenate((fc_feat, np.tile(c3d_feat, (fc_feat.shape[0], 1))), axis=1)\n        label = np.zeros(self.max_len)\n        mask = np.zeros(self.max_len)\n        captions = self.captions[\'video%i\'%(ix)][\'final_captions\']\n        gts = np.zeros((len(captions), self.max_len))\n        for i, cap in enumerate(captions):\n            if len(cap) > self.max_len:\n                cap = cap[:self.max_len]\n                cap[-1] = \'<eos>\'\n            for j, w in enumerate(cap):\n                gts[i, j] = self.word_to_ix[w]\n\n        # random select a caption for this video\n        cap_ix = random.randint(0, len(captions) - 1)\n        label = gts[cap_ix]\n        non_zero = (label == 0).nonzero()\n        mask[:int(non_zero[0][0]) + 1] = 1\n\n        data = {}\n        data[\'fc_feats\'] = torch.from_numpy(fc_feat).type(torch.FloatTensor)\n        data[\'labels\'] = torch.from_numpy(label).type(torch.LongTensor)\n        data[\'masks\'] = torch.from_numpy(mask).type(torch.FloatTensor)\n        data[\'gts\'] = torch.from_numpy(gts).long()\n        data[\'video_ids\'] = \'video%i\'%(ix)\n        return data\n\n    def __len__(self):\n        return len(self.splits[self.mode])\n'"
eval.py,4,"b'import json\nimport os\nimport argparse\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom models import EncoderRNN, DecoderRNN, S2VTAttModel, S2VTModel\nfrom dataloader import VideoDataset\nimport misc.utils as utils\nfrom misc.cocoeval import suppress_stdout_stderr, COCOScorer\n\nfrom pandas.io.json import json_normalize\n\n\ndef convert_data_to_coco_scorer_format(data_frame):\n    gts = {}\n    for row in zip(data_frame[""caption""], data_frame[""video_id""]):\n        if row[1] in gts:\n            gts[row[1]].append(\n                {\'image_id\': row[1], \'cap_id\': len(gts[row[1]]), \'caption\': row[0]})\n        else:\n            gts[row[1]] = []\n            gts[row[1]].append(\n                {\'image_id\': row[1], \'cap_id\': len(gts[row[1]]), \'caption\': row[0]})\n    return gts\n\n\ndef test(model, crit, dataset, vocab, opt):\n    model.eval()\n    loader = DataLoader(dataset, batch_size=opt[""batch_size""], shuffle=True)\n    scorer = COCOScorer()\n    gt_dataframe = json_normalize(\n        json.load(open(opt[""input_json""]))[\'sentences\'])\n    gts = convert_data_to_coco_scorer_format(gt_dataframe)\n    results = []\n    samples = {}\n    for data in loader:\n        # forward the model to get loss\n        fc_feats = data[\'fc_feats\'].cuda()\n        labels = data[\'labels\'].cuda()\n        masks = data[\'masks\'].cuda()\n        video_ids = data[\'video_ids\']\n      \n        # forward the model to also get generated samples for each image\n        with torch.no_grad():\n            seq_probs, seq_preds = model(\n                fc_feats, mode=\'inference\', opt=opt)\n\n        sents = utils.decode_sequence(vocab, seq_preds)\n\n        for k, sent in enumerate(sents):\n            video_id = video_ids[k]\n            samples[video_id] = [{\'image_id\': video_id, \'caption\': sent}]\n\n    with suppress_stdout_stderr():\n        valid_score = scorer.score(gts, samples, samples.keys())\n    results.append(valid_score)\n    print(valid_score)\n\n    if not os.path.exists(opt[""results_path""]):\n        os.makedirs(opt[""results_path""])\n\n    with open(os.path.join(opt[""results_path""], ""scores.txt""), \'a\') as scores_table:\n        scores_table.write(json.dumps(results[0]) + ""\\n"")\n    with open(os.path.join(opt[""results_path""],\n                           opt[""model""].split(""/"")[-1].split(\'.\')[0] + "".json""), \'w\') as prediction_results:\n        json.dump({""predictions"": samples, ""scores"": valid_score},\n                  prediction_results)\n\n\ndef main(opt):\n    dataset = VideoDataset(opt, ""test"")\n    opt[""vocab_size""] = dataset.get_vocab_size()\n    opt[""seq_length""] = dataset.max_len\n    if opt[""model""] == \'S2VTModel\':\n        model = S2VTModel(opt[""vocab_size""], opt[""max_len""], opt[""dim_hidden""], opt[""dim_word""],\n                          rnn_dropout_p=opt[""rnn_dropout_p""]).cuda()\n    elif opt[""model""] == ""S2VTAttModel"":\n        encoder = EncoderRNN(opt[""dim_vid""], opt[""dim_hidden""], bidirectional=opt[""bidirectional""],\n                             input_dropout_p=opt[""input_dropout_p""], rnn_dropout_p=opt[""rnn_dropout_p""])\n        decoder = DecoderRNN(opt[""vocab_size""], opt[""max_len""], opt[""dim_hidden""], opt[""dim_word""],\n                             input_dropout_p=opt[""input_dropout_p""],\n                             rnn_dropout_p=opt[""rnn_dropout_p""], bidirectional=opt[""bidirectional""])\n        model = S2VTAttModel(encoder, decoder).cuda()\n    #model = nn.DataParallel(model)\n    # Setup the model\n    model.load_state_dict(torch.load(opt[""saved_model""]))\n    crit = utils.LanguageModelCriterion()\n\n    test(model, crit, dataset, dataset.get_vocab(), opt)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--recover_opt\', type=str, required=True,\n                        help=\'recover train opts from saved opt_json\')\n    parser.add_argument(\'--saved_model\', type=str, default=\'\',\n                        help=\'path to saved model to evaluate\')\n\n    parser.add_argument(\'--dump_json\', type=int, default=1,\n                        help=\'Dump json with predictions into vis folder? (1=yes,0=no)\')\n    parser.add_argument(\'--results_path\', type=str, default=\'results/\')\n    parser.add_argument(\'--dump_path\', type=int, default=0,\n                        help=\'Write image paths along with predictions into vis json? (1=yes,0=no)\')\n    parser.add_argument(\'--gpu\', type=str, default=\'0\',\n                        help=\'gpu device number\')\n    parser.add_argument(\'--batch_size\', type=int, default=128,\n                        help=\'minibatch size\')\n    parser.add_argument(\'--sample_max\', type=int, default=1,\n                        help=\'0/1. whether sample max probs  to get next word in inference stage\')\n    parser.add_argument(\'--temperature\', type=float, default=1.0)\n    parser.add_argument(\'--beam_size\', type=int, default=1,\n                        help=\'used when sample_max = 1. Usually 2 or 3 works well.\')\n\n    args = parser.parse_args()\n    args = vars((args))\n    opt = json.load(open(args[""recover_opt""]))\n    for k, v in args.items():\n        opt[k] = v\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = opt[""gpu""]\n    main(opt)\n'"
opts.py,0,"b'import argparse\n\n\ndef parse_opt():\n    parser = argparse.ArgumentParser()\n    # Data input settings\n    parser.add_argument(\n        \'--input_json\',\n        type=str,\n        default=\'data/videodatainfo_2017.json\',\n        help=\'path to the json file containing video info\')\n    parser.add_argument(\n        \'--info_json\',\n        type=str,\n        default=\'data/info.json\',\n        help=\'path to the json file containing additional info and vocab\')\n    parser.add_argument(\n        \'--caption_json\',\n        type=str,\n        default=\'data/caption.json\',\n        help=\'path to the processed video caption json\')\n\n    parser.add_argument(\n        \'--feats_dir\',\n        nargs=\'*\',\n        type=str,\n        default=[\'data/feats/resnet152/\'],\n        help=\'path to the directory containing the preprocessed fc feats\')\n\n    parser.add_argument(\'--c3d_feats_dir\', type=str, default=\'data/c3d_feats\')\n    parser.add_argument(\n        \'--with_c3d\', type=int, default=0, help=\'whether to use c3d features\')\n\n    parser.add_argument(\n        \'--cached_tokens\',\n        type=str,\n        default=\'msr-all-idxs\',\n        help=\'Cached token file for calculating cider score \\\n                        during self critical training.\')\n\n    # Model settings\n    parser.add_argument(\n        ""--model"", type=str, default=\'S2VTModel\', help=""with model to use"")\n\n    parser.add_argument(\n        ""--max_len"",\n        type=int,\n        default=28,\n        help=\'max length of captions(containing <sos>,<eos>)\')\n    parser.add_argument(\n        ""--bidirectional"",\n        type=int,\n        default=0,\n        help=""0 for disable, 1 for enable. encoder/decoder bidirectional."")\n\n    parser.add_argument(\n        \'--dim_hidden\',\n        type=int,\n        default=512,\n        help=\'size of the rnn hidden layer\')\n    parser.add_argument(\n        \'--num_layers\', type=int, default=1, help=\'number of layers in the RNN\')\n    parser.add_argument(\n        \'--input_dropout_p\',\n        type=float,\n        default=0.2,\n        help=\'strength of dropout in the Language Model RNN\')\n    parser.add_argument(\n        \'--rnn_type\', type=str, default=\'gru\', help=\'lstm or gru\')\n    parser.add_argument(\n        \'--rnn_dropout_p\',\n        type=float,\n        default=0.5,\n        help=\'strength of dropout in the Language Model RNN\')\n    parser.add_argument(\n        \'--dim_word\',\n        type=int,\n        default=512,\n        help=\'the encoding size of each token in the vocabulary, and the video.\'\n    )\n\n    parser.add_argument(\n        \'--dim_vid\',\n        type=int,\n        default=2048,\n        help=\'dim of features of video frames\')\n\n    # Optimization: General\n\n    parser.add_argument(\n        \'--epochs\', type=int, default=6001, help=\'number of epochs\')\n    parser.add_argument(\n        \'--batch_size\', type=int, default=128, help=\'minibatch size\')\n    parser.add_argument(\n        \'--grad_clip\',\n        type=float,\n        default=5,  # 5.,\n        help=\'clip gradients at this value\')\n\n    parser.add_argument(\n        \'--self_crit_after\',\n        type=int,\n        default=-1,\n        help=\'After what epoch do we start finetuning the CNN? \\\n                        (-1 = disable; never finetune, 0 = finetune from start)\'\n    )\n\n    parser.add_argument(\n        \'--learning_rate\', type=float, default=4e-4, help=\'learning rate\')\n\n    parser.add_argument(\n        \'--learning_rate_decay_every\',\n        type=int,\n        default=200,\n        help=\'every how many iterations thereafter to drop LR?(in epoch)\')\n    parser.add_argument(\'--learning_rate_decay_rate\', type=float, default=0.8)\n    parser.add_argument(\n        \'--optim_alpha\', type=float, default=0.9, help=\'alpha for adam\')\n    parser.add_argument(\n        \'--optim_beta\', type=float, default=0.999, help=\'beta used for adam\')\n    parser.add_argument(\n        \'--optim_epsilon\',\n        type=float,\n        default=1e-8,\n        help=\'epsilon that goes into denominator for smoothing\')\n    parser.add_argument(\n        \'--weight_decay\',\n        type=float,\n        default=5e-4,\n        help=\'weight_decay. strength of weight regularization\')\n\n    parser.add_argument(\n        \'--save_checkpoint_every\',\n        type=int,\n        default=50,\n        help=\'how often to save a model checkpoint (in epoch)?\')\n    parser.add_argument(\n        \'--checkpoint_path\',\n        type=str,\n        default=\'save\',\n        help=\'directory to store checkpointed models\')\n\n    parser.add_argument(\n        \'--gpu\', type=str, default=\'0\', help=\'gpu device number\')\n\n    args = parser.parse_args()\n\n    return args\n'"
prepro_feats.py,3,"b'import shutil\nimport subprocess\nimport glob\nfrom tqdm import tqdm\nimport numpy as np\nimport os\nimport argparse\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pretrainedmodels\nfrom pretrainedmodels import utils\n\nC, H, W = 3, 224, 224\n\n\ndef extract_frames(video, dst):\n    with open(os.devnull, ""w"") as ffmpeg_log:\n        if os.path.exists(dst):\n            print("" cleanup: "" + dst + ""/"")\n            shutil.rmtree(dst)\n        os.makedirs(dst)\n        video_to_frames_command = [""ffmpeg"",\n                                   # (optional) overwrite output file if it exists\n                                   \'-y\',\n                                   \'-i\', video,  # input file\n                                   \'-vf\', ""scale=400:300"",  # input file\n                                   \'-qscale:v\', ""2"",  # quality for JPEG\n                                   \'{0}/%06d.jpg\'.format(dst)]\n        subprocess.call(video_to_frames_command,\n                        stdout=ffmpeg_log, stderr=ffmpeg_log)\n\n\ndef extract_feats(params, model, load_image_fn):\n    global C, H, W\n    model.eval()\n\n    dir_fc = params[\'output_dir\']\n    if not os.path.isdir(dir_fc):\n        os.mkdir(dir_fc)\n    print(""save video feats to %s"" % (dir_fc))\n    video_list = glob.glob(os.path.join(params[\'video_path\'], \'*.mp4\'))\n    for video in tqdm(video_list):\n        video_id = video.split(""/"")[-1].split(""."")[0]\n        dst = params[\'model\'] + \'_\' + video_id\n        extract_frames(video, dst)\n\n        image_list = sorted(glob.glob(os.path.join(dst, \'*.jpg\')))\n        samples = np.round(np.linspace(\n            0, len(image_list) - 1, params[\'n_frame_steps\']))\n        image_list = [image_list[int(sample)] for sample in samples]\n        images = torch.zeros((len(image_list), C, H, W))\n        for iImg in range(len(image_list)):\n            img = load_image_fn(image_list[iImg])\n            images[iImg] = img\n        with torch.no_grad():\n            fc_feats = model(images.cuda()).squeeze()\n        img_feats = fc_feats.cpu().numpy()\n        # Save the inception features\n        outfile = os.path.join(dir_fc, video_id + \'.npy\')\n        np.save(outfile, img_feats)\n        # cleanup\n        shutil.rmtree(dst)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--gpu"", dest=\'gpu\', type=str, default=\'0\',\n                        help=\'Set CUDA_VISIBLE_DEVICES environment variable, optional\')\n    parser.add_argument(""--output_dir"", dest=\'output_dir\', type=str,\n                        default=\'data/feats/resnet152\', help=\'directory to store features\')\n    parser.add_argument(""--n_frame_steps"", dest=\'n_frame_steps\', type=int, default=40,\n                        help=\'how many frames to sampler per video\')\n\n    parser.add_argument(""--video_path"", dest=\'video_path\', type=str,\n                        default=\'data/train-video\', help=\'path to video dataset\')\n    parser.add_argument(""--model"", dest=""model"", type=str, default=\'resnet152\',\n                        help=\'the CNN model you want to use to extract_feats\')\n    \n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n    params = vars(args)\n    if params[\'model\'] == \'inception_v3\':\n        C, H, W = 3, 299, 299\n        model = pretrainedmodels.inceptionv3(pretrained=\'imagenet\')\n        load_image_fn = utils.LoadTransformImage(model)\n\n    elif params[\'model\'] == \'resnet152\':\n        C, H, W = 3, 224, 224\n        model = pretrainedmodels.resnet152(pretrained=\'imagenet\')\n        load_image_fn = utils.LoadTransformImage(model)\n\n    elif params[\'model\'] == \'inception_v4\':\n        C, H, W = 3, 299, 299\n        model = pretrainedmodels.inceptionv4(\n            num_classes=1000, pretrained=\'imagenet\')\n        load_image_fn = utils.LoadTransformImage(model)\n\n    else:\n        print(""doesn\'t support %s"" % (params[\'model\']))\n\n    model.last_linear = utils.Identity()\n    model = nn.DataParallel(model)\n    \n    model = model.cuda()\n    extract_feats(params, model, load_image_fn)\n'"
prepro_ngrams.py,0,"b'import json\nimport argparse\nimport pickle as pkl\nfrom collections import defaultdict\n\n\ndef precook(s, n=4):\n    """"""\n    Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\n    :param s: string : sentence to be converted into ngrams\n    :param n: int    : number of ngrams for which representation is calculated\n    :return: term frequency vector for occuring ngrams\n    """"""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1, n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return counts\n\n\ndef cook_refs(refs, n=4):  # lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\n    :param refs: list of string : reference sentences for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (list of dict)\n    \'\'\'\n    return [precook(ref, n) for ref in refs]\n\n\ndef create_crefs(refs):\n    crefs = []\n    for ref in refs:\n        # ref is a list of 5 captions\n        crefs.append(cook_refs(ref))\n    return crefs\n\n\ndef compute_doc_freq(crefs):\n    \'\'\'\n    Compute term frequency for reference data.\n    This will be used to compute idf (inverse document frequency later)\n    The term frequency is stored in the object\n    :return: None\n    \'\'\'\n    document_frequency = defaultdict(float)\n    for refs in crefs:\n        # refs, k ref captions of one image\n        for ngram in set([ngram for ref in refs for (ngram, count) in ref.items()]):\n            document_frequency[ngram] += 1\n      # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n    return document_frequency\n\n\ndef build_dict(vids, wtoi):\n    refs_words = []\n    refs_idxs = []\n    count_vids = 0\n    for vid in vids:\n        ref_words = []\n        ref_idxs = []\n        for cap in vids[vid][\'final_captions\']:\n            tmp_tokens = cap\n            tmp_tokens = [_ if _ in wtoi else \'<UNK>\' for _ in tmp_tokens]\n            ref_words.append(\' \'.join(tmp_tokens))\n            ref_idxs.append(\' \'.join([str(wtoi[_]) for _ in tmp_tokens]))\n        refs_words.append(ref_words)\n        refs_idxs.append(ref_idxs)\n        count_vids += 1\n    ngram_words = compute_doc_freq(create_crefs(refs_words))\n    ngram_idxs = compute_doc_freq(create_crefs(refs_idxs))\n    return ngram_words, ngram_idxs, count_vids\n\n\ndef main(params):\n    vids = json.load(open(params[\'caption_json\']))\n    wtoi = json.load(open(params[\'info_json\']))[\'word_to_ix\']\n\n    ngram_words, ngram_idxs, ref_len = build_dict(vids, wtoi)\n\n    pkl.dump({\'document_frequency\': ngram_words, \'ref_len\': ref_len}, open(\n        params[\'output_pkl\']+\'-words.p\', \'wb\'))\n    pkl.dump({\'document_frequency\': ngram_idxs, \'ref_len\': ref_len}, open(\n        params[\'output_pkl\']+\'-idxs.p\', \'wb\'))\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n\n    # input json\n    parser.add_argument(\'--caption_json\', default=\'data/caption.json\',\n                        help=\'input json file to containing video captions\')\n    parser.add_argument(\'--info_json\', default=\'data/info.json\', help=\'vocab info json file\')\n    parser.add_argument(\'--output_pkl\', default=\'data/msr-all\', help=\'output pickle file\')\n    args = parser.parse_args()\n    params = vars(args)  # convert to ordinary dict\n\n    main(params)\n'"
prepro_vocab.py,0,"b'import re\nimport json\nimport argparse\nimport numpy as np\n\n\ndef build_vocab(vids, params):\n    count_thr = params[\'word_count_threshold\']\n    # count up the number of words\n    counts = {}\n    for vid, caps in vids.items():\n        for cap in caps[\'captions\']:\n            ws = re.sub(r\'[.!,;?]\', \' \', cap).split()\n            for w in ws:\n                counts[w] = counts.get(w, 0) + 1\n    # cw = sorted([(count, w) for w, count in counts.items()], reverse=True)\n    total_words = sum(counts.values())\n    bad_words = [w for w, n in counts.items() if n <= count_thr]\n    vocab = [w for w, n in counts.items() if n > count_thr]\n    bad_count = sum(counts[w] for w in bad_words)\n    print(\'number of bad words: %d/%d = %.2f%%\' %\n          (len(bad_words), len(counts), len(bad_words) * 100.0 / len(counts)))\n    print(\'number of words in vocab would be %d\' % (len(vocab), ))\n    print(\'number of UNKs: %d/%d = %.2f%%\' %\n          (bad_count, total_words, bad_count * 100.0 / total_words))\n    # lets now produce the final annotations\n    if bad_count > 0:\n        # additional special UNK token we will use below to map infrequent words to\n        print(\'inserting the special UNK token\')\n        vocab.append(\'<UNK>\')\n    for vid, caps in vids.items():\n        caps = caps[\'captions\']\n        vids[vid][\'final_captions\'] = []\n        for cap in caps:\n            ws = re.sub(r\'[.!,;?]\', \' \', cap).split()\n            caption = [\n                \'<sos>\'] + [w if counts.get(w, 0) > count_thr else \'<UNK>\' for w in ws] + [\'<eos>\']\n            vids[vid][\'final_captions\'].append(caption)\n    return vocab\n\n\ndef main(params):\n    videos = json.load(open(params[\'input_json\'], \'r\'))[\'sentences\']\n    video_caption = {}\n    for i in videos:\n        if i[\'video_id\'] not in video_caption.keys():\n            video_caption[i[\'video_id\']] = {\'captions\': []}\n        video_caption[i[\'video_id\']][\'captions\'].append(i[\'caption\'])\n    # create the vocab\n    vocab = build_vocab(video_caption, params)\n    itow = {i + 2: w for i, w in enumerate(vocab)}\n    wtoi = {w: i + 2 for i, w in enumerate(vocab)}  # inverse table\n    wtoi[\'<eos>\'] = 0\n    itow[0] = \'<eos>\'\n    wtoi[\'<sos>\'] = 1\n    itow[1] = \'<sos>\'\n\n    out = {}\n    out[\'ix_to_word\'] = itow\n    out[\'word_to_ix\'] = wtoi\n    out[\'videos\'] = {\'train\': [], \'val\': [], \'test\': []}\n    videos = json.load(open(params[\'input_json\'], \'r\'))[\'videos\']\n    for i in videos:\n        out[\'videos\'][i[\'split\']].append(int(i[\'id\']))\n    json.dump(out, open(params[\'info_json\'], \'w\'))\n    json.dump(video_caption, open(params[\'caption_json\'], \'w\'))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n\n    # input json\n    parser.add_argument(\'--input_json\', type=str, default=\'data/videodatainfo_2017.json\',\n                        help=\'msr_vtt videoinfo json\')\n    parser.add_argument(\'--info_json\', default=\'data/info.json\',\n                        help=\'info about iw2word and word2ix\')\n    parser.add_argument(\'--caption_json\', default=\'data/caption.json\', help=\'caption json file\')\n\n\n    parser.add_argument(\'--word_count_threshold\', default=1, type=int,\n                        help=\'only words that occur more than this number of times will be put in vocab\')\n\n    args = parser.parse_args()\n    params = vars(args)  # convert to ordinary dict\n    main(params)\n'"
train.py,7,"b'import json\nimport os\n\nimport numpy as np\n\nimport misc.utils as utils\nimport opts\nimport torch\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_value_\nfrom dataloader import VideoDataset\nfrom misc.rewards import get_self_critical_reward, init_cider_scorer\nfrom models import DecoderRNN, EncoderRNN, S2VTAttModel, S2VTModel\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n\ndef train(loader, model, crit, optimizer, lr_scheduler, opt, rl_crit=None):\n    model.train()\n    #model = nn.DataParallel(model)\n    for epoch in range(opt[""epochs""]):\n        lr_scheduler.step()\n\n        iteration = 0\n        # If start self crit training\n        if opt[""self_crit_after""] != -1 and epoch >= opt[""self_crit_after""]:\n            sc_flag = True\n            init_cider_scorer(opt[""cached_tokens""])\n        else:\n            sc_flag = False\n\n        for data in loader:\n            torch.cuda.synchronize()\n            fc_feats = data[\'fc_feats\'].cuda()\n            labels = data[\'labels\'].cuda()\n            masks = data[\'masks\'].cuda()\n\n            optimizer.zero_grad()\n            if not sc_flag:\n                seq_probs, _ = model(fc_feats, labels, \'train\')\n                loss = crit(seq_probs, labels[:, 1:], masks[:, 1:])\n            else:\n                seq_probs, seq_preds = model(\n                    fc_feats, mode=\'inference\', opt=opt)\n                reward = get_self_critical_reward(model, fc_feats, data,\n                                                  seq_preds)\n                print(reward.shape)\n                loss = rl_crit(seq_probs, seq_preds,\n                               torch.from_numpy(reward).float().cuda())\n\n            loss.backward()\n            clip_grad_value_(model.parameters(), opt[\'grad_clip\'])\n            optimizer.step()\n            train_loss = loss.item()\n            torch.cuda.synchronize()\n            iteration += 1\n\n            if not sc_flag:\n                print(""iter %d (epoch %d), train_loss = %.6f"" %\n                      (iteration, epoch, train_loss))\n            else:\n                print(""iter %d (epoch %d), avg_reward = %.6f"" %\n                      (iteration, epoch, np.mean(reward[:, 0])))\n\n        if epoch % opt[""save_checkpoint_every""] == 0:\n            model_path = os.path.join(opt[""checkpoint_path""],\n                                      \'model_%d.pth\' % (epoch))\n            model_info_path = os.path.join(opt[""checkpoint_path""],\n                                           \'model_score.txt\')\n            torch.save(model.state_dict(), model_path)\n            print(""model saved to %s"" % (model_path))\n            with open(model_info_path, \'a\') as f:\n                f.write(""model_%d, loss: %.6f\\n"" % (epoch, train_loss))\n\n\ndef main(opt):\n    dataset = VideoDataset(opt, \'train\')\n    dataloader = DataLoader(dataset, batch_size=opt[""batch_size""], shuffle=True)\n    opt[""vocab_size""] = dataset.get_vocab_size()\n    if opt[""model""] == \'S2VTModel\':\n        model = S2VTModel(\n            opt[""vocab_size""],\n            opt[""max_len""],\n            opt[""dim_hidden""],\n            opt[""dim_word""],\n            opt[\'dim_vid\'],\n            rnn_cell=opt[\'rnn_type\'],\n            n_layers=opt[\'num_layers\'],\n            rnn_dropout_p=opt[""rnn_dropout_p""])\n    elif opt[""model""] == ""S2VTAttModel"":\n        encoder = EncoderRNN(\n            opt[""dim_vid""],\n            opt[""dim_hidden""],\n            bidirectional=opt[""bidirectional""],\n            input_dropout_p=opt[""input_dropout_p""],\n            rnn_cell=opt[\'rnn_type\'],\n            rnn_dropout_p=opt[""rnn_dropout_p""])\n        decoder = DecoderRNN(\n            opt[""vocab_size""],\n            opt[""max_len""],\n            opt[""dim_hidden""],\n            opt[""dim_word""],\n            input_dropout_p=opt[""input_dropout_p""],\n            rnn_cell=opt[\'rnn_type\'],\n            rnn_dropout_p=opt[""rnn_dropout_p""],\n            bidirectional=opt[""bidirectional""])\n        model = S2VTAttModel(encoder, decoder)\n    model = model.cuda()\n    crit = utils.LanguageModelCriterion()\n    rl_crit = utils.RewardCriterion()\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=opt[""learning_rate""],\n        weight_decay=opt[""weight_decay""])\n    exp_lr_scheduler = optim.lr_scheduler.StepLR(\n        optimizer,\n        step_size=opt[""learning_rate_decay_every""],\n        gamma=opt[""learning_rate_decay_rate""])\n\n    train(dataloader, model, crit, optimizer, exp_lr_scheduler, opt, rl_crit)\n\n\nif __name__ == \'__main__\':\n    opt = opts.parse_opt()\n    opt = vars(opt)\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = opt[""gpu""]\n    opt_json = os.path.join(opt[""checkpoint_path""], \'opt_info.json\')\n    if not os.path.isdir(opt[""checkpoint_path""]):\n        os.mkdir(opt[""checkpoint_path""])\n    with open(opt_json, \'w\') as f:\n        json.dump(opt, f)\n    print(\'save opt details to %s\' % (opt_json))\n    main(opt)\n'"
misc/__init__.py,0,b''
misc/cocoeval.py,0,"b'\'\'\'\nWrapper for evaluation on CIDEr, ROUGE_L, METEOR and Bleu_N\nusing coco-caption repo https://github.com/tylin/coco-caption\n\nclass COCOScorer is taken from https://github.com/yaoli/arctic-capgen-vid\n\'\'\'\n\nimport json\nimport os\nimport sys\nsys.path.append(\'coco-caption\')\n\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n# Define a context manager to suppress stdout and stderr.\n\n\nclass suppress_stdout_stderr:\n    \'\'\'\n    A context manager for doing a ""deep suppression"" of stdout and stderr in\n    Python, i.e. will suppress all print, even if the print originates in a\n    compiled C/Fortran sub-function.\n       This will not suppress raised exceptions, since exceptions are printed\n    to stderr just before a script exits, and after the context manager has\n    exited (at least, I think that is why it lets exceptions through).\n\n    \'\'\'\n\n    def __init__(self):\n        # Open a pair of null files\n        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n        # Save the actual stdout (1) and stderr (2) file descriptors.\n        self.save_fds = (os.dup(1), os.dup(2))\n\n    def __enter__(self):\n        # Assign the null pointers to stdout and stderr.\n        os.dup2(self.null_fds[0], 1)\n        os.dup2(self.null_fds[1], 2)\n\n    def __exit__(self, *_):\n        # Re-assign the real stdout/stderr back to (1) and (2)\n        os.dup2(self.save_fds[0], 1)\n        os.dup2(self.save_fds[1], 2)\n        # Close the null files\n        os.close(self.null_fds[0])\n        os.close(self.null_fds[1])\n\n\nclass COCOScorer(object):\n    def __init__(self):\n        print(\'init COCO-EVAL scorer\')\n\n    def score(self, GT, RES, IDs):\n        self.eval = {}\n        self.imgToEval = {}\n        gts = {}\n        res = {}\n        for ID in IDs:\n            #            print ID\n            gts[ID] = GT[ID]\n            res[ID] = RES[ID]\n        print(\'tokenization...\')\n        tokenizer = PTBTokenizer()\n        gts = tokenizer.tokenize(gts)\n        res = tokenizer.tokenize(res)\n\n        # =================================================\n        # Set up scorers\n        # =================================================\n        print(\'setting up scorers...\')\n        scorers = [\n            (Bleu(4), [""Bleu_1"", ""Bleu_2"", ""Bleu_3"", ""Bleu_4""]),\n            (Meteor(),""METEOR""),\n            (Rouge(), ""ROUGE_L""),\n            (Cider(), ""CIDEr""),\n            #(Spice(), ""SPICE"")\n        ]\n\n        # =================================================\n        # Compute scores\n        # =================================================\n        eval = {}\n        for scorer, method in scorers:\n            print(\'computing %s score...\' % (scorer.method()))\n            score, scores = scorer.compute_score(gts, res)\n            if type(method) == list:\n                for sc, scs, m in zip(score, scores, method):\n                    self.setEval(sc, m)\n                    self.setImgToEvalImgs(scs, IDs, m)\n                    print(""%s: %0.3f"" % (m, sc))\n            else:\n                self.setEval(score, method)\n                self.setImgToEvalImgs(scores, IDs, method)\n                print(""%s: %0.3f"" % (method, score))\n\n        # for metric, score in self.eval.items():\n        #    print \'%s: %.3f\'%(metric, score)\n        return self.eval\n\n    def setEval(self, score, method):\n        self.eval[method] = score\n\n    def setImgToEvalImgs(self, scores, imgIds, method):\n        for imgId, score in zip(imgIds, scores):\n            if imgId not in self.imgToEval:\n                self.imgToEval[imgId] = {}\n                self.imgToEval[imgId][""image_id""] = imgId\n            self.imgToEval[imgId][method] = score\n\n\ndef score(ref, sample):\n    # ref and sample are both dict\n    scorers = [\n        (Bleu(4), [""Bleu_1"", ""Bleu_2"", ""Bleu_3"", ""Bleu_4""]),\n        (Rouge(), ""ROUGE_L""),\n        (Cider(), ""CIDEr"")\n    ]\n    final_scores = {}\n    for scorer, method in scorers:\n        print(\'computing %s score with COCO-EVAL...\' % (scorer.method()))\n        score, scores = scorer.compute_score(ref, sample)\n        if type(score) == list:\n            for m, s in zip(method, score):\n                final_scores[m] = s\n        else:\n            final_scores[method] = score\n    return final_scores\n\n\ndef test_cocoscorer():\n    \'\'\'gts = {\n        184321:[\n        {u\'image_id\': 184321, u\'id\': 352188, u\'caption\': u\'A train traveling down-tracks next to lights.\'},\n        {u\'image_id\': 184321, u\'id\': 356043, u\'caption\': u""A blue and silver train next to train\'s station and trees.""},\n        {u\'image_id\': 184321, u\'id\': 356382, u\'caption\': u\'A blue train is next to a sidewalk on the rails.\'},\n        {u\'image_id\': 184321, u\'id\': 361110, u\'caption\': u\'A passenger train pulls into a train station.\'},\n        {u\'image_id\': 184321, u\'id\': 362544, u\'caption\': u\'A train coming down the tracks arriving at a station.\'}],\n        81922: [\n        {u\'image_id\': 81922, u\'id\': 86779, u\'caption\': u\'A large jetliner flying over a traffic filled street.\'},\n        {u\'image_id\': 81922, u\'id\': 90172, u\'caption\': u\'An airplane flies low in the sky over a city street. \'},\n        {u\'image_id\': 81922, u\'id\': 91615, u\'caption\': u\'An airplane flies over a street with many cars.\'},\n        {u\'image_id\': 81922, u\'id\': 92689, u\'caption\': u\'An airplane comes in to land over a road full of cars\'},\n        {u\'image_id\': 81922, u\'id\': 823814, u\'caption\': u\'The plane is flying over top of the cars\'}]\n        }\n\n    samples = {\n        184321: [{u\'image_id\': 184321, \'id\': 111, u\'caption\': u\'train traveling down a track in front of a road\'}],\n        81922: [{u\'image_id\': 81922, \'id\': 219, u\'caption\': u\'plane is flying through the sky\'}],\n        }\n    \'\'\'\n    gts = {\n        \'184321\': [\n            {u\'image_id\': \'184321\', u\'cap_id\': 0, u\'caption\': u\'A train traveling down tracks next to lights.\',\n             \'tokenized\': \'a train traveling down tracks next to lights\'},\n            {u\'image_id\': \'184321\', u\'cap_id\': 1, u\'caption\': u\'A train coming down the tracks arriving at a station.\',\n             \'tokenized\': \'a train coming down the tracks arriving at a station\'}],\n        \'81922\': [\n            {u\'image_id\': \'81922\', u\'cap_id\': 0, u\'caption\': u\'A large jetliner flying over a traffic filled street.\',\n             \'tokenized\': \'a large jetliner flying over a traffic filled street\'},\n            {u\'image_id\': \'81922\', u\'cap_id\': 1, u\'caption\': u\'The plane is flying over top of the cars\',\n             \'tokenized\': \'the plan is flying over top of the cars\'}, ]\n    }\n\n    samples = {\n        \'184321\': [{u\'image_id\': \'184321\', u\'caption\': u\'train traveling down a track in front of a road\'}],\n        \'81922\': [{u\'image_id\': \'81922\', u\'caption\': u\'plane is flying through the sky\'}],\n    }\n    IDs = [\'184321\', \'81922\']\n    scorer = COCOScorer()\n    scorer.score(gts, samples, IDs)\n\n\nif __name__ == \'__main__\':\n    test_cocoscorer()\n'"
misc/rewards.py,0,"b'import numpy as np\nfrom collections import OrderedDict\nimport torch\nimport sys\nsys.path.append(""coco-caption"")\nfrom pyciderevalcap.ciderD.ciderD import CiderD\n\nCiderD_scorer = None\n# CiderD_scorer = CiderD(df=\'corpus\')\n\n\ndef init_cider_scorer(cached_tokens):\n    global CiderD_scorer\n    CiderD_scorer = CiderD_scorer or CiderD(df=cached_tokens)\n\n\ndef array_to_str(arr):\n    out = \'\'\n    for i in range(len(arr)):\n        out += str(arr[i]) + \' \'\n        if arr[i] == 0:\n            break\n    return out.strip()\n\n\ndef get_self_critical_reward(model, fc_feats, data, gen_result):\n    batch_size = gen_result.size(0)\n\n    # get greedy decoding baseline\n    _, greedy_res = model(fc_feats, mode=\'inference\')\n\n    res = OrderedDict()\n\n    gen_result = gen_result.cpu().data.numpy()\n    greedy_res = greedy_res.cpu().data.numpy()\n    for i in range(batch_size):\n        res[i] = [array_to_str(gen_result[i])]\n    for i in range(batch_size):\n        res[batch_size + i] = [array_to_str(greedy_res[i])]\n\n    gts = OrderedDict()\n    for i in range(data[\'gts\'].size(0)):\n        gts[i] = [array_to_str(data[\'gts\'][i][j])\n                  for j in range(data[\'gts\'].size(1))]\n\n    res = [{\'image_id\': i, \'caption\': res[i]} for i in range(2 * batch_size)]\n    gts = {i: gts[i % batch_size] for i in range(2 * batch_size)}\n    _, scores = CiderD_scorer.compute_score(gts, res)\n    print(\'Cider scores:\', _)\n\n    scores = scores[:batch_size] - scores[batch_size:]\n\n    rewards = np.repeat(scores[:, np.newaxis], gen_result.shape[1], 1)\n\n    return rewards\n'"
misc/utils.py,5,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\n# Input: seq, N*D numpy array, with element 0 .. vocab_size. 0 is END token.\ndef decode_sequence(ix_to_word, seq):\n    seq = seq.cpu()\n    N, D = seq.size()\n    out = []\n    for i in range(N):\n        txt = \'\'\n        for j in range(D):\n            ix = seq[i, j].item()\n            if ix > 0:\n                if j >= 1:\n                    txt = txt + \' \'\n                txt = txt + ix_to_word[str(ix)]\n            else:\n                break\n        out.append(txt)\n    return out\n\n\nclass RewardCriterion(nn.Module):\n\n    def __init__(self):\n        super(RewardCriterion, self).__init__()\n\n    def forward(self, input, seq, reward):\n        input = input.contiguous().view(-1)\n        reward = reward.contiguous().view(-1)\n        mask = (seq > 0).float()\n        mask = torch.cat([mask.new(mask.size(0), 1).fill_(1).cuda(),\n                         mask[:, :-1]], 1).contiguous().view(-1)\n        output = - input * reward * mask\n        output = torch.sum(output) / torch.sum(mask)\n\n        return output\n\n\nclass LanguageModelCriterion(nn.Module):\n\n    def __init__(self):\n        super(LanguageModelCriterion, self).__init__()\n        self.loss_fn = nn.NLLLoss(reduce=False)\n\n    def forward(self, logits, target, mask):\n        """"""\n        logits: shape of (N, seq_len, vocab_size)\n        target: shape of (N, seq_len)\n        mask: shape of (N, seq_len)\n        """"""\n        # truncate to the same size\n        batch_size = logits.shape[0]\n        target = target[:, :logits.shape[1]]\n        mask = mask[:, :logits.shape[1]]\n        logits = logits.contiguous().view(-1, logits.shape[2])\n        target = target.contiguous().view(-1)\n        mask = mask.contiguous().view(-1)\n        loss = self.loss_fn(logits, target)\n        output = torch.sum(loss * mask) / batch_size\n        return output'"
models/Attention.py,4,"b'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Attention(nn.Module):\r\n    """"""\r\n    Applies an attention mechanism on the output features from the decoder.\r\n    """"""\r\n\r\n    def __init__(self, dim):\r\n        super(Attention, self).__init__()\r\n        self.dim = dim\r\n        self.linear1 = nn.Linear(dim * 2, dim)\r\n        self.linear2 = nn.Linear(dim, 1, bias=False)\r\n        #self._init_hidden()\r\n\r\n    def _init_hidden(self):\r\n        nn.init.xavier_normal_(self.linear1.weight)\r\n        nn.init.xavier_normal_(self.linear2.weight)\r\n\r\n    def forward(self, hidden_state, encoder_outputs):\r\n        """"""\r\n        Arguments:\r\n            hidden_state {Variable} -- batch_size x dim\r\n            encoder_outputs {Variable} -- batch_size x seq_len x dim\r\n\r\n        Returns:\r\n            Variable -- context vector of size batch_size x dim\r\n        """"""\r\n        batch_size, seq_len, _ = encoder_outputs.size()\r\n        hidden_state = hidden_state.unsqueeze(1).repeat(1, seq_len, 1)\r\n        inputs = torch.cat((encoder_outputs, hidden_state),\r\n                           2).view(-1, self.dim * 2)\r\n        o = self.linear2(F.tanh(self.linear1(inputs)))\r\n        e = o.view(batch_size, seq_len)\r\n        alpha = F.softmax(e, dim=1)\r\n        context = torch.bmm(alpha.unsqueeze(1), encoder_outputs).squeeze(1)\r\n        return context\r\n'"
models/DecoderRNN.py,13,"b'import random\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nfrom .Attention import Attention\r\n\r\n\r\nclass DecoderRNN(nn.Module):\r\n    """"""\r\n    Provides functionality for decoding in a seq2seq framework, with an option for attention.\r\n    Args:\r\n        vocab_size (int): size of the vocabulary\r\n        max_len (int): a maximum allowed length for the sequence to be processed\r\n        dim_hidden (int): the number of features in the hidden state `h`\r\n        n_layers (int, optional): number of recurrent layers (default: 1)\r\n        rnn_cell (str, optional): type of RNN cell (default: gru)\r\n        bidirectional (bool, optional): if the encoder is bidirectional (default False)\r\n        input_dropout_p (float, optional): dropout probability for the input sequence (default: 0)\r\n        rnn_dropout_p (float, optional): dropout probability for the output sequence (default: 0)\r\n\r\n    """"""\r\n\r\n    def __init__(self,\r\n                 vocab_size,\r\n                 max_len,\r\n                 dim_hidden,\r\n                 dim_word,\r\n                 n_layers=1,\r\n                 rnn_cell=\'gru\',\r\n                 bidirectional=False,\r\n                 input_dropout_p=0.1,\r\n                 rnn_dropout_p=0.1):\r\n        super(DecoderRNN, self).__init__()\r\n\r\n        self.bidirectional_encoder = bidirectional\r\n\r\n        self.dim_output = vocab_size\r\n        self.dim_hidden = dim_hidden * 2 if bidirectional else dim_hidden\r\n        self.dim_word = dim_word\r\n        self.max_length = max_len\r\n        self.sos_id = 1\r\n        self.eos_id = 0\r\n        self.input_dropout = nn.Dropout(input_dropout_p)\r\n        self.embedding = nn.Embedding(self.dim_output, dim_word)\r\n        self.attention = Attention(self.dim_hidden)\r\n        if rnn_cell.lower() == \'lstm\':\r\n            self.rnn_cell = nn.LSTM\r\n        elif rnn_cell.lower() == \'gru\':\r\n            self.rnn_cell = nn.GRU\r\n        self.rnn = self.rnn_cell(\r\n            self.dim_hidden + dim_word,\r\n            self.dim_hidden,\r\n            n_layers,\r\n            batch_first=True,\r\n            dropout=rnn_dropout_p)\r\n\r\n        self.out = nn.Linear(self.dim_hidden, self.dim_output)\r\n\r\n        self._init_weights()\r\n\r\n    def forward(self,\r\n                encoder_outputs,\r\n                encoder_hidden,\r\n                targets=None,\r\n                mode=\'train\',\r\n                opt={}):\r\n        """"""\r\n\r\n        Inputs: inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio\r\n        - **encoder_hidden** (num_layers * num_directions, batch_size, dim_hidden): tensor containing the features in the\r\n          hidden state `h` of encoder. Used as the initial hidden state of the decoder. (default `None`)\r\n        - **encoder_outputs** (batch, seq_len, dim_hidden * num_directions): (default is `None`).\r\n        - **targets** (batch, max_length): targets labels of the ground truth sentences\r\n\r\n        Outputs: seq_probs,\r\n        - **seq_logprobs** (batch_size, max_length, vocab_size): tensors containing the outputs of the decoding function.\r\n        - **seq_preds** (batch_size, max_length): predicted symbols\r\n        """"""\r\n        sample_max = opt.get(\'sample_max\', 1)\r\n        beam_size = opt.get(\'beam_size\', 1)\r\n        temperature = opt.get(\'temperature\', 1.0)\r\n\r\n        batch_size, _, _ = encoder_outputs.size()\r\n        decoder_hidden = self._init_rnn_state(encoder_hidden)\r\n\r\n        seq_logprobs = []\r\n        seq_preds = []\r\n        self.rnn.flatten_parameters()\r\n        if mode == \'train\':\r\n            # use targets as rnn inputs\r\n            targets_emb = self.embedding(targets)\r\n            for i in range(self.max_length - 1):\r\n                current_words = targets_emb[:, i, :]\r\n                context = self.attention(decoder_hidden.squeeze(0), encoder_outputs)\r\n                decoder_input = torch.cat([current_words, context], dim=1)\r\n                decoder_input = self.input_dropout(decoder_input).unsqueeze(1)\r\n                decoder_output, decoder_hidden = self.rnn(\r\n                    decoder_input, decoder_hidden)\r\n                logprobs = F.log_softmax(\r\n                    self.out(decoder_output.squeeze(1)), dim=1)\r\n                seq_logprobs.append(logprobs.unsqueeze(1))\r\n\r\n            seq_logprobs = torch.cat(seq_logprobs, 1)\r\n\r\n        elif mode == \'inference\':\r\n            if beam_size > 1:\r\n                return self.sample_beam(encoder_outputs, decoder_hidden, opt)\r\n\r\n            for t in range(self.max_length - 1):\r\n                context = self.attention(\r\n                    decoder_hidden.squeeze(0), encoder_outputs)\r\n\r\n                if t == 0:  # input <bos>\r\n                    it = torch.LongTensor([self.sos_id] * batch_size).cuda()\r\n                elif sample_max:\r\n                    sampleLogprobs, it = torch.max(logprobs, 1)\r\n                    seq_logprobs.append(sampleLogprobs.view(-1, 1))\r\n                    it = it.view(-1).long()\r\n\r\n                else:\r\n                    # sample according to distribuition\r\n                    if temperature == 1.0:\r\n                        prob_prev = torch.exp(logprobs)\r\n                    else:\r\n                        # scale logprobs by temperature\r\n                        prob_prev = torch.exp(torch.div(logprobs, temperature))\r\n                    it = torch.multinomial(prob_prev, 1).cuda()\r\n                    sampleLogprobs = logprobs.gather(1, it)\r\n                    seq_logprobs.append(sampleLogprobs.view(-1, 1))\r\n                    it = it.view(-1).long()\r\n\r\n                seq_preds.append(it.view(-1, 1))\r\n\r\n                xt = self.embedding(it)\r\n                decoder_input = torch.cat([xt, context], dim=1)\r\n                decoder_input = self.input_dropout(decoder_input).unsqueeze(1)\r\n                decoder_output, decoder_hidden = self.rnn(\r\n                    decoder_input, decoder_hidden)\r\n                logprobs = F.log_softmax(\r\n                    self.out(decoder_output.squeeze(1)), dim=1)\r\n\r\n            seq_logprobs = torch.cat(seq_logprobs, 1)\r\n            seq_preds = torch.cat(seq_preds[1:], 1)\r\n\r\n        return seq_logprobs, seq_preds\r\n\r\n    def _init_weights(self):\r\n        """""" init the weight of some layers\r\n        """"""\r\n        nn.init.xavier_normal_(self.out.weight)\r\n\r\n    def _init_rnn_state(self, encoder_hidden):\r\n        """""" Initialize the encoder hidden state. """"""\r\n        if encoder_hidden is None:\r\n            return None\r\n        if isinstance(encoder_hidden, tuple):\r\n            encoder_hidden = tuple(\r\n                [self._cat_directions(h) for h in encoder_hidden])\r\n        else:\r\n            encoder_hidden = self._cat_directions(encoder_hidden)\r\n        return encoder_hidden\r\n\r\n    def _cat_directions(self, h):\r\n        """""" If the encoder is bidirectional, do the following transformation.\r\n            (#directions * #layers, #batch, dim_hidden) -> (#layers, #batch, #directions * dim_hidden)\r\n        """"""\r\n        if self.bidirectional_encoder:\r\n            h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\r\n        return h\r\n'"
models/EncoderRNN.py,1,"b'import torch.nn as nn\r\n\r\n\r\nclass EncoderRNN(nn.Module):\r\n    def __init__(self, dim_vid, dim_hidden, input_dropout_p=0.2, rnn_dropout_p=0.5,\r\n                 n_layers=1, bidirectional=False, rnn_cell=\'gru\'):\r\n        """"""\r\n\r\n        Args:\r\n            hidden_dim (int): dim of hidden state of rnn\r\n            input_dropout_p (int): dropout probability for the input sequence\r\n            dropout_p (float): dropout probability for the output sequence\r\n            n_layers (int): number of rnn layers\r\n            rnn_cell (str): type of RNN cell (\'LSTM\'/\'GRU\')\r\n        """"""\r\n        super(EncoderRNN, self).__init__()\r\n        self.dim_vid = dim_vid\r\n        self.dim_hidden = dim_hidden\r\n        self.input_dropout_p = input_dropout_p\r\n        self.rnn_dropout_p = rnn_dropout_p\r\n        self.n_layers = n_layers\r\n        self.bidirectional = bidirectional\r\n        self.rnn_cell = rnn_cell\r\n\r\n        self.vid2hid = nn.Linear(dim_vid, dim_hidden)\r\n        self.input_dropout = nn.Dropout(input_dropout_p)\r\n\r\n        if rnn_cell.lower() == \'lstm\':\r\n            self.rnn_cell = nn.LSTM\r\n        elif rnn_cell.lower() == \'gru\':\r\n            self.rnn_cell = nn.GRU\r\n\r\n        self.rnn = self.rnn_cell(dim_hidden, dim_hidden, n_layers, batch_first=True,\r\n                                bidirectional=bidirectional, dropout=self.rnn_dropout_p)\r\n\r\n        self._init_hidden()\r\n\r\n    def _init_hidden(self):\r\n        nn.init.xavier_normal_(self.vid2hid.weight)\r\n\r\n    def forward(self, vid_feats):\r\n        """"""\r\n        Applies a multi-layer RNN to an input sequence.\r\n        Args:\r\n            input_var (batch, seq_len): tensor containing the features of the input sequence.\r\n            input_lengths (list of int, optional): A list that contains the lengths of sequences\r\n              in the mini-batch\r\n        Returns: output, hidden\r\n            - **output** (batch, seq_len, hidden_size): variable containing the encoded features of the input sequence\r\n            - **hidden** (num_layers * num_directions, batch, hidden_size): variable containing the features in the hidden state h\r\n        """"""\r\n        batch_size, seq_len, dim_vid = vid_feats.size()\r\n        vid_feats = self.vid2hid(vid_feats.view(-1, dim_vid))\r\n        vid_feats = self.input_dropout(vid_feats)\r\n        vid_feats = vid_feats.view(batch_size, seq_len, self.dim_hidden)\r\n        self.rnn.flatten_parameters()\r\n        output, hidden = self.rnn(vid_feats)\r\n        return output, hidden\r\n'"
models/S2VTAttModel.py,1,"b'import torch.nn as nn\r\n\r\n\r\nclass S2VTAttModel(nn.Module):\r\n    def __init__(self, encoder, decoder):\r\n        """"""\r\n\r\n        Args:\r\n            encoder (nn.Module): Encoder rnn\r\n            decoder (nn.Module): Decoder rnn\r\n        """"""\r\n        super(S2VTAttModel, self).__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n\r\n    def forward(self, vid_feats, target_variable=None,\r\n                mode=\'train\', opt={}):\r\n        """"""\r\n\r\n        Args:\r\n            vid_feats (Variable): video feats of shape [batch_size, seq_len, dim_vid]\r\n            target_variable (None, optional): groung truth labels\r\n\r\n        Returns:\r\n            seq_prob: Variable of shape [batch_size, max_len-1, vocab_size]\r\n            seq_preds: [] or Variable of shape [batch_size, max_len-1]\r\n        """"""\r\n        encoder_outputs, encoder_hidden = self.encoder(vid_feats)\r\n        seq_prob, seq_preds = self.decoder(encoder_outputs, encoder_hidden, target_variable, mode, opt)\r\n        return seq_prob, seq_preds\r\n'"
models/S2VTModel.py,10,"b""import torch\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\nimport random\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass S2VTModel(nn.Module):\r\n    def __init__(self, vocab_size, max_len, dim_hidden, dim_word, dim_vid=2048, sos_id=1, eos_id=0,\r\n                 n_layers=1, rnn_cell='gru', rnn_dropout_p=0.2):\r\n        super(S2VTModel, self).__init__()\r\n        if rnn_cell.lower() == 'lstm':\r\n            self.rnn_cell = nn.LSTM\r\n        elif rnn_cell.lower() == 'gru':\r\n            self.rnn_cell = nn.GRU\r\n        self.rnn1 = self.rnn_cell(dim_vid, dim_hidden, n_layers,\r\n                                  batch_first=True, dropout=rnn_dropout_p)\r\n        self.rnn2 = self.rnn_cell(dim_hidden + dim_word, dim_hidden, n_layers,\r\n                                  batch_first=True, dropout=rnn_dropout_p)\r\n\r\n        self.dim_vid = dim_vid\r\n        self.dim_output = vocab_size\r\n        self.dim_hidden = dim_hidden\r\n        self.dim_word = dim_word\r\n        self.max_length = max_len\r\n        self.sos_id = sos_id\r\n        self.eos_id = eos_id\r\n        self.embedding = nn.Embedding(self.dim_output, self.dim_word)\r\n\r\n        self.out = nn.Linear(self.dim_hidden, self.dim_output)\r\n\r\n    def forward(self, vid_feats, target_variable=None,\r\n                mode='train', opt={}):\r\n        batch_size, n_frames, _ = vid_feats.shape\r\n        padding_words = Variable(vid_feats.data.new(batch_size, n_frames, self.dim_word)).zero_()\r\n        padding_frames = Variable(vid_feats.data.new(batch_size, 1, self.dim_vid)).zero_()\r\n        state1 = None\r\n        state2 = None\r\n        #self.rnn1.flatten_parameters()\r\n        #self.rnn2.flatten_parameters()\r\n        output1, state1 = self.rnn1(vid_feats, state1)\r\n        input2 = torch.cat((output1, padding_words), dim=2)\r\n        output2, state2 = self.rnn2(input2, state2)\r\n\r\n        seq_probs = []\r\n        seq_preds = []\r\n        if mode == 'train':\r\n            for i in range(self.max_length - 1):\r\n                # <eos> doesn't input to the network\r\n                current_words = self.embedding(target_variable[:, i])\r\n                self.rnn1.flatten_parameters()\r\n                self.rnn2.flatten_parameters()\r\n                output1, state1 = self.rnn1(padding_frames, state1)\r\n                input2 = torch.cat(\r\n                    (output1, current_words.unsqueeze(1)), dim=2)\r\n                output2, state2 = self.rnn2(input2, state2)\r\n                logits = self.out(output2.squeeze(1))\r\n                logits = F.log_softmax(logits, dim=1)\r\n                seq_probs.append(logits.unsqueeze(1))\r\n            seq_probs = torch.cat(seq_probs, 1)\r\n\r\n        else:\r\n            current_words = self.embedding(\r\n                Variable(torch.LongTensor([self.sos_id] * batch_size)).cuda())\r\n            for i in range(self.max_length - 1):\r\n                self.rnn1.flatten_parameters()\r\n                self.rnn2.flatten_parameters()\r\n                output1, state1 = self.rnn1(padding_frames, state1)\r\n                input2 = torch.cat(\r\n                    (output1, current_words.unsqueeze(1)), dim=2)\r\n                output2, state2 = self.rnn2(input2, state2)\r\n                logits = self.out(output2.squeeze(1))\r\n                logits = F.log_softmax(logits, dim=1)\r\n                seq_probs.append(logits.unsqueeze(1))\r\n                _, preds = torch.max(logits, 1)\r\n                current_words = self.embedding(preds)\r\n                seq_preds.append(preds.unsqueeze(1))\r\n            seq_probs = torch.cat(seq_probs, 1)\r\n            seq_preds = torch.cat(seq_preds, 1)\r\n        return seq_probs, seq_preds\r\n"""
models/__init__.py,0,b'from .EncoderRNN import EncoderRNN\r\nfrom .DecoderRNN import DecoderRNN\r\nfrom .S2VTAttModel import S2VTAttModel\r\nfrom .S2VTModel import S2VTModel\r\n'
coco-caption/pyciderevalcap/__init__.py,0,"b""__author__ = 'tylin'\n"""
coco-caption/pyciderevalcap/eval.py,0,"b'__author__ = \'rama\'\nfrom tokenizer.ptbtokenizer import PTBTokenizer\nfrom cider.cider import Cider\nfrom ciderD.ciderD import CiderD\n\n\nclass CIDErEvalCap:\n    def __init__(self, gts, res, df):\n        print \'tokenization...\'\n        tokenizer = PTBTokenizer(\'gts\')\n        _gts = tokenizer.tokenize(gts)\n        print \'tokenized refs\'\n        tokenizer = PTBTokenizer(\'res\')\n        _res = tokenizer.tokenize(res)\n        print \'tokenized cands\'\n\n        self.gts = _gts\n        self.res = _res\n        self.df = df\n\n    def evaluate(self):\n        # =================================================\n        # Set up scorers\n        # =================================================\n\n        print \'setting up scorers...\'\n        scorers = [\n            (Cider(df=self.df), ""CIDEr""), (CiderD(df=self.df), ""CIDErD"")\n        ]\n\n        # =================================================\n        # Compute scores\n        # =================================================\n        metric_scores = {}\n        for scorer, method in scorers:\n            print \'computing %s score...\' % (scorer.method())\n            score, scores = scorer.compute_score(self.gts, self.res)\n            print ""Mean %s score: %0.3f"" % (method, score)\n            metric_scores[method] = list(scores)\n        return metric_scores\n'"
coco-caption/pycocoevalcap/__init__.py,0,"b""__author__ = 'tylin'\n"""
coco-caption/pycocoevalcap/eval.py,0,"b'__author__ = \'tylin\'\nfrom .tokenizer.ptbtokenizer import PTBTokenizer\nfrom .bleu.bleu import Bleu\nfrom .meteor.meteor import Meteor\nfrom .rouge.rouge import Rouge\nfrom .cider.cider import Cider\nfrom .spice.spice import Spice\n\nclass COCOEvalCap:\n    def __init__(self, coco, cocoRes):\n        self.evalImgs = []\n        self.eval = {}\n        self.imgToEval = {}\n        self.coco = coco\n        self.cocoRes = cocoRes\n        self.params = {\'image_id\': coco.getImgIds()}\n\n    def evaluate(self):\n        imgIds = self.params[\'image_id\']\n        # imgIds = self.coco.getImgIds()\n        gts = {}\n        res = {}\n        for imgId in imgIds:\n            gts[imgId] = self.coco.imgToAnns[imgId]\n            res[imgId] = self.cocoRes.imgToAnns[imgId]\n\n        # =================================================\n        # Set up scorers\n        # =================================================\n        print(\'tokenization...\')\n        tokenizer = PTBTokenizer()\n        gts  = tokenizer.tokenize(gts)\n        res = tokenizer.tokenize(res)\n\n        # =================================================\n        # Set up scorers\n        # =================================================\n        print(\'setting up scorers...\')\n        scorers = [\n            (Bleu(4), [""Bleu_1"", ""Bleu_2"", ""Bleu_3"", ""Bleu_4""]),\n            (Meteor(),""METEOR""),\n            (Rouge(), ""ROUGE_L""),\n            (Cider(), ""CIDEr""),\n            (Spice(), ""SPICE"")\n        ]\n\n        # =================================================\n        # Compute scores\n        # =================================================\n        for scorer, method in scorers:\n            print(\'computing %s score...\'%(scorer.method()))\n            score, scores = scorer.compute_score(gts, res)\n            if type(method) == list:\n                for sc, scs, m in zip(score, scores, method):\n                    self.setEval(sc, m)\n                    self.setImgToEvalImgs(scs, gts.keys(), m)\n                    print(""%s: %0.3f""%(m, sc))\n            else:\n                self.setEval(score, method)\n                self.setImgToEvalImgs(scores, gts.keys(), method)\n                print(""%s: %0.3f""%(method, score))\n        self.setEvalImgs()\n\n    def setEval(self, score, method):\n        self.eval[method] = score\n\n    def setImgToEvalImgs(self, scores, imgIds, method):\n        for imgId, score in zip(sorted(imgIds), scores):\n            if not imgId in self.imgToEval:\n                self.imgToEval[imgId] = {}\n                self.imgToEval[imgId][""image_id""] = imgId\n            self.imgToEval[imgId][method] = score\n\n    def setEvalImgs(self):\n        self.evalImgs = [self.imgToEval[imgId] for imgId in sorted(self.imgToEval.keys())]\n'"
coco-caption/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
coco-caption/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\n\ndef _isArrayLike(obj):\n    return hasattr(obj, \'__iter__\') and hasattr(obj, \'__len__\')\n\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if _isArrayLike(catNms) else [catNms]\n        supNms = supNms if _isArrayLike(supNms) else [supNms]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
coco-caption/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n        scores      = -np.ones((T,R,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n                    dtScoresSorted = dtScores[inds]\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n                        ss = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                                ss[ri] = dtScoresSorted[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n                        scores[t,:,k,a,m] = np.array(ss)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n            \'scores\': scores,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
coco-caption/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
coco-caption/pyciderevalcap/cider/__init__.py,0,"b""__author__ = 'tylin'\n"""
coco-caption/pyciderevalcap/cider/cider.py,0,"b'# Filename: cider.py\n#\n#\n# Description: Describes the class to compute the CIDEr\n# (Consensus-Based Image Description Evaluation) Metric\n#          by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)\n#\n# Creation Date: Sun Feb  8 14:16:54 2015\n#\n# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and\n# Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom cider_scorer import CiderScorer\n\n\nclass Cider:\n    """"""\n    Main Class to compute the CIDEr metric\n\n    """"""\n    def __init__(self, n=4, df=""corpus""):\n        """"""\n        Initialize the CIDEr scoring function\n        : param n (int): n-gram size\n        : param df (string): specifies where to get the IDF values from\n                    takes values \'corpus\', \'coco-train\'\n        : return: None\n        """"""\n        # set cider to sum over 1 to 4-grams\n        self._n = n\n        self._df = df\n        self.cider_scorer = CiderScorer(n=self._n, df_mode=self._df)\n\n    def compute_score(self, gts, res):\n        """"""\n        Main function to compute CIDEr score\n        : param  gts (dict) : {image:tokenized reference sentence}\n        : param res (dict)  : {image:tokenized candidate sentence}\n        : return: cider (float) : computed CIDEr score for the corpus\n        """"""\n\n        # clear all the previous hypos and refs\n        self.cider_scorer.clear()\n\n        for res_id in res:\n\n            hypo = res_id[\'caption\']\n            ref = gts[res_id[\'image_id\']]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) > 0)\n            self.cider_scorer += (hypo[0], ref)\n\n        (score, scores) = self.cider_scorer.compute_score()\n\n        return score, scores\n\n    def method(self):\n        return ""CIDEr""\n'"
coco-caption/pyciderevalcap/cider/cider_scorer.py,0,"b'#!/usr/bin/env python\n# Tsung-Yi Lin <tl483@cornell.edu>\n# Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport copy\nimport pickle\nfrom collections import defaultdict\nimport numpy as np\nimport math\nimport os\n\ndef precook(s, n=4, out=False):\n    """"""\n    Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\n    :param s: string : sentence to be converted into ngrams\n    :param n: int    : number of ngrams for which representation is calculated\n    :return: term frequency vector for occuring ngrams\n    """"""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in xrange(1,n+1):\n        for i in xrange(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return counts\n\ndef cook_refs(refs, n=4): ## lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\n    :param refs: list of string : reference sentences for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (list of dict)\n    \'\'\'\n    return [precook(ref, n) for ref in refs]\n\ndef cook_test(test, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\n    :param test: list of string : hypothesis sentence for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (dict)\n    \'\'\'\n    return precook(test, n, True)\n\nclass CiderScorer(object):\n    """"""CIDEr scorer.\n    """"""\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = CiderScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        return new\n\n    def __init__(self, df_mode=""corpus"", test=None, refs=None, n=4, sigma=6.0):\n        \'\'\' singular instance \'\'\'\n        self.n = n\n        self.sigma = sigma\n        self.crefs = []\n        self.ctest = []\n        self.df_mode = df_mode\n        if self.df_mode != ""corpus"":\n            self.document_frequency = pickle.load(open(os.path.join(\'data\', df_mode + \'.p\'),\'r\'))       \n        self.cook_append(test, refs)\n        self.ref_len = None\n    \n    def clear(self):\n        self.crefs = []\n        self.ctest = []\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to avoid creating new instances.\'\'\'\n\n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                self.ctest.append(cook_test(test)) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if type(other) is tuple:\n            ## avoid creating new CiderScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n\n        return self\n    def compute_doc_freq(self):\n        \'\'\'\n        Compute term frequency for reference data.\n        This will be used to compute idf (inverse document frequency later)\n        The term frequency is stored in the object\n        :return: None\n        \'\'\'\n        for refs in self.crefs:\n            # refs, k ref captions of one image\n            for ngram in set([ngram for ref in refs for (ngram,count) in ref.iteritems()]):\n                self.document_frequency[ngram] += 1\n            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    def compute_cider(self):\n        def counts2vec(cnts):\n            """"""\n            Function maps counts of ngram to vector of tfidf weights.\n            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n            The n-th entry of array denotes length of n-grams.\n            :param cnts:\n            :return: vec (array of dict), norm (array of float), length (int)\n            """"""\n            vec = [defaultdict(float) for _ in range(self.n)]\n            length = 0\n            norm = [0.0 for _ in range(self.n)]\n            for (ngram,term_freq) in cnts.iteritems():\n                # give word count 1 if it doesn\'t appear in reference corpus\n                df = np.log(max(1.0, self.document_frequency[ngram]))\n                # ngram index\n                n = len(ngram)-1\n                # tf (term_freq) * idf (precomputed idf) for n-grams\n                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n                # compute norm for the vector.  the norm will be used for\n                # computing similarity\n                norm[n] += pow(vec[n][ngram], 2)\n\n                if n == 1:\n                    length += term_freq\n            norm = [np.sqrt(n) for n in norm]\n            return vec, norm, length\n\n        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            \'\'\'\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            \'\'\'\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].iteritems():\n                    val[n] += vec_hyp[n][ngram] * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n            return val\n\n        # compute log reference length\n        if self.df_mode == ""corpus"":\n            self.ref_len = np.log(float(len(self.crefs)))\n        elif self.df_mode == ""coco-val"":\n            # if coco option selected, use length of coco-val set\n            self.ref_len = np.log(float(40504))\n\n        scores = []\n        for test, refs in zip(self.ctest, self.crefs):\n            # compute vector for test captions\n            vec, norm, length = counts2vec(test)\n            # compute vector for ref captions\n            score = np.array([0.0 for _ in range(self.n)])\n            for ref in refs:\n                vec_ref, norm_ref, length_ref = counts2vec(ref)\n                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n            # change by vrama91 - mean of ngram scores, instead of sum\n            score_avg = np.mean(score)\n            # divide by number of references\n            score_avg /= len(refs)\n            # multiply score by 10\n            score_avg *= 10.0\n            # append score of an image to the score list\n            scores.append(score_avg)\n        return scores\n\n    def compute_score(self, option=None, verbose=0):\n        # compute idf\n        if self.df_mode == ""corpus"":\n            self.document_frequency = defaultdict(float)\n            self.compute_doc_freq()\n            # assert to check document frequency\n            assert(len(self.ctest) >= max(self.document_frequency.values()))\n            # import json for now and write the corresponding files\n        # compute cider score\n        score = self.compute_cider()\n        # debug\n        # print score\n        return np.mean(np.array(score)), np.array(score)\n'"
coco-caption/pyciderevalcap/ciderD/__init__.py,0,"b""__author__ = 'tylin'\n"""
coco-caption/pyciderevalcap/ciderD/ciderD.py,0,"b'# Filename: ciderD.py\n#\n# Description: Describes the class to compute the CIDEr-D (Consensus-Based Image Description Evaluation) Metric\n#               by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)\n#\n# Creation Date: Sun Feb  8 14:16:54 2015\n#\n# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom .ciderD_scorer import CiderScorer\n\n\nclass CiderD:\n    """"""\n    Main Class to compute the CIDEr metric\n\n    """"""\n    def __init__(self, n=4, sigma=6.0, df=""corpus""):\n        # set cider to sum over 1 to 4-grams\n        self._n = n\n        # set the standard deviation parameter for gaussian penalty\n        self._sigma = sigma\n        # set which where to compute document frequencies from\n        self._df = df\n        self.cider_scorer = CiderScorer(n=self._n, df_mode=self._df)\n\n    def compute_score(self, gts, res):\n        """"""\n        Main function to compute CIDEr score\n        :param  hypo_for_image (dict) : dictionary with key <image> and value <tokenized hypothesis / candidate sentence>\n                ref_for_image (dict)  : dictionary with key <image> and value <tokenized reference sentence>\n        :return: cider (float) : computed CIDEr score for the corpus\n        """"""\n\n        # clear all the previous hypos and refs\n        self.cider_scorer.clear()\n        for res_id in res:\n\n            hypo = res_id[\'caption\']\n            ref = gts[res_id[\'image_id\']]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) > 0)\n            self.cider_scorer += (hypo[0], ref)\n\n        (score, scores) = self.cider_scorer.compute_score()\n\n        return score, scores\n\n    def method(self):\n        return ""CIDEr-D""\n'"
coco-caption/pyciderevalcap/ciderD/ciderD_scorer.py,0,"b'#!/usr/bin/env python\n# Tsung-Yi Lin <tl483@cornell.edu>\n# Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport copy\nfrom collections import defaultdict\nimport numpy as np\nimport pdb\nimport math\nimport pickle\nimport os\n\ndef precook(s, n=4, out=False):\n    """"""\n    Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\n    :param s: string : sentence to be converted into ngrams\n    :param n: int    : number of ngrams for which representation is calculated\n    :return: term frequency vector for occuring ngrams\n    """"""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return counts\n\ndef cook_refs(refs, n=4): ## lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\n    :param refs: list of string : reference sentences for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (list of dict)\n    \'\'\'\n    return [precook(ref, n) for ref in refs]\n\ndef cook_test(test, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\n    :param test: list of string : hypothesis sentence for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (dict)\n    \'\'\'\n    return precook(test, n, True)\n\nclass CiderScorer(object):\n    """"""CIDEr scorer.\n    """"""\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = CiderScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        return new\n\n    def __init__(self, df_mode=""corpus"", test=None, refs=None, n=4, sigma=6.0):\n        \'\'\' singular instance \'\'\'\n        self.n = n\n        self.sigma = sigma\n        self.crefs = []\n        self.ctest = []\n        self.df_mode = df_mode\n        self.ref_len = None\n        if self.df_mode != ""corpus"":\n            pkl_file = pickle.load(open(os.path.join(\'data\', df_mode + \'.p\'),\'rb\'))\n            self.ref_len = pkl_file[\'ref_len\']\n            self.document_frequency = pkl_file[\'document_frequency\']\n        self.cook_append(test, refs)\n    \n    def clear(self):\n        self.crefs = []\n        self.ctest = []\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to avoid creating new instances.\'\'\'\n\n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                self.ctest.append(cook_test(test)) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if type(other) is tuple:\n            ## avoid creating new CiderScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n\n        return self\n    def compute_doc_freq(self):\n        \'\'\'\n        Compute term frequency for reference data.\n        This will be used to compute idf (inverse document frequency later)\n        The term frequency is stored in the object\n        :return: None\n        \'\'\'\n        for refs in self.crefs:\n            # refs, k ref captions of one image\n            for ngram in set([ngram for ref in refs for (ngram,count) in ref.items()]):\n                self.document_frequency[ngram] += 1\n            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    def compute_cider(self):\n        def counts2vec(cnts):\n            """"""\n            Function maps counts of ngram to vector of tfidf weights.\n            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n            The n-th entry of array denotes length of n-grams.\n            :param cnts:\n            :return: vec (array of dict), norm (array of float), length (int)\n            """"""\n            vec = [defaultdict(float) for _ in range(self.n)]\n            length = 0\n            norm = [0.0 for _ in range(self.n)]\n            for (ngram,term_freq) in cnts.items():\n                # give word count 1 if it doesn\'t appear in reference corpus\n                df = np.log(max(1.0, self.document_frequency[ngram]))\n                # ngram index\n                n = len(ngram)-1\n                # tf (term_freq) * idf (precomputed idf) for n-grams\n                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n                # compute norm for the vector.  the norm will be used for computing similarity\n                norm[n] += pow(vec[n][ngram], 2)\n\n                if n == 1:\n                    length += term_freq\n            norm = [np.sqrt(n) for n in norm]\n            return vec, norm, length\n\n        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            \'\'\'\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            \'\'\'\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].items():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val\n\n        # compute log reference length\n        if self.df_mode == ""corpus"":\n            self.ref_len = np.log(float(len(self.crefs)))\n        #elif self.df_mode == ""coco-val"":\n            # if coco option selected, use length of coco-val set\n        #    self.ref_len = np.log(float(40504))\n\n        scores = []\n        for test, refs in zip(self.ctest, self.crefs):\n            # compute vector for test captions\n            vec, norm, length = counts2vec(test)\n            # compute vector for ref captions\n            score = np.array([0.0 for _ in range(self.n)])\n            for ref in refs:\n                vec_ref, norm_ref, length_ref = counts2vec(ref)\n                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n            # change by vrama91 - mean of ngram scores, instead of sum\n            score_avg = np.mean(score)\n            # divide by number of references\n            score_avg /= len(refs)\n            # multiply score by 10\n            score_avg *= 10.0\n            # append score of an image to the score list\n            scores.append(score_avg)\n        return scores\n\n    def compute_score(self, option=None, verbose=0):\n        # compute idf\n        if self.df_mode == ""corpus"":\n            self.document_frequency = defaultdict(float)\n            self.compute_doc_freq()\n            # assert to check document frequency\n            assert(len(self.ctest) >= max(self.document_frequency.values()))\n            # import json for now and write the corresponding files\n        # compute cider score\n        score = self.compute_cider()\n        # debug\n        # print score\n        return np.mean(np.array(score)), np.array(score)\n'"
coco-caption/pyciderevalcap/tokenizer/__init__.py,0,"b""__author__ = 'hfang'\n"""
coco-caption/pyciderevalcap/tokenizer/ptbtokenizer.py,0,"b'#!/usr/bin/env python\n# \n# File Name : ptbtokenizer.py\n#\n# Description : Do the PTB Tokenization and remove punctuations.\n#\n# Creation Date : 29-12-2014\n# Last Modified : Thu Mar 19 09:53:35 2015\n# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nimport os\nimport pdb # python debugger\nimport sys\nimport subprocess\nimport re\nimport tempfile\nimport itertools\n\n# path to the stanford corenlp jar\nSTANFORD_CORENLP_3_4_1_JAR = \'stanford-corenlp-3.4.1.jar\'\n\n# punctuations to be removed from the sentences\nPUNCTUATIONS = [""\'\'"", ""\'"", ""``"", ""`"", ""-LRB-"", ""-RRB-"", ""-LCB-"", ""-RCB-"", \\\n        ""."", ""?"", ""!"", "","", "":"", ""-"", ""--"", ""..."", "";""] \n\nclass PTBTokenizer:\n    """"""Python wrapper of Stanford PTBTokenizer""""""\n    def __init__(self, _source=\'gts\'):\n        self.source = _source\n\n    def tokenize(self, captions_for_image):\n        cmd = [\'java\', \'-cp\', STANFORD_CORENLP_3_4_1_JAR, \\\n                \'edu.stanford.nlp.process.PTBTokenizer\', \\\n                \'-preserveLines\', \'-lowerCase\']\n\n        # ======================================================\n        # prepare data for PTB Tokenizer\n        # ======================================================\n\n        if self.source == \'gts\':\n            image_id = [k for k, v in captions_for_image.items() for _ in range(len(v))]\n            sentences = \'\\n\'.join([c[\'caption\'].replace(\'\\n\', \' \') for k, v in captions_for_image.items() for c in v])\n            final_tokenized_captions_for_image = {}\n\n        elif self.source == \'res\':\n            index = [i for i, v in enumerate(captions_for_image)]\n            image_id = [v[""image_id""] for v in captions_for_image]\n            sentences = \'\\n\'.join(v[""caption""].replace(\'\\n\', \' \') for v in captions_for_image )\n            final_tokenized_captions_for_index = []\n\n        # ======================================================\n        # save sentences to temporary file\n        # ======================================================\n        path_to_jar_dirname=os.path.dirname(os.path.abspath(__file__))\n        tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=path_to_jar_dirname)\n        tmp_file.write(sentences)\n        tmp_file.close()\n\n        # ======================================================\n        # tokenize sentence\n        # ======================================================\n        cmd.append(os.path.basename(tmp_file.name))\n        p_tokenizer = subprocess.Popen(cmd, cwd=path_to_jar_dirname, \\\n                stdout=subprocess.PIPE)\n        token_lines = p_tokenizer.communicate(input=sentences.rstrip())[0]\n        lines = token_lines.split(\'\\n\')\n        # remove temp file\n        os.remove(tmp_file.name)\n\n        # ======================================================\n        # create dictionary for tokenized captions\n        # ======================================================\n        if self.source == \'gts\':\n            for k, line in zip(image_id, lines):\n                if not k in final_tokenized_captions_for_image:\n                    final_tokenized_captions_for_image[k] = []\n                tokenized_caption = \' \'.join([w for w in line.rstrip().split(\' \') \\\n                        if w not in PUNCTUATIONS])\n                final_tokenized_captions_for_image[k].append(tokenized_caption)\n\n            return final_tokenized_captions_for_image\n\n        elif self.source == \'res\':\n            for k, img, line in zip(index, image_id, lines):\n                tokenized_caption = \' \'.join([w for w in line.rstrip().split(\' \') \\\n                        if w not in PUNCTUATIONS])\n                final_tokenized_captions_for_index.append({\'image_id\': img, \'caption\': [tokenized_caption]})\n\n            return final_tokenized_captions_for_index\n'"
coco-caption/pycocoevalcap/bleu/__init__.py,0,"b""__author__ = 'tylin'\n"""
coco-caption/pycocoevalcap/bleu/bleu.py,0,"b'#!/usr/bin/env python\n# \n# File Name : bleu.py\n#\n# Description : Wrapper for BLEU scorer.\n#\n# Creation Date : 06-01-2015\n# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom .bleu_scorer import BleuScorer\n\n\nclass Bleu:\n    def __init__(self, n=4):\n        # default compute Blue score up to 4\n        self._n = n\n        self._hypo_for_image = {}\n        self.ref_for_image = {}\n\n    def compute_score(self, gts, res):\n\n        assert(sorted(gts.keys()) == sorted(res.keys()))\n        imgIds = sorted(gts.keys())\n\n        bleu_scorer = BleuScorer(n=self._n)\n        for id in imgIds:\n            hypo = res[id]\n            ref = gts[id]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) >= 1)\n\n            bleu_scorer += (hypo[0], ref)\n\n        #score, scores = bleu_scorer.compute_score(option=\'shortest\')\n        score, scores = bleu_scorer.compute_score(option=\'closest\', verbose=1)\n        #score, scores = bleu_scorer.compute_score(option=\'average\', verbose=1)\n\n        # return (bleu, bleu_info)\n        return score, scores\n\n    def method(self):\n        return ""Bleu""\n'"
coco-caption/pycocoevalcap/bleu/bleu_scorer.py,0,"b'#!/usr/bin/env python\n\n# bleu_scorer.py\n# David Chiang <chiang@isi.edu>\n\n# Copyright (c) 2004-2006 University of Maryland. All rights\n# reserved. Do not redistribute without permission from the\n# author. Not for commercial use.\n\n# Modified by: \n# Hao Fang <hfang@uw.edu>\n# Tsung-Yi Lin <tl483@cornell.edu>\n\n\'\'\'Provides:\ncook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\ncook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n\'\'\'\n\nimport copy\nimport sys, math, re\nfrom collections import defaultdict\n\ndef precook(s, n=4, out=False):\n    """"""Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.""""""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return (len(words), counts)\n\ndef cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\'\'\'\n\n    reflen = []\n    maxcounts = {}\n    for ref in refs:\n        rl, counts = precook(ref, n)\n        reflen.append(rl)\n        for (ngram,count) in counts.items():\n            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    # Calculate effective reference sentence length.\n    if eff == ""shortest"":\n        reflen = min(reflen)\n    elif eff == ""average"":\n        reflen = float(sum(reflen))/len(reflen)\n\n    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n    \n    ## lhuang: N.B.: in case of ""closest"", keep a list of reflens!! (bad design)\n\n    return (reflen, maxcounts)\n\ndef cook_test(test, reflen, refmaxcounts, eff=None, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\'\'\'\n\n    testlen, counts = precook(test, n, True)\n\n    result = {}\n\n    # Calculate effective reference sentence length.\n    \n    if eff == ""closest"":\n        result[""reflen""] = min((abs(l-testlen), l) for l in reflen)[1]\n    else: ## i.e., ""average"" or ""shortest"" or None\n        result[""reflen""] = reflen\n\n    result[""testlen""] = testlen\n\n    result[""guess""] = [max(0,testlen-k+1) for k in range(1,n+1)]\n\n    result[\'correct\'] = [0]*n\n    for (ngram, count) in counts.items():\n        result[""correct""][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n\n    return result\n\nclass BleuScorer(object):\n    """"""Bleu scorer.\n    """"""\n\n    __slots__ = ""n"", ""crefs"", ""ctest"", ""_score"", ""_ratio"", ""_testlen"", ""_reflen"", ""special_reflen""\n    # special_reflen is used in oracle (proportional effective ref len for a node).\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = BleuScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        new._score = None\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n        \'\'\' singular instance \'\'\'\n\n        self.n = n\n        self.crefs = []\n        self.ctest = []\n        self.cook_append(test, refs)\n        self.special_reflen = special_reflen\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to avoid creating new instances.\'\'\'\n        \n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                cooked_test = cook_test(test, *self.crefs[-1])\n                self.ctest.append(cooked_test) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n        self._score = None ## need to recompute\n\n    def ratio(self, option=None):\n        self.compute_score(option=option)\n        return self._ratio\n\n    def score_ratio(self, option=None):\n        \'\'\'return (bleu, len_ratio) pair\'\'\'\n        return (self.fscore(option=option), self.ratio(option=option))\n\n    def score_ratio_str(self, option=None):\n        return ""%.4f (%.2f)"" % self.score_ratio(option)\n\n    def reflen(self, option=None):\n        self.compute_score(option=option)\n        return self._reflen\n\n    def testlen(self, option=None):\n        self.compute_score(option=option)\n        return self._testlen        \n\n    def retest(self, new_test):\n        if type(new_test) is str:\n            new_test = [new_test]\n        assert len(new_test) == len(self.crefs), new_test\n        self.ctest = []\n        for t, rs in zip(new_test, self.crefs):\n            self.ctest.append(cook_test(t, *rs))\n        self._score = None\n\n        return self\n\n    def rescore(self, new_test):\n        \'\'\' replace test(s) with new test(s), and returns the new score.\'\'\'\n        \n        return self.retest(new_test).compute_score()\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if type(other) is tuple:\n            ## avoid creating new BleuScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            assert self.compatible(other), ""incompatible BLEUs.""\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n            self._score = None ## need to recompute\n\n        return self        \n\n    def compatible(self, other):\n        return isinstance(other, BleuScorer) and self.n == other.n\n\n    def single_reflen(self, option=""average""):\n        return self._single_reflen(self.crefs[0][0], option)\n\n    def _single_reflen(self, reflens, option=None, testlen=None):\n        \n        if option == ""shortest"":\n            reflen = min(reflens)\n        elif option == ""average"":\n            reflen = float(sum(reflens))/len(reflens)\n        elif option == ""closest"":\n            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n        else:\n            assert False, ""unsupported reflen option %s"" % option\n\n        return reflen\n\n    def recompute_score(self, option=None, verbose=0):\n        self._score = None\n        return self.compute_score(option, verbose)\n        \n    def compute_score(self, option=None, verbose=0):\n        n = self.n\n        small = 1e-9\n        tiny = 1e-15 ## so that if guess is 0 still return 0\n        bleu_list = [[] for _ in range(n)]\n\n        if self._score is not None:\n            return self._score\n\n        if option is None:\n            option = ""average"" if len(self.crefs) == 1 else ""closest""\n\n        self._testlen = 0\n        self._reflen = 0\n        totalcomps = {\'testlen\':0, \'reflen\':0, \'guess\':[0]*n, \'correct\':[0]*n}\n\n        # for each sentence\n        for comps in self.ctest:            \n            testlen = comps[\'testlen\']\n            self._testlen += testlen\n\n            if self.special_reflen is None: ## need computation\n                reflen = self._single_reflen(comps[\'reflen\'], option, testlen)\n            else:\n                reflen = self.special_reflen\n\n            self._reflen += reflen\n                \n            for key in [\'guess\',\'correct\']:\n                for k in range(n):\n                    totalcomps[key][k] += comps[key][k]\n\n            # append per image bleu score\n            bleu = 1.\n            for k in range(n):\n                bleu *= (float(comps[\'correct\'][k]) + tiny) \\\n                        /(float(comps[\'guess\'][k]) + small) \n                bleu_list[k].append(bleu ** (1./(k+1)))\n            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n            if ratio < 1:\n                for k in range(n):\n                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n\n            if verbose > 1:\n                print(comps, reflen)\n\n        totalcomps[\'reflen\'] = self._reflen\n        totalcomps[\'testlen\'] = self._testlen\n\n        bleus = []\n        bleu = 1.\n        for k in range(n):\n            bleu *= float(totalcomps[\'correct\'][k] + tiny) \\\n                    / (totalcomps[\'guess\'][k] + small)\n            bleus.append(bleu ** (1./(k+1)))\n        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n        if ratio < 1:\n            for k in range(n):\n                bleus[k] *= math.exp(1 - 1/ratio)\n\n        if verbose > 0:\n            print(totalcomps)\n            print(""ratio:%f""%ratio)\n\n        self._score = bleus\n        return self._score, bleu_list\n'"
coco-caption/pycocoevalcap/cider/__init__.py,0,"b""__author__ = 'tylin'\n"""
coco-caption/pycocoevalcap/cider/cider.py,0,"b'# Filename: cider.py\n#\n# Description: Describes the class to compute the CIDEr (Consensus-Based Image Description Evaluation) Metric \n#               by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)\n#\n# Creation Date: Sun Feb  8 14:16:54 2015\n#\n# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom .cider_scorer import CiderScorer\nimport pdb\n\nclass Cider:\n    """"""\n    Main Class to compute the CIDEr metric \n\n    """"""\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        # set cider to sum over 1 to 4-grams\n        self._n = n\n        # set the standard deviation parameter for gaussian penalty\n        self._sigma = sigma\n\n    def compute_score(self, gts, res):\n        """"""\n        Main function to compute CIDEr score\n        :param  hypo_for_image (dict) : dictionary with key <image> and value <tokenized hypothesis / candidate sentence>\n                ref_for_image (dict)  : dictionary with key <image> and value <tokenized reference sentence>\n        :return: cider (float) : computed CIDEr score for the corpus \n        """"""\n\n        assert(sorted(gts.keys()) == sorted(res.keys()))\n        imgIds = sorted(gts.keys())\n\n        cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n\n        for id in imgIds:\n            hypo = res[id]\n            ref = gts[id]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) >= 1)\n\n            cider_scorer += (hypo[0], ref)\n\n        (score, scores) = cider_scorer.compute_score()\n\n        return score, scores\n\n    def method(self):\n        return ""CIDEr""\n'"
coco-caption/pycocoevalcap/cider/cider_scorer.py,0,"b'#!/usr/bin/env python\n# Tsung-Yi Lin <tl483@cornell.edu>\n# Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport copy\nfrom collections import defaultdict\nimport numpy as np\nimport pdb\nimport math\n\ndef precook(s, n=4, out=False):\n    """"""\n    Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\n    :param s: string : sentence to be converted into ngrams\n    :param n: int    : number of ngrams for which representation is calculated\n    :return: term frequency vector for occuring ngrams\n    """"""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return counts\n\ndef cook_refs(refs, n=4): ## lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\n    :param refs: list of string : reference sentences for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (list of dict)\n    \'\'\'\n    return [precook(ref, n) for ref in refs]\n\ndef cook_test(test, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\n    :param test: list of string : hypothesis sentence for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (dict)\n    \'\'\'\n    return precook(test, n, True)\n\nclass CiderScorer(object):\n    """"""CIDEr scorer.\n    """"""\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = CiderScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        \'\'\' singular instance \'\'\'\n        self.n = n\n        self.sigma = sigma\n        self.crefs = []\n        self.ctest = []\n        self.document_frequency = defaultdict(float)\n        self.cook_append(test, refs)\n        self.ref_len = None\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to avoid creating new instances.\'\'\'\n\n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                self.ctest.append(cook_test(test)) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if type(other) is tuple:\n            ## avoid creating new CiderScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n\n        return self\n    def compute_doc_freq(self):\n        \'\'\'\n        Compute term frequency for reference data.\n        This will be used to compute idf (inverse document frequency later)\n        The term frequency is stored in the object\n        :return: None\n        \'\'\'\n        for refs in self.crefs:\n            # refs, k ref captions of one image\n            for ngram in set([ngram for ref in refs for (ngram,count) in ref.items()]):\n                self.document_frequency[ngram] += 1\n            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    def compute_cider(self):\n        def counts2vec(cnts):\n            """"""\n            Function maps counts of ngram to vector of tfidf weights.\n            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n            The n-th entry of array denotes length of n-grams.\n            :param cnts:\n            :return: vec (array of dict), norm (array of float), length (int)\n            """"""\n            vec = [defaultdict(float) for _ in range(self.n)]\n            length = 0\n            norm = [0.0 for _ in range(self.n)]\n            for (ngram,term_freq) in cnts.items():\n                # give word count 1 if it doesn\'t appear in reference corpus\n                df = np.log(max(1.0, self.document_frequency[ngram]))\n                # ngram index\n                n = len(ngram)-1\n                # tf (term_freq) * idf (precomputed idf) for n-grams\n                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n                # compute norm for the vector.  the norm will be used for computing similarity\n                norm[n] += pow(vec[n][ngram], 2)\n\n                if n == 1:\n                    length += term_freq\n            norm = [np.sqrt(n) for n in norm]\n            return vec, norm, length\n\n        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            \'\'\'\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            \'\'\'\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].items():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val\n\n        # compute log reference length\n        self.ref_len = np.log(float(len(self.crefs)))\n\n        scores = []\n        for test, refs in zip(self.ctest, self.crefs):\n            # compute vector for test captions\n            vec, norm, length = counts2vec(test)\n            # compute vector for ref captions\n            score = np.array([0.0 for _ in range(self.n)])\n            for ref in refs:\n                vec_ref, norm_ref, length_ref = counts2vec(ref)\n                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n            # change by vrama91 - mean of ngram scores, instead of sum\n            score_avg = np.mean(score)\n            # divide by number of references\n            score_avg /= len(refs)\n            # multiply score by 10\n            score_avg *= 10.0\n            # append score of an image to the score list\n            scores.append(score_avg)\n        return scores\n\n    def compute_score(self, option=None, verbose=0):\n        # compute idf\n        self.compute_doc_freq()\n        # assert to check document frequency\n        assert(len(self.ctest) >= max(self.document_frequency.values()))\n        # compute cider score\n        score = self.compute_cider()\n        # debug\n        # print score\n        return np.mean(np.array(score)), np.array(score)'"
coco-caption/pycocoevalcap/meteor/__init__.py,0,"b""__author__ = 'tylin'\n"""
coco-caption/pycocoevalcap/meteor/meteor.py,0,"b'#!/usr/bin/env python\n\n# Python wrapper for METEOR implementation, by Xinlei Chen\n# Acknowledge Michael Denkowski for the generous discussion and help \n\nimport os\nimport sys\nimport subprocess\nimport threading\n\n# Assumes meteor-1.5.jar is in the same directory as meteor.py.  Change as needed.\nMETEOR_JAR = \'meteor-1.5.jar\'\n# print METEOR_JAR\n\nclass Meteor:\n\n    def __init__(self):\n        self.meteor_cmd = [\'java\', \'-jar\', \'-Xmx2G\', METEOR_JAR, \\\n                \'-\', \'-\', \'-stdio\', \'-l\', \'en\', \'-norm\']\n        self.meteor_p = subprocess.Popen(self.meteor_cmd, \\\n                cwd=os.path.dirname(os.path.abspath(__file__)), \\\n                stdin=subprocess.PIPE, \\\n                stdout=subprocess.PIPE, \\\n                stderr=subprocess.PIPE)\n        # Used to guarantee thread safety\n        self.lock = threading.Lock()\n\n    def compute_score(self, gts, res):\n        assert(sorted(gts.keys()) == sorted(res.keys()))\n        imgIds = sorted(gts.keys())\n        scores = []\n\n        eval_line = \'EVAL\'\n        self.lock.acquire()\n        for i in imgIds:\n            assert(len(res[i]) == 1)\n            stat = self._stat(res[i][0], gts[i])\n            eval_line += \' ||| {}\'.format(stat)\n\n        self.meteor_p.stdin.write(\'{}\\n\'.format(eval_line).encode())\n        self.meteor_p.stdin.flush()\n        for i in range(0, len(imgIds)):\n            scores.append(float(self.meteor_p.stdout.readline().decode().strip()))\n        score = float(self.meteor_p.stdout.readline().decode().strip())\n        self.lock.release()\n\n        return score, scores\n\n    def method(self):\n        return ""METEOR""\n\n    def _stat(self, hypothesis_str, reference_list):\n        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n        hypothesis_str = hypothesis_str.replace(\'|||\',\'\').replace(\'  \',\' \')\n        score_line = \' ||| \'.join((\'SCORE\', \' ||| \'.join(reference_list), hypothesis_str))\n        self.meteor_p.stdin.write(\'{}\\n\'.format(score_line).encode())\n        self.meteor_p.stdin.flush()\n        return self.meteor_p.stdout.readline().decode().strip()\n\n    def _score(self, hypothesis_str, reference_list):\n        self.lock.acquire()\n        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n        hypothesis_str = hypothesis_str.replace(\'|||\',\'\').replace(\'  \',\' \')\n        score_line = \' ||| \'.join((\'SCORE\', \' ||| \'.join(reference_list), hypothesis_str))\n        self.meteor_p.stdin.write(\'{}\\n\'.format(score_line).encode())\n        self.meteor_p.stdin.flush()\n        stats = self.meteor_p.stdout.readline().decode().strip()\n        eval_line = \'EVAL ||| {}\'.format(stats)\n        # EVAL ||| stats \n        self.meteor_p.stdin.write(\'{}\\n\'.format(eval_line).encode())\n        self.meteor_p.stdin.flush()\n        score = float(self.meteor_p.stdout.readline().decode().strip())\n        # bug fix: there are two values returned by the jar file, one average, and one all, so do it twice\n        # thanks for Andrej for pointing this out\n        score = float(self.meteor_p.stdout.readline().strip())\n        self.lock.release()\n        return score\n \n    def __exit__(self):\n        self.lock.acquire()\n        self.meteor_p.stdin.close()\n        self.meteor_p.kill()\n        self.meteor_p.wait()\n        self.lock.release()\n'"
coco-caption/pycocoevalcap/rouge/__init__.py,0,"b""__author__ = 'vrama91'\n"""
coco-caption/pycocoevalcap/rouge/rouge.py,0,"b'#!/usr/bin/env python\n# \n# File Name : rouge.py\n#\n# Description : Computes ROUGE-L metric as described by Lin and Hovey (2004)\n#\n# Creation Date : 2015-01-07 06:03\n# Author : Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport numpy as np\nimport pdb\n\ndef my_lcs(string, sub):\n    """"""\n    Calculates longest common subsequence for a pair of tokenized strings\n    :param string : list of str : tokens from a string split using whitespace\n    :param sub : list of str : shorter string, also split using whitespace\n    :returns: length (list of int): length of the longest common subsequence between the two strings\n\n    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n    """"""\n    if(len(string)< len(sub)):\n        sub, string = string, sub\n\n    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n\n    for j in range(1,len(sub)+1):\n        for i in range(1,len(string)+1):\n            if(string[i-1] == sub[j-1]):\n                lengths[i][j] = lengths[i-1][j-1] + 1\n            else:\n                lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n\n    return lengths[len(string)][len(sub)]\n\nclass Rouge():\n    \'\'\'\n    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\n\n    \'\'\'\n    def __init__(self):\n        # vrama91: updated the value below based on discussion with Hovey\n        self.beta = 1.2\n\n    def calc_score(self, candidate, refs):\n        """"""\n        Compute ROUGE-L score given one candidate and references for an image\n        :param candidate: str : candidate sentence to be evaluated\n        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n        """"""\n        assert(len(candidate)==1)\t\n        assert(len(refs)>0)         \n        prec = []\n        rec = []\n\n        # split into tokens\n        token_c = candidate[0].split("" "")\n    \t\n        for reference in refs:\n            # split into tokens\n            token_r = reference.split("" "")\n            # compute the longest common subsequence\n            lcs = my_lcs(token_r, token_c)\n            prec.append(lcs/float(len(token_c)))\n            rec.append(lcs/float(len(token_r)))\n\n        prec_max = max(prec)\n        rec_max = max(rec)\n\n        if(prec_max!=0 and rec_max !=0):\n            score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\n        else:\n            score = 0.0\n        return score\n\n    def compute_score(self, gts, res):\n        """"""\n        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\n        Invoked by evaluate_captions.py \n        :param hypo_for_image: dict : candidate / test sentences with ""image name"" key and ""tokenized sentences"" as values \n        :param ref_for_image: dict : reference MS-COCO sentences with ""image name"" key and ""tokenized sentences"" as values\n        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n        """"""\n        assert(sorted(gts.keys()) == sorted(res.keys()))\n        imgIds = sorted(gts.keys())\n\n        score = []\n        for id in imgIds:\n            hypo = res[id]\n            ref  = gts[id]\n\n            score.append(self.calc_score(hypo, ref))\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) >= 1)\n\n        average_score = np.mean(np.array(score))\n        return average_score, np.array(score)\n\n    def method(self):\n        return ""Rouge""\n'"
coco-caption/pycocoevalcap/tokenizer/__init__.py,0,"b""__author__ = 'hfang'\n"""
coco-caption/pycocoevalcap/tokenizer/ptbtokenizer.py,0,"b'#!/usr/bin/env python\n# \n# File Name : ptbtokenizer.py\n#\n# Description : Do the PTB Tokenization and remove punctuations.\n#\n# Creation Date : 29-12-2014\n# Last Modified : Thu Mar 19 09:53:35 2015\n# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nimport os\nimport sys\nimport subprocess\nimport tempfile\nimport itertools\n\n# path to the stanford corenlp jar\nSTANFORD_CORENLP_3_4_1_JAR = \'stanford-corenlp-3.4.1.jar\'\n\n# punctuations to be removed from the sentences\nPUNCTUATIONS = [""\'\'"", ""\'"", ""``"", ""`"", ""-LRB-"", ""-RRB-"", ""-LCB-"", ""-RCB-"", \\\n        ""."", ""?"", ""!"", "","", "":"", ""-"", ""--"", ""..."", "";""] \n\nclass PTBTokenizer:\n    """"""Python wrapper of Stanford PTBTokenizer""""""\n\n    def tokenize(self, captions_for_image):\n        cmd = [\'java\', \'-cp\', STANFORD_CORENLP_3_4_1_JAR, \\\n                \'edu.stanford.nlp.process.PTBTokenizer\', \\\n                \'-preserveLines\', \'-lowerCase\']\n\n        # ======================================================\n        # prepare data for PTB Tokenizer\n        # ======================================================\n        final_tokenized_captions_for_image = {}\n        image_id = [k for k, v in captions_for_image.items() for _ in range(len(v))]\n        sentences = \'\\n\'.join([c[\'caption\'].replace(\'\\n\', \' \') for k, v in captions_for_image.items() for c in v])\n\n        # ======================================================\n        # save sentences to temporary file\n        # ======================================================\n        path_to_jar_dirname=os.path.dirname(os.path.abspath(__file__))\n        tmp_file = tempfile.NamedTemporaryFile(mode=\'w+\', delete=False, dir=path_to_jar_dirname)\n        tmp_file.write(sentences)\n        tmp_file.close()\n\n        # ======================================================\n        # tokenize sentence\n        # ======================================================\n        cmd.append(os.path.basename(tmp_file.name))\n        p_tokenizer = subprocess.Popen(cmd, cwd=path_to_jar_dirname, \\\n                stdout=subprocess.PIPE)\n        token_lines = p_tokenizer.communicate(input=sentences.rstrip())[0]\n        lines = token_lines.decode().split(\'\\n\')\n        # remove temp file\n        os.remove(tmp_file.name)\n\n        # ======================================================\n        # create dictionary for tokenized captions\n        # ======================================================\n        for k, line in zip(image_id, lines):\n            if not k in final_tokenized_captions_for_image:\n                final_tokenized_captions_for_image[k] = []\n            tokenized_caption = \' \'.join([w for w in line.rstrip().split(\' \') \\\n                    if w not in PUNCTUATIONS])\n            final_tokenized_captions_for_image[k].append(tokenized_caption)\n\n        return final_tokenized_captions_for_image\n'"
