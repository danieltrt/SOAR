file_path,api_count,code
predict.py,5,"b'import os\nimport time\nimport torch\nimport numpy as np\nimport torch.backends.cudnn as cudnn\nfrom argparse import ArgumentParser\n# user\nfrom builders.model_builder import build_model\nfrom builders.dataset_builder import build_dataset_test\nfrom utils.utils import save_predict\nfrom utils.convert_state import convert_state_dict\n\n\ndef parse_args():\n    parser = ArgumentParser(description=\'Efficient semantic segmentation\')\n    # model and dataset\n    parser.add_argument(\'--model\', default=""ENet"", help=""model name: (default ENet)"")\n    parser.add_argument(\'--dataset\', default=""camvid"", help=""dataset: cityscapes or camvid"")\n    parser.add_argument(\'--num_workers\', type=int, default=2, help=""the number of parallel threads"")\n    parser.add_argument(\'--batch_size\', type=int, default=1,\n                        help="" the batch_size is set to 1 when evaluating or testing"")\n    parser.add_argument(\'--checkpoint\', type=str,default="""",\n                        help=""use the file to load the checkpoint for evaluating or testing "")\n    parser.add_argument(\'--save_seg_dir\', type=str, default=""./server/"",\n                        help=""saving path of prediction result"")\n    parser.add_argument(\'--cuda\', default=True, help=""run on CPU or GPU"")\n    parser.add_argument(""--gpus"", default=""0"", type=str, help=""gpu ids (default: 0)"")\n    args = parser.parse_args()\n\n    return args\n\n\n\ndef predict(args, test_loader, model):\n    """"""\n    args:\n      test_loader: loaded for test dataset, for those that do not provide label on the test set\n      model: model\n    return: class IoU and mean IoU\n    """"""\n    # evaluation or test mode\n    model.eval()\n    total_batches = len(test_loader)\n    for i, (input, size, name) in enumerate(test_loader):\n        with torch.no_grad():\n            input_var = input.cuda()\n        start_time = time.time()\n        output = model(input_var)\n        torch.cuda.synchronize()\n        time_taken = time.time() - start_time\n        print(\'[%d/%d]  time: %.2f\' % (i + 1, total_batches, time_taken))\n        output = output.cpu().data[0].numpy()\n        output = output.transpose(1, 2, 0)\n        output = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n\n        # Save the predict greyscale output for Cityscapes official evaluation\n        # Modify image name to meet official requirement\n        name[0] = name[0].rsplit(\'_\', 1)[0] + \'*\'\n        save_predict(output, None, name[0], args.dataset, args.save_seg_dir,\n                     output_grey=True, output_color=False, gt_color=False)\n\n\ndef test_model(args):\n    """"""\n     main function for testing\n     param args: global arguments\n     return: None\n    """"""\n    print(args)\n\n    if args.cuda:\n        print(""=====> use gpu id: \'{}\'"".format(args.gpus))\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpus\n        if not torch.cuda.is_available():\n            raise Exception(""no GPU found or wrong gpu id, please run without --cuda"")\n\n    # build the model\n    model = build_model(args.model, num_classes=args.classes)\n\n    if args.cuda:\n        model = model.cuda()  # using GPU for inference\n        cudnn.benchmark = True\n\n    if not os.path.exists(args.save_seg_dir):\n        os.makedirs(args.save_seg_dir)\n\n    # load the test set\n    datas, testLoader = build_dataset_test(args.dataset, args.num_workers, none_gt=True)\n\n    if args.checkpoint:\n        if os.path.isfile(args.checkpoint):\n            print(""=====> loading checkpoint \'{}\'"".format(args.checkpoint))\n            checkpoint = torch.load(args.checkpoint)\n            model.load_state_dict(checkpoint[\'model\'])\n            # model.load_state_dict(convert_state_dict(checkpoint[\'model\']))\n        else:\n            print(""=====> no checkpoint found at \'{}\'"".format(args.checkpoint))\n            raise FileNotFoundError(""no checkpoint found at \'{}\'"".format(args.checkpoint))\n\n    print(""=====> beginning testing"")\n    print(""test set length: "", len(testLoader))\n    predict(args, testLoader, model)\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n\n    args.save_seg_dir = os.path.join(args.save_seg_dir, args.dataset, \'predict\', args.model)\n\n    if args.dataset == \'cityscapes\':\n        args.classes = 19\n    elif args.dataset == \'camvid\':\n        args.classes = 11\n    else:\n        raise NotImplementedError(\n            ""This repository now supports two datasets: cityscapes and camvid, %s is not included"" % args.dataset)\n\n    test_model(args)\n'"
test.py,6,"b'import os\nimport time\nimport torch\nimport numpy as np\nimport torch.backends.cudnn as cudnn\nfrom argparse import ArgumentParser\n# user\nfrom builders.model_builder import build_model\nfrom builders.dataset_builder import build_dataset_test\nfrom utils.utils import save_predict\nfrom utils.metric.metric import get_iou\nfrom utils.convert_state import convert_state_dict\n\n\ndef parse_args():\n    parser = ArgumentParser(description=\'Efficient semantic segmentation\')\n    parser.add_argument(\'--model\', default=""ENet"", help=""model name: (default ENet)"")\n    parser.add_argument(\'--dataset\', default=""camvid"", help=""dataset: cityscapes or camvid"")\n    parser.add_argument(\'--num_workers\', type=int, default=1, help=""the number of parallel threads"")\n    parser.add_argument(\'--batch_size\', type=int, default=1,\n                        help="" the batch_size is set to 1 when evaluating or testing"")\n    parser.add_argument(\'--checkpoint\', type=str,default="""",\n                        help=""use the file to load the checkpoint for evaluating or testing "")\n    parser.add_argument(\'--save_seg_dir\', type=str, default=""./result/"",\n                        help=""saving path of prediction result"")\n    parser.add_argument(\'--best\', action=\'store_true\', help=""Get the best result among last few checkpoints"")\n    parser.add_argument(\'--save\', action=\'store_true\', help=""Save the predicted image"")\n    parser.add_argument(\'--cuda\', default=True, help=""run on CPU or GPU"")\n    parser.add_argument(""--gpus"", default=""0"", type=str, help=""gpu ids (default: 0)"")\n    args = parser.parse_args()\n\n    return args\n\n\n\n\ndef test(args, test_loader, model):\n    """"""\n    args:\n      test_loader: loaded for test dataset\n      model: model\n    return: class IoU and mean IoU\n    """"""\n    # evaluation or test mode\n    model.eval()\n    total_batches = len(test_loader)\n\n    data_list = []\n    for i, (input, label, size, name) in enumerate(test_loader):\n        with torch.no_grad():\n            input_var = input.cuda()\n        start_time = time.time()\n        output = model(input_var)\n        torch.cuda.synchronize()\n        time_taken = time.time() - start_time\n        print(\'[%d/%d]  time: %.2f\' % (i + 1, total_batches, time_taken))\n        output = output.cpu().data[0].numpy()\n        gt = np.asarray(label[0].numpy(), dtype=np.uint8)\n        output = output.transpose(1, 2, 0)\n        output = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n        data_list.append([gt.flatten(), output.flatten()])\n\n        # save the predicted image\n        if args.save:\n            save_predict(output, gt, name[0], args.dataset, args.save_seg_dir,\n                         output_grey=False, output_color=True, gt_color=True)\n\n    meanIoU, per_class_iu = get_iou(data_list, args.classes)\n    return meanIoU, per_class_iu\n\n\ndef test_model(args):\n    """"""\n     main function for testing\n     param args: global arguments\n     return: None\n    """"""\n    print(args)\n\n    if args.cuda:\n        print(""=====> use gpu id: \'{}\'"".format(args.gpus))\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpus\n        if not torch.cuda.is_available():\n            raise Exception(""no GPU found or wrong gpu id, please run without --cuda"")\n\n    # build the model\n    model = build_model(args.model, num_classes=args.classes)\n\n    if args.cuda:\n        model = model.cuda()  # using GPU for inference\n        cudnn.benchmark = True\n\n    if args.save:\n        if not os.path.exists(args.save_seg_dir):\n            os.makedirs(args.save_seg_dir)\n\n    # load the test set\n    datas, testLoader = build_dataset_test(args.dataset, args.num_workers)\n\n    if not args.best:\n        if args.checkpoint:\n            if os.path.isfile(args.checkpoint):\n                print(""=====> loading checkpoint \'{}\'"".format(args.checkpoint))\n                checkpoint = torch.load(args.checkpoint)\n                model.load_state_dict(checkpoint[\'model\'])\n                # model.load_state_dict(convert_state_dict(checkpoint[\'model\']))\n            else:\n                print(""=====> no checkpoint found at \'{}\'"".format(args.checkpoint))\n                raise FileNotFoundError(""no checkpoint found at \'{}\'"".format(args.checkpoint))\n\n        print(""=====> beginning validation"")\n        print(""validation set length: "", len(testLoader))\n        mIOU_val, per_class_iu = test(args, testLoader, model)\n        print(mIOU_val)\n        print(per_class_iu)\n\n    # Get the best test result among the last 10 model records.\n    else:\n        if args.checkpoint:\n            if os.path.isfile(args.checkpoint):\n                dirname, basename = os.path.split(args.checkpoint)\n                epoch = int(os.path.splitext(basename)[0].split(\'_\')[1])\n                mIOU_val = []\n                per_class_iu = []\n                for i in range(epoch - 9, epoch + 1):\n                    basename = \'model_\' + str(i) + \'.pth\'\n                    resume = os.path.join(dirname, basename)\n                    checkpoint = torch.load(resume)\n                    model.load_state_dict(checkpoint[\'model\'])\n                    print(""=====> beginning test the "" + basename)\n                    print(""validation set length: "", len(testLoader))\n                    mIOU_val_0, per_class_iu_0 = test(args, testLoader, model)\n                    mIOU_val.append(mIOU_val_0)\n                    per_class_iu.append(per_class_iu_0)\n\n                index = list(range(epoch - 9, epoch + 1))[np.argmax(mIOU_val)]\n                print(""The best mIoU among the last 10 models is"", index)\n                print(mIOU_val)\n                per_class_iu = per_class_iu[np.argmax(mIOU_val)]\n                mIOU_val = np.max(mIOU_val)\n                print(mIOU_val)\n                print(per_class_iu)\n\n            else:\n                print(""=====> no checkpoint found at \'{}\'"".format(args.checkpoint))\n                raise FileNotFoundError(""no checkpoint found at \'{}\'"".format(args.checkpoint))\n\n    # Save the result\n    if not args.best:\n        model_path = os.path.splitext(os.path.basename(args.checkpoint))\n        args.logFile = \'test_\' + model_path[0] + \'.txt\'\n        logFileLoc = os.path.join(os.path.dirname(args.checkpoint), args.logFile)\n    else:\n        args.logFile = \'test_\' + \'best\' + str(index) + \'.txt\'\n        logFileLoc = os.path.join(os.path.dirname(args.checkpoint), args.logFile)\n\n    # Save the result\n    if os.path.isfile(logFileLoc):\n        logger = open(logFileLoc, \'a\')\n    else:\n        logger = open(logFileLoc, \'w\')\n        logger.write(""Mean IoU: %.4f"" % mIOU_val)\n        logger.write(""\\nPer class IoU: "")\n        for i in range(len(per_class_iu)):\n            logger.write(""%.4f\\t"" % per_class_iu[i])\n    logger.flush()\n    logger.close()\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n\n    args.save_seg_dir = os.path.join(args.save_seg_dir, args.dataset, args.model)\n\n    if args.dataset == \'cityscapes\':\n        args.classes = 19\n    elif args.dataset == \'camvid\':\n        args.classes = 11\n    else:\n        raise NotImplementedError(\n            ""This repository now supports two datasets: cityscapes and camvid, %s is not included"" % args.dataset)\n\n    test_model(args)\n'"
train.py,16,"b'import os,sys\nimport time\nimport torch\nfrom torch import optim\nimport torch.nn as nn\nimport timeit\nimport math\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plt\nimport torch.backends.cudnn as cudnn\nfrom argparse import ArgumentParser\n# user\nfrom builders.model_builder import build_model\nfrom builders.dataset_builder import build_dataset_train\nfrom utils.utils import setup_seed, init_weight, netParams\nfrom utils.metric.metric import get_iou\nfrom utils.losses.loss import LovaszSoftmax, CrossEntropyLoss2d, CrossEntropyLoss2dLabelSmooth,\\\n    ProbOhemCrossEntropy2d, FocalLoss2d\nfrom utils.optim import RAdam, Ranger, AdamW\nfrom utils.scheduler.lr_scheduler import WarmupPolyLR\n\n\nsys.setrecursionlimit(1000000)  # solve problem \'maximum recursion depth exceeded\'\n\ntorch_ver = torch.__version__[:3]\nif torch_ver == \'0.3\':\n    from torch.autograd import Variable\nprint(torch_ver)\n\nGLOBAL_SEED = 1234\n\n\n\ndef parse_args():\n    parser = ArgumentParser(description=\'Efficient semantic segmentation\')\n    # model and dataset\n    parser.add_argument(\'--model\', type=str, default=""ENet"", help=""model name: (default ENet)"")\n    parser.add_argument(\'--dataset\', type=str, default=""camvid"", help=""dataset: cityscapes or camvid"")\n    parser.add_argument(\'--input_size\', type=str, default=""360,480"", help=""input size of model"")\n    parser.add_argument(\'--num_workers\', type=int, default=4, help="" the number of parallel threads"")\n    parser.add_argument(\'--classes\', type=int, default=11,\n                        help=""the number of classes in the dataset. 19 and 11 for cityscapes and camvid, respectively"")\n    parser.add_argument(\'--train_type\', type=str, default=""trainval"",\n                        help=""ontrain for training on train set, ontrainval for training on train+val set"")\n    # training hyper params\n    parser.add_argument(\'--max_epochs\', type=int, default=1000,\n                        help=""the number of epochs: 300 for train set, 350 for train+val set"")\n    parser.add_argument(\'--random_mirror\', type=bool, default=True, help=""input image random mirror"")\n    parser.add_argument(\'--random_scale\', type=bool, default=True, help=""input image resize 0.5 to 2"")\n    parser.add_argument(\'--lr\', type=float, default=5e-4, help=""initial learning rate"")\n    parser.add_argument(\'--batch_size\', type=int, default=8, help=""the batch size is set to 16 for 2 GPUs"")\n    parser.add_argument(\'--optim\',type=str.lower,default=\'adam\',choices=[\'sgd\',\'adam\',\'radam\',\'ranger\'],help=""select optimizer"")\n    parser.add_argument(\'--lr_schedule\', type=str, default=\'warmpoly\', help=\'name of lr schedule: poly\')\n    parser.add_argument(\'--num_cycles\', type=int, default=1, help=\'Cosine Annealing Cyclic LR\')\n    parser.add_argument(\'--poly_exp\', type=float, default=0.9,help=\'polynomial LR exponent\')\n    parser.add_argument(\'--warmup_iters\', type=int, default=500, help=\'warmup iterations\')\n    parser.add_argument(\'--warmup_factor\', type=float, default=1.0 / 3, help=\'warm up start lr=warmup_factor*lr\')\n    parser.add_argument(\'--use_label_smoothing\', action=\'store_true\', default=False, help=""CrossEntropy2d Loss with label smoothing or not"")\n    parser.add_argument(\'--use_ohem\', action=\'store_true\', default=False, help=\'OhemCrossEntropy2d Loss for cityscapes dataset\')\n    parser.add_argument(\'--use_lovaszsoftmax\', action=\'store_true\', default=False, help=\'LovaszSoftmax Loss for cityscapes dataset\')\n    parser.add_argument(\'--use_focal\', action=\'store_true\', default=False,help=\' FocalLoss2d for cityscapes dataset\')\n    # cuda setting\n    parser.add_argument(\'--cuda\', type=bool, default=True, help=""running on CPU or GPU"")\n    parser.add_argument(\'--gpus\', type=str, default=""0"", help=""default GPU devices (0,1)"")\n    # checkpoint and log\n    parser.add_argument(\'--resume\', type=str, default="""",\n                        help=""use this file to load last checkpoint for continuing training"")\n    parser.add_argument(\'--savedir\', default=""./checkpoint/"", help=""directory to save the model snapshot"")\n    parser.add_argument(\'--logFile\', default=""log.txt"", help=""storing the training and validation logs"")\n    args = parser.parse_args()\n\n    return args\n\n\n\ndef train_model(args):\n    """"""\n    args:\n       args: global arguments\n    """"""\n    h, w = map(int, args.input_size.split(\',\'))\n    input_size = (h, w)\n    print(""=====> input size:{}"".format(input_size))\n\n    print(args)\n\n    if args.cuda:\n        print(""=====> use gpu id: \'{}\'"".format(args.gpus))\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpus\n        if not torch.cuda.is_available():\n            raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\n\n\n    # set the seed\n    setup_seed(GLOBAL_SEED)\n    print(""=====> set Global Seed: "", GLOBAL_SEED)\n\n    cudnn.enabled = True\n    print(""=====> building network"")\n\n    # build the model and initialization\n    model = build_model(args.model, num_classes=args.classes)\n    init_weight(model, nn.init.kaiming_normal_,\n                nn.BatchNorm2d, 1e-3, 0.1,\n                mode=\'fan_in\')\n\n    print(""=====> computing network parameters and FLOPs"")\n    total_paramters = netParams(model)\n    print(""the number of parameters: %d ==> %.2f M"" % (total_paramters, (total_paramters / 1e6)))\n\n    # load data and data augmentation\n    datas, trainLoader, valLoader = build_dataset_train(args.dataset, input_size, args.batch_size, args.train_type,\n                                                        args.random_scale, args.random_mirror, args.num_workers)\n\n    args.per_iter = len(trainLoader)\n    args.max_iter = args.max_epochs * args.per_iter\n\n\n    print(\'=====> Dataset statistics\')\n    print(""data[\'classWeights\']: "", datas[\'classWeights\'])\n    print(\'mean and std: \', datas[\'mean\'], datas[\'std\'])\n\n    # define loss function, respectively\n    weight = torch.from_numpy(datas[\'classWeights\'])\n\n    if args.dataset == \'camvid\':\n        criteria = CrossEntropyLoss2d(weight=weight, ignore_label=ignore_label)\n    elif args.dataset == \'camvid\' and args.use_label_smoothing:\n        criteria = CrossEntropyLoss2dLabelSmooth(weight=weight, ignore_label=ignore_label)\n\n    elif args.dataset == \'cityscapes\' and args.use_ohem:\n        min_kept = int(args.batch_size // len(args.gpus) * h * w // 16)\n        criteria = ProbOhemCrossEntropy2d(use_weight=True, ignore_label=ignore_label, thresh=0.7, min_kept=min_kept)\n    elif args.dataset == \'cityscapes\' and args.use_label_smoothing:\n        criteria = CrossEntropyLoss2dLabelSmooth(weight=weight, ignore_label=ignore_label)\n    elif args.dataset == \'cityscapes\' and args.use_lovaszsoftmax:\n        criteria = LovaszSoftmax(ignore_index=ignore_label)\n    elif args.dataset == \'cityscapes\' and args.use_focal:\n        criteria = FocalLoss2d(weight=weight, ignore_index=ignore_label)\n    else:\n        raise NotImplementedError(\n            ""This repository now supports two datasets: cityscapes and camvid, %s is not included"" % args.dataset)\n\n    if args.cuda:\n        criteria = criteria.cuda()\n        if torch.cuda.device_count() > 1:\n            print(""torch.cuda.device_count()="", torch.cuda.device_count())\n            args.gpu_nums = torch.cuda.device_count()\n            model = nn.DataParallel(model).cuda()  # multi-card data parallel\n        else:\n            args.gpu_nums = 1\n            print(""single GPU for training"")\n            model = model.cuda()  # 1-card data parallel\n\n    args.savedir = (args.savedir + args.dataset + \'/\' + args.model + \'bs\'\n                    + str(args.batch_size) + \'gpu\' + str(args.gpu_nums) + ""_"" + str(args.train_type) + \'/\')\n\n\n    if not os.path.exists(args.savedir):\n        os.makedirs(args.savedir)\n\n    start_epoch = 0\n\n    # continue training\n    if args.resume:\n        if os.path.isfile(args.resume):\n            checkpoint = torch.load(args.resume)\n            start_epoch = checkpoint[\'epoch\']\n            model.load_state_dict(checkpoint[\'model\'])\n            # model.load_state_dict(convert_state_dict(checkpoint[\'model\']))\n            print(""=====> loaded checkpoint \'{}\' (epoch {})"".format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=====> no checkpoint found at \'{}\'"".format(args.resume))\n\n    model.train()\n    cudnn.benchmark = True\n    # cudnn.deterministic = True ## my add\n\n    logFileLoc = args.savedir + args.logFile\n    if os.path.isfile(logFileLoc):\n        logger = open(logFileLoc, \'a\')\n    else:\n        logger = open(logFileLoc, \'w\')\n        logger.write(""Parameters: %s Seed: %s"" % (str(total_paramters), GLOBAL_SEED))\n        logger.write(""\\n%s\\t\\t%s\\t%s\\t%s"" % (\'Epoch\', \'Loss(Tr)\', \'mIOU (val)\', \'lr\'))\n    logger.flush()\n\n\n    # define optimization strategy\n    if args.optim == \'sgd\':\n        optimizer = torch.optim.SGD(\n            filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.9, weight_decay=1e-4)\n    elif args.optim == \'adam\':\n        optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n    elif args.optim == \'radam\':\n        optimizer = RAdam(\n            filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, betas=(0.90, 0.999), eps=1e-08, weight_decay=1e-4)\n    elif args.optim == \'ranger\':\n        optimizer = Ranger(\n            filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, betas=(0.95, 0.999), eps=1e-08, weight_decay=1e-4)\n    elif args.optim == \'adamw\':\n        optimizer = AdamW(\n            filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n\n\n    lossTr_list = []\n    epoches = []\n    mIOU_val_list = []\n\n    print(\'=====> beginning training\')\n    for epoch in range(start_epoch, args.max_epochs):\n        # training\n\n        lossTr, lr = train(args, trainLoader, model, criteria, optimizer, epoch)\n        lossTr_list.append(lossTr)\n\n        # validation\n        if epoch % 50 == 0 or epoch == (args.max_epochs - 1):\n            epoches.append(epoch)\n            mIOU_val, per_class_iu = val(args, valLoader, model)\n            mIOU_val_list.append(mIOU_val)\n            # record train information\n            logger.write(""\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.7f"" % (epoch, lossTr, mIOU_val, lr))\n            logger.flush()\n            print(""Epoch : "" + str(epoch) + \' Details\')\n            print(""Epoch No.: %d\\tTrain Loss = %.4f\\t mIOU(val) = %.4f\\t lr= %.6f\\n"" % (epoch,\n                                                                                        lossTr,\n                                                                                        mIOU_val, lr))\n        else:\n            # record train information\n            logger.write(""\\n%d\\t\\t%.4f\\t\\t\\t\\t%.7f"" % (epoch, lossTr, lr))\n            logger.flush()\n            print(""Epoch : "" + str(epoch) + \' Details\')\n            print(""Epoch No.: %d\\tTrain Loss = %.4f\\t lr= %.6f\\n"" % (epoch, lossTr, lr))\n\n        # save the model\n        model_file_name = args.savedir + \'/model_\' + str(epoch + 1) + \'.pth\'\n        state = {""epoch"": epoch + 1, ""model"": model.state_dict()}\n\n        # Individual Setting for save model !!!\n        if args.dataset == \'camvid\':\n            torch.save(state, model_file_name)\n        elif args.dataset == \'cityscapes\':\n            if epoch >= args.max_epochs - 10:\n                torch.save(state, model_file_name)\n            elif not epoch % 50:\n                torch.save(state, model_file_name)\n\n\n\n        # draw plots for visualization\n        if epoch % 50 == 0 or epoch == (args.max_epochs - 1):\n            # Plot the figures per 50 epochs\n            fig1, ax1 = plt.subplots(figsize=(11, 8))\n\n            ax1.plot(range(start_epoch, epoch + 1), lossTr_list)\n            ax1.set_title(""Average training loss vs epochs"")\n            ax1.set_xlabel(""Epochs"")\n            ax1.set_ylabel(""Current loss"")\n\n            plt.savefig(args.savedir + ""loss_vs_epochs.png"")\n\n            plt.clf()\n\n            fig2, ax2 = plt.subplots(figsize=(11, 8))\n\n            ax2.plot(epoches, mIOU_val_list, label=""Val IoU"")\n            ax2.set_title(""Average IoU vs epochs"")\n            ax2.set_xlabel(""Epochs"")\n            ax2.set_ylabel(""Current IoU"")\n            plt.legend(loc=\'lower right\')\n\n            plt.savefig(args.savedir + ""iou_vs_epochs.png"")\n\n            plt.close(\'all\')\n\n    logger.close()\n\n\ndef train(args, train_loader, model, criterion, optimizer, epoch):\n    """"""\n    args:\n       train_loader: loaded for training dataset\n       model: model\n       criterion: loss function\n       optimizer: optimization algorithm, such as ADAM or SGD\n       epoch: epoch number\n    return: average loss, per class IoU, and mean IoU\n    """"""\n\n    model.train()\n    epoch_loss = []\n\n    total_batches = len(train_loader)\n    print(""=====> the number of iterations per epoch: "", total_batches)\n    st = time.time()\n    for iteration, batch in enumerate(train_loader, 0):\n\n\n        args.per_iter = total_batches\n        args.max_iter = args.max_epochs * args.per_iter\n        args.cur_iter = epoch * args.per_iter + iteration\n        # learming scheduling\n        if args.lr_schedule == \'poly\':\n            lambda1 = lambda epoch: math.pow((1 - (args.cur_iter / args.max_iter)), args.poly_exp)\n            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n        elif args.lr_schedule == \'warmpoly\':\n            scheduler = WarmupPolyLR(optimizer, T_max=args.max_iter, cur_iter=args.cur_iter, warmup_factor=1.0 / 3,\n                                 warmup_iters=args.warmup_iters, power=0.9)\n\n\n\n        lr = optimizer.param_groups[0][\'lr\']\n\n        start_time = time.time()\n        images, labels, _, _ = batch\n\n        if torch_ver == \'0.3\':\n            images = Variable(images).cuda()\n            labels = Variable(labels.long()).cuda()\n        else:\n            images = images.cuda()\n            labels = labels.long().cuda()\n\n        output = model(images)\n        loss = criterion(output, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step() # In pytorch 1.1.0 and later, should call \'optimizer.step()\' before \'lr_scheduler.step()\'\n        epoch_loss.append(loss.item())\n        time_taken = time.time() - start_time\n\n\n        print(\'=====> epoch[%d/%d] iter: (%d/%d) \\tcur_lr: %.6f loss: %.3f time:%.2f\' % (epoch + 1, args.max_epochs,\n                                                                                         iteration + 1, total_batches,\n                                                                                         lr, loss.item(), time_taken))\n\n    time_taken_epoch = time.time() - st\n    remain_time = time_taken_epoch * (args.max_epochs - 1 - epoch)\n    m, s = divmod(remain_time, 60)\n    h, m = divmod(m, 60)\n    print(""Remaining training time = %d hour %d minutes %d seconds"" % (h, m, s))\n\n    average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)\n\n    return average_epoch_loss_train, lr\n\n\ndef val(args, val_loader, model):\n    """"""\n    args:\n      val_loader: loaded for validation dataset\n      model: model\n    return: mean IoU and IoU class\n    """"""\n    # evaluation mode\n    model.eval()\n    total_batches = len(val_loader)\n\n    data_list = []\n    for i, (input, label, size, name) in enumerate(val_loader):\n        start_time = time.time()\n        with torch.no_grad():\n            # input_var = Variable(input).cuda()\n            input_var = input.cuda()\n            output = model(input_var)\n        time_taken = time.time() - start_time\n        print(""[%d/%d]  time: %.2f"" % (i + 1, total_batches, time_taken))\n        output = output.cpu().data[0].numpy()\n        gt = np.asarray(label[0].numpy(), dtype=np.uint8)\n        output = output.transpose(1, 2, 0)\n        output = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\n        data_list.append([gt.flatten(), output.flatten()])\n\n    meanIoU, per_class_iu = get_iou(data_list, args.classes)\n    return meanIoU, per_class_iu\n\n\n\nif __name__ == \'__main__\':\n    start = timeit.default_timer()\n    args = parse_args()\n\n    if args.dataset == \'cityscapes\':\n        args.classes = 19\n        args.input_size = \'512,1024\'\n        ignore_label = 255\n    elif args.dataset == \'camvid\':\n        args.classes = 11\n        args.input_size = \'360,480\'\n        ignore_label = 11\n    else:\n        raise NotImplementedError(\n            ""This repository now supports two datasets: cityscapes and camvid, %s is not included"" % args.dataset)\n\n    train_model(args)\n    end = timeit.default_timer()\n    hour = 1.0 * (end - start) / 3600\n    minute = (hour - int(hour)) * 60\n    print(""training time: %d hour %d minutes"" % (int(hour), int(minute)))\n'"
builders/__init__.py,0,b''
builders/dataset_builder.py,1,"b'import os\nimport pickle\nfrom torch.utils import data\nfrom dataset.cityscapes import CityscapesDataSet, CityscapesTrainInform, CityscapesValDataSet, CityscapesTestDataSet\nfrom dataset.camvid import CamVidDataSet, CamVidValDataSet, CamVidTrainInform, CamVidTestDataSet\n\n\ndef build_dataset_train(dataset, input_size, batch_size, train_type, random_scale, random_mirror, num_workers):\n    data_dir = os.path.join(\'./dataset/\', dataset)\n    dataset_list = dataset + \'_trainval_list.txt\'\n    train_data_list = os.path.join(data_dir, dataset + \'_\' + train_type + \'_list.txt\')\n    val_data_list = os.path.join(data_dir, dataset + \'_val\' + \'_list.txt\')\n    inform_data_file = os.path.join(\'./dataset/inform/\', dataset + \'_inform.pkl\')\n\n    # inform_data_file collect the information of mean, std and weigth_class\n    if not os.path.isfile(inform_data_file):\n        print(""%s is not found"" % (inform_data_file))\n        if dataset == ""cityscapes"":\n            dataCollect = CityscapesTrainInform(data_dir, 19, train_set_file=dataset_list,\n                                                inform_data_file=inform_data_file)\n        elif dataset == \'camvid\':\n            dataCollect = CamVidTrainInform(data_dir, 11, train_set_file=dataset_list,\n                                            inform_data_file=inform_data_file)\n        else:\n            raise NotImplementedError(\n                ""This repository now supports two datasets: cityscapes and camvid, %s is not included"" % dataset)\n\n        datas = dataCollect.collectDataAndSave()\n        if datas is None:\n            print(""error while pickling data. Please check."")\n            exit(-1)\n    else:\n        print(""find file: "", str(inform_data_file))\n        datas = pickle.load(open(inform_data_file, ""rb""))\n\n    if dataset == ""cityscapes"":\n\n        trainLoader = data.DataLoader(\n            CityscapesDataSet(data_dir, train_data_list, crop_size=input_size, scale=random_scale,\n                              mirror=random_mirror, mean=datas[\'mean\']),\n            batch_size=batch_size, shuffle=True, num_workers=num_workers,\n            pin_memory=True, drop_last=True)\n\n        valLoader = data.DataLoader(\n            CityscapesValDataSet(data_dir, val_data_list, f_scale=1, mean=datas[\'mean\']),\n            batch_size=1, shuffle=True, num_workers=num_workers, pin_memory=True,\n            drop_last=True)\n\n        return datas, trainLoader, valLoader\n\n    elif dataset == ""camvid"":\n\n        trainLoader = data.DataLoader(\n            CamVidDataSet(data_dir, train_data_list, crop_size=input_size, scale=random_scale,\n                          mirror=random_mirror, mean=datas[\'mean\']),\n            batch_size=batch_size, shuffle=True, num_workers=num_workers,\n            pin_memory=True, drop_last=True)\n\n        valLoader = data.DataLoader(\n            CamVidValDataSet(data_dir, val_data_list, f_scale=1, mean=datas[\'mean\']),\n            batch_size=1, shuffle=True, num_workers=num_workers, pin_memory=True)\n\n        return datas, trainLoader, valLoader\n\n\ndef build_dataset_test(dataset, num_workers, none_gt=False):\n    data_dir = os.path.join(\'./dataset/\', dataset)\n    dataset_list = dataset + \'_trainval_list.txt\'\n    test_data_list = os.path.join(data_dir, dataset + \'_test\' + \'_list.txt\')\n    inform_data_file = os.path.join(\'./dataset/inform/\', dataset + \'_inform.pkl\')\n\n    # inform_data_file collect the information of mean, std and weigth_class\n    if not os.path.isfile(inform_data_file):\n        print(""%s is not found"" % (inform_data_file))\n        if dataset == ""cityscapes"":\n            dataCollect = CityscapesTrainInform(data_dir, 19, train_set_file=dataset_list,\n                                                inform_data_file=inform_data_file)\n        elif dataset == \'camvid\':\n            dataCollect = CamVidTrainInform(data_dir, 11, train_set_file=dataset_list,\n                                            inform_data_file=inform_data_file)\n        else:\n            raise NotImplementedError(\n                ""This repository now supports two datasets: cityscapes and camvid, %s is not included"" % dataset)\n        \n        datas = dataCollect.collectDataAndSave()\n        if datas is None:\n            print(""error while pickling data. Please check."")\n            exit(-1)\n    else:\n        print(""find file: "", str(inform_data_file))\n        datas = pickle.load(open(inform_data_file, ""rb""))\n\n    if dataset == ""cityscapes"":\n        # for cityscapes, if test on validation set, set none_gt to False\n        # if test on the test set, set none_gt to True\n        if none_gt:\n            testLoader = data.DataLoader(\n                CityscapesTestDataSet(data_dir, test_data_list, mean=datas[\'mean\']),\n                batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True)\n        else:\n            test_data_list = os.path.join(data_dir, dataset + \'_val\' + \'_list.txt\')\n            testLoader = data.DataLoader(\n                CityscapesValDataSet(data_dir, test_data_list, mean=datas[\'mean\']),\n                batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True)\n\n        return datas, testLoader\n\n    elif dataset == ""camvid"":\n\n        testLoader = data.DataLoader(\n            CamVidValDataSet(data_dir, test_data_list, mean=datas[\'mean\']),\n            batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True)\n\n        return datas, testLoader\n'"
builders/model_builder.py,0,"b""from model.SQNet import SQNet\nfrom model.LinkNet import LinkNet\nfrom model.SegNet import SegNet\nfrom model.UNet import UNet\nfrom model.ENet import ENet\nfrom model.ERFNet import ERFNet\nfrom model.CGNet import CGNet\nfrom model.EDANet import EDANet\nfrom model.ESNet import ESNet\nfrom model.ESPNet import ESPNet\nfrom model.LEDNet import LEDNet\nfrom model.ESPNet_v2.SegmentationModel import EESPNet_Seg\nfrom model.ContextNet import ContextNet\nfrom model.FastSCNN import FastSCNN\nfrom model.DABNet import DABNet\nfrom model.FSSNet import FSSNet\nfrom model.FPENet import FPENet\n\n\n\ndef build_model(model_name, num_classes):\n    if model_name == 'SQNet':\n        return SQNet(classes=num_classes)\n    elif model_name == 'LinkNet':\n        return LinkNet(classes=num_classes)\n    elif model_name == 'SegNet':\n        return SegNet(classes=num_classes)\n    elif model_name == 'UNet':\n        return UNet(classes=num_classes)\n    elif model_name == 'ENet':\n        return ENet(classes=num_classes)\n    elif model_name == 'ERFNet':\n        return ERFNet(classes=num_classes)\n    elif model_name == 'CGNet':\n        return CGNet(classes=num_classes)\n    elif model_name == 'EDANet':\n        return EDANet(classes=num_classes)\n    elif model_name == 'ESNet':\n        return ESNet(classes=num_classes)\n    elif model_name == 'ESPNet':\n        return ESPNet(classes=num_classes)\n    elif model_name == 'LEDNet':\n        return LEDNet(classes=num_classes)\n    elif model_name == 'ESPNet_v2':\n        return EESPNet_Seg(classes=num_classes)\n    elif model_name == 'ContextNet':\n        return ContextNet(classes=num_classes)\n    elif model_name == 'FastSCNN':\n        return FastSCNN(classes=num_classes)\n    elif model_name == 'DABNet':\n        return DABNet(classes=num_classes)\n    elif model_name == 'FSSNet':\n        return FSSNet(classes=num_classes)\n    elif model_name == 'FPENet':\n        return FPENet(classes=num_classes)"""
dataset/__init__.py,0,b'from .camvid import *\nfrom .cityscapes import *\n'
dataset/camvid.py,1,"b'import os.path as osp\nimport numpy as np\nimport random\nimport cv2\nfrom torch.utils import data\nimport pickle\n\n""""""\nCamVid is a road scene understanding dataset with 367 training images and 233 testing images of day and dusk scenes. \nThe challenge is to segment 11 classes such as road, building, cars, pedestrians, signs, poles, side-walk etc. We \nresize images to 360x480 pixels for training and testing.\n""""""\n\n\nclass CamVidDataSet(data.Dataset):\n    """""" \n       CamVidDataSet is employed to load train set\n       Args:\n        root: the CamVid dataset path, \n        list_path: camvid_train_list.txt, include partial path\n\n    """"""\n\n    def __init__(self, root=\'\', list_path=\'\', max_iters=None, crop_size=(360, 360),\n                 mean=(128, 128, 128), scale=True, mirror=True, ignore_label=11):\n        self.root = root\n        self.list_path = list_path\n        self.crop_h, self.crop_w = crop_size\n        self.scale = scale\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.is_mirror = mirror\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        if not max_iters == None:\n            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n        self.files = []\n\n        # for split in [""train"", ""trainval"", ""val""]:\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            # print(img_file)\n            label_file = osp.join(self.root, name.split()[1])\n            # print(label_file)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": name\n            })\n\n        print(""length of train set: "", len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n        size = image.shape\n        name = datafiles[""name""]\n        if self.scale:\n            scale = [0.75, 1.0, 1.25, 1.5, 1.75, 2.0]  # random resize between 0.5 and 2\n            f_scale = scale[random.randint(0, 5)]\n            # f_scale = 0.5 + random.randint(0, 15) / 10.0  #random resize between 0.5 and 2\n            image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation=cv2.INTER_LINEAR)\n            label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation=cv2.INTER_NEAREST)\n\n        image = np.asarray(image, np.float32)\n\n        image -= self.mean\n        # image = image.astype(np.float32) / 255.0\n        image = image[:, :, ::-1]  # change to RGB\n        img_h, img_w = label.shape\n        pad_h = max(self.crop_h - img_h, 0)\n        pad_w = max(self.crop_w - img_w, 0)\n        if pad_h > 0 or pad_w > 0:\n            img_pad = cv2.copyMakeBorder(image, 0, pad_h, 0,\n                                         pad_w, cv2.BORDER_CONSTANT,\n                                         value=(0.0, 0.0, 0.0))\n            label_pad = cv2.copyMakeBorder(label, 0, pad_h, 0,\n                                           pad_w, cv2.BORDER_CONSTANT,\n                                           value=(self.ignore_label,))\n        else:\n            img_pad, label_pad = image, label\n\n        img_h, img_w = label_pad.shape\n        h_off = random.randint(0, img_h - self.crop_h)\n        w_off = random.randint(0, img_w - self.crop_w)\n        # roi = cv2.Rect(w_off, h_off, self.crop_w, self.crop_h);\n        image = np.asarray(img_pad[h_off: h_off + self.crop_h, w_off: w_off + self.crop_w], np.float32)\n        label = np.asarray(label_pad[h_off: h_off + self.crop_h, w_off: w_off + self.crop_w], np.float32)\n\n        image = image.transpose((2, 0, 1))  # NHWC -> NCHW\n\n        if self.is_mirror:\n            flip = np.random.choice(2) * 2 - 1\n            image = image[:, :, ::flip]\n            label = label[:, ::flip]\n\n        return image.copy(), label.copy(), np.array(size), name\n\n\nclass CamVidValDataSet(data.Dataset):\n    """""" \n       CamVidValDataSet is employed to load val set\n       Args:\n        root: the CamVid dataset path, \n        list_path: camvid_val_list.txt, include partial path\n\n    """"""\n\n    def __init__(self, root=\'\', list_path=\'\',\n                 f_scale=1, mean=(128, 128, 128), ignore_label=11):\n        self.root = root\n        self.list_path = list_path\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.f_scale = f_scale\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        self.files = []\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            # print(img_file)\n            label_file = osp.join(self.root, name.split()[1])\n            # print(label_file)\n            image_name = name.strip().split()[0].strip().split(\'/\', 1)[1].split(\'.\')[0]\n            # print(""image_name:  "",image_name)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": image_name\n            })\n\n        print(""length of Validation set: "", len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n        size = image.shape\n        name = datafiles[""name""]\n        if self.f_scale != 1:\n            image = cv2.resize(image, None, fx=self.f_scale, fy=self.f_scale, interpolation=cv2.INTER_LINEAR)\n            # label = cv2.resize(label, None, fx=self.f_scale, fy=self.f_scale, interpolation = cv2.INTER_NEAREST)\n\n        image = np.asarray(image, np.float32)\n\n        image -= self.mean\n        # image = image.astype(np.float32) / 255.0\n        image = image[:, :, ::-1]  # revert to RGB\n        image = image.transpose((2, 0, 1))  # HWC -> CHW\n\n        # print(\'image.shape:\',image.shape)\n        return image.copy(), label.copy(), np.array(size), name\n\n\nclass CamVidTestDataSet(data.Dataset):\n    """""" \n       CamVidTestDataSet is employed to load test set\n       Args:\n        root: the CamVid dataset path, \n        list_path: camvid_test_list.txt, include partial path\n\n    """"""\n\n    def __init__(self, root=\'\', list_path=\'\',\n                 mean=(128, 128, 128), ignore_label=11):\n        self.root = root\n        self.list_path = list_path\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        self.files = []\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            # print(img_file)\n            image_name = name.strip().split()[0].strip().split(\'/\', 1)[1].split(\'.\')[0]\n            # print(image_name)\n            self.files.append({\n                ""img"": img_file,\n                ""name"": image_name\n            })\n        print(""lenth of test set "", len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        name = datafiles[""name""]\n\n        image = np.asarray(image, np.float32)\n\n        size = image.shape\n        image -= self.mean\n        # image = image.astype(np.float32) / 255.0\n        image = image[:, :, ::-1]  # change to RGB\n        image = image.transpose((2, 0, 1))  # HWC -> CHW\n\n        return image.copy(), np.array(size), name\n\n\nclass CamVidTrainInform:\n    """""" To get statistical information about the train set, such as mean, std, class distribution.\n        The class is employed for tackle class imbalance.\n    """"""\n\n    def __init__(self, data_dir=\'\', classes=11, train_set_file="""",\n                 inform_data_file="""", normVal=1.10):\n        """"""\n        Args:\n           data_dir: directory where the dataset is kept\n           classes: number of classes in the dataset\n           inform_data_file: location where cached file has to be stored\n           normVal: normalization value, as defined in ERFNet paper\n        """"""\n        self.data_dir = data_dir\n        self.classes = classes\n        self.classWeights = np.ones(self.classes, dtype=np.float32)\n        self.normVal = normVal\n        self.mean = np.zeros(3, dtype=np.float32)\n        self.std = np.zeros(3, dtype=np.float32)\n        self.train_set_file = train_set_file\n        self.inform_data_file = inform_data_file\n\n    def compute_class_weights(self, histogram):\n        """"""to compute the class weights\n        Args:\n            histogram: distribution of class samples\n        """"""\n        normHist = histogram / np.sum(histogram)\n        for i in range(self.classes):\n            self.classWeights[i] = 1 / (np.log(self.normVal + normHist[i]))\n\n    def readWholeTrainSet(self, fileName, train_flag=True):\n        """"""to read the whole train set of current dataset.\n        Args:\n        fileName: train set file that stores the image locations\n        trainStg: if processing training or validation data\n        \n        return: 0 if successful\n        """"""\n        global_hist = np.zeros(self.classes, dtype=np.float32)\n\n        no_files = 0\n        min_val_al = 0\n        max_val_al = 0\n        with open(self.data_dir + \'/\' + fileName, \'r\') as textFile:\n            # with open(fileName, \'r\') as textFile:\n            for line in textFile:\n                # we expect the text file to contain the data in following format\n                # <RGB Image> <Label Image>\n                line_arr = line.split()\n                img_file = ((self.data_dir).strip() + \'/\' + line_arr[0].strip()).strip()\n                label_file = ((self.data_dir).strip() + \'/\' + line_arr[1].strip()).strip()\n\n                label_img = cv2.imread(label_file, 0)\n                unique_values = np.unique(label_img)\n                max_val = max(unique_values)\n                min_val = min(unique_values)\n\n                max_val_al = max(max_val, max_val_al)\n                min_val_al = min(min_val, min_val_al)\n\n                if train_flag == True:\n                    hist = np.histogram(label_img, self.classes, [0, self.classes - 1])\n                    global_hist += hist[0]\n\n                    rgb_img = cv2.imread(img_file)\n                    self.mean[0] += np.mean(rgb_img[:, :, 0])\n                    self.mean[1] += np.mean(rgb_img[:, :, 1])\n                    self.mean[2] += np.mean(rgb_img[:, :, 2])\n\n                    self.std[0] += np.std(rgb_img[:, :, 0])\n                    self.std[1] += np.std(rgb_img[:, :, 1])\n                    self.std[2] += np.std(rgb_img[:, :, 2])\n\n                else:\n                    print(""we can only collect statistical information of train set, please check"")\n\n                if max_val > (self.classes - 1) or min_val < 0:\n                    print(\'Labels can take value between 0 and number of classes.\')\n                    print(\'Some problem with labels. Please check. label_set:\', unique_values)\n                    print(\'Label Image ID: \' + label_file)\n                no_files += 1\n\n        # divide the mean and std values by the sample space size\n        self.mean /= no_files\n        self.std /= no_files\n\n        # compute the class imbalance information\n        self.compute_class_weights(global_hist)\n        return 0\n\n    def collectDataAndSave(self):\n        """""" To collect statistical information of train set and then save it.\n        The file train.txt should be inside the data directory.\n        """"""\n        print(\'Processing training data\')\n        return_val = self.readWholeTrainSet(fileName=self.train_set_file)\n\n        print(\'Pickling data\')\n        if return_val == 0:\n            data_dict = dict()\n            data_dict[\'mean\'] = self.mean\n            data_dict[\'std\'] = self.std\n            data_dict[\'classWeights\'] = self.classWeights\n            pickle.dump(data_dict, open(self.inform_data_file, ""wb""))\n            return data_dict\n        return None\n'"
dataset/cityscapes.py,1,"b'import os.path as osp\nimport numpy as np\nimport random\nimport cv2\nfrom torch.utils import data\nimport pickle\n\n\nclass CityscapesDataSet(data.Dataset):\n    """""" \n       CityscapesDataSet is employed to load train set\n       Args:\n        root: the Cityscapes dataset path, \n         cityscapes\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 gtFine\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 leftImg8bit\n        list_path: cityscapes_train_list.txt, include partial path\n        mean: bgr_mean (73.15835921, 82.90891754, 72.39239876)\n\n    """"""\n\n    def __init__(self, root=\'\', list_path=\'\', max_iters=None,\n                 crop_size=(512, 1024), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=255):\n        self.root = root\n        self.list_path = list_path\n        self.crop_h, self.crop_w = crop_size\n        self.scale = scale\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.is_mirror = mirror\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        if not max_iters == None:\n            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n        self.files = []\n\n        # for split in [""train"", ""trainval"", ""val""]:\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            # print(img_file)\n            label_file = osp.join(self.root, name.split()[1])\n            # print(label_file)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": name\n            })\n\n        print(""length of dataset: "", len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n        size = image.shape\n        name = datafiles[""name""]\n        if self.scale:\n            scale = [0.75, 1.0, 1.25, 1.5, 1.75, 2.0]\n            f_scale = scale[random.randint(0, 5)]\n            # f_scale = 0.5 + random.randint(0, 15) / 10.0  # random resize between 0.5 and 2\n            image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation=cv2.INTER_LINEAR)\n            label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation=cv2.INTER_NEAREST)\n\n        image = np.asarray(image, np.float32)\n\n        image -= self.mean\n        # image = image.astype(np.float32) / 255.0\n        image = image[:, :, ::-1]  # change to RGB\n        img_h, img_w = label.shape\n        pad_h = max(self.crop_h - img_h, 0)\n        pad_w = max(self.crop_w - img_w, 0)\n        if pad_h > 0 or pad_w > 0:\n            img_pad = cv2.copyMakeBorder(image, 0, pad_h, 0,\n                                         pad_w, cv2.BORDER_CONSTANT,\n                                         value=(0.0, 0.0, 0.0))\n            label_pad = cv2.copyMakeBorder(label, 0, pad_h, 0,\n                                           pad_w, cv2.BORDER_CONSTANT,\n                                           value=(self.ignore_label,))\n        else:\n            img_pad, label_pad = image, label\n\n        img_h, img_w = label_pad.shape\n        h_off = random.randint(0, img_h - self.crop_h)\n        w_off = random.randint(0, img_w - self.crop_w)\n        # roi = cv2.Rect(w_off, h_off, self.crop_w, self.crop_h);\n        image = np.asarray(img_pad[h_off: h_off + self.crop_h, w_off: w_off + self.crop_w], np.float32)\n        label = np.asarray(label_pad[h_off: h_off + self.crop_h, w_off: w_off + self.crop_w], np.float32)\n\n        image = image.transpose((2, 0, 1))  # NHWC -> NCHW\n\n        if self.is_mirror:\n            flip = np.random.choice(2) * 2 - 1\n            image = image[:, :, ::flip]\n            label = label[:, ::flip]\n\n        return image.copy(), label.copy(), np.array(size), name\n\n\nclass CityscapesValDataSet(data.Dataset):\n    """""" \n       CityscapesDataSet is employed to load val set\n       Args:\n        root: the Cityscapes dataset path, \n         cityscapes\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 gtFine\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 leftImg8bit\n        list_path: cityscapes_val_list.txt, include partial path\n\n    """"""\n\n    def __init__(self, root=\'\',\n                 list_path=\'\',\n                 f_scale=1, mean=(128, 128, 128), ignore_label=255):\n        self.root = root\n        self.list_path = list_path\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.f_scale = f_scale\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        self.files = []\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            # print(img_file)\n            label_file = osp.join(self.root, name.split()[1])\n            # print(label_file)\n            image_name = name.strip().split()[0].strip().split(\'/\', 3)[3].split(\'.\')[0]\n            # print(""image_name:  "",image_name)\n            self.files.append({\n                ""img"": img_file,\n                ""label"": label_file,\n                ""name"": image_name\n            })\n\n        print(""length of dataset: "", len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\n\n        size = image.shape\n        name = datafiles[""name""]\n        if self.f_scale != 1:\n            image = cv2.resize(image, None, fx=self.f_scale, fy=self.f_scale, interpolation=cv2.INTER_LINEAR)\n            label = cv2.resize(label, None, fx=self.f_scale, fy=self.f_scale, interpolation=cv2.INTER_NEAREST)\n\n        image = np.asarray(image, np.float32)\n\n        image -= self.mean\n        # image = image.astype(np.float32) / 255.0\n        image = image[:, :, ::-1]  # change to RGB\n        image = image.transpose((2, 0, 1))  # HWC -> CHW\n\n        # print(\'image.shape:\',image.shape)\n        return image.copy(), label.copy(), np.array(size), name\n\n\nclass CityscapesTestDataSet(data.Dataset):\n    """""" \n       CityscapesDataSet is employed to load test set\n       Args:\n        root: the Cityscapes dataset path,\n        list_path: cityscapes_test_list.txt, include partial path\n\n    """"""\n\n    def __init__(self, root=\'\',\n                 list_path=\'\', mean=(128, 128, 128),\n                 ignore_label=255):\n        self.root = root\n        self.list_path = list_path\n        self.ignore_label = ignore_label\n        self.mean = mean\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n        self.files = []\n        for name in self.img_ids:\n            img_file = osp.join(self.root, name.split()[0])\n            # print(img_file)\n            image_name = name.strip().split()[0].strip().split(\'/\', 3)[3].split(\'.\')[0]\n            # print(image_name)\n            self.files.append({\n                ""img"": img_file,\n                ""name"": image_name\n            })\n        print(""lenth of dataset: "", len(self.files))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        datafiles = self.files[index]\n\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\n        name = datafiles[""name""]\n        image = np.asarray(image, np.float32)\n        size = image.shape\n\n        image -= self.mean\n        # image = image.astype(np.float32) / 255.0\n        image = image[:, :, ::-1]  # change to RGB\n        image = image.transpose((2, 0, 1))  # HWC -> CHW\n        return image.copy(), np.array(size), name\n\n\nclass CityscapesTrainInform:\n    """""" To get statistical information about the train set, such as mean, std, class distribution.\n        The class is employed for tackle class imbalance.\n    """"""\n\n    def __init__(self, data_dir=\'\', classes=19,\n                 train_set_file="""", inform_data_file="""", normVal=1.10):\n        """"""\n        Args:\n           data_dir: directory where the dataset is kept\n           classes: number of classes in the dataset\n           inform_data_file: location where cached file has to be stored\n           normVal: normalization value, as defined in ERFNet paper\n        """"""\n        self.data_dir = data_dir\n        self.classes = classes\n        self.classWeights = np.ones(self.classes, dtype=np.float32)\n        self.normVal = normVal\n        self.mean = np.zeros(3, dtype=np.float32)\n        self.std = np.zeros(3, dtype=np.float32)\n        self.train_set_file = train_set_file\n        self.inform_data_file = inform_data_file\n\n    def compute_class_weights(self, histogram):\n        """"""to compute the class weights\n        Args:\n            histogram: distribution of class samples\n        """"""\n        normHist = histogram / np.sum(histogram)\n        for i in range(self.classes):\n            self.classWeights[i] = 1 / (np.log(self.normVal + normHist[i]))\n\n    def readWholeTrainSet(self, fileName, train_flag=True):\n        """"""to read the whole train set of current dataset.\n        Args:\n        fileName: train set file that stores the image locations\n        trainStg: if processing training or validation data\n        \n        return: 0 if successful\n        """"""\n        global_hist = np.zeros(self.classes, dtype=np.float32)\n\n        no_files = 0\n        min_val_al = 0\n        max_val_al = 0\n        with open(self.data_dir + \'/\' + fileName, \'r\') as textFile:\n            # with open(fileName, \'r\') as textFile:\n            for line in textFile:\n                # we expect the text file to contain the data in following format\n                # <RGB Image> <Label Image>\n                line_arr = line.split()\n                img_file = ((self.data_dir).strip() + \'/\' + line_arr[0].strip()).strip()\n                label_file = ((self.data_dir).strip() + \'/\' + line_arr[1].strip()).strip()\n\n                label_img = cv2.imread(label_file, 0)\n                unique_values = np.unique(label_img)\n                max_val = max(unique_values)\n                min_val = min(unique_values)\n\n                max_val_al = max(max_val, max_val_al)\n                min_val_al = min(min_val, min_val_al)\n\n                if train_flag == True:\n                    hist = np.histogram(label_img, self.classes, range=(0, 18))\n                    global_hist += hist[0]\n\n                    rgb_img = cv2.imread(img_file)\n                    self.mean[0] += np.mean(rgb_img[:, :, 0])\n                    self.mean[1] += np.mean(rgb_img[:, :, 1])\n                    self.mean[2] += np.mean(rgb_img[:, :, 2])\n\n                    self.std[0] += np.std(rgb_img[:, :, 0])\n                    self.std[1] += np.std(rgb_img[:, :, 1])\n                    self.std[2] += np.std(rgb_img[:, :, 2])\n\n                else:\n                    print(""we can only collect statistical information of train set, please check"")\n\n                if max_val > (self.classes - 1) or min_val < 0:\n                    print(\'Labels can take value between 0 and number of classes.\')\n                    print(\'Some problem with labels. Please check. label_set:\', unique_values)\n                    print(\'Label Image ID: \' + label_file)\n                no_files += 1\n\n        # divide the mean and std values by the sample space size\n        self.mean /= no_files\n        self.std /= no_files\n\n        # compute the class imbalance information\n        self.compute_class_weights(global_hist)\n        return 0\n\n    def collectDataAndSave(self):\n        """""" To collect statistical information of train set and then save it.\n        The file train.txt should be inside the data directory.\n        """"""\n        print(\'Processing training data\')\n        return_val = self.readWholeTrainSet(fileName=self.train_set_file)\n\n        print(\'Pickling data\')\n        if return_val == 0:\n            data_dict = dict()\n            data_dict[\'mean\'] = self.mean\n            data_dict[\'std\'] = self.std\n            data_dict[\'classWeights\'] = self.classWeights\n            pickle.dump(data_dict, open(self.inform_data_file, ""wb""))\n            return data_dict\n        return None\n'"
dataset/create_dataset_list.py,0,"b""# -*- coding: utf-8 -*-\nimport os\nimport glob\nroot_path=os.path.expanduser('./cityscapes')\nimage_path='leftImg8bit'\nannotation_path='gtFine'\nsplits=['train','val','test']\n\n#train glob images 2975\n#train glob annotations 2975\n#val glob images 500\n#val glob annotations 500\n#test glob images 1525\n#test glob annotations 1525\n\nfor split in splits:\n    glob_images=glob.glob(os.path.join(root_path,image_path,split,'*','*leftImg8bit.png'))\n    glob_annotations=glob.glob(os.path.join(root_path,annotation_path,split,'*','*labelTrainIds.png'))\n    print('%s glob images'%split,len(glob_images))\n    print('%s glob annotations'%split,len(glob_annotations))\n    \n    write_file=open('./cityscapes/cityscapes_'+split+'_list.txt','w')\n    for g_img in glob_images:\n        #img_p: eg leftImg8bit/val/frankfurt/frankfurt_000001_083852_leftImg8bit.png\n        #ann_p: eg gtFine/val/frankfurt/frankfurt_000001_083852_gtFine_labelTrainIds.png\n        img_p=g_img.replace(root_path+'/','')\n        #replace will not change img_p\n        ann_p=img_p.replace('leftImg8bit/','gtFine/').replace('leftImg8bit.png','gtFine_labelTrainIds.png')\n        assert os.path.join(root_path,img_p) in glob_images,'%s not exist'%img_p\n        assert os.path.join(root_path,ann_p) in glob_annotations,'%s not exist'%ann_p\n        write_file.write(img_p+' '+ann_p+'\\n')\n    write_file.close()\n"""
model/CGNet.py,8,"b'###########################################################################\n#CGNet: A Light-weight Context Guided Network for Semantic Segmentation\n#Paper-Link: https://arxiv.org/pdf/1811.08201.pdf\n###########################################################################\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\n__all__ = [""CGNet""]\n\nclass ConvBNPReLU(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        """"""\n        args:\n            nIn: number of input channels\n            nOut: number of output channels\n            kSize: kernel size\n            stride: stride rate for down-sampling. Default is 1\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BNPReLU(nn.Module):\n    def __init__(self, nOut):\n        """"""\n        args:\n           nOut: channels of output feature maps\n        """"""\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: normalized and thresholded feature map\n        """"""\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\nclass ConvBN(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        """"""\n        args:\n           nIn: number of input channels\n           nOut: number of output channels\n           kSize: kernel size\n           stride: optinal stide for down-sampling\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\nclass Conv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        """"""\n        args:\n            nIn: number of input channels\n            nOut: number of output channels\n            kSize: kernel size\n            stride: optional stride rate for down-sampling\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        return output\n\nclass ChannelWiseConv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        """"""\n        Args:\n            nIn: number of input channels\n            nOut: number of output channels, default (nIn == nOut)\n            kSize: kernel size\n            stride: optional stride rate for down-sampling\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), groups=nIn, bias=False)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        return output\nclass DilatedConv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n        """"""\n        args:\n           nIn: number of input channels\n           nOut: number of output channels\n           kSize: kernel size\n           stride: optional stride rate for down-sampling\n           d: dilation rate\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2) * d\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False, dilation=d)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        return output\n\nclass ChannelWiseDilatedConv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n        """"""\n        args:\n           nIn: number of input channels\n           nOut: number of output channels, default (nIn == nOut)\n           kSize: kernel size\n           stride: optional stride rate for down-sampling\n           d: dilation rate\n        """"""\n        super().__init__()\n        padding = int((kSize - 1)/2) * d\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), groups= nIn, bias=False, dilation=d)\n\n    def forward(self, input):\n        """"""\n        args:\n           input: input feature map\n           return: transformed feature map\n        """"""\n        output = self.conv(input)\n        return output\n\nclass FGlo(nn.Module):\n    """"""\n    the FGlo class is employed to refine the joint feature of both local feature and surrounding context.\n    """"""\n    def __init__(self, channel, reduction=16):\n        super(FGlo, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, channel // reduction),\n                nn.ReLU(inplace=True),\n                nn.Linear(channel // reduction, channel),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass ContextGuidedBlock_Down(nn.Module):\n    """"""\n    the size of feature map divided 2, (H,W,C)---->(H/2, W/2, 2C)\n    """"""\n    def __init__(self, nIn, nOut, dilation_rate=2, reduction=16):\n        """"""\n        args:\n           nIn: the channel of input feature map\n           nOut: the channel of output feature map, and nOut=2*nIn\n        """"""\n        super().__init__()\n        self.conv1x1 = ConvBNPReLU(nIn, nOut, 3, 2)  #  size/2, channel: nIn--->nOut\n        \n        self.F_loc = ChannelWiseConv(nOut, nOut, 3, 1)\n        self.F_sur = ChannelWiseDilatedConv(nOut, nOut, 3, 1, dilation_rate)\n        \n        self.bn = nn.BatchNorm2d(2*nOut, eps=1e-3)\n        self.act = nn.PReLU(2*nOut)\n        self.reduce = Conv(2*nOut, nOut,1,1)  #reduce dimension: 2*nOut--->nOut\n        \n        self.F_glo = FGlo(nOut, reduction)    \n\n    def forward(self, input):\n        output = self.conv1x1(input)\n        loc = self.F_loc(output)\n        sur = self.F_sur(output)\n\n        joi_feat = torch.cat([loc, sur],1)  #  the joint feature\n        joi_feat = self.bn(joi_feat)\n        joi_feat = self.act(joi_feat)\n        joi_feat = self.reduce(joi_feat)     #channel= nOut\n        \n        output = self.F_glo(joi_feat)  # F_glo is employed to refine the joint feature\n\n        return output\n\n\nclass ContextGuidedBlock(nn.Module):\n    def __init__(self, nIn, nOut, dilation_rate=2, reduction=16, add=True):\n        """"""\n        args:\n           nIn: number of input channels\n           nOut: number of output channels, \n           add: if true, residual learning\n        """"""\n        super().__init__()\n        n= int(nOut/2)\n        self.conv1x1 = ConvBNPReLU(nIn, n, 1, 1)  #1x1 Conv is employed to reduce the computation\n        self.F_loc = ChannelWiseConv(n, n, 3, 1) # local feature\n        self.F_sur = ChannelWiseDilatedConv(n, n, 3, 1, dilation_rate) # surrounding context\n        self.bn_prelu = BNPReLU(nOut)\n        self.add = add\n        self.F_glo= FGlo(nOut, reduction)\n\n    def forward(self, input):\n        output = self.conv1x1(input)\n        loc = self.F_loc(output)\n        sur = self.F_sur(output)\n        \n        joi_feat = torch.cat([loc, sur], 1) \n\n        joi_feat = self.bn_prelu(joi_feat)\n\n        output = self.F_glo(joi_feat)  #F_glo is employed to refine the joint feature\n        # if residual version\n        if self.add:\n            output  = input + output\n        return output\n\nclass InputInjection(nn.Module):\n    def __init__(self, downsamplingRatio):\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, downsamplingRatio):\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n    def forward(self, input):\n        for pool in self.pool:\n            input = pool(input)\n        return input\n\n\nclass CGNet(nn.Module):\n    """"""\n    This class defines the proposed Context Guided Network (CGNet) in this work.\n    """"""\n    def __init__(self, classes=19, M= 3, N= 21, dropout_flag = False):\n        """"""\n        args:\n          classes: number of classes in the dataset. Default is 19 for the cityscapes\n          M: the number of blocks in stage 2\n          N: the number of blocks in stage 3\n        """"""\n        super().__init__()\n        self.level1_0 = ConvBNPReLU(3, 32, 3, 2)      # feature map size divided 2, 1/2\n        self.level1_1 = ConvBNPReLU(32, 32, 3, 1)                          \n        self.level1_2 = ConvBNPReLU(32, 32, 3, 1)      \n\n        self.sample1 = InputInjection(1)  #down-sample for Input Injection, factor=2\n        self.sample2 = InputInjection(2)  #down-sample for Input Injiection, factor=4\n\n        self.b1 = BNPReLU(32 + 3)\n        \n        #stage 2\n        self.level2_0 = ContextGuidedBlock_Down(32 +3, 64, dilation_rate=2,reduction=8)  \n        self.level2 = nn.ModuleList()\n        for i in range(0, M-1):\n            self.level2.append(ContextGuidedBlock(64 , 64, dilation_rate=2, reduction=8))  #CG block\n        self.bn_prelu_2 = BNPReLU(128 + 3)\n        \n        #stage 3\n        self.level3_0 = ContextGuidedBlock_Down(128 + 3, 128, dilation_rate=4, reduction=16) \n        self.level3 = nn.ModuleList()\n        for i in range(0, N-1):\n            self.level3.append(ContextGuidedBlock(128 , 128, dilation_rate=4, reduction=16)) # CG block\n        self.bn_prelu_3 = BNPReLU(256)\n\n        if dropout_flag:\n            print(""have droput layer"")\n            self.classifier = nn.Sequential(nn.Dropout2d(0.1, False),Conv(256, classes, 1, 1))\n        else:\n            self.classifier = nn.Sequential(Conv(256, classes, 1, 1))\n\n        #init weights\n        for m in self.modules():\n            classname = m.__class__.__name__\n            if classname.find(\'Conv2d\')!= -1:\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n                elif classname.find(\'ConvTranspose2d\')!= -1:\n                    nn.init.kaiming_normal_(m.weight)\n                    if m.bias is not None:\n                        m.bias.data.zero_()\n\n    def forward(self, input):\n        """"""\n        args:\n            input: Receives the input RGB image\n            return: segmentation map\n        """"""\n        # stage 1\n        output0 = self.level1_0(input)\n        output0 = self.level1_1(output0)\n        output0 = self.level1_2(output0)\n        inp1 = self.sample1(input)\n        inp2 = self.sample2(input)\n\n        # stage 2\n        output0_cat = self.b1(torch.cat([output0, inp1], 1))\n        output1_0 = self.level2_0(output0_cat) # down-sampled\n        \n        for i, layer in enumerate(self.level2):\n            if i==0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.bn_prelu_2(torch.cat([output1,  output1_0, inp2], 1))\n\n        # stage 3\n        output2_0 = self.level3_0(output1_cat) # down-sampled\n        for i, layer in enumerate(self.level3):\n            if i==0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.bn_prelu_3(torch.cat([output2_0, output2], 1))\n       \n        # classifier\n        classifier = self.classifier(output2_cat)\n\n        # upsample segmenation map ---> the input image size\n        out = F.interpolate(classifier, input.size()[2:], mode=\'bilinear\',align_corners = False)   #Upsample score map, factor=8\n        return out\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = CGNet(classes=19).to(device)\n    summary(model,(3,512,1024))\n'"
model/ContextNet.py,3,"b'##################################################################################\n#ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time\n#Paper-Link: https://arxiv.org/abs/1805.04554\n##################################################################################\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\n\n__all__ = [""ContextNet""]\n\nclass Custom_Conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, **kwargs):\n        super(Custom_Conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass DepthSepConv(nn.Module):\n    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n        super(DepthSepConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(dw_channels, dw_channels, 3, stride, 1, groups=dw_channels, bias=False),\n            nn.BatchNorm2d(dw_channels),\n            nn.ReLU(True),\n            nn.Conv2d(dw_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass DepthConv(nn.Module):\n    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n        super(DepthConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(dw_channels, out_channels, 3, stride, 1, groups=dw_channels, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass LinearBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels, t=6, stride=2, **kwargs):\n        super(LinearBottleneck, self).__init__()\n        self.use_shortcut = stride == 1 and in_channels == out_channels\n        self.block = nn.Sequential(\n            Custom_Conv(in_channels, in_channels * t, 1),\n            DepthConv(in_channels * t, in_channels * t, stride),\n            nn.Conv2d(in_channels * t, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def forward(self, x):\n        out = self.block(x)\n        if self.use_shortcut:\n            out = x + out\n        return out\n\n\n\n    \nclass Shallow_net(nn.Module):\n    def __init__(self, dw_channels1=32, dw_channels2=64, out_channels=128, **kwargs):\n        super(Shallow_net, self).__init__()\n        self.conv = Custom_Conv(3, dw_channels1, 3, 2)\n        self.dsconv1 = DepthSepConv(dw_channels1, dw_channels2, 2)\n        self.dsconv2 = DepthSepConv(dw_channels2, out_channels, 2)\n        self.dsconv3 = DepthSepConv(out_channels, out_channels, 1)\n\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        x = self.dsconv3(x)\n        return x\n\nclass Deep_net(nn.Module):\n    def __init__(self, in_channels, block_channels,\n                 t, num_blocks, **kwargs):\n        super(Deep_net, self).__init__()\n        self.block_channels = block_channels\n        self.t = t\n        self.num_blocks = num_blocks\n\n        self.conv_ = Custom_Conv(3, in_channels, 3, 2)\n        self.bottleneck1 = self._layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t[0], 1)\n        self.bottleneck2 = self._layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t[1], 1)\n        self.bottleneck3 = self._layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t[2], 2)\n        self.bottleneck4 = self._layer(LinearBottleneck, block_channels[2], block_channels[3], num_blocks[3], t[3], 2)\n        self.bottleneck5 = self._layer(LinearBottleneck, block_channels[3], block_channels[4], num_blocks[4], t[4], 1)\n        self.bottleneck6 = self._layer(LinearBottleneck, block_channels[4], block_channels[5], num_blocks[5], t[5], 1)\n\n    def _layer(self, block, in_channels, out_channels, blocks, t, stride):\n        layers = []\n        layers.append(block(in_channels, out_channels, t, stride))\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels, t, 1))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv_(x)\n        x = self.bottleneck1(x)\n        x = self.bottleneck2(x)\n        x = self.bottleneck3(x)\n        x = self.bottleneck4(x)\n        x = self.bottleneck5(x)\n        x = self.bottleneck6(x)\n        return x\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, highter_in_channels, lower_in_channels, out_channels, scale_factor=4, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.scale_factor = scale_factor\n        self.dwconv = DepthConv(lower_in_channels, out_channels, 1)\n        self.conv_lower_res = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.conv_higher_res = nn.Sequential(\n            nn.Conv2d(highter_in_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.relu = nn.ReLU(True)\n\n    def forward(self, higher_res_feature, lower_res_feature):\n        _, _, h, w = higher_res_feature.size()\n        lower_res_feature = F.interpolate(lower_res_feature, size=(h,w), mode=\'bilinear\', align_corners=True)\n        lower_res_feature = self.dwconv(lower_res_feature)\n        lower_res_feature = self.conv_lower_res(lower_res_feature)\n\n        higher_res_feature = self.conv_higher_res(higher_res_feature)\n        out = higher_res_feature + lower_res_feature\n        return self.relu(out)\n\nclass Classifer(nn.Module):\n    def __init__(self, dw_channels, num_classes, stride=1, **kwargs):\n        super(Classifer, self).__init__()\n        self.dsconv1 = DepthSepConv(dw_channels, dw_channels, stride)\n        self.dsconv2 = DepthSepConv(dw_channels, dw_channels, stride)\n        self.conv = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(dw_channels, num_classes, 1)\n        )\n\n    def forward(self, x):\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        x = self.conv(x)\n        return x\n\n\n\nclass ContextNet(nn.Module):\n    def __init__(self, classes, aux=False, **kwargs):\n        super(ContextNet, self).__init__()\n        self.aux = aux\n        self.spatial_detail = Shallow_net(32, 64, 128)\n        self.context_feature_extractor = Deep_net(32, [32, 32, 48, 64, 96, 128], [1, 6, 6, 6, 6, 6], [1, 1, 3, 3, 2, 2])\n        self.feature_fusion = FeatureFusionModule(128, 128, 128)\n        self.classifier = Classifer(128, classes)\n        if self.aux:\n            self.auxlayer = nn.Sequential(\n                nn.Conv2d(128, 32, 3, padding=1, bias=False),\n                nn.BatchNorm2d(32),\n                nn.ReLU(True),\n                nn.Dropout(0.1),\n                nn.Conv2d(32, classes, 1)\n            )\n\n    def forward(self, x):\n        size = x.size()[2:]\n\n        higher_res_features = self.spatial_detail(x)\n\n        x_low = F.interpolate(x, scale_factor = 0.25, mode=\'bilinear\', align_corners=True)\n\n        x = self.context_feature_extractor(x_low)\n\n        x = self.feature_fusion(higher_res_features, x)\n\n        x = self.classifier(x)\n\n        outputs = []\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n\n        outputs.append(x)\n        if self.aux:\n            auxout = self.auxlayer(higher_res_features)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return x\n\n        # return tuple(outputs)\n\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = ContextNet(classes=19).to(device)\n    summary(model,(3,512,1024))\n'"
model/DABNet.py,7,"b'######################################################################################\n#DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic Segmentation\n#Paper-Link: https://arxiv.org/pdf/1907.11357.pdf\n######################################################################################\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n__all__ = [""DABNet""]\n\n\nclass Conv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride, padding, dilation=(1, 1), groups=1, bn_acti=False, bias=False):\n        super().__init__()\n\n        self.bn_acti = bn_acti\n\n        self.conv = nn.Conv2d(nIn, nOut, kernel_size=kSize,\n                              stride=stride, padding=padding,\n                              dilation=dilation, groups=groups, bias=bias)\n\n        if self.bn_acti:\n            self.bn_prelu = BNPReLU(nOut)\n\n    def forward(self, input):\n        output = self.conv(input)\n\n        if self.bn_acti:\n            output = self.bn_prelu(output)\n\n        return output\n\n\nclass BNPReLU(nn.Module):\n    def __init__(self, nIn):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nIn, eps=1e-3)\n        self.acti = nn.PReLU(nIn)\n\n    def forward(self, input):\n        output = self.bn(input)\n        output = self.acti(output)\n\n        return output\n\n\nclass DABModule(nn.Module):\n    def __init__(self, nIn, d=1, kSize=3, dkSize=3):\n        super().__init__()\n\n        self.bn_relu_1 = BNPReLU(nIn)\n        self.conv3x3 = Conv(nIn, nIn // 2, kSize, 1, padding=1, bn_acti=True)\n\n        self.dconv3x1 = Conv(nIn // 2, nIn // 2, (dkSize, 1), 1,\n                             padding=(1, 0), groups=nIn // 2, bn_acti=True)\n        self.dconv1x3 = Conv(nIn // 2, nIn // 2, (1, dkSize), 1,\n                             padding=(0, 1), groups=nIn // 2, bn_acti=True)\n        self.ddconv3x1 = Conv(nIn // 2, nIn // 2, (dkSize, 1), 1,\n                              padding=(1 * d, 0), dilation=(d, 1), groups=nIn // 2, bn_acti=True)\n        self.ddconv1x3 = Conv(nIn // 2, nIn // 2, (1, dkSize), 1,\n                              padding=(0, 1 * d), dilation=(1, d), groups=nIn // 2, bn_acti=True)\n\n        self.bn_relu_2 = BNPReLU(nIn // 2)\n        self.conv1x1 = Conv(nIn // 2, nIn, 1, 1, padding=0, bn_acti=False)\n\n    def forward(self, input):\n        output = self.bn_relu_1(input)\n        output = self.conv3x3(output)\n\n        br1 = self.dconv3x1(output)\n        br1 = self.dconv1x3(br1)\n        br2 = self.ddconv3x1(output)\n        br2 = self.ddconv1x3(br2)\n\n        output = br1 + br2\n        output = self.bn_relu_2(output)\n        output = self.conv1x1(output)\n\n        return output + input\n\n\nclass DownSamplingBlock(nn.Module):\n    def __init__(self, nIn, nOut):\n        super().__init__()\n        self.nIn = nIn\n        self.nOut = nOut\n\n        if self.nIn < self.nOut:\n            nConv = nOut - nIn\n        else:\n            nConv = nOut\n\n        self.conv3x3 = Conv(nIn, nConv, kSize=3, stride=2, padding=1)\n        self.max_pool = nn.MaxPool2d(2, stride=2)\n        self.bn_prelu = BNPReLU(nOut)\n\n    def forward(self, input):\n        output = self.conv3x3(input)\n\n        if self.nIn < self.nOut:\n            max_pool = self.max_pool(input)\n            output = torch.cat([output, max_pool], 1)\n\n        output = self.bn_prelu(output)\n\n        return output\n\n\nclass InputInjection(nn.Module):\n    def __init__(self, ratio):\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, ratio):\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, input):\n        for pool in self.pool:\n            input = pool(input)\n\n        return input\n\n\nclass DABNet(nn.Module):\n    def __init__(self, classes=19, block_1=3, block_2=6):\n        super().__init__()\n        self.init_conv = nn.Sequential(\n            Conv(3, 32, 3, 2, padding=1, bn_acti=True),\n            Conv(32, 32, 3, 1, padding=1, bn_acti=True),\n            Conv(32, 32, 3, 1, padding=1, bn_acti=True),\n        )\n\n        self.down_1 = InputInjection(1)  # down-sample the image 1 times\n        self.down_2 = InputInjection(2)  # down-sample the image 2 times\n        self.down_3 = InputInjection(3)  # down-sample the image 3 times\n\n        self.bn_prelu_1 = BNPReLU(32 + 3)\n\n        # DAB Block 1\n        self.downsample_1 = DownSamplingBlock(32 + 3, 64)\n        self.DAB_Block_1 = nn.Sequential()\n        for i in range(0, block_1):\n            self.DAB_Block_1.add_module(""DAB_Module_1_"" + str(i), DABModule(64, d=2))\n        self.bn_prelu_2 = BNPReLU(128 + 3)\n\n        # DAB Block 2\n        dilation_block_2 = [4, 4, 8, 8, 16, 16]\n        self.downsample_2 = DownSamplingBlock(128 + 3, 128)\n        self.DAB_Block_2 = nn.Sequential()\n        for i in range(0, block_2):\n            self.DAB_Block_2.add_module(""DAB_Module_2_"" + str(i),\n                                        DABModule(128, d=dilation_block_2[i]))\n        self.bn_prelu_3 = BNPReLU(256 + 3)\n\n        self.classifier = nn.Sequential(Conv(259, classes, 1, 1, padding=0))\n\n    def forward(self, input):\n\n        output0 = self.init_conv(input)\n\n        down_1 = self.down_1(input)\n        down_2 = self.down_2(input)\n        down_3 = self.down_3(input)\n\n        output0_cat = self.bn_prelu_1(torch.cat([output0, down_1], 1))\n\n        # DAB Block 1\n        output1_0 = self.downsample_1(output0_cat)\n        output1 = self.DAB_Block_1(output1_0)\n        output1_cat = self.bn_prelu_2(torch.cat([output1, output1_0, down_2], 1))\n\n        # DAB Block 2\n        output2_0 = self.downsample_2(output1_cat)\n        output2 = self.DAB_Block_2(output2_0)\n        output2_cat = self.bn_prelu_3(torch.cat([output2, output2_0, down_3], 1))\n\n        out = self.classifier(output2_cat)\n        out = F.interpolate(out, input.size()[2:], mode=\'bilinear\', align_corners=False)\n\n        return out\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = DABNet(classes=19).to(device)\n    summary(model,(3,512,1024))\n'"
model/EDANet.py,5,"b'###################################################################################################\n#EDANet:Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation\n#Paper-Link: https://arxiv.org/ftp/arxiv/papers/1809/1809.06323.pdf\n###################################################################################################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\n__all__ = [""EDANet""]\n\nclass DownsamplerBlock(nn.Module):\n    def __init__(self, ninput, noutput):\n        super(DownsamplerBlock,self).__init__()\n\n        self.ninput = ninput\n        self.noutput = noutput\n\n        if self.ninput < self.noutput:\n            # Wout > Win\n            self.conv = nn.Conv2d(ninput, noutput-ninput, kernel_size=3, stride=2, padding=1)\n            self.pool = nn.MaxPool2d(2, stride=2)\n        else:\n            # Wout < Win\n            self.conv = nn.Conv2d(ninput, noutput, kernel_size=3, stride=2, padding=1)\n\n        self.bn = nn.BatchNorm2d(noutput)\n\n    def forward(self, x):\n        if self.ninput < self.noutput:\n            output = torch.cat([self.conv(x), self.pool(x)], 1)\n        else:\n            output = self.conv(x)\n\n        output = self.bn(output)\n        return F.relu(output)\n    \n# --- Build the EDANet Module --- #\nclass EDAModule(nn.Module):\n    def __init__(self, ninput, dilated, k = 40, dropprob = 0.02):\n        super().__init__()\n\n        # k: growthrate\n        # dropprob:a dropout layer between the last ReLU and the concatenation of each module\n\n        self.conv1x1 = nn.Conv2d(ninput, k, kernel_size=1)\n        self.bn0 = nn.BatchNorm2d(k)\n\n        self.conv3x1_1 = nn.Conv2d(k, k, kernel_size=(3, 1),padding=(1,0))\n        self.conv1x3_1 = nn.Conv2d(k, k, kernel_size=(1, 3),padding=(0,1))\n        self.bn1 = nn.BatchNorm2d(k)\n\n        self.conv3x1_2 = nn.Conv2d(k, k, (3,1), stride=1, padding=(dilated,0), dilation = dilated)\n        self.conv1x3_2 = nn.Conv2d(k, k, (1,3), stride=1, padding=(0,dilated), dilation =  dilated)\n        self.bn2 = nn.BatchNorm2d(k)\n\n        self.dropout = nn.Dropout2d(dropprob)\n        \n\n    def forward(self, x):\n        input = x\n\n        output = self.conv1x1(x)\n        output = self.bn0(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_1(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n        output = F.relu(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n\n        output = torch.cat([output,input],1)\n        # print output.size() #check the output\n        return output\n\n\n# --- Build the EDANet Block --- #\nclass EDANetBlock(nn.Module):\n    def __init__(self, in_channels, num_dense_layer, dilated, growth_rate):\n        """"""\n        :param in_channels: input channel size\n        :param num_dense_layer: the number of RDB layers\n        :param growth_rate: growth_rate\n        """"""\n        super().__init__()\n        _in_channels = in_channels\n        modules = []\n        for i in range(num_dense_layer):\n            modules.append(EDAModule(_in_channels, dilated[i], growth_rate))\n            _in_channels += growth_rate\n        self.residual_dense_layers = nn.Sequential(*modules)\n        #self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        out = self.residual_dense_layers(x)\n        #out = self.conv_1x1(out)\n        # out = out + x\n        return out\n\n\nclass EDANet(nn.Module):\n    def __init__(self, classes=19):\n        super(EDANet,self).__init__()\n\n        self.layers = nn.ModuleList()\n\n        # DownsamplerBlock1\n        self.layers.append(DownsamplerBlock(3, 15))\n\n        # DownsamplerBlock2\n        self.layers.append(DownsamplerBlock(15, 60))\n\n        # EDA Block1\n        self.layers.append(EDANetBlock(60, 5, [1,1,1,2,2], 40))\n\n        # DownsamplerBlock3\n        self.layers.append(DownsamplerBlock(260, 130))\n\n        # # EDA Block2\n        self.layers.append(EDANetBlock(130, 8, [2,2,4,4,8,8,16,16], 40))\n\n        # Projection layer\n        self.project_layer = nn.Conv2d(450,classes,kernel_size = 1)\n\n        self.weights_init()\n\n    def weights_init(self):\n        for idx, m in enumerate(self.modules()):\n            classname = m.__class__.__name__\n            if classname.find(\'Conv\') != -1:\n                m.weight.data.normal_(0.0, 0.02)\n            elif classname.find(\'BatchNorm\') != -1:\n                m.weight.data.normal_(1.0, 0.02)\n                m.bias.data.fill_(0)\n\n    def forward(self, x):\n\n        output = x\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.project_layer(output)\n\n        # Bilinear interpolation x8\n        output = F.interpolate(output,scale_factor = 8,mode = \'bilinear\',align_corners=True)\n\n        return output\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = EDANet(classes=19).to(device)\n    summary(model,(3,512,1024))\n'"
model/ENet.py,6,"b'##################################################################################\n#ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation\n#Paper-Link:  https://arxiv.org/pdf/1606.02147.pdf\n##################################################################################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n__all__ = [""ENet""]\n\n\nclass InitialBlock(nn.Module):\n    def __init__(self, in_channels,out_channels, kernel_size, padding=0, bias=False,relu=True):\n        super(InitialBlock, self).__init__()\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        self.main_branch = nn.Conv2d(\n            in_channels,\n            out_channels-3,\n            kernel_size=kernel_size,\n            stride=2,\n            padding=padding,\n            bias=bias,\n        )\n        # MP need padding too\n        self.ext_branch = nn.MaxPool2d(kernel_size, stride=2, padding=padding)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.out_prelu = activation\n\n\n    def forward(self, input):\n        main = self.main_branch(input)\n        ext = self.ext_branch(input)\n\n        out = torch.cat((main, ext), dim=1)\n\n        out = self.batch_norm(out)\n        return self.out_prelu(out)\n\nclass RegularBottleneck(nn.Module):\n    def __init__(self, channels, internal_ratio=4, kernel_size=3, padding=0,\n                 dilation=1, asymmetric=False, dropout_prob=0., bias=False, relu=True):\n        super(RegularBottleneck, self).__init__()\n\n        internal_channels = channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # 1x1 projection conv\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(channels, internal_channels, kernel_size=1, stride=1, bias=bias),\n            nn.BatchNorm2d(internal_channels),\n            activation,\n        )\n        if asymmetric:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(internal_channels, internal_channels, kernel_size=(kernel_size,1),\n                          stride=1, padding=(padding,0), dilation=dilation, bias=bias),\n                nn.BatchNorm2d(internal_channels),\n                activation,\n                nn.Conv2d(internal_channels, internal_channels, kernel_size=(1,kernel_size),\n                          stride=1, padding=(0, padding), dilation=dilation, bias=bias),\n                nn.BatchNorm2d(internal_channels),\n                activation,\n            )\n        else:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(internal_channels, internal_channels, kernel_size=kernel_size,\n                          stride=1, padding=padding, dilation=dilation, bias=bias),\n                nn.BatchNorm2d(internal_channels),\n                activation,\n            )\n\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(internal_channels, channels, kernel_size=1, stride=1, bias=bias),\n            nn.BatchNorm2d(channels),\n            activation,\n        )\n        self.ext_regu1 = nn.Dropout2d(p=dropout_prob)\n        self.out_prelu = activation\n\n    def forward(self, input):\n         main = input\n\n         ext = self.ext_conv1(input)\n         ext = self.ext_conv2(ext)\n         ext = self.ext_conv3(ext)\n         ext = self.ext_regu1(ext)\n\n         out = main + ext\n         return self.out_prelu(out)\n\nclass DownsamplingBottleneck(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 return_indices=False,\n                 dropout_prob=0.,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Store parameters that are needed later\n        self.return_indices = return_indices\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_max1 = nn.MaxPool2d(\n            kernel_size,\n            stride=2,\n            padding=padding,\n            return_indices=return_indices)\n\n        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 2x2 projection convolution with stride 2, no padding\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels,internal_channels,kernel_size=2,stride=2,bias=bias),\n            nn.BatchNorm2d(internal_channels),\n            activation\n        )\n\n        # Convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=kernel_size,\n                stride=1,\n                padding=padding,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                out_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(out_channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x):\n        # Main branch shortcut\n        if self.return_indices:\n            main, max_indices = self.main_max1(x)\n        else:\n            main = self.main_max1(x)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Main branch channel padding\n        # calculate for padding ch_ext - ch_main\n        n, ch_ext, h, w = ext.size()\n        ch_main = main.size()[1]\n        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n\n        # Before concatenating, check if main is on the CPU or GPU and\n        # convert padding accordingly\n        if main.is_cuda:\n            padding = padding.cuda()\n\n        # Concatenate, padding for less channels of main branch\n        main = torch.cat((main, padding), 1)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out), max_indices\n\nclass UpsamplingBottleneck(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 dropout_prob=0.,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU()\n        else:\n            activation = nn.PReLU()\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels))\n\n        # Remember that the stride is the same as the kernel_size, just like\n        # the max pooling layers\n        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 1x1 projection convolution with stride 1\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels, internal_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(internal_channels), activation)\n\n        # Transposed convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.ConvTranspose2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=kernel_size,\n                stride=2,\n                padding=padding,\n                output_padding=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation)\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels), activation)\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = activation\n\n    def forward(self, x, max_indices):\n        # Main branch shortcut\n        main = self.main_conv1(x)\n        main = self.main_unpool1(main, max_indices)\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_prelu(out)\n\nclass ENet(nn.Module):\n    def __init__(self, classes, encoder_relu=False, decoder_relu=True):\n        super().__init__()\n        # source code\n        self.name=\'BaseLine_ENet_trans\'\n\n        self.initial_block = InitialBlock(3, 16, kernel_size=3 ,padding=1, relu=encoder_relu)\n\n        # Stage 1 - Encoder\n        self.downsample1_0 = DownsamplingBottleneck(\n            16,\n            64,\n            padding=1,\n            return_indices=True,\n            dropout_prob=0.01,\n            relu=encoder_relu)\n        self.regular1_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_3 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_4 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n\n        # Stage 2 - Encoder\n        self.downsample2_0 = DownsamplingBottleneck(\n            64,\n            128,\n            padding=1,\n            return_indices=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.regular2_1 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_2 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_3 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_4 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular2_5 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_6 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_7 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_8 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 3 - Encoder\n        self.regular3_0 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_1 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_2 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_3 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular3_4 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_5 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_6 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_7 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 4 - Decoder\n        self.upsample4_0 = UpsamplingBottleneck(\n            128, 64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n\n        # Stage 5 - Decoder\n        self.upsample5_0 = UpsamplingBottleneck(\n            64, 16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular5_1 = RegularBottleneck(\n            16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.transposed_conv = nn.ConvTranspose2d(\n            16,\n            classes,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            output_padding=1,\n            bias=False)\n\n        self.project_layer = nn.Conv2d(128, classes, 1, bias=False)\n\n    def forward(self, x):\n        # Initial block\n        x = self.initial_block(x)\n\n        # Stage 1 - Encoder\n        x, max_indices1_0 = self.downsample1_0(x)\n        x = self.regular1_1(x)\n        x = self.regular1_2(x)\n        x = self.regular1_3(x)\n        x = self.regular1_4(x)\n\n        # Stage 2 - Encoder\n        x, max_indices2_0 = self.downsample2_0(x)\n        x = self.regular2_1(x)\n        x = self.dilated2_2(x)\n        x = self.asymmetric2_3(x)\n        x = self.dilated2_4(x)\n        x = self.regular2_5(x)\n        x = self.dilated2_6(x)\n        x = self.asymmetric2_7(x)\n        x = self.dilated2_8(x)\n\n        # Stage 3 - Encoder\n        x = self.regular3_0(x)\n        x = self.dilated3_1(x)\n        x = self.asymmetric3_2(x)\n        x = self.dilated3_3(x)\n        x = self.regular3_4(x)\n        x = self.dilated3_5(x)\n        x = self.asymmetric3_6(x)\n        x = self.dilated3_7(x)\n\n        #x = self.project_layer(x)\n        #x = F.interpolate(x, scale_factor=8, mode=\'bilinear\', align_corners=True)\n\n        # Stage 4 - Decoder\n        x = self.upsample4_0(x, max_indices2_0)\n        x = self.regular4_1(x)\n        x = self.regular4_2(x)\n\n        # Stage 5 - Decoder\n        x = self.upsample5_0(x, max_indices1_0)\n        x = self.regular5_1(x)\n        x = self.transposed_conv(x)\n\n\n        return x\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = ENet(classes=19).to(device)\n    summary(model,(3,512,1024))\n    # print(model)'"
model/ERFNet.py,4,"b'######################################################################################\n#ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic Segmentation\n#Paper-Link: http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera17tits.pdf\n######################################################################################\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\n__all__ = [""ERFNet""]\n\n\nclass DownsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n    \n\nclass non_bottleneck_1d (nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n        \n\n    def forward(self, input):\n\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n        \n        return F.relu(output+input)    #+input = identity (residual connection)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(3,16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16,64))\n\n        for x in range(0, 5):    #5 times\n           self.layers.append(non_bottleneck_1d(64, 0.03, 1)) \n\n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 2):    #2 times\n            self.layers.append(non_bottleneck_1d(128, 0.3, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.3, 16))\n\n        #Only in encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass UpsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.bn(output)\n        return F.relu(output)\n\nclass Decoder (nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(UpsamplerBlock(128,64))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n\n        self.layers.append(UpsamplerBlock(64,16))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n\n        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n\n    def forward(self, input):\n        output = input\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.output_conv(output)\n\n        return output\n\n#ERFNet\nclass ERFNet(nn.Module):\n    def __init__(self, classes, encoder=None):  #use encoder to pass pretrained encoder\n        super().__init__()\n\n        if (encoder == None):\n            self.encoder = Encoder(classes)\n        else:\n            self.encoder = encoder\n        self.decoder = Decoder(classes)\n\n    def forward(self, input, only_encode=False):\n        if only_encode:\n            return self.encoder.forward(input, predict=True)\n        else:\n            output = self.encoder(input)    #predict=False by default\n            return self.decoder.forward(output)\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = ERFNet(classes=19).to(device)\n    summary(model,(3,512,1024))'"
model/ESNet.py,5,"b'###################################################################################################\n#ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation\n#Paper-Link: https://arxiv.org/pdf/1906.09826.pdf\n###################################################################################################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\nclass DownsamplerBlock(nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, input):\n        x1 = self.pool(input)\n        x2 = self.conv(input)\n\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n\n        output = torch.cat([x2, x1], 1)\n        output = self.bn(output)\n        output = self.relu(output)\n        return output\n\nclass UpsamplerBlock (nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n\n        output = self.conv(input)\n        output = self.bn(output)\n\n        return F.relu(output)\n\t\t\nclass FCU(nn.Module):\n    def __init__(self, chann, kernel_size,dropprob, dilated): \n        """"""\n        Factorized Convolution Unit\n\n        """"""     \n        super(FCU,self).__init__()\n\n        padding = int((kernel_size-1)//2) * dilated\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (kernel_size,1), stride=1, padding=(int((kernel_size-1)//2)*1,0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,kernel_size), stride=1, padding=(0,int((kernel_size-1)//2)*1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (kernel_size,1), stride=1, padding=(padding,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,kernel_size), stride=1, padding=(0,padding), bias=True, dilation = (1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n        \n        self.relu = nn.ReLU(inplace = True)\n        self.dropout = nn.Dropout2d(dropprob)\n        \n    def forward(self, input):\n        residual = input\n        output = self.conv3x1_1(input)\n        output = self.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = self.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = self.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)   \n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n        \n        return F.relu(residual+output,inplace=True) \n\n\nclass PFCU(nn.Module):\n    def __init__(self,chann):\n        """"""\n        Parallel Factorized Convolution Unit\n\n        """"""         \n    \n        super(PFCU,self).__init__()\n        \n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3,1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_22 = nn.Conv2d(chann, chann, (3,1), stride=1, padding=(2,0), bias=True, dilation = (2,1))\n        self.conv1x3_22 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,2), bias=True, dilation = (1,2))\n\n        self.conv3x1_25 = nn.Conv2d(chann, chann, (3,1), stride=1, padding=(5,0), bias=True, dilation = (5,1))\n        self.conv1x3_25 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,5), bias=True, dilation = (1,5))\n\n        self.conv3x1_29 = nn.Conv2d(chann, chann, (3,1), stride=1, padding=(9,0), bias=True, dilation = (9,1))\n        self.conv1x3_29 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,9), bias=True, dilation = (1,9))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(0.3)\n\n    def forward(self, input):\n        residual = input\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output2 = self.conv3x1_22(output)\n        output2 = F.relu(output2)\n        output2 = self.conv1x3_22(output2)\n        output2 = self.bn2(output2)\n        if (self.dropout.p != 0):\n            output2 = self.dropout(output2)\n\n        output5 = self.conv3x1_25(output)\n        output5 = F.relu(output5)\n        output5 = self.conv1x3_25(output5)\n        output5 = self.bn2(output5)\n        if (self.dropout.p != 0):\n            output5 = self.dropout(output5)\n\n        output9 = self.conv3x1_29(output)\n        output9 = F.relu(output9)\n        output9 = self.conv1x3_29(output9)\n        output9 = self.bn2(output9)\n        if (self.dropout.p != 0):\n            output9 = self.dropout(output9)\n\n        return F.relu(residual+output2+output5+output9,inplace=True)\n\n\t\t\nclass ESNet(nn.Module):\n    def __init__(self, classes):\n        super().__init__()\n        #-----ESNET---------#\n        self.initial_block = DownsamplerBlock(3,16)\n\n        self.layers = nn.ModuleList()\n        \n        for x in range(0, 3):\n           self.layers.append(FCU(16, 3, 0.03, 1))  \n        \n        self.layers.append(DownsamplerBlock(16,64))\n\n        for x in range(0, 2):\n           self.layers.append(FCU(64, 5, 0.03, 1))  \n\n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 3):   \n            self.layers.append(PFCU(chann=128)) \n\n        self.layers.append(UpsamplerBlock(128,64))\n        self.layers.append(FCU(64, 5, 0, 1))\n        self.layers.append(FCU(64, 5, 0, 1))\n\n        self.layers.append(UpsamplerBlock(64,16))\n        self.layers.append(FCU(16, 3, 0, 1))\n        self.layers.append(FCU(16, 3, 0, 1))\n\n        self.output_conv = nn.ConvTranspose2d( 16, classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n\n    def forward(self, input):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.output_conv(output)\n\n        return output\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = ESNet(classes=11).to(device)\n    summary(model,(3,360,480))\n'"
model/ESPNet.py,14,"b'########################################################################################\n#ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation\n#Paper-Link:  https://arxiv.org/pdf/1803.06815v2.pdf\n########################################################################################\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\n__all__ = [""ESPNet_Encoder"", ""ESPNet""]\n\nclass CBR(nn.Module):\n    \'\'\'\n    This class defines the convolution layer with batch normalization and PReLU activation\n    \'\'\'\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BR(nn.Module):\n    \'\'\'\n        This class groups the batch normalization and PReLU activation\n    \'\'\'\n    def __init__(self, nOut):\n        \'\'\'\n        :param nOut: output feature maps\n        \'\'\'\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: normalized and thresholded feature map\n        \'\'\'\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\nclass CB(nn.Module):\n    \'\'\'\n       This class groups the convolution and batch normalization\n    \'\'\'\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optinal stide for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-03)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\nclass C(nn.Module):\n    \'\'\'\n    This class is for a convolutional layer.\n    \'\'\'\n    def __init__(self, nIn, nOut, kSize, stride=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1)/2)\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\nclass CDilated(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution, which can maintain feature map size\n    \'\'\'\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1)/2) * d\n        self.conv = nn.Conv2d(nIn, nOut, (kSize, kSize), stride=stride, padding=(padding, padding), bias=False, dilation=d)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\nclass DownSamplerB(nn.Module):\n    def __init__(self, nIn, nOut):\n        super().__init__()\n        n = int(nOut/5)\n        n1 = nOut - 4*n\n        self.c1 = C(nIn, n, 3, 2)\n        self.d1 = CDilated(n, n1, 3, 1, 1)\n        self.d2 = CDilated(n, n, 3, 1, 2)\n        self.d4 = CDilated(n, n, 3, 1, 4)\n        self.d8 = CDilated(n, n, 3, 1, 8)\n        self.d16 = CDilated(n, n, 3, 1, 16)\n        self.bn = nn.BatchNorm2d(nOut, eps=1e-3)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        output1 = self.c1(input)\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n         \n        # Using hierarchical feature fusion (HFF) to ease the gridding artifacts which is introduced \n        # by the large effective receptive filed of the ESP module \n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        combine = torch.cat([d1, add1, add2, add3, add4],1)\n        #combine_in_out = input + combine  #shotcut path\n        output = self.bn(combine)\n        output = self.act(output)\n        return output\n#ESP block\nclass DilatedParllelResidualBlockB(nn.Module):\n    \'\'\'\n    This class defines the ESP block, which is based on the following principle\n        Reduce ---> Split ---> Transform --> Merge\n    \'\'\'\n    def __init__(self, nIn, nOut, add=True):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param add: if true, add a residual connection through identity operation. You can use projection too as\n                in ResNet paper, but we avoid to use it if the dimensions are not the same because we do not want to\n                increase the module complexity\n        \'\'\'\n        super().__init__()\n        n = int(nOut/5)  #K=5, \n        n1 = nOut - 4*n  #(N-(K-1)INT(N/K)) for dilation rate of 2^0, for producing an output feature map of channel=nOut\n        self.c1 = C(nIn, n, 1, 1)  #the point-wise convolutions with 1x1 help in reducing the computation, channel=c\n\n        #K=5, dilation rate: 2^{k-1},k={1,2,3,...,K}\n        self.d1 = CDilated(n, n1, 3, 1, 1) # dilation rate of 2^0\n        self.d2 = CDilated(n, n, 3, 1, 2) # dilation rate of 2^1\n        self.d4 = CDilated(n, n, 3, 1, 4) # dilation rate of 2^2\n        self.d8 = CDilated(n, n, 3, 1, 8) # dilation rate of 2^3\n        self.d16 = CDilated(n, n, 3, 1, 16) # dilation rate of 2^4\n        self.bn = BR(nOut)\n        self.add = add\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        # reduce\n        output1 = self.c1(input)\n        # split and transform\n        d1 = self.d1(output1)\n        d2 = self.d2(output1)\n        d4 = self.d4(output1)\n        d8 = self.d8(output1)\n        d16 = self.d16(output1)\n\n        \n        # Using hierarchical feature fusion (HFF) to ease the gridding artifacts which is introduced \n        # by the large effective receptive filed of the ESP module \n        add1 = d2\n        add2 = add1 + d4\n        add3 = add2 + d8\n        add4 = add3 + d16\n\n        #merge\n        combine = torch.cat([d1, add1, add2, add3, add4], 1)\n\n        # if residual version\n        if self.add:\n            combine = input + combine\n        output = self.bn(combine)\n        return output\n\nclass InputProjectionA(nn.Module):\n    \'\'\'\n    This class projects the input image to the same spatial dimensions as the feature map.\n    For example, if the input image is 512 x512 x3 and spatial dimensions of feature map size are 56x56xF, then\n    this class will generate an output of 56x56x3, for input reinforcement, which establishes a direct link between \n    the input image and encoding stage, improving the flow of information.    \n    \'\'\'\n    def __init__(self, samplingTimes):\n        \'\'\'\n        :param samplingTimes: The rate at which you want to down-sample the image\n        \'\'\'\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, samplingTimes):\n            #pyramid-based approach for down-sampling\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, input):\n        \'\'\'\n        :param input: Input RGB Image\n        :return: down-sampled image (pyramid-based approach)\n        \'\'\'\n        for pool in self.pool:\n            input = pool(input)\n        return input\n\n\nclass ESPNet_Encoder(nn.Module):\n    \'\'\'\n    This class defines the ESPNet-C network in the paper\n    \'\'\'\n    def __init__(self, classes=19, p=5, q=3):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        \'\'\'\n        super().__init__()\n        self.level1 = CBR(3, 16, 3, 2)      # feature map size divided 2,                         1/2\n        self.sample1 = InputProjectionA(1)  #down-sample for input reinforcement, factor=2\n        self.sample2 = InputProjectionA(2)  #down-sample for input reinforcement, factor=4\n\n        self.b1 = BR(16 + 3)\n        self.level2_0 = DownSamplerB(16 +3, 64)  # Downsample Block, feature map size divided 2,    1/4\n\n        self.level2 = nn.ModuleList()\n        for i in range(0, p):\n            self.level2.append(DilatedParllelResidualBlockB(64 , 64))  #ESP block\n        self.b2 = BR(128 + 3)\n\n        self.level3_0 = DownSamplerB(128 + 3, 128) #Downsample Block, feature map size divided 2,   1/8\n        self.level3 = nn.ModuleList()\n        for i in range(0, q):\n            self.level3.append(DilatedParllelResidualBlockB(128 , 128)) # ESPblock\n        self.b3 = BR(256)\n\n        self.classifier = C(256, classes, 1, 1)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: Receives the input RGB image\n        :return: the transformed feature map with spatial dimensions 1/8th of the input image\n        \'\'\'\n        output0 = self.level1(input)\n        inp1 = self.sample1(input)\n        inp2 = self.sample2(input)\n\n        output0_cat = self.b1(torch.cat([output0, inp1], 1))\n        output1_0 = self.level2_0(output0_cat) # down-sampled\n        \n        for i, layer in enumerate(self.level2):\n            if i==0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.b2(torch.cat([output1,  output1_0, inp2], 1))\n\n        output2_0 = self.level3_0(output1_cat) # down-sampled\n        for i, layer in enumerate(self.level3):\n            if i==0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.b3(torch.cat([output2_0, output2], 1))\n\n        classifier = self.classifier(output2_cat)\n\n        #return classifier\n        out = F.upsample(classifier, input.size()[2:], mode=\'bilinear\')   #Upsample score map, factor=8\n        return out\n        \nclass ESPNet(nn.Module):\n    \'\'\'\n    This class defines the ESPNet network\n    \'\'\'\n\n    def __init__(self, classes=19, p=2, q=3, encoderFile=None):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param p: depth multiplier\n        :param q: depth multiplier\n        :param encoderFile: pretrained encoder weights. Recall that we first trained the ESPNet-C and then attached the\n                            RUM-based light weight decoder. See paper for more details.\n        \'\'\'\n        super().__init__()\n        self.encoder = ESPNet_Encoder(classes, p, q)\n        if encoderFile != None:\n            self.encoder.load_state_dict(torch.load(encoderFile))\n            print(\'Encoder loaded!\')\n        # load the encoder modules\n        self.en_modules = []\n        for i, m in enumerate(self.encoder.children()):\n            self.en_modules.append(m)\n\n        # light-weight decoder\n        self.level3_C = C(128 + 3, classes, 1, 1)\n        self.br = nn.BatchNorm2d(classes, eps=1e-03)\n        self.conv = CBR(19 + classes, classes, 3, 1)\n\n        self.up_l3 = nn.Sequential(nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False))\n        self.combine_l2_l3 = nn.Sequential(BR(2*classes), DilatedParllelResidualBlockB(2*classes , classes, add=False))\n\n        self.up_l2 = nn.Sequential(nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False), BR(classes))\n\n        self.classifier = nn.ConvTranspose2d(classes, classes, 2, stride=2, padding=0, output_padding=0, bias=False)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: RGB image\n        :return: transformed feature map\n        \'\'\'\n        output0 = self.en_modules[0](input)\n        inp1 = self.en_modules[1](input)\n        inp2 = self.en_modules[2](input)\n\n        output0_cat = self.en_modules[3](torch.cat([output0, inp1], 1))\n        output1_0 = self.en_modules[4](output0_cat)  # down-sampled\n\n        for i, layer in enumerate(self.en_modules[5]):\n            if i == 0:\n                output1 = layer(output1_0)\n            else:\n                output1 = layer(output1)\n\n        output1_cat = self.en_modules[6](torch.cat([output1, output1_0, inp2], 1))\n\n        output2_0 = self.en_modules[7](output1_cat)  # down-sampled\n        for i, layer in enumerate(self.en_modules[8]):\n            if i == 0:\n                output2 = layer(output2_0)\n            else:\n                output2 = layer(output2)\n\n        output2_cat = self.en_modules[9](torch.cat([output2_0, output2], 1)) # concatenate for feature map width expansion\n\n        output2_c = self.up_l3(self.br(self.en_modules[10](output2_cat))) #RUM\n\n        output1_C = self.level3_C(output1_cat) # project to C-dimensional space\n        comb_l2_l3 = self.up_l2(self.combine_l2_l3(torch.cat([output1_C, output2_c], 1))) #RUM\n\n        concat_features = self.conv(torch.cat([comb_l2_l3, output0_cat], 1))\n\n        classifier = self.classifier(concat_features)\n\n        return classifier\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = ESPNet(classes=19).to(device)\n    summary(model,(3,256,512))'"
model/FPENet.py,6,"b'###################################################################################################\n#FPENet:Feature Pyramid Encoding Network for Real-time Semantic Segmentation\n#Paper-Link: https://arxiv.org/pdf/1909.08599v1.pdf\n###################################################################################################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\n__all__ = [""FPENet""]\n\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1, groups=1, bias=False):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=padding, dilation=dilation, groups=groups,bias=bias)\n\n\ndef conv1x1(in_planes, out_planes, stride=1, bias=False):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias)\n\n\nclass SEModule(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input):\n        x = self.avg_pool(input)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return input * x\n\n\nclass FPEBlock(nn.Module):\n\n    def __init__(self, inplanes, outplanes, dilat, downsample=None, stride=1, t=1, scales=4, se=False, norm_layer=None):\n        super(FPEBlock, self).__init__()\n        if inplanes % scales != 0:\n            raise ValueError(\'Planes must be divisible by scales\')\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        bottleneck_planes = inplanes * t\n        self.conv1 = conv1x1(inplanes, bottleneck_planes, stride)\n        self.bn1 = norm_layer(bottleneck_planes)\n        self.conv2 = nn.ModuleList([conv3x3(bottleneck_planes // scales, bottleneck_planes // scales,\n                                            groups=(bottleneck_planes // scales),dilation=dilat[i],\n                                            padding=1*dilat[i]) for i in range(scales)])\n        self.bn2 = nn.ModuleList([norm_layer(bottleneck_planes // scales) for _ in range(scales)])\n        self.conv3 = conv1x1(bottleneck_planes, outplanes)\n        self.bn3 = norm_layer(outplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SEModule(outplanes) if se else None\n        self.downsample = downsample\n        self.stride = stride\n        self.scales = scales\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        xs = torch.chunk(out, self.scales, 1)\n        ys = []\n        for s in range(self.scales):\n            if s == 0:\n                ys.append(self.relu(self.bn2[s](self.conv2[s](xs[s]))))\n            else:\n                ys.append(self.relu(self.bn2[s](self.conv2[s](xs[s] + ys[-1]))))\n        out = torch.cat(ys, 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.se is not None:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(identity)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\n\nclass MEUModule(nn.Module):\n    def __init__(self, channels_high, channels_low, channel_out):\n        super(MEUModule, self).__init__()\n\n        self.conv1x1_low = nn.Conv2d(channels_low, channel_out, kernel_size=1, bias=False)\n        self.bn_low = nn.BatchNorm2d(channel_out)\n        self.sa_conv = nn.Conv2d(1, 1, kernel_size=1, bias=False)\n\n        self.conv1x1_high = nn.Conv2d(channels_high, channel_out, kernel_size=1, bias=False)\n        self.bn_high = nn.BatchNorm2d(channel_out)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.ca_conv = nn.Conv2d(channel_out, channel_out, kernel_size=1, bias=False)\n\n        self.sa_sigmoid = nn.Sigmoid()\n        self.ca_sigmoid = nn.Sigmoid()\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, fms_high, fms_low):\n        """"""\n        :param fms_high:  High level Feature map. Tensor.\n        :param fms_low: Low level Feature map. Tensor.\n        """"""\n        _, _, h, w = fms_low.shape\n\n        #\n        fms_low = self.conv1x1_low(fms_low)\n        fms_low= self.bn_low(fms_low)\n        sa_avg_out = self.sa_sigmoid(self.sa_conv(torch.mean(fms_low, dim=1, keepdim=True)))\n\n        #\n        fms_high = self.conv1x1_high(fms_high)\n        fms_high = self.bn_high(fms_high)\n        ca_avg_out = self.ca_sigmoid(self.relu(self.ca_conv(self.avg_pool(fms_high))))\n\n        #\n        fms_high_up = F.interpolate(fms_high, size=(h,w), mode=\'bilinear\', align_corners=True)\n        fms_sa_att = sa_avg_out * fms_high_up\n        #\n        fms_ca_att = ca_avg_out * fms_low\n\n        out = fms_ca_att + fms_sa_att\n\n        return out\n\n\nclass FPENet(nn.Module):\n    def __init__(self, classes=19, zero_init_residual=False,\n                 width=16, scales=4, se=False, norm_layer=None):\n        super(FPENet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        outplanes = [int(width * 2 ** i) for i in range(3)] # planes=[16,32,64]\n\n        self.block_num = [1,3,9]\n        self.dilation = [1,2,4,8]\n\n        self.inplanes = outplanes[0]\n        self.conv1 = nn.Conv2d(3, outplanes[0], kernel_size=3, stride=2, padding=1,bias=False)\n        self.bn1 = norm_layer(outplanes[0])\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(FPEBlock, outplanes[0], self.block_num[0], dilation=self.dilation,\n                                       stride=1, t=1, scales=scales, se=se, norm_layer=norm_layer)\n        self.layer2 = self._make_layer(FPEBlock, outplanes[1], self.block_num[1], dilation=self.dilation,\n                                       stride=2, t=4, scales=scales, se=se, norm_layer=norm_layer)\n        self.layer3 = self._make_layer(FPEBlock, outplanes[2], self.block_num[2], dilation=self.dilation,\n                                       stride=2, t=4, scales=scales, se=se, norm_layer=norm_layer)\n        self.meu1 = MEUModule(64,32,64)\n        self.meu2 = MEUModule(64,16,32)\n\n        # Projection layer\n        self.project_layer = nn.Conv2d(32, classes, kernel_size = 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, FPEBlock):\n                    nn.init.constant_(m.bn3.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, dilation, stride=1, t=1, scales=4, se=False, norm_layer=None):\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        downsample = None\n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes, stride),\n                norm_layer(planes),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, dilat=dilation, downsample=downsample, stride=stride, t=t, scales=scales, se=se,\n                            norm_layer=norm_layer))\n        self.inplanes = planes\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilat=dilation, scales=scales, se=se, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        ## stage 1\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x_1 = self.layer1(x)\n\n        ## stage 2\n        x_2_0 = self.layer2[0](x_1)\n        x_2_1 = self.layer2[1](x_2_0)\n        x_2_2 = self.layer2[2](x_2_1)\n        x_2 = x_2_0 + x_2_2\n\n        ## stage 3\n        x_3_0 = self.layer3[0](x_2)\n        x_3_1 = self.layer3[1](x_3_0)\n        x_3_2 = self.layer3[2](x_3_1)\n        x_3_3 = self.layer3[3](x_3_2)\n        x_3_4 = self.layer3[4](x_3_3)\n        x_3_5 = self.layer3[5](x_3_4)\n        x_3_6 = self.layer3[6](x_3_5)\n        x_3_7 = self.layer3[7](x_3_6)\n        x_3_8 = self.layer3[8](x_3_7)\n        x_3 = x_3_0 + x_3_8\n\n\n\n        x2 = self.meu1(x_3, x_2)\n\n        x1 = self.meu2(x2, x_1)\n\n        output = self.project_layer(x1)\n\n        # Bilinear interpolation x2\n        output = F.interpolate(output,scale_factor=2, mode = \'bilinear\', align_corners=True)\n\n        return output\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = FPENet(classes=19).to(device)\n    summary(model,(3,512,1024))\n\n'"
model/FSSNet.py,6,"b'######################################################################################\n#FSSNet: Fast Semantic Segmentation for Scene Perception\n#Paper-Link: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8392426\n######################################################################################\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\nfrom utils.activations import NON_LINEARITY\n\n\n__all__ = [""FSSNet""]\n\n# NON_LINEARITY = {\n#     \'ReLU\': nn.ReLU(inplace=True),\n#     \'PReLU\': nn.PReLU(),\n# \t\'ReLu6\': nn.ReLU6(inplace=True)\n# }\n\nclass InitialBlock(nn.Module):\n    def __init__(self, ninput, noutput, non_linear=\'ReLU\'):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=False)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput-ninput, eps=1e-3)\n        self.relu = NON_LINEARITY[non_linear]\n\n    def forward(self, input):\n        output = self.relu(self.bn(self.conv(input)))\n        output = torch.cat([output, self.pool(input)], 1)\n\n        return output\n\n\nclass DownsamplingBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels, internal_ratio=4, kernel_size=3,\n                 padding=0, dropout_prob=0., bias=False, non_linear=\'ReLU\'):\n        super().__init__()\n        # Store parameters that are needed later\n        internal_channels = in_channels // internal_ratio\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_max1 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=bias),\n        )\n\n        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 2x2 projection convolution with stride 2, no padding\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, internal_channels, kernel_size=2, stride=2, bias=bias),\n            nn.BatchNorm2d(internal_channels),\n            NON_LINEARITY[non_linear]\n        )\n        # Convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.Conv2d(internal_channels, internal_channels, kernel_size=kernel_size, stride=1, padding=padding,\n                      bias=bias),\n            nn.BatchNorm2d(internal_channels),\n            NON_LINEARITY[non_linear]\n        )\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(internal_channels, out_channels, kernel_size=1, stride=1, bias=bias),\n            nn.BatchNorm2d(out_channels),\n            NON_LINEARITY[non_linear]\n        )\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = NON_LINEARITY[non_linear]\n\n    def forward(self, x):\n        # Main branch shortcut\n        main = self.main_max1(x)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = self.out_prelu(main + ext)\n\n        return out\n\n\nclass UpsamplingBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels, internal_ratio=4, kernel_size=2,\n                 padding=0, dropout_prob=0., bias=False, non_linear=\'ReLU\'):\n        super().__init__()\n        internal_channels = in_channels // internal_ratio\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels))\n\n        # Remember that the stride is the same as the kernel_size, just like\n        # the max pooling layers\n        # self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 1x1 projection convolution with stride 1\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, internal_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(internal_channels),\n            NON_LINEARITY[non_linear]\n        )\n        # Transposed convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.ConvTranspose2d(internal_channels, internal_channels, kernel_size=kernel_size, stride=2, padding=padding,\n                               output_padding=0, bias=bias),\n            nn.BatchNorm2d(internal_channels),\n            NON_LINEARITY[non_linear]\n        )\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(internal_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels),\n            NON_LINEARITY[non_linear]\n        )\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_prelu = NON_LINEARITY[non_linear]\n\n    def forward(self, x, x_pre):\n        # Main branch shortcut         # here different origin paper, Fig 4 contradict to Fig 9\n        main = x + x_pre\n\n        main = self.main_conv1(main)     # 2. conv first, follow up\n\n        main = F.interpolate(main, scale_factor=2, mode=""bilinear"", align_corners=True)     # 1. up first, follow conv\n        # main = self.main_conv1(main)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = self.out_prelu(main + ext)\n\n        return out\n\n\nclass DilatedBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n                 dropout_prob=0., bias=False, non_linear=\'ReLU\'):\n        super(DilatedBlock, self).__init__()\n        self.relu = NON_LINEARITY[non_linear]\n        self.internal_channels = in_channels // 4\n        # compress conv\n        self.conv1 = nn.Conv2d(in_channels, self.internal_channels, 1, bias=bias)\n        self.conv1_bn = nn.BatchNorm2d(self.internal_channels)\n        # a relu\n        self.conv2 = nn.Conv2d(self.internal_channels, self.internal_channels, kernel_size,\n                               stride, padding=int((kernel_size - 1) / 2 * dilation), dilation=dilation, groups=1,\n                               bias=bias)\n        self.conv2_bn = nn.BatchNorm2d(self.internal_channels)\n        # a relu\n        self.conv4 = nn.Conv2d(self.internal_channels, out_channels, 1, bias=bias)\n        self.conv4_bn = nn.BatchNorm2d(out_channels)\n        self.regul = nn.Dropout2d(p=dropout_prob)\n\n    def forward(self, x):\n        residual = x\n        main = self.relu(self.conv1_bn(self.conv1(x)))\n        main = self.relu(self.conv2_bn(self.conv2(main)))\n        main = self.conv4_bn(self.conv4(main))\n        main = self.regul(main)\n        out = self.relu(torch.add(main, residual))\n        return out\n\n\nclass Factorized_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n                 dropout_prob=0., bias=False, non_linear=\'ReLU\'):\n        super(Factorized_Block, self).__init__()\n        self.relu = NON_LINEARITY[non_linear]\n        self.internal_channels = in_channels // 4\n        self.compress_conv1 = nn.Conv2d(in_channels, self.internal_channels, 1, padding=0, bias=bias)\n        self.conv1_bn = nn.BatchNorm2d(self.internal_channels)\n        # here is relu\n        self.conv2_1 = nn.Conv2d(self.internal_channels, self.internal_channels, (kernel_size, 1), stride=(stride, 1),\n                                 padding=(int((kernel_size - 1) / 2 * dilation), 0), dilation=(dilation, 1), bias=bias)\n        self.conv2_1_bn = nn.BatchNorm2d(self.internal_channels)\n        self.conv2_2 = nn.Conv2d(self.internal_channels, self.internal_channels, (1, kernel_size), stride=(1, stride),\n                                 padding=(0, int((kernel_size - 1) / 2 * dilation)), dilation=(1, dilation), bias=bias)\n        self.conv2_2_bn = nn.BatchNorm2d(self.internal_channels)\n        # here is relu\n        self.extend_conv3 = nn.Conv2d(self.internal_channels, out_channels, 1, padding=0, bias=bias)\n\n        self.conv3_bn = nn.BatchNorm2d(out_channels)\n        self.regul = nn.Dropout2d(p=dropout_prob)\n\n    def forward(self, x):\n        residual = x\n        main = self.relu((self.conv1_bn(self.compress_conv1(x))))\n        main = self.relu(self.conv2_1_bn(self.conv2_1(main)))\n        main = self.relu(self.conv2_2_bn(self.conv2_2(main)))\n\n        main = self.conv3_bn(self.extend_conv3(main))\n        main = self.regul(main)\n        out = self.relu((torch.add(residual, main)))\n        return out\n\n\nclass FSSNet(nn.Module):\n    def __init__(self, classes):\n        super().__init__()\n\n        self.initial_block = InitialBlock(3, 16)\n\n        # Stage 1 - Encoder\n        self.downsample1_0 = DownsamplingBottleneck(16, 64, padding=1, dropout_prob=0.03)\n        self.factorized1_1 = Factorized_Block(64, 64, dropout_prob=0.03)\n        self.factorized1_2 = Factorized_Block(64, 64, dropout_prob=0.03)\n        self.factorized1_3 = Factorized_Block(64, 64, dropout_prob=0.03)\n        self.factorized1_4 = Factorized_Block(64, 64, dropout_prob=0.03)\n\n        # Stage 2 - Encoder\n        self.downsample2_0 = DownsamplingBottleneck(64, 128, padding=1, dropout_prob=0.3)\n        self.dilated2_1 = DilatedBlock(128, 128, dilation=2, dropout_prob=0.3)\n        self.dilated2_2 = DilatedBlock(128, 128, dilation=5, dropout_prob=0.3)\n        self.dilated2_3 = DilatedBlock(128, 128, dilation=9, dropout_prob=0.3)\n        self.dilated2_4 = DilatedBlock(128, 128, dilation=2, dropout_prob=0.3)\n        self.dilated2_5 = DilatedBlock(128, 128, dilation=5, dropout_prob=0.3)\n        self.dilated2_6 = DilatedBlock(128, 128, dilation=9, dropout_prob=0.3)\n\n        # Stage 4 - Decoder\n        self.upsample4_0 = UpsamplingBottleneck(128, 64, dropout_prob=0.3)\n        self.bottleneck4_1 = DilatedBlock(64, 64, dropout_prob=0.3)\n        self.bottleneck4_2 = DilatedBlock(64, 64, dropout_prob=0.3)\n\n        # Stage 5 - Decoder\n        self.upsample5_0 = UpsamplingBottleneck(64, 16, dropout_prob=0.3)\n        self.bottleneck5_1 = DilatedBlock(16, 16, dropout_prob=0.3)\n        self.bottleneck5_2 = DilatedBlock(16, 16, dropout_prob=0.3)\n\n        self.transposed_conv = nn.ConvTranspose2d(16, classes, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n\n    def forward(self, x):\n        # Initial block\n        # Initial block\n        x = self.initial_block(x)\n\n        # Encoder - Block 1\n        x_1= self.downsample1_0(x)\n        x = self.factorized1_1(x_1)\n        x = self.factorized1_2(x)\n        x = self.factorized1_3(x)\n        x = self.factorized1_4(x)\n\n        # Encoder - Block 2\n        x_2 = self.downsample2_0(x)\n        # print(x_2.shape)\n        x = self.dilated2_1(x_2)\n        x = self.dilated2_2(x)\n        x = self.dilated2_3(x)\n        x = self.dilated2_4(x)\n        x = self.dilated2_5(x)\n        x = self.dilated2_6(x)\n        # print(x.shape)\n\n        # Decoder - Block 3\n        x = self.upsample4_0(x, x_2)\n        x = self.bottleneck4_1(x)\n        x = self.bottleneck4_2(x)\n\n        # Decoder - Block 4\n        x = self.upsample5_0(x, x_1)\n        x = self.bottleneck5_1(x)\n        x = self.bottleneck5_2(x)\n\n        # Fullconv - DeConv\n        x = self.transposed_conv(x)\n\n        return x\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = FSSNet(classes=19).to(device)\n    summary(model,(3,512,1024))\n'"
model/FastSCNN.py,4,"b'##################################################################################\n#Fast-SCNN: Fast Semantic Segmentation Network\n#Paper-Link: https://arxiv.org/pdf/1902.04502.pdf\n##################################################################################\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\n__all__ = [""FastSCNN""]\n\nclass _ConvBNReLU(nn.Module):\n    """"""Conv-BN-ReLU""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, **kwargs):\n        super(_ConvBNReLU, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass _DSConv(nn.Module):\n    """"""Depthwise Separable Convolutions""""""\n\n    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n        super(_DSConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(dw_channels, dw_channels, 3, stride, 1, groups=dw_channels, bias=False),\n            nn.BatchNorm2d(dw_channels),\n            nn.ReLU(True),\n            nn.Conv2d(dw_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass _DWConv(nn.Module):\n    """"""Depthwise Convolutions""""""\n    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n        super(_DWConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(dw_channels, out_channels, 3, stride, 1, groups=dw_channels, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass LinearBottleneck(nn.Module):\n    """"""LinearBottleneck used in MobileNetV2""""""\n\n    def __init__(self, in_channels, out_channels, t=6, stride=2, **kwargs):\n        super(LinearBottleneck, self).__init__()\n        self.use_shortcut = stride == 1 and in_channels == out_channels\n        self.block = nn.Sequential(\n            # pw\n            _ConvBNReLU(in_channels, in_channels * t, 1),\n            # dw\n            _DWConv(in_channels * t, in_channels * t, stride),\n            # pw-linear\n            nn.Conv2d(in_channels * t, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def forward(self, x):\n        out = self.block(x)\n        if self.use_shortcut:\n            out = x + out\n        return out\n\n\nclass PyramidPooling(nn.Module):\n    """"""Pyramid pooling module""""""\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(PyramidPooling, self).__init__()\n        inter_channels = int(in_channels / 4)\n        self.conv1 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n        self.conv2 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n        self.conv3 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n        self.conv4 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n        self.out = _ConvBNReLU(in_channels * 2, out_channels, 1)\n\n    def pool(self, x, size):\n        avgpool = nn.AdaptiveAvgPool2d(size)\n        return avgpool(x)\n\n    def upsample(self, x, size):\n        return F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n\n    def forward(self, x):\n        size = x.size()[2:]\n        feat1 = self.upsample(self.conv1(self.pool(x, 1)), size)\n        feat2 = self.upsample(self.conv2(self.pool(x, 2)), size)\n        feat3 = self.upsample(self.conv3(self.pool(x, 3)), size)\n        feat4 = self.upsample(self.conv4(self.pool(x, 6)), size)\n        x = torch.cat([x, feat1, feat2, feat3, feat4], dim=1)\n        x = self.out(x)\n        return x\n\n\nclass LearningToDownsample(nn.Module):\n    """"""Learning to downsample module""""""\n\n    def __init__(self, dw_channels1=32, dw_channels2=48, out_channels=64, **kwargs):\n        super(LearningToDownsample, self).__init__()\n        self.conv = _ConvBNReLU(3, dw_channels1, 3, 2)\n        self.dsconv1 = _DSConv(dw_channels1, dw_channels2, 2)\n        self.dsconv2 = _DSConv(dw_channels2, out_channels, 2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        return x\n\n\nclass GlobalFeatureExtractor(nn.Module):\n    """"""Global feature extractor module""""""\n\n    def __init__(self, in_channels=64, block_channels=(64, 96, 128),\n                 out_channels=128, t=6, num_blocks=(3, 3, 3), **kwargs):\n        super(GlobalFeatureExtractor, self).__init__()\n        self.bottleneck1 = self._make_layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t, 2)\n        self.bottleneck2 = self._make_layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t, 2)\n        self.bottleneck3 = self._make_layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t, 1)\n        self.ppm = PyramidPooling(block_channels[2], out_channels)\n\n    def _make_layer(self, block, inplanes, planes, blocks, t=6, stride=1):\n        layers = []\n        layers.append(block(inplanes, planes, t, stride))\n        for i in range(1, blocks):\n            layers.append(block(planes, planes, t, 1))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.bottleneck1(x)\n        x = self.bottleneck2(x)\n        x = self.bottleneck3(x)\n        x = self.ppm(x)\n        return x\n\n\nclass FeatureFusionModule(nn.Module):\n    """"""Feature fusion module""""""\n\n    def __init__(self, highter_in_channels, lower_in_channels, out_channels, scale_factor=4, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.scale_factor = scale_factor\n        self.dwconv = _DWConv(lower_in_channels, out_channels, 1)\n        self.conv_lower_res = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.conv_higher_res = nn.Sequential(\n            nn.Conv2d(highter_in_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.relu = nn.ReLU(True)\n\n    def forward(self, higher_res_feature, lower_res_feature):\n        _, _, h, w = higher_res_feature.size()\n        lower_res_feature = F.interpolate(lower_res_feature, size=(h,w), mode=\'bilinear\', align_corners=True)\n        lower_res_feature = self.dwconv(lower_res_feature)\n        lower_res_feature = self.conv_lower_res(lower_res_feature)\n\n        higher_res_feature = self.conv_higher_res(higher_res_feature)\n        out = higher_res_feature + lower_res_feature\n        return self.relu(out)\n\n\nclass Classifer(nn.Module):\n    """"""Classifer""""""\n\n    def __init__(self, dw_channels, num_classes, stride=1, **kwargs):\n        super(Classifer, self).__init__()\n        self.dsconv1 = _DSConv(dw_channels, dw_channels, stride)\n        self.dsconv2 = _DSConv(dw_channels, dw_channels, stride)\n        self.conv = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(dw_channels, num_classes, 1)\n        )\n\n    def forward(self, x):\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        x = self.conv(x)\n        return x\n\n\nclass FastSCNN(nn.Module):\n    def __init__(self, classes, aux=False, **kwargs):\n        super(FastSCNN, self).__init__()\n        self.aux = aux\n        self.learning_to_downsample = LearningToDownsample(32, 48, 64)\n        self.global_feature_extractor = GlobalFeatureExtractor(64, [64, 96, 128], 128, 6, [3, 3, 3])\n        self.feature_fusion = FeatureFusionModule(64, 128, 128)\n        self.classifier = Classifer(128, classes)\n        if self.aux:\n            self.auxlayer = nn.Sequential(\n                nn.Conv2d(64, 32, 3, padding=1, bias=False),\n                nn.BatchNorm2d(32),\n                nn.ReLU(True),\n                nn.Dropout(0.1),\n                nn.Conv2d(32, classes, 1)\n            )\n\n    def forward(self, x):\n        size = x.size()[2:]\n        higher_res_features = self.learning_to_downsample(x)\n        x = self.global_feature_extractor(higher_res_features)\n        x = self.feature_fusion(higher_res_features, x)\n        x = self.classifier(x)\n        outputs = []\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n        if self.aux:\n            auxout = self.auxlayer(higher_res_features)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return x\n        # return tuple(outputs)\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = FastSCNN(classes=19).to(device)\n    summary(model,(3,512,1024))\n\n'"
model/LEDNet.py,7,"b'######################################################################################\n#LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation\n#Paper-Link: https://arxiv.org/abs/1905.02423\n######################################################################################\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary  \n\n\n\n__all__ = [""LEDNet""]\n\ndef Split(x):\n    c = int(x.size()[1])\n    c1 = round(c * 0.5)\n    x1 = x[:, :c1, :, :].contiguous()\n    x2 = x[:, c1:, :, :].contiguous()\n    return x1, x2 \n\n\ndef Merge(x1,x2):\n    return torch.cat((x1,x2),1) \n    \n\ndef Channel_shuffle(x,groups):\n    batchsize, num_channels, height, width = x.data.size()\n    \n    channels_per_group = num_channels // groups\n    \n    #reshape\n    x = x.view(batchsize,groups,\n        channels_per_group,height,width)\n    \n    x = torch.transpose(x,1,2).contiguous()\n    \n    #flatten\n    x = x.view(batchsize,-1,height,width)\n    \n    return x\n\n\nclass PermutationBlock(nn.Module):\n    def __init__(self, groups):\n        super(PermutationBlock, self).__init__()\n        self.groups = groups\n\n    def forward(self, input):\n        n, c, h, w = input.size()\n        G = self.groups\n        output = input.view(n, G, c // G, h, w).permute(0, 2, 1, 3, 4).contiguous().view(n, c, h, w)\n        return output\n\n\n\nclass Conv2dBnRelu(nn.Module):\n    def __init__(self,in_ch,out_ch,kernel_size=3,stride=1,padding=0,dilation=1,bias=True):\n        super(Conv2dBnRelu,self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch,out_ch,kernel_size,stride,padding,dilation=dilation,bias=bias),\n            nn.BatchNorm2d(out_ch, eps=1e-3),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass DownsamplerBlock(nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, input):\n        x1 = self.pool(input)\n        x2 = self.conv(input)\n\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n\n        output = torch.cat([x2, x1], 1)\n        output = self.bn(output)\n        output = self.relu(output)\n        return output\n\n\n\n# class Interpolate(nn.Module):\n#     def __init__(self,size,mode):\n#         super(Interpolate,self).__init__()\n#         self.size = size\n#         self.mode = mode\n#     def forward(self,x):\n#         x = F.interpolate(x,size=self.size,mode=self.mode,align_corners=True)\n#         return x\n\n        \n\nclass SS_nbt_module_paper(nn.Module):\n    def __init__(self, chann, dropprob, dilated):        \n        super().__init__()\n\n        oup_inc = chann//2\n        \n        #dw\n        self.conv3x1_1_l = nn.Conv2d(oup_inc, oup_inc, (3,1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1_l = nn.Conv2d(oup_inc, oup_inc, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1_l = nn.BatchNorm2d(oup_inc, eps=1e-03)\n\n        self.conv3x1_2_l = nn.Conv2d(oup_inc, oup_inc, (3,1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2_l = nn.Conv2d(oup_inc, oup_inc, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1,dilated))\n\n        self.bn2_l = nn.BatchNorm2d(oup_inc, eps=1e-03)\n        \n        #dw\n        self.conv3x1_1_r = nn.Conv2d(oup_inc, oup_inc, (3,1), stride=1, padding=(1,0), bias=True)\n\n        self.conv1x3_1_r = nn.Conv2d(oup_inc, oup_inc, (1,3), stride=1, padding=(0,1), bias=True)\n\n        self.bn1_r = nn.BatchNorm2d(oup_inc, eps=1e-03)\n\n        self.conv3x1_2_r = nn.Conv2d(oup_inc, oup_inc, (3,1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n\n        self.conv1x3_2_r = nn.Conv2d(oup_inc, oup_inc, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1,dilated))\n\n        self.bn2_r = nn.BatchNorm2d(oup_inc, eps=1e-03)       \n        \n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout2d(dropprob)\n\n        # self.channel_shuffle = PermutationBlock(2)\n       \n    \n    def forward(self, x):\n    \n        residual = x\n    \n        x1, x2 = Split(x)\n    \n        output1 = self.conv3x1_1_l(x1)\n        output1 = self.relu(output1)\n        output1 = self.conv1x3_1_l(output1)\n        output1 = self.bn1_l(output1)\n        output1_mid = self.relu(output1)\n\n        output2 = self.conv1x3_1_r(x2)\n        output2 = self.relu(output2)\n        output2 = self.conv3x1_1_r(output2)\n        output2 = self.bn1_r(output2)\n        output2_mid = self.relu(output2)\n\n        output1 = self.conv3x1_2_l(output1_mid)\n        output1 = self.relu(output1)\n        output1 = self.conv1x3_2_l(output1)\n        output1 = self.bn2_l(output1)\n      \n        output2 = self.conv1x3_2_r(output2_mid)\n        output2 = self.relu(output2)\n        output2 = self.conv3x1_2_r(output2)\n        output2 = self.bn2_r(output2)\n\n        if (self.dropout.p != 0):\n            output1 = self.dropout(output1)\n            output2 = self.dropout(output2)\n\n        out = Merge(output1, output2)\n        \n        out = F.relu(residual + out)\n\n        # out = self.channel_shuffle(out)   ### channel shuffle\n        out = Channel_shuffle(out,2)   ### channel shuffle\n\n        return out\n\n        # return    ### channel shuffle\n\n\nclass APNModule(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(APNModule, self).__init__()\n        # global pooling branch\n        self.branch1 = nn.Sequential(\n                nn.AdaptiveAvgPool2d(1),\n                Conv2dBnRelu(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n        )\n\n        # midddle branch\n        self.mid = nn.Sequential(\n            Conv2dBnRelu(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n        )\n\n\n        self.down1 = nn.Sequential(\n            nn.Conv2d(in_ch,1,kernel_size=(7,1),stride=(2,1),padding=(3,0),bias=True),\n            nn.Conv2d(1,1,kernel_size=(1,7),stride=(1,2),padding=(0,3),bias=True),\n            nn.BatchNorm2d(1, eps=1e-03),\n            nn.ReLU(inplace=True)\n        )\n\n\n        self.down2 = nn.Sequential(\n            nn.Conv2d(1,1,kernel_size=(5,1),stride=(2,1),padding=(2,0),bias=True),\n            nn.Conv2d(1,1,kernel_size=(1,5),stride=(1,2),padding=(0,2),bias=True),\n            nn.BatchNorm2d(1, eps=1e-03),\n            nn.ReLU(inplace=True)\n        )\n\n        self.down3 = nn.Sequential(\n            nn.Conv2d(1,1,kernel_size=(3,1),stride=(2,1),padding=(1,0),bias=True),\n            nn.Conv2d(1,1,kernel_size=(1,3),stride=(1,2),padding=(0,1),bias=True),\n            nn.BatchNorm2d(1, eps=1e-03),\n            nn.ReLU(inplace=True),\n            #\n            nn.Conv2d(1,1,kernel_size=(3,1),stride=1,padding=(1,0),bias=True),\n            nn.Conv2d(1,1,kernel_size=(1,3),stride=1,padding=(0,1),bias=True),\n            nn.BatchNorm2d(1, eps=1e-03),\n            nn.ReLU(inplace=True)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(1,1,kernel_size=(5,1),stride=1,padding=(2,0),bias=True),\n            nn.Conv2d(1,1,kernel_size=(1,5),stride=1,padding=(0,2),bias=True),\n            nn.BatchNorm2d(1, eps=1e-03),\n            nn.ReLU(inplace=True)\n        )\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1,1,kernel_size=(7,1),stride=1,padding=(3,0),bias=True),\n            nn.Conv2d(1,1,kernel_size=(1,7),stride=1,padding=(0,3),bias=True),\n            nn.BatchNorm2d(1, eps=1e-03),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n\n        h,w = x.size()[2:]\n\n        b1 = self.branch1(x)\n        b1= F.interpolate(b1, size=(h, w), mode=""bilinear"", align_corners=True)\n\n        mid = self.mid(x)\n\n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x3= F.interpolate(x3, size=((h+3) // 4, (w+3) // 4), mode=""bilinear"", align_corners=True)\n\n        x2 = self.conv2(x2)\n        x = x2 + x3\n        x= F.interpolate(x, size=((h+1) // 2, (w+1) // 2), mode=""bilinear"", align_corners=True)\n\n\n        x1 = self.conv1(x1)\n        x = x + x1\n        x= F.interpolate(x, size=(h, w), mode=""bilinear"", align_corners=True)\n\n        x = torch.mul(x, mid)\n\n        x = x + b1\n\n        return x\n\n\nclass LEDNet(nn.Module):\n    def __init__(self, classes):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(3,32)\n        \n        self.layers = nn.ModuleList()\n\n        for x in range(0, 3):   \n           self.layers.append(SS_nbt_module_paper(32, 0.03, 1)) \n        \n\n        self.layers.append(DownsamplerBlock(32,64))\n        \n\n        for x in range(0, 2):   \n           self.layers.append(SS_nbt_module_paper(64, 0.03, 1)) \n  \n        self.layers.append(DownsamplerBlock(64,128))\n\n        for x in range(0, 1):    \n            self.layers.append(SS_nbt_module_paper(128, 0.3, 1))\n            self.layers.append(SS_nbt_module_paper(128, 0.3, 2))\n            self.layers.append(SS_nbt_module_paper(128, 0.3, 5))\n            self.layers.append(SS_nbt_module_paper(128, 0.3, 9))\n            \n        for x in range(0, 1):    \n            self.layers.append(SS_nbt_module_paper(128, 0.3, 2))\n            self.layers.append(SS_nbt_module_paper(128, 0.3, 5))\n            self.layers.append(SS_nbt_module_paper(128, 0.3, 9))\n            self.layers.append(SS_nbt_module_paper(128, 0.3, 17))\n                    \n        self.apn = APNModule(in_ch=128,out_ch=classes)\n\n        #self.output_conv = nn.ConvTranspose2d(128, num_classes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True)\n        #self.output_conv = nn.ConvTranspose2d(128, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True)\n        #self.output_conv = nn.ConvTranspose2d(128, num_classes, kernel_size=2, stride=2, padding=0, output_padding=0, bias=True)\n\n        # self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input):\n        \n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n            \n        output = self.apn(output)\n        out = F.interpolate(output, input.size()[2:], mode=""bilinear"", align_corners=True)\n        # print(out.shape)\n\n        return out\n\n         \n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = LEDNet(classes=19).to(device)\n    summary(model,(3,360,480))\n'"
model/LinkNet.py,3,"b'############################################################################################\n#LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation\n#Paper-Link:   https://arxiv.org/pdf/1707.03718.pdf\n############################################################################################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\nfrom torchvision.models import resnet\n\n\n\n__all__ = [""LinkNet""]\n\nclass BasicBlock(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, groups=1, bias=False):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=bias)\n        self.bn1 = nn.BatchNorm2d(out_planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size, 1, padding, groups=groups, bias=bias)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n        self.downsample = None\n        if stride > 1:\n            self.downsample = nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n                                            nn.BatchNorm2d(out_planes),)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.relu(out+residual)\n\n        return out\n\n\nclass Encoder(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, groups=1, bias=False):\n        super(Encoder, self).__init__()\n        self.block1 = BasicBlock(in_planes, out_planes, kernel_size, stride, padding, groups, bias)\n        self.block2 = BasicBlock(out_planes, out_planes, kernel_size, 1, padding, groups, bias)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n\n        return x\n\n\nclass Decoder(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=False):\n        # TODO bias=True\n        super(Decoder, self).__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(in_planes, in_planes//4, 1, 1, 0, bias=bias),\n                                nn.BatchNorm2d(in_planes//4),\n                                nn.ReLU(inplace=True))\n        self.tp_conv = nn.Sequential(nn.ConvTranspose2d(in_planes//4, in_planes//4, kernel_size, stride, padding, output_padding, bias=bias),\n                                nn.BatchNorm2d(in_planes//4),\n                                nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(nn.Conv2d(in_planes//4, out_planes, 1, 1, 0, bias=bias),\n                                nn.BatchNorm2d(out_planes),\n                                nn.ReLU(inplace=True))\n\n    def forward(self, x_high_level, x_low_level):\n        x = self.conv1(x_high_level)\n        x = self.tp_conv(x)\n\n        # solution for padding issues\n        # diffY = x_low_level.size()[2] - x_high_level.size()[2]\n        # diffX = x_low_level.size()[3] - x_high_level.size()[3]\n        # x = F.pad(x, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n\n        x = center_crop(x, x_low_level.size()[2], x_low_level.size()[3])\n\n        x = self.conv2(x)\n\n        return x\n\ndef center_crop(layer, max_height, max_width):\n    _, _, h, w = layer.size()\n    diffy = (h - max_height) // 2\n    diffx = (w -max_width) // 2\n    return layer[:,:,diffy:(diffy + max_height),diffx:(diffx + max_width)]\n\n\ndef up_pad(layer, skip_height, skip_width):\n    _, _, h, w = layer.size()\n    diffy = skip_height - h\n    diffx = skip_width -w\n    return F.pad(layer,[diffx // 2, diffx - diffx // 2,\n                        diffy // 2, diffy - diffy // 2])\n\n\nclass LinkNetImprove(nn.Module):\n    """"""\n    Generate Model Architecture\n    """"""\n\n    def __init__(self, classes=19):\n        """"""\n        Model initialization\n        :param x_n: number of input neurons\n        :type x_n: int\n        """"""\n        super().__init__()\n\n        base = resnet.resnet18(pretrained=True)\n\n        self.in_block = nn.Sequential(\n            base.conv1,\n            base.bn1,\n            base.relu,\n            base.maxpool\n        )\n\n        self.encoder1 = base.layer1\n        self.encoder2 = base.layer2\n        self.encoder3 = base.layer3\n        self.encoder4 = base.layer4\n\n        self.decoder1 = Decoder(64, 64, 3, 1, 1, 0)\n        self.decoder2 = Decoder(128, 64, 3, 2, 1, 1)\n        self.decoder3 = Decoder(256, 128, 3, 2, 1, 1)\n        self.decoder4 = Decoder(512, 256, 3, 2, 1, 1)\n\n        # Classifier\n        self.tp_conv1 = nn.Sequential(nn.ConvTranspose2d(64, 32, 3, 2, 1, 1),\n                                      nn.BatchNorm2d(32),\n                                      nn.ReLU(inplace=True),)\n        self.conv2 = nn.Sequential(nn.Conv2d(32, 32, 3, 1, 1),\n                                nn.BatchNorm2d(32),\n                                nn.ReLU(inplace=True),)\n        self.tp_conv2 = nn.ConvTranspose2d(32, classes, 2, 2, 0)\n\n\n    def forward(self, x):\n        # Initial block\n        x = self.in_block(x)\n\n        # Encoder blocks\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder blocks\n        d4 = e3 + self.decoder4(e4, e3)\n        d3 = e2 + self.decoder3(d4, e2)\n        d2 = e1 + self.decoder2(d3, e1)\n        d1 = x + self.decoder1(d2, x)\n\n        # Classifier\n        y = self.tp_conv1(d1)\n        y = self.conv2(y)\n        y = self.tp_conv2(y)\n\n        return y\n\n\nclass LinkNet(nn.Module):\n    """"""\n    Generate model architecture\n    """"""\n\n    def __init__(self, classes=19):\n        """"""\n        Model initialization\n        :param x_n: number of input neurons\n        :type x_n: int\n        """"""\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(3, 2, 1)\n\n        self.encoder1 = Encoder(64, 64, 3, 1, 1)\n        self.encoder2 = Encoder(64, 128, 3, 2, 1)\n        self.encoder3 = Encoder(128, 256, 3, 2, 1)\n        self.encoder4 = Encoder(256, 512, 3, 2, 1)\n\n\n        self.decoder4 = Decoder(512, 256, 3, 2, 1, 1)\n        self.decoder3 = Decoder(256, 128, 3, 2, 1, 1)\n        self.decoder2 = Decoder(128, 64, 3, 2, 1, 1)\n        self.decoder1 = Decoder(64, 64, 3, 1, 1, 0)\n\n\n        # Classifier\n        self.tp_conv1 = nn.Sequential(nn.ConvTranspose2d(64, 32, 3, 2, 1, 1),\n                                      nn.BatchNorm2d(32),\n                                      nn.ReLU(inplace=True),)\n        self.conv2 = nn.Sequential(nn.Conv2d(32, 32, 3, 1, 1),\n                                nn.BatchNorm2d(32),\n                                nn.ReLU(inplace=True),)\n        self.tp_conv2 = nn.ConvTranspose2d(32, classes, 2, 2, 0)\n\n    def forward(self, x):\n        # Initial block\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        # Encoder blocks\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Decoder blocks\n        d4 = e3 + self.decoder4(e4, e3)\n        d3 = e2 + self.decoder3(d4, e2)\n        d2 = e1 + self.decoder2(d3, e1)\n        d1 = x + self.decoder1(d2, x)\n\n        # Classifier\n        y = self.tp_conv1(d1)\n        y = self.conv2(y)\n        y = self.tp_conv2(y)\n\n\n        return y\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = LinkNet(classes=11).to(device)\n    summary(model,(3,512,1024))\n'"
model/SQNet.py,9,"b'###################################################################\n# SQNet: Speeding up semantic segmentation for autonomous driving\n#Paper-Link:   https://openreview.net/pdf?id=S1uHiFyyg\n###################################################################\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\nimport torch.optim as optim\nimport math\nfrom torchsummary import summary\n\n\n\n__all__ = [""SQNet""]\n\nclass Fire(nn.Module):\n    def __init__(self, inplanes, squeeze_planes, expand_planes):\n        super(Fire, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1, stride=1)\n        # self.bn1 = nn.BatchNorm2d(squeeze_planes)\n        self.relu1 = nn.ELU(inplace=True)\n        self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1)\n        # self.bn2 = nn.BatchNorm2d(expand_planes)\n        self.conv3 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=3, stride=1, padding=1)\n        # self.bn3 = nn.BatchNorm2d(expand_planes)\n        self.relu2 = nn.ELU(inplace=True)\n\n        # using MSR initilization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n                m.weight.data.normal_(0, math.sqrt(2./n))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        # x = self.bn1(x)\n        x = self.relu1(x)\n        out1 = self.conv2(x)\n        # out1 = self.bn2(out1)\n        out2 = self.conv3(x)\n        # out2 = self.bn3(out2)\n        out = torch.cat([out1, out2], 1)\n        out = self.relu2(out)\n        return out\n\n\nclass ParallelDilatedConv(nn.Module):\n    def __init__(self, inplanes, planes):\n        super(ParallelDilatedConv, self).__init__()\n        self.dilated_conv_1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=1, dilation=1) \n        self.dilated_conv_2 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=2, dilation=2)\n        self.dilated_conv_3 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=3, dilation=3)\n        self.dilated_conv_4 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=4, dilation=4)\n        self.relu1 = nn.ELU(inplace=True)\n        self.relu2 = nn.ELU(inplace=True)\n        self.relu3 = nn.ELU(inplace=True)\n        self.relu4 = nn.ELU(inplace=True)\n\n    def forward(self, x):\n        out1 = self.dilated_conv_1(x)\n        out2 = self.dilated_conv_2(x)\n        out3 = self.dilated_conv_3(x)\n        out4 = self.dilated_conv_4(x)\n        out1 = self.relu1(out1)\n        out2 = self.relu2(out2)\n        out3 = self.relu3(out3)\n        out4 = self.relu4(out4)\n        out = out1 + out2 + out3 + out4\n        return out\n\nclass SQNet(nn.Module):\n    def __init__(self, classes):\n        super().__init__()\n\n        self.num_classes = classes\n\n        self.conv1 = nn.Conv2d(3, 96, kernel_size=3, stride=2, padding=1) # 32\n        # self.bn1 = nn.BatchNorm2d(96)\n        self.relu1 = nn.ELU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 16\n        self.fire1_1 = Fire(96, 16, 64)\n        self.fire1_2 = Fire(128, 16, 64)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 8\n        self.fire2_1 = Fire(128, 32, 128)\n        self.fire2_2 = Fire(256, 32, 128)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 4\n        self.fire3_1 = Fire(256, 64, 256)\n        self.fire3_2 = Fire(512, 64, 256)\n        self.fire3_3 = Fire(512, 64, 256)\n        self.parallel = ParallelDilatedConv(512, 512)\n        self.deconv1 = nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1)\n        # self.bn2 = nn.BatchNorm2d(256)\n        self.relu2 = nn.ELU(inplace=True)\n        self.deconv2 = nn.ConvTranspose2d(512, 128, 3, stride=2, padding=1, output_padding=1)\n        # self.bn3 = nn.BatchNorm2d(128)\n        self.relu3 = nn.ELU(inplace=True)\n        self.deconv3 = nn.ConvTranspose2d(256, 96, 3, stride=2, padding=1, output_padding=1)\n        # self.bn4 = nn.BatchNorm2d(96)\n        self.relu4 = nn.ELU(inplace=True)\n        self.deconv4 = nn.ConvTranspose2d(192, self.num_classes, 3, stride=2, padding=1, output_padding=1)\n\n        self.conv3_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) # 32\n        self.conv3_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1) # 32\n        self.conv2_1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1) # 32\n        self.conv2_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) # 32\n        self.conv1_1 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1) # 32\n        self.conv1_2 = nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1) # 32\n\n        self.relu1_1 = nn.ELU(inplace=True)\n        self.relu1_2 = nn.ELU(inplace=True)\n        self.relu2_1 = nn.ELU(inplace=True)\n        self.relu2_2 = nn.ELU(inplace=True)\n        self.relu3_1 = nn.ELU(inplace=True)\n        self.relu3_2 = nn.ELU(inplace=True)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        # x = self.bn1(x)\n        x_1 = self.relu1(x)\n        # print ""x_1: %s"" % str(x_1.size())\n        x = self.maxpool1(x_1)\n        x = self.fire1_1(x)\n        x_2 = self.fire1_2(x)\n        # print ""x_2: %s"" % str(x_2.size())\n        x = self.maxpool2(x_2)\n        x = self.fire2_1(x)\n        x_3 = self.fire2_2(x)\n        # print ""x_3: %s"" % str(x_3.size())\n        x = self.maxpool3(x_3)\n        x = self.fire3_1(x)\n        x = self.fire3_2(x)\n        x = self.fire3_3(x)\n        x = self.parallel(x)\n        # print ""x: %s"" % str(x.size())\n        y_3 = self.deconv1(x)\n        y_3 = self.relu2(y_3)\n        x_3 = self.conv3_1(x_3)\n        x_3 = self.relu3_1(x_3)\n        # print ""y_3: %s"" % str(y_3.size())\n        # x = x.transpose(1, 2, 0)\n        # print(\'x_3.size():\', x_3.size())\n        # print(\'y_3.size():\', y_3.size())\n        x_3 = F.interpolate(x_3, y_3.size()[2:], mode=""bilinear"", align_corners=True)\n        x = torch.cat([x_3, y_3], 1)\n        x = self.conv3_2(x)\n        x = self.relu3_2(x)\n        # concat x_3\n        y_2 = self.deconv2(x)\n        y_2 = self.relu3(y_2)\n        x_2 = self.conv2_1(x_2)\n        x_2 = self.relu2_1(x_2)\n        # print ""y_2: %s"" % str(y_2.size())\n        # concat x_2\n        # print(\'x_2.size():\', x_2.size())\n        # print(\'y_2.size():\', y_2.size())\n        y_2 = F.interpolate(y_2, x_2.size()[2:], mode=""bilinear"", align_corners=True)\n        x = torch.cat([x_2, y_2], 1)\n        x = self.conv2_2(x)\n        x = self.relu2_2(x)\n        y_1 = self.deconv3(x)\n        y_1 = self.relu4(y_1)\n        x_1 = self.conv1_1(x_1)\n        x_1 = self.relu1_1(x_1)\n        # print ""y_1: %s"" % str(y_1.size())\n        # concat x_1\n        x = torch.cat([x_1, y_1], 1)\n        x = self.conv1_2(x)\n        x = self.relu1_2(x)\n        x = self.deconv4(x)\n        return x #, x_1, x_2, x_3, y_1, y_2, y_3\n\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = SQNet(classes=19).to(device)\n    summary(model,(3,512,1024))'"
model/SegNet.py,4,"b'##################################################################################\n#SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation\n#Paper-Link: https://arxiv.org/pdf/1511.00561.pdf\n##################################################################################\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\n\n__all__ = [""SegNet""]\n\nclass SegNet(nn.Module):\n    def __init__(self,classes= 19):\n        super(SegNet, self).__init__()\n\n        batchNorm_momentum = 0.1\n\n        self.conv11 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.bn11 = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn12 = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n\n        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn21 = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n        self.conv22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn22 = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n\n        self.conv31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn31 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n        self.conv32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn32 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n        self.conv33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn33 = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n\n        self.conv41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.bn41 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn42 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn43 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n\n        self.conv51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn51 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn52 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv53 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn53 = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n\n        self.conv53d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn53d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv52d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn52d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv51d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn51d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n\n        self.conv43d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn43d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv42d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.bn42d = nn.BatchNorm2d(512, momentum= batchNorm_momentum)\n        self.conv41d = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n        self.bn41d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n\n        self.conv33d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn33d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n        self.conv32d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.bn32d = nn.BatchNorm2d(256, momentum= batchNorm_momentum)\n        self.conv31d = nn.Conv2d(256,  128, kernel_size=3, padding=1)\n        self.bn31d = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n\n        self.conv22d = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn22d = nn.BatchNorm2d(128, momentum= batchNorm_momentum)\n        self.conv21d = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.bn21d = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n\n        self.conv12d = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn12d = nn.BatchNorm2d(64, momentum= batchNorm_momentum)\n        self.conv11d = nn.Conv2d(64, classes, kernel_size=3, padding=1)\n\n\n    def forward(self, x):\n\n        # Stage 1\n        x11 = F.relu(self.bn11(self.conv11(x)))\n        x12 = F.relu(self.bn12(self.conv12(x11)))\n        x1_size = x12.size()\n        x1p, id1 = F.max_pool2d(x12,kernel_size=2, stride=2,return_indices=True)\n\n        # Stage 2\n        x21 = F.relu(self.bn21(self.conv21(x1p)))\n        x22 = F.relu(self.bn22(self.conv22(x21)))\n        x2_size = x22.size()\n        x2p, id2 = F.max_pool2d(x22,kernel_size=2, stride=2,return_indices=True)\n\n        # Stage 3\n        x31 = F.relu(self.bn31(self.conv31(x2p)))\n        x32 = F.relu(self.bn32(self.conv32(x31)))\n        x33 = F.relu(self.bn33(self.conv33(x32)))\n        x3_size = x33.size()\n        x3p, id3 = F.max_pool2d(x33,kernel_size=2, stride=2,return_indices=True)\n\n        # Stage 4\n        x41 = F.relu(self.bn41(self.conv41(x3p)))\n        x42 = F.relu(self.bn42(self.conv42(x41)))\n        x43 = F.relu(self.bn43(self.conv43(x42)))\n        x4_size = x43.size()\n        x4p, id4 = F.max_pool2d(x43,kernel_size=2, stride=2,return_indices=True)\n\n        # Stage 5\n        x51 = F.relu(self.bn51(self.conv51(x4p)))\n        x52 = F.relu(self.bn52(self.conv52(x51)))\n        x53 = F.relu(self.bn53(self.conv53(x52)))\n        x5_size = x53.size()\n        x5p, id5 = F.max_pool2d(x53,kernel_size=2, stride=2,return_indices=True)\n\n\n        # Stage 5d\n        x5d = F.max_unpool2d(x5p, id5, kernel_size=2, stride=2, output_size=x5_size)\n        x53d = F.relu(self.bn53d(self.conv53d(x5d)))\n        x52d = F.relu(self.bn52d(self.conv52d(x53d)))\n        x51d = F.relu(self.bn51d(self.conv51d(x52d)))\n\n        # Stage 4d\n        x4d = F.max_unpool2d(x51d, id4, kernel_size=2, stride=2, output_size=x4_size)\n        x43d = F.relu(self.bn43d(self.conv43d(x4d)))\n        x42d = F.relu(self.bn42d(self.conv42d(x43d)))\n        x41d = F.relu(self.bn41d(self.conv41d(x42d)))\n\n        # Stage 3d\n        x3d = F.max_unpool2d(x41d, id3, kernel_size=2, stride=2, output_size=x3_size)\n        x33d = F.relu(self.bn33d(self.conv33d(x3d)))\n        x32d = F.relu(self.bn32d(self.conv32d(x33d)))\n        x31d = F.relu(self.bn31d(self.conv31d(x32d)))\n\n        # Stage 2d\n        x2d = F.max_unpool2d(x31d, id2, kernel_size=2, stride=2, output_size=x2_size)\n        x22d = F.relu(self.bn22d(self.conv22d(x2d)))\n        x21d = F.relu(self.bn21d(self.conv21d(x22d)))\n\n        # Stage 1d\n        x1d = F.max_unpool2d(x21d, id1, kernel_size=2, stride=2, output_size=x1_size)\n        x12d = F.relu(self.bn12d(self.conv12d(x1d)))\n        x11d = self.conv11d(x12d)\n\n        return x11d\n\n    def load_from_segnet(self, model_path):\n        s_dict = self.state_dict()# create a copy of the state dict\n        th = torch.load(model_path).state_dict() # load the weigths\n        # for name in th:\n            # s_dict[corresp_name[name]] = th[name]\n        self.load_state_dict(th)\n\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = SegNet(classes=19).to(device)\n    summary(model,(3,512,1024))'"
model/UNet.py,4,"b'######################################################################################\n#U-Net: Convolutional Networks for BiomedicalImage Segmentation\n#Paper-Link: https://arxiv.org/pdf/1505.04597.pdf\n######################################################################################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\n\n\n__all__ = [""UNet""]\n\n\nclass double_conv(nn.Module):\n    \'\'\'(conv => BN => ReLU) * 2\'\'\'\n\n    def __init__(self, in_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass inconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(inconv, self).__init__()\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(down, self).__init__()\n        self.mpconv = nn.Sequential(\n            nn.MaxPool2d(2),\n            double_conv(in_ch, out_ch)\n        )\n\n    def forward(self, x):\n        x = self.mpconv(x)\n        return x\n\n\nclass up(nn.Module):\n    def __init__(self, in_ch, out_ch, bilinear=True):\n        super(up, self).__init__()\n        self.bilinear = bilinear\n\n        self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x1, x2):\n        if self.bilinear:\n            x1 = F.interpolate(x1, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        else:\n            x1 = self.up(x1)\n\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n\n        # for padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x\n\n\nclass outconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(outconv, self).__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\n\nclass UNet(nn.Module):\n    def __init__(self, classes):\n        super(UNet, self).__init__()\n        self.inc = inconv(3, 64)\n        self.down1 = down(64, 128)\n        self.down2 = down(128, 256)\n        self.down3 = down(256, 512)\n        self.down4 = down(512, 512)\n        self.up1 = up(1024, 256)\n        self.up2 = up(512, 128)\n        self.up3 = up(256, 64)\n        self.up4 = up(128, 64)\n        self.outc = outconv(64, classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        #return F.sigmoid(x)\n\n        return x\n\n\n\n\n""""""print layers and params of network""""""\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = UNet(classes=19).to(device)\n    summary(model,(3,512,1024))'"
tools/trainID2labelID.py,0,"b""# converting trainIDs to labelIDs for evaluating the test set segmenatation results of the cityscapes dataset\n\nimport numpy as np\nimport os\nfrom PIL import Image\n\n\n\n# index: trainId from 0 to 18, 19 semantic class   val: labelIDs\ncityscapes_trainIds2labelIds = np.array([7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33],\n                                        dtype=np.uint8)\n\n\ndef trainIDs2LabelID(trainID_png_dir, save_dir):\n    print('save_dir:  ', save_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    png_list = os.listdir(trainID_png_dir)\n    for index, png_filename in enumerate(png_list):\n        #\n        png_path = os.path.join(trainID_png_dir, png_filename)\n        # print(png_path)\n        print('processing(', index, '/', len(png_list), ') ....')\n        image = Image.open(png_path)  # image is a PIL #image\n        pngdata = np.array(image)\n        trainID = pngdata  # model prediction\n        row, col = pngdata.shape\n        labelID = np.zeros((row, col), dtype=np.uint8)\n        for i in range(row):\n            for j in range(col):\n                labelID[i][j] = cityscapes_trainIds2labelIds[trainID[i][j]]\n\n        res_path = os.path.join(save_dir, png_filename)\n        new_im = Image.fromarray(labelID)\n        new_im.save(res_path)\n\n\nif __name__ == '__main__':\n    trainID_png_dir = '../server/cityscapes/predict/ENet'\n    save_dir = '../server/cityscapes/predict/cityscapes_submit/'\n    trainIDs2LabelID(trainID_png_dir, save_dir)\n"""
utils/activations.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\n\n\n\n\'\'\'\nScript provides functional interface for Mish activation function.\nMish - ""Mish: A Self Regularized Non-Monotonic Neural Activation Function""\nhttps://arxiv.org/abs/1908.08681v1\n\'\'\'\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # print(""Mish activation loaded..."")\n\n    def forward(self, x):\n        return x *( torch.tanh(F.softplus(x)))\n\n\n\nclass BetaMish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        beta=1.5\n        return x * torch.tanh(torch.log(torch.pow((1+torch.exp(x)),beta)))\n\n\n\'\'\'\nSwish - https://arxiv.org/pdf/1710.05941v1.pdf\n\'\'\'\nclass Swish(nn.Module):\n    def __init__(self):\n        super(Swish, self).__init__()\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        return x * self.sigmoid(x)\n\n\nclass Hswish(nn.Module):\n    def __init__(self, inplace=True):\n        super(Hswish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x * F.relu6(x + 3., inplace=self.inplace) / 6.\n\n\n\nclass Hsigmoid(nn.Module):\n    def __init__(self, inplace=True):\n        super(Hsigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return F.relu6(x + 3., inplace=self.inplace) / 6.\n\n\nclass SEModule(nn.Module):\n    def __init__(self, channel, act, reduction=4):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Sequential(\n            nn.Conv2d(channel, channel // reduction, 1, 1, 0, bias=True),\n            act\n        )\n        self.fc = nn.Sequential(\n            nn.Conv2d(channel // reduction, channel, 1, 1, 0, bias=True),\n            Hsigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv(y)\n        y = self.fc(y)\n        return torch.mul(x, y)\n\n\n\n\n\n\nNON_LINEARITY = {\n    \'ReLU\': nn.ReLU(inplace=True),\n    \'PReLU\': nn.PReLU(),\n    \'ReLu6\': nn.ReLU6(inplace=True),\n    \'Mish\': Mish(),\n    \'BetaMish\': BetaMish(),\n    \'Swish\': Swish(),\n    \'Hswish\': Hswish(),\n    \'tanh\': nn.Tanh(),\n    \'sigmoid\': nn.Sigmoid()\n}'"
utils/colorize_mask.py,1,"b""from PIL import Image\nimport torch\nimport numpy as np\n\ncityscapes_palette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n                      220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0,\n                      70,\n                      0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\n\ncamvid_palette = [128, 128, 128, 128, 0, 0, 192, 192, 128, 128, 64, 128, 60, 40, 222, 128, 128, 0, 192, 128, 128, 64,\n                  64,\n                  128, 64, 0, 128, 64, 64, 0, 0, 128, 192]\n\nzero_pad = 256 * 3 - len(cityscapes_palette)\nfor i in range(zero_pad):\n    cityscapes_palette.append(0)\n\n\n# zero_pad = 256 * 3 - len(camvid_palette)\n# for i in range(zero_pad):\n#     camvid_palette.append(0)\n\ndef cityscapes_colorize_mask(mask):\n    # mask: numpy array of the mask\n    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n    new_mask.putpalette(cityscapes_palette)\n\n    return new_mask\n\n\ndef camvid_colorize_mask(mask):\n    # mask: numpy array of the mask\n    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n    new_mask.putpalette(camvid_palette)\n\n    return new_mask\n\n\nclass VOCColorize(object):\n    def __init__(self, n=22):\n        self.cmap = voc_color_map(22)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.shape\n        color_image = np.zeros((3, size[0], size[1]), dtype=np.uint8)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image)\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        # handle void\n        mask = (255 == gray_image)\n        color_image[0][mask] = color_image[1][mask] = color_image[2][mask] = 255\n\n        return color_image\n\n\ndef voc_color_map(N=256, normalized=False):\n    def bitget(byteval, idx):\n        return ((byteval & (1 << idx)) != 0)\n\n    dtype = 'float32' if normalized else 'uint8'\n    cmap = np.zeros((N, 3), dtype=dtype)\n    for i in range(N):\n        r = g = b = 0\n        c = i\n        for j in range(8):\n            r = r | (bitget(c, 0) << 7 - j)\n            g = g | (bitget(c, 1) << 7 - j)\n            b = b | (bitget(c, 2) << 7 - j)\n            c = c >> 3\n\n        cmap[i] = np.array([r, g, b])\n\n    cmap = cmap / 255 if normalized else cmap\n    return cmap\n"""
utils/convert_state.py,0,"b'from collections import OrderedDict\nimport os\nimport numpy as np\n\n\ndef convert_state_dict(state_dict):\n    """"""\n    Converts a state dict saved from a dataParallel module to normal module state_dict inplace\n    Args:   \n        state_dict is the loaded DataParallel model_state\n    """"""\n    state_dict_new = OrderedDict()\n    # print(type(state_dict))\n    for k, v in state_dict.items():\n        # print(k)\n        name = k[7:]  # remove the prefix module.\n        # My heart is borken, the pytorch have no ability to do with the problem.\n        state_dict_new[name] = v\n    return state_dict_new\n'"
utils/debug.py,0,"b'import sys\nimport traceback\nimport pdb\n\n""""""\nThis module is for debugging without modifying scripts.\n\nBy just adding `import debug` to a script which you want to debug,\nautomatically pdb debugger starts at the point exception raised.\nAfter launching debugger, `from IPython import embed; embed()` enables us to run IPython.\n""""""\n\n\ndef info(exctype, value, tb):\n    # we are in interactive mode or we don\'t have a tty-like\n    # device, so we call the default hook\n    if hasattr(sys, \'ps1\') or not sys.stderr.isatty():\n        sys.__excepthook__(exctype, value, tb)\n    else:\n        traceback.print_exception(exctype, value, tb)\n        pdb.post_mortem(tb)\n\n\nsys.excepthook = info\n'"
utils/utils.py,4,"b'import os\nimport random\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom utils.colorize_mask import cityscapes_colorize_mask, camvid_colorize_mask\n\n\ndef __init_weight(feature, conv_init, norm_layer, bn_eps, bn_momentum,\n                  **kwargs):\n    for name, m in feature.named_modules():\n        if isinstance(m, (nn.Conv2d, nn.Conv3d)):\n            conv_init(m.weight, **kwargs)\n        elif isinstance(m, norm_layer):\n            m.eps = bn_eps\n            m.momentum = bn_momentum\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n\n\ndef init_weight(module_list, conv_init, norm_layer, bn_eps, bn_momentum,\n                **kwargs):\n    if isinstance(module_list, list):\n        for feature in module_list:\n            __init_weight(feature, conv_init, norm_layer, bn_eps, bn_momentum,\n                          **kwargs)\n    else:\n        __init_weight(module_list, conv_init, norm_layer, bn_eps, bn_momentum,\n                      **kwargs)\n\n\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef save_predict(output, gt, img_name, dataset, save_path, output_grey=False, output_color=True, gt_color=False):\n    if output_grey:\n        output_grey = Image.fromarray(output)\n        output_grey.save(os.path.join(save_path, img_name + \'.png\'))\n\n    if output_color:\n        if dataset == \'cityscapes\':\n            output_color = cityscapes_colorize_mask(output)\n        elif dataset == \'camvid\':\n            output_color = camvid_colorize_mask(output)\n\n        output_color.save(os.path.join(save_path, img_name + \'_color.png\'))\n\n    if gt_color:\n        if dataset == \'cityscapes\':\n            gt_color = cityscapes_colorize_mask(gt)\n        elif dataset == \'camvid\':\n            gt_color = camvid_colorize_mask(gt)\n\n        gt_color.save(os.path.join(save_path, img_name + \'_gt.png\'))\n\n\ndef netParams(model):\n    """"""\n    computing total network parameters\n    args:\n       model: model\n    return: the number of parameters\n    """"""\n    total_paramters = 0\n    for parameter in model.parameters():\n        i = len(parameter.size())\n        p = 1\n        for j in range(i):\n            p *= parameter.size(j)\n        total_paramters += p\n\n    return total_paramters\n'"
dataset/cityscape_scripts/__init__.py,0,b''
dataset/cityscape_scripts/generate_mappings.py,0,"b'\nimport glob\nimport os\nfrom utilities.print_utils import *\n\ndef get_mappings(root_dir, files, annot_name):\n    pairs = []\n    for f in files:\n        f = f.replace(root_dir, \'/\')\n        img_f = f.replace(annot_name, \'leftImg8bit\')\n        img_f = img_f.replace(\'_labelTrainIds.png\', \'.png\')\n        if not os.path.isfile(root_dir + img_f):\n            print_error_message(\'{} file does not exist. Please check\'.format(root_dir + img_f))\n            exit()\n        line = img_f + \',\'  + f\n        pairs.append(line)\n    return pairs\n\ndef main(cityscapesPath, split):\n    searchFine = os.path.join(cityscapesPath, ""gtFine"", split, ""*"", \'*_labelTrainIds.png\')\n    filesFine = glob.glob(searchFine)\n    filesFine.sort()\n\n    if not filesFine:\n        print_warning_message(""Did not find any files. Please check root directory: {}."".format(cityscapesPath))\n        fine_pairs = []\n    else:\n        print_info_message(\'{} files found for {} split\'.format(len(filesFine), split))\n        fine_pairs = get_mappings(cityscapesPath, filesFine, \'gtFine\')\n\n    if not fine_pairs:\n        print_error_message(\'No pair exist. Exiting\')\n        exit()\n    else:\n        print_info_message(\'Creating train and val files.\')\n    f_name = split + \'.txt\'\n    with open(os.path.join(cityscapesPath, f_name), \'w\') as txtFile:\n        for pair in fine_pairs:\n            txtFile.write(pair + \'\\n\')\n    print_info_message(\'{} created in {} with {} pairs\'.format(f_name, cityscapesPath, len(fine_pairs)))\n\n    if split == \'train\':\n        split_orig = split\n        split = split + \'_extra\'\n        searchCoarse = os.path.join(cityscapesPath, ""gtCoarse"", split, ""*"", \'*_labelTrainIds.png\')\n        filesCoarse = glob.glob(searchCoarse)\n        filesCoarse.sort()\n        if not filesCoarse:\n            print_warning_message(""Did not find any files. Please check root directory: {}."".format(cityscapesPath))\n            course_pairs = []\n        else:\n            print_info_message(\'{} files found for {} split\'.format(len(filesCoarse), split))\n            course_pairs = get_mappings(cityscapesPath, filesCoarse, \'gtCoarse\')\n        if not course_pairs:\n            print_warning_message(\'No pair exist for coarse data\')\n            return\n        else:\n            print_info_message(\'Creating train and val files.\')\n        f_name = split_orig + \'_coarse.txt\'\n        with open(os.path.join(cityscapesPath, f_name), \'w\') as txtFile:\n            for pair in course_pairs:\n                txtFile.write(pair + \'\\n\')\n        print_info_message(\'{} created in {} with {} pairs\'.format(f_name, cityscapesPath, len(course_pairs)))\n\nif __name__ == \'__main__\':\n    cityscapes_path = \'../../../vision_datasets/cityscapes/\'\n    main(cityscapes_path, ""train"")\n    main(cityscapes_path, ""val"")'"
dataset/cityscape_scripts/print_utils.py,0,"b'\nimport time\n\ntext_colors = {\n               \'logs\': \'\\033[34m\', # 033 is the escape code and 34 is the color code\n               \'info\': \'\\033[32m\',\n               \'warning\': \'\\033[33m\',\n               \'error\': \'\\033[31m\',\n               \'bold\': \'\\033[1m\',\n               \'end_color\': \'\\033[0m\'\n               }\n\n\ndef get_curr_time_stamp():\n    return time.strftime(""%Y-%m-%d %H:%M:%S"")\n\n\ndef print_error_message(message):\n    time_stamp = get_curr_time_stamp()\n    error_str = text_colors[\'error\'] + text_colors[\'bold\'] + \'ERROR  \' + text_colors[\'end_color\']\n    print(\'{} - {} - {}\'.format(time_stamp, error_str, message))\n    print(\'{} - {} - {}\'.format(time_stamp, error_str, \'Exiting!!!\'))\n    exit(-1)\n\n\ndef print_log_message(message):\n    time_stamp = get_curr_time_stamp()\n    log_str = text_colors[\'logs\'] + text_colors[\'bold\'] + \'LOGS   \' + text_colors[\'end_color\']\n    print(\'{} - {} - {}\'.format(time_stamp, log_str, message))\n\n\ndef print_warning_message(message):\n    time_stamp = get_curr_time_stamp()\n    warn_str = text_colors[\'warning\'] + text_colors[\'bold\'] + \'WARNING\' + text_colors[\'end_color\']\n    print(\'{} - {} - {}\'.format(time_stamp, warn_str, message))\n\n\ndef print_info_message(message):\n    time_stamp = get_curr_time_stamp()\n    info_str = text_colors[\'info\'] + text_colors[\'bold\'] + \'INFO   \' + text_colors[\'end_color\']\n    print(\'{} - {} - {}\'.format(time_stamp, info_str, message))\n\n\nif __name__ == \'__main__\':\n    print_log_message(\'Testing\')\n    print_warning_message(\'Testing\')\n    print_info_message(\'Testing\')\n    print_error_message(\'Testing\')'"
dataset/cityscape_scripts/process_cityscapes.py,0,"b'#!/usr/bin/python\n#\n# Converts the polygonal annotations of the Cityscapes dataset\n# to images, where pixel values encode ground truth classes.\n#\n# The Cityscapes downloads already include such images\n#   a) *color.png             : the class is encoded by its color\n#   b) *labelIds.png          : the class is encoded by its ID\n#   c) *instanceIds.png       : the class and the instance are encoded by an instance ID\n# \n# With this tool, you can generate option\n#   d) *labelTrainIds.png     : the class is encoded by its training ID\n# This encoding might come handy for training purposes. You can use\n# the file labes.py to define the training IDs that suit your needs.\n# Note however, that once you submit or evaluate results, the regular\n# IDs are needed.\n#\n# Uses the converter tool in \'json2labelImg.py\'\n# Uses the mapping defined in \'labels.py\'\n#\n\n# python imports\nfrom __future__ import print_function, absolute_import, division\nimport os, glob, sys\nfrom PIL import Image\nfrom PIL import ImageDraw\n## annotation file\nimport os\nimport json\nfrom collections import namedtuple\n\n# get current date and time\nimport datetime\nimport locale\n\n# A point in a polygon\nPoint = namedtuple(\'Point\', [\'x\', \'y\'])\n\nfrom abc import ABCMeta, abstractmethod\n\n# Type of an object\nclass CsObjectType():\n    POLY = 1 # polygon\n    BBOX = 2 # bounding box\n\n# Abstract base class for annotation objects\nclass CsObject:\n    __metaclass__ = ABCMeta\n\n    def __init__(self, objType):\n        self.objectType = objType\n        # the label\n        self.label    = """"\n\n        # If deleted or not\n        self.deleted  = 0\n        # If verified or not\n        self.verified = 0\n        # The date string\n        self.date     = """"\n        # The username\n        self.user     = """"\n        # Draw the object\n        # Not read from or written to JSON\n        # Set to False if deleted object\n        # Might be set to False by the application for other reasons\n        self.draw     = True\n\n    @abstractmethod\n    def __str__(self): pass\n\n    @abstractmethod\n    def fromJsonText(self, jsonText, objId=-1): pass\n\n    @abstractmethod\n    def toJsonText(self): pass\n\n    def updateDate( self ):\n        try:\n            locale.setlocale( locale.LC_ALL , \'en_US.utf8\' )\n        except locale.Error:\n            locale.setlocale( locale.LC_ALL , \'en_US\' )\n        except locale.Error:\n            locale.setlocale( locale.LC_ALL , \'us_us.utf8\' )\n        except locale.Error:\n            locale.setlocale( locale.LC_ALL , \'us_us\' )\n        except:\n            pass\n        self.date = datetime.datetime.now().strftime(""%d-%b-%Y %H:%M:%S"")\n\n    # Mark the object as deleted\n    def delete(self):\n        self.deleted = 1\n        self.draw    = False\n\n# Class that contains the information of a single annotated object as polygon\nclass CsPoly(CsObject):\n    # Constructor\n    def __init__(self):\n        CsObject.__init__(self, CsObjectType.POLY)\n        # the polygon as list of points\n        self.polygon    = []\n        # the object ID\n        self.id         = -1\n\n    def __str__(self):\n        polyText = """"\n        if self.polygon:\n            if len(self.polygon) <= 4:\n                for p in self.polygon:\n                    polyText += \'({},{}) \'.format( p.x , p.y )\n            else:\n                polyText += \'({},{}) ({},{}) ... ({},{}) ({},{})\'.format(\n                    self.polygon[ 0].x , self.polygon[ 0].y ,\n                    self.polygon[ 1].x , self.polygon[ 1].y ,\n                    self.polygon[-2].x , self.polygon[-2].y ,\n                    self.polygon[-1].x , self.polygon[-1].y )\n        else:\n            polyText = ""none""\n        text = ""Object: {} - {}"".format( self.label , polyText )\n        return text\n\n    def fromJsonText(self, jsonText, objId):\n        self.id = objId\n        self.label = str(jsonText[\'label\'])\n        self.polygon = [ Point(p[0],p[1]) for p in jsonText[\'polygon\'] ]\n        if \'deleted\' in jsonText.keys():\n            self.deleted = jsonText[\'deleted\']\n        else:\n            self.deleted = 0\n        if \'verified\' in jsonText.keys():\n            self.verified = jsonText[\'verified\']\n        else:\n            self.verified = 1\n        if \'user\' in jsonText.keys():\n            self.user = jsonText[\'user\']\n        else:\n            self.user = \'\'\n        if \'date\' in jsonText.keys():\n            self.date = jsonText[\'date\']\n        else:\n            self.date = \'\'\n        if self.deleted == 1:\n            self.draw = False\n        else:\n            self.draw = True\n\n    def toJsonText(self):\n        objDict = {}\n        objDict[\'label\'] = self.label\n        objDict[\'id\'] = self.id\n        objDict[\'deleted\'] = self.deleted\n        objDict[\'verified\'] = self.verified\n        objDict[\'user\'] = self.user\n        objDict[\'date\'] = self.date\n        objDict[\'polygon\'] = []\n        for pt in self.polygon:\n            objDict[\'polygon\'].append([pt.x, pt.y])\n\n        return objDict\n\n# Class that contains the information of a single annotated object as bounding box\nclass CsBbox(CsObject):\n    # Constructor\n    def __init__(self):\n        CsObject.__init__(self, CsObjectType.BBOX)\n        # the polygon as list of points\n        self.bbox  = []\n        self.bboxVis  = []\n\n        # the ID of the corresponding object\n        self.instanceId = -1\n\n    def __str__(self):\n        bboxText = """"\n        bboxText += \'[(x1: {}, y1: {}), (w: {}, h: {})]\'.format(\n            self.bbox[0] , self.bbox[1] ,  self.bbox[2] ,  self.bbox[3] )\n\n        bboxVisText = """"\n        bboxVisText += \'[(x1: {}, y1: {}), (w: {}, h: {})]\'.format(\n            self.bboxVis[0] , self.bboxVis[1] , self.bboxVis[2], self.bboxVis[3] )\n\n        text = ""Object: {} - bbox {} - visible {}"".format( self.label , bboxText, bboxVisText )\n        return text\n\n    def fromJsonText(self, jsonText, objId=-1):\n        self.bbox = jsonText[\'bbox\']\n        self.bboxVis = jsonText[\'bboxVis\']\n        self.label = str(jsonText[\'label\'])\n        self.instanceId = jsonText[\'instanceId\']\n\n    def toJsonText(self):\n        objDict = {}\n        objDict[\'label\'] = self.label\n        objDict[\'instanceId\'] = self.instanceId\n        objDict[\'bbox\'] = self.bbox\n        objDict[\'bboxVis\'] = self.bboxVis\n\n        return objDict\n\n# The annotation of a whole image (doesn\'t support mixed annotations, i.e. combining CsPoly and CsBbox)\nclass Annotation:\n    # Constructor\n    def __init__(self, objType=CsObjectType.POLY):\n        # the width of that image and thus of the label image\n        self.imgWidth  = 0\n        # the height of that image and thus of the label image\n        self.imgHeight = 0\n        # the list of objects\n        self.objects = []\n        assert objType in CsObjectType.__dict__.values()\n        self.objectType = objType\n\n    def toJson(self):\n        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n\n    def fromJsonText(self, jsonText):\n        jsonDict = json.loads(jsonText)\n        self.imgWidth  = int(jsonDict[\'imgWidth\'])\n        self.imgHeight = int(jsonDict[\'imgHeight\'])\n        self.objects   = []\n        for objId, objIn in enumerate(jsonDict[ \'objects\' ]):\n            if self.objectType == CsObjectType.POLY:\n                obj = CsPoly()\n            elif self.objectType == CsObjectType.BBOX:\n                obj = CsBbox()\n            obj.fromJsonText(objIn, objId)\n            self.objects.append(obj)\n\n    def toJsonText(self):\n        jsonDict = {}\n        jsonDict[\'imgWidth\'] = self.imgWidth\n        jsonDict[\'imgHeight\'] = self.imgHeight\n        jsonDict[\'objects\'] = []\n        for obj in self.objects:\n            objDict = obj.toJsonText()\n            jsonDict[\'objects\'].append(objDict)\n\n        return jsonDict\n\n    # Read a json formatted polygon file and return the annotation\n    def fromJsonFile(self, jsonFile):\n        if not os.path.isfile(jsonFile):\n            print(\'Given json file not found: {}\'.format(jsonFile))\n            return\n        with open(jsonFile, \'r\') as f:\n            jsonText = f.read()\n            self.fromJsonText(jsonText)\n\n    def toJsonFile(self, jsonFile):\n        with open(jsonFile, \'w\') as f:\n            f.write(self.toJson())\n\n## from labels.py file\n# a label and all meta information\nLabel = namedtuple( \'Label\' , [\n\n    \'name\'        , # The identifier of this label, e.g. \'car\', \'person\', ... .\n                    # We use them to uniquely name a class\n\n    \'id\'          , # An integer ID that is associated with this label.\n                    # The IDs are used to represent the label in ground truth images\n                    # An ID of -1 means that this label does not have an ID and thus\n                    # is ignored when creating ground truth images (e.g. license plate).\n                    # Do not modify these IDs, since exactly these IDs are expected by the\n                    # evaluation server.\n\n    \'trainId\'     , # Feel free to modify these IDs as suitable for your method. Then create\n                    # ground truth images with train IDs, using the tools provided in the\n                    # \'preparation\' folder. However, make sure to validate or submit results\n                    # to our evaluation server using the regular IDs above!\n                    # For trainIds, multiple labels might have the same ID. Then, these labels\n                    # are mapped to the same class in the ground truth images. For the inverse\n                    # mapping, we use the label that is defined first in the list below.\n                    # For example, mapping all void-type classes to the same ID in training,\n                    # might make sense for some approaches.\n                    # Max value is 255!\n\n    \'category\'    , # The name of the category that this label belongs to\n\n    \'categoryId\'  , # The ID of this category. Used to create ground truth images\n                    # on category level.\n\n    \'hasInstances\', # Whether this label distinguishes between single instances or not\n\n    \'ignoreInEval\', # Whether pixels having this class as ground truth label are ignored\n                    # during evaluations or not\n\n    \'color\'       , # The color of this label\n    ] )\n\n\n#--------------------------------------------------------------------------------\n# A list of all labels\n#--------------------------------------------------------------------------------\n\n# Please adapt the train IDs as appropriate for your approach.\n# Note that you might want to ignore labels with ID 255 during training.\n# Further note that the current train IDs are only a suggestion. You can use whatever you like.\n# Make sure to provide your results using the original IDs and not the training IDs.\n# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label(  \'unlabeled\'            ,  0 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'ego vehicle\'          ,  1 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'rectification border\' ,  2 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'out of roi\'           ,  3 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'static\'               ,  4 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'dynamic\'              ,  5 ,      255 , \'void\'            , 0       , False        , True         , (111, 74,  0) ),\n    Label(  \'ground\'               ,  6 ,      255 , \'void\'            , 0       , False        , True         , ( 81,  0, 81) ),\n    Label(  \'road\'                 ,  7 ,        0 , \'flat\'            , 1       , False        , False        , (128, 64,128) ),\n    Label(  \'sidewalk\'             ,  8 ,        1 , \'flat\'            , 1       , False        , False        , (244, 35,232) ),\n    Label(  \'parking\'              ,  9 ,      255 , \'flat\'            , 1       , False        , True         , (250,170,160) ),\n    Label(  \'rail track\'           , 10 ,      255 , \'flat\'            , 1       , False        , True         , (230,150,140) ),\n    Label(  \'building\'             , 11 ,        2 , \'construction\'    , 2       , False        , False        , ( 70, 70, 70) ),\n    Label(  \'wall\'                 , 12 ,        3 , \'construction\'    , 2       , False        , False        , (102,102,156) ),\n    Label(  \'fence\'                , 13 ,        4 , \'construction\'    , 2       , False        , False        , (190,153,153) ),\n    Label(  \'guard rail\'           , 14 ,      255 , \'construction\'    , 2       , False        , True         , (180,165,180) ),\n    Label(  \'bridge\'               , 15 ,      255 , \'construction\'    , 2       , False        , True         , (150,100,100) ),\n    Label(  \'tunnel\'               , 16 ,      255 , \'construction\'    , 2       , False        , True         , (150,120, 90) ),\n    Label(  \'pole\'                 , 17 ,        5 , \'object\'          , 3       , False        , False        , (153,153,153) ),\n    Label(  \'polegroup\'            , 18 ,      255 , \'object\'          , 3       , False        , True         , (153,153,153) ),\n    Label(  \'traffic light\'        , 19 ,        6 , \'object\'          , 3       , False        , False        , (250,170, 30) ),\n    Label(  \'traffic sign\'         , 20 ,        7 , \'object\'          , 3       , False        , False        , (220,220,  0) ),\n    Label(  \'vegetation\'           , 21 ,        8 , \'nature\'          , 4       , False        , False        , (107,142, 35) ),\n    Label(  \'terrain\'              , 22 ,        9 , \'nature\'          , 4       , False        , False        , (152,251,152) ),\n    Label(  \'sky\'                  , 23 ,       10 , \'sky\'             , 5       , False        , False        , ( 70,130,180) ),\n    Label(  \'person\'               , 24 ,       11 , \'human\'           , 6       , True         , False        , (220, 20, 60) ),\n    Label(  \'rider\'                , 25 ,       12 , \'human\'           , 6       , True         , False        , (255,  0,  0) ),\n    Label(  \'car\'                  , 26 ,       13 , \'vehicle\'         , 7       , True         , False        , (  0,  0,142) ),\n    Label(  \'truck\'                , 27 ,       14 , \'vehicle\'         , 7       , True         , False        , (  0,  0, 70) ),\n    Label(  \'bus\'                  , 28 ,       15 , \'vehicle\'         , 7       , True         , False        , (  0, 60,100) ),\n    Label(  \'caravan\'              , 29 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0, 90) ),\n    Label(  \'trailer\'              , 30 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0,110) ),\n    Label(  \'train\'                , 31 ,       16 , \'vehicle\'         , 7       , True         , False        , (  0, 80,100) ),\n    Label(  \'motorcycle\'           , 32 ,       17 , \'vehicle\'         , 7       , True         , False        , (  0,  0,230) ),\n    Label(  \'bicycle\'              , 33 ,       18 , \'vehicle\'         , 7       , True         , False        , (119, 11, 32) ),\n    Label(  \'license plate\'        , -1 ,       -1 , \'vehicle\'         , 7       , False        , True         , (  0,  0,142) ),\n]\n\n\n#--------------------------------------------------------------------------------\n# Create dictionaries for a fast lookup\n#--------------------------------------------------------------------------------\n\n# Please refer to the main method below for example usages!\n\n# name to label object\nname2label      = { label.name    : label for label in labels           }\n# id to label object\nid2label        = { label.id      : label for label in labels           }\n# trainId to label object\ntrainId2label   = { label.trainId : label for label in reversed(labels) }\n# category to list of label objects\ncategory2labels = {}\nfor label in labels:\n    category = label.category\n    if category in category2labels:\n        category2labels[category].append(label)\n    else:\n        category2labels[category] = [label]\n\n# Convert the given annotation to a label image\ndef createLabelImage(annotation, encoding, outline=None):\n    # the size of the image\n    size = ( annotation.imgWidth , annotation.imgHeight )\n\n    # the background\n    if encoding == ""ids"":\n        background = name2label[\'unlabeled\'].id\n    elif encoding == ""trainIds"":\n        background = name2label[\'unlabeled\'].trainId\n    elif encoding == ""color"":\n        background = name2label[\'unlabeled\'].color\n    else:\n        print(""Unknown encoding \'{}\'"".format(encoding))\n        return None\n\n    # this is the image that we want to create\n    if encoding == ""color"":\n        labelImg = Image.new(""RGBA"", size, background)\n    else:\n        labelImg = Image.new(""L"", size, background)\n\n    # a drawer to draw into the image\n    drawer = ImageDraw.Draw( labelImg )\n\n    # loop over all objects\n    for obj in annotation.objects:\n        label   = obj.label\n        polygon = obj.polygon\n\n        # If the object is deleted, skip it\n        if obj.deleted:\n            continue\n\n        # If the label is not known, but ends with a \'group\' (e.g. cargroup)\n        # try to remove the s and see if that works\n        if ( not label in name2label ) and label.endswith(\'group\'):\n            label = label[:-len(\'group\')]\n\n        if not label in name2label:\n            print( ""Label \'{}\' not known."".format(label) )\n\n        # If the ID is negative that polygon should not be drawn\n        if name2label[label].id < 0:\n            continue\n\n        if encoding == ""ids"":\n            val = name2label[label].id\n        elif encoding == ""trainIds"":\n            val = name2label[label].trainId\n        elif encoding == ""color"":\n            val = name2label[label].color\n\n        try:\n            if outline:\n                drawer.polygon( polygon, fill=val, outline=outline )\n            else:\n                drawer.polygon( polygon, fill=val )\n        except:\n            print(""Failed to draw polygon with label {}"".format(label))\n            raise\n\n    return labelImg\n\n# A method that does all the work\n# inJson is the filename of the json file\n# outImg is the filename of the label image that is generated\n# encoding can be set to\n#     - ""ids""      : classes are encoded using the regular label IDs\n#     - ""trainIds"" : classes are encoded using the training IDs\n#     - ""color""    : classes are encoded using the corresponding colors\ndef json2labelImg(inJson,outImg,encoding=""ids""):\n    annotation = Annotation()\n    annotation.fromJsonFile(inJson)\n    labelImg   = createLabelImage( annotation , encoding )\n    labelImg.save( outImg )\n\n# The main method\ndef main(cityscapesPath):\n    # how to search for all ground truth\n    searchFine   = os.path.join( cityscapesPath , ""gtFine""   , ""*"" , ""*"" , ""*_gt*_polygons.json"" )\n    searchCoarse = os.path.join( cityscapesPath , ""gtCoarse"" , ""*"" , ""*"" , ""*_gt*_polygons.json"" )\n\n    # search files\n    filesFine = glob.glob( searchFine )\n    filesFine.sort()\n    filesCoarse = glob.glob( searchCoarse )\n    filesCoarse.sort()\n\n    # concatenate fine and coarse\n    files = filesFine + filesCoarse\n    # files = filesFine # use this line if fine is enough for now.\n\n    # quit if we did not find anything\n    if not files:\n        print( ""Did not find any files. Please consult the README."" )\n\n    # a bit verbose\n    print(""Processing {} annotation files"".format(len(files)))\n\n    # iterate through files\n    progress = 0\n    print(""Progress: {:>3} %"".format( progress * 100 / len(files) ), end=\' \')\n    for f in files:\n        # create the output filename\n        dst = f.replace( ""_polygons.json"" , ""_labelTrainIds.png"" )\n\n        # do the conversion\n        try:\n            json2labelImg( f , dst , ""trainIds"" )\n        except:\n            print(""Failed to convert: {}"".format(f))\n            raise\n\n        # status\n        progress += 1\n        print(""\\rProgress: {:>3} %"".format( progress * 100 / len(files) ), end=\' \')\n        sys.stdout.flush()\n\n\n# call the main\nif __name__ == ""__main__"":\n    cityscapes_path = \'../../../vision_datasets/cityscapes/\'\n    main(cityscapes_path)'"
model/ESPNet_v2/Model.py,5,"b'\nimport math\nimport torch\nfrom torch.nn import init\nimport torch.nn.functional as F\nfrom model.ESPNet_v2.cnn_utils import *\n\n\n__author__ = ""Sachin Mehta""\n__version__ = ""1.0.1""\n__maintainer__ = ""Sachin Mehta""\n\n\n\nclass EESP(nn.Module):\n    \'\'\'\n    This class defines the EESP block, which is based on the following principle\n        REDUCE ---> SPLIT ---> TRANSFORM --> MERGE\n    \'\'\'\n\n    def __init__(self, nIn, nOut, stride=1, k=4, r_lim=7, down_method=\'esp\'): #down_method --> [\'avg\' or \'esp\']\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param stride: factor by which we should skip (useful for down-sampling). If 2, then down-samples the feature map by 2\n        :param k: # of parallel branches\n        :param r_lim: A maximum value of receptive field allowed for EESP block\n        :param g: number of groups to be used in the feature map reduction step.\n        \'\'\'\n        super().__init__()\n        self.stride = stride\n        n = int(nOut / k)\n        n1 = nOut - (k - 1) * n\n        assert down_method in [\'avg\', \'esp\'], \'One of these is suppported (avg or esp)\'\n        assert n == n1, ""n(={}) and n1(={}) should be equal for Depth-wise Convolution "".format(n, n1)\n        #assert nIn%k == 0, ""Number of input channels ({}) should be divisible by # of branches ({})"".format(nIn, k)\n        #assert n % k == 0, ""Number of output channels ({}) should be divisible by # of branches ({})"".format(n, k)\n        self.proj_1x1 = CBR(nIn, n, 1, stride=1, groups=k)\n\n        # (For convenience) Mapping between dilation rate and receptive field for a 3x3 kernel\n        map_receptive_ksize = {3: 1, 5: 2, 7: 3, 9: 4, 11: 5, 13: 6, 15: 7, 17: 8}\n        self.k_sizes = list()\n        for i in range(k):\n            ksize = int(3 + 2 * i)\n            # After reaching the receptive field limit, fall back to the base kernel size of 3 with a dilation rate of 1\n            ksize = ksize if ksize <= r_lim else 3\n            self.k_sizes.append(ksize)\n        # sort (in ascending order) these kernel sizes based on their receptive field\n        # This enables us to ignore the kernels (3x3 in our case) with the same effective receptive field in hierarchical\n        # feature fusion because kernels with 3x3 receptive fields does not have gridding artifact.\n        self.k_sizes.sort()\n        self.spp_dw = nn.ModuleList()\n        #self.bn = nn.ModuleList()\n        for i in range(k):\n            d_rate = map_receptive_ksize[self.k_sizes[i]]\n            self.spp_dw.append(CDilated(n, n, kSize=3, stride=stride, groups=n, d=d_rate))\n            #self.bn.append(nn.BatchNorm2d(n))\n        self.conv_1x1_exp = CB(nOut, nOut, 1, 1, groups=k)\n        self.br_after_cat = BR(nOut)\n        self.module_act = nn.PReLU(nOut)\n        self.downAvg = True if down_method == \'avg\' else False\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n\n        # Reduce --> project high-dimensional feature maps to low-dimensional space\n        output1 = self.proj_1x1(input)\n        output = [self.spp_dw[0](output1)]\n        # compute the output for each branch and hierarchically fuse them\n        # i.e. Split --> Transform --> HFF\n        for k in range(1, len(self.spp_dw)):\n            out_k = self.spp_dw[k](output1)\n            # HFF\n            # We donot combine the branches that have the same effective receptive (3x3 in our case)\n            # because there are no holes in those kernels.\n            out_k = out_k + output[k - 1]\n            #apply batch norm after fusion and then append to the list\n            output.append(out_k)\n        # Merge\n        expanded = self.conv_1x1_exp( # Aggregate the feature maps using point-wise convolution\n            self.br_after_cat( # apply batch normalization followed by activation function (PRelu in this case)\n                torch.cat(output, 1) # concatenate the output of different branches\n            )\n        )\n        del output\n        # if down-sampling, then return the concatenated vector\n        # as Downsampling function will combine it with avg. pooled feature map and then threshold it\n        if self.stride == 2 and self.downAvg:\n            return expanded\n\n        # if dimensions of input and concatenated vector are the same, add them (RESIDUAL LINK)\n        if expanded.size() == input.size():\n            expanded = expanded + input\n\n        # Threshold the feature map using activation function (PReLU in this case)\n        return self.module_act(expanded)\n\n\nclass DownSampler(nn.Module):\n    \'\'\'\n    Down-sampling fucntion that has two parallel branches: (1) avg pooling\n    and (2) EESP block with stride of 2. The output feature maps of these branches\n    are then concatenated and thresholded using an activation function (PReLU in our\n    case) to produce the final output.\n    \'\'\'\n\n    def __init__(self, nin, nout, k=4, r_lim=9, reinf=True):\n        \'\'\'\n            :param nin: number of input channels\n            :param nout: number of output channels\n            :param k: # of parallel branches\n            :param r_lim: A maximum value of receptive field allowed for EESP block\n            :param g: number of groups to be used in the feature map reduction step.\n        \'\'\'\n        super().__init__()\n        nout_new = nout - nin\n        self.eesp = EESP(nin, nout_new, stride=2, k=k, r_lim=r_lim, down_method=\'avg\')\n        self.avg = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n        if reinf:\n            self.inp_reinf = nn.Sequential(\n                CBR(config_inp_reinf, config_inp_reinf, 3, 1),\n                CB(config_inp_reinf, nout, 1, 1)\n            )\n        self.act =  nn.PReLU(nout)\n\n    def forward(self, input, input2=None):\n        \'\'\'\n        :param input: input feature map\n        :return: feature map down-sampled by a factor of 2\n        \'\'\'\n        avg_out = self.avg(input)\n        eesp_out = self.eesp(input)\n        output = torch.cat([avg_out, eesp_out], 1)\n        if input2 is not None:\n            #assuming the input is a square image\n            w1 = avg_out.size(2)\n            while True:\n                input2 = F.avg_pool2d(input2, kernel_size=3, padding=1, stride=2)\n                w2 = input2.size(2)\n                if w2 == w1:\n                    break\n            output = output + self.inp_reinf(input2)\n\n        return self.act(output) #self.act(output)\n\nclass EESPNet(nn.Module):\n    \'\'\'\n    This class defines the ESPNetv2 architecture for the ImageNet classification\n    \'\'\'\n\n    def __init__(self, classes=19, s=1):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param s: factor that scales the number of output feature maps\n        \'\'\'\n        super().__init__()\n        reps = [0, 3, 7, 3]  # how many times EESP blocks should be repeated.\n        channels = 3\n\n        r_lim = [13, 11, 9, 7, 5]  # receptive field at each spatial level\n        K = [4]*len(r_lim) # No. of parallel branches at different levels\n\n        base = 32 #base configuration\n        config_len = 5\n        config = [base] * config_len\n        base_s = 0\n        for i in range(config_len):\n            if i== 0:\n                base_s = int(base * s)\n                base_s = math.ceil(base_s / K[0]) * K[0]\n                config[i] = base if base_s > base else base_s\n            else:\n                config[i] = base_s * pow(2, i)\n        if s <= 1.5:\n            config.append(1024)\n        elif s in [1.5, 2]:\n            config.append(1280)\n        else:\n            ValueError(\'Configuration not supported\')\n\n        #print(\'Config: \', config)\n\n        global config_inp_reinf\n        config_inp_reinf = 3\n        self.input_reinforcement = True\n        assert len(K) == len(r_lim), \'Length of branching factor array and receptive field array should be the same.\'\n\n        self.level1 = CBR(channels, config[0], 3, 2)  # 112 L1\n\n        self.level2_0 = DownSampler(config[0], config[1], k=K[0], r_lim=r_lim[0], reinf=self.input_reinforcement)  # out = 56\n        self.level3_0 = DownSampler(config[1], config[2], k=K[1], r_lim=r_lim[1], reinf=self.input_reinforcement) # out = 28\n        self.level3 = nn.ModuleList()\n        for i in range(reps[1]):\n            self.level3.append(EESP(config[2], config[2], stride=1, k=K[2], r_lim=r_lim[2]))\n\n        self.level4_0 = DownSampler(config[2], config[3], k=K[2], r_lim=r_lim[2], reinf=self.input_reinforcement) #out = 14\n        self.level4 = nn.ModuleList()\n        for i in range(reps[2]):\n            self.level4.append(EESP(config[3], config[3], stride=1, k=K[3], r_lim=r_lim[3]))\n\n        self.level5_0 = DownSampler(config[3], config[4], k=K[3], r_lim=r_lim[3]) #7\n        self.level5 = nn.ModuleList()\n        for i in range(reps[3]):\n            self.level5.append(EESP(config[4], config[4], stride=1, k=K[4], r_lim=r_lim[4]))\n\n        # expand the feature maps using depth-wise separable convolution\n        self.level5.append(CBR(config[4], config[4], 3, 1, groups=config[4]))\n        self.level5.append(CBR(config[4], config[5], 1, 1, groups=K[4]))\n\n\n\n        #self.level5_exp = nn.ModuleList()\n        #assert config[5]%config[4] == 0, \'{} should be divisible by {}\'.format(config[5], config[4])\n        #gr = int(config[5]/config[4])\n        #for i in range(gr):\n        #    self.level5_exp.append(CBR(config[4], config[4], 1, 1, groups=pow(2, i)))\n\n        self.classifier = nn.Linear(config[5], classes)\n        self.init_params()\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, input, p=0.2, seg=True):\n        \'\'\'\n        :param input: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n        out_l1 = self.level1(input)  # 112\n        if not self.input_reinforcement:\n            del input\n            input = None\n\n        out_l2 = self.level2_0(out_l1, input)  # 56\n\n        out_l3_0 = self.level3_0(out_l2, input)  # out_l2_inp_rein\n        for i, layer in enumerate(self.level3):\n            if i == 0:\n                out_l3 = layer(out_l3_0)\n            else:\n                out_l3 = layer(out_l3)\n\n        out_l4_0 = self.level4_0(out_l3, input)  # down-sampled\n        for i, layer in enumerate(self.level4):\n            if i == 0:\n                out_l4 = layer(out_l4_0)\n            else:\n                out_l4 = layer(out_l4)\n\n        if not seg:\n            out_l5_0 = self.level5_0(out_l4)  # down-sampled\n            for i, layer in enumerate(self.level5):\n                if i == 0:\n                    out_l5 = layer(out_l5_0)\n                else:\n                    out_l5 = layer(out_l5)\n\n            #out_e = []\n            #for layer in self.level5_exp:\n            #    out_e.append(layer(out_l5))\n            #out_exp = torch.cat(out_e, dim=1)\n\n\n\n            output_g = F.adaptive_avg_pool2d(out_l5, output_size=1)\n            output_g = F.dropout(output_g, p=p, training=self.training)\n            output_1x1 = output_g.view(output_g.size(0), -1)\n\n            return self.classifier(output_1x1)\n        return out_l1, out_l2, out_l3, out_l4\n\n\n'"
model/ESPNet_v2/SegmentationModel.py,7,"b'###################################################################################################\n#ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network\n#Paper-Link: https://arxiv.org/pdf/1811.11431.pdf\n###################################################################################################\n\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\n\nfrom model.ESPNet_v2.Model import EESPNet, EESP\nfrom model.ESPNet_v2.cnn_utils import *\n\n\n__all__ = [""EESPNet_Seg""]\n\n\nclass EESPNet_Seg(nn.Module):\n    def __init__(self, classes=19, s=2, pretrained=None, gpus=1):\n        super().__init__()\n        classificationNet = EESPNet(classes=1000, s=s)\n        if gpus >=1:\n            classificationNet = nn.DataParallel(classificationNet)\n            # print(classificationNet)\n        # load the pretrained weights\n        if pretrained:\n            if not os.path.isfile(pretrained):\n                print(\'Weight file does not exist. Training without pre-trained weights\')\n            print(\'Model initialized with pretrained weights\')\n            classificationNet.load_state_dict(torch.load(pretrained))\n\n        self.net = classificationNet.module\n\n        del classificationNet\n        # delete last few layers\n        del self.net.classifier\n        del self.net.level5\n        del self.net.level5_0\n        if s <=0.5:\n            p = 0.1\n        else:\n            p=0.2\n\n        self.proj_L4_C = CBR(self.net.level4[-1].module_act.num_parameters, self.net.level3[-1].module_act.num_parameters, 1, 1)\n        pspSize = 2*self.net.level3[-1].module_act.num_parameters\n        self.pspMod = nn.Sequential(EESP(pspSize, pspSize //2, stride=1, k=4, r_lim=7),\n                PSPModule(pspSize // 2, pspSize //2))\n        self.project_l3 = nn.Sequential(nn.Dropout2d(p=p), C(pspSize // 2, classes, 1, 1))\n        self.act_l3 = BR(classes)\n        self.project_l2 = CBR(self.net.level2_0.act.num_parameters + classes, classes, 1, 1)\n        self.project_l1 = nn.Sequential(nn.Dropout2d(p=p), C(self.net.level1.act.num_parameters + classes, classes, 1, 1))\n\n    def hierarchicalUpsample(self, x, factor=3):\n        for i in range(factor):\n            x = F.interpolate(x, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        return x\n\n\n    def forward(self, input):\n        out_l1, out_l2, out_l3, out_l4 = self.net(input, seg=True)\n        out_l4_proj = self.proj_L4_C(out_l4)\n        up_l4_to_l3 = F.interpolate(out_l4_proj, size=out_l3.size()[2:], mode=\'bilinear\', align_corners=True)\n        merged_l3_upl4 = self.pspMod(torch.cat([out_l3, up_l4_to_l3], 1))\n        proj_merge_l3_bef_act = self.project_l3(merged_l3_upl4)\n        proj_merge_l3 = self.act_l3(proj_merge_l3_bef_act)\n        out_up_l3 = F.interpolate(proj_merge_l3, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merge_l2 = self.project_l2(torch.cat([out_l2, out_up_l3], 1))\n        out_up_l2 = F.interpolate(merge_l2, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merge_l1 = self.project_l1(torch.cat([out_l1, out_up_l2], 1))\n        # if self.training:\n        #     return F.interpolate(merge_l1, scale_factor=2, mode=\'bilinear\', align_corners=True), self.hierarchicalUpsample(proj_merge_l3_bef_act)\n        # else:\n        #     return F.interpolate(merge_l1, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        output = F.interpolate(merge_l1, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        return output\n\nif __name__ == \'__main__\':\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = EESPNet_Seg(classes=19, s=2).to(device)\n    summary(model,(3,512,1024))'"
model/ESPNet_v2/cnn_utils.py,3,"b'import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n\n__author__ = ""Sachin Mehta""\n__version__ = ""1.0.1""\n__maintainer__ = ""Sachin Mehta""\n\n\nclass PSPModule(nn.Module):\n    def __init__(self, features, out_features=1024, sizes=(1, 2, 4, 8)):\n        super().__init__()\n        self.stages = []\n        self.stages = nn.ModuleList([C(features, features, 3, 1, groups=features) for size in sizes])\n        self.project = CBR(features * (len(sizes) + 1), out_features, 1, 1)\n \n    def forward(self, feats):\n        h, w = feats.size(2), feats.size(3)\n        out = [feats]\n        for stage in self.stages:\n            feats = F.avg_pool2d(feats, kernel_size=3, stride=2, padding=1)\n            upsampled = F.interpolate(input=stage(feats), size=(h, w), mode=\'bilinear\', align_corners=True)\n            out.append(upsampled)\n        return self.project(torch.cat(out, dim=1))\n\nclass CBR(nn.Module):\n    \'\'\'\n    This class defines the convolution layer with batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False, groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        # output = self.conv1(output)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BR(nn.Module):\n    \'\'\'\n        This class groups the batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nOut):\n        \'\'\'\n        :param nOut: output feature maps\n        \'\'\'\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: normalized and thresholded feature map\n        \'\'\'\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\n\nclass CB(nn.Module):\n    \'\'\'\n       This class groups the convolution and batch normalization\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optinal stide for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False,\n                              groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n\n    def forward(self, input):\n        \'\'\'\n\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\n\nclass C(nn.Module):\n    \'\'\'\n    This class is for a convolutional layer.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False,\n                              groups=groups)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\n\nclass CDilated(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2) * d\n        self.conv = nn.Conv2d(nIn, nOut,kSize, stride=stride, padding=padding, bias=False,\n                              dilation=d, groups=groups)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\nclass CDilatedB(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution with batch normalization.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2) * d\n        self.conv = nn.Conv2d(nIn, nOut,kSize, stride=stride, padding=padding, bias=False,\n                              dilation=d, groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        return self.bn(self.conv(input))\n'"
tools/flops_counter/ENet_Flops_test.py,1,"b""import sys\nimport argparse\nimport torch\n\n\nfrom model.ENet import ENet\nfrom model.ERFNet import ERFNet\nfrom model.CGNet import CGNet\nfrom model.EDANet import EDANet\nfrom model.ESNet import ESNet\nfrom model.ESPNet import ESPNet\nfrom model.LEDNet import LEDNet\nfrom model.ESPNet_v2.SegmentationModel import EESPNet_Seg\nfrom model.FastSCNN import FastSCNN\nfrom model.DABNet import DABNet\nfrom model.FPENet import FPENet\n\n\n\n\n\n\nfrom tools.flops_counter.ptflops import get_model_complexity_info\n\n\n\npt_models = {\n\n    'ENet': ENet,\n    'ERFNet': ERFNet,\n    'CGNet': CGNet,\n    'EDANet': EDANet,\n    'ESNet': ESNet,\n    'ESPNet': ESPNet,\n    'LEDNet': LEDNet,\n    'EESPNet_Seg': EESPNet_Seg,\n    'FastSCNN': FastSCNN,\n    'DABNet': DABNet,\n    'FPENet': FPENet\n    }\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='ptflops sample script')\n    parser.add_argument('--device', type=int, default=0,\n                        help='Device to store the model.')\n    parser.add_argument('--model', choices=list(pt_models.keys()),\n                        type=str, default='ENet')\n    parser.add_argument('--result', type=str, default=None)\n    args = parser.parse_args()\n\n    if args.result is None:\n        ost = sys.stdout\n    else:\n        ost = open(args.result, 'w')\n\n    with torch.cuda.device(args.device):\n        net = pt_models[args.model](classes=19).cuda()\n\n        flops, params = get_model_complexity_info(net, (3, 512, 1024),\n                                                  as_strings=True,\n                                                  print_per_layer_stat=True,\n                                                  ost=ost)\n        print('Flops: ' + flops)\n        print('Params: ' + params)"""
tools/flops_counter/sample.py,1,"b""import sys\nimport argparse\n\nimport torchvision.models as models\nimport torch\n\nfrom ptflops import get_model_complexity_info\n\npt_models = {'resnet18': models.resnet18, 'resnet50': models.resnet50,\n             'alexnet': models.alexnet,\n             'vgg16': models.vgg16,\n             'squeezenet': models.squeezenet1_0,\n             'densenet': models.densenet161,\n             'inception': models.inception_v3}\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='ptflops sample script')\n    parser.add_argument('--device', type=int, default=0,\n                        help='Device to store the model.')\n    parser.add_argument('--model', choices=list(pt_models.keys()),\n                        type=str, default='resnet18')\n    parser.add_argument('--result', type=str, default=None)\n    args = parser.parse_args()\n\n    if args.result is None:\n        ost = sys.stdout\n    else:\n        ost = open(args.result, 'w')\n\n    with torch.cuda.device(args.device):\n        net = pt_models[args.model]().cuda()\n\n        flops, params = get_model_complexity_info(net, (3, 224, 224),\n                                                  as_strings=True,\n                                                  print_per_layer_stat=True,\n                                                  ost=ost)\n        print('Flops: ' + flops)\n        print('Params: ' + params)\n"""
tools/flops_counter/setup.py,0,"b""import os\nimport shutil\nimport sys\nfrom setuptools import setup, find_packages\n\nreadme = open('README.md').read()\n\nVERSION = '0.3'\n\nrequirements = [\n    'torch',\n]\n\nsetup(\n    # Metadata\n    name='ptflops',\n    version=VERSION,\n    author='Vladislav Sovrasov',\n    author_email='sovrasov.vlad@gmail.com',\n    url='https://github.com/sovrasov/flops-counter.pytorch',\n    description='Flops counter for convolutional networks in pytorch framework',\n    long_description=readme,\n    long_description_content_type='text/markdown',\n    license='MIT',\n\n    # Package info\n    packages=find_packages(exclude=('*test*',)),\n\n    #\n    zip_safe=True,\n    install_requires=requirements,\n\n    # Classifiers\n    classifiers=[\n        'Programming Language :: Python :: 3',\n    ],\n)\n"""
tools/fps_test/eval_forward_time.py,5,"b'import time\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom argparse import ArgumentParser\nfrom builders.model_builder import build_model\n\n\ndef compute_speed(model, input_size, device, iteration=100):\n    torch.cuda.set_device(device)\n    cudnn.benchmark = True\n\n    model.eval()\n    model = model.cuda()\n\n    input = torch.randn(*input_size, device=device)\n\n    for _ in range(50):\n        model(input)\n\n    print(\'=========Eval Forward Time=========\')\n    torch.cuda.synchronize()\n    t_start = time.time()\n    for _ in range(iteration):\n        model(input)\n    torch.cuda.synchronize()\n    elapsed_time = time.time() - t_start\n\n    speed_time = elapsed_time / iteration * 1000\n    fps = iteration / elapsed_time\n\n    print(\'Elapsed Time: [%.2f s / %d iter]\' % (elapsed_time, iteration))\n    print(\'Speed Time: %.2f ms / iter   FPS: %.2f\' % (speed_time, fps))\n    return speed_time, fps\n\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n\n    parser.add_argument(""--size"", type=str, default=""512,1024"", help=""input size of model"")\n    parser.add_argument(\'--num-channels\', type=int, default=3)\n    parser.add_argument(\'--batch-size\', type=int, default=1)\n    parser.add_argument(\'--classes\', type=int, default=19)\n    parser.add_argument(\'--iter\', type=int, default=100)\n    parser.add_argument(\'--model\', type=str, default=\'ENet\')\n    parser.add_argument(""--gpus"", type=str, default=""0"", help=""gpu ids (default: 0)"")\n    args = parser.parse_args()\n\n    h, w = map(int, args.size.split(\',\'))\n    model = build_model(args.model, num_classes=args.classes)\n    compute_speed(model, (args.batch_size, args.num_channels, h, w), int(args.gpus), iteration=args.iter)\n'"
utils/losses/__init__.py,0,b'from .loss import *\n'
utils/losses/loss.py,24,"b'import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom utils.losses.lovasz_losses import lovasz_softmax\nfrom torch.nn.modules.loss import _Loss, _WeightedLoss\nfrom torch.nn import NLLLoss2d\n\n\n__all__ = [""CrossEntropyLoss2d"", ""CrossEntropyLoss2dLabelSmooth"",\n           ""FocalLoss2d"", ""LDAMLoss"", ""ProbOhemCrossEntropy2d"",\n           ""LovaszSoftmax""]\n\n\nclass CrossEntropyLoss2d(_WeightedLoss):\n    """"""\n    Standard pytorch weighted nn.CrossEntropyLoss\n    """"""\n\n    def __init__(self, weight=None, ignore_label=255, reduction=\'mean\'):\n        super(CrossEntropyLoss2d, self).__init__()\n\n        self.nll_loss = nn.CrossEntropyLoss(weight, ignore_index=ignore_label, reduction=reduction)\n\n    def forward(self, output, target):\n        """"""\n        Forward pass\n        :param output: torch.tensor (NxC)\n        :param target: torch.tensor (N)\n        :return: scalar\n        """"""\n        return self.nll_loss(output, target)\n\n\n# class CrossEntropyLoss2d(nn.Module):\n#     \'\'\'\n#     This file defines a cross entropy loss for 2D images\n#     \'\'\'\n#\n#     def __init__(self, weight=None, ignore_label=255):\n#         \'\'\'\n#         :param weight: 1D weight vector to deal with the class-imbalance\n#         Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network.\n#         You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.\n#         \'\'\'\n#         super().__init__()\n#\n#         # self.loss = nn.NLLLoss2d(weight, ignore_index=255)\n#         self.loss = nn.NLLLoss(weight, ignore_index=ignore_label)\n#\n#     def forward(self, outputs, targets):\n#         return self.loss(F.log_softmax(outputs, dim=1), targets)\n\n\n\nclass CrossEntropyLoss2dLabelSmooth(_WeightedLoss):\n    """"""\n    Refer from https://arxiv.org/pdf/1512.00567.pdf\n    :param target: N,\n    :param n_classes: int\n    :param eta: float\n    :return:\n        N x C onehot smoothed vector\n    """"""\n\n    def __init__(self, weight=None, ignore_label=255, epsilon=0.1, reduction=\'mean\'):\n        super(CrossEntropyLoss2dLabelSmooth, self).__init__()\n        self.epsilon = epsilon\n        self.nll_loss = nn.CrossEntropyLoss(weight, ignore_index=ignore_label, reduction=reduction)\n\n    def forward(self, output, target):\n        """"""\n        Forward pass\n        :param output: torch.tensor (NxC)\n        :param target: torch.tensor (N)\n        :return: scalar\n        """"""\n        n_classes = output.size(1)\n        # batchsize, num_class = input.size()\n        # log_probs = F.log_softmax(inputs, dim=1)\n        targets = torch.zeros_like(output).scatter_(1, target.unsqueeze(1), 1)\n        targets = (1 - self.epsilon) * targets + self.epsilon / n_classes\n\n        return self.nll_loss(output, targets)\n\n\n""""""\nhttps://arxiv.org/abs/1708.02002\n# Credit to https://github.com/clcarwin/focal_loss_pytorch\n""""""\nclass FocalLoss2d(nn.Module):\n    def __init__(self, alpha=0.5, gamma=2, weight=None, ignore_index=255, size_average=True):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.weight = weight\n        self.ignore_index = ignore_index\n        self.size_average = size_average\n        self.ce_fn = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index)\n\n    def forward(self, output, target):\n\n        if output.dim()>2:\n            output = output.contiguous().view(output.size(0), output.size(1), -1)\n            output = output.transpose(1,2)\n            output = output.contiguous().view(-1, output.size(2)).squeeze()\n        if target.dim()==4:\n            target = target.contiguous().view(target.size(0), target.size(1), -1)\n            target = target.transpose(1,2)\n            target = target.contiguous().view(-1, target.size(2)).squeeze()\n        elif target.dim()==3:\n            target = target.view(-1)\n        else:\n            target = target.view(-1, 1)\n\n        logpt = self.ce_fn(output, target)\n        pt = torch.exp(-logpt)\n        loss = ((1-pt) ** self.gamma) * self.alpha * logpt\n        if self.size_average:\n            return loss.mean()\n        else:\n            return loss.sum()\n\n\n""""""\nhttps://arxiv.org/pdf/1906.07413.pdf\n""""""\nclass LDAMLoss(nn.Module):\n\n    def __init__(self, cls_num_list, max_m=0.5, weight=None, s=30):\n        super(LDAMLoss, self).__init__()\n        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n        m_list = m_list * (max_m / np.max(m_list))\n        m_list = torch.cuda.FloatTensor(m_list)\n        self.m_list = m_list\n        assert s > 0\n        self.s = s\n        self.weight = weight\n\n    def forward(self, x, target):\n        index = torch.zeros_like(x, dtype=torch.uint8)\n        index.scatter_(1, target.data.view(-1, 1), 1)\n\n        index_float = index.type(torch.cuda.FloatTensor)\n        batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0, 1))\n        batch_m = batch_m.view((-1, 1))\n        x_m = x - batch_m\n\n        output = torch.where(index, x_m, x)\n        return F.cross_entropy(self.s * output, target, weight=self.weight)\n\n\n\n\n# Adapted from OCNet Repository (https://github.com/PkuRainBow/OCNet)\nclass ProbOhemCrossEntropy2d(nn.Module):\n    def __init__(self, ignore_label=255, reduction=\'mean\', thresh=0.6, min_kept=256,\n                 down_ratio=1, use_weight=False):\n        super(ProbOhemCrossEntropy2d, self).__init__()\n        self.ignore_label = ignore_label\n        self.thresh = float(thresh)\n        self.min_kept = int(min_kept)\n        self.down_ratio = down_ratio\n        if use_weight:\n            print(""w/ class balance"")\n            weight = torch.FloatTensor(\n                [0.8373, 0.918, 0.866, 1.0345, 1.0166, 0.9969, 0.9754, 1.0489,\n                 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037, 1.0865, 1.0955,\n                 1.0865, 1.1529, 1.0507])\n            self.criterion = nn.CrossEntropyLoss(reduction=reduction,\n                                                       weight=weight,\n                                                       ignore_index=ignore_label)\n        else:\n            print(""w/o class balance"")\n            self.criterion = nn.CrossEntropyLoss(reduction=reduction,\n                                                       ignore_index=ignore_label)\n\n    def forward(self, pred, target):\n        b, c, h, w = pred.size()\n        target = target.view(-1)\n        valid_mask = target.ne(self.ignore_label)\n        target = target * valid_mask.long()\n        num_valid = valid_mask.sum()\n\n        prob = F.softmax(pred, dim=1)\n        prob = (prob.transpose(0, 1)).reshape(c, -1)\n\n        if self.min_kept > num_valid:\n            print(\'Labels: {}\'.format(num_valid))\n            pass\n        elif num_valid > 0:\n            prob = prob.masked_fill_(1 - valid_mask, 1)     #\n            mask_prob = prob[\n                target, torch.arange(len(target), dtype=torch.long)]\n            threshold = self.thresh\n            if self.min_kept > 0:\n                index = mask_prob.argsort()\n                threshold_index = index[min(len(index), self.min_kept) - 1]\n                if mask_prob[threshold_index] > self.thresh:\n                    threshold = mask_prob[threshold_index]\n                kept_mask = mask_prob.le(threshold)\n                target = target * kept_mask.long()\n                valid_mask = valid_mask * kept_mask\n                print(\'Valid Mask: {}\'.format(valid_mask.sum()))\n\n        target = target.masked_fill_(1 - valid_mask, self.ignore_label)\n        target = target.view(b, h, w)\n\n        return self.criterion(pred, target)\n\n\n# ==========================================================================================================================\n# ==========================================================================================================================\n# class-balanced loss\nclass CrossEntropy2d(nn.Module):\n\n    def __init__(self, size_average=True, ignore_label=255, use_weight=True):\n        super(CrossEntropy2d, self).__init__()\n        self.size_average = size_average\n        self.ignore_label = ignore_label\n        self.use_weight   = use_weight\n        # if self.use_weight:\n        #     self.weight = torch.FloatTensor(\n        #         [0.8373, 0.918, 0.866, 1.0345, 1.0166, 0.9969, 0.9754, 1.0489,\n        #          0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037, 1.0865, 1.0955,\n        #          1.0865, 1.1529, 1.0507])\n        #     print(\'CrossEntropy2d weights : {}\'.format(self.weight))\n        # else:\n        #     self.weight = None\n\n\n    def forward(self, predict, target, weight=None):\n\n        """"""\n            Args:\n                predict:(n, c, h, w)\n                target:(n, h, w)\n                weight (Tensor, optional): a manual rescaling weight given to each class.\n                                           If given, has to be a Tensor of size ""nclasses""\n        """"""\n        # Variable(torch.randn(2,10)\n        if self.use_weight:\n            print(\'target size {}\'.format(target.shape))\n            freq = np.zeros(19)\n            for k in range(19):\n                mask = (target[:, :, :] == k)\n                freq[k] = torch.sum(mask)\n                print(\'{}th frequency {}\'.format(k, freq[k]))\n            weight = freq / np.sum(freq)\n            print(weight)\n            self.weight = torch.FloatTensor(weight)\n            print(\'Online class weight: {}\'.format(self.weight))\n        else:\n            self.weight = None\n\n\n        criterion = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_label)\n        # torch.FloatTensor([2.87, 13.19, 5.11, 37.98, 35.14, 30.9, 26.23, 40.24, 6.66, 32.07, 21.08, 28.14, 46.01, 10.35, 44.25, 44.9, 44.25, 47.87, 40.39])\n        #weight = Variable(torch.FloatTensor([1, 1.49, 1.28, 1.62, 1.62, 1.62, 1.64, 1.62, 1.49, 1.62, 1.43, 1.62, 1.64, 1.43, 1.64, 1.64, 1.64, 1.64, 1.62]), requires_grad=False).cuda()\n        assert not target.requires_grad\n        assert predict.dim() == 4\n        assert target.dim() == 3\n        assert predict.size(0) == target.size(0), ""{0} vs {1} "".format(predict.size(0), target.size(0))\n        assert predict.size(2) == target.size(1), ""{0} vs {1} "".format(predict.size(2), target.size(1))\n        assert predict.size(3) == target.size(2), ""{0} vs {1} "".format(predict.size(3), target.size(3))\n        n, c, h, w = predict.size()\n        target_mask = (target >= 0) * (target != self.ignore_label)\n        target = target[target_mask]\n        if not target.data.dim():\n            return torch.zeros(1)\n        predict = predict.transpose(1, 2).transpose(2, 3).contiguous()\n        predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)\n        loss = criterion(predict, target)\n        return loss\n# ==========================================================================================================================\n# ==========================================================================================================================\n\n\n\n\nclass LovaszSoftmax(nn.Module):\n    def __init__(self, classes=\'present\', per_image=False, ignore_index=255):\n        super(LovaszSoftmax, self).__init__()\n        self.smooth = classes\n        self.per_image = per_image\n        self.ignore_index = ignore_index\n\n    def forward(self, output, target):\n        logits = F.softmax(output, dim=1)\n        loss = lovasz_softmax(logits, target, ignore=self.ignore_index)\n        return loss\n'"
utils/losses/lovasz_losses.py,7,"b'""""""\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\nhttps://github.com/bermanmaxim/LovaszSoftmax/blob/master/pytorch/lovasz_losses.py\n""""""\n\nfrom __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\ntry:\n    from itertools import  ifilterfalse\nexcept ImportError: # py3k\n    from itertools import  filterfalse as ifilterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    """"""\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / float(union)\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    """"""\n    Array of IoU for each (non ignored) class\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []    \n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / float(union))\n        ious.append(iou)\n    ious = [mean(iou) for iou in zip(*ious)] # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n         super(StableBCELoss, self).__init__()\n    def forward(self, input, target):\n         neg_abs = - input.abs()\n         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n         return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    """"""\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    """"""\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, classes=\'present\', per_image=False, ignore=None):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    """"""\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, classes=\'present\'):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n    """"""\n    if probas.numel() == 0:\n        # only void pixels, the gradients should be 0\n        return probas * 0.\n    C = probas.size(1)\n    losses = []\n    class_to_sum = list(range(C)) if classes in [\'all\', \'present\'] else classes\n    for c in class_to_sum:\n        fg = (labels == c).float() # foreground for class c\n        if (classes is \'present\' and fg.sum() == 0):\n            continue\n        if C == 1:\n            if len(classes) > 1:\n                raise ValueError(\'Sigmoid output possible only with 1 class\')\n            class_pred = probas[:, 0]\n        else:\n            class_pred = probas[:, c]\n        errors = (Variable(fg) - class_pred).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    if probas.dim() == 3:\n        # assumes output of a sigmoid layer\n        B, H, W = probas.size()\n        probas = probas.view(B, 1, H, W)\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\ndef xloss(logits, labels, ignore=None):\n    """"""\n    Cross entropy loss\n    """"""\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\ndef isnan(x):\n    return x != x\n    \n    \ndef mean(l, ignore_nan=False, empty=0):\n    """"""\n    nanmean compatible with generators.\n    """"""\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == \'raise\':\n            raise ValueError(\'Empty mean\')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n'"
utils/metric/__init__.py,0,b'from .metric import *\n'
utils/metric/metric.py,0,"b'import os, sys\nimport cv2\nimport numpy as np\n\nfrom multiprocessing import Pool\n# import copy_reg\nimport copyreg\nimport types\n\n\ndef _pickle_method(m):\n    if m.im_self is None:\n        return getattr, (m.im_class, m.im_func.func_name)\n    else:\n        return getattr, (m.im_self, m.im_func.func_name)\n\n\ncopyreg.pickle(types.MethodType, _pickle_method)\n\n\nclass ConfusionMatrix(object):\n\n    def __init__(self, nclass, classes=None, ignore_label=255):\n        self.nclass = nclass\n        self.classes = classes\n        self.M = np.zeros((nclass, nclass))\n        self.ignore_label = ignore_label\n\n    def add(self, gt, pred):\n        assert (np.max(pred) <= self.nclass)\n        assert (len(gt) == len(pred))\n        for i in range(len(gt)):\n            if not gt[i] == self.ignore_label:\n                self.M[gt[i], pred[i]] += 1.0\n\n    def addM(self, matrix):\n        assert (matrix.shape == self.M.shape)\n        self.M += matrix\n\n    def __str__(self):\n        pass\n\n    # Pii\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8cPij\xe5\x92\x8cPji\xe5\x88\x86\xe5\x88\xab\xe8\xa2\xab\xe8\xa7\xa3\xe9\x87\x8a\xe4\xb8\xba\xe5\x81\x87\xe6\xad\xa3\xe5\x92\x8c\xe5\x81\x87\xe8\xb4\x9f\xef\xbc\x8c\xe5\xb0\xbd\xe7\xae\xa1\xe4\xb8\xa4\xe8\x80\x85\xe9\x83\xbd\xe6\x98\xaf\xe5\x81\x87\xe6\xad\xa3\xe4\xb8\x8e\xe5\x81\x87\xe8\xb4\x9f\xe4\xb9\x8b\xe5\x92\x8c\n    def recall(self):  # \xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\xba\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe4\xb8\xad\xe7\xa1\xae\xe8\xae\xa4\xe4\xb8\xba\xe6\xad\xa3\xe7\xa1\xae\xe5\x83\x8f\xe7\xb4\xa0\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n        recall = 0.0\n        for i in range(self.nclass):\n            recall += self.M[i, i] / np.sum(self.M[:, i])\n\n        return recall / self.nclass\n\n    def accuracy(self):  # \xe5\x88\x86\xe5\x89\xb2\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe9\x99\xa4\xe4\xbb\xa5\xe6\x80\xbb\xe5\x83\x8f\xe7\xb4\xa0\n        accuracy = 0.0\n        for i in range(self.nclass):\n            accuracy += self.M[i, i] / np.sum(self.M[i, :])\n\n        return accuracy / self.nclass\n\n    # \xe9\x9b\x85\xe5\x8d\xa1\xe5\xb0\x94\xe6\x8c\x87\xe6\x95\xb0\xef\xbc\x8c\xe5\x8f\x88\xe7\xa7\xb0\xe4\xb8\xba\xe4\xba\xa4\xe5\xb9\xb6\xe6\xaf\x94\xef\xbc\x88IOU\xef\xbc\x89\n    def jaccard(self):\n        jaccard = 0.0\n        jaccard_perclass = []\n        for i in range(self.nclass):\n            if not self.M[i, i] == 0:\n                jaccard_perclass.append(self.M[i, i] / (np.sum(self.M[i, :]) + np.sum(self.M[:, i]) - self.M[i, i]))\n\n        return np.sum(jaccard_perclass) / len(jaccard_perclass), jaccard_perclass, self.M\n\n    def generateM(self, item):\n        gt, pred = item\n        m = np.zeros((self.nclass, self.nclass))\n        assert (len(gt) == len(pred))\n        for i in range(len(gt)):\n            if gt[i] < self.nclass:  # and pred[i] < self.nclass:\n                m[gt[i], pred[i]] += 1.0\n        return m\n\n\ndef get_iou(data_list, class_num, save_path=None):\n    """""" \n    Args:\n      data_list: a list, its elements [gt, output]\n      class_num: the number of label\n    """"""\n    from multiprocessing import Pool\n\n    ConfM = ConfusionMatrix(class_num)\n    f = ConfM.generateM\n    pool = Pool()\n    m_list = pool.map(f, data_list)\n    pool.close()\n    pool.join()\n\n    for m in m_list:\n        ConfM.addM(m)\n\n    aveJ, j_list, M = ConfM.jaccard()\n    # print(j_list)\n    # print(M)\n    # print(\'meanIOU: \' + str(aveJ) + \'\\n\')\n\n    if save_path:\n        with open(save_path, \'w\') as f:\n            f.write(\'meanIOU: \' + str(aveJ) + \'\\n\')\n            f.write(str(j_list) + \'\\n\')\n            f.write(str(M) + \'\\n\')\n    return aveJ, j_list\n'"
utils/optim/AdamW.py,6,"b'import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\nclass AdamW(Optimizer):\n    """"""Implements Adam algorithm.\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        super(AdamW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsgrad\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsgrad = group[\'amsgrad\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsgrad:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # if group[\'weight_decay\'] != 0:\n                #     grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                # p.data.addcdiv_(-step_size, exp_avg, denom)\n                p.data.add_(-step_size,  torch.mul(p.data, group[\'weight_decay\']).addcdiv_(1, exp_avg, denom) )\n\n        return loss'"
utils/optim/Lookahead.py,3,"b'from collections import defaultdict\nfrom itertools import chain\nfrom torch.optim import Optimizer\nimport torch\nimport warnings\n\nclass Lookahead(Optimizer):\n    def __init__(self, optimizer, k=5, alpha=0.5):\n        self.optimizer = optimizer\n        self.k = k\n        self.alpha = alpha\n        self.param_groups = self.optimizer.param_groups\n        self.state = defaultdict(dict)\n        self.fast_state = self.optimizer.state\n        for group in self.param_groups:\n            group[""counter""] = 0\n    \n    def update(self, group):\n        for fast in group[""params""]:\n            param_state = self.state[fast]\n            if ""slow_param"" not in param_state:\n                param_state[""slow_param""] = torch.zeros_like(fast.data)\n                param_state[""slow_param""].copy_(fast.data)\n            slow = param_state[""slow_param""]\n            slow += (fast.data - slow) * self.alpha\n            fast.data.copy_(slow)\n    \n    def update_lookahead(self):\n        for group in self.param_groups:\n            self.update(group)\n\n    def step(self, closure=None):\n        loss = self.optimizer.step(closure)\n        for group in self.param_groups:\n            if group[""counter""] == 0:\n                self.update(group)\n            group[""counter""] += 1\n            if group[""counter""] >= self.k:\n                group[""counter""] = 0\n        return loss\n\n    def state_dict(self):\n        fast_state_dict = self.optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict[""state""]\n        param_groups = fast_state_dict[""param_groups""]\n        return {\n            ""fast_state"": fast_state,\n            ""slow_state"": slow_state,\n            ""param_groups"": param_groups,\n        }\n\n    def load_state_dict(self, state_dict):\n        slow_state_dict = {\n            ""state"": state_dict[""slow_state""],\n            ""param_groups"": state_dict[""param_groups""],\n        }\n        fast_state_dict = {\n            ""state"": state_dict[""fast_state""],\n            ""param_groups"": state_dict[""param_groups""],\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.optimizer.load_state_dict(fast_state_dict)\n        self.fast_state = self.optimizer.state\n\n    def add_param_group(self, param_group):\n        param_group[""counter""] = 0\n        self.optimizer.add_param_group(param_group)'"
utils/optim/RAdam.py,3,"b""import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:                    \n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss"""
utils/optim/Ranger.py,5,"b'import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\nimport itertools as it\n\n\n\nclass Ranger(Optimizer):\n\n    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5, betas=(.95,0.999), eps=1e-5, weight_decay=0):\n        #parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\'Invalid slow update rate: {alpha}\')\n        if not 1 <= k:\n            raise ValueError(f\'Invalid lookahead steps: {k}\')\n        if not lr > 0:\n            raise ValueError(f\'Invalid Learning Rate: {lr}\')\n        if not eps > 0:\n            raise ValueError(f\'Invalid eps: {eps}\')\n\n        #parameter comments:\n        # beta1 (momentum) of .95 seems to work better than .90...\n        #N_sma_threshold of 5 seems better in testing than 4.\n        #In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n\n        #prep defaults and init torch.optim base\n        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas, N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n        super().__init__(params,defaults)\n\n        #adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        #now we can get to work...\n        #removed as we now use step from RAdam...no need for duplicate step counting\n        #for group in self.param_groups:\n        #    group[""step_counter""] = 0\n            #print(""group step counter init"")\n\n        #look ahead params\n        self.alpha = alpha\n        self.k = k \n\n        #radam buffer for state\n        self.radam_buffer = [[None,None,None] for ind in range(10)]\n\n        #self.first_run_check=0\n\n        #lookahead weights\n        #9/2/19 - lookahead param tensors have been moved to state storage.  \n        #This should resolve issues with load/save where weights were left in GPU memory from first load, slowing down future runs.\n\n        #self.slow_weights = [[p.clone().detach() for p in group[\'params\']]\n        #                     for group in self.param_groups]\n\n        #don\'t use grad for lookahead weights\n        #for w in it.chain(*self.slow_weights):\n        #    w.requires_grad = False\n\n    def __setstate__(self, state):\n        print(""set state called"")\n        super(Ranger, self).__setstate__(state)\n\n\n    def step(self, closure=None):\n        loss = None\n        #note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.  \n        #Uncomment if you need to use the actual closure...\n\n        #if closure is not None:\n            #loss = closure()\n\n        #Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'Ranger optimizer does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]  #get state dict for this param\n\n                if len(state) == 0:   #if first time to run...init dictionary with our desired entries\n                    #if self.first_run_check==0:\n                        #self.first_run_check=1\n                        #print(""Initializing slow buffer...should not see this at load from saved model!"")\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n\n                    #look ahead weight storage now in state dict \n                    state[\'slow_buffer\'] = torch.empty_like(p.data)\n                    state[\'slow_buffer\'].copy_(p.data)\n\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                #begin computations \n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                #compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                #compute mean moving avg\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\'step\'] += 1\n\n\n                buffered = self.radam_buffer[int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state[\'step\'])\n                    buffered[2] = step_size\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(-step_size * group[\'lr\'], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group[\'lr\'], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n                #integrated look ahead...\n                #we do it at the param level instead of group level\n                if state[\'step\'] % group[\'k\'] == 0:\n                    slow_p = state[\'slow_buffer\'] #get access to slow param tensor\n                    slow_p.add_(self.alpha, p.data - slow_p)  #(fast weights - slow weights) * alpha\n                    p.data.copy_(slow_p)  #copy interpolated weights to RAdam param tensor\n\n        return loss'"
utils/optim/__init__.py,0,b'from .RAdam import *\nfrom .AdamW import *\nfrom .Lookahead import *\nfrom .Ranger import *'
utils/scheduler/__init__.py,0,b'from .lr_scheduler import *\n'
utils/scheduler/lr_scheduler.py,1,"b'import math\nfrom torch.optim.lr_scheduler import MultiStepLR, _LRScheduler\n\n\nclass WarmupMultiStepLR(MultiStepLR):\n    def __init__(self, optimizer, milestones, gamma=0.1, warmup_factor=1.0 / 3,\n                 warmup_iters=500, last_epoch=-1):\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        super().__init__(optimizer, milestones, gamma, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch <= self.warmup_iters:\n            alpha = self.last_epoch / self.warmup_iters\n            warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n            # print(self.base_lrs[0]*warmup_factor)\n            return [lr * warmup_factor for lr in self.base_lrs]\n        else:\n            lr = super().get_lr()\n        return lr\n\n\nclass WarmupCosineLR(_LRScheduler):\n    def __init__(self, optimizer, T_max, warmup_factor=1.0 / 3, warmup_iters=500,\n                 eta_min=0, last_epoch=-1):\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.T_max, self.eta_min = T_max, eta_min\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch <= self.warmup_iters:\n            alpha = self.last_epoch / self.warmup_iters\n            warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n            # print(self.base_lrs[0]*warmup_factor)\n            return [lr * warmup_factor for lr in self.base_lrs]\n        else:\n            return [self.eta_min + (base_lr - self.eta_min) *\n                    (1 + math.cos(\n                        math.pi * (self.last_epoch - self.warmup_iters) / (self.T_max - self.warmup_iters))) / 2\n                    for base_lr in self.base_lrs]\n\n\n\nclass WarmupPolyLR(_LRScheduler):\n    def __init__(self, optimizer, T_max, cur_iter, warmup_factor=1.0 / 3, warmup_iters=500,\n                 eta_min=0, power=0.9):\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.power = power\n        self.T_max, self.eta_min = T_max, eta_min\n        self.cur_iter = cur_iter\n        super().__init__(optimizer)\n\n    def get_lr(self):\n        if self.cur_iter <= self.warmup_iters:\n            alpha = self.cur_iter / self.warmup_iters\n            warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n            # print(self.base_lrs[0]*warmup_factor)\n            return [lr * warmup_factor for lr in self.base_lrs]\n        else:\n            return [self.eta_min + (base_lr - self.eta_min) *\n                    math.pow(1 - (self.cur_iter - self.warmup_iters) / (self.T_max - self.warmup_iters),\n                             self.power) for base_lr in self.base_lrs]\n\n\ndef poly_learning_rate(cur_epoch, max_epoch, curEpoch_iter, perEpoch_iter, baselr):\n    cur_iter = cur_epoch * perEpoch_iter + curEpoch_iter\n    max_iter = max_epoch * perEpoch_iter\n    lr = baselr * pow((1 - 1.0 * cur_iter / max_iter), 0.9)\n\n    return lr\n\n\n\nclass GradualWarmupScheduler(_LRScheduler):\n    """""" Gradually warm-up(increasing) learning rate in optimizer.\n    Proposed in \'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\'.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        min_lr_mul: target learning rate = base lr * min_lr_mul\n        total_epoch: target learning rate is reached at total_epoch, gradually\n        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n    """"""\n\n    def __init__(self, optimizer, total_epoch, min_lr_mul=0.1, after_scheduler=None):\n        self.min_lr_mul = min_lr_mul\n        if self.min_lr_mul > 1. or self.min_lr_mul < 0.:\n            raise ValueError(\'min_lr_mul should be [0., 1.]\')\n        self.total_epoch = total_epoch\n        self.after_scheduler = after_scheduler\n        self.finished = False\n        super(GradualWarmupScheduler, self).__init__(optimizer)\n\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = self.base_lrs\n                    self.finished = True\n                return self.after_scheduler.get_lr()\n            else:\n                return self.base_lrs\n        else:\n            return [base_lr * (self.min_lr_mul + (1. - self.min_lr_mul) * (self.last_epoch / float(self.total_epoch))) for base_lr in self.base_lrs]\n\n    def step(self, epoch=None):\n        if self.finished and self.after_scheduler:\n            return self.after_scheduler.step(epoch - self.total_epoch)\n        else:\n            return super(GradualWarmupScheduler, self).step(epoch)\n\n\n\n\nif __name__ == \'__main__\':\n    optim = WarmupPolyLR()\n'"
tools/flops_counter/ptflops/__init__.py,0,b'from .flops_counter import get_model_complexity_info\n'
tools/flops_counter/ptflops/flops_counter.py,21,"b'import sys\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef get_model_complexity_info(model, input_res,\n                              print_per_layer_stat=True,\n                              as_strings=True,\n                              input_constructor=None, ost=sys.stdout):\n    assert type(input_res) is tuple\n    assert len(input_res) >= 2\n    flops_model = add_flops_counting_methods(model)\n    flops_model.eval().start_flops_count()\n    if input_constructor:\n        input = input_constructor(input_res)\n        _ = flops_model(**input)\n    else:\n        batch = torch.ones(()).new_empty((1, *input_res),\n                                         dtype=next(flops_model.parameters()).dtype,\n                                         device=next(flops_model.parameters()).device)\n        _ = flops_model(batch)\n\n    if print_per_layer_stat:\n        print_model_with_flops(flops_model, ost=ost)\n    flops_count = flops_model.compute_average_flops_cost()\n    params_count = get_model_parameters_number(flops_model)\n    flops_model.stop_flops_count()\n\n    if as_strings:\n        return flops_to_string(flops_count), params_to_string(params_count)\n\n    return flops_count, params_count\n\n\ndef flops_to_string(flops, units=\'GMac\', precision=2):\n    if units is None:\n        if flops // 10**9 > 0:\n            return str(round(flops / 10.**9, precision)) + \' GMac\'\n        elif flops // 10**6 > 0:\n            return str(round(flops / 10.**6, precision)) + \' MMac\'\n        elif flops // 10**3 > 0:\n            return str(round(flops / 10.**3, precision)) + \' KMac\'\n        else:\n            return str(flops) + \' Mac\'\n    else:\n        if units == \'GMac\':\n            return str(round(flops / 10.**9, precision)) + \' \' + units\n        elif units == \'MMac\':\n            return str(round(flops / 10.**6, precision)) + \' \' + units\n        elif units == \'KMac\':\n            return str(round(flops / 10.**3, precision)) + \' \' + units\n        else:\n            return str(flops) + \' Mac\'\n\n\ndef params_to_string(params_num):\n    if params_num // 10 ** 6 > 0:\n        return str(round(params_num / 10 ** 6, 2)) + \' M\'\n    elif params_num // 10 ** 3:\n        return str(round(params_num / 10 ** 3, 2)) + \' k\'\n    else:\n        return str(params_num)\n\n\ndef print_model_with_flops(model, units=\'GMac\', precision=3, ost=sys.stdout):\n    total_flops = model.compute_average_flops_cost()\n\n    def accumulate_flops(self):\n        if is_supported_instance(self):\n            return self.__flops__ / model.__batch_counter__\n        else:\n            sum = 0\n            for m in self.children():\n                sum += m.accumulate_flops()\n            return sum\n\n    def flops_repr(self):\n        accumulated_flops_cost = self.accumulate_flops()\n        return \', \'.join([flops_to_string(accumulated_flops_cost, units=units, precision=precision),\n                          \'{:.3%} MACs\'.format(accumulated_flops_cost / total_flops),\n                          self.original_extra_repr()])\n\n    def add_extra_repr(m):\n        m.accumulate_flops = accumulate_flops.__get__(m)\n        flops_extra_repr = flops_repr.__get__(m)\n        if m.extra_repr != flops_extra_repr:\n            m.original_extra_repr = m.extra_repr\n            m.extra_repr = flops_extra_repr\n            assert m.extra_repr != m.original_extra_repr\n\n    def del_extra_repr(m):\n        if hasattr(m, \'original_extra_repr\'):\n            m.extra_repr = m.original_extra_repr\n            del m.original_extra_repr\n        if hasattr(m, \'accumulate_flops\'):\n            del m.accumulate_flops\n\n    model.apply(add_extra_repr)\n    print(model, file=ost)\n    model.apply(del_extra_repr)\n\n\ndef get_model_parameters_number(model):\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return params_num\n\n\ndef add_flops_counting_methods(net_main_module):\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n\n    net_main_module.reset_flops_count()\n\n    # Adding variables necessary for masked flops computation\n    net_main_module.apply(add_flops_mask_variable_or_reset)\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Returns current mean flops consumption per image.\n\n    """"""\n\n    batches_count = self.__batch_counter__\n    flops_sum = 0\n    for module in self.modules():\n        if is_supported_instance(module):\n            flops_sum += module.__flops__\n\n    return flops_sum / batches_count\n\n\ndef start_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n\n    """"""\n    add_batch_counter_hook_function(self)\n    self.apply(add_flops_counter_hook_function)\n\n\ndef stop_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n\n    """"""\n    remove_batch_counter_hook_function(self)\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Resets statistics computed so far.\n\n    """"""\n    add_batch_counter_variables_or_reset(self)\n    self.apply(add_flops_counter_variable_or_reset)\n\n\ndef add_flops_mask(module, mask):\n    def add_flops_mask_func(module):\n        if isinstance(module, torch.nn.Conv2d):\n            module.__mask__ = mask\n    module.apply(add_flops_mask_func)\n\n\ndef remove_flops_mask(module):\n    module.apply(add_flops_mask_variable_or_reset)\n\n\n# ---- Internal functions\ndef is_supported_instance(module):\n    if isinstance(module, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d,\n                           torch.nn.ReLU, torch.nn.PReLU, torch.nn.ELU, \\\n                           torch.nn.LeakyReLU, torch.nn.ReLU6, torch.nn.Linear, \\\n                           torch.nn.MaxPool2d, torch.nn.AvgPool2d, torch.nn.BatchNorm2d, \\\n                           torch.nn.Upsample, nn.AdaptiveMaxPool2d, nn.AdaptiveAvgPool2d, \\\n                           torch.nn.MaxPool1d, torch.nn.AvgPool1d, torch.nn.BatchNorm1d, \\\n                           nn.AdaptiveMaxPool1d, nn.AdaptiveAvgPool1d, \\\n                           nn.ConvTranspose2d, torch.nn.BatchNorm3d,\n                           torch.nn.MaxPool3d, torch.nn.AvgPool3d, nn.AdaptiveMaxPool3d, nn.AdaptiveAvgPool3d)):\n        return True\n\n    return False\n\n\ndef empty_flops_counter_hook(module, input, output):\n    module.__flops__ += 0\n\n\ndef upsample_flops_counter_hook(module, input, output):\n    output_size = output[0]\n    batch_size = output_size.shape[0]\n    output_elements_count = batch_size\n    for val in output_size.shape[1:]:\n        output_elements_count *= val\n    module.__flops__ += int(output_elements_count)\n\n\ndef relu_flops_counter_hook(module, input, output):\n    active_elements_count = output.numel()\n    module.__flops__ += int(active_elements_count)\n\n\ndef linear_flops_counter_hook(module, input, output):\n    input = input[0]\n    batch_size = input.shape[0]\n    module.__flops__ += int(batch_size * input.shape[1] * output.shape[1])\n\n\ndef pool_flops_counter_hook(module, input, output):\n    input = input[0]\n    module.__flops__ += int(np.prod(input.shape))\n\ndef bn_flops_counter_hook(module, input, output):\n    module.affine\n    input = input[0]\n\n    batch_flops = np.prod(input.shape)\n    if module.affine:\n        batch_flops *= 2\n    module.__flops__ += int(batch_flops)\n\ndef deconv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    input_height, input_width = input.shape[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = kernel_height * kernel_width * in_channels * filters_per_channel\n\n    active_elements_count = batch_size * input_height * input_width\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n    bias_flops = 0\n    if conv_module.bias is not None:\n        output_height, output_width = output.shape[2:]\n        bias_flops = out_channels * batch_size * output_height * output_height\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += int(overall_flops)\n\n\ndef conv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    output_dims = list(output.shape[2:])\n\n    kernel_dims = list(conv_module.kernel_size)\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = np.prod(kernel_dims) * in_channels * filters_per_channel\n\n    active_elements_count = batch_size * np.prod(output_dims)\n\n    if conv_module.__mask__ is not None:\n        # (b, 1, h, w)\n        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n        active_elements_count = flops_mask.sum()\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += int(overall_flops)\n\n\ndef batch_counter_hook(module, input, output):\n    batch_size = 1\n    if len(input) > 0:\n        # Can have multiple inputs, getting the first one\n        input = input[0]\n        batch_size = len(input)\n    else:\n        pass\n        print(\'Warning! No positional inputs found for a module, assuming batch size is 1.\')\n    module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_variables_or_reset(module):\n\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        module.__batch_counter_handle__.remove()\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            return\n\n        if isinstance(module, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d)):\n            handle = module.register_forward_hook(conv_flops_counter_hook)\n        elif isinstance(module, (torch.nn.ReLU, torch.nn.PReLU, torch.nn.ELU, \\\n                                 torch.nn.LeakyReLU, torch.nn.ReLU6)):\n            handle = module.register_forward_hook(relu_flops_counter_hook)\n        elif isinstance(module, torch.nn.Linear):\n            handle = module.register_forward_hook(linear_flops_counter_hook)\n        elif isinstance(module, (torch.nn.AvgPool2d, torch.nn.MaxPool2d, nn.AdaptiveMaxPool2d, \\\n                                 nn.AdaptiveAvgPool2d, torch.nn.MaxPool3d, torch.nn.AvgPool3d, \\\n                                 torch.nn.AvgPool1d, torch.nn.MaxPool1d, nn.AdaptiveMaxPool1d, \\\n                                 nn.AdaptiveAvgPool1d, nn.AdaptiveMaxPool3d, nn.AdaptiveAvgPool3d)):\n            handle = module.register_forward_hook(pool_flops_counter_hook)\n        elif isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):\n            handle = module.register_forward_hook(bn_flops_counter_hook)\n        elif isinstance(module, torch.nn.Upsample):\n            handle = module.register_forward_hook(upsample_flops_counter_hook)\n        elif isinstance(module, torch.nn.ConvTranspose2d):\n            handle = module.register_forward_hook(deconv_flops_counter_hook)\n        else:\n            handle = module.register_forward_hook(empty_flops_counter_hook)\n        module.__flops_handle__ = handle\n\n\ndef remove_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            module.__flops_handle__.remove()\n            del module.__flops_handle__\n# --- Masked flops counting\n\n\n# Also being run in the initialization\ndef add_flops_mask_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__mask__ = None\n'"
