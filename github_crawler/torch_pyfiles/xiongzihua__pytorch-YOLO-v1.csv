file_path,api_count,code
dataset.py,11,"b""#encoding:utf-8\n#\n#created by xiongzihua\n#\n'''\ntxt\xe6\x8f\x8f\xe8\xbf\xb0\xe6\x96\x87\xe4\xbb\xb6 image_name.jpg x y w h c x y w h c \xe8\xbf\x99\xe6\xa0\xb7\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9b\xae\xe6\xa0\x87\n'''\nimport os\nimport sys\nimport os.path\n\nimport random\nimport numpy as np\n\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nimport cv2\nimport matplotlib.pyplot as plt\n\nclass yoloDataset(data.Dataset):\n    image_size = 448\n    def __init__(self,root,list_file,train,transform):\n        print('data init')\n        self.root=root\n        self.train = train\n        self.transform=transform\n        self.fnames = []\n        self.boxes = []\n        self.labels = []\n        self.mean = (123,117,104)#RGB\n\n        if isinstance(list_file, list):\n            # Cat multiple list files together.\n            # This is especially useful for voc07/voc12 combination.\n            tmp_file = '/tmp/listfile.txt'\n            os.system('cat %s > %s' % (' '.join(list_file), tmp_file))\n            list_file = tmp_file\n\n        with open(list_file) as f:\n            lines  = f.readlines()\n\n        for line in lines:\n            splited = line.strip().split()\n            self.fnames.append(splited[0])\n            num_boxes = (len(splited) - 1) // 5\n            box=[]\n            label=[]\n            for i in range(num_boxes):\n                x = float(splited[1+5*i])\n                y = float(splited[2+5*i])\n                x2 = float(splited[3+5*i])\n                y2 = float(splited[4+5*i])\n                c = splited[5+5*i]\n                box.append([x,y,x2,y2])\n                label.append(int(c)+1)\n            self.boxes.append(torch.Tensor(box))\n            self.labels.append(torch.LongTensor(label))\n        self.num_samples = len(self.boxes)\n\n    def __getitem__(self,idx):\n        fname = self.fnames[idx]\n        img = cv2.imread(os.path.join(self.root+fname))\n        boxes = self.boxes[idx].clone()\n        labels = self.labels[idx].clone()\n\n        if self.train:\n            #img = self.random_bright(img)\n            img, boxes = self.random_flip(img, boxes)\n            img,boxes = self.randomScale(img,boxes)\n            img = self.randomBlur(img)\n            img = self.RandomBrightness(img)\n            img = self.RandomHue(img)\n            img = self.RandomSaturation(img)\n            img,boxes,labels = self.randomShift(img,boxes,labels)\n            img,boxes,labels = self.randomCrop(img,boxes,labels)\n        # #debug\n        # box_show = boxes.numpy().reshape(-1)\n        # print(box_show)\n        # img_show = self.BGR2RGB(img)\n        # pt1=(int(box_show[0]),int(box_show[1])); pt2=(int(box_show[2]),int(box_show[3]))\n        # cv2.rectangle(img_show,pt1=pt1,pt2=pt2,color=(0,255,0),thickness=1)\n        # plt.figure()\n        \n        # # cv2.rectangle(img,pt1=(10,10),pt2=(100,100),color=(0,255,0),thickness=1)\n        # plt.imshow(img_show)\n        # plt.show()\n        # #debug\n        h,w,_ = img.shape\n        boxes /= torch.Tensor([w,h,w,h]).expand_as(boxes)\n        img = self.BGR2RGB(img) #because pytorch pretrained model use RGB\n        img = self.subMean(img,self.mean) #\xe5\x87\x8f\xe5\x8e\xbb\xe5\x9d\x87\xe5\x80\xbc\n        img = cv2.resize(img,(self.image_size,self.image_size))\n        target = self.encoder(boxes,labels)# 7x7x30\n        for t in self.transform:\n            img = t(img)\n\n        return img,target\n    def __len__(self):\n        return self.num_samples\n\n    def encoder(self,boxes,labels):\n        '''\n        boxes (tensor) [[x1,y1,x2,y2],[]]\n        labels (tensor) [...]\n        return 7x7x30\n        '''\n        grid_num = 14\n        target = torch.zeros((grid_num,grid_num,30))\n        cell_size = 1./grid_num\n        wh = boxes[:,2:]-boxes[:,:2]\n        cxcy = (boxes[:,2:]+boxes[:,:2])/2\n        for i in range(cxcy.size()[0]):\n            cxcy_sample = cxcy[i]\n            ij = (cxcy_sample/cell_size).ceil()-1 #\n            target[int(ij[1]),int(ij[0]),4] = 1\n            target[int(ij[1]),int(ij[0]),9] = 1\n            target[int(ij[1]),int(ij[0]),int(labels[i])+9] = 1\n            xy = ij*cell_size #\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x9b\xb8\xe5\xaf\xb9\xe5\x9d\x90\xe6\xa0\x87\n            delta_xy = (cxcy_sample -xy)/cell_size\n            target[int(ij[1]),int(ij[0]),2:4] = wh[i]\n            target[int(ij[1]),int(ij[0]),:2] = delta_xy\n            target[int(ij[1]),int(ij[0]),7:9] = wh[i]\n            target[int(ij[1]),int(ij[0]),5:7] = delta_xy\n        return target\n    def BGR2RGB(self,img):\n        return cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    def BGR2HSV(self,img):\n        return cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n    def HSV2BGR(self,img):\n        return cv2.cvtColor(img,cv2.COLOR_HSV2BGR)\n    \n    def RandomBrightness(self,bgr):\n        if random.random() < 0.5:\n            hsv = self.BGR2HSV(bgr)\n            h,s,v = cv2.split(hsv)\n            adjust = random.choice([0.5,1.5])\n            v = v*adjust\n            v = np.clip(v, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h,s,v))\n            bgr = self.HSV2BGR(hsv)\n        return bgr\n    def RandomSaturation(self,bgr):\n        if random.random() < 0.5:\n            hsv = self.BGR2HSV(bgr)\n            h,s,v = cv2.split(hsv)\n            adjust = random.choice([0.5,1.5])\n            s = s*adjust\n            s = np.clip(s, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h,s,v))\n            bgr = self.HSV2BGR(hsv)\n        return bgr\n    def RandomHue(self,bgr):\n        if random.random() < 0.5:\n            hsv = self.BGR2HSV(bgr)\n            h,s,v = cv2.split(hsv)\n            adjust = random.choice([0.5,1.5])\n            h = h*adjust\n            h = np.clip(h, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h,s,v))\n            bgr = self.HSV2BGR(hsv)\n        return bgr\n\n    def randomBlur(self,bgr):\n        if random.random()<0.5:\n            bgr = cv2.blur(bgr,(5,5))\n        return bgr\n\n    def randomShift(self,bgr,boxes,labels):\n        #\xe5\xb9\xb3\xe7\xa7\xbb\xe5\x8f\x98\xe6\x8d\xa2\n        center = (boxes[:,2:]+boxes[:,:2])/2\n        if random.random() <0.5:\n            height,width,c = bgr.shape\n            after_shfit_image = np.zeros((height,width,c),dtype=bgr.dtype)\n            after_shfit_image[:,:,:] = (104,117,123) #bgr\n            shift_x = random.uniform(-width*0.2,width*0.2)\n            shift_y = random.uniform(-height*0.2,height*0.2)\n            #print(bgr.shape,shift_x,shift_y)\n            #\xe5\x8e\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xb9\xb3\xe7\xa7\xbb\n            if shift_x>=0 and shift_y>=0:\n                after_shfit_image[int(shift_y):,int(shift_x):,:] = bgr[:height-int(shift_y),:width-int(shift_x),:]\n            elif shift_x>=0 and shift_y<0:\n                after_shfit_image[:height+int(shift_y),int(shift_x):,:] = bgr[-int(shift_y):,:width-int(shift_x),:]\n            elif shift_x <0 and shift_y >=0:\n                after_shfit_image[int(shift_y):,:width+int(shift_x),:] = bgr[:height-int(shift_y),-int(shift_x):,:]\n            elif shift_x<0 and shift_y<0:\n                after_shfit_image[:height+int(shift_y),:width+int(shift_x),:] = bgr[-int(shift_y):,-int(shift_x):,:]\n\n            shift_xy = torch.FloatTensor([[int(shift_x),int(shift_y)]]).expand_as(center)\n            center = center + shift_xy\n            mask1 = (center[:,0] >0) & (center[:,0] < width)\n            mask2 = (center[:,1] >0) & (center[:,1] < height)\n            mask = (mask1 & mask2).view(-1,1)\n            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n            if len(boxes_in) == 0:\n                return bgr,boxes,labels\n            box_shift = torch.FloatTensor([[int(shift_x),int(shift_y),int(shift_x),int(shift_y)]]).expand_as(boxes_in)\n            boxes_in = boxes_in+box_shift\n            labels_in = labels[mask.view(-1)]\n            return after_shfit_image,boxes_in,labels_in\n        return bgr,boxes,labels\n\n    def randomScale(self,bgr,boxes):\n        #\xe5\x9b\xba\xe5\xae\x9a\xe4\xbd\x8f\xe9\xab\x98\xe5\xba\xa6\xef\xbc\x8c\xe4\xbb\xa50.8-1.2\xe4\xbc\xb8\xe7\xbc\xa9\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\x81\x9a\xe5\x9b\xbe\xe5\x83\x8f\xe5\xbd\xa2\xe5\x8f\x98\n        if random.random() < 0.5:\n            scale = random.uniform(0.8,1.2)\n            height,width,c = bgr.shape\n            bgr = cv2.resize(bgr,(int(width*scale),height))\n            scale_tensor = torch.FloatTensor([[scale,1,scale,1]]).expand_as(boxes)\n            boxes = boxes * scale_tensor\n            return bgr,boxes\n        return bgr,boxes\n\n    def randomCrop(self,bgr,boxes,labels):\n        if random.random() < 0.5:\n            center = (boxes[:,2:]+boxes[:,:2])/2\n            height,width,c = bgr.shape\n            h = random.uniform(0.6*height,height)\n            w = random.uniform(0.6*width,width)\n            x = random.uniform(0,width-w)\n            y = random.uniform(0,height-h)\n            x,y,h,w = int(x),int(y),int(h),int(w)\n\n            center = center - torch.FloatTensor([[x,y]]).expand_as(center)\n            mask1 = (center[:,0]>0) & (center[:,0]<w)\n            mask2 = (center[:,1]>0) & (center[:,1]<h)\n            mask = (mask1 & mask2).view(-1,1)\n\n            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n            if(len(boxes_in)==0):\n                return bgr,boxes,labels\n            box_shift = torch.FloatTensor([[x,y,x,y]]).expand_as(boxes_in)\n\n            boxes_in = boxes_in - box_shift\n            boxes_in[:,0]=boxes_in[:,0].clamp_(min=0,max=w)\n            boxes_in[:,2]=boxes_in[:,2].clamp_(min=0,max=w)\n            boxes_in[:,1]=boxes_in[:,1].clamp_(min=0,max=h)\n            boxes_in[:,3]=boxes_in[:,3].clamp_(min=0,max=h)\n\n            labels_in = labels[mask.view(-1)]\n            img_croped = bgr[y:y+h,x:x+w,:]\n            return img_croped,boxes_in,labels_in\n        return bgr,boxes,labels\n\n\n\n\n    def subMean(self,bgr,mean):\n        mean = np.array(mean, dtype=np.float32)\n        bgr = bgr - mean\n        return bgr\n\n    def random_flip(self, im, boxes):\n        if random.random() < 0.5:\n            im_lr = np.fliplr(im).copy()\n            h,w,_ = im.shape\n            xmin = w - boxes[:,2]\n            xmax = w - boxes[:,0]\n            boxes[:,0] = xmin\n            boxes[:,2] = xmax\n            return im_lr, boxes\n        return im, boxes\n    def random_bright(self, im, delta=16):\n        alpha = random.random()\n        if alpha > 0.3:\n            im = im * alpha + random.randrange(-delta,delta)\n            im = im.clip(min=0,max=255).astype(np.uint8)\n        return im\n\ndef main():\n    from torch.utils.data import DataLoader\n    import torchvision.transforms as transforms\n    file_root = '/home/xzh/data/VOCdevkit/VOC2012/allimgs/'\n    train_dataset = yoloDataset(root=file_root,list_file='voc12_trainval.txt',train=True,transform = [transforms.ToTensor()] )\n    train_loader = DataLoader(train_dataset,batch_size=1,shuffle=False,num_workers=0)\n    train_iter = iter(train_loader)\n    for i in range(100):\n        img,target = next(train_iter)\n        print(img,target)\n\n\nif __name__ == '__main__':\n    main()\n\n\n"""
eval_voc.py,1,"b'#encoding:utf-8\n#\n#created by xiongzihua\n#\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""1""\nimport numpy as np\nVOC_CLASSES = (    # always index 0\n    \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n    \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n    \'cow\', \'diningtable\', \'dog\', \'horse\',\n    \'motorbike\', \'person\', \'pottedplant\',\n    \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\nColor = [[0, 0, 0],\n                    [128, 0, 0],\n                    [0, 128, 0],\n                    [128, 128, 0],\n                    [0, 0, 128],\n                    [128, 0, 128],\n                    [0, 128, 128],\n                    [128, 128, 128],\n                    [64, 0, 0],\n                    [192, 0, 0],\n                    [64, 128, 0],\n                    [192, 128, 0],\n                    [64, 0, 128],\n                    [192, 0, 128],\n                    [64, 128, 128],\n                    [192, 128, 128],\n                    [0, 64, 0],\n                    [128, 64, 0],\n                    [0, 192, 0],\n                    [128, 192, 0],\n                    [0, 64, 128]]\ndef voc_ap(rec,prec,use_07_metric=False):\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0.,1.1,0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec>=t])\n            ap = ap + p/11.\n\n    else:\n        # correct ap caculation\n        mrec = np.concatenate(([0.],rec,[1.]))\n        mpre = np.concatenate(([0.],prec,[0.]))\n\n        for i in range(mpre.size -1, 0, -1):\n            mpre[i-1] = np.maximum(mpre[i-1],mpre[i])\n\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n\n    return ap\n\ndef voc_eval(preds,target,VOC_CLASSES=VOC_CLASSES,threshold=0.5,use_07_metric=False,):\n    \'\'\'\n    preds {\'cat\':[[image_id,confidence,x1,y1,x2,y2],...],\'dog\':[[],...]}\n    target {(image_id,class):[[],]}\n    \'\'\'\n    aps = []\n    for i,class_ in enumerate(VOC_CLASSES):\n        pred = preds[class_] #[[image_id,confidence,x1,y1,x2,y2],...]\n        if len(pred) == 0: #\xe5\xa6\x82\xe6\x9e\x9c\xe8\xbf\x99\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe4\xb8\x80\xe4\xb8\xaa\xe9\x83\xbd\xe6\xb2\xa1\xe6\x9c\x89\xe6\xa3\x80\xe6\xb5\x8b\xe5\x88\xb0\xe7\x9a\x84\xe5\xbc\x82\xe5\xb8\xb8\xe6\x83\x85\xe5\x86\xb5\n            ap = -1\n            print(\'---class {} ap {}---\'.format(class_,ap))\n            aps += [ap]\n            break\n        #print(pred)\n        image_ids = [x[0] for x in pred]\n        confidence = np.array([float(x[1]) for x in pred])\n        BB = np.array([x[2:] for x in pred])\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        npos = 0.\n        for (key1,key2) in target:\n            if key2 == class_:\n                npos += len(target[(key1,key2)]) #\xe7\xbb\x9f\xe8\xae\xa1\xe8\xbf\x99\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x8c\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe7\xbb\x9f\xe8\xae\xa1\xe6\x89\x8d\xe4\xb8\x8d\xe4\xbc\x9a\xe9\x81\x97\xe6\xbc\x8f\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d,image_id in enumerate(image_ids):\n            bb = BB[d] #\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n            if (image_id,class_) in target:\n                BBGT = target[(image_id,class_)] #[[],]\n                for bbgt in BBGT:\n                    # compute overlaps\n                    # intersection\n                    ixmin = np.maximum(bbgt[0], bb[0])\n                    iymin = np.maximum(bbgt[1], bb[1])\n                    ixmax = np.minimum(bbgt[2], bb[2])\n                    iymax = np.minimum(bbgt[3], bb[3])\n                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n                    ih = np.maximum(iymax - iymin + 1., 0.)\n                    inters = iw * ih\n\n                    union = (bb[2]-bb[0]+1.)*(bb[3]-bb[1]+1.) + (bbgt[2]-bbgt[0]+1.)*(bbgt[3]-bbgt[1]+1.) - inters\n                    if union == 0:\n                        print(bb,bbgt)\n                    \n                    overlaps = inters/union\n                    if overlaps > threshold:\n                        tp[d] = 1\n                        BBGT.remove(bbgt) #\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa1\x86\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe4\xba\x86\xef\xbc\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe5\x86\x8d\xe5\x8c\xb9\xe9\x85\x8d\n                        if len(BBGT) == 0:\n                            del target[(image_id,class_)] #\xe5\x88\xa0\xe9\x99\xa4\xe6\xb2\xa1\xe6\x9c\x89box\xe7\x9a\x84\xe9\x94\xae\xe5\x80\xbc\n                        break\n                fp[d] = 1-tp[d]\n            else:\n                fp[d] = 1\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp/float(npos)\n        prec = tp/np.maximum(tp + fp, np.finfo(np.float64).eps)\n        #print(rec,prec)\n        ap = voc_ap(rec, prec, use_07_metric)\n        print(\'---class {} ap {}---\'.format(class_,ap))\n        aps += [ap]\n    print(\'---map {}---\'.format(np.mean(aps)))\n\ndef test_eval():\n    preds = {\'cat\':[[\'image01\',0.9,20,20,40,40],[\'image01\',0.8,20,20,50,50],[\'image02\',0.8,30,30,50,50]],\'dog\':[[\'image01\',0.78,60,60,90,90]]}\n    target = {(\'image01\',\'cat\'):[[20,20,41,41]],(\'image01\',\'dog\'):[[60,60,91,91]],(\'image02\',\'cat\'):[[30,30,51,51]]}\n    voc_eval(preds,target,VOC_CLASSES=[\'cat\',\'dog\'])\n\nif __name__ == \'__main__\':\n    #test_eval()\n    from predict import *\n    from collections import defaultdict\n    from tqdm import tqdm\n\n    target =  defaultdict(list)\n    preds = defaultdict(list)\n    image_list = [] #image path list\n\n    f = open(\'voc2007test.txt\')\n    lines = f.readlines()\n    file_list = []\n    for line in lines:\n        splited = line.strip().split()\n        file_list.append(splited)\n    f.close()\n    print(\'---prepare target---\')\n    for index,image_file in enumerate(file_list):\n        image_id = image_file[0]\n\n        image_list.append(image_id)\n        num_obj = (len(image_file) - 1) // 5\n        for i in range(num_obj):\n            x1 = int(image_file[1+5*i])\n            y1 = int(image_file[2+5*i])\n            x2 = int(image_file[3+5*i])\n            y2 = int(image_file[4+5*i])\n            c = int(image_file[5+5*i])\n            class_name = VOC_CLASSES[c]\n            target[(image_id,class_name)].append([x1,y1,x2,y2])\n    #\n    #start test\n    #\n    print(\'---start test---\')\n    # model = vgg16_bn(pretrained=False)\n    model = resnet50()\n    # model.classifier = nn.Sequential(\n    #             nn.Linear(512 * 7 * 7, 4096),\n    #             nn.ReLU(True),\n    #             nn.Dropout(),\n    #             #nn.Linear(4096, 4096),\n    #             #nn.ReLU(True),\n    #             #nn.Dropout(),\n    #             nn.Linear(4096, 1470),\n    #         )\n    model.load_state_dict(torch.load(\'best.pth\'))\n    model.eval()\n    model.cuda()\n    count = 0\n    for image_path in tqdm(image_list):\n        result = predict_gpu(model,image_path,root_path=\'/home/xzh/data/VOCdevkit/VOC2012/allimgs/\') #result[[left_up,right_bottom,class_name,image_path],]\n        for (x1,y1),(x2,y2),class_name,image_id,prob in result: #image_id is actually image_path\n            preds[class_name].append([image_id,prob,x1,y1,x2,y2])\n        # print(image_path)\n        # image = cv2.imread(\'/home/xzh/data/VOCdevkit/VOC2012/allimgs/\'+image_path)\n        # for left_up,right_bottom,class_name,_,prob in result:\n        #     color = Color[VOC_CLASSES.index(class_name)]\n        #     cv2.rectangle(image,left_up,right_bottom,color,2)\n        #     label = class_name+str(round(prob,2))\n        #     text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n        #     p1 = (left_up[0], left_up[1]- text_size[1])\n        #     cv2.rectangle(image, (p1[0] - 2//2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]), color, -1)\n        #     cv2.putText(image, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, 8)\n\n        # cv2.imwrite(\'testimg/\'+image_path,image)\n        # count += 1\n        # if count == 100:\n        #     break\n    \n    print(\'---start evaluate---\')\n    voc_eval(preds,target,VOC_CLASSES=VOC_CLASSES)'"
net.py,13,"b'#encoding:utf-8\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport math\nimport torch.nn.functional as F\n\n\n__all__ = [\n    \'VGG\', \'vgg11\', \'vgg11_bn\', \'vgg13\', \'vgg13_bn\', \'vgg16\', \'vgg16_bn\',\n    \'vgg19_bn\', \'vgg19\',\n]\n\n\nmodel_urls = {\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n}\n\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000, image_size=448):\n        super(VGG, self).__init__()\n        self.features = features\n        self.image_size = image_size\n        # self.classifier = nn.Sequential(\n        #     nn.Linear(512 * 7 * 7, 4096),\n        #     nn.ReLU(True),\n        #     nn.Dropout(),\n        #     nn.Linear(4096, 4096),\n        #     nn.ReLU(True),\n        #     nn.Dropout(),\n        #     nn.Linear(4096, num_classes),\n        # )\n        # if self.image_size == 448:\n        #     self.extra_conv1 = conv_bn_relu(512,512)\n        #     self.extra_conv2 = conv_bn_relu(512,512)\n        #     self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 1470),\n        )\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        # if self.image_size == 448:\n        #     x = self.extra_conv1(x)\n        #     x = self.extra_conv2(x)\n        #     x = self.downsample(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        x = F.sigmoid(x) #\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x88\xb00-1\n        x = x.view(-1,7,7,30)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    s = 1\n    first_flag=True\n    for v in cfg:\n        s=1\n        if (v==64 and first_flag):\n            s=2\n            first_flag=False\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, stride=s, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\ndef conv_bn_relu(in_channels,out_channels,kernel_size=3,stride=2,padding=1):\n    return nn.Sequential(\n        nn.Conv2d(in_channels,out_channels,kernel_size=kernel_size,padding=padding,stride=stride),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(True)\n    )\n\n\ncfg = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\n\ndef vgg11(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'A\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11\']))\n    return model\n\n\ndef vgg11_bn(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'A\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11_bn\']))\n    return model\n\n\ndef vgg13(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'B\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13\']))\n    return model\n\n\ndef vgg13_bn(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'B\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13_bn\']))\n    return model\n\n\ndef vgg16(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'D\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16\']))\n    return model\n\n\ndef vgg16_bn(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'D\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16_bn\']))\n    return model\n\n\ndef vgg19(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration ""E"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'E\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19\']))\n    return model\n\n\ndef vgg19_bn(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration \'E\') with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'E\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19_bn\']))\n    return model\n\ndef test():\n    import torch\n    from torch.autograd import Variable\n    model = vgg16()\n    model.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 1470),\n        )\n    print(model.classifier[6]) \n    #print(model)\n    img = torch.rand(2,3,224,224)\n    img = Variable(img)\n    output = model(img)\n    print(output.size())\n\nif __name__ == \'__main__\':\n    test()'"
predict.py,16,"b""#encoding:utf-8\n#\n#created by xiongzihua\n#\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\n\nfrom net import vgg16, vgg16_bn\nfrom resnet_yolo import resnet50\nimport torchvision.transforms as transforms\nimport cv2\nimport numpy as np\n\nVOC_CLASSES = (    # always index 0\n    'aeroplane', 'bicycle', 'bird', 'boat',\n    'bottle', 'bus', 'car', 'cat', 'chair',\n    'cow', 'diningtable', 'dog', 'horse',\n    'motorbike', 'person', 'pottedplant',\n'sheep', 'sofa', 'train', 'tvmonitor')\n\nColor = [[0, 0, 0],\n                    [128, 0, 0],\n                    [0, 128, 0],\n                    [128, 128, 0],\n                    [0, 0, 128],\n                    [128, 0, 128],\n                    [0, 128, 128],\n                    [128, 128, 128],\n                    [64, 0, 0],\n                    [192, 0, 0],\n                    [64, 128, 0],\n                    [192, 128, 0],\n                    [64, 0, 128],\n                    [192, 0, 128],\n                    [64, 128, 128],\n                    [192, 128, 128],\n                    [0, 64, 0],\n                    [128, 64, 0],\n                    [0, 192, 0],\n                    [128, 192, 0],\n                    [0, 64, 128]]\n\ndef decoder(pred):\n    '''\n    pred (tensor) 1x7x7x30\n    return (tensor) box[[x1,y1,x2,y2]] label[...]\n    '''\n    grid_num = 14\n    boxes=[]\n    cls_indexs=[]\n    probs = []\n    cell_size = 1./grid_num\n    pred = pred.data\n    pred = pred.squeeze(0) #7x7x30\n    contain1 = pred[:,:,4].unsqueeze(2)\n    contain2 = pred[:,:,9].unsqueeze(2)\n    contain = torch.cat((contain1,contain2),2)\n    mask1 = contain > 0.1 #\xe5\xa4\xa7\xe4\xba\x8e\xe9\x98\x88\xe5\x80\xbc\n    mask2 = (contain==contain.max()) #we always select the best contain_prob what ever it>0.9\n    mask = (mask1+mask2).gt(0)\n    # min_score,min_index = torch.min(contain,2) #\xe6\xaf\x8f\xe4\xb8\xaacell\xe5\x8f\xaa\xe9\x80\x89\xe6\x9c\x80\xe5\xa4\xa7\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n    for i in range(grid_num):\n        for j in range(grid_num):\n            for b in range(2):\n                # index = min_index[i,j]\n                # mask[i,j,index] = 0\n                if mask[i,j,b] == 1:\n                    #print(i,j,b)\n                    box = pred[i,j,b*5:b*5+4]\n                    contain_prob = torch.FloatTensor([pred[i,j,b*5+4]])\n                    xy = torch.FloatTensor([j,i])*cell_size #cell\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92  up left of cell\n                    box[:2] = box[:2]*cell_size + xy # return cxcy relative to image\n                    box_xy = torch.FloatTensor(box.size())#\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90xy\xe5\xbd\xa2\xe5\xbc\x8f    convert[cx,cy,w,h] to [x1,xy1,x2,y2]\n                    box_xy[:2] = box[:2] - 0.5*box[2:]\n                    box_xy[2:] = box[:2] + 0.5*box[2:]\n                    max_prob,cls_index = torch.max(pred[i,j,10:],0)\n                    if float((contain_prob*max_prob)[0]) > 0.1:\n                        boxes.append(box_xy.view(1,4))\n                        cls_indexs.append(cls_index)\n                        probs.append(contain_prob*max_prob)\n    if len(boxes) ==0:\n        boxes = torch.zeros((1,4))\n        probs = torch.zeros(1)\n        cls_indexs = torch.zeros(1)\n    else:\n        boxes = torch.cat(boxes,0) #(n,4)\n        probs = torch.cat(probs,0) #(n,)\n        cls_indexs = torch.cat(cls_indexs,0) #(n,)\n    keep = nms(boxes,probs)\n    return boxes[keep],cls_indexs[keep],probs[keep]\n\ndef nms(bboxes,scores,threshold=0.5):\n    '''\n    bboxes(tensor) [N,4]\n    scores(tensor) [N,]\n    '''\n    x1 = bboxes[:,0]\n    y1 = bboxes[:,1]\n    x2 = bboxes[:,2]\n    y2 = bboxes[:,3]\n    areas = (x2-x1) * (y2-y1)\n\n    _,order = scores.sort(0,descending=True)\n    keep = []\n    while order.numel() > 0:\n        i = order[0]\n        keep.append(i)\n\n        if order.numel() == 1:\n            break\n\n        xx1 = x1[order[1:]].clamp(min=x1[i])\n        yy1 = y1[order[1:]].clamp(min=y1[i])\n        xx2 = x2[order[1:]].clamp(max=x2[i])\n        yy2 = y2[order[1:]].clamp(max=y2[i])\n\n        w = (xx2-xx1).clamp(min=0)\n        h = (yy2-yy1).clamp(min=0)\n        inter = w*h\n\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n        ids = (ovr<=threshold).nonzero().squeeze()\n        if ids.numel() == 0:\n            break\n        order = order[ids+1]\n    return torch.LongTensor(keep)\n#\n#start predict one image\n#\ndef predict_gpu(model,image_name,root_path=''):\n\n    result = []\n    image = cv2.imread(root_path+image_name)\n    h,w,_ = image.shape\n    img = cv2.resize(image,(448,448))\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    mean = (123,117,104)#RGB\n    img = img - np.array(mean,dtype=np.float32)\n\n    transform = transforms.Compose([transforms.ToTensor(),])\n    img = transform(img)\n    img = Variable(img[None,:,:,:],volatile=True)\n    img = img.cuda()\n\n    pred = model(img) #1x7x7x30\n    pred = pred.cpu()\n    boxes,cls_indexs,probs =  decoder(pred)\n\n    for i,box in enumerate(boxes):\n        x1 = int(box[0]*w)\n        x2 = int(box[2]*w)\n        y1 = int(box[1]*h)\n        y2 = int(box[3]*h)\n        cls_index = cls_indexs[i]\n        cls_index = int(cls_index) # convert LongTensor to int\n        prob = probs[i]\n        prob = float(prob)\n        result.append([(x1,y1),(x2,y2),VOC_CLASSES[cls_index],image_name,prob])\n    return result\n        \n\n\n\nif __name__ == '__main__':\n    model = resnet50()\n    print('load model...')\n    model.load_state_dict(torch.load('best.pth'))\n    model.eval()\n    model.cuda()\n    image_name = 'dog.jpg'\n    image = cv2.imread(image_name)\n    print('predicting...')\n    result = predict_gpu(model,image_name)\n    for left_up,right_bottom,class_name,_,prob in result:\n        color = Color[VOC_CLASSES.index(class_name)]\n        cv2.rectangle(image,left_up,right_bottom,color,2)\n        label = class_name+str(round(prob,2))\n        text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n        p1 = (left_up[0], left_up[1]- text_size[1])\n        cv2.rectangle(image, (p1[0] - 2//2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]), color, -1)\n        cv2.putText(image, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, 8)\n\n    cv2.imwrite('result.jpg',image)\n\n\n\n\n"""
resnet_yolo.py,8,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn.functional as F\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass detnet_bottleneck(nn.Module):\n    # no expansion\n    # dilation = 2\n    # type B use 1x1 conv\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, block_type=\'A\'):\n        super(detnet_bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=2, bias=False,dilation=2)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.downsample = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes or block_type==\'B\':\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.downsample(x)\n        out = F.relu(out)\n        return out\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1470):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        # self.layer5 = self._make_layer(block, 512, layers[3], stride=2)\n        self.layer5 = self._make_detnet_layer(in_channels=2048)\n        # self.avgpool = nn.AvgPool2d(14) #fit 448 input size\n        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n        self.conv_end = nn.Conv2d(256, 30, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn_end = nn.BatchNorm2d(30)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n    \n    def _make_detnet_layer(self,in_channels):\n        layers = []\n        layers.append(detnet_bottleneck(in_planes=in_channels, planes=256, block_type=\'B\'))\n        layers.append(detnet_bottleneck(in_planes=256, planes=256, block_type=\'A\'))\n        layers.append(detnet_bottleneck(in_planes=256, planes=256, block_type=\'A\'))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        # x = self.avgpool(x)\n        # x = x.view(x.size(0), -1)\n        # x = self.fc(x)\n        x = self.conv_end(x)\n        x = self.bn_end(x)\n        x = F.sigmoid(x) #\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x88\xb00-1\n        # x = x.view(-1,7,7,30)\n        x = x.permute(0,2,3,1) #(-1,7,7,30)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model'"
train.py,12,"b'import os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom torch.autograd import Variable\n\nfrom net import vgg16, vgg16_bn\nfrom resnet_yolo import resnet50, resnet18\nfrom yoloLoss import yoloLoss\nfrom dataset import yoloDataset\n\nfrom visualize import Visualizer\nimport numpy as np\n\nuse_gpu = torch.cuda.is_available()\n\nfile_root = \'/home/xzh/data/VOCdevkit/VOC2012/allimgs/\'\nlearning_rate = 0.001\nnum_epochs = 50\nbatch_size = 24\nuse_resnet = True\nif use_resnet:\n    net = resnet50()\nelse:\n    net = vgg16_bn()\n# net.classifier = nn.Sequential(\n#             nn.Linear(512 * 7 * 7, 4096),\n#             nn.ReLU(True),\n#             nn.Dropout(),\n#             #nn.Linear(4096, 4096),\n#             #nn.ReLU(True),\n#             #nn.Dropout(),\n#             nn.Linear(4096, 1470),\n#         )\n#net = resnet18(pretrained=True)\n#net.fc = nn.Linear(512,1470)\n# initial Linear\n# for m in net.modules():\n#     if isinstance(m, nn.Linear):\n#         m.weight.data.normal_(0, 0.01)\n#         m.bias.data.zero_()\nprint(net)\n#net.load_state_dict(torch.load(\'yolo.pth\'))\nprint(\'load pre-trined model\')\nif use_resnet:\n    resnet = models.resnet50(pretrained=True)\n    new_state_dict = resnet.state_dict()\n    dd = net.state_dict()\n    for k in new_state_dict.keys():\n        print(k)\n        if k in dd.keys() and not k.startswith(\'fc\'):\n            print(\'yes\')\n            dd[k] = new_state_dict[k]\n    net.load_state_dict(dd)\nelse:\n    vgg = models.vgg16_bn(pretrained=True)\n    new_state_dict = vgg.state_dict()\n    dd = net.state_dict()\n    for k in new_state_dict.keys():\n        print(k)\n        if k in dd.keys() and k.startswith(\'features\'):\n            print(\'yes\')\n            dd[k] = new_state_dict[k]\n    net.load_state_dict(dd)\nif False:\n    net.load_state_dict(torch.load(\'best.pth\'))\nprint(\'cuda\', torch.cuda.current_device(), torch.cuda.device_count())\n\ncriterion = yoloLoss(7,2,5,0.5)\nif use_gpu:\n    net.cuda()\n\nnet.train()\n# different learning rate\nparams=[]\nparams_dict = dict(net.named_parameters())\nfor key,value in params_dict.items():\n    if key.startswith(\'features\'):\n        params += [{\'params\':[value],\'lr\':learning_rate*1}]\n    else:\n        params += [{\'params\':[value],\'lr\':learning_rate}]\noptimizer = torch.optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n# optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate,weight_decay=1e-4)\n\n# train_dataset = yoloDataset(root=file_root,list_file=[\'voc12_trainval.txt\',\'voc07_trainval.txt\'],train=True,transform = [transforms.ToTensor()] )\ntrain_dataset = yoloDataset(root=file_root,list_file=[\'voc2012.txt\',\'voc2007.txt\'],train=True,transform = [transforms.ToTensor()] )\ntrain_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4)\n# test_dataset = yoloDataset(root=file_root,list_file=\'voc07_test.txt\',train=False,transform = [transforms.ToTensor()] )\ntest_dataset = yoloDataset(root=file_root,list_file=\'voc2007test.txt\',train=False,transform = [transforms.ToTensor()] )\ntest_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=4)\nprint(\'the dataset has %d images\' % (len(train_dataset)))\nprint(\'the batch_size is %d\' % (batch_size))\nlogfile = open(\'log.txt\', \'w\')\n\nnum_iter = 0\nvis = Visualizer(env=\'xiong\')\nbest_test_loss = np.inf\n\nfor epoch in range(num_epochs):\n    net.train()\n    # if epoch == 1:\n    #     learning_rate = 0.0005\n    # if epoch == 2:\n    #     learning_rate = 0.00075\n    # if epoch == 3:\n    #     learning_rate = 0.001\n    if epoch == 30:\n        learning_rate=0.0001\n    if epoch == 40:\n        learning_rate=0.00001\n    # optimizer = torch.optim.SGD(net.parameters(),lr=learning_rate*0.1,momentum=0.9,weight_decay=1e-4)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = learning_rate\n    \n    print(\'\\n\\nStarting epoch %d / %d\' % (epoch + 1, num_epochs))\n    print(\'Learning Rate for this epoch: {}\'.format(learning_rate))\n    \n    total_loss = 0.\n    \n    for i,(images,target) in enumerate(train_loader):\n        images = Variable(images)\n        target = Variable(target)\n        if use_gpu:\n            images,target = images.cuda(),target.cuda()\n        \n        pred = net(images)\n        loss = criterion(pred,target)\n        total_loss += loss.data[0]\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if (i+1) % 5 == 0:\n            print (\'Epoch [%d/%d], Iter [%d/%d] Loss: %.4f, average_loss: %.4f\' \n            %(epoch+1, num_epochs, i+1, len(train_loader), loss.data[0], total_loss / (i+1)))\n            num_iter += 1\n            vis.plot_train_val(loss_train=total_loss/(i+1))\n\n    #validation\n    validation_loss = 0.0\n    net.eval()\n    for i,(images,target) in enumerate(test_loader):\n        images = Variable(images,volatile=True)\n        target = Variable(target,volatile=True)\n        if use_gpu:\n            images,target = images.cuda(),target.cuda()\n        \n        pred = net(images)\n        loss = criterion(pred,target)\n        validation_loss += loss.data[0]\n    validation_loss /= len(test_loader)\n    vis.plot_train_val(loss_val=validation_loss)\n    \n    if best_test_loss > validation_loss:\n        best_test_loss = validation_loss\n        print(\'get best test loss %.5f\' % best_test_loss)\n        torch.save(net.state_dict(),\'best.pth\')\n    logfile.writelines(str(epoch) + \'\\t\' + str(validation_loss) + \'\\n\')  \n    logfile.flush()      \n    torch.save(net.state_dict(),\'yolo.pth\')\n    \n\n'"
visualize.py,0,"b""import visdom \nimport numpy as np\n\nclass Visualizer():\n    def __init__(self, env='main', **kwargs):\n        '''\n        **kwargs, dict option\n        '''\n        self.vis = visdom.Visdom(env=env)\n        self.index = {}  # x, dict\n        self.log_text = ''\n        self.env = env\n    \n    def plot_train_val(self, loss_train=None, loss_val=None):\n        '''\n        plot val loss and train loss in one figure\n        '''\n        x = self.index.get('train_val', 0)\n\n        if x == 0:\n            loss = loss_train if loss_train else loss_val\n            win_y = np.column_stack((loss, loss))\n            win_x = np.column_stack((x, x))\n            self.win = self.vis.line(Y=win_y, X=win_x, \n                                env=self.env)\n                                # opts=dict(\n                                #     title='train_test_loss',\n                                # ))\n            self.index['train_val'] = x + 1\n            return \n\n        if loss_train != None:\n            self.vis.line(Y=np.array([loss_train]), X=np.array([x]),\n                        win=self.win,\n                        name='1',\n                        update='append',\n                        env=self.env)\n            self.index['train_val'] = x + 5\n        else:\n            self.vis.line(Y=np.array([loss_val]), X=np.array([x]),\n                        win=self.win,\n                        name='2',\n                        update='append',\n                        env=self.env)\n\n    def plot_many(self, d):\n        '''\n        d: dict {name, value}\n        '''\n        for k, v in d.iteritems():\n            self.plot(k, v)\n\n    def plot(self, name, y, **kwargs):\n        '''\n        plot('loss', 1.00)\n        '''\n        x = self.index.get(name, 0) # if none, return 0\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                    win=name,\n                    opts=dict(title=name),\n                    update=None if x== 0 else 'append',\n                    **kwargs)\n        self.index[name] = x + 1\n    \n    def log(self, info, win='log_text'):\n        '''\n        show text in box not write into txt?\n        '''\n        pass\n"""
xml_2_txt.py,0,"b'import xml.etree.ElementTree as ET\nimport os\n\nVOC_CLASSES = (    # always index 0\n    \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n    \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n    \'cow\', \'diningtable\', \'dog\', \'horse\',\n    \'motorbike\', \'person\', \'pottedplant\',\n    \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        difficult = int(obj.find(\'difficult\').text)\n        if difficult == 1:\n            # print(filename)\n            continue\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        #obj_struct[\'pose\'] = obj.find(\'pose\').text\n        #obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        #obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(float(bbox.find(\'xmin\').text)),\n                              int(float(bbox.find(\'ymin\').text)),\n                              int(float(bbox.find(\'xmax\').text)),\n                              int(float(bbox.find(\'ymax\').text))]\n        objects.append(obj_struct)\n\n    return objects\n\ntxt_file = open(\'voc2007test.txt\',\'w\')\ntest_file = open(\'voc07testimg.txt\',\'r\')\nlines = test_file.readlines()\nlines = [x[:-1] for x in lines]\nprint(lines)\n\nAnnotations = \'/home/xzh/data/VOCdevkit/VOC2007/Annotations/\'\nxml_files = os.listdir(Annotations)\n\ncount = 0\nfor xml_file in xml_files:\n    count += 1\n    if xml_file.split(\'.\')[0] not in lines:\n        # print(xml_file.split(\'.\')[0])\n        continue\n    image_path = xml_file.split(\'.\')[0] + \'.jpg\'\n    results = parse_rec(Annotations + xml_file)\n    if len(results)==0:\n        print(xml_file)\n        continue\n    txt_file.write(image_path)\n    # num_obj = len(results)\n    # txt_file.write(str(num_obj)+\' \')\n    for result in results:\n        class_name = result[\'name\']\n        bbox = result[\'bbox\']\n        class_name = VOC_CLASSES.index(class_name)\n        txt_file.write(\' \'+str(bbox[0])+\' \'+str(bbox[1])+\' \'+str(bbox[2])+\' \'+str(bbox[3])+\' \'+str(class_name))\n    txt_file.write(\'\\n\')\n    #if count == 10:\n    #    break\ntxt_file.close()'"
yoloLoss.py,13,"b""#encoding:utf-8\r\n#\r\n#created by xiongzihua 2017.12.26\r\n#\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nclass yoloLoss(nn.Module):\r\n    def __init__(self,S,B,l_coord,l_noobj):\r\n        super(yoloLoss,self).__init__()\r\n        self.S = S\r\n        self.B = B\r\n        self.l_coord = l_coord\r\n        self.l_noobj = l_noobj\r\n\r\n    def compute_iou(self, box1, box2):\r\n        '''Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\r\n        Args:\r\n          box1: (tensor) bounding boxes, sized [N,4].\r\n          box2: (tensor) bounding boxes, sized [M,4].\r\n        Return:\r\n          (tensor) iou, sized [N,M].\r\n        '''\r\n        N = box1.size(0)\r\n        M = box2.size(0)\r\n\r\n        lt = torch.max(\r\n            box1[:,:2].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\r\n            box2[:,:2].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\r\n        )\r\n\r\n        rb = torch.min(\r\n            box1[:,2:].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\r\n            box2[:,2:].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\r\n        )\r\n\r\n        wh = rb - lt  # [N,M,2]\r\n        wh[wh<0] = 0  # clip at 0\r\n        inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\r\n\r\n        area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,]\r\n        area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,]\r\n        area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\r\n        area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\r\n\r\n        iou = inter / (area1 + area2 - inter)\r\n        return iou\r\n    def forward(self,pred_tensor,target_tensor):\r\n        '''\r\n        pred_tensor: (tensor) size(batchsize,S,S,Bx5+20=30) [x,y,w,h,c]\r\n        target_tensor: (tensor) size(batchsize,S,S,30)\r\n        '''\r\n        N = pred_tensor.size()[0]\r\n        coo_mask = target_tensor[:,:,:,4] > 0\r\n        noo_mask = target_tensor[:,:,:,4] == 0\r\n        coo_mask = coo_mask.unsqueeze(-1).expand_as(target_tensor)\r\n        noo_mask = noo_mask.unsqueeze(-1).expand_as(target_tensor)\r\n\r\n        coo_pred = pred_tensor[coo_mask].view(-1,30)\r\n        box_pred = coo_pred[:,:10].contiguous().view(-1,5) #box[x1,y1,w1,h1,c1]\r\n        class_pred = coo_pred[:,10:]                       #[x2,y2,w2,h2,c2]\r\n        \r\n        coo_target = target_tensor[coo_mask].view(-1,30)\r\n        box_target = coo_target[:,:10].contiguous().view(-1,5)\r\n        class_target = coo_target[:,10:]\r\n\r\n        # compute not contain obj loss\r\n        noo_pred = pred_tensor[noo_mask].view(-1,30)\r\n        noo_target = target_tensor[noo_mask].view(-1,30)\r\n        noo_pred_mask = torch.cuda.ByteTensor(noo_pred.size())\r\n        noo_pred_mask.zero_()\r\n        noo_pred_mask[:,4]=1;noo_pred_mask[:,9]=1\r\n        noo_pred_c = noo_pred[noo_pred_mask] #noo pred\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97 c \xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1 size[-1,2]\r\n        noo_target_c = noo_target[noo_pred_mask]\r\n        nooobj_loss = F.mse_loss(noo_pred_c,noo_target_c,size_average=False)\r\n\r\n        #compute contain obj loss\r\n        coo_response_mask = torch.cuda.ByteTensor(box_target.size())\r\n        coo_response_mask.zero_()\r\n        coo_not_response_mask = torch.cuda.ByteTensor(box_target.size())\r\n        coo_not_response_mask.zero_()\r\n        box_target_iou = torch.zeros(box_target.size()).cuda()\r\n        for i in range(0,box_target.size()[0],2): #choose the best iou box\r\n            box1 = box_pred[i:i+2]\r\n            box1_xyxy = Variable(torch.FloatTensor(box1.size()))\r\n            box1_xyxy[:,:2] = box1[:,:2]/14. -0.5*box1[:,2:4]\r\n            box1_xyxy[:,2:4] = box1[:,:2]/14. +0.5*box1[:,2:4]\r\n            box2 = box_target[i].view(-1,5)\r\n            box2_xyxy = Variable(torch.FloatTensor(box2.size()))\r\n            box2_xyxy[:,:2] = box2[:,:2]/14. -0.5*box2[:,2:4]\r\n            box2_xyxy[:,2:4] = box2[:,:2]/14. +0.5*box2[:,2:4]\r\n            iou = self.compute_iou(box1_xyxy[:,:4],box2_xyxy[:,:4]) #[2,1]\r\n            max_iou,max_index = iou.max(0)\r\n            max_index = max_index.data.cuda()\r\n            \r\n            coo_response_mask[i+max_index]=1\r\n            coo_not_response_mask[i+1-max_index]=1\r\n\r\n            #####\r\n            # we want the confidence score to equal the\r\n            # intersection over union (IOU) between the predicted box\r\n            # and the ground truth\r\n            #####\r\n            box_target_iou[i+max_index,torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\r\n        box_target_iou = Variable(box_target_iou).cuda()\r\n        #1.response loss\r\n        box_pred_response = box_pred[coo_response_mask].view(-1,5)\r\n        box_target_response_iou = box_target_iou[coo_response_mask].view(-1,5)\r\n        box_target_response = box_target[coo_response_mask].view(-1,5)\r\n        contain_loss = F.mse_loss(box_pred_response[:,4],box_target_response_iou[:,4],size_average=False)\r\n        loc_loss = F.mse_loss(box_pred_response[:,:2],box_target_response[:,:2],size_average=False) + F.mse_loss(torch.sqrt(box_pred_response[:,2:4]),torch.sqrt(box_target_response[:,2:4]),size_average=False)\r\n        #2.not response loss\r\n        box_pred_not_response = box_pred[coo_not_response_mask].view(-1,5)\r\n        box_target_not_response = box_target[coo_not_response_mask].view(-1,5)\r\n        box_target_not_response[:,4]= 0\r\n        #not_contain_loss = F.mse_loss(box_pred_response[:,4],box_target_response[:,4],size_average=False)\r\n        \r\n        #I believe this bug is simply a typo\r\n        not_contain_loss = F.mse_loss(box_pred_not_response[:,4], box_target_not_response[:,4],size_average=False)\r\n\r\n        #3.class loss\r\n        class_loss = F.mse_loss(class_pred,class_target,size_average=False)\r\n\r\n        return (self.l_coord*loc_loss + 2*contain_loss + not_contain_loss + self.l_noobj*nooobj_loss + class_loss)/N\r\n\r\n\r\n\r\n\r\n"""
