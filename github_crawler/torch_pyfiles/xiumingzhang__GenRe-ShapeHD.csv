file_path,api_count,code
test.py,3,"b'import os\nimport time\nfrom shutil import rmtree\nfrom tqdm import tqdm\nimport torch\nfrom options import options_test\nimport datasets\nimport models\nfrom util.util_print import str_error, str_stage, str_verbose\nimport util.util_loadlib as loadlib\nfrom loggers import loggers\n\n\nprint(""Testing Pipeline"")\n\n###################################################\n\nprint(str_stage, ""Parsing arguments"")\nopt = options_test.parse()\nopt.full_logdir = None\nprint(opt)\n\n###################################################\n\nprint(str_stage, ""Setting device"")\nif opt.gpu == \'-1\':\n    device = torch.device(\'cpu\')\nelse:\n    loadlib.set_gpu(opt.gpu)\n    device = torch.device(\'cuda\')\nif opt.manual_seed is not None:\n    loadlib.set_manual_seed(opt.manual_seed)\n\n###################################################\n\nprint(str_stage, ""Setting up output directory"")\noutput_dir = opt.output_dir\noutput_dir += (\'_\' + opt.suffix.format(**vars(opt))) \\\n    if opt.suffix != \'\' else \'\'\nopt.output_dir = output_dir\n\nif os.path.isdir(output_dir):\n    if opt.overwrite:\n        rmtree(output_dir)\n    else:\n        raise ValueError(str_error +\n                         "" %s already exists, but no overwrite flag""\n                         % output_dir)\nos.makedirs(output_dir)\n\n###################################################\n\nprint(str_stage, ""Setting up loggers"")\nlogger_list = [\n    loggers.TerminateOnNaN(),\n]\nlogger = loggers.ComposeLogger(logger_list)\n\n###################################################\n\nprint(str_stage, ""Setting up models"")\nModel = models.get_model(opt.net, test=True)\nmodel = Model(opt, logger)\nmodel.to(device)\nmodel.eval()\nprint(model)\nprint(""# model parameters: {:,d}"".format(model.num_parameters()))\n\n###################################################\n\nprint(str_stage, ""Setting up data loaders"")\nstart_time = time.time()\nDataset = datasets.get_dataset(\'test\')\ndataset = Dataset(opt, model=model)\ndataloader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=opt.batch_size,\n    num_workers=opt.workers,\n    pin_memory=True,\n    drop_last=False,\n    shuffle=False\n)\nn_batches = len(dataloader)\ndataiter = iter(dataloader)\nprint(str_verbose, ""Time spent in data IO initialization: %.2fs"" %\n      (time.time() - start_time))\nprint(str_verbose, ""# test points: "" + str(len(dataset)))\nprint(str_verbose, ""# test batches: "" + str(n_batches))\n\n###################################################\n\nprint(str_stage, ""Testing"")\nfor i in tqdm(range(n_batches)):\n    batch = next(dataiter)\n    model.test_on_batch(i, batch)\n'"
train.py,6,"b'import sys\nimport os\nimport time\nimport pandas as pd\nimport torch\nfrom options import options_train\nimport datasets\nimport models\nfrom loggers import loggers\nfrom util.util_print import str_error, str_stage, str_verbose, str_warning\nfrom util import util_loadlib as loadlib\n\n\n###################################################\n\nprint(str_stage, ""Parsing arguments"")\nopt, unique_opt_params = options_train.parse()\n# Get all parse done, including subparsers\nprint(opt)\n\n###################################################\n\nprint(str_stage, ""Setting device"")\nif opt.gpu == \'-1\':\n    device = torch.device(\'cpu\')\nelse:\n    loadlib.set_gpu(opt.gpu)\n    device = torch.device(\'cuda\')\nif opt.manual_seed is not None:\n    loadlib.set_manual_seed(opt.manual_seed)\n\n###################################################\n\nprint(str_stage, ""Setting up logging directory"")\nexprdir = \'{}_{}_{}\'.format(opt.net, opt.dataset, opt.lr)\nexprdir += (\'_\' + opt.suffix.format(**vars(opt))) if opt.suffix != \'\' else \'\'\nlogdir = os.path.join(opt.logdir, exprdir, str(opt.expr_id))\n\nif opt.resume == 0:\n    if os.path.isdir(logdir):\n        if opt.expr_id <= 0:\n            print(\n                str_warning, (\n                    ""Will remove Experiment %d at\\n\\t%s\\n""\n                    ""Do you want to continue? (y/n)""\n                ) % (opt.expr_id, logdir)\n            )\n            need_input = True\n            while need_input:\n                response = input().lower()\n                if response in (\'y\', \'n\'):\n                    need_input = False\n            if response == \'n\':\n                print(str_stage, ""User decides to quit"")\n                sys.exit()\n            os.system(\'rm -rf \' + logdir)\n        else:\n            raise ValueError(str_error +\n                             "" Refuse to remove positive expr_id"")\n    os.system(\'mkdir -p \' + logdir)\nelse:\n    assert os.path.isdir(logdir)\n    opt_f_old = os.path.join(logdir, \'opt.pt\')\n    opt = options_train.overwrite(opt, opt_f_old, unique_opt_params)\n\n# Save opt\ntorch.save(vars(opt), os.path.join(logdir, \'opt.pt\'))\nwith open(os.path.join(logdir, \'opt.txt\'), \'w\') as fout:\n    for k, v in vars(opt).items():\n        fout.write(\'%20s\\t%-20s\\n\' % (k, v))\n\nopt.full_logdir = logdir\nprint(str_verbose, ""Logging directory set to: %s"" % logdir)\n\n###################################################\n\nprint(str_stage, ""Setting up loggers"")\nif opt.resume != 0 and os.path.isfile(os.path.join(logdir, \'best.pt\')):\n    try:\n        prev_best_data = torch.load(os.path.join(logdir, \'best.pt\'))\n        prev_best = prev_best_data[\'loss_eval\']\n        del prev_best_data\n    except KeyError:\n        prev_best = None\nelse:\n    prev_best = None\nbest_model_logger = loggers.ModelSaveLogger(\n    os.path.join(logdir, \'best.pt\'),\n    period=1,\n    save_optimizer=True,\n    save_best=True,\n    prev_best=prev_best\n)\nlogger_list = [\n    loggers.TerminateOnNaN(),\n    loggers.ProgbarLogger(allow_unused_fields=\'all\'),\n    loggers.CsvLogger(\n        os.path.join(logdir, \'epoch_loss.csv\'),\n        allow_unused_fields=\'all\'\n    ),\n    loggers.ModelSaveLogger(\n        os.path.join(logdir, \'nets\', \'{epoch:04d}.pt\'),\n        period=opt.save_net,\n        save_optimizer=opt.save_net_opt\n    ),\n    loggers.ModelSaveLogger(\n        os.path.join(logdir, \'checkpoint.pt\'),\n        period=1,\n        save_optimizer=True\n    ),\n    best_model_logger,\n]\nif opt.log_batch:\n    logger_list.append(\n        loggers.BatchCsvLogger(\n            os.path.join(logdir, \'batch_loss.csv\'),\n            allow_unused_fields=\'all\'\n        )\n    )\nif opt.tensorboard:\n    tf_logdir = os.path.join(\n        opt.logdir, \'tensorboard\', exprdir, str(opt.expr_id))\n    if os.path.isdir(tf_logdir) and opt.resume == 0:\n        os.system(\'rm -r \' + tf_logdir)  # remove previous tensorboard log if overwriting\n    if not os.path.isdir(os.path.join(logdir, \'tensorboard\')):\n        os.symlink(tf_logdir, os.path.join(logdir, \'tensorboard\'))\n    logger_list.append(\n        loggers.TensorBoardLogger(\n            tf_logdir,\n            allow_unused_fields=\'all\'\n        )\n    )\nlogger = loggers.ComposeLogger(logger_list)\n\n###################################################\n\nprint(str_stage, ""Setting up models"")\nModel = models.get_model(opt.net)\nmodel = Model(opt, logger)\nmodel.to(device)\nprint(model)\nprint(""# model parameters: {:,d}"".format(model.num_parameters()))\n\ninitial_epoch = 1\nif opt.resume != 0:\n    if opt.resume == -1:\n        net_filename = os.path.join(logdir, \'checkpoint.pt\')\n    elif opt.resume == -2:\n        net_filename = os.path.join(logdir, \'best.pt\')\n    else:\n        net_filename = os.path.join(\n            logdir, \'nets\', \'{epoch:04d}.pt\').format(epoch=opt.resume)\n    if not os.path.isfile(net_filename):\n        print(str_warning, (""Network file not found for opt.resume=%d. ""\n                            ""Starting from scratch"") % opt.resume)\n    else:\n        additional_values = model.load_state_dict(net_filename, load_optimizer=\'auto\')\n        try:\n            initial_epoch += additional_values[\'epoch\']\n        except KeyError as err:\n            # Old saved model does not have epoch as additional values\n            epoch_loss_csv = os.path.join(logdir, \'epoch_loss.csv\')\n            if opt.resume == -1:\n                try:\n                    initial_epoch += pd.read_csv(epoch_loss_csv)[\'epoch\'].max()\n                except pd.errors.ParserError:\n                    with open(epoch_loss_csv, \'r\') as f:\n                        lines = f.readlines()\n                    initial_epoch += max([int(l.split(\',\')[0]) for l in lines[1:]])\n            else:\n                initial_epoch += opt.resume\n\n###################################################\n\nprint(str_stage, ""Setting up data loaders"")\nstart_time = time.time()\ndataset = datasets.get_dataset(opt.dataset)\ndataset_train = dataset(opt, mode=\'train\', model=model)\ndataset_vali = dataset(opt, mode=\'vali\', model=model)\ndataloader_train = torch.utils.data.DataLoader(\n    dataset_train,\n    batch_size=opt.batch_size,\n    shuffle=True,\n    num_workers=opt.workers,\n    pin_memory=True,\n    drop_last=True\n)\ndataloader_vali = torch.utils.data.DataLoader(\n    dataset_vali,\n    batch_size=opt.batch_size,\n    num_workers=opt.workers,\n    pin_memory=True,\n    drop_last=True,\n    shuffle=False\n)\nprint(str_verbose, ""Time spent in data IO initialization: %.2fs"" %\n      (time.time() - start_time))\nprint(str_verbose, ""# training points: "" + str(len(dataset_train)))\nprint(str_verbose, ""# training batches per epoch: "" + str(len(dataloader_train)))\nprint(str_verbose, ""# test batches: "" + str(len(dataloader_vali)))\n\n###################################################\n\nif opt.epoch > 0:\n    print(str_stage, ""Training"")\n    model.train_epoch(\n        dataloader_train,\n        dataloader_eval=dataloader_vali,\n        max_batches_per_train=opt.epoch_batches,\n        epochs=opt.epoch,\n        initial_epoch=initial_epoch,\n        max_batches_per_eval=opt.eval_batches,\n        eval_at_start=opt.eval_at_start\n    )\n'"
datasets/__init__.py,0,"b""import importlib\n\n\ndef get_dataset(alias):\n    dataset_module = importlib.import_module('datasets.' + alias.lower())\n    return dataset_module.Dataset\n"""
datasets/shapenet.py,1,"b'from os.path import join\nimport random\nimport numpy as np\nfrom scipy.io import loadmat\nimport torch.utils.data as data\nimport util.util_img\n\n\nclass Dataset(data.Dataset):\n    data_root = \'./downloads/data/shapenet\'\n    list_root = join(data_root, \'status\')\n    status_and_suffix = {\n        \'rgb\': {\n            \'status\': \'rgb.txt\',\n            \'suffix\': \'_rgb.png\',\n        },\n        \'depth\': {\n            \'status\': \'depth.txt\',\n            \'suffix\': \'_depth.png\',\n        },\n        \'depth_minmax\': {\n            \'status\': \'depth_minmax.txt\',\n            \'suffix\': \'.npy\',\n        },\n        \'silhou\': {\n            \'status\': \'silhou.txt\',\n            \'suffix\': \'_silhouette.png\',\n        },\n        \'normal\': {\n            \'status\': \'normal.txt\',\n            \'suffix\': \'_normal.png\'\n        },\n        \'voxel\': {\n            \'status\': \'vox_rot.txt\',\n            \'suffix\': \'_gt_rotvox_samescale_128.npz\'\n        },\n        \'spherical\': {\n            \'status\': \'spherical.txt\',\n            \'suffix\': \'_spherical.npz\'\n        },\n        \'voxel_canon\': {\n            \'status\': \'vox_canon.txt\',\n            \'suffix\': \'_voxel_normalized_128.mat\'\n        },\n    }\n    class_aliases = {\n        \'drc\': \'03001627+02691156+02958343\',\n        \'chair\': \'03001627\',\n        \'table\': \'04379243\',\n        \'sofa\': \'04256520\',\n        \'couch\': \'04256520\',\n        \'cabinet\': \'03337140\',\n        \'bed\': \'02818832\',\n        \'plane\': \'02691156\',\n        \'car\': \'02958343\',\n        \'bench\': \'02828884\',\n        \'monitor\': \'03211117\',\n        \'lamp\': \'03636649\',\n        \'speaker\': \'03691459\',\n        \'firearm\': \'03948459+04090263\',\n        \'cellphone\': \'02992529+04401088\',\n        \'watercraft\': \'04530566\',\n        \'hat\': \'02954340\',\n        \'pot\': \'03991062\',\n        \'rocket\': \'04099429\',\n        \'train\': \'04468005\',\n        \'bus\': \'02924116\',\n        \'pistol\': \'03948459\',\n        \'faucet\': \'03325088\',\n        \'helmet\': \'03513137\',\n        \'clock\': \'03046257\',\n        \'phone\': \'04401088\',\n        \'display\': \'03211117\',\n        \'vessel\': \'04530566\',\n        \'rifle\': \'04090263\',\n        \'small\': \'03001627+04379243+02933112+04256520+02958343+03636649+02691156+04530566\',\n        \'all-but-table\': \'02691156+02747177+02773838+02801938+02808440+02818832+02828884+02843684+02871439+02876657+02880940+02924116+02933112+02942699+02946921+02954340+02958343+02992529+03001627+03046257+03085013+03207941+03211117+03261776+03325088+03337140+03467517+03513137+03593526+03624134+03636649+03642806+03691459+03710193+03759954+03761084+03790512+03797390+03928116+03938244+03948459+03991062+04004475+04074963+04090263+04099429+04225987+04256520+04330267+04401088+04460130+04468005+04530566+04554684\',\n        \'all-but-chair\': \'02691156+02747177+02773838+02801938+02808440+02818832+02828884+02843684+02871439+02876657+02880940+02924116+02933112+02942699+02946921+02954340+02958343+02992529+03046257+03085013+03207941+03211117+03261776+03325088+03337140+03467517+03513137+03593526+03624134+03636649+03642806+03691459+03710193+03759954+03761084+03790512+03797390+03928116+03938244+03948459+03991062+04004475+04074963+04090263+04099429+04225987+04256520+04330267+04379243+04401088+04460130+04468005+04530566+04554684\',\n        \'all\': \'02691156+02747177+02773838+02801938+02808440+02818832+02828884+02843684+02871439+02876657+02880940+02924116+02933112+02942699+02946921+02954340+02958343+02992529+03001627+03046257+03085013+03207941+03211117+03261776+03325088+03337140+03467517+03513137+03593526+03624134+03636649+03642806+03691459+03710193+03759954+03761084+03790512+03797390+03928116+03938244+03948459+03991062+04004475+04074963+04090263+04099429+04225987+04256520+04330267+04379243+04401088+04460130+04468005+04530566+04554684\',\n    }\n    class_list = class_aliases[\'all\'].split(\'+\')\n\n    @classmethod\n    def add_arguments(cls, parser):\n        return parser, set()\n\n    @classmethod\n    def read_bool_status(cls, status_file):\n        with open(join(cls.list_root, status_file)) as f:\n            lines = f.read()\n        return [x == \'True\' for x in lines.split(\'\\n\')[:-1]]\n\n    def __init__(self, opt, mode=\'train\', model=None):\n        assert mode in (\'train\', \'vali\')\n        self.mode = mode\n        if model is None:\n            required = [\'rgb\']\n            self.preproc = None\n        else:\n            required = model.requires\n            self.preproc = model.preprocess\n\n        # Parse classes\n        classes = []  # alias to real for locating data\n        class_str = \'\'  # real to alias for logging\n        for c in opt.classes.split(\'+\'):\n            class_str += c + \'+\'\n            if c in self.class_aliases:  # nickname given\n                classes += self.class_aliases[c].split(\'+\')\n            else:\n                classes = c.split(\'+\')\n        class_str = class_str[:-1]  # removes the final +\n        classes = sorted(list(set(classes)))\n\n        # Load items and train-test split\n        with open(join(self.list_root, \'items_all.txt\')) as f:\n            lines = f.read()\n        item_list = lines.split(\'\\n\')[:-1]\n        is_train = self.read_bool_status(\'is_train.txt\')\n        assert len(item_list) == len(is_train)\n\n        # Load status the network requires\n        has = {}\n        for data_type in required:\n            assert data_type in self.status_and_suffix.keys(), \\\n                ""%s required, but unspecified in status_and_suffix"" % data_type\n            has[data_type] = self.read_bool_status(\n                self.status_and_suffix[data_type][\'status\']\n            )\n            assert len(has[data_type]) == len(item_list)\n\n        # Pack paths into a dict\n        samples = []\n        for i, item in enumerate(item_list):\n            class_id, _ = item.split(\'/\')[:2]\n            item_in_split = ((self.mode == \'train\') == is_train[i])\n            if item_in_split and class_id in classes:\n                # Look up subclass_id for this item\n                sample_dict = {\'item\': join(self.data_root, item)}\n                # As long as a type is required, it appears as a key\n                # If it doens\'t exist, its value will be None\n                for data_type in required:\n                    suffix = self.status_and_suffix[data_type][\'suffix\']\n                    k = data_type + \'_path\'\n                    if data_type == \'voxel_canon\':\n                        # All different views share the same canonical voxel\n                        sample_dict[k] = join(self.data_root, item.split(\'_view\')[0] + suffix) \\\n                            if has[data_type][i] else None\n                    else:\n                        sample_dict[k] = join(self.data_root, item + suffix) \\\n                            if has[data_type][i] else None\n                if None not in sample_dict.values():\n                    # All that are required exist\n                    samples.append(sample_dict)\n\n        # If validation, dataloader shuffle will be off, so need to DETERMINISTICALLY\n        # shuffle here to have a bit of every class\n        if self.mode == \'vali\':\n            if opt.manual_seed:\n                seed = opt.manual_seed\n            else:\n                seed = 0\n            random.Random(seed).shuffle(samples)\n        self.samples = samples\n\n    def __getitem__(self, i):\n        sample_loaded = {}\n        for k, v in self.samples[i].items():\n            sample_loaded[k] = v  # as-is\n            if k.endswith(\'_path\'):\n                if v.endswith(\'.png\'):\n                    im = util.util_img.imread_wrapper(\n                        v, util.util_img.IMREAD_UNCHANGED,\n                        output_channel_order=\'RGB\')\n                    # Normalize to [0, 1] floats\n                    im = im.astype(float) / float(np.iinfo(im.dtype).max)\n                    sample_loaded[k[:-5]] = im\n                elif v.endswith(\'.npy\'):\n                    # Right now .npy must be depth_minmax\n                    sample_loaded[\'depth_minmax\'] = np.load(v)\n                elif v.endswith(\'_128.npz\'):\n                    sample_loaded[\'voxel\'] = np.load(v)[\'voxel\'][None, ...]\n                elif v.endswith(\'_spherical.npz\'):\n                    spherical_data = np.load(v)\n                    sample_loaded[\'spherical_object\'] = spherical_data[\'obj_spherical\'][None, ...]\n                    sample_loaded[\'spherical_depth\'] = spherical_data[\'depth_spherical\'][None, ...]\n                elif v.endswith(\'.mat\'):\n                    # Right now .mat must be voxel_canon\n                    sample_loaded[\'voxel_canon\'] = loadmat(v)[\'voxel\'][None, ...]\n                else:\n                    raise NotImplementedError(v)\n            # Three identical channels for grayscale images\n        if self.preproc is not None:\n            sample_loaded = self.preproc(sample_loaded, mode=self.mode)\n        # convert all types to float32 for better copy speed\n        self.convert_to_float32(sample_loaded)\n        return sample_loaded\n\n    @staticmethod\n    def convert_to_float32(sample_loaded):\n        for k, v in sample_loaded.items():\n            if isinstance(v, np.ndarray):\n                if v.dtype != np.float32:\n                    sample_loaded[k] = v.astype(np.float32)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def get_classes(self):\n        return self._class_str\n'"
datasets/test.py,1,"b'from glob import glob\nimport numpy as np\nimport torch.utils.data as data\nimport util.util_img\n\n\nclass Dataset(data.Dataset):\n    @classmethod\n    def add_arguments(cls, parser):\n        return parser, set()\n\n    def __init__(self, opt, model):\n        # Get required keys and preprocessing from the model\n        required = model.requires\n        self.preproc = model.preprocess_wrapper\n        # Wrapper usually crops and resizes the input image (so that it\'s just\n        # like our renders) before sending it to the actual preprocessing\n\n        # Associate each data type required by the model with input paths\n        type2filename = {}\n        for k in required:\n            type2filename[k] = getattr(opt, \'input_\' + k)\n\n        # Generate a sorted filelist for each data type\n        type2files = {}\n        for k, v in type2filename.items():\n            type2files[k] = sorted(glob(v))\n        ns = [len(x) for x in type2files.values()]\n        assert len(set(ns)) == 1, \\\n            (""Filelists for different types must be of the same length ""\n             ""(1-to-1 correspondance)"")\n        self.length = ns[0]\n\n        samples = []\n        for i in range(self.length):\n            sample = {}\n            for k, v in type2files.items():\n                sample[k + \'_path\'] = v[i]\n            samples.append(sample)\n        self.samples = samples\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        sample = self.samples[i]\n\n        # Actually loading the item\n        sample_loaded = {}\n        for k, v in sample.items():\n            sample_loaded[k] = v  # as-is\n            if k == \'rgb_path\':\n                im = util.util_img.imread_wrapper(\n                    v, util.util_img.IMREAD_COLOR, output_channel_order=\'RGB\')\n                # Normalize to [0, 1] floats\n                im = im.astype(float) / float(np.iinfo(im.dtype).max)\n                sample_loaded[\'rgb\'] = im\n            elif k == \'mask_path\':\n                im = util.util_img.imread_wrapper(\n                    v, util.util_img.IMREAD_GRAYSCALE)\n                # Normalize to [0, 1] floats\n                im = im.astype(float) / float(np.iinfo(im.dtype).max)\n                sample_loaded[\'silhou\'] = im\n            else:\n                raise NotImplementedError(v)\n\n        # Preprocessing specified by the model\n        sample_loaded = self.preproc(sample_loaded)\n        # Convert all types to float32 for faster copying\n        self.convert_to_float32(sample_loaded)\n        return sample_loaded\n\n    @staticmethod\n    def convert_to_float32(sample_loaded):\n        for k, v in sample_loaded.items():\n            if isinstance(v, np.ndarray):\n                if v.dtype != np.float32:\n                    sample_loaded[k] = v.astype(np.float32)\n'"
loggers/Progbar.py,0,"b'# taken from Keras (https://github.com/fchollet/keras/blob/d687c6eda4d9cb58756822fd77402274db309da8/keras/utils/generic_utils.py)\nimport sys\nimport time\nimport numpy as np\n\n\nclass Progbar(object):\n    """"""Displays a progress bar.\n    # Arguments\n        target: Total number of steps expected, None if unknown.\n        interval: Minimum visual progress update interval (in seconds).\n    """"""\n\n    def __init__(self, target, width=30, verbose=1, interval=0.05):\n        self.width = width\n        if target is None:\n            target = -1\n        self.target = target\n        self.sum_values = {}\n        self.unique_values = []\n        self.start = time.time()\n        self.last_update = 0\n        self.interval = interval\n        self.total_width = 0\n        self.seen_so_far = 0\n        self.verbose = verbose\n\n    def update(self, current, values=None, force=False):\n        """"""Updates the progress bar.\n        # Arguments\n            current: Index of current step.\n            values: List of tuples (name, value_for_last_step).\n                The progress bar will display averages for these values.\n            force: Whether to force visual progress update.\n        """"""\n        values = values or []\n        for k, v in values:\n            if k not in self.sum_values:\n                self.sum_values[k] = [v * (current - self.seen_so_far),\n                                      current - self.seen_so_far]\n                self.unique_values.append(k)\n            else:\n                self.sum_values[k][0] += v * (current - self.seen_so_far)\n                self.sum_values[k][1] += (current - self.seen_so_far)\n        self.seen_so_far = current\n\n        now = time.time()\n        if self.verbose == 1:\n            if not force and (now - self.last_update) < self.interval:\n                return\n\n            prev_total_width = self.total_width\n            sys.stdout.write(\'\\b\' * prev_total_width)\n            sys.stdout.write(\'\\r\')\n\n            if self.target is not -1:\n                numdigits = int(np.floor(np.log10(self.target))) + 1\n                barstr = \'%%%dd/%%%dd [\' % (numdigits, numdigits)\n                bar = barstr % (current, self.target)\n                prog = float(current) / self.target\n                prog_width = int(self.width * prog)\n                if prog_width > 0:\n                    bar += (\'=\' * (prog_width - 1))\n                    if current < self.target:\n                        bar += \'>\'\n                    else:\n                        bar += \'=\'\n                bar += (\'.\' * (self.width - prog_width))\n                bar += \']\'\n                sys.stdout.write(bar)\n                self.total_width = len(bar)\n\n            if current:\n                time_per_unit = (now - self.start) / current\n            else:\n                time_per_unit = 0\n            eta = time_per_unit * (self.target - current)\n            info = \'\'\n            if current < self.target and self.target is not -1:\n                info += \' - ETA: %ds\' % eta\n            else:\n                info += \' - %ds\' % (now - self.start)\n            for k in self.unique_values:\n                info += \' - %s:\' % k\n                if isinstance(self.sum_values[k], list):\n                    avg = np.mean(self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                    if abs(avg) > 1e-3:\n                        info += \' %.4f\' % avg\n                    else:\n                        info += \' %.4e\' % avg\n                else:\n                    info += \' %s\' % self.sum_values[k]\n\n            self.total_width += len(info)\n            if prev_total_width > self.total_width:\n                info += ((prev_total_width - self.total_width) * \' \')\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n            if current >= self.target:\n                sys.stdout.write(\'\\n\')\n\n        if self.verbose == 2:\n            if current >= self.target:\n                info = \'%ds\' % (now - self.start)\n                for k in self.unique_values:\n                    info += \' - %s:\' % k\n                    avg = np.mean(self.sum_values[k][0] / max(1, self.sum_values[k][1]))\n                    if avg > 1e-3:\n                        info += \' %.4f\' % avg\n                    else:\n                        info += \' %.4e\' % avg\n                sys.stdout.write(info + ""\\n"")\n\n        self.last_update = now\n\n    def add(self, n, values=None):\n        self.update(self.seen_so_far + n, values)\n'"
loggers/__init__.py,0,b''
loggers/loggers.py,0,"b'# Common loggers used during training\n\n# design follows Callback from Keras\nimport os\nimport csv\nfrom collections import OrderedDict, defaultdict\nimport numpy as np\nfrom util.util_print import str_error, str_warning\nfrom .Progbar import Progbar\n\n\nclass BaseLogger(object):\n    """""" base class for all logger.\n    Each logger should expect an batch (batch index) and batch log\n    for batch end, an epoch (epoch index) and epoch log for\n    epoch end. no logs are given at batch/epoch begin, only the index.\n\n    Note: epoch_log will be used for all loggers, and should not be modified\n    in any logger\'s on_epoch_end() """"""\n\n    def __init__(self):\n        raise NotImplementedError\n\n    def on_train_begin(self):\n        pass\n\n    def on_train_end(self):\n        pass\n\n    def on_epoch_begin(self, epoch):\n        pass\n\n    def on_epoch_end(self, epoch, epoch_log):\n        pass\n\n    def on_batch_begin(self, batch):\n        pass\n\n    def on_batch_end(self, batch, batch_log):\n        pass\n\n    def set_params(self, params):\n        self.params = params\n\n    def set_model(self, model):\n        self.model = model\n\n    def train(self):\n        self.training = True\n\n    def eval(self):\n        self.training = False\n\n    def _set_unused_metric_mode(self, mode=\'none\'):\n        if mode in (\'all\', \'always\', \'both\'):\n            mode = \'all\'\n        elif mode in (\'none\', \'neither\', \'never\'):\n            mode = \'none\'\n        assert mode in (\'none\', \'train\', \'test\', \'all\')\n        self._allow_unused_metric_training = False\n        self._allow_unused_metric_testing = False\n        if mode in (\'train\', \'all\'):\n            self._allow_unused_metric_training = True\n        if mode in (\'test\', \'all\'):\n            self._allow_unused_metric_testing = True\n\n    def _allow_unused(self):\n        return self._allow_unused_metric_training if self.training else self._allow_unused_metric_testing\n\n\nclass _LogCumulator(BaseLogger):\n    """""" cumulate the batch_log and generate an epoch_log\n    Note that this logger is used for generating epoch_log,\n    and thus does not take epoch_log as input""""""\n\n    def __init__(self):\n        pass\n\n    def on_epoch_begin(self, epoch):\n        self.log_values = defaultdict(list)\n        self.sizes = list()\n        self.epoch_log = None\n\n    def on_batch_end(self, batch, batch_log):\n        for k, v in batch_log.items():\n            self.log_values[k].append(v)\n        self.sizes.append(batch_log[\'size\'])\n\n    def get_epoch_log(self):\n        epoch_log = dict()\n        for k in self.log_values:\n            epoch_log[k] = (np.array(self.log_values[k]) *\n                            np.array(self.sizes)).sum() / np.array(self.sizes).sum()\n        return epoch_log\n\n\nclass ProgbarLogger(BaseLogger):\n    """""" display a progbar """"""\n\n    def __init__(self, count_mode=\'samples\', allow_unused_fields=\'none\'):\n        if count_mode == \'samples\':\n            self.use_steps = False\n        elif count_mode == \'steps\':\n            self.use_steps = True\n        else:\n            raise ValueError(\'Unknown `count_mode`: \' + str(count_mode))\n        self._set_unused_metric_mode(allow_unused_fields)\n\n    def on_train_begin(self):\n        self.verbose = self.params[\'verbose\']\n        self.epochs = self.params[\'epochs\']\n\n    def on_epoch_begin(self, epoch):\n        if self.verbose:\n            if self.training:\n                desc = \'Epoch %d/%d\' % (epoch, self.epochs)\n                print(desc)\n                if self.use_steps:\n                    target = self.params[\'steps\']\n                else:\n                    target = self.params[\'samples\']\n                self.target = target\n                self.progbar = Progbar(target=self.target,\n                                       verbose=self.verbose)\n            else:\n                print(\'Eval %d/%d\' % (epoch, self.epochs))\n                if self.use_steps:\n                    target = self.params[\'steps_eval\']\n                else:\n                    target = self.params[\'samples_eval\']\n                self.target = target\n                self.progbar = Progbar(target=self.target,\n                                       verbose=self.verbose)\n\n        self.seen = 0\n\n    def on_batch_begin(self, batch):\n        if self.seen < self.target:\n            self.log_values = []\n\n    def on_batch_end(self, batch, batch_log):\n        if self.use_steps:\n            self.seen += 1\n        else:\n            self.seen += batch_log[\'size\']\n\n        for k in self.params[\'metrics\']:\n            if self._allow_unused() and (k not in batch_log):\n                continue\n            self.log_values.append((k, batch_log[k]))\n\n        if self.verbose and self.seen < self.target:\n            self.progbar.update(self.seen, self.log_values)\n\n    def on_epoch_end(self, epoch, epoch_log):\n        # Note: epoch_log not used\n        if self.verbose:\n            self.progbar.update(self.seen, self.log_values, force=True)\n\n\nclass CsvLogger(BaseLogger):\n    """""" loss logger to csv files """"""\n\n    def __init__(self, filename, allow_unused_fields=\'none\'):\n        self.sep = \',\'\n        self.filename = filename\n        self._set_unused_metric_mode(allow_unused_fields)\n\n    def on_train_begin(self):\n        if not os.path.isfile(self.filename):\n            newfile = True\n        else:\n            newfile = False\n        if not os.path.isdir(os.path.dirname(self.filename)):\n            os.system(\'mkdir -p \' + os.path.dirname(self.filename))\n        self.metrics = self.params[\'metrics\']\n\n        self.csv_file = open(self.filename, \'a+\')\n        self.writer = csv.DictWriter(self.csv_file, fieldnames=[\n                                     \'epoch\', \'mode\'] + self.metrics)\n        if newfile:\n            self.writer.writeheader()\n            self.csv_file.flush()\n\n    def on_epoch_end(self, epoch, epoch_log):\n        row_dict = OrderedDict(\n            {\'epoch\': epoch, \'mode\': \'train\' if self.training else \' eval\'})\n        for k in self.metrics:\n            if self._allow_unused() and (k not in epoch_log):\n                continue\n            row_dict[k] = epoch_log[k]\n        self.writer.writerow(row_dict)\n        self.csv_file.flush()\n\n    def on_train_end(self):\n        self.csv_file.close()\n        self.writer = None\n\n\nclass BatchCsvLogger(BaseLogger):\n    """""" loss logger to csv files """"""\n\n    def __init__(self, filename, allow_unused_fields=\'none\'):\n        self.sep = \',\'\n        self.filename = filename\n        self._set_unused_metric_mode(allow_unused_fields)\n\n    def on_train_begin(self):\n        if not os.path.isfile(self.filename):\n            newfile = True\n        else:\n            newfile = False\n        if not os.path.isdir(os.path.dirname(self.filename)):\n            os.system(\'mkdir -p \' + os.path.dirname(self.filename))\n        self.metrics = self.params[\'metrics\']\n\n        self.csv_file = open(self.filename, \'a+\')\n        self.writer = csv.DictWriter(self.csv_file, fieldnames=[\n                                     \'epoch\', \'mode\'] + self.metrics)\n        if newfile:\n            self.writer.writeheader()\n            self.csv_file.flush()\n\n    def on_batch_end(self, batch, batch_log=None):\n        row_dict = OrderedDict(\n            {\'epoch\': batch_log[\'epoch\'], \'mode\': \'train\' if self.training else \' eval\'})\n        for k in self.metrics:\n            if self._allow_unused() and (k not in batch_log):\n                continue\n            row_dict[k] = batch_log[k]\n        self.writer.writerow(row_dict)\n        self.csv_file.flush()\n\n    def on_train_end(self):\n        self.csv_file.close()\n        self.writer = None\n\n\nclass ModelSaveLogger(BaseLogger):\n    """"""\n    A logger that saves model periodically.\n    The logger can be configured to save the model with the best eval score.\n    """"""\n\n    def __init__(self, filepath, period=1, save_optimizer=False, save_best=False, prev_best=None):\n        self.filepath = filepath\n        self.period = period\n        self.save_optimizer = save_optimizer\n        self.save_best = save_best\n        self.loss_name = \'loss\'\n        self.current_best_eval = prev_best\n        self.current_best_epoch = None\n\n        # search for previous best\n        if self.save_best and prev_best is None:\n            # try:\n            #     # parse epoch_loss. overwrite previous best if fail\n            #     if os.path.isfile(filepath):\n            #         prev_loss = pd.read_csv(os.path.join(os.path.dirname(filepath), \'epoch_loss.csv\'))\n            #         prev_eval_loss = prev_loss[prev_loss[\'mode\'] == \'eval\']\n            #         if prev_eval_loss.size == 0:\n            #             raise ValueError(\'loaded epoch loss file has no eval loss\')\n            #         self.current_best_eval = prev_eval_loss[self.loss_name].min()\n            #         self.current_best_epoch = prev_eval_loss[prev_eval_loss[self.loss_name] == self.current_best_eval][\'epoch\'].iloc[0]\n            # except: # (IOError, pd.errors.ParserError, KeyError):\n            print(\n                str_warning, \'Previous best eval loss not given. Best validation model WILL be overwritten.\')\n\n    def on_train_begin(self):\n        if not os.path.isdir(self.filepath):\n            os.system(\'mkdir -p \' + os.path.dirname(self.filepath))\n        self.epochs_since_last_save = 0\n\n    def on_epoch_end(self, epoch, epoch_log):\n        # avoid saving twice (once after training, once after eval)\n        if self.training:\n            if self.save_best:  # save_best mode is not used right after training\n                return\n            self.epochs_since_last_save += 1\n            if self.epochs_since_last_save >= self.period:\n                filepath = self.filepath.format(epoch=epoch)\n                self.model.save_state_dict(\n                    filepath, save_optimizer=self.save_optimizer, additional_values={\'epoch\': epoch})\n                self.epochs_since_last_save = 0\n        else:\n            if self.save_best:\n                if self.loss_name not in epoch_log:\n                    print(\n                        str_warning, \'Loss name %s not found in batch_log. ""Best model saving"" is turned off""\' % self.loss_name)\n                else:\n                    current_eval = epoch_log[\'loss\']\n                    if self.current_best_eval is None or current_eval < self.current_best_eval:\n                        self.current_best_eval = current_eval\n                        self.current_best_epoch = epoch\n                        filepath = self.filepath.format(epoch=epoch)\n                        self.model.save_state_dict(filepath, save_optimizer=self.save_optimizer, additional_values={\n                                                   \'epoch\': epoch, \'loss_eval\': self.current_best_eval})\n\n\nclass TerminateOnNaN(BaseLogger):\n    def __init__(self):\n        self._training = True\n\n    def on_batch_begin(self, batch):\n        if not self._training:\n            raise ValueError(str_error, \'inf/nan found\')\n\n    def on_batch_end(self, batch, batch_log):\n        if batch_log:\n            for k, v in batch_log.items():\n                if np.isnan(v): # or np.isinf(v):\n                    self._training = False\n                    break\n\n\nclass TensorBoardLogger(BaseLogger):\n    def __init__(self, filepath, allow_unused_fields=\'none\'):\n        try:\n            import tensorflow as tf\n            self.tf = tf\n        except Exception as err:\n            print(str_warning, ""TensorBoard logger disabled due to an error while importing tensorflow: \\n%s"" % str(err))\n            self.tf = None\n        self.filepath = filepath\n        self._set_unused_metric_mode(allow_unused_fields)\n\n    def on_train_begin(self):\n        if not self.tf:\n            return\n        if not os.path.isdir((self.filepath)):\n            os.system(\'mkdir -p \' + (self.filepath))\n        self.metrics = self.params[\'metrics\']\n        self.writer_train = None\n        self.writer_test = None\n\n    def on_epoch_end(self, epoch, epoch_log):\n        if not self.tf:\n            return\n        else:\n            tf = self.tf\n        if self.training:\n            if not self.writer_train:\n                self.writer_train = tf.summary.FileWriter(os.path.join(self.filepath, \'train\'))\n            writer = self.writer_train\n        else:\n            if not self.writer_test:\n                self.writer_test = tf.summary.FileWriter(os.path.join(self.filepath, \'eval\'))\n            writer = self.writer_test\n\n        row_dict = dict()\n        for k in self.metrics:\n            if self._allow_unused() and (k not in epoch_log):\n                continue\n            row_dict[k] = epoch_log[k]\n\n        summary = tf.Summary(value=[tf.Summary.Value(tag=k, simple_value=v) for k, v in row_dict.items()])\n        writer.add_summary(summary, epoch)\n        writer.flush()\n\n    def on_train_end(self):\n        if not self.tf:\n            return\n        if self.writer_train:\n            self.writer_train.flush()\n            self.writer_train = None\n        if self.writer_test:\n            self.writer_test.flush()\n            self.writer_test = None\n\n\nclass ComposeLogger(BaseLogger):\n    """""" loss logger to csv files """"""\n\n    def __init__(self, loggers):\n        self.loggers = loggers\n        self.params = None\n        self.model = None\n        self._in_training = False\n\n    def add_logger(self, logger):\n        assert not self._in_training, str_error + \\\n            \' Unsafe to add logger during training\'\n        self.loggers.append(logger)\n\n    def on_train_begin(self):\n        self._in_training = True\n        for logger in self.loggers:\n            logger.on_train_begin()\n\n    def on_train_end(self):\n        self._in_training = False\n        for logger in self.loggers:\n            logger.on_train_end()\n\n    def on_epoch_begin(self, epoch):\n        for logger in self.loggers:\n            logger.on_epoch_begin(epoch)\n\n    def on_epoch_end(self, epoch, epoch_log):\n        for logger in self.loggers:\n            logger.on_epoch_end(epoch, epoch_log)\n\n    def on_batch_begin(self, batch):\n        for logger in self.loggers:\n            logger.on_batch_begin(batch)\n\n    def on_batch_end(self, batch, batch_log):\n        for logger in self.loggers:\n            logger.on_batch_end(batch, batch_log)\n\n    def set_params(self, params):\n        self.params = params\n        for logger in self.loggers:\n            logger.set_params(params)\n\n    def set_model(self, model):\n        self.model = model\n        for logger in self.loggers:\n            logger.set_model(model)\n\n    def train(self):\n        self.training = True\n        for logger in self.loggers:\n            logger.train()\n\n    def eval(self):\n        self.training = False\n        for logger in self.loggers:\n            logger.eval()\n\n\n################################################\n# Test BatchLogger, CsvLogger and ProgbarLogger\nif __name__ == \'__main__\':\n    test_logdir = \'./test_logger_dir\'\n    if os.path.isdir(test_logdir):\n        os.system(\'rm -r \' + test_logdir)\n    internal_logger = _LogCumulator()\n    logger = ComposeLogger([internal_logger, ProgbarLogger(), BatchCsvLogger(\n        test_logdir + \'/batch_loss.csv\'), CsvLogger(test_logdir + \'/epoch_loss.csv\')])\n    logger.set_params({\n        \'epochs\': 5,\n        \'steps\': 20,\n        \'steps_eval\': 5,\n        \'samples\': 100,\n        \'samples_eval\': 25,\n        \'verbose\': 1,\n        \'metrics\': [\'loss\']\n    })\n    logger.on_train_begin()\n    for epoch in range(5):\n        logger.train()\n        logger.on_epoch_begin(epoch)\n        for i in range(logger.params[\'steps\']):\n            logger.on_batch_begin(i)\n            batch_log = {\'batch\': i, \'epoch\': epoch, \'loss\': np.random.rand(\n                1)[0], \'size\': np.random.randint(9) + 1}\n            logger.on_batch_end(i, batch_log)\n        epoch_log = internal_logger.get_epoch_log()\n        logger.on_epoch_end(epoch, epoch_log)\n\n        logger.eval()\n        logger.on_epoch_begin(epoch)\n        for i in range(logger.params[\'steps_eval\']):\n            logger.on_batch_begin(i)\n            batch_log = {\'batch\': i, \'epoch\': epoch,\n                         \'loss\': np.random.rand(1)[0], \'size\': 5}\n            logger.on_batch_end(i, batch_log)\n        epoch_log = internal_logger.get_epoch_log()\n        logger.on_epoch_end(epoch, epoch_log)\n    logger.on_train_end()\n'"
models/__init__.py,0,"b""import importlib\n\n\ndef get_model(alias, test=False):\n    module = importlib.import_module('models.' + alias)\n    if test:\n        return module.Model_test\n    return module.Model\n"""
models/depth_pred_with_sph_inpaint.py,6,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nfrom models.marrnet1 import Model as DepthModel\nfrom models.marrnet1 import Net as Net1\nfrom networks.uresnet import Net_inpaint as Uresnet\nfrom toolbox.cam_bp.cam_bp.modules.camera_backprojection_module import Camera_back_projection_layer\nfrom toolbox.spherical_proj import render_spherical, sph_pad\nimport torch.nn.functional as F\n\n\nclass Model(DepthModel):\n    @classmethod\n    def add_arguments(cls, parser):\n        parser.add_argument(\'--pred_depth_minmax\', action=\'store_true\', default=True,\n                            help=""GenRe needs minmax prediction"")\n        parser.add_argument(\'--load_offline\', action=\'store_true\',\n                            help=""load offline prediction results"")\n        parser.add_argument(\'--joint_train\', action=\'store_true\',\n                            help=""joint train net1 and net2"")\n        parser.add_argument(\'--net1_path\', default=None, type=str,\n                            help=""path to pretrained net1"")\n        parser.add_argument(\'--padding_margin\', default=16, type=int,\n                            help=""padding margin for spherical maps"")\n        unique_params = {\'joint_train\'}\n        return parser, unique_params\n\n    def __init__(self, opt, logger):\n        super(Model, self).__init__(opt, logger)\n        self.joint_train = opt.joint_train\n        if not self.joint_train:\n            self.requires = [\'silhou\', \'rgb\', \'spherical\']\n            self.gt_names = [\'spherical_object\']\n            self._metrics = [\'spherical\']\n        else:\n            self.requires.append(\'spherical\')\n            self.gt_names = [\'depth\', \'silhou\', \'normal\', \'depth_minmax\', \'spherical_object\']\n            self._metrics.append(\'spherical\')\n        self.input_names = [\'rgb\', \'silhou\', \'spherical_depth\']\n        self.net = Net(opt, Model)\n        self.optimizer = self.adam(\n            self.net.parameters(),\n            lr=opt.lr,\n            **self.optim_params\n        )\n        self._nets = [self.net]\n        self._optimizers = [self.optimizer]\n        self.init_vars(add_path=True)\n        self.init_weight(self.net.net2)\n\n    def __str__(self):\n        string = ""Depth Prediction with Spherical Refinement""\n        if self.joint_train:\n            string += \' Jointly training all the modules.\'\n        else:\n            string += \' Only training the inpainting module.\'\n        return string\n\n    def compute_loss(self, pred):\n        loss_data = {}\n        loss = 0\n        if self.joint_train:\n            loss, loss_data = super(Model, self).compute_loss(pred)\n        sph_loss = F.mse_loss(pred[\'pred_sph_full\'], self._gt.spherical_object)\n        loss_data[\'spherical\'] = sph_loss.mean().item()\n        loss += sph_loss\n        loss_data[\'loss\'] = loss.mean().item()\n        return loss, loss_data\n\n    def pack_output(self, pred, batch, add_gt=True):\n        pack = {}\n        if self.joint_train:\n            pack = super(Model, self).pack_output(pred, batch, add_gt=False)\n        pack[\'pred_spherical_full\'] = pred[\'pred_sph_full\'].data.cpu().numpy()\n        pack[\'pred_spherical_partial\'] = pred[\'pred_sph_partial\'].data.cpu().numpy()\n        pack[\'proj_depth\'] = pred[\'proj_depth\'].data.cpu().numpy()\n        pack[\'rgb_path\'] = batch[\'rgb_path\']\n        if add_gt:\n            pack[\'gt_spherical_full\'] = batch[\'spherical_object\'].numpy()\n        return pack\n\n    @classmethod\n    def preprocess(cls, data, mode=\'train\'):\n        dataout = DepthModel.preprocess(data, mode)\n        if \'spherical_object\' in dataout.keys():\n            val = dataout[\'spherical_object\']\n            assert(val.shape[1] == val.shape[2])\n            assert(val.shape[1] == 128)\n            sph_padded = np.pad(val, ((0, 0), (0, 0), (16, 16)), \'wrap\')\n            sph_padded = np.pad(sph_padded, ((0, 0), (16, 16), (0, 0)), \'edge\')\n            dataout[\'spherical_object\'] = sph_padded\n        return dataout\n\n\nclass Net(nn.Module):\n    def __init__(self, opt, base_class=Model):\n        super().__init__()\n        self.net1 = Net1(\n            [3, 1, 1],\n            [\'normal\', \'depth\', \'silhou\'],\n            pred_depth_minmax=True)\n        self.net2 = Uresnet([1], [\'spherical\'], input_planes=1)\n        self.base_class = base_class\n        self.proj_depth = Camera_back_projection_layer()\n        self.render_spherical = render_spherical()\n        self.joint_train = opt.joint_train\n        self.load_offline = opt.load_offline\n        self.padding_margin = opt.padding_margin\n        if opt.net1_path:\n            state_dicts = torch.load(opt.net1_path)\n            self.net1.load_state_dict(state_dicts[\'nets\'][0])\n\n    def forward(self, input_struct):\n        if not self.joint_train:\n            with torch.no_grad():\n                out_1 = self.net1(input_struct)\n        else:\n            out_1 = self.net1(input_struct)\n        pred_abs_depth = self.get_abs_depth(out_1, input_struct)\n        proj = self.proj_depth(pred_abs_depth)\n        if self.load_offline:\n            sph_in = input_struct.spherical_depth\n        else:\n            sph_in = self.render_spherical(torch.clamp(proj * 50, 1e-5, 1 - 1e-5))\n        # pad sph_in to approximate boundary conditions\n        sph_in = sph_pad(sph_in, self.padding_margin)\n        out_2 = self.net2(sph_in)\n        out_1[\'proj_depth\'] = proj * 50\n        out_1[\'pred_sph_partial\'] = sph_in\n        out_1[\'pred_sph_full\'] = out_2[\'spherical\']\n        return out_1\n\n    def get_abs_depth(self, pred, input_struct):\n        pred_depth = pred[\'depth\']\n        pred_depth = self.base_class.postprocess(pred_depth)\n        pred_depth_minmax = pred[\'depth_minmax\'].detach()\n        pred_abs_depth = self.base_class.to_abs_depth(1 - pred_depth, pred_depth_minmax)\n        silhou = self.base_class.postprocess(input_struct.silhou).detach()\n        pred_abs_depth[silhou < 0.5] = 0\n        pred_abs_depth = pred_abs_depth.permute(0, 1, 3, 2)\n        pred_abs_depth = torch.flip(pred_abs_depth, [2])\n        return pred_abs_depth\n'"
models/genre_full_model.py,17,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nfrom scipy.ndimage.morphology import binary_erosion\nfrom models.depth_pred_with_sph_inpaint import Net as Depth_inpaint_net\nfrom models.depth_pred_with_sph_inpaint import Model as DepthInpaintModel\nfrom networks.networks import Unet_3D\nfrom toolbox.cam_bp.cam_bp.modules.camera_backprojection_module import Camera_back_projection_layer\nfrom toolbox.cam_bp.cam_bp.functions import SphericalBackProjection\nfrom toolbox.spherical_proj import gen_sph_grid\nfrom os import makedirs\nfrom os.path import join\nfrom util import util_img\nfrom util import util_sph\nimport torch.nn.functional as F\nfrom toolbox.spherical_proj import sph_pad\n\n\nclass Model(DepthInpaintModel):\n    @classmethod\n    def add_arguments(cls, parser):\n        parser, unique_params = DepthInpaintModel.add_arguments(parser)\n        parser.add_argument(\'--inpaint_path\', default=None, type=str,\n                            help=""path to pretrained inpainting module"")\n        parser.add_argument(\'--surface_weight\', default=1.0, type=float,\n                            help=""weight for voxel surface prediction"")\n        unique_params_model = {\'surface_weight\', \'joint_train\', \'inpaint_path\'}\n        return parser, unique_params.union(unique_params_model)\n\n    def __init__(self, opt, logger):\n        super(Model, self).__init__(opt, logger)\n        self.joint_train = opt.joint_train\n        if self.joint_train:\n            self.requires.append(\'voxel\')\n        else:\n            self.requires = [\'rgb\', \'silhou\', \'voxel\']\n        self.gt_names.append(\'voxel\')\n        self._metrics += [\'voxel_loss\', \'surface_loss\']\n        self.net = Net(opt, Model)\n        self.optimizer = self.adam(\n            self.net.parameters(),\n            lr=opt.lr,\n            **self.optim_params\n        )\n        self._nets = [self.net]\n        self._optimizers = [self.optimizer]\n        self.init_vars(add_path=True)\n        if not self.joint_train:\n            self.init_weight(self.net.refine_net)\n\n    def __str__(self):\n        string = ""Full model of GenRe.""\n        if self.joint_train:\n            string += \' Jointly training all the modules.\'\n        else:\n            string += \' Only training the refinement module\'\n        return string\n\n    def compute_loss(self, pred):\n        loss = 0\n        loss_data = {}\n        if self.joint_train:\n            loss, loss_data = super(Model, self).compute_loss(pred)\n        voxel_loss = F.binary_cross_entropy_with_logits(pred[\'pred_voxel\'], self._gt.voxel)\n        sigmoid_voxel = torch.sigmoid(pred[\'pred_voxel\'])\n        surface_loss = F.binary_cross_entropy(sigmoid_voxel * self._gt.voxel, self._gt.voxel)\n        loss += voxel_loss.mean()\n        loss += surface_loss.mean() * self.opt.surface_weight\n        loss_data[\'voxel_loss\'] = voxel_loss.mean().item()\n        loss_data[\'surface_loss\'] = surface_loss.mean().item() * self.opt.surface_weight\n        loss_data[\'loss\'] = loss.mean().item()\n        return loss, loss_data\n\n    def pack_output(self, pred, batch, add_gt=True):\n        pack = {}\n        if self.joint_train:\n            pack = super(Model, self).pack_output(pred, batch, add_gt=add_gt)\n        pack[\'pred_voxel\'] = pred[\'pred_voxel\'].cpu().numpy()\n        pack[\'pred_proj_sph_partial\'] = pred[\'pred_voxel\'].cpu().numpy()\n        pack[\'pred_proj_depth\'] = pred[\'pred_proj_depth\'].cpu().numpy()\n        pack[\'pred_proj_sph_full\'] = pred[\'pred_proj_sph_full\'].cpu().numpy()\n        if add_gt:\n            pack[\'gt_voxel\'] = batch[\'voxel\'].numpy()\n        return pack\n\n    @classmethod\n    def preprocess(cls, data, mode=\'train\'):\n        dataout = DepthInpaintModel.preprocess(data, mode)\n        if \'voxel\' in dataout:\n            val = dataout[\'voxel\'][0, :, :, :]\n            val = np.transpose(val, (0, 2, 1))\n            val = np.flip(val, 2)\n            voxel_surface = val - binary_erosion(val, structure=np.ones((3, 3, 3)), iterations=2).astype(float)\n            voxel_surface = voxel_surface[None, ...]\n            voxel_surface = np.clip(voxel_surface, 0, 1)\n            dataout[\'voxel\'] = voxel_surface\n        return dataout\n\n\nclass Net(nn.Module):\n    def __init__(self, opt, base_class):\n        super().__init__()\n        self.base_class = base_class\n        self.depth_and_inpaint = Depth_inpaint_net(opt, base_class)\n        self.refine_net = Unet_3D()\n        self.proj_depth = Camera_back_projection_layer()\n        self.joint_train = opt.joint_train\n        self.register_buffer(\'grid\', gen_sph_grid())\n        self.grid = self.grid.expand(1, -1, -1, -1, -1)\n        self.proj_spherical = SphericalBackProjection().apply\n        self.margin = opt.padding_margin\n        if opt.inpaint_path is not None:\n            state_dicts = torch.load(opt.inpaint_path)\n            self.depth_and_inpaint.load_state_dict(state_dicts[\'nets\'][0])\n\n    def forward(self, input_struct):\n        if not self.joint_train:\n            with torch.no_grad():\n                out_1 = self.depth_and_inpaint(input_struct)\n        else:\n            out_1 = self.depth_and_inpaint(input_struct)\n        # use proj_depth and sph_in\n        proj_depth = out_1[\'proj_depth\']\n        pred_sph = out_1[\'pred_sph_full\']\n        pred_proj_sph = self.backproject_spherical(pred_sph)\n        proj_depth = torch.clamp(proj_depth / 50, 1e-5, 1 - 1e-5)\n        refine_input = torch.cat((pred_proj_sph, proj_depth), dim=1)\n        pred_voxel = self.refine_net(refine_input)\n        out_1[\'pred_proj_depth\'] = proj_depth\n        out_1[\'pred_voxel\'] = pred_voxel\n        out_1[\'pred_proj_sph_full\'] = pred_proj_sph\n        return out_1\n\n    def backproject_spherical(self, sph):\n        batch_size, _, h, w = sph.shape\n        grid = self.grid[0, :, :, :, :]\n        grid = grid.expand(batch_size, -1, -1, -1, -1)\n        crop_sph = sph[:, :, self.margin:h - self.margin, self.margin:w - self.margin]\n        proj_df, cnt = self.proj_spherical(1 - crop_sph, grid, 128)\n        mask = torch.clamp(cnt.detach(), 0, 1)\n        proj_df = (-proj_df + 1 / 128) * 128\n        proj_df = proj_df * mask\n        return proj_df\n\n\nclass Model_test(Model):\n    def __init__(self, opt, logger):\n        super().__init__(opt, logger)\n        self.requires = [\'rgb\', \'mask\']  # mask for bbox cropping only\n        self.input_names = [\'rgb\']\n        self.init_vars(add_path=True)\n        self.load_state_dict(opt.net_file, load_optimizer=\'auto\')\n        self.output_dir = opt.output_dir\n        self.input_names.append(\'silhou\')\n\n    def __str__(self):\n        return ""Testing GenRe""\n\n    @classmethod\n    def preprocess_wrapper(cls, in_dict):\n        silhou_thres = 0.95\n        in_size = 480\n        pad = 85\n        im = in_dict[\'rgb\']\n        mask = in_dict[\'silhou\']\n        bbox = util_img.get_bbox(mask, th=silhou_thres)\n        im_crop = util_img.crop(im, bbox, in_size, pad, pad_zero=False)\n        silhou_crop = util_img.crop(in_dict[\'silhou\'], bbox, in_size, pad, pad_zero=False)\n        in_dict[\'rgb\'] = im_crop\n        in_dict[\'silhou\'] = silhou_crop\n        # Now the image is just like those we rendered\n        out_dict = cls.preprocess(in_dict, mode=\'test\')\n        return out_dict\n\n    def test_on_batch(self, batch_i, batch, use_trimesh=True):\n        outdir = join(self.output_dir, \'batch%04d\' % batch_i)\n        makedirs(outdir, exist_ok=True)\n        if not use_trimesh:\n            pred = self.predict(batch, load_gt=False, no_grad=True)\n        else:\n            assert self.opt.batch_size == 1\n            pred = self.forward_with_trimesh(batch)\n\n        output = self.pack_output(pred, batch, add_gt=False)\n        self.visualizer.visualize(output, batch_i, outdir)\n        np.savez(outdir + \'.npz\', **output)\n\n    def pack_output(self, pred, batch, add_gt=True):\n        pack = {}\n        pack[\'pred_voxel\'] = pred[\'pred_voxel\'].cpu().numpy()\n        pack[\'rgb_path\'] = batch[\'rgb_path\']\n        #pack[\'pred_proj_depth\'] = pred[\'pred_proj_depth\'].cpu().numpy()\n        #pack[\'pred_proj_sph_full\'] = pred[\'pred_proj_sph_full\'].cpu().numpy()\n        #pack[\'pred_sph_partial\'] = pred[\'pred_sph_partial\'].cpu().numpy()\n        #pack[\'pred_depth\'] = pred[\'pred_depth\'].cpu().numpy()\n        #pack[\'pred_depth_minmax\'] = pred[\'depth_minmax\'].cpu().numpy()\n        #pack[\'pred__minmax\'] = pred[\'depth_minmax\'].cpu().numpy()\n        if add_gt:\n            pack[\'gt_voxel\'] = batch[\'voxel\'].numpy()\n        return pack\n\n    def forward_with_trimesh(self, batch):\n        self.load_batch(batch, include_gt=False)\n        with torch.no_grad():\n            pred_1 = self.net.depth_and_inpaint.net1.forward(self._input)\n        pred_abs_depth = self.net.depth_and_inpaint.get_abs_depth(pred_1, self._input)\n        proj = self.net.depth_and_inpaint.proj_depth(pred_abs_depth)\n        pred_depth = self.net.depth_and_inpaint.base_class.postprocess(pred_1[\'depth\'].detach())\n        silhou = self.net.base_class.postprocess(self._input.silhou).detach()\n        pred_depth = pred_depth.cpu().numpy()\n        pred_depth_minmax = pred_1[\'depth_minmax\'].detach().cpu().numpy()[0, :]\n        silhou = silhou.cpu().numpy()[0, 0, :, :]\n        pack = {\'depth\': pred_depth, \'depth_minmax\': pred_depth_minmax}\n        rendered_sph = util_sph.render_spherical(pack, silhou)[None, None, ...]\n        rendered_sph = torch.from_numpy(rendered_sph).float().cuda()\n        rendered_sph = sph_pad(rendered_sph)\n        with torch.no_grad():\n            out2 = self.net.depth_and_inpaint.net2(rendered_sph)\n        pred_proj_sph = self.net.backproject_spherical(out2[\'spherical\'])\n        pred_proj_sph = torch.transpose(pred_proj_sph, 3, 4)\n        pred_proj_sph = torch.flip(pred_proj_sph, [3])\n        proj = torch.transpose(proj, 3, 4)\n        proj = torch.flip(proj, [3])\n\n        refine_input = torch.cat((pred_proj_sph, proj), dim=1)\n        with torch.no_grad():\n            pred_voxel = self.net.refine_net(refine_input)\n        pred_1[\'pred_sph_full\'] = out2[\'spherical\']\n        pred_1[\'pred_sph_partial\'] = rendered_sph\n        pred_1[\'pred_proj_depth\'] = proj\n        pred_1[\'pred_voxel\'] = pred_voxel.flip([3]).transpose(3, 4)\n        pred_1[\'pred_proj_sph_full\'] = pred_proj_sph\n        return pred_1\n'"
models/marrnet.py,6,"b'from os import makedirs\nfrom os.path import join\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom util import util_img\nfrom .marrnet1 import Net as Marrnet1\nfrom .marrnet2 import Net as Marrnet2, Model as Marrnet2_model\n\n\nclass Model(Marrnet2_model):\n    @classmethod\n    def add_arguments(cls, parser):\n        parser.add_argument(\n            \'--canon_sup\',\n            action=\'store_true\',\n            help=""Use canonical-pose voxels as supervision""\n        )\n        parser.add_argument(\n            \'--marrnet1\',\n            type=str, default=None,\n            help=""Path to pretrained MarrNet-1""\n        )\n        parser.add_argument(\n            \'--marrnet2\',\n            type=str, default=None,\n            help=""Path to pretrained MarrNet-2 (to be finetuned)""\n        )\n        return parser, set()\n\n    def __init__(self, opt, logger):\n        super().__init__(opt, logger)\n        pred_silhou_thres = self.pred_silhou_thres * self.scale_25d\n        self.requires = [\'rgb\', self.voxel_key]\n        self.net = Net(opt.marrnet1, opt.marrnet2, pred_silhou_thres)\n        self._nets = [self.net]\n        self.optimizer = self.adam(\n            self.net.marrnet2.parameters(),\n            lr=opt.lr,\n            **self.optim_params\n        ) # just finetune MarrNet-2\n        self._optimizers[-1] = self.optimizer\n        self.input_names = [\'rgb\']\n        self.init_vars(add_path=True)\n\n    def __str__(self):\n        return ""Finetuning MarrNet-2 with MarrNet-1 predictions""\n\n    def pack_output(self, pred, batch, add_gt=True):\n        pred_normal = pred[\'normal\'].detach().cpu()\n        pred_silhou = pred[\'silhou\'].detach().cpu()\n        pred_depth = pred[\'depth\'].detach().cpu()\n        out = {}\n        out[\'rgb_path\'] = batch[\'rgb_path\']\n        out[\'rgb\'] = util_img.denormalize_colors(batch[\'rgb\'].detach().numpy())\n        pred_silhou = self.postprocess(pred_silhou)\n        pred_silhou = torch.clamp(pred_silhou, 0, 1)\n        pred_silhou[pred_silhou < 0] = 0\n        out[\'pred_silhou\'] = pred_silhou.numpy()\n        out[\'pred_normal\'] = self.postprocess(\n            pred_normal, bg=1.0, input_mask=pred_silhou\n        ).numpy()\n        out[\'pred_depth\'] = self.postprocess(\n            pred_depth, bg=0.0, input_mask=pred_silhou\n        ).numpy()\n        out[\'pred_voxel\'] = pred[\'voxel\'].detach().cpu().numpy()\n        if add_gt:\n            out[\'gt_voxel\'] = batch[self.voxel_key].numpy()\n        return out\n\n    def compute_loss(self, pred):\n        loss = self.criterion(\n            pred[\'voxel\'],\n            getattr(self._gt, self.voxel_key)\n        )\n        loss_data = {}\n        loss_data[\'loss\'] = loss.mean().item()\n        return loss, loss_data\n\n\nclass Net(nn.Module):\n    """"""\n       MarrNet-1    MarrNet-2\n    RGB ------> 2.5D ------> 3D\n         fixed      finetuned\n    """"""\n\n    def __init__(self, marrnet1_path=None, marrnet2_path=None, pred_silhou_thres=0.3):\n        super().__init__()\n        # Init MarrNet-1 and load weights\n        self.marrnet1 = Marrnet1(\n            [3, 1, 1],\n            [\'normal\', \'depth\', \'silhou\'],\n            pred_depth_minmax=True, # not used in MarrNet\n        )\n        if marrnet1_path:\n            state_dict = torch.load(marrnet1_path)[\'nets\'][0]\n            self.marrnet1.load_state_dict(state_dict)\n        # Init MarrNet-2 and load weights\n        self.marrnet2 = Marrnet2(4)\n        if marrnet2_path:\n            state_dict = torch.load(marrnet2_path)[\'nets\'][0]\n            self.marrnet2.load_state_dict(state_dict)\n        # Fix MarrNet-1, but finetune 2\n        for p in self.marrnet1.parameters():\n            p.requires_grad = False\n        for p in self.marrnet2.parameters():\n            p.requires_grad = True\n        self.pred_silhou_thres = pred_silhou_thres\n\n    def forward(self, input_struct):\n        # Predict 2.5D sketches\n        with torch.no_grad():\n            pred = self.marrnet1(input_struct)\n        depth = pred[\'depth\']\n        normal = pred[\'normal\']\n        silhou = pred[\'silhou\']\n        # Mask\n        is_bg = silhou < self.pred_silhou_thres\n        depth[is_bg] = 0\n        normal[is_bg.repeat(1, 3, 1, 1)] = 0\n        x = torch.cat((depth, normal), 1)\n        # Forward\n        latent_vec = self.marrnet2.encoder(x)\n        vox = self.marrnet2.decoder(latent_vec)\n        pred[\'voxel\'] = vox\n        return pred\n\n\nclass Model_test(Model):\n    def __init__(self, opt, logger):\n        super().__init__(opt, logger)\n        self.requires = [\'rgb\', \'mask\'] # mask for bbox cropping only\n        self.load_state_dict(opt.net_file, load_optimizer=\'auto\')\n        self.input_names = [\'rgb\']\n        self.init_vars(add_path=True)\n        self.output_dir = opt.output_dir\n\n    def __str__(self):\n        return ""Testing MarrNet""\n\n    @classmethod\n    def preprocess_wrapper(cls, in_dict):\n        silhou_thres = 0.95\n        in_size = 480\n        pad = 85\n        im = in_dict[\'rgb\']\n        mask = in_dict[\'silhou\']\n        bbox = util_img.get_bbox(mask, th=silhou_thres)\n        im_crop = util_img.crop(im, bbox, in_size, pad, pad_zero=False)\n        in_dict[\'rgb\'] = im_crop\n        del in_dict[\'silhou\'] # just for cropping -- done its job\n        # Now the image is just like those we rendered\n        out_dict = cls.preprocess(in_dict, mode=\'test\')\n        return out_dict\n\n    def test_on_batch(self, batch_i, batch):\n        outdir = join(self.output_dir, \'batch%04d\' % batch_i)\n        makedirs(outdir, exist_ok=True)\n        pred = self.predict(batch, load_gt=False, no_grad=True)\n        output = self.pack_output(pred, batch, add_gt=False)\n        self.visualizer.visualize(output, batch_i, outdir)\n        np.savez(outdir + \'.npz\', **output)\n'"
models/marrnet1.py,2,"b'from os import makedirs\nfrom os.path import join\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom networks.networks import ViewAsLinear\nfrom networks.uresnet import Net as Uresnet\nfrom .marrnetbase import MarrnetBaseModel\n\n\nclass Model(MarrnetBaseModel):\n    @classmethod\n    def add_arguments(cls, parser):\n        parser.add_argument(\n            \'--pred_depth_minmax\',\n            action=\'store_true\',\n            help=""Also predicts depth minmax (for GenRe)"",\n        )\n        return parser, set()\n\n    def __init__(self, opt, logger):\n        super(Model, self).__init__(opt, logger)\n        self.requires = [\'rgb\', \'depth\', \'silhou\', \'normal\']\n        if opt.pred_depth_minmax:\n            self.requires.append(\'depth_minmax\')\n        self.net = Net(\n            [3, 1, 1],\n            [\'normal\', \'depth\', \'silhou\'],\n            pred_depth_minmax=opt.pred_depth_minmax,\n        )\n        self.criterion = nn.functional.mse_loss\n        self.optimizer = self.adam(\n            self.net.parameters(),\n            lr=opt.lr,\n            **self.optim_params\n        )\n        self._nets = [self.net]\n        self._optimizers.append(self.optimizer)\n        self.input_names = [\'rgb\']\n        self.gt_names = [\'depth\', \'silhou\', \'normal\']\n        if opt.pred_depth_minmax:\n            self.gt_names.append(\'depth_minmax\')\n        self.init_vars(add_path=True)\n        self._metrics = [\'loss\', \'depth\', \'silhou\', \'normal\']\n        if opt.pred_depth_minmax:\n            self._metrics.append(\'depth_minmax\')\n        self.init_weight(self.net)\n\n    def __str__(self):\n        return ""MarrNet-1 predicting 2.5D sketches""\n\n    def _train_on_batch(self, epoch, batch_idx, batch):\n        self.net.zero_grad()\n        pred = self.predict(batch)\n        loss, loss_data = self.compute_loss(pred)\n        loss.backward()\n        self.optimizer.step()\n        batch_size = len(batch[\'rgb_path\'])\n        batch_log = {\'size\': batch_size, **loss_data}\n        return batch_log\n\n    def _vali_on_batch(self, epoch, batch_idx, batch):\n        pred = self.predict(batch, no_grad=True)\n        _, loss_data = self.compute_loss(pred)\n        if np.mod(epoch, self.opt.vis_every_vali) == 0:\n            if batch_idx < self.opt.vis_batches_vali:\n                outdir = join(self.full_logdir, \'epoch%04d_vali\' % epoch)\n                makedirs(outdir, exist_ok=True)\n                output = self.pack_output(pred, batch)\n                self.visualizer.visualize(output, batch_idx, outdir)\n                np.savez(join(outdir, \'batch%04d\' % batch_idx), **output)\n        batch_size = len(batch[\'rgb_path\'])\n        batch_log = {\'size\': batch_size, **loss_data}\n        return batch_log\n\n    def pack_output(self, pred, batch, add_gt=True):\n        pred_normal = pred[\'normal\'].detach().cpu()\n        pred_silhou = pred[\'silhou\'].detach().cpu()\n        pred_depth = pred[\'depth\'].detach().cpu()\n        gt_silhou = self.postprocess(batch[\'silhou\'])\n        out = {}\n        out[\'rgb_path\'] = batch[\'rgb_path\']\n        out[\'pred_normal\'] = self.postprocess(pred_normal, bg=1.0, input_mask=gt_silhou).numpy()\n        out[\'pred_silhou\'] = self.postprocess(pred_silhou).numpy()\n        pred_depth = self.postprocess(pred_depth, bg=0.0, input_mask=gt_silhou)\n        out[\'pred_depth\'] = pred_depth.numpy()\n        if self.opt.pred_depth_minmax:\n            pred_depth_minmax = pred[\'depth_minmax\'].detach()\n            pred_abs_depth = self.to_abs_depth(\n                (1 - pred_depth).to(torch.device(\'cuda\')),\n                pred_depth_minmax\n            )  # background is max now\n            pred_abs_depth[gt_silhou < 1] = 0  # set background to 0\n            out[\'proj_depth\'] = self.proj_depth(pred_abs_depth).cpu().numpy()\n            out[\'pred_depth_minmax\'] = pred_depth_minmax.cpu().numpy()\n        if add_gt:\n            out[\'normal_path\'] = batch[\'normal_path\']\n            out[\'silhou_path\'] = batch[\'silhou_path\']\n            out[\'depth_path\'] = batch[\'depth_path\']\n            if self.opt.pred_depth_minmax:\n                out[\'gt_depth_minmax\'] = batch[\'depth_minmax\'].numpy()\n        return out\n\n    def compute_loss(self, pred):\n        """"""\n        TODO: we should add normal and depth consistency loss here in the future.\n        """"""\n        pred_normal = pred[\'normal\']\n        pred_depth = pred[\'depth\']\n        pred_silhou = pred[\'silhou\']\n        is_fg = self._gt.silhou != 0  # excludes background\n        is_fg_full = is_fg.expand_as(pred_normal)\n        loss_normal = self.criterion(\n            pred_normal[is_fg_full], self._gt.normal[is_fg_full]\n        )\n        loss_depth = self.criterion(\n            pred_depth[is_fg], self._gt.depth[is_fg]\n        )\n        loss_silhou = self.criterion(pred_silhou, self._gt.silhou)\n        loss = loss_normal + loss_depth + loss_silhou\n        loss_data = {}\n        loss_data[\'loss\'] = loss.mean().item()\n        loss_data[\'normal\'] = loss_normal.mean().item()\n        loss_data[\'depth\'] = loss_depth.mean().item()\n        loss_data[\'silhou\'] = loss_silhou.mean().item()\n        if self.opt.pred_depth_minmax:\n            w_minmax = (256 ** 2) / 2  # matching scale of pixel predictions very roughly\n            loss_depth_minmax = w_minmax * self.criterion(\n                pred[\'depth_minmax\'],\n                self._gt.depth_minmax\n            )\n            loss += loss_depth_minmax\n            loss_data[\'depth_minmax\'] = loss_depth_minmax.mean().item()\n        return loss, loss_data\n\n\nclass Net(Uresnet):\n    def __init__(self, *args, pred_depth_minmax=True):\n        super().__init__(*args)\n        self.pred_depth_minmax = pred_depth_minmax\n        if self.pred_depth_minmax:\n            module_list = nn.Sequential(\n                nn.Conv2d(512, 512, 2, stride=2),\n                nn.Conv2d(512, 512, 4, stride=1),\n                ViewAsLinear(),\n                nn.Linear(512, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(inplace=True),\n                nn.Linear(256, 128),\n                nn.BatchNorm1d(128),\n                nn.ReLU(inplace=True),\n                nn.Linear(128, 2)\n            )\n            self.decoder_minmax = module_list\n\n    def forward(self, input_struct):\n        x = input_struct.rgb\n        out_dict = super().forward(x)\n        if self.pred_depth_minmax:\n            out_dict[\'depth_minmax\'] = self.decoder_minmax(self.encoder_out)\n        return out_dict\n'"
models/marrnet2.py,2,"b'from os import makedirs\nfrom os.path import join\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom networks.networks import ImageEncoder, VoxelDecoder\nfrom .marrnetbase import MarrnetBaseModel\n\n\nclass Model(MarrnetBaseModel):\n    @classmethod\n    def add_arguments(cls, parser):\n        parser.add_argument(\n            \'--canon_sup\',\n            action=\'store_true\',\n            help=""Use canonical-pose voxels as supervision""\n        )\n        return parser, set()\n\n    def __init__(self, opt, logger):\n        super(Model, self).__init__(opt, logger)\n        if opt.canon_sup:\n            voxel_key = \'voxel_canon\'\n        else:\n            voxel_key = \'voxel\'\n        self.voxel_key = voxel_key\n        self.requires = [\'rgb\', \'depth\', \'normal\', \'silhou\', voxel_key]\n        self.net = Net(4)\n        self.criterion = nn.BCEWithLogitsLoss(reduction=\'elementwise_mean\')\n        self.optimizer = self.adam(\n            self.net.parameters(),\n            lr=opt.lr,\n            **self.optim_params\n        )\n        self._nets = [self.net]\n        self._optimizers.append(self.optimizer)\n        self.input_names = [\'depth\', \'normal\', \'silhou\']\n        self.gt_names = [voxel_key]\n        self.init_vars(add_path=True)\n        self._metrics = [\'loss\']\n        self.init_weight(self.net)\n\n    def __str__(self):\n        return ""MarrNet-2 predicting voxels from 2.5D sketches""\n\n    def _train_on_batch(self, epoch, batch_idx, batch):\n        self.net.zero_grad()\n        pred = self.predict(batch)\n        loss, loss_data = self.compute_loss(pred)\n        loss.backward()\n        self.optimizer.step()\n        batch_size = len(batch[\'rgb_path\'])\n        batch_log = {\'size\': batch_size, **loss_data}\n        return batch_log\n\n    def _vali_on_batch(self, epoch, batch_idx, batch):\n        pred = self.predict(batch, no_grad=True)\n        _, loss_data = self.compute_loss(pred)\n        if np.mod(epoch, self.opt.vis_every_vali) == 0:\n            if batch_idx < self.opt.vis_batches_vali:\n                outdir = join(self.full_logdir, \'epoch%04d_vali\' % epoch)\n                makedirs(outdir, exist_ok=True)\n                output = self.pack_output(pred, batch)\n                self.visualizer.visualize(output, batch_idx, outdir)\n                np.savez(join(outdir, \'batch%04d\' % batch_idx), **output)\n        batch_size = len(batch[\'rgb_path\'])\n        batch_log = {\'size\': batch_size, **loss_data}\n        return batch_log\n\n    def pack_output(self, pred, batch, add_gt=True):\n        out = {}\n        out[\'rgb_path\'] = batch[\'rgb_path\']\n        out[\'pred_voxel\'] = pred.detach().cpu().numpy()\n        if add_gt:\n            out[\'gt_voxel\'] = batch[self.voxel_key].numpy()\n            out[\'normal_path\'] = batch[\'normal_path\']\n            out[\'depth_path\'] = batch[\'depth_path\']\n            out[\'silhou_path\'] = batch[\'silhou_path\']\n        return out\n\n    def compute_loss(self, pred):\n        loss = self.criterion(pred, getattr(self._gt, self.voxel_key))\n        loss_data = {}\n        loss_data[\'loss\'] = loss.mean().item()\n        return loss, loss_data\n\n\nclass Net(nn.Module):\n    """"""\n    2.5D maps to 3D voxel\n    """"""\n\n    def __init__(self, in_planes, encode_dims=200, silhou_thres=0):\n        super().__init__()\n        self.encoder = ImageEncoder(in_planes, encode_dims=encode_dims)\n        self.decoder = VoxelDecoder(n_dims=encode_dims, nf=512)\n        self.silhou_thres = silhou_thres\n\n    def forward(self, input_struct):\n        depth = input_struct.depth\n        normal = input_struct.normal\n        silhou = input_struct.silhou\n        # Mask\n        is_bg = silhou <= self.silhou_thres\n        depth[is_bg] = 0\n        normal[is_bg.repeat(1, 3, 1, 1)] = 0 # NOTE: if old net2, set to white (100),\n        x = torch.cat((depth, normal), 1) # and swap depth and normal\n        # Forward\n        latent_vec = self.encoder(x)\n        vox = self.decoder(latent_vec)\n        return vox\n'"
models/marrnetbase.py,2,"b'from os import makedirs\nfrom os.path import join\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom models.netinterface import NetInterface\nfrom toolbox.cam_bp.cam_bp.functions import CameraBackProjection\nimport util.util_img\n\n\nclass MarrnetBaseModel(NetInterface):\n    im_size = 256\n    rgb_jitter_d = 0.4\n    rgb_light_noise = 0.1\n    silhou_thres = 0.999\n    pred_silhou_thres = 0.3\n    scale_25d = 100\n\n    def __init__(self, opt, logger):\n        super(MarrnetBaseModel, self).__init__(opt, logger)\n        self.opt = opt\n        self.n_batches_per_epoch = opt.epoch_batches\n        self.n_batches_to_vis_train = opt.vis_batches_train\n        self.n_batches_to_vis_vali = opt.vis_batches_vali\n        self.full_logdir = opt.full_logdir\n        self._metrics = []\n        self.batches_to_vis = {}\n        self.dataset = opt.dataset\n        self._nets = []\n        self._optimizers = []\n        self._moveable_vars = []\n        self.cam_bp = Camera_back_projection_layer(128)\n        if opt.log_time:\n            self._metrics += [\'batch_time\', \'data_time\']\n        # Parameters for different optimization methods\n        self.optim_params = dict()\n        if opt.optim == \'adam\':\n            self.optim_params[\'betas\'] = (opt.adam_beta1, opt.adam_beta2)\n        elif opt.optim == \'sgd\':\n            self.optim_params[\'momentum\'] = opt.sgd_momentum\n            self.optim_params[\'dampening\'] = opt.sgd_dampening\n            self.optim_params[\'weight_decay\'] = opt.sgd_wdecay\n        else:\n            raise NotImplementedError(opt.optim)\n\n    def _train_on_batch(self, batch_idx, batch):\n        self.net.zero_grad()\n        pred = self.predict(batch)\n        loss, loss_data = self.compute_loss(pred)\n        loss.backward()\n        self.optimizer.step()\n        batch_size = len(batch[\'rgb_path\'])\n        batch_log = {\'size\': batch_size, **loss_data}\n        self.record_batch(batch_idx, batch)\n        return batch_log\n\n    def _vali_on_batch(self, epoch, batch_idx, batch):\n        pred = self.predict(batch, no_grad=True)\n        _, loss_data = self.compute_loss(pred)\n        if np.mod(epoch, self.opt.vis_every_vali) == 0:\n            if batch_idx < self.opt.vis_batches_vali:\n                outdir = join(self.full_logdir, \'epoch%04d_vali\' % epoch)\n                makedirs(outdir, exist_ok=True)\n                output = self.pack_output(pred, batch)\n                self.visualizer.visualize(output, batch_idx, outdir)\n                np.savez(join(outdir, \'batch%04d\' % batch_idx), **output)\n        batch_size = len(batch[\'rgb_path\'])\n        batch_log = {\'size\': batch_size, **loss_data}\n        return batch_log\n\n    @classmethod\n    def preprocess(cls, data, mode=\'train\'):\n        """"""\n        This function should be applied to [0, 1] floats, except absolute depth\n        """"""\n        data_proc = {}\n        for key, val in data.items():\n            if key == \'rgb\':\n                im = val\n                # H x W x 3\n                im = util.util_img.resize(im, cls.im_size, \'horizontal\')\n                if mode == \'train\':\n                    im = util.util_img.jitter_colors(\n                        im,\n                        d_brightness=cls.rgb_jitter_d,\n                        d_contrast=cls.rgb_jitter_d,\n                        d_saturation=cls.rgb_jitter_d\n                    )\n                    im = util.util_img.add_lighting_noise(\n                        im, cls.rgb_light_noise)\n                im = util.util_img.normalize_colors(im)\n                val = im.transpose(2, 0, 1)\n\n            elif key == \'depth\':\n                im = val\n                if im.ndim == 3:\n                    im = im[:, :, 0]\n                im = util.util_img.resize(\n                    im, cls.im_size, \'horizontal\', clamp=(im.min(), im.max()))\n                im *= cls.scale_25d\n                val = im[np.newaxis, :, :]\n                # 1 x H x W, scaled\n\n            elif key == \'silhou\':\n                im = val\n                if im.ndim == 3:\n                    im = im[:, :, 0]\n                im = util.util_img.resize(\n                    im, cls.im_size, \'horizontal\', clamp=(im.min(), im.max()))\n                im = util.util_img.binarize(\n                    im, cls.silhou_thres, gt_is_1=True)\n                im *= cls.scale_25d\n                val = im[np.newaxis, :, :]\n                # 1 x H x W, binarized, scaled\n\n            elif key == \'normal\':\n                # H x W x 3\n                im = val\n                im = util.util_img.resize(\n                    im, cls.im_size, \'horizontal\', clamp=(im.min(), im.max()))\n                im *= cls.scale_25d\n                val = im.transpose(2, 0, 1)\n                # 3 x H x W, scaled\n\n            data_proc[key] = val\n        return data_proc\n\n    @staticmethod\n    def mask(input_image, input_mask, bg=1.0):\n        assert isinstance(bg, (int, float))\n        assert (input_mask >= 0).all() and (input_mask <= 1).all()\n        input_mask = input_mask.expand_as(input_image)\n        bg = bg * input_image.new_ones(input_image.size())\n        output = input_mask * input_image + (1 - input_mask) * bg\n        return output\n\n    @classmethod\n    def postprocess(cls, tensor, bg=1.0, input_mask=None):\n        scaled = tensor / cls.scale_25d\n        if input_mask is not None:\n            return cls.mask(scaled, input_mask, bg=bg)\n        return scaled\n\n    @staticmethod\n    def to_abs_depth(rel_depth, depth_minmax):\n        bmin = depth_minmax[:, 0]\n        bmax = depth_minmax[:, 1]\n        depth_min = bmin.view(-1, 1, 1, 1)\n        depth_max = bmax.view(-1, 1, 1, 1)\n        abs_depth = rel_depth * (depth_max - depth_min + 1e-4) + depth_min\n        return abs_depth\n\n    def proj_depth(self, abs_depth):\n        proj_depth = self.cam_bp(abs_depth)\n        return self.cam_bp.shift_tdf(proj_depth)\n\n\nclass Camera_back_projection_layer(nn.Module):\n    def __init__(self, res):\n        super(Camera_back_projection_layer, self).__init__()\n        self.res = res\n\n    def forward(self, depth_t, fl=784.4645406, cam_dist=2.2):\n        # print(cam_dist)\n        n = depth_t.size(0)\n        if isinstance(fl, float):\n            fl_v = fl\n            fl = torch.FloatTensor(n, 1).cuda()\n            fl.fill_(fl_v)\n        if isinstance(cam_dist, float):\n            cmd_v = cam_dist\n            cam_dist = torch.FloatTensor(n, 1).cuda()\n            cam_dist.fill_(cmd_v)\n        return CameraBackProjection.apply(depth_t, fl, cam_dist, self.res)\n\n    @staticmethod\n    def shift_tdf(input_tdf, res=128):\n        out_tdf = 1 - res * input_tdf\n        return out_tdf\n'"
models/netinterface.py,11,"b'# A general training procedure\nimport time\nimport torch.optim as optim\nfrom torch.nn import init\nimport torch\nfrom torch import FloatTensor, tensor\nfrom loggers.loggers import _LogCumulator\nfrom util.util_print import str_warning\nfrom visualize.visualizer import Visualizer\n\n\ndef _get_num_samples(dataloader):\n    # import torch.utils.data.sampler as samplers\n    batch_sampler = dataloader.batch_sampler\n    if batch_sampler.drop_last:\n        return len(batch_sampler.sampler) // batch_sampler.batch_size * \\\n            batch_sampler.batch_size\n    return len(batch_sampler.sampler)\n\n\nclass NetInterface(object):\n    """""" base class of all Model Interface\n    all derived classes should overwrite __init__ and _train_on_batch(),\n    and defines variables \'_moveable_vars\', \'_nets\', \'_metrics\' and \'_optimizers\'.\n\n    Requirements for derived class:\n    Variables:\n        \'_moveable_vars\' for cuda();\n        \'_nets\' and \'_optimizers\' for saving;\n        \'_metrics\' for logging\n    batch_log: all _train_on_batch() and _eval_on_batch() should return a batch_log for logging.\n               All values in the batch_log are considerred as sample-wise mean.\n    This log must have the key:\n        \'loss\': the standard loss for choosing best performing eval model\n        \'size\': the batch size for the batch\n    """"""\n\n    def init_weight(self, net=None, init_type=\'kaiming\', init_param=0.02):\n        """"""\n        This is borrowed from Junyan\n        """"""\n        def init_func(m, init_type=init_type):\n            classname = m.__class__.__name__\n            if hasattr(m, \'weight\') and (classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1):\n                if init_type == \'normal\':\n                    init.normal_(m.weight.data, 0.0, init_param)\n                elif init_type == \'xavier\':\n                    init.xavier_normal_(m.weight.data, gain=init_param)\n                elif init_type == \'kaiming\':\n                    init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n                elif init_type == \'orth\':\n                    init.orthogonal_(m.weight.data, gain=init_param)\n                else:\n                    raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n                if hasattr(m, \'bias\') and m.bias is not None:\n                    init.constant_(m.bias.data, 0.0)\n            elif classname.find(\'BatchNorm\') != -1:\n                init.normal_(m.weight.data, 1.0, init_param)\n                init.constant_(m.bias.data, 0.0)\n        if net is not None:\n            net.apply(init_func)\n        else:\n            self.net.apply(init_func)\n\n    @classmethod\n    def add_arguments(cls, parser):\n        unique_params = set()\n        return parser, unique_params\n\n    def __init__(self, opt, logger):\n        self._internal_logger = _LogCumulator()\n        logger.add_logger(self._internal_logger)\n        assert opt.optim == \'adam\', ""All projects here used adam""\n        self.adam = optim.Adam\n        self._logger = logger\n        self.opt = opt\n        self.full_logdir = opt.full_logdir\n        self.grad_hook_gen, self.grad_stats = self.dict_grad_hook_factory(\n            add_func=lambda x: {\n                \'mean\': x.mean(),\n                \'std\': x.std()\n            }\n        )\n        self._nets = []\n        self._moveable_vars = []\n        self._optimizers = []\n        self.visualizer = Visualizer(\n            n_workers=getattr(opt, \'vis_workers\', 4),\n            param_f=getattr(opt, \'vis_param_f\', None),\n        ) # getattr used for backward compatibility\n        self.batches_to_vis = {}\n        self.input_names = []\n        self._input = lambda: None\n        self.gt_names = []\n        self._gt = lambda: None\n        self.aux_names = []\n        self._aux = lambda: None  # auxiliary tensors that need moving to GPU\n        # Use cases include the one for D in WGAN\n\n    def init_vars(self, add_path=True):\n        """"""\n        Also add stuff to movable_vars\n        """"""\n        for net in self._nets:\n            self._moveable_vars.append(net)\n        for name in self.input_names:\n            setattr(self._input, name, FloatTensor())\n            self._moveable_vars.append(\'_input.\' + name)\n            if add_path:\n                setattr(self._input, name + \'_path\', None)\n        for name in self.gt_names:\n            setattr(self._gt, name, FloatTensor())\n            self._moveable_vars.append(\'_gt.\' + name)\n            if add_path:\n                setattr(self._gt, name + \'_path\', None)\n        for name in self.aux_names:\n            if name == \'one\':\n                setattr(self._aux, name, tensor(1).float())\n            elif name == \'neg_one\':\n                setattr(self._aux, name, tensor(-1).float())\n            else:\n                setattr(self._aux, name, FloatTensor([]))\n            self._moveable_vars.append(\'_aux.\' + name)\n\n    def load_batch(self, batch, include_gt=True):\n        for name in self.input_names:\n            if name in batch.keys():\n                var = getattr(self._input, name)\n                if var.shape != batch[name].shape:\n                    var.resize_(batch[name].shape)\n                var.copy_(batch[name])\n                setattr(self._input, name, var)\n            if name + \'_path\' in batch.keys():\n                setattr(self._input, name + \'_path\', batch[name + \'_path\'])\n        if include_gt:\n            for name in self.gt_names:\n                if name in batch.keys():\n                    var = getattr(self._gt, name)\n                    if var.shape != batch[name].shape:\n                        var.resize_(batch[name].shape)\n                    var.copy_(batch[name])\n                    setattr(self._gt, name, var)\n                if name + \'_path\' in batch.keys():\n                    setattr(self._gt, name + \'_path\', batch[name + \'_path\'])\n\n    def _train_on_batch(self, epoch, batch_ind, dataloader_out):\n        """""" function that trains the model over one batch and return batch_log (including size and loss) """"""\n        raise NotImplementedError\n\n    def _vali_on_batch(self, epoch, batch_ind, dataloader_out):\n        """""" function that trains the model over one batch and return batch_log (including size and loss) """"""\n        raise NotImplementedError\n\n    def test_on_batch(self, batch_ind, dataloader_out):\n        raise NotImplementedError\n\n    def train_epoch(\n            self,\n            dataloader,\n            *,\n            dataloader_eval=None,\n            max_batches_per_train=None,\n            max_batches_per_eval=None,\n            epochs=1,\n            initial_epoch=1,\n            verbose=1,\n            reset_dataset=None,\n            eval_at_start=False):\n        """"""\n        Train the model with given dataloader and run evaluation with the given dataloader_eval\n        max_batches_per_train: limit the number of batches for each epoch\n        reset_dataset: if the dataset needs to be reset for each training epoch,\n            define a reset() method in the dataset and provide it as this argument.\n            reset() will be called at the beginning of each epoch, and the procedure will\n            check for dataset size change and change accordingly.\n        """"""\n        logger = self._logger\n        # set logger params and number of batches in an epoch\n        steps_per_epoch = len(dataloader)\n        samples_per_epoch = _get_num_samples(dataloader)\n        if max_batches_per_train is not None:\n            steps_per_epoch = min(max_batches_per_train, steps_per_epoch)\n            samples_per_epoch = min(samples_per_epoch, steps_per_epoch * dataloader.batch_sampler.batch_size)\n        if dataloader_eval is not None:\n            steps_per_eval = len(dataloader_eval)\n            samples_per_eval = _get_num_samples(dataloader_eval)\n            if max_batches_per_eval is not None:\n                steps_per_eval = min(steps_per_eval, max_batches_per_eval)\n                samples_per_eval = min(samples_per_eval, steps_per_eval * dataloader.batch_sampler.batch_size)\n        else:\n            steps_per_eval = 0\n            samples_per_eval = 0\n        logger.set_params({\n            \'epochs\': epochs + initial_epoch - 1,\n            \'steps\': steps_per_epoch,\n            \'steps_eval\': steps_per_eval,\n            \'samples\': samples_per_epoch,\n            \'samples_eval\': samples_per_eval,\n            \'verbose\': 1,\n            \'metrics\': self._metrics,\n        })\n        logger.set_model(self)\n        logger.on_train_begin()\n        dataset_size = 0            # monitor if dataset size change due to reset_dataset.reset(). update steps_per_epoch if needed\n\n        # define train closure\n        def _train(epoch):\n            nonlocal dataset_size\n            nonlocal steps_per_epoch\n            nonlocal samples_per_epoch\n            nonlocal steps_per_eval\n            nonlocal samples_per_eval\n            self.train()\n            logger.train()\n            if reset_dataset is not None:\n                reset_dataset.reset()\n                # reset steps if necessary\n                if dataset_size != len(reset_dataset):\n                    steps_per_epoch = len(dataloader)\n                    samples_per_epoch = _get_num_samples(dataloader)\n                    if max_batches_per_train is not None:\n                        steps_per_epoch = min(max_batches_per_train, steps_per_epoch)\n                        samples_per_epoch = min(samples_per_epoch, steps_per_epoch * dataloader.batch_sampler.batch_size)\n                    if dataloader_eval is not None:\n                        steps_per_eval = len(dataloader_eval)\n                        samples_per_eval = _get_num_samples(dataloader_eval)\n                        if max_batches_per_eval is not None:\n                            steps_per_eval = min(steps_per_eval, max_batches_per_eval)\n                            samples_per_eval = min(samples_per_eval, steps_per_eval * dataloader.batch_sampler.batch_size)\n                    logger.set_params({\n                        \'epochs\': epochs + initial_epoch - 1,\n                        \'steps\': steps_per_epoch,\n                        \'steps_eval\': steps_per_eval,\n                        \'samples\': samples_per_epoch,\n                        \'samples_eval\': samples_per_eval,\n                        \'verbose\': 1,\n                        \'metrics\': self._metrics,\n                    })\n                    dataset_size = len(reset_dataset)\n\n            dataiter = iter(dataloader)\n            logger.on_epoch_begin(epoch)\n            for i in range(steps_per_epoch):\n                start_time = time.time()\n                data = next(dataiter)\n                data_time = time.time() - start_time\n                logger.on_batch_begin(i)\n                batch_log = self._train_on_batch(epoch, i, data)\n                batch_log[\'batch\'] = i\n                batch_log[\'epoch\'] = epoch\n                batch_log[\'data_time\'] = data_time\n                batch_log[\'batch_time\'] = time.time() - start_time\n                logger.on_batch_end(i, batch_log)\n            epoch_log = self._internal_logger.get_epoch_log()\n            logger.on_epoch_end(epoch, epoch_log)\n\n        # define eval closure\n        def _eval(epoch):\n            self.eval()\n            logger.eval()\n            dataiter = iter(dataloader_eval)\n            logger.on_epoch_begin(epoch)\n            for i in range(steps_per_eval):\n                start_time = time.time()\n                data = next(dataiter)\n                data_time = time.time() - start_time\n                logger.on_batch_begin(i)\n                batch_log = self._vali_on_batch(epoch, i, data)\n                batch_log[\'batch\'] = i\n                batch_log[\'epoch\'] = epoch\n                batch_log[\'data_time\'] = data_time\n                batch_log[\'batch_time\'] = time.time() - start_time\n                logger.on_batch_end(i, batch_log)\n            epoch_log = self._internal_logger.get_epoch_log()\n            logger.on_epoch_end(epoch, epoch_log)\n\n        # run actual training\n        if eval_at_start:\n            if dataloader_eval is None:\n                raise ValueError(\'eval_at_beginning is set to True but no eval data is given.\')\n            _eval(initial_epoch - 1)\n        for epoch in range(initial_epoch, initial_epoch + epochs):\n            _train(epoch)\n            if dataloader_eval is not None:\n                _eval(epoch)\n\n        logger.on_train_end()\n\n    @staticmethod\n    def circular_grad_hook_factory(num_to_keep, add_func=lambda x: x):\n        class CircularList(object):\n            def __init__(self, num):\n                self.vals = [None] * num\n                self.ncyc = 0\n                self.c = 0\n                self.n = num\n\n            def append(self, value):\n                self.vals[self.c] = value\n                self.c += 1\n                if self.c == self.n:\n                    self.c = 0\n                    self.ncyc += 1\n\n            def full(self):\n                return self.c == 0\n\n            def __iter__(self):\n                return iter(self.vals)\n\n            def __len__(self):\n                return self.n\n\n            def __getitem__(self, index):\n                assert index < self.n\n                return self.vals[index]\n\n            def __repr__(self):\n                return str(self.vals)\n\n        saved_grads = CircularList(num_to_keep)\n\n        def grad_hook(grad):\n            saved_tensor = add_func(grad)\n            saved_grads.append(saved_tensor)\n        return grad_hook, saved_grads\n\n    @staticmethod\n    def dict_grad_hook_factory(add_func=lambda x: x):\n        saved_dict = dict()\n\n        def hook_gen(name):\n            def grad_hook(grad):\n                saved_vals = add_func(grad)\n                saved_dict[name] = saved_vals\n            return grad_hook\n\n        return hook_gen, saved_dict\n\n    def predict(self, batch, net=\'net\', load_gt=True, no_grad=False):\n        net = getattr(self, net)\n        self.load_batch(batch, include_gt=load_gt)\n        if no_grad:\n            with torch.no_grad():\n                pred = net(self._input)  # a structure\n                # How to extract data and then forward them\n                # should be dealt with in model file\n        else:\n            pred = net(self._input)\n        return pred\n\n    def train(self):\n        for m in self._nets:\n            m.train()\n\n    def eval(self):\n        for m in self._nets:\n            m.eval()\n\n    def num_parameters(self, return_list=False):\n        nparams_list = list()\n        for net in self._nets:\n            parameters = list(net.parameters())\n            nparams_list.append(sum([x.numel() for x in parameters]))\n        if return_list:\n            return nparams_list\n        return sum(nparams_list)\n\n    def cuda(self):\n        for v in self._moveable_vars:\n            if isinstance(v, str):\n                if \'.\' in v:\n                    var_type, var_name = v.split(\'.\')\n                    var = getattr(getattr(self, var_type), var_name)\n                    setattr(getattr(self, var_type), var_name, var.cuda())\n                else:\n                    setattr(self, v, getattr(self, v).cuda())\n            else:\n                v.cuda()\n\n    def cpu(self):\n        for v in self._moveable_vars:\n            if isinstance(v, str):\n                if \'.\' in v:\n                    var_type, var_name = v.split(\'.\')\n                    var = getattr(getattr(self, var_type), var_name)\n                    setattr(getattr(self, var_type), var_name, var.cpu())\n                else:\n                    setattr(self, v, getattr(self, v).cpu())\n            else:\n                v.cpu()\n\n    def to(self, device):\n        for v in self._moveable_vars:\n            if isinstance(v, str):\n                if \'.\' in v:\n                    var_type, var_name = v.split(\'.\')\n                    var = getattr(getattr(self, var_type), var_name)\n                    setattr(getattr(self, var_type), var_name, var.to(device))\n                else:\n                    setattr(self, v, getattr(self, v).to(device))\n            else:\n                v.to(device)\n\n    def save_state_dict(self, filepath, *, save_optimizer=False, additional_values={}):\n        state_dicts = dict()\n        state_dicts[\'nets\'] = [net.state_dict() for net in self._nets]\n        if save_optimizer:\n            state_dicts[\'optimizers\'] = [optimizer.state_dict() for optimizer in self._optimizers]\n        for k, v in additional_values.items():\n            state_dicts[k] = v\n        torch.save(state_dicts, filepath)\n\n    def load_state_dict(self, filepath, *, load_optimizer=\'auto\'):\n        state_dicts = torch.load(filepath)\n\n        if load_optimizer == \'auto\':\n            load_optimizer = (\'optimizers\' in state_dicts)\n            if not load_optimizer:\n                print(str_warning, \'Model loaded without optimizer states. \')\n\n        assert len(self._nets) == len(state_dicts[\'nets\'])\n        for i in range(len(self._nets)):\n            self._nets[i].load_state_dict(state_dicts[\'nets\'][i])\n\n        if load_optimizer:\n            assert len(self._optimizers) == len(state_dicts[\'optimizers\'])\n            for i in range(len(self._optimizers)):\n                optimizer = self._optimizers[i]\n                state = state_dicts[\'optimizers\'][i]\n\n                # load optimizer state without overwriting training hyper-parameters, e.g. lr\n                optimizer_load_state_dict(optimizer, state, keep_training_params=True)\n\n        additional_values = {k: v for k, v in state_dicts.items() if k not in (\'optimizers\', \'nets\')}\n        return additional_values\n\n\ndef optimizer_load_state_dict(optimizer, state, keep_training_params=False):\n    if keep_training_params:\n        oldstate = optimizer.state_dict()\n        assert len(oldstate[\'param_groups\']) == len(state[\'param_groups\'])\n        # use oldstate to override this state\n        for oldg, g in zip(oldstate[\'param_groups\'], state[\'param_groups\']):\n            for k, v in oldg.items():\n                if k != \'params\':   # parameter id not overwriten\n                    g[k] = v\n    optimizer.load_state_dict(state)\n\n\ndef top_n_err(output, label, nlist):\n    output = output.detach().cpu().numpy()\n    label = label.detach().cpu().numpy().reshape(-1, 1)\n    idx_sort = output.argsort()\n    errlist = list()\n    for n in nlist:\n        errlist.append(1 - (idx_sort[:, -n:] - label == 0).any(1).mean())\n    return errlist\n\n\ndef parse_optimizer_specific_params(optimizer_name, opt):\n    optim_params = dict()\n    if optimizer_name == \'adam\':\n        optim_params[\'betas\'] = (opt.adam_beta1, opt.adam_beta2)\n    elif optimizer_name == \'sgd\':\n        optim_params[\'momentum\'] = opt.sgd_momentum\n        optim_params[\'dampening\'] = opt.sgd_dampening\n        optim_params[\'weight_decay\'] = opt.sgd_wdecay\n    return optim_params\n\n\ndef data_parallel_decorator(ModuleClass):\n    """"""\n    A decorator for forward function to use multiGPU training\n    while maintaining network layout (and thus saved parameter\n    names).\n\n    Note: the way pytorch replicate a module only guarantees\n    that pytorch internal parameters, buffers and modules are\n    properly copied and set over devices. Any other class attributes\n    are copied in a shallow copy manner - any list, dictionary, etc.\n    that uses class attributes may not be set properly, and\n    will point to the original copy instead of the copied one\n    on the new GPU.\n    """"""\n    from torch.nn.parallel.scatter_gather import scatter_kwargs, gather\n    from torch.nn.parallel.replicate import replicate\n    from torch.nn.parallel.parallel_apply import parallel_apply\n    assert not hasattr(ModuleClass, \'_forward_worker\'), \'data_parallel_decorator cannot be used on a class twice\'\n    ModuleClass._forward_worker = ModuleClass.forward\n\n    def wrapped(self, *inputs, **module_kwargs):\n        if (not hasattr(self, \'_is_replica\')) and inputs[0].is_cuda:\n            device_count = torch.cuda.device_count()\n            if inputs[0].shape[0] % device_count != 0:\n                import os\n                cuda_visible_devices = os.environ[\'CUDA_VISIBLE_DEVICES\'] if \'CUDA_VISIBLE_DEVICES\' in os.environ else \'\'\n                raise ValueError(\'batch size (%d) must be divisible by the number of GPUs (%d) used\\n CUDA_VISIBLE_DEVICES: %s\' % (inputs[0].shape[0], device_count, cuda_visible_devices))\n            if device_count > 1:\n                # modified from pytorch (torch.nn.parallel.DataParallel)\n                device_ids = list(range(device_count))\n                output_device = device_ids[0]\n                inputs, kwargs = scatter_kwargs(inputs, module_kwargs, device_ids)\n                replicas = replicate(self, device_ids[:len(inputs)])\n\n                # add a _is_replica flag to avoid infinite loop\n                # from recursively calling parallel_apply\n                for replica in replicas:\n                    replica._is_replica = True\n                outputs = parallel_apply(replicas, inputs, kwargs)\n                return gather(outputs, output_device)\n\n        return self._forward_worker(*inputs, **module_kwargs)\n    ModuleClass.forward = wrapped\n    return ModuleClass\n\n\ndef print_grad_stats(grad):\n    grad_ = grad.detach()\n    print(\'\\nmin, max, mean, std: %e, %e, %e, %e\' % (grad_.min().item(), grad_.max().item(), grad_.mean().item(), grad_.std().item()))\n'"
models/shapehd.py,4,"b'from os import makedirs\nfrom os.path import join\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom util import util_img\nfrom .wgangp import D\nfrom .marrnet2 import Net as Marrnet2, Model as Marrnet2_model\nfrom .marrnet1 import Model as Marrnet1_model\n\n\nclass Model(Marrnet2_model):\n    @classmethod\n    def add_arguments(cls, parser):\n        parser.add_argument(\n            \'--canon_sup\',\n            action=\'store_true\',\n            help=""Use canonical-pose voxels as supervision""\n        )\n        parser.add_argument(\n            \'--marrnet2\',\n            type=str, default=None,\n            help=""Path to pretrained MarrNet-2 (to be finetuned)""\n        )\n        parser.add_argument(\n            \'--gan\',\n            type=str, default=None,\n            help=""Path to pretrained WGANGP""\n        )\n        parser.add_argument(\n            \'--w_gan_loss\',\n            type=float, default=0,\n            help=""Weight for perceptual loss relative to supervised loss""\n        )\n        return parser, set()\n\n    def __init__(self, opt, logger):\n        super().__init__(opt, logger)\n        assert opt.canon_sup, ""ShapeHD uses canonical-pose voxels""\n        self.net = Net(opt.marrnet2, opt.gan)\n        self._nets = [self.net]\n        self.optimizer = self.adam(\n            self.net.marrnet2.parameters(),\n            lr=opt.lr,\n            **self.optim_params\n        ) # just finetune MarrNet-2\n        self._optimizers[-1] = self.optimizer\n        self._metrics += [\'sup\', \'gan\']\n        self.init_vars(add_path=True)\n        assert opt.w_gan_loss >= 0\n\n    def __str__(self):\n        return ""Finetuning 3D estimator of ShapeHD with GAN loss""\n\n    def pack_output(self, pred, batch, add_gt=True):\n        out = {}\n        out[\'rgb_path\'] = batch[\'rgb_path\']\n        out[\'pred_voxel_noft\'] = pred[\'voxel_noft\'].detach().cpu().numpy()\n        out[\'pred_voxel\'] = pred[\'voxel\'].detach().cpu().numpy()\n        if add_gt:\n            out[\'gt_voxel\'] = batch[self.voxel_key].numpy()\n            out[\'normal_path\'] = batch[\'normal_path\']\n            out[\'depth_path\'] = batch[\'depth_path\']\n            out[\'silhou_path\'] = batch[\'silhou_path\']\n        return out\n\n    def compute_loss(self, pred):\n        loss_sup = self.criterion(\n            pred[\'voxel\'], # will be sigmoid\'ed\n            getattr(self._gt, self.voxel_key)\n        )\n        loss_gan = -pred[\'is_real\'].mean() # negate to maximize\n        loss_gan *= self.opt.w_gan_loss\n        loss = loss_sup + loss_gan\n        loss_data = {}\n        loss_data[\'sup\'] = loss_sup.item()\n        loss_data[\'gan\'] = loss_gan.item()\n        loss_data[\'loss\'] = loss.item()\n        return loss, loss_data\n\n\nclass Net(nn.Module):\n    """"""\n       3D Estimator   D of GAN\n    2.5D --------> 3D --------> real/fake\n         finetuned     fixed\n    """"""\n\n    def __init__(self, marrnet2_path=None, gan_path=None):\n        super().__init__()\n        # Init MarrNet-2 and load weights\n        self.marrnet2 = Marrnet2(4)\n        self.marrnet2_noft = Marrnet2(4)\n        if marrnet2_path:\n            state_dicts = torch.load(marrnet2_path)\n            state_dict = state_dicts[\'nets\'][0]\n            self.marrnet2.load_state_dict(state_dict)\n            self.marrnet2_noft.load_state_dict(state_dict)\n        # Init discriminator and load weights\n        self.d = D()\n        if gan_path:\n            state_dicts = torch.load(gan_path)\n            self.d.load_state_dict(state_dicts[\'nets\'][1])\n        # Fix D, but finetune MarrNet-2\n        for p in self.d.parameters():\n            p.requires_grad = False\n        for p in self.marrnet2_noft.parameters():\n            p.requires_grad = False\n        for p in self.marrnet2.parameters():\n            p.requires_grad = True\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_struct):\n        pred = {}\n        pred[\'voxel_noft\'] = self.marrnet2_noft(input_struct) # unfinetuned\n        pred[\'voxel\'] = self.marrnet2(input_struct)\n        pred[\'is_real\'] = self.d(self.sigmoid(pred[\'voxel\']))\n        return pred\n\n\nclass Model_test(Model):\n    @classmethod\n    def add_arguments(cls, parser):\n        parser, unique_params = Model.add_arguments(parser)\n        parser.add_argument(\n            \'--marrnet1_file\',\n            type=str, required=True,\n            help=""Path to pretrained MarrNet-1""\n        )\n        return parser, unique_params\n\n    def __init__(self, opt, logger):\n        opt.canon_sup = True # dummy, for network init only\n        super().__init__(opt, logger)\n        self.requires = [\'rgb\', \'mask\'] # mask for bbox cropping only\n        self.input_names = [\'rgb\']\n        self.init_vars(add_path=True)\n        self.output_dir = opt.output_dir\n        # Load MarrNet-2 and D (though unused at test time)\n        self.load_state_dict(opt.net_file, load_optimizer=\'auto\')\n        # Load MarrNet-1 whose outputs are inputs to D-tuned MarrNet-2\n        opt.pred_depth_minmax = True # dummy\n        self.marrnet1 = Marrnet1_model(opt, logger)\n        self.marrnet1.load_state_dict(opt.marrnet1_file)\n        self._nets.append(self.marrnet1.net)\n\n    def __str__(self):\n        return ""Testing ShapeHD""\n\n    @classmethod\n    def preprocess_wrapper(cls, in_dict):\n        silhou_thres = 0.95\n        in_size = 480\n        pad = 85\n        im = in_dict[\'rgb\']\n        mask = in_dict[\'silhou\']\n        bbox = util_img.get_bbox(mask, th=silhou_thres)\n        im_crop = util_img.crop(im, bbox, in_size, pad, pad_zero=False)\n        in_dict[\'rgb\'] = im_crop\n        del in_dict[\'silhou\'] # just for cropping -- done its job\n        # Now the image is just like those we rendered\n        out_dict = cls.preprocess(in_dict, mode=\'test\')\n        return out_dict\n\n    def test_on_batch(self, batch_i, batch):\n        outdir = join(self.output_dir, \'batch%04d\' % batch_i)\n        makedirs(outdir, exist_ok=True)\n        # Forward MarrNet-1\n        pred1 = self.marrnet1.predict(batch, load_gt=False, no_grad=True)\n        # Forward MarrNet-2\n        for net_name in (\'marrnet2\', \'marrnet2_noft\'):\n            net = getattr(self.net, net_name)\n            net.silhou_thres = self.pred_silhou_thres * self.scale_25d\n        self.input_names = [\'depth\', \'normal\', \'silhou\']\n        pred2 = self.predict(pred1, load_gt=False, no_grad=True)\n        # Pack, visualize, and save outputs\n        output = self.pack_output(pred1, pred2, batch)\n        self.visualizer.visualize(output, batch_i, outdir)\n        np.savez(outdir + \'.npz\', **output)\n\n    def pack_output(self, pred1, pred2, batch):\n        out = {}\n        # MarrNet-1 outputs\n        pred_normal = pred1[\'normal\'].detach().cpu()\n        pred_silhou = pred1[\'silhou\'].detach().cpu()\n        pred_depth = pred1[\'depth\'].detach().cpu()\n        out[\'rgb_path\'] = batch[\'rgb_path\']\n        out[\'rgb\'] = util_img.denormalize_colors(batch[\'rgb\'].detach().numpy())\n        pred_silhou = self.postprocess(pred_silhou)\n        pred_silhou = torch.clamp(pred_silhou, 0, 1)\n        pred_silhou[pred_silhou < 0] = 0\n        out[\'pred_silhou\'] = pred_silhou.numpy()\n        out[\'pred_normal\'] = self.postprocess(\n            pred_normal, bg=1.0, input_mask=pred_silhou\n        ).numpy()\n        out[\'pred_depth\'] = self.postprocess(\n            pred_depth, bg=0.0, input_mask=pred_silhou\n        ).numpy()\n        # D-tuned MarrNet-2 outputs\n        out[\'pred_voxel\'] = pred2[\'voxel\'].detach().cpu().numpy()\n        out[\'pred_voxel_noft\'] = pred2[\'voxel_noft\'].detach().cpu().numpy()\n        return out\n'"
models/wgangp.py,6,"b'from os import makedirs\nfrom os.path import join\nfrom time import time\nimport numpy as np\nimport torch\nfrom networks.networks import VoxelGenerator, VoxelDiscriminator\nfrom .netinterface import NetInterface\n\n\nclass Model(NetInterface):\n    @classmethod\n    def add_arguments(cls, parser):\n        parser.add_argument(\n            \'--canon_voxel\',\n            action=\'store_true\',\n            help=""Generate/discriminate canonical-pose voxels""\n        )\n        parser.add_argument(\n            \'--wgangp_lambda\',\n            type=float,\n            default=10,\n            help=""WGANGP gradient penalty coefficient""\n        )\n        parser.add_argument(\n            \'--wgangp_norm\',\n            type=float,\n            default=1,\n            help=""WGANGP gradient penalty norm""\n        )\n        parser.add_argument(\n            \'--gan_d_iter\',\n            type=int,\n            default=1,\n            help=""# iterations D is trained per G\'s iteration""\n        )\n        return parser, set()\n\n    def __init__(self, opt, logger):\n        super().__init__(opt, logger)\n        assert opt.canon_voxel, ""GAN requires canonical-pose voxels to work""\n        self.requires = [\'voxel_canon\']\n        self.nz = 200\n        self.net_g = G(self.nz)\n        self.net_d = D()\n        self._nets = [self.net_g, self.net_d]\n        # Optimizers\n        self.optim_params = dict()\n        self.optim_params[\'betas\'] = (opt.adam_beta1, opt.adam_beta2)\n        self.optimizer_g = self.adam(\n            self.net_g.parameters(),\n            lr=opt.lr,\n            **self.optim_params\n        )\n        self.optimizer_d = self.adam(\n            self.net_d.parameters(),\n            lr=opt.lr,\n            **self.optim_params\n        )\n        self._optimizers = [self.optimizer_g, self.optimizer_d]\n        #\n        self.opt = opt\n        self.preprocess = None\n        self._metrics = [\'err_d_real\', \'err_d_fake\', \'err_d_gp\', \'err_d\', \'err_g\', \'loss\']\n        if opt.log_time:\n            self._metrics += [\'t_d_real\', \'t_d_fake\', \'t_d_grad\', \'t_g\']\n        self.input_names = [\'voxel_canon\']\n        self.aux_names = [\'one\', \'neg_one\']\n        self.init_vars(add_path=True)\n        self.init_weight(self.net_d)\n        self.init_weight(self.net_g)\n        self._last_err_g = None\n\n    def __str__(self):\n        s = ""3D-WGANGP""\n        return s\n\n    def _train_on_batch(self, epoch, batch_idx, batch):\n        net_d, net_g = self.net_d, self.net_g\n        opt_d, opt_g = self.optimizer_d, self.optimizer_g\n        one = self._aux.one\n        neg_one = self._aux.neg_one\n        real = batch[\'voxel_canon\'].cuda()\n        batch_size = real.shape[0]\n        batch_log = {\'size\': batch_size}\n\n        # Train D ...\n        net_d.zero_grad()\n        for p in net_d.parameters():\n            p.requires_grad = True\n        for p in net_g.parameters():\n            p.requires_grad = False\n        # with real\n        t0 = time()\n        err_d_real = self.net_d(real).mean()\n        err_d_real.backward(neg_one)\n        batch_log[\'err_d_real\'] = -err_d_real.item()\n        d_real_t = time() - t0\n        # with fake\n        t0 = time()\n        with torch.no_grad():\n            _, fake = self.net_g(batch_size)\n        err_d_fake = self.net_d(fake).mean()\n        err_d_fake.backward(one)\n        batch_log[\'err_d_fake\'] = err_d_fake.item()\n        d_fake_t = time() - t0\n        # with grad penalty\n        t0 = time()\n        if self.opt.wgangp_lambda > 0:\n            grad_penalty = self.calc_grad_penalty(real, fake)\n            grad_penalty.backward()\n            batch_log[\'err_d_gp\'] = grad_penalty.item()\n        else:\n            batch_log[\'err_d_gp\'] = 0\n        batch_log[\'err_d\'] = batch_log[\'err_d_fake\'] + batch_log[\'err_d_real\'] \\\n            + batch_log[\'err_d_gp\']\n        d_grad_t = time() - t0\n        opt_d.step()\n\n        # Train G\n        t0 = time()\n        for p in net_d.parameters():\n            p.requires_grad = False\n        for p in net_g.parameters():\n            p.requires_grad = True\n        net_g.zero_grad()\n        if batch_idx % self.opt.gan_d_iter == 0:\n            _, gen = self.net_g(batch_size)\n            err_g = self.net_d(gen).mean()\n            err_g.backward(neg_one)\n            opt_g.step()\n            batch_log[\'err_g\'] = -err_g.item()\n            self._last_err_g = batch_log[\'err_g\']\n        else:\n            batch_log[\'err_g\'] = self._last_err_g\n        g_t = time() - t0\n\n        if self.opt.log_time:\n            batch_log[\'t_d_real\'] = d_real_t\n            batch_log[\'t_d_fake\'] = d_fake_t\n            batch_log[\'t_d_grad\'] = d_grad_t\n            batch_log[\'t_g\'] = g_t\n        return batch_log\n\n    def calc_grad_penalty(self, real, fake):\n        alpha = torch.rand(real.shape[0], 1)\n        alpha = alpha.expand(\n            real.shape[0], real.nelement() // real.shape[0]\n        ).contiguous().view(*real.shape).cuda()\n        inter = alpha * real + (1 - alpha) * fake\n        inter.requires_grad = True\n        err_d_inter = self.net_d(inter)\n        grads = torch.autograd.grad(\n            outputs=err_d_inter,\n            inputs=inter,\n            grad_outputs=torch.ones(err_d_inter.size()).cuda(),\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True\n        )[0]\n        grads = grads.view(grads.size(0), -1)\n        grad_penalty = (\n            ((grads + 1e-16).norm(2, dim=1) - self.opt.wgangp_norm) ** 2\n        ).mean() * self.opt.wgangp_lambda\n        return grad_penalty\n\n    def _vali_on_batch(self, epoch, batch_idx, batch):\n        batch_size = batch[\'voxel_canon\'].shape[0]\n        batch_log = {\'size\': batch_size}\n        with torch.no_grad():\n            noise, gen = self.net_g(batch_size)\n            disc = self.net_d(gen)\n        batch_log[\'loss\'] = -disc.mean().item()\n        # Save and visualize\n        if np.mod(epoch, self.opt.vis_every_train) == 0:\n            if batch_idx < self.opt.vis_batches_train:\n                outdir = join(self.full_logdir, \'epoch%04d_vali\' % epoch)\n                makedirs(outdir, exist_ok=True)\n                output = self.pack_output(noise, gen, disc)\n                self.visualizer.visualize(output, batch_idx, outdir)\n                np.savez(join(outdir, \'batch%04d\' % batch_idx), **output)\n        return batch_log\n\n    @staticmethod\n    def pack_output(noise, gen, disc):\n        out = {\n            \'noise\': noise.cpu().numpy(),\n            \'gen_voxel\': gen.cpu().numpy(),\n            \'disc\': disc.cpu().numpy(),\n        }\n        return out\n\n\nclass G(VoxelGenerator):\n    def __init__(self, nz):\n        super().__init__(nz=nz, nf=64, bias=False, res=128)\n        self.nz = nz\n        self.noise = torch.FloatTensor().cuda()\n\n    def forward(self, batch_size):\n        x = self.noise\n        x.resize_(batch_size, self.nz, 1, 1, 1).normal_(0, 1)\n        y = super().forward(x)\n        return x, y\n\n\nclass D(VoxelDiscriminator):\n    def __init__(self):\n        super().__init__(nf=64, bias=False, res=128)\n\n    def forward(self, x):\n        if x.dim() == 4:\n            x.unsqueeze_(1)\n        y = super().forward(x)\n        return y\n'"
networks/__init__.py,0,b''
networks/networks.py,1,"b'import torch.nn as nn\nfrom .revresnet import resnet18\nfrom torch import cat\n\n\nclass ImageEncoder(nn.Module):\n    """"""\n    Used for 2.5D maps to 3D voxels\n    """"""\n\n    def __init__(self, input_nc, encode_dims=200):\n        super().__init__()\n        resnet_m = resnet18(pretrained=True)\n        resnet_m.conv1 = nn.Conv2d(\n            input_nc, 64, 7, stride=2, padding=3, bias=False\n        )\n        resnet_m.avgpool = nn.AdaptiveAvgPool2d(1)\n        resnet_m.fc = nn.Linear(512, encode_dims)\n        self.main = nn.Sequential(resnet_m)\n\n    def forward(self, x):\n        return self.main(x)\n\n\nclass VoxelDecoder(nn.Module):\n    """"""\n    Used for 2.5D maps to 3D voxels\n    """"""\n\n    def __init__(self, n_dims=200, nf=512):\n        super().__init__()\n        self.main = nn.Sequential(\n            # volconv1\n            deconv3d_add3(n_dims, nf, True),\n            batchnorm3d(nf),\n            relu(),\n            # volconv2\n            deconv3d_2x(nf, nf // 2, True),\n            batchnorm3d(nf // 2),\n            relu(),\n            # volconv3\n            nn.Sequential(),  # NOTE: no-op for backward compatibility; consider removing\n            nn.Sequential(),  # NOTE\n            deconv3d_2x(nf // 2, nf // 4, True),\n            batchnorm3d(nf // 4),\n            relu(),\n            # volconv4\n            deconv3d_2x(nf // 4, nf // 8, True),\n            batchnorm3d(nf // 8),\n            relu(),\n            # volconv5\n            deconv3d_2x(nf // 8, nf // 16, True),\n            batchnorm3d(nf // 16),\n            relu(),\n            # volconv6\n            deconv3d_2x(nf // 16, 1, True)\n        )\n\n    def forward(self, x):\n        x_vox = x.view(x.size(0), -1, 1, 1, 1)\n        return self.main(x_vox)\n\n\nclass VoxelGenerator(nn.Module):\n    def __init__(self, nz=200, nf=64, bias=False, res=128):\n        super().__init__()\n        layers = [\n            # nzx1x1x1\n            deconv3d_add3(nz, nf * 8, bias),\n            batchnorm3d(nf * 8),\n            relu(),\n            # (nf*8)x4x4x4\n            deconv3d_2x(nf * 8, nf * 4, bias),\n            batchnorm3d(nf * 4),\n            relu(),\n            # (nf*4)x8x8x8\n            deconv3d_2x(nf * 4, nf * 2, bias),\n            batchnorm3d(nf * 2),\n            relu(),\n            # (nf*2)x16x16x16\n            deconv3d_2x(nf * 2, nf, bias),\n            batchnorm3d(nf),\n            relu(),\n            # nfx32x32x32\n        ]\n        if res == 64:\n            layers.append(deconv3d_2x(nf, 1, bias))\n            # 1x64x64x64\n        elif res == 128:\n            layers += [\n                deconv3d_2x(nf, nf, bias),\n                batchnorm3d(nf),\n                relu(),\n                # nfx64x64x64\n                deconv3d_2x(nf, 1, bias),\n                # 1x128x128x128\n            ]\n        else:\n            raise NotImplementedError(res)\n        layers.append(nn.Sigmoid())\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.main(x)\n\n\nclass VoxelDiscriminator(nn.Module):\n    def __init__(self, nf=64, bias=False, res=128):\n        super().__init__()\n        layers = [\n            # 1x64x64x64\n            conv3d_half(1, nf, bias),\n            relu_leaky(),\n            # nfx32x32x32\n            conv3d_half(nf, nf * 2, bias),\n            # batchnorm3d(nf * 2),\n            relu_leaky(),\n            # (nf*2)x16x16x16\n            conv3d_half(nf * 2, nf * 4, bias),\n            # batchnorm3d(nf * 4),\n            relu_leaky(),\n            # (nf*4)x8x8x8\n            conv3d_half(nf * 4, nf * 8, bias),\n            # batchnorm3d(nf * 8),\n            relu_leaky(),\n            # (nf*8)x4x4\n            conv3d_minus3(nf * 8, 1, bias),\n            # 1x1x1\n        ]\n        if res == 64:\n            pass\n        elif res == 128:\n            extra_layers = [\n                conv3d_half(nf, nf, bias),\n                relu_leaky(),\n            ]\n            layers = layers[:2] + extra_layers + layers[2:]\n        else:\n            raise NotImplementedError(res)\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x):\n        y = self.main(x)\n        return y.view(-1, 1).squeeze(1)\n\n\nclass Unet_3D(nn.Module):\n    def __init__(self, nf=20, in_channel=2, no_linear=False):\n        super(Unet_3D, self).__init__()\n        self.nf = nf\n        self.enc1 = Conv3d_block(in_channel, nf, 8, 2, 3)  # =>64\n        self.enc2 = Conv3d_block(nf, 2 * nf, 4, 2, 1)  # =>32\n        self.enc3 = Conv3d_block(2 * nf, 4 * nf, 4, 2, 1)  # =>16\n        self.enc4 = Conv3d_block(4 * nf, 8 * nf, 4, 2, 1)  # =>8\n        self.enc5 = Conv3d_block(8 * nf, 16 * nf, 4, 2, 1)  # =>4\n        self.enc6 = Conv3d_block(16 * nf, 32 * nf, 4, 1, 0)  # =>1\n        self.full_conv_block = nn.Sequential(\n            nn.Linear(32 * nf, 32 * nf),\n            nn.LeakyReLU(),\n        )\n        self.dec1 = Deconv3d_skip(32 * 2 * nf, 16 * nf, 4, 1, 0, 0)  # =>4\n        self.dec2 = Deconv3d_skip(16 * 2 * nf, 8 * nf, 4, 2, 1, 0)  # =>8\n        self.dec3 = Deconv3d_skip(8 * 2 * nf, 4 * nf, 4, 2, 1, 0)  # =>16\n        self.dec4 = Deconv3d_skip(4 * 2 * nf, 2 * nf, 4, 2, 1, 0)  # =>32\n        self.dec5 = Deconv3d_skip(4 * nf, nf, 8, 2, 3, 0)  # =>64\n        self.dec6 = Deconv3d_skip(\n            2 * nf, 1, 4, 2, 1, 0, is_activate=False)  # =>128\n        self.no_linear = no_linear\n\n    def forward(self, x):\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        enc4 = self.enc4(enc3)\n        enc5 = self.enc5(enc4)\n        enc6 = self.enc6(enc5)\n        # print(enc6.size())\n        if not self.no_linear:\n            flatten = enc6.view(enc6.size()[0], self.nf * 32)\n            bottleneck = self.full_conv_block(flatten)\n            bottleneck = bottleneck.view(enc6.size()[0], self.nf * 32, 1, 1, 1)\n            dec1 = self.dec1(bottleneck, enc6)\n        else:\n            dec1 = self.dec1(enc6, enc6)\n        dec2 = self.dec2(dec1, enc5)\n        dec3 = self.dec3(dec2, enc4)\n        dec4 = self.dec4(dec3, enc3)\n        dec5 = self.dec5(dec4, enc2)\n        out = self.dec6(dec5, enc1)\n        return out\n\n\nclass Conv3d_block(nn.Module):\n    def __init__(self, ncin, ncout, kernel_size, stride, pad, dropout=False):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv3d(ncin, ncout, kernel_size, stride, pad),\n            nn.BatchNorm3d(ncout),\n            nn.LeakyReLU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Deconv3d_skip(nn.Module):\n    def __init__(self, ncin, ncout, kernel_size, stride, pad, extra=0, is_activate=True):\n        super(Deconv3d_skip, self).__init__()\n        if is_activate:\n            self.net = nn.Sequential(\n                nn.ConvTranspose3d(ncin, ncout, kernel_size,\n                                   stride, pad, extra),\n                nn.BatchNorm3d(ncout),\n                nn.LeakyReLU()\n            )\n        else:\n            self.net = nn.ConvTranspose3d(\n                ncin, ncout, kernel_size, stride, pad, extra)\n\n    def forward(self, x, skip_in):\n        y = cat((x, skip_in), dim=1)\n        return self.net(y)\n\n\nclass ViewAsLinear(nn.Module):\n    @staticmethod\n    def forward(x):\n        return x.view(x.shape[0], -1)\n\n\ndef relu():\n    return nn.ReLU(inplace=True)\n\n\ndef relu_leaky():\n    return nn.LeakyReLU(0.2, inplace=True)\n\n\ndef maxpool():\n    return nn.MaxPool2d(3, stride=2, padding=0)\n\n\ndef dropout():\n    return nn.Dropout(p=0.5, inplace=False)\n\n\ndef conv3d_half(n_ch_in, n_ch_out, bias):\n    return nn.Conv3d(\n        n_ch_in, n_ch_out, 4, stride=2, padding=1, dilation=1, groups=1, bias=bias\n    )\n\n\ndef deconv3d_2x(n_ch_in, n_ch_out, bias):\n    return nn.ConvTranspose3d(\n        n_ch_in, n_ch_out, 4, stride=2, padding=1, dilation=1, groups=1, bias=bias\n    )\n\n\ndef conv3d_minus3(n_ch_in, n_ch_out, bias):\n    return nn.Conv3d(\n        n_ch_in, n_ch_out, 4, stride=1, padding=0, dilation=1, groups=1, bias=bias\n    )\n\n\ndef deconv3d_add3(n_ch_in, n_ch_out, bias):\n    return nn.ConvTranspose3d(\n        n_ch_in, n_ch_out, 4, stride=1, padding=0, dilation=1, groups=1, bias=bias\n    )\n\n\ndef batchnorm1d(n_feat):\n    return nn.BatchNorm1d(n_feat, eps=1e-5, momentum=0.1, affine=True)\n\n\ndef batchnorm(n_feat):\n    return nn.BatchNorm2d(n_feat, eps=1e-5, momentum=0.1, affine=True)\n\n\ndef batchnorm3d(n_feat):\n    return nn.BatchNorm3d(n_feat, eps=1e-5, momentum=0.1, affine=True)\n\n\ndef fc(n_in, n_out):\n    return nn.Linear(n_in, n_out, bias=True)\n'"
networks/revresnet.py,2,"b'""""""\nThis is an implementation of a U-Net using ResNet-18 blocks\n""""""\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet18\n\n\ndef deconv3x3(in_planes, out_planes, stride=1, output_padding=0):\n    return nn.ConvTranspose2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False,\n        output_padding=output_padding\n    )\n\n\nclass RevBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, upsample=None):\n        super(RevBasicBlock, self).__init__()\n        self.deconv1 = deconv3x3(inplanes, planes, stride=1)\n        # Note that in ResNet, the stride is on the second layer\n        # Here we put it on the first layer as the mirrored block\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.deconv2 = deconv3x3(planes, planes, stride=stride,\n                                 output_padding=1 if stride > 1 else 0)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.upsample = upsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.deconv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.deconv2(out)\n        out = self.bn2(out)\n        if self.upsample is not None:\n            residual = self.upsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass RevBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, upsample=None):\n        super(RevBottleneck, self).__init__()\n        bottleneck_planes = int(inplanes / 4)\n        self.deconv1 = nn.ConvTranspose2d(\n            inplanes,\n            bottleneck_planes,\n            kernel_size=1,\n            bias=False,\n            stride=1\n        ) # conv and deconv are the same when kernel size is 1\n        self.bn1 = nn.BatchNorm2d(bottleneck_planes)\n        self.deconv2 = nn.ConvTranspose2d(\n            bottleneck_planes,\n            bottleneck_planes,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(bottleneck_planes)\n        self.deconv3 = nn.ConvTranspose2d(\n            bottleneck_planes,\n            planes,\n            kernel_size=1,\n            bias=False,\n            stride=stride,\n            output_padding=1 if stride > 0 else 0\n        )\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.upsample = upsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.deconv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.deconv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.deconv3(out)\n        out = self.bn3(out)\n        if self.upsample is not None:\n            residual = self.upsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass RevResNet(nn.Module):\n    def __init__(self, block, layers, planes, inplanes=None, out_planes=5):\n        """"""\n        planes: # output channels for each block\n        inplanes: # input channels for the input at each layer\n            If missing, it will be inferred.\n        """"""\n        if inplanes is None:\n            inplanes = [512]\n        self.inplanes = inplanes[0]\n        super(RevResNet, self).__init__()\n        inplanes_after_blocks = inplanes[4] if len(inplanes) > 4 else planes[3]\n        self.deconv1 = nn.ConvTranspose2d(\n            inplanes_after_blocks,\n            planes[3],\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            output_padding=1\n        )\n        self.deconv2 = nn.ConvTranspose2d(\n            planes[3],\n            out_planes,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False,\n            output_padding=1\n        )\n        self.bn1 = nn.BatchNorm2d(planes[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, planes[0], layers[0], stride=2)\n        if len(inplanes) > 1:\n            self.inplanes = inplanes[1]\n        self.layer2 = self._make_layer(block, planes[1], layers[1], stride=2)\n        if len(inplanes) > 2:\n            self.inplanes = inplanes[2]\n        self.layer3 = self._make_layer(block, planes[2], layers[2], stride=2)\n        if len(inplanes) > 3:\n            self.inplanes = inplanes[3]\n        self.layer4 = self._make_layer(block, planes[3], layers[3])\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        upsample = None\n        if stride != 1 or self.inplanes != planes:\n            upsample = nn.Sequential(\n                nn.ConvTranspose2d(\n                    self.inplanes,\n                    planes,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                    output_padding=1 if stride > 1 else 0\n                ),\n                nn.BatchNorm2d(planes),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, upsample))\n        self.inplanes = planes\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.deconv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.deconv2(x)\n        return x\n\n\ndef revresnet18(**kwargs):\n    model = RevResNet(\n        RevBasicBlock,\n        [2, 2, 2, 2],\n        [512, 256, 128, 64],\n        **kwargs\n    )\n    return model\n\n\ndef revuresnet18(**kwargs):\n    """"""\n    Reverse ResNet-18 compatible with the U-Net setting\n    """"""\n    model = RevResNet(\n        RevBasicBlock,\n        [2, 2, 2, 2],\n        [256, 128, 64, 64],\n        inplanes=[512, 512, 256, 128, 128],\n        **kwargs\n    )\n    return model\n\n\ndef _num_parameters(net):\n    return sum([\n        x.numel() for x in list(net.parameters())\n    ])\n\n\ndef main():\n    net = resnet18()\n    revnet = revresnet18()\n    net.avgpool = nn.AvgPool2d(kernel_size=8)\n    for name, mod in net.named_children():\n        mod.__name = name\n        mod.register_forward_hook(\n            lambda mod, input, output: print(mod.__name, output.shape)\n        )\n    for name, mod in revnet.named_children():\n        mod.__name = name\n        mod.register_forward_hook(\n            lambda mod, input, output: print(mod.__name, output.shape)\n        )\n    # print(net)\n    print(\'resnet\', _num_parameters(net))\n    net(torch.zeros(2, 3, 256, 256))\n    print(\'\')\n    print(\'revresnet\', _num_parameters(revnet))\n    # print(revnet)\n    revnet(torch.zeros(2, 512, 8, 8))\n    print(\'\')\n    revunet = RevResNet(RevBasicBlock, [2, 2, 2, 2], [512, 512, 256, 128])\n    print(\'revunet\', _num_parameters(revunet))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
networks/uresnet.py,2,"b'import torch\nfrom torch import nn\nfrom networks.revresnet import revuresnet18, resnet18\n\n\nclass Net(nn.Module):\n    """"""\n    Used for RGB to 2.5D maps\n    """"""\n\n    def __init__(self, out_planes, layer_names, input_planes=3):\n        super().__init__()\n\n        # Encoder\n        module_list = list()\n        resnet = resnet18(pretrained=True)\n        in_conv = nn.Conv2d(input_planes, 64, kernel_size=7, stride=2, padding=3,\n                            bias=False)\n        module_list.append(\n            nn.Sequential(\n                resnet.conv1 if input_planes == 3 else in_conv,\n                resnet.bn1,\n                resnet.relu,\n                resnet.maxpool\n            )\n        )\n        module_list.append(resnet.layer1)\n        module_list.append(resnet.layer2)\n        module_list.append(resnet.layer3)\n        module_list.append(resnet.layer4)\n        self.encoder = nn.ModuleList(module_list)\n        self.encoder_out = None\n\n        # Decoder\n        self.decoders = {}\n        for out_plane, layer_name in zip(out_planes, layer_names):\n            module_list = list()\n            revresnet = revuresnet18(out_planes=out_plane)\n            module_list.append(revresnet.layer1)\n            module_list.append(revresnet.layer2)\n            module_list.append(revresnet.layer3)\n            module_list.append(revresnet.layer4)\n            module_list.append(\n                nn.Sequential(\n                    revresnet.deconv1,\n                    revresnet.bn1,\n                    revresnet.relu,\n                    revresnet.deconv2\n                )\n            )\n            module_list = nn.ModuleList(module_list)\n            setattr(self, \'decoder_\' + layer_name, module_list)\n            self.decoders[layer_name] = module_list\n\n    def forward(self, im):\n        # Encode\n        feat = im\n        feat_maps = list()\n        for f in self.encoder:\n            feat = f(feat)\n            feat_maps.append(feat)\n        self.encoder_out = feat_maps[-1]\n        # Decode\n        outputs = {}\n        for layer_name, decoder in self.decoders.items():\n            x = feat_maps[-1]\n            for idx, f in enumerate(decoder):\n                x = f(x)\n                if idx < len(decoder) - 1:\n                    feat_map = feat_maps[-(idx + 2)]\n                    assert feat_map.shape[2:4] == x.shape[2:4]\n                    x = torch.cat((x, feat_map), dim=1)\n            outputs[layer_name] = x\n        return outputs\n\n\nclass Net_inpaint(nn.Module):\n    """"""\n    Used for RGB to 2.5D maps\n    """"""\n\n    def __init__(self, out_planes, layer_names, input_planes=3):\n        super().__init__()\n\n        # Encoder\n        module_list = list()\n        resnet = resnet18(pretrained=True)\n        in_conv = nn.Conv2d(input_planes, 64, kernel_size=7, stride=2, padding=3,\n                            bias=False)\n        module_list.append(\n            nn.Sequential(\n                resnet.conv1 if input_planes == 3 else in_conv,\n                resnet.bn1,\n                resnet.relu,\n                resnet.maxpool\n            )\n        )\n        module_list.append(resnet.layer1)\n        module_list.append(resnet.layer2)\n        module_list.append(resnet.layer3)\n        module_list.append(resnet.layer4)\n        self.encoder = nn.ModuleList(module_list)\n        self.encoder_out = None\n        self.deconv2 = nn.ConvTranspose2d(64, 1, kernel_size=8, stride=2, padding=3, bias=False, output_padding=0)\n        # Decoder\n        self.decoders = {}\n        for out_plane, layer_name in zip(out_planes, layer_names):\n            module_list = list()\n            revresnet = revuresnet18(out_planes=out_plane)\n            module_list.append(revresnet.layer1)\n            module_list.append(revresnet.layer2)\n            module_list.append(revresnet.layer3)\n            module_list.append(revresnet.layer4)\n            module_list.append(\n                nn.Sequential(\n                    revresnet.deconv1,\n                    revresnet.bn1,\n                    revresnet.relu,\n                    self.deconv2\n                )\n            )\n            module_list = nn.ModuleList(module_list)\n            setattr(self, \'decoder_\' + layer_name, module_list)\n            self.decoders[layer_name] = module_list\n\n    def forward(self, im):\n        # Encode\n        feat = im\n        feat_maps = list()\n        for f in self.encoder:\n            feat = f(feat)\n            feat_maps.append(feat)\n        self.encoder_out = feat_maps[-1]\n        # Decode\n        outputs = {}\n        for layer_name, decoder in self.decoders.items():\n            x = feat_maps[-1]\n            for idx, f in enumerate(decoder):\n                x = f(x)\n                if idx < len(decoder) - 1:\n                    feat_map = feat_maps[-(idx + 2)]\n                    assert feat_map.shape[2:4] == x.shape[2:4]\n                    x = torch.cat((x, feat_map), dim=1)\n            outputs[layer_name] = x\n        return outputs\n'"
options/__init__.py,0,b''
options/options_test.py,0,"b'import sys\nimport argparse\nfrom datasets import get_dataset\nfrom models import get_model\nfrom options import options_train\n\n\ndef add_general_arguments(parser):\n    parser, _ = options_train.add_general_arguments(parser)\n\n    # Dataset IO\n    parser.add_argument(\'--input_rgb\', type=str, required=True,\n                        help=""Input RGB filename patterns, e.g., \'/path/to/images/*_rgb.png\'"")\n    parser.add_argument(\'--input_mask\', type=str, required=True,\n                        help=(""Corresponding mask filename patterns, e.g., \'/path/to/images/*_mask.png\'. ""\n                              ""For MarrNet/ShapeHD, masks are not required, so used only for bbox cropping. ""\n                              ""For GenRe, masks are input together with RGB""))\n\n    # Network\n    parser.add_argument(\'--net_file\', type=str, required=True,\n                        help=""Path to the trained network"")\n\n    # Output\n    parser.add_argument(\'--output_dir\', type=str, required=True,\n                        help=""Output directory"")\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n                        help=""Whether to overwrite the output folder if it exists"")\n\n    return parser\n\n\ndef parse(add_additional_arguments=None):\n    parser = argparse.ArgumentParser()\n    parser = add_general_arguments(parser)\n    if add_additional_arguments:\n        parser, _ = add_additional_arguments(parser)\n    opt_general, _ = parser.parse_known_args()\n    net_name = opt_general.net\n    del opt_general\n    dataset_name = \'test\'\n\n    # Add parsers depending on dataset and models\n    parser, _ = get_dataset(dataset_name).add_arguments(parser)\n    parser, _ = get_model(net_name, test=True).add_arguments(parser)\n\n    # Manually add \'-h\' after adding all parser arguments\n    if \'--printhelp\' in sys.argv:\n        sys.argv.append(\'-h\')\n\n    opt = parser.parse_args()\n    return opt\n'"
options/options_train.py,1,"b'import sys\nimport argparse\nimport torch\nfrom util.util_print import str_warning\nfrom datasets import get_dataset\nfrom models import get_model\n\n\ndef add_general_arguments(parser):\n    # Parameters that will NOT be overwritten when resuming\n    unique_params = {\'gpu\', \'resume\', \'epoch\', \'workers\', \'batch_size\', \'save_net\', \'epoch_batches\', \'logdir\'}\n\n    parser.add_argument(\'--gpu\', default=\'0\', type=str,\n                        help=\'gpu to use\')\n    parser.add_argument(\'--manual_seed\', type=int, default=None,\n                        help=\'manual seed for randomness\')\n    parser.add_argument(\'--resume\', type=int, default=0,\n                        help=\'resume training by loading checkpoint.pt or best.pt. Use 0 for training from scratch, -1 for last and -2 for previous best. Use positive number for a specific epoch. \\\n                            Most options will be overwritten to resume training with exactly same environment\')\n    parser.add_argument(\n        \'--suffix\', default=\'\', type=str,\n        help=""Suffix for `logdir` that will be formatted with `opt`, e.g., \'{classes}_lr{lr}\'""\n    )\n    parser.add_argument(\'--epoch\', type=int, default=0,\n                        help=\'number of epochs to train\')\n\n    # Dataset IO\n    parser.add_argument(\'--dataset\', type=str, default=None,\n                        help=\'dataset to use\')\n    parser.add_argument(\'--workers\', type=int, default=4,\n                        help=\'number of data loading workers\')\n    parser.add_argument(\'--classes\', default=\'chair\', type=str,\n                        help=\'class to use\')\n    parser.add_argument(\'--batch_size\', type=int, default=16,\n                        help=\'training batch size\')\n    parser.add_argument(\'--epoch_batches\', default=None, type=int, help=\'number of batches used per epoch\')\n    parser.add_argument(\'--eval_batches\', default=None,\n                        type=int, help=\'max number of batches used for evaluation per epoch\')\n    parser.add_argument(\'--eval_at_start\', action=\'store_true\',\n                        help=\'run evaluation before starting to train\')\n    parser.add_argument(\'--log_time\', action=\'store_true\', help=\'adding time log\')\n\n    # Network name\n    parser.add_argument(\'--net\', type=str, required=True,\n                        help=\'network type to use\')\n\n    # Optimizer\n    parser.add_argument(\'--optim\', type=str, default=\'adam\',\n                        help=\'optimizer to use\')\n    parser.add_argument(\'--lr\', type=float, default=1e-4,\n                        help=\'learning rate\')\n    parser.add_argument(\'--adam_beta1\', type=float, default=0.5,\n                        help=\'beta1 of adam\')\n    parser.add_argument(\'--adam_beta2\', type=float, default=0.9,\n                        help=\'beta2 of adam\')\n    parser.add_argument(\'--sgd_momentum\', type=float, default=0.9,\n                        help=""momentum factor of SGD"")\n    parser.add_argument(\'--sgd_dampening\', type=float, default=0,\n                        help=""dampening for momentum of SGD"")\n    parser.add_argument(\'--wdecay\', type=float, default=0.0,\n                        help=\'weight decay\')\n\n    # Logging and visualization\n    parser.add_argument(\'--logdir\', type=str, default=None,\n                        help=\'Root directory for logging. Actual dir is [logdir]/[net_classes_dataset]/[expr_id]\')\n    parser.add_argument(\'--log_batch\', action=\'store_true\',\n                        help=\'Log batch loss\')\n    parser.add_argument(\'--expr_id\', type=int, default=0,\n                        help=\'Experiment index. non-positive ones are overwritten by default. Use 0 for code test. \')\n    parser.add_argument(\'--save_net\', type=int, default=1,\n                        help=\'Period of saving network weights\')\n    parser.add_argument(\'--save_net_opt\', action=\'store_true\',\n                        help=\'Save optimizer state in regular network saving\')\n    parser.add_argument(\'--vis_every_vali\', default=1, type=int,\n                        help=""Visualize every N epochs during validation"")\n    parser.add_argument(\'--vis_every_train\', default=1, type=int,\n                        help=""Visualize every N epochs during training"")\n    parser.add_argument(\'--vis_batches_vali\', type=int, default=10,\n                        help=""# batches to visualize during validation"")\n    parser.add_argument(\'--vis_batches_train\', type=int, default=10,\n                        help=""# batches to visualize during training"")\n    parser.add_argument(\'--tensorboard\', action=\'store_true\',\n                        help=\'Use tensorboard for logging. If enabled, the output log will be at [logdir]/[tensorboard]/[net_classes_dataset]/[expr_id]\')\n    parser.add_argument(\'--vis_workers\', default=4, type=int, help=""# workers for the visualizer"")\n    parser.add_argument(\'--vis_param_f\', default=None, type=str,\n                        help=""Parameter file read by the visualizer on every batch; defaults to \'visualize/config.json\'"")\n\n    return parser, unique_params\n\n\ndef overwrite(opt, opt_f_old, unique_params):\n    opt_dict = vars(opt)\n    opt_dict_old = torch.load(opt_f_old)\n    for k, v in opt_dict_old.items():\n        if k in opt_dict:\n            if (k not in unique_params) and (opt_dict[k] != v):\n                print(str_warning, ""Overwriting %s for resuming training: %s -> %s""\n                      % (k, str(opt_dict[k]), str(v)))\n                opt_dict[k] = v\n        else:\n            print(str_warning, ""Ignoring %s, an old option that no longer exists"" % k)\n    opt = argparse.Namespace(**opt_dict)\n    return opt\n\n\ndef parse(add_additional_arguments=None):\n    parser = argparse.ArgumentParser()\n    parser, unique_params = add_general_arguments(parser)\n    if add_additional_arguments is not None:\n        parser, unique_params_additional = add_additional_arguments(parser)\n        unique_params = unique_params.union(unique_params_additional)\n    opt_general, _ = parser.parse_known_args()\n    dataset_name, net_name = opt_general.dataset, opt_general.net\n    del opt_general\n\n    # Add parsers depending on dataset and models\n    parser, unique_params_dataset = get_dataset(dataset_name).add_arguments(parser)\n    parser, unique_params_model = get_model(net_name).add_arguments(parser)\n\n    # Manually add \'-h\' after adding all parser arguments\n    if \'--printhelp\' in sys.argv:\n        sys.argv.append(\'-h\')\n\n    opt = parser.parse_args()\n    unique_params = unique_params.union(unique_params_dataset)\n    unique_params = unique_params.union(unique_params_model)\n    return opt, unique_params\n'"
toolbox/__init__.py,0,b''
toolbox/spherical_proj.py,9,"b""import numpy as np\nimport torch\nfrom .calc_prob.calc_prob.functions.calc_prob import CalcStopProb\n\n\ndef gen_sph_grid(res=128):\n    pi = np.pi\n    phi = np.linspace(0, 180, res * 2 + 1)[1::2]\n    theta = np.linspace(0, 360, res + 1)[:-1]\n    grid = np.zeros([res, res, 3])\n    for idp, p in enumerate(phi):\n        for idt, t in enumerate(theta):\n            grid[idp, idt, 2] = np.cos((p * pi / 180))\n            proj = np.sin((p * pi / 180))\n            grid[idp, idt, 0] = proj * np.cos(t * pi / 180)\n            grid[idp, idt, 1] = proj * np.sin(t * pi / 180)\n    grid = np.reshape(grid, (1, 1, res, res, 3))\n    return torch.from_numpy(grid).float()\n\n\ndef sph_pad(sph_tensor, padding_margin=16):\n    F = torch.nn.functional\n    pad2d = (padding_margin, padding_margin, padding_margin, padding_margin)\n    rep_padded_sph = F.pad(sph_tensor, pad2d, mode='replicate')\n    _, _, h, w = rep_padded_sph.shape\n    rep_padded_sph[:, :, :, 0:padding_margin] = rep_padded_sph[:, :, :, w - 2 * padding_margin:w - padding_margin]\n    rep_padded_sph[:, :, :, h - padding_margin:] = rep_padded_sph[:, :, :, padding_margin:2 * padding_margin]\n    return rep_padded_sph\n\n\nclass render_spherical(torch.nn.Module):\n    def __init__(self, sph_res=128, z_res=256):\n        super().__init__()\n        self.sph_res = sph_res\n        self.z_res = z_res\n        self.gen_grid()\n        self.calc_stop_prob = CalcStopProb().apply\n\n    def gen_grid(self):\n        res = self.sph_res\n        z_res = self.z_res\n        pi = np.pi\n        phi = np.linspace(0, 180, res * 2 + 1)[1::2]\n        theta = np.linspace(0, 360, res + 1)[:-1]\n        grid = np.zeros([res, res, 3])\n        for idp, p in enumerate(phi):\n            for idt, t in enumerate(theta):\n                grid[idp, idt, 2] = np.cos((p * pi / 180))\n                proj = np.sin((p * pi / 180))\n                grid[idp, idt, 0] = proj * np.cos(t * pi / 180)\n                grid[idp, idt, 1] = proj * np.sin(t * pi / 180)\n        grid = np.reshape(grid * 2, (res, res, 3))\n        alpha = np.zeros([1, 1, z_res, 1])\n        alpha[0, 0, :, 0] = np.linspace(0, 1, z_res)\n        grid = grid[:, :, np.newaxis, :]\n        grid = grid * (1 - alpha)\n        grid = torch.from_numpy(grid).float()\n        depth_weight = torch.linspace(0, 1, self.z_res)\n        self.register_buffer('depth_weight', depth_weight)\n        self.register_buffer('grid', grid)\n\n    def forward(self, vox):\n        grid = self.grid.expand(vox.shape[0], -1, -1, -1, -1)\n        vox = vox.permute(0, 1, 4, 3, 2)\n        prob_sph = torch.nn.functional.grid_sample(vox, grid)\n        prob_sph = torch.clamp(prob_sph, 1e-5, 1 - 1e-5)\n        sph_stop_prob = self.calc_stop_prob(prob_sph)\n        exp_depth = torch.matmul(sph_stop_prob, self.depth_weight)\n        back_groud_prob = torch.prod(1.0 - prob_sph, dim=4)\n        back_groud_prob = back_groud_prob * 1.0\n        exp_depth = exp_depth + back_groud_prob\n        return exp_depth\n"""
util/__init__.py,0,b''
util/util_cam_para.py,0,"b'import numpy as np\n\n\ndef read_cam_para_from_xml(xml_name):\n    # azi ele only\n    import xml.etree.ElementTree\n    e = xml.etree.ElementTree.parse(xml_name).getroot()\n\n    assert len(e.findall(\'sensor\')) == 1\n    for x in e.findall(\'sensor\'):\n        assert len(x.findall(\'transform\')) == 1\n        for y in x.findall(\'transform\'):\n            assert len(y.findall(\'lookAt\')) == 1\n            for z in y.findall(\'lookAt\'):\n                origin = np.array(z.get(\'origin\').split(\',\'), dtype=np.float32)\n                # up = np.array(z.get(\'up\').split(\',\'), dtype=np.float32)\n\n    x, y, z = origin\n    elevation = np.arctan2(y, np.sqrt(x ** 2 + z ** 2))\n    azimuth = np.arctan2(x, z) + np.pi\n    if azimuth >= np.pi:\n        azimuth -= 2 * np.pi\n    assert azimuth >= -np.pi and azimuth <= np.pi\n    assert elevation >= -np.pi / 2. and elevation <= np.pi / 2.\n    return azimuth, elevation\n\n\ndef raw_camparam_from_xml(path, pose=""lookAt""):\n    import xml.etree.ElementTree as ET\n    tree = ET.parse(path)\n    elm = tree.find(""./sensor/transform/"" + pose)\n    camparam = elm.attrib\n    origin = np.fromstring(camparam[\'origin\'], dtype=np.float32, sep=\',\')\n    target = np.fromstring(camparam[\'target\'], dtype=np.float32, sep=\',\')\n    up = np.fromstring(camparam[\'up\'], dtype=np.float32, sep=\',\')\n    height = int(\n        tree.find(""./sensor/film/integer[@name=\'height\']"").attrib[\'value\'])\n    width = int(\n        tree.find(""./sensor/film/integer[@name=\'width\']"").attrib[\'value\'])\n\n    camparam = dict()\n    camparam[\'origin\'] = origin\n    camparam[\'up\'] = up\n    camparam[\'target\'] = target\n    camparam[\'height\'] = height\n    camparam[\'width\'] = width\n    return camparam\n\n\ndef get_object_rotation(xml_path, style=\'zup\'):\n    style_set = [\'yup\', \'zup\', \'spherical_proj\']\n    assert(style in style_set)\n    camparam = raw_camparam_from_xml(xml_path)\n    if style == \'zup\':\n        Rx = camparam[\'target\'] - camparam[\'origin\']\n        up = camparam[\'up\']\n        Rz = np.cross(Rx, up)\n        Ry = np.cross(Rz, Rx)\n        Rx /= np.linalg.norm(Rx)\n        Ry /= np.linalg.norm(Ry)\n        Rz /= np.linalg.norm(Rz)\n        R = np.array([Rx, Ry, Rz])\n        R_coord = np.array([[1, 0, 0], [0, 0, -1], [0, 1, 0]])\n        R = R_coord @ R\n        R = R @ R_coord.transpose()\n    elif style == \'yup\':\n        Rx = camparam[\'target\'] - camparam[\'origin\']\n        up = camparam[\'up\']\n        Rz = np.cross(Rx, up)\n        Ry = np.cross(Rz, Rx)\n        Rx /= np.linalg.norm(Rx)\n        Ry /= np.linalg.norm(Ry)\n        Rz /= np.linalg.norm(Rz)\n        #print(Rx, Ry, Rz)\n        # no transpose needed!\n        R = np.array([Rx, Ry, Rz])\n    elif style == \'spherical_proj\':\n        Rx = camparam[\'target\'] - camparam[\'origin\']\n        up = camparam[\'up\']\n        Rz = np.cross(Rx, up)\n        Ry = np.cross(Rz, Rx)\n        Rx /= np.linalg.norm(Rx)\n        Ry /= np.linalg.norm(Ry)\n        Rz /= np.linalg.norm(Rz)\n        #print(Rx, Ry, Rz)\n        # no transpose needed!\n        R = np.array([Rx, Ry, Rz])\n\n        raise NotImplementedError\n    return R\n\n\ndef get_object_rotation_translation(xml_path, style=\'zup\'):\n    pass\n\n\ndef _devide_into_section(angle, num_section, lower_bound, upper_bound):\n    rst = np.zeros(num_section)\n    per_section_size = (upper_bound - lower_bound) / num_section\n    angle -= per_section_size / 2\n    if angle < lower_bound:\n        angle += upper_bound - lower_bound\n    idx = int((angle - lower_bound) / per_section_size)\n    rst[idx] = 1\n    return rst\n\n\ndef _section_to_angle(idx, num_section, lower_bound, upper_bound):\n    per_section_size = (upper_bound - lower_bound) / num_section\n\n    angle = (idx + 0.5) * per_section_size + lower_bound\n    angle += per_section_size / 2\n    if angle > upper_bound:\n        angle -= upper_bound - lower_bound\n    return angle\n\n\ndef azimuth_to_onehot(azimuth, num_azimuth):\n    return _devide_into_section(azimuth, num_azimuth, -np.pi, np.pi)\n\n\ndef elevation_to_onehot(elevation, num_elevation):\n    return _devide_into_section(elevation, num_elevation, -np.pi / 2., np.pi / 2.)\n\n\ndef onehot_to_azimuth(v, num_azimuth):\n    idx = np.argmax(v)\n    return _section_to_angle(idx, num_azimuth, -np.pi, np.pi)\n\n\ndef onehot_to_elevation(v, num_elevation):\n    idx = np.argmax(v)\n    return _section_to_angle(idx, num_elevation, -np.pi / 2., np.pi / 2.)\n\n\nif __name__ == \'__main__\':\n    num_azimuth = 24\n    num_elevation = 12\n    for i in range(num_azimuth):\n        rst = np.zeros(num_azimuth)\n        rst[i] = 1\n        print(onehot_to_azimuth(rst, num_azimuth))\n\n    \'\'\'\n    for i in range(100):\n        angle = (np.random.rand() - 0.5) * np.pi * 2\n        print(angle, np.argmax(azimuth_to_onehot(angle, 24)), onehot_to_azimuth(azimuth_to_onehot(angle, 24), 24))\n        assert np.abs(angle - onehot_to_azimuth(azimuth_to_onehot(angle, 24), 24)) < 2 * np.pi / 24\n    \'\'\'\n'"
util/util_camera.py,0,"b""import numpy as np\nfrom scipy.misc import imresize\nfrom numba import jit\n\n\n@jit\ndef calc_ptnum(triangle, density):\n    pt_num_tr = np.zeros(len(triangle)).astype(int)\n    pt_num_total = 0\n    for tr_id, tr in enumerate(triangle):\n        a = np.linalg.norm(np.cross(tr[1] - tr[0], tr[2] - tr[0])) / 2\n        ptnum = max(int(a * density), 1)\n        pt_num_tr[tr_id] = ptnum\n        pt_num_total += ptnum\n    return pt_num_tr, pt_num_total\n\n\nclass Camera():\n    # camera coordinates: y up, z forward, x right.\n    # consistent with blender definitions.\n    # res = [w,h]\n    def __init__(self):\n        self.position = np.array([1.6, 0, 0])\n        self.rx = np.array([0, 1, 0])\n        self.ry = np.array([0, 0, 1])\n        self.rz = np.array([1, 0, 0])\n        self.res = [800, 600]\n        self.focal_length = 0.05\n        # set the diagnal to be 35mm film's diagnal\n        self.set_diagal((0.036**2 + 0.024**2)**0.5)\n\n    def rotate(self, rot_mat):\n        self.rx = rot_mat[:, 0]\n        self.ry = rot_mat[:, 1]\n        self.rz = rot_mat[:, 2]\n\n    def move_cam(self, new_pos):\n        self.position = new_pos\n\n    def set_pose(self, inward, up):\n        self.rx = np.cross(up, inward)\n        self.ry = np.array(up)\n        self.rz = np.array(inward)\n        self.rx /= np.linalg.norm(self.rx)\n        self.ry /= np.linalg.norm(self.ry)\n        self.rz /= np.linalg.norm(self.rz)\n\n    def set_diagal(self, diag):\n        h_relative = self.res[1] / self.res[0]\n        self.sensor_width = np.sqrt(diag**2 / (1 + h_relative**2))\n\n    def lookat(self, orig, target, up):\n        self.position = np.array(orig)\n        target = np.array(target)\n        inward = self.position - target\n        right = np.cross(up, inward)\n        up = np.cross(inward, right)\n        self.set_pose(inward, up)\n\n    def set_cam_from_mitsuba(self, path):\n        camparam = util.cam_from_mitsuba(path)\n        self.lookat(orig=camparam['origin'],\n                    up=camparam['up'], target=camparam['target'])\n        self.res = [camparam['width'], camparam['height']]\n        self.focal_length = 0.05\n        # set the diagnal to be 35mm film's diagnal\n        self.set_diagal((0.036**2 + 0.024**2)**0.5)\n\n    def project_point(self, pt):\n        # project global point to image coordinates in pixels (float not\n        # integer).\n        res = self.res\n        rel = np.array(pt) - self.position\n        depth = -np.dot(rel, self.rz)\n        if rel.ndim != 1:\n            depth = depth.reshape([np.size(depth, axis=0), 1])\n        rel_plane = rel * self.focal_length / depth\n        rel_width = np.dot(rel_plane, self.rx)\n        rel_height = np.dot(rel_plane, self.ry)\n        topleft = np.array([-self.sensor_width / 2,\n                            self.sensor_width * (res[1] / res[0]) / 2])\n        pix_size = self.sensor_width / res[0]\n        topleft += np.array([pix_size / 2, -pix_size / 2])\n        im_pix_x = (topleft[1] - rel_height) / pix_size\n        im_pix_y = (rel_width - topleft[0]) / pix_size\n        return im_pix_x, im_pix_y\n\n    def project_depth(self, pt, depth_type='ray'):\n        if depth_type == 'ray':\n            if np.array(pt).ndim == 1:\n                return np.linalg.norm(pt - self.position)\n            return np.linalg.norm(pt - self.position, axis=1)\n        else:\n            return np.dot(pt - self.position, -self.rz)\n\n    def pack(self):\n        params = []\n        params += self.res\n        params += [self.sensor_width]\n        params += self.position.tolist()\n        params += self.rx.tolist()\n        params += self.ry.tolist()\n        params += self.rz.tolist()\n        params += [self.focal_length]\n        return params\n\n\nclass tsdf_renderer:\n    def __init__(self):\n        self.camera = Camera()\n        self.depth = []\n\n    def load_depth_map_npy(self, path):\n        self.depth = np.load(path)\n\n    def back_project_ptcloud(self, upsample=1.0, depth_type='ray'):\n        if not self.check_valid():\n            return\n        mask = np.where(self.depth < 0, 0, 1)\n        depth = imresize(self.depth, upsample, mode='F', interp='bilinear')\n        up_mask = imresize(mask, upsample, mode='F', interp='bilinear')\n        up_mask = np.where(up_mask < 1, 0, 1)\n        ind = np.where(up_mask == 0)\n        depth[ind] = -1\n        # res = self.camera.res\n        res = np.array([0, 0])\n        res[0] = np.shape(depth)[1]  # width\n        res[1] = np.shape(depth)[0]  # height\n        self.check_depth = np.zeros([res[1], res[0]], dtype=np.float32) - 1\n        pt_pos = np.where(up_mask == 1)\n        ptnum = len(pt_pos[0])\n        ptcld = np.zeros([ptnum, 3])\n        half_width = self.camera.sensor_width / 2\n        half_height = half_width * res[1] / res[0]\n        pix_size = self.camera.sensor_width / res[0]\n        top_left = self.camera.position \\\n            - self.camera.focal_length * self.camera.rz\\\n            - half_width * self.camera.rx\\\n            + half_height * self.camera.ry\n\n        for x in range(ptnum):\n            height_id = pt_pos[0][x]\n            width_id = pt_pos[1][x]\n            pix_depth = depth[height_id, width_id]\n            pix_coord = - (height_id + 0.5) * pix_size * self.camera.ry\\\n                + (width_id + 0.5) * pix_size * self.camera.rx\\\n                + top_left\n            pix_rel = pix_coord - self.camera.position\n            if depth_type == 'plane':\n                ptcld_pos = (pix_rel)\\\n                    * (pix_depth / self.camera.focal_length) \\\n                    + self.camera.position\n                back_project_depth = -np.dot(pix_rel, self.camera.rz)\n            else:\n                ptcld_pos = (pix_rel / np.linalg.norm(pix_rel))\\\n                    * (pix_depth) + self.camera.position\n                back_project_depth = np.linalg.norm(\n                    ptcld_pos - self.camera.position)\n            ptcld[x, :] = ptcld_pos\n            self.check_depth[height_id, width_id] = back_project_depth\n        self.ptcld = ptcld\n        self.pt_pos = pt_pos\n\n    def check_valid(self, warning=True):\n        if self.depth == []:\n            print('No depth map available!')\n            return False\n        shape = np.shape(self.depth)\n        if warning and (shape[0] != self.camera.res[1] or shape[1] != self.camera.res[0]):\n            print('depth map and camera resolution mismatch!')\n            print('camera: {}'.format(self.camera.res))\n            print('depth:  {}'.format(shape))\n            return True\n        return True\n"""
util/util_img.py,0,"b'""""""\nImage Transforms for Data Augmentation and Input Normalization\n\nAdapted from https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua\n\nGeneral notes:\n    - If input image is called \'im\', then it doesn\'t matter if it\'s RGB or BGR,\n        but if one is called \'rgb\' or \'bgr\', then the channel order matters.\n    - If input image has suffix \'_0to1\' in its variable name, then it needs to be normalized\n        (at least roughly) to that range.\n""""""\nfrom copy import deepcopy\nimport numpy as np\nimport cv2\n\n# cv2 convinience constants so that no need to import cv2 just for these\nIMREAD_UNCHANGED = cv2.IMREAD_UNCHANGED\nIMREAD_COLOR = cv2.IMREAD_COLOR\nIMREAD_GRAYSCALE = cv2.IMREAD_GRAYSCALE\n\n\ndef imread_wrapper(*args, output_channel_order=\'RGB\'):\n    """"""\n    Convinience wrapper for cv2.imread() that can return result in RGB order\n\n    Args:\n        *args: Positional parameters that imread() takes\n            See documentation for cv2.imread()\n        output_channel_order: Whether to output RGB or BGR orders; has effects only when\n            number of channels is three or four (fourth being alpha)\n            \'RGB\' or \'BGR\' (case-insensitive)\n            Optional; defaults to \'RGB\'\n\n    Returns:\n        im: Loaded image\n            Numpy array of shape (m, n) or (m, n, c)\n    """"""\n    output_channel_order = output_channel_order.lower()\n    assert ((output_channel_order == \'rgb\') or (output_channel_order == \'bgr\')), \\\n        ""\'output_channel_order\' has to be either \'RGB\' or \'BGR\' (case-insensitive)""\n\n    im = cv2.imread(*args)\n    assert (im is not None), ""%s not existent"" % args[0]\n\n    if (im.ndim == 3) and (output_channel_order == \'rgb\'):\n        if im.shape[2] == 3:\n            im = im[:, :, ::-1]\n        elif im.shape[2] == 4:  # with alpha\n            im = im[:, :, [2, 1, 0, 3]]\n    return im\n\n\ndef depth_to_mesh_df(depth_im, th, jitter, upsample=0.6, cam_dist=2.0):\n    from util.util_camera import tsdf_renderer\n    depth = depth_im[:, :, 0]\n    mask = np.where(depth == 0, -1.0, 1.0)\n    depth = 1 - depth\n    t = tsdf_renderer()\n    thl = th[0]\n    thh = th[1]\n    if jitter:\n        th = th + (np.random.rand(2) - 0.5) * 0.1\n        thl = np.min(th)\n        thh = np.max(th)\n    scale = thh - thl\n    depth = depth * scale\n    t.depth = (depth + thl) * mask\n    t.camera.focal_length = 0.05\n    t.camera.sensor_width = 0.03059411708155671\n    t.camera.position = np.array([-cam_dist, 0, 0])\n    t.camera.res = [480, 480]\n    t.camera.rx = np.array([0, 0, 1])\n    t.camera.ry = np.array([0, 1, 0])\n    t.camera.rz = -np.array([1, 0, 0])\n    t.back_project_ptcloud(upsample=upsample)\n    tdf = np.ones([128, 128, 128]) / 128\n    cnt = np.zeros([128, 128, 128])\n    for pts in t.ptcld:\n        pt = pts  # np.array([-pts[2], -pts[0], pts[1]])\n        ids = np.floor((pt + 0.5) * 128).astype(int)\n        if np.any(np.abs(pt) >= 0.5):\n            continue\n        center = ((ids + 0.5) * 1 / 128) - 0.5\n        dist = ((center[0] - pt[0])**2 + (center[1] - pt[1])\n                ** 2 + (center[2] - pt[2])**2)**0.5\n        n = cnt[ids[0], ids[1], ids[2]]\n        tdf[ids[0], ids[1], ids[2]] = (\n            tdf[ids[0], ids[1], ids[2]] * n + dist) / (n + 1)\n        cnt[ids[0], ids[1], ids[2]] += 1\n    return tdf\n\n\ndef imwrite_wrapper(*args, input_channel_order=\'RGB\'):\n    """"""\n    Convinience wrapper for cv2.imwrite() that can write RGB image correctly\n\n    Args:\n        *args: Positional parameters that imwrite() takes\n            See documentation for cv2.imwrite()\n        input_channel_order: Whether the input is in RGB or BGR orders; has effects\n            only when number of channels is three or four (fourth being alpha)\n            \'RGB\' or \'BGR\' (case-insensitive)\n            Optional; defaults to \'RGB\'\n    """"""\n    input_channel_order = input_channel_order.lower()\n    assert ((input_channel_order == \'rgb\') or (input_channel_order == \'bgr\')), \\\n        ""\'input_channel_order\' has to be either \'RGB\' or \'BGR\' (case-insensitive)""\n\n    im = args[1]\n\n    if (im.ndim == 3) and (input_channel_order == \'rgb\'):\n        if im.shape[2] == 3:\n            im = im[:, :, ::-1]\n        elif im.shape[2] == 4:  # with alpha\n            im = im[:, :, [2, 1, 0, 3]]\n\n    args_list = list(args)\n    args_list[1] = im\n    args_tuple = tuple(args_list)\n\n    cv2.imwrite(*args_tuple)\n\n\ndef resize(im, target_size, which_dim, interpolation=\'bicubic\', clamp=None):\n    """"""\n    Resize one dimension of the image to a certain size while maintaining the aspect ratio\n\n    Args:\n        im: Image to resize\n            Any type that cv2.resize() accepts\n        target_size: Target horizontal or vertical dimension\n            Integer\n        which_dim: Which dimension to match target_size\n            \'horizontal\' or \'vertical\'\n        interpolation: Interpolation method\n            \'bicubic\'\n            Optional; defaults to \'bicubic\'\n        clamp: Clamp the resized image with minimum and maximum values\n            Array_likes of one smaller float and another larger float\n            Optional; defaults to None (no clamping)\n\n    Returns:\n        im_resized: Resized image\n            Numpy array with new horizontal and vertical dimensions\n    """"""\n    h, w = im.shape[:2]\n\n    if interpolation == \'bicubic\':\n        interpolation = cv2.INTER_CUBIC\n    else:\n        raise NotImplementedError(interpolation)\n\n    if which_dim == \'horizontal\':\n        scale_factor = target_size / w\n    elif which_dim == \'vertical\':\n        scale_factor = target_size / h\n    else:\n        raise ValueError(which_dim)\n\n    im_resized = cv2.resize(im, None, fx=scale_factor, fy=scale_factor,\n                            interpolation=interpolation)\n\n    if clamp is not None:\n        min_val, max_val = clamp\n        im_resized[im_resized < min_val] = min_val\n        im_resized[im_resized > max_val] = max_val\n\n    return im_resized\n\n\ndef alpha_blend(im1, im2, alpha):\n    """"""\n    Alpha blending of two images or one image and a scalar\n\n    Args:\n        im1, im2: Image or scalar\n            Numpy array and a scalar or two numpy arrays of the same shape\n        alpha: Weight of im1\n            Float ranging usually from 0 to 1\n\n    Returns:\n        im_blend: Blended image -- alpha * im1 + (1 - alpha) * im2\n            Numpy array of the same shape as input image\n    """"""\n    im_blend = alpha * im1 + (1 - alpha) * im2\n\n    return im_blend\n\n\ndef rgb2gray(rgb):\n    """"""\n    Convert a RGB image to a grayscale image\n        Differences from cv2.cvtColor():\n            1. Input image can be float\n            2. Output image has three repeated channels, other than a single channel\n\n    Args:\n        rgb: Image in RGB format\n            Numpy array of shape (h, w, 3)\n\n    Returns:\n        gs: Grayscale image\n            Numpy array of the same shape as input; the three channels are the same\n    """"""\n    ch = 0.299 * rgb[:, :, 0] + 0.587 * rgb[:, :, 1] + 0.114 * rgb[:, :, 2]\n    gs = np.dstack((ch, ch, ch))\n\n    return gs\n\n\ndef adjust_image_attribute(rgb, attr, d, random=False):\n    """"""\n    Adjust or randomize the specified attribute of the image\n\n    Args:\n        rgb: Image in RGB format\n            Numpy array of shape (h, w, 3)\n        attr: Image attribute to adjust or randomize\n            \'brightness\', \'saturation\', or \'contrast\'\n        d: If random, d must be positive, and alpha for blending is randomly drawn from\n            [1 - d, 1 + d]; else, alpha will be just 1 + d\n            Float\n        random: Whether to set or randomize the attribute\n            Boolean\n            Optional; defaults to False\n\n    Returns:\n        rgb_out: Output image in RGB format\n            Numpy array of the same shape as input\n    """"""\n    gs = rgb2gray(rgb)\n\n    if random:\n        assert (\n            d > 0), ""\'d\' must be positive for range [1 - d, 1 + d] to be valid""\n        alpha = 1 + np.random.uniform(low=-d, high=d)\n    else:\n        alpha = 1 + d\n\n    if attr == \'contrast\':\n        rgb_out = alpha_blend(rgb, np.mean(gs[:, :, 0]), alpha)\n    elif attr == \'saturation\':\n        rgb_out = alpha_blend(rgb, gs, alpha)\n    elif attr == \'brightness\':\n        rgb_out = alpha_blend(rgb, 0, alpha)\n    else:\n        raise NotImplementedError(attr)\n\n    return rgb_out\n\n\ndef jitter_colors(rgb, d_brightness=0, d_contrast=0, d_saturation=0):\n    """"""\n    Color jittering by randomizing brightness, contrast and saturation, in random order\n\n    Args:\n        rgb: Image in RGB format\n            Numpy array of shape (h, w, 3)\n        d_brightness, d_contrast, d_saturation: Alpha for blending drawn from [1 - d, 1 + d]\n            Nonnegative float\n            Optional; defaults to 0, i.e., no randomization\n\n    Returns:\n        rgb_out: Color-jittered image in RGB format\n            Numpy array of the same shape as input\n    """"""\n    attrs = [\'brightness\', \'contrast\', \'saturation\']\n    ds = [d_brightness, d_contrast, d_saturation]\n\n    # In random order\n    ind = np.array(range(len(attrs)))\n    np.random.shuffle(ind)  # in-place\n\n    rgb_out = deepcopy(rgb)\n    for idx in ind:\n        rgb_out = adjust_image_attribute(\n            rgb_out, attrs[idx], ds[idx], random=True)\n\n    return rgb_out\n\n\ndef add_lighting_noise(rgb_0to1,\n                       alpha_std,\n                       eigvals=(0.2175, 0.0188, 0.0045),\n                       eigvecs=((-0.5675, 0.7192, 0.4009),\n                                (-0.5808, -0.0045, -0.8140),\n                                (-0.5836, -0.6948, 0.4203))):\n    """"""\n    Add AlexNet-style PCA-based noise\n\n    Args:\n        rgb_0to1: Image in RGB format, normalized within [0, 1]; values can fall outside [0, 1] due to\n            some preceding processing, but eigenvalues/vectors should match the magnitude order\n            Numpy array of shape (h, w, 3)\n        alpha_std: Standard deviation of the Gaussian from which alpha is drawn\n            Positive float\n        eigvals, eigvecs: Eigenvalues and their eigenvectors\n            Array_likes of length 3 and shape (3, 3), respectively\n            Optional; default to results from AlexNet\n\n    Returns:\n        rgb_0to1_out: Output image in RGB format, with lighting noise added\n            Numpy array of the same shape as input\n    """"""\n    assert (rgb_0to1.dtype.name ==\n            \'float64\'), ""Input image must be normalized and hence be float""\n    assert (alpha_std > 0), ""Standard deviation must be positive""\n\n    eigvals = np.array(eigvals)\n    eigvecs = np.array(eigvecs)\n\n    alpha = np.random.normal(loc=0, scale=alpha_std, size=3)\n    noise_rgb = \\\n        np.sum(\n            np.multiply(\n                np.multiply(\n                    eigvecs,\n                    np.tile(alpha, (3, 1))\n                ),\n                np.tile(eigvals, (3, 1))\n            ),\n            axis=1\n        )\n\n    rgb_0to1_out = deepcopy(rgb_0to1)\n    for i in range(3):\n        rgb_0to1_out[:, :, i] += noise_rgb[i]\n\n    return rgb_0to1_out\n\n\ndef normalize_colors(rgb_0to1, mean_rgb=(0.485, 0.456, 0.406), std_rgb=(0.229, 0.224, 0.225)):\n    """"""\n    Normalize colors\n\n    Args:\n        rgb_0to1: Image in RGB format, normalized within [0, 1]; values can fall outside [0, 1] due to\n            some preceding processing, but mean and standard deviation should match the magnitude order\n            Numpy array of shape (h, w, 3)\n        mean_rgb, std_rgb: Mean and standard deviation for RGB channels\n            Array_likes of length 3\n            Optional; default to results computed from a random subset of ImageNet training images\n\n    Returns:\n        rgb_0to1_out: Output image in RGB format, with channels normalized\n            Numpy array of the same shape as input\n    """"""\n    assert (\'float\' in rgb_0to1.dtype.name), ""Input image must be normalized and hence be float""\n    assert rgb_0to1.ndim == 3, ""Nx3xHxW? This function was written for HxWx3""\n\n    rgb_0to1_out = deepcopy(rgb_0to1)\n    for i in range(3):\n        rgb_0to1_out[:, :, i] = (\n            rgb_0to1_out[:, :, i] - mean_rgb[i]) / std_rgb[i]\n\n    return rgb_0to1_out\n\n\ndef denormalize_colors(rgb_norm, mean_rgb=(0.485, 0.456, 0.406), std_rgb=(0.229, 0.224, 0.225)):\n    """"""\n    Denormalize colors\n\n    Args:\n        rgb_norm: Image in RGB format, normalized by normalize_colors()\n            Numpy array of shape (h, w, 3)\n        mean_rgb, std_rgb: Mean and standard deviation for RGB channels used\n            Array_likes of length 3\n            Optional; default to results computed from a random subset of ImageNet training images\n\n    Returns:\n        rgb_0to1_out: Output image in RGB format, with channels normalized\n            Numpy array of the same shape as input\n    """"""\n    assert (\'float\' in rgb_norm.dtype.name), ""Input image must be color-normalized and hence be float""\n\n    if rgb_norm.ndim == 3:\n        # HxWx3\n        for i in range(3):\n            rgb_norm[:, :, i] = rgb_norm[:, :, i] * std_rgb[i] + mean_rgb[i]\n    elif rgb_norm.ndim == 4:\n        # Nx3xHxW\n        for i in range(3):\n            rgb_norm[:, i, :, :] = rgb_norm[:, i, :, :] * std_rgb[i] + mean_rgb[i]\n    else:\n        raise NotImplementedError(rgb_norm.ndim)\n\n    return rgb_norm\n\n\ndef binarize(im, thres, gt_is_1=True):\n    """"""\n    Binarize image\n\n    Args:\n        im: Image to binarize\n            Numpy array\n        thres: Threshold\n            Float\n        gt_is_1: Whether 1 is for ""greater than"" or ""less than or equal to""\n            Boolean\n            Optional; defaults to True\n\n    Returns:\n        im_bin: Binarized image consisting of only 0\'s and 1\'s\n            Numpy array of the same shape as input\n    """"""\n    if gt_is_1:\n        ind_for_1 = im > thres\n    else:\n        ind_for_1 = im <= thres\n\n    ind_for_0 = np.logical_not(ind_for_1)\n\n    im_bin = deepcopy(im)\n    im_bin[ind_for_1] = 1\n    im_bin[ind_for_0] = 0\n\n    return im_bin\n\n\ndef get_bbox(mask_0to1, th=0.95):\n    indh, indw = np.where(mask_0to1 > th)\n    tl_h = np.min(indh)\n    tl_w = np.min(indw)\n    br_h = np.max(indh)\n    br_w = np.max(indw)\n    return [tl_w, tl_h, br_w, br_h]\n\n\ndef crop(img, img_bbox, out_size, pad, pad_zero=True):\n    y1, x1, y2, x2 = img_bbox\n    w, h = img.shape[1], img.shape[0]\n    x_mid = (x1 + x2) / 2.\n    y_mid = (y1 + y2) / 2.\n    l = max(x2 - x1, y2 - y1) * out_size / (out_size - 2. * pad)\n    x1 = int(np.round(x_mid - l / 2.))\n    x2 = int(np.round(x_mid + l / 2.))\n    y1 = int(np.round(y_mid - l / 2.))\n    y2 = int(np.round(y_mid + l / 2.))\n    b_x = 0\n    if x1 < 0:\n        b_x = -x1\n        x1 = 0\n    b_y = 0\n    if y1 < 0:\n        b_y = -y1\n        y1 = 0\n    a_x = 0\n    if x2 >= h:\n        a_x = x2 - (h - 1)\n        x2 = h - 1\n    a_y = 0\n    if y2 >= w:\n        a_y = y2 - (w - 1)\n        y2 = w - 1\n    pad_style = {\n        \'mode\': \'constant\',\n        \'constant_values\': 0\n    } if pad_zero else {\n        \'mode\': \'edge\'\n    }\n    if img.ndim == 2:\n        img_crop = np.pad(\n            img[x1:(x2 + 1), y1:(y2 + 1)],\n            ((b_x, a_x), (b_y, a_y)),\n            **pad_style\n        )\n    else:\n        img_crop = np.pad(\n            img[x1:(x2 + 1), y1:(y2 + 1)],\n            ((b_x, a_x), (b_y, a_y), (0, 0)),\n            **pad_style\n        )\n    return cv2.resize(img_crop, (out_size, out_size))\n'"
util/util_io.py,8,"b'import os\nimport numpy as np\nimport torch\nimport collections\nimport time\n# from torch._six import string_classes\ntry:\n    from .util_print import str_warning\nexcept ImportError:\n    str_warning = \'[Warning]\'\n\ndef default_collate(batches):\n    """""" Merge batches of data into a larger batch. Input must be a list of batch outputs (list/dict).\n    The each element of the batch outputs must be a numpy array, torch tensor or list """"""\n    assert isinstance(batches, collections.Sequence)\n    if isinstance(batches[0], collections.Mapping):\n        return {key: _collate_list([d[key] for d in batches]) for key in batches[0]}\n    elif isinstance(batches[0], collections.Sequence):\n        transposed = zip(*batches)\n        return [_collate_list(samples) for samples in transposed]\n\n\ndef _collate_list(list_of_data):\n    # list_of_data = [elem.data if type(elem) is torch.autograd.Variable else elem for elem in list_of_data]   # remove torch tensors\n    # list_of_data = [elem.cpu().numpy() if torch.is_tensor(elem) else elem for elem in list_of_data]   # remove torch tensors\n    if type(list_of_data[0]).__module__ == \'numpy\':\n        return np.concatenate(list_of_data)\n    elif isinstance(list_of_data[0], int):\n        return list_of_data\n    elif isinstance(list_of_data[0], float):\n        return list_of_data\n    elif isinstance(list_of_data[0], str):\n        return list_of_data\n    elif isinstance(list_of_data[0], collections.Sequence):\n        return [x for subitem in list_of_data for x in subitem]\n\n    raise TypeError((""batch must contain tensors, numbers, dicts or lists; found {}""\n                     .format(type(batches[0]))))\n\n\ndef default_subset(batch, indl, indh):\n    # import pdb; pdb.set_trace()\n    if isinstance(batch, collections.Sequence):\n        return [elem[indl:indh] for elem in batch]\n    elif isinstance(batch, collections.Mapping):\n        return {key: batch[key][indl:indh] for key in batch}\n\n    raise TypeError((""batch must contain dicts or lists; found {}""\n                     .format(type(batch))))\n\n\ndef default_len(batch):\n    if isinstance(batch, collections.Mapping):\n        lens = {len(batch[key]) for key in batch}\n    elif isinstance(batch, collections.Sequence):\n        lens = {_item_len(elem) for elem in batch}\n    else:\n        raise TypeError((""batch must contain dicts or lists; found {}""\n                         .format(type(elem))))\n\n    assert len(lens) == 1, \'items of a batch output should have the same length. Got lengths: \' + str(lens)\n    return next(iter(lens))     # get the only element from a set\n\n\ndef _item_len(elem):\n    if isinstance(elem, int) or isinstance(elem, float) or isinstance(elem, str):\n        return 1\n    elif type(elem).__module__ == \'numpy\':\n        return elem.shape[0]\n    elif isinstance(elem, collections.Sequence):\n        return len(elem)\n    else:\n        raise TypeError((""batch must contain dicts or lists; found {}""\n                         .format(type(elem))))\n\n\ndef default_save(savepath, batch):\n    # print(batch)\n    if type(batch).__module__ == \'numpy\':\n        np.savez_compressed(savepath, batch)\n    elif isinstance(batch, collections.Sequence):\n        np.savez_compressed(savepath, *batch)\n    elif isinstance(batch, collections.Mapping):\n        np.savez_compressed(savepath, **batch)\n    else:\n        raise TypeError((""batch must contain numpy arrays, dicts or lists; found {}""\n                        .format(type(batch))))\n\n\ndef default_clean(batch):\n    if isinstance(batch, str) or isinstance(batch, int) or isinstance(batch, float) or type(batch).__module__ == \'numpy\':\n        return batch\n    if isinstance(batch, collections.Mapping):\n        return {k: default_clean(v) for k, v in batch.items()}\n    elif isinstance(batch, collections.Sequence):\n        return [default_clean(v) for v in batch]\n    elif type(batch) is torch.autograd.Variable: # class removed in pytorch 0.4.0\n        return batch.detach().cpu().numpy()\n    elif torch.is_tensor(batch):\n        return batch.detach().cpu().numpy()\n    else:\n        raise TypeError((""batch elements must be int, float, str, torch tensor/variable, numpy arrays, dicts or lists; found {}""\n                        .format(type(batch))))\n\n\nclass BatchSave(object):\n    """"""\n    This class is a general IO class aiming to offer saving flexibility.\n    Note: data is only copied when saving, not at the time of adding (lazy copy).\n          Do not mutate list/numpy arrays after adding. (torch tensors are fine as they\'re copied to numpy arrays at time of adding)\n    """"""\n\n    def __init__(self, savepath, filesize, *, collate_fn=default_collate, subset_fn=default_subset, len_fn=default_len, clean_fn=default_clean, verbose=False):\n        """"""\n        savepath: filename pattern for saved packed file. Use {ind} or formats like {ind:04d} for saved file index.\n        filesize: number of data points per file. NOT size of file in bytes.\n        collate_fn: Function to merge different batches of data.\n        subset_fn: Function to get a subset of each added batch data.\n        len_fn: Function to get length of each added batch data.\n        clean_fn: Clean up input by changing data type. By default it maps all torch tensors to CPU numpy arrays.\n        """"""\n        self.savepath = savepath\n        self.collate_fn = collate_fn\n        self.subset_fn = subset_fn\n        self.len_fn = len_fn\n        self.clean_fn = clean_fn\n        if os.path.isdir(os.path.dirname(savepath)):\n            print(str_warning, \'Saving into an existing directory: %s\' % os.path.dirname(savepath))\n        else:\n            os.system(\'mkdir -p %s\' % os.path.dirname(savepath))\n        self._saveind = 0\n        self._buffer = list()\n        self._buffer_size = 0\n        self.filesize = filesize\n        self.closed = False\n        self.verbose = verbose\n\n    def add_data(self, batch):\n        """""" Accept a batch of data and put it into a buffer. Save when buffer size is over file size.\n        The batch must be a list/dict of list/np.array/torch.Tensor (each element must be iterable since it\'s defined on a batch)\n        """"""\n        assert not self.closed\n        times = [time.time()]\n        batch = self.clean_fn(batch)\n        times.append(time.time())\n        self._buffer_size += self.len_fn(batch)\n        times.append(time.time())\n        self._buffer.append(batch)\n        times.append(time.time())\n        while self._buffer_size >= self.filesize:\n            buffer_data = self.collate_fn(self._buffer)\n            times.append(time.time())\n            data_to_save = self.subset_fn(buffer_data, 0, self.filesize)\n            times.append(time.time())\n            default_save(self.savepath.format(ind=self._saveind), data_to_save)\n            times.append(time.time())\n            self._buffer = [self.subset_fn(buffer_data, self.filesize, self._buffer_size)]\n            times.append(time.time())\n            self._buffer_size -= self.filesize\n            self._saveind += 1\n        times.append(time.time())\n        if self.verbose:\n            print(*[times[i+1] - times[i] for i in range(len(times)-1)])\n\n    def close(self):\n        if self._buffer_size > 0:\n            buffer_data = self.collate_fn(self._buffer)\n            default_save(self.savepath.format(ind=self._saveind), buffer_data)\n            self._saveind += 1\n        self.closed = True\n\n    def get_fileind(self):\n        return self._saveind\n\n    def get_buffer_size(self):\n        return self._buffer_size\n\n\n#################################################\n# Test\nif __name__ == \'__main__\':\n    fout = BatchSave(\'./test_{ind}.npz\', 3)\n    for i in range(10):\n        fout.add_data([[i]])\n    fout.close()\n\n    for i in range(fout._saveind):\n        data = (np.load(\'./test_{ind}.npz\'.format(ind=i)))\n        for k, v in data.items():\n            print(k, v)\n\n    fout = BatchSave(\'./test_{ind}.npz\', 3)\n    for i in range(10):\n        fout.add_data([np.array([i])])\n    fout.close()\n\n    for i in range(fout._saveind):\n        data = (np.load(\'./test_{ind}.npz\'.format(ind=i)))\n        for k, v in data.items():\n            print(k, v)\n\n    fout = BatchSave(\'./test_{ind}.npz\', 3)\n    for i in range(10):\n        fout.add_data([torch.FloatTensor([i])])\n    fout.close()\n\n    for i in range(fout._saveind):\n        data = (np.load(\'./test_{ind}.npz\'.format(ind=i)))\n        for k, v in data.items():\n            print(k, v)\n\n    fout = BatchSave(\'./test_{ind}.npz\', 3)\n    for i in range(10):\n        fout.add_data([torch.FloatTensor([i, i, i, i])])\n    fout.close()\n\n    for i in range(fout._saveind):\n        data = (np.load(\'./test_{ind}.npz\'.format(ind=i)))\n        for k, v in data.items():\n            print(k, v)\n\n    fout = BatchSave(\'./test_{ind}.npz\', 3)\n    for i in range(10):\n        fout.add_data([[0, 0], np.array([i, i])])\n    fout.close()\n\n    for i in range(fout._saveind):\n        data = (np.load(\'./test_{ind}.npz\'.format(ind=i)))\n        for k, v in data.items():\n            print(k, v)\n\n    fout = BatchSave(\'./test_{ind}.npz\', 3)\n    for i in range(10):\n        fout.add_data({\'a\': [0, 0], \'b\': np.array([i, i])})\n    fout.close()\n\n    for i in range(fout._saveind):\n        data = (np.load(\'./test_{ind}.npz\'.format(ind=i)))\n        for k, v in data.items():\n            print(k, v)\n\n    fout = BatchSave(\'./test_{ind}.npz\', 1)\n    for i in range(10):\n        fout.add_data({\'a\': [0, 0], \'b\': np.array([i, i])})\n    fout.close()\n\n    for i in range(fout._saveind):\n        data = (np.load(\'./test_{ind}.npz\'.format(ind=i)))\n        for k, v in data.items():\n            print(k, v)\n\n    fout = BatchSave(\'./test_{ind}.npz\', 21)\n    for i in range(10):\n        fout.add_data({\'a\': [0, 0], \'b\': np.array([i, i])})\n    fout.close()\n\n    for i in range(fout._saveind):\n        data = (np.load(\'./test_{ind}.npz\'.format(ind=i)))\n        for k, v in data.items():\n            print(k, v)\n\n    os.system(\'rm ./test_*.npz\')\n'"
util/util_loadlib.py,6,"b'import subprocess\nfrom .util_print import str_warning, str_verbose\n\n\ndef set_gpu(gpu, check=True):\n    import os\n    _check_gpu(gpu)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = gpu\n    import torch\n    import torch.backends.cudnn as cudnn\n    cudnn.benchmark = True\n    if check:\n        if not _check_gpu_setting_in_use(gpu):\n            print(\'[Warning] gpu setting overwritten. torch.cuda may be initialized before running this function.\')\n\n\ndef _check_gpu_setting_in_use(gpu):\n    \'\'\'\n    check that CUDA_VISIBLE_DEVICES is actually working\n    by starting a clean thread with the same CUDA_VISIBLE_DEVICES\n    \'\'\'\n    import subprocess\n    output = subprocess.check_output(\'CUDA_VISIBLE_DEVICES=%s python -c ""import torch; print(torch.cuda.device_count())""\' % gpu, shell=True)\n    output = output.decode().strip()\n    import torch\n    return torch.cuda.device_count() == int(output)\n\n\ndef _check_gpu(gpu):\n    msg = subprocess.check_output(\'nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,nounits,noheader -i %s\' % (gpu,), shell=True)\n    msg = msg.decode(\'utf-8\')\n    all_ok = True\n    for line in msg.split(\'\\n\'):\n        if line == \'\':\n            break\n        stats = [x.strip() for x in line.split(\',\')]\n        gpu = stats[0]\n        util = int(stats[1])\n        mem_used = int(stats[2])\n        if util > 10 or mem_used > 1000:  # util in percentage and mem_used in MiB\n            print(str_warning, \'Designated GPU in use: id=%s, util=%d%%, memory in use: %d MiB\' % (gpu, util, mem_used))\n            all_ok = False\n    if all_ok:\n        print(str_verbose, \'All designated GPU(s) free to use. \')\n\n\ndef set_manual_seed(seed):\n    import random\n    random.seed(seed)\n    try:\n        import numpy as np\n        np.random.seed(seed)\n    except ImportError as err:\n        print(\'Numpy not found. Random seed for numpy not set. \')\n    try:\n        import torch\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    except ImportError as err:\n        print(\'Pytorch not found. Random seed for pytorch not set. \')\n'"
util/util_print.py,0,"b""class bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\nstr_stage = bcolors.OKBLUE + '==>' + bcolors.ENDC\nstr_verbose = bcolors.OKGREEN + '[Verbose]' + bcolors.ENDC\nstr_warning = bcolors.WARNING + '[Warning]' + bcolors.ENDC\nstr_error = bcolors.FAIL + '[Error]' + bcolors.ENDC\n"""
util/util_reproj.py,22,"b""import time\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\n\ndef cross_prod(u, v):\n    # Cross pruduct between a set of vectors and a vector\n    if len(u.size()) == 2:\n        i = u[:, 1] * v[2] - u[:, 2] * v[1]\n        j = u[:, 2] * v[0] - u[:, 0] * v[2]\n        k = u[:, 0] * v[1] - u[:, 1] * v[0]\n        return torch.stack((i, j, k), 1)\n    elif len(u.size()) == 3:\n        i = u[:, :, 1] * v[2] - u[:, :, 2] * v[1]\n        j = u[:, :, 2] * v[0] - u[:, :, 0] * v[2]\n        k = u[:, :, 0] * v[1] - u[:, :, 1] * v[0]\n        return torch.stack((i, j, k), 2)\n    raise Exception()\n\n\ndef criterion_single(v, x, x_0, n_0, l, alpha=np.sqrt(2) / 2, beta=1, gamma=1.):\n    v = v.view(-1)\n    x = x.view(-1, 3)\n    n_0 /= torch.sum(n_0 ** 2)\n\n    # Find the voxel which is nearest to x_0\n    _, index = torch.min(torch.sum((x - x_0) ** 2, dim=1), dim=0)\n    i_0 = index.data.cpu().numpy()[0]\n\n    # loss for (i_0, j_0, k_0)\n    loss_1 = (1 - v[i_0]) ** 2\n\n    # loss for others\n    d = torch.sum(cross_prod((x - x_0), n_0) ** 2, dim=1) ** 0.5\n    mask_1 = (d < alpha * l).float()\n    mask_2 = torch.ones(*v.size())\n    mask_2[i_0] = 0\n    mask_2 = Variable(mask_2.cuda())\n    loss_2 = torch.sum((gamma * (1 - d / (alpha * l)) ** beta * v ** 2) * mask_1 * mask_2)\n\n    return loss_1 + loss_2\n\n\ndef criterion(v, x, x_0, n_0, l, alpha=np.sqrt(2) / 2, beta=1, gamma=1.):\n    n_sample = x_0.size(0)\n    v = v.view(-1)\n    x = x.view(-1, 3)\n    n_0 /= torch.sum(n_0 ** 2)\n\n    # Find the voxel which is nearest to x_0\n    x_repeat = x.view(x.size(0), 1, x.size(1)).repeat(1, n_sample, 1)\n    x_sub = x_repeat - x_0\n    _, index = torch.min(torch.sum(x_sub ** 2, dim=2), dim=0)\n    i_0 = index.data.cpu().numpy()\n    \n    # loss for (i_0, j_0, k_0)\n    loss_1 = Variable(torch.zeros(1).cuda())\n    for i in range(n_sample):\n        loss_1 += (1 - v[i_0[i]]) ** 2\n    \n    # loss for others\n    d = torch.sum(cross_prod(x_sub, n_0) ** 2, dim=2) ** 0.5\n    mask_1 = (d < alpha * l).float()\n    mask_2 = torch.ones(v.size(0), n_sample)\n    for i in range(n_sample):\n        mask_2[i_0[i]][i] = 0\n    mask_2 = Variable(mask_2.cuda())\n    v_repeat = v.view(v.size(0), 1).repeat(1, n_sample)\n    loss_2 = torch.sum((gamma * (1 - d / (alpha * l)) ** beta * v_repeat ** 2) * mask_1 * mask_2)\n    return loss_2\n\n\nif __name__ == '__main__':\n    torch.manual_seed(70)\n    n_sample = 90\n    N = 128\n    l = 1.\n    v = Variable(torch.rand(N, N, N).cuda(), requires_grad=True)\n    x = Variable(torch.rand(N, N, N, 3).cuda())\n    x_0 = Variable(torch.rand(n_sample, 3).cuda())\n    n_0 = Variable(torch.rand(3).cuda())\n\n    start = time.time()\n    \n    loss = criterion(v, x, x_0, n_0, l)\n\n    '''\n    loss = Variable(torch.zeros(1).cuda())\n    for i in range(n_sample):\n        loss += criterion_single(v, x, x_0[i], n_0, l)\n    '''\n    \n    loss.backward()\n    print(v.grad[0, 0, 0])\n    \n    end = time.time()\n    print(end - start)\n    \n\n    u = Variable(torch.rand(N, 3).cuda())\n    v = Variable(torch.rand(3).cuda())\n    # print(cross_prod(u, v))\n\n    # print(np.cross(u.data.cpu().numpy()[0], v.data.cpu().numpy()))\n"""
util/util_sph.py,0,"b""import trimesh\nfrom util.util_img import depth_to_mesh_df, resize\nfrom skimage import measure\nimport numpy as np\n\n\ndef render_model(mesh, sgrid):\n    index_tri, index_ray, loc = mesh.ray.intersects_id(\n        ray_origins=sgrid, ray_directions=-sgrid, multiple_hits=False, return_locations=True)\n    loc = loc.reshape((-1, 3))\n\n    grid_hits = sgrid[index_ray]\n    dist = np.linalg.norm(grid_hits - loc, axis=-1)\n    dist_im = np.ones(sgrid.shape[0])\n    dist_im[index_ray] = dist\n    im = dist_im\n    return im\n\n\ndef make_sgrid(b, alpha, beta, gamma):\n    res = b * 2\n    pi = np.pi\n    phi = np.linspace(0, 180, res * 2 + 1)[1::2]\n    theta = np.linspace(0, 360, res + 1)[:-1]\n    grid = np.zeros([res, res, 3])\n    for idp, p in enumerate(phi):\n        for idt, t in enumerate(theta):\n            grid[idp, idt, 2] = np.cos((p * pi / 180))\n            proj = np.sin((p * pi / 180))\n            grid[idp, idt, 0] = proj * np.cos(t * pi / 180)\n            grid[idp, idt, 1] = proj * np.sin(t * pi / 180)\n    grid = np.reshape(grid, (res * res, 3))\n    return grid\n\n\ndef render_spherical(data, mask, obj_path=None, debug=False):\n    depth_im = data['depth'][0, 0, :, :]\n    th = data['depth_minmax']\n    depth_im = resize(depth_im, 480, 'vertical')\n    im = resize(mask, 480, 'vertical')\n    gt_sil = np.where(im > 0.95, 1, 0)\n    depth_im = depth_im * gt_sil\n    depth_im = depth_im[:, :, np.newaxis]\n    b = 64\n    tdf = depth_to_mesh_df(depth_im, th, False, 1.0, 2.2)\n    try:\n        verts, faces, normals, values = measure.marching_cubes_lewiner(\n            tdf, 0.999 / 128, spacing=(1 / 128, 1 / 128, 1 / 128))\n        mesh = trimesh.Trimesh(vertices=verts - 0.5, faces=faces)\n        sgrid = make_sgrid(b, 0, 0, 0)\n        im_depth = render_model(mesh, sgrid)\n        im_depth = im_depth.reshape(2 * b, 2 * b)\n        im_depth = np.where(im_depth > 1, 1, im_depth)\n    except:\n        im_depth = np.ones([128, 128])\n        return im_depth\n    return im_depth\n"""
util/util_voxel.py,0,"b'import numpy as np\nimport numba\nfrom scipy.interpolate import RegularGridInterpolator as rgi\ntry:\n    from .util_print import str_warning\nexcept ImportError:\n    str_warning = \'[Warning]\'\n\n\n@numba.jit(nopython=True, cache=True)\ndef downsample(vox_in, times, use_max=True):\n    if vox_in.shape[0] % times != 0:\n        print(\'WARNING: not dividing the sapce evenly.\')\n    dim = vox_in.shape[0] // times\n    vox_out = np.zeros((dim, dim, dim))\n    for x in range(dim):\n        for y in range(dim):\n            for z in range(dim):\n                subx = x * times\n                suby = y * times\n                subz = z * times\n                subvox = vox_in[subx:subx + times,\n                                suby:suby + times, subz:subz + times]\n                if use_max:\n                    vox_out[x, y, z] = np.max(subvox)\n                else:\n                    vox_out[x, y, z] = np.mean(subvox)\n    return vox_out\n\n\ndef find_bound(voxel, *, threshold=0.5):\n    """""" find the boundary of a 3D voxel matrix. return boundaries in two matrices.\n    Note that lower bound is inclusive while the higher bound is not""""""\n    assert voxel.ndim == 3\n    bmin = np.zeros(voxel.ndim, dtype=int)\n    bmax = np.zeros(voxel.ndim, dtype=int)\n\n    voxel_binary = (voxel > threshold)\n    if not voxel_binary.any():\n        print(str_warning, \'Empty voxel found\')\n        return bmin, bmax\n\n    for dim in range(voxel.ndim):\n        voxel_dim = voxel_binary\n        for i in range(dim):\n            voxel_dim = voxel_dim.any(0)\n        for i in range(voxel.ndim - dim - 1):\n            voxel_dim = voxel_dim.any(1)\n        inds = voxel_dim.nonzero()[0]\n        bmin[dim] = inds.min()\n        bmax[dim] = inds.max() + 1\n    return bmin, bmax\n\n\ndef bounding_box_align(voxel, gt, *, threshold=0.5):\n    bminv, bmaxv = find_bound(voxel, threshold=threshold)\n    bming, bmaxg = find_bound(voxel, threshold=threshold)\n    scale = (bmaxg - bming).max() / (bmaxv - bminv).max()\n    scales = np.array((scale, scale, scale))\n    offset = (bmaxg + bming) / 2 - (bmaxv + bminv) / 2\n    return transform(voxel, scales=scales, offset=offset)\n\n\ndef translate(voxel, *, offset=None, translate_type=None):\n    """""" Translate a voxel by a specific offset """"""\n    assert voxel.ndim == 3\n    bmin, bmax = find_bound(voxel, threshold=0.5)\n    if offset is None:\n        assert translate_type is not None\n        min_offset = -bmin\n        max_offset = np.array(voxel.shape) - bmax\n        if translate_type == \'random\':\n            offset = np.random.rand(\n                voxel.ndim) * (max_offset - min_offset + 1) + min_offset - 1\n            offset = np.ceil(offset).astype(int)\n        elif translate_type == \'origin\':\n            offset = min_offset\n        elif translate_type == \'middle\':\n            offset = np.ceil((min_offset + max_offset) / 2).astype(int)\n        else:\n            raise ValueError(\'unknown translate_type: \' + str(translate_type))\n    else:\n        assert translate_type is None\n        offset = np.array(offset).astype(int)\n    assert (bmin + offset >= 0).all() and (bmax +\n                                           offset <= np.array(voxel.shape)).all()\n    res = np.zeros(voxel.shape)\n    res[bmin[0] + offset[0]:bmax[0] + offset[0], bmin[1] + offset[1]:bmax[1] + offset[1], bmin[2] +\n        offset[2]:bmax[2] + offset[2]] = voxel[bmin[0]:bmax[0], bmin[1]:bmax[1], bmin[2]:bmax[2]]\n    return res\n\n\ndef dim_unify(voxel):\n    if voxel.ndim == 5:\n        assert voxel.shape[1] == 1\n        voxel = voxel[:, 0, :, :, :]\n    elif voxel.ndim == 3:\n        voxel = np.expand_dims(voxel, 0)\n    else:\n        assert voxel.ndim == 4, \'voxel matrix must have dimensions of 3, 4, 5\'\n    return voxel\n\n###########################################################\n# For non-discretized transformation\n\n\ndef _get_centeralized_mesh_grid(sx, sy, sz):\n    x = np.arange(sx) - sx / 2.\n    y = np.arange(sy) - sy / 2.\n    z = np.arange(sz) - sz / 2.\n    return np.meshgrid(x, y, z, indexing=\'ij\')\n\n\ndef get_rotation_matrix(angles):\n    # # legacy code\n    # alpha, beta, gamma = angles\n    # R_alpha = np.array([[np.cos(alpha), -np.sin(alpha), 0], [np.sin(alpha), np.cos(alpha), 0], [0, 0, 1]])\n    # R_beta = np.array([[1, 0, 0], [0, np.cos(beta), -np.sin(beta)], [0, np.sin(beta), np.cos(beta)]])\n    # R_gamma = np.array([[np.cos(gamma), 0, -np.sin(gamma)], [0, 1, 0], [np.sin(gamma), 0, np.cos(gamma)]])\n    # R = np.dot(np.dot(R_alpha, R_beta), R_gamma)\n    alpha, beta, gamma = angles\n    R_alpha = np.array([[1, 0, 0], [0, np.cos(alpha), -np.sin(alpha)], [0, np.sin(alpha), np.cos(alpha)]])\n    R_beta = np.array([[np.cos(beta), 0, -np.sin(beta)], [0, 1, 0], [np.sin(beta), 0, np.cos(beta)]])\n    R_gamma = np.array([[np.cos(gamma), -np.sin(gamma), 0], [np.sin(gamma), np.cos(gamma), 0], [0, 0, 1]])\n    R = np.dot(np.dot(R_alpha, R_beta), R_gamma)\n    return R\n\n\ndef get_scale_matrix(scales):\n    return np.diag(scales)\n\n\ndef transform_by_matrix(voxel, matrix, offset):\n    """"""\n    transform a voxel by matrix, then apply an offset\n    Note that the offset is applied after the transformation\n    """"""\n    sx, sy, sz = voxel.shape\n    gridx, gridy, gridz = _get_centeralized_mesh_grid(sx, sy, sz)  # the coordinate grid of the new voxel\n    mesh = np.array([gridx.reshape(-1), gridy.reshape(-1), gridz.reshape(-1)])\n    mesh_rot = np.dot(np.linalg.inv(matrix), mesh) + np.array([sx / 2, sy / 2, sz / 2]).reshape(3, 1)\n    mesh_rot = mesh_rot - np.array(offset).reshape(3, 1)    # grid for new_voxel should get a negative offset\n\n    interp = rgi((np.arange(sx), np.arange(sy), np.arange(sz)), voxel,\n                 method=\'linear\', bounds_error=False, fill_value=0)\n    new_voxel = interp(mesh_rot.T).reshape(sx, sy, sz)  # todo: move mesh to center\n    return new_voxel\n\n\ndef transform(voxel, angles=(0, 0, 0), scales=(1, 1, 1), offset=(0, 0, 0), threshold=None, clamp=False):\n    """"""\n    transform a voxel by first rotate, then scale, then add offset.\n    shortcut for transform_by_matrix\n    """"""\n    matrix = np.dot(get_rotation_matrix(angles), get_scale_matrix(scales))\n    new_voxel = transform_by_matrix(voxel, matrix, offset)\n    if clamp:\n        new_voxel = np.clip(new_voxel, 0, 1)\n    if threshold is not None:\n        new_voxel = (new_voxel > threshold).astype(np.uint8)\n    return new_voxel\n\n\n############################################################\n# floodfill\n\ndef _fill(*, input_array, six_way=True):\n    """"""\n    fill an voxel array with dfs\n    The algorithm pad the input_array with 2 voxels in each direction\n        (1 to avoid complex border cases when checking neighbors,\n         1 for making outer voxels connected so that we only search for one connected component)\n    Note that this script considers ALL non-zero values as surface voxels\n    """"""\n    UNKNOWN = 200   # must be in [0, 255] for uint8\n\n    sz0, sz1, sz2 = input_array.shape\n    output_array = np.zeros((sz0 + 4, sz1 + 4, sz2 + 4), dtype=np.uint8)\n    output_array[1:-1, 1:-1, 1:-1] = UNKNOWN\n    input_padded = np.zeros((sz0 + 4, sz1 + 4, sz2 + 4), dtype=np.uint8)\n    input_padded[2:-2, 2:-2, 2:-2] = input_array\n\n    stack = [(1, 1, 1)]\n    output_array[1, 1, 1] = 0\n    while len(stack) > 0:\n        i, j, k = stack.pop()\n        output_array[i, j, k] = 0\n        if six_way:\n            neighbors = [(i - 1, j, k),\n                         (i, j - 1, k),\n                         (i, j, k - 1),\n                         (i, j, k + 1),\n                         (i, j + 1, k),\n                         (i + 1, j, k), ]\n        else:\n            neighbors = [(i - 1, j - 1, k - 1),\n                         (i - 1, j - 1, k),\n                         (i - 1, j - 1, k + 1),\n                         (i - 1, j, k - 1),\n                         (i - 1, j, k),\n                         (i - 1, j, k + 1),\n                         (i - 1, j + 1, k - 1),\n                         (i - 1, j + 1, k),\n                         (i - 1, j + 1, k + 1),\n                         (i, j - 1, k - 1),\n                         (i, j - 1, k),\n                         (i, j - 1, k + 1),\n                         (i, j, k - 1),\n                         (i, j, k + 1),\n                         (i, j + 1, k - 1),\n                         (i, j + 1, k),\n                         (i, j + 1, k + 1),\n                         (i + 1, j - 1, k - 1),\n                         (i + 1, j - 1, k),\n                         (i + 1, j - 1, k + 1),\n                         (i + 1, j, k - 1),\n                         (i + 1, j, k),\n                         (i + 1, j, k + 1),\n                         (i + 1, j + 1, k - 1),\n                         (i + 1, j + 1, k),\n                         (i + 1, j + 1, k + 1), ]\n        for i_, j_, k_ in neighbors:\n            if output_array[i_, j_, k_] == UNKNOWN and input_padded[i_, j_, k_] == 0:\n                stack.append((i_, j_, k_))\n                output_array[i_, j_, k_] = 0\n    output_array = (output_array != 0).astype(np.uint8)\n    return output_array[2:-2, 2:-2, 2:-2]\n\n\ndef fill(use_compile=False, compile_flag={\'cache\': True, \'nopython\': True}, **kwargs):\n    """"""\n    common compile flags: {\'cache\': True, \'nopython\': True}\n    """"""\n    if use_compile:\n        from numba import jit\n        return jit(**compile_flag)(_fill)(**kwargs)\n    else:\n        return _fill(**kwargs)\n############################################################\n'"
visualize/visualizer.py,0,"b'from os.path import join, dirname\nfrom os import makedirs\nfrom shutil import copyfile\nfrom multiprocessing import Pool\nimport atexit\nimport json\nimport numpy as np\nfrom skimage import measure\nfrom util.util_img import imwrite_wrapper\n\n\nclass Visualizer():\n    """"""\n    Unified Visulization Worker\n    """"""\n    paths = [\n        \'rgb_path\',\n        \'silhou_path\',\n        \'depth_path\',\n        \'normal_path\',\n    ]\n    imgs = [\n        \'rgb\',\n        \'pred_depth\',\n        \'pred_silhou\',\n        \'pred_normal\',\n    ]\n    voxels = [\n        \'pred_voxel_noft\',\n        \'pred_voxel\',\n        \'gen_voxel\',\n    ]  # will go through sigmoid\n    txts = [\n        \'gt_depth_minmax\',\n        \'pred_depth_minmax\',\n        \'disc\',\n        \'scores\'\n    ]\n    sphmaps = [\n        \'pred_spherical_full\',\n        \'pred_spherical_partial\',\n        \'gt_spherical_full\',\n    ]\n    voxels_gt = [\n        \'pred_proj_depth\',\n        \'gt_voxel\',\n        \'pred_proj_sph_full\',\n    ]\n\n    def __init__(self, n_workers=4, param_f=None):\n        if n_workers == 0:\n            pool = None\n        elif n_workers > 0:\n            pool = Pool(n_workers)\n        else:\n            raise ValueError(n_workers)\n        self.pool = pool\n        if param_f:\n            self.param_f = param_f\n        else:\n            self.param_f = join(dirname(__file__), \'config.json\')\n\n        def cleanup():\n            if pool:\n                pool.close()\n                pool.join()\n        atexit.register(cleanup)\n\n    def visualize(self, pack, batch_idx, outdir):\n        if self.pool:\n            self.pool.apply_async(\n                self._visualize,\n                [pack, batch_idx, self.param_f, outdir],\n                error_callback=self._error_callback\n            )\n        else:\n            self._visualize(pack, batch_idx, self.param_f, outdir)\n\n    @classmethod\n    def _visualize(cls, pack, batch_idx, param_f, outdir):\n        makedirs(outdir, exist_ok=True)\n\n        # Dynamically read parameters from disk\n        #param_dict = cls._read_params(param_f)\n        voxel_isosurf_th = 0.25  # param_dict[\'voxel\'][\'isosurf_thres\']\n\n        batch_size = cls._get_batch_size(pack)\n        instance_cnt = batch_idx * batch_size\n        counter = 0\n        for k in cls.paths:\n            prefix = \'{:04d}_%02d_\' % counter + k.split(\'_\')[0] + \'.png\'\n            cls._cp_img(pack.get(k), join(outdir, prefix), instance_cnt)\n            counter += 1\n        for k in cls.imgs:\n            prefix = \'{:04d}_%02d_\' % counter + k + \'.png\'\n            cls._vis_img(pack.get(k), join(outdir, prefix), instance_cnt)\n            counter += 1\n        for k in cls.voxels_gt:\n            prefix = \'{:04d}_%02d_\' % counter + k + \'.obj\'\n            cls._vis_voxel(pack.get(k), join(outdir, prefix), instance_cnt,\n                           voxel_isosurf_th, False)\n            counter += 1\n        for k in cls.voxels:\n            prefix = \'{:04d}_%02d_\' % counter + k + \'.obj\'\n            cls._vis_voxel(pack.get(k), join(outdir, prefix), instance_cnt,\n                           voxel_isosurf_th)\n            counter += 1\n        for k in cls.txts:\n            prefix = \'{:04d}_%02d_\' % counter + k + \'.txt\'\n            cls._vis_txt(pack.get(k), join(outdir, prefix), instance_cnt)\n            counter += 1\n        for k in cls.sphmaps:\n            prefix = \'{:04d}_%02d_\' % counter + k + \'.png\'\n            cls._vis_sph(pack.get(k), join(outdir, prefix), instance_cnt)\n            counter += 1\n\n    @staticmethod\n    def _read_params(param_f):\n        with open(param_f, \'r\') as h:\n            param_dict = json.load(h)\n        return param_dict\n\n    @staticmethod\n    def _get_batch_size(pack):\n        batch_size = None\n        for v in pack.values():\n            if hasattr(v, \'shape\'):\n                if batch_size is None or batch_size == 0:\n                    batch_size = v.shape[0]\n                else:\n                    assert batch_size == v.shape[0]\n        return batch_size\n\n    @staticmethod\n    def _sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    @staticmethod\n    def _to_obj_str(verts, faces):\n        text = """"\n        for p in verts:\n            text += ""v ""\n            for x in p:\n                text += ""{} "".format(x)\n            text += ""\\n""\n        for f in faces:\n            text += ""f ""\n            for x in f:\n                text += ""{} "".format(x + 1)\n            text += ""\\n""\n        return text\n\n    @classmethod\n    def _save_iso_obj(cls, df, path, th, shift=True):\n        if th < np.min(df):\n            df[0, 0, 0] = th - 1\n        if th > np.max(df):\n            df[-1, -1, -1] = th + 1\n        spacing = (1 / 128, 1 / 128, 1 / 128)\n        verts, faces, _, _ = measure.marching_cubes_lewiner(\n            df, th, spacing=spacing)\n        if shift:\n            verts -= np.array([0.5, 0.5, 0.5])\n        obj_str = cls._to_obj_str(verts, faces)\n        with open(path, \'w\') as f:\n            f.write(obj_str)\n\n    @staticmethod\n    def _vis_img(img, output_pattern, counter=0):\n        if img is not None and not isinstance(img, str):\n            assert img.shape[0] != 0\n            img = np.clip(img * 255, 0, 255).astype(int)\n            img = np.transpose(img, (0, 2, 3, 1))\n            bsize = img.shape[0]\n            for batch_id in range(bsize):\n                im = img[batch_id, :, :, :]\n                imwrite_wrapper(output_pattern.format(counter + batch_id), im)\n\n    @staticmethod\n    def _vis_sph(img, output_pattern, counter=0):\n        if img is not None and not isinstance(img, str):\n            assert img.shape[0] != 0\n            img = np.transpose(img, (0, 2, 3, 1))\n            bsize = img.shape[0]\n            for batch_id in range(bsize):\n                im = img[batch_id, :, :, 0]\n                im = im / im.max()\n                im = np.clip(im * 255, 0, 255).astype(int)\n                imwrite_wrapper(output_pattern.format(counter + batch_id), im)\n\n    @staticmethod\n    def _cp_img(paths, output_pattern, counter=0):\n        if paths is not None:\n            for batch_id, path in enumerate(paths):\n                copyfile(path, output_pattern.format(counter + batch_id))\n\n    @classmethod\n    def _vis_voxel(cls, voxels, output_pattern, counter=0, th=0.5, use_sigmoid=True):\n        if voxels is not None:\n            assert voxels.shape[0] != 0\n            for batch_id, voxel in enumerate(voxels):\n                if voxel.ndim == 4:\n                    voxel = voxel[0, ...]\n                voxel = cls._sigmoid(voxel) if use_sigmoid else voxel\n                cls._save_iso_obj(voxel, output_pattern.format(counter + batch_id), th=th)\n\n    @staticmethod\n    def _vis_txt(txts, output_pattern, counter=0):\n        if txts is not None:\n            for batch_id, txt in enumerate(txts):\n                with open(output_pattern.format(counter + batch_id), \'w\') as h:\n                    h.write(""%s\\n"" % txt)\n\n    @staticmethod\n    def _error_callback(e):\n        print(str(e))\n'"
toolbox/calc_prob/build.py,2,"b""import os\nimport sys\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\n\nextra_compile_args = list()\n\n\nextra_objects = list()\nassert(torch.cuda.is_available())\nsources = ['calc_prob/src/calc_prob.c']\nheaders = ['calc_prob/src/calc_prob.h']\ndefines = [('WITH_CUDA', True)]\nwith_cuda = True\n\nextra_objects = ['calc_prob/src/calc_prob_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi_params = {\n    'headers': headers,\n    'sources': sources,\n    'define_macros': defines,\n    'relative_to': __file__,\n    'with_cuda': with_cuda,\n    'extra_objects': extra_objects,\n    'include_dirs': [os.path.join(this_file, 'calc_prob/src')],\n    'extra_compile_args': extra_compile_args,\n}\n\n\nif __name__ == '__main__':\n    ext = create_extension(\n        'calc_prob._ext.calc_prob_lib',\n        package=False,\n        **ffi_params)\n    #from setuptools import setup\n    # setup()\n    ext.build()\n\n    # ffi.build()\n"""
toolbox/calc_prob/setup.py,0,"b'import os\nimport sys\n\nfrom setuptools import setup, find_packages\n\nimport build\n\nthis_file = os.path.dirname(__file__)\n\nsetup(\n    name=""pytorch_calc_stop_problility"",\n    version=""0.1.0"",\n    description=""Pytorch extension of calcualting ray stop probability"",\n    url=""https://bluhbluhbluh"",\n    author=""Zhoutong Zhang"",\n    author_email=""ztzhang@mit.edu"",\n    # Require cffi.\n    install_requires=[""cffi>=1.0.0""],\n    setup_requires=[""cffi>=1.0.0""],\n    # Exclude the build files.\n    packages=find_packages(exclude=[""build"", ""test""]),\n    # Package where to put the extensions. Has to be a prefix of build.py.\n    ext_package="""",\n    # Extensions to compile.\n    cffi_modules=[\n        os.path.join(this_file, ""build.py:ffi"")\n    ],\n)\n'"
toolbox/cam_bp/build.py,2,"b""import os\nimport sys\nimport torch\nfrom torch.utils.ffi import create_extension\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\n\nextra_compile_args = list()\n\nextra_objects = list()\nassert(torch.cuda.is_available())\nsources = ['cam_bp/src/back_projection.c']\nheaders = ['cam_bp/src/back_projection.h']\ndefines = [('WITH_CUDA', True)]\nwith_cuda = True\n\nextra_objects = ['cam_bp/src/back_projection_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi_params = {\n    'headers': headers,\n    'sources': sources,\n    'define_macros': defines,\n    'relative_to': __file__,\n    'with_cuda': with_cuda,\n    'extra_objects': extra_objects,\n    'include_dirs': [os.path.join(this_file, 'cam_bp/src')],\n    'extra_compile_args': extra_compile_args,\n}\n\nffi = create_extension(\n    'cam_bp._ext.cam_bp_lib',\n    package=True,\n    **ffi_params\n)\n\nif __name__ == '__main__':\n    ffi = create_extension(\n        'cam_bp._ext.cam_bp_lib',\n        package=False,\n        **ffi_params)\n    ffi.build()\n"""
toolbox/cam_bp/setup.py,0,"b'import os\nimport sys\n\nfrom setuptools import setup, find_packages\n\nimport build\n\nthis_file = os.path.dirname(__file__)\n\nsetup(\n    name=""pytorch_camera_back_projection"",\n    version=""0.1.0"",\n    description=""Pytorch extension of back projecting depth"",\n    url=""https://bluhbluhbluh"",\n    author=""Zhoutong Zhang"",\n    author_email=""ztzhang@mit.edu"",\n    # Require cffi.\n    install_requires=[""cffi>=1.0.0""],\n    setup_requires=[""cffi>=1.0.0""],\n    # Exclude the build files.\n    packages=find_packages(exclude=[""build"", ""test""]),\n    # Package where to put the extensions. Has to be a prefix of build.py.\n    ext_package="""",\n    # Extensions to compile.\n    cffi_modules=[\n        os.path.join(this_file, ""build.py:ffi"")\n    ],\n)\n'"
toolbox/nndistance/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\nthis_file = os.path.dirname(__file__)\n\nsources = ['src/my_lib.c']\nheaders = ['src/my_lib.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/my_lib_cuda.c']\n    headers += ['src/my_lib_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/nnd_cuda.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.my_lib',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
toolbox/nndistance/test.py,5,"b""import torch\nfrom torch.autograd import Variable\n\ntry:\n    from nndistance.modules.nnd import NNDModule\nexcept ImportError as err:\n    raise ImportError('This file should be copied to its parent directory for import to work properly.')\n\ndist = NNDModule()\n\np1 = torch.rand(1,50,3)*20\np2 = torch.rand(1,50,3)*20\n# p1 = p1.int()\n# p1.random_(0,2)\n# p1 = p1.float()\n# p2 = p2.int()\n# p2.random_(0,2)\np2 = p2.float()\n# print(p1)\n# print(p2)\n\nprint('cpu')\npoints1 = Variable(p1, requires_grad=True)\npoints2 = Variable(p2, requires_grad=True)\ndist1, dist2 = dist(points1, points2)\nprint(dist1, dist2)\nloss = torch.sum(dist1)\nprint(loss)\nloss.backward()\nprint(points1.grad, points2.grad)\n\nprint('gpu')\npoints1_cuda = Variable(p1.cuda(), requires_grad=True)\npoints2_cuda = Variable(p2.cuda(), requires_grad=True)\ndist1_cuda, dist2_cuda = dist(points1_cuda, points2_cuda)\nprint(dist1_cuda, dist2_cuda)\nloss_cuda = torch.sum(dist1_cuda)\nprint(loss_cuda)\nloss_cuda.backward()\nprint(points1_cuda.grad, points2_cuda.grad)\n\nprint('stats:')\nprint('loss:', loss.data[0], loss_cuda.data[0])\nprint('loss diff:', loss.data[0] - loss_cuda.data[0])\nprint('grad diff:', (points1.grad.data.cpu() - points1_cuda.grad.data.cpu()).abs().max(), (points2.grad.data.cpu() - points2_cuda.grad.data.cpu()).abs().max())\n\nfrom nndistance.functions.nnd import nndistance_score\nprint('total score:', nndistance_score(points1, points2))\n"""
toolbox/calc_prob/calc_prob/__init__.py,0,b''
toolbox/cam_bp/cam_bp/__init__.py,0,b''
toolbox/nndistance/functions/__init__.py,0,"b'from .nnd import nndistance, nndistance_w_idx, nndistance_score\n'"
toolbox/nndistance/functions/nnd.py,7,"b'# functions/add.py\nimport torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom nndistance._ext import my_lib\n\n\nclass NNDFunction(Function):\n    @staticmethod\n    def forward(ctx, xyz1, xyz2):\n        assert xyz1.dim() == 3 and xyz2.dim() == 3\n        assert xyz1.size(0) == xyz2.size(0)\n        assert xyz1.size(2) == 3 and xyz2.size(2) == 3\n        assert xyz1.is_cuda == xyz2.is_cuda\n        assert xyz1.type().endswith(\'FloatTensor\') and xyz2.type().endswith(\'FloatTensor\'), \'only FloatTensor are supported for NNDistance\'\n        assert xyz1.is_contiguous() and xyz2.is_contiguous()  # the CPU and GPU code are not robust and will break if the storage is not contiguous\n        ctx.is_cuda = xyz1.is_cuda\n\n        batchsize, n, _ = xyz1.size()\n        _, m, _ = xyz2.size()\n        dist1 = torch.zeros(batchsize, n)\n        dist2 = torch.zeros(batchsize, m)\n\n        idx1 = torch.zeros(batchsize, n).type(torch.IntTensor)\n        idx2 = torch.zeros(batchsize, m).type(torch.IntTensor)\n\n        if not xyz1.is_cuda:\n            my_lib.nnd_forward(xyz1, xyz2, dist1, dist2, idx1, idx2)\n        else:\n            dist1 = dist1.cuda()\n            dist2 = dist2.cuda()\n            idx1 = idx1.cuda()\n            idx2 = idx2.cuda()\n            my_lib.nnd_forward_cuda(xyz1, xyz2, dist1, dist2, idx1, idx2)\n\n        ctx.save_for_backward(xyz1, xyz2, idx1, idx2)\n        return dist1, dist2, idx1, idx2\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, graddist1, graddist2, gradidx1, gradidx2):\n        """"""\n        Note that this function needs gradidx placeholders\n        """"""\n        assert ctx.is_cuda == graddist1.is_cuda and ctx.is_cuda == graddist2.is_cuda\n        xyz1, xyz2, idx1, idx2 = ctx.saved_tensors\n        graddist1 = graddist1.contiguous()\n        graddist2 = graddist2.contiguous()\n        assert xyz1.is_contiguous()\n        assert xyz2.is_contiguous()\n        assert idx1.is_contiguous()\n        assert idx2.is_contiguous()\n        assert graddist1.type().endswith(\'FloatTensor\') and graddist2.type().endswith(\'FloatTensor\'), \'only FloatTensor are supported for NNDistance\'\n\n        gradxyz1 = xyz1.new(xyz1.size())\n        gradxyz2 = xyz1.new(xyz2.size())\n\n        if not graddist1.is_cuda:\n            my_lib.nnd_backward(xyz1, xyz2, gradxyz1, gradxyz2, graddist1, graddist2, idx1, idx2)\n        else:\n            my_lib.nnd_backward_cuda(xyz1, xyz2, gradxyz1, gradxyz2, graddist1, graddist2, idx1, idx2)\n\n        return gradxyz1, gradxyz2\n\n\ndef nndistance_w_idx(xyz1, xyz2):\n    xyz1 = xyz1.contiguous()\n    xyz2 = xyz2.contiguous()\n    return NNDFunction.apply(xyz1, xyz2)\n\n\ndef nndistance(xyz1, xyz2):\n    if xyz1.size(2) != 3:\n        xyz1 = xyz1.transpose(1, 2)\n    if xyz2.size(2) != 3:\n        xyz2 = xyz2.transpose(1, 2)\n    xyz1 = xyz1.contiguous()\n    xyz2 = xyz2.contiguous()\n    dist1, dist2, _, _ = NNDFunction.apply(xyz1, xyz2)\n    return dist1, dist2\n\n\ndef nndistance_score(xyz1, xyz2, eps=1e-10):\n    dist1, dist2 = nndistance(xyz1, xyz2)\n    return torch.sqrt(dist1 + eps).mean(1) + torch.sqrt(dist2 + eps).mean(1)\n'"
toolbox/nndistance/modules/__init__.py,0,b''
toolbox/nndistance/modules/nnd.py,1,"b'from torch.nn import Module\nfrom nndistance.functions.nnd import nndistance\n\n\nclass NNDModule(Module):\n    def forward(self, input1, input2):\n        return nndistance(input1, input2)\n'"
toolbox/calc_prob/calc_prob/functions/__init__.py,0,b'from .calc_prob import CalcStopProb\n'
toolbox/calc_prob/calc_prob/functions/calc_prob.py,3,"b'import torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom .._ext import calc_prob_lib\nfrom cffi import FFI\nffi = FFI()\n\n\nclass CalcStopProb(Function):\n    @staticmethod\n    def forward(ctx, prob_in):\n        assert prob_in.dim() == 5\n        assert prob_in.dtype == torch.float32\n        assert prob_in.is_cuda\n        stop_prob = prob_in.new(prob_in.shape)\n        stop_prob.zero_()\n        calc_prob_lib.calc_prob_forward(prob_in, stop_prob)\n        ctx.save_for_backward(prob_in, stop_prob)\n        return stop_prob\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_in):\n        prob_in, stop_prob = ctx.saved_tensors\n        grad_out = grad_in.new(grad_in.shape)\n        grad_out.zero_()\n        stop_prob_weighted = stop_prob * grad_in\n        calc_prob_lib.calc_prob_backward(prob_in, stop_prob_weighted, grad_out)\n        return grad_out\n'"
toolbox/cam_bp/cam_bp/functions/__init__.py,0,b'from .cam_back_projection import CameraBackProjection\nfrom .get_surface_mask import get_surface_mask\nfrom .sperical_to_tdf import SphericalBackProjection\n'
toolbox/cam_bp/cam_bp/functions/cam_back_projection.py,2,"b'import torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom .._ext import cam_bp_lib\nfrom cffi import FFI\nffi = FFI()\n\n\nclass CameraBackProjection(Function):\n\n    @staticmethod\n    def forward(ctx, depth_t, fl, cam_dist, res=128):\n        assert depth_t.dim() == 4\n        assert fl.dim() == 2 and fl.size(1) == depth_t.size(1)\n        assert cam_dist.dim() == 2 and cam_dist.size(1) == depth_t.size(1)\n        assert cam_dist.size(0) == depth_t.size(0)\n        assert fl.size(0) == depth_t.size(0)\n        assert depth_t.is_cuda\n        assert fl.is_cuda\n        assert cam_dist.is_cuda\n        in_shape = depth_t.shape\n        cnt = depth_t.new(in_shape[0], in_shape[1], res, res, res).zero_()\n        tdf = depth_t.new(in_shape[0], in_shape[1],\n                          res, res, res).zero_() + 1 / res\n        cam_bp_lib.back_projection_forward(depth_t, cam_dist, fl, tdf, cnt)\n        # print(cnt)\n        ctx.save_for_backward(depth_t, fl, cam_dist)\n        ctx.cnt_forward = cnt\n        ctx.depth_shape = in_shape\n        return tdf\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        assert grad_output.is_cuda\n        # print(grad_output.type())\n        depth_t, fl, cam_dist = ctx.saved_tensors\n        cnt = ctx.cnt_forward\n        grad_depth = grad_output.new(ctx.depth_shape).zero_()\n        grad_fl = grad_output.new(\n            ctx.depth_shape[0], ctx.depth_shape[1]).zero_()\n        grad_camdist = grad_output.new(\n            ctx.depth_shape[0], ctx.depth_shape[1]).zero_()\n        cam_bp_lib.back_projection_backward(\n            depth_t, fl, cam_dist, cnt, grad_output, grad_depth, grad_camdist, grad_fl)\n        return grad_depth, grad_fl, grad_camdist, None\n'"
toolbox/cam_bp/cam_bp/functions/get_surface_mask.py,3,"b'import torch\nimport numpy as np\nfrom .._ext import cam_bp_lib\nfrom cffi import FFI\nffi = FFI()\n\n\ndef get_vox_surface_cnt(depth_t, fl, cam_dist, res=128):\n    assert depth_t.dim() == 4\n    assert fl.dim() == 2 and fl.size(1) == depth_t.size(1)\n    assert cam_dist.dim() == 2 and cam_dist.size(1) == depth_t.size(1)\n    assert cam_dist.size(0) == depth_t.size(0)\n    assert fl.size(0) == depth_t.size(0)\n    assert depth_t.is_cuda\n    assert fl.is_cuda\n    assert cam_dist.is_cuda\n    in_shape = depth_t.shape\n    cnt = depth_t.new(in_shape[0], in_shape[1], res, res, res).zero_()\n    tdf = depth_t.new(in_shape[0], in_shape[1], res,\n                      res, res).zero_() + 1 / res\n    cam_bp_lib.back_projection_forward(depth_t, cam_dist, fl, tdf, cnt)\n    return cnt\n\n\ndef get_surface_mask(depth_t, fl=784.4645406, cam_dist=2.0, res=128):\n    n = depth_t.size(0)\n    nc = depth_t.size(1)\n    if type(fl) == float:\n        fl_v = fl\n        fl = torch.FloatTensor(n, nc).cuda()\n        fl.fill_(fl_v)\n    if type(cam_dist) == float:\n        cmd_v = cam_dist\n        cam_dist = torch.FloatTensor(n, nc).cuda()\n        cam_dist.fill_(cmd_v)\n    cnt = get_vox_surface_cnt(depth_t, fl, cam_dist, res)\n    mask = cnt.new(n, nc, res, res, res).zero_()\n    cam_bp_lib.get_surface_mask(depth_t, cam_dist, fl, cnt, mask)\n    surface_vox = torch.clamp(cnt, min=0.0, max=1.0)\n    return surface_vox, mask\n'"
toolbox/cam_bp/cam_bp/functions/sperical_to_tdf.py,4,"b'import torch\nimport numpy as np\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom .._ext import cam_bp_lib\nfrom cffi import FFI\nffi = FFI()\n\n\nclass SphericalBackProjection(Function):\n\n    @staticmethod\n    def forward(ctx, spherical, grid, res=128):\n        assert spherical.dim() == 4\n        assert grid.dim() == 5\n        assert spherical.size(0) == grid.size(0)\n        assert spherical.size(1) == grid.size(1)\n        assert spherical.size(2) == grid.size(2)\n        assert spherical.size(3) == grid.size(3)\n        assert grid.size(4) == 3\n        assert spherical.is_cuda\n        assert grid.is_cuda\n        in_shape = spherical.shape\n        cnt = spherical.new(in_shape[0], in_shape[1], res, res, res).zero_()\n        tdf = spherical.new(in_shape[0], in_shape[1],\n                            res, res, res).zero_()\n        cam_bp_lib.spherical_back_proj_forward(spherical, grid, tdf, cnt)\n        # print(cnt)\n        ctx.save_for_backward(spherical.detach(), grid, cnt)\n        ctx.depth_shape = in_shape\n        return tdf, cnt\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output, grad_phony):\n        assert grad_output.is_cuda\n        assert not np.isnan(torch.sum(grad_output.detach()))\n        spherical, grid, cnt = ctx.saved_tensors\n        grad_depth = grad_output.new(ctx.depth_shape).zero_()\n        cam_bp_lib.spherical_back_proj_backward(\n            spherical, grid, cnt, grad_output, grad_depth)\n        try:\n            assert not np.isnan(torch.sum(grad_depth))\n        except:\n            import pdb\n            pdb.set_trace()\n        return grad_depth, None, None\n'"
toolbox/cam_bp/cam_bp/modules/Spherical_backproj.py,2,"b'import torch\nfrom torch import nn\nfrom ..functions import SphericalBackProjection\nfrom torch.autograd import Variable\n\n\nclass spherical_backprojection(nn.Module):\n\n    def __init__(self, grid, vox_res=128):\n        super(camera_backprojection, self).__init__()\n        self.vox_res = vox_res\n        self.backprojection_layer = SphericalBackProjection()\n        assert type(grid) == torch.FloatTensor\n        self.grid = Variable(grid.cuda())\n\n    def forward(self, spherical):\n        return self.backprojection_layer(spherical, self.grid, self.vox_res)\n'"
toolbox/cam_bp/cam_bp/modules/__init__.py,0,b''
toolbox/cam_bp/cam_bp/modules/camera_backprojection_module.py,2,"b'from torch import nn\nfrom ..functions import CameraBackProjection\nimport torch\n\n\nclass Camera_back_projection_layer(nn.Module):\n    def __init__(self, res=128):\n        super(Camera_back_projection_layer, self).__init__()\n        assert res == 128\n        self.res = 128\n\n    def forward(self, depth_t, fl=418.3, cam_dist=2.2, shift=True):\n        n = depth_t.size(0)\n        if type(fl) == float:\n            fl_v = fl\n            fl = torch.FloatTensor(n, 1).cuda()\n            fl.fill_(fl_v)\n        if type(cam_dist) == float:\n            cmd_v = cam_dist\n            cam_dist = torch.FloatTensor(n, 1).cuda()\n            cam_dist.fill_(cmd_v)\n        df = CameraBackProjection.apply(depth_t, fl, cam_dist, self.res)\n        return self.shift_tdf(df) if shift else df\n\n    @staticmethod\n    def shift_tdf(input_tdf, res=128):\n        out_tdf = 1 - res * (input_tdf)\n        return out_tdf\n\n\nclass camera_backprojection(nn.Module):\n\n    def __init__(self, vox_res=128):\n        super(camera_backprojection, self).__init__()\n        self.vox_res = vox_res\n        self.backprojection_layer = CameraBackProjection()\n\n    def forward(self, depth, fl, camdist):\n        return self.backprojection_layer(depth, fl, camdist, self.voxel_res)\n'"
