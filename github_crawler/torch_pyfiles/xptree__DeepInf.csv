file_path,api_count,code
src/data_loader.py,5,"b'#!/usr/bin/env python\n# encoding: utf-8\n# File Name: data_loader.py\n# Author: Jiezhong Qiu\n# Create Time: 2017/12/13 16:41\n# TODO:\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport os\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import Sampler\nfrom utils import load_w2v_feature\nimport sklearn\nimport itertools\nimport logging\nimport igraph\nfrom sklearn import preprocessing\n\nlogger = logging.getLogger(__name__)\n\n\nclass ChunkSampler(Sampler):\n    """"""\n    Samples elements sequentially from some offset.\n    Arguments:\n        num_samples: # of desired datapoints\n        start: offset where we should start selecting from\n    """"""\n    def __init__(self, num_samples, start=0):\n        self.num_samples = num_samples\n        self.start = start\n\n    def __iter__(self):\n        return iter(range(self.start, self.start + self.num_samples))\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass InfluenceDataSet(Dataset):\n    def __init__(self, file_dir, embedding_dim, seed, shuffle, model):\n        self.graphs = np.load(os.path.join(file_dir, ""adjacency_matrix.npy"")).astype(np.float32)\n\n        # self-loop trick, the input graphs should have no self-loop\n        identity = np.identity(self.graphs.shape[1])\n        self.graphs += identity\n        self.graphs[self.graphs != 0] = 1.0\n        if model == ""gat"" or model == ""pscn"":\n            self.graphs = self.graphs.astype(np.dtype(\'B\'))\n        elif model == ""gcn"":\n            # normalized graph laplacian for GCN: D^{-1/2}AD^{-1/2}\n            for i in range(len(self.graphs)):\n                graph = self.graphs[i]\n                d_root_inv = 1. / np.sqrt(np.sum(graph, axis=1))\n                graph = (graph.T * d_root_inv).T * d_root_inv\n                self.graphs[i] = graph\n        else:\n            raise NotImplementedError\n        logger.info(""graphs loaded!"")\n\n        # wheather a user has been influenced\n        # wheather he/she is the ego user\n        self.influence_features = np.load(\n                os.path.join(file_dir, ""influence_feature.npy"")).astype(np.float32)\n        logger.info(""influence features loaded!"")\n\n        self.labels = np.load(os.path.join(file_dir, ""label.npy""))\n        logger.info(""labels loaded!"")\n\n        self.vertices = np.load(os.path.join(file_dir, ""vertex_id.npy""))\n        logger.info(""vertex ids loaded!"")\n\n        if shuffle:\n            self.graphs, self.influence_features, self.labels, self.vertices = \\\n                    sklearn.utils.shuffle(\n                        self.graphs, self.influence_features,\n                        self.labels, self.vertices,\n                        random_state=seed\n                    )\n\n        vertex_features = np.load(os.path.join(file_dir, ""vertex_feature.npy""))\n        vertex_features = preprocessing.scale(vertex_features)\n        self.vertex_features = torch.FloatTensor(vertex_features)\n        logger.info(""global vertex features loaded!"")\n\n        embedding_path = os.path.join(file_dir, ""deepwalk.emb_%d"" % embedding_dim)\n        max_vertex_idx = np.max(self.vertices)\n        embedding = load_w2v_feature(embedding_path, max_vertex_idx)\n        self.embedding = torch.FloatTensor(embedding)\n        logger.info(""%d-dim embedding loaded!"", embedding_dim)\n\n        self.N = self.graphs.shape[0]\n        logger.info(""%d ego networks loaded, each with size %d"" % (self.N, self.graphs.shape[1]))\n\n        n_classes = self.get_num_class()\n        class_weight = self.N / (n_classes * np.bincount(self.labels))\n        self.class_weight = torch.FloatTensor(class_weight)\n\n    def get_embedding(self):\n        return self.embedding\n\n    def get_vertex_features(self):\n        return self.vertex_features\n\n    def get_feature_dimension(self):\n        return self.influence_features.shape[-1]\n\n    def get_num_class(self):\n        return np.unique(self.labels).shape[0]\n\n    def get_class_weight(self):\n        return self.class_weight\n\n    def __len__(self):\n        return self.N\n\n    def __getitem__(self, idx):\n        return self.graphs[idx], self.influence_features[idx], self.labels[idx], self.vertices[idx]\n\n\nclass PatchySanDataSet(InfluenceDataSet):\n    def get_bfs_order(self, g, v, size, key):\n        order, indices, _ = g.bfs(v, mode=""ALL"")\n        for j, start in enumerate(indices[:-1]):\n            if start >= size:\n                break\n            end = indices[j + 1]\n            order[start:end] = sorted(order[start:end],\n                    key=lambda x: key[x][0],\n                    reverse=True)\n\n        return order[:size]\n\n    def __init__(self, file_dir, embedding_dim, seed, shuffle,\n            model, sequence_size=8, stride=1, neighbor_size=8):\n        assert model == ""pscn""\n        super(PatchySanDataSet, self).__init__(file_dir,\n                embedding_dim, seed, shuffle, model)\n        n_vertices = self.graphs.shape[1]\n\n        logger.info(""generating receptive fields..."")\n        self.receptive_fields = []\n        for i in range(self.graphs.shape[0]):\n            adj = self.graphs[i]\n            edges = list(zip(*np.where(adj)))\n            g = igraph.Graph(edges=edges, directed=False)\n            assert(g.vcount() == n_vertices)\n            g.simplify()\n\n            sequence = self.get_bfs_order(g, n_vertices - 1,\n                    sequence_size, self.influence_features[i])\n            neighborhoods = np.zeros((sequence_size, neighbor_size), dtype=np.int32)\n            neighborhoods.fill(-1)\n\n            for j, v in enumerate(sequence):\n                if v < 0:\n                    break\n                shortest = list(itertools.islice(g.bfsiter(int(v), mode=\'ALL\'), neighbor_size))\n                for k, vtx in enumerate(shortest):\n                    neighborhoods[j][k] = vtx.index\n\n            neighborhoods = neighborhoods.reshape(sequence_size * neighbor_size)\n            self.receptive_fields.append(neighborhoods)\n        self.receptive_fields = np.array(self.receptive_fields, dtype=np.int32)\n        logger.info(""receptive fields generated!"")\n\n    def __getitem__(self, idx):\n        return self.receptive_fields[idx], self.influence_features[idx], \\\n                self.labels[idx], self.vertices[idx]\n'"
src/gat.py,5,"b'#!/usr/bin/env python\n# encoding: utf-8\n# File Name: gat.py\n# Author: Jiezhong Qiu\n# Create Time: 2017/12/18 21:40\n# TODO:\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom gat_layers import BatchMultiHeadGraphAttention\n\n\nclass BatchGAT(nn.Module):\n    def __init__(self, pretrained_emb, vertex_feature, use_vertex_feature,\n            n_units=[1433, 8, 7], n_heads=[8, 1],\n            dropout=0.1, attn_dropout=0.0, fine_tune=False,\n            instance_normalization=False):\n        super(BatchGAT, self).__init__()\n        self.n_layer = len(n_units) - 1\n        self.dropout = dropout\n        self.inst_norm = instance_normalization\n        if self.inst_norm:\n            self.norm = nn.InstanceNorm1d(pretrained_emb.size(1), momentum=0.0, affine=True)\n\n        # https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222/2\n        self.embedding = nn.Embedding(pretrained_emb.size(0), pretrained_emb.size(1))\n        self.embedding.weight = nn.Parameter(pretrained_emb)\n        self.embedding.weight.requires_grad = fine_tune\n        n_units[0] += pretrained_emb.size(1)\n\n        self.use_vertex_feature = use_vertex_feature\n        if self.use_vertex_feature:\n            self.vertex_feature = nn.Embedding(vertex_feature.size(0), vertex_feature.size(1))\n            self.vertex_feature.weight = nn.Parameter(vertex_feature)\n            self.vertex_feature.weight.requires_grad = False\n            n_units[0] += vertex_feature.size(1)\n\n        self.layer_stack = nn.ModuleList()\n        for i in range(self.n_layer):\n            # consider multi head from last layer\n            f_in = n_units[i] * n_heads[i - 1] if i else n_units[i]\n            self.layer_stack.append(\n                    BatchMultiHeadGraphAttention(n_heads[i], f_in=f_in,\n                        f_out=n_units[i + 1], attn_dropout=attn_dropout)\n                    )\n\n    def forward(self, x, vertices, adj):\n        emb = self.embedding(vertices)\n        if self.inst_norm:\n            emb = self.norm(emb.transpose(1, 2)).transpose(1, 2)\n        x = torch.cat((x, emb), dim=2)\n        if self.use_vertex_feature:\n            vfeature = self.vertex_feature(vertices)\n            x = torch.cat((x, vfeature), dim=2)\n        bs, n = adj.size()[:2]\n        for i, gat_layer in enumerate(self.layer_stack):\n            x = gat_layer(x, adj) # bs x n_head x n x f_out\n            if i + 1 == self.n_layer:\n                x = x.mean(dim=1)\n            else:\n                x = F.elu(x.transpose(1, 2).contiguous().view(bs, n, -1))\n                x = F.dropout(x, self.dropout, training=self.training)\n        return F.log_softmax(x, dim=-1)\n'"
src/gat_layers.py,20,"b'#!/usr/bin/env python\n# encoding: utf-8\n# File Name: gat_layers.py\n# Author: Jiezhong Qiu\n# Create Time: 2017/12/18 15:11\n# TODO:\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\n\nclass MultiHeadGraphAttention(nn.Module):\n    def __init__(self, n_head, f_in, f_out, attn_dropout, bias=True):\n        super(MultiHeadGraphAttention, self).__init__()\n        self.n_head = n_head\n        self.w = Parameter(torch.Tensor(n_head, f_in, f_out))\n        self.a_src = Parameter(torch.Tensor(n_head, f_out, 1))\n        self.a_dst = Parameter(torch.Tensor(n_head, f_out, 1))\n\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n        self.softmax = nn.Softmax(dim=-1)\n        self.dropout = nn.Dropout(attn_dropout)\n\n        if bias:\n            self.bias = Parameter(torch.Tensor(f_out))\n            init.constant_(self.bias, 0)\n        else:\n            self.register_parameter(\'bias\', None)\n\n        init.xavier_uniform_(self.w)\n        init.xavier_uniform_(self.a_src)\n        init.xavier_uniform_(self.a_dst)\n\n    def forward(self, h, adj):\n        n = h.size(0) # h is of size n x f_in\n        h_prime = torch.matmul(h.unsqueeze(0), self.w) #  n_head x n x f_out\n        attn_src = torch.bmm(h_prime, self.a_src) # n_head x n x 1\n        attn_dst = torch.bmm(h_prime, self.a_dst) # n_head x n x 1\n        attn = attn_src.expand(-1, -1, n) + attn_dst.expand(-1, -1, n).permute(0, 2, 1) # n_head x n x n\n\n        attn = self.leaky_relu(attn)\n        attn.data.masked_fill_(1 - adj, float(""-inf""))\n        attn = self.softmax(attn) # n_head x n x n\n        attn = self.dropout(attn)\n        output = torch.bmm(attn, h_prime) # n_head x n x f_out\n\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n\n\nclass BatchMultiHeadGraphAttention(nn.Module):\n    def __init__(self, n_head, f_in, f_out, attn_dropout, bias=True):\n        super(BatchMultiHeadGraphAttention, self).__init__()\n        self.n_head = n_head\n        self.w = Parameter(torch.Tensor(n_head, f_in, f_out))\n        self.a_src = Parameter(torch.Tensor(n_head, f_out, 1))\n        self.a_dst = Parameter(torch.Tensor(n_head, f_out, 1))\n\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n        self.softmax = nn.Softmax(dim=-1)\n        self.dropout = nn.Dropout(attn_dropout)\n        if bias:\n            self.bias = Parameter(torch.Tensor(f_out))\n            init.constant_(self.bias, 0)\n        else:\n            self.register_parameter(\'bias\', None)\n\n        init.xavier_uniform_(self.w)\n        init.xavier_uniform_(self.a_src)\n        init.xavier_uniform_(self.a_dst)\n\n    def forward(self, h, adj):\n        bs, n = h.size()[:2] # h is of size bs x n x f_in\n        h_prime = torch.matmul(h.unsqueeze(1), self.w) # bs x n_head x n x f_out\n        attn_src = torch.matmul(F.tanh(h_prime), self.a_src) # bs x n_head x n x 1\n        attn_dst = torch.matmul(F.tanh(h_prime), self.a_dst) # bs x n_head x n x 1\n        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(0, 1, 3, 2) # bs x n_head x n x n\n\n        attn = self.leaky_relu(attn)\n        mask = 1 - adj.unsqueeze(1) # bs x 1 x n x n\n        attn.data.masked_fill_(mask, float(""-inf""))\n        attn = self.softmax(attn) # bs x n_head x n x n\n        attn = self.dropout(attn)\n        output = torch.matmul(attn, h_prime) # bs x n_head x n x f_out\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n'"
src/gcn.py,5,"b'#!/usr/bin/env python\n# encoding: utf-8\n# File Name: gcn.py\n# Author: Jiezhong Qiu\n# Create Time: 2017/12/17 14:11\n# TODO:\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom gcn_layers import BatchGraphConvolution\n\n\nclass BatchGCN(nn.Module):\n    def __init__(self, n_units, dropout, pretrained_emb, vertex_feature,\n            use_vertex_feature, fine_tune=False, instance_normalization=False):\n        super(BatchGCN, self).__init__()\n        self.num_layer = len(n_units) - 1\n        self.dropout = dropout\n        self.inst_norm = instance_normalization\n        if self.inst_norm:\n            self.norm = nn.InstanceNorm1d(pretrained_emb.size(1), momentum=0.0, affine=True)\n\n        # https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222/2\n        self.embedding = nn.Embedding(pretrained_emb.size(0), pretrained_emb.size(1))\n        self.embedding.weight = nn.Parameter(pretrained_emb)\n        self.embedding.weight.requires_grad = fine_tune\n        n_units[0] += pretrained_emb.size(1)\n\n        self.use_vertex_feature = use_vertex_feature\n        if self.use_vertex_feature:\n            self.vertex_feature = nn.Embedding(vertex_feature.size(0), vertex_feature.size(1))\n            self.vertex_feature.weight = nn.Parameter(vertex_feature)\n            self.vertex_feature.weight.requires_grad = False\n            n_units[0] += vertex_feature.size(1)\n\n        self.layer_stack = nn.ModuleList()\n\n        for i in range(self.num_layer):\n            self.layer_stack.append(\n                    BatchGraphConvolution(n_units[i], n_units[i + 1])\n                    )\n\n    def forward(self, x, vertices, lap):\n        emb = self.embedding(vertices)\n        if self.inst_norm:\n            emb = self.norm(emb.transpose(1, 2)).transpose(1, 2)\n        x = torch.cat((x, emb), dim=2)\n        if self.use_vertex_feature:\n            vfeature = self.vertex_feature(vertices)\n            x = torch.cat((x, vfeature), dim=2)\n        for i, gcn_layer in enumerate(self.layer_stack):\n            x = gcn_layer(x, lap)\n            if i + 1 < self.num_layer:\n                x = F.elu(x)\n                x = F.dropout(x, self.dropout, training=self.training)\n        return F.log_softmax(x, dim=-1)\n'"
src/gcn_layers.py,7,"b""#!/usr/bin/env python\n# encoding: utf-8\n# File Name: gcn_layers.py\n# Author: Jiezhong Qiu\n# Create Time: 2017/12/18 15:11\n# TODO:\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn.init as init\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.modules.module import Module\n\n\nclass BatchGraphConvolution(Module):\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(BatchGraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_features))\n            init.constant_(self.bias, 0)\n        else:\n            self.register_parameter('bias', None)\n        init.xavier_uniform_(self.weight)\n\n    def forward(self, x, lap):\n        expand_weight = self.weight.expand(x.shape[0], -1, -1)\n        support = torch.bmm(x, expand_weight)\n        output = torch.bmm(lap, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n"""
src/pscn.py,7,"b'#!/usr/bin/env python\n# encoding: utf-8\n# File Name: pscn.py\n# Author: Jiezhong Qiu\n# Create Time: 2018/01/28 17:01\n# TODO:\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nclass BatchPSCN(nn.Module):\n    def __init__(self, n_units, dropout, pretrained_emb, vertex_feature,\n            use_vertex_feature, instance_normalization,\n            neighbor_size, sequence_size, fine_tune=False):\n        super(BatchPSCN, self).__init__()\n        assert len(n_units) == 4\n        self.dropout = dropout\n        self.inst_norm = instance_normalization\n        if self.inst_norm:\n            self.norm = nn.InstanceNorm1d(pretrained_emb.size(1), momentum=0.0, affine=True)\n\n        # https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222/2\n        self.embedding = nn.Embedding(pretrained_emb.size(0), pretrained_emb.size(1))\n        self.embedding.weight = nn.Parameter(pretrained_emb)\n        self.embedding.weight.requires_grad = fine_tune\n        n_units[0] += pretrained_emb.size(1)\n\n        self.use_vertex_feature = use_vertex_feature\n        if self.use_vertex_feature:\n            self.vertex_feature = nn.Embedding(vertex_feature.size(0), vertex_feature.size(1))\n            self.vertex_feature.weight = nn.Parameter(vertex_feature)\n            self.vertex_feature.weight.requires_grad = False\n            n_units[0] += vertex_feature.size(1)\n\n        # input is of shape bs x num_feature x l where l = w*k\n        # after conv1, shape=(bs x ? x w)\n        # after conv2 shape=(bs x ? x w/2)\n        self.conv1 = nn.Conv1d(in_channels=n_units[0],\n                    out_channels=n_units[1], kernel_size=neighbor_size,\n                    stride=neighbor_size)\n        k = 1\n        self.conv2 = nn.Conv1d(in_channels=n_units[1],\n                    out_channels=n_units[2], kernel_size=k, stride=1)\n        self.fc = nn.Linear(in_features=n_units[2] * (sequence_size - k + 1),\n                    out_features=n_units[3])\n\n    def forward(self, x, vertices, recep):\n        emb = self.embedding(vertices)\n        if self.inst_norm:\n            emb = self.norm(emb.transpose(1, 2)).transpose(1, 2)\n        x = torch.cat((x, emb), dim=2)\n        if self.use_vertex_feature:\n            vfeature = self.vertex_feature(vertices)\n            x = torch.cat((x, vfeature), dim=2)\n        bs, l = recep.size()\n        n = x.size()[1] # x is of shape bs x n x num_feature\n        offset = torch.ger(torch.arange(0, bs).long(), torch.ones(l).long() * n)\n        offset = Variable(offset, requires_grad=False)\n        offset = offset.cuda()\n        recep = (recep.long() + offset).view(-1)\n        x = x.view(bs * n, -1)\n        x = x.index_select(dim=0, index=recep)\n        x = x.view(bs, l, -1) # x is of shape bs x l x num_feature, l=w*k\n        x = x.transpose(1, 2) # x is of shape bs x num_feature x l, l=w*k\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = x.view(bs, -1)\n        x = self.fc(x)\n        return F.log_softmax(x, dim=-1)\n'"
src/train.py,7,"b'#!/usr/bin/env python\n# encoding: utf-8\n# File Name: train.py\n# Author: Jiezhong Qiu\n# Create Time: 2017/11/08 07:43\n# TODO:\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom gcn import BatchGCN\nfrom gat import BatchGAT\nfrom pscn import BatchPSCN\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom data_loader import ChunkSampler\nfrom data_loader import InfluenceDataSet\nfrom data_loader import PatchySanDataSet\n\nimport os\nimport shutil\nimport logging\nfrom tensorboard_logger import tensorboard_logger\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, format=\'%(asctime)s %(message)s\') # include timestamp\n\n# Training settings\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--tensorboard-log\', type=str, default=\'\', help=""name of this run"")\nparser.add_argument(\'--model\', type=str, default=\'gcn\', help=""models used"")\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False, help=\'Disables CUDA training.\')\nparser.add_argument(\'--seed\', type=int, default=42, help=\'Random seed.\')\nparser.add_argument(\'--epochs\', type=int, default=1000, help=\'Number of epochs to train.\')\nparser.add_argument(\'--lr\', type=float, default=1e-3, help=\'Initial learning rate.\')\nparser.add_argument(\'--weight-decay\', type=float, default=5e-4,\n                    help=\'Weight decay (L2 loss on parameters).\')\nparser.add_argument(\'--dropout\', type=float, default=0.2,\n                    help=\'Dropout rate (1 - keep probability).\')\nparser.add_argument(\'--hidden-units\', type=str, default=""16,8"",\n                    help=""Hidden units in each hidden layer, splitted with comma"")\nparser.add_argument(\'--heads\', type=str, default=""1,1,1"",\n                    help=""Heads in each layer, splitted with comma"")\nparser.add_argument(\'--batch\', type=int, default=2048, help=""Batch size"")\nparser.add_argument(\'--dim\', type=int, default=64, help=""Embedding dimension"")\nparser.add_argument(\'--check-point\', type=int, default=10, help=""Eheck point"")\nparser.add_argument(\'--instance-normalization\', action=\'store_true\', default=False,\n                    help=""Enable instance normalization"")\nparser.add_argument(\'--shuffle\', action=\'store_true\', default=False, help=""Shuffle dataset"")\nparser.add_argument(\'--file-dir\', type=str, required=True, help=""Input file directory"")\nparser.add_argument(\'--train-ratio\', type=float, default=50, help=""Training ratio (0, 100)"")\nparser.add_argument(\'--valid-ratio\', type=float, default=25, help=""Validation ratio (0, 100)"")\nparser.add_argument(\'--class-weight-balanced\', action=\'store_true\', default=False,\n                    help=""Adjust weights inversely proportional""\n                    "" to class frequencies in the input data"")\nparser.add_argument(\'--use-vertex-feature\', action=\'store_true\', default=False,\n                    help=""Whether to use vertices\' structural features"")\nparser.add_argument(\'--sequence-size\', type=int, default=16,\n                    help=""Sequence size (only useful for pscn)"")\nparser.add_argument(\'--neighbor-size\', type=int, default=5,\n                    help=""Neighborhood size (only useful for pscn)"")\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\ntensorboard_log_dir = \'tensorboard/%s_%s\' % (args.model, args.tensorboard_log)\nos.makedirs(tensorboard_log_dir, exist_ok=True)\nshutil.rmtree(tensorboard_log_dir)\ntensorboard_logger.configure(tensorboard_log_dir)\nlogger.info(\'tensorboard logging to %s\', tensorboard_log_dir)\n\n# adj N*n*n\n# feature N*n*f\n# labels N*n*c\n# Load data\n# vertex: vertex id in global network N*n\n\nif args.model == ""pscn"":\n    influence_dataset = PatchySanDataSet(\n            args.file_dir, args.dim, args.seed, args.shuffle, args.model,\n            sequence_size=args.sequence_size, stride=1, neighbor_size=args.neighbor_size)\nelse:\n    influence_dataset = InfluenceDataSet(\n            args.file_dir, args.dim, args.seed, args.shuffle, args.model)\n\nN = len(influence_dataset)\nn_classes = 2\nclass_weight = influence_dataset.get_class_weight() \\\n        if args.class_weight_balanced else torch.ones(n_classes)\nlogger.info(""class_weight=%.2f:%.2f"", class_weight[0], class_weight[1])\n\nfeature_dim = influence_dataset.get_feature_dimension()\nn_units = [feature_dim] + [int(x) for x in args.hidden_units.strip().split("","")] + [n_classes]\nlogger.info(""feature dimension=%d"", feature_dim)\nlogger.info(""number of classes=%d"", n_classes)\n\ntrain_start,  valid_start, test_start = \\\n        0, int(N * args.train_ratio / 100), int(N * (args.train_ratio + args.valid_ratio) / 100)\ntrain_loader = DataLoader(influence_dataset, batch_size=args.batch,\n                        sampler=ChunkSampler(valid_start - train_start, 0))\nvalid_loader = DataLoader(influence_dataset, batch_size=args.batch,\n                        sampler=ChunkSampler(test_start - valid_start, valid_start))\ntest_loader = DataLoader(influence_dataset, batch_size=args.batch,\n                        sampler=ChunkSampler(N - test_start, test_start))\n\n# Model and optimizer\nif args.model == ""gcn"":\n    model = BatchGCN(pretrained_emb=influence_dataset.get_embedding(),\n                vertex_feature=influence_dataset.get_vertex_features(),\n                use_vertex_feature=args.use_vertex_feature,\n                n_units=n_units,\n                dropout=args.dropout,\n                instance_normalization=args.instance_normalization)\nelif args.model == ""gat"":\n    n_heads = [int(x) for x in args.heads.strip().split("","")]\n    model = BatchGAT(pretrained_emb=influence_dataset.get_embedding(),\n            vertex_feature=influence_dataset.get_vertex_features(),\n            use_vertex_feature=args.use_vertex_feature,\n            n_units=n_units, n_heads=n_heads,\n            dropout=args.dropout, instance_normalization=args.instance_normalization)\nelif args.model == ""pscn"":\n    model = BatchPSCN(pretrained_emb=influence_dataset.get_embedding(),\n                vertex_feature=influence_dataset.get_vertex_features(),\n                use_vertex_feature=args.use_vertex_feature,\n                n_units=n_units,\n                dropout=args.dropout,\n                instance_normalization=args.instance_normalization,\n                sequence_size=args.sequence_size,\n                neighbor_size=args.neighbor_size)\nelse:\n    raise NotImplementedError\n\nif args.cuda:\n    model.cuda()\n    class_weight = class_weight.cuda()\n\nparams = [{\'params\': filter(lambda p: p.requires_grad, model.parameters())\n    if args.model == ""pscn"" else model.layer_stack.parameters()}]\n\noptimizer = optim.Adagrad(params, lr=args.lr, weight_decay=args.weight_decay)\n\n\ndef evaluate(epoch, loader, thr=None, return_best_thr=False, log_desc=\'valid_\'):\n    model.eval()\n    total = 0.\n    loss, prec, rec, f1 = 0., 0., 0., 0.\n    y_true, y_pred, y_score = [], [], []\n    for i_batch, batch in enumerate(loader):\n        graph, features, labels, vertices = batch\n        bs = graph.size(0)\n\n        if args.cuda:\n            features = features.cuda()\n            graph = graph.cuda()\n            labels = labels.cuda()\n            vertices = vertices.cuda()\n\n        output = model(features, vertices, graph)\n        if args.model == ""gcn"" or args.model == ""gat"":\n            output = output[:, -1, :]\n        loss_batch = F.nll_loss(output, labels, class_weight)\n        loss += bs * loss_batch.item()\n\n        y_true += labels.data.tolist()\n        y_pred += output.max(1)[1].data.tolist()\n        y_score += output[:, 1].data.tolist()\n        total += bs\n\n    model.train()\n\n    if thr is not None:\n        logger.info(""using threshold %.4f"", thr)\n        y_score = np.array(y_score)\n        y_pred = np.zeros_like(y_score)\n        y_pred[y_score > thr] = 1\n\n    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=""binary"")\n    auc = roc_auc_score(y_true, y_score)\n    logger.info(""%sloss: %.4f AUC: %.4f Prec: %.4f Rec: %.4f F1: %.4f"",\n            log_desc, loss / total, auc, prec, rec, f1)\n\n    tensorboard_logger.log_value(log_desc + \'loss\', loss / total, epoch + 1)\n    tensorboard_logger.log_value(log_desc + \'auc\', auc, epoch + 1)\n    tensorboard_logger.log_value(log_desc + \'prec\', prec, epoch + 1)\n    tensorboard_logger.log_value(log_desc + \'rec\', rec, epoch + 1)\n    tensorboard_logger.log_value(log_desc + \'f1\', f1, epoch + 1)\n\n    if return_best_thr:\n        precs, recs, thrs = precision_recall_curve(y_true, y_score)\n        f1s = 2 * precs * recs / (precs + recs)\n        f1s = f1s[:-1]\n        thrs = thrs[~np.isnan(f1s)]\n        f1s = f1s[~np.isnan(f1s)]\n        best_thr = thrs[np.argmax(f1s)]\n        logger.info(""best threshold=%4f, f1=%.4f"", best_thr, np.max(f1s))\n        return best_thr\n    else:\n        return None\n\n\ndef train(epoch, train_loader, valid_loader, test_loader, log_desc=\'train_\'):\n    model.train()\n\n    loss = 0.\n    total = 0.\n    for i_batch, batch in enumerate(train_loader):\n        graph, features, labels, vertices = batch\n        bs = graph.size(0)\n\n        if args.cuda:\n            features = features.cuda()\n            graph = graph.cuda()\n            labels = labels.cuda()\n            vertices = vertices.cuda()\n\n        optimizer.zero_grad()\n        output = model(features, vertices, graph)\n        if args.model == ""gcn"" or args.model == ""gat"":\n            output = output[:, -1, :]\n        loss_train = F.nll_loss(output, labels, class_weight)\n        loss += bs * loss_train.item()\n        total += bs\n        loss_train.backward()\n        optimizer.step()\n    logger.info(""train loss in this epoch %f"", loss / total)\n    tensorboard_logger.log_value(\'train_loss\', loss / total, epoch + 1)\n    if (epoch + 1) % args.check_point == 0:\n        logger.info(""epoch %d, checkpoint!"", epoch)\n        best_thr = evaluate(epoch, valid_loader, return_best_thr=True, log_desc=\'valid_\')\n        evaluate(epoch, test_loader, thr=best_thr, log_desc=\'test_\')\n\n\n# Train model\nt_total = time.time()\nlogger.info(""training..."")\nfor epoch in range(args.epochs):\n    train(epoch, train_loader, valid_loader, test_loader)\nlogger.info(""optimization Finished!"")\nlogger.info(""total time elapsed: {:.4f}s"".format(time.time() - t_total))\n\nlogger.info(""retrieve best threshold..."")\nbest_thr = evaluate(args.epochs, valid_loader, return_best_thr=True, log_desc=\'valid_\')\n\n# Testing\nlogger.info(""testing..."")\nevaluate(args.epochs, test_loader, thr=best_thr, log_desc=\'test_\')\n'"
src/utils.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n# File Name: utils.py\n# Author: Jiezhong Qiu\n# Create Time: 2018/07/22 20:49\n# TODO:\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef load_w2v_feature(file, max_idx=0):\n    with open(file, ""rb"") as f:\n        nu = 0\n        for line in f:\n            content = line.strip().split()\n            nu += 1\n            if nu == 1:\n                n, d = int(content[0]), int(content[1])\n                feature = [[0.] * d for i in range(max(n, max_idx + 1))]\n                continue\n            index = int(content[0])\n            while len(feature) <= index:\n                feature.append([0.] * d)\n            for i, x in enumerate(content[1:]):\n                feature[index][i] = float(x)\n    for item in feature:\n        assert len(item) == d\n    return np.array(feature, dtype=np.float32)\n'"
