file_path,api_count,code
provider.py,0,"b'import numpy as np\n\ndef normalize_data(batch_data):\n    """""" Normalize the batch data, use coordinates of the block centered at origin,\n        Input:\n            BxNxC array\n        Output:\n            BxNxC array\n    """"""\n    B, N, C = batch_data.shape\n    normal_data = np.zeros((B, N, C))\n    for b in range(B):\n        pc = batch_data[b]\n        centroid = np.mean(pc, axis=0)\n        pc = pc - centroid\n        m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n        pc = pc / m\n        normal_data[b] = pc\n    return normal_data\n\n\ndef shuffle_data(data, labels):\n    """""" Shuffle data and labels.\n        Input:\n          data: B,N,... numpy array\n          label: B,... numpy array\n        Return:\n          shuffled data, label and shuffle indices\n    """"""\n    idx = np.arange(len(labels))\n    np.random.shuffle(idx)\n    return data[idx, ...], labels[idx], idx\n\ndef shuffle_points(batch_data):\n    """""" Shuffle orders of points in each point cloud -- changes FPS behavior.\n        Use the same shuffling idx for the entire batch.\n        Input:\n            BxNxC array\n        Output:\n            BxNxC array\n    """"""\n    idx = np.arange(batch_data.shape[1])\n    np.random.shuffle(idx)\n    return batch_data[:,idx,:]\n\ndef rotate_point_cloud(batch_data):\n    """""" Randomly rotate the point clouds to augument the dataset\n        rotation is per shape based along up direction\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    """"""\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, 0, sinval],\n                                    [0, 1, 0],\n                                    [-sinval, 0, cosval]])\n        shape_pc = batch_data[k, ...]\n        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n    return rotated_data\n\ndef rotate_point_cloud_z(batch_data):\n    """""" Randomly rotate the point clouds to augument the dataset\n        rotation is per shape based along up direction\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    """"""\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, sinval, 0],\n                                    [-sinval, cosval, 0],\n                                    [0, 0, 1]])\n        shape_pc = batch_data[k, ...]\n        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n    return rotated_data\n\ndef rotate_point_cloud_with_normal(batch_xyz_normal):\n    \'\'\' Randomly rotate XYZ, normal point cloud.\n        Input:\n            batch_xyz_normal: B,N,6, first three channels are XYZ, last 3 all normal\n        Output:\n            B,N,6, rotated XYZ, normal point cloud\n    \'\'\'\n    for k in range(batch_xyz_normal.shape[0]):\n        rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, 0, sinval],\n                                    [0, 1, 0],\n                                    [-sinval, 0, cosval]])\n        shape_pc = batch_xyz_normal[k,:,0:3]\n        shape_normal = batch_xyz_normal[k,:,3:6]\n        batch_xyz_normal[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n        batch_xyz_normal[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), rotation_matrix)\n    return batch_xyz_normal\n\ndef rotate_perturbation_point_cloud_with_normal(batch_data, angle_sigma=0.06, angle_clip=0.18):\n    """""" Randomly perturb the point clouds by small rotations\n        Input:\n          BxNx6 array, original batch of point clouds and point normals\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    """"""\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)\n        Rx = np.array([[1,0,0],\n                       [0,np.cos(angles[0]),-np.sin(angles[0])],\n                       [0,np.sin(angles[0]),np.cos(angles[0])]])\n        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],\n                       [0,1,0],\n                       [-np.sin(angles[1]),0,np.cos(angles[1])]])\n        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],\n                       [np.sin(angles[2]),np.cos(angles[2]),0],\n                       [0,0,1]])\n        R = np.dot(Rz, np.dot(Ry,Rx))\n        shape_pc = batch_data[k,:,0:3]\n        shape_normal = batch_data[k,:,3:6]\n        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), R)\n        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1, 3)), R)\n    return rotated_data\n\n\ndef rotate_point_cloud_by_angle(batch_data, rotation_angle):\n    """""" Rotate the point cloud along up direction with certain angle.\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    """"""\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        #rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, 0, sinval],\n                                    [0, 1, 0],\n                                    [-sinval, 0, cosval]])\n        shape_pc = batch_data[k,:,0:3]\n        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n    return rotated_data\n\ndef rotate_point_cloud_by_angle_with_normal(batch_data, rotation_angle):\n    """""" Rotate the point cloud along up direction with certain angle.\n        Input:\n          BxNx6 array, original batch of point clouds with normal\n          scalar, angle of rotation\n        Return:\n          BxNx6 array, rotated batch of point clouds iwth normal\n    """"""\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        #rotation_angle = np.random.uniform() * 2 * np.pi\n        cosval = np.cos(rotation_angle)\n        sinval = np.sin(rotation_angle)\n        rotation_matrix = np.array([[cosval, 0, sinval],\n                                    [0, 1, 0],\n                                    [-sinval, 0, cosval]])\n        shape_pc = batch_data[k,:,0:3]\n        shape_normal = batch_data[k,:,3:6]\n        rotated_data[k,:,0:3] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n        rotated_data[k,:,3:6] = np.dot(shape_normal.reshape((-1,3)), rotation_matrix)\n    return rotated_data\n\n\n\ndef rotate_perturbation_point_cloud(batch_data, angle_sigma=0.06, angle_clip=0.18):\n    """""" Randomly perturb the point clouds by small rotations\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, rotated batch of point clouds\n    """"""\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n    for k in range(batch_data.shape[0]):\n        angles = np.clip(angle_sigma*np.random.randn(3), -angle_clip, angle_clip)\n        Rx = np.array([[1,0,0],\n                       [0,np.cos(angles[0]),-np.sin(angles[0])],\n                       [0,np.sin(angles[0]),np.cos(angles[0])]])\n        Ry = np.array([[np.cos(angles[1]),0,np.sin(angles[1])],\n                       [0,1,0],\n                       [-np.sin(angles[1]),0,np.cos(angles[1])]])\n        Rz = np.array([[np.cos(angles[2]),-np.sin(angles[2]),0],\n                       [np.sin(angles[2]),np.cos(angles[2]),0],\n                       [0,0,1]])\n        R = np.dot(Rz, np.dot(Ry,Rx))\n        shape_pc = batch_data[k, ...]\n        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), R)\n    return rotated_data\n\n\ndef jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n    """""" Randomly jitter points. jittering is per point.\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, jittered batch of point clouds\n    """"""\n    B, N, C = batch_data.shape\n    assert(clip > 0)\n    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1*clip, clip)\n    jittered_data += batch_data\n    return jittered_data\n\ndef shift_point_cloud(batch_data, shift_range=0.1):\n    """""" Randomly shift point cloud. Shift is per point cloud.\n        Input:\n          BxNx3 array, original batch of point clouds\n        Return:\n          BxNx3 array, shifted batch of point clouds\n    """"""\n    B, N, C = batch_data.shape\n    shifts = np.random.uniform(-shift_range, shift_range, (B,3))\n    for batch_index in range(B):\n        batch_data[batch_index,:,:] += shifts[batch_index,:]\n    return batch_data\n\n\ndef random_scale_point_cloud(batch_data, scale_low=0.8, scale_high=1.25):\n    """""" Randomly scale the point cloud. Scale is per point cloud.\n        Input:\n            BxNx3 array, original batch of point clouds\n        Return:\n            BxNx3 array, scaled batch of point clouds\n    """"""\n    B, N, C = batch_data.shape\n    scales = np.random.uniform(scale_low, scale_high, B)\n    for batch_index in range(B):\n        batch_data[batch_index,:,:] *= scales[batch_index]\n    return batch_data\n\ndef random_point_dropout(batch_pc, max_dropout_ratio=0.875):\n    \'\'\' batch_pc: BxNx3 \'\'\'\n    for b in range(batch_pc.shape[0]):\n        dropout_ratio =  np.random.random()*max_dropout_ratio # 0~0.875\n        drop_idx = np.where(np.random.random((batch_pc.shape[1]))<=dropout_ratio)[0]\n        if len(drop_idx)>0:\n            batch_pc[b,drop_idx,:] = batch_pc[b,0,:] # set to the first point\n    return batch_pc\n\n\n\n'"
test_cls.py,4,"b'""""""\nAuthor: Benny\nDate: Nov 2019\n""""""\nfrom data_utils.ModelNetDataLoader import ModelNetDataLoader\nimport argparse\nimport numpy as np\nimport os\nimport torch\nimport logging\nfrom tqdm import tqdm\nimport sys\nimport importlib\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, \'models\'))\n\n\ndef parse_args():\n    \'\'\'PARAMETERS\'\'\'\n    parser = argparse.ArgumentParser(\'PointNet\')\n    parser.add_argument(\'--batch_size\', type=int, default=24, help=\'batch size in training\')\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', help=\'specify gpu device\')\n    parser.add_argument(\'--num_point\', type=int, default=1024, help=\'Point Number [default: 1024]\')\n    parser.add_argument(\'--log_dir\', type=str, default=\'pointnet2_ssg_normal\', help=\'Experiment root\')\n    parser.add_argument(\'--normal\', action=\'store_true\', default=True, help=\'Whether to use normal information [default: False]\')\n    parser.add_argument(\'--num_votes\', type=int, default=3, help=\'Aggregate classification scores with voting [default: 3]\')\n    return parser.parse_args()\n\ndef test(model, loader, num_class=40, vote_num=1):\n    mean_correct = []\n    class_acc = np.zeros((num_class,3))\n    for j, data in tqdm(enumerate(loader), total=len(loader)):\n        points, target = data\n        target = target[:, 0]\n        points = points.transpose(2, 1)\n        points, target = points.cuda(), target.cuda()\n        classifier = model.eval()\n        vote_pool = torch.zeros(target.size()[0],num_class).cuda()\n        for _ in range(vote_num):\n            pred, _ = classifier(points)\n            vote_pool += pred\n        pred = vote_pool/vote_num\n        pred_choice = pred.data.max(1)[1]\n        for cat in np.unique(target.cpu()):\n            classacc = pred_choice[target==cat].eq(target[target==cat].long().data).cpu().sum()\n            class_acc[cat,0]+= classacc.item()/float(points[target==cat].size()[0])\n            class_acc[cat,1]+=1\n        correct = pred_choice.eq(target.long().data).cpu().sum()\n        mean_correct.append(correct.item()/float(points.size()[0]))\n    class_acc[:,2] =  class_acc[:,0]/ class_acc[:,1]\n    class_acc = np.mean(class_acc[:,2])\n    instance_acc = np.mean(mean_correct)\n    return instance_acc, class_acc\n\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    \'\'\'HYPER PARAMETER\'\'\'\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n    \'\'\'CREATE DIR\'\'\'\n    experiment_dir = \'log/classification/\' + args.log_dir\n\n    \'\'\'LOG\'\'\'\n    args = parse_args()\n    logger = logging.getLogger(""Model"")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    file_handler = logging.FileHandler(\'%s/eval.txt\' % experiment_dir)\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string(\'PARAMETER ...\')\n    log_string(args)\n\n    \'\'\'DATA LOADING\'\'\'\n    log_string(\'Load dataset ...\')\n    DATA_PATH = \'data/modelnet40_normal_resampled/\'\n    TEST_DATASET = ModelNetDataLoader(root=DATA_PATH, npoint=args.num_point, split=\'test\', normal_channel=args.normal)\n    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=args.batch_size, shuffle=False, num_workers=4)\n\n    \'\'\'MODEL LOADING\'\'\'\n    num_class = 40\n    model_name = os.listdir(experiment_dir+\'/logs\')[0].split(\'.\')[0]\n    MODEL = importlib.import_module(model_name)\n\n    classifier = MODEL.get_model(num_class,normal_channel=args.normal).cuda()\n\n    checkpoint = torch.load(str(experiment_dir) + \'/checkpoints/best_model.pth\')\n    classifier.load_state_dict(checkpoint[\'model_state_dict\'])\n\n    with torch.no_grad():\n        instance_acc, class_acc = test(classifier.eval(), testDataLoader, vote_num=args.num_votes)\n        log_string(\'Test Instance Accuracy: %f, Class Accuracy: %f\' % (instance_acc, class_acc))\n\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    main(args)\n'"
test_partseg.py,5,"b'""""""\nAuthor: Benny\nDate: Nov 2019\n""""""\nimport argparse\nimport os\nfrom data_utils.ShapeNetDataLoader import PartNormalDataset\nimport torch\nimport logging\nimport sys\nimport importlib\nfrom tqdm import tqdm\nimport numpy as np\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, \'models\'))\n\nseg_classes = {\'Earphone\': [16, 17, 18], \'Motorbike\': [30, 31, 32, 33, 34, 35], \'Rocket\': [41, 42, 43], \'Car\': [8, 9, 10, 11], \'Laptop\': [28, 29], \'Cap\': [6, 7], \'Skateboard\': [44, 45, 46], \'Mug\': [36, 37], \'Guitar\': [19, 20, 21], \'Bag\': [4, 5], \'Lamp\': [24, 25, 26, 27], \'Table\': [47, 48, 49], \'Airplane\': [0, 1, 2, 3], \'Pistol\': [38, 39, 40], \'Chair\': [12, 13, 14, 15], \'Knife\': [22, 23]}\nseg_label_to_cat = {} # {0:Airplane, 1:Airplane, ...49:Table}\nfor cat in seg_classes.keys():\n    for label in seg_classes[cat]:\n        seg_label_to_cat[label] = cat\n\ndef to_categorical(y, num_classes):\n    """""" 1-hot encodes a tensor """"""\n    new_y = torch.eye(num_classes)[y.cpu().data.numpy(),]\n    if (y.is_cuda):\n        return new_y.cuda()\n    return new_y\n\n\ndef parse_args():\n    \'\'\'PARAMETERS\'\'\'\n    parser = argparse.ArgumentParser(\'PointNet\')\n    parser.add_argument(\'--batch_size\', type=int, default=24, help=\'batch size in testing [default: 24]\')\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', help=\'specify gpu device [default: 0]\')\n    parser.add_argument(\'--num_point\', type=int, default=2048, help=\'Point Number [default: 2048]\')\n    parser.add_argument(\'--log_dir\', type=str, default=\'pointnet2_part_seg_ssg\', help=\'Experiment root\')\n    parser.add_argument(\'--normal\', action=\'store_true\', default=False, help=\'Whether to use normal information [default: False]\')\n    parser.add_argument(\'--num_votes\', type=int, default=3, help=\'Aggregate segmentation scores with voting [default: 3]\')\n    return parser.parse_args()\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    \'\'\'HYPER PARAMETER\'\'\'\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n    experiment_dir = \'log/part_seg/\' + args.log_dir\n\n    \'\'\'LOG\'\'\'\n    args = parse_args()\n    logger = logging.getLogger(""Model"")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    file_handler = logging.FileHandler(\'%s/eval.txt\' % experiment_dir)\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string(\'PARAMETER ...\')\n    log_string(args)\n\n    root = \'data/shapenetcore_partanno_segmentation_benchmark_v0_normal/\'\n\n    TEST_DATASET = PartNormalDataset(root = root, npoints=args.num_point, split=\'test\', normal_channel=args.normal)\n    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=args.batch_size,shuffle=False, num_workers=4)\n    log_string(""The number of test data is: %d"" %  len(TEST_DATASET))\n    num_classes = 16\n    num_part = 50\n\n    \'\'\'MODEL LOADING\'\'\'\n    model_name = os.listdir(experiment_dir+\'/logs\')[0].split(\'.\')[0]\n    MODEL = importlib.import_module(model_name)\n    classifier = MODEL.get_model(num_part, normal_channel=args.normal).cuda()\n    checkpoint = torch.load(str(experiment_dir) + \'/checkpoints/best_model.pth\')\n    classifier.load_state_dict(checkpoint[\'model_state_dict\'])\n\n\n    with torch.no_grad():\n        test_metrics = {}\n        total_correct = 0\n        total_seen = 0\n        total_seen_class = [0 for _ in range(num_part)]\n        total_correct_class = [0 for _ in range(num_part)]\n        shape_ious = {cat: [] for cat in seg_classes.keys()}\n        seg_label_to_cat = {}  # {0:Airplane, 1:Airplane, ...49:Table}\n        for cat in seg_classes.keys():\n            for label in seg_classes[cat]:\n                seg_label_to_cat[label] = cat\n\n        for batch_id, (points, label, target) in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n            batchsize, num_point, _ = points.size()\n            cur_batch_size, NUM_POINT, _ = points.size()\n            points, label, target = points.float().cuda(), label.long().cuda(), target.long().cuda()\n            points = points.transpose(2, 1)\n            classifier = classifier.eval()\n            vote_pool = torch.zeros(target.size()[0], target.size()[1], num_part).cuda()\n            for _ in range(args.num_votes):\n                seg_pred, _ = classifier(points, to_categorical(label, num_classes))\n                vote_pool += seg_pred\n            seg_pred = vote_pool / args.num_votes\n            cur_pred_val = seg_pred.cpu().data.numpy()\n            cur_pred_val_logits = cur_pred_val\n            cur_pred_val = np.zeros((cur_batch_size, NUM_POINT)).astype(np.int32)\n            target = target.cpu().data.numpy()\n            for i in range(cur_batch_size):\n                cat = seg_label_to_cat[target[i, 0]]\n                logits = cur_pred_val_logits[i, :, :]\n                cur_pred_val[i, :] = np.argmax(logits[:, seg_classes[cat]], 1) + seg_classes[cat][0]\n            correct = np.sum(cur_pred_val == target)\n            total_correct += correct\n            total_seen += (cur_batch_size * NUM_POINT)\n\n            for l in range(num_part):\n                total_seen_class[l] += np.sum(target == l)\n                total_correct_class[l] += (np.sum((cur_pred_val == l) & (target == l)))\n\n            for i in range(cur_batch_size):\n                segp = cur_pred_val[i, :]\n                segl = target[i, :]\n                cat = seg_label_to_cat[segl[0]]\n                part_ious = [0.0 for _ in range(len(seg_classes[cat]))]\n                for l in seg_classes[cat]:\n                    if (np.sum(segl == l) == 0) and (\n                            np.sum(segp == l) == 0):  # part is not present, no prediction as well\n                        part_ious[l - seg_classes[cat][0]] = 1.0\n                    else:\n                        part_ious[l - seg_classes[cat][0]] = np.sum((segl == l) & (segp == l)) / float(\n                            np.sum((segl == l) | (segp == l)))\n                shape_ious[cat].append(np.mean(part_ious))\n\n        all_shape_ious = []\n        for cat in shape_ious.keys():\n            for iou in shape_ious[cat]:\n                all_shape_ious.append(iou)\n            shape_ious[cat] = np.mean(shape_ious[cat])\n        mean_shape_ious = np.mean(list(shape_ious.values()))\n        test_metrics[\'accuracy\'] = total_correct / float(total_seen)\n        test_metrics[\'class_avg_accuracy\'] = np.mean(\n            np.array(total_correct_class) / np.array(total_seen_class, dtype=np.float))\n        for cat in sorted(shape_ious.keys()):\n            log_string(\'eval mIoU of %s %f\' % (cat + \' \' * (14 - len(cat)), shape_ious[cat]))\n        test_metrics[\'class_avg_iou\'] = mean_shape_ious\n        test_metrics[\'inctance_avg_iou\'] = np.mean(all_shape_ious)\n\n\n    log_string(\'Accuracy is: %.5f\'%test_metrics[\'accuracy\'])\n    log_string(\'Class avg accuracy is: %.5f\'%test_metrics[\'class_avg_accuracy\'])\n    log_string(\'Class avg mIOU is: %.5f\'%test_metrics[\'class_avg_iou\'])\n    log_string(\'Inctance avg mIOU is: %.5f\'%test_metrics[\'inctance_avg_iou\'])\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    main(args)\n\n'"
test_semseg.py,3,"b'""""""\nAuthor: Benny\nDate: Nov 2019\n""""""\nimport argparse\nimport os\nfrom data_utils.S3DISDataLoader import ScannetDatasetWholeScene\nfrom data_utils.indoor3d_util import g_label2color\nimport torch\nimport logging\nfrom pathlib import Path\nimport sys\nimport importlib\nfrom tqdm import tqdm\nimport provider\nimport numpy as np\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, \'models\'))\n\nclasses = [\'ceiling\',\'floor\',\'wall\',\'beam\',\'column\',\'window\',\'door\',\'table\',\'chair\',\'sofa\',\'bookcase\',\'board\',\'clutter\']\nclass2label = {cls: i for i,cls in enumerate(classes)}\nseg_classes = class2label\nseg_label_to_cat = {}\nfor i,cat in enumerate(seg_classes.keys()):\n    seg_label_to_cat[i] = cat\n\ndef parse_args():\n    \'\'\'PARAMETERS\'\'\'\n    parser = argparse.ArgumentParser(\'Model\')\n    parser.add_argument(\'--batch_size\', type=int, default=32, help=\'batch size in testing [default: 32]\')\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', help=\'specify gpu device\')\n    parser.add_argument(\'--num_point\', type=int, default=4096, help=\'Point Number [default: 4096]\')\n    parser.add_argument(\'--log_dir\', type=str, default=\'pointnet2_sem_seg\', help=\'Experiment root\')\n    parser.add_argument(\'--visual\', action=\'store_true\', default=False, help=\'Whether visualize result [default: False]\')\n    parser.add_argument(\'--test_area\', type=int, default=5, help=\'Which area to use for test, option: 1-6 [default: 5]\')\n    parser.add_argument(\'--num_votes\', type=int, default=5, help=\'Aggregate segmentation scores with voting [default: 5]\')\n    return parser.parse_args()\n\ndef add_vote(vote_label_pool, point_idx, pred_label, weight):\n    B = pred_label.shape[0]\n    N = pred_label.shape[1]\n    for b in range(B):\n        for n in range(N):\n            if weight[b,n]:\n                vote_label_pool[int(point_idx[b, n]), int(pred_label[b, n])] += 1\n    return vote_label_pool\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    \'\'\'HYPER PARAMETER\'\'\'\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n    experiment_dir = \'log/sem_seg/\' + args.log_dir\n    visual_dir = experiment_dir + \'/visual/\'\n    visual_dir = Path(visual_dir)\n    visual_dir.mkdir(exist_ok=True)\n\n    \'\'\'LOG\'\'\'\n    args = parse_args()\n    logger = logging.getLogger(""Model"")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    file_handler = logging.FileHandler(\'%s/eval.txt\' % experiment_dir)\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string(\'PARAMETER ...\')\n    log_string(args)\n\n    NUM_CLASSES = 13\n    BATCH_SIZE = args.batch_size\n    NUM_POINT = args.num_point\n\n    root = \'data/stanford_indoor3d/\'\n\n    TEST_DATASET_WHOLE_SCENE = ScannetDatasetWholeScene(root, split=\'test\', test_area=args.test_area, block_points=NUM_POINT)\n    log_string(""The number of test data is: %d"" %  len(TEST_DATASET_WHOLE_SCENE))\n\n    \'\'\'MODEL LOADING\'\'\'\n    model_name = os.listdir(experiment_dir+\'/logs\')[0].split(\'.\')[0]\n    MODEL = importlib.import_module(model_name)\n    classifier = MODEL.get_model(NUM_CLASSES).cuda()\n    checkpoint = torch.load(str(experiment_dir) + \'/checkpoints/best_model.pth\')\n    classifier.load_state_dict(checkpoint[\'model_state_dict\'])\n\n    with torch.no_grad():\n        scene_id = TEST_DATASET_WHOLE_SCENE.file_list\n        scene_id = [x[:-4] for x in scene_id]\n        num_batches = len(TEST_DATASET_WHOLE_SCENE)\n\n        total_seen_class = [0 for _ in range(NUM_CLASSES)]\n        total_correct_class = [0 for _ in range(NUM_CLASSES)]\n        total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n\n        log_string(\'---- EVALUATION WHOLE SCENE----\')\n\n        for batch_idx in range(num_batches):\n            print(""visualize [%d/%d] %s ..."" % (batch_idx+1, num_batches, scene_id[batch_idx]))\n            total_seen_class_tmp = [0 for _ in range(NUM_CLASSES)]\n            total_correct_class_tmp = [0 for _ in range(NUM_CLASSES)]\n            total_iou_deno_class_tmp = [0 for _ in range(NUM_CLASSES)]\n            if args.visual:\n                fout = open(os.path.join(visual_dir, scene_id[batch_idx] + \'_pred.obj\'), \'w\')\n                fout_gt = open(os.path.join(visual_dir, scene_id[batch_idx] + \'_gt.obj\'), \'w\')\n\n            whole_scene_data = TEST_DATASET_WHOLE_SCENE.scene_points_list[batch_idx]\n            whole_scene_label = TEST_DATASET_WHOLE_SCENE.semantic_labels_list[batch_idx]\n            vote_label_pool = np.zeros((whole_scene_label.shape[0], NUM_CLASSES))\n            for _ in tqdm(range(args.num_votes), total=args.num_votes):\n                scene_data, scene_label, scene_smpw, scene_point_index = TEST_DATASET_WHOLE_SCENE[batch_idx]\n                num_blocks = scene_data.shape[0]\n                s_batch_num = (num_blocks + BATCH_SIZE - 1) // BATCH_SIZE\n                batch_data = np.zeros((BATCH_SIZE, NUM_POINT, 9))\n\n                batch_label = np.zeros((BATCH_SIZE, NUM_POINT))\n                batch_point_index = np.zeros((BATCH_SIZE, NUM_POINT))\n                batch_smpw = np.zeros((BATCH_SIZE, NUM_POINT))\n                for sbatch in range(s_batch_num):\n                    start_idx = sbatch * BATCH_SIZE\n                    end_idx = min((sbatch + 1) * BATCH_SIZE, num_blocks)\n                    real_batch_size = end_idx - start_idx\n                    batch_data[0:real_batch_size, ...] = scene_data[start_idx:end_idx, ...]\n                    batch_label[0:real_batch_size, ...] = scene_label[start_idx:end_idx, ...]\n                    batch_point_index[0:real_batch_size, ...] = scene_point_index[start_idx:end_idx, ...]\n                    batch_smpw[0:real_batch_size, ...] = scene_smpw[start_idx:end_idx, ...]\n                    batch_data[:, :, 3:6] /= 1.0\n\n                    torch_data = torch.Tensor(batch_data)\n                    torch_data= torch_data.float().cuda()\n                    torch_data = torch_data.transpose(2, 1)\n                    seg_pred, _ = classifier(torch_data)\n                    batch_pred_label = seg_pred.contiguous().cpu().data.max(2)[1].numpy()\n\n                    vote_label_pool = add_vote(vote_label_pool, batch_point_index[0:real_batch_size, ...],\n                                               batch_pred_label[0:real_batch_size, ...],\n                                               batch_smpw[0:real_batch_size, ...])\n\n            pred_label = np.argmax(vote_label_pool, 1)\n\n            for l in range(NUM_CLASSES):\n                total_seen_class_tmp[l] += np.sum((whole_scene_label == l))\n                total_correct_class_tmp[l] += np.sum((pred_label == l) & (whole_scene_label == l))\n                total_iou_deno_class_tmp[l] += np.sum(((pred_label == l) | (whole_scene_label == l)))\n                total_seen_class[l] += total_seen_class_tmp[l]\n                total_correct_class[l] += total_correct_class_tmp[l]\n                total_iou_deno_class[l] += total_iou_deno_class_tmp[l]\n\n            iou_map = np.array(total_correct_class_tmp) / (np.array(total_iou_deno_class_tmp, dtype=np.float) + 1e-6)\n            print(iou_map)\n            arr = np.array(total_seen_class_tmp)\n            tmp_iou = np.mean(iou_map[arr != 0])\n            log_string(\'Mean IoU of %s: %.4f\' % (scene_id[batch_idx], tmp_iou))\n            print(\'----------------------------\')\n\n            filename = os.path.join(visual_dir, scene_id[batch_idx] + \'.txt\')\n            with open(filename, \'w\') as pl_save:\n                for i in pred_label:\n                    pl_save.write(str(int(i)) + \'\\n\')\n                pl_save.close()\n            for i in range(whole_scene_label.shape[0]):\n                color = g_label2color[pred_label[i]]\n                color_gt = g_label2color[whole_scene_label[i]]\n                if args.visual:\n                    fout.write(\'v %f %f %f %d %d %d\\n\' % (\n                    whole_scene_data[i, 0], whole_scene_data[i, 1], whole_scene_data[i, 2], color[0], color[1],\n                    color[2]))\n                    fout_gt.write(\n                        \'v %f %f %f %d %d %d\\n\' % (\n                        whole_scene_data[i, 0], whole_scene_data[i, 1], whole_scene_data[i, 2], color_gt[0],\n                        color_gt[1], color_gt[2]))\n            if args.visual:\n                fout.close()\n                fout_gt.close()\n\n        IoU = np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=np.float) + 1e-6)\n        iou_per_class_str = \'------- IoU --------\\n\'\n        for l in range(NUM_CLASSES):\n            iou_per_class_str += \'class %s, IoU: %.3f \\n\' % (\n                seg_label_to_cat[l] + \' \' * (14 - len(seg_label_to_cat[l])),\n                total_correct_class[l] / float(total_iou_deno_class[l]))\n        log_string(iou_per_class_str)\n        log_string(\'eval point avg class IoU: %f\' % np.mean(IoU))\n        log_string(\'eval whole scene point avg class acc: %f\' % (\n            np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=np.float) + 1e-6))))\n        log_string(\'eval whole scene point accuracy: %f\' % (\n                    np.sum(total_correct_class) / float(np.sum(total_seen_class) + 1e-6)))\n\n        print(""Done!"")\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    main(args)\n'"
train_cls.py,9,"b'""""""\nAuthor: Benny\nDate: Nov 2019\n""""""\nfrom data_utils.ModelNetDataLoader import ModelNetDataLoader\nimport argparse\nimport numpy as np\nimport os\nimport torch\nimport datetime\nimport logging\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport sys\nimport provider\nimport importlib\nimport shutil\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, \'models\'))\n\n\ndef parse_args():\n    \'\'\'PARAMETERS\'\'\'\n    parser = argparse.ArgumentParser(\'PointNet\')\n    parser.add_argument(\'--batch_size\', type=int, default=24, help=\'batch size in training [default: 24]\')\n    parser.add_argument(\'--model\', default=\'pointnet_cls\', help=\'model name [default: pointnet_cls]\')\n    parser.add_argument(\'--epoch\',  default=200, type=int, help=\'number of epoch in training [default: 200]\')\n    parser.add_argument(\'--learning_rate\', default=0.001, type=float, help=\'learning rate in training [default: 0.001]\')\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', help=\'specify gpu device [default: 0]\')\n    parser.add_argument(\'--num_point\', type=int, default=1024, help=\'Point Number [default: 1024]\')\n    parser.add_argument(\'--optimizer\', type=str, default=\'Adam\', help=\'optimizer for training [default: Adam]\')\n    parser.add_argument(\'--log_dir\', type=str, default=None, help=\'experiment root\')\n    parser.add_argument(\'--decay_rate\', type=float, default=1e-4, help=\'decay rate [default: 1e-4]\')\n    parser.add_argument(\'--normal\', action=\'store_true\', default=False, help=\'Whether to use normal information [default: False]\')\n    return parser.parse_args()\n\ndef test(model, loader, num_class=40):\n    mean_correct = []\n    class_acc = np.zeros((num_class,3))\n    for j, data in tqdm(enumerate(loader), total=len(loader)):\n        points, target = data\n        target = target[:, 0]\n        points = points.transpose(2, 1)\n        points, target = points.cuda(), target.cuda()\n        classifier = model.eval()\n        pred, _ = classifier(points)\n        pred_choice = pred.data.max(1)[1]\n        for cat in np.unique(target.cpu()):\n            classacc = pred_choice[target==cat].eq(target[target==cat].long().data).cpu().sum()\n            class_acc[cat,0]+= classacc.item()/float(points[target==cat].size()[0])\n            class_acc[cat,1]+=1\n        correct = pred_choice.eq(target.long().data).cpu().sum()\n        mean_correct.append(correct.item()/float(points.size()[0]))\n    class_acc[:,2] =  class_acc[:,0]/ class_acc[:,1]\n    class_acc = np.mean(class_acc[:,2])\n    instance_acc = np.mean(mean_correct)\n    return instance_acc, class_acc\n\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    \'\'\'HYPER PARAMETER\'\'\'\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n    \'\'\'CREATE DIR\'\'\'\n    timestr = str(datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M\'))\n    experiment_dir = Path(\'./log/\')\n    experiment_dir.mkdir(exist_ok=True)\n    experiment_dir = experiment_dir.joinpath(\'classification\')\n    experiment_dir.mkdir(exist_ok=True)\n    if args.log_dir is None:\n        experiment_dir = experiment_dir.joinpath(timestr)\n    else:\n        experiment_dir = experiment_dir.joinpath(args.log_dir)\n    experiment_dir.mkdir(exist_ok=True)\n    checkpoints_dir = experiment_dir.joinpath(\'checkpoints/\')\n    checkpoints_dir.mkdir(exist_ok=True)\n    log_dir = experiment_dir.joinpath(\'logs/\')\n    log_dir.mkdir(exist_ok=True)\n\n    \'\'\'LOG\'\'\'\n    args = parse_args()\n    logger = logging.getLogger(""Model"")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    file_handler = logging.FileHandler(\'%s/%s.txt\' % (log_dir, args.model))\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string(\'PARAMETER ...\')\n    log_string(args)\n\n    \'\'\'DATA LOADING\'\'\'\n    log_string(\'Load dataset ...\')\n    DATA_PATH = \'data/modelnet40_normal_resampled/\'\n\n    TRAIN_DATASET = ModelNetDataLoader(root=DATA_PATH, npoint=args.num_point, split=\'train\',\n                                                     normal_channel=args.normal)\n    TEST_DATASET = ModelNetDataLoader(root=DATA_PATH, npoint=args.num_point, split=\'test\',\n                                                    normal_channel=args.normal)\n    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=args.batch_size, shuffle=True, num_workers=4)\n    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=args.batch_size, shuffle=False, num_workers=4)\n\n    \'\'\'MODEL LOADING\'\'\'\n    num_class = 40\n    MODEL = importlib.import_module(args.model)\n    shutil.copy(\'./models/%s.py\' % args.model, str(experiment_dir))\n    shutil.copy(\'./models/pointnet_util.py\', str(experiment_dir))\n\n    classifier = MODEL.get_model(num_class,normal_channel=args.normal).cuda()\n    criterion = MODEL.get_loss().cuda()\n\n    try:\n        checkpoint = torch.load(str(experiment_dir) + \'/checkpoints/best_model.pth\')\n        start_epoch = checkpoint[\'epoch\']\n        classifier.load_state_dict(checkpoint[\'model_state_dict\'])\n        log_string(\'Use pretrain model\')\n    except:\n        log_string(\'No existing model, starting training from scratch...\')\n        start_epoch = 0\n\n\n    if args.optimizer == \'Adam\':\n        optimizer = torch.optim.Adam(\n            classifier.parameters(),\n            lr=args.learning_rate,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=args.decay_rate\n        )\n    else:\n        optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)\n    global_epoch = 0\n    global_step = 0\n    best_instance_acc = 0.0\n    best_class_acc = 0.0\n    mean_correct = []\n\n    \'\'\'TRANING\'\'\'\n    logger.info(\'Start training...\')\n    for epoch in range(start_epoch,args.epoch):\n        log_string(\'Epoch %d (%d/%s):\' % (global_epoch + 1, epoch + 1, args.epoch))\n\n        scheduler.step()\n        for batch_id, data in tqdm(enumerate(trainDataLoader, 0), total=len(trainDataLoader), smoothing=0.9):\n            points, target = data\n            points = points.data.numpy()\n            points = provider.random_point_dropout(points)\n            points[:,:, 0:3] = provider.random_scale_point_cloud(points[:,:, 0:3])\n            points[:,:, 0:3] = provider.shift_point_cloud(points[:,:, 0:3])\n            points = torch.Tensor(points)\n            target = target[:, 0]\n\n            points = points.transpose(2, 1)\n            points, target = points.cuda(), target.cuda()\n            optimizer.zero_grad()\n\n            classifier = classifier.train()\n            pred, trans_feat = classifier(points)\n            loss = criterion(pred, target.long(), trans_feat)\n            pred_choice = pred.data.max(1)[1]\n            correct = pred_choice.eq(target.long().data).cpu().sum()\n            mean_correct.append(correct.item() / float(points.size()[0]))\n            loss.backward()\n            optimizer.step()\n            global_step += 1\n\n        train_instance_acc = np.mean(mean_correct)\n        log_string(\'Train Instance Accuracy: %f\' % train_instance_acc)\n\n\n        with torch.no_grad():\n            instance_acc, class_acc = test(classifier.eval(), testDataLoader)\n\n            if (instance_acc >= best_instance_acc):\n                best_instance_acc = instance_acc\n                best_epoch = epoch + 1\n\n            if (class_acc >= best_class_acc):\n                best_class_acc = class_acc\n            log_string(\'Test Instance Accuracy: %f, Class Accuracy: %f\'% (instance_acc, class_acc))\n            log_string(\'Best Instance Accuracy: %f, Class Accuracy: %f\'% (best_instance_acc, best_class_acc))\n\n            if (instance_acc >= best_instance_acc):\n                logger.info(\'Save model...\')\n                savepath = str(checkpoints_dir) + \'/best_model.pth\'\n                log_string(\'Saving at %s\'% savepath)\n                state = {\n                    \'epoch\': best_epoch,\n                    \'instance_acc\': instance_acc,\n                    \'class_acc\': class_acc,\n                    \'model_state_dict\': classifier.state_dict(),\n                    \'optimizer_state_dict\': optimizer.state_dict(),\n                }\n                torch.save(state, savepath)\n            global_epoch += 1\n\n    logger.info(\'End of training...\')\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    main(args)\n'"
train_partseg.py,14,"b'""""""\nAuthor: Benny\nDate: Nov 2019\n""""""\nimport argparse\nimport os\nfrom data_utils.ShapeNetDataLoader import PartNormalDataset\nimport torch\nimport datetime\nimport logging\nfrom pathlib import Path\nimport sys\nimport importlib\nimport shutil\nfrom tqdm import tqdm\nimport provider\nimport numpy as np\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, \'models\'))\n\nseg_classes = {\'Earphone\': [16, 17, 18], \'Motorbike\': [30, 31, 32, 33, 34, 35], \'Rocket\': [41, 42, 43], \'Car\': [8, 9, 10, 11], \'Laptop\': [28, 29], \'Cap\': [6, 7], \'Skateboard\': [44, 45, 46], \'Mug\': [36, 37], \'Guitar\': [19, 20, 21], \'Bag\': [4, 5], \'Lamp\': [24, 25, 26, 27], \'Table\': [47, 48, 49], \'Airplane\': [0, 1, 2, 3], \'Pistol\': [38, 39, 40], \'Chair\': [12, 13, 14, 15], \'Knife\': [22, 23]}\nseg_label_to_cat = {} # {0:Airplane, 1:Airplane, ...49:Table}\nfor cat in seg_classes.keys():\n    for label in seg_classes[cat]:\n        seg_label_to_cat[label] = cat\n\ndef to_categorical(y, num_classes):\n    """""" 1-hot encodes a tensor """"""\n    new_y = torch.eye(num_classes)[y.cpu().data.numpy(),]\n    if (y.is_cuda):\n        return new_y.cuda()\n    return new_y\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\'Model\')\n    parser.add_argument(\'--model\', type=str, default=\'pointnet2_part_seg_msg\', help=\'model name [default: pointnet2_part_seg_msg]\')\n    parser.add_argument(\'--batch_size\', type=int, default=16, help=\'Batch Size during training [default: 16]\')\n    parser.add_argument(\'--epoch\',  default=251, type=int, help=\'Epoch to run [default: 251]\')\n    parser.add_argument(\'--learning_rate\', default=0.001, type=float, help=\'Initial learning rate [default: 0.001]\')\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', help=\'GPU to use [default: GPU 0]\')\n    parser.add_argument(\'--optimizer\', type=str, default=\'Adam\', help=\'Adam or SGD [default: Adam]\')\n    parser.add_argument(\'--log_dir\', type=str, default=None, help=\'Log path [default: None]\')\n    parser.add_argument(\'--decay_rate\', type=float, default=1e-4, help=\'weight decay [default: 1e-4]\')\n    parser.add_argument(\'--npoint\', type=int,  default=2048, help=\'Point Number [default: 2048]\')\n    parser.add_argument(\'--normal\', action=\'store_true\', default=False, help=\'Whether to use normal information [default: False]\')\n    parser.add_argument(\'--step_size\', type=int,  default=20, help=\'Decay step for lr decay [default: every 20 epochs]\')\n    parser.add_argument(\'--lr_decay\', type=float,  default=0.5, help=\'Decay rate for lr decay [default: 0.5]\')\n\n    return parser.parse_args()\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    \'\'\'HYPER PARAMETER\'\'\'\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n    \'\'\'CREATE DIR\'\'\'\n    timestr = str(datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M\'))\n    experiment_dir = Path(\'./log/\')\n    experiment_dir.mkdir(exist_ok=True)\n    experiment_dir = experiment_dir.joinpath(\'part_seg\')\n    experiment_dir.mkdir(exist_ok=True)\n    if args.log_dir is None:\n        experiment_dir = experiment_dir.joinpath(timestr)\n    else:\n        experiment_dir = experiment_dir.joinpath(args.log_dir)\n    experiment_dir.mkdir(exist_ok=True)\n    checkpoints_dir = experiment_dir.joinpath(\'checkpoints/\')\n    checkpoints_dir.mkdir(exist_ok=True)\n    log_dir = experiment_dir.joinpath(\'logs/\')\n    log_dir.mkdir(exist_ok=True)\n\n    \'\'\'LOG\'\'\'\n    args = parse_args()\n    logger = logging.getLogger(""Model"")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    file_handler = logging.FileHandler(\'%s/%s.txt\' % (log_dir, args.model))\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string(\'PARAMETER ...\')\n    log_string(args)\n\n    root = \'data/shapenetcore_partanno_segmentation_benchmark_v0_normal/\'\n\n    TRAIN_DATASET = PartNormalDataset(root = root, npoints=args.npoint, split=\'trainval\', normal_channel=args.normal)\n    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=args.batch_size,shuffle=True, num_workers=4)\n    TEST_DATASET = PartNormalDataset(root = root, npoints=args.npoint, split=\'test\', normal_channel=args.normal)\n    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=args.batch_size,shuffle=False, num_workers=4)\n    log_string(""The number of training data is: %d"" % len(TRAIN_DATASET))\n    log_string(""The number of test data is: %d"" %  len(TEST_DATASET))\n    num_classes = 16\n    num_part = 50\n    \'\'\'MODEL LOADING\'\'\'\n    MODEL = importlib.import_module(args.model)\n    shutil.copy(\'models/%s.py\' % args.model, str(experiment_dir))\n    shutil.copy(\'models/pointnet_util.py\', str(experiment_dir))\n\n    classifier = MODEL.get_model(num_part, normal_channel=args.normal).cuda()\n    criterion = MODEL.get_loss().cuda()\n\n\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv2d\') != -1:\n            torch.nn.init.xavier_normal_(m.weight.data)\n            torch.nn.init.constant_(m.bias.data, 0.0)\n        elif classname.find(\'Linear\') != -1:\n            torch.nn.init.xavier_normal_(m.weight.data)\n            torch.nn.init.constant_(m.bias.data, 0.0)\n\n    try:\n        checkpoint = torch.load(str(experiment_dir) + \'/checkpoints/best_model.pth\')\n        start_epoch = checkpoint[\'epoch\']\n        classifier.load_state_dict(checkpoint[\'model_state_dict\'])\n        log_string(\'Use pretrain model\')\n    except:\n        log_string(\'No existing model, starting training from scratch...\')\n        start_epoch = 0\n        classifier = classifier.apply(weights_init)\n\n    if args.optimizer == \'Adam\':\n        optimizer = torch.optim.Adam(\n            classifier.parameters(),\n            lr=args.learning_rate,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=args.decay_rate\n        )\n    else:\n        optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n\n    def bn_momentum_adjust(m, momentum):\n        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n            m.momentum = momentum\n\n    LEARNING_RATE_CLIP = 1e-5\n    MOMENTUM_ORIGINAL = 0.1\n    MOMENTUM_DECCAY = 0.5\n    MOMENTUM_DECCAY_STEP = args.step_size\n\n    best_acc = 0\n    global_epoch = 0\n    best_class_avg_iou = 0\n    best_inctance_avg_iou = 0\n\n    for epoch in range(start_epoch,args.epoch):\n        log_string(\'Epoch %d (%d/%s):\' % (global_epoch + 1, epoch + 1, args.epoch))\n        \'\'\'Adjust learning rate and BN momentum\'\'\'\n        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n        log_string(\'Learning rate:%f\' % lr)\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr\n        mean_correct = []\n        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n        if momentum < 0.01:\n            momentum = 0.01\n        print(\'BN momentum updated to: %f\' % momentum)\n        classifier = classifier.apply(lambda x: bn_momentum_adjust(x,momentum))\n\n        \'\'\'learning one epoch\'\'\'\n        for i, data in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n            points, label, target = data\n            points = points.data.numpy()\n            points[:,:, 0:3] = provider.random_scale_point_cloud(points[:,:, 0:3])\n            points[:,:, 0:3] = provider.shift_point_cloud(points[:,:, 0:3])\n            points = torch.Tensor(points)\n            points, label, target = points.float().cuda(),label.long().cuda(), target.long().cuda()\n            points = points.transpose(2, 1)\n            optimizer.zero_grad()\n            classifier = classifier.train()\n            seg_pred, trans_feat = classifier(points, to_categorical(label, num_classes))\n            seg_pred = seg_pred.contiguous().view(-1, num_part)\n            target = target.view(-1, 1)[:, 0]\n            pred_choice = seg_pred.data.max(1)[1]\n            correct = pred_choice.eq(target.data).cpu().sum()\n            mean_correct.append(correct.item() / (args.batch_size * args.npoint))\n            loss = criterion(seg_pred, target, trans_feat)\n            loss.backward()\n            optimizer.step()\n        train_instance_acc = np.mean(mean_correct)\n        log_string(\'Train accuracy is: %.5f\' % train_instance_acc)\n\n        with torch.no_grad():\n            test_metrics = {}\n            total_correct = 0\n            total_seen = 0\n            total_seen_class = [0 for _ in range(num_part)]\n            total_correct_class = [0 for _ in range(num_part)]\n            shape_ious = {cat: [] for cat in seg_classes.keys()}\n            seg_label_to_cat = {}  # {0:Airplane, 1:Airplane, ...49:Table}\n            for cat in seg_classes.keys():\n                for label in seg_classes[cat]:\n                    seg_label_to_cat[label] = cat\n\n            for batch_id, (points, label, target) in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n                cur_batch_size, NUM_POINT, _ = points.size()\n                points, label, target = points.float().cuda(), label.long().cuda(), target.long().cuda()\n                points = points.transpose(2, 1)\n                classifier = classifier.eval()\n                seg_pred, _ = classifier(points, to_categorical(label, num_classes))\n                cur_pred_val = seg_pred.cpu().data.numpy()\n                cur_pred_val_logits = cur_pred_val\n                cur_pred_val = np.zeros((cur_batch_size, NUM_POINT)).astype(np.int32)\n                target = target.cpu().data.numpy()\n                for i in range(cur_batch_size):\n                    cat = seg_label_to_cat[target[i, 0]]\n                    logits = cur_pred_val_logits[i, :, :]\n                    cur_pred_val[i, :] = np.argmax(logits[:, seg_classes[cat]], 1) + seg_classes[cat][0]\n                correct = np.sum(cur_pred_val == target)\n                total_correct += correct\n                total_seen += (cur_batch_size * NUM_POINT)\n\n                for l in range(num_part):\n                    total_seen_class[l] += np.sum(target == l)\n                    total_correct_class[l] += (np.sum((cur_pred_val == l) & (target == l)))\n\n                for i in range(cur_batch_size):\n                    segp = cur_pred_val[i, :]\n                    segl = target[i, :]\n                    cat = seg_label_to_cat[segl[0]]\n                    part_ious = [0.0 for _ in range(len(seg_classes[cat]))]\n                    for l in seg_classes[cat]:\n                        if (np.sum(segl == l) == 0) and (\n                                np.sum(segp == l) == 0):  # part is not present, no prediction as well\n                            part_ious[l - seg_classes[cat][0]] = 1.0\n                        else:\n                            part_ious[l - seg_classes[cat][0]] = np.sum((segl == l) & (segp == l)) / float(\n                                np.sum((segl == l) | (segp == l)))\n                    shape_ious[cat].append(np.mean(part_ious))\n\n            all_shape_ious = []\n            for cat in shape_ious.keys():\n                for iou in shape_ious[cat]:\n                    all_shape_ious.append(iou)\n                shape_ious[cat] = np.mean(shape_ious[cat])\n            mean_shape_ious = np.mean(list(shape_ious.values()))\n            test_metrics[\'accuracy\'] = total_correct / float(total_seen)\n            test_metrics[\'class_avg_accuracy\'] = np.mean(\n                np.array(total_correct_class) / np.array(total_seen_class, dtype=np.float))\n            for cat in sorted(shape_ious.keys()):\n                log_string(\'eval mIoU of %s %f\' % (cat + \' \' * (14 - len(cat)), shape_ious[cat]))\n            test_metrics[\'class_avg_iou\'] = mean_shape_ious\n            test_metrics[\'inctance_avg_iou\'] = np.mean(all_shape_ious)\n\n\n        log_string(\'Epoch %d test Accuracy: %f  Class avg mIOU: %f   Inctance avg mIOU: %f\' % (\n                 epoch+1, test_metrics[\'accuracy\'],test_metrics[\'class_avg_iou\'],test_metrics[\'inctance_avg_iou\']))\n        if (test_metrics[\'inctance_avg_iou\'] >= best_inctance_avg_iou):\n            logger.info(\'Save model...\')\n            savepath = str(checkpoints_dir) + \'/best_model.pth\'\n            log_string(\'Saving at %s\'% savepath)\n            state = {\n                \'epoch\': epoch,\n                \'train_acc\': train_instance_acc,\n                \'test_acc\': test_metrics[\'accuracy\'],\n                \'class_avg_iou\': test_metrics[\'class_avg_iou\'],\n                \'inctance_avg_iou\': test_metrics[\'inctance_avg_iou\'],\n                \'model_state_dict\': classifier.state_dict(),\n                \'optimizer_state_dict\': optimizer.state_dict(),\n            }\n            torch.save(state, savepath)\n            log_string(\'Saving model....\')\n\n        if test_metrics[\'accuracy\'] > best_acc:\n            best_acc = test_metrics[\'accuracy\']\n        if test_metrics[\'class_avg_iou\'] > best_class_avg_iou:\n            best_class_avg_iou = test_metrics[\'class_avg_iou\']\n        if test_metrics[\'inctance_avg_iou\'] > best_inctance_avg_iou:\n            best_inctance_avg_iou = test_metrics[\'inctance_avg_iou\']\n        log_string(\'Best accuracy is: %.5f\'%best_acc)\n        log_string(\'Best class avg mIOU is: %.5f\'%best_class_avg_iou)\n        log_string(\'Best inctance avg mIOU is: %.5f\'%best_inctance_avg_iou)\n        global_epoch+=1\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    main(args)\n\n'"
train_semseg.py,16,"b'""""""\nAuthor: Benny\nDate: Nov 2019\n""""""\nimport argparse\nimport os\nfrom data_utils.S3DISDataLoader import S3DISDataset\nimport torch\nimport datetime\nimport logging\nfrom pathlib import Path\nimport sys\nimport importlib\nimport shutil\nfrom tqdm import tqdm\nimport provider\nimport numpy as np\nimport time\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, \'models\'))\n\n\nclasses = [\'ceiling\',\'floor\',\'wall\',\'beam\',\'column\',\'window\',\'door\',\'table\',\'chair\',\'sofa\',\'bookcase\',\'board\',\'clutter\']\nclass2label = {cls: i for i,cls in enumerate(classes)}\nseg_classes = class2label\nseg_label_to_cat = {}\nfor i,cat in enumerate(seg_classes.keys()):\n    seg_label_to_cat[i] = cat\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\'Model\')\n    parser.add_argument(\'--model\', type=str, default=\'pointnet_sem_seg\', help=\'model name [default: pointnet_sem_seg]\')\n    parser.add_argument(\'--batch_size\', type=int, default=16, help=\'Batch Size during training [default: 16]\')\n    parser.add_argument(\'--epoch\',  default=128, type=int, help=\'Epoch to run [default: 128]\')\n    parser.add_argument(\'--learning_rate\', default=0.001, type=float, help=\'Initial learning rate [default: 0.001]\')\n    parser.add_argument(\'--gpu\', type=str, default=\'0\', help=\'GPU to use [default: GPU 0]\')\n    parser.add_argument(\'--optimizer\', type=str, default=\'Adam\', help=\'Adam or SGD [default: Adam]\')\n    parser.add_argument(\'--log_dir\', type=str, default=None, help=\'Log path [default: None]\')\n    parser.add_argument(\'--decay_rate\', type=float, default=1e-4, help=\'weight decay [default: 1e-4]\')\n    parser.add_argument(\'--npoint\', type=int,  default=4096, help=\'Point Number [default: 4096]\')\n    parser.add_argument(\'--step_size\', type=int,  default=10, help=\'Decay step for lr decay [default: every 10 epochs]\')\n    parser.add_argument(\'--lr_decay\', type=float,  default=0.7, help=\'Decay rate for lr decay [default: 0.7]\')\n    parser.add_argument(\'--test_area\', type=int, default=5, help=\'Which area to use for test, option: 1-6 [default: 5]\')\n\n    return parser.parse_args()\n\ndef main(args):\n    def log_string(str):\n        logger.info(str)\n        print(str)\n\n    \'\'\'HYPER PARAMETER\'\'\'\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n    \'\'\'CREATE DIR\'\'\'\n    timestr = str(datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M\'))\n    experiment_dir = Path(\'./log/\')\n    experiment_dir.mkdir(exist_ok=True)\n    experiment_dir = experiment_dir.joinpath(\'sem_seg\')\n    experiment_dir.mkdir(exist_ok=True)\n    if args.log_dir is None:\n        experiment_dir = experiment_dir.joinpath(timestr)\n    else:\n        experiment_dir = experiment_dir.joinpath(args.log_dir)\n    experiment_dir.mkdir(exist_ok=True)\n    checkpoints_dir = experiment_dir.joinpath(\'checkpoints/\')\n    checkpoints_dir.mkdir(exist_ok=True)\n    log_dir = experiment_dir.joinpath(\'logs/\')\n    log_dir.mkdir(exist_ok=True)\n\n    \'\'\'LOG\'\'\'\n    args = parse_args()\n    logger = logging.getLogger(""Model"")\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n    file_handler = logging.FileHandler(\'%s/%s.txt\' % (log_dir, args.model))\n    file_handler.setLevel(logging.INFO)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    log_string(\'PARAMETER ...\')\n    log_string(args)\n\n    root = \'data/stanford_indoor3d/\'\n    NUM_CLASSES = 13\n    NUM_POINT = args.npoint\n    BATCH_SIZE = args.batch_size\n\n    print(""start loading training data ..."")\n    TRAIN_DATASET = S3DISDataset(split=\'train\', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n    print(""start loading test data ..."")\n    TEST_DATASET = S3DISDataset(split=\'test\', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n    trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True, worker_init_fn = lambda x: np.random.seed(x+int(time.time())))\n    testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)\n    weights = torch.Tensor(TRAIN_DATASET.labelweights).cuda()\n\n    log_string(""The number of training data is: %d"" % len(TRAIN_DATASET))\n    log_string(""The number of test data is: %d"" % len(TEST_DATASET))\n\n    \'\'\'MODEL LOADING\'\'\'\n    MODEL = importlib.import_module(args.model)\n    shutil.copy(\'models/%s.py\' % args.model, str(experiment_dir))\n    shutil.copy(\'models/pointnet_util.py\', str(experiment_dir))\n\n    classifier = MODEL.get_model(NUM_CLASSES).cuda()\n    criterion = MODEL.get_loss().cuda()\n\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find(\'Conv2d\') != -1:\n            torch.nn.init.xavier_normal_(m.weight.data)\n            torch.nn.init.constant_(m.bias.data, 0.0)\n        elif classname.find(\'Linear\') != -1:\n            torch.nn.init.xavier_normal_(m.weight.data)\n            torch.nn.init.constant_(m.bias.data, 0.0)\n\n    try:\n        checkpoint = torch.load(str(experiment_dir) + \'/checkpoints/best_model.pth\')\n        start_epoch = checkpoint[\'epoch\']\n        classifier.load_state_dict(checkpoint[\'model_state_dict\'])\n        log_string(\'Use pretrain model\')\n    except:\n        log_string(\'No existing model, starting training from scratch...\')\n        start_epoch = 0\n        classifier = classifier.apply(weights_init)\n\n    if args.optimizer == \'Adam\':\n        optimizer = torch.optim.Adam(\n            classifier.parameters(),\n            lr=args.learning_rate,\n            betas=(0.9, 0.999),\n            eps=1e-08,\n            weight_decay=args.decay_rate\n        )\n    else:\n        optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n\n    def bn_momentum_adjust(m, momentum):\n        if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n            m.momentum = momentum\n\n    LEARNING_RATE_CLIP = 1e-5\n    MOMENTUM_ORIGINAL = 0.1\n    MOMENTUM_DECCAY = 0.5\n    MOMENTUM_DECCAY_STEP = args.step_size\n\n    global_epoch = 0\n    best_iou = 0\n\n    for epoch in range(start_epoch,args.epoch):\n        \'\'\'Train on chopped scenes\'\'\'\n        log_string(\'**** Epoch %d (%d/%s) ****\' % (global_epoch + 1, epoch + 1, args.epoch))\n        lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n        log_string(\'Learning rate:%f\' % lr)\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr\n        momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n        if momentum < 0.01:\n            momentum = 0.01\n        print(\'BN momentum updated to: %f\' % momentum)\n        classifier = classifier.apply(lambda x: bn_momentum_adjust(x,momentum))\n        num_batches = len(trainDataLoader)\n        total_correct = 0\n        total_seen = 0\n        loss_sum = 0\n        for i, data in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n            points, target = data\n            points = points.data.numpy()\n            points[:,:, :3] = provider.rotate_point_cloud_z(points[:,:, :3])\n            points = torch.Tensor(points)\n            points, target = points.float().cuda(),target.long().cuda()\n            points = points.transpose(2, 1)\n            optimizer.zero_grad()\n            classifier = classifier.train()\n            seg_pred, trans_feat = classifier(points)\n            seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n            batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n            target = target.view(-1, 1)[:, 0]\n            loss = criterion(seg_pred, target, trans_feat, weights)\n            loss.backward()\n            optimizer.step()\n            pred_choice = seg_pred.cpu().data.max(1)[1].numpy()\n            correct = np.sum(pred_choice == batch_label)\n            total_correct += correct\n            total_seen += (BATCH_SIZE * NUM_POINT)\n            loss_sum += loss\n        log_string(\'Training mean loss: %f\' % (loss_sum / num_batches))\n        log_string(\'Training accuracy: %f\' % (total_correct / float(total_seen)))\n\n        if epoch % 5 == 0:\n            logger.info(\'Save model...\')\n            savepath = str(checkpoints_dir) + \'/model.pth\'\n            log_string(\'Saving at %s\' % savepath)\n            state = {\n                \'epoch\': epoch,\n                \'model_state_dict\': classifier.state_dict(),\n                \'optimizer_state_dict\': optimizer.state_dict(),\n            }\n            torch.save(state, savepath)\n            log_string(\'Saving model....\')\n\n        \'\'\'Evaluate on chopped scenes\'\'\'\n        with torch.no_grad():\n            num_batches = len(testDataLoader)\n            total_correct = 0\n            total_seen = 0\n            loss_sum = 0\n            labelweights = np.zeros(NUM_CLASSES)\n            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n            total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n            log_string(\'---- EPOCH %03d EVALUATION ----\' % (global_epoch + 1))\n            for i, data in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n                points, target = data\n                points = points.data.numpy()\n                points = torch.Tensor(points)\n                points, target = points.float().cuda(), target.long().cuda()\n                points = points.transpose(2, 1)\n                classifier = classifier.eval()\n                seg_pred, trans_feat = classifier(points)\n                pred_val = seg_pred.contiguous().cpu().data.numpy()\n                seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n                batch_label = target.cpu().data.numpy()\n                target = target.view(-1, 1)[:, 0]\n                loss = criterion(seg_pred, target, trans_feat, weights)\n                loss_sum += loss\n                pred_val = np.argmax(pred_val, 2)\n                correct = np.sum((pred_val == batch_label))\n                total_correct += correct\n                total_seen += (BATCH_SIZE * NUM_POINT)\n                tmp, _ = np.histogram(batch_label, range(NUM_CLASSES + 1))\n                labelweights += tmp\n                for l in range(NUM_CLASSES):\n                    total_seen_class[l] += np.sum((batch_label == l) )\n                    total_correct_class[l] += np.sum((pred_val == l) & (batch_label == l) )\n                    total_iou_deno_class[l] += np.sum(((pred_val == l) | (batch_label == l)) )\n            labelweights = labelweights.astype(np.float32) / np.sum(labelweights.astype(np.float32))\n            mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=np.float) + 1e-6))\n            log_string(\'eval mean loss: %f\' % (loss_sum / float(num_batches)))\n            log_string(\'eval point avg class IoU: %f\' % (mIoU))\n            log_string(\'eval point accuracy: %f\' % (total_correct / float(total_seen)))\n            log_string(\'eval point avg class acc: %f\' % (\n                np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=np.float) + 1e-6))))\n            iou_per_class_str = \'------- IoU --------\\n\'\n            for l in range(NUM_CLASSES):\n                iou_per_class_str += \'class %s weight: %.3f, IoU: %.3f \\n\' % (\n                    seg_label_to_cat[l] + \' \' * (14 - len(seg_label_to_cat[l])), labelweights[l - 1],\n                    total_correct_class[l] / float(total_iou_deno_class[l]))\n\n            log_string(iou_per_class_str)\n            log_string(\'Eval mean loss: %f\' % (loss_sum / num_batches))\n            log_string(\'Eval accuracy: %f\' % (total_correct / float(total_seen)))\n            if mIoU >= best_iou:\n                best_iou = mIoU\n                logger.info(\'Save model...\')\n                savepath = str(checkpoints_dir) + \'/best_model.pth\'\n                log_string(\'Saving at %s\' % savepath)\n                state = {\n                    \'epoch\': epoch,\n                    \'class_avg_iou\': mIoU,\n                    \'model_state_dict\': classifier.state_dict(),\n                    \'optimizer_state_dict\': optimizer.state_dict(),\n                }\n                torch.save(state, savepath)\n                log_string(\'Saving model....\')\n            log_string(\'Best mIoU: %f\' % best_iou)\n        global_epoch += 1\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    main(args)\n\n'"
data_utils/ModelNetDataLoader.py,2,"b'import numpy as np\nimport warnings\nimport os\nfrom torch.utils.data import Dataset\nwarnings.filterwarnings(\'ignore\')\n\n\n\ndef pc_normalize(pc):\n    centroid = np.mean(pc, axis=0)\n    pc = pc - centroid\n    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n    pc = pc / m\n    return pc\n\ndef farthest_point_sample(point, npoint):\n    """"""\n    Input:\n        xyz: pointcloud data, [N, D]\n        npoint: number of samples\n    Return:\n        centroids: sampled pointcloud index, [npoint, D]\n    """"""\n    N, D = point.shape\n    xyz = point[:,:3]\n    centroids = np.zeros((npoint,))\n    distance = np.ones((N,)) * 1e10\n    farthest = np.random.randint(0, N)\n    for i in range(npoint):\n        centroids[i] = farthest\n        centroid = xyz[farthest, :]\n        dist = np.sum((xyz - centroid) ** 2, -1)\n        mask = dist < distance\n        distance[mask] = dist[mask]\n        farthest = np.argmax(distance, -1)\n    point = point[centroids.astype(np.int32)]\n    return point\n\nclass ModelNetDataLoader(Dataset):\n    def __init__(self, root,  npoint=1024, split=\'train\', uniform=False, normal_channel=True, cache_size=15000):\n        self.root = root\n        self.npoints = npoint\n        self.uniform = uniform\n        self.catfile = os.path.join(self.root, \'modelnet40_shape_names.txt\')\n\n        self.cat = [line.rstrip() for line in open(self.catfile)]\n        self.classes = dict(zip(self.cat, range(len(self.cat))))\n        self.normal_channel = normal_channel\n\n        shape_ids = {}\n        shape_ids[\'train\'] = [line.rstrip() for line in open(os.path.join(self.root, \'modelnet40_train.txt\'))]\n        shape_ids[\'test\'] = [line.rstrip() for line in open(os.path.join(self.root, \'modelnet40_test.txt\'))]\n\n        assert (split == \'train\' or split == \'test\')\n        shape_names = [\'_\'.join(x.split(\'_\')[0:-1]) for x in shape_ids[split]]\n        # list of (shape_name, shape_txt_file_path) tuple\n        self.datapath = [(shape_names[i], os.path.join(self.root, shape_names[i], shape_ids[split][i]) + \'.txt\') for i\n                         in range(len(shape_ids[split]))]\n        print(\'The size of %s data is %d\'%(split,len(self.datapath)))\n\n        self.cache_size = cache_size  # how many data points to cache in memory\n        self.cache = {}  # from index to (point_set, cls) tuple\n\n    def __len__(self):\n        return len(self.datapath)\n\n    def _get_item(self, index):\n        if index in self.cache:\n            point_set, cls = self.cache[index]\n        else:\n            fn = self.datapath[index]\n            cls = self.classes[self.datapath[index][0]]\n            cls = np.array([cls]).astype(np.int32)\n            point_set = np.loadtxt(fn[1], delimiter=\',\').astype(np.float32)\n            if self.uniform:\n                point_set = farthest_point_sample(point_set, self.npoints)\n            else:\n                point_set = point_set[0:self.npoints,:]\n\n            point_set[:, 0:3] = pc_normalize(point_set[:, 0:3])\n\n            if not self.normal_channel:\n                point_set = point_set[:, 0:3]\n\n            if len(self.cache) < self.cache_size:\n                self.cache[index] = (point_set, cls)\n\n        return point_set, cls\n\n    def __getitem__(self, index):\n        return self._get_item(index)\n\n\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    data = ModelNetDataLoader(\'/data/modelnet40_normal_resampled/\',split=\'train\', uniform=False, normal_channel=True,)\n    DataLoader = torch.utils.data.DataLoader(data, batch_size=12, shuffle=True)\n    for point,label in DataLoader:\n        print(point.shape)\n        print(label.shape)'"
data_utils/S3DISDataLoader.py,4,"b'import os\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\nclass S3DISDataset(Dataset):\n    def __init__(self, split=\'train\', data_root=\'trainval_fullarea\', num_point=4096, test_area=5, block_size=1.0, sample_rate=1.0, transform=None):\n        super().__init__()\n        self.num_point = num_point\n        self.block_size = block_size\n        self.transform = transform\n        rooms = sorted(os.listdir(data_root))\n        rooms = [room for room in rooms if \'Area_\' in room]\n        if split == \'train\':\n            rooms_split = [room for room in rooms if not \'Area_{}\'.format(test_area) in room]\n        else:\n            rooms_split = [room for room in rooms if \'Area_{}\'.format(test_area) in room]\n        self.room_points, self.room_labels = [], []\n        self.room_coord_min, self.room_coord_max = [], []\n        num_point_all = []\n        labelweights = np.zeros(13)\n        for room_name in rooms_split:\n            room_path = os.path.join(data_root, room_name)\n            room_data = np.load(room_path)  # xyzrgbl, N*7\n            points, labels = room_data[:, 0:6], room_data[:, 6]  # xyzrgb, N*6; l, N\n            tmp, _ = np.histogram(labels, range(14))\n            labelweights += tmp\n            coord_min, coord_max = np.amin(points, axis=0)[:3], np.amax(points, axis=0)[:3]\n            self.room_points.append(points), self.room_labels.append(labels)\n            self.room_coord_min.append(coord_min), self.room_coord_max.append(coord_max)\n            num_point_all.append(labels.size)\n        labelweights = labelweights.astype(np.float32)\n        labelweights = labelweights / np.sum(labelweights)\n        self.labelweights = np.power(np.amax(labelweights) / labelweights, 1 / 3.0)\n        print(self.labelweights)\n        sample_prob = num_point_all / np.sum(num_point_all)\n        num_iter = int(np.sum(num_point_all) * sample_rate / num_point)\n        room_idxs = []\n        for index in range(len(rooms_split)):\n            room_idxs.extend([index] * int(round(sample_prob[index] * num_iter)))\n        self.room_idxs = np.array(room_idxs)\n        print(""Totally {} samples in {} set."".format(len(self.room_idxs), split))\n\n    def __getitem__(self, idx):\n        room_idx = self.room_idxs[idx]\n        points = self.room_points[room_idx]   # N * 6\n        labels = self.room_labels[room_idx]   # N\n        N_points = points.shape[0]\n\n        while (True):\n            center = points[np.random.choice(N_points)][:3]\n            block_min = center - [self.block_size / 2.0, self.block_size / 2.0, 0]\n            block_max = center + [self.block_size / 2.0, self.block_size / 2.0, 0]\n            point_idxs = np.where((points[:, 0] >= block_min[0]) & (points[:, 0] <= block_max[0]) & (points[:, 1] >= block_min[1]) & (points[:, 1] <= block_max[1]))[0]\n            if point_idxs.size > 1024:\n                break\n\n        if point_idxs.size >= self.num_point:\n            selected_point_idxs = np.random.choice(point_idxs, self.num_point, replace=False)\n        else:\n            selected_point_idxs = np.random.choice(point_idxs, self.num_point, replace=True)\n\n        # normalize\n        selected_points = points[selected_point_idxs, :]  # num_point * 6\n        current_points = np.zeros((self.num_point, 9))  # num_point * 9\n        current_points[:, 6] = selected_points[:, 0] / self.room_coord_max[room_idx][0]\n        current_points[:, 7] = selected_points[:, 1] / self.room_coord_max[room_idx][1]\n        current_points[:, 8] = selected_points[:, 2] / self.room_coord_max[room_idx][2]\n        selected_points[:, 0] = selected_points[:, 0] - center[0]\n        selected_points[:, 1] = selected_points[:, 1] - center[1]\n        selected_points[:, 3:6] /= 255.0\n        current_points[:, 0:6] = selected_points\n        current_labels = labels[selected_point_idxs]\n        if self.transform is not None:\n            current_points, current_labels = self.transform(current_points, current_labels)\n        return current_points, current_labels\n\n    def __len__(self):\n        return len(self.room_idxs)\n\nclass ScannetDatasetWholeScene():\n    # prepare to give prediction on each points\n    def __init__(self, root, block_points=4096, split=\'test\', test_area=5, stride=0.5, block_size=1.0, padding=0.001):\n        self.block_points = block_points\n        self.block_size = block_size\n        self.padding = padding\n        self.root = root\n        self.split = split\n        self.stride = stride\n        self.scene_points_num = []\n        assert split in [\'train\', \'test\']\n        if self.split == \'train\':\n            self.file_list = [d for d in os.listdir(root) if d.find(\'Area_%d\' % test_area) is -1]\n        else:\n            self.file_list = [d for d in os.listdir(root) if d.find(\'Area_%d\' % test_area) is not -1]\n        self.scene_points_list = []\n        self.semantic_labels_list = []\n        self.room_coord_min, self.room_coord_max = [], []\n        for file in self.file_list:\n            data = np.load(root + file)\n            points = data[:, :3]\n            self.scene_points_list.append(data[:, :6])\n            self.semantic_labels_list.append(data[:, 6])\n            coord_min, coord_max = np.amin(points, axis=0)[:3], np.amax(points, axis=0)[:3]\n            self.room_coord_min.append(coord_min), self.room_coord_max.append(coord_max)\n        assert len(self.scene_points_list) == len(self.semantic_labels_list)\n\n        labelweights = np.zeros(13)\n        for seg in self.semantic_labels_list:\n            tmp, _ = np.histogram(seg, range(14))\n            self.scene_points_num.append(seg.shape[0])\n            labelweights += tmp\n        labelweights = labelweights.astype(np.float32)\n        labelweights = labelweights / np.sum(labelweights)\n        self.labelweights = np.power(np.amax(labelweights) / labelweights, 1 / 3.0)\n\n    def __getitem__(self, index):\n        point_set_ini = self.scene_points_list[index]\n        points = point_set_ini[:,:6]\n        labels = self.semantic_labels_list[index]\n        coord_min, coord_max = np.amin(points, axis=0)[:3], np.amax(points, axis=0)[:3]\n        grid_x = int(np.ceil(float(coord_max[0] - coord_min[0] - self.block_size) / self.stride) + 1)\n        grid_y = int(np.ceil(float(coord_max[1] - coord_min[1] - self.block_size) / self.stride) + 1)\n        data_room, label_room, sample_weight, index_room = np.array([]), np.array([]), np.array([]),  np.array([])\n        for index_y in range(0, grid_y):\n            for index_x in range(0, grid_x):\n                s_x = coord_min[0] + index_x * self.stride\n                e_x = min(s_x + self.block_size, coord_max[0])\n                s_x = e_x - self.block_size\n                s_y = coord_min[1] + index_y * self.stride\n                e_y = min(s_y + self.block_size, coord_max[1])\n                s_y = e_y - self.block_size\n                point_idxs = np.where(\n                    (points[:, 0] >= s_x - self.padding) & (points[:, 0] <= e_x + self.padding) & (points[:, 1] >= s_y - self.padding) & (\n                                points[:, 1] <= e_y + self.padding))[0]\n                if point_idxs.size == 0:\n                    continue\n                num_batch = int(np.ceil(point_idxs.size / self.block_points))\n                point_size = int(num_batch * self.block_points)\n                replace = False if (point_size - point_idxs.size <= point_idxs.size) else True\n                point_idxs_repeat = np.random.choice(point_idxs, point_size - point_idxs.size, replace=replace)\n                point_idxs = np.concatenate((point_idxs, point_idxs_repeat))\n                np.random.shuffle(point_idxs)\n                data_batch = points[point_idxs, :]\n                normlized_xyz = np.zeros((point_size, 3))\n                normlized_xyz[:, 0] = data_batch[:, 0] / coord_max[0]\n                normlized_xyz[:, 1] = data_batch[:, 1] / coord_max[1]\n                normlized_xyz[:, 2] = data_batch[:, 2] / coord_max[2]\n                data_batch[:, 0] = data_batch[:, 0] - (s_x + self.block_size / 2.0)\n                data_batch[:, 1] = data_batch[:, 1] - (s_y + self.block_size / 2.0)\n                data_batch[:, 3:6] /= 255.0\n                data_batch = np.concatenate((data_batch, normlized_xyz), axis=1)\n                label_batch = labels[point_idxs].astype(int)\n                batch_weight = self.labelweights[label_batch]\n\n                data_room = np.vstack([data_room, data_batch]) if data_room.size else data_batch\n                label_room = np.hstack([label_room, label_batch]) if label_room.size else label_batch\n                sample_weight = np.hstack([sample_weight, batch_weight]) if label_room.size else batch_weight\n                index_room = np.hstack([index_room, point_idxs]) if index_room.size else point_idxs\n        data_room = data_room.reshape((-1, self.block_points, data_room.shape[1]))\n        label_room = label_room.reshape((-1, self.block_points))\n        sample_weight = sample_weight.reshape((-1, self.block_points))\n        index_room = index_room.reshape((-1, self.block_points))\n        return data_room, label_room, sample_weight, index_room\n\n    def __len__(self):\n        return len(self.scene_points_list)\n\nif __name__ == \'__main__\':\n    data_root = \'/data/yxu/PointNonLocal/data/stanford_indoor3d/\'\n    num_point, test_area, block_size, sample_rate = 4096, 5, 1.0, 0.01\n\n    point_data = S3DISDataset(split=\'train\', data_root=data_root, num_point=num_point, test_area=test_area, block_size=block_size, sample_rate=sample_rate, transform=None)\n    print(\'point data size:\', point_data.__len__())\n    print(\'point data 0 shape:\', point_data.__getitem__(0)[0].shape)\n    print(\'point label 0 shape:\', point_data.__getitem__(0)[1].shape)\n    import torch, time, random\n    manual_seed = 123\n    random.seed(manual_seed)\n    np.random.seed(manual_seed)\n    torch.manual_seed(manual_seed)\n    torch.cuda.manual_seed_all(manual_seed)\n    def worker_init_fn(worker_id):\n        random.seed(manual_seed + worker_id)\n    train_loader = torch.utils.data.DataLoader(point_data, batch_size=16, shuffle=True, num_workers=16, pin_memory=True, worker_init_fn=worker_init_fn)\n    for idx in range(4):\n        end = time.time()\n        for i, (input, target) in enumerate(train_loader):\n            print(\'time: {}/{}--{}\'.format(i+1, len(train_loader), time.time() - end))\n            end = time.time()'"
data_utils/ShapeNetDataLoader.py,1,"b""# *_*coding:utf-8 *_*\nimport os\nimport json\nimport warnings\nimport numpy as np\nfrom torch.utils.data import Dataset\nwarnings.filterwarnings('ignore')\n\ndef pc_normalize(pc):\n    centroid = np.mean(pc, axis=0)\n    pc = pc - centroid\n    m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n    pc = pc / m\n    return pc\n\nclass PartNormalDataset(Dataset):\n    def __init__(self,root = './data/shapenetcore_partanno_segmentation_benchmark_v0_normal', npoints=2500, split='train', class_choice=None, normal_channel=False):\n        self.npoints = npoints\n        self.root = root\n        self.catfile = os.path.join(self.root, 'synsetoffset2category.txt')\n        self.cat = {}\n        self.normal_channel = normal_channel\n\n\n        with open(self.catfile, 'r') as f:\n            for line in f:\n                ls = line.strip().split()\n                self.cat[ls[0]] = ls[1]\n        self.cat = {k: v for k, v in self.cat.items()}\n        self.classes_original = dict(zip(self.cat, range(len(self.cat))))\n\n        if not class_choice is  None:\n            self.cat = {k:v for k,v in self.cat.items() if k in class_choice}\n        # print(self.cat)\n\n        self.meta = {}\n        with open(os.path.join(self.root, 'train_test_split', 'shuffled_train_file_list.json'), 'r') as f:\n            train_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n        with open(os.path.join(self.root, 'train_test_split', 'shuffled_val_file_list.json'), 'r') as f:\n            val_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n        with open(os.path.join(self.root, 'train_test_split', 'shuffled_test_file_list.json'), 'r') as f:\n            test_ids = set([str(d.split('/')[2]) for d in json.load(f)])\n        for item in self.cat:\n            # print('category', item)\n            self.meta[item] = []\n            dir_point = os.path.join(self.root, self.cat[item])\n            fns = sorted(os.listdir(dir_point))\n            # print(fns[0][0:-4])\n            if split == 'trainval':\n                fns = [fn for fn in fns if ((fn[0:-4] in train_ids) or (fn[0:-4] in val_ids))]\n            elif split == 'train':\n                fns = [fn for fn in fns if fn[0:-4] in train_ids]\n            elif split == 'val':\n                fns = [fn for fn in fns if fn[0:-4] in val_ids]\n            elif split == 'test':\n                fns = [fn for fn in fns if fn[0:-4] in test_ids]\n            else:\n                print('Unknown split: %s. Exiting..' % (split))\n                exit(-1)\n\n            # print(os.path.basename(fns))\n            for fn in fns:\n                token = (os.path.splitext(os.path.basename(fn))[0])\n                self.meta[item].append(os.path.join(dir_point, token + '.txt'))\n\n        self.datapath = []\n        for item in self.cat:\n            for fn in self.meta[item]:\n                self.datapath.append((item, fn))\n\n        self.classes = {}\n        for i in self.cat.keys():\n            self.classes[i] = self.classes_original[i]\n\n        # Mapping from category ('Chair') to a list of int [10,11,12,13] as segmentation labels\n        self.seg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43],\n                            'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46],\n                            'Mug': [36, 37], 'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27],\n                            'Table': [47, 48, 49], 'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40],\n                            'Chair': [12, 13, 14, 15], 'Knife': [22, 23]}\n\n        # for cat in sorted(self.seg_classes.keys()):\n        #     print(cat, self.seg_classes[cat])\n\n        self.cache = {}  # from index to (point_set, cls, seg) tuple\n        self.cache_size = 20000\n\n\n    def __getitem__(self, index):\n        if index in self.cache:\n            ppoint_set, cls, seg = self.cache[index]\n        else:\n            fn = self.datapath[index]\n            cat = self.datapath[index][0]\n            cls = self.classes[cat]\n            cls = np.array([cls]).astype(np.int32)\n            data = np.loadtxt(fn[1]).astype(np.float32)\n            if not self.normal_channel:\n                point_set = data[:, 0:3]\n            else:\n                point_set = data[:, 0:6]\n            seg = data[:, -1].astype(np.int32)\n            if len(self.cache) < self.cache_size:\n                self.cache[index] = (point_set, cls, seg)\n        point_set[:, 0:3] = pc_normalize(point_set[:, 0:3])\n\n        choice = np.random.choice(len(seg), self.npoints, replace=True)\n        # resample\n        point_set = point_set[choice, :]\n        seg = seg[choice]\n\n        return point_set, cls, seg\n\n    def __len__(self):\n        return len(self.datapath)\n\n\n\n"""
data_utils/collect_indoor3d_data.py,0,"b""import os\nimport sys\nfrom indoor3d_util import DATA_PATH, collect_point_label\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = os.path.dirname(BASE_DIR)\nsys.path.append(BASE_DIR)\n\nanno_paths = [line.rstrip() for line in open(os.path.join(BASE_DIR, 'meta/anno_paths.txt'))]\nanno_paths = [os.path.join(DATA_PATH, p) for p in anno_paths]\n\noutput_folder = os.path.join(ROOT_DIR, 'data/stanford_indoor3d')\nif not os.path.exists(output_folder):\n    os.mkdir(output_folder)\n\n# Note: there is an extra character in the v1.2 data in Area_5/hallway_6. It's fixed manually.\nfor anno_path in anno_paths:\n    print(anno_path)\n    try:\n        elements = anno_path.split('/')\n        out_filename = elements[-3]+'_'+elements[-2]+'.npy' # Area_1_hallway_1.npy\n        collect_point_label(anno_path, os.path.join(output_folder, out_filename), 'numpy')\n    except:\n        print(anno_path, 'ERROR!!')\n"""
data_utils/indoor3d_util.py,0,"b'import numpy as np\nimport glob\nimport os\nimport sys\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = os.path.dirname(BASE_DIR)\nsys.path.append(BASE_DIR)\n\nDATA_PATH = os.path.join(ROOT_DIR, \'data\',\'s3dis\', \'Stanford3dDataset_v1.2_Aligned_Version\')\ng_classes = [x.rstrip() for x in open(os.path.join(BASE_DIR, \'meta/class_names.txt\'))]\ng_class2label = {cls: i for i,cls in enumerate(g_classes)}\ng_class2color = {\'ceiling\':\t[0,255,0],\n                 \'floor\':\t[0,0,255],\n                 \'wall\':\t[0,255,255],\n                 \'beam\':        [255,255,0],\n                 \'column\':      [255,0,255],\n                 \'window\':      [100,100,255],\n                 \'door\':        [200,200,100],\n                 \'table\':       [170,120,200],\n                 \'chair\':       [255,0,0],\n                 \'sofa\':        [200,100,100],\n                 \'bookcase\':    [10,200,100],\n                 \'board\':       [200,200,200],\n                 \'clutter\':     [50,50,50]} \ng_easy_view_labels = [7,8,9,10,11,1]\ng_label2color = {g_classes.index(cls): g_class2color[cls] for cls in g_classes}\n\n\n# -----------------------------------------------------------------------------\n# CONVERT ORIGINAL DATA TO OUR DATA_LABEL FILES\n# -----------------------------------------------------------------------------\n\ndef collect_point_label(anno_path, out_filename, file_format=\'txt\'):\n    """""" Convert original dataset files to data_label file (each line is XYZRGBL).\n        We aggregated all the points from each instance in the room.\n\n    Args:\n        anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n        out_filename: path to save collected points and labels (each line is XYZRGBL)\n        file_format: txt or numpy, determines what file format to save.\n    Returns:\n        None\n    Note:\n        the points are shifted before save, the most negative point is now at origin.\n    """"""\n    points_list = []\n    for f in glob.glob(os.path.join(anno_path, \'*.txt\')):\n        cls = os.path.basename(f).split(\'_\')[0]\n        print(f)\n        if cls not in g_classes: # note: in some room there is \'staris\' class..\n            cls = \'clutter\'\n\n        points = np.loadtxt(f)\n        labels = np.ones((points.shape[0],1)) * g_class2label[cls]\n        points_list.append(np.concatenate([points, labels], 1)) # Nx7\n    \n    data_label = np.concatenate(points_list, 0)\n    xyz_min = np.amin(data_label, axis=0)[0:3]\n    data_label[:, 0:3] -= xyz_min\n    \n    if file_format==\'txt\':\n        fout = open(out_filename, \'w\')\n        for i in range(data_label.shape[0]):\n            fout.write(\'%f %f %f %d %d %d %d\\n\' % \\\n                          (data_label[i,0], data_label[i,1], data_label[i,2],\n                           data_label[i,3], data_label[i,4], data_label[i,5],\n                           data_label[i,6]))\n        fout.close()\n    elif file_format==\'numpy\':\n        np.save(out_filename, data_label)\n    else:\n        print(\'ERROR!! Unknown file format: %s, please use txt or numpy.\' % \\\n            (file_format))\n        exit()\n\ndef data_to_obj(data,name=\'example.obj\',no_wall=True):\n    fout = open(name, \'w\')\n    label = data[:, -1].astype(int)\n    for i in range(data.shape[0]):\n        if no_wall and ((label[i] == 2) or (label[i]==0)):\n            continue\n        fout.write(\'v %f %f %f %d %d %d\\n\' % \\\n                   (data[i, 0], data[i, 1], data[i, 2], data[i, 3], data[i, 4], data[i, 5]))\n    fout.close()\n\ndef point_label_to_obj(input_filename, out_filename, label_color=True, easy_view=False, no_wall=False):\n    """""" For visualization of a room from data_label file,\n\tinput_filename: each line is X Y Z R G B L\n\tout_filename: OBJ filename,\n            visualize input file by coloring point with label color\n        easy_view: only visualize furnitures and floor\n    """"""\n    data_label = np.loadtxt(input_filename)\n    data = data_label[:, 0:6]\n    label = data_label[:, -1].astype(int)\n    fout = open(out_filename, \'w\')\n    for i in range(data.shape[0]):\n        color = g_label2color[label[i]]\n        if easy_view and (label[i] not in g_easy_view_labels):\n            continue\n        if no_wall and ((label[i] == 2) or (label[i]==0)):\n            continue\n        if label_color:\n            fout.write(\'v %f %f %f %d %d %d\\n\' % \\\n                (data[i,0], data[i,1], data[i,2], color[0], color[1], color[2]))\n        else:\n            fout.write(\'v %f %f %f %d %d %d\\n\' % \\\n                (data[i,0], data[i,1], data[i,2], data[i,3], data[i,4], data[i,5]))\n    fout.close()\n \n\n\n# -----------------------------------------------------------------------------\n# PREPARE BLOCK DATA FOR DEEPNETS TRAINING/TESTING\n# -----------------------------------------------------------------------------\n\ndef sample_data(data, num_sample):\n    """""" data is in N x ...\n        we want to keep num_samplexC of them.\n        if N > num_sample, we will randomly keep num_sample of them.\n        if N < num_sample, we will randomly duplicate samples.\n    """"""\n    N = data.shape[0]\n    if (N == num_sample):\n        return data, range(N)\n    elif (N > num_sample):\n        sample = np.random.choice(N, num_sample)\n        return data[sample, ...], sample\n    else:\n        sample = np.random.choice(N, num_sample-N)\n        dup_data = data[sample, ...]\n        return np.concatenate([data, dup_data], 0), list(range(N))+list(sample)\n\ndef sample_data_label(data, label, num_sample):\n    new_data, sample_indices = sample_data(data, num_sample)\n    new_label = label[sample_indices]\n    return new_data, new_label\n    \ndef room2blocks(data, label, num_point, block_size=1.0, stride=1.0,\n                random_sample=False, sample_num=None, sample_aug=1):\n    """""" Prepare block training data.\n    Args:\n        data: N x 6 numpy array, 012 are XYZ in meters, 345 are RGB in [0,1]\n            assumes the data is shifted (min point is origin) and aligned\n            (aligned with XYZ axis)\n        label: N size uint8 numpy array from 0-12\n        num_point: int, how many points to sample in each block\n        block_size: float, physical size of the block in meters\n        stride: float, stride for block sweeping\n        random_sample: bool, if True, we will randomly sample blocks in the room\n        sample_num: int, if random sample, how many blocks to sample\n            [default: room area]\n        sample_aug: if random sample, how much aug\n    Returns:\n        block_datas: K x num_point x 6 np array of XYZRGB, RGB is in [0,1]\n        block_labels: K x num_point x 1 np array of uint8 labels\n        \n    TODO: for this version, blocking is in fixed, non-overlapping pattern.\n    """"""\n    assert(stride<=block_size)\n\n    limit = np.amax(data, 0)[0:3]\n     \n    # Get the corner location for our sampling blocks    \n    xbeg_list = []\n    ybeg_list = []\n    if not random_sample:\n        num_block_x = int(np.ceil((limit[0] - block_size) / stride)) + 1\n        num_block_y = int(np.ceil(collect_point_label(limit[1] - block_size) / stride)) + 1\n        for i in range(num_block_x):\n            for j in range(num_block_y):\n                xbeg_list.append(i*stride)\n                ybeg_list.append(j*stride)\n    else:\n        num_block_x = int(np.ceil(limit[0] / block_size))\n        num_block_y = int(np.ceil(limit[1] / block_size))\n        if sample_num is None:\n            sample_num = num_block_x * num_block_y * sample_aug\n        for _ in range(sample_num):\n            xbeg = np.random.uniform(-block_size, limit[0]) \n            ybeg = np.random.uniform(-block_size, limit[1]) \n            xbeg_list.append(xbeg)\n            ybeg_list.append(ybeg)\n\n    # Collect blocks\n    block_data_list = []\n    block_label_list = []\n    idx = 0\n    for idx in range(len(xbeg_list)): \n       xbeg = xbeg_list[idx]\n       ybeg = ybeg_list[idx]\n       xcond = (data[:,0]<=xbeg+block_size) & (data[:,0]>=xbeg)\n       ycond = (data[:,1]<=ybeg+block_size) & (data[:,1]>=ybeg)\n       cond = xcond & ycond\n       if np.sum(cond) < 100: # discard block if there are less than 100 pts.\n           continue\n       \n       block_data = data[cond, :]\n       block_label = label[cond]\n       \n       # randomly subsample data\n       block_data_sampled, block_label_sampled = \\\n           sample_data_label(block_data, block_label, num_point)\n       block_data_list.append(np.expand_dims(block_data_sampled, 0))\n       block_label_list.append(np.expand_dims(block_label_sampled, 0))\n            \n    return np.concatenate(block_data_list, 0), \\\n           np.concatenate(block_label_list, 0)\n\n\ndef room2blocks_plus(data_label, num_point, block_size, stride,\n                     random_sample, sample_num, sample_aug):\n    """""" room2block with input filename and RGB preprocessing.\n    """"""\n    data = data_label[:,0:6]\n    data[:,3:6] /= 255.0\n    label = data_label[:,-1].astype(np.uint8)\n    \n    return room2blocks(data, label, num_point, block_size, stride,\n                       random_sample, sample_num, sample_aug)\n   \ndef room2blocks_wrapper(data_label_filename, num_point, block_size=1.0, stride=1.0,\n                        random_sample=False, sample_num=None, sample_aug=1):\n    if data_label_filename[-3:] == \'txt\':\n        data_label = np.loadtxt(data_label_filename)\n    elif data_label_filename[-3:] == \'npy\':\n        data_label = np.load(data_label_filename)\n    else:\n        print(\'Unknown file type! exiting.\')\n        exit()\n    return room2blocks_plus(data_label, num_point, block_size, stride,\n                            random_sample, sample_num, sample_aug)\n\ndef room2blocks_plus_normalized(data_label, num_point, block_size, stride,\n                                random_sample, sample_num, sample_aug):\n    """""" room2block, with input filename and RGB preprocessing.\n        for each block centralize XYZ, add normalized XYZ as 678 channels\n    """"""\n    data = data_label[:,0:6]\n    data[:,3:6] /= 255.0\n    label = data_label[:,-1].astype(np.uint8)\n    max_room_x = max(data[:,0])\n    max_room_y = max(data[:,1])\n    max_room_z = max(data[:,2])\n    \n    data_batch, label_batch = room2blocks(data, label, num_point, block_size, stride,\n                                          random_sample, sample_num, sample_aug)\n    new_data_batch = np.zeros((data_batch.shape[0], num_point, 9))\n    for b in range(data_batch.shape[0]):\n        new_data_batch[b, :, 6] = data_batch[b, :, 0]/max_room_x\n        new_data_batch[b, :, 7] = data_batch[b, :, 1]/max_room_y\n        new_data_batch[b, :, 8] = data_batch[b, :, 2]/max_room_z\n        minx = min(data_batch[b, :, 0])\n        miny = min(data_batch[b, :, 1])\n        data_batch[b, :, 0] -= (minx+block_size/2)\n        data_batch[b, :, 1] -= (miny+block_size/2)\n    new_data_batch[:, :, 0:6] = data_batch\n    return new_data_batch, label_batch\n\n\ndef room2blocks_wrapper_normalized(data_label_filename, num_point, block_size=1.0, stride=1.0,\n                                   random_sample=False, sample_num=None, sample_aug=1):\n    if data_label_filename[-3:] == \'txt\':\n        data_label = np.loadtxt(data_label_filename)\n    elif data_label_filename[-3:] == \'npy\':\n        data_label = np.load(data_label_filename)\n    else:\n        print(\'Unknown file type! exiting.\')\n        exit()\n    return room2blocks_plus_normalized(data_label, num_point, block_size, stride,\n                                       random_sample, sample_num, sample_aug)\n\ndef room2samples(data, label, sample_num_point):\n    """""" Prepare whole room samples.\n\n    Args:\n        data: N x 6 numpy array, 012 are XYZ in meters, 345 are RGB in [0,1]\n            assumes the data is shifted (min point is origin) and\n            aligned (aligned with XYZ axis)\n        label: N size uint8 numpy array from 0-12\n        sample_num_point: int, how many points to sample in each sample\n    Returns:\n        sample_datas: K x sample_num_point x 9\n                     numpy array of XYZRGBX\'Y\'Z\', RGB is in [0,1]\n        sample_labels: K x sample_num_point x 1 np array of uint8 labels\n    """"""\n    N = data.shape[0]\n    order = np.arange(N)\n    np.random.shuffle(order) \n    data = data[order, :]\n    label = label[order]\n\n    batch_num = int(np.ceil(N / float(sample_num_point)))\n    sample_datas = np.zeros((batch_num, sample_num_point, 6))\n    sample_labels = np.zeros((batch_num, sample_num_point, 1))\n\n    for i in range(batch_num):\n        beg_idx = i*sample_num_point\n        end_idx = min((i+1)*sample_num_point, N)\n        num = end_idx - beg_idx\n        sample_datas[i,0:num,:] = data[beg_idx:end_idx, :]\n        sample_labels[i,0:num,0] = label[beg_idx:end_idx]\n        if num < sample_num_point:\n            makeup_indices = np.random.choice(N, sample_num_point - num)\n            sample_datas[i,num:,:] = data[makeup_indices, :]\n            sample_labels[i,num:,0] = label[makeup_indices]\n    return sample_datas, sample_labels\n\ndef room2samples_plus_normalized(data_label, num_point):\n    """""" room2sample, with input filename and RGB preprocessing.\n        for each block centralize XYZ, add normalized XYZ as 678 channels\n    """"""\n    data = data_label[:,0:6]\n    data[:,3:6] /= 255.0\n    label = data_label[:,-1].astype(np.uint8)\n    max_room_x = max(data[:,0])\n    max_room_y = max(data[:,1])\n    max_room_z = max(data[:,2])\n    #print(max_room_x, max_room_y, max_room_z)\n    \n    data_batch, label_batch = room2samples(data, label, num_point)\n    new_data_batch = np.zeros((data_batch.shape[0], num_point, 9))\n    for b in range(data_batch.shape[0]):\n        new_data_batch[b, :, 6] = data_batch[b, :, 0]/max_room_x\n        new_data_batch[b, :, 7] = data_batch[b, :, 1]/max_room_y\n        new_data_batch[b, :, 8] = data_batch[b, :, 2]/max_room_z\n        #minx = min(data_batch[b, :, 0])\n        #miny = min(data_batch[b, :, 1])\n        #data_batch[b, :, 0] -= (minx+block_size/2)\n        #data_batch[b, :, 1] -= (miny+block_size/2)\n    new_data_batch[:, :, 0:6] = data_batch\n    return new_data_batch, label_batch\n\n\ndef room2samples_wrapper_normalized(data_label_filename, num_point):\n    if data_label_filename[-3:] == \'txt\':\n        data_label = np.loadtxt(data_label_filename)\n    elif data_label_filename[-3:] == \'npy\':\n        data_label = np.load(data_label_filename)\n    else:\n        print(\'Unknown file type! exiting.\')\n        exit()\n    return room2samples_plus_normalized(data_label, num_point)\n\n\n# -----------------------------------------------------------------------------\n# EXTRACT INSTANCE BBOX FROM ORIGINAL DATA (for detection evaluation)\n# -----------------------------------------------------------------------------\n\ndef collect_bounding_box(anno_path, out_filename):\n    """""" Compute bounding boxes from each instance in original dataset files on\n        one room. **We assume the bbox is aligned with XYZ coordinate.**\n    \n    Args:\n        anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n        out_filename: path to save instance bounding boxes for that room.\n            each line is x1 y1 z1 x2 y2 z2 label,\n            where (x1,y1,z1) is the point on the diagonal closer to origin\n    Returns:\n        None\n    Note:\n        room points are shifted, the most negative point is now at origin.\n    """"""\n    bbox_label_list = []\n\n    for f in glob.glob(os.path.join(anno_path, \'*.txt\')):\n        cls = os.path.basename(f).split(\'_\')[0]\n        if cls not in g_classes: # note: in some room there is \'staris\' class..\n            cls = \'clutter\'\n        points = np.loadtxt(f)\n        label = g_class2label[cls]\n        # Compute tightest axis aligned bounding box\n        xyz_min = np.amin(points[:, 0:3], axis=0)\n        xyz_max = np.amax(points[:, 0:3], axis=0)\n        ins_bbox_label = np.expand_dims(\n            np.concatenate([xyz_min, xyz_max, np.array([label])], 0), 0)\n        bbox_label_list.append(ins_bbox_label)\n\n    bbox_label = np.concatenate(bbox_label_list, 0)\n    room_xyz_min = np.amin(bbox_label[:, 0:3], axis=0)\n    bbox_label[:, 0:3] -= room_xyz_min \n    bbox_label[:, 3:6] -= room_xyz_min \n\n    fout = open(out_filename, \'w\')\n    for i in range(bbox_label.shape[0]):\n        fout.write(\'%f %f %f %f %f %f %d\\n\' % \\\n                      (bbox_label[i,0], bbox_label[i,1], bbox_label[i,2],\n                       bbox_label[i,3], bbox_label[i,4], bbox_label[i,5],\n                       bbox_label[i,6]))\n    fout.close()\n\ndef bbox_label_to_obj(input_filename, out_filename_prefix, easy_view=False):\n    """""" Visualization of bounding boxes.\n    \n    Args:\n        input_filename: each line is x1 y1 z1 x2 y2 z2 label\n        out_filename_prefix: OBJ filename prefix,\n            visualize object by g_label2color\n        easy_view: if True, only visualize furniture and floor\n    Returns:\n        output a list of OBJ file and MTL files with the same prefix\n    """"""\n    bbox_label = np.loadtxt(input_filename)\n    bbox = bbox_label[:, 0:6]\n    label = bbox_label[:, -1].astype(int)\n    v_cnt = 0 # count vertex\n    ins_cnt = 0 # count instance\n    for i in range(bbox.shape[0]):\n        if easy_view and (label[i] not in g_easy_view_labels):\n            continue\n        obj_filename = out_filename_prefix+\'_\'+g_classes[label[i]]+\'_\'+str(ins_cnt)+\'.obj\'\n        mtl_filename = out_filename_prefix+\'_\'+g_classes[label[i]]+\'_\'+str(ins_cnt)+\'.mtl\'\n        fout_obj = open(obj_filename, \'w\')\n        fout_mtl = open(mtl_filename, \'w\')\n        fout_obj.write(\'mtllib %s\\n\' % (os.path.basename(mtl_filename)))\n\n        length = bbox[i, 3:6] - bbox[i, 0:3]\n        a = length[0]\n        b = length[1]\n        c = length[2]\n        x = bbox[i, 0]\n        y = bbox[i, 1]\n        z = bbox[i, 2]\n        color = np.array(g_label2color[label[i]], dtype=float) / 255.0\n\n        material = \'material%d\' % (ins_cnt)\n        fout_obj.write(\'usemtl %s\\n\' % (material))\n        fout_obj.write(\'v %f %f %f\\n\' % (x,y,z+c))\n        fout_obj.write(\'v %f %f %f\\n\' % (x,y+b,z+c))\n        fout_obj.write(\'v %f %f %f\\n\' % (x+a,y+b,z+c))\n        fout_obj.write(\'v %f %f %f\\n\' % (x+a,y,z+c))\n        fout_obj.write(\'v %f %f %f\\n\' % (x,y,z))\n        fout_obj.write(\'v %f %f %f\\n\' % (x,y+b,z))\n        fout_obj.write(\'v %f %f %f\\n\' % (x+a,y+b,z))\n        fout_obj.write(\'v %f %f %f\\n\' % (x+a,y,z))\n        fout_obj.write(\'g default\\n\')\n        v_cnt = 0 # for individual box\n        fout_obj.write(\'f %d %d %d %d\\n\' % (4+v_cnt, 3+v_cnt, 2+v_cnt, 1+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (1+v_cnt, 2+v_cnt, 6+v_cnt, 5+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (7+v_cnt, 6+v_cnt, 2+v_cnt, 3+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (4+v_cnt, 8+v_cnt, 7+v_cnt, 3+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (5+v_cnt, 8+v_cnt, 4+v_cnt, 1+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (5+v_cnt, 6+v_cnt, 7+v_cnt, 8+v_cnt))\n        fout_obj.write(\'\\n\')\n\n        fout_mtl.write(\'newmtl %s\\n\' % (material))\n        fout_mtl.write(\'Kd %f %f %f\\n\' % (color[0], color[1], color[2]))\n        fout_mtl.write(\'\\n\')\n        fout_obj.close()\n        fout_mtl.close() \n\n        v_cnt += 8\n        ins_cnt += 1\n\ndef bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):\n    """""" Visualization of bounding boxes.\n    \n    Args:\n        input_filename: each line is x1 y1 z1 x2 y2 z2 label\n        out_filename_prefix: OBJ filename prefix,\n            visualize object by g_label2color\n        easy_view: if True, only visualize furniture and floor\n        permute: if not None, permute XYZ for rendering, e.g. [0 2 1]\n        center: if True, move obj to have zero origin\n    Returns:\n        output a list of OBJ file and MTL files with the same prefix\n    """"""\n    bbox_label = np.loadtxt(input_filename)\n    bbox = bbox_label[:, 0:6]\n    if permute is not None:\n        assert(len(permute)==3)\n        permute = np.array(permute)\n        bbox[:,0:3] = bbox[:,permute]\n        bbox[:,3:6] = bbox[:,permute+3]\n    if center:\n        xyz_max = np.amax(bbox[:,3:6], 0)\n        bbox[:,0:3] -= (xyz_max/2.0)\n        bbox[:,3:6] -= (xyz_max/2.0)\n        bbox /= np.max(xyz_max/2.0)\n    label = bbox_label[:, -1].astype(int)\n    obj_filename = out_filename_prefix+\'.obj\' \n    mtl_filename = out_filename_prefix+\'.mtl\'\n\n    fout_obj = open(obj_filename, \'w\')\n    fout_mtl = open(mtl_filename, \'w\')\n    fout_obj.write(\'mtllib %s\\n\' % (os.path.basename(mtl_filename)))\n    v_cnt = 0 # count vertex\n    ins_cnt = 0 # count instance\n    for i in range(bbox.shape[0]):\n        if easy_view and (label[i] not in g_easy_view_labels):\n            continue\n        if exclude_table and label[i] == g_classes.index(\'table\'):\n            continue\n\n        length = bbox[i, 3:6] - bbox[i, 0:3]\n        a = length[0]\n        b = length[1]\n        c = length[2]\n        x = bbox[i, 0]\n        y = bbox[i, 1]\n        z = bbox[i, 2]\n        color = np.array(g_label2color[label[i]], dtype=float) / 255.0\n\n        material = \'material%d\' % (ins_cnt)\n        fout_obj.write(\'usemtl %s\\n\' % (material))\n        fout_obj.write(\'v %f %f %f\\n\' % (x,y,z+c))\n        fout_obj.write(\'v %f %f %f\\n\' % (x,y+b,z+c))\n        fout_obj.write(\'v %f %f %f\\n\' % (x+a,y+b,z+c))\n        fout_obj.write(\'v %f %f %f\\n\' % (x+a,y,z+c))\n        fout_obj.write(\'v %f %f %f\\n\' % (x,y,z))\n        fout_obj.write(\'v %f %f %f\\n\' % (x,y+b,z))\n        fout_obj.write(\'v %f %f %f\\n\' % (x+a,y+b,z))\n        fout_obj.write(\'v %f %f %f\\n\' % (x+a,y,z))\n        fout_obj.write(\'g default\\n\')\n        fout_obj.write(\'f %d %d %d %d\\n\' % (4+v_cnt, 3+v_cnt, 2+v_cnt, 1+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (1+v_cnt, 2+v_cnt, 6+v_cnt, 5+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (7+v_cnt, 6+v_cnt, 2+v_cnt, 3+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (4+v_cnt, 8+v_cnt, 7+v_cnt, 3+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (5+v_cnt, 8+v_cnt, 4+v_cnt, 1+v_cnt))\n        fout_obj.write(\'f %d %d %d %d\\n\' % (5+v_cnt, 6+v_cnt, 7+v_cnt, 8+v_cnt))\n        fout_obj.write(\'\\n\')\n\n        fout_mtl.write(\'newmtl %s\\n\' % (material))\n        fout_mtl.write(\'Kd %f %f %f\\n\' % (color[0], color[1], color[2]))\n        fout_mtl.write(\'\\n\')\n\n        v_cnt += 8\n        ins_cnt += 1\n\n    fout_obj.close()\n    fout_mtl.close() \n\n\ndef collect_point_bounding_box(anno_path, out_filename, file_format):\n    """""" Compute bounding boxes from each instance in original dataset files on\n        one room. **We assume the bbox is aligned with XYZ coordinate.**\n        Save both the point XYZRGB and the bounding box for the point\'s\n        parent element.\n \n    Args:\n        anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n        out_filename: path to save instance bounding boxes for each point,\n            plus the point\'s XYZRGBL\n            each line is XYZRGBL offsetX offsetY offsetZ a b c,\n            where cx = X+offsetX, cy=X+offsetY, cz=Z+offsetZ\n            where (cx,cy,cz) is center of the box, a,b,c are distances from center\n            to the surfaces of the box, i.e. x1 = cx-a, x2 = cx+a, y1=cy-b etc.\n        file_format: output file format, txt or numpy\n    Returns:\n        None\n\n    Note:\n        room points are shifted, the most negative point is now at origin.\n    """"""\n    point_bbox_list = []\n\n    for f in glob.glob(os.path.join(anno_path, \'*.txt\')):\n        cls = os.path.basename(f).split(\'_\')[0]\n        if cls not in g_classes: # note: in some room there is \'staris\' class..\n            cls = \'clutter\'\n        points = np.loadtxt(f) # Nx6\n        label = g_class2label[cls] # N,\n        # Compute tightest axis aligned bounding box\n        xyz_min = np.amin(points[:, 0:3], axis=0) # 3,\n        xyz_max = np.amax(points[:, 0:3], axis=0) # 3,\n        xyz_center = (xyz_min + xyz_max) / 2\n        dimension = (xyz_max - xyz_min) / 2\n\n        xyz_offsets = xyz_center - points[:,0:3] # Nx3\n        dimensions = np.ones((points.shape[0],3)) * dimension # Nx3\n        labels = np.ones((points.shape[0],1)) * label # N\n        point_bbox_list.append(np.concatenate([points, labels,\n                                           xyz_offsets, dimensions], 1)) # Nx13\n\n    point_bbox = np.concatenate(point_bbox_list, 0) # KxNx13\n    room_xyz_min = np.amin(point_bbox[:, 0:3], axis=0)\n    point_bbox[:, 0:3] -= room_xyz_min \n\n    if file_format == \'txt\':\n        fout = open(out_filename, \'w\')\n        for i in range(point_bbox.shape[0]):\n            fout.write(\'%f %f %f %d %d %d %d %f %f %f %f %f %f\\n\' % \\\n                          (point_bbox[i,0], point_bbox[i,1], point_bbox[i,2],\n                           point_bbox[i,3], point_bbox[i,4], point_bbox[i,5],\n                           point_bbox[i,6],\n                           point_bbox[i,7], point_bbox[i,8], point_bbox[i,9],\n                           point_bbox[i,10], point_bbox[i,11], point_bbox[i,12]))\n        \n        fout.close()\n    elif file_format == \'numpy\':\n        np.save(out_filename, point_bbox)\n    else:\n        print(\'ERROR!! Unknown file format: %s, please use txt or numpy.\' % \\\n            (file_format))\n        exit()\n\n\n'"
models/pointnet.py,25,"b'import torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn.functional as F\n\n\nclass STN3d(nn.Module):\n    def __init__(self, channel):\n        super(STN3d, self).__init__()\n        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 9)\n        self.relu = nn.ReLU()\n\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.bn5 = nn.BatchNorm1d(256)\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 1024)\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32))).view(1, 9).repeat(\n            batchsize, 1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, 3, 3)\n        return x\n\n\nclass STNkd(nn.Module):\n    def __init__(self, k=64):\n        super(STNkd, self).__init__()\n        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, k * k)\n        self.relu = nn.ReLU()\n\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.bn5 = nn.BatchNorm1d(256)\n\n        self.k = k\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 1024)\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(\n            batchsize, 1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, self.k, self.k)\n        return x\n\n\nclass PointNetEncoder(nn.Module):\n    def __init__(self, global_feat=True, feature_transform=False, channel=3):\n        super(PointNetEncoder, self).__init__()\n        self.stn = STN3d(channel)\n        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n        self.global_feat = global_feat\n        self.feature_transform = feature_transform\n        if self.feature_transform:\n            self.fstn = STNkd(k=64)\n\n    def forward(self, x):\n        B, D, N = x.size()\n        trans = self.stn(x)\n        x = x.transpose(2, 1)\n        if D >3 :\n            x, feature = x.split(3,dim=2)\n        x = torch.bmm(x, trans)\n        if D > 3:\n            x = torch.cat([x,feature],dim=2)\n        x = x.transpose(2, 1)\n        x = F.relu(self.bn1(self.conv1(x)))\n\n        if self.feature_transform:\n            trans_feat = self.fstn(x)\n            x = x.transpose(2, 1)\n            x = torch.bmm(x, trans_feat)\n            x = x.transpose(2, 1)\n        else:\n            trans_feat = None\n\n        pointfeat = x\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.bn3(self.conv3(x))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 1024)\n        if self.global_feat:\n            return x, trans, trans_feat\n        else:\n            x = x.view(-1, 1024, 1).repeat(1, 1, N)\n            return torch.cat([x, pointfeat], 1), trans, trans_feat\n\n\ndef feature_transform_reguliarzer(trans):\n    d = trans.size()[1]\n    I = torch.eye(d)[None, :, :]\n    if trans.is_cuda:\n        I = I.cuda()\n    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2, 1) - I), dim=(1, 2)))\n    return loss\n'"
models/pointnet2_cls_msg.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom pointnet_util import PointNetSetAbstractionMsg, PointNetSetAbstraction\n\n\nclass get_model(nn.Module):\n    def __init__(self,num_class,normal_channel=True):\n        super(get_model, self).__init__()\n        in_channel = 3 if normal_channel else 0\n        self.normal_channel = normal_channel\n        self.sa1 = PointNetSetAbstractionMsg(512, [0.1, 0.2, 0.4], [16, 32, 128], in_channel,[[32, 32, 64], [64, 64, 128], [64, 96, 128]])\n        self.sa2 = PointNetSetAbstractionMsg(128, [0.2, 0.4, 0.8], [32, 64, 128], 320,[[64, 64, 128], [128, 128, 256], [128, 128, 256]])\n        self.sa3 = PointNetSetAbstraction(None, None, None, 640 + 3, [256, 512, 1024], True)\n        self.fc1 = nn.Linear(1024, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.drop1 = nn.Dropout(0.4)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.drop2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(256, num_class)\n\n    def forward(self, xyz):\n        B, _, _ = xyz.shape\n        if self.normal_channel:\n            norm = xyz[:, 3:, :]\n            xyz = xyz[:, :3, :]\n        else:\n            norm = None\n        l1_xyz, l1_points = self.sa1(xyz, norm)\n        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n        x = l3_points.view(B, 1024)\n        x = self.drop1(F.relu(self.bn1(self.fc1(x))))\n        x = self.drop2(F.relu(self.bn2(self.fc2(x))))\n        x = self.fc3(x)\n        x = F.log_softmax(x, -1)\n\n\n        return x,l3_points\n\n\nclass get_loss(nn.Module):\n    def __init__(self):\n        super(get_loss, self).__init__()\n\n    def forward(self, pred, target, trans_feat):\n        total_loss = F.nll_loss(pred, target)\n\n        return total_loss\n\n\n'"
models/pointnet2_cls_ssg.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom pointnet_util import PointNetSetAbstraction\n\n\nclass get_model(nn.Module):\n    def __init__(self,num_class,normal_channel=True):\n        super(get_model, self).__init__()\n        in_channel = 6 if normal_channel else 3\n        self.normal_channel = normal_channel\n        self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False)\n        self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False)\n        self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True)\n        self.fc1 = nn.Linear(1024, 512)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.drop1 = nn.Dropout(0.4)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.drop2 = nn.Dropout(0.4)\n        self.fc3 = nn.Linear(256, num_class)\n\n    def forward(self, xyz):\n        B, _, _ = xyz.shape\n        if self.normal_channel:\n            norm = xyz[:, 3:, :]\n            xyz = xyz[:, :3, :]\n        else:\n            norm = None\n        l1_xyz, l1_points = self.sa1(xyz, norm)\n        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n        x = l3_points.view(B, 1024)\n        x = self.drop1(F.relu(self.bn1(self.fc1(x))))\n        x = self.drop2(F.relu(self.bn2(self.fc2(x))))\n        x = self.fc3(x)\n        x = F.log_softmax(x, -1)\n\n\n        return x, l3_points\n\n\n\nclass get_loss(nn.Module):\n    def __init__(self):\n        super(get_loss, self).__init__()\n\n    def forward(self, pred, target, trans_feat):\n        total_loss = F.nll_loss(pred, target)\n\n        return total_loss\n'"
models/pointnet2_part_seg_msg.py,3,"b'import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom models.pointnet_util import PointNetSetAbstractionMsg,PointNetSetAbstraction,PointNetFeaturePropagation\n\n\nclass get_model(nn.Module):\n    def __init__(self, num_classes, normal_channel=False):\n        super(get_model, self).__init__()\n        if normal_channel:\n            additional_channel = 3\n        else:\n            additional_channel = 0\n        self.normal_channel = normal_channel\n        self.sa1 = PointNetSetAbstractionMsg(512, [0.1, 0.2, 0.4], [32, 64, 128], 3+additional_channel, [[32, 32, 64], [64, 64, 128], [64, 96, 128]])\n        self.sa2 = PointNetSetAbstractionMsg(128, [0.4,0.8], [64, 128], 128+128+64, [[128, 128, 256], [128, 196, 256]])\n        self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=512 + 3, mlp=[256, 512, 1024], group_all=True)\n        self.fp3 = PointNetFeaturePropagation(in_channel=1536, mlp=[256, 256])\n        self.fp2 = PointNetFeaturePropagation(in_channel=576, mlp=[256, 128])\n        self.fp1 = PointNetFeaturePropagation(in_channel=150+additional_channel, mlp=[128, 128])\n        self.conv1 = nn.Conv1d(128, 128, 1)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.drop1 = nn.Dropout(0.5)\n        self.conv2 = nn.Conv1d(128, num_classes, 1)\n\n    def forward(self, xyz, cls_label):\n        # Set Abstraction layers\n        B,C,N = xyz.shape\n        if self.normal_channel:\n            l0_points = xyz\n            l0_xyz = xyz[:,:3,:]\n        else:\n            l0_points = xyz\n            l0_xyz = xyz\n        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)\n        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n        # Feature Propagation layers\n        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n        cls_label_one_hot = cls_label.view(B,16,1).repeat(1,1,N)\n        l0_points = self.fp1(l0_xyz, l1_xyz, torch.cat([cls_label_one_hot,l0_xyz,l0_points],1), l1_points)\n        # FC layers\n        feat = F.relu(self.bn1(self.conv1(l0_points)))\n        x = self.drop1(feat)\n        x = self.conv2(x)\n        x = F.log_softmax(x, dim=1)\n        x = x.permute(0, 2, 1)\n        return x, l3_points\n\n\nclass get_loss(nn.Module):\n    def __init__(self):\n        super(get_loss, self).__init__()\n\n    def forward(self, pred, target, trans_feat):\n        total_loss = F.nll_loss(pred, target)\n\n        return total_loss'"
models/pointnet2_part_seg_ssg.py,3,"b'import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom models.pointnet_util import PointNetSetAbstraction,PointNetFeaturePropagation\n\n\nclass get_model(nn.Module):\n    def __init__(self, num_classes, normal_channel=False):\n        super(get_model, self).__init__()\n        if normal_channel:\n            additional_channel = 3\n        else:\n            additional_channel = 0\n        self.normal_channel = normal_channel\n        self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=6+additional_channel, mlp=[64, 64, 128], group_all=False)\n        self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False)\n        self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True)\n        self.fp3 = PointNetFeaturePropagation(in_channel=1280, mlp=[256, 256])\n        self.fp2 = PointNetFeaturePropagation(in_channel=384, mlp=[256, 128])\n        self.fp1 = PointNetFeaturePropagation(in_channel=128+16+6+additional_channel, mlp=[128, 128, 128])\n        self.conv1 = nn.Conv1d(128, 128, 1)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.drop1 = nn.Dropout(0.5)\n        self.conv2 = nn.Conv1d(128, num_classes, 1)\n\n    def forward(self, xyz, cls_label):\n        # Set Abstraction layers\n        B,C,N = xyz.shape\n        if self.normal_channel:\n            l0_points = xyz\n            l0_xyz = xyz[:,:3,:]\n        else:\n            l0_points = xyz\n            l0_xyz = xyz\n        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)\n        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n        # Feature Propagation layers\n        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n        cls_label_one_hot = cls_label.view(B,16,1).repeat(1,1,N)\n        l0_points = self.fp1(l0_xyz, l1_xyz, torch.cat([cls_label_one_hot,l0_xyz,l0_points],1), l1_points)\n        # FC layers\n        feat =  F.relu(self.bn1(self.conv1(l0_points)))\n        x = self.drop1(feat)\n        x = self.conv2(x)\n        x = F.log_softmax(x, dim=1)\n        x = x.permute(0, 2, 1)\n        return x, l3_points\n\n\nclass get_loss(nn.Module):\n    def __init__(self):\n        super(get_loss, self).__init__()\n\n    def forward(self, pred, target, trans_feat):\n        total_loss = F.nll_loss(pred, target)\n\n        return total_loss'"
models/pointnet2_sem_seg.py,3,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom models.pointnet_util import PointNetSetAbstraction,PointNetFeaturePropagation\n\n\nclass get_model(nn.Module):\n    def __init__(self, num_classes):\n        super(get_model, self).__init__()\n        self.sa1 = PointNetSetAbstraction(1024, 0.1, 32, 9 + 3, [32, 32, 64], False)\n        self.sa2 = PointNetSetAbstraction(256, 0.2, 32, 64 + 3, [64, 64, 128], False)\n        self.sa3 = PointNetSetAbstraction(64, 0.4, 32, 128 + 3, [128, 128, 256], False)\n        self.sa4 = PointNetSetAbstraction(16, 0.8, 32, 256 + 3, [256, 256, 512], False)\n        self.fp4 = PointNetFeaturePropagation(768, [256, 256])\n        self.fp3 = PointNetFeaturePropagation(384, [256, 256])\n        self.fp2 = PointNetFeaturePropagation(320, [256, 128])\n        self.fp1 = PointNetFeaturePropagation(128, [128, 128, 128])\n        self.conv1 = nn.Conv1d(128, 128, 1)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.drop1 = nn.Dropout(0.5)\n        self.conv2 = nn.Conv1d(128, num_classes, 1)\n\n    def forward(self, xyz):\n        l0_points = xyz\n        l0_xyz = xyz[:,:3,:]\n\n        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)\n        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n        l4_xyz, l4_points = self.sa4(l3_xyz, l3_points)\n\n        l3_points = self.fp4(l3_xyz, l4_xyz, l3_points, l4_points)\n        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n        l0_points = self.fp1(l0_xyz, l1_xyz, None, l1_points)\n\n        x = self.drop1(F.relu(self.bn1(self.conv1(l0_points))))\n        x = self.conv2(x)\n        x = F.log_softmax(x, dim=1)\n        x = x.permute(0, 2, 1)\n        return x, l4_points\n\n\nclass get_loss(nn.Module):\n    def __init__(self):\n        super(get_loss, self).__init__()\n    def forward(self, pred, target, trans_feat, weight):\n        total_loss = F.nll_loss(pred, target, weight=weight)\n\n        return total_loss\n\nif __name__ == '__main__':\n    import  torch\n    model = get_model(13)\n    xyz = torch.rand(6, 9, 2048)\n    (model(xyz))"""
models/pointnet2_sem_seg_msg.py,3,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom models.pointnet_util import PointNetSetAbstractionMsg,PointNetFeaturePropagation\n\n\nclass get_model(nn.Module):\n    def __init__(self, num_classes):\n        super(get_model, self).__init__()\n\n        self.sa1 = PointNetSetAbstractionMsg(1024, [0.05, 0.1], [16, 32], 9, [[16, 16, 32], [32, 32, 64]])\n        self.sa2 = PointNetSetAbstractionMsg(256, [0.1, 0.2], [16, 32], 32+64, [[64, 64, 128], [64, 96, 128]])\n        self.sa3 = PointNetSetAbstractionMsg(64, [0.2, 0.4], [16, 32], 128+128, [[128, 196, 256], [128, 196, 256]])\n        self.sa4 = PointNetSetAbstractionMsg(16, [0.4, 0.8], [16, 32], 256+256, [[256, 256, 512], [256, 384, 512]])\n        self.fp4 = PointNetFeaturePropagation(512+512+256+256, [256, 256])\n        self.fp3 = PointNetFeaturePropagation(128+128+256, [256, 256])\n        self.fp2 = PointNetFeaturePropagation(32+64+256, [256, 128])\n        self.fp1 = PointNetFeaturePropagation(128, [128, 128, 128])\n        self.conv1 = nn.Conv1d(128, 128, 1)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.drop1 = nn.Dropout(0.5)\n        self.conv2 = nn.Conv1d(128, num_classes, 1)\n\n    def forward(self, xyz):\n        l0_points = xyz\n        l0_xyz = xyz[:,:3,:]\n\n        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)\n        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n        l4_xyz, l4_points = self.sa4(l3_xyz, l3_points)\n\n        l3_points = self.fp4(l3_xyz, l4_xyz, l3_points, l4_points)\n        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n        l0_points = self.fp1(l0_xyz, l1_xyz, None, l1_points)\n\n        x = self.drop1(F.relu(self.bn1(self.conv1(l0_points))))\n        x = self.conv2(x)\n        x = F.log_softmax(x, dim=1)\n        x = x.permute(0, 2, 1)\n        return x, l4_points\n\n\nclass get_loss(nn.Module):\n    def __init__(self):\n        super(get_loss, self).__init__()\n    def forward(self, pred, target, trans_feat, weight):\n        total_loss = F.nll_loss(pred, target, weight=weight)\n\n        return total_loss\n\nif __name__ == '__main__':\n    import  torch\n    model = get_model(13)\n    xyz = torch.rand(6, 9, 2048)\n    (model(xyz))"""
models/pointnet_cls.py,4,"b'import torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom pointnet import PointNetEncoder, feature_transform_reguliarzer\n\nclass get_model(nn.Module):\n    def __init__(self, k=40, normal_channel=True):\n        super(get_model, self).__init__()\n        if normal_channel:\n            channel = 6\n        else:\n            channel = 3\n        self.feat = PointNetEncoder(global_feat=True, feature_transform=True, channel=channel)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, k)\n        self.dropout = nn.Dropout(p=0.4)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x, trans, trans_feat = self.feat(x)\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n        x = self.fc3(x)\n        x = F.log_softmax(x, dim=1)\n        return x, trans_feat\n\nclass get_loss(torch.nn.Module):\n    def __init__(self, mat_diff_loss_scale=0.001):\n        super(get_loss, self).__init__()\n        self.mat_diff_loss_scale = mat_diff_loss_scale\n\n    def forward(self, pred, target, trans_feat):\n        loss = F.nll_loss(pred, target)\n        mat_diff_loss = feature_transform_reguliarzer(trans_feat)\n\n        total_loss = loss + mat_diff_loss * self.mat_diff_loss_scale\n        return total_loss\n'"
models/pointnet_part_seg.py,20,"b'import torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom pointnet import STN3d, STNkd, feature_transform_reguliarzer\n\n\nclass get_model(nn.Module):\n    def __init__(self, part_num=50, normal_channel=True):\n        super(get_model, self).__init__()\n        if normal_channel:\n            channel = 6\n        else:\n            channel = 3\n        self.part_num = part_num\n        self.stn = STN3d(channel)\n        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 128, 1)\n        self.conv4 = torch.nn.Conv1d(128, 512, 1)\n        self.conv5 = torch.nn.Conv1d(512, 2048, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.bn5 = nn.BatchNorm1d(2048)\n        self.fstn = STNkd(k=128)\n        self.convs1 = torch.nn.Conv1d(4944, 256, 1)\n        self.convs2 = torch.nn.Conv1d(256, 256, 1)\n        self.convs3 = torch.nn.Conv1d(256, 128, 1)\n        self.convs4 = torch.nn.Conv1d(128, part_num, 1)\n        self.bns1 = nn.BatchNorm1d(256)\n        self.bns2 = nn.BatchNorm1d(256)\n        self.bns3 = nn.BatchNorm1d(128)\n\n    def forward(self, point_cloud, label):\n        B, D, N = point_cloud.size()\n        trans = self.stn(point_cloud)\n        point_cloud = point_cloud.transpose(2, 1)\n        if D > 3:\n            point_cloud, feature = point_cloud.split(3, dim=2)\n        point_cloud = torch.bmm(point_cloud, trans)\n        if D > 3:\n            point_cloud = torch.cat([point_cloud, feature], dim=2)\n\n        point_cloud = point_cloud.transpose(2, 1)\n\n        out1 = F.relu(self.bn1(self.conv1(point_cloud)))\n        out2 = F.relu(self.bn2(self.conv2(out1)))\n        out3 = F.relu(self.bn3(self.conv3(out2)))\n\n        trans_feat = self.fstn(out3)\n        x = out3.transpose(2, 1)\n        net_transformed = torch.bmm(x, trans_feat)\n        net_transformed = net_transformed.transpose(2, 1)\n\n        out4 = F.relu(self.bn4(self.conv4(net_transformed)))\n        out5 = self.bn5(self.conv5(out4))\n        out_max = torch.max(out5, 2, keepdim=True)[0]\n        out_max = out_max.view(-1, 2048)\n\n        out_max = torch.cat([out_max,label.squeeze(1)],1)\n        expand = out_max.view(-1, 2048+16, 1).repeat(1, 1, N)\n        concat = torch.cat([expand, out1, out2, out3, out4, out5], 1)\n        net = F.relu(self.bns1(self.convs1(concat)))\n        net = F.relu(self.bns2(self.convs2(net)))\n        net = F.relu(self.bns3(self.convs3(net)))\n        net = self.convs4(net)\n        net = net.transpose(2, 1).contiguous()\n        net = F.log_softmax(net.view(-1, self.part_num), dim=-1)\n        net = net.view(B, N, self.part_num) # [B, N, 50]\n\n        return net, trans_feat\n\n\nclass get_loss(torch.nn.Module):\n    def __init__(self, mat_diff_loss_scale=0.001):\n        super(get_loss, self).__init__()\n        self.mat_diff_loss_scale = mat_diff_loss_scale\n\n    def forward(self, pred, target, trans_feat):\n        loss = F.nll_loss(pred, target)\n        mat_diff_loss = feature_transform_reguliarzer(trans_feat)\n        total_loss = loss + mat_diff_loss * self.mat_diff_loss_scale\n        return total_loss'"
models/pointnet_sem_seg.py,10,"b""import torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom pointnet import PointNetEncoder, feature_transform_reguliarzer\n\n\nclass get_model(nn.Module):\n    def __init__(self, num_class, with_rgb=True):\n        super(get_model, self).__init__()\n        if with_rgb:\n            channel = 6\n        else:\n            channel = 3\n        self.k = num_class\n        self.feat = PointNetEncoder(global_feat=False, feature_transform=True, channel=channel)\n        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.bn3 = nn.BatchNorm1d(128)\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        x, trans, trans_feat = self.feat(x)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.conv4(x)\n        x = x.transpose(2,1).contiguous()\n        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n        x = x.view(batchsize, n_pts, self.k)\n        return x, trans_feat\n\nclass get_loss(torch.nn.Module):\n    def __init__(self, mat_diff_loss_scale=0.001):\n        super(get_loss, self).__init__()\n        self.mat_diff_loss_scale = mat_diff_loss_scale\n\n    def forward(self, pred, target, trans_feat, weight):\n        loss = F.nll_loss(pred, target, weight = weight)\n        mat_diff_loss = feature_transform_reguliarzer(trans_feat)\n        total_loss = loss + mat_diff_loss * self.mat_diff_loss_scale\n        return total_loss\n\n\nif __name__ == '__main__':\n    model = get_model(13, with_rgb=False)\n    xyz = torch.rand(12, 3, 2048)\n    (model(xyz))"""
models/pointnet_util.py,28,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom time import time\nimport numpy as np\n\ndef timeit(tag, t):\n    print(""{}: {}s"".format(tag, time() - t))\n    return time()\n\ndef pc_normalize(pc):\n    l = pc.shape[0]\n    centroid = np.mean(pc, axis=0)\n    pc = pc - centroid\n    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n    pc = pc / m\n    return pc\n\ndef square_distance(src, dst):\n    """"""\n    Calculate Euclid distance between each two points.\n\n    src^T * dst = xn * xm + yn * ym + zn * zm\xef\xbc\x9b\n    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n\n    Input:\n        src: source points, [B, N, C]\n        dst: target points, [B, M, C]\n    Output:\n        dist: per-point square distance, [B, N, M]\n    """"""\n    B, N, _ = src.shape\n    _, M, _ = dst.shape\n    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n    return dist\n\n\ndef index_points(points, idx):\n    """"""\n\n    Input:\n        points: input points data, [B, N, C]\n        idx: sample index data, [B, S]\n    Return:\n        new_points:, indexed points data, [B, S, C]\n    """"""\n    device = points.device\n    B = points.shape[0]\n    view_shape = list(idx.shape)\n    view_shape[1:] = [1] * (len(view_shape) - 1)\n    repeat_shape = list(idx.shape)\n    repeat_shape[0] = 1\n    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n    new_points = points[batch_indices, idx, :]\n    return new_points\n\n\ndef farthest_point_sample(xyz, npoint):\n    """"""\n    Input:\n        xyz: pointcloud data, [B, N, 3]\n        npoint: number of samples\n    Return:\n        centroids: sampled pointcloud index, [B, npoint]\n    """"""\n    device = xyz.device\n    B, N, C = xyz.shape\n    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n    distance = torch.ones(B, N).to(device) * 1e10\n    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n    for i in range(npoint):\n        centroids[:, i] = farthest\n        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n        dist = torch.sum((xyz - centroid) ** 2, -1)\n        mask = dist < distance\n        distance[mask] = dist[mask]\n        farthest = torch.max(distance, -1)[1]\n    return centroids\n\n\ndef query_ball_point(radius, nsample, xyz, new_xyz):\n    """"""\n    Input:\n        radius: local region radius\n        nsample: max sample number in local region\n        xyz: all points, [B, N, 3]\n        new_xyz: query points, [B, S, 3]\n    Return:\n        group_idx: grouped points index, [B, S, nsample]\n    """"""\n    device = xyz.device\n    B, N, C = xyz.shape\n    _, S, _ = new_xyz.shape\n    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n    sqrdists = square_distance(new_xyz, xyz)\n    group_idx[sqrdists > radius ** 2] = N\n    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n    mask = group_idx == N\n    group_idx[mask] = group_first[mask]\n    return group_idx\n\n\ndef sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n    """"""\n    Input:\n        npoint:\n        radius:\n        nsample:\n        xyz: input points position data, [B, N, 3]\n        points: input points data, [B, N, D]\n    Return:\n        new_xyz: sampled points position data, [B, npoint, nsample, 3]\n        new_points: sampled points data, [B, npoint, nsample, 3+D]\n    """"""\n    B, N, C = xyz.shape\n    S = npoint\n    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n    torch.cuda.empty_cache()\n    new_xyz = index_points(xyz, fps_idx)\n    torch.cuda.empty_cache()\n    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n    torch.cuda.empty_cache()\n    grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n    torch.cuda.empty_cache()\n    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n    torch.cuda.empty_cache()\n\n    if points is not None:\n        grouped_points = index_points(points, idx)\n        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n    else:\n        new_points = grouped_xyz_norm\n    if returnfps:\n        return new_xyz, new_points, grouped_xyz, fps_idx\n    else:\n        return new_xyz, new_points\n\n\ndef sample_and_group_all(xyz, points):\n    """"""\n    Input:\n        xyz: input points position data, [B, N, 3]\n        points: input points data, [B, N, D]\n    Return:\n        new_xyz: sampled points position data, [B, 1, 3]\n        new_points: sampled points data, [B, 1, N, 3+D]\n    """"""\n    device = xyz.device\n    B, N, C = xyz.shape\n    new_xyz = torch.zeros(B, 1, C).to(device)\n    grouped_xyz = xyz.view(B, 1, N, C)\n    if points is not None:\n        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n    else:\n        new_points = grouped_xyz\n    return new_xyz, new_points\n\n\nclass PointNetSetAbstraction(nn.Module):\n    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n        super(PointNetSetAbstraction, self).__init__()\n        self.npoint = npoint\n        self.radius = radius\n        self.nsample = nsample\n        self.mlp_convs = nn.ModuleList()\n        self.mlp_bns = nn.ModuleList()\n        last_channel = in_channel\n        for out_channel in mlp:\n            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n            last_channel = out_channel\n        self.group_all = group_all\n\n    def forward(self, xyz, points):\n        """"""\n        Input:\n            xyz: input points position data, [B, C, N]\n            points: input points data, [B, D, N]\n        Return:\n            new_xyz: sampled points position data, [B, C, S]\n            new_points_concat: sample points feature data, [B, D\', S]\n        """"""\n        xyz = xyz.permute(0, 2, 1)\n        if points is not None:\n            points = points.permute(0, 2, 1)\n\n        if self.group_all:\n            new_xyz, new_points = sample_and_group_all(xyz, points)\n        else:\n            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n        # new_xyz: sampled points position data, [B, npoint, C]\n        # new_points: sampled points data, [B, npoint, nsample, C+D]\n        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n        for i, conv in enumerate(self.mlp_convs):\n            bn = self.mlp_bns[i]\n            new_points =  F.relu(bn(conv(new_points)))\n\n        new_points = torch.max(new_points, 2)[0]\n        new_xyz = new_xyz.permute(0, 2, 1)\n        return new_xyz, new_points\n\n\nclass PointNetSetAbstractionMsg(nn.Module):\n    def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list):\n        super(PointNetSetAbstractionMsg, self).__init__()\n        self.npoint = npoint\n        self.radius_list = radius_list\n        self.nsample_list = nsample_list\n        self.conv_blocks = nn.ModuleList()\n        self.bn_blocks = nn.ModuleList()\n        for i in range(len(mlp_list)):\n            convs = nn.ModuleList()\n            bns = nn.ModuleList()\n            last_channel = in_channel + 3\n            for out_channel in mlp_list[i]:\n                convs.append(nn.Conv2d(last_channel, out_channel, 1))\n                bns.append(nn.BatchNorm2d(out_channel))\n                last_channel = out_channel\n            self.conv_blocks.append(convs)\n            self.bn_blocks.append(bns)\n\n    def forward(self, xyz, points):\n        """"""\n        Input:\n            xyz: input points position data, [B, C, N]\n            points: input points data, [B, D, N]\n        Return:\n            new_xyz: sampled points position data, [B, C, S]\n            new_points_concat: sample points feature data, [B, D\', S]\n        """"""\n        xyz = xyz.permute(0, 2, 1)\n        if points is not None:\n            points = points.permute(0, 2, 1)\n\n        B, N, C = xyz.shape\n        S = self.npoint\n        new_xyz = index_points(xyz, farthest_point_sample(xyz, S))\n        new_points_list = []\n        for i, radius in enumerate(self.radius_list):\n            K = self.nsample_list[i]\n            group_idx = query_ball_point(radius, K, xyz, new_xyz)\n            grouped_xyz = index_points(xyz, group_idx)\n            grouped_xyz -= new_xyz.view(B, S, 1, C)\n            if points is not None:\n                grouped_points = index_points(points, group_idx)\n                grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1)\n            else:\n                grouped_points = grouped_xyz\n\n            grouped_points = grouped_points.permute(0, 3, 2, 1)  # [B, D, K, S]\n            for j in range(len(self.conv_blocks[i])):\n                conv = self.conv_blocks[i][j]\n                bn = self.bn_blocks[i][j]\n                grouped_points =  F.relu(bn(conv(grouped_points)))\n            new_points = torch.max(grouped_points, 2)[0]  # [B, D\', S]\n            new_points_list.append(new_points)\n\n        new_xyz = new_xyz.permute(0, 2, 1)\n        new_points_concat = torch.cat(new_points_list, dim=1)\n        return new_xyz, new_points_concat\n\n\nclass PointNetFeaturePropagation(nn.Module):\n    def __init__(self, in_channel, mlp):\n        super(PointNetFeaturePropagation, self).__init__()\n        self.mlp_convs = nn.ModuleList()\n        self.mlp_bns = nn.ModuleList()\n        last_channel = in_channel\n        for out_channel in mlp:\n            self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1))\n            self.mlp_bns.append(nn.BatchNorm1d(out_channel))\n            last_channel = out_channel\n\n    def forward(self, xyz1, xyz2, points1, points2):\n        """"""\n        Input:\n            xyz1: input points position data, [B, C, N]\n            xyz2: sampled input points position data, [B, C, S]\n            points1: input points data, [B, D, N]\n            points2: input points data, [B, D, S]\n        Return:\n            new_points: upsampled points data, [B, D\', N]\n        """"""\n        xyz1 = xyz1.permute(0, 2, 1)\n        xyz2 = xyz2.permute(0, 2, 1)\n\n        points2 = points2.permute(0, 2, 1)\n        B, N, C = xyz1.shape\n        _, S, _ = xyz2.shape\n\n        if S == 1:\n            interpolated_points = points2.repeat(1, N, 1)\n        else:\n            dists = square_distance(xyz1, xyz2)\n            dists, idx = dists.sort(dim=-1)\n            dists, idx = dists[:, :, :3], idx[:, :, :3]  # [B, N, 3]\n\n            dist_recip = 1.0 / (dists + 1e-8)\n            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n            weight = dist_recip / norm\n            interpolated_points = torch.sum(index_points(points2, idx) * weight.view(B, N, 3, 1), dim=2)\n\n        if points1 is not None:\n            points1 = points1.permute(0, 2, 1)\n            new_points = torch.cat([points1, interpolated_points], dim=-1)\n        else:\n            new_points = interpolated_points\n\n        new_points = new_points.permute(0, 2, 1)\n        for i, conv in enumerate(self.mlp_convs):\n            bn = self.mlp_bns[i]\n            new_points = F.relu(bn(conv(new_points)))\n        return new_points\n\n'"
visualizer/eulerangles.py,0,"b'# emacs: -*- mode: python-mode; py-indent-offset: 4; indent-tabs-mode: nil -*-\n# vi: set ft=python sts=4 ts=4 sw=4 et:\n### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\n#\n#   See COPYING file distributed along with the NiBabel package for the\n#   copyright and license terms.\n#\n### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\n\'\'\' Module implementing Euler angle rotations and their conversions\nSee:\n* http://en.wikipedia.org/wiki/Rotation_matrix\n* http://en.wikipedia.org/wiki/Euler_angles\n* http://mathworld.wolfram.com/EulerAngles.html\nSee also: *Representing Attitude with Euler Angles and Quaternions: A\nReference* (2006) by James Diebel. A cached PDF link last found here:\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.110.5134\nEuler\'s rotation theorem tells us that any rotation in 3D can be\ndescribed by 3 angles.  Let\'s call the 3 angles the *Euler angle vector*\nand call the angles in the vector :math:`alpha`, :math:`beta` and\n:math:`gamma`.  The vector is [ :math:`alpha`,\n:math:`beta`. :math:`gamma` ] and, in this description, the order of the\nparameters specifies the order in which the rotations occur (so the\nrotation corresponding to :math:`alpha` is applied first).\nIn order to specify the meaning of an *Euler angle vector* we need to\nspecify the axes around which each of the rotations corresponding to\n:math:`alpha`, :math:`beta` and :math:`gamma` will occur.\nThere are therefore three axes for the rotations :math:`alpha`,\n:math:`beta` and :math:`gamma`; let\'s call them :math:`i` :math:`j`,\n:math:`k`.\nLet us express the rotation :math:`alpha` around axis `i` as a 3 by 3\nrotation matrix `A`.  Similarly :math:`beta` around `j` becomes 3 x 3\nmatrix `B` and :math:`gamma` around `k` becomes matrix `G`.  Then the\nwhole rotation expressed by the Euler angle vector [ :math:`alpha`,\n:math:`beta`. :math:`gamma` ], `R` is given by::\n   R = np.dot(G, np.dot(B, A))\nSee http://mathworld.wolfram.com/EulerAngles.html\nThe order :math:`G B A` expresses the fact that the rotations are\nperformed in the order of the vector (:math:`alpha` around axis `i` =\n`A` first).\nTo convert a given Euler angle vector to a meaningful rotation, and a\nrotation matrix, we need to define:\n* the axes `i`, `j`, `k`\n* whether a rotation matrix should be applied on the left of a vector to\n  be transformed (vectors are column vectors) or on the right (vectors\n  are row vectors).\n* whether the rotations move the axes as they are applied (intrinsic\n  rotations) - compared the situation where the axes stay fixed and the\n  vectors move within the axis frame (extrinsic)\n* the handedness of the coordinate system\nSee: http://en.wikipedia.org/wiki/Rotation_matrix#Ambiguities\nWe are using the following conventions:\n* axes `i`, `j`, `k` are the `z`, `y`, and `x` axes respectively.  Thus\n  an Euler angle vector [ :math:`alpha`, :math:`beta`. :math:`gamma` ]\n  in our convention implies a :math:`alpha` radian rotation around the\n  `z` axis, followed by a :math:`beta` rotation around the `y` axis,\n  followed by a :math:`gamma` rotation around the `x` axis.\n* the rotation matrix applies on the left, to column vectors on the\n  right, so if `R` is the rotation matrix, and `v` is a 3 x N matrix\n  with N column vectors, the transformed vector set `vdash` is given by\n  ``vdash = np.dot(R, v)``.\n* extrinsic rotations - the axes are fixed, and do not move with the\n  rotations.\n* a right-handed coordinate system\nThe convention of rotation around ``z``, followed by rotation around\n``y``, followed by rotation around ``x``, is known (confusingly) as\n""xyz"", pitch-roll-yaw, Cardan angles, or Tait-Bryan angles.\n\'\'\'\n\nimport math\n\nimport sys\nif sys.version_info >= (3,0):\n    from functools import reduce\n\nimport numpy as np\n\n\n_FLOAT_EPS_4 = np.finfo(float).eps * 4.0\n\n\ndef euler2mat(z=0, y=0, x=0):\n    \'\'\' Return matrix for rotations around z, y and x axes\n    Uses the z, then y, then x convention above\n    Parameters\n    ----------\n    z : scalar\n       Rotation angle in radians around z-axis (performed first)\n    y : scalar\n       Rotation angle in radians around y-axis\n    x : scalar\n       Rotation angle in radians around x-axis (performed last)\n    Returns\n    -------\n    M : array shape (3,3)\n       Rotation matrix giving same rotation as for given angles\n    Examples\n    --------\n    >>> zrot = 1.3 # radians\n    >>> yrot = -0.1\n    >>> xrot = 0.2\n    >>> M = euler2mat(zrot, yrot, xrot)\n    >>> M.shape == (3, 3)\n    True\n    The output rotation matrix is equal to the composition of the\n    individual rotations\n    >>> M1 = euler2mat(zrot)\n    >>> M2 = euler2mat(0, yrot)\n    >>> M3 = euler2mat(0, 0, xrot)\n    >>> composed_M = np.dot(M3, np.dot(M2, M1))\n    >>> np.allclose(M, composed_M)\n    True\n    You can specify rotations by named arguments\n    >>> np.all(M3 == euler2mat(x=xrot))\n    True\n    When applying M to a vector, the vector should column vector to the\n    right of M.  If the right hand side is a 2D array rather than a\n    vector, then each column of the 2D array represents a vector.\n    >>> vec = np.array([1, 0, 0]).reshape((3,1))\n    >>> v2 = np.dot(M, vec)\n    >>> vecs = np.array([[1, 0, 0],[0, 1, 0]]).T # giving 3x2 array\n    >>> vecs2 = np.dot(M, vecs)\n    Rotations are counter-clockwise.\n    >>> zred = np.dot(euler2mat(z=np.pi/2), np.eye(3))\n    >>> np.allclose(zred, [[0, -1, 0],[1, 0, 0], [0, 0, 1]])\n    True\n    >>> yred = np.dot(euler2mat(y=np.pi/2), np.eye(3))\n    >>> np.allclose(yred, [[0, 0, 1],[0, 1, 0], [-1, 0, 0]])\n    True\n    >>> xred = np.dot(euler2mat(x=np.pi/2), np.eye(3))\n    >>> np.allclose(xred, [[1, 0, 0],[0, 0, -1], [0, 1, 0]])\n    True\n    Notes\n    -----\n    The direction of rotation is given by the right-hand rule (orient\n    the thumb of the right hand along the axis around which the rotation\n    occurs, with the end of the thumb at the positive end of the axis;\n    curl your fingers; the direction your fingers curl is the direction\n    of rotation).  Therefore, the rotations are counterclockwise if\n    looking along the axis of rotation from positive to negative.\n    \'\'\'\n    Ms = []\n    if z:\n        cosz = math.cos(z)\n        sinz = math.sin(z)\n        Ms.append(np.array(\n                [[cosz, -sinz, 0],\n                 [sinz, cosz, 0],\n                 [0, 0, 1]]))\n    if y:\n        cosy = math.cos(y)\n        siny = math.sin(y)\n        Ms.append(np.array(\n                [[cosy, 0, siny],\n                 [0, 1, 0],\n                 [-siny, 0, cosy]]))\n    if x:\n        cosx = math.cos(x)\n        sinx = math.sin(x)\n        Ms.append(np.array(\n                [[1, 0, 0],\n                 [0, cosx, -sinx],\n                 [0, sinx, cosx]]))\n    if Ms:\n        return reduce(np.dot, Ms[::-1])\n    return np.eye(3)\n\n\ndef mat2euler(M, cy_thresh=None):\n    \'\'\' Discover Euler angle vector from 3x3 matrix\n    Uses the conventions above.\n    Parameters\n    ----------\n    M : array-like, shape (3,3)\n    cy_thresh : None or scalar, optional\n       threshold below which to give up on straightforward arctan for\n       estimating x rotation.  If None (default), estimate from\n       precision of input.\n    Returns\n    -------\n    z : scalar\n    y : scalar\n    x : scalar\n       Rotations in radians around z, y, x axes, respectively\n    Notes\n    -----\n    If there was no numerical error, the routine could be derived using\n    Sympy expression for z then y then x rotation matrix, which is::\n      [                       cos(y)*cos(z),                       -cos(y)*sin(z),         sin(y)],\n      [cos(x)*sin(z) + cos(z)*sin(x)*sin(y), cos(x)*cos(z) - sin(x)*sin(y)*sin(z), -cos(y)*sin(x)],\n      [sin(x)*sin(z) - cos(x)*cos(z)*sin(y), cos(z)*sin(x) + cos(x)*sin(y)*sin(z),  cos(x)*cos(y)]\n    with the obvious derivations for z, y, and x\n       z = atan2(-r12, r11)\n       y = asin(r13)\n       x = atan2(-r23, r33)\n    Problems arise when cos(y) is close to zero, because both of::\n       z = atan2(cos(y)*sin(z), cos(y)*cos(z))\n       x = atan2(cos(y)*sin(x), cos(x)*cos(y))\n    will be close to atan2(0, 0), and highly unstable.\n    The ``cy`` fix for numerical instability below is from: *Graphics\n    Gems IV*, Paul Heckbert (editor), Academic Press, 1994, ISBN:\n    0123361559.  Specifically it comes from EulerAngles.c by Ken\n    Shoemake, and deals with the case where cos(y) is close to zero:\n    See: http://www.graphicsgems.org/\n    The code appears to be licensed (from the website) as ""can be used\n    without restrictions"".\n    \'\'\'\n    M = np.asarray(M)\n    if cy_thresh is None:\n        try:\n            cy_thresh = np.finfo(M.dtype).eps * 4\n        except ValueError:\n            cy_thresh = _FLOAT_EPS_4\n    r11, r12, r13, r21, r22, r23, r31, r32, r33 = M.flat\n    # cy: sqrt((cos(y)*cos(z))**2 + (cos(x)*cos(y))**2)\n    cy = math.sqrt(r33*r33 + r23*r23)\n    if cy > cy_thresh: # cos(y) not close to zero, standard form\n        z = math.atan2(-r12,  r11) # atan2(cos(y)*sin(z), cos(y)*cos(z))\n        y = math.atan2(r13,  cy) # atan2(sin(y), cy)\n        x = math.atan2(-r23, r33) # atan2(cos(y)*sin(x), cos(x)*cos(y))\n    else: # cos(y) (close to) zero, so x -> 0.0 (see above)\n        # so r21 -> sin(z), r22 -> cos(z) and\n        z = math.atan2(r21,  r22)\n        y = math.atan2(r13,  cy) # atan2(sin(y), cy)\n        x = 0.0\n    return z, y, x\n\n\ndef euler2quat(z=0, y=0, x=0):\n    \'\'\' Return quaternion corresponding to these Euler angles\n    Uses the z, then y, then x convention above\n    Parameters\n    ----------\n    z : scalar\n       Rotation angle in radians around z-axis (performed first)\n    y : scalar\n       Rotation angle in radians around y-axis\n    x : scalar\n       Rotation angle in radians around x-axis (performed last)\n    Returns\n    -------\n    quat : array shape (4,)\n       Quaternion in w, x, y z (real, then vector) format\n    Notes\n    -----\n    We can derive this formula in Sympy using:\n    1. Formula giving quaternion corresponding to rotation of theta radians\n       about arbitrary axis:\n       http://mathworld.wolfram.com/EulerParameters.html\n    2. Generated formulae from 1.) for quaternions corresponding to\n       theta radians rotations about ``x, y, z`` axes\n    3. Apply quaternion multiplication formula -\n       http://en.wikipedia.org/wiki/Quaternions#Hamilton_product - to\n       formulae from 2.) to give formula for combined rotations.\n    \'\'\'\n    z = z/2.0\n    y = y/2.0\n    x = x/2.0\n    cz = math.cos(z)\n    sz = math.sin(z)\n    cy = math.cos(y)\n    sy = math.sin(y)\n    cx = math.cos(x)\n    sx = math.sin(x)\n    return np.array([\n             cx*cy*cz - sx*sy*sz,\n             cx*sy*sz + cy*cz*sx,\n             cx*cz*sy - sx*cy*sz,\n             cx*cy*sz + sx*cz*sy])\n\n\ndef quat2euler(q):\n    \'\'\' Return Euler angles corresponding to quaternion `q`\n    Parameters\n    ----------\n    q : 4 element sequence\n       w, x, y, z of quaternion\n    Returns\n    -------\n    z : scalar\n       Rotation angle in radians around z-axis (performed first)\n    y : scalar\n       Rotation angle in radians around y-axis\n    x : scalar\n       Rotation angle in radians around x-axis (performed last)\n    Notes\n    -----\n    It\'s possible to reduce the amount of calculation a little, by\n    combining parts of the ``quat2mat`` and ``mat2euler`` functions, but\n    the reduction in computation is small, and the code repetition is\n    large.\n    \'\'\'\n    # delayed import to avoid cyclic dependencies\n    import nibabel.quaternions as nq\n    return mat2euler(nq.quat2mat(q))\n\n\ndef euler2angle_axis(z=0, y=0, x=0):\n    \'\'\' Return angle, axis corresponding to these Euler angles\n    Uses the z, then y, then x convention above\n    Parameters\n    ----------\n    z : scalar\n       Rotation angle in radians around z-axis (performed first)\n    y : scalar\n       Rotation angle in radians around y-axis\n    x : scalar\n       Rotation angle in radians around x-axis (performed last)\n    Returns\n    -------\n    theta : scalar\n       angle of rotation\n    vector : array shape (3,)\n       axis around which rotation occurs\n    Examples\n    --------\n    >>> theta, vec = euler2angle_axis(0, 1.5, 0)\n    >>> print(theta)\n    1.5\n    >>> np.allclose(vec, [0, 1, 0])\n    True\n    \'\'\'\n    # delayed import to avoid cyclic dependencies\n    import nibabel.quaternions as nq\n    return nq.quat2angle_axis(euler2quat(z, y, x))\n\n\ndef angle_axis2euler(theta, vector, is_normalized=False):\n    \'\'\' Convert angle, axis pair to Euler angles\n    Parameters\n    ----------\n    theta : scalar\n       angle of rotation\n    vector : 3 element sequence\n       vector specifying axis for rotation.\n    is_normalized : bool, optional\n       True if vector is already normalized (has norm of 1).  Default\n       False\n    Returns\n    -------\n    z : scalar\n    y : scalar\n    x : scalar\n       Rotations in radians around z, y, x axes, respectively\n    Examples\n    --------\n    >>> z, y, x = angle_axis2euler(0, [1, 0, 0])\n    >>> np.allclose((z, y, x), 0)\n    True\n    Notes\n    -----\n    It\'s possible to reduce the amount of calculation a little, by\n    combining parts of the ``angle_axis2mat`` and ``mat2euler``\n    functions, but the reduction in computation is small, and the code\n    repetition is large.\n    \'\'\'\n    # delayed import to avoid cyclic dependencies\n    import nibabel.quaternions as nq\n    M = nq.angle_axis2mat(theta, vector, is_normalized)\n    return mat2euler(M)'"
visualizer/pc_utils.py,0,"b'"""""" Utility functions for processing point clouds.\nAuthor: Charles R. Qi, Hao Su\nDate: November 2016\n""""""\n\nimport os\nimport sys\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\n\n# Draw point cloud\nfrom visualizer.eulerangles import euler2mat\n\n# Point cloud IO\nimport numpy as np\nfrom visualizer.plyfile import PlyData, PlyElement\n\n# ----------------------------------------\n# Point Cloud/Volume Conversions\n# ----------------------------------------\n\ndef point_cloud_to_volume_batch(point_clouds, vsize=12, radius=1.0, flatten=True):\n    """""" Input is BxNx3 batch of point cloud\n        Output is Bx(vsize^3)\n    """"""\n    vol_list = []\n    for b in range(point_clouds.shape[0]):\n        vol = point_cloud_to_volume(np.squeeze(point_clouds[b, :, :]), vsize, radius)\n        if flatten:\n            vol_list.append(vol.flatten())\n        else:\n            vol_list.append(np.expand_dims(np.expand_dims(vol, -1), 0))\n    if flatten:\n        return np.vstack(vol_list)\n    else:\n        return np.concatenate(vol_list, 0)\n\n\ndef point_cloud_to_volume(points, vsize, radius=1.0):\n    """""" input is Nx3 points.\n        output is vsize*vsize*vsize\n        assumes points are in range [-radius, radius]\n    """"""\n    vol = np.zeros((vsize, vsize, vsize))\n    voxel = 2 * radius / float(vsize)\n    locations = (points + radius) / voxel\n    locations = locations.astype(int)\n    vol[locations[:, 0], locations[:, 1], locations[:, 2]] = 1.0\n    return vol\n\n\n# a = np.zeros((16,1024,3))\n# print point_cloud_to_volume_batch(a, 12, 1.0, False).shape\n\ndef volume_to_point_cloud(vol):\n    """""" vol is occupancy grid (value = 0 or 1) of size vsize*vsize*vsize\n        return Nx3 numpy array.\n    """"""\n    vsize = vol.shape[0]\n    assert (vol.shape[1] == vsize and vol.shape[1] == vsize)\n    points = []\n    for a in range(vsize):\n        for b in range(vsize):\n            for c in range(vsize):\n                if vol[a, b, c] == 1:\n                    points.append(np.array([a, b, c]))\n    if len(points) == 0:\n        return np.zeros((0, 3))\n    points = np.vstack(points)\n    return points\n\n\n# ----------------------------------------\n# Point cloud IO\n# ----------------------------------------\n\ndef read_ply(filename):\n    """""" read XYZ point cloud from filename PLY file """"""\n    plydata = PlyData.read(filename)\n    pc = plydata[\'vertex\'].data\n    pc_array = np.array([[x, y, z] for x, y, z in pc])\n    return pc_array\n\n\ndef write_ply(points, filename, text=True):\n    """""" input: Nx3, write points to filename as PLY format. """"""\n    points = [(points[i, 0], points[i, 1], points[i, 2]) for i in range(points.shape[0])]\n    vertex = np.array(points, dtype=[(\'x\', \'f4\'), (\'y\', \'f4\'), (\'z\', \'f4\')])\n    el = PlyElement.describe(vertex, \'vertex\', comments=[\'vertices\'])\n    PlyData([el], text=text).write(filename)\n\n\n# ----------------------------------------\n# Simple Point cloud and Volume Renderers\n# ----------------------------------------\n\ndef draw_point_cloud(input_points, canvasSize=500, space=200, diameter=25,\n                     xrot=0, yrot=0, zrot=0, switch_xyz=[0, 1, 2], normalize=True):\n    """""" Render point cloud to image with alpha channel.\n        Input:\n            points: Nx3 numpy array (+y is up direction)\n        Output:\n            gray image as numpy array of size canvasSizexcanvasSize\n    """"""\n    image = np.zeros((canvasSize, canvasSize))\n    if input_points is None or input_points.shape[0] == 0:\n        return image\n\n    points = input_points[:, switch_xyz]\n    M = euler2mat(zrot, yrot, xrot)\n    points = (np.dot(M, points.transpose())).transpose()\n\n    # Normalize the point cloud\n    # We normalize scale to fit points in a unit sphere\n    if normalize:\n        centroid = np.mean(points, axis=0)\n        points -= centroid\n        furthest_distance = np.max(np.sqrt(np.sum(abs(points) ** 2, axis=-1)))\n        points /= furthest_distance\n\n    # Pre-compute the Gaussian disk\n    radius = (diameter - 1) / 2.0\n    disk = np.zeros((diameter, diameter))\n    for i in range(diameter):\n        for j in range(diameter):\n            if (i - radius) * (i - radius) + (j - radius) * (j - radius) <= radius * radius:\n                disk[i, j] = np.exp((-(i - radius) ** 2 - (j - radius) ** 2) / (radius ** 2))\n    mask = np.argwhere(disk > 0)\n    dx = mask[:, 0]\n    dy = mask[:, 1]\n    dv = disk[disk > 0]\n\n    # Order points by z-buffer\n    zorder = np.argsort(points[:, 2])\n    points = points[zorder, :]\n    points[:, 2] = (points[:, 2] - np.min(points[:, 2])) / (np.max(points[:, 2] - np.min(points[:, 2])))\n    max_depth = np.max(points[:, 2])\n\n    for i in range(points.shape[0]):\n        j = points.shape[0] - i - 1\n        x = points[j, 0]\n        y = points[j, 1]\n        xc = canvasSize / 2 + (x * space)\n        yc = canvasSize / 2 + (y * space)\n        xc = int(np.round(xc))\n        yc = int(np.round(yc))\n\n        px = dx + xc\n        py = dy + yc\n\n        image[px, py] = image[px, py] * 0.7 + dv * (max_depth - points[j, 2]) * 0.3\n\n    image = image / np.max(image)\n    return image\n\n\ndef point_cloud_three_views(points):\n    """""" input points Nx3 numpy array (+y is up direction).\n        return an numpy array gray image of size 500x1500. """"""\n    # +y is up direction\n    # xrot is azimuth\n    # yrot is in-plane\n    # zrot is elevation\n    img1 = draw_point_cloud(points, zrot=110 / 180.0 * np.pi, xrot=45 / 180.0 * np.pi, yrot=0 / 180.0 * np.pi)\n    img2 = draw_point_cloud(points, zrot=70 / 180.0 * np.pi, xrot=135 / 180.0 * np.pi, yrot=0 / 180.0 * np.pi)\n    img3 = draw_point_cloud(points, zrot=180.0 / 180.0 * np.pi, xrot=90 / 180.0 * np.pi, yrot=0 / 180.0 * np.pi)\n    image_large = np.concatenate([img1, img2, img3], 1)\n    return image_large\n\n\nfrom PIL import Image\n\n\ndef point_cloud_three_views_demo():\n    """""" Demo for draw_point_cloud function """"""\n    DATA_PATH = \'../data/ShapeNet/\'\n    train_data, _, _, _, _, _ = load_data(DATA_PATH,classification=False)\n    points = train_data[1]\n    im_array = point_cloud_three_views(points)\n    img = Image.fromarray(np.uint8(im_array * 255.0))\n    img.save(\'example.jpg\')\n\n\nif __name__ == ""__main__"":\n    from data_utils.ShapeNetDataLoader import load_data\n    point_cloud_three_views_demo()\n\nimport matplotlib.pyplot as plt\n\n\ndef pyplot_draw_point_cloud(points, output_filename):\n    """""" points is a Nx3 numpy array """"""\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=\'3d\')\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n    ax.set_xlabel(\'x\')\n    ax.set_ylabel(\'y\')\n    ax.set_zlabel(\'z\')\n    # savefig(output_filename)\n\n\ndef pyplot_draw_volume(vol, output_filename):\n    """""" vol is of size vsize*vsize*vsize\n        output an image to output_filename\n    """"""\n    points = volume_to_point_cloud(vol)\n    pyplot_draw_point_cloud(points, output_filename)\n\n\n\n'"
visualizer/plyfile.py,0,"b'#   Copyright 2014 Darsh Ranjan\n#\n#   This file is part of python-plyfile.\n#\n#   python-plyfile is free software: you can redistribute it and/or\n#   modify it under the terms of the GNU General Public License as\n#   published by the Free Software Foundation, either version 3 of the\n#   License, or (at your option) any later version.\n#\n#   python-plyfile is distributed in the hope that it will be useful,\n#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n#   General Public License for more details.\n#\n#   You should have received a copy of the GNU General Public License\n#   along with python-plyfile.  If not, see\n#       <http://www.gnu.org/licenses/>.\n\nfrom itertools import islice as _islice\n\nimport numpy as _np\nfrom sys import byteorder as _byteorder\n\n\ntry:\n    _range = xrange\nexcept NameError:\n    _range = range\n\n\n# Many-many relation\n_data_type_relation = [\n    (\'int8\', \'i1\'),\n    (\'char\', \'i1\'),\n    (\'uint8\', \'u1\'),\n    (\'uchar\', \'b1\'),\n    (\'uchar\', \'u1\'),\n    (\'int16\', \'i2\'),\n    (\'short\', \'i2\'),\n    (\'uint16\', \'u2\'),\n    (\'ushort\', \'u2\'),\n    (\'int32\', \'i4\'),\n    (\'int\', \'i4\'),\n    (\'uint32\', \'u4\'),\n    (\'uint\', \'u4\'),\n    (\'float32\', \'f4\'),\n    (\'float\', \'f4\'),\n    (\'float64\', \'f8\'),\n    (\'double\', \'f8\')\n]\n\n_data_types = dict(_data_type_relation)\n_data_type_reverse = dict((b, a) for (a, b) in _data_type_relation)\n\n_types_list = []\n_types_set = set()\nfor (_a, _b) in _data_type_relation:\n    if _a not in _types_set:\n        _types_list.append(_a)\n        _types_set.add(_a)\n    if _b not in _types_set:\n        _types_list.append(_b)\n        _types_set.add(_b)\n\n\n_byte_order_map = {\n    \'ascii\': \'=\',\n    \'binary_little_endian\': \'<\',\n    \'binary_big_endian\': \'>\'\n}\n\n_byte_order_reverse = {\n    \'<\': \'binary_little_endian\',\n    \'>\': \'binary_big_endian\'\n}\n\n_native_byte_order = {\'little\': \'<\', \'big\': \'>\'}[_byteorder]\n\n\ndef _lookup_type(type_str):\n    if type_str not in _data_type_reverse:\n        try:\n            type_str = _data_types[type_str]\n        except KeyError:\n            raise ValueError(""field type %r not in %r"" %\n                             (type_str, _types_list))\n\n    return _data_type_reverse[type_str]\n\n\ndef _split_line(line, n):\n    fields = line.split(None, n)\n    if len(fields) == n:\n        fields.append(\'\')\n\n    assert len(fields) == n + 1\n\n    return fields\n\n\ndef make2d(array, cols=None, dtype=None):\n    \'\'\'\n    Make a 2D array from an array of arrays.  The `cols\' and `dtype\'\n    arguments can be omitted if the array is not empty.\n    \'\'\'\n    if (cols is None or dtype is None) and not len(array):\n        raise RuntimeError(""cols and dtype must be specified for empty ""\n                           ""array"")\n\n    if cols is None:\n        cols = len(array[0])\n\n    if dtype is None:\n        dtype = array[0].dtype\n\n    return _np.fromiter(array, [(\'_\', dtype, (cols,))],\n                        count=len(array))[\'_\']\n\n\nclass PlyParseError(Exception):\n\n    \'\'\'\n    Raised when a PLY file cannot be parsed.\n    The attributes `element\', `row\', `property\', and `message\' give\n    additional information.\n    \'\'\'\n\n    def __init__(self, message, element=None, row=None, prop=None):\n        self.message = message\n        self.element = element\n        self.row = row\n        self.prop = prop\n\n        s = \'\'\n        if self.element:\n            s += \'element %r: \' % self.element.name\n        if self.row is not None:\n            s += \'row %d: \' % self.row\n        if self.prop:\n            s += \'property %r: \' % self.prop.name\n        s += self.message\n\n        Exception.__init__(self, s)\n\n    def __repr__(self):\n        return (\'PlyParseError(%r, element=%r, row=%r, prop=%r)\' %\n                self.message, self.element, self.row, self.prop)\n\n\nclass PlyData(object):\n\n    \'\'\'\n    PLY file header and data.\n    A PlyData instance is created in one of two ways: by the static\n    method PlyData.read (to read a PLY file), or directly from __init__\n    given a sequence of elements (which can then be written to a PLY\n    file).\n    \'\'\'\n\n    def __init__(self, elements=[], text=False, byte_order=\'=\',\n                 comments=[], obj_info=[]):\n        \'\'\'\n        elements: sequence of PlyElement instances.\n        text: whether the resulting PLY file will be text (True) or\n            binary (False).\n        byte_order: \'<\' for little-endian, \'>\' for big-endian, or \'=\'\n            for native.  This is only relevant if `text\' is False.\n        comments: sequence of strings that will be placed in the header\n            between the \'ply\' and \'format ...\' lines.\n        obj_info: like comments, but will be placed in the header with\n            ""obj_info ..."" instead of ""comment ..."".\n        \'\'\'\n        if byte_order == \'=\' and not text:\n            byte_order = _native_byte_order\n\n        self.byte_order = byte_order\n        self.text = text\n\n        self.comments = list(comments)\n        self.obj_info = list(obj_info)\n        self.elements = elements\n\n    def _get_elements(self):\n        return self._elements\n\n    def _set_elements(self, elements):\n        self._elements = tuple(elements)\n        self._index()\n\n    elements = property(_get_elements, _set_elements)\n\n    def _get_byte_order(self):\n        return self._byte_order\n\n    def _set_byte_order(self, byte_order):\n        if byte_order not in [\'<\', \'>\', \'=\']:\n            raise ValueError(""byte order must be \'<\', \'>\', or \'=\'"")\n\n        self._byte_order = byte_order\n\n    byte_order = property(_get_byte_order, _set_byte_order)\n\n    def _index(self):\n        self._element_lookup = dict((elt.name, elt) for elt in\n                                    self._elements)\n        if len(self._element_lookup) != len(self._elements):\n            raise ValueError(""two elements with same name"")\n\n    @staticmethod\n    def _parse_header(stream):\n        \'\'\'\n        Parse a PLY header from a readable file-like stream.\n        \'\'\'\n        lines = []\n        comments = {\'comment\': [], \'obj_info\': []}\n        while True:\n            line = stream.readline().decode(\'ascii\').strip()\n            fields = _split_line(line, 1)\n\n            if fields[0] == \'end_header\':\n                break\n\n            elif fields[0] in comments.keys():\n                lines.append(fields)\n            else:\n                lines.append(line.split())\n\n        a = 0\n        if lines[a] != [\'ply\']:\n            raise PlyParseError(""expected \'ply\'"")\n\n        a += 1\n        while lines[a][0] in comments.keys():\n            comments[lines[a][0]].append(lines[a][1])\n            a += 1\n\n        if lines[a][0] != \'format\':\n            raise PlyParseError(""expected \'format\'"")\n\n        if lines[a][2] != \'1.0\':\n            raise PlyParseError(""expected version \'1.0\'"")\n\n        if len(lines[a]) != 3:\n            raise PlyParseError(""too many fields after \'format\'"")\n\n        fmt = lines[a][1]\n\n        if fmt not in _byte_order_map:\n            raise PlyParseError(""don\'t understand format %r"" % fmt)\n\n        byte_order = _byte_order_map[fmt]\n        text = fmt == \'ascii\'\n\n        a += 1\n        while a < len(lines) and lines[a][0] in comments.keys():\n            comments[lines[a][0]].append(lines[a][1])\n            a += 1\n\n        return PlyData(PlyElement._parse_multi(lines[a:]),\n                       text, byte_order,\n                       comments[\'comment\'], comments[\'obj_info\'])\n\n    @staticmethod\n    def read(stream):\n        \'\'\'\n        Read PLY data from a readable file-like object or filename.\n        \'\'\'\n        (must_close, stream) = _open_stream(stream, \'read\')\n        try:\n            data = PlyData._parse_header(stream)\n            for elt in data:\n                elt._read(stream, data.text, data.byte_order)\n        finally:\n            if must_close:\n                stream.close()\n\n        return data\n\n    def write(self, stream):\n        \'\'\'\n        Write PLY data to a writeable file-like object or filename.\n        \'\'\'\n        (must_close, stream) = _open_stream(stream, \'write\')\n        try:\n            stream.write(self.header.encode(\'ascii\'))\n            stream.write(b\'\\r\\n\')\n            for elt in self:\n                elt._write(stream, self.text, self.byte_order)\n        finally:\n            if must_close:\n                stream.close()\n\n    @property\n    def header(self):\n        \'\'\'\n        Provide PLY-formatted metadata for the instance.\n        \'\'\'\n        lines = [\'ply\']\n\n        if self.text:\n            lines.append(\'format ascii 1.0\')\n        else:\n            lines.append(\'format \' +\n                         _byte_order_reverse[self.byte_order] +\n                         \' 1.0\')\n\n        # Some information is lost here, since all comments are placed\n        # between the \'format\' line and the first element.\n        for c in self.comments:\n            lines.append(\'comment \' + c)\n\n        for c in self.obj_info:\n            lines.append(\'obj_info \' + c)\n\n        lines.extend(elt.header for elt in self.elements)\n        lines.append(\'end_header\')\n        return \'\\r\\n\'.join(lines)\n\n    def __iter__(self):\n        return iter(self.elements)\n\n    def __len__(self):\n        return len(self.elements)\n\n    def __contains__(self, name):\n        return name in self._element_lookup\n\n    def __getitem__(self, name):\n        return self._element_lookup[name]\n\n    def __str__(self):\n        return self.header\n\n    def __repr__(self):\n        return (\'PlyData(%r, text=%r, byte_order=%r, \'\n                \'comments=%r, obj_info=%r)\' %\n                (self.elements, self.text, self.byte_order,\n                 self.comments, self.obj_info))\n\n\ndef _open_stream(stream, read_or_write):\n    if hasattr(stream, read_or_write):\n        return (False, stream)\n    try:\n        return (True, open(stream, read_or_write[0] + \'b\'))\n    except TypeError:\n        raise RuntimeError(""expected open file or filename"")\n\n\nclass PlyElement(object):\n\n    \'\'\'\n    PLY file element.\n    A client of this library doesn\'t normally need to instantiate this\n    directly, so the following is only for the sake of documenting the\n    internals.\n    Creating a PlyElement instance is generally done in one of two ways:\n    as a byproduct of PlyData.read (when reading a PLY file) and by\n    PlyElement.describe (before writing a PLY file).\n    \'\'\'\n\n    def __init__(self, name, properties, count, comments=[]):\n        \'\'\'\n        This is not part of the public interface.  The preferred methods\n        of obtaining PlyElement instances are PlyData.read (to read from\n        a file) and PlyElement.describe (to construct from a numpy\n        array).\n        \'\'\'\n        self._name = str(name)\n        self._check_name()\n        self._count = count\n\n        self._properties = tuple(properties)\n        self._index()\n\n        self.comments = list(comments)\n\n        self._have_list = any(isinstance(p, PlyListProperty)\n                              for p in self.properties)\n\n    @property\n    def count(self):\n        return self._count\n\n    def _get_data(self):\n        return self._data\n\n    def _set_data(self, data):\n        self._data = data\n        self._count = len(data)\n        self._check_sanity()\n\n    data = property(_get_data, _set_data)\n\n    def _check_sanity(self):\n        for prop in self.properties:\n            if prop.name not in self._data.dtype.fields:\n                raise ValueError(""dangling property %r"" % prop.name)\n\n    def _get_properties(self):\n        return self._properties\n\n    def _set_properties(self, properties):\n        self._properties = tuple(properties)\n        self._check_sanity()\n        self._index()\n\n    properties = property(_get_properties, _set_properties)\n\n    def _index(self):\n        self._property_lookup = dict((prop.name, prop)\n                                     for prop in self._properties)\n        if len(self._property_lookup) != len(self._properties):\n            raise ValueError(""two properties with same name"")\n\n    def ply_property(self, name):\n        return self._property_lookup[name]\n\n    @property\n    def name(self):\n        return self._name\n\n    def _check_name(self):\n        if any(c.isspace() for c in self._name):\n            msg = ""element name %r contains spaces"" % self._name\n            raise ValueError(msg)\n\n    def dtype(self, byte_order=\'=\'):\n        \'\'\'\n        Return the numpy dtype of the in-memory representation of the\n        data.  (If there are no list properties, and the PLY format is\n        binary, then this also accurately describes the on-disk\n        representation of the element.)\n        \'\'\'\n        return [(prop.name, prop.dtype(byte_order))\n                for prop in self.properties]\n\n    @staticmethod\n    def _parse_multi(header_lines):\n        \'\'\'\n        Parse a list of PLY element definitions.\n        \'\'\'\n        elements = []\n        while header_lines:\n            (elt, header_lines) = PlyElement._parse_one(header_lines)\n            elements.append(elt)\n\n        return elements\n\n    @staticmethod\n    def _parse_one(lines):\n        \'\'\'\n        Consume one element definition.  The unconsumed input is\n        returned along with a PlyElement instance.\n        \'\'\'\n        a = 0\n        line = lines[a]\n\n        if line[0] != \'element\':\n            raise PlyParseError(""expected \'element\'"")\n        if len(line) > 3:\n            raise PlyParseError(""too many fields after \'element\'"")\n        if len(line) < 3:\n            raise PlyParseError(""too few fields after \'element\'"")\n\n        (name, count) = (line[1], int(line[2]))\n\n        comments = []\n        properties = []\n        while True:\n            a += 1\n            if a >= len(lines):\n                break\n\n            if lines[a][0] == \'comment\':\n                comments.append(lines[a][1])\n            elif lines[a][0] == \'property\':\n                properties.append(PlyProperty._parse_one(lines[a]))\n            else:\n                break\n\n        return (PlyElement(name, properties, count, comments),\n                lines[a:])\n\n    @staticmethod\n    def describe(data, name, len_types={}, val_types={},\n                 comments=[]):\n        \'\'\'\n        Construct a PlyElement from an array\'s metadata.\n        len_types and val_types can be given as mappings from list\n        property names to type strings (like \'u1\', \'f4\', etc., or\n        \'int8\', \'float32\', etc.). These can be used to define the length\n        and value types of list properties.  List property lengths\n        always default to type \'u1\' (8-bit unsigned integer), and value\n        types default to \'i4\' (32-bit integer).\n        \'\'\'\n        if not isinstance(data, _np.ndarray):\n            raise TypeError(""only numpy arrays are supported"")\n\n        if len(data.shape) != 1:\n            raise ValueError(""only one-dimensional arrays are ""\n                             ""supported"")\n\n        count = len(data)\n\n        properties = []\n        descr = data.dtype.descr\n\n        for t in descr:\n            if not isinstance(t[1], str):\n                raise ValueError(""nested records not supported"")\n\n            if not t[0]:\n                raise ValueError(""field with empty name"")\n\n            if len(t) != 2 or t[1][1] == \'O\':\n                # non-scalar field, which corresponds to a list\n                # property in PLY.\n\n                if t[1][1] == \'O\':\n                    if len(t) != 2:\n                        raise ValueError(""non-scalar object fields not ""\n                                         ""supported"")\n\n                len_str = _data_type_reverse[len_types.get(t[0], \'u1\')]\n                if t[1][1] == \'O\':\n                    val_type = val_types.get(t[0], \'i4\')\n                    val_str = _lookup_type(val_type)\n                else:\n                    val_str = _lookup_type(t[1][1:])\n\n                prop = PlyListProperty(t[0], len_str, val_str)\n            else:\n                val_str = _lookup_type(t[1][1:])\n                prop = PlyProperty(t[0], val_str)\n\n            properties.append(prop)\n\n        elt = PlyElement(name, properties, count, comments)\n        elt.data = data\n\n        return elt\n\n    def _read(self, stream, text, byte_order):\n        \'\'\'\n        Read the actual data from a PLY file.\n        \'\'\'\n        if text:\n            self._read_txt(stream)\n        else:\n            if self._have_list:\n                # There are list properties, so a simple load is\n                # impossible.\n                self._read_bin(stream, byte_order)\n            else:\n                # There are no list properties, so loading the data is\n                # much more straightforward.\n                self._data = _np.fromfile(stream,\n                                          self.dtype(byte_order),\n                                          self.count)\n\n        if len(self._data) < self.count:\n            k = len(self._data)\n            del self._data\n            raise PlyParseError(""early end-of-file"", self, k)\n\n        self._check_sanity()\n\n    def _write(self, stream, text, byte_order):\n        \'\'\'\n        Write the data to a PLY file.\n        \'\'\'\n        if text:\n            self._write_txt(stream)\n        else:\n            if self._have_list:\n                # There are list properties, so serialization is\n                # slightly complicated.\n                self._write_bin(stream, byte_order)\n            else:\n                # no list properties, so serialization is\n                # straightforward.\n                self.data.astype(self.dtype(byte_order),\n                                 copy=False).tofile(stream)\n\n    def _read_txt(self, stream):\n        \'\'\'\n        Load a PLY element from an ASCII-format PLY file.  The element\n        may contain list properties.\n        \'\'\'\n        self._data = _np.empty(self.count, dtype=self.dtype())\n\n        k = 0\n        for line in _islice(iter(stream.readline, b\'\'), self.count):\n            fields = iter(line.strip().split())\n            for prop in self.properties:\n                try:\n                    self._data[prop.name][k] = prop._from_fields(fields)\n                except StopIteration:\n                    raise PlyParseError(""early end-of-line"",\n                                        self, k, prop)\n                except ValueError:\n                    raise PlyParseError(""malformed input"",\n                                        self, k, prop)\n            try:\n                next(fields)\n            except StopIteration:\n                pass\n            else:\n                raise PlyParseError(""expected end-of-line"", self, k)\n            k += 1\n\n        if k < self.count:\n            del self._data\n            raise PlyParseError(""early end-of-file"", self, k)\n\n    def _write_txt(self, stream):\n        \'\'\'\n        Save a PLY element to an ASCII-format PLY file.  The element may\n        contain list properties.\n        \'\'\'\n        for rec in self.data:\n            fields = []\n            for prop in self.properties:\n                fields.extend(prop._to_fields(rec[prop.name]))\n\n            _np.savetxt(stream, [fields], \'%.18g\', newline=\'\\r\\n\')\n\n    def _read_bin(self, stream, byte_order):\n        \'\'\'\n        Load a PLY element from a binary PLY file.  The element may\n        contain list properties.\n        \'\'\'\n        self._data = _np.empty(self.count, dtype=self.dtype(byte_order))\n\n        for k in _range(self.count):\n            for prop in self.properties:\n                try:\n                    self._data[prop.name][k] = \\\n                        prop._read_bin(stream, byte_order)\n                except StopIteration:\n                    raise PlyParseError(""early end-of-file"",\n                                        self, k, prop)\n\n    def _write_bin(self, stream, byte_order):\n        \'\'\'\n        Save a PLY element to a binary PLY file.  The element may\n        contain list properties.\n        \'\'\'\n        for rec in self.data:\n            for prop in self.properties:\n                prop._write_bin(rec[prop.name], stream, byte_order)\n\n    @property\n    def header(self):\n        \'\'\'\n        Format this element\'s metadata as it would appear in a PLY\n        header.\n        \'\'\'\n        lines = [\'element %s %d\' % (self.name, self.count)]\n\n        # Some information is lost here, since all comments are placed\n        # between the \'element\' line and the first property definition.\n        for c in self.comments:\n            lines.append(\'comment \' + c)\n\n        lines.extend(list(map(str, self.properties)))\n\n        return \'\\r\\n\'.join(lines)\n\n    def __getitem__(self, key):\n        return self.data[key]\n\n    def __setitem__(self, key, value):\n        self.data[key] = value\n\n    def __str__(self):\n        return self.header\n\n    def __repr__(self):\n        return (\'PlyElement(%r, %r, count=%d, comments=%r)\' %\n                (self.name, self.properties, self.count,\n                 self.comments))\n\n\nclass PlyProperty(object):\n\n    \'\'\'\n    PLY property description.  This class is pure metadata; the data\n    itself is contained in PlyElement instances.\n    \'\'\'\n\n    def __init__(self, name, val_dtype):\n        self._name = str(name)\n        self._check_name()\n        self.val_dtype = val_dtype\n\n    def _get_val_dtype(self):\n        return self._val_dtype\n\n    def _set_val_dtype(self, val_dtype):\n        self._val_dtype = _data_types[_lookup_type(val_dtype)]\n\n    val_dtype = property(_get_val_dtype, _set_val_dtype)\n\n    @property\n    def name(self):\n        return self._name\n\n    def _check_name(self):\n        if any(c.isspace() for c in self._name):\n            msg = ""Error: property name %r contains spaces"" % self._name\n            raise RuntimeError(msg)\n\n    @staticmethod\n    def _parse_one(line):\n        assert line[0] == \'property\'\n\n        if line[1] == \'list\':\n            if len(line) > 5:\n                raise PlyParseError(""too many fields after ""\n                                    ""\'property list\'"")\n            if len(line) < 5:\n                raise PlyParseError(""too few fields after ""\n                                    ""\'property list\'"")\n\n            return PlyListProperty(line[4], line[2], line[3])\n\n        else:\n            if len(line) > 3:\n                raise PlyParseError(""too many fields after ""\n                                    ""\'property\'"")\n            if len(line) < 3:\n                raise PlyParseError(""too few fields after ""\n                                    ""\'property\'"")\n\n            return PlyProperty(line[2], line[1])\n\n    def dtype(self, byte_order=\'=\'):\n        \'\'\'\n        Return the numpy dtype description for this property (as a tuple\n        of strings).\n        \'\'\'\n        return byte_order + self.val_dtype\n\n    def _from_fields(self, fields):\n        \'\'\'\n        Parse from generator.  Raise StopIteration if the property could\n        not be read.\n        \'\'\'\n        return _np.dtype(self.dtype()).type(next(fields))\n\n    def _to_fields(self, data):\n        \'\'\'\n        Return generator over one item.\n        \'\'\'\n        yield _np.dtype(self.dtype()).type(data)\n\n    def _read_bin(self, stream, byte_order):\n        \'\'\'\n        Read data from a binary stream.  Raise StopIteration if the\n        property could not be read.\n        \'\'\'\n        try:\n            return _np.fromfile(stream, self.dtype(byte_order), 1)[0]\n        except IndexError:\n            raise StopIteration\n\n    def _write_bin(self, data, stream, byte_order):\n        \'\'\'\n        Write data to a binary stream.\n        \'\'\'\n        _np.dtype(self.dtype(byte_order)).type(data).tofile(stream)\n\n    def __str__(self):\n        val_str = _data_type_reverse[self.val_dtype]\n        return \'property %s %s\' % (val_str, self.name)\n\n    def __repr__(self):\n        return \'PlyProperty(%r, %r)\' % (self.name,\n                                        _lookup_type(self.val_dtype))\n\n\nclass PlyListProperty(PlyProperty):\n\n    \'\'\'\n    PLY list property description.\n    \'\'\'\n\n    def __init__(self, name, len_dtype, val_dtype):\n        PlyProperty.__init__(self, name, val_dtype)\n\n        self.len_dtype = len_dtype\n\n    def _get_len_dtype(self):\n        return self._len_dtype\n\n    def _set_len_dtype(self, len_dtype):\n        self._len_dtype = _data_types[_lookup_type(len_dtype)]\n\n    len_dtype = property(_get_len_dtype, _set_len_dtype)\n\n    def dtype(self, byte_order=\'=\'):\n        \'\'\'\n        List properties always have a numpy dtype of ""object"".\n        \'\'\'\n        return \'|O\'\n\n    def list_dtype(self, byte_order=\'=\'):\n        \'\'\'\n        Return the pair (len_dtype, val_dtype) (both numpy-friendly\n        strings).\n        \'\'\'\n        return (byte_order + self.len_dtype,\n                byte_order + self.val_dtype)\n\n    def _from_fields(self, fields):\n        (len_t, val_t) = self.list_dtype()\n\n        n = int(_np.dtype(len_t).type(next(fields)))\n\n        data = _np.loadtxt(list(_islice(fields, n)), val_t, ndmin=1)\n        if len(data) < n:\n            raise StopIteration\n\n        return data\n\n    def _to_fields(self, data):\n        \'\'\'\n        Return generator over the (numerical) PLY representation of the\n        list data (length followed by actual data).\n        \'\'\'\n        (len_t, val_t) = self.list_dtype()\n\n        data = _np.asarray(data, dtype=val_t).ravel()\n\n        yield _np.dtype(len_t).type(data.size)\n        for x in data:\n            yield x\n\n    def _read_bin(self, stream, byte_order):\n        (len_t, val_t) = self.list_dtype(byte_order)\n\n        try:\n            n = _np.fromfile(stream, len_t, 1)[0]\n        except IndexError:\n            raise StopIteration\n\n        data = _np.fromfile(stream, val_t, n)\n        if len(data) < n:\n            raise StopIteration\n\n        return data\n\n    def _write_bin(self, data, stream, byte_order):\n        \'\'\'\n        Write data to a binary stream.\n        \'\'\'\n        (len_t, val_t) = self.list_dtype(byte_order)\n\n        data = _np.asarray(data, dtype=val_t).ravel()\n\n        _np.array(data.size, dtype=len_t).tofile(stream)\n        data.tofile(stream)\n\n    def __str__(self):\n        len_str = _data_type_reverse[self.len_dtype]\n        val_str = _data_type_reverse[self.val_dtype]\n        return \'property list %s %s %s\' % (len_str, val_str, self.name)\n\n    def __repr__(self):\n        return (\'PlyListProperty(%r, %r, %r)\' %\n                (self.name,\n                 _lookup_type(self.len_dtype),\n                 _lookup_type(self.val_dtype)))\n'"
visualizer/show3d_balls.py,0,"b'"""""" Original Author: Haoqiang Fan """"""\nimport numpy as np\nimport ctypes as ct\nimport cv2\nimport sys\nimport os\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nshowsz = 800\nmousex, mousey = 0.5, 0.5\nzoom = 1.0\nchanged = True\n\n\ndef onmouse(*args):\n    global mousex, mousey, changed\n    y = args[1]\n    x = args[2]\n    mousex = x / float(showsz)\n    mousey = y / float(showsz)\n    changed = True\n\n\ncv2.namedWindow(\'show3d\')\ncv2.moveWindow(\'show3d\', 0, 0)\ncv2.setMouseCallback(\'show3d\', onmouse)\n\ndll = np.ctypeslib.load_library(os.path.join(BASE_DIR, \'render_balls_so\'), \'.\')\n\n\ndef showpoints(xyz, c_gt=None, c_pred=None, waittime=0, showrot=False, magnifyBlue=0, freezerot=False,\n               background=(0, 0, 0), normalizecolor=True, ballradius=10):\n    global showsz, mousex, mousey, zoom, changed\n    xyz = xyz - xyz.mean(axis=0)\n    radius = ((xyz ** 2).sum(axis=-1) ** 0.5).max()\n    xyz /= (radius * 2.2) / showsz\n    if c_gt is None:\n        c0 = np.zeros((len(xyz),), dtype=\'float32\') + 255\n        c1 = np.zeros((len(xyz),), dtype=\'float32\') + 255\n        c2 = np.zeros((len(xyz),), dtype=\'float32\') + 255\n    else:\n        c0 = c_gt[:, 0]\n        c1 = c_gt[:, 1]\n        c2 = c_gt[:, 2]\n\n    if normalizecolor:\n        c0 /= (c0.max() + 1e-14) / 255.0\n        c1 /= (c1.max() + 1e-14) / 255.0\n        c2 /= (c2.max() + 1e-14) / 255.0\n\n    c0 = np.require(c0, \'float32\', \'C\')\n    c1 = np.require(c1, \'float32\', \'C\')\n    c2 = np.require(c2, \'float32\', \'C\')\n\n    show = np.zeros((showsz, showsz, 3), dtype=\'uint8\')\n\n    def render():\n        rotmat = np.eye(3)\n        if not freezerot:\n            xangle = (mousey - 0.5) * np.pi * 1.2\n        else:\n            xangle = 0\n        rotmat = rotmat.dot(np.array([\n            [1.0, 0.0, 0.0],\n            [0.0, np.cos(xangle), -np.sin(xangle)],\n            [0.0, np.sin(xangle), np.cos(xangle)],\n        ]))\n        if not freezerot:\n            yangle = (mousex - 0.5) * np.pi * 1.2\n        else:\n            yangle = 0\n        rotmat = rotmat.dot(np.array([\n            [np.cos(yangle), 0.0, -np.sin(yangle)],\n            [0.0, 1.0, 0.0],\n            [np.sin(yangle), 0.0, np.cos(yangle)],\n        ]))\n        rotmat *= zoom\n        nxyz = xyz.dot(rotmat) + [showsz / 2, showsz / 2, 0]\n\n        ixyz = nxyz.astype(\'int32\')\n        show[:] = background\n        dll.render_ball(\n            ct.c_int(show.shape[0]),\n            ct.c_int(show.shape[1]),\n            show.ctypes.data_as(ct.c_void_p),\n            ct.c_int(ixyz.shape[0]),\n            ixyz.ctypes.data_as(ct.c_void_p),\n            c0.ctypes.data_as(ct.c_void_p),\n            c1.ctypes.data_as(ct.c_void_p),\n            c2.ctypes.data_as(ct.c_void_p),\n            ct.c_int(ballradius)\n        )\n\n        if magnifyBlue > 0:\n            show[:, :, 0] = np.maximum(show[:, :, 0], np.roll(show[:, :, 0], 1, axis=0))\n            if magnifyBlue >= 2:\n                show[:, :, 0] = np.maximum(show[:, :, 0], np.roll(show[:, :, 0], -1, axis=0))\n            show[:, :, 0] = np.maximum(show[:, :, 0], np.roll(show[:, :, 0], 1, axis=1))\n            if magnifyBlue >= 2:\n                show[:, :, 0] = np.maximum(show[:, :, 0], np.roll(show[:, :, 0], -1, axis=1))\n        if showrot:\n            cv2.putText(show, \'xangle %d\' % (int(xangle / np.pi * 180)), (30, showsz - 30), 0, 0.5,\n                        cv2.cv.CV_RGB(255, 0, 0))\n            cv2.putText(show, \'yangle %d\' % (int(yangle / np.pi * 180)), (30, showsz - 50), 0, 0.5,\n                        cv2.cv.CV_RGB(255, 0, 0))\n            cv2.putText(show, \'zoom %d%%\' % (int(zoom * 100)), (30, showsz - 70), 0, 0.5, cv2.cv.CV_RGB(255, 0, 0))\n\n    changed = True\n    while True:\n        if changed:\n            render()\n            changed = False\n        cv2.imshow(\'show3d\', show)\n        if waittime == 0:\n            cmd = cv2.waitKey(10) % 256\n        else:\n            cmd = cv2.waitKey(waittime) % 256\n        if cmd == ord(\'q\'):\n            break\n        elif cmd == ord(\'Q\'):\n            sys.exit(0)\n\n        if cmd == ord(\'t\') or cmd == ord(\'p\'):\n            if cmd == ord(\'t\'):\n                if c_gt is None:\n                    c0 = np.zeros((len(xyz),), dtype=\'float32\') + 255\n                    c1 = np.zeros((len(xyz),), dtype=\'float32\') + 255\n                    c2 = np.zeros((len(xyz),), dtype=\'float32\') + 255\n                else:\n                    c0 = c_gt[:, 0]\n                    c1 = c_gt[:, 1]\n                    c2 = c_gt[:, 2]\n            else:\n                if c_pred is None:\n                    c0 = np.zeros((len(xyz),), dtype=\'float32\') + 255\n                    c1 = np.zeros((len(xyz),), dtype=\'float32\') + 255\n                    c2 = np.zeros((len(xyz),), dtype=\'float32\') + 255\n                else:\n                    c0 = c_pred[:, 0]\n                    c1 = c_pred[:, 1]\n                    c2 = c_pred[:, 2]\n            if normalizecolor:\n                c0 /= (c0.max() + 1e-14) / 255.0\n                c1 /= (c1.max() + 1e-14) / 255.0\n                c2 /= (c2.max() + 1e-14) / 255.0\n            c0 = np.require(c0, \'float32\', \'C\')\n            c1 = np.require(c1, \'float32\', \'C\')\n            c2 = np.require(c2, \'float32\', \'C\')\n            changed = True\n\n        if cmd == ord(\'n\'):\n            zoom *= 1.1\n            changed = True\n        elif cmd == ord(\'m\'):\n            zoom /= 1.1\n            changed = True\n        elif cmd == ord(\'r\'):\n            zoom = 1.0\n            changed = True\n        elif cmd == ord(\'s\'):\n            cv2.imwrite(\'show3d.png\', show)\n        if waittime != 0:\n            break\n    return cmd\n\n\nif __name__ == \'__main__\':\n    import os\n    import numpy as np\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dataset\', type=str, default=\'../data/shapenet\', help=\'dataset path\')\n    parser.add_argument(\'--category\', type=str, default=\'Airplane\', help=\'select category\')\n    parser.add_argument(\'--npoints\', type=int, default=2500, help=\'resample points number\')\n    parser.add_argument(\'--ballradius\', type=int, default=10, help=\'ballradius\')\n    opt = parser.parse_args()\n    \'\'\'\n    Airplane\t02691156\n    Bag\t        02773838\n    Cap\t        02954340\n    Car\t        02958343\n    Chair\t    03001627\n    Earphone\t03261776\n    Guitar\t    03467517\n    Knife\t    03624134\n    Lamp\t    03636649\n    Laptop\t    03642806\n    Motorbike   03790512\n    Mug\t        03797390\n    Pistol\t    03948459\n    Rocket\t    04099429\n    Skateboard  04225987\n    Table\t    04379243\'\'\'\n\n    cmap = np.array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n                     [3.12493437e-02, 1.00000000e+00, 1.31250131e-06],\n                     [0.00000000e+00, 6.25019688e-02, 1.00000000e+00],\n                     [1.00000000e+00, 0.00000000e+00, 9.37500000e-02],\n                     [1.00000000e+00, 0.00000000e+00, 9.37500000e-02],\n                     [1.00000000e+00, 0.00000000e+00, 9.37500000e-02],\n                     [1.00000000e+00, 0.00000000e+00, 9.37500000e-02],\n                     [1.00000000e+00, 0.00000000e+00, 9.37500000e-02],\n                     [1.00000000e+00, 0.00000000e+00, 9.37500000e-02],\n                     [1.00000000e+00, 0.00000000e+00, 9.37500000e-02]])\n    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n    ROOT_DIR = os.path.dirname(BASE_DIR)\n    sys.path.append(BASE_DIR)\n    sys.path.append(os.path.join(ROOT_DIR, \'data_utils\'))\n\n    from ShapeNetDataLoader import PartNormalDataset\n    root = \'../data/shapenetcore_partanno_segmentation_benchmark_v0_normal/\'\n    dataset = PartNormalDataset(root = root, npoints=2048, split=\'test\', normal_channel=False)\n    idx = np.random.randint(0, len(dataset))\n    data = dataset[idx]\n    point_set, _, seg = data\n    choice = np.random.choice(point_set.shape[0], opt.npoints, replace=True)\n    point_set, seg = point_set[choice, :], seg[choice]\n    seg = seg - seg.min()\n    gt = cmap[seg, :]\n    pred = cmap[seg, :]\n    showpoints(point_set, gt, c_pred=pred, waittime=0, showrot=False, magnifyBlue=0, freezerot=False,\n               background=(255, 255, 255), normalizecolor=True, ballradius=opt.ballradius)\n\n\n'"
