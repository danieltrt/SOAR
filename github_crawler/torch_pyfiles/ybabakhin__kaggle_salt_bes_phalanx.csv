file_path,api_count,code
bes/augmentations.py,0,"b'from albumentations import (HorizontalFlip, ShiftScaleRotate, RandomContrast, RandomBrightness, Compose)\n\n\ndef get_augmentations(augmentation, p):\n    if augmentation == \'valid\':\n        augmentations = Compose([\n            HorizontalFlip(p=0.5),\n            RandomBrightness(p=0.2, limit=0.2),\n            RandomContrast(p=0.1, limit=0.2),\n            ShiftScaleRotate(shift_limit=0.1625, scale_limit=0.6, rotate_limit=0, p=0.7)\n        ], p=p)\n\n    else:\n        raise ValueError(""Unknown Augmentations"")\n\n    return augmentations\n'"
bes/ensemble.py,0,"b'import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm\nfrom params import args\n\n\n# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order=\'F\', format=True):\n    """"""\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    """"""\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = \'\'\n\n        for rr in runs:\n            z += \'{} {} \'.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs\n\n\ndef read_phalanx_test(path):\n    phalanx = np.load(path)\n\n    min_prob = float(""inf"")\n    max_prob = -float(""inf"")\n    for m in phalanx:\n        mi = np.min(m)\n        ma = np.max(m)\n        if mi < min_prob:\n            min_prob = mi\n        if ma > max_prob:\n            max_prob = ma\n    \n    test_id = [x[:-4] for x in os.listdir(args.test_folder) if x[-4:] == \'.png\']\n\n    phalanx_dict = {}\n    for idx, val in enumerate(test_id):\n        phalanx_dict[val] = (phalanx[idx] - min_prob) / (max_prob - min_prob)\n\n    return phalanx_dict\n\n\ndef ensemble(model_dirs, folds, ids, thr, phalanx_dicts=None, weights=None, inner_weights=None):\n    rles = []\n    pseudolabels = {}\n\n    if weights is None:\n        weights = [1] * len(model_dirs)\n\n    for img_id in tqdm(ids):\n        preds = []\n        for d, w in zip(model_dirs, weights):\n            pred_folds = []\n            for fold in folds:\n                path = os.path.join(d, \'fold_{}\'.format(fold))\n                mask = cv2.imread(os.path.join(path, \'{}.png\'.format(img_id)), cv2.IMREAD_GRAYSCALE)\n\n                img = cv2.imread(os.path.join(args.test_folder, \'{}.png\'.format(img_id)), cv2.IMREAD_GRAYSCALE)\n                if np.unique(img).shape[0] == 1:\n                    pred_folds.append(np.zeros(mask.shape))\n                else:\n                    pred_folds.append(np.array(mask / 255, np.float32))\n            preds.append(np.mean(np.array(pred_folds, np.float32), axis=0) * w)\n\n        if phalanx_dicts is None:\n            final_pred = np.sum(np.array(preds), axis=0) / sum(weights)\n        else:\n\n            if len(model_dirs) == 0:\n                final_pred = []\n            else:\n                final_pred = [np.sum(np.array(preds), axis=0) / sum(weights) * inner_weights[0]]\n\n            i = 1\n            for phalanx_dict in phalanx_dicts:\n                final_pred.append(phalanx_dict[img_id] * inner_weights[i])\n                i += 1\n            final_pred = np.sum(np.array(final_pred), axis=0) / sum(inner_weights)\n\n        confidence = (np.sum(final_pred < 0.2) + np.sum(final_pred > 0.8)) / (101 ** 2)\n        mask = final_pred > thr\n        pseudolabels[img_id] = (confidence, mask * 255)\n\n        rle = RLenc(mask)\n        rles.append(rle)\n\n    return rles, pseudolabels\n\n\ndef generate_pseudolabels(pseudolabels, pseudolabels_path=\'pseudolabels\'):\n    pseudolabels_df = pd.DataFrame(pseudolabels).T\n    pseudolabels_df.columns = [\'confidence\', \'mask\']\n\n    df_test = pd.read_csv(os.path.join(args.data_root, \'sample_submission.csv\'), index_col=\'id\')\n    depths = pd.read_csv(os.path.join(args.data_root, \'depths.csv\'), index_col=\'id\')\n    df_test = df_test.join(depths)\n    # Label suspicious images\n    dist = []\n    mask_pixels = []\n    for id in df_test.index:\n        img = cv2.imread(os.path.join(args.test_folder, \'{}.png\'.format(id)), cv2.IMREAD_GRAYSCALE)\n        mask = pseudolabels_df.loc[id][1]\n        dist.append(np.unique(img).shape[0])\n        mask_pixels.append(np.sum(mask / 255))\n    df_test[\'unique_pixels\'] = dist\n    df_test[\'mask_pixels\'] = mask_pixels\n\n    df_test = df_test.join(pseudolabels_df)\n\n    os.system(""mkdir {}"".format(os.path.join(args.data_root, pseudolabels_path)))\n    for idx, row in df_test.iterrows():\n        cv2.imwrite(\n            os.path.join(args.data_root, pseudolabels_path, str(idx) + \'.png\'),\n            np.array(row[\'mask\'], np.uint8))\n\n    df_test[[\'unique_pixels\', \'mask_pixels\', \'confidence\']].sample(frac=1, random_state=123).to_csv(\n        os.path.join(args.data_root, pseudolabels_path + \'.csv\'))\n\n    pseudo = df_test[(df_test.confidence >= 0.9) & (df_test.unique_pixels > 1)].sample(frac=1, random_state=123)\n    pseudo[\'fold\'] = -1\n    pseudo[[\'fold\', \'unique_pixels\']].to_csv(\n        os.path.join(args.data_root, pseudolabels_path + \'_confident.csv\'))\n\n\ndef postprocessing(test):\n    df_train = pd.read_csv(os.path.join(args.data_root, \'train.csv\'))\n\n    masks = []\n    dist = []\n\n    for id in df_train.id.values:\n        mask = cv2.imread(os.path.join(args.masks_dir, \'{}.png\'.format(id)), cv2.IMREAD_GRAYSCALE)\n        masks.append(np.array(mask) / 255.)\n        img = cv2.imread(os.path.join(args.images_dir, \'{}.png\'.format(id)), cv2.IMREAD_GRAYSCALE)\n        dist.append(np.unique(img).shape[0])\n\n    df_train[\'unique_pixels\'] = dist\n    df_train[\'masks\'] = masks\n\n    df_train = df_train[df_train.unique_pixels > 1]\n\n    def get_mask_type(mask):\n        if ((mask[-50:, :]).sum() > 0) and np.all(mask[-50:, :] == mask[-1]) and np.any(mask[-1] != 1):\n            return 1\n        else:\n            return 0\n\n    df_train[""is_vertical_soft""] = df_train.masks.map(get_mask_type)\n\n    def get_mask_type(mask):\n        if (mask.sum() > 0) and np.all(mask == mask[-1]) and np.any(mask[-1] != 1):\n            return 1\n        else:\n            return 0\n\n    df_train[""is_vertical""] = df_train.masks.map(get_mask_type)\n\n    verts_dict = {}\n    for mos_csv in os.listdir(os.path.join(args.data_root, \'mos_numpy_v2/\')):\n        mos = pd.read_csv(os.path.join(args.data_root, \'mos_numpy_v2/\', mos_csv), header=None)\n        for col in range(mos.shape[1]):\n            for idx, row in mos.iterrows():\n                ans_down = None\n                ans_up = None\n                if row[col] in list(df_train[df_train[""is_vertical""] == 1].id.values):\n                    new_mask = np.array([list(df_train[df_train.id == row[col]].masks.values[0][-1]), ] * 101)\n                    ans_down = RLenc(new_mask)\n                    new_mask = np.array([list(df_train[df_train.id == row[col]].masks.values[0][0]), ] * 101)\n                    ans_up = RLenc(new_mask)\n                    break\n                elif row[col] in list(df_train[df_train[""is_vertical_soft""] == 1].id.values):\n                    new_mask = np.array([list(df_train[df_train.id == row[col]].masks.values[0][-1]), ] * 101)\n                    ans_down = RLenc(new_mask)\n                    break\n            if ans_down is not None:\n                for id in mos.loc[idx + 1:, col]:\n                    verts_dict[id] = ans_down\n            if ans_up is not None and idx >= 3:\n                for id in mos.loc[idx - 1:idx, col]:\n                    verts_dict[id] = ans_up\n\n    final_pred = []\n    for idx, row in test.iterrows():\n        if row[\'id\'] in verts_dict.keys():\n            final_pred.append(verts_dict[row[\'id\']])\n        else:\n            final_pred.append(row[\'rle_mask\'])\n    test[\'rle_mask\'] = final_pred\n\n    return test\n\n\nif __name__ == \'__main__\':\n    test_ids = [x[:-4] for x in os.listdir(args.test_folder) if x[-4:] == \'.png\']\n    print(\'Generating Predictions for Stage\', args.stage)\n    if args.stage == 1:\n        models = [\n            \'unet_resnext_50_lovasz_stage_1_5\',\n            \'unet_resnext_50_lovasz_stage_1_4\',\n            \'unet_resnext_50_lovasz_stage_1_3\',\n            \'unet_resnext_50_lovasz_stage_1_2\'\n        ]\n\n        model_pathes = [args.models_dir + x for x in models]\n        phalanx_dicts = [read_phalanx_test(\'/workdir/phalanx/predictions/phalanx_stage_1.npy\')]\n\n        pred, pseudolabels = ensemble(model_pathes, [0, 1, 2, 3, 4],\n                                      test_ids, 0.5,\n                                      phalanx_dicts=phalanx_dicts,\n                                      weights=[1, 1, 1, 1],\n                                      inner_weights=[1, 1])\n\n        generate_pseudolabels(pseudolabels, \'pseudolabels\')\n\n    elif args.stage == 2:\n        models = [\n            \'unet_resnext_50_lovasz_stage_2_5\',\n            \'unet_resnext_50_lovasz_stage_2_4\',\n            \'unet_resnext_50_lovasz_stage_2_3\',\n            \'unet_resnext_50_lovasz_stage_2_2\'\n        ]\n\n        model_pathes = [args.models_dir + x for x in models]\n        phalanx_dicts = [read_phalanx_test(\'/workdir/phalanx/predictions/phalanx_stage_2.npy\')]\n\n        pred, pseudolabels = ensemble(model_pathes, [0, 1, 2, 3, 4],\n                                      test_ids, 0.5,\n                                      phalanx_dicts=phalanx_dicts,\n                                      weights=[1, 1, 1, 3],\n                                      inner_weights=[1, 1])\n\n        generate_pseudolabels(pseudolabels, \'pseudolabels_v2\')\n\n    elif args.stage == 3:\n        models = [\n            \'unet_resnext_50_lovasz_stage_2_5\',\n            \'unet_resnext_50_lovasz_stage_2_4\',\n            \'unet_resnext_50_lovasz_stage_2_3\',\n            \'unet_resnext_50_lovasz_stage_2_2\'\n        ]\n\n        model_pathes = [args.models_dir + x for x in models]\n        phalanx_dicts = [read_phalanx_test(\'/workdir/phalanx/predictions/phalanx_stage_3.npy\')]\n\n        pred, pseudolabels = ensemble(model_pathes, [0, 1, 2, 3, 4],\n                                      test_ids, 0.5,\n                                      phalanx_dicts=phalanx_dicts,\n                                      weights=[1, 1, 1, 3],\n                                      inner_weights=[1, 1])\n\n        # Whether to generate test predictions?\n        # generate_pseudolabels(pseudolabels, \'test_predictions\')\n\n        test = pd.DataFrame({\'id\': test_ids, \'rle_mask\': pred})\n\n        if args.postprocessing == 1:\n            print(\'Applying Postrpocessing...\')\n            test = postprocessing(test)\n\n        test[[\'id\', \'rle_mask\']].to_csv(args.test_predictions_path, index=False)\n'"
bes/losses.py,0,"b'import keras.backend as K\nimport tensorflow as tf\nimport numpy as np\n\n\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef binary_crossentropy(y, p):\n    return K.mean(K.binary_crossentropy(y, p))\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\n\ndef dice_coef_loss_bce(y_true, y_pred, dice=0.5, bce=0.5):\n    return binary_crossentropy(y_true, y_pred) * bce + dice_coef_loss(y_true, y_pred) * dice\n\n\ndef Kaggle_IoU_Precision(y_true, y_pred, threshold=0.5):\n    y_pred = K.squeeze(tf.to_int32(y_pred > threshold), -1)\n    y_true = K.cast(y_true[..., 0], K.floatx())\n    y_pred = K.cast(y_pred, K.floatx())\n    truth_areas = K.sum(y_true, axis=[1, 2])\n    pred_areas = K.sum(y_pred, axis=[1, 2])\n    intersection = K.sum(y_true * y_pred, axis=[1, 2])\n    union = K.clip(truth_areas + pred_areas - intersection, 1e-9, 128 * 128)\n    check = K.map_fn(lambda x: K.equal(x, 0), truth_areas + pred_areas, dtype=tf.bool)\n    p = intersection / union\n    iou = K.switch(check, p + 1., p)\n\n    prec = K.map_fn(lambda x: K.mean(K.greater(x, np.arange(0.5, 1.0, 0.05))), iou, dtype=tf.float32)\n    prec_iou = K.mean(prec)\n    return prec_iou\n\n\n# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n\n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins=true_objects)[0]\n    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:, 1:]\n    union = union[1:, 1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(""Thresh\\tTP\\tFP\\tFN\\tPrec."")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(""{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}"".format(t, tp, fp, fn, p))\n        prec.append(p)\n\n    if print_table:\n        print(""AP\\t-\\t-\\t-\\t{:1.3f}"".format(np.mean(prec)))\n    return np.mean(prec)\n\n\n# """"""\n# Lovasz-Softmax and Jaccard hinge loss in Tensorflow\n# Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n# """"""\n\ndef lovasz_loss(y_true, y_pred):\n    y_true, y_pred = K.cast(K.squeeze(y_true, -1), \'int32\'), K.cast(K.squeeze(y_pred, -1), \'float32\')\n    logits = K.log(y_pred / (1. - y_pred))\n    loss = lovasz_hinge(logits, y_true, per_image=True, ignore=None)\n    return loss\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n\n        # Fixed python3\n        losses.set_shape((None,))\n\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=""descending_sort"")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        # loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=""loss_non_void"")\n        # ELU + 1\n        loss = tf.tensordot(tf.nn.elu(errors_sorted) + 1., tf.stop_gradient(grad), 1, name=""loss_non_void"")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=""loss""\n                   )\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name=\'valid_scores\')\n    vlabels = tf.boolean_mask(labels, valid, name=\'valid_labels\')\n    return vscores, vlabels\n\n\ndef make_loss(loss_name):\n    if loss_name == \'bce_dice\':\n        def loss(y, p):\n            return dice_coef_loss_bce(y, p, dice=0.5, bce=0.5)\n\n        return loss\n\n    elif loss_name == \'lovasz\':\n        def loss(y, p):\n            return lovasz_hinge(p, y, per_image=True, ignore=None)\n\n        return loss\n\n    else:\n        ValueError(""Unknown loss"")\n'"
bes/params.py,0,"b""import argparse\nimport sys\n\nparser = argparse.ArgumentParser()\narg = parser.add_argument\narg('--epochs', type=int, default=50)\narg('--n_snapshots', type=int, default=1)\narg('--fold', default='0')\narg('--pretrain_weights')\narg('--prediction_weights', default='fold_{}.hdf5')\narg('--prediction_folder', default='oof')\narg('--learning_rate', type=float, default=0.0001)\narg('--input_size', type=int, default=192)\narg('--resize_size', type=int, default=160)\narg('--batch_size', type=int, default=24)\n\narg('--loss_function', default='bce_jacard')\narg('--augmentation_name', default='valid')\narg('--augmentation_prob', type=float, default=1.0)\narg('--network', default='unet_resnet_50')\narg('--alias', default='')\narg('--callback', default='snapshot')\narg('--freeze_encoder', type=int, default=0)\n\narg('--models_dir', default='/workdir/bes/weights/')\narg('--data_root', default='/workdir/data/')\narg('--images_dir', default='/workdir/data/train/images/')\narg('--pseudolabels_dir', default='')\narg('--masks_dir', default='/workdir/data/train/masks/')\narg('--test_folder', default='/test_data/')\narg('--folds_csv', default='/workdir/data/train_proc_v2_gr.csv')\narg('--pseudolabels_csv', default='/workdir/data/pseudolabels_confident.csv')\n\narg('--initial_size', type=int, default=101)\narg('--num_workers', type=int, default=12)\narg('--early_stop_patience',  type=int, default=15)\narg('--reduce_lr_factor',  type=float, default=0.25)\narg('--reduce_lr_patience',  type=int, default=7)\narg('--reduce_lr_min',  type=float, default=0.000001)\n\narg('--stage',  type=int, default=3)\narg('--postprocessing',  type=int, default=0)\narg('--test_predictions_path', default='/workdir/predictions/test_predictions.csv')\n\nargs = parser.parse_args()\n"""
bes/predict_test.py,0,"b'import pandas as pd\nimport numpy as np\nimport os\nimport gc\nfrom keras import backend as K\nfrom keras.optimizers import RMSprop\nfrom utils import predict_test\nfrom params import args\nfrom models.models import get_model\nfrom losses import Kaggle_IoU_Precision, make_loss\n\n\ndef main():\n    test_ids = np.array([x[:-4] for x in os.listdir(args.test_folder) if x[-4:] == \'.png\'])\n\n    MODEL_PATH = os.path.join(args.models_dir, args.network + args.alias)\n    folds = [int(f) for f in args.fold.split(\',\')]\n\n    print(\'Predicting Model:\', args.network + args.alias)\n\n    for fold in folds:\n        K.clear_session()\n        print(\'***************************** FOLD {} *****************************\'.format(fold))\n\n        # Initialize Model\n        weights_path = os.path.join(MODEL_PATH, args.prediction_weights.format(fold))\n\n        model, preprocess = get_model(args.network,\n                                      input_shape=(args.input_size, args.input_size, 3),\n                                      freeze_encoder=args.freeze_encoder)\n        model.compile(optimizer=RMSprop(lr=args.learning_rate), loss=make_loss(args.loss_function),\n                      metrics=[Kaggle_IoU_Precision])\n\n        model.load_weights(weights_path)\n\n        # Save test predictions to disk\n        dir_path = os.path.join(MODEL_PATH, args.prediction_folder.format(fold))\n        os.system(""mkdir {}"".format(dir_path))\n        predict_test(model=model,\n                     preds_path=dir_path,\n                     ids=test_ids,\n                     batch_size=args.batch_size * 2,\n                     TTA=\'flip\',\n                     preprocess=preprocess)\n\n        gc.collect()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
bes/train.py,0,"b'import pandas as pd\nimport numpy as np\nimport os\nimport gc\nfrom keras import backend as K\nfrom keras.optimizers import RMSprop\nfrom utils import ThreadsafeIter\nfrom datasets.generators import SegmentationDataGenerator\nfrom params import args\nfrom callbacks.callbacks import get_callback\nfrom augmentations import get_augmentations\nfrom models.models import get_model\nfrom losses import make_loss, Kaggle_IoU_Precision\n\n\ndef main():\n    train = pd.read_csv(args.folds_csv)\n    MODEL_PATH = os.path.join(args.models_dir, args.network + args.alias)\n    folds = [int(f) for f in args.fold.split(\',\')]\n\n    print(\'Training Model:\', args.network + args.alias)\n\n    for fold in folds:\n\n        K.clear_session()\n        print(\'***************************** FOLD {} *****************************\'.format(fold))\n\n        if fold == 0:\n            if os.path.isdir(MODEL_PATH):\n                raise ValueError(\'Such Model already exists\')\n            os.system(""mkdir {}"".format(MODEL_PATH))\n\n        # Train/Validation sampling\n        df_train = train[train.fold != fold].copy().reset_index(drop=True)\n        df_valid = train[train.fold == fold].copy().reset_index(drop=True)\n\n        # Train on pseudolabels only\n        if args.pseudolabels_dir != \'\':\n            pseudolabels = pd.read_csv(args.pseudolabels_csv)\n            df_train = pseudolabels.sample(frac=1, random_state=13).reset_index(drop=True)\n\n        # Keep only non-black images\n        ids_train, ids_valid = df_train[df_train.unique_pixels > 1].id.values, df_valid[\n            df_valid.unique_pixels > 1].id.values\n\n        print(\'Training on {} samples\'.format(ids_train.shape[0]))\n        print(\'Validating on {} samples\'.format(ids_valid.shape[0]))\n\n        # Initialize model\n        weights_path = os.path.join(MODEL_PATH, \'fold_{fold}.hdf5\'.format(fold=fold))\n\n        # Get the model\n        model, preprocess = get_model(args.network,\n                                      input_shape=(args.input_size, args.input_size, 3),\n                                      freeze_encoder=args.freeze_encoder)\n\n        # LB metric threshold\n        def lb_metric(y_true, y_pred):\n            return Kaggle_IoU_Precision(y_true, y_pred, threshold=0 if args.loss_function == \'lovasz\' else 0.5)\n\n        model.compile(optimizer=RMSprop(lr=args.learning_rate), loss=make_loss(args.loss_function),\n                      metrics=[lb_metric])\n\n        if args.pretrain_weights is None:\n            print(\'No weights passed, training from scratch\')\n        else:\n            wp = args.pretrain_weights.format(fold)\n            print(\'Loading weights from {}\'.format(wp))\n            model.load_weights(wp, by_name=True)\n\n        # Get augmentations\n        augs = get_augmentations(args.augmentation_name, p=args.augmentation_prob)\n\n        # Data generator\n        dg = SegmentationDataGenerator(input_shape=(args.input_size, args.input_size),\n                                       batch_size=args.batch_size,\n                                       augs=augs,\n                                       preprocess=preprocess)\n\n        train_generator = dg.train_batch_generator(ids_train)\n        validation_generator = dg.evaluation_batch_generator(ids_valid)\n\n        # Get callbacks\n        callbacks = get_callback(args.callback,\n                                 weights_path=weights_path,\n                                 fold=fold)\n\n        # Fit the model with Generators:\n        model.fit_generator(generator=ThreadsafeIter(train_generator),\n                            steps_per_epoch=ids_train.shape[0] // args.batch_size * 2,\n                            epochs=args.epochs,\n                            callbacks=callbacks,\n                            validation_data=ThreadsafeIter(validation_generator),\n                            validation_steps=np.ceil(ids_valid.shape[0] / args.batch_size),\n                            workers=args.num_workers)\n\n        gc.collect()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
bes/utils.py,0,"b'import os\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nimport threading\nfrom params import args\nfrom albumentations import PadIfNeeded, CenterCrop, HorizontalFlip\n\n\nclass ThreadsafeIter(object):\n    def __init__(self, it):\n        self.lock = threading.Lock()\n        self.it = it.__iter__()\n\n    def __iter__(self): return self\n\n    def __next__(self):\n        with self.lock:\n            return next(self.it)\n\n\ndef do_tta(img, TTA, preprocess):\n    imgs = []\n\n    if TTA == \'flip\':\n        augmentation = HorizontalFlip(p=1)\n        data = {\'image\': img}\n        img2 = augmentation(**data)[\'image\']\n\n        for im in [img, img2]:\n            im = np.array(im, np.float32)\n\n            im = cv2.resize(im, (args.resize_size, args.resize_size))\n            augmentation = PadIfNeeded(min_height=args.input_size, min_width=args.input_size, p=1.0, border_mode=4)\n            data = {""image"": im}\n            im = augmentation(**data)[""image""]\n            im = np.array(im, np.float32)\n\n            imgs.append(preprocess(im))\n\n    return imgs\n\n\ndef undo_tta(imgs, TTA):\n    part = []\n    for img in imgs:\n        augmentation = CenterCrop(height=args.resize_size, width=args.resize_size, p=1.0)\n        data = {""image"": img}\n        prob = augmentation(**data)[""image""]\n        prob = cv2.resize(prob, (args.initial_size, args.initial_size))\n\n        part.append(prob)\n\n    if TTA == \'flip\':\n        augmentation = HorizontalFlip(p=1)\n        data = {\'image\': part[1]}\n        part[1] = augmentation(**data)[\'image\']\n\n    part = np.mean(np.array(part), axis=0)\n\n    return part\n\n\ndef read_image_test(id, TTA, preprocess):\n    img = cv2.imread(os.path.join(args.test_folder, \'{}.png\'.format(id)), cv2.IMREAD_COLOR)\n    imgs = do_tta(img, TTA, preprocess)\n\n    return imgs\n\n\ndef _get_augmentations_count(TTA=\'\'):\n    if TTA == \'\':\n        return 1\n\n    elif TTA == \'flip\':\n        return 2\n\n    else:\n        raise ValueError(\'No Such TTA\')\n\n\ndef predict_test(model, preds_path, ids, batch_size, TTA=\'\', preprocess=None):\n    num_images = ids.shape[0]\n\n    for start in tqdm(range(0, num_images, batch_size)):\n        end = min(start + batch_size, num_images)\n\n        augment_number_per_image = _get_augmentations_count(TTA)\n        images = [read_image_test(x, TTA=TTA, preprocess=preprocess) for x in ids[start:end]]\n        images = [item for sublist in images for item in sublist]\n\n        X = np.array([x for x in images])\n        preds = model.predict_on_batch(X)\n\n        total = 0\n        for idx in range(end - start):\n            part = undo_tta(preds[total:total + augment_number_per_image], TTA)\n            total += augment_number_per_image\n\n            cv2.imwrite(os.path.join(preds_path, str(ids[start + idx]) + \'.png\'), np.array(part * 255, np.uint8))\n\n'"
phalanx/lovasz_losses.py,9,"b'""""""\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nfrom __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\ntry:\n    from itertools import  ifilterfalse\nexcept ImportError: # py3k\n    from itertools import  filterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    """"""\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    """"""\n    Array of IoU for each (non ignored) class\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []    \n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious)) # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n    return loss\n\ndef lovasz_hinge2(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(lovasz_hinge_flat2(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat2(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat2(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    weight = 1\n    if labels.sum() == 0:\n        weight = 0\n    loss = torch.dot(F.relu(errors_sorted), Variable(grad)) * weight\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n         super(StableBCELoss, self).__init__()\n    def forward(self, input, target):\n         neg_abs = - input.abs()\n         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n         return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    """"""\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    """"""\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    """"""\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, only_present=False):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n    """"""\n    C = probas.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float() # foreground for class c\n        if only_present and fg.sum() == 0:\n            continue\n        errors = (Variable(fg) - probas[:, c]).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\ndef xloss(logits, labels, ignore=None):\n    """"""\n    Cross entropy loss\n    """"""\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\n\ndef mean(l, ignore_nan=False, empty=0):\n    """"""\n    nanmean compatible with generators.\n    """"""\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == \'raise\':\n            raise ValueError(\'Empty mean\')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n'"
phalanx/make_pseudo.py,0,"b""import numpy as np\nimport pandas as pd\n\ntrain_id = pd.read_csv('../data/train.csv')['id'].values\ndepth_id = pd.read_csv('../data/depths.csv')['id'].values\ntest_id = np.setdiff1d(depth_id, train_id)\n\nif __name__ == '__main__':\n    df = pd.read_csv('../data/pseudolabels.csv')\n    pseudo_id = df['id'].values\n    pseudo_pix = df['mask_pixels'].values\n    pseudo_conf = df['confidence'].values\n    pseudo_unique = df['unique_pixels'].values\n\n    split0 = [] # salt_size == 0\n    split1 = [] # 20 <= salt_size <= 500\n    split2 = [] # 500 < salt_size <= 101*101/4\n    split3 = [] # 101*101/4 < salt_size <= 101*101/2\n    split4 = [] # 101*101/2 < salt_size <= 101*101*3/4\n    split5 = [] # 101*101*3/4 < salt_size < 101*101*0.9\n\n    for idx, mask_id in enumerate(pseudo_id):\n        if pseudo_conf[idx] >= 0.97:\n            if pseudo_pix[idx] == 0:\n                if pseudo_unique[idx] != 1:\n                    split0.append(mask_id+'.png')\n            elif 20 <= pseudo_pix[idx] and pseudo_pix[idx] <= 500:\n                if pseudo_conf[idx] >= 0.99:\n                    split1.append(mask_id+'.png')\n            elif 500 < pseudo_pix[idx] and pseudo_pix[idx] <= 101*101/4:\n                split2.append(mask_id+'.png')\n            elif 101*101/4 < pseudo_pix[idx] and pseudo_pix[idx] <= 101*101/2:\n                split3.append(mask_id+'.png')\n            elif 101*101/2 < pseudo_pix[idx] and pseudo_pix[idx] <= 101*101*3/4:\n                split4.append(mask_id+'.png')\n            elif 101*101*3/4 < pseudo_pix[idx] and pseudo_pix[idx] < 101*101 * 0.9:\n                split5.append(mask_id+'.png')\n    split0 = split0[::2]\n\n    fold0 = split0[:600] + split1[:110] + split2[:300] + split3[:240] + split4[:200] + split5[:130]\n    fold1 = split0[600:1200] + split1[110:220] + split2[300:600] + split3[240:480] + split4[200:400] + split5[130:260]\n    fold2 = split0[1200:1800] + split1[220:330] + split2[600:900] + split3[480:740] + split4[400:600] + split5[260:390]\n    fold3 = split0[1800:2400] + split1[330:440] + split2[900:1200] + split3[740:980] + split4[600:800] + split5[390:520]\n    fold4 = split0[2400:] + split1[440:] + split2[1200:] + split3[980:] + split4[800:] + split5[520:]\n\n    np.save('../data/stage2_fold0', fold0)\n    np.save('../data/stage2_fold1', fold1)\n    np.save('../data/stage2_fold2', fold2)\n    np.save('../data/stage2_fold3', fold3)\n    np.save('../data/stage2_fold4', fold4)"""
phalanx/precisioncv.py,7,"b""from tqdm import tqdm\nimport os\nimport argparse\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nfrom salt_dataset import SaltDataset, testImageFetch\nfrom utils import get_model\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nparser = argparse.ArgumentParser(description='')\nparser.add_argument('--model', default='res34v5', type=str, help='Model version')\nparser.add_argument('--fine_size', default=101, type=int, help='Resized image size')\nparser.add_argument('--pad_left', default=13, type=int, help='Left padding size')\nparser.add_argument('--pad_right', default=14, type=int, help='Right padding size')\nparser.add_argument('--batch_size', default=18, type=int, help='Batch size for training')\nparser.add_argument('--epoch', default=300, type=int, help='Number of training epochs')\nparser.add_argument('--snapshot', default=5, type=int, help='Number of snapshots per fold')\nparser.add_argument('--cuda', default=True, type=bool, help='Use cuda to train model')\nparser.add_argument('--save_weight', default='weights/', type=str, help='weight save space')\nparser.add_argument('--save_pred', default='fold_predictions/', type=str, help='prediction save space')\nparser.add_argument('--max_lr', default=0.01, type=float, help='max learning rate')\nparser.add_argument('--min_lr', default=0.001, type=float, help='min learning rate')\nparser.add_argument('--fold', default='0', type=str, help='number of split fold')\nparser.add_argument('--start_snap', default=0, type=int)\nparser.add_argument('--end_snap', default=3, type=int)\nparser.add_argument('--test_folder', default='/test_data/', type=str, help='path to the folder with test images')\n\nargs = parser.parse_args()\nfine_size = args.fine_size + args.pad_left + args.pad_right\nargs.weight_name = 'model_' + str(fine_size) + '_' + args.model\nargs.save_pred += args.model + '_'\n\ndevice = torch.device('cuda' if args.cuda else 'cpu')\n\ntest_id = [x[:-4] for x in os.listdir(args.test_folder) if x[-4:] == '.png']\n\nif __name__ == '__main__':\n    # Load test data\n    image_test = testImageFetch(test_id)\n\n    overall_pred = np.zeros((len(test_id), args.fine_size, args.fine_size), dtype=np.float32)\n\n    # Get model\n    salt = get_model(args.model)\n    salt = salt.to(device)\n\n    # Start prediction\n    for step in range(args.start_snap, args.end_snap + 1):\n        print('Predicting Snapshot', step)\n        pred_null = []\n        pred_flip = []\n        # Load weight\n        param = torch.load(args.save_weight + args.weight_name + args.fold + str(step) + '.pth')\n        salt.load_state_dict(param)\n\n        # Create DataLoader\n        test_data = SaltDataset(image_test, mode='test', fine_size=args.fine_size, pad_left=args.pad_left, pad_right=args.pad_right)\n        test_loader = DataLoader(\n                            test_data,\n                            shuffle=False,\n                            batch_size=args.batch_size,\n                            num_workers=8,\n                            pin_memory=True)\n\n        # Prediction with no TTA test data\n        salt.eval()\n        for images in tqdm(test_loader, total=len(test_loader)):\n            images = images.to(device)\n            with torch.set_grad_enabled(False):\n                if args.model == 'res34v3':\n                    pred, _, _ = salt(images)\n                else:\n                    pred = salt(images)\n                pred = F.sigmoid(pred).squeeze(1).cpu().numpy()\n            pred = pred[:, args.pad_left:args.fine_size + args.pad_left, args.pad_left:args.fine_size + args.pad_left]\n            pred_null.append(pred)\n\n        # Prediction with horizontal flip TTA test data\n        test_data = SaltDataset(image_test, mode='test', is_tta=True, fine_size=args.fine_size, pad_left=args.pad_left,\n                                pad_right=args.pad_right)\n        test_loader = DataLoader(\n                            test_data,\n                            shuffle=False,\n                            batch_size=args.batch_size,\n                            num_workers=8,\n                            pin_memory=True)\n\n        salt.eval()\n        for images in tqdm(test_loader, total=len(test_loader)):\n            images = images.to(device)\n            with torch.set_grad_enabled(False):\n                if args.model == 'res34v3':\n                    pred, _, _ = salt(images)\n                else:\n                    pred = salt(images)\n                pred = F.sigmoid(pred).squeeze(1).cpu().numpy()\n            pred = pred[:, args.pad_left:args.fine_size + args.pad_left, args.pad_left:args.fine_size + args.pad_left]\n            for idx in range(len(pred)):\n                pred[idx] = cv2.flip(pred[idx], 1)\n            pred_flip.append(pred)\n\n        pred_null = np.concatenate(pred_null).reshape(-1, args.fine_size, args.fine_size)\n        pred_flip = np.concatenate(pred_flip).reshape(-1, args.fine_size, args.fine_size)\n        overall_pred += (pred_null + pred_flip) / 2\n\n    overall_pred /= (args.end_snap - args.start_snap + 1)\n\n    # Save prediction\n    if args.fine_size != 101:\n        overall_pred_101 = np.zeros((len(test_id), 101, 101), dtype=np.float32)\n        for idx in range(len(test_id)):\n            overall_pred_101[idx] = cv2.resize(overall_pred[idx], dsize=(101, 101))\n        np.save(args.save_pred + 'pred' + args.fold, overall_pred_101)\n    else:\n        np.save(args.save_pred + 'pred' + args.fold, overall_pred)\n"""
phalanx/salt_dataset.py,6,"b""from copy import deepcopy\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom transform import *\n\ntrain_root = '../data/train/'\ntest_root = '../data/test/'\n\ntrain_id = pd.read_csv('../data/train.csv')['id'].values\ndepth_id = pd.read_csv('../data/depths.csv')['id'].values\ntest_id = np.setdiff1d(depth_id, train_id)\n\n\ndef add_depth_channels(image_tensor):\n    _, h, w = image_tensor.size()\n    image = torch.zeros([3, h, w])\n    image[0] = image_tensor\n    for row, const in enumerate(np.linspace(0, 1, h)):\n        image[1, row, :] = const\n    image[2] = image[0] * image[1]\n    return image\n\n\ndef train_aug(image, mask):\n    if np.random.rand() < 0.5:\n        image, mask = do_horizontal_flip2(image, mask)\n\n    if np.random.rand() < 0.5:\n        c = np.random.choice(3)\n        if c == 0:\n            image, mask = do_random_shift_scale_crop_pad2(image, mask, 0.2)\n\n        if c == 1:\n            image, mask = do_horizontal_shear2(image, mask, dx=np.random.uniform(-0.07, 0.07))\n\n        if c == 2:\n            image, mask = do_shift_scale_rotate2(image, mask, dx=0, dy=0, scale=1, angle=np.random.uniform(0, 15))\n\n    if np.random.rand() < 0.5:\n        c = np.random.choice(2)\n        if c == 0:\n            image = do_brightness_shift(image, np.random.uniform(-0.1, +0.1))\n        if c == 1:\n            image = do_brightness_multiply(image, np.random.uniform(1 - 0.08, 1 + 0.08))\n\n    return image, mask\n\n\nclass SaltDataset(Dataset):\n    def __init__(self, image_list, mode, mask_list=None, is_tta=False, is_semi=False,\n                 fine_size=202, pad_left=0, pad_right=0):\n        self.imagelist = image_list\n        self.mode = mode\n        self.masklist = mask_list\n        self.is_tta = is_tta\n        self.is_semi = is_semi\n        self.fine_size = fine_size\n        self.pad_left = pad_left\n        self.pad_right = pad_right\n\n    def __len__(self):\n        return len(self.imagelist)\n\n    def __getitem__(self, idx):\n        image = deepcopy(self.imagelist[idx])\n\n        if self.mode == 'train':\n            mask = deepcopy(self.masklist[idx])\n\n            image, mask = train_aug(image, mask)\n            label = np.where(mask.sum() == 0, 1.0, 0.0).astype(np.float32)\n\n            if self.fine_size != image.shape[0]:\n                image, mask = do_resize2(image, mask, self.fine_size, self.fine_size)\n\n            if self.pad_left != 0:\n                image, mask = do_center_pad2(image, mask, self.pad_left, self.pad_right)\n\n            image = image.reshape(1, self.fine_size + self.pad_left + self.pad_right,\n                                  self.fine_size + self.pad_left + self.pad_right)\n            mask = mask.reshape(1, self.fine_size + self.pad_left + self.pad_right,\n                                self.fine_size + self.pad_left + self.pad_right)\n            image, mask = torch.from_numpy(image), torch.from_numpy(mask)\n            image = add_depth_channels(image)\n            return image, mask, torch.from_numpy(label)\n\n        elif self.mode == 'val':\n            mask = deepcopy(self.masklist[idx])\n            if self.fine_size != image.shape[0]:\n                image, mask = do_resize2(image, mask, self.fine_size, self.fine_size)\n            if self.pad_left != 0:\n                image = do_center_pad(image, self.pad_left, self.pad_right)\n\n            image = image.reshape(1, self.fine_size + self.pad_left + self.pad_right,\n                                  self.fine_size + self.pad_left + self.pad_right)\n            mask = mask.reshape(1, self.fine_size, self.fine_size)\n\n            image, mask = torch.from_numpy(image), torch.from_numpy(mask)\n            image = add_depth_channels(image)\n\n            return image, mask\n\n        elif self.mode == 'test':\n            if self.is_tta:\n                image = cv2.flip(image, 1)\n            if self.fine_size != image.shape[0]:\n                image = cv2.resize(image, dsize=(self.fine_size, self.fine_size))\n            if self.pad_left != 0:\n                image = do_center_pad(image, self.pad_left, self.pad_right)\n\n            image = image.reshape(1, self.fine_size + self.pad_left + self.pad_right,\n                                self.fine_size + self.pad_left + self.pad_right)\n            image = torch.from_numpy(image)\n            image = add_depth_channels(image)\n            return image\n\n\n\ndef trainImageFetch(images_id):\n    image_train = np.zeros((images_id.shape[0], 101, 101), dtype=np.float32)\n    mask_train = np.zeros((images_id.shape[0], 101, 101), dtype=np.float32)\n\n    for idx, image_id in tqdm(enumerate(images_id), total=images_id.shape[0]):\n        image_path = '../data/train/images/' + image_id + '.png'\n        mask_path = '../data/train/masks/' + image_id + '.png'\n\n        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n        image_train[idx] = image\n        mask_train[idx] = mask\n\n    return image_train, mask_train\n\n\ndef semi_trainImageFetch(pseudo_path, list_id=None):\n    if list_id is None:\n        images_id = os.listdir(pseudo_path)\n    else:\n        images_id = list_id\n        \n    image_train = np.zeros((len(images_id), 101, 101), dtype=np.float32)\n    mask_train = np.zeros((len(images_id), 101, 101), dtype=np.float32)\n\n    for idx, image_id in tqdm(enumerate(images_id)):\n        image_path = '/test_data/' + image_id\n        mask_path = pseudo_path + image_id\n\n        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n        image_train[idx] = image\n        mask_train[idx] = mask\n\n    return image_train, mask_train\n\n\ndef testImageFetch(test_id):\n    image_test = np.zeros((len(test_id), 101, 101), dtype=np.float32)\n\n    for n, image_id in tqdm(enumerate(test_id), total=len(test_id)):\n        image_path = '/test_data/' + image_id + '.png'\n\n        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n        image_test[n] = img\n\n    return image_test\n"""
phalanx/submit34.py,0,"b""import argparse\nimport numpy as np\nimport pandas as pd\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--model', default='res34', type=str, help='Model version')\nparser.add_argument('--save_pred', default='/workdir/phalanx/fold_predictions/', type=str, help='prediction save space')\nparser.add_argument('--pred_path', default='/workdir/predictions/pred.npy', type=str, help='final prediction')\nargs = parser.parse_args()\nargs.save_pred += args.model + '_'\n\nif __name__ == '__main__':\n    a = np.load(args.save_pred +'pred0.npy')\n    b = np.load(args.save_pred +'pred1.npy')\n    c = np.load(args.save_pred +'pred2.npy')\n    d = np.load(args.save_pred +'pred3.npy')\n    e = np.load(args.save_pred +'pred4.npy')\n\n    a = (a+b+c+d+e) / 5.0\n    np.save(args.pred_path, a)\n"""
phalanx/train_cv.py,13,"b""import os\nimport argparse\nimport numpy as np\nimport pandas as pd\n\nfrom utils import do_kaggle_metric, get_model\nfrom lovasz_losses import lovasz_hinge, lovasz_hinge2\nfrom salt_dataset import SaltDataset, trainImageFetch, semi_trainImageFetch\nfrom unet_model import Res34Unetv3, Res34Unetv4, Res34Unetv5\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import RandomSampler\n\n# fastprogress available only for Python3.6+ and Tensorflow Docker image for Python < 3.6\n# from fastprogress import master_bar, progress_bar\n\nparser = argparse.ArgumentParser(description='')\nparser.add_argument('--model', default='res34v5', type=str, help='Model version')\nparser.add_argument('--fine_size', default=101, type=int, help='Resized image size')\nparser.add_argument('--pad_left', default=13, type=int, help='Left padding size')\nparser.add_argument('--pad_right', default=14, type=int, help='Right padding size')\nparser.add_argument('--batch_size', default=64, type=int, help='Batch size for training')\nparser.add_argument('--epoch', default=300, type=int, help='Number of training epochs')\nparser.add_argument('--snapshot', default=5, type=int, help='Number of snapshots per fold')\nparser.add_argument('--cuda', default=True, type=bool, help='Use cuda to train model')\nparser.add_argument('--save_weight', default='weights/', type=str, help='weight save space')\nparser.add_argument('--max_lr', default=0.01, type=float, help='max learning rate')\nparser.add_argument('--min_lr', default=0.001, type=float, help='min learning rate')\nparser.add_argument('--momentum', default=0.9, type=float, help='momentum for SGD')\nparser.add_argument('--weight_decay', default=1e-4, type=float, help='Weight decay for SGD')\nparser.add_argument('--is_pseudo', default=False, type=bool, help='Use pseudolabels or not')\n\nargs = parser.parse_args()\nfine_size = args.fine_size + args.pad_left + args.pad_right\nargs.weight_name = 'model_' + str(fine_size) + '_' + args.model\n\nif not os.path.isdir(args.save_weight):\n    os.mkdir(args.save_weight)\n\ndevice = torch.device('cuda' if args.cuda else 'cpu')\n\nall_id = pd.read_csv('../data/train.csv')['id'].values\ndf = pd.read_csv('../data/train_proc_v2_gr.csv')\nfold = []\nfor i in range(5):\n    fold.append(df[df['fold'] == i]['id'].values)\n\n\ndef test(test_loader, model):\n    running_loss = 0.0\n    predicts = []\n    truths = []\n\n    model.eval()\n    for inputs, masks in test_loader:\n        inputs, masks = inputs.to(device), masks.to(device)\n        with torch.set_grad_enabled(False):\n            if args.is_pseudo:\n                outputs, _, _ = model(inputs)\n            else:\n                outputs = model(inputs)\n\n            outputs = outputs[:, :, args.pad_left:args.pad_left + args.fine_size,\n                      args.pad_left:args.pad_left + args.fine_size].contiguous()\n            loss = lovasz_hinge(outputs.squeeze(1), masks.squeeze(1))\n\n        predicts.append(F.sigmoid(outputs).detach().cpu().numpy())\n        truths.append(masks.detach().cpu().numpy())\n        running_loss += loss.item() * inputs.size(0)\n\n    predicts = np.concatenate(predicts).squeeze()\n    truths = np.concatenate(truths).squeeze()\n    precision, _, _ = do_kaggle_metric(predicts, truths, 0.5)\n    precision = precision.mean()\n    epoch_loss = running_loss / val_data.__len__()\n    return epoch_loss, precision\n\n\ndef train(train_loader, model):\n    running_loss = 0.0\n    data_size = train_data.__len__()\n\n    model.train()\n    # for inputs, masks, labels in progress_bar(train_loader, parent=mb):\n    for inputs, masks, labels in train_loader:\n        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(True):\n            if args.is_pseudo:\n                logit, logit_pixel, logit_image = model(inputs)\n                loss1 = lovasz_hinge(logit.squeeze(1), masks.squeeze(1))\n                loss2 = nn.BCELoss()(logit_image, labels)\n                loss3 = lovasz_hinge2(logit_pixel.squeeze(1), masks.squeeze(1))\n                loss = loss1 + loss2 + loss3\n            else:\n                logit = model(inputs)\n                loss = lovasz_hinge(logit.squeeze(1), masks.squeeze(1))\n\n            loss.backward()\n            optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        # mb.child.comment = 'loss: {}'.format(loss.item())\n    epoch_loss = running_loss / data_size\n    return epoch_loss\n\n\nif __name__ == '__main__':\n    scheduler_step = args.epoch // args.snapshot\n    # Get Model\n    salt = get_model(args.model)\n    salt.to(device)\n    if args.model == 'res34v5':\n        param = torch.load(\n            args.save_weight + args.weight_name + '.pth')  # stage3 use model pretrained with pseudo-labels\n\n    for idx in range(5):\n        if args.model == 'res34v5':\n            salt.load_state_dict(param)  # initialize with pretained weight\n\n        # Setup optimizer\n        optimizer = torch.optim.SGD(salt.parameters(), lr=args.max_lr, momentum=args.momentum,\n                                    weight_decay=args.weight_decay)\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, args.min_lr)\n\n        # Load data\n        train_id = np.setdiff1d(all_id, fold[idx])\n        val_id = fold[idx]\n        X_train, y_train = trainImageFetch(train_id)\n        X_val, y_val = trainImageFetch(val_id)\n\n        # Load pseudo labels for stage2\n        if args.is_pseudo:\n            pseudo_id = np.load('../data/stage2_fold' + str(idx) + '.npy')\n            X_pseudo, y_pseudo = semi_trainImageFetch('../data/pseudolabels/', pseudo_id)\n            X_train = np.concatenate((X_train, X_pseudo))\n            y_train = np.concatenate((y_train, y_pseudo))\n\n        train_data = SaltDataset(X_train, mode='train', mask_list=y_train, fine_size=args.fine_size, pad_left=args.pad_left,\n                                 pad_right=args.pad_right)\n        train_loader = DataLoader(\n                            train_data,\n                            shuffle=RandomSampler(train_data),\n                            batch_size=args.batch_size,\n                            num_workers=8,\n                            pin_memory=True)\n        \n        val_data = SaltDataset(X_val, mode='val', mask_list=y_val, fine_size=args.fine_size, pad_left=args.pad_left,\n                               pad_right=args.pad_right)\n        val_loader = DataLoader(\n                            val_data,\n                            shuffle=False,\n                            batch_size=args.batch_size,\n                            num_workers=8,\n                            pin_memory=True)\n\n        num_snapshot = 0\n        best_acc = 0\n\n        # mb = master_bar(range(args.epoch))\n        # for epoch in mb:\n        for epoch in range(args.epoch):\n            train_loss = train(train_loader, salt)\n            val_loss, accuracy = test(val_loader, salt)\n            lr_scheduler.step()\n\n            if accuracy > best_acc:\n                best_acc = accuracy\n                best_param = salt.state_dict()\n\n            if (epoch + 1) % scheduler_step == 0:\n                torch.save(best_param, args.save_weight + args.weight_name + str(idx) + str(num_snapshot) + '.pth')\n                optimizer = torch.optim.SGD(salt.parameters(), lr=args.max_lr, momentum=args.momentum,\n                                            weight_decay=args.weight_decay)\n                lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, args.min_lr)\n                num_snapshot += 1\n                best_acc = 0\n\n            # mb.write('epoch: {} train_loss: {:.3f} val_loss: {:.3f} val_accuracy: {:.3f}'.format(epoch + 1, train_loss,\n            #                                                                                     val_loss, accuracy))\n            print('epoch: {} train_loss: {:.3f} val_loss: {:.3f} val_accuracy: {:.3f}'.format(epoch + 1, train_loss,\n                                                                                              val_loss, accuracy))\n"""
phalanx/train_pseudo.py,12,"b""import os\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import RandomSampler\n\n# fastprogress available only for Python3.6+ and Tensorflow Docker image for Python < 3.6\n# from fastprogress import master_bar, progress_bar\n\nfrom lovasz_losses import lovasz_hinge\nfrom utils import do_kaggle_metric, get_model\nfrom salt_dataset import SaltDataset, trainImageFetch, semi_trainImageFetch\nfrom unet_model import Res34Unetv3, Res34Unetv4, Res34Unetv5\n\nparser = argparse.ArgumentParser(description='')\nparser.add_argument('--model', default='res34v5', type=str, help='Model version')\nparser.add_argument('--fine_size', default=101, type=int, help='Resized image size')\nparser.add_argument('--pad_left', default=13, type=int, help='Left padding size')\nparser.add_argument('--pad_right', default=14, type=int, help='Right padding size')\nparser.add_argument('--batch_size', default=64, type=int, help='Batch size for training')\nparser.add_argument('--epoch', default=300, type=int, help='Number of training epochs')\nparser.add_argument('--snapshot', default=5, type=int, help='Number of snapshots per fold')\nparser.add_argument('--cuda', default=True, type=bool, help='Use cuda to train model')\nparser.add_argument('--save_weight', default='weights/stage3/', type=str, help='weight save space')\nparser.add_argument('--max_lr', default=0.01, type=float, help='max learning rate')\nparser.add_argument('--min_lr', default=0.001, type=float, help='min learning rate')\nparser.add_argument('--momentum', default=0.9, type=float, help='momentum for SGD')\nparser.add_argument('--weight_decay', default=1e-4, type=float, help='Weight decay for SGD')\nparser.add_argument('--pseudo_path', default='/workdir/data/pseudolabels_v2/', type=str, help='pseudo labels path')\n\nargs = parser.parse_args()\nimage_size = args.fine_size + args.pad_left + args.pad_right\nargs.weight_name = 'model_' + str(image_size) + '_' + args.model\n\nif not os.path.isdir(args.save_weight):\n    os.mkdir(args.save_weight)\n\ndevice = torch.device('cuda' if args.cuda and torch.cuda.is_available() else 'cpu')\n\n\n# Validation function\ndef test(test_loader, model):\n    running_loss = 0.0\n    predicts = []\n    truths = []\n\n    model.eval()\n    for inputs, masks in test_loader:\n        inputs, masks = inputs.to(device), masks.to(device)\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            outputs = outputs[:, :, args.pad_left:args.pad_left + args.fine_size,\n                      args.pad_left:args.pad_left + args.fine_size].contiguous()\n            loss = lovasz_hinge(outputs.squeeze(1), masks.squeeze(1))\n\n        predicts.append(F.sigmoid(outputs).detach().cpu().numpy())\n        truths.append(masks.detach().cpu().numpy())\n        running_loss += loss.item() * inputs.size(0)\n\n    predicts = np.concatenate(predicts).squeeze()\n    truths = np.concatenate(truths).squeeze()\n    precision, _, _ = do_kaggle_metric(predicts, truths, 0.52)\n    precision = precision.mean()\n    epoch_loss = running_loss / val_data.__len__()\n    return epoch_loss, precision\n\n\n# Training function\ndef train(train_loader, model):\n    running_loss = 0.0\n\n    model.train()\n    # for inputs, masks, labels in progress_bar(train_loader, parent=mb):\n    for inputs, masks, labels in train_loader:\n        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(True):\n            logit = model(inputs)\n            loss = lovasz_hinge(logit.squeeze(1), masks.squeeze(1))\n            loss.backward()\n            optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        # mb.child.comment = 'loss: {}'.format(loss.item())\n    epoch_loss = running_loss / train_data.__len__()\n    return epoch_loss\n\n\nif __name__ == '__main__':\n    best_acc = 0  # validation accuracy\n\n    # Setup data\n    val_id = np.load('../data/valid_id.npy')\n    image_val, mask_val = trainImageFetch(val_id)\n    image_train, mask_train = semi_trainImageFetch(args.pseudo_path)\n\n    train_data = SaltDataset(image_train, mode='train', mask_list=mask_train, fine_size=args.fine_size, pad_left=args.pad_left,\n                             pad_right=args.pad_right)\n    train_loader = DataLoader(\n                            train_data,\n                            shuffle=RandomSampler(train_data),\n                            batch_size=args.batch_size,\n                            num_workers=8,\n                            pin_memory=True)\n\n    val_data = SaltDataset(image_val, mode='val', mask_list=mask_val, fine_size=args.fine_size, pad_left=args.pad_left,\n                           pad_right=args.pad_right)\n    val_loader = DataLoader(\n                            val_data,\n                            shuffle=RandomSampler(val_data),\n                            batch_size=args.batch_size,\n                            num_workers=8,\n                            pin_memory=True)\n    # Get model\n    salt = get_model(args.model)\n    salt = salt.to(device)\n\n    # Setup optimizer and scheduler\n    scheduler_step = args.epoch // args.snapshot\n    optimizer = torch.optim.SGD(salt.parameters(), lr=args.max_lr, momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, args.min_lr)\n\n    # Start train\n    # mb = master_bar(range(args.epoch))\n    # for epoch in mb:\n    for epoch in range(args.epoch):\n        train_loss = train(train_loader, salt)\n        val_loss, precision = test(val_loader, salt)\n        lr_scheduler.step()\n\n        if (epoch + 1) % scheduler_step == 0:\n            optimizer = torch.optim.SGD(salt.parameters(), lr=args.max_lr, momentum=args.momentum,\n                                        weight_decay=args.weight_decay)\n            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_step, args.min_lr)\n            best_param = salt.state_dict()\n\n        if precision > best_acc:\n            best_acc = precision\n\n        # print train/val loss and val accuracy\n        # mb.write(\n        #     'epoch: {} train_loss: {:.3f} test_loss: {:.3f} accuracy: {:.3f}'.format(epoch + 1, train_loss, val_loss,\n        #                                                                              precision))\n        print('epoch: {} train_loss: {:.3f} test_loss: {:.3f} accuracy: {:.3f}'.format(epoch + 1, train_loss, val_loss,\n                                                                                     precision))\n\n    # save best accuracy weight\n    torch.save(best_param, args.save_weight + args.weight_name + '.pth')\n"""
phalanx/transform.py,0,"b""import cv2\nimport numpy as np\nimport math\nimport random\n\n\ndef do_resize2(image, mask, H, W):\n    image = cv2.resize(image,dsize=(W,H))\n    mask = cv2.resize(mask,dsize=(W,H))\n    mask  = (mask>0.5).astype(np.float32)\n\n    return image,mask\n\ndef do_horizontal_flip(image):\n    #flip left-right\n    image = cv2.flip(image,1)\n    return image\n\ndef do_horizontal_flip2(image,mask):\n    image = do_horizontal_flip(image)\n    mask  = do_horizontal_flip(mask )\n    return image, mask\n\n\ndef do_center_pad(image, pad_left, pad_right):\n    return np.pad(image, (pad_left, pad_right), 'edge')\n\ndef do_center_pad2(image, mask, pad_left, pad_right):\n    image = do_center_pad(image, pad_left, pad_right)\n    mask = do_center_pad(mask, pad_left, pad_right)\n    return image, mask\n\ndef do_invert_intensity(image):\n    #flip left-right\n    image = np.clip(1-image,0,1)\n    return image\n\ndef do_brightness_shift(image, alpha=0.125):\n    image = image + alpha\n    image = np.clip(image, 0, 1)\n    return image\n\n\ndef do_brightness_multiply(image, alpha=1):\n    image = alpha*image\n    image = np.clip(image, 0, 1)\n    return image\n\n\n#https://www.pyimagesearch.com/2015/10/05/opencv-gamma-correction/\ndef do_gamma(image, gamma=1.0):\n\n    image = image ** (1.0 / gamma)\n    image = np.clip(image, 0, 1)\n    return image\n\ndef do_shift_scale_crop( image, mask, x0=0, y0=0, x1=1, y1=1 ):\n    #cv2.BORDER_REFLECT_101\n    #cv2.BORDER_CONSTANT\n\n    height, width = image.shape[:2]\n    image = image[y0:y1,x0:x1]\n    mask  = mask [y0:y1,x0:x1]\n\n    image = cv2.resize(image,dsize=(width,height))\n    mask  = cv2.resize(mask,dsize=(width,height))\n    mask  = (mask>0.5).astype(np.float32)\n    return image, mask\n\n\ndef do_random_shift_scale_crop_pad2(image, mask, limit=0.10):\n\n    H, W = image.shape[:2]\n\n    dy = int(H*limit)\n    y0 =   np.random.randint(0,dy)\n    y1 = H-np.random.randint(0,dy)\n\n    dx = int(W*limit)\n    x0 =   np.random.randint(0,dx)\n    x1 = W-np.random.randint(0,dx)\n\n    #y0, y1, x0, x1\n    image, mask = do_shift_scale_crop( image, mask, x0, y0, x1, y1 )\n    return image, mask\n\n#===========================================================================\n\ndef do_shift_scale_rotate2( image, mask, dx=0, dy=0, scale=1, angle=0 ):\n    borderMode=cv2.BORDER_REFLECT_101\n    #cv2.BORDER_REFLECT_101  cv2.BORDER_CONSTANT\n\n    height, width = image.shape[:2]\n    sx = scale\n    sy = scale\n    cc = math.cos(angle/180*math.pi)*(sx)\n    ss = math.sin(angle/180*math.pi)*(sy)\n    rotate_matrix = np.array([ [cc,-ss], [ss,cc] ])\n\n    box0 = np.array([ [0,0], [width,0],  [width,height], [0,height], ],np.float32)\n    box1 = box0 - np.array([width/2,height/2])\n    box1 = np.dot(box1,rotate_matrix.T) + np.array([width/2+dx,height/2+dy])\n\n    box0 = box0.astype(np.float32)\n    box1 = box1.astype(np.float32)\n    mat  = cv2.getPerspectiveTransform(box0,box1)\n\n    image = cv2.warpPerspective(image, mat, (width,height),flags=cv2.INTER_LINEAR,\n                                borderMode=borderMode,borderValue=(0,0,0,))  #cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n    mask = cv2.warpPerspective(mask, mat, (width,height),flags=cv2.INTER_NEAREST,#cv2.INTER_LINEAR\n                                borderMode=borderMode,borderValue=(0,0,0,))  #cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n    mask  = (mask>0.5).astype(np.float32)\n    return image, mask\n\n#https://www.kaggle.com/ori226/data-augmentation-with-elastic-deformations\n#https://github.com/letmaik/lensfunpy/blob/master/lensfunpy/util.py\ndef do_elastic_transform2(image, mask, grid=32, distort=0.2):\n    borderMode=cv2.BORDER_REFLECT_101\n    height, width = image.shape[:2]\n\n    x_step = int(grid)\n    xx = np.zeros(width,np.float32)\n    prev = 0\n    for x in range(0, width, x_step):\n        start = x\n        end   = x + x_step\n        if end > width:\n            end = width\n            cur = width\n        else:\n            cur = prev + x_step*(1+random.uniform(-distort,distort))\n\n        xx[start:end] = np.linspace(prev,cur,end-start)\n        prev=cur\n\n\n    y_step = int(grid)\n    yy = np.zeros(height,np.float32)\n    prev = 0\n    for y in range(0, height, y_step):\n        start = y\n        end   = y + y_step\n        if end > height:\n            end = height\n            cur = height\n        else:\n            cur = prev + y_step*(1+random.uniform(-distort,distort))\n\n        yy[start:end] = np.linspace(prev,cur,end-start)\n        prev=cur\n\n    #grid\n    map_x,map_y =  np.meshgrid(xx, yy)\n    map_x = map_x.astype(np.float32)\n    map_y = map_y.astype(np.float32)\n\n    #image = map_coordinates(image, coords, order=1, mode='reflect').reshape(shape)\n    image = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=borderMode,borderValue=(0,0,0,))\n\n\n    mask = cv2.remap(mask, map_x, map_y, interpolation=cv2.INTER_NEAREST, borderMode=borderMode,borderValue=(0,0,0,))\n    mask  = (mask>0.5).astype(np.float32)\n    return image, mask\n\n\n\n\ndef do_horizontal_shear2( image, mask, dx=0 ):\n    borderMode=cv2.BORDER_REFLECT_101\n    #cv2.BORDER_REFLECT_101  cv2.BORDER_CONSTANT\n\n    height, width = image.shape[:2]\n    dx = int(dx*width)\n\n    box0 = np.array([ [0,0], [width,0],  [width,height], [0,height], ],np.float32)\n    box1 = np.array([ [+dx,0], [width+dx,0],  [width-dx,height], [-dx,height], ],np.float32)\n\n    box0 = box0.astype(np.float32)\n    box1 = box1.astype(np.float32)\n    mat = cv2.getPerspectiveTransform(box0,box1)\n\n    image = cv2.warpPerspective(image, mat, (width,height),flags=cv2.INTER_LINEAR,\n                                borderMode=borderMode,borderValue=(0,0,0,))  #cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n    mask  = cv2.warpPerspective(mask, mat, (width,height),flags=cv2.INTER_NEAREST,#cv2.INTER_LINEAR\n                                borderMode=borderMode,borderValue=(0,0,0,))  #cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n    mask  = (mask>0.5).astype(np.float32)\n    return image, mask"""
phalanx/unet_model.py,8,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\n\nclass FPAv2(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(FPAv2, self).__init__()\n        self.glob = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n                                  nn.Conv2d(input_dim, output_dim, kernel_size=1, bias=False))\n\n        self.down2_1 = nn.Sequential(nn.Conv2d(input_dim, input_dim, kernel_size=5, stride=2, padding=2, bias=False),\n                                     nn.BatchNorm2d(input_dim),\n                                     nn.ELU(True))\n        self.down2_2 = nn.Sequential(nn.Conv2d(input_dim, output_dim, kernel_size=5, padding=2, bias=False),\n                                     nn.BatchNorm2d(output_dim),\n                                     nn.ELU(True))\n\n        self.down3_1 = nn.Sequential(nn.Conv2d(input_dim, input_dim, kernel_size=3, stride=2, padding=1, bias=False),\n                                     nn.BatchNorm2d(input_dim),\n                                     nn.ELU(True))\n        self.down3_2 = nn.Sequential(nn.Conv2d(input_dim, output_dim, kernel_size=3, padding=1, bias=False),\n                                     nn.BatchNorm2d(output_dim),\n                                     nn.ELU(True))\n\n        self.conv1 = nn.Sequential(nn.Conv2d(input_dim, output_dim, kernel_size=1, bias=False),\n                                   nn.BatchNorm2d(output_dim),\n                                   nn.ELU(True))\n\n    def forward(self, x):\n        # x shape: 512, 16, 16\n        x_glob = self.glob(x)  # 256, 1, 1\n        x_glob = F.upsample(x_glob, scale_factor=16, mode='bilinear', align_corners=True)  # 256, 16, 16\n\n        d2 = self.down2_1(x)  # 512, 8, 8\n        d3 = self.down3_1(d2)  # 512, 4, 4\n\n        d2 = self.down2_2(d2)  # 256, 8, 8\n        d3 = self.down3_2(d3)  # 256, 4, 4\n\n        d3 = F.upsample(d3, scale_factor=2, mode='bilinear', align_corners=True)  # 256, 8, 8\n        d2 = d2 + d3\n\n        d2 = F.upsample(d2, scale_factor=2, mode='bilinear', align_corners=True)  # 256, 16, 16\n        x = self.conv1(x)  # 256, 16, 16\n        x = x * d2\n\n        x = x + x_glob\n\n        return x\n\n\ndef conv3x3(input_dim, output_dim, rate=1):\n    return nn.Sequential(nn.Conv2d(input_dim, output_dim, kernel_size=3, dilation=rate, padding=rate, bias=False),\n                         nn.BatchNorm2d(output_dim),\n                         nn.ELU(True))\n\n\nclass SpatialAttention2d(nn.Module):\n    def __init__(self, channel):\n        super(SpatialAttention2d, self).__init__()\n        self.squeeze = nn.Conv2d(channel, 1, kernel_size=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        z = self.squeeze(x)\n        z = self.sigmoid(z)\n        return x * z\n\n\nclass GAB(nn.Module):\n    def __init__(self, input_dim, reduction=4):\n        super(GAB, self).__init__()\n        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(input_dim, input_dim // reduction, kernel_size=1, stride=1)\n        self.conv2 = nn.Conv2d(input_dim // reduction, input_dim, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        z = self.global_avgpool(x)\n        z = self.relu(self.conv1(z))\n        z = self.sigmoid(self.conv2(z))\n        return x * z\n\n\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, channels, out_channels):\n        super(Decoder, self).__init__()\n        self.conv1 = conv3x3(in_channels, channels)\n        self.conv2 = conv3x3(channels, out_channels)\n        self.s_att = SpatialAttention2d(out_channels)\n        self.c_att = GAB(out_channels, 16)\n\n    def forward(self, x, e=None):\n        x = F.upsample(input=x, scale_factor=2, mode='bilinear', align_corners=True)\n        if e is not None:\n            x = torch.cat([x, e], 1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        s = self.s_att(x)\n        c = self.c_att(x)\n        output = s + c\n        return output\n\n\nclass Decoderv2(nn.Module):\n    def __init__(self, up_in, x_in, n_out):\n        super(Decoderv2, self).__init__()\n        up_out = x_out = n_out // 2\n        self.x_conv = nn.Conv2d(x_in, x_out, 1, bias=False)\n        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n        self.bn = nn.BatchNorm2d(n_out)\n        self.relu = nn.ReLU(True)\n        self.s_att = SpatialAttention2d(n_out)\n        self.c_att = GAB(n_out, 16)\n\n    def forward(self, up_p, x_p):\n        up_p = self.tr_conv(up_p)\n        x_p = self.x_conv(x_p)\n\n        cat_p = torch.cat([up_p, x_p], 1)\n        cat_p = self.relu(self.bn(cat_p))\n        s = self.s_att(cat_p)\n        c = self.c_att(cat_p)\n        return s + c\n\n\nclass SCse(nn.Module):\n    def __init__(self, dim):\n        super(SCse, self).__init__()\n        self.satt = SpatialAttention2d(dim)\n        self.catt = GAB(dim)\n\n    def forward(self, x):\n        return self.satt(x) + self.catt(x)\n\n\n# stage1 model\nclass Res34Unetv4(nn.Module):\n    def __init__(self):\n        super(Res34Unetv4, self).__init__()\n        self.resnet = torchvision.models.resnet34(True)\n\n        self.conv1 = nn.Sequential(\n            self.resnet.conv1,\n            self.resnet.bn1,\n            self.resnet.relu)\n\n        self.encode2 = nn.Sequential(self.resnet.layer1,\n                                     SCse(64))\n        self.encode3 = nn.Sequential(self.resnet.layer2,\n                                     SCse(128))\n        self.encode4 = nn.Sequential(self.resnet.layer3,\n                                     SCse(256))\n        self.encode5 = nn.Sequential(self.resnet.layer4,\n                                     SCse(512))\n\n        self.center = nn.Sequential(FPAv2(512, 256),\n                                    nn.MaxPool2d(2, 2))\n\n        self.decode5 = Decoderv2(256, 512, 64)\n        self.decode4 = Decoderv2(64, 256, 64)\n        self.decode3 = Decoderv2(64, 128, 64)\n        self.decode2 = Decoderv2(64, 64, 64)\n        self.decode1 = Decoder(64, 32, 64)\n\n        self.logit = nn.Sequential(nn.Conv2d(320, 64, kernel_size=3, padding=1),\n                                   nn.ELU(True),\n                                   nn.Conv2d(64, 1, kernel_size=1, bias=False))\n\n    def forward(self, x):\n        # x: (batch_size, 3, 256, 256)\n\n        x = self.conv1(x)  # 64, 128, 128\n        e2 = self.encode2(x)  # 64, 128, 128\n        e3 = self.encode3(e2)  # 128, 64, 64\n        e4 = self.encode4(e3)  # 256, 32, 32\n        e5 = self.encode5(e4)  # 512, 16, 16\n\n        f = self.center(e5)  # 256, 8, 8\n\n        d5 = self.decode5(f, e5)  # 64, 16, 16\n        d4 = self.decode4(d5, e4)  # 64, 32, 32\n        d3 = self.decode3(d4, e3)  # 64, 64, 64\n        d2 = self.decode2(d3, e2)  # 64, 128, 128\n        d1 = self.decode1(d2)  # 64, 256, 256\n\n        f = torch.cat((d1,\n                       F.upsample(d2, scale_factor=2, mode='bilinear', align_corners=True),\n                       F.upsample(d3, scale_factor=4, mode='bilinear', align_corners=True),\n                       F.upsample(d4, scale_factor=8, mode='bilinear', align_corners=True),\n                       F.upsample(d5, scale_factor=16, mode='bilinear', align_corners=True)), 1)  # 320, 256, 256\n\n        logit = self.logit(f)  # 1, 256, 256\n\n        return logit\n\n\n# stage2 model\nclass Res34Unetv3(nn.Module):\n    def __init__(self):\n        super(Res34Unetv3, self).__init__()\n        self.resnet = torchvision.models.resnet34(True)\n\n        self.conv1 = nn.Sequential(\n            self.resnet.conv1,\n            self.resnet.bn1,\n            self.resnet.relu)\n\n        self.encode2 = nn.Sequential(self.resnet.layer1,\n                                     SCse(64))\n        self.encode3 = nn.Sequential(self.resnet.layer2,\n                                     SCse(128))\n        self.encode4 = nn.Sequential(self.resnet.layer3,\n                                     SCse(256))\n        self.encode5 = nn.Sequential(self.resnet.layer4,\n                                     SCse(512))\n\n        self.center = nn.Sequential(FPAv2(512, 256),\n                                    nn.MaxPool2d(2, 2))\n\n        self.decode5 = Decoderv2(256, 512, 64)\n        self.decode4 = Decoderv2(64, 256, 64)\n        self.decode3 = Decoderv2(64, 128, 64)\n        self.decode2 = Decoderv2(64, 64, 64)\n        self.decode1 = Decoder(64, 32, 64)\n\n        self.dropout2d = nn.Dropout2d(0.4)\n        self.dropout = nn.Dropout(0.4)\n\n        self.fuse_pixel = conv3x3(320, 64)\n        self.logit_pixel = nn.Conv2d(64, 1, kernel_size=1, bias=False)\n\n        self.fuse_image = nn.Sequential(nn.Linear(512, 64),\n                                        nn.ELU(True))\n        self.logit_image = nn.Sequential(nn.Linear(64, 1),\n                                         nn.Sigmoid())\n        self.logit = nn.Sequential(nn.Conv2d(128, 64, kernel_size=3, padding=1, bias=False),\n                                   nn.ELU(True),\n                                   nn.Conv2d(64, 1, kernel_size=1, bias=False))\n\n    def forward(self, x):\n        # x: (batch_size, 3, 256, 256)\n        batch_size, c, h, w = x.shape\n\n        x = self.conv1(x)  # 64, 128, 128\n        e2 = self.encode2(x)  # 64, 128, 128\n        e3 = self.encode3(e2)  # 128, 64, 64\n        e4 = self.encode4(e3)  # 256, 32, 32\n        e5 = self.encode5(e4)  # 512, 16, 16\n\n        e = F.adaptive_avg_pool2d(e5, output_size=1).view(batch_size, -1)  # 512\n        e = self.dropout(e)\n\n        f = self.center(e5)  # 256, 8, 8\n\n        d5 = self.decode5(f, e5)  # 64, 16, 16\n        d4 = self.decode4(d5, e4)  # 64, 32, 32\n        d3 = self.decode3(d4, e3)  # 64, 64, 64\n        d2 = self.decode2(d3, e2)  # 64, 128, 128\n        d1 = self.decode1(d2)  # 64, 256, 256\n\n        f = torch.cat((d1,\n                       F.upsample(d2, scale_factor=2, mode='bilinear', align_corners=True),\n                       F.upsample(d3, scale_factor=4, mode='bilinear', align_corners=True),\n                       F.upsample(d4, scale_factor=8, mode='bilinear', align_corners=True),\n                       F.upsample(d5, scale_factor=16, mode='bilinear', align_corners=True)), 1)  # 320, 256, 256\n        f = self.dropout2d(f)\n\n        # segmentation process\n        fuse_pixel = self.fuse_pixel(f)  # 64, 256, 256\n        logit_pixel = self.logit_pixel(fuse_pixel)  # 1, 256, 256\n\n        # classification process\n        fuse_image = self.fuse_image(e)  # 64\n        logit_image = self.logit_image(fuse_image)  # 1\n\n        # combine segmentation and classification\n        fuse = torch.cat([fuse_pixel,\n                          F.upsample(fuse_image.view(batch_size, -1, 1, 1), scale_factor=256, mode='bilinear',\n                                     align_corners=True)], 1)  # 128, 256, 256\n        logit = self.logit(fuse)  # 1, 256, 256\n\n        return logit, logit_pixel, logit_image.view(-1)\n\n\n# stage3 model\nclass Res34Unetv5(nn.Module):\n    def __init__(self):\n        super(Res34Unetv5, self).__init__()\n        self.resnet = torchvision.models.resnet34(True)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n            self.resnet.bn1,\n            self.resnet.relu)\n\n        self.encode2 = nn.Sequential(self.resnet.layer1,\n                                     SCse(64))\n        self.encode3 = nn.Sequential(self.resnet.layer2,\n                                     SCse(128))\n        self.encode4 = nn.Sequential(self.resnet.layer3,\n                                     SCse(256))\n        self.encode5 = nn.Sequential(self.resnet.layer4,\n                                     SCse(512))\n\n        self.center = nn.Sequential(FPAv2(512, 256),\n                                    nn.MaxPool2d(2, 2))\n\n        self.decode5 = Decoderv2(256, 512, 64)\n        self.decode4 = Decoderv2(64, 256, 64)\n        self.decode3 = Decoderv2(64, 128, 64)\n        self.decode2 = Decoderv2(64, 64, 64)\n\n        self.logit = nn.Sequential(nn.Conv2d(256, 32, kernel_size=3, padding=1),\n                                   nn.ELU(True),\n                                   nn.Conv2d(32, 1, kernel_size=1, bias=False))\n\n    def forward(self, x):\n        # x: batch_size, 3, 128, 128\n        x = self.conv1(x)  # 64, 128, 128\n        e2 = self.encode2(x)  # 64, 128, 128\n        e3 = self.encode3(e2)  # 128, 64, 64\n        e4 = self.encode4(e3)  # 256, 32, 32\n        e5 = self.encode5(e4)  # 512, 16, 16\n\n        f = self.center(e5)  # 256, 8, 8\n\n        d5 = self.decode5(f, e5)  # 64, 16, 16\n        d4 = self.decode4(d5, e4)  # 64, 32, 32\n        d3 = self.decode3(d4, e3)  # 64, 64, 64\n        d2 = self.decode2(d3, e2)  # 64, 128, 128\n\n        f = torch.cat((d2,\n                       F.upsample(d3, scale_factor=2, mode='bilinear', align_corners=True),\n                       F.upsample(d4, scale_factor=4, mode='bilinear', align_corners=True),\n                       F.upsample(d5, scale_factor=8, mode='bilinear', align_corners=True)), 1)  # 256, 128, 128\n\n        f = F.dropout2d(f, p=0.4)\n        logit = self.logit(f)  # 1, 128, 128\n\n        return logit\n"""
phalanx/utils.py,0,"b'import numpy as np\nfrom unet_model import Res34Unetv3, Res34Unetv4, Res34Unetv5\n\ndef do_kaggle_metric(predict,truth, threshold=0.5):\n\n    N = len(predict)\n    predict = predict.reshape(N,-1)\n    truth   = truth.reshape(N,-1)\n\n    predict = predict>threshold\n    truth   = truth>0.5\n    intersection = truth & predict\n    union        = truth | predict\n    iou = intersection.sum(1)/(union.sum(1)+1e-8)\n\n    #-------------------------------------------\n    result = []\n    precision = []\n    is_empty_truth   = (truth.sum(1)==0)\n    is_empty_predict = (predict.sum(1)==0)\n\n    threshold = np.array([0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95])\n    for t in threshold:\n        p = iou>=t\n\n        tp  = (~is_empty_truth)  & (~is_empty_predict) & (iou> t)\n        fp  = (~is_empty_truth)  & (~is_empty_predict) & (iou<=t)\n        fn  = (~is_empty_truth)  & ( is_empty_predict)\n        fp_empty = ( is_empty_truth)  & (~is_empty_predict)\n        tn_empty = ( is_empty_truth)  & ( is_empty_predict)\n\n        p = (tp + tn_empty) / (tp + tn_empty + fp + fp_empty + fn)\n\n        result.append( np.column_stack((tp,fp,fn,tn_empty,fp_empty)) )\n        precision.append(p)\n\n    result = np.array(result).transpose(1,2,0)\n    precision = np.column_stack(precision)\n    precision = precision.mean(1)\n\n    return precision, result, threshold\n\ndef get_model(model):\n    if model == \'res34v3\':\n        return Res34Unetv3()\n    \n    elif model == \'res34v4\':\n        return Res34Unetv4()\n    \n    elif model == \'res34v5\':\n        return Res34Unetv5()\n    \n    else:\n        print(\'Error: \', model, \' is not defined.\')\n        return\n\ndef rle_decode(rle_mask):\n    \'\'\'\n    rle_mask: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    \'\'\'\n    s = rle_mask.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(101*101, dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(101,101)\n\ndef rle_encode(im):\n    \'\'\'\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    \'\'\'\n    pixels = im.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \' \'.join(str(x) for x in runs)\n\ndef RLenc(img, order=\'F\', format=True):\n    """"""\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    """"""\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = \'\'\n\n        for rr in runs:\n            z += \'{} {} \'.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs\n'"
bes/callbacks/__init__.py,0,b''
bes/callbacks/callbacks.py,0,"b'from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\nfrom callbacks.snapshot import SnapshotCallbackBuilder\nfrom params import args\n\n\ndef get_callback(callback, **kwargs):\n    if callback == \'reduce_lr\':\n        es_callback = EarlyStopping(monitor=""val_lb_metric"", patience=args.early_stop_patience, mode=\'max\')\n        reduce_lr = ReduceLROnPlateau(monitor=\'val_lb_metric\', factor=args.reduce_lr_factor,\n                                      patience=args.reduce_lr_patience,\n                                      min_lr=args.reduce_lr_min, verbose=1, mode=\'max\')\n\n        callbacks = [es_callback, reduce_lr]\n    elif callback == \'snapshot\':\n        snapshot = SnapshotCallbackBuilder(kwargs[\'weights_path\'], args.epochs, args.n_snapshots, args.learning_rate)\n        callbacks = snapshot.get_callbacks(model_prefix=\'snapshot\', fold=kwargs[\'fold\'])\n    else:\n        ValueError(""Unknown callback"")\n\n    mc_callback_best = ModelCheckpoint(kwargs[\'weights_path\'], monitor=\'val_lb_metric\', verbose=0, save_best_only=True,\n                                       save_weights_only=True, mode=\'max\', period=1)\n    callbacks.append(mc_callback_best)\n\n    return callbacks\n'"
bes/callbacks/snapshot.py,0,"b'import numpy as np\nimport os\n\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\n\nclass SnapshotModelCheckpoint(Callback):\n    """"""Callback that saves the snapshot weights of the model.\n\n    Saves the model weights on certain epochs (which can be considered the\n    snapshot of the model at that epoch).\n\n    Should be used with the cosine annealing learning rate schedule to save\n    the weight just before learning rate is sharply increased.\n\n    # Arguments:\n        nb_epochs: total number of epochs that the model will be trained for.\n        nb_snapshots: number of times the weights of the model will be saved.\n        fn_prefix: prefix for the filename of the weights.\n    """"""\n\n    def __init__(self, nb_epochs, nb_snapshots, fn_prefix, fold):\n        super(SnapshotModelCheckpoint, self).__init__()\n\n        self.check = nb_epochs // nb_snapshots\n        self.fn_prefix = fn_prefix\n        self.fold = fold\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch != 0 and (epoch + 1) % self.check == 0:\n            filepath = self.fn_prefix + ""_{snap}_fold_{fold}.hdf5"".format(snap=((epoch + 1) // self.check), fold=self.fold)\n            self.model.save_weights(filepath, overwrite=True)\n            #print(""Saved snapshot at weights/%s_%d.h5"" % (self.fn_prefix, epoch))\n\n\n\nclass SnapshotCallbackBuilder:\n    """"""Callback builder for snapshot ensemble training of a model.\n\n    Creates a list of callbacks, which are provided when training a model\n    so as to save the model weights at certain epochs, and then sharply\n    increase the learning rate.\n    """"""\n\n    def __init__(self, weights_path, nb_epochs, nb_snapshots, init_lr=0.1):\n        """"""\n        Initialize a snapshot callback builder.\n\n        # Arguments:\n            nb_epochs: total number of epochs that the model will be trained for.\n            nb_snapshots: number of times the weights of the model will be saved.\n            init_lr: initial learning rate\n        """"""\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n        self.weights_path = \'/\'.join(weights_path.split(\'/\')[:-1])\n\n    def get_callbacks(self, model_prefix=\'Model\', fold=0):\n        """"""\n        Creates a list of callbacks that can be used during training to create a\n        snapshot ensemble of the model.\n\n        Args:\n            model_prefix: prefix for the filename of the weights.\n\n        Returns: list of 3 callbacks [ModelCheckpoint, LearningRateScheduler,\n                 SnapshotModelCheckpoint] which can be provided to the \'fit\' function\n        """"""\n        if not os.path.exists(self.weights_path):\n            os.makedirs(self.weights_path)\n\n        callback_list = [\n                         callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule),\n                         SnapshotModelCheckpoint(self.T, self.M, os.path.join(self.weights_path, model_prefix), fold)]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        #return max(float(self.alpha_zero / 2 * cos_out),0.000005)\n        return float(self.alpha_zero / 2 * cos_out)\n\n    \n\n'"
bes/datasets/__init__.py,0,b''
bes/datasets/generate_folds.py,0,"b""import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom params import args\n\nif __name__ == '__main__':\n    n_fold = 5\n    depths = pd.read_csv(os.path.join('../', args.data_root, 'depths.csv'))\n    depths.sort_values('z', inplace=True)\n    depths.drop('z', axis=1, inplace=True)\n    depths['fold'] = (list(range(n_fold)) * depths.shape[0])[:depths.shape[0]]\n    df_train = pd.read_csv(os.path.join('../', args.data_root, 'train.csv'))\n\n    df_train = df_train.merge(depths)\n\n    dist = []\n    for id in df_train.id.values:\n        img = cv2.imread(os.path.join('../', args.images_dir, '{}.png'.format(id)), cv2.IMREAD_GRAYSCALE)\n        dist.append(np.unique(img).shape[0])\n    df_train['unique_pixels'] = dist\n\n    df_train[['id', 'fold', 'unique_pixels']].sample(frac=1, random_state=123).to_csv(\n        os.path.join('../', args.data_root, 'train_proc_v2_gr.csv'), index=False)\n"""
bes/datasets/generators.py,0,"b'import cv2\nimport numpy as np\nfrom params import args\nimport os\nfrom albumentations import PadIfNeeded\n\n\nclass SegmentationDataGenerator:\n    def __init__(self, input_shape=(128, 128), batch_size=32, preprocess=None, augs=None):\n        self.input_shape = input_shape\n        self.batch_size = batch_size\n        self.preprocess = preprocess\n        self.augs = augs\n\n    def _read_image_train(self, id):\n        if os.path.isfile(os.path.join(args.images_dir, \'{}.png\'.format(id))):\n            img = cv2.imread(os.path.join(args.images_dir, \'{}.png\'.format(id)), cv2.IMREAD_COLOR)\n            mask = cv2.imread(os.path.join(args.masks_dir, \'{}.png\'.format(id)), cv2.IMREAD_GRAYSCALE)\n\n        else:\n            img = cv2.imread(os.path.join(args.test_folder, \'{}.png\'.format(id)), cv2.IMREAD_COLOR)\n            mask = cv2.imread(os.path.join(args.pseudolabels_dir, \'{}.png\'.format(id)), cv2.IMREAD_GRAYSCALE)\n\n        img = np.array(img, np.float32)\n\n        if self.augs:\n            data = {""image"": img, ""mask"": mask}\n            augmented = self.augs(**data)\n            img, mask = augmented[""image""], augmented[""mask""]\n\n        img = cv2.resize(img, (args.resize_size, args.resize_size))\n        mask = cv2.resize(mask, (args.resize_size, args.resize_size))\n        augmentation = PadIfNeeded(min_height=self.input_shape[0], min_width=self.input_shape[1], p=1.0, border_mode=4)\n        data = {""image"": img, ""mask"": mask}\n        augmented = augmentation(**data)\n        img, mask = augmented[""image""], augmented[""mask""]\n\n        img = np.array(img, np.float32)\n        img = self.preprocess(img)\n\n        mask = np.array(mask / 255., np.float32)\n        if len(mask.shape) < 3:\n            mask = np.expand_dims(mask, axis=2)\n        return (img, mask)\n\n    def _read_image_valid(self, id):\n        img = cv2.imread(os.path.join(args.images_dir, \'{}.png\'.format(id)), cv2.IMREAD_COLOR)\n        mask = cv2.imread(os.path.join(args.masks_dir, \'{}.png\'.format(id)), cv2.IMREAD_GRAYSCALE)\n\n        img = np.array(img, np.float32)\n\n        img = cv2.resize(img, (args.resize_size, args.resize_size))\n        mask = cv2.resize(mask, (args.resize_size, args.resize_size))\n        augmentation = PadIfNeeded(min_height=self.input_shape[0], min_width=self.input_shape[1], p=1.0, border_mode=4)\n        data = {""image"": img, ""mask"": mask}\n        augmented = augmentation(**data)\n        img, mask = augmented[""image""], augmented[""mask""]\n\n        img = np.array(img, np.float32)\n        img = self.preprocess(img)\n\n        mask = np.array(mask / 255., np.float32)\n        if len(mask.shape) < 3:\n            mask = np.expand_dims(mask, axis=2)\n\n        return img, mask\n\n    def train_batch_generator(self, ids):\n        num_images = ids.shape[0]\n        while True:\n            idx_batch = np.random.randint(low=0, high=num_images, size=self.batch_size)\n\n            image_masks = [self._read_image_train(x) for x in ids[idx_batch]]\n\n            X = np.array([x[0] for x in image_masks])\n            y = np.array([x[1] for x in image_masks])\n\n            yield X, y\n\n    def evaluation_batch_generator(self, ids):\n        num_images = ids.shape[0]\n        while True:\n            for start in range(0, num_images, self.batch_size):\n                end = min(start + self.batch_size, num_images)\n\n                image_masks = [self._read_image_valid(x) for x in ids[start:end]]\n\n                X = np.array([x[0] for x in image_masks])\n                y = np.array([x[1] for x in image_masks])\n\n                yield X, y\n'"
bes/models/__init__.py,0,b''
bes/models/models.py,0,"b""from models.models_zoo import unet_resnext_50, unet_resnext_50_lovasz\nfrom segmentation_models.backbones.preprocessing import get_preprocessing\n\n\ndef get_model(network, input_shape, freeze_encoder):\n    if network == 'unet_resnext_50':\n        model = unet_resnext_50(input_shape, freeze_encoder)\n        return model, get_preprocessing('resnext50')\n    elif network == 'unet_resnext_50_lovasz':\n        model = unet_resnext_50_lovasz(input_shape, freeze_encoder)\n        return model, get_preprocessing('resnext50')\n    else:\n        raise ValueError('Unknown network ' + network)\n\n    return model, preprocess\n"""
bes/models/models_zoo.py,0,"b'from keras.engine.training import Model\nfrom keras.layers import SpatialDropout2D, Conv2D\nfrom segmentation_models import Unet\n\n\ndef unet_resnext_50(input_shape, freeze_encoder):\n    resnet_base, hyper_list = Unet(backbone_name=\'resnext50\',\n                                   input_shape=input_shape,\n                                   input_tensor=None,\n                                   encoder_weights=\'imagenet\',\n                                   freeze_encoder=freeze_encoder,\n                                   skip_connections=\'default\',\n                                   decoder_block_type=\'transpose\',\n                                   decoder_filters=(128, 64, 32, 16, 8),\n                                   decoder_use_batchnorm=True,\n                                   n_upsample_blocks=5,\n                                   upsample_rates=(2, 2, 2, 2, 2),\n                                   classes=1,\n                                   activation=\'sigmoid\')\n\n    x = SpatialDropout2D(0.2)(resnet_base.output)\n    x = Conv2D(1, (1, 1), activation=""sigmoid"", name=""prediction"")(x)\n\n    model = Model(resnet_base.input, x)\n\n    return model\n\n\ndef unet_resnext_50_lovasz(input_shape, freeze_encoder):\n    resnet_base, hyper_list = Unet(backbone_name=\'resnext50\',\n                                   input_shape=input_shape,\n                                   input_tensor=None,\n                                   encoder_weights=\'imagenet\',\n                                   freeze_encoder=freeze_encoder,\n                                   skip_connections=\'default\',\n                                   decoder_block_type=\'transpose\',\n                                   decoder_filters=(128, 64, 32, 16, 8),\n                                   decoder_use_batchnorm=True,\n                                   n_upsample_blocks=5,\n                                   upsample_rates=(2, 2, 2, 2, 2),\n                                   classes=1,\n                                   activation=\'sigmoid\')\n\n    x = SpatialDropout2D(0.2)(resnet_base.output)\n    x = Conv2D(1, (1, 1), name=""prediction"")(x)\n\n    model = Model(resnet_base.input, x)\n\n    return model\n'"
bes/segmentation_models/__init__.py,0,"b'name = ""segmentation_models""\n\nfrom .unet import Unet\nfrom .fpn import FPN\nfrom .linknet import Linknet\nfrom .pspnet import PSPNet'"
bes/segmentation_models/__version__.py,0,"b""VERSION = (0, 1, 1)\n\n__version__ = '.'.join(map(str, VERSION))"""
bes/segmentation_models/utils.py,0,"b'"""""" Utility functions for segmentation models """"""\nfrom functools import wraps\nimport numpy as np\n\ndef get_layer_number(model, layer_name):\n    """"""\n    Help find layer in Keras model by name\n    Args:\n        model: Keras `Model`\n        layer_name: str, name of layer\n\n    Returns:\n        index of layer\n\n    Raises:\n        ValueError: if model does not contains layer with such name\n    """"""\n    for i, l in enumerate(model.layers):\n        if l.name == layer_name:\n            return i\n    raise ValueError(\'No layer with name {} in  model {}.\'.format(layer_name, model.name))\n\n\ndef extract_outputs(model, layers, include_top=False):\n    """"""\n    Help extract intermediate layer outputs from model\n    Args:\n        model: Keras `Model`\n        layer: list of integers/str, list of layers indexes or names to extract output\n        include_top: bool, include final model layer output\n\n    Returns:\n        list of tensors (outputs)\n    """"""\n    layers_indexes = ([get_layer_number(model, l) if isinstance(l, str) else l\n                      for l in layers])\n    outputs = [model.layers[i].output for i in layers_indexes]\n\n    if include_top:\n        outputs.insert(0, model.output)\n\n    return outputs\n\n\ndef reverse(l):\n    """"""Reverse list""""""\n    return list(reversed(l))\n\n\n# decorator for models aliases, to add doc string\ndef add_docstring(doc_string=None):\n    def decorator(fn):\n        if fn.__doc__:\n            fn.__doc__ += doc_string\n        else:\n            fn.__doc__ = doc_string\n\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            return fn(*args, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef recompile(model):\n    model.compile(model.optimizer, model.loss, model.metrics)    \n\n    \ndef freeze_model(model):\n    for layer in model.layers:\n        layer.trainable = False\n    return\n\n\ndef set_trainable(model):\n    for layer in model.layers:\n        layer.trainable = True\n    recompile(model)\n\n\ndef to_tuple(x):\n    if isinstance(x, tuple):\n        if len(x) == 2:\n            return x\n    elif np.isscalar(x):\n        return (x, x)\n\n    raise ValueError(\'Value should be tuple of length 2 or int value, got ""{}""\'.format(x))\n'"
bes/segmentation_models/backbones/__init__.py,0,b'from .classification_models.classification_models import *\nfrom .inception_resnet_v2 import InceptionResNetV2\nfrom .inception_v3 import InceptionV3\n\nfrom .backbones import get_backbone\nfrom .preprocessing import get_preprocessing'
bes/segmentation_models/backbones/backbones.py,0,"b'\nfrom .classification_models.classification_models import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\nfrom .classification_models.classification_models import ResNeXt50, ResNeXt101\n\nfrom .inception_resnet_v2 import InceptionResNetV2\nfrom .inception_v3 import InceptionV3\n\nfrom keras.applications import DenseNet121, DenseNet169, DenseNet201\nfrom keras.applications import VGG16\nfrom keras.applications import VGG19\n\n\nbackbones = {\n    ""vgg16"": VGG16,\n    ""vgg19"": VGG19,\n    ""resnet18"": ResNet18,\n    ""resnet34"": ResNet34,\n    ""resnet50"": ResNet50,\n    ""resnet101"": ResNet101,\n    ""resnet152"": ResNet152,\n    ""resnext50"": ResNeXt50,\n    ""resnext101"": ResNeXt101,\n    ""inceptionresnetv2"": InceptionResNetV2,\n    ""inceptionv3"": InceptionV3,\n    ""densenet121"": DenseNet121,\n    ""densenet169"": DenseNet169,\n    ""densenet201"": DenseNet201,\n\n}\n\ndef get_backbone(name, *args, **kwargs):\n    return backbones[name](*args, **kwargs)'"
bes/segmentation_models/backbones/inception_resnet_v2.py,0,"b'# -*- coding: utf-8 -*-\n""""""Inception-ResNet V2 model for Keras.\nModel naming and structure follows TF-slim implementation (which has some additional\nlayers and different number of filters from the original arXiv paper):\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py\nPre-trained ImageNet weights are also converted from TF-slim, which can be found in:\nhttps://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models\n# Reference\n- [Inception-v4, Inception-ResNet and the Impact of\n   Residual Connections on Learning](https://arxiv.org/abs/1602.07261)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Activation\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Conv2D\nfrom keras.layers import Concatenate\nfrom keras.layers import Dense\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers import Input\nfrom keras.layers import Lambda\nfrom keras.layers import MaxPooling2D\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.applications import imagenet_utils\nfrom keras import backend as K\n\n\nimport keras\nfrom distutils.version import StrictVersion\n\nif StrictVersion(keras.__version__) < StrictVersion(\'2.2.0\'):\n    from keras.applications.imagenet_utils import _obtain_input_shape\nelse:\n    from keras_applications.imagenet_utils import _obtain_input_shape\n\n\nBASE_WEIGHT_URL = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.7/\'\n\n\ndef preprocess_input(x):\n    """"""Preprocesses a numpy array encoding a batch of images.\n    # Arguments\n        x: a 4D numpy array consists of RGB values within [0, 255].\n    # Returns\n        Preprocessed array.\n    """"""\n    return imagenet_utils.preprocess_input(x, mode=\'tf\')\n\n\ndef conv2d_bn(x,\n              filters,\n              kernel_size,\n              strides=1,\n              padding=\'same\',\n              activation=\'relu\',\n              use_bias=False,\n              name=None):\n    """"""Utility function to apply conv + BN.\n    # Arguments\n        x: input tensor.\n        filters: filters in `Conv2D`.\n        kernel_size: kernel size as in `Conv2D`.\n        strides: strides in `Conv2D`.\n        padding: padding mode in `Conv2D`.\n        activation: activation in `Conv2D`.\n        use_bias: whether to use a bias in `Conv2D`.\n        name: name of the ops; will become `name + \'_ac\'` for the activation\n            and `name + \'_bn\'` for the batch norm layer.\n    # Returns\n        Output tensor after applying `Conv2D` and `BatchNormalization`.\n    """"""\n    x = Conv2D(filters,\n               kernel_size,\n               strides=strides,\n               padding=padding,\n               use_bias=use_bias,\n               name=name)(x)\n    if not use_bias:\n        bn_axis = 1 if K.image_data_format() == \'channels_first\' else 3\n        bn_name = None if name is None else name + \'_bn\'\n        x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n    if activation is not None:\n        ac_name = None if name is None else name + \'_ac\'\n        x = Activation(activation, name=ac_name)(x)\n    return x\n\n\ndef inception_resnet_block(x, scale, block_type, block_idx, activation=\'relu\'):\n    """"""Adds a Inception-ResNet block.\n    This function builds 3 types of Inception-ResNet blocks mentioned\n    in the paper, controlled by the `block_type` argument (which is the\n    block name used in the official TF-slim implementation):\n        - Inception-ResNet-A: `block_type=\'block35\'`\n        - Inception-ResNet-B: `block_type=\'block17\'`\n        - Inception-ResNet-C: `block_type=\'block8\'`\n    # Arguments\n        x: input tensor.\n        scale: scaling factor to scale the residuals (i.e., the output of\n            passing `x` through an inception module) before adding them\n            to the shortcut branch. Let `r` be the output from the residual branch,\n            the output of this block will be `x + scale * r`.\n        block_type: `\'block35\'`, `\'block17\'` or `\'block8\'`, determines\n            the network structure in the residual branch.\n        block_idx: an `int` used for generating layer names. The Inception-ResNet blocks\n            are repeated many times in this network. We use `block_idx` to identify\n            each of the repetitions. For example, the first Inception-ResNet-A block\n            will have `block_type=\'block35\', block_idx=0`, ane the layer names will have\n            a common prefix `\'block35_0\'`.\n        activation: activation function to use at the end of the block\n            (see [activations](../activations.md)).\n            When `activation=None`, no activation is applied\n            (i.e., ""linear"" activation: `a(x) = x`).\n    # Returns\n        Output tensor for the block.\n    # Raises\n        ValueError: if `block_type` is not one of `\'block35\'`,\n            `\'block17\'` or `\'block8\'`.\n    """"""\n    if block_type == \'block35\':\n        branch_0 = conv2d_bn(x, 32, 1)\n        branch_1 = conv2d_bn(x, 32, 1)\n        branch_1 = conv2d_bn(branch_1, 32, 3)\n        branch_2 = conv2d_bn(x, 32, 1)\n        branch_2 = conv2d_bn(branch_2, 48, 3)\n        branch_2 = conv2d_bn(branch_2, 64, 3)\n        branches = [branch_0, branch_1, branch_2]\n    elif block_type == \'block17\':\n        branch_0 = conv2d_bn(x, 192, 1)\n        branch_1 = conv2d_bn(x, 128, 1)\n        branch_1 = conv2d_bn(branch_1, 160, [1, 7])\n        branch_1 = conv2d_bn(branch_1, 192, [7, 1])\n        branches = [branch_0, branch_1]\n    elif block_type == \'block8\':\n        branch_0 = conv2d_bn(x, 192, 1)\n        branch_1 = conv2d_bn(x, 192, 1)\n        branch_1 = conv2d_bn(branch_1, 224, [1, 3])\n        branch_1 = conv2d_bn(branch_1, 256, [3, 1])\n        branches = [branch_0, branch_1]\n    else:\n        raise ValueError(\'Unknown Inception-ResNet block type. \'\n                         \'Expects ""block35"", ""block17"" or ""block8"", \'\n                         \'but got: \' + str(block_type))\n\n    block_name = block_type + \'_\' + str(block_idx)\n    channel_axis = 1 if K.image_data_format() == \'channels_first\' else 3\n    mixed = Concatenate(axis=channel_axis, name=block_name + \'_mixed\')(branches)\n    up = conv2d_bn(mixed,\n                   K.int_shape(x)[channel_axis],\n                   1,\n                   activation=None,\n                   use_bias=True,\n                   name=block_name + \'_conv\')\n\n    x = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n               output_shape=K.int_shape(x)[1:],\n               arguments={\'scale\': scale},\n               name=block_name)([x, up])\n\n    if activation is not None:\n        x = Activation(activation, name=block_name + \'_ac\')(x)\n    return x\n\n\ndef InceptionResNetV2(include_top=True,\n                      weights=\'imagenet\',\n                      input_tensor=None,\n                      input_shape=None,\n                      pooling=None,\n                      classes=1000):\n    """"""Instantiates the Inception-ResNet v2 architecture.\n    Optionally loads weights pre-trained on ImageNet.\n    Note that when using TensorFlow, for best performance you should\n    set `""image_data_format"": ""channels_last""` in your Keras config\n    at `~/.keras/keras.json`.\n    The model and the weights are compatible with TensorFlow, Theano and\n    CNTK backends. The data format convention used by the model is\n    the one specified in your Keras config file.\n    Note that the default input image size for this model is 299x299, instead\n    of 224x224 as in the VGG16 and ResNet models. Also, the input preprocessing\n    function is different (i.e., do not use `imagenet_utils.preprocess_input()`\n    with this model. Use `preprocess_input()` defined in this module instead).\n    # Arguments\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: one of `None` (random initialization),\n              \'imagenet\' (pre-training on ImageNet),\n              or the path to the weights file to be loaded.\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is `False` (otherwise the input shape\n            has to be `(299, 299, 3)` (with `\'channels_last\'` data format)\n            or `(3, 299, 299)` (with `\'channels_first\'` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 139.\n            E.g. `(150, 150, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the last convolutional layer.\n            - `\'avg\'` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `\'max\'` means that global max pooling will be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is `True`, and\n            if no `weights` argument is specified.\n    # Returns\n        A Keras `Model` instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    """"""\n    if not (weights in {\'imagenet\', None} or os.path.exists(weights)):\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization), `imagenet` \'\n                         \'(pre-training on ImageNet), \'\n                         \'or the path to the weights file to be loaded.\')\n\n    if weights == \'imagenet\' and include_top and classes != 1000:\n        raise ValueError(\'If using `weights` as imagenet with `include_top`\'\n                         \' as true, `classes` should be 1000\')\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(\n        input_shape,\n        default_size=299,\n        min_size=139,\n        data_format=K.image_data_format(),\n        require_flatten=False,\n        weights=weights)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    # Stem block: 35 x 35 x 192\n    x = conv2d_bn(img_input, 32, 3, strides=2, padding=\'same\')\n    x = conv2d_bn(x, 32, 3, padding=\'same\')\n    x = conv2d_bn(x, 64, 3)\n    x = MaxPooling2D(3, strides=2, padding=\'same\')(x)\n    x = conv2d_bn(x, 80, 1, padding=\'same\')\n    x = conv2d_bn(x, 192, 3, padding=\'same\')\n    x = MaxPooling2D(3, strides=2, padding=\'same\')(x)\n\n    # Mixed 5b (Inception-A block): 35 x 35 x 320\n    branch_0 = conv2d_bn(x, 96, 1)\n    branch_1 = conv2d_bn(x, 48, 1)\n    branch_1 = conv2d_bn(branch_1, 64, 5)\n    branch_2 = conv2d_bn(x, 64, 1)\n    branch_2 = conv2d_bn(branch_2, 96, 3)\n    branch_2 = conv2d_bn(branch_2, 96, 3)\n    branch_pool = AveragePooling2D(3, strides=1, padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 64, 1)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    channel_axis = 1 if K.image_data_format() == \'channels_first\' else 3\n    x = Concatenate(axis=channel_axis, name=\'mixed_5b\')(branches)\n\n    # 10x block35 (Inception-ResNet-A block): 35 x 35 x 320\n    for block_idx in range(1, 11):\n        x = inception_resnet_block(x,\n                                   scale=0.17,\n                                   block_type=\'block35\',\n                                   block_idx=block_idx)\n\n    # Mixed 6a (Reduction-A block): 17 x 17 x 1088\n    branch_0 = conv2d_bn(x, 384, 3, strides=2, padding=\'same\')\n    branch_1 = conv2d_bn(x, 256, 1)\n    branch_1 = conv2d_bn(branch_1, 256, 3)\n    branch_1 = conv2d_bn(branch_1, 384, 3, strides=2, padding=\'same\')\n    branch_pool = MaxPooling2D(3, strides=2, padding=\'same\')(x)\n    branches = [branch_0, branch_1, branch_pool]\n    x = Concatenate(axis=channel_axis, name=\'mixed_6a\')(branches)\n\n    # 20x block17 (Inception-ResNet-B block): 17 x 17 x 1088\n    for block_idx in range(1, 21):\n        x = inception_resnet_block(x,\n                                   scale=0.1,\n                                   block_type=\'block17\',\n                                   block_idx=block_idx)\n\n    # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n    branch_0 = conv2d_bn(x, 256, 1)\n    branch_0 = conv2d_bn(branch_0, 384, 3, strides=2, padding=\'same\')\n    branch_1 = conv2d_bn(x, 256, 1)\n    branch_1 = conv2d_bn(branch_1, 288, 3, strides=2, padding=\'same\')\n    branch_2 = conv2d_bn(x, 256, 1)\n    branch_2 = conv2d_bn(branch_2, 288, 3)\n    branch_2 = conv2d_bn(branch_2, 320, 3, strides=2, padding=\'same\')\n    branch_pool = MaxPooling2D(3, strides=2, padding=\'same\')(x)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    x = Concatenate(axis=channel_axis, name=\'mixed_7a\')(branches)\n\n    # 10x block8 (Inception-ResNet-C block): 8 x 8 x 2080\n    for block_idx in range(1, 10):\n        x = inception_resnet_block(x,\n                                   scale=0.2,\n                                   block_type=\'block8\',\n                                   block_idx=block_idx)\n    x = inception_resnet_block(x,\n                               scale=1.,\n                               activation=None,\n                               block_type=\'block8\',\n                               block_idx=10)\n\n    # Final convolution block: 8 x 8 x 1536\n    x = conv2d_bn(x, 1536, 1, name=\'conv_7b\')\n\n    if include_top:\n        # Classification block\n        x = GlobalAveragePooling2D(name=\'avg_pool\')(x)\n        x = Dense(classes, activation=\'softmax\', name=\'predictions\')(x)\n    else:\n        if pooling == \'avg\':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == \'max\':\n            x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n\n    # Create model\n    model = Model(inputs, x, name=\'inception_resnet_v2\')\n\n    # Load weights\n    if weights == \'imagenet\':\n        if K.image_data_format() == \'channels_first\':\n            if K.backend() == \'tensorflow\':\n                warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                              \'are using the Theano \'\n                              \'image data format convention \'\n                              \'(`image_data_format=""channels_first""`). \'\n                              \'For best performance, set \'\n                              \'`image_data_format=""channels_last""` in \'\n                              \'your Keras config \'\n                              \'at ~/.keras/keras.json.\')\n        if include_top:\n            fname = \'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\'\n            weights_path = get_file(fname,\n                                    BASE_WEIGHT_URL + fname,\n                                    cache_subdir=\'models\',\n                                    file_hash=\'e693bd0210a403b3192acc6073ad2e96\')\n        else:\n            fname = \'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\'\n            weights_path = get_file(fname,\n                                    BASE_WEIGHT_URL + fname,\n                                    cache_subdir=\'models\',\n                                    file_hash=\'d19885ff4a710c122648d3b5c3b684e4\')\n        model.load_weights(weights_path)\n    elif weights is not None:\n        model.load_weights(weights)\n\n    return model'"
bes/segmentation_models/backbones/inception_v3.py,0,"b'# -*- coding: utf-8 -*-\n""""""Inception V3 model for Keras.\nNote that the input image format for this model is different than for\nthe VGG16 and ResNet models (299x299 instead of 224x224),\nand that the input preprocessing function is also different (same as Xception).\n# Reference\n- [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport warnings\n\nfrom keras.models import Model\nfrom keras import layers\nfrom keras.layers import Activation\nfrom keras.layers import Dense\nfrom keras.layers import Input\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.engine.topology import get_source_inputs\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\nfrom keras.applications import imagenet_utils\n\nimport keras\nfrom distutils.version import StrictVersion\n\nif StrictVersion(keras.__version__) < StrictVersion(\'2.2.0\'):\n    from keras.applications.imagenet_utils import _obtain_input_shape\nelse:\n    from keras_applications.imagenet_utils import _obtain_input_shape\n\n\nWEIGHTS_PATH = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\'\nWEIGHTS_PATH_NO_TOP = \'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\'\n\n\ndef conv2d_bn(x,\n              filters,\n              num_row,\n              num_col,\n              padding=\'same\',\n              strides=(1, 1),\n              name=None):\n    """"""Utility function to apply conv + BN.\n    # Arguments\n        x: input tensor.\n        filters: filters in `Conv2D`.\n        num_row: height of the convolution kernel.\n        num_col: width of the convolution kernel.\n        padding: padding mode in `Conv2D`.\n        strides: strides in `Conv2D`.\n        name: name of the ops; will become `name + \'_conv\'`\n            for the convolution and `name + \'_bn\'` for the\n            batch norm layer.\n    # Returns\n        Output tensor after applying `Conv2D` and `BatchNormalization`.\n    """"""\n    if name is not None:\n        bn_name = name + \'_bn\'\n        conv_name = name + \'_conv\'\n    else:\n        bn_name = None\n        conv_name = None\n    if K.image_data_format() == \'channels_first\':\n        bn_axis = 1\n    else:\n        bn_axis = 3\n    x = Conv2D(\n        filters, (num_row, num_col),\n        strides=strides,\n        padding=padding,\n        use_bias=False,\n        name=conv_name)(x)\n    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n    x = Activation(\'relu\', name=name)(x)\n    return x\n\n\ndef InceptionV3(include_top=True,\n                weights=\'imagenet\',\n                input_tensor=None,\n                input_shape=None,\n                pooling=None,\n                classes=1000):\n    """"""Instantiates the Inception v3 architecture.\n    Optionally loads weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_data_format=\'channels_last\'` in your Keras config\n    at ~/.keras/keras.json.\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The data format\n    convention used by the model is the one\n    specified in your Keras config file.\n    Note that the default input image size for this model is 299x299.\n    # Arguments\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: one of `None` (random initialization),\n              \'imagenet\' (pre-training on ImageNet),\n              or the path to the weights file to be loaded.\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(299, 299, 3)` (with `channels_last` data format)\n            or `(3, 299, 299)` (with `channels_first` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 139.\n            E.g. `(150, 150, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    """"""\n    if not (weights in {\'imagenet\', None} or os.path.exists(weights)):\n        raise ValueError(\'The `weights` argument should be either \'\n                         \'`None` (random initialization), `imagenet` \'\n                         \'(pre-training on ImageNet), \'\n                         \'or the path to the weights file to be loaded.\')\n\n    if weights == \'imagenet\' and include_top and classes != 1000:\n        raise ValueError(\'If using `weights` as imagenet with `include_top`\'\n                         \' as true, `classes` should be 1000\')\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(\n        input_shape,\n        default_size=299,\n        min_size=139,\n        data_format=K.image_data_format(),\n        require_flatten=False,\n        weights=weights)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    if K.image_data_format() == \'channels_first\':\n        channel_axis = 1\n    else:\n        channel_axis = 3\n\n    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding=\'same\')\n    x = conv2d_bn(x, 32, 3, 3, padding=\'same\')\n    x = conv2d_bn(x, 64, 3, 3)\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n\n    x = conv2d_bn(x, 80, 1, 1, padding=\'same\')\n    x = conv2d_bn(x, 192, 3, 3, padding=\'same\')\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n\n    # mixed 0, 1, 2: 35 x 35 x 256\n    branch1x1 = conv2d_bn(x, 64, 1, 1)\n\n    branch5x5 = conv2d_bn(x, 48, 1, 1)\n    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n\n    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n\n    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed0\')\n\n    # mixed 1: 35 x 35 x 256\n    branch1x1 = conv2d_bn(x, 64, 1, 1)\n\n    branch5x5 = conv2d_bn(x, 48, 1, 1)\n    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n\n    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n\n    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed1\')\n\n    # mixed 2: 35 x 35 x 256\n    branch1x1 = conv2d_bn(x, 64, 1, 1)\n\n    branch5x5 = conv2d_bn(x, 48, 1, 1)\n    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n\n    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n\n    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed2\')\n\n    # mixed 3: 17 x 17 x 768\n    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding=\'same\')\n\n    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n    branch3x3dbl = conv2d_bn(\n        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding=\'same\')\n\n    branch_pool = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n    x = layers.concatenate(\n        [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name=\'mixed3\')\n\n    # mixed 4: 17 x 17 x 768\n    branch1x1 = conv2d_bn(x, 192, 1, 1)\n\n    branch7x7 = conv2d_bn(x, 128, 1, 1)\n    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n\n    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n\n    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed4\')\n\n    # mixed 5, 6: 17 x 17 x 768\n    for i in range(2):\n        branch1x1 = conv2d_bn(x, 192, 1, 1)\n\n        branch7x7 = conv2d_bn(x, 160, 1, 1)\n        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n\n        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n\n        branch_pool = AveragePooling2D(\n            (3, 3), strides=(1, 1), padding=\'same\')(x)\n        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n        x = layers.concatenate(\n            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n            axis=channel_axis,\n            name=\'mixed\' + str(5 + i))\n\n    # mixed 7: 17 x 17 x 768\n    branch1x1 = conv2d_bn(x, 192, 1, 1)\n\n    branch7x7 = conv2d_bn(x, 192, 1, 1)\n    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n\n    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n\n    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding=\'same\')(x)\n    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n    x = layers.concatenate(\n        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n        axis=channel_axis,\n        name=\'mixed7\')\n\n    # mixed 8: 8 x 8 x 1280\n    branch3x3 = conv2d_bn(x, 192, 1, 1)\n    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n                          strides=(2, 2), padding=\'same\')\n\n    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n    branch7x7x3 = conv2d_bn(\n        branch7x7x3, 192, 3, 3, strides=(2, 2), padding=\'same\')\n\n    branch_pool = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n    x = layers.concatenate(\n        [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name=\'mixed8\')\n\n    # mixed 9: 8 x 8 x 2048\n    for i in range(2):\n        branch1x1 = conv2d_bn(x, 320, 1, 1)\n\n        branch3x3 = conv2d_bn(x, 384, 1, 1)\n        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n        branch3x3 = layers.concatenate(\n            [branch3x3_1, branch3x3_2], axis=channel_axis, name=\'mixed9_\' + str(i))\n\n        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n        branch3x3dbl = layers.concatenate(\n            [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n\n        branch_pool = AveragePooling2D(\n            (3, 3), strides=(1, 1), padding=\'same\')(x)\n        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n        x = layers.concatenate(\n            [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n            axis=channel_axis,\n            name=\'mixed\' + str(9 + i))\n\n    if include_top:\n        # Classification block\n        x = GlobalAveragePooling2D(name=\'avg_pool\')(x)\n        x = Dense(classes, activation=\'softmax\', name=\'predictions\')(x)\n    else:\n        if pooling == \'avg\':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == \'max\':\n            x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name=\'inception_v3\')\n\n    # load weights\n    if weights == \'imagenet\':\n        if K.image_data_format() == \'channels_first\':\n            if K.backend() == \'tensorflow\':\n                warnings.warn(\'You are using the TensorFlow backend, yet you \'\n                              \'are using the Theano \'\n                              \'image data format convention \'\n                              \'(`image_data_format=""channels_first""`). \'\n                              \'For best performance, set \'\n                              \'`image_data_format=""channels_last""` in \'\n                              \'your Keras config \'\n                              \'at ~/.keras/keras.json.\')\n        if include_top:\n            weights_path = get_file(\n                \'inception_v3_weights_tf_dim_ordering_tf_kernels.h5\',\n                WEIGHTS_PATH,\n                cache_subdir=\'models\',\n                file_hash=\'9a0d58056eeedaa3f26cb7ebd46da564\')\n        else:\n            weights_path = get_file(\n                \'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\',\n                WEIGHTS_PATH_NO_TOP,\n                cache_subdir=\'models\',\n                file_hash=\'bcbd6486424b2319ff4ef7d526e38f63\')\n        model.load_weights(weights_path)\n    elif weights is not None:\n        model.load_weights(weights)\n\n    return model\n\n\ndef preprocess_input(x):\n    """"""Preprocesses a numpy array encoding a batch of images.\n    # Arguments\n        x: a 4D numpy array consists of RGB values within [0, 255].\n    # Returns\n        Preprocessed array.\n    """"""\n    return imagenet_utils.preprocess_input(x, mode=\'tf\')\n'"
bes/segmentation_models/backbones/preprocessing.py,0,"b'""""""\nImage pre-processing functions.\nImages are assumed to be read in uint8 format (range 0-255).\n""""""\n\nfrom keras.applications import vgg16\nfrom keras.applications import vgg19\nfrom keras.applications import densenet\nfrom keras.applications import inception_v3\nfrom keras.applications import inception_resnet_v2\n\nidentical = lambda x: x\nbgr_transpose = lambda x: x[..., ::-1]\n\nmodels_preprocessing = {\n    \'vgg16\': vgg16.preprocess_input,\n    \'vgg19\': vgg19.preprocess_input,\n    \'resnet18\': bgr_transpose,\n    \'resnet34\': bgr_transpose,\n    \'resnet50\': bgr_transpose,\n    \'resnet101\': bgr_transpose,\n    \'resnet152\': bgr_transpose,\n    \'resnext50\': identical,\n    \'resnext101\': identical,\n    \'densenet121\': densenet.preprocess_input,\n    \'densenet169\': densenet.preprocess_input,\n    \'densenet201\': densenet.preprocess_input,\n    \'inceptionv3\': inception_v3.preprocess_input,\n    \'inceptionresnetv2\': inception_resnet_v2.preprocess_input,\n}\n\n\ndef get_preprocessing(backbone):\n    return models_preprocessing[backbone]\n'"
bes/segmentation_models/common/__init__.py,0,b'from .blocks import Conv2DBlock\nfrom .layers import ResizeImage'
bes/segmentation_models/common/blocks.py,0,"b'from keras.layers import Conv2D\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\n\n\ndef Conv2DBlock(n_filters, kernel_size,\n                activation=\'relu\',\n                use_batchnorm=True,\n                name=\'conv_block\',\n                **kwargs):\n    """"""Extension of Conv2D layer with batchnorm""""""\n    def layer(input_tensor):\n\n        x = Conv2D(n_filters, kernel_size, use_bias=not(use_batchnorm),\n                   name=name+\'_conv\', **kwargs)(input_tensor)\n        if use_batchnorm:\n            x = BatchNormalization(name=name+\'_bn\',)(x)\n        x = Activation(activation, name=name+\'_\'+activation)(x)\n\n        return x\n    return layer'"
bes/segmentation_models/common/functions.py,0,"b'import numpy as np\nimport tensorflow as tf\n\n\ndef transpose_shape(shape, target_format, spatial_axes):\n    """"""Converts a tuple or a list to the correct `data_format`.\n    It does so by switching the positions of its elements.\n    # Arguments\n        shape: Tuple or list, often representing shape,\n            corresponding to `\'channels_last\'`.\n        target_format: A string, either `\'channels_first\'` or `\'channels_last\'`.\n        spatial_axes: A tuple of integers.\n            Correspond to the indexes of the spatial axes.\n            For example, if you pass a shape\n            representing (batch_size, timesteps, rows, cols, channels),\n            then `spatial_axes=(2, 3)`.\n    # Returns\n        A tuple or list, with the elements permuted according\n        to `target_format`.\n    # Example\n    # Raises\n        ValueError: if `value` or the global `data_format` invalid.\n    """"""\n    if target_format == \'channels_first\':\n        new_values = shape[:spatial_axes[0]]\n        new_values += (shape[-1],)\n        new_values += tuple(shape[x] for x in spatial_axes)\n\n        if isinstance(shape, list):\n            return list(new_values)\n        return new_values\n    elif target_format == \'channels_last\':\n        return shape\n    else:\n        raise ValueError(\'The `data_format` argument must be one of \'\n                         \'""channels_first"", ""channels_last"". Received: \' +\n                         str(target_format))\n\n\ndef permute_dimensions(x, pattern):\n    """"""Permutes axes in a tensor.\n    # Arguments\n        x: Tensor or variable.\n        pattern: A tuple of\n            dimension indices, e.g. `(0, 2, 1)`.\n    # Returns\n        A tensor.\n    """"""\n    return tf.transpose(x, perm=pattern)\n\n\ndef int_shape(x):\n    """"""Returns the shape of tensor or variable as a tuple of int or None entries.\n    # Arguments\n        x: Tensor or variable.\n    # Returns\n        A tuple of integers (or None entries).\n    """"""\n    if hasattr(x, \'_keras_shape\'):\n        return x._keras_shape\n    try:\n        return tuple(x.get_shape().as_list())\n    except ValueError:\n        return None\n\n\ndef resize_images(x,\n                  height_factor,\n                  width_factor,\n                  data_format,\n                  interpolation=\'nearest\'):\n    """"""Resizes the images contained in a 4D tensor.\n    # Arguments\n        x: Tensor or variable to resize.\n        height_factor: Positive integer.\n        width_factor: Positive integer.\n        data_format: string, `""channels_last""` or `""channels_first""`.\n        interpolation: A string, one of `nearest` or `bilinear`.\n    # Returns\n        A tensor.\n    # Raises\n        ValueError: if `data_format` is neither `""channels_last""` or `""channels_first""`.\n    """"""\n    if data_format == \'channels_first\':\n        rows, cols = 2, 3\n    else:\n        rows, cols = 1, 2\n\n    original_shape = int_shape(x)\n    new_shape = tf.shape(x)[rows:cols + 1]\n    new_shape *= tf.constant(np.array([height_factor, width_factor], dtype=\'int32\'))\n\n    if data_format == \'channels_first\':\n        x = permute_dimensions(x, [0, 2, 3, 1])\n    if interpolation == \'nearest\':\n        x = tf.image.resize_nearest_neighbor(x, new_shape)\n    elif interpolation == \'bilinear\':\n        x = tf.image.resize_bilinear(x, new_shape)\n    else:\n        raise ValueError(\'interpolation should be one \'\n                         \'of ""nearest"" or ""bilinear"".\')\n    if data_format == \'channels_first\':\n        x = permute_dimensions(x, [0, 3, 1, 2])\n\n    if original_shape[rows] is None:\n        new_height = None\n    else:\n        new_height = original_shape[rows] * height_factor\n\n    if original_shape[cols] is None:\n        new_width = None\n    else:\n        new_width = original_shape[cols] * width_factor\n\n    output_shape = (None, new_height, new_width, None)\n    x.set_shape(transpose_shape(output_shape, data_format, spatial_axes=(1, 2)))\n    return x'"
bes/segmentation_models/common/layers.py,0,"b'from keras.engine import Layer\nfrom keras.engine import InputSpec\nfrom keras.utils import conv_utils\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom .functions import resize_images\n\n\nclass ResizeImage(Layer):\n    """"""ResizeImage layer for 2D inputs.\n    Repeats the rows and columns of the data\n    by factor[0] and factor[1] respectively.\n    # Arguments\n        factor: int, or tuple of 2 integers.\n            The upsampling factors for rows and columns.\n        data_format: A string,\n            one of `""channels_last""` or `""channels_first""`.\n            The ordering of the dimensions in the inputs.\n            `""channels_last""` corresponds to inputs with shape\n            `(batch, height, width, channels)` while `""channels_first""`\n            corresponds to inputs with shape\n            `(batch, channels, height, width)`.\n            It defaults to the `image_data_format` value found in your\n            Keras config file at `~/.keras/keras.json`.\n            If you never set it, then it will be ""channels_last"".\n        interpolation: A string, one of `nearest` or `bilinear`.\n            Note that CNTK does not support yet the `bilinear` upscaling\n            and that with Theano, only `factor=(2, 2)` is possible.\n    # Input shape\n        4D tensor with shape:\n        - If `data_format` is `""channels_last""`:\n            `(batch, rows, cols, channels)`\n        - If `data_format` is `""channels_first""`:\n            `(batch, channels, rows, cols)`\n    # Output shape\n        4D tensor with shape:\n        - If `data_format` is `""channels_last""`:\n            `(batch, upsampled_rows, upsampled_cols, channels)`\n        - If `data_format` is `""channels_first""`:\n            `(batch, channels, upsampled_rows, upsampled_cols)`\n    """"""\n\n    @interfaces.legacy_upsampling2d_support\n    def __init__(self, factor=(2, 2), data_format=\'channels_last\', interpolation=\'nearest\', **kwargs):\n        super(ResizeImage, self).__init__(**kwargs)\n        self.data_format = data_format\n        self.factor = conv_utils.normalize_tuple(factor, 2, \'factor\')\n        self.input_spec = InputSpec(ndim=4)\n        if interpolation not in [\'nearest\', \'bilinear\']:\n            raise ValueError(\'interpolation should be one \'\n                             \'of ""nearest"" or ""bilinear"".\')\n        self.interpolation = interpolation\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == \'channels_first\':\n            height = self.factor[0] * input_shape[2] if input_shape[2] is not None else None\n            width = self.factor[1] * input_shape[3] if input_shape[3] is not None else None\n            return (input_shape[0],\n                    input_shape[1],\n                    height,\n                    width)\n        elif self.data_format == \'channels_last\':\n            height = self.factor[0] * input_shape[1] if input_shape[1] is not None else None\n            width = self.factor[1] * input_shape[2] if input_shape[2] is not None else None\n            return (input_shape[0],\n                    height,\n                    width,\n                    input_shape[3])\n\n    def call(self, inputs):\n        return resize_images(inputs, self.factor[0], self.factor[1],\n                             self.data_format, self.interpolation)\n\n    def get_config(self):\n        config = {\'factor\': self.factor,\n                  \'data_format\': self.data_format}\n        base_config = super(ResizeImage, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nget_custom_objects().update({\'ResizeImage\': ResizeImage})\n'"
bes/segmentation_models/fpn/__init__.py,0,b'from .model import FPN\n\n'
bes/segmentation_models/fpn/blocks.py,0,"b'from keras.layers import Conv2D\nfrom keras.layers import Add\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\n\nimport keras\nfrom distutils.version import StrictVersion\n\nif  StrictVersion(keras.__version__) < StrictVersion(\'2.2.3\'):\n    from .layers import UpSampling2D\nelse:\n    from keras.layers import UpSampling2D\n\ndef Conv(n_filters, kernel_size, activation=\'relu\', use_batchnorm=False, **kwargs):\n    """"""Extension of Conv2aaD layer with batchnorm""""""\n    def layer(input_tensor):\n\n        x = Conv2D(n_filters, kernel_size, use_bias=not(use_batchnorm), **kwargs)(input_tensor)\n        if use_batchnorm:\n            x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n        return x\n    return layer\n\n\ndef pyramid_block(pyramid_filters=256, segmentation_filters=128, upsample_rate=2,\n                  use_batchnorm=False):\n    """"""\n    Pyramid block according to:\n        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n    This block generate `M` and `P` blocks.\n\n    Args:\n        pyramid_filters: integer, filters in `M` block of top-down FPN branch\n        segmentation_filters: integer, number of filters in segmentation head,\n            basically filters in convolution layers between `M` and `P` blocks\n        upsample_rate: integer, uspsample rate for `M` block of top-down FPN branch\n        use_batchnorm: bool, include batchnorm in convolution blocks\n\n    Returns:\n        Pyramid block function (as Keras layers functional API)\n    """"""\n    def layer(c, m=None):\n\n        x = Conv2D(pyramid_filters, (1, 1))(c)\n\n        if m is not None:\n            up = UpSampling2D((upsample_rate, upsample_rate))(m)\n            x = Add()([x, up])\n\n        # segmentation head\n        p = Conv(segmentation_filters, (3, 3), padding=\'same\', use_batchnorm=use_batchnorm)(x)\n        p = Conv(segmentation_filters, (3, 3), padding=\'same\', use_batchnorm=use_batchnorm)(p)\n        m = x\n\n        return m, p\n    return layer\n'"
bes/segmentation_models/fpn/builder.py,0,"b'from keras.layers import Conv2D\nfrom keras.layers import Concatenate\nfrom keras.layers import Activation\nfrom keras.layers import SpatialDropout2D\nfrom keras.models import Model\n\nfrom .blocks import pyramid_block\nfrom .blocks import Conv\nfrom ..utils import extract_outputs, to_tuple\n\nimport numpy as np\n\nimport keras\nfrom distutils.version import StrictVersion\n\nif StrictVersion(keras.__version__) < StrictVersion(\'2.2.3\'):\n    from .layers import UpSampling2D\nelse:\n    from keras.layers import UpSampling2D\n\ndef build_fpn(backbone,\n              fpn_layers,\n              classes=21,\n              activation=\'softmax\',\n              upsample_rates=(2,2,2),\n              last_upsample=4,\n              pyramid_filters=256,\n              segmentation_filters=128,\n              use_batchnorm=False,\n              dropout=None,\n              interpolation=\'nearest\'):\n    """"""\n    Implementation of FPN head for segmentation models according to:\n        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n    Args:\n        backbone: Keras `Model`, some classification model without top\n        layers: list of layer names or indexes, used for pyramid building\n        classes: int, number of output feature maps\n        activation: activation in last layer, e.g. \'sigmoid\' or \'softmax\'\n        upsample_rates: tuple of integers, scaling rates between pyramid blocks\n        pyramid_filters: int, number of filters in `M` blocks of top-down FPN branch\n        segmentation_filters: int, number of filters in `P` blocks of FPN\n        last_upsample: rate for upsumpling concatenated pyramid predictions to\n            match spatial resolution of input data\n        last_upsampling_type: \'nn\' or \'bilinear\'\n        dropout: float [0, 1), dropout rate\n        use_batchnorm: bool, include batch normalization to FPN between `conv`\n            and `relu` layers\n\n    Returns:\n        model: Keras `Model`\n    """"""\n\n    if len(upsample_rates) != len(fpn_layers):\n        raise ValueError(\'Number of intermediate feature maps and upsample steps should match\')\n\n    # extract model layer outputs\n    outputs = extract_outputs(backbone, fpn_layers, include_top=True)\n\n    # add upsample rate `1` for first block\n    upsample_rates = [1] + list(upsample_rates)\n\n    # top - down path, build pyramid\n    m = None\n    pyramid = []\n    for i, c in enumerate(outputs):\n        m, p = pyramid_block(pyramid_filters=pyramid_filters,\n                            segmentation_filters=segmentation_filters,\n                            upsample_rate=upsample_rates[i],\n                            use_batchnorm=use_batchnorm)(c, m)\n        pyramid.append(p)\n\n\n    # upsample and concatenate all pyramid layer\n    upsampled_pyramid = []\n\n    for i, p in enumerate(pyramid[::-1]):\n        if upsample_rates[i] > 1:\n            upsample_rate = to_tuple(np.prod(upsample_rates[:i+1]))\n            p = UpSampling2D(size=upsample_rate, interpolation=interpolation)(p)\n        upsampled_pyramid.append(p)\n\n    x = Concatenate()(upsampled_pyramid)\n\n    # final convolution\n    n_filters = segmentation_filters * len(pyramid)\n    x = Conv(n_filters, (3, 3), use_batchnorm=use_batchnorm, padding=\'same\')(x)\n    if dropout is not None:\n        x = SpatialDropout2D(dropout)(x)\n\n    x = Conv2D(classes, (1, 1), padding=\'same\')(x)\n    x = Activation(activation)(x)\n\n    # upsampling to original spatial resolution\n    x = UpSampling2D(size=to_tuple(last_upsample), interpolation=interpolation)(x)\n\n    model = Model(backbone.input, x)\n    return model\n'"
bes/segmentation_models/fpn/layers.py,0,"b'from keras.engine import Layer\nfrom keras.engine import InputSpec\nfrom keras.utils import conv_utils\nfrom keras.legacy import interfaces\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef transpose_shape(shape, target_format, spatial_axes):\n    """"""Converts a tuple or a list to the correct `data_format`.\n    It does so by switching the positions of its elements.\n    # Arguments\n        shape: Tuple or list, often representing shape,\n            corresponding to `\'channels_last\'`.\n        target_format: A string, either `\'channels_first\'` or `\'channels_last\'`.\n        spatial_axes: A tuple of integers.\n            Correspond to the indexes of the spatial axes.\n            For example, if you pass a shape\n            representing (batch_size, timesteps, rows, cols, channels),\n            then `spatial_axes=(2, 3)`.\n    # Returns\n        A tuple or list, with the elements permuted according\n        to `target_format`.\n    # Example\n    # Raises\n        ValueError: if `value` or the global `data_format` invalid.\n    """"""\n    if target_format == \'channels_first\':\n        new_values = shape[:spatial_axes[0]]\n        new_values += (shape[-1],)\n        new_values += tuple(shape[x] for x in spatial_axes)\n\n        if isinstance(shape, list):\n            return list(new_values)\n        return new_values\n    elif target_format == \'channels_last\':\n        return shape\n    else:\n        raise ValueError(\'The `data_format` argument must be one of \'\n                         \'""channels_first"", ""channels_last"". Received: \' +\n                         str(target_format))\n\n\ndef permute_dimensions(x, pattern):\n    """"""Permutes axes in a tensor.\n    # Arguments\n        x: Tensor or variable.\n        pattern: A tuple of\n            dimension indices, e.g. `(0, 2, 1)`.\n    # Returns\n        A tensor.\n    """"""\n    return tf.transpose(x, perm=pattern)\n\n\ndef int_shape(x):\n    """"""Returns the shape of tensor or variable as a tuple of int or None entries.\n    # Arguments\n        x: Tensor or variable.\n    # Returns\n        A tuple of integers (or None entries).\n    """"""\n    if hasattr(x, \'_keras_shape\'):\n        return x._keras_shape\n    try:\n        return tuple(x.get_shape().as_list())\n    except ValueError:\n        return None\n\n\ndef resize_images(x,\n                  height_factor,\n                  width_factor,\n                  data_format,\n                  interpolation=\'nearest\'):\n    """"""Resizes the images contained in a 4D tensor.\n    # Arguments\n        x: Tensor or variable to resize.\n        height_factor: Positive integer.\n        width_factor: Positive integer.\n        data_format: string, `""channels_last""` or `""channels_first""`.\n        interpolation: A string, one of `nearest` or `bilinear`.\n    # Returns\n        A tensor.\n    # Raises\n        ValueError: if `data_format` is neither `""channels_last""` or `""channels_first""`.\n    """"""\n    if data_format == \'channels_first\':\n        rows, cols = 2, 3\n    else:\n        rows, cols = 1, 2\n\n    original_shape = int_shape(x)\n    new_shape = tf.shape(x)[rows:cols + 1]\n    new_shape *= tf.constant(np.array([height_factor, width_factor], dtype=\'int32\'))\n\n    if data_format == \'channels_first\':\n        x = permute_dimensions(x, [0, 2, 3, 1])\n    if interpolation == \'nearest\':\n        x = tf.image.resize_nearest_neighbor(x, new_shape)\n    elif interpolation == \'bilinear\':\n        x = tf.image.resize_bilinear(x, new_shape)\n    else:\n        raise ValueError(\'interpolation should be one \'\n                         \'of ""nearest"" or ""bilinear"".\')\n    if data_format == \'channels_first\':\n        x = permute_dimensions(x, [0, 3, 1, 2])\n\n    if original_shape[rows] is None:\n        new_height = None\n    else:\n        new_height = original_shape[rows] * height_factor\n\n    if original_shape[cols] is None:\n        new_width = None\n    else:\n        new_width = original_shape[cols] * width_factor\n\n    output_shape = (None, new_height, new_width, None)\n    x.set_shape(transpose_shape(output_shape, data_format, spatial_axes=(1, 2)))\n    return x\n\n\nclass UpSampling2D(Layer):\n    """"""Upsampling layer for 2D inputs.\n    Repeats the rows and columns of the data\n    by size[0] and size[1] respectively.\n    # Arguments\n        size: int, or tuple of 2 integers.\n            The upsampling factors for rows and columns.\n        data_format: A string,\n            one of `""channels_last""` or `""channels_first""`.\n            The ordering of the dimensions in the inputs.\n            `""channels_last""` corresponds to inputs with shape\n            `(batch, height, width, channels)` while `""channels_first""`\n            corresponds to inputs with shape\n            `(batch, channels, height, width)`.\n            It defaults to the `image_data_format` value found in your\n            Keras config file at `~/.keras/keras.json`.\n            If you never set it, then it will be ""channels_last"".\n        interpolation: A string, one of `nearest` or `bilinear`.\n            Note that CNTK does not support yet the `bilinear` upscaling\n            and that with Theano, only `size=(2, 2)` is possible.\n    # Input shape\n        4D tensor with shape:\n        - If `data_format` is `""channels_last""`:\n            `(batch, rows, cols, channels)`\n        - If `data_format` is `""channels_first""`:\n            `(batch, channels, rows, cols)`\n    # Output shape\n        4D tensor with shape:\n        - If `data_format` is `""channels_last""`:\n            `(batch, upsampled_rows, upsampled_cols, channels)`\n        - If `data_format` is `""channels_first""`:\n            `(batch, channels, upsampled_rows, upsampled_cols)`\n    """"""\n\n    @interfaces.legacy_upsampling2d_support\n    def __init__(self, size=(2, 2), data_format=\'channels_last\', interpolation=\'nearest\', **kwargs):\n        super(UpSampling2D, self).__init__(**kwargs)\n        self.data_format = data_format\n        self.size = conv_utils.normalize_tuple(size, 2, \'size\')\n        self.input_spec = InputSpec(ndim=4)\n        if interpolation not in [\'nearest\', \'bilinear\']:\n            raise ValueError(\'interpolation should be one \'\n                             \'of ""nearest"" or ""bilinear"".\')\n        self.interpolation = interpolation\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == \'channels_first\':\n            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n            return (input_shape[0],\n                    input_shape[1],\n                    height,\n                    width)\n        elif self.data_format == \'channels_last\':\n            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n            return (input_shape[0],\n                    height,\n                    width,\n                    input_shape[3])\n\n    def call(self, inputs):\n        return resize_images(inputs, self.size[0], self.size[1],\n                               self.data_format, self.interpolation)\n\n    def get_config(self):\n        config = {\'size\': self.size,\n                  \'data_format\': self.data_format}\n        base_config = super(UpSampling2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
bes/segmentation_models/fpn/model.py,0,"b'from .builder import build_fpn\nfrom ..backbones import get_backbone\nfrom ..utils import freeze_model\n\n\nDEFAULT_FEATURE_PYRAMID_LAYERS = {\n    \'vgg16\':            (\'block5_conv3\', \'block4_conv3\', \'block3_conv3\'),\n    \'vgg19\':            (\'block5_conv4\', \'block4_conv4\', \'block3_conv4\'),\n    \'resnet18\':         (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnet34\':         (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnet50\':         (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnet101\':        (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnet152\':        (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnext50\':        (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnext101\':       (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'inceptionv3\':          (228, 86, 16),\n    \'inceptionresnetv2\':    (594, 260, 16),\n    \'densenet121\':          (311, 139, 51),\n    \'densenet169\':          (367, 139, 51),\n    \'densenet201\':          (479, 139, 51),\n}\n\n\ndef FPN(backbone_name=\'vgg16\',\n        classes=1,\n        input_shape=(None, None, 3),\n        input_tensor=None,\n        encoder_weights=\'imagenet\',\n        freeze_encoder=False,\n        fpn_layers=\'default\',\n        pyramid_block_filters=256,\n        segmentation_block_filters=128,\n        upsample_rates=(2, 2, 2),\n        last_upsample=4,\n        interpolation=\'nearest\',\n        use_batchnorm=False,\n        activation=\'sigmoid\',\n        dropout=None):\n    """"""\n    Implementation of FPN head for segmentation models according to:\n        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n    Args:\n        backbone_name: (str) see available backbones\n        classes: (int) a number of classes for output\n        input_shape: (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \'imagenet\' (pre-training on ImageNet)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        fpn_layers: (list) of layer names or indexes, used for pyramid\n        pyramid_block_filters: (int) number of filters in `M` blocks of top-down FPN branch\n        segmentation_block_filters: (int) number of filters in `P` blocks of FPN\n        upsample_rates: (tuple) rates for upsampling pyramid blocks\n        last_upsample: (int) rate for upsumpling concatenated pyramid predictions to\n            match spatial resolution of input data\n        interpolation: (str) interpolation type for upsampling layers - \'nearest\' or \'bilinear\'\n        use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        activation: (str) one of keras activations\n        dropout: None or float [0, 1), dropout rate\n\n    Returns:\n        keras.models.Model\n\n    """"""\n\n    backbone = get_backbone(backbone_name,\n                            input_shape=input_shape,\n                            input_tensor=input_tensor,\n                            weights=encoder_weights,\n                            include_top=False)\n\n    if fpn_layers == \'default\':\n        fpn_layers = DEFAULT_FEATURE_PYRAMID_LAYERS[backbone_name]\n\n    model = build_fpn(backbone, fpn_layers,\n                      classes=classes,\n                      pyramid_filters=pyramid_block_filters,\n                      segmentation_filters=segmentation_block_filters,\n                      upsample_rates=upsample_rates,\n                      use_batchnorm=use_batchnorm,\n                      dropout=dropout,\n                      last_upsample=last_upsample,\n                      interpolation=interpolation,\n                      activation=activation)\n\n    if freeze_encoder:\n        freeze_model(backbone)\n\n\n    return model\n'"
bes/segmentation_models/linknet/__init__.py,0,b'from .model import Linknet\n'
bes/segmentation_models/linknet/blocks.py,0,"b'import keras.backend as K\nfrom keras.layers import Conv2DTranspose as Transpose\nfrom keras.layers import UpSampling2D\nfrom keras.layers import Conv2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Add\n\n\ndef handle_block_names(stage):\n    conv_name = \'decoder_stage{}_conv\'.format(stage)\n    bn_name = \'decoder_stage{}_bn\'.format(stage)\n    relu_name = \'decoder_stage{}_relu\'.format(stage)\n    up_name = \'decoder_stage{}_upsample\'.format(stage)\n    return conv_name, bn_name, relu_name, up_name\n\n\ndef ConvRelu(filters,\n             kernel_size,\n             use_batchnorm=False,\n             conv_name=\'conv\',\n             bn_name=\'bn\',\n             relu_name=\'relu\'):\n\n    def layer(x):\n\n        x = Conv2D(filters,\n                   kernel_size,\n                   padding=""same"",\n                   name=conv_name,\n                   use_bias=not(use_batchnorm))(x)\n\n        if use_batchnorm:\n            x = BatchNormalization(name=bn_name)(x)\n\n        x = Activation(\'relu\', name=relu_name)(x)\n\n        return x\n    return layer\n\n\ndef Conv2DUpsample(filters,\n                   upsample_rate,\n                   kernel_size=(3,3),\n                   up_name=\'up\',\n                   conv_name=\'conv\',\n                   **kwargs):\n\n    def layer(input_tensor):\n        x = UpSampling2D(upsample_rate, name=up_name)(input_tensor)\n        x = Conv2D(filters,\n                   kernel_size,\n                   padding=\'same\',\n                   name=conv_name,\n                   **kwargs)(x)\n        return x\n    return layer\n\n\ndef Conv2DTranspose(filters,\n                    upsample_rate,\n                    kernel_size=(4,4),\n                    up_name=\'up\',\n                    **kwargs):\n\n    if not tuple(upsample_rate) == (2,2):\n        raise NotImplementedError(\n            \'Conv2DTranspose support only upsample_rate=(2, 2), got {}\'.format(upsample_rate))\n\n    def layer(input_tensor):\n        x = Transpose(filters,\n                      kernel_size=kernel_size,\n                      strides=upsample_rate,\n                      padding=\'same\',\n                      name=up_name)(input_tensor)\n        return x\n    return layer\n\n\ndef UpsampleBlock(filters,\n                  upsample_rate,\n                  kernel_size,\n                  use_batchnorm=False,\n                  upsample_layer=\'upsampling\',\n                  conv_name=\'conv\',\n                  bn_name=\'bn\',\n                  relu_name=\'relu\',\n                  up_name=\'up\',\n                  **kwargs):\n\n    if upsample_layer == \'upsampling\':\n        UpBlock = Conv2DUpsample\n\n    elif upsample_layer == \'transpose\':\n        UpBlock = Conv2DTranspose\n\n    else:\n        raise ValueError(\'Not supported up layer type {}\'.format(upsample_layer))\n\n    def layer(input_tensor):\n\n        x = UpBlock(filters,\n                    upsample_rate=upsample_rate,\n                    kernel_size=kernel_size,\n                    use_bias=not(use_batchnorm),\n                    conv_name=conv_name,\n                    up_name=up_name,\n                    **kwargs)(input_tensor)\n\n        if use_batchnorm:\n            x = BatchNormalization(name=bn_name)(x)\n\n        x = Activation(\'relu\', name=relu_name)(x)\n\n        return x\n    return layer\n\n\ndef DecoderBlock(stage,\n                 filters=None,\n                 kernel_size=(3,3),\n                 upsample_rate=(2,2),\n                 use_batchnorm=False,\n                 skip=None,\n                 upsample_layer=\'upsampling\'):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n        input_filters = K.int_shape(input_tensor)[-1]\n\n        if skip is not None:\n            output_filters = K.int_shape(skip)[-1]\n        else:\n            output_filters = filters\n\n        x = ConvRelu(input_filters // 4,\n                     kernel_size=(1, 1),\n                     use_batchnorm=use_batchnorm,\n                     conv_name=conv_name + \'1\',\n                     bn_name=bn_name + \'1\',\n                     relu_name=relu_name + \'1\')(input_tensor)\n\n        x = UpsampleBlock(filters=input_filters // 4,\n                          kernel_size=kernel_size,\n                          upsample_layer=upsample_layer,\n                          upsample_rate=upsample_rate,\n                          use_batchnorm=use_batchnorm,\n                          conv_name=conv_name + \'2\',\n                          bn_name=bn_name + \'2\',\n                          up_name=up_name + \'2\',\n                          relu_name=relu_name + \'2\')(x)\n\n        x = ConvRelu(output_filters,\n                     kernel_size=(1, 1),\n                     use_batchnorm=use_batchnorm,\n                     conv_name=conv_name + \'3\',\n                     bn_name=bn_name + \'3\',\n                     relu_name=relu_name + \'3\')(x)\n\n        if skip is not None:\n            x = Add()([x, skip])\n\n        return x\n    return layer\n'"
bes/segmentation_models/linknet/builder.py,0,"b""from keras.layers import Conv2D\nfrom keras.layers import Activation\nfrom keras.models import Model\n\nfrom .blocks import DecoderBlock\nfrom ..utils import get_layer_number, to_tuple\n\n\ndef build_linknet(backbone,\n                  classes,\n                  skip_connection_layers,\n                  decoder_filters=(None, None, None, None, 16),\n                  upsample_rates=(2, 2, 2, 2, 2),\n                  n_upsample_blocks=5,\n                  upsample_kernel_size=(3, 3),\n                  upsample_layer='upsampling',\n                  activation='sigmoid',\n                  use_batchnorm=False):\n\n    input = backbone.input\n    x = backbone.output\n\n    # convert layer names to indices\n    skip_connection_idx = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n                            for l in skip_connection_layers])\n\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        skip_connection = None\n        if i < len(skip_connection_idx):\n            skip_connection = backbone.layers[skip_connection_idx[i]].output\n\n        upsample_rate = to_tuple(upsample_rates[i])\n\n        x = DecoderBlock(stage=i,\n                         filters=decoder_filters[i],\n                         kernel_size=upsample_kernel_size,\n                         upsample_rate=upsample_rate,\n                         use_batchnorm=use_batchnorm,\n                         upsample_layer=upsample_layer,\n                         skip=skip_connection)(x)\n\n    x = Conv2D(classes, (3, 3), padding='same', name='final_conv')(x)\n    x = Activation(activation, name=activation)(x)\n\n    model = Model(input, x)\n\n    return model\n"""
bes/segmentation_models/linknet/model.py,0,"b'from .builder import build_linknet\nfrom ..utils import freeze_model\nfrom ..backbones import get_backbone\n\n\nDEFAULT_SKIP_CONNECTIONS = {\n    \'vgg16\':                (\'block5_conv3\', \'block4_conv3\', \'block3_conv3\', \'block2_conv2\'),\n    \'vgg19\':                (\'block5_conv4\', \'block4_conv4\', \'block3_conv4\', \'block2_conv2\'),\n    \'resnet18\':             (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnet34\':             (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnet50\':             (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnet101\':            (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnet152\':            (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnext50\':            (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnext101\':           (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'inceptionv3\':          (228, 86, 16, 9),\n    \'inceptionresnetv2\':    (594, 260, 16, 9),\n    \'densenet121\':          (311, 139, 51, 4),\n    \'densenet169\':          (367, 139, 51, 4),\n    \'densenet201\':          (479, 139, 51, 4),\n}\n\n\ndef Linknet(backbone_name=\'vgg16\',\n            input_shape=(None, None, 3),\n            input_tensor=None,\n            encoder_weights=\'imagenet\',\n            freeze_encoder=False,\n            skip_connections=\'default\',\n            n_upsample_blocks=5,\n            decoder_filters=(None, None, None, None, 16),\n            decoder_use_batchnorm=False,\n            upsample_layer=\'upsampling\',\n            upsample_kernel_size=(3, 3),\n            classes=1,\n            activation=\'sigmoid\'):\n    """"""\n    Version of Linkent model (https://arxiv.org/pdf/1707.03718.pdf)\n    This implementation by default has 4 skip connection links (original - 3).\n\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape: (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \'imagenet\' (pre-training on ImageNet)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        skip_connections: if \'default\' is used take default skip connections,\n        decoder_filters: (tuple of int) a number of convolution filters in decoder blocks,\n            for block with skip connection a number of filters is equal to number of filters in\n            corresponding encoder block (estimates automatically and can be passed as `None` value).\n        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        n_upsample_blocks: (int) a number of upsampling blocks in decoder\n        upsample_layer: (str) one of \'upsampling\' and \'transpose\'\n        upsample_kernel_size: (tuple of int) convolution kernel size in upsampling block\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations\n\n    Returns:\n        model: instance of Keras Model\n\n    """"""\n\n    backbone = get_backbone(backbone_name,\n                            input_shape=input_shape,\n                            input_tensor=input_tensor,\n                            weights=encoder_weights,\n                            include_top=False)\n\n    if skip_connections == \'default\':\n        skip_connections = DEFAULT_SKIP_CONNECTIONS[backbone_name]\n\n    model = build_linknet(backbone,\n                          classes,\n                          skip_connections,\n                          decoder_filters=decoder_filters,\n                          upsample_layer=upsample_layer,\n                          activation=activation,\n                          n_upsample_blocks=n_upsample_blocks,\n                          upsample_rates=(2, 2, 2, 2, 2),\n                          upsample_kernel_size=upsample_kernel_size,\n                          use_batchnorm=decoder_use_batchnorm)\n\n    # lock encoder weights for fine-tuning\n    if freeze_encoder:\n        freeze_model(backbone)\n\n    model.name = \'link-{}\'.format(backbone_name)\n\n    return model\n'"
bes/segmentation_models/pspnet/__init__.py,0,b'from .model import PSPNet'
bes/segmentation_models/pspnet/blocks.py,0,"b'import numpy as np\nfrom keras.layers import MaxPool2D\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import Concatenate\nfrom keras.layers import Permute\nfrom keras.layers import Reshape\nfrom keras.backend import int_shape\n\nfrom ..common import Conv2DBlock\nfrom ..common import ResizeImage\n\n\ndef InterpBlock(level, feature_map_shape,\n                conv_filters=512,\n                conv_kernel_size=(1,1),\n                conv_padding=\'same\',\n                pooling_type=\'avg\',\n                pool_padding=\'same\',\n                use_batchnorm=True,\n                activation=\'relu\',\n                interpolation=\'bilinear\'):\n\n    if pooling_type == \'max\':\n        Pool2D = MaxPool2D\n    elif pooling_type == \'avg\':\n        Pool2D = AveragePooling2D\n    else:\n        raise ValueError(\'Unsupported pooling type - `{}`.\'.format(pooling_type) +\n                         \'Use `avg` or `max`.\')\n\n    def layer(input_tensor):\n        # Compute the kernel and stride sizes according to how large the final feature map will be\n        # When the kernel factor and strides are equal, then we can compute the final feature map factor\n        # by simply dividing the current factor by the kernel or stride factor\n        # The final feature map sizes are 1x1, 2x2, 3x3, and 6x6. We round to the closest integer\n        pool_size = [int(np.round(feature_map_shape[0] / level)),\n                       int(np.round(feature_map_shape[1] / level))]\n        strides = pool_size\n\n        x = Pool2D(pool_size, strides=strides, padding=pool_padding)(input_tensor)\n        x = Conv2DBlock(conv_filters,\n                        kernel_size=conv_kernel_size,\n                        padding=conv_padding,\n                        use_batchnorm=use_batchnorm,\n                        activation=activation,\n                        name=\'level{}\'.format(level))(x)\n        x = ResizeImage(strides, interpolation=interpolation)(x)\n        return x\n    return layer\n\n\ndef DUC(factor=(8, 8)):\n\n    if factor[0] != factor[1]:\n        raise ValueError(\'DUC upconvolution support only equal factors, \'\n                         \'got {}\'.format(factor))\n    factor = factor[0]\n\n    def layer(input_tensor):\n\n        h, w, c = int_shape(input_tensor)[1:]\n        H = h * factor\n        W = w * factor\n\n        x = Conv2DBlock(c*factor**2, (1,1),\n                        padding=\'same\',\n                        name=\'duc_{}\'.format(factor))(input_tensor)\n        x = Permute((3, 1, 2))(x)\n        x = Reshape((c, factor, factor, h, w))(x)\n        x = Permute((1, 4, 2, 5, 3))(x)\n        x = Reshape((c, H, W))(x)\n        x = Permute((2, 3, 1))(x)\n        return x\n    return layer\n\n\ndef PyramidPoolingModule(**params):\n    """"""\n    Build the Pyramid Pooling Module.\n    """"""\n\n    _params = {\n        \'conv_filters\': 512,\n        \'conv_kernel_size\': (1, 1),\n        \'conv_padding\': \'same\',\n        \'pooling_type\': \'avg\',\n        \'pool_padding\': \'same\',\n        \'use_batchnorm\': True,\n        \'activation\': \'relu\',\n        \'interpolation\': \'bilinear\',\n    }\n\n    _params.update(params)\n\n    def module(input_tensor):\n\n        feature_map_shape = int_shape(input_tensor)[1:3]\n\n        x1 = InterpBlock(1, feature_map_shape, **_params)(input_tensor)\n        x2 = InterpBlock(2, feature_map_shape, **_params)(input_tensor)\n        x3 = InterpBlock(3, feature_map_shape, **_params)(input_tensor)\n        x6 = InterpBlock(6, feature_map_shape, **_params)(input_tensor)\n\n        x = Concatenate()([input_tensor, x1, x2, x3, x6])\n        return x\n    return module'"
bes/segmentation_models/pspnet/builder.py,0,"b'""""""\nCode is constructed based on following repositories:\n    https://github.com/ykamikawa/PSPNet/\n    https://github.com/hujh14/PSPNet-Keras/\n    https://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow/\n\nAnd original paper of PSPNet:\n    https://arxiv.org/pdf/1612.01105.pdf\n""""""\n\nfrom keras.layers import Conv2D\nfrom keras.layers import Activation\nfrom keras.layers import SpatialDropout2D\nfrom keras.models import Model\n\nfrom .blocks import PyramidPoolingModule, DUC\nfrom ..common import Conv2DBlock\nfrom ..common import ResizeImage\nfrom ..utils import extract_outputs\nfrom ..utils import to_tuple\n\n\ndef build_psp(backbone,\n              psp_layer,\n              last_upsampling_factor,\n              classes=21,\n              activation=\'softmax\',\n              conv_filters=512,\n              pooling_type=\'avg\',\n              dropout=None,\n              final_interpolation=\'bilinear\',\n              use_batchnorm=True):\n\n    input = backbone.input\n\n    x = extract_outputs(backbone, [psp_layer])[0]\n\n    x = PyramidPoolingModule(\n        conv_filters=conv_filters,\n        pooling_type=pooling_type,\n        use_batchnorm=use_batchnorm)(x)\n\n    x = Conv2DBlock(512, (1, 1), activation=\'relu\', padding=\'same\',\n                    use_batchnorm=use_batchnorm)(x)\n\n    if dropout is not None:\n        x = SpatialDropout2D(dropout)(x)\n\n    x = Conv2D(classes, (3,3), padding=\'same\', name=\'final_conv\')(x)\n\n    if final_interpolation == \'bilinear\':\n        x = ResizeImage(to_tuple(last_upsampling_factor))(x)\n    elif final_interpolation == \'duc\':\n        x = DUC(to_tuple(last_upsampling_factor))(x)\n    else:\n        raise ValueError(\'Unsupported interpolation type {}. \'.format(final_interpolation) +\n                         \'Use `duc` or `bilinear`.\')\n\n    x = Activation(activation, name=activation)(x)\n\n    model = Model(input, x)\n\n    return model\n'"
bes/segmentation_models/pspnet/model.py,0,"b'from .builder import build_psp\nfrom ..utils import freeze_model\nfrom ..backbones import get_backbone\n\n\nPSP_BASE_LAYERS = {\n    \'vgg16\':                (\'block5_conv3\', \'block4_conv3\', \'block3_conv3\'),\n    \'vgg19\':                (\'block5_conv4\', \'block4_conv4\', \'block3_conv4\'),\n    \'resnet18\':             (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnet34\':             (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnet50\':             (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnet101\':            (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnet152\':            (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnext50\':            (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'resnext101\':           (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\'),\n    \'inceptionv3\':          (228, 86, 16),\n    \'inceptionresnetv2\':    (594, 260, 16),\n    \'densenet121\':          (311, 139, 51),\n    \'densenet169\':          (367, 139, 51),\n    \'densenet201\':          (479, 139, 51),\n}\n\ndef _get_layer_by_factor(backbone_name, factor):\n\n    if factor == 4:\n        return PSP_BASE_LAYERS[backbone_name][-1]\n    elif factor == 8:\n        return PSP_BASE_LAYERS[backbone_name][-2]\n    elif factor == 16:\n        return PSP_BASE_LAYERS[backbone_name][-3]\n    else:\n        raise ValueError(\'Unsupported factor - `{}`, Use 4, 8 or 16.\'.format(factor))\n\n\ndef _shape_guard(factor, shape):\n\n    h, w = shape[:2]\n    min_size = factor * 6\n\n    res = (h % min_size != 0 or w % min_size != 0 or\n           h < min_size or w < min_size)\n    if res:\n        raise ValueError(\'Wrong shape {}, input H and W should \'.format(shape) +\n                         \'be divisible by `{}`\'.format(min_size))\n\n\ndef PSPNet(backbone_name=\'vgg16\',\n           input_shape=(384, 384, 3),\n           input_tensor=None,\n           encoder_weights=\'imagenet\',\n           freeze_encoder=False,\n           downsample_factor=8,\n           psp_conv_filters=512,\n           psp_pooling_type=\'avg\',\n           use_batchnorm=False,\n           dropout=None,\n           final_interpolation=\'bilinear\',\n           classes=21,\n           activation=\'softmax\'):\n    """"""\n    Exploit the capability of global context information by different-regionbased\n    context aggregation through pyramid pooling module together with the proposed\n    pyramid scene parsing network (PSPNet).\n\n    https://arxiv.org/pdf/1612.01105.pdf\n\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape: (tuple) dimensions of input data (H, W, C).\n            H and W should be divisible by (6 * `downsample_factor`) and **NOT** `None`!\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \'imagenet\' (pre-\n            training on ImageNet)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Use-\n            ful for fine-tuning\n        downsample_factor: int, one of 4, 8 and 16. Specify layer of backbone or\n            backbone depth to construct PSP module on it.\n        psp_conv_filters: (int), number of filters in `Conv2D` layer in each psp block\n        psp_pooling_type: \'avg\' or \'max\', psp block pooling type (maximum or average)\n        use_batchnorm: (bool) if True add batch normalisation layer between\n            `Conv2D` ad `Activation` layers\n        dropout: None or float in range 0-1, if specified add SpatialDropout after PSP module\n        final_interpolation: \'duc\' or \'bilinear\' - interpolation type for final\n            upsampling layer.\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations\n\n\n    Returns:\n        keras Model instance\n    """"""\n\n    # control image input shape\n    _shape_guard(downsample_factor, input_shape)\n\n    backbone = get_backbone(backbone_name,\n                            input_shape=input_shape,\n                            input_tensor=input_tensor,\n                            weights=encoder_weights,\n                            include_top=False)\n\n    psp_layer = _get_layer_by_factor(backbone_name, downsample_factor)\n\n    model = build_psp(backbone,\n                      psp_layer,\n                      last_upsampling_factor=downsample_factor,\n                      classes=classes,\n                      conv_filters=psp_conv_filters,\n                      pooling_type=psp_pooling_type,\n                      activation=activation,\n                      use_batchnorm=use_batchnorm,\n                      dropout=dropout,\n                      final_interpolation=final_interpolation)\n\n    # lock encoder weights for fine-tuning\n    if freeze_encoder:\n        freeze_model(backbone)\n\n    model.name = \'psp-{}\'.format(backbone_name)\n\n    return model'"
bes/segmentation_models/unet/__init__.py,0,b'from .model import Unet\n'
bes/segmentation_models/unet/blocks.py,0,"b'from keras.layers import Conv2DTranspose\nfrom keras.layers import UpSampling2D\nfrom keras.layers import Conv2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\n\n\ndef handle_block_names(stage):\n    conv_name = \'decoder_stage{}_conv\'.format(stage)\n    bn_name = \'decoder_stage{}_bn\'.format(stage)\n    relu_name = \'decoder_stage{}_relu\'.format(stage)\n    up_name = \'decoder_stage{}_upsample\'.format(stage)\n    return conv_name, bn_name, relu_name, up_name\n\n\ndef ConvRelu(filters, kernel_size, use_batchnorm=False, conv_name=\'conv\', bn_name=\'bn\', relu_name=\'relu\'):\n    def layer(x):\n        x = Conv2D(filters, kernel_size, padding=""same"", name=conv_name, use_bias=not(use_batchnorm))(x)\n        # x = Conv2D(filters, kernel_size, padding=""same"", name=conv_name, use_bias=True)(x)\n        if use_batchnorm:\n            x = BatchNormalization(name=bn_name)(x)\n        x = Activation(\'relu\', name=relu_name)(x)\n        return x\n    return layer\n\n\ndef Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                     use_batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n\n        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n                     conv_name=conv_name + \'1\', bn_name=bn_name + \'1\', relu_name=relu_name + \'1\')(x)\n\n        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n                     conv_name=conv_name + \'2\', bn_name=bn_name + \'2\', relu_name=relu_name + \'2\')(x)\n\n        return x\n    return layer\n\n\ndef Transpose2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                      transpose_kernel_size=(4,4), use_batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n\n        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate,\n                            padding=\'same\', name=up_name, use_bias=not(use_batchnorm))(input_tensor)\n        if use_batchnorm:\n            x = BatchNormalization(name=bn_name+\'1\')(x)\n        x = Activation(\'relu\', name=relu_name+\'1\')(x)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n                     conv_name=conv_name + \'2\', bn_name=bn_name + \'2\', relu_name=relu_name + \'2\')(x)\n\n        return x\n    return layer'"
bes/segmentation_models/unet/builder.py,0,"b'from keras.layers import Conv2D, Lambda, Dense, Multiply, Add\nfrom keras.models import Model\nimport keras.backend as K\n\nfrom .blocks import Transpose2D_block\nfrom .blocks import Upsample2D_block\nfrom ..utils import get_layer_number, to_tuple\n\n\ndef cse_block(prevlayer, prefix):\n    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + \'cse_lin1\', activation=\'relu\')(mean)\n    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + \'cse_lin2\', activation=\'sigmoid\')(lin1)\n    x = Multiply()([prevlayer, lin2])\n    return x\n\n\ndef sse_block(prevlayer, prefix):\n    # Bug? Should be 1 here?\n    conv = Conv2D(K.int_shape(prevlayer)[3], (1, 1), padding=""same"", kernel_initializer=""he_normal"",\n                  activation=\'sigmoid\', strides=(1, 1),\n                  name=prefix + ""_conv"")(prevlayer)\n    conv = Multiply(name=prefix + ""_mul"")([prevlayer, conv])\n    return conv\n\n\ndef csse_block(x, prefix):\n    \'\'\'\n    Implementation of Concurrent Spatial and Channel \xe2\x80\x98Squeeze & Excitation\xe2\x80\x99 in Fully Convolutional Networks\n    https://arxiv.org/abs/1803.02579\n    \'\'\'\n    cse = cse_block(x, prefix)\n    sse = sse_block(x, prefix)\n    x = Add(name=prefix + ""_csse_mul"")([cse, sse])\n\n    return x\n\n\ndef build_unet(backbone, classes, skip_connection_layers,\n               decoder_filters=(256, 128, 64, 32, 16),\n               upsample_rates=(2, 2, 2, 2, 2),\n               n_upsample_blocks=5,\n               block_type=\'upsampling\',\n               activation=\'sigmoid\',\n               use_batchnorm=False):\n    input = backbone.input\n    x = backbone.output\n\n    if block_type == \'transpose\':\n        up_block = Transpose2D_block\n    else:\n        up_block = Upsample2D_block\n\n    # convert layer names to indices\n    skip_connection_idx = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n                            for l in skip_connection_layers])\n\n    hyper_list = []\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        skip_connection = None\n        if i < len(skip_connection_idx):\n            skip_connection = backbone.layers[skip_connection_idx[i]].output\n\n        upsample_rate = to_tuple(upsample_rates[i])\n\n        x = up_block(decoder_filters[i], i, upsample_rate=upsample_rate,\n                     skip=skip_connection, use_batchnorm=use_batchnorm)(x)\n\n        x = csse_block(x, prefix=\'csse_block_{}\'.format(i))\n\n        hyper_list.append(x)\n\n    model = Model(input, x)\n\n    return model, hyper_list\n'"
bes/segmentation_models/unet/model.py,0,"b'from .builder import build_unet\nfrom ..utils import freeze_model\nfrom ..backbones import get_backbone\n\n\nDEFAULT_SKIP_CONNECTIONS = {\n    \'vgg16\':            (\'block5_conv3\', \'block4_conv3\', \'block3_conv3\', \'block2_conv2\', \'block1_conv2\'),\n    \'vgg19\':            (\'block5_conv4\', \'block4_conv4\', \'block3_conv4\', \'block2_conv2\', \'block1_conv2\'),\n    \'resnet18\':         (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'), # check \'bn_data\'\n    \'resnet34\':         (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnet50\':         (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnet101\':        (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnet152\':        (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnext50\':        (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'resnext101\':       (\'stage4_unit1_relu1\', \'stage3_unit1_relu1\', \'stage2_unit1_relu1\', \'relu0\'),\n    \'inceptionv3\':          (228, 86, 16, 9),\n    \'inceptionresnetv2\':    (594, 260, 16, 9),\n    \'densenet121\':          (311, 139, 51, 4),\n    \'densenet169\':          (367, 139, 51, 4),\n    \'densenet201\':          (479, 139, 51, 4),\n}\n\n\ndef Unet(backbone_name=\'vgg16\',\n         input_shape=(None, None, 3),\n         input_tensor=None,\n         encoder_weights=\'imagenet\',\n         freeze_encoder=False,\n         skip_connections=\'default\',\n         decoder_block_type=\'upsampling\',\n         decoder_filters=(256,128,64,32,16),\n         decoder_use_batchnorm=False,\n         n_upsample_blocks=5,\n         upsample_rates=(2,2,2,2,2),\n         classes=1,\n         activation=\'sigmoid\'):\n    """"""\n\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape:  (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \'imagenet\' (pre-training on ImageNet)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        skip_connections: if \'default\' is used take default skip connections,\n            else provide a list of layer numbers or names starting from top of model\n        decoder_block_type: (str) one of \'upsampling\' and \'transpose\' (look at blocks.py)\n        decoder_filters: (int) number of convolution filters in last upsample block\n        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        n_upsample_blocks: (int) a number of upsampling blocks\n        upsample_rates: (tuple of int) upsampling rates decoder blocks\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations\n\n    Returns:\n        keras.models.Model instance\n\n    """"""\n\n\n\n    backbone = get_backbone(backbone_name,\n                            input_shape=input_shape,\n                            input_tensor=input_tensor,\n                            weights=encoder_weights,\n                            include_top=False)\n\n    if skip_connections == \'default\':\n        skip_connections = DEFAULT_SKIP_CONNECTIONS[backbone_name]\n\n    model, hyper_list = build_unet(backbone,\n                       classes,\n                       skip_connections,\n                       decoder_filters=decoder_filters,\n                       block_type=decoder_block_type,\n                       activation=activation,\n                       n_upsample_blocks=n_upsample_blocks,\n                       upsample_rates=upsample_rates,\n                       use_batchnorm=decoder_use_batchnorm)\n\n    # lock encoder weights for fine-tuning\n    if freeze_encoder:\n        freeze_model(backbone)\n\n    return model, hyper_list'"
bes/segmentation_models/backbones/classification_models/__init__.py,0,b'\n'
bes/segmentation_models/backbones/classification_models/classification_models/__init__.py,0,"b""from .resnet.models import ResNet18\nfrom .resnet.models import ResNet34\nfrom .resnet.models import ResNet50\nfrom .resnet.models import ResNet101\nfrom .resnet.models import ResNet152\nfrom .resnext.models import ResNeXt50\nfrom .resnext.models import ResNeXt101\n\n__all__ = ['ResNet18', 'ResNet34', 'ResNet50', 'ResNet101', 'ResNet152',\n           'ResNeXt50', 'ResNeXt101']"""
bes/segmentation_models/backbones/classification_models/classification_models/utils.py,0,"b""from keras.utils import get_file\n\n\ndef find_weights(weights_collection, model_name, dataset, include_top):\n    w = list(filter(lambda x: x['model'] == model_name, weights_collection))\n    w = list(filter(lambda x: x['dataset'] == dataset, w))\n    w = list(filter(lambda x: x['include_top'] == include_top, w))\n    return w\n\n\ndef load_model_weights(weights_collection, model, dataset, classes, include_top):\n    weights = find_weights(weights_collection, model.name, dataset, include_top)\n\n    if weights:\n        weights = weights[0]\n\n        if include_top and weights['classes'] != classes:\n            raise ValueError('If using `weights` and `include_top`'\n                             ' as true, `classes` should be {}'.format(weights['classes']))\n\n        weights_path = get_file(weights['name'],\n                                weights['url'],\n                                cache_subdir='models',\n                                md5_hash=weights['md5'])\n\n        #model.load_weights(weights_path)\n        model.load_weights(weights_path, by_name=True)\n\n    else:\n        raise ValueError('There is no weights for such configuration: ' +\n                         'model = {}, dataset = {}, '.format(model.name, dataset) +\n                         'classes = {}, include_top = {}.'.format(classes, include_top))\n"""
bes/segmentation_models/backbones/classification_models/classification_models/weights.py,0,"b""weights_collection = [\n\n    # ResNet18\n    {\n        'model': 'resnet18',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet18_imagenet_1000.h5',\n        'name': 'resnet18_imagenet_1000.h5',\n        'md5': '64da73012bb70e16c901316c201d9803',\n    },\n\n    {\n        'model': 'resnet18',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet18_imagenet_1000_no_top.h5',\n        'name': 'resnet18_imagenet_1000.h5',\n        'md5': '318e3ac0cd98d51e917526c9f62f0b50',\n    },\n\n    # ResNet34\n    {\n        'model': 'resnet34',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000.h5',\n        'name': 'resnet34_imagenet_1000.h5',\n        'md5': '2ac8277412f65e5d047f255bcbd10383',\n    },\n\n    {\n        'model': 'resnet34',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5',\n        'name': 'resnet34_imagenet_1000_no_top.h5',\n        'md5': '8caaa0ad39d927cb8ba5385bf945d582',\n    },\n\n    # ResNet50\n    {\n        'model': 'resnet50',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_1000.h5',\n        'name': 'resnet50_imagenet_1000.h5',\n        'md5': 'd0feba4fc650e68ac8c19166ee1ba87f',\n    },\n\n    {\n        'model': 'resnet50',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_1000_no_top.h5',\n        'name': 'resnet50_imagenet_1000_no_top.h5',\n        'md5': 'db3b217156506944570ac220086f09b6',\n    },\n\n    {\n        'model': 'resnet50',\n        'dataset': 'imagenet11k-places365ch',\n        'classes': 11586,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_places365_11586.h5',\n        'name': 'resnet50_places365_11586.h5',\n        'md5': 'bb8963db145bc9906452b3d9c9917275',\n    },\n\n    {\n        'model': 'resnet50',\n        'dataset': 'imagenet11k-places365ch',\n        'classes': 11586,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_11586_no_top.h5',\n        'name': 'resnet50_imagenet_11586_no_top.h5',\n        'md5': 'd8bf4e7ea082d9d43e37644da217324a',\n    },\n\n    # ResNet101\n    {\n        'model': 'resnet101',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet101_imagenet_1000.h5',\n        'name': 'resnet101_imagenet_1000.h5',\n        'md5': '9489ed2d5d0037538134c880167622ad',\n    },\n\n    {\n        'model': 'resnet101',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet101_imagenet_1000_no_top.h5',\n        'name': 'resnet101_imagenet_1000_no_top.h5',\n        'md5': '1016e7663980d5597a4e224d915c342d',\n    },\n\n\n    # ResNet152\n    {\n        'model': 'resnet152',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet152_imagenet_1000.h5',\n        'name': 'resnet152_imagenet_1000.h5',\n        'md5': '1efffbcc0708fb0d46a9d096ae14f905',\n    },\n\n    {\n        'model': 'resnet152',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet152_imagenet_1000_no_top.h5',\n        'name': 'resnet152_imagenet_1000_no_top.h5',\n        'md5': '5867b94098df4640918941115db93734',\n    },\n\n    {\n        'model': 'resnet152',\n        'dataset': 'imagenet11k',\n        'classes': 11221,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet152_imagenet11k_11221.h5',\n        'name': 'resnet152_imagenet11k_11221.h5',\n        'md5': '24791790f6ef32f274430ce4a2ffee5d',\n    },\n\n    {\n        'model': 'resnet152',\n        'dataset': 'imagenet11k',\n        'classes': 11221,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet152_imagenet11k_11221_no_top.h5',\n        'name': 'resnet152_imagenet11k_11221_no_top.h5',\n        'md5': '25ab66dec217cb774a27d0f3659cafb3',\n    },\n\n\n    # ResNeXt50\n    {\n        'model': 'resnext50',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnext50_imagenet_1000.h5',\n        'name': 'resnext50_imagenet_1000.h5',\n        'md5': '7c5c40381efb044a8dea5287ab2c83db',\n    },\n\n    {\n        'model': 'resnext50',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnext50_imagenet_1000_no_top.h5',\n        'name': 'resnext50_imagenet_1000_no_top.h5',\n        'md5': '7ade5c8aac9194af79b1724229bdaa50',\n    },\n\n\n    # ResNeXt101\n    {\n        'model': 'resnext101',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnext101_imagenet_1000.h5',\n        'name': 'resnext101_imagenet_1000.h5',\n        'md5': '432536e85ee811568a0851c328182735',\n    },\n\n    {\n        'model': 'resnext101',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnext101_imagenet_1000_no_top.h5',\n        'name': 'resnext101_imagenet_1000_no_top.h5',\n        'md5': '91fe0126320e49f6ee607a0719828c7e',\n    },\n\n]\n"""
bes/segmentation_models/backbones/classification_models/tests/__init__.py,0,b''
bes/segmentation_models/backbones/classification_models/tests/test_imagenet.py,0,"b""import numpy as np\nfrom skimage.io import imread\nfrom keras.applications.imagenet_utils import decode_predictions\n\nimport sys\nsys.path.insert(0, '..')\n\nfrom classification_models import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\nfrom classification_models import ResNeXt50, ResNeXt101\nfrom classification_models import resnet\nfrom classification_models import resnext\n\n\nmodels_zoo = {\n    'resnet18': {\n        'model': ResNet18,\n        'params': [\n            {\n                'input_shape': (224,224,3),\n                'dataset': 'imagenet',\n                'ground_truth': [(144, 0.5189058), (23, 0.17232688), (21, 0.098873824), (22, 0.03640686), (315, 0.023893135)],\n                'preprocessing_function': lambda x:resnet.preprocess_input(x, (224, 224), True),\n            }\n        ]\n    },\n    \n    'resnet34': {\n        'model': ResNet34,\n        'params': [\n            {\n                'input_shape': (224,224,3),\n                'dataset': 'imagenet',\n                'ground_truth': [(144, 0.88104683), (23, 0.031556014), (21, 0.024246644), (146, 0.022548646), (94, 0.0057696267)],\n                'preprocessing_function': lambda x:resnet.preprocess_input(x, (224, 224), True),\n            }\n        ]\n    },\n\n    'resnet50': {\n        'model': ResNet50,\n        'params': [\n            {\n                'input_shape': (224, 224, 3),\n                'dataset': 'imagenet',\n                'ground_truth': [(21, 0.53156805), (144, 0.37913376), (23, 0.057184655), (146, 0.024926249), (22, 0.0015899206)],\n                'preprocessing_function': lambda x: resnet.preprocess_input(x, (224, 224), True),\n            },\n        ]\n    },\n\n    'resnet101': {\n        'model': ResNet101,\n        'params': [\n            {\n                'input_shape': (224,224,3),\n                'dataset': 'imagenet',\n                'ground_truth': [(21, 0.96975815), (144, 0.016729029), (146, 0.00535842), (99, 0.0017561398), (22, 0.0010300555)],\n                'preprocessing_function': lambda x:resnet.preprocess_input(x, (224, 224), True),\n            }\n        ]\n    },\n\n    'resnet152': {\n        'model': ResNet152,\n        'params': [\n            {\n                'input_shape': (224,224,3),\n                'dataset': 'imagenet',\n                'ground_truth': [(21, 0.59152377), (144, 0.2688002), (97, 0.0474935), (146, 0.035076432), (99, 0.014631907)],\n                'preprocessing_function': lambda x:resnet.preprocess_input(x, (224, 224), True),\n            }\n        ]\n    },\n\n    'resnext50': {\n        'model': ResNeXt50,\n        'params': [\n            {\n                'input_shape': (224,224,3),\n                'dataset': 'imagenet',\n                'ground_truth': [(396, 0.97365075), (398, 0.0096320715), (409, 0.005558599), (438, 0.0028824762), (440, 0.0019731398)],\n                'preprocessing_function': lambda x:resnext.preprocess_input(x, (224, 224)),\n            }\n        ]\n    },\n\n    'resnext101': {\n        'model': ResNeXt101,\n        'params': [\n            {\n                'input_shape': (224,224,3),\n                'dataset': 'imagenet',\n                'ground_truth': [(396, 0.95073587), (440, 0.016645206), (426, 0.004068849), (398, 0.0032844676), (392, 0.0022560472)],\n                'preprocessing_function': lambda x:resnext.preprocess_input(x, (224, 224)),\n            }\n        ]\n    },\n}\n\n\ndef get_top(y, top=5):\n    y = y.squeeze()\n    idx = y.argsort()[::-1]\n    top_idx = idx[:top]\n    top_pred = y[top_idx]\n    return list(zip(top_idx, top_pred))\n\n\ndef is_equal(gt, pr, eps=10e-5):\n\n    for i in range(len(gt)):\n        idx_gt, prob_gt = gt[i]\n        idx_pr, prob_pr = pr[i]\n\n        if idx_gt != idx_pr:\n            return False\n\n        if not np.allclose(prob_gt, prob_pr, atol=eps):\n            return False\n    return True\n\n\ndef test_model(model, preprocessing_func, sample, ground_truth):\n\n    x = preprocessing_func(sample)\n    x = np.expand_dims(x, 0)\n    y = model.predict(x)\n\n    print('[INFO]', decode_predictions(y))\n\n    pred = get_top(y)\n    if is_equal(pred, ground_truth):\n        print('[INFO] Test passed...\\n')\n    else:\n        print('[WARN] TEST FAILED...')\n        print('[WARN] PREDICTION', pred)\n        print('[WARN] GROUND TRUTH', ground_truth)\n        print()\n\n\ndef main():\n\n    path = ('../imgs/tests/seagull.jpg')\n    img = imread(path)\n    for model_type in models_zoo:\n        for params in models_zoo[model_type]['params']:\n\n            input_shape = params['input_shape']\n            dataset = params['dataset']\n            preprocessing_function = params['preprocessing_function']\n            groud_truth = params['ground_truth']\n\n            print('[INFO] Loading model {} with weights {}....'.format(model_type, dataset))\n            model = models_zoo[model_type]['model']\n            model = model(input_shape, weights=dataset, classes=1000)\n\n            test_model(model, preprocessing_function, img, groud_truth)\n\nif __name__ == '__main__':\n    main()\n"""
bes/segmentation_models/backbones/classification_models/classification_models/resnet/__init__.py,0,"b""from .preprocessing import preprocess_input\n\n__all__ = ['preprocess_input']"""
bes/segmentation_models/backbones/classification_models/classification_models/resnet/blocks.py,0,"b'from keras.layers import Conv2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Add\nfrom keras.layers import ZeroPadding2D\n\nfrom .params import get_conv_params\nfrom .params import get_bn_params\n\n\ndef handle_block_names(stage, block):\n    name_base = \'stage{}_unit{}_\'.format(stage + 1, block + 1)\n    conv_name = name_base + \'conv\'\n    bn_name = name_base + \'bn\'\n    relu_name = name_base + \'relu\'\n    sc_name = name_base + \'sc\'\n    return conv_name, bn_name, relu_name, sc_name\n\n\ndef basic_identity_block(filters, stage, block):\n    """"""The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    """"""\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + \'1\', **bn_params)(input_tensor)\n        x = Activation(\'relu\', name=relu_name + \'1\')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + \'1\', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + \'2\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'2\')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + \'2\', **conv_params)(x)\n\n        x = Add()([x, input_tensor])\n        return x\n\n    return layer\n\n\ndef basic_conv_block(filters, stage, block, strides=(2, 2)):\n    """"""The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    """"""\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + \'1\', **bn_params)(input_tensor)\n        x = Activation(\'relu\', name=relu_name + \'1\')(x)\n        shortcut = x\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + \'1\', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + \'2\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'2\')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + \'2\', **conv_params)(x)\n\n        shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n        x = Add()([x, shortcut])\n        return x\n\n    return layer\n\n\ndef conv_block(filters, stage, block, strides=(2, 2)):\n    """"""The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    """"""\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + \'1\', **bn_params)(input_tensor)\n        x = Activation(\'relu\', name=relu_name + \'1\')(x)\n        shortcut = x\n        x = Conv2D(filters, (1, 1), name=conv_name + \'1\', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + \'2\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'2\')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + \'2\', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + \'3\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'3\')(x)\n        x = Conv2D(filters*4, (1, 1), name=conv_name + \'3\', **conv_params)(x)\n\n        shortcut = Conv2D(filters*4, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n        x = Add()([x, shortcut])\n        return x\n\n    return layer\n\n\ndef identity_block(filters, stage, block):\n    """"""The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    """"""\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + \'1\', **bn_params)(input_tensor)\n        x = Activation(\'relu\', name=relu_name + \'1\')(x)\n        x = Conv2D(filters, (1, 1), name=conv_name + \'1\', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + \'2\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'2\')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + \'2\', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + \'3\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'3\')(x)\n        x = Conv2D(filters*4, (1, 1), name=conv_name + \'3\', **conv_params)(x)\n\n        x = Add()([x, input_tensor])\n        return x\n\n    return layer\n\n\n\n'"
bes/segmentation_models/backbones/classification_models/classification_models/resnet/builder.py,0,"b'import keras.backend as K\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import Dense\nfrom keras.models import Model\nfrom keras.engine import get_source_inputs\n\nimport keras\nfrom distutils.version import StrictVersion\n\nif StrictVersion(keras.__version__) < StrictVersion(\'2.2.0\'):\n    from keras.applications.imagenet_utils import _obtain_input_shape\nelse:\n    from keras_applications.imagenet_utils import _obtain_input_shape\n\nfrom .params import get_conv_params\nfrom .params import get_bn_params\nfrom .blocks import basic_conv_block\nfrom .blocks import basic_identity_block\nfrom .blocks import conv_block as usual_conv_block\nfrom .blocks import identity_block as usual_identity_block\n\ndef build_resnet(\n     repetitions=(2, 2, 2, 2),\n     include_top=True,\n     input_tensor=None,\n     input_shape=None,\n     classes=1000,\n     block_type=\'usual\'):\n    \n    """"""\n    TODO\n    """"""\n    \n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=96,\n                                      data_format=\'channels_last\',\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape, name=\'data\')\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    \n    # get parameters for model layers\n    no_scale_bn_params = get_bn_params(scale=False)\n    bn_params = get_bn_params()\n    conv_params = get_conv_params()\n    init_filters = 64\n\n    if block_type == \'basic\':\n        conv_block = basic_conv_block\n        identity_block = basic_identity_block\n    else:\n        conv_block = usual_conv_block\n        identity_block = usual_identity_block\n    \n    # resnet bottom\n    x = BatchNormalization(name=\'bn_data\', **no_scale_bn_params)(img_input)\n    x = ZeroPadding2D(padding=(3, 3))(x)\n    x = Conv2D(init_filters, (7, 7), strides=(2, 2), name=\'conv0\', **conv_params)(x)\n    #x = Conv2D(init_filters, (7, 7), name=\'conv0\', **conv_params)(x)\n    x = BatchNormalization(name=\'bn0\', **bn_params)(x)\n    x = Activation(\'relu\', name=\'relu0\')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'valid\', name=\'pooling0\')(x)\n    \n    # resnet body\n    for stage, rep in enumerate(repetitions):\n        for block in range(rep):\n            \n            filters = init_filters * (2**stage)\n            \n            # first block of first stage without strides because we have maxpooling before\n            if block == 0 and stage == 0:\n                x = conv_block(filters, stage, block, strides=(1, 1))(x)\n                \n            elif block == 0:\n                x = conv_block(filters, stage, block, strides=(2, 2))(x)\n                \n            else:\n                x = identity_block(filters, stage, block)(x)\n                \n    x = BatchNormalization(name=\'bn1\', **bn_params)(x)\n    x = Activation(\'relu\', name=\'relu1\')(x)\n\n    # resnet top\n    if include_top:\n        x = GlobalAveragePooling2D(name=\'pool1\')(x)\n        x = Dense(classes, name=\'fc1\')(x)\n        x = Activation(\'softmax\', name=\'softmax\')(x)\n\n    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n        \n    # Create model.\n    model = Model(inputs, x)\n\n    return model\n'"
bes/segmentation_models/backbones/classification_models/classification_models/resnet/models.py,0,"b""from .builder import build_resnet\nfrom ..utils import load_model_weights\nfrom ..weights import weights_collection\n\n\ndef ResNet18(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n    model = build_resnet(input_tensor=input_tensor,\n                         input_shape=input_shape,\n                         repetitions=(2, 2, 2, 2),\n                         classes=classes,\n                         include_top=include_top,\n                         block_type='basic')\n    model.name = 'resnet18'\n\n    if weights:\n        load_model_weights(weights_collection, model, weights, classes, include_top)\n    return model\n\n\ndef ResNet34(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n    model = build_resnet(input_tensor=input_tensor,\n                         input_shape=input_shape,\n                         repetitions=(3, 4, 6, 3),\n                         classes=classes,\n                         include_top=include_top,\n                         block_type='basic')\n    model.name = 'resnet34'\n\n    if weights:\n        load_model_weights(weights_collection, model, weights, classes, include_top)\n    return model\n\n\ndef ResNet50(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n    model = build_resnet(input_tensor=input_tensor,\n                         input_shape=input_shape,\n                         repetitions=(3, 4, 6, 3),\n                         classes=classes,\n                         include_top=include_top)\n    model.name = 'resnet50'\n\n    if weights:\n        load_model_weights(weights_collection, model, weights, classes, include_top)\n    return model\n\n\ndef ResNet101(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n    model = build_resnet(input_tensor=input_tensor,\n                         input_shape=input_shape,\n                         repetitions=(3, 4, 23, 3),\n                         classes=classes,\n                         include_top=include_top)\n    model.name = 'resnet101'\n\n    if weights:\n        load_model_weights(weights_collection, model, weights, classes, include_top)\n    return model\n\n\ndef ResNet152(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n    model = build_resnet(input_tensor=input_tensor,\n                         input_shape=input_shape,\n                         repetitions=(3, 8, 36, 3),\n                         classes=classes,\n                         include_top=include_top)\n    model.name = 'resnet152'\n\n    if weights:\n        load_model_weights(weights_collection, model, weights, classes, include_top)\n    return model\n"""
bes/segmentation_models/backbones/classification_models/classification_models/resnet/params.py,0,"b""# default parameters for convolution and batchnorm layers of ResNet models\n# parameters are obtained from MXNet converted model\n\n\ndef get_conv_params(**params):\n    default_conv_params = {\n        'kernel_initializer': 'glorot_uniform',\n        'use_bias': False,\n        'padding': 'valid',\n    }\n    default_conv_params.update(params)\n    return default_conv_params\n\n\ndef get_bn_params(**params):\n    default_bn_params = {\n        'axis': 3,\n        'momentum': 0.99,\n        'epsilon': 2e-5,\n        'center': True,\n        'scale': True,\n    }\n    default_bn_params.update(params)\n    return default_bn_params\n"""
bes/segmentation_models/backbones/classification_models/classification_models/resnet/preprocessing.py,0,"b'import numpy as np\nfrom skimage.transform import resize\n\ndef preprocess_input(x, size=None, BGRTranspose=True):\n    """"""input standardizing function\n    Args:\n        x: numpy.ndarray with shape (H, W, C)\n        size: tuple (H_new, W_new), resized input shape\n    Return:\n        x: numpy.ndarray\n    """"""\n    if size:\n        x = resize(x, size) * 255\n\n    if BGRTranspose:\n        x = x[..., ::-1]\n\n    return x'"
bes/segmentation_models/backbones/classification_models/classification_models/resnext/__init__.py,0,b'from .preprocessing import preprocess_input'
bes/segmentation_models/backbones/classification_models/classification_models/resnext/blocks.py,0,"b'from keras.layers import Conv2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Add\nfrom keras.layers import Lambda\nfrom keras.layers import Concatenate\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import *\n\nfrom .params import get_conv_params\nfrom .params import get_bn_params\n\ndef cse_block(prevlayer, prefix):\n    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n    lin1 = Dense(K.int_shape(prevlayer)[3]//8, name=prefix + \'cse_lin1\', activation=\'relu\')(mean)\n    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + \'cse_lin2\', activation=\'sigmoid\')(lin1)\n    x = Multiply()([prevlayer, lin2])\n    return x\n\ndef sse_block(prevlayer, prefix):\n    conv = Conv2D(1, (1, 1), padding=""same"", kernel_initializer=""he_normal"", activation=\'sigmoid\', strides=(1, 1),\n              name=prefix + ""_conv"")(prevlayer)\n    conv = Multiply(name=prefix + ""_mul"")([prevlayer, conv])\n    return conv\n\ndef csse_block(x, prefix):\n    \'\'\'\n    Implementation of Concurrent Spatial and Channel \xe2\x80\x98Squeeze & Excitation\xe2\x80\x99 in Fully Convolutional Networks\n    https://arxiv.org/abs/1803.02579\n    \'\'\'\n    cse = cse_block(x, prefix)\n    sse = sse_block(x, prefix)\n    x = Add(name=prefix + ""_csse_mul"")([cse, sse])\n\n    return x\n\n\ndef handle_block_names(stage, block):\n    name_base = \'stage{}_unit{}_\'.format(stage + 1, block + 1)\n    conv_name = name_base + \'conv\'\n    bn_name = name_base + \'bn\'\n    relu_name = name_base + \'relu\'\n    sc_name = name_base + \'sc\'\n    return conv_name, bn_name, relu_name, sc_name\n\n\ndef GroupConv2D(filters, kernel_size, conv_params, conv_name, strides=(1,1), cardinality=32):\n\n    def layer(input_tensor):\n\n        grouped_channels = int(input_tensor.shape[-1]) // cardinality\n\n        blocks = []\n        for c in range(cardinality):\n            x = Lambda(lambda z: z[:, :, :, c * grouped_channels:(c + 1) * grouped_channels])(input_tensor)\n            name = conv_name + \'_\' + str(c)\n            x = Conv2D(grouped_channels, kernel_size, strides=strides,\n                       name=name, **conv_params)(x)\n            blocks.append(x)\n\n        x = Concatenate(axis=-1)(blocks)\n        return x\n    return layer\n\n\ndef conv_block(filters, stage, block, strides=(2, 2)):\n    """"""The conv block is the block that has conv layer at shortcut.\n    # Arguments\n        filters: integer, used for first and second conv layers, third conv layer double this value\n        strides: tuple of integers, strides for conv (3x3) layer in block\n        stage: integer, current stage label, used for generating layer names\n        block: integer, current block label, used for generating layer names\n    # Returns\n        Output layer for the block.\n    """"""\n\n    def layer(input_tensor):\n\n        # extracting params and names for layers\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = Conv2D(filters, (1, 1), name=conv_name + \'1\', **conv_params)(input_tensor)\n        x = BatchNormalization(name=bn_name + \'1\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'1\')(x)\n\n        #if stage > 0:\n        #    x = csse_block(x, prefix=\'csse_block_encoder_{}\'.format(stage))\n        \n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = GroupConv2D(filters, (3, 3), conv_params, conv_name + \'2\', strides=strides)(x)\n        x = BatchNormalization(name=bn_name + \'2\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'2\')(x)\n\n        x = Conv2D(filters * 2, (1, 1), name=conv_name + \'3\', **conv_params)(x)\n        x = BatchNormalization(name=bn_name + \'3\', **bn_params)(x)\n\n        shortcut = Conv2D(filters*2, (1, 1), name=sc_name, strides=strides, **conv_params)(input_tensor)\n        shortcut = BatchNormalization(name=sc_name+\'_bn\', **bn_params)(shortcut)\n        x = Add()([x, shortcut])\n\n        x = Activation(\'relu\', name=relu_name)(x)\n        return x\n\n    return layer\n\n\ndef identity_block(filters, stage, block):\n    """"""The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        filters: integer, used for first and second conv layers, third conv layer double this value\n        stage: integer, current stage label, used for generating layer names\n        block: integer, current block label, used for generating layer names\n    # Returns\n        Output layer for the block.\n    """"""\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = Conv2D(filters, (1, 1), name=conv_name + \'1\', **conv_params)(input_tensor)\n        x = BatchNormalization(name=bn_name + \'1\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'1\')(x)\n\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = GroupConv2D(filters, (3, 3), conv_params, conv_name + \'2\')(x)\n        x = BatchNormalization(name=bn_name + \'2\', **bn_params)(x)\n        x = Activation(\'relu\', name=relu_name + \'2\')(x)\n\n        x = Conv2D(filters * 2, (1, 1), name=conv_name + \'3\', **conv_params)(x)\n        x = BatchNormalization(name=bn_name + \'3\', **bn_params)(x)\n\n        x = Add()([x, input_tensor])\n\n        x = Activation(\'relu\', name=relu_name)(x)\n        return x\n\n    return layer\n'"
bes/segmentation_models/backbones/classification_models/classification_models/resnext/builder.py,0,"b'import keras.backend as K\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import Dense\nfrom keras.models import Model\nfrom keras.engine import get_source_inputs\nfrom keras.layers import *\n\nimport keras\nfrom distutils.version import StrictVersion\n\nif StrictVersion(keras.__version__) < StrictVersion(\'2.2.0\'):\n    from keras.applications.imagenet_utils import _obtain_input_shape\nelse:\n    from keras_applications.imagenet_utils import _obtain_input_shape\n\nfrom .params import get_conv_params\nfrom .params import get_bn_params\n\nfrom .blocks import conv_block\nfrom .blocks import identity_block\n\n\ndef build_resnext(\n     repetitions=(2, 2, 2, 2),\n     include_top=True,\n     input_tensor=None,\n     input_shape=None,\n     classes=1000,\n     first_conv_filters=64,\n     first_block_filters=64):\n    \n    """"""\n    TODO\n    """"""\n    \n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=128,\n                                      data_format=\'channels_last\',\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape, name=\'data\')\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    \n    # get parameters for model layers\n    no_scale_bn_params = get_bn_params(scale=False)\n    bn_params = get_bn_params()\n    conv_params = get_conv_params()\n    init_filters = first_block_filters\n    \n    # resnext bottom\n    x = BatchNormalization(name=\'bn_data\', **no_scale_bn_params)(img_input)\n    x = ZeroPadding2D(padding=(3, 3))(x)\n    x = Conv2D(first_conv_filters, (7, 7), strides=(2, 2), name=\'conv0\', **conv_params)(x)\n    x = BatchNormalization(name=\'bn0\', **bn_params)(x)\n    x = Activation(\'relu\', name=\'relu0\')(x)\n    #x = csse_block(x, prefix=\'csse_block_encoder_relu0\')\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'valid\', name=\'pooling0\')(x)\n    \n    # resnext body\n    for stage, rep in enumerate(repetitions):\n        for block in range(rep):\n            \n            filters = init_filters * (2**stage)\n            \n            # first block of first stage without strides because we have maxpooling before\n            if stage == 0 and block == 0:\n                x = conv_block(filters, stage, block, strides=(1, 1))(x)\n                \n            elif block == 0:\n                x = conv_block(filters, stage, block, strides=(2, 2))(x)\n                \n            else:\n                x = identity_block(filters, stage, block)(x)\n\n    # resnext top\n    if include_top:\n        x = GlobalAveragePooling2D(name=\'pool1\')(x)\n        x = Dense(classes, name=\'fc1\')(x)\n        x = Activation(\'softmax\', name=\'softmax\')(x)\n\n    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n        \n    # Create model\n    model = Model(inputs, x)\n\n    return model\n'"
bes/segmentation_models/backbones/classification_models/classification_models/resnext/models.py,0,"b""from .builder import build_resnext\nfrom ..utils import load_model_weights\nfrom ..weights import weights_collection\n\n\ndef ResNeXt50(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n    model = build_resnext(input_tensor=input_tensor,\n                         input_shape=input_shape,\n                         first_block_filters=128,\n                         repetitions=(3, 4, 6, 3),\n                         classes=classes,\n                         include_top=include_top)\n    model.name = 'resnext50'\n\n    if weights:\n        load_model_weights(weights_collection, model, weights, classes, include_top)\n    return model\n\n\ndef ResNeXt101(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n    model = build_resnext(input_tensor=input_tensor,\n                         input_shape=input_shape,\n                         first_block_filters=128,\n                         repetitions=(3, 4, 23, 3),\n                         classes=classes,\n                         include_top=include_top)\n    model.name = 'resnext101'\n\n    if weights:\n        load_model_weights(weights_collection, model, weights, classes, include_top)\n    return model\n\n"""
bes/segmentation_models/backbones/classification_models/classification_models/resnext/params.py,0,"b""# default parameters for convolution and batchnorm layers of ResNet models\n# parameters are obtained from MXNet converted model\n\n\ndef get_conv_params(**params):\n    default_conv_params = {\n        'kernel_initializer': 'glorot_uniform',\n        'use_bias': False,\n        'padding': 'valid',\n    }\n    default_conv_params.update(params)\n    return default_conv_params\n\n\ndef get_bn_params(**params):\n    default_bn_params = {\n        'axis': 3,\n        'momentum': 0.99,\n        'epsilon': 2e-5,\n        'center': True,\n        'scale': True,\n    }\n    default_bn_params.update(params)\n    return default_bn_params\n"""
bes/segmentation_models/backbones/classification_models/classification_models/resnext/preprocessing.py,0,"b'import numpy as np\nfrom skimage.transform import resize\n\ndef preprocess_input(x, size=None):\n    """"""input standardizing function\n    Args:\n        x: numpy.ndarray with shape (H, W, C)\n        size: tuple (H_new, W_new), resized input shape\n    Return:\n        x: numpy.ndarray\n    """"""\n    if size:\n        x = resize(x, size) * 255\n\n    return x'"
