file_path,api_count,code
datasets.py,1,"b'import os\nimport os.path as osp\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport collections\nimport torch\nimport torchvision\nfrom torch.utils import data\nfrom transform import HorizontalFlip, VerticalFlip\n\ndef default_loader(path):\n    return Image.open(path)\n\nclass VOCDataSet(data.Dataset):\n    def __init__(self, root, split=""trainval"", img_transform=None, label_transform=None):\n        self.root = root\n        self.split = split\n        # self.mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\n        self.files = collections.defaultdict(list)\n        self.img_transform = img_transform\n        self.label_transform = label_transform\n        self.h_flip = HorizontalFlip()\n        self.v_flip = VerticalFlip()\n\n        data_dir = osp.join(root, ""VOC2012"")\n        # for split in [""train"", ""trainval"", ""val""]:\n        imgsets_dir = osp.join(data_dir, ""ImageSets/Segmentation/%s.txt"" % split)\n        with open(imgsets_dir) as imgset_file:\n            for name in imgset_file:\n                name = name.strip()\n                img_file = osp.join(data_dir, ""JPEGImages/%s.jpg"" % name)\n                label_file = osp.join(data_dir, ""SegmentationClass/%s.png"" % name)\n                self.files[split].append({\n                    ""img"": img_file,\n                    ""label"": label_file\n                })\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n        datafiles = self.files[self.split][index]\n\n        img_file = datafiles[""img""]\n        img = Image.open(img_file).convert(\'RGB\')\n        # img = img.resize((256, 256), Image.NEAREST)\n        # img = np.array(img, dtype=np.uint8)\n\n        label_file = datafiles[""label""]\n        label = Image.open(label_file).convert(""P"")\n        label_size = label.size\n        # label image has categorical value, not continuous, so we have to\n        # use NEAREST not BILINEAR\n        # label = label.resize((256, 256), Image.NEAREST)\n        # label = np.array(label, dtype=np.uint8)\n        # label[label == 255] = 21\n\n        if self.img_transform is not None:\n            img_o = self.img_transform(img)\n            # img_h = self.img_transform(self.h_flip(img))\n            # img_v = self.img_transform(self.v_flip(img))\n            imgs = [img_o]\n        else:\n            imgs = img\n\n        if self.label_transform is not None:\n            label_o = self.label_transform(label)\n            # label_h = self.label_transform(self.h_flip(label))\n            # label_v = self.label_transform(self.v_flip(label))\n            labels = [label_o]\n        else:\n            labels = label\n\n        return imgs, labels\n\n\nclass VOCTestSet(data.Dataset):\n    def __init__(self, root, transform=None):\n        self.root = root\n        self.transform = transform\n        self.files = collections.defaultdict(list)\n\n        self.data_dir = osp.join(root, ""VOC2012test"")\n        self.img_names = os.listdir(osp.join(self.data_dir, ""JPEGImages""))\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, index):\n        name = self.img_names[index]\n        img = Image.open(osp.join(self.data_dir, ""JPEGImages"", name)).convert(\'RGB\')\n        size = img.size\n        name = name.split(""."")[0]\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, name, size\n\nif __name__ == \'__main__\':\n    dst = VOCDataSet(""./data"", is_transform=True)\n    trainloader = data.DataLoader(dst, batch_size=4)\n    for i, data in enumerate(trainloader):\n        imgs, labels = data\n        if i == 0:\n            img = torchvision.utils.make_grid(imgs).numpy()\n            img = np.transpose(img, (1, 2, 0))\n            img = img[:, :, ::-1]\n            plt.imshow(img)\n            plt.show()\n'"
duc.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nimport torch.utils.model_zoo as model_zoo\nfrom torchvision import models\n\nimport math\n\n\nclass DUC(nn.Module):\n    def __init__(self, inplanes, planes, upscale_factor=2):\n        super(DUC, self).__init__()\n        self.relu = nn.ReLU()\n        self.conv = nn.Conv2d(inplanes, planes, kernel_size=3,\n                              padding=1)\n        self.bn = nn.BatchNorm2d(planes)\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.pixel_shuffle(x)\n        return x\n\nclass FCN(nn.Module):\n    def __init__(self, num_classes):\n        super(FCN, self).__init__()\n\n        self.num_classes = num_classes\n\n        resnet = models.resnet50(pretrained=True)\n\n        self.conv1 = resnet.conv1\n        self.bn0 = resnet.bn1\n        self.relu = resnet.relu\n        self.maxpool = resnet.maxpool\n\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        self.duc1 = DUC(2048, 2048*2)\n        self.duc2 = DUC(1024, 1024*2)\n        self.duc3 = DUC(512, 512*2)\n        self.duc4 = DUC(128, 128*2)\n        self.duc5 = DUC(64, 64*2)\n\n        self.out1 = self._classifier(1024)\n        self.out2 = self._classifier(512)\n        self.out3 = self._classifier(128)\n        self.out4 = self._classifier(64)\n        self.out5 = self._classifier(32)\n\n        self.transformer = nn.Conv2d(320, 128, kernel_size=1)\n\n    def _classifier(self, inplanes):\n        if inplanes == 32:\n            return nn.Sequential(\n                nn.Conv2d(inplanes, self.num_classes, 1),\n                nn.Conv2d(self.num_classes, self.num_classes,\n                          kernel_size=3, padding=1)\n            )\n        return nn.Sequential(\n            nn.Conv2d(inplanes, inplanes/2, 3, padding=1, bias=False),\n            nn.BatchNorm2d(inplanes/2, momentum=.95),\n            nn.ReLU(inplace=True),\n            nn.Dropout(.1),\n            nn.Conv2d(inplanes/2, self.num_classes, 1),\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn0(x)\n        x = self.relu(x)\n        conv_x = x\n        x = self.maxpool(x)\n        pool_x = x\n\n        fm1 = self.layer1(x)\n        fm2 = self.layer2(fm1)\n        fm3 = self.layer3(fm2)\n        fm4 = self.layer4(fm3)\n\n        dfm1 = fm3 + self.duc1(fm4)\n        out16 = self.out1(dfm1)\n\n        dfm2 = fm2 + self.duc2(dfm1)\n        out8 = self.out2(dfm2)\n\n        dfm3 = fm1 + self.duc3(dfm2)\n\n        dfm3_t = self.transformer(torch.cat((dfm3, pool_x), 1))\n        out4 = self.out3(dfm3_t)\n\n        dfm4 = conv_x + self.duc4(dfm3_t)\n        out2 = self.out4(dfm4)\n\n        dfm5 = self.duc5(dfm4)\n        out = self.out5(dfm5)\n\n        return out, out2, out4, out8, out16\n'"
gcn.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nimport torch.utils.model_zoo as model_zoo\nfrom torchvision import models\n\nimport math\n\n\nclass GCN(nn.Module):\n    def __init__(self, inplanes, planes, ks=7):\n        super(GCN, self).__init__()\n        self.conv_l1 = nn.Conv2d(inplanes, planes, kernel_size=(ks, 1),\n                                 padding=(ks/2, 0))\n\n        self.conv_l2 = nn.Conv2d(planes, planes, kernel_size=(1, ks),\n                                 padding=(0, ks/2))\n        self.conv_r1 = nn.Conv2d(inplanes, planes, kernel_size=(1, ks),\n                                 padding=(0, ks/2))\n        self.conv_r2 = nn.Conv2d(planes, planes, kernel_size=(ks, 1),\n                                 padding=(ks/2, 0))\n\n    def forward(self, x):\n        x_l = self.conv_l1(x)\n        x_l = self.conv_l2(x_l)\n\n        x_r = self.conv_r1(x)\n        x_r = self.conv_r2(x_r)\n\n        x = x_l + x_r\n\n        return x\n\n\nclass Refine(nn.Module):\n    def __init__(self, planes):\n        super(Refine, self).__init__()\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        residual = x\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n\n        out = residual + x\n        return out\n\n\nclass FCN(nn.Module):\n    def __init__(self, num_classes):\n        super(FCN, self).__init__()\n\n        self.num_classes = num_classes\n\n        resnet = models.resnet50(pretrained=True)\n\n        self.conv1 = resnet.conv1\n        self.bn0 = resnet.bn1\n        self.relu = resnet.relu\n        self.maxpool = resnet.maxpool\n\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        self.gcn1 = GCN(2048, self.num_classes)\n        self.gcn2 = GCN(1024, self.num_classes)\n        self.gcn3 = GCN(512, self.num_classes)\n        self.gcn4 = GCN(64, self.num_classes)\n        self.gcn5 = GCN(64, self.num_classes)\n\n        self.refine1 = Refine(self.num_classes)\n        self.refine2 = Refine(self.num_classes)\n        self.refine3 = Refine(self.num_classes)\n        self.refine4 = Refine(self.num_classes)\n        self.refine5 = Refine(self.num_classes)\n        self.refine6 = Refine(self.num_classes)\n        self.refine7 = Refine(self.num_classes)\n        self.refine8 = Refine(self.num_classes)\n        self.refine9 = Refine(self.num_classes)\n        self.refine10 = Refine(self.num_classes)\n\n        self.out0 = self._classifier(2048)\n        self.out1 = self._classifier(1024)\n        self.out2 = self._classifier(512)\n        self.out_e = self._classifier(256)\n        self.out3 = self._classifier(64)\n        self.out4 = self._classifier(64)\n        self.out5 = self._classifier(32)\n\n        self.transformer = nn.Conv2d(256, 64, kernel_size=1)\n\n    def _classifier(self, inplanes):\n        return nn.Sequential(\n            nn.Conv2d(inplanes, inplanes, 3, padding=1, bias=False),\n            nn.BatchNorm2d(inplanes/2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(.1),\n            nn.Conv2d(inplanes/2, self.num_classes, 1),\n        )\n\n    def forward(self, x):\n        input = x\n        x = self.conv1(x)\n        x = self.bn0(x)\n        x = self.relu(x)\n        conv_x = x\n        x = self.maxpool(x)\n        pool_x = x\n\n        fm1 = self.layer1(x)\n        fm2 = self.layer2(fm1)\n        fm3 = self.layer3(fm2)\n        fm4 = self.layer4(fm3)\n\n        gcfm1 = self.refine1(self.gcn1(fm4))\n        gcfm2 = self.refine2(self.gcn2(fm3))\n        gcfm3 = self.refine3(self.gcn3(fm2))\n        gcfm4 = self.refine4(self.gcn4(pool_x))\n        gcfm5 = self.refine5(self.gcn5(conv_x))\n\n        fs1 = self.refine6(F.upsample_bilinear(gcfm1, fm3.size()[2:]) + gcfm2)\n        fs2 = self.refine7(F.upsample_bilinear(fs1, fm2.size()[2:]) + gcfm3)\n        fs3 = self.refine8(F.upsample_bilinear(fs2, pool_x.size()[2:]) + gcfm4)\n        fs4 = self.refine9(F.upsample_bilinear(fs3, conv_x.size()[2:]) + gcfm5)\n        out = self.refine10(F.upsample_bilinear(fs4, input.size()[2:]))\n\n        return out, fs4, fs3, fs2, fs1, gcfm1\n'"
loss.py,2,"b'import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n# Recommend\nclass CrossEntropyLoss2d(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(CrossEntropyLoss2d, self).__init__()\n        self.nll_loss = nn.NLLLoss2d(weight, size_average)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss(F.log_softmax(inputs), targets)\n\n# this may be unstable sometimes.Notice set the size_average\ndef CrossEntropy2d(input, target, weight=None, size_average=False):\n    # input:(n, c, h, w) target:(n, h, w)\n    n, c, h, w = input.size()\n\n    input = input.transpose(1, 2).transpose(2, 3).contiguous()\n    input = input[target.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0].view(-1, c)\n\n    target_mask = target >= 0\n    target = target[target_mask]\n    #loss = F.nll_loss(F.log_softmax(input), target, weight=weight, size_average=False)\n    loss = F.cross_entropy(input, target, weight=weight, size_average=False)\n    if size_average:\n        loss /= target_mask.sum().data[0]\n\n    return loss\n'"
resnet.py,1,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom pairwise import Pairwise\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               stride=stride, bias=False, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.downsample = downsample\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.relu(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out += shortcut\n        out = self.relu(out)\n\n        return out\n\nclass DeconvBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels, expansion=2, stride=1, upsample=None):\n        super(DeconvBottleneck, self).__init__()\n        self.expansion = expansion\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        if stride == 1:\n            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                                   stride=stride, bias=False, padding=1)\n        else:\n            self.conv2 = nn.ConvTranspose2d(out_channels, out_channels,\n                                            kernel_size=3,\n                                            stride=stride, bias=False,\n                                            padding=1,\n                                            output_padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.upsample = upsample\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.relu(out)\n\n        if self.upsample is not None:\n            shortcut = self.upsample(x)\n\n        out += shortcut\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, downblock, upblock, num_layers, n_classes):\n        super(ResNet, self).__init__()\n\n        self.in_channels = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.dlayer1 = self._make_downlayer(downblock, 64, num_layers[0])\n        self.dlayer2 = self._make_downlayer(downblock, 128, num_layers[1],\n                                            stride=2)\n        self.dlayer3 = self._make_downlayer(downblock, 256, num_layers[2],\n                                            stride=2)\n        self.dlayer4 = self._make_downlayer(downblock, 512, num_layers[3],\n                                            stride=2)\n\n        self.uplayer1 = self._make_up_block(upblock, 512, 1, stride=2)\n        self.uplayer2 = self._make_up_block(upblock, 256, num_layers[2], stride=2)\n        self.uplayer3 = self._make_up_block(upblock, 128, num_layers[1], stride=2)\n        self.uplayer4 = self._make_up_block(upblock, 64, 2, stride=2)\n\n        upsample = nn.Sequential(\n            nn.ConvTranspose2d(self.in_channels,  # 256\n                               64,\n                               kernel_size=1, stride=2,\n                               bias=False, output_padding=1),\n            nn.BatchNorm2d(64),\n        )\n        self.uplayer_top = DeconvBottleneck(self.in_channels, 64, 1, 2, upsample)\n\n        self.conv1_1 = nn.ConvTranspose2d(64, n_classes, kernel_size=1, stride=1,\n                                 bias=False)\n\n    def _make_downlayer(self, block, init_channels, num_layer, stride=1):\n        downsample = None\n        if stride != 1 or self.in_channels != init_channels * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channels, init_channels*block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(init_channels*block.expansion),\n            )\n        layers = []\n        layers.append(block(self.in_channels, init_channels, stride, downsample))\n        self.in_channels = init_channels * block.expansion\n        for i in range(1, num_layer):\n            layers.append(block(self.in_channels, init_channels))\n\n        return nn.Sequential(*layers)\n\n    def _make_up_block(self, block, init_channels, num_layer, stride=1):\n        upsample = None\n        # expansion = block.expansion\n        if stride != 1 or self.in_channels != init_channels * 2:\n            upsample = nn.Sequential(\n                nn.ConvTranspose2d(self.in_channels, init_channels*2,\n                                   kernel_size=1, stride=stride,\n                                   bias=False, output_padding=1),\n                nn.BatchNorm2d(init_channels*2),\n            )\n        layers = []\n        for i in range(1, num_layer):\n            layers.append(block(self.in_channels, init_channels, 4))\n        layers.append(block(self.in_channels, init_channels, 2, stride, upsample))\n        self.in_channels = init_channels * 2\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        img = x\n        x_size = x.size()\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.dlayer1(x)\n        x = self.dlayer2(x)\n        x = self.dlayer3(x)\n        x = self.dlayer4(x)\n\n        x = self.uplayer1(x)\n        x = self.uplayer2(x)\n        x = self.uplayer3(x)\n        x = self.uplayer4(x)\n        x = self.uplayer_top(x)\n\n        x = self.conv1_1(x, output_size=img.size())\n\n        return x\n\n\ndef ResNet50(**kwargs):\n    return ResNet(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 22, **kwargs)\n\ndef ResNet101(**kwargs):\n    return ResNet(Bottleneck, [3, 4, 23, 2], 22, **kwargs)\n'"
tester.py,5,"b'import torch\nfrom torch.utils import data\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom transform import Colorize\nfrom torchvision.transforms import ToPILImage, Compose, ToTensor, CenterCrop\nfrom transform import Scale\n# from resnet import FCN\nfrom upsample import FCN\n# from gcn import FCN\nfrom datasets import VOCTestSet\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\n\n\nlabel_transform = Compose([Scale((256, 256), Image.BILINEAR), ToTensor()])\nbatch_size = 1\ndst = VOCTestSet(""./data"", transform=label_transform)\n\ntestloader = data.DataLoader(dst, batch_size=batch_size,\n                             num_workers=8)\n\n\nmodel = torch.nn.DataParallel(FCN(22), device_ids=[0, 1, 2, 3])\n# model = FCN(22)\nmodel.cuda()\nmodel.load_state_dict(torch.load(""./pth/fcn-deconv-40.pth""))\nmodel.eval()\n\n\n# 10 13 48 86 101\nimg = Image.open(""./data/VOC2012test/JPEGImages/2008_000101.jpg"").convert(""RGB"")\noriginal_size = img.size\nimg.save(""original.png"")\nimg = img.resize((256, 256), Image.BILINEAR)\nimg = ToTensor()(img)\nimg = Variable(img).unsqueeze(0)\noutputs = model(img)\n# 22 256 256\nfor i, output in enumerate(outputs):\n    output = output[0].data.max(0)[1]\n    output = Colorize()(output)\n    output = np.transpose(output.numpy(), (1, 2, 0))\n    img = Image.fromarray(output, ""RGB"")\n    if i == 0:\n        img = img.resize(original_size, Image.NEAREST)\n    img.save(""test-%d.png"" % i)\n\n\'\'\'\n\nfor index, (imgs, name, size) in tqdm(enumerate(testloader)):\n    imgs = Variable(imgs.cuda())\n    outputs = model(imgs)\n\n    output = outputs[0][0].data.max(0)[1]\n    output = Colorize()(output)\n    print(output)\n    output = np.transpose(output.numpy(), (1, 2, 0))\n    img = Image.fromarray(output, ""RGB"")\n    # img = Image.fromarray(output[0].cpu().numpy(), ""P"")\n    img = img.resize((size[0].numpy(), size[1].numpy()), Image.NEAREST)\n    img.save(""./results/VOC2012/Segmentation/comp5_test_cls/%s.png"" % name)\n\'\'\'\n'"
trainer.py,10,"b'from __future__ import division\nimport torch\nfrom torch.autograd import Variable\nfrom torch.utils import data\n# from resnet import FCN\nfrom upsample import FCN\n# from gcn import FCN\nfrom datasets import VOCDataSet\nfrom loss import CrossEntropy2d, CrossEntropyLoss2d\nfrom visualize import LinePlotter\nfrom transform import ReLabel, ToLabel, ToSP, Scale\nfrom torchvision.transforms import Compose, CenterCrop, Normalize, ToTensor\nimport tqdm\nfrom PIL import Image\nimport numpy as np\n\ninput_transform = Compose([\n    Scale((256, 256), Image.BILINEAR),\n    ToTensor(),\n    Normalize([.485, .456, .406], [.229, .224, .225]),\n\n])\ntarget_transform = Compose([\n    Scale((256, 256), Image.NEAREST),\n    ToSP(256),\n    ToLabel(),\n    ReLabel(255, 21),\n])\n\ntrainloader = data.DataLoader(VOCDataSet(""./data"", img_transform=input_transform,\n                                         label_transform=target_transform),\n                              batch_size=16, shuffle=True, pin_memory=True)\n\nif torch.cuda.is_available():\n    model = torch.nn.DataParallel(FCN(22))\n    model.cuda()\n\nepoches = 80\nlr = 1e-4\nweight_decay = 2e-5\nmomentum = 0.9\nweight = torch.ones(22)\nweight[21] = 0\nmax_iters = 92*epoches\n\ncriterion = CrossEntropyLoss2d(weight.cuda())\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum,\n                            weight_decay=weight_decay)\nploter = LinePlotter()\n\nmodel.train()\nfor epoch in range(epoches):\n    running_loss = 0.0\n    for i, (images, labels_group) in tqdm.tqdm(enumerate(trainloader)):\n        if torch.cuda.is_available():\n            images = [Variable(image.cuda()) for image in images]\n            labels_group = [labels for labels in labels_group]\n        else:\n            images = [Variable(image) for image in images]\n            labels_group = [labels for labels in labels_group]\n\n        optimizer.zero_grad()\n        losses = []\n        for img, labels in zip(images, labels_group):\n            outputs = model(img)\n            labels = [Variable(label.cuda()) for label in labels]\n            for pair in zip(outputs, labels):\n                losses.append(criterion(pair[0], pair[1]))\n\n        if epoch < 40:\n            loss_weight = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]\n        else:\n            loss_weight = [0.5, 0.1, 0.1, 0.1, 0.1, 0.1]\n\n        loss = 0\n        for w, l in zip(loss_weight, losses):\n            loss += w*l\n\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.data[0]\n\n        # lr = lr * (1-(92*epoch+i)/max_iters)**0.9\n        # for parameters in optimizer.param_groups:\n        #     parameters[\'lr\'] = lr\n\n    print(""Epoch [%d] Loss: %.4f"" % (epoch+1, running_loss/i))\n    ploter.plot(""loss"", ""train"", epoch+1, running_loss/i)\n    running_loss = 0\n\n    if (epoch+1) % 20 == 0:\n        lr /= 10\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr,\n                                    momentum=momentum,\n                                    weight_decay=weight_decay)\n        torch.save(model.state_dict(), ""./pth/fcn-deconv-%d.pth"" % (epoch+1))\n\n\ntorch.save(model.state_dict(), ""./pth/fcn-deconv.pth"")\n'"
transform.py,4,"b'import numpy as np\nimport torch\nfrom PIL import Image\nimport collections\n\n\nclass Scale(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        if isinstance(self.size, int):\n            w, h = img.size\n            if (w <= h and w == self.size) or (h <= w and h == self.size):\n                return img\n            if w < h:\n                ow = self.size\n                oh = int(self.size * h / w)\n                return img.resize((ow, oh), self.interpolation)\n            else:\n                oh = self.size\n                ow = int(self.size * w / h)\n                return img.resize((ow, oh), self.interpolation)\n        else:\n            return img.resize(self.size, self.interpolation)\n\n\nclass ToParallel(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        yield img\n        for t in self.transforms:\n            yield t(img)\n\n\nclass ToLabel(object):\n    def __call__(self, inputs):\n        tensors = []\n        for i in inputs:\n            tensors.append(torch.from_numpy(np.array(i)).long())\n        return tensors\n\n\nclass ReLabel(object):\n    def __init__(self, olabel, nlabel):\n        self.olabel = olabel\n        self.nlabel = nlabel\n\n    def __call__(self, inputs):\n        # assert isinstance(input, torch.LongTensor), \'tensor needs to be LongTensor\'\n        for i in inputs:\n            i[i == self.olabel] = self.nlabel\n        return inputs\n\n\nclass ToSP(object):\n    def __init__(self, size):\n        self.scale2 = Scale(size/2, Image.NEAREST)\n        self.scale4 = Scale(size/4, Image.NEAREST)\n        self.scale8 = Scale(size/8, Image.NEAREST)\n        self.scale16 = Scale(size/16, Image.NEAREST)\n        self.scale32 = Scale(size/32, Image.NEAREST)\n\n    def __call__(self, input):\n        input2 = self.scale2(input)\n        input4 = self.scale4(input)\n        input8 = self.scale8(input)\n        input16 = self.scale16(input)\n        input32 = self.scale32(input)\n        inputs = [input, input2, input4, input8, input16, input32]\n        # inputs = [input]\n\n        return inputs\n\n\nclass HorizontalFlip(object):\n    """"""Horizontally flips the given PIL.Image with a probability of 0.5.""""""\n\n    def __call__(self, img):\n        return img.transpose(Image.FLIP_LEFT_RIGHT)\n\n\nclass VerticalFlip(object):\n    def __call__(self, img):\n        return img.transpose(Image.FLIP_TOP_BOTTOM)\n\ndef uint82bin(n, count=8):\n    """"""returns the binary of integer n, count refers to amount of bits""""""\n    return \'\'.join([str((n >> y) & 1) for y in range(count-1, -1, -1)])\n\ndef labelcolormap(N):\n    cmap = np.zeros((N, 3), dtype=np.uint8)\n    for i in range(N):\n        r = 0\n        g = 0\n        b = 0\n        id = i\n        for j in range(7):\n            str_id = uint82bin(id)\n            r = r ^ (np.uint8(str_id[-1]) << (7-j))\n            g = g ^ (np.uint8(str_id[-2]) << (7-j))\n            b = b ^ (np.uint8(str_id[-3]) << (7-j))\n            id = id >> 3\n        cmap[i, 0] = r\n        cmap[i, 1] = g\n        cmap[i, 2] = b\n    return cmap\n\ndef colormap(n):\n    cmap = np.zeros([n, 3]).astype(np.uint8)\n\n    for i in np.arange(n):\n        r, g, b = np.zeros(3)\n\n        for j in np.arange(8):\n            r = r + (1 << (7-j))*((i & (1 << (3*j))) >> (3*j))\n            g = g + (1 << (7-j))*((i & (1 << (3*j+1))) >> (3*j+1))\n            b = b + (1 << (7-j))*((i & (1 << (3*j+2))) >> (3*j+2))\n\n        cmap[i, :] = np.array([r, g, b])\n\n    return cmap\n\n\nclass Colorize(object):\n    def __init__(self, n=22):\n        self.cmap = labelcolormap(22)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.size()\n        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image[0]).cpu()\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        return color_image\n'"
upsample.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nimport torch.utils.model_zoo as model_zoo\nfrom torchvision import models\n\n\nclass Upsample(nn.Module):\n    def __init__(self, inplanes, planes):\n        super(Upsample, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=5, padding=2)\n        self.bn = nn.BatchNorm2d(planes)\n\n    def forward(self, x, size):\n        x = F.upsample_bilinear(x, size=size)\n        x = self.conv1(x)\n        x = self.bn(x)\n        return x\n\n\nclass Fusion(nn.Module):\n    def __init__(self, inplanes):\n        super(Fusion, self).__init__()\n        self.conv = nn.Conv2d(inplanes, inplanes, kernel_size=1)\n        self.bn = nn.BatchNorm2d(inplanes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x1, x2):\n        out = self.bn(self.conv(x1)) + x2\n        out = self.relu(out)\n\n        return out\n\n\nclass FCN(nn.Module):\n    def __init__(self, num_classes):\n        super(FCN, self).__init__()\n\n        self.num_classes = num_classes\n\n        resnet = models.resnet101(pretrained=True)\n\n        self.conv1 = resnet.conv1\n        self.bn0 = resnet.bn1\n        self.relu = resnet.relu\n        self.maxpool = resnet.maxpool\n\n        self.layer1 = resnet.layer1\n        self.layer2 = resnet.layer2\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n\n        self.upsample1 = Upsample(2048, 1024)\n        self.upsample2 = Upsample(1024, 512)\n        self.upsample3 = Upsample(512, 64)\n        self.upsample4 = Upsample(64, 64)\n        self.upsample5 = Upsample(64, 32)\n\n        self.fs1 = Fusion(1024)\n        self.fs2 = Fusion(512)\n        self.fs3 = Fusion(256)\n        self.fs4 = Fusion(64)\n        self.fs5 = Fusion(64)\n\n        self.out0 = self._classifier(2048)\n        self.out1 = self._classifier(1024)\n        self.out2 = self._classifier(512)\n        self.out_e = self._classifier(256)\n        self.out3 = self._classifier(64)\n        self.out4 = self._classifier(64)\n        self.out5 = self._classifier(32)\n\n        self.transformer = nn.Conv2d(256, 64, kernel_size=1)\n\n    def _classifier(self, inplanes):\n        if inplanes == 32:\n            return nn.Sequential(\n                nn.Conv2d(inplanes, self.num_classes, 1),\n                nn.Conv2d(self.num_classes, self.num_classes,\n                          kernel_size=3, padding=1)\n            )\n        return nn.Sequential(\n            nn.Conv2d(inplanes, inplanes/2, 3, padding=1, bias=False),\n            nn.BatchNorm2d(inplanes/2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(.1),\n            nn.Conv2d(inplanes/2, self.num_classes, 1),\n        )\n\n    def forward(self, x):\n        input = x\n        x = self.conv1(x)\n        x = self.bn0(x)\n        x = self.relu(x)\n        conv_x = x\n        x = self.maxpool(x)\n        pool_x = x\n\n        fm1 = self.layer1(x)\n        fm2 = self.layer2(fm1)\n        fm3 = self.layer3(fm2)\n        fm4 = self.layer4(fm3)\n\n        out32 = self.out0(fm4)\n\n        fsfm1 = self.fs1(fm3, self.upsample1(fm4, fm3.size()[2:]))\n        out16 = self.out1(fsfm1)\n\n        fsfm2 = self.fs2(fm2, self.upsample2(fsfm1, fm2.size()[2:]))\n        out8 = self.out2(fsfm2)\n\n        fsfm3 = self.fs4(pool_x, self.upsample3(fsfm2, pool_x.size()[2:]))\n        # print(fsfm3.size())\n        out4 = self.out3(fsfm3)\n\n        fsfm4 = self.fs5(conv_x, self.upsample4(fsfm3, conv_x.size()[2:]))\n        out2 = self.out4(fsfm4)\n\n        fsfm5 = self.upsample5(fsfm4, input.size()[2:])\n        out = self.out5(fsfm5)\n\n        return out, out2, out4, out8, out16, out32\n'"
visualize.py,0,"b'import visdom\nimport numpy as np\n\nclass LinePlotter(object):\n    def __init__(self, env_name=""main""):\n        self.vis = visdom.Visdom()\n        self.env = env_name\n        self.plots = {}\n\n    def plot(self, var_name, split_name, x, y):\n        if var_name not in self.plots:\n            self.plots[var_name] = self.vis.line(X=np.array([x, x]),\n                                    Y=np.array([y, y]), env=self.env, opts=dict(\n                                    legend=[split_name],\n                                    title=var_name,\n                                    xlabel=""Iters"",\n                                    ylabel=var_name\n                                    ))\n        else:\n            self.vis.updateTrace(X=np.array([x, x]), Y=np.array([y, y]), env=self.env,\n                                win=self.plots[var_name], name=split_name)\n'"
