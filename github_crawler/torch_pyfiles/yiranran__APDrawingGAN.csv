file_path,api_count,code
test.py,0,"b""import os\nfrom options.test_options import TestOptions\nfrom data import CreateDataLoader\nfrom models import create_model\nfrom util.visualizer import save_images\nfrom util import html\n\n\nif __name__ == '__main__':\n    opt = TestOptions().parse()\n    opt.num_threads = 1   # test code only supports num_threads = 1\n    opt.batch_size = 1  # test code only supports batch_size = 1\n    opt.serial_batches = True  # no shuffle\n    opt.no_flip = True  # no flip\n    opt.display_id = -1  # no visdom display\n    data_loader = CreateDataLoader(opt)\n    dataset = data_loader.load_data()\n    model = create_model(opt)\n    model.setup(opt)\n    # create website\n    web_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\n    webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))\n    # test\n    for i, data in enumerate(dataset):\n        if i >= opt.how_many:#test code only supports batch_size = 1, how_many means how many test images to run\n            break\n        model.set_input(data)\n        model.test()\n        visuals = model.get_current_visuals()#in test the loadSize is set to the same as fineSize\n        img_path = model.get_image_paths()\n        if i % 5 == 0:\n            print('processing (%04d)-th image... %s' % (i, img_path))\n        save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize)\n\n    webpage.save()\n"""
train.py,0,"b""import time\nfrom options.train_options import TrainOptions\nfrom data import CreateDataLoader\nfrom models import create_model\nfrom util.visualizer import Visualizer\n\nif __name__ == '__main__':\n    start = time.time()\n    opt = TrainOptions().parse()\n    data_loader = CreateDataLoader(opt)\n    dataset = data_loader.load_data()\n    dataset_size = len(data_loader)\n    print('#training images = %d' % dataset_size)\n\n    model = create_model(opt)\n    model.setup(opt)\n    visualizer = Visualizer(opt)\n    total_steps = 0\n\n    for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n        epoch_start_time = time.time()\n        iter_data_time = time.time()\n        epoch_iter = 0\n\n        for i, data in enumerate(dataset):\n            iter_start_time = time.time()\n            if total_steps % opt.print_freq == 0:\n                t_data = iter_start_time - iter_data_time\n            visualizer.reset()\n            total_steps += opt.batch_size\n            epoch_iter += opt.batch_size\n            model.set_input(data)\n            model.optimize_parameters()\n\n            if total_steps % opt.display_freq == 0:\n                save_result = total_steps % opt.update_html_freq == 0\n                visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n\n            if total_steps % opt.print_freq == 0:\n                losses = model.get_current_losses()\n                t = (time.time() - iter_start_time) / opt.batch_size\n                visualizer.print_current_losses(epoch, epoch_iter, losses, t, t_data)\n                if opt.display_id > 0:\n                    visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, opt, losses)\n            \n            if total_steps % opt.save_latest_freq == 0:\n                print('saving the latest model (epoch %d, total_steps %d)' %\n                      (epoch, total_steps))\n                #model.save_networks('latest')\n                model.save_networks2('latest')\n\n            iter_data_time = time.time()\n        if epoch % opt.save_epoch_freq == 0:\n            print('saving the model at the end of epoch %d, iters %d' %\n                  (epoch, total_steps))\n            #model.save_networks('latest')\n            #model.save_networks(epoch)\n            model.save_networks2('latest')\n            model.save_networks2(epoch)\n\n        print('End of epoch %d / %d \\t Time Taken: %d sec' %\n              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n        model.update_learning_rate()\n\n    print('Total Time Taken: %d sec' % (time.time() - start))\n"""
data/__init__.py,2,"b'import importlib\nimport torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\nfrom data.base_dataset import BaseDataset\n\n\ndef find_dataset_using_name(dataset_name):\n    # Given the option --dataset_mode [datasetname],\n    # the file ""data/datasetname_dataset.py""\n    # will be imported.\n    dataset_filename = ""data."" + dataset_name + ""_dataset""\n    datasetlib = importlib.import_module(dataset_filename)\n\n    # In the file, the class called DatasetNameDataset() will\n    # be instantiated. It has to be a subclass of BaseDataset,\n    # and it is case-insensitive.\n    dataset = None\n    target_dataset_name = dataset_name.replace(\'_\', \'\') + \'dataset\'\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() \\\n           and issubclass(cls, BaseDataset):\n            dataset = cls\n\n    if dataset is None:\n        print(""In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase."" % (dataset_filename, target_dataset_name))\n        exit(0)\n\n    return dataset\n\n\ndef get_option_setter(dataset_name):\n    dataset_class = find_dataset_using_name(dataset_name)\n    return dataset_class.modify_commandline_options\n\n\ndef create_dataset(opt):\n    dataset = find_dataset_using_name(opt.dataset_mode)\n    instance = dataset()\n    instance.initialize(opt)\n    print(""dataset [%s] was created"" % (instance.name()))\n    return instance\n\n\ndef CreateDataLoader(opt):\n    data_loader = CustomDatasetDataLoader()\n    data_loader.initialize(opt)\n    return data_loader\n\n\n# Wrapper class of Dataset class that performs\n# multi-threaded data loading\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = create_dataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batch_size,\n            shuffle=not opt.serial_batches,#in training, serial_batches by default is false, shuffle=true\n            num_workers=int(opt.num_threads))\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        for i, data in enumerate(self.dataloader):\n            if i * self.opt.batch_size >= self.opt.max_dataset_size:\n                break\n            yield data\n'"
data/aligned_dataset.py,10,"b""import os.path\nimport random\nimport torchvision.transforms as transforms\nimport torch\nfrom data.base_dataset import BaseDataset\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport csv\n\ndef getfeats(featpath):\n\ttrans_points = np.empty([5,2],dtype=np.int64) \n\twith open(featpath, 'r') as csvfile:\n\t\treader = csv.reader(csvfile, delimiter=' ')\n\t\tfor ind,row in enumerate(reader):\n\t\t\ttrans_points[ind,:] = row\n\treturn trans_points\n\ndef tocv2(ts):\n    img = (ts.numpy()/2+0.5)*255\n    img = img.astype('uint8')\n    img = np.transpose(img,(1,2,0))\n    img = img[:,:,::-1]#rgb->bgr\n    return img\n\ndef dt(img):\n    if(img.shape[2]==3):\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    #convert to BW\n    ret1,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n    ret2,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)\n    dt1 = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)\n    dt2 = cv2.distanceTransform(thresh2,cv2.DIST_L2,5)\n    dt1 = dt1/dt1.max()#->[0,1]\n    dt2 = dt2/dt2.max()\n    return dt1, dt2\n\ndef getSoft(size,xb,yb,boundwidth=5.0):\n    xarray = np.tile(np.arange(0,size[1]),(size[0],1))\n    yarray = np.tile(np.arange(0,size[0]),(size[1],1)).transpose()\n    cxdists = []\n    cydists = []\n    for i in range(len(xb)):\n        xba = np.tile(xb[i],(size[1],1)).transpose()\n        yba = np.tile(yb[i],(size[0],1))\n        cxdists.append(np.abs(xarray-xba))\n        cydists.append(np.abs(yarray-yba))\n    xdist = np.minimum.reduce(cxdists)\n    ydist = np.minimum.reduce(cydists)\n    manhdist = np.minimum.reduce([xdist,ydist])\n    im = (manhdist+1) / (boundwidth+1) * 1.0\n    im[im>=1.0] = 1.0\n    return im\n\nclass AlignedDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_AB = os.path.join(opt.dataroot, opt.phase)\n        self.AB_paths = sorted(make_dataset(self.dir_AB))\n        assert(opt.resize_or_crop == 'resize_and_crop')\n\n    def __getitem__(self, index):\n        AB_path = self.AB_paths[index]\n        AB = Image.open(AB_path).convert('RGB')\n        w, h = AB.size\n        w2 = int(w / 2)\n        A = AB.crop((0, 0, w2, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        B = AB.crop((w2, 0, w, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        A = transforms.ToTensor()(A)\n        B = transforms.ToTensor()(B)\n        w_offset = random.randint(0, max(0, self.opt.loadSize - self.opt.fineSize - 1))\n        h_offset = random.randint(0, max(0, self.opt.loadSize - self.opt.fineSize - 1))\n\n        A = A[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]#C,H,W\n        B = B[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n\n        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A)\n        B = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(B)\n\n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n            output_nc = self.opt.input_nc\n        else:\n            input_nc = self.opt.input_nc\n            output_nc = self.opt.output_nc\n\n        flipped = False\n        if (not self.opt.no_flip) and random.random() < 0.5:\n            flipped = True\n            idx = [i for i in range(A.size(2) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            A = A.index_select(2, idx)\n            B = B.index_select(2, idx)\n\n        if input_nc == 1:  # RGB to gray\n            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n            A = tmp.unsqueeze(0)\n\n        if output_nc == 1:  # RGB to gray\n            tmp = B[0, ...] * 0.299 + B[1, ...] * 0.587 + B[2, ...] * 0.114\n            B = tmp.unsqueeze(0)\n        \n        item = {'A': A, 'B': B,\n                'A_paths': AB_path, 'B_paths': AB_path}\n\n        if self.opt.use_local:\n            regions = ['eyel','eyer','nose','mouth']\n            basen = os.path.basename(AB_path)[:-4]+'.txt'\n            featdir = self.opt.lm_dir\n            featpath = os.path.join(featdir,basen)\n            feats = getfeats(featpath)\n            if flipped:\n                for i in range(5):\n                    feats[i,0] = self.opt.fineSize - feats[i,0] - 1\n                tmp = [feats[0,0],feats[0,1]]\n                feats[0,:] = [feats[1,0],feats[1,1]]\n                feats[1,:] = tmp\n            mouth_x = int((feats[3,0]+feats[4,0])/2.0)\n            mouth_y = int((feats[3,1]+feats[4,1])/2.0)\n            ratio = self.opt.fineSize / 256\n            EYE_H = self.opt.EYE_H * ratio\n            EYE_W = self.opt.EYE_W * ratio\n            NOSE_H = self.opt.NOSE_H * ratio\n            NOSE_W = self.opt.NOSE_W * ratio\n            MOUTH_H = self.opt.MOUTH_H * ratio\n            MOUTH_W = self.opt.MOUTH_W * ratio\n            center = torch.tensor([[feats[0,0],feats[0,1]-4*ratio],[feats[1,0],feats[1,1]-4*ratio],[feats[2,0],feats[2,1]-NOSE_H/2+16*ratio],[mouth_x,mouth_y]])\n            item['center'] = center\n            rhs = [EYE_H,EYE_H,NOSE_H,MOUTH_H]\n            rws = [EYE_W,EYE_W,NOSE_W,MOUTH_W]\n            if self.opt.soft_border:\n                soft_border_mask4 = []\n                for i in range(4):\n                    xb = [np.zeros(rhs[i]),np.ones(rhs[i])*(rws[i]-1)]\n                    yb = [np.zeros(rws[i]),np.ones(rws[i])*(rhs[i]-1)]\n                    soft_border_mask = getSoft([rhs[i],rws[i]],xb,yb)\n                    soft_border_mask4.append(torch.Tensor(soft_border_mask).unsqueeze(0))\n                    item['soft_'+regions[i]+'_mask'] = soft_border_mask4[i]\n            for i in range(4):\n                item[regions[i]+'_A'] = A[:,center[i,1]-rhs[i]/2:center[i,1]+rhs[i]/2,center[i,0]-rws[i]/2:center[i,0]+rws[i]/2]\n                item[regions[i]+'_B'] = B[:,center[i,1]-rhs[i]/2:center[i,1]+rhs[i]/2,center[i,0]-rws[i]/2:center[i,0]+rws[i]/2]\n                if self.opt.soft_border:\n                    item[regions[i]+'_A'] = item[regions[i]+'_A'] * soft_border_mask4[i].repeat(input_nc/output_nc,1,1)\n                    item[regions[i]+'_B'] = item[regions[i]+'_B'] * soft_border_mask4[i]\n            \n            mask = torch.ones(B.shape) # mask out eyes, nose, mouth\n            for i in range(4):\n                mask[:,center[i,1]-rhs[i]/2:center[i,1]+rhs[i]/2,center[i,0]-rws[i]/2:center[i,0]+rws[i]/2] = 0\n            if self.opt.soft_border:\n                imgsize = self.opt.fineSize\n                maskn = mask[0].numpy()\n                masks = [np.ones([imgsize,imgsize]),np.ones([imgsize,imgsize]),np.ones([imgsize,imgsize]),np.ones([imgsize,imgsize])]\n                masks[0][1:] = maskn[:-1]\n                masks[1][:-1] = maskn[1:]\n                masks[2][:,1:] = maskn[:,:-1]\n                masks[3][:,:-1] = maskn[:,1:]\n                masks2 = [maskn-e for e in masks]\n                bound = np.minimum.reduce(masks2)\n                bound = -bound\n                xb = []\n                yb = []\n                for i in range(4):\n                    xbi = [center[i,0]-rws[i]/2, center[i,0]+rws[i]/2-1]\n                    ybi = [center[i,1]-rhs[i]/2, center[i,1]+rhs[i]/2-1]\n                    for j in range(2):\n                        maskx = bound[:,xbi[j]]\n                        masky = bound[ybi[j],:]\n                        xb += [(1-maskx)*10000 + maskx*xbi[j]]\n                        yb += [(1-masky)*10000 + masky*ybi[j]]\n                soft = 1-getSoft([imgsize,imgsize],xb,yb)\n                soft = torch.Tensor(soft).unsqueeze(0)\n                mask = (torch.ones(mask.shape)-mask)*soft + mask\n            \n            bgdir = self.opt.bg_dir\n            bgpath = os.path.join(bgdir,basen[:-4]+'.png')\n            im_bg = Image.open(bgpath)\n            mask2 = transforms.ToTensor()(im_bg) # mask out background\n            if flipped:\n                mask2 = mask2.index_select(2, idx)\n            mask2 = (mask2 >= 0.5).float()\n\n            hair_A = (A/2+0.5) * mask.repeat(input_nc/output_nc,1,1) * mask2.repeat(input_nc/output_nc,1,1) * 2 - 1\n            hair_B = (B/2+0.5) * mask * mask2 * 2 - 1\n            bg_A = (A/2+0.5) * (torch.ones(mask2.shape)-mask2).repeat(input_nc/output_nc,1,1) * 2 - 1\n            bg_B = (B/2+0.5) * (torch.ones(mask2.shape)-mask2) * 2 - 1\n            item['hair_A'] = hair_A\n            item['hair_B'] = hair_B\n            item['bg_A'] = bg_A\n            item['bg_B'] = bg_B\n            item['mask'] = mask\n            item['mask2'] = mask2\n        \n        if self.opt.isTrain:\n            if self.opt.which_direction == 'AtoB':\n                img = tocv2(B)\n            else:\n                img = tocv2(A)\n            dt1, dt2 = dt(img)\n            dt1 = torch.from_numpy(dt1)\n            dt2 = torch.from_numpy(dt2)\n            dt1 = dt1.unsqueeze(0)\n            dt2 = dt2.unsqueeze(0)\n            item['dt1gt'] = dt1\n            item['dt2gt'] = dt2\n\n        return item\n\n    def __len__(self):\n        return len(self.AB_paths)\n\n    def name(self):\n        return 'AlignedDataset'\n"""
data/base_data_loader.py,0,"b'class BaseDataLoader():\n    def __init__(self):\n        pass\n\n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n'"
data/base_dataset.py,1,"b'import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return \'BaseDataset\'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        pass\n\n    def __len__(self):\n        return 0\n\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == \'resize_and_crop\':\n        osize = [opt.loadSize, opt.fineSize]\n        transform_list.append(transforms.Resize(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'crop\':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'scale_width\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == \'scale_width_and_crop\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSize)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'none\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __adjust(img)))\n    else:\n        raise ValueError(\'--resize_or_crop %s is not a valid option.\' % opt.resize_or_crop)\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\n# just modify the width and height to be multiple of 4\ndef __adjust(img):\n    ow, oh = img.size\n\n    # the size needs to be a multiple of this number, \n    # because going through generator network may change img size\n    # and eventually cause size mismatch error\n    mult = 4 \n    if ow % mult == 0 and oh % mult == 0:\n        return img\n    w = (ow - 1) // mult\n    w = (w + 1) * mult\n    h = (oh - 1) // mult\n    h = (h + 1) * mult\n\n    if ow != w or oh != h:\n        __print_size_warning(ow, oh, w, h)\n        \n    return img.resize((w, h), Image.BICUBIC)\n\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    \n    # the size needs to be a multiple of this number, \n    # because going through generator network may change img size\n    # and eventually cause size mismatch error    \n    mult = 4\n    assert target_width % mult == 0, ""the target width needs to be multiple of %d."" % mult\n    if (ow == target_width and oh % mult == 0):\n        return img\n    w = target_width\n    target_height = int(target_width * oh / ow)\n    m = (target_height - 1) // mult\n    h = (m + 1) * mult\n\n    if target_height != h:\n        __print_size_warning(target_width, target_height, w, h)\n    \n    return img.resize((w, h), Image.BICUBIC)\n\n\ndef __print_size_warning(ow, oh, w, h):\n    if not hasattr(__print_size_warning, \'has_printed\'):\n        print(""The image size needs to be a multiple of 4. ""\n              ""The loaded image size was (%d, %d), so it was adjusted to ""\n              ""(%d, %d). This adjustment will be done to all images ""\n              ""whose sizes are not multiples of 4"" % (ow, oh, w, h))\n        __print_size_warning.has_printed = True\n\n\n'"
data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
data/single_dataset.py,7,"b""import os.path\nimport torch\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport numpy as np\nimport csv\n\ndef getfeats(featpath):\n\ttrans_points = np.empty([5,2],dtype=np.int64) \n\twith open(featpath, 'r') as csvfile:\n\t\treader = csv.reader(csvfile, delimiter=' ')\n\t\tfor ind,row in enumerate(reader):\n\t\t\ttrans_points[ind,:] = row\n\treturn trans_points\n\ndef getSoft(size,xb,yb,boundwidth=5.0):\n    xarray = np.tile(np.arange(0,size[1]),(size[0],1))\n    yarray = np.tile(np.arange(0,size[0]),(size[1],1)).transpose()\n    cxdists = []\n    cydists = []\n    for i in range(len(xb)):\n        xba = np.tile(xb[i],(size[1],1)).transpose()\n        yba = np.tile(yb[i],(size[0],1))\n        cxdists.append(np.abs(xarray-xba))\n        cydists.append(np.abs(yarray-yba))\n    xdist = np.minimum.reduce(cxdists)\n    ydist = np.minimum.reduce(cydists)\n    manhdist = np.minimum.reduce([xdist,ydist])\n    im = (manhdist+1) / (boundwidth+1) * 1.0\n    im[im>=1.0] = 1.0\n    return im\n\nclass SingleDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot)\n\n        self.A_paths = make_dataset(self.dir_A)\n\n        self.A_paths = sorted(self.A_paths)\n\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index]\n        A_img = Image.open(A_path).convert('RGB')\n        A = self.transform(A_img)\n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n            output_nc = self.opt.input_nc\n        else:\n            input_nc = self.opt.input_nc\n            output_nc = self.opt.output_nc\n\n        if input_nc == 1:  # RGB to gray\n            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n            A = tmp.unsqueeze(0)\n\n        item = {'A': A, 'A_paths': A_path}\n        \n        if self.opt.use_local:\n            regions = ['eyel','eyer','nose','mouth']\n            basen = os.path.basename(A_path)[:-4]+'.txt'\n            featdir = self.opt.lm_dir\n            featpath = os.path.join(featdir,basen)\n            feats = getfeats(featpath)\n            mouth_x = int((feats[3,0]+feats[4,0])/2.0)\n            mouth_y = int((feats[3,1]+feats[4,1])/2.0)\n            ratio = self.opt.fineSize / 256\n            EYE_H = self.opt.EYE_H * ratio\n            EYE_W = self.opt.EYE_W * ratio\n            NOSE_H = self.opt.NOSE_H * ratio\n            NOSE_W = self.opt.NOSE_W * ratio\n            MOUTH_H = self.opt.MOUTH_H * ratio\n            MOUTH_W = self.opt.MOUTH_W * ratio\n            center = torch.tensor([[feats[0,0],feats[0,1]-4*ratio],[feats[1,0],feats[1,1]-4*ratio],[feats[2,0],feats[2,1]-NOSE_H/2+16*ratio],[mouth_x,mouth_y]])\n            item['center'] = center\n            rhs = [EYE_H,EYE_H,NOSE_H,MOUTH_H]\n            rws = [EYE_W,EYE_W,NOSE_W,MOUTH_W]\n            if self.opt.soft_border:\n                soft_border_mask4 = []\n                for i in range(4):\n                    xb = [np.zeros(rhs[i]),np.ones(rhs[i])*(rws[i]-1)]\n                    yb = [np.zeros(rws[i]),np.ones(rws[i])*(rhs[i]-1)]\n                    soft_border_mask = getSoft([rhs[i],rws[i]],xb,yb)\n                    soft_border_mask4.append(torch.Tensor(soft_border_mask).unsqueeze(0))\n                    item['soft_'+regions[i]+'_mask'] = soft_border_mask4[i]\n            for i in range(4):\n                item[regions[i]+'_A'] = A[:,int(center[i,1]-rhs[i]/2):int(center[i,1]+rhs[i]/2),int(center[i,0]-rws[i]/2):int(center[i,0]+rws[i]/2)]\n                if self.opt.soft_border:\n                    item[regions[i]+'_A'] = item[regions[i]+'_A'] * soft_border_mask4[i].repeat(input_nc/output_nc,1,1)\n            \n            mask = torch.ones([output_nc,A.shape[1],A.shape[2]]) # mask out eyes, nose, mouth\n            for i in range(4):\n                mask[:,int(center[i,1]-rhs[i]/2):int(center[i,1]+rhs[i]/2),int(center[i,0]-rws[i]/2):int(center[i,0]+rws[i]/2)] = 0\n            if self.opt.soft_border:\n                imgsize = self.opt.fineSize\n                maskn = mask[0].numpy()\n                masks = [np.ones([imgsize,imgsize]),np.ones([imgsize,imgsize]),np.ones([imgsize,imgsize]),np.ones([imgsize,imgsize])]\n                masks[0][1:] = maskn[:-1]\n                masks[1][:-1] = maskn[1:]\n                masks[2][:,1:] = maskn[:,:-1]\n                masks[3][:,:-1] = maskn[:,1:]\n                masks2 = [maskn-e for e in masks]\n                bound = np.minimum.reduce(masks2)\n                bound = -bound\n                xb = []\n                yb = []\n                for i in range(4):\n                    xbi = [center[i,0]-rws[i]/2, center[i,0]+rws[i]/2-1]\n                    ybi = [center[i,1]-rhs[i]/2, center[i,1]+rhs[i]/2-1]\n                    for j in range(2):\n                        maskx = bound[:,xbi[j]]\n                        masky = bound[ybi[j],:]\n                        xb += [(1-maskx)*10000 + maskx*xbi[j]]\n                        yb += [(1-masky)*10000 + masky*ybi[j]]\n                soft = 1-getSoft([imgsize,imgsize],xb,yb)\n                soft = torch.Tensor(soft).unsqueeze(0)\n                mask = (torch.ones(mask.shape)-mask)*soft + mask\n                \n            bgdir = self.opt.bg_dir\n            bgpath = os.path.join(bgdir,basen[:-4]+'.png')\n            im_bg = Image.open(bgpath)\n            mask2 = transforms.ToTensor()(im_bg) # mask out background\n            mask2 = (mask2 >= 0.5).float()\n            # hair_A = (A/2+0.5) * mask.repeat(input_nc/output_nc,1,1) * mask2.repeat(input_nc/output_nc,1,1) * 2 - 1\n            # bg_A = (A/2+0.5) * (torch.ones(mask2.shape)-mask2).repeat(input_nc/output_nc,1,1) * 2 - 1\n            hair_A = (A/2+0.5) * mask.repeat(3,1,1) * mask2.repeat(3,1,1) * 2 - 1\n            bg_A = (A/2+0.5) * (torch.ones(mask2.shape)-mask2).repeat(3,1,1) * 2 - 1\n            item['hair_A'] = hair_A\n            item['bg_A'] = bg_A\n            item['mask'] = mask\n            item['mask2'] = mask2\n\n        return item\n\n    def __len__(self):\n        return len(self.A_paths)\n\n    def name(self):\n        return 'SingleImageDataset'\n"""
models/__init__.py,0,"b'import importlib\nfrom models.base_model import BaseModel\n\n\ndef find_model_using_name(model_name):\n    # Given the option --model [modelname],\n    # the file ""models/modelname_model.py""\n    # will be imported.\n    model_filename = ""models."" + model_name + ""_model""\n    modellib = importlib.import_module(model_filename)\n\n    # In the file, the class called ModelNameModel() will\n    # be instantiated. It has to be a subclass of BaseModel,\n    # and it is case-insensitive.\n    model = None\n    target_model_name = model_name.replace(\'_\', \'\') + \'model\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n           and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        print(""In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase."" % (model_filename, target_model_name))\n        exit(0)\n\n    return model\n\n\ndef get_option_setter(model_name):\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt):\n    model = find_model_using_name(opt.model)\n    instance = model()\n    instance.initialize(opt)\n    print(""model [%s] was created"" % (instance.name()))\n    return instance\n'"
models/apdrawing_gan_model.py,9,"b""import torch\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\n\n\nclass APDrawingGANModel(BaseModel):\n    def name(self):\n        return 'APDrawingGANModel'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n\n        # changing the default values\n        parser.set_defaults(pool_size=0, no_lsgan=True, norm='batch')# no_lsgan=True, use_lsgan=False\n        parser.set_defaults(dataset_mode='aligned')\n\n        return parser\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        self.isTrain = opt.isTrain\n        # specify the training losses you want to print out. The program will call base_model.get_current_losses\n        self.loss_names = ['G_GAN', 'G_L1', 'D_real', 'D_fake']\n        if self.isTrain and self.opt.no_l1_loss:\n            self.loss_names = ['G_GAN', 'D_real', 'D_fake']\n        if self.isTrain and self.opt.use_local and not self.opt.no_G_local_loss:\n            self.loss_names.append('G_local')\n        if self.isTrain and self.opt.discriminator_local:\n            self.loss_names.append('D_real_local')\n            self.loss_names.append('D_fake_local')\n            self.loss_names.append('G_GAN_local')\n        if self.isTrain:\n            self.loss_names.append('G_chamfer')\n            self.loss_names.append('G_chamfer2')\n        self.loss_names.append('G')\n        print 'loss_names', self.loss_names\n        # specify the images you want to save/display. The program will call base_model.get_current_visuals\n        self.visual_names = ['real_A', 'fake_B', 'real_B']\n        if self.opt.use_local:\n            self.visual_names += ['fake_B0', 'fake_B1']\n            self.visual_names += ['fake_B_hair', 'real_B_hair', 'real_A_hair']\n            self.visual_names += ['fake_B_bg', 'real_B_bg', 'real_A_bg']\n        if self.isTrain:\n            self.visual_names += ['dt1', 'dt2', 'dt1gt', 'dt2gt']\n        if not self.isTrain and self.opt.save2:\n            self.visual_names = ['real_A', 'fake_B']\n        print 'visuals', self.visual_names\n        # specify the models you want to save to the disk. The program will call base_model.save_networks and base_model.load_networks\n        if self.isTrain:\n            self.model_names = ['G', 'D']\n            if self.opt.discriminator_local:\n                self.model_names += ['DLEyel','DLEyer','DLNose','DLMouth','DLHair','DLBG']\n            # auxiliary nets for loss calculation\n            self.auxiliary_model_names = ['DT1', 'DT2', 'Line1', 'Line2']\n        else:  # during test time, only load Gs\n            self.model_names = ['G']\n            self.auxiliary_model_names = []\n        if self.opt.use_local:\n            self.model_names += ['GLEyel','GLEyer','GLNose','GLMouth','GLHair','GLBG','GCombine']\n        print 'model_names', self.model_names\n        print 'auxiliary_model_names', self.auxiliary_model_names\n        # define networks (both generator and discriminator)\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids,\n                                      opt.nnG)\n        print 'netG', opt.netG\n\n        if self.isTrain:\n            # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n            use_sigmoid = opt.no_lsgan\n            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)\n            print 'netD', opt.netD, opt.n_layers_D\n            if self.opt.discriminator_local:\n                self.netDLEyel = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)\n                self.netDLEyer = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)\n                self.netDLNose = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)\n                self.netDLMouth = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)\n                self.netDLHair = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)\n                self.netDLBG = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, opt.init_gain, self.gpu_ids)\n                \n        \n        if self.opt.use_local:\n            self.netGLEyel = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 3)\n            self.netGLEyer = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 3)\n            self.netGLNose = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 3)\n            self.netGLMouth = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 3)\n            self.netGLHair = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet2', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 4)\n            self.netGLBG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet2', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 4)\n            self.netGCombine = networks.define_G(2*opt.output_nc, opt.output_nc, opt.ngf, 'combiner', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 2)\n        \n\n        if self.isTrain:\n            self.fake_AB_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan).to(self.device)\n            self.criterionL1 = torch.nn.L1Loss()\n\n            # initialize optimizers\n            self.optimizers = []\n            if not self.opt.use_local:\n                print('G_params 1 components')\n                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            else:\n                G_params = list(self.netG.parameters()) + list(self.netGLEyel.parameters()) + list(self.netGLEyer.parameters()) + list(self.netGLNose.parameters()) + list(self.netGLMouth.parameters()) + list(self.netGLHair.parameters()) + list(self.netGLBG.parameters()) + list(self.netGCombine.parameters()) \n                print('G_params 8 components')\n                self.optimizer_G = torch.optim.Adam(G_params,\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            if not self.opt.discriminator_local:\n                print('D_params 1 components')\n                self.optimizer_D = torch.optim.Adam(self.netD.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            else:\n                D_params = list(self.netD.parameters()) + list(self.netDLEyel.parameters()) +list(self.netDLEyer.parameters()) + list(self.netDLNose.parameters()) + list(self.netDLMouth.parameters()) + list(self.netDLHair.parameters()) + list(self.netDLBG.parameters())\n                print('D_params 7 components')\n                self.optimizer_D = torch.optim.Adam(D_params,\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer_G)\n            self.optimizers.append(self.optimizer_D)\n        \n        # ==================================auxiliary nets (loaded, parameters fixed)=============================\n        if self.isTrain:\n            self.nc = 1\n            self.netDT1 = networks.define_G(self.nc, self.nc, opt.ngf, opt.netG_dt, opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n            self.netDT2 = networks.define_G(self.nc, self.nc, opt.ngf, opt.netG_dt, opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n            self.set_requires_grad(self.netDT1, False)\n            self.set_requires_grad(self.netDT2, False)\n            \n            self.netLine1 = networks.define_G(self.nc, self.nc, opt.ngf, opt.netG_line, opt.norm,\n                                    not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n            self.netLine2 = networks.define_G(self.nc, self.nc, opt.ngf, opt.netG_line, opt.norm,\n                                    not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n            self.set_requires_grad(self.netLine1, False)\n            self.set_requires_grad(self.netLine2, False)\n        \n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == 'AtoB'\n        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n        if self.opt.use_local:\n            self.real_A_eyel = input['eyel_A'].to(self.device)\n            self.real_A_eyer = input['eyer_A'].to(self.device)\n            self.real_A_nose = input['nose_A'].to(self.device)\n            self.real_A_mouth = input['mouth_A'].to(self.device)\n            self.real_B_eyel = input['eyel_B'].to(self.device)\n            self.real_B_eyer = input['eyer_B'].to(self.device)\n            self.real_B_nose = input['nose_B'].to(self.device)\n            self.real_B_mouth = input['mouth_B'].to(self.device)\n            self.center = input['center']\n            self.real_A_hair = input['hair_A'].to(self.device)\n            self.real_B_hair = input['hair_B'].to(self.device)\n            self.real_A_bg = input['bg_A'].to(self.device)\n            self.real_B_bg = input['bg_B'].to(self.device)\n            self.mask = input['mask'].to(self.device) # mask for non-eyes,nose,mouth\n            self.mask2 = input['mask2'].to(self.device) # mask for non-bg\n        if self.isTrain:\n            self.dt1gt = input['dt1gt'].to(self.device)\n            self.dt2gt = input['dt2gt'].to(self.device)\n        \n\n    def forward(self):\n        if not self.opt.use_local:\n            self.fake_B = self.netG(self.real_A)\n        else:\n            self.fake_B0 = self.netG(self.real_A)\n            # EYES, NOSE, MOUTH\n            fake_B_eyel = self.netGLEyel(self.real_A_eyel)\n            fake_B_eyer = self.netGLEyer(self.real_A_eyer)\n            fake_B_nose = self.netGLNose(self.real_A_nose)\n            fake_B_mouth = self.netGLMouth(self.real_A_mouth)\n            self.fake_B_nose = fake_B_nose\n            self.fake_B_eyel = fake_B_eyel\n            self.fake_B_eyer = fake_B_eyer\n            self.fake_B_mouth = fake_B_mouth\n            \n            # HAIR, BG AND PARTCOMBINE\n            fake_B_hair = self.netGLHair(self.real_A_hair)\n            fake_B_bg = self.netGLBG(self.real_A_bg)\n            self.fake_B_hair = self.masked(fake_B_hair,self.mask*self.mask2)\n            self.fake_B_bg = self.masked(fake_B_bg,self.inverse_mask(self.mask2))\n            self.fake_B1 = self.partCombiner2_bg(fake_B_eyel,fake_B_eyer,fake_B_nose,fake_B_mouth,fake_B_hair,fake_B_bg,self.mask*self.mask2,self.inverse_mask(self.mask2),self.opt.comb_op)\n            \n            # FUSION NET\n            self.fake_B = self.netGCombine(torch.cat([self.fake_B0,self.fake_B1],1))\n\n    \n        \n    def backward_D(self):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1)) # we use conditional GANs; we need to feed both input and output to the discriminator\n        pred_fake = self.netD(fake_AB.detach())\n        self.loss_D_fake = self.criterionGAN(pred_fake, False)\n        if self.opt.discriminator_local:\n            fake_AB_parts = self.getLocalParts(fake_AB)\n            local_names = ['DLEyel','DLEyer','DLNose','DLMouth','DLHair','DLBG']\n            self.loss_D_fake_local = 0\n            for i in range(len(fake_AB_parts)):\n                net = getattr(self, 'net' + local_names[i])\n                pred_fake_tmp = net(fake_AB_parts[i].detach())\n                addw = self.getaddw(local_names[i])\n                self.loss_D_fake_local = self.loss_D_fake_local + self.criterionGAN(pred_fake_tmp, False) * addw\n            self.loss_D_fake = self.loss_D_fake + self.loss_D_fake_local\n\n        # Real\n        real_AB = torch.cat((self.real_A, self.real_B), 1)\n        pred_real = self.netD(real_AB)\n        self.loss_D_real = self.criterionGAN(pred_real, True)\n        if self.opt.discriminator_local:\n            real_AB_parts = self.getLocalParts(real_AB)\n            local_names = ['DLEyel','DLEyer','DLNose','DLMouth','DLHair','DLBG']\n            self.loss_D_real_local = 0\n            for i in range(len(real_AB_parts)):\n                net = getattr(self, 'net' + local_names[i])\n                pred_real_tmp = net(real_AB_parts[i])\n                addw = self.getaddw(local_names[i])\n                self.loss_D_real_local = self.loss_D_real_local + self.criterionGAN(pred_real_tmp, True) * addw\n            self.loss_D_real = self.loss_D_real + self.loss_D_real_local\n\n        # Combined loss\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n\n        self.loss_D.backward()\n\n    def backward_G(self):\n        # First, G(A) should fake the discriminator\n        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n        pred_fake = self.netD(fake_AB)\n        self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n        if self.opt.discriminator_local:\n            fake_AB_parts = self.getLocalParts(fake_AB)\n            local_names = ['DLEyel','DLEyer','DLNose','DLMouth','DLHair','DLBG']\n            self.loss_G_GAN_local = 0\n            for i in range(len(fake_AB_parts)):\n                net = getattr(self, 'net' + local_names[i])\n                pred_fake_tmp = net(fake_AB_parts[i])\n                addw = self.getaddw(local_names[i])\n                self.loss_G_GAN_local = self.loss_G_GAN_local + self.criterionGAN(pred_fake_tmp, True) * addw\n            if self.opt.gan_loss_strategy == 1:\n                self.loss_G_GAN = (self.loss_G_GAN + self.loss_G_GAN_local) / (len(fake_AB_parts) + 1)\n            elif self.opt.gan_loss_strategy == 2:\n                self.loss_G_GAN_local = self.loss_G_GAN_local * 0.25\n                self.loss_G_GAN = self.loss_G_GAN + self.loss_G_GAN_local\n\n        # Second, G(A) = B\n        if not self.opt.no_l1_loss:\n            self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n        \n        if self.opt.use_local and not self.opt.no_G_local_loss:\n            local_names = ['eyel','eyer','nose','mouth','hair','bg']\n            self.loss_G_local = 0\n            for i in range(len(local_names)):\n                fakeblocal = getattr(self, 'fake_B_' + local_names[i])\n                realblocal = getattr(self, 'real_B_' + local_names[i])\n                addw = self.getaddw(local_names[i])\n                self.loss_G_local = self.loss_G_local + self.criterionL1(fakeblocal,realblocal) * self.opt.lambda_local * addw\n\n        # Third, distance transform loss (chamfer matching)\n        if self.fake_B.shape[1] == 3:\n            tmp = self.fake_B[:,0,...]*0.299+self.fake_B[:,1,...]*0.587+self.fake_B[:,2,...]*0.114\n            fake_B_gray = tmp.unsqueeze(1)\n        else:\n            fake_B_gray = self.fake_B\n        if self.real_B.shape[1] == 3:\n            tmp = self.real_B[:,0,...]*0.299+self.real_B[:,1,...]*0.587+self.real_B[:,2,...]*0.114\n            real_B_gray = tmp.unsqueeze(1)\n        else:\n            real_B_gray = self.real_B\n        \n        # d_CM(a_i,G(p_i))\n        self.dt1 = self.netDT1(fake_B_gray)\n        self.dt2 = self.netDT2(fake_B_gray)\n        dt1 = self.dt1/2.0+0.5#[-1,1]->[0,1]\n        dt2 = self.dt2/2.0+0.5\n        \n        bs = real_B_gray.shape[0]\n        real_B_gray_line1 = self.netLine1(real_B_gray)\n        real_B_gray_line2 = self.netLine2(real_B_gray)\n        self.loss_G_chamfer = (dt1[(real_B_gray<0)&(real_B_gray_line1<0)].sum() + dt2[(real_B_gray>=0)&(real_B_gray_line2>=0)].sum()) / bs * self.opt.lambda_chamfer\n\n        # d_CM(G(p_i),a_i)\n        dt1gt = self.dt1gt\n        dt2gt = self.dt2gt\n        self.dt1gt = (self.dt1gt-0.5)*2\n        self.dt2gt = (self.dt2gt-0.5)*2\n\n        fake_B_gray_line1 = self.netLine1(fake_B_gray)\n        fake_B_gray_line2 = self.netLine2(fake_B_gray)\n        self.loss_G_chamfer2 = (dt1gt[(fake_B_gray<0)&(fake_B_gray_line1<0)].sum() + dt2gt[(fake_B_gray>=0)&(fake_B_gray_line2>=0)].sum()) / bs * self.opt.lambda_chamfer2\n                \n\n        self.loss_G = self.loss_G_GAN\n        if 'G_L1' in self.loss_names:\n            self.loss_G = self.loss_G + self.loss_G_L1\n        if 'G_local' in self.loss_names:\n            self.loss_G = self.loss_G + self.loss_G_local\n        if 'G_chamfer' in self.loss_names:\n            self.loss_G = self.loss_G + self.loss_G_chamfer\n        if 'G_chamfer2' in self.loss_names:\n            self.loss_G = self.loss_G + self.loss_G_chamfer2\n\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        self.forward()\n        # update D\n        self.set_requires_grad(self.netD, True) # enable backprop for D\n        if self.opt.discriminator_local:\n            self.set_requires_grad(self.netDLEyel, True)\n            self.set_requires_grad(self.netDLEyer, True)\n            self.set_requires_grad(self.netDLNose, True)\n            self.set_requires_grad(self.netDLMouth, True)\n            self.set_requires_grad(self.netDLHair, True)\n            self.set_requires_grad(self.netDLBG, True)\n        self.optimizer_D.zero_grad()\n        self.backward_D()\n        self.optimizer_D.step()\n\n        # update G\n        self.set_requires_grad(self.netD, False) # D requires no gradients when optimizing G\n        if self.opt.discriminator_local:\n            self.set_requires_grad(self.netDLEyel, False)\n            self.set_requires_grad(self.netDLEyer, False)\n            self.set_requires_grad(self.netDLNose, False)\n            self.set_requires_grad(self.netDLMouth, False)\n            self.set_requires_grad(self.netDLHair, False)\n            self.set_requires_grad(self.netDLBG, False)\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n"""
models/base_model.py,61,"b""import os\nimport torch\nfrom collections import OrderedDict\nfrom . import networks\n\n\nclass BaseModel():\n\n    # modify parser to add command line options,\n    # and also change the default values if needed\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        self.auxiliary_dir = os.path.join(opt.checkpoints_dir, opt.auxiliary_root)\n        if opt.resize_or_crop != 'scale_width':\n            torch.backends.cudnn.benchmark = True\n        self.loss_names = []\n        self.model_names = []\n        self.visual_names = []\n        self.image_paths = []\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # load and print networks; create schedulers\n    def setup(self, opt, parser=None):\n        if self.isTrain:\n            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n\n        if not self.isTrain or opt.continue_train:\n            self.load_networks(opt.which_epoch)\n        if self.isTrain:\n            self.load_auxiliary_networks()\n        self.print_networks(opt.verbose)\n\n    # make models eval mode during test time\n    def eval(self):\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                net.eval()\n\n    # used in test time, wrapping `forward` in no_grad() so we don't save\n    # intermediate steps for backprop\n    def test(self):\n        with torch.no_grad():\n            self.forward()\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def optimize_parameters(self):\n        pass\n\n    # update learning rate (called once every epoch)\n    def update_learning_rate(self):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        lr = self.optimizers[0].param_groups[0]['lr']\n        print('learning rate = %.7f' % lr)\n\n    # return visualization images. train.py will display these images, and save the images to a html\n    def get_current_visuals(self):\n        visual_ret = OrderedDict()\n        for name in self.visual_names:\n            if isinstance(name, str):\n                visual_ret[name] = getattr(self, name)\n        return visual_ret\n\n    # return traning losses/errors. train.py will print out these errors as debugging information\n    def get_current_losses(self):\n        errors_ret = OrderedDict()\n        for name in self.loss_names:\n            if isinstance(name, str):\n                # float(...) works for both scalar tensor and float number\n                errors_ret[name] = float(getattr(self, 'loss_' + name))\n        return errors_ret\n\n    # save models to the disk\n    def save_networks(self, which_epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                save_filename = '%s_net_%s.pth' % (which_epoch, name)\n                save_path = os.path.join(self.save_dir, save_filename)\n                net = getattr(self, 'net' + name)\n\n                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                    torch.save(net.module.cpu().state_dict(), save_path)\n                    net.cuda(self.gpu_ids[0])\n                else:\n                    torch.save(net.cpu().state_dict(), save_path)\n    \n    # save generators to one file and discriminators to another file\n    def save_networks2(self, which_epoch):\n        gen_name = os.path.join(self.save_dir, '%s_net_gen.pt' % (which_epoch))\n        dis_name = os.path.join(self.save_dir, '%s_net_dis.pt' % (which_epoch))\n        dict_gen = {}\n        dict_dis = {}\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n\n                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                    state_dict = net.module.cpu().state_dict()\n                    net.cuda(self.gpu_ids[0])\n                else:\n                    state_dict = net.cpu().state_dict()\n                \n                if name[0] == 'G':\n                    dict_gen[name] = state_dict\n                elif name[0] == 'D':\n                    dict_dis[name] = state_dict\n        torch.save(dict_gen, gen_name)\n        torch.save(dict_dis, dis_name)\n\n    def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0):\n        key = keys[i]\n        if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer\n            if module.__class__.__name__.startswith('InstanceNorm') and \\\n                    (key == 'running_mean' or key == 'running_var'):\n                if getattr(module, key) is None:\n                    state_dict.pop('.'.join(keys))\n            if module.__class__.__name__.startswith('InstanceNorm') and \\\n               (key == 'num_batches_tracked'):\n                state_dict.pop('.'.join(keys))\n        else:\n            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)\n\n    # load models from the disk\n    def load_networks(self, which_epoch):\n        gen_name = os.path.join(self.save_dir, '%s_net_gen.pt' % (which_epoch))\n        if os.path.exists(gen_name):\n            self.load_networks2(which_epoch)\n            return\n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % (which_epoch, name)\n                load_path = os.path.join(self.save_dir, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                # if you are using PyTorch newer than 0.4 (e.g., built from\n                # GitHub source), you can remove str() on self.device\n                state_dict = torch.load(load_path, map_location=str(self.device))\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                # patch InstanceNorm checkpoints prior to 0.4\n                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n                net.load_state_dict(state_dict)\n    \n    def load_networks2(self, which_epoch):\n        gen_name = os.path.join(self.save_dir, '%s_net_gen.pt' % (which_epoch))\n        gen_state_dict = torch.load(gen_name, map_location=str(self.device))\n        if self.isTrain:\n            dis_name = os.path.join(self.save_dir, '%s_net_dis.pt' % (which_epoch))\n            dis_state_dict = torch.load(dis_name, map_location=str(self.device))\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                if name[0] == 'G':\n                    print('loading the model from %s' % gen_name)\n                    state_dict = gen_state_dict[name]\n                elif name[0] == 'D':\n                    print('loading the model from %s' % dis_name)\n                    state_dict = dis_state_dict[name]\n                \n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n                # patch InstanceNorm checkpoints prior to 0.4\n                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n                net.load_state_dict(state_dict)\n    \n    # load auxiliary net models from the disk\n    def load_auxiliary_networks(self):\n        for name in self.auxiliary_model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % ('latest', name)\n                load_path = os.path.join(self.auxiliary_dir, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                # if you are using PyTorch newer than 0.4 (e.g., built from\n                # GitHub source), you can remove str() on self.device\n                state_dict = torch.load(load_path, map_location=str(self.device))\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                # patch InstanceNorm checkpoints prior to 0.4\n                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n                net.load_state_dict(state_dict)\n\n    # print network information\n    def print_networks(self, verbose):\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                if verbose:\n                    print(net)\n                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    # set requies_grad=Fasle to avoid computation\n    def set_requires_grad(self, nets, requires_grad=False):\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n\n    # =============================================================================================================\n    def inverse_mask(self, mask):\n        return torch.ones(mask.shape).to(self.device)-mask\n    \n    def masked(self, A,mask):\n        return (A/2+0.5)*mask*2-1\n    \n    def add_with_mask(self, A,B,mask):\n        return ((A/2+0.5)*mask+(B/2+0.5)*(torch.ones(mask.shape).to(self.device)-mask))*2-1\n    \n    def addone_with_mask(self, A,mask):\n        return ((A/2+0.5)*mask+(torch.ones(mask.shape).to(self.device)-mask))*2-1\n    \n    def partCombiner2(self, eyel, eyer, nose, mouth, hair, mask, comb_op = 1):\n        if comb_op == 0:\n            # use max pooling, pad black for eyes etc\n            padvalue = -1\n            hair = self.masked(hair, mask)\n        else:\n            # use min pooling, pad white for eyes etc\n            padvalue = 1\n            hair = self.addone_with_mask(hair, mask)\n        IMAGE_SIZE = self.opt.fineSize\n        ratio = IMAGE_SIZE / 256\n        EYE_W = self.opt.EYE_W * ratio\n        EYE_H = self.opt.EYE_H * ratio\n        NOSE_W = self.opt.NOSE_W * ratio\n        NOSE_H = self.opt.NOSE_H * ratio\n        MOUTH_W = self.opt.MOUTH_W * ratio\n        MOUTH_H = self.opt.MOUTH_H * ratio\n        bs,nc,_,_ = eyel.shape\n        eyel_p = torch.ones((bs,nc,IMAGE_SIZE,IMAGE_SIZE)).to(self.device)\n        eyer_p = torch.ones((bs,nc,IMAGE_SIZE,IMAGE_SIZE)).to(self.device)\n        nose_p = torch.ones((bs,nc,IMAGE_SIZE,IMAGE_SIZE)).to(self.device)\n        mouth_p = torch.ones((bs,nc,IMAGE_SIZE,IMAGE_SIZE)).to(self.device)\n        for i in range(bs):\n            center = self.center[i]#x,y\n            eyel_p[i] = torch.nn.ConstantPad2d((center[0,0] - EYE_W / 2, IMAGE_SIZE - (center[0,0]+EYE_W/2), center[0,1] - EYE_H / 2, IMAGE_SIZE - (center[0,1]+EYE_H/2)),padvalue)(eyel[i])\n            eyer_p[i] = torch.nn.ConstantPad2d((center[1,0] - EYE_W / 2, IMAGE_SIZE - (center[1,0]+EYE_W/2), center[1,1] - EYE_H / 2, IMAGE_SIZE - (center[1,1]+EYE_H/2)),padvalue)(eyer[i])\n            nose_p[i] = torch.nn.ConstantPad2d((center[2,0] - NOSE_W / 2, IMAGE_SIZE - (center[2,0]+NOSE_W/2), center[2,1] - NOSE_H / 2, IMAGE_SIZE - (center[2,1]+NOSE_H/2)),padvalue)(nose[i])\n            mouth_p[i] = torch.nn.ConstantPad2d((center[3,0] - MOUTH_W / 2, IMAGE_SIZE - (center[3,0]+MOUTH_W/2), center[3,1] - MOUTH_H / 2, IMAGE_SIZE - (center[3,1]+MOUTH_H/2)),padvalue)(mouth[i])\n        if comb_op == 0:\n            # use max pooling\n            eyes = torch.max(eyel_p, eyer_p)\n            eye_nose = torch.max(eyes, nose_p)\n            eye_nose_mouth = torch.max(eye_nose, mouth_p)\n            result = torch.max(hair,eye_nose_mouth)\n        else:\n            # use min pooling\n            eyes = torch.min(eyel_p, eyer_p)\n            eye_nose = torch.min(eyes, nose_p)\n            eye_nose_mouth = torch.min(eye_nose, mouth_p)\n            result = torch.min(hair,eye_nose_mouth)\n        return result\n    \n    def partCombiner2_bg(self, eyel, eyer, nose, mouth, hair, bg, maskh, maskb, comb_op = 1):\n        if comb_op == 0:\n            # use max pooling, pad black for eyes etc\n            padvalue = -1\n            hair = self.masked(hair, maskh)\n            bg = self.masked(bg, maskb)\n        else:\n            # use min pooling, pad white for eyes etc\n            padvalue = 1\n            hair = self.addone_with_mask(hair, maskh)\n            bg = self.addone_with_mask(bg, maskb)\n        IMAGE_SIZE = self.opt.fineSize\n        ratio = IMAGE_SIZE / 256\n        EYE_W = self.opt.EYE_W * ratio\n        EYE_H = self.opt.EYE_H * ratio\n        NOSE_W = self.opt.NOSE_W * ratio\n        NOSE_H = self.opt.NOSE_H * ratio\n        MOUTH_W = self.opt.MOUTH_W * ratio\n        MOUTH_H = self.opt.MOUTH_H * ratio\n        bs,nc,_,_ = eyel.shape\n        eyel_p = torch.ones((bs,nc,IMAGE_SIZE,IMAGE_SIZE)).to(self.device)\n        eyer_p = torch.ones((bs,nc,IMAGE_SIZE,IMAGE_SIZE)).to(self.device)\n        nose_p = torch.ones((bs,nc,IMAGE_SIZE,IMAGE_SIZE)).to(self.device)\n        mouth_p = torch.ones((bs,nc,IMAGE_SIZE,IMAGE_SIZE)).to(self.device)\n        for i in range(bs):\n            center = self.center[i]#x,y\n            eyel_p[i] = torch.nn.ConstantPad2d((int(center[0,0] - EYE_W / 2), int(IMAGE_SIZE - (center[0,0]+EYE_W/2)), int(center[0,1] - EYE_H / 2), int(IMAGE_SIZE - (center[0,1]+EYE_H/2))),padvalue)(eyel[i])\n            eyer_p[i] = torch.nn.ConstantPad2d((int(center[1,0] - EYE_W / 2), int(IMAGE_SIZE - (center[1,0]+EYE_W/2)), int(center[1,1] - EYE_H / 2), int(IMAGE_SIZE - (center[1,1]+EYE_H/2))), padvalue)(eyer[i])\n            nose_p[i] = torch.nn.ConstantPad2d((int(center[2,0] - NOSE_W / 2), int(IMAGE_SIZE - (center[2,0]+NOSE_W/2)), int(center[2,1] - NOSE_H / 2), int(IMAGE_SIZE - (center[2,1]+NOSE_H/2))),padvalue)(nose[i])\n            mouth_p[i] = torch.nn.ConstantPad2d((int(center[3,0] - MOUTH_W / 2), int(IMAGE_SIZE - (center[3,0]+MOUTH_W/2)), int(center[3,1] - MOUTH_H / 2), int(IMAGE_SIZE - (center[3,1]+MOUTH_H/2))),padvalue)(mouth[i])\n        if comb_op == 0:\n            eyes = torch.max(eyel_p, eyer_p)\n            eye_nose = torch.max(eyes, nose_p)\n            eye_nose_mouth = torch.max(eye_nose, mouth_p)\n            eye_nose_mouth_hair = torch.max(hair,eye_nose_mouth)\n            result = torch.max(bg,eye_nose_mouth_hair)\n        else:\n            eyes = torch.min(eyel_p, eyer_p)\n            eye_nose = torch.min(eyes, nose_p)\n            eye_nose_mouth = torch.min(eye_nose, mouth_p)\n            eye_nose_mouth_hair = torch.min(hair,eye_nose_mouth)\n            result = torch.min(bg,eye_nose_mouth_hair)\n        return result\n    \n    def partCombiner3(self, face, hair, maskf, maskh, comb_op = 1):\n        if comb_op == 0:\n            # use max pooling, pad black etc\n            padvalue = -1\n            face = self.masked(face, maskf)\n            hair = self.masked(hair, maskh)\n        else:\n            # use min pooling, pad white etc\n            padvalue = 1\n            face = self.addone_with_mask(face, maskf)\n            hair = self.addone_with_mask(hair, maskh)\n        if comb_op == 0:\n            result = torch.max(face,hair)\n        else:\n            result = torch.min(face,hair)\n        return result\n    \n    def getLocalParts(self,fakeAB):\n        bs,nc,_,_ = fakeAB.shape #dtype torch.float32\n        ncr = nc / self.opt.output_nc\n        ratio = self.opt.fineSize / 256\n        EYE_H = self.opt.EYE_H * ratio\n        EYE_W = self.opt.EYE_W * ratio\n        NOSE_H = self.opt.NOSE_H * ratio\n        NOSE_W = self.opt.NOSE_W * ratio\n        MOUTH_H = self.opt.MOUTH_H * ratio\n        MOUTH_W = self.opt.MOUTH_W * ratio\n        eyel = torch.ones((bs,nc,EYE_H,EYE_W)).to(self.device)\n        eyer = torch.ones((bs,nc,EYE_H,EYE_W)).to(self.device)\n        nose = torch.ones((bs,nc,NOSE_H,NOSE_W)).to(self.device)\n        mouth = torch.ones((bs,nc,MOUTH_H,MOUTH_W)).to(self.device)\n        for i in range(bs):\n            center = self.center[i]\n            eyel[i] = fakeAB[i,:,center[0,1]-EYE_H/2:center[0,1]+EYE_H/2,center[0,0]-EYE_W/2:center[0,0]+EYE_W/2]\n            eyer[i] = fakeAB[i,:,center[1,1]-EYE_H/2:center[1,1]+EYE_H/2,center[1,0]-EYE_W/2:center[1,0]+EYE_W/2]\n            nose[i] = fakeAB[i,:,center[2,1]-NOSE_H/2:center[2,1]+NOSE_H/2,center[2,0]-NOSE_W/2:center[2,0]+NOSE_W/2]\n            mouth[i] = fakeAB[i,:,center[3,1]-MOUTH_H/2:center[3,1]+MOUTH_H/2,center[3,0]-MOUTH_W/2:center[3,0]+MOUTH_W/2]\n        hair = (fakeAB/2+0.5) * self.mask.repeat(1,ncr,1,1) * self.mask2.repeat(1,ncr,1,1) * 2 - 1\n        bg = (fakeAB/2+0.5) * (torch.ones(fakeAB.shape).to(self.device)-self.mask2.repeat(1,ncr,1,1)) * 2 - 1\n        return eyel, eyer, nose, mouth, hair, bg\n    \n    def getaddw(self,local_name):\n        addw = 1\n        if local_name in ['DLEyel','DLEyer','eyel','eyer']:\n            addw = self.opt.addw_eye\n        elif local_name in ['DLNose', 'nose']:\n            addw = self.opt.addw_nose\n        elif local_name in ['DLMouth', 'mouth']:\n            addw = self.opt.addw_mouth\n        elif local_name in ['DLHair', 'hair']:\n            addw = self.opt.addw_hair\n        elif local_name in ['DLBG', 'bg']:\n            addw = self.opt.addw_bg\n        return addw"""
models/networks.py,8,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\n\n###############################################################################\n# Helper Functions\n###############################################################################\n\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n    elif norm_type == 'none':\n        norm_layer = None\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == 'lambda':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == 'plateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n    elif opt.lr_policy == 'cosine':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\n    else:\n        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n    return scheduler\n\n\ndef init_weights(net, init_type='normal', gain=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            if init_type == 'normal':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == 'xavier':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == 'kaiming':\n                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            elif init_type == 'orthogonal':\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print('initialize network with %s' % init_type)\n    net.apply(init_func)\n\n\ndef init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[]):\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)\n    init_weights(net, init_type, gain=init_gain)\n    return net\n\n\ndef define_G(input_nc, output_nc, ngf, netG, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[], nnG=9):\n    net = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if netG == 'resnet_9blocks':\n        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n    elif netG == 'resnet_6blocks':\n        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n    elif netG == 'resnet_nblocks':\n        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=nnG)\n    elif netG == 'unet_128':\n        net = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif netG == 'unet_256':#default for pix2pix\n        net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif netG == 'unet_512':\n        net = UnetGenerator(input_nc, output_nc, 9, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif netG == 'unet_ndown':\n        net = UnetGenerator(input_nc, output_nc, nnG, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif netG == 'partunet':\n        net = PartUnet(input_nc, output_nc, nnG, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif netG == 'partunet2':\n        net = PartUnet2(input_nc, output_nc, nnG, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif netG == 'combiner':\n        net = Combiner(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=2)\n    else:\n        raise NotImplementedError('Generator model name [%s] is not recognized' % netG)\n    return init_net(net, init_type, init_gain, gpu_ids)\n\n\ndef define_D(input_nc, ndf, netD,\n             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', init_gain=0.02, gpu_ids=[]):\n    net = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if netD == 'basic':\n        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif netD == 'n_layers':\n        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif netD == 'pixel':\n        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    else:\n        raise NotImplementedError('Discriminator model name [%s] is not recognized' % net)\n    return init_net(net, init_type, init_gain, gpu_ids)\n\n\n##############################################################################\n# Classes\n##############################################################################\n\n\n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n        super(GANLoss, self).__init__()\n        self.register_buffer('real_label', torch.tensor(target_real_label))\n        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:#no_lsgan\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(input)\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson's architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                        kernel_size=3, stride=2,\n                                        padding=1, output_padding=1,\n                                        bias=use_bias),\n                    norm_layer(int(ngf * mult / 2)),\n                    nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        return self.model(input)\n\nclass Combiner(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(Combiner, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        return self.model(input)\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass UnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetGenerator, self).__init__()\n\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n\n        self.model = unet_block\n\n    def forward(self, input):\n        return self.model(input)\n\nclass PartUnet(nn.Module):\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(PartUnet, self).__init__()\n\n        # construct unet structure\n        # 3 downs \n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n\n        self.model = unet_block\n\n    def forward(self, input):\n        return self.model(input)\n\nclass PartUnet2(nn.Module):\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(PartUnet2, self).__init__()\n\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 2, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n        for i in range(num_downs - 3):\n            unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n\n        self.model = unet_block\n\n    def forward(self, input):\n        return self.model(input)\n\n\n# Defines the submodule with skip connection.\n# X -------------------identity---------------------- X\n#   |-- downsampling -- |submodule| -- upsampling --|\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc, input_nc=None,\n                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1, bias=use_bias)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([x, self.model(x)], 1)\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:#no_lsgan, use sigmoid before calculating bceloss(binary cross entropy)\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass PixelDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(PixelDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        self.net = [\n            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n            norm_layer(ndf * 2),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n\n        if use_sigmoid:\n            self.net.append(nn.Sigmoid())\n\n        self.net = nn.Sequential(*self.net)\n\n    def forward(self, input):\n        return self.net(input)\n"""
models/test_model.py,1,"b""from .base_model import BaseModel\nfrom . import networks\nimport torch\n\n\nclass TestModel(BaseModel):\n    def name(self):\n        return 'TestModel'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        assert not is_train, 'TestModel cannot be used in train mode'\n         \n        parser.set_defaults(dataset_mode='single')\n\n        return parser\n\n    def initialize(self, opt):\n        assert(not opt.isTrain)\n        BaseModel.initialize(self, opt)\n\n        # specify the training losses you want to print out. The program will call base_model.get_current_losses\n        self.loss_names = []\n        # specify the images you want to save/display. The program will call base_model.get_current_visuals\n        self.visual_names = ['real_A', 'fake_B']\n        # specify the models you want to save to the disk. The program will call base_model.save_networks and base_model.load_networks\n        self.model_names = ['G']\n        self.auxiliary_model_names = []\n        if self.opt.use_local:\n            self.model_names += ['GLEyel','GLEyer','GLNose','GLMouth','GLHair','GLBG','GCombine']\n\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids,\n                                      opt.nnG)\n        if self.opt.use_local:\n            self.netGLEyel = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 3)\n            self.netGLEyer = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 3)\n            self.netGLNose = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 3)\n            self.netGLMouth = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 3)\n            self.netGLHair = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet2', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 4)\n            self.netGLBG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, 'partunet2', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 4)\n            self.netGCombine = networks.define_G(2*opt.output_nc, opt.output_nc, opt.ngf, 'combiner', opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids, 2)\n        \n\n    def set_input(self, input):\n        # we need to use single_dataset mode\n        self.real_A = input['A'].to(self.device)\n        self.image_paths = input['A_paths']\n        if self.opt.use_local:\n            self.real_A_eyel = input['eyel_A'].to(self.device)\n            self.real_A_eyer = input['eyer_A'].to(self.device)\n            self.real_A_nose = input['nose_A'].to(self.device)\n            self.real_A_mouth = input['mouth_A'].to(self.device)\n            self.center = input['center']\n            self.real_A_hair = input['hair_A'].to(self.device)\n            self.real_A_bg = input['bg_A'].to(self.device)\n            self.mask = input['mask'].to(self.device)\n            self.mask2 = input['mask2'].to(self.device)\n\n    def forward(self):\n        if not self.opt.use_local:\n            self.fake_B = self.netG(self.real_A)\n        else:\n            self.fake_B0 = self.netG(self.real_A)\n            # EYES, NOSE, MOUTH\n            fake_B_eyel = self.netGLEyel(self.real_A_eyel)\n            fake_B_eyer = self.netGLEyer(self.real_A_eyer)\n            fake_B_nose = self.netGLNose(self.real_A_nose)\n            fake_B_mouth = self.netGLMouth(self.real_A_mouth)\n            \n            # HAIR, BG AND PARTCOMBINE\n            fake_B_hair = self.netGLHair(self.real_A_hair)\n            fake_B_bg = self.netGLBG(self.real_A_bg)\n            self.fake_B_hair = self.masked(fake_B_hair,self.mask*self.mask2)\n            self.fake_B_bg = self.masked(fake_B_bg,self.inverse_mask(self.mask2))\n            self.fake_B1 = self.partCombiner2_bg(fake_B_eyel,fake_B_eyer,fake_B_nose,fake_B_mouth,fake_B_hair,fake_B_bg,self.mask*self.mask2,self.inverse_mask(self.mask2),self.opt.comb_op)\n            \n            # FUSION NET\n            self.fake_B = self.netGCombine(torch.cat([self.fake_B0,self.fake_B1],1))\n"""
options/__init__.py,0,b''
options/base_options.py,1,"b'import argparse\nimport os\nfrom util import util\nimport torch\nimport models\nimport data\n\n\nclass BaseOptions():\n    def __init__(self):\n        self.initialized = False\n\n    def initialize(self, parser):\n        parser.add_argument(\'--dataroot\', required=True, help=\'path to images (should have subfolders train, test etc)\')\n        parser.add_argument(\'--batch_size\', type=int, default=1, help=\'input batch size\')\n        parser.add_argument(\'--loadSize\', type=int, default=512, help=\'scale images to this size\')\n        parser.add_argument(\'--fineSize\', type=int, default=512, help=\'then crop to this size\')\n        parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n        parser.add_argument(\'--output_nc\', type=int, default=1, help=\'# of output image channels\')\n        parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n        parser.add_argument(\'--netD\', type=str, default=\'basic\', help=\'selects model to use for netD\')\n        parser.add_argument(\'--netG\', type=str, default=\'unet_256\', help=\'selects model to use for netG\')\n        parser.add_argument(\'--nnG\', type=int, default=9, help=\'specify nblock for resnet_nblocks, ndown for unet for unet_ndown\')\n        parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if netD==n_layers\')\n        parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        parser.add_argument(\'--name\', type=str, default=\'experiment_name\', help=\'name of the experiment. It decides where to store samples and models\')\n        parser.add_argument(\'--dataset_mode\', type=str, default=\'aligned\', help=\'chooses how datasets are loaded. [aligned | single]\')\n        parser.add_argument(\'--model\', type=str, default=\'apdrawing_gan\',\n                            help=\'chooses which model to use. [apdrawing_gan | test]\')\n        parser.add_argument(\'--use_local\', action=\'store_true\', help=\'use local part network\')\n        parser.add_argument(\'--comb_op\', type=int, default=1, help=\'use min-pooling(1) or max-pooling(0) for overlapping regions\')\n        parser.add_argument(\'--lm_dir\', type=str, default=\'dataset/landmark/ALL\', help=\'path to facial landmarks\')\n        parser.add_argument(\'--bg_dir\', type=str, default=\'dataset/mask/ALL\', help=\'path to background masks\')\n        parser.add_argument(\'--soft_border\', type=int, default=0, help=\'use mask with soft border\')\n        parser.add_argument(\'--EYE_H\', type=int, default=40, help=\'EYE_H\')\n        parser.add_argument(\'--EYE_W\', type=int, default=56, help=\'EYE_W\')\n        parser.add_argument(\'--NOSE_H\', type=int, default=48, help=\'NOSE_H\')\n        parser.add_argument(\'--NOSE_W\', type=int, default=48, help=\'NOSE_W\')\n        parser.add_argument(\'--MOUTH_H\', type=int, default=40, help=\'MOUTH_H\')\n        parser.add_argument(\'--MOUTH_W\', type=int, default=64, help=\'MOUTH_W\')\n        parser.add_argument(\'--which_direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n        parser.add_argument(\'--num_threads\', default=4, type=int, help=\'# threads for loading data\')\n        parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        parser.add_argument(\'--auxiliary_root\', type=str, default=\'auxiliary\', help=\'auxiliary model folder\')\n        parser.add_argument(\'--norm\', type=str, default=\'batch\', help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        parser.add_argument(\'--display_winsize\', type=int, default=256, help=\'display window size\')\n        parser.add_argument(\'--display_id\', type=int, default=1, help=\'window id of the web display\')\n        parser.add_argument(\'--display_server\', type=str, default=""http://localhost"", help=\'visdom server of the web display\')\n        parser.add_argument(\'--display_env\', type=str, default=\'main\', help=\'visdom display environment name (default is ""main"")\')\n        parser.add_argument(\'--display_port\', type=int, default=8097, help=\'visdom port of the web display\')\n        parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout for the generator\')\n        parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        parser.add_argument(\'--resize_or_crop\', type=str, default=\'resize_and_crop\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data augmentation\')\n        parser.add_argument(\'--init_type\', type=str, default=\'normal\', help=\'network initialization [normal|xavier|kaiming|orthogonal]\')\n        parser.add_argument(\'--init_gain\', type=float, default=0.02, help=\'scaling factor for normal, xavier and orthogonal.\')\n        parser.add_argument(\'--verbose\', action=\'store_true\', help=\'if specified, print more debugging information\')\n        parser.add_argument(\'--suffix\', default=\'\', type=str, help=\'customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{loadSize}\')\n        self.initialized = True\n        return parser\n\n    def gather_options(self):\n        # initialize parser with basic options\n        if not self.initialized:\n            parser = argparse.ArgumentParser(\n                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n            parser = self.initialize(parser)\n\n        # get the basic options\n        opt, _ = parser.parse_known_args()\n\n        # modify model-related parser options\n        model_name = opt.model\n        model_option_setter = models.get_option_setter(model_name)\n        parser = model_option_setter(parser, self.isTrain)\n        opt, _ = parser.parse_known_args()  # parse again with the new defaults\n\n        # modify dataset-related parser options\n        dataset_name = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_name)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        self.parser = parser\n\n        return parser.parse_args()\n\n    def print_options(self, opt):\n        message = \'\'\n        message += \'----------------- Options ---------------\\n\'\n        for k, v in sorted(vars(opt).items()):\n            comment = \'\'\n            default = self.parser.get_default(k)\n            if v != default:\n                comment = \'\\t[default: %s]\' % str(default)\n            message += \'{:>25}: {:<30}{}\\n\'.format(str(k), str(v), comment)\n        message += \'----------------- End -------------------\'\n        print(message)\n\n        # save to the disk\n        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt.txt\')\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(message)\n            opt_file.write(\'\\n\')\n\n    def parse(self):\n\n        opt = self.gather_options()\n        if opt.use_local:\n            opt.loadSize = opt.fineSize\n        opt.isTrain = self.isTrain   # train or test\n\n        # process opt.suffix\n        if opt.suffix:\n            suffix = (\'_\' + opt.suffix.format(**vars(opt))) if opt.suffix != \'\' else \'\'\n            opt.name = opt.name + suffix\n\n        self.print_options(opt)\n\n        # set gpu ids\n        str_ids = opt.gpu_ids.split(\',\')\n        opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                opt.gpu_ids.append(id)\n        if len(opt.gpu_ids) > 0:\n            torch.cuda.set_device(opt.gpu_ids[0])\n\n        self.opt = opt\n        return self.opt\n'"
options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        parser.add_argument(\'--how_many\', type=int, default=70, help=\'how many test images to run\')\n        parser.add_argument(\'--save2\', action=\'store_true\', help=\'only save real_A and fake_B\')\n\n        # To avoid cropping, the loadSize should be the same as fineSize\n        parser.set_defaults(loadSize=parser.get_default(\'fineSize\'))\n        self.isTrain = False\n        return parser\n'"
options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        parser.add_argument('--display_freq', type=int, default=400, help='frequency of showing training results on screen')\n        parser.add_argument('--display_ncols', type=int, default=4, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n        parser.add_argument('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n        parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n        parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n        parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau|cosine')\n        parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n        # ============================================loss=========================================================\n        # L1 and local\n        parser.add_argument('--lambda_L1', type=float, default=100.0, help='weight for L1 loss')\n        parser.add_argument('--lambda_local', type=float, default=25.0, help='weight for Local loss')\n        # chamfer loss\n        parser.add_argument('--lambda_chamfer', type=float, default=0.1, help='weight for chamfer loss')\n        parser.add_argument('--lambda_chamfer2', type=float, default=0.1, help='weight for chamfer loss2')\n        # =====================================auxilary net structure===============================================\n        # dt & line net structure\n        parser.add_argument('--netG_dt', type=str, default='unet_512', help='selects model to use for netG_dt, for chamfer loss')\n        parser.add_argument('--netG_line', type=str, default='unet_512', help='selects model to use for netG_line, for chamfer loss')\n        # multiple discriminators\n        parser.add_argument('--discriminator_local', action='store_true', help='use six diffent local discriminator for 6 local regions')\n        parser.add_argument('--gan_loss_strategy', type=int, default=2, help='specify how to calculate gan loss for g, 1: average global and local discriminators; 2: not change global discriminator weight, 0.25 for local')\n        parser.add_argument('--addw_eye', type=float, default=1.0, help='additional weight for eye region')\n        parser.add_argument('--addw_nose', type=float, default=1.0, help='additional weight for nose region')\n        parser.add_argument('--addw_mouth', type=float, default=1.0, help='additional weight for mouth region')\n        parser.add_argument('--addw_hair', type=float, default=1.0, help='additional weight for hair region')\n        parser.add_argument('--addw_bg', type=float, default=1.0, help='additional weight for bg region')\n        # ==========================================ablation========================================================\n        parser.add_argument('--no_l1_loss', action='store_true', help='no l1 loss')\n        parser.add_argument('--no_G_local_loss', action='store_true', help='not using local transfer loss for local generator output')\n\n        self.isTrain = True\n        return parser\n"""
preprocess/combine_A_and_B.py,0,"b""import os\nimport numpy as np\nimport cv2\nimport argparse\n\nparser = argparse.ArgumentParser('create image pairs')\nparser.add_argument('--fold_A', dest='fold_A', help='input directory for image A', type=str, default='../dataset/50kshoes_edges')\nparser.add_argument('--fold_B', dest='fold_B', help='input directory for image B', type=str, default='../dataset/50kshoes_jpg')\nparser.add_argument('--fold_AB', dest='fold_AB', help='output directory', type=str, default='../dataset/test_AB')\nparser.add_argument('--num_imgs', dest='num_imgs', help='number of images',type=int, default=1000000)\nparser.add_argument('--use_AB', dest='use_AB', help='if true: (0001_A, 0001_B) to (0001_AB)',action='store_true')\nargs = parser.parse_args()\n\nfor arg in vars(args):\n    print('[%s] = ' % arg,  getattr(args, arg))\n\nsplits = os.listdir(args.fold_A)\n\nfor sp in splits:\n    img_fold_A = os.path.join(args.fold_A, sp)\n    img_fold_B = os.path.join(args.fold_B, sp)\n    img_list = os.listdir(img_fold_A)\n    if args.use_AB:\n        img_list = [img_path for img_path in img_list if '_A.' in img_path]\n\n    num_imgs = min(args.num_imgs, len(img_list))\n    print('split = %s, use %d/%d images' % (sp, num_imgs, len(img_list)))\n    img_fold_AB = os.path.join(args.fold_AB, sp)\n    if not os.path.isdir(img_fold_AB):\n        os.makedirs(img_fold_AB)\n    print('split = %s, number of images = %d' % (sp, num_imgs))\n    for n in range(num_imgs):\n        name_A = img_list[n]\n        path_A = os.path.join(img_fold_A, name_A)\n        if args.use_AB:\n            name_B = name_A.replace('_A.', '_B.')\n        else:\n            name_B = name_A\n        path_B = os.path.join(img_fold_B, name_B)\n        if os.path.isfile(path_A) and os.path.isfile(path_B):\n            name_AB = name_A\n            if args.use_AB:\n                name_AB = name_AB.replace('_A.', '.') # remove _A\n            path_AB = os.path.join(img_fold_AB, name_AB)\n            im_A = cv2.imread(path_A, cv2.IMREAD_COLOR)\n            im_B = cv2.imread(path_B, cv2.IMREAD_COLOR)\n            im_AB = np.concatenate([im_A, im_B], 1)\n            cv2.imwrite(path_AB, im_AB)\n"""
util/__init__.py,0,b''
util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
util/image_pool.py,2,"b'import random\nimport torch\n\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = torch.cat(return_images, 0)\n        return return_images\n'"
util/util.py,2,"b""from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport os\n\n\n# Converts a Tensor into an image array (numpy)\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(input_image, imtype=np.uint8):\n    if isinstance(input_image, torch.Tensor):\n        image_tensor = input_image.data\n    else:\n        return input_image\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name='network'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print('shape,', x.shape)\n    if val:\n        x = x.flatten()\n        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n"""
util/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\nfrom scipy.misc import imresize\n\n\n# save image to the disk\ndef save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256):\n    image_dir = webpage.get_image_dir()\n    short_path = ntpath.basename(image_path[0])\n    name = os.path.splitext(short_path)[0]\n\n    webpage.add_header(name)\n    ims, txts, links = [], [], []\n\n    for label, im_data in visuals.items():\n        im = util.tensor2im(im_data)#tensor to numpy array [-1,1]->[0,1]->[0,255]\n        image_name = \'%s_%s.png\' % (name, label)\n        save_path = os.path.join(image_dir, image_name)\n        h, w, _ = im.shape\n        if aspect_ratio > 1.0:\n            im = imresize(im, (h, int(w * aspect_ratio)), interp=\'bicubic\')\n        if aspect_ratio < 1.0:\n            im = imresize(im, (int(h / aspect_ratio), w), interp=\'bicubic\')\n        util.save_image(im, save_path)\n\n        ims.append(image_name)\n        txts.append(label)\n        links.append(image_name)\n    webpage.add_images(ims, txts, links, width=width)\n\n\nclass Visualizer():\n    def __init__(self, opt):\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        self.opt = opt\n        self.saved = False\n        if self.display_id > 0:\n            import visdom\n            self.ncols = opt.display_ncols\n            self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port, env=opt.display_env, raise_exceptions=True)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    def reset(self):\n        self.saved = False\n\n    def throw_visdom_connection_error(self):\n        print(\'\\n\\nCould not connect to Visdom server (https://github.com/facebookresearch/visdom) for displaying training progress.\\nYou can suppress connection to Visdom using the option --display_id -1. To install visdom, run \\n$ pip install visdom\\n, and start the server by \\n$ python -m visdom.server.\\n\\n\')\n        exit(1)\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, save_result):\n        if self.display_id > 0:  # show images in the browser\n            ncols = self.ncols\n            if ncols > 0:\n                ncols = min(ncols, len(visuals))\n                h, w = next(iter(visuals.values())).shape[:2]\n                table_css = """"""<style>\n                        table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n                        table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n                        </style>"""""" % (w, h)\n                title = self.name\n                label_html = \'\'\n                label_html_row = \'\'\n                images = []\n                idx = 0\n                for label, image in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    label_html_row += \'<td>%s</td>\' % label\n                    images.append(image_numpy.transpose([2, 0, 1]))\n                    idx += 1\n                    if idx % ncols == 0:\n                        label_html += \'<tr>%s</tr>\' % label_html_row\n                        label_html_row = \'\'\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1])) * 255\n                while idx % ncols != 0:\n                    images.append(white_image)\n                    label_html_row += \'<td></td>\'\n                    idx += 1\n                if label_html_row != \'\':\n                    label_html += \'<tr>%s</tr>\' % label_html_row\n                # pane col = image row\n                try:\n                    self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n                                    padding=2, opts=dict(title=title + \' images\'))\n                    label_html = \'<table>%s</table>\' % label_html\n                    self.vis.text(table_css + label_html, win=self.display_id + 2,\n                                  opts=dict(title=title + \' labels\'))\n                except ConnectionError:\n                    self.throw_visdom_connection_error()\n\n            else:\n                idx = 1\n                for label, image in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    self.vis.image(image_numpy.transpose([2, 0, 1]), opts=dict(title=label),\n                                   win=self.display_id + idx)\n                    idx += 1\n\n        if self.use_html and (save_result or not self.saved):  # save images to a html file\n            self.saved = True\n            for label, image in visuals.items():\n                image_numpy = util.tensor2im(image)\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.png\' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims, txts, links = [], [], []\n\n                for label, image_numpy in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    img_path = \'epoch%.3d_%s.png\' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n    \n    def save_current_results1(self, visuals, epoch, epoch_iter):\n        if not os.path.exists(self.img_dir+\'/detailed\'):\n            os.mkdir(self.img_dir+\'/detailed\')\n        for label, image in visuals.items():\n            image_numpy = util.tensor2im(image)\n            img_path = os.path.join(self.img_dir, \'detailed\', \'epoch%.3d_%.3d_%s.png\' % (epoch, epoch_iter, label))\n            util.save_image(image_numpy, img_path)\n\n    # losses: dictionary of error labels and values\n    def plot_current_losses(self, epoch, counter_ratio, opt, losses):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\': [], \'Y\': [], \'legend\': list(losses.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([losses[k] for k in self.plot_data[\'legend\']])\n        try:\n            self.vis.line(\n                X=np.stack([np.array(self.plot_data[\'X\'])] * len(self.plot_data[\'legend\']), 1),\n                Y=np.array(self.plot_data[\'Y\']),\n                opts={\n                    \'title\': self.name + \' loss over time\',\n                    \'legend\': self.plot_data[\'legend\'],\n                    \'xlabel\': \'epoch\',\n                    \'ylabel\': \'loss\'},\n                win=self.display_id)\n        except ConnectionError:\n            self.throw_visdom_connection_error()\n\n    # losses: same format as |losses| of plot_current_losses\n    def print_current_losses(self, epoch, i, losses, t, t_data):\n        message = \'(epoch: %d, iters: %d, time: %.3f, data: %.3f) \' % (epoch, i, t, t_data)\n        for k, v in losses.items():\n            message += \'%s: %.6f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n'"
