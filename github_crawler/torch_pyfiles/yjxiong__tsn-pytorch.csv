file_path,api_count,code
dataset.py,1,"b'import torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\nfrom numpy.random import randint\n\nclass VideoRecord(object):\n    def __init__(self, row):\n        self._data = row\n\n    @property\n    def path(self):\n        return self._data[0]\n\n    @property\n    def num_frames(self):\n        return int(self._data[1])\n\n    @property\n    def label(self):\n        return int(self._data[2])\n\n\nclass TSNDataSet(data.Dataset):\n    def __init__(self, root_path, list_file,\n                 num_segments=3, new_length=1, modality=\'RGB\',\n                 image_tmpl=\'img_{:05d}.jpg\', transform=None,\n                 force_grayscale=False, random_shift=True, test_mode=False):\n\n        self.root_path = root_path\n        self.list_file = list_file\n        self.num_segments = num_segments\n        self.new_length = new_length\n        self.modality = modality\n        self.image_tmpl = image_tmpl\n        self.transform = transform\n        self.random_shift = random_shift\n        self.test_mode = test_mode\n\n        if self.modality == \'RGBDiff\':\n            self.new_length += 1# Diff needs one more image to calculate diff\n\n        self._parse_list()\n\n    def _load_image(self, directory, idx):\n        if self.modality == \'RGB\' or self.modality == \'RGBDiff\':\n            return [Image.open(os.path.join(directory, self.image_tmpl.format(idx))).convert(\'RGB\')]\n        elif self.modality == \'Flow\':\n            x_img = Image.open(os.path.join(directory, self.image_tmpl.format(\'x\', idx))).convert(\'L\')\n            y_img = Image.open(os.path.join(directory, self.image_tmpl.format(\'y\', idx))).convert(\'L\')\n\n            return [x_img, y_img]\n\n    def _parse_list(self):\n        self.video_list = [VideoRecord(x.strip().split(\' \')) for x in open(self.list_file)]\n\n    def _sample_indices(self, record):\n        """"""\n\n        :param record: VideoRecord\n        :return: list\n        """"""\n\n        average_duration = (record.num_frames - self.new_length + 1) // self.num_segments\n        if average_duration > 0:\n            offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration, size=self.num_segments)\n        elif record.num_frames > self.num_segments:\n            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n        else:\n            offsets = np.zeros((self.num_segments,))\n        return offsets + 1\n\n    def _get_val_indices(self, record):\n        if record.num_frames > self.num_segments + self.new_length - 1:\n            tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n        else:\n            offsets = np.zeros((self.num_segments,))\n        return offsets + 1\n\n    def _get_test_indices(self, record):\n\n        tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)\n\n        offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n\n        return offsets + 1\n\n    def __getitem__(self, index):\n        record = self.video_list[index]\n\n        if not self.test_mode:\n            segment_indices = self._sample_indices(record) if self.random_shift else self._get_val_indices(record)\n        else:\n            segment_indices = self._get_test_indices(record)\n\n        return self.get(record, segment_indices)\n\n    def get(self, record, indices):\n\n        images = list()\n        for seg_ind in indices:\n            p = int(seg_ind)\n            for i in range(self.new_length):\n                seg_imgs = self._load_image(record.path, p)\n                images.extend(seg_imgs)\n                if p < record.num_frames:\n                    p += 1\n\n        process_data = self.transform(images)\n        return process_data, record.label\n\n    def __len__(self):\n        return len(self.video_list)\n'"
main.py,15,"b'import argparse\nimport os\nimport time\nimport shutil\nimport torch\nimport torchvision\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nfrom torch.nn.utils import clip_grad_norm\n\nfrom dataset import TSNDataSet\nfrom models import TSN\nfrom transforms import *\nfrom opts import parser\n\nbest_prec1 = 0\n\n\ndef main():\n    global args, best_prec1\n    args = parser.parse_args()\n\n    if args.dataset == \'ucf101\':\n        num_class = 101\n    elif args.dataset == \'hmdb51\':\n        num_class = 51\n    elif args.dataset == \'kinetics\':\n        num_class = 400\n    else:\n        raise ValueError(\'Unknown dataset \'+args.dataset)\n\n    model = TSN(num_class, args.num_segments, args.modality,\n                base_model=args.arch,\n                consensus_type=args.consensus_type, dropout=args.dropout, partial_bn=not args.no_partialbn)\n\n    crop_size = model.crop_size\n    scale_size = model.scale_size\n    input_mean = model.input_mean\n    input_std = model.input_std\n    policies = model.get_optim_policies()\n    train_augmentation = model.get_augmentation()\n\n    model = torch.nn.DataParallel(model, device_ids=args.gpus).cuda()\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print((""=> loading checkpoint \'{}\'"".format(args.resume)))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_prec1 = checkpoint[\'best_prec1\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            print((""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.evaluate, checkpoint[\'epoch\'])))\n        else:\n            print((""=> no checkpoint found at \'{}\'"".format(args.resume)))\n\n    cudnn.benchmark = True\n\n    # Data loading code\n    if args.modality != \'RGBDiff\':\n        normalize = GroupNormalize(input_mean, input_std)\n    else:\n        normalize = IdentityTransform()\n\n    if args.modality == \'RGB\':\n        data_length = 1\n    elif args.modality in [\'Flow\', \'RGBDiff\']:\n        data_length = 5\n\n    train_loader = torch.utils.data.DataLoader(\n        TSNDataSet("""", args.train_list, num_segments=args.num_segments,\n                   new_length=data_length,\n                   modality=args.modality,\n                   image_tmpl=""img_{:05d}.jpg"" if args.modality in [""RGB"", ""RGBDiff""] else args.flow_prefix+""{}_{:05d}.jpg"",\n                   transform=torchvision.transforms.Compose([\n                       train_augmentation,\n                       Stack(roll=args.arch == \'BNInception\'),\n                       ToTorchFormatTensor(div=args.arch != \'BNInception\'),\n                       normalize,\n                   ])),\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TSNDataSet("""", args.val_list, num_segments=args.num_segments,\n                   new_length=data_length,\n                   modality=args.modality,\n                   image_tmpl=""img_{:05d}.jpg"" if args.modality in [""RGB"", ""RGBDiff""] else args.flow_prefix+""{}_{:05d}.jpg"",\n                   random_shift=False,\n                   transform=torchvision.transforms.Compose([\n                       GroupScale(int(scale_size)),\n                       GroupCenterCrop(crop_size),\n                       Stack(roll=args.arch == \'BNInception\'),\n                       ToTorchFormatTensor(div=args.arch != \'BNInception\'),\n                       normalize,\n                   ])),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    # define loss function (criterion) and optimizer\n    if args.loss_type == \'nll\':\n        criterion = torch.nn.CrossEntropyLoss().cuda()\n    else:\n        raise ValueError(""Unknown loss type"")\n\n    for group in policies:\n        print((\'group: {} has {} params, lr_mult: {}, decay_mult: {}\'.format(\n            group[\'name\'], len(group[\'params\']), group[\'lr_mult\'], group[\'decay_mult\'])))\n\n    optimizer = torch.optim.SGD(policies,\n                                args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    if args.evaluate:\n        validate(val_loader, model, criterion, 0)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch, args.lr_steps)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        if (epoch + 1) % args.eval_freq == 0 or epoch == args.epochs - 1:\n            prec1 = validate(val_loader, model, criterion, (epoch + 1) * len(train_loader))\n\n            # remember best prec@1 and save checkpoint\n            is_best = prec1 > best_prec1\n            best_prec1 = max(prec1, best_prec1)\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'best_prec1\': best_prec1,\n            }, is_best)\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    if args.no_partialbn:\n        model.module.partialBN(False)\n    else:\n        model.module.partialBN(True)\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        if args.clip_gradient is not None:\n            total_norm = clip_grad_norm(model.parameters(), args.clip_gradient)\n            if total_norm > args.clip_gradient:\n                print(""clipping gradient: {} with coef {}"".format(total_norm, args.clip_gradient / total_norm))\n\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print((\'Epoch: [{0}][{1}/{2}], lr: {lr:.5f}\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   epoch, i, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1, top5=top5, lr=optimizer.param_groups[-1][\'lr\'])))\n\n\ndef validate(val_loader, model, criterion, iter, logger=None):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input, volatile=True)\n        target_var = torch.autograd.Variable(target, volatile=True)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print((\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   i, len(val_loader), batch_time=batch_time, loss=losses,\n                   top1=top1, top5=top5)))\n\n    print((\'Testing Results: Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Loss {loss.avg:.5f}\'\n          .format(top1=top1, top5=top5, loss=losses)))\n\n    return top1.avg\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    filename = \'_\'.join((args.snapshot_pref, args.modality.lower(), filename))\n    torch.save(state, filename)\n    if is_best:\n        best_name = \'_\'.join((args.snapshot_pref, args.modality.lower(), \'model_best.pth.tar\'))\n        shutil.copyfile(filename, best_name)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch, lr_steps):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    decay = 0.1 ** (sum(epoch >= np.array(lr_steps)))\n    lr = args.lr * decay\n    decay = args.weight_decay\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr * param_group[\'lr_mult\']\n        param_group[\'weight_decay\'] = decay * param_group[\'decay_mult\']\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\nif __name__ == \'__main__\':\n    main()\n'"
models.py,6,"b'from torch import nn\n\nfrom ops.basic_ops import ConsensusModule, Identity\nfrom transforms import *\nfrom torch.nn.init import normal, constant\n\nclass TSN(nn.Module):\n    def __init__(self, num_class, num_segments, modality,\n                 base_model=\'resnet101\', new_length=None,\n                 consensus_type=\'avg\', before_softmax=True,\n                 dropout=0.8,\n                 crop_num=1, partial_bn=True):\n        super(TSN, self).__init__()\n        self.modality = modality\n        self.num_segments = num_segments\n        self.reshape = True\n        self.before_softmax = before_softmax\n        self.dropout = dropout\n        self.crop_num = crop_num\n        self.consensus_type = consensus_type\n        if not before_softmax and consensus_type != \'avg\':\n            raise ValueError(""Only avg consensus can be used after Softmax"")\n\n        if new_length is None:\n            self.new_length = 1 if modality == ""RGB"" else 5\n        else:\n            self.new_length = new_length\n\n        print((""""""\nInitializing TSN with base model: {}.\nTSN Configurations:\n    input_modality:     {}\n    num_segments:       {}\n    new_length:         {}\n    consensus_module:   {}\n    dropout_ratio:      {}\n        """""".format(base_model, self.modality, self.num_segments, self.new_length, consensus_type, self.dropout)))\n\n        self._prepare_base_model(base_model)\n\n        feature_dim = self._prepare_tsn(num_class)\n\n        if self.modality == \'Flow\':\n            print(""Converting the ImageNet model to a flow init model"")\n            self.base_model = self._construct_flow_model(self.base_model)\n            print(""Done. Flow model ready..."")\n        elif self.modality == \'RGBDiff\':\n            print(""Converting the ImageNet model to RGB+Diff init model"")\n            self.base_model = self._construct_diff_model(self.base_model)\n            print(""Done. RGBDiff model ready."")\n\n        self.consensus = ConsensusModule(consensus_type)\n\n        if not self.before_softmax:\n            self.softmax = nn.Softmax()\n\n        self._enable_pbn = partial_bn\n        if partial_bn:\n            self.partialBN(True)\n\n    def _prepare_tsn(self, num_class):\n        feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n        if self.dropout == 0:\n            setattr(self.base_model, self.base_model.last_layer_name, nn.Linear(feature_dim, num_class))\n            self.new_fc = None\n        else:\n            setattr(self.base_model, self.base_model.last_layer_name, nn.Dropout(p=self.dropout))\n            self.new_fc = nn.Linear(feature_dim, num_class)\n\n        std = 0.001\n        if self.new_fc is None:\n            normal(getattr(self.base_model, self.base_model.last_layer_name).weight, 0, std)\n            constant(getattr(self.base_model, self.base_model.last_layer_name).bias, 0)\n        else:\n            normal(self.new_fc.weight, 0, std)\n            constant(self.new_fc.bias, 0)\n        return feature_dim\n\n    def _prepare_base_model(self, base_model):\n\n        if \'resnet\' in base_model or \'vgg\' in base_model:\n            self.base_model = getattr(torchvision.models, base_model)(True)\n            self.base_model.last_layer_name = \'fc\'\n            self.input_size = 224\n            self.input_mean = [0.485, 0.456, 0.406]\n            self.input_std = [0.229, 0.224, 0.225]\n\n            if self.modality == \'Flow\':\n                self.input_mean = [0.5]\n                self.input_std = [np.mean(self.input_std)]\n            elif self.modality == \'RGBDiff\':\n                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n        elif base_model == \'BNInception\':\n            import tf_model_zoo\n            self.base_model = getattr(tf_model_zoo, base_model)()\n            self.base_model.last_layer_name = \'fc\'\n            self.input_size = 224\n            self.input_mean = [104, 117, 128]\n            self.input_std = [1]\n\n            if self.modality == \'Flow\':\n                self.input_mean = [128]\n            elif self.modality == \'RGBDiff\':\n                self.input_mean = self.input_mean * (1 + self.new_length)\n\n        elif \'inception\' in base_model:\n            import tf_model_zoo\n            self.base_model = getattr(tf_model_zoo, base_model)()\n            self.base_model.last_layer_name = \'classif\'\n            self.input_size = 299\n            self.input_mean = [0.5]\n            self.input_std = [0.5]\n        else:\n            raise ValueError(\'Unknown base model: {}\'.format(base_model))\n\n    def train(self, mode=True):\n        """"""\n        Override the default train() to freeze the BN parameters\n        :return:\n        """"""\n        super(TSN, self).train(mode)\n        count = 0\n        if self._enable_pbn:\n            print(""Freezing BatchNorm2D except the first one."")\n            for m in self.base_model.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    count += 1\n                    if count >= (2 if self._enable_pbn else 1):\n                        m.eval()\n\n                        # shutdown update in frozen mode\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n\n    def partialBN(self, enable):\n        self._enable_pbn = enable\n\n    def get_optim_policies(self):\n        first_conv_weight = []\n        first_conv_bias = []\n        normal_weight = []\n        normal_bias = []\n        bn = []\n\n        conv_cnt = 0\n        bn_cnt = 0\n        for m in self.modules():\n            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv1d):\n                ps = list(m.parameters())\n                conv_cnt += 1\n                if conv_cnt == 1:\n                    first_conv_weight.append(ps[0])\n                    if len(ps) == 2:\n                        first_conv_bias.append(ps[1])\n                else:\n                    normal_weight.append(ps[0])\n                    if len(ps) == 2:\n                        normal_bias.append(ps[1])\n            elif isinstance(m, torch.nn.Linear):\n                ps = list(m.parameters())\n                normal_weight.append(ps[0])\n                if len(ps) == 2:\n                    normal_bias.append(ps[1])\n                  \n            elif isinstance(m, torch.nn.BatchNorm1d):\n                bn.extend(list(m.parameters()))\n            elif isinstance(m, torch.nn.BatchNorm2d):\n                bn_cnt += 1\n                # later BN\'s are frozen\n                if not self._enable_pbn or bn_cnt == 1:\n                    bn.extend(list(m.parameters()))\n            elif len(m._modules) == 0:\n                if len(list(m.parameters())) > 0:\n                    raise ValueError(""New atomic module type: {}. Need to give it a learning policy"".format(type(m)))\n\n        return [\n            {\'params\': first_conv_weight, \'lr_mult\': 5 if self.modality == \'Flow\' else 1, \'decay_mult\': 1,\n             \'name\': ""first_conv_weight""},\n            {\'params\': first_conv_bias, \'lr_mult\': 10 if self.modality == \'Flow\' else 2, \'decay_mult\': 0,\n             \'name\': ""first_conv_bias""},\n            {\'params\': normal_weight, \'lr_mult\': 1, \'decay_mult\': 1,\n             \'name\': ""normal_weight""},\n            {\'params\': normal_bias, \'lr_mult\': 2, \'decay_mult\': 0,\n             \'name\': ""normal_bias""},\n            {\'params\': bn, \'lr_mult\': 1, \'decay_mult\': 0,\n             \'name\': ""BN scale/shift""},\n        ]\n\n    def forward(self, input):\n        sample_len = (3 if self.modality == ""RGB"" else 2) * self.new_length\n\n        if self.modality == \'RGBDiff\':\n            sample_len = 3 * self.new_length\n            input = self._get_diff(input)\n\n        base_out = self.base_model(input.view((-1, sample_len) + input.size()[-2:]))\n\n        if self.dropout > 0:\n            base_out = self.new_fc(base_out)\n\n        if not self.before_softmax:\n            base_out = self.softmax(base_out)\n        if self.reshape:\n            base_out = base_out.view((-1, self.num_segments) + base_out.size()[1:])\n\n        output = self.consensus(base_out)\n        return output.squeeze(1)\n\n    def _get_diff(self, input, keep_rgb=False):\n        input_c = 3 if self.modality in [""RGB"", ""RGBDiff""] else 2\n        input_view = input.view((-1, self.num_segments, self.new_length + 1, input_c,) + input.size()[2:])\n        if keep_rgb:\n            new_data = input_view.clone()\n        else:\n            new_data = input_view[:, :, 1:, :, :, :].clone()\n\n        for x in reversed(list(range(1, self.new_length + 1))):\n            if keep_rgb:\n                new_data[:, :, x, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n            else:\n                new_data[:, :, x - 1, :, :, :] = input_view[:, :, x, :, :, :] - input_view[:, :, x - 1, :, :, :]\n\n        return new_data\n\n\n    def _construct_flow_model(self, base_model):\n        # modify the convolution layers\n        # Torch models are usually defined in a hierarchical way.\n        # nn.modules.children() return all sub modules in a DFS manner\n        modules = list(self.base_model.modules())\n        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n        conv_layer = modules[first_conv_idx]\n        container = modules[first_conv_idx - 1]\n\n        # modify parameters, assume the first blob contains the convolution kernels\n        params = [x.clone() for x in conv_layer.parameters()]\n        kernel_size = params[0].size()\n        new_kernel_size = kernel_size[:1] + (2 * self.new_length, ) + kernel_size[2:]\n        new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n\n        new_conv = nn.Conv2d(2 * self.new_length, conv_layer.out_channels,\n                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n                             bias=True if len(params) == 2 else False)\n        new_conv.weight.data = new_kernels\n        if len(params) == 2:\n            new_conv.bias.data = params[1].data # add bias if neccessary\n        layer_name = list(container.state_dict().keys())[0][:-7] # remove .weight suffix to get the layer name\n\n        # replace the first convlution layer\n        setattr(container, layer_name, new_conv)\n        return base_model\n\n    def _construct_diff_model(self, base_model, keep_rgb=False):\n        # modify the convolution layers\n        # Torch models are usually defined in a hierarchical way.\n        # nn.modules.children() return all sub modules in a DFS manner\n        modules = list(self.base_model.modules())\n        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n        conv_layer = modules[first_conv_idx]\n        container = modules[first_conv_idx - 1]\n\n        # modify parameters, assume the first blob contains the convolution kernels\n        params = [x.clone() for x in conv_layer.parameters()]\n        kernel_size = params[0].size()\n        if not keep_rgb:\n            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n            new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n        else:\n            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n            new_kernels = torch.cat((params[0].data, params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()),\n                                    1)\n            new_kernel_size = kernel_size[:1] + (3 + 3 * self.new_length,) + kernel_size[2:]\n\n        new_conv = nn.Conv2d(new_kernel_size[1], conv_layer.out_channels,\n                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n                             bias=True if len(params) == 2 else False)\n        new_conv.weight.data = new_kernels\n        if len(params) == 2:\n            new_conv.bias.data = params[1].data  # add bias if neccessary\n        layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n\n        # replace the first convolution layer\n        setattr(container, layer_name, new_conv)\n        return base_model\n\n    @property\n    def crop_size(self):\n        return self.input_size\n\n    @property\n    def scale_size(self):\n        return self.input_size * 256 // 224\n\n    def get_augmentation(self):\n        if self.modality == \'RGB\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66]),\n                                                   GroupRandomHorizontalFlip(is_flow=False)])\n        elif self.modality == \'Flow\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n                                                   GroupRandomHorizontalFlip(is_flow=True)])\n        elif self.modality == \'RGBDiff\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n                                                   GroupRandomHorizontalFlip(is_flow=False)])\n'"
opts.py,0,"b'import argparse\nparser = argparse.ArgumentParser(description=""PyTorch implementation of Temporal Segment Networks"")\nparser.add_argument(\'dataset\', type=str, choices=[\'ucf101\', \'hmdb51\', \'kinetics\'])\nparser.add_argument(\'modality\', type=str, choices=[\'RGB\', \'Flow\', \'RGBDiff\'])\nparser.add_argument(\'train_list\', type=str)\nparser.add_argument(\'val_list\', type=str)\n\n# ========================= Model Configs ==========================\nparser.add_argument(\'--arch\', type=str, default=""resnet101"")\nparser.add_argument(\'--num_segments\', type=int, default=3)\nparser.add_argument(\'--consensus_type\', type=str, default=\'avg\',\n                    choices=[\'avg\', \'max\', \'topk\', \'identity\', \'rnn\', \'cnn\'])\nparser.add_argument(\'--k\', type=int, default=3)\n\nparser.add_argument(\'--dropout\', \'--do\', default=0.5, type=float,\n                    metavar=\'DO\', help=\'dropout ratio (default: 0.5)\')\nparser.add_argument(\'--loss_type\', type=str, default=""nll"",\n                    choices=[\'nll\'])\n\n# ========================= Learning Configs ==========================\nparser.add_argument(\'--epochs\', default=45, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.001, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--lr_steps\', default=[20, 40], type=float, nargs=""+"",\n                    metavar=\'LRSteps\', help=\'epochs to decay learning rate by 10\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=5e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 5e-4)\')\nparser.add_argument(\'--clip-gradient\', \'--gd\', default=None, type=float,\n                    metavar=\'W\', help=\'gradient norm clipping (default: disabled)\')\nparser.add_argument(\'--no_partialbn\', \'--npb\', default=False, action=""store_true"")\n\n# ========================= Monitor Configs ==========================\nparser.add_argument(\'--print-freq\', \'-p\', default=20, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--eval-freq\', \'-ef\', default=5, type=int,\n                    metavar=\'N\', help=\'evaluation frequency (default: 5)\')\n\n\n# ========================= Runtime Configs ==========================\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--snapshot_pref\', type=str, default="""")\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--gpus\', nargs=\'+\', type=int, default=None)\nparser.add_argument(\'--flow_prefix\', default="""", type=str)\n\n\n\n\n\n\n\n\n'"
test_models.py,6,"b'import argparse\nimport time\n\nimport numpy as np\nimport torch.nn.parallel\nimport torch.optim\nfrom sklearn.metrics import confusion_matrix\n\nfrom dataset import TSNDataSet\nfrom models import TSN\nfrom transforms import *\nfrom ops import ConsensusModule\n\n# options\nparser = argparse.ArgumentParser(\n    description=""Standard video-level testing"")\nparser.add_argument(\'dataset\', type=str, choices=[\'ucf101\', \'hmdb51\', \'kinetics\'])\nparser.add_argument(\'modality\', type=str, choices=[\'RGB\', \'Flow\', \'RGBDiff\'])\nparser.add_argument(\'test_list\', type=str)\nparser.add_argument(\'weights\', type=str)\nparser.add_argument(\'--arch\', type=str, default=""resnet101"")\nparser.add_argument(\'--save_scores\', type=str, default=None)\nparser.add_argument(\'--test_segments\', type=int, default=25)\nparser.add_argument(\'--max_num\', type=int, default=-1)\nparser.add_argument(\'--test_crops\', type=int, default=10)\nparser.add_argument(\'--input_size\', type=int, default=224)\nparser.add_argument(\'--crop_fusion_type\', type=str, default=\'avg\',\n                    choices=[\'avg\', \'max\', \'topk\'])\nparser.add_argument(\'--k\', type=int, default=3)\nparser.add_argument(\'--dropout\', type=float, default=0.7)\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--gpus\', nargs=\'+\', type=int, default=None)\nparser.add_argument(\'--flow_prefix\', type=str, default=\'\')\n\nargs = parser.parse_args()\n\n\nif args.dataset == \'ucf101\':\n    num_class = 101\nelif args.dataset == \'hmdb51\':\n    num_class = 51\nelif args.dataset == \'kinetics\':\n    num_class = 400\nelse:\n    raise ValueError(\'Unknown dataset \'+args.dataset)\n\nnet = TSN(num_class, 1, args.modality,\n          base_model=args.arch,\n          consensus_type=args.crop_fusion_type,\n          dropout=args.dropout)\n\ncheckpoint = torch.load(args.weights)\nprint(""model epoch {} best prec@1: {}"".format(checkpoint[\'epoch\'], checkpoint[\'best_prec1\']))\n\nbase_dict = {\'.\'.join(k.split(\'.\')[1:]): v for k,v in list(checkpoint[\'state_dict\'].items())}\nnet.load_state_dict(base_dict)\n\nif args.test_crops == 1:\n    cropping = torchvision.transforms.Compose([\n        GroupScale(net.scale_size),\n        GroupCenterCrop(net.input_size),\n    ])\nelif args.test_crops == 10:\n    cropping = torchvision.transforms.Compose([\n        GroupOverSample(net.input_size, net.scale_size)\n    ])\nelse:\n    raise ValueError(""Only 1 and 10 crops are supported while we got {}"".format(args.test_crops))\n\ndata_loader = torch.utils.data.DataLoader(\n        TSNDataSet("""", args.test_list, num_segments=args.test_segments,\n                   new_length=1 if args.modality == ""RGB"" else 5,\n                   modality=args.modality,\n                   image_tmpl=""img_{:05d}.jpg"" if args.modality in [\'RGB\', \'RGBDiff\'] else args.flow_prefix+""{}_{:05d}.jpg"",\n                   test_mode=True,\n                   transform=torchvision.transforms.Compose([\n                       cropping,\n                       Stack(roll=args.arch == \'BNInception\'),\n                       ToTorchFormatTensor(div=args.arch != \'BNInception\'),\n                       GroupNormalize(net.input_mean, net.input_std),\n                   ])),\n        batch_size=1, shuffle=False,\n        num_workers=args.workers * 2, pin_memory=True)\n\nif args.gpus is not None:\n    devices = [args.gpus[i] for i in range(args.workers)]\nelse:\n    devices = list(range(args.workers))\n\n\nnet = torch.nn.DataParallel(net.cuda(devices[0]), device_ids=devices)\nnet.eval()\n\ndata_gen = enumerate(data_loader)\n\ntotal_num = len(data_loader.dataset)\noutput = []\n\n\ndef eval_video(video_data):\n    i, data, label = video_data\n    num_crop = args.test_crops\n\n    if args.modality == \'RGB\':\n        length = 3\n    elif args.modality == \'Flow\':\n        length = 10\n    elif args.modality == \'RGBDiff\':\n        length = 18\n    else:\n        raise ValueError(""Unknown modality ""+args.modality)\n\n    input_var = torch.autograd.Variable(data.view(-1, length, data.size(2), data.size(3)),\n                                        volatile=True)\n    rst = net(input_var).data.cpu().numpy().copy()\n    return i, rst.reshape((num_crop, args.test_segments, num_class)).mean(axis=0).reshape(\n        (args.test_segments, 1, num_class)\n    ), label[0]\n\n\nproc_start_time = time.time()\nmax_num = args.max_num if args.max_num > 0 else len(data_loader.dataset)\n\nfor i, (data, label) in data_gen:\n    if i >= max_num:\n        break\n    rst = eval_video((i, data, label))\n    output.append(rst[1:])\n    cnt_time = time.time() - proc_start_time\n    print(\'video {} done, total {}/{}, average {} sec/video\'.format(i, i+1,\n                                                                    total_num,\n                                                                    float(cnt_time) / (i+1)))\n\nvideo_pred = [np.argmax(np.mean(x[0], axis=0)) for x in output]\n\nvideo_labels = [x[1] for x in output]\n\n\ncf = confusion_matrix(video_labels, video_pred).astype(float)\n\ncls_cnt = cf.sum(axis=1)\ncls_hit = np.diag(cf)\n\ncls_acc = cls_hit / cls_cnt\n\nprint(cls_acc)\n\nprint(\'Accuracy {:.02f}%\'.format(np.mean(cls_acc) * 100))\n\nif args.save_scores is not None:\n\n    # reorder before saving\n    name_list = [x.strip().split()[0] for x in open(args.test_list)]\n\n    order_dict = {e:i for i, e in enumerate(sorted(name_list))}\n\n    reorder_output = [None] * len(output)\n    reorder_label = [None] * len(output)\n\n    for i in range(len(output)):\n        idx = order_dict[name_list[i]]\n        reorder_output[idx] = output[i]\n        reorder_label[idx] = video_labels[i]\n\n    np.savez(args.save_scores, scores=reorder_output, labels=reorder_label)\n\n\n'"
transforms.py,3,"b'import torchvision\nimport random\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport numbers\nimport math\nimport torch\n\n\nclass GroupRandomCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img_group):\n\n        w, h = img_group[0].size\n        th, tw = self.size\n\n        out_images = list()\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n\n        for img in img_group:\n            assert(img.size[0] == w and img.size[1] == h)\n            if w == tw and h == th:\n                out_images.append(img)\n            else:\n                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n\n        return out_images\n\n\nclass GroupCenterCrop(object):\n    def __init__(self, size):\n        self.worker = torchvision.transforms.CenterCrop(size)\n\n    def __call__(self, img_group):\n        return [self.worker(img) for img in img_group]\n\n\nclass GroupRandomHorizontalFlip(object):\n    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5\n    """"""\n    def __init__(self, is_flow=False):\n        self.is_flow = is_flow\n\n    def __call__(self, img_group, is_flow=False):\n        v = random.random()\n        if v < 0.5:\n            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n            if self.is_flow:\n                for i in range(0, len(ret), 2):\n                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n            return ret\n        else:\n            return img_group\n\n\nclass GroupNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n        rep_std = self.std * (tensor.size()[0]//len(self.std))\n\n        # TODO: make efficient\n        for t, m, s in zip(tensor, rep_mean, rep_std):\n            t.sub_(m).div_(s)\n\n        return tensor\n\n\nclass GroupScale(object):\n    """""" Rescales the input PIL.Image to the given \'size\'.\n    \'size\' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.worker = torchvision.transforms.Scale(size, interpolation)\n\n    def __call__(self, img_group):\n        return [self.worker(img) for img in img_group]\n\n\nclass GroupOverSample(object):\n    def __init__(self, crop_size, scale_size=None):\n        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n\n        if scale_size is not None:\n            self.scale_worker = GroupScale(scale_size)\n        else:\n            self.scale_worker = None\n\n    def __call__(self, img_group):\n\n        if self.scale_worker is not None:\n            img_group = self.scale_worker(img_group)\n\n        image_w, image_h = img_group[0].size\n        crop_w, crop_h = self.crop_size\n\n        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n        oversample_group = list()\n        for o_w, o_h in offsets:\n            normal_group = list()\n            flip_group = list()\n            for i, img in enumerate(img_group):\n                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n                normal_group.append(crop)\n                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n\n                if img.mode == \'L\' and i % 2 == 0:\n                    flip_group.append(ImageOps.invert(flip_crop))\n                else:\n                    flip_group.append(flip_crop)\n\n            oversample_group.extend(normal_group)\n            oversample_group.extend(flip_group)\n        return oversample_group\n\n\nclass GroupMultiScaleCrop(object):\n\n    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n        self.scales = scales if scales is not None else [1, .875, .75, .66]\n        self.max_distort = max_distort\n        self.fix_crop = fix_crop\n        self.more_fix_crop = more_fix_crop\n        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n        self.interpolation = Image.BILINEAR\n\n    def __call__(self, img_group):\n\n        im_size = img_group[0].size\n\n        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n                         for img in crop_img_group]\n        return ret_img_group\n\n    def _sample_crop_size(self, im_size):\n        image_w, image_h = im_size[0], im_size[1]\n\n        # find a crop size\n        base_size = min(image_w, image_h)\n        crop_sizes = [int(base_size * x) for x in self.scales]\n        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n\n        pairs = []\n        for i, h in enumerate(crop_h):\n            for j, w in enumerate(crop_w):\n                if abs(i - j) <= self.max_distort:\n                    pairs.append((w, h))\n\n        crop_pair = random.choice(pairs)\n        if not self.fix_crop:\n            w_offset = random.randint(0, image_w - crop_pair[0])\n            h_offset = random.randint(0, image_h - crop_pair[1])\n        else:\n            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n\n        return crop_pair[0], crop_pair[1], w_offset, h_offset\n\n    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n        return random.choice(offsets)\n\n    @staticmethod\n    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n        w_step = (image_w - crop_w) // 4\n        h_step = (image_h - crop_h) // 4\n\n        ret = list()\n        ret.append((0, 0))  # upper left\n        ret.append((4 * w_step, 0))  # upper right\n        ret.append((0, 4 * h_step))  # lower left\n        ret.append((4 * w_step, 4 * h_step))  # lower right\n        ret.append((2 * w_step, 2 * h_step))  # center\n\n        if more_fix_crop:\n            ret.append((0, 2 * h_step))  # center left\n            ret.append((4 * w_step, 2 * h_step))  # center right\n            ret.append((2 * w_step, 4 * h_step))  # lower center\n            ret.append((2 * w_step, 0 * h_step))  # upper center\n\n            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n\n        return ret\n\n\nclass GroupRandomSizedCrop(object):\n    """"""Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio\n    This is popularly used to train the Inception networks\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    """"""\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img_group):\n        for attempt in range(10):\n            area = img_group[0].size[0] * img_group[0].size[1]\n            target_area = random.uniform(0.08, 1.0) * area\n            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n                x1 = random.randint(0, img_group[0].size[0] - w)\n                y1 = random.randint(0, img_group[0].size[1] - h)\n                found = True\n                break\n        else:\n            found = False\n            x1 = 0\n            y1 = 0\n\n        if found:\n            out_group = list()\n            for img in img_group:\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                assert(img.size == (w, h))\n                out_group.append(img.resize((self.size, self.size), self.interpolation))\n            return out_group\n        else:\n            # Fallback\n            scale = GroupScale(self.size, interpolation=self.interpolation)\n            crop = GroupRandomCrop(self.size)\n            return crop(scale(img_group))\n\n\nclass Stack(object):\n\n    def __init__(self, roll=False):\n        self.roll = roll\n\n    def __call__(self, img_group):\n        if img_group[0].mode == \'L\':\n            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n        elif img_group[0].mode == \'RGB\':\n            if self.roll:\n                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n            else:\n                return np.concatenate(img_group, axis=2)\n\n\nclass ToTorchFormatTensor(object):\n    """""" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] """"""\n    def __init__(self, div=True):\n        self.div = div\n\n    def __call__(self, pic):\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n        else:\n            # handle PIL Image\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n            # put it from HWC to CHW format\n            # yikes, this transpose takes 80% of the loading time/CPU\n            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        return img.float().div(255) if self.div else img.float()\n\n\nclass IdentityTransform(object):\n\n    def __call__(self, data):\n        return data\n\n\nif __name__ == ""__main__"":\n    trans = torchvision.transforms.Compose([\n        GroupScale(256),\n        GroupRandomCrop(224),\n        Stack(),\n        ToTorchFormatTensor(),\n        GroupNormalize(\n            mean=[.485, .456, .406],\n            std=[.229, .224, .225]\n        )]\n    )\n\n    im = Image.open(\'../tensorflow-model-zoo.torch/lena_299.png\')\n\n    color_group = [im] * 3\n    rst = trans(color_group)\n\n    gray_group = [im.convert(\'L\')] * 9\n    gray_rst = trans(gray_group)\n\n    trans2 = torchvision.transforms.Compose([\n        GroupRandomSizedCrop(256),\n        Stack(),\n        ToTorchFormatTensor(),\n        GroupNormalize(\n            mean=[.485, .456, .406],\n            std=[.229, .224, .225])\n    ])\n    print(trans2(color_group))\n'"
ops/__init__.py,0,b'from ops.basic_ops import *'
ops/basic_ops.py,3,"b""import torch\nimport math\n\n\nclass Identity(torch.nn.Module):\n    def forward(self, input):\n        return input\n\n\nclass SegmentConsensus(torch.autograd.Function):\n\n    def __init__(self, consensus_type, dim=1):\n        self.consensus_type = consensus_type\n        self.dim = dim\n        self.shape = None\n\n    def forward(self, input_tensor):\n        self.shape = input_tensor.size()\n        if self.consensus_type == 'avg':\n            output = input_tensor.mean(dim=self.dim, keepdim=True)\n        elif self.consensus_type == 'identity':\n            output = input_tensor\n        else:\n            output = None\n\n        return output\n\n    def backward(self, grad_output):\n        if self.consensus_type == 'avg':\n            grad_in = grad_output.expand(self.shape) / float(self.shape[self.dim])\n        elif self.consensus_type == 'identity':\n            grad_in = grad_output\n        else:\n            grad_in = None\n\n        return grad_in\n\n\nclass ConsensusModule(torch.nn.Module):\n\n    def __init__(self, consensus_type, dim=1):\n        super(ConsensusModule, self).__init__()\n        self.consensus_type = consensus_type if consensus_type != 'rnn' else 'identity'\n        self.dim = dim\n\n    def forward(self, input):\n        return SegmentConsensus(self.consensus_type, self.dim)(input)\n"""
ops/utils.py,0,"b'import torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\ndef get_grad_hook(name):\n    def hook(m, grad_in, grad_out):\n        print((name, grad_out[0].data.abs().mean(), grad_in[0].data.abs().mean()))\n        print((grad_out[0].size()))\n        print((grad_in[0].size()))\n\n        print((grad_out[0]))\n        print((grad_in[0]))\n\n    return hook\n\n\ndef softmax(scores):\n    es = np.exp(scores - scores.max(axis=-1)[..., None])\n    return es / es.sum(axis=-1)[..., None]\n\n\ndef log_add(log_a, log_b):\n    return log_a + np.log(1 + np.exp(log_b - log_a))\n\n\ndef class_accuracy(prediction, label):\n    cf = confusion_matrix(prediction, label)\n    cls_cnt = cf.sum(axis=1)\n    cls_hit = np.diag(cf)\n\n    cls_acc = cls_hit / cls_cnt.astype(float)\n\n    mean_cls_acc = cls_acc.mean()\n\n    return cls_acc, mean_cls_acc'"
