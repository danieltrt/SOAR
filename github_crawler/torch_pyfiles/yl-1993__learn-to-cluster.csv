file_path,api_count,code
baseline/__init__.py,0,"b'from .sklearn_cluster import *\nfrom .aro import (aro, knn_aro)\nfrom .chinese_whispers import (chinese_whispers, chinese_whispers_fast)\n'"
baseline/aro.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport numpy as np\nfrom functools import partial\nfrom tqdm import tqdm\n\nfrom utils import build_knns, knns2ordered_nbrs, Timer\n""""""\npaper: https://arxiv.org/pdf/1604.00989.pdf\noriginal code https://github.com/varun-suresh/Clustering\n\nTo run `aro`:\n1. pip install pyflann\n2. 2to3 -w path/site-packages/pyflann/\nRefer [No module named \'index\'](https://github.com/primetang/pyflann/issues/1) for more details.\n\nFor `knn_aro`, we replace the pyflann with more advanced knn searching methods.\n""""""\n\n__all__ = [\'aro\', \'knn_aro\']\n\n\ndef build_index(dataset, n_neighbors):\n    """"""\n    Takes a dataset, returns the ""n"" nearest neighbors\n    """"""\n    # Initialize FLANN\n    import pyflann\n    pyflann.set_distance_type(distance_type=\'euclidean\')\n    flann = pyflann.FLANN()\n    params = flann.build_index(dataset, algorithm=\'kdtree\', trees=4)\n    #print params\n    nbrs, dists = flann.nn_index(dataset, n_neighbors, checks=params[\'checks\'])\n\n    return nbrs, dists\n\n\ndef create_neighbor_lookup(nbrs):\n    """"""\n    Key is the reference face, values are the neighbors.\n    """"""\n    nn_lookup = {}\n    for i in range(nbrs.shape[0]):\n        nn_lookup[i] = nbrs[i, :]\n    return nn_lookup\n\n\ndef calculate_symmetric_dist_row(nbrs, nn_lookup, row_no):\n    """"""\n    This function calculates the symmetric distances for one row in the\n    matrix.\n    """"""\n    dist_row = np.zeros([1, nbrs.shape[1]])\n    f1 = nn_lookup[row_no]\n    for idx, neighbor in enumerate(f1[1:]):\n        Oi = idx + 1\n        co_neighbor = True\n        try:\n            row = nn_lookup[neighbor]\n            Oj = np.where(row == row_no)[0][0] + 1\n        except IndexError:\n            Oj = nbrs.shape[1] + 1\n            co_neighbor = False\n        # dij\n        f11 = set(f1[0:Oi])\n        f21 = set(nn_lookup[neighbor])\n        dij = len(f11.difference(f21))\n        # dji\n        f12 = set(f1)\n        f22 = set(nn_lookup[neighbor][0:Oj])\n        dji = len(f22.difference(f12))\n\n        if not co_neighbor:\n            dist_row[0, Oi] = 9999.0\n        else:\n            dist_row[0, Oi] = float(dij + dji) / min(Oi, Oj)\n\n    return dist_row\n\n\ndef calculate_symmetric_dist(nbrs, num_process):\n    """"""\n    This function calculates the symmetric distance matrix.\n    """"""\n    d = np.zeros(nbrs.shape)\n    if num_process > 1:\n        from multiprocessing import Pool\n        p = Pool(processes=num_process)\n        num = nbrs.shape[0]\n        batch_size = 2000000\n        batch_num = int(num / batch_size) + 1\n        results = []\n        for i in range(batch_num):\n            start = i * batch_size\n            end = min(num, (i + 1) * batch_size)\n            sub_nbrs = nbrs[start:end]\n            nn_lookup = create_neighbor_lookup(sub_nbrs)\n            func = partial(calculate_symmetric_dist_row, sub_nbrs, nn_lookup)\n            results += p.map(func, range(sub_nbrs.shape[0]))\n        for row_no, row_val in enumerate(results):\n            d[row_no, :] = row_val\n    else:\n        nn_lookup = create_neighbor_lookup(nbrs)\n        for row_no in tqdm(range(nbrs.shape[0])):\n            row_val = calculate_symmetric_dist_row(nbrs, nn_lookup, row_no)\n            d[row_no, :] = row_val\n    return d\n\n\ndef aro_clustering(nbrs, dists, thresh):\n    \'\'\'\n    Approximate rank-order clustering. Takes in the nearest neighbors matrix\n    and outputs clusters - list of lists.\n    \'\'\'\n    # Clustering\n    clusters = []\n    # Start with the first face\n    nodes = set(list(np.arange(0, dists.shape[0])))\n    plausible_neighbors = create_plausible_neighbor_lookup(nbrs, dists, thresh)\n    while nodes:\n        # Get a node\n        n = nodes.pop()\n        # This contains the set of connected nodes\n        group = {n}\n        # Build a queue with this node in it\n        queue = [n]\n        # Iterate over the queue\n        while queue:\n            n = queue.pop(0)\n            neighbors = plausible_neighbors[n]\n            # Remove neighbors we\'ve already visited\n            neighbors = nodes.intersection(neighbors)\n            neighbors.difference_update(group)\n\n            # Remove nodes from the global set\n            nodes.difference_update(neighbors)\n\n            # Add the connected neighbors\n            group.update(neighbors)\n\n            # Add the neighbors to the queue to visit them next\n            queue.extend(neighbors)\n        # Add the group to the list of groups\n        clusters.append(group)\n    return clusters\n\n\ndef create_plausible_neighbor_lookup(nbrs, dists, thresh):\n    """"""\n    Create a dictionary where the keys are the row numbers(face numbers) and\n    the values are the plausible neighbors.\n    """"""\n    n_vectors = nbrs.shape[0]\n    plausible_neighbors = {}\n    for i in range(n_vectors):\n        plausible_neighbors[i] = set(\n            list(nbrs[i, np.where(dists[i, :] <= thresh)][0]))\n    return plausible_neighbors\n\n\ndef clusters2labels(clusters, num):\n    labels_ = -1 * np.ones((num), dtype=np.int)\n    for lb, c in enumerate(clusters):\n        idxs = np.array([int(x) for x in list(c)])\n        labels_[idxs] = lb\n    return labels_\n\n\ndef aro(feats, knn, th_sim, num_process, **kwargs):\n    """"""\n    Master function. Takes the descriptor matrix and returns clusters.\n    n_neighbors are the number of nearest neighbors considered and thresh\n    is the clustering distance threshold\n    """"""\n    with Timer(\'[aro] search knn with pyflann\'):\n        nbrs, _ = build_index(feats, n_neighbors=knn)\n    dists = calculate_symmetric_dist(nbrs, num_process)\n    print(\'symmetric dist:\', dists.max(), dists.min(), dists.mean())\n    clusters = aro_clustering(nbrs, dists, 1. - th_sim)\n    labels_ = clusters2labels(clusters, feats.shape[0])\n    return labels_\n\n\ndef knn_aro(feats, prefix, name, knn_method, knn, th_sim, num_process,\n            **kwargs):\n    knn_prefix = os.path.join(prefix, \'knns\', name)\n    knns = build_knns(knn_prefix, feats, knn_method, knn)\n    _, nbrs = knns2ordered_nbrs(knns, sort=False)\n    dists = calculate_symmetric_dist(nbrs, num_process)\n    clusters = aro_clustering(nbrs, dists, 1. - th_sim)\n    labels_ = clusters2labels(clusters, feats.shape[0])\n    return labels_\n'"
baseline/chinese_whispers.py,0,"b'import os\nimport random\nimport numpy as np\nfrom scipy.sparse import identity, csr_matrix\n\nfrom utils import (build_knns, knns2ordered_nbrs, fast_knns2spmat,\n                   build_symmetric_adj, clusters2labels, Timer)\n\n\ndef chinese_whispers(feats, prefix, name, knn_method, knn, th_sim, iters,\n                     **kwargs):\n    """""" Chinese Whispers Clustering Algorithm\n\n    Paper: Chinese whispers: an efficient graph clustering algorithm\n            and its application to natural language processing problems.\n    Reference code:\n        - http://alexloveless.co.uk/data/chinese-whispers-graph-clustering-in-python/\n        - https://github.com/zhly0/facenet-face-cluster-chinese-whispers-\n    """"""\n    import networkx as nx\n\n    assert len(feats) > 1\n\n    with Timer(\'create graph\'):\n        knn_prefix = os.path.join(prefix, \'knns\', name)\n        knns = build_knns(knn_prefix, feats, knn_method, knn)\n        spmat = fast_knns2spmat(knns, knn, th_sim, use_sim=True)\n\n        size = len(feats)\n        nodes = [(n_i, {\'cluster\': n_i}) for n_i in range(size)]\n        c = spmat.tocoo()\n        edges = [(n_i, n_j, {\n            \'weight\': s\n        }) for n_i, n_j, s in zip(c.row, c.col, c.data)]\n\n        G = nx.Graph()\n        G.add_nodes_from(nodes)\n        G.add_edges_from(edges)\n        node_num = G.number_of_nodes()\n        edge_num = G.number_of_edges()\n        assert size == node_num\n        print(\'#nodes: {}, #edges: {}\'.format(node_num, edge_num))\n\n    with Timer(\'whisper iteratively (iters={})\'.format(iters)):\n        cluster_nodes = list(G.nodes())\n        for _ in range(iters):\n            idxs = [i for i in range(node_num)]\n            random.shuffle(idxs)\n            for idx in idxs:\n                node = cluster_nodes[idx]\n                nbrs = G[node]\n                if len(nbrs) == 0:\n                    continue\n                cluster2weight = {}\n                for nbr in nbrs:\n                    assigned_cluster = G.nodes[nbr][\'cluster\']\n                    edge_weight = G[node][nbr][\'weight\']\n                    if assigned_cluster not in cluster2weight:\n                        cluster2weight[assigned_cluster] = 0\n                    cluster2weight[assigned_cluster] += edge_weight\n\n                # set the class of node to its neighbor with largest weight\n                cluster2weight = sorted(cluster2weight.items(),\n                                        key=lambda kv: kv[1],\n                                        reverse=True)\n                G.nodes[node][\'cluster\'] = cluster2weight[0][0]\n\n    clusters = {}\n    for (node, data) in G.nodes.items():\n        assigned_cluster = data[\'cluster\']\n\n        if assigned_cluster not in clusters:\n            clusters[assigned_cluster] = []\n        clusters[assigned_cluster].append(node)\n\n    print(\'#cluster: {}\'.format(len(clusters)))\n    labels = clusters2labels(clusters.values())\n    labels = list(labels.values())\n\n    return labels\n\n\ndef _matrix2array(m):\n    return np.asarray(m).reshape(-1)\n\n\ndef _maxrow(D, n):\n    row = np.arange(n)\n    col = _matrix2array(D.argmax(axis=1))\n    data = _matrix2array(D[row, col])\n    D = csr_matrix((data, (row, col)), shape=(n, n))\n    return D\n\n\ndef chinese_whispers_fast(feats, prefix, name, knn_method, knn, th_sim, iters,\n                          **kwargs):\n    """""" Chinese Whispers Clustering Algorithm\n\n    Paper: Chinese whispers: an efficient graph clustering algorithm\n            and its application to natural language processing problems.\n    This implementation follows the matrix operation as described in Figure.4\n    int the paper. We switch the `maxrow` and `D^{t-1} * A_G` to make it\n    easier for post-processing.\n    The current result is inferior to `chinese_whispers` as it lacks of the\n    random mechanism as the iterative algorithm. The paper introduce two\n    operations to tackle this issue, namely `random mutation` and `keep class`.\n    However, it is not very clear how to set this two hyper-parameters.\n    """"""\n    assert len(feats) > 1\n\n    with Timer(\'create graph\'):\n        knn_prefix = os.path.join(prefix, \'knns\', name)\n        knns = build_knns(knn_prefix, feats, knn_method, knn)\n        spmat = fast_knns2spmat(knns, knn, th_sim, use_sim=True)\n        A = build_symmetric_adj(spmat, self_loop=False)\n\n        node_num = len(feats)\n        edge_num = A.nnz\n        print(\'#nodes: {}, #edges: {}\'.format(node_num, edge_num))\n\n    with Timer(\'whisper iteratively (iters={})\'.format(iters)):\n        D = identity(node_num)\n        for _ in range(iters):\n            D = D * A  # it is equal to D.dot(A)\n            D = _maxrow(D, node_num)\n\n        assert D.nnz == node_num\n\n    clusters = {}\n    assigned_clusters = D.tocoo().col\n    for (node, assigned_cluster) in enumerate(assigned_clusters):\n        if assigned_cluster not in clusters:\n            clusters[assigned_cluster] = []\n        clusters[assigned_cluster].append(node)\n\n    print(\'#cluster: {}\'.format(len(clusters)))\n    labels = clusters2labels(clusters.values())\n    labels = list(labels.values())\n\n    return labels\n'"
baseline/sklearn_cluster.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport sklearn.cluster as cluster\nimport multiprocessing as mp\n\nfrom utils import fast_knns2spmat, build_knns\n\n\ndef kmeans(feat, n_clusters, **kwargs):\n    kmeans = cluster.KMeans(n_clusters=n_clusters,\n                            n_jobs=mp.cpu_count(),\n                            random_state=0).fit(feat)\n    return kmeans.labels_\n\n\ndef mini_batch_kmeans(feat, n_clusters, batch_size, **kwargs):\n    kmeans = cluster.MiniBatchKMeans(n_clusters=n_clusters,\n                                     batch_size=batch_size,\n                                     random_state=0).fit(feat)\n    return kmeans.labels_\n\n\ndef spectral(feat, n_clusters, **kwargs):\n    spectral = cluster.SpectralClustering(n_clusters=n_clusters,\n                                          assign_labels=""discretize"",\n                                          affinity=""nearest_neighbors"",\n                                          random_state=0).fit(feat)\n    return spectral.labels_\n\n\ndef dask_spectral(feat, n_clusters, **kwargs):\n    from dask_ml.cluster import SpectralClustering\n    spectral = SpectralClustering(n_clusters=n_clusters,\n                                  affinity=\'rbf\',\n                                  random_state=0).fit(feat)\n    return spectral.labels_.compute()\n\n\ndef hierarchy(feat, n_clusters, knn, **kwargs):\n    from sklearn.neighbors import kneighbors_graph\n    knn_graph = kneighbors_graph(feat, knn, include_self=False)\n    hierarchy = cluster.AgglomerativeClustering(n_clusters=n_clusters,\n                                                connectivity=knn_graph,\n                                                linkage=\'ward\').fit(feat)\n    return hierarchy.labels_\n\n\ndef fast_hierarchy(feat, distance, hmethod=\'single\', **kwargs):\n    import fastcluster\n    import scipy.cluster\n    links = fastcluster.linkage_vector(feat, method=hmethod)\n    labels_ = scipy.cluster.hierarchy.fcluster(links,\n                                               distance,\n                                               criterion=\'distance\')\n    return labels_\n\n\ndef dbscan(feat, eps, min_samples, **kwargs):\n    db = cluster.DBSCAN(eps=eps,\n                        min_samples=min_samples,\n                        n_jobs=mp.cpu_count()).fit(feat)\n    return db.labels_\n\n\ndef knn_dbscan(feats, eps, min_samples, prefix, name, knn_method, knn, th_sim,\n               **kwargs):\n    knn_prefix = os.path.join(prefix, \'knns\', name)\n    knns = build_knns(knn_prefix, feats, knn_method, knn)\n    sparse_affinity = fast_knns2spmat(knns, knn, th_sim, use_sim=False)\n    db = cluster.DBSCAN(eps=eps,\n                        min_samples=min_samples,\n                        n_jobs=mp.cpu_count(),\n                        metric=\'precomputed\').fit(sparse_affinity)\n    return db.labels_\n\n\ndef hdbscan(feat, min_samples, **kwargs):\n    import hdbscan\n    db = hdbscan.HDBSCAN(min_cluster_size=min_samples)\n    labels_ = db.fit_predict(feat)\n    return labels_\n\n\ndef meanshift(feat, bw, num_process, min_bin_freq, **kwargs):\n    print(\'#num_process:\', num_process)\n    print(\'min_bin_freq:\', min_bin_freq)\n    ms = cluster.MeanShift(bandwidth=bw,\n                           n_jobs=num_process,\n                           min_bin_freq=min_bin_freq).fit(feat)\n    return ms.labels_\n'"
dsgcn/__init__.py,0,"b'from .test_cluster_det import test_cluster_det\nfrom .test_cluster_seg import test_cluster_seg\nfrom .train_cluster_det import train_cluster_det\nfrom .train_cluster_seg import train_cluster_seg\n\n__factory__ = {\n    \'test_det\': test_cluster_det,\n    \'test_seg\': test_cluster_seg,\n    \'train_det\': train_cluster_det,\n    \'train_seg\': train_cluster_seg,\n}\n\n\ndef build_handler(phase, stage):\n    key_handler = \'{}_{}\'.format(phase, stage)\n    if key_handler not in __factory__:\n        raise KeyError(""Unknown op:"", key_handler)\n    return __factory__[key_handler]\n'"
dsgcn/main.py,3,"b""from __future__ import division\n\nimport os\nimport torch\nimport argparse\n\nfrom mmcv import Config\n\nfrom utils import (create_logger, set_random_seed, rm_suffix,\n                   mkdir_if_no_exists)\n\nfrom dsgcn.models import build_model\nfrom dsgcn import build_handler\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Cluster Detection and Segmentation')\n    parser.add_argument('--config', help='train config file path')\n    parser.add_argument('--seed', type=int, default=42, help='random seed')\n    parser.add_argument('--stage', choices=['det', 'seg'], default='det')\n    parser.add_argument('--phase', choices=['test', 'train'], default='test')\n    parser.add_argument('--det_label', choices=['iou', 'iop'], default='iou')\n    parser.add_argument('--work_dir', help='the dir to save logs and models')\n    parser.add_argument('--load_from',\n                        default=None,\n                        help='the checkpoint file to load from')\n    parser.add_argument('--resume_from',\n                        default=None,\n                        help='the checkpoint file to resume from')\n    parser.add_argument('--pred_iou_score',\n                        type=str,\n                        default=None,\n                        help='predicted iou for segmentation post-process')\n    parser.add_argument('--pred_iop_score',\n                        type=str,\n                        default=None,\n                        help='predicted iop for filtering proposals for seg')\n    parser.add_argument(\n        '--gpus',\n        type=int,\n        default=1,\n        help='number of gpus(only applicable to non-distributed training)')\n    parser.add_argument('--distributed', action='store_true', default=False)\n    parser.add_argument('--save_output', action='store_true', default=False)\n    parser.add_argument('--no_cuda', action='store_true', default=False)\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n    cfg = Config.fromfile(args.config)\n\n    # set cuda\n    cfg.cuda = not args.no_cuda and torch.cuda.is_available()\n\n    # set cudnn_benchmark & cudnn_deterministic\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    if cfg.get('cudnn_deterministic', False):\n        torch.backends.cudnn.deterministic = True\n\n    # update configs according to args\n    if not hasattr(cfg, 'work_dir'):\n        if args.work_dir is not None:\n            cfg.work_dir = args.work_dir\n        else:\n            cfg_name = rm_suffix(os.path.basename(args.config))\n            cfg.work_dir = os.path.join('./data/work_dir', cfg_name)\n    mkdir_if_no_exists(cfg.work_dir, is_folder=True)\n    if not hasattr(cfg, 'stage'):\n        cfg.stage = args.stage\n\n    if not hasattr(cfg, 'test_batch_size_per_gpu'):\n        cfg.test_batch_size_per_gpu = cfg.batch_size_per_gpu\n\n    cfg.load_from = args.load_from\n    cfg.resume_from = args.resume_from\n\n    cfg.pred_iou_score = args.pred_iou_score\n    cfg.pred_iop_score = args.pred_iop_score\n\n    cfg.gpus = args.gpus\n    cfg.det_label = args.det_label\n    cfg.distributed = args.distributed\n    cfg.save_output = args.save_output\n\n    logger = create_logger()\n\n    # set random seeds\n    if args.seed is not None:\n        logger.info('Set random seed to {}'.format(args.seed))\n        set_random_seed(args.seed)\n\n    model = build_model(cfg.model['type'], **cfg.model['kwargs'])\n    handler = build_handler(args.phase, args.stage)\n\n    handler(model, cfg, logger)\n\n\nif __name__ == '__main__':\n    main()\n"""
dsgcn/test_cluster_det.py,1,"b""from __future__ import division\n\nimport os\nimport torch\nimport numpy as np\n\nfrom mmcv.runner import load_checkpoint\nfrom mmcv.parallel import MMDataParallel\n\nfrom utils import list2dict, write_meta\nfrom dsgcn.datasets import (build_dataset, build_processor, build_dataloader)\nfrom post_process import deoverlap\nfrom evaluation import evaluate\n\n\ndef test_cluster_det(model, cfg, logger):\n    if cfg.load_from:\n        logger.info('load pretrained model from: {}'.format(cfg.load_from))\n        load_checkpoint(model, cfg.load_from, strict=True, logger=logger)\n\n    for k, v in cfg.model['kwargs'].items():\n        setattr(cfg.test_data, k, v)\n    dataset = build_dataset(cfg.test_data)\n    processor = build_processor(cfg.stage)\n\n    losses = []\n    pred_scores = []\n\n    if cfg.gpus == 1:\n        data_loader = build_dataloader(dataset,\n                                       processor,\n                                       cfg.test_batch_size_per_gpu,\n                                       cfg.workers_per_gpu,\n                                       train=False)\n\n        model = MMDataParallel(model, device_ids=range(cfg.gpus))\n        if cfg.cuda:\n            model.cuda()\n\n        model.eval()\n        for i, data in enumerate(data_loader):\n            with torch.no_grad():\n                output, loss = model(data, return_loss=True)\n                losses += [loss.item()]\n                if i % cfg.log_config.interval == 0:\n                    if dataset.ignore_label:\n                        logger.info('[Test] Iter {}/{}'.format(\n                            i, len(data_loader)))\n                    else:\n                        logger.info('[Test] Iter {}/{}: Loss {:.4f}'.format(\n                            i, len(data_loader), loss))\n                if cfg.save_output:\n                    output = output.view(-1)\n                    prob = output.data.cpu().numpy()\n                    pred_scores.append(prob)\n    else:\n        raise NotImplementedError\n\n    if not dataset.ignore_label:\n        avg_loss = sum(losses) / len(losses)\n        logger.info('[Test] Overall Loss {:.4f}'.format(avg_loss))\n\n    # save predicted scores\n    if cfg.save_output:\n        if cfg.load_from:\n            fn = os.path.basename(cfg.load_from)\n        else:\n            fn = 'random'\n        opath = os.path.join(cfg.work_dir, fn[:fn.rfind('.pth')] + '.npz')\n        meta = {\n            'tot_inst_num': dataset.inst_num,\n            'proposal_folders': cfg.test_data.proposal_folders,\n        }\n        print('dump pred_score to {}'.format(opath))\n        pred_scores = np.concatenate(pred_scores).ravel()\n        np.savez_compressed(opath, data=pred_scores, meta=meta)\n\n    # de-overlap\n    proposals = [fn_node for fn_node, _ in dataset.lst]\n    pred_labels = deoverlap(pred_scores, proposals, dataset.inst_num,\n                            cfg.th_pos, cfg.th_iou)\n\n    # save predicted labels\n    if cfg.save_output:\n        ofn_meta = os.path.join(cfg.work_dir, 'pred_labels.txt')\n        print('save predicted labels to {}'.format(ofn_meta))\n        pred_idx2lb = list2dict(pred_labels, ignore_value=-1)\n        write_meta(ofn_meta, pred_idx2lb, inst_num=dataset.inst_num)\n\n    # evaluation\n    if not dataset.ignore_label:\n        print('==> evaluation')\n        gt_labels = dataset.labels\n        for metric in cfg.metrics:\n            evaluate(gt_labels, pred_labels, metric)\n"""
dsgcn/test_cluster_seg.py,2,"b""from __future__ import division\n\nimport glob\nimport torch\nimport numpy as np\nimport os.path as osp\nimport torch.nn.functional as F\n\nfrom mmcv.runner import load_checkpoint\nfrom mmcv.parallel import MMDataParallel\n\nfrom utils import list2dict, write_meta\nfrom dsgcn.datasets import (build_dataset, build_processor, build_dataloader)\nfrom post_process import deoverlap\nfrom evaluation import evaluate\n\n\ndef test_cluster_seg(model, cfg, logger):\n    assert osp.isfile(cfg.pred_iou_score)\n\n    if cfg.load_from:\n        logger.info('load pretrained model from: {}'.format(cfg.load_from))\n        load_checkpoint(model, cfg.load_from, strict=True, logger=logger)\n\n    for k, v in cfg.model['kwargs'].items():\n        setattr(cfg.test_data, k, v)\n\n    setattr(cfg.test_data, 'pred_iop_score', cfg.pred_iop_score)\n\n    dataset = build_dataset(cfg.test_data)\n    processor = build_processor(cfg.stage)\n\n    inst_num = dataset.inst_num\n\n    # read pred_scores from file and do sanity check\n    d = np.load(cfg.pred_iou_score, allow_pickle=True)\n    pred_scores = d['data']\n    meta = d['meta'].item()\n    assert inst_num == meta['tot_inst_num'], '{} vs {}'.format(\n        inst_num, meta['tot_inst_num'])\n\n    proposals = [fn_node for fn_node, _ in dataset.tot_lst]\n    _proposals = []\n    fn_node_pattern = '*_node.npz'\n    for proposal_folder in meta['proposal_folders']:\n        fn_clusters = sorted(\n            glob.glob(osp.join(proposal_folder, fn_node_pattern)))\n        _proposals.extend([fn_node for fn_node in fn_clusters])\n    assert proposals == _proposals, '{} vs {}'.format(len(proposals),\n                                                      len(_proposals))\n\n    losses = []\n    pred_outlier_scores = []\n    stats = {'mean': []}\n\n    if cfg.gpus == 1:\n        data_loader = build_dataloader(dataset,\n                                       processor,\n                                       cfg.test_batch_size_per_gpu,\n                                       cfg.workers_per_gpu,\n                                       train=False)\n\n        model = MMDataParallel(model, device_ids=range(cfg.gpus))\n        if cfg.cuda:\n            model.cuda()\n\n        model.eval()\n        for i, data in enumerate(data_loader):\n            with torch.no_grad():\n                output, loss = model(data, return_loss=True)\n                losses += [loss.item()]\n                if i % cfg.log_config.interval == 0:\n                    if dataset.ignore_label:\n                        logger.info('[Test] Iter {}/{}'.format(\n                            i, len(data_loader)))\n                    else:\n                        logger.info('[Test] Iter {}/{}: Loss {:.4f}'.format(\n                            i, len(data_loader), loss))\n                if cfg.save_output:\n                    output = F.softmax(output, dim=1)\n                    output = output[:, 1, :]\n                    scores = output.data.cpu().numpy()\n                    pred_outlier_scores.extend(list(scores))\n                    stats['mean'] += [scores.mean()]\n    else:\n        raise NotImplementedError\n\n    if not dataset.ignore_label:\n        avg_loss = sum(losses) / len(losses)\n        logger.info('[Test] Overall Loss {:.4f}'.format(avg_loss))\n\n    scores_mean = 1. * sum(stats['mean']) / len(stats['mean'])\n    logger.info('mean of pred_outlier_scores: {:.4f}'.format(scores_mean))\n\n    # save predicted scores\n    if cfg.save_output:\n        if cfg.load_from:\n            fn = osp.basename(cfg.load_from)\n        else:\n            fn = 'random'\n        opath = osp.join(cfg.work_dir, fn[:fn.rfind('.pth')] + '.npz')\n        meta = {\n            'tot_inst_num': inst_num,\n            'proposal_folders': cfg.test_data.proposal_folders,\n        }\n        logger.info('dump pred_outlier_scores ({}) to {}'.format(\n            len(pred_outlier_scores), opath))\n        np.savez_compressed(opath, data=pred_outlier_scores, meta=meta)\n\n    # post-process\n    outlier_scores = {\n        fn_node: outlier_score\n        for (fn_node,\n             _), outlier_score in zip(dataset.lst, pred_outlier_scores)\n    }\n\n    # de-overlap (w gcn-s)\n    pred_labels_w_seg = deoverlap(pred_scores,\n                                  proposals,\n                                  inst_num,\n                                  cfg.th_pos,\n                                  cfg.th_iou,\n                                  outlier_scores=outlier_scores,\n                                  th_outlier=cfg.th_outlier,\n                                  keep_outlier=cfg.keep_outlier)\n\n    # de-overlap (wo gcn-s)\n    pred_labels_wo_seg = deoverlap(pred_scores, proposals, inst_num,\n                                   cfg.th_pos, cfg.th_iou)\n\n    # save predicted labels\n    if cfg.save_output:\n        ofn_meta_w_seg = osp.join(cfg.work_dir, 'pred_labels_w_seg.txt')\n        ofn_meta_wo_seg = osp.join(cfg.work_dir, 'pred_labels_wo_seg.txt')\n        print('save predicted labels to {} and {}'.format(\n            ofn_meta_w_seg, ofn_meta_wo_seg))\n        pred_idx2lb_w_seg = list2dict(pred_labels_w_seg, ignore_value=-1)\n        pred_idx2lb_wo_seg = list2dict(pred_labels_wo_seg, ignore_value=-1)\n        write_meta(ofn_meta_w_seg, pred_idx2lb_w_seg, inst_num=inst_num)\n        write_meta(ofn_meta_wo_seg, pred_idx2lb_wo_seg, inst_num=inst_num)\n\n    # evaluation\n    if not dataset.ignore_label:\n        gt_labels = dataset.labels\n        print('==> evaluation (with gcn-s)')\n        for metric in cfg.metrics:\n            evaluate(gt_labels, pred_labels_w_seg, metric)\n        print('==> evaluation (without gcn-s)')\n        for metric in cfg.metrics:\n            evaluate(gt_labels, pred_labels_wo_seg, metric)\n"""
dsgcn/train.py,1,"b'from __future__ import division\n\nimport torch\nfrom mmcv.runner import DistSamplerSeedHook, obj_from_dict\nfrom mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n\nfrom dsgcn.datasets import build_dataset, build_processor, build_dataloader\nfrom dsgcn.runner import Runner\n\n\ndef train_cluster(model, cfg, logger, batch_processor):\n    # prepare data loaders\n    for k, v in cfg.model[\'kwargs\'].items():\n        setattr(cfg.train_data, k, v)\n\n    dataset = build_dataset(cfg.train_data)\n    assert not dataset.ignore_label, \'Please specify label_path for training\'\n\n    processor = build_processor(cfg.stage)\n    data_loaders = [\n        build_dataloader(dataset,\n                         processor,\n                         cfg.batch_size_per_gpu,\n                         cfg.workers_per_gpu,\n                         train=True,\n                         shuffle=True)\n    ]\n\n    # train\n    if cfg.distributed:\n        _dist_train(model, data_loaders, batch_processor, cfg)\n    else:\n        _single_train(model, data_loaders, batch_processor, cfg)\n\n\ndef build_optimizer(model, optimizer_cfg):\n    """"""Build optimizer from configs.\n    """"""\n    if hasattr(model, \'module\'):\n        model = model.module\n\n    optimizer_cfg = optimizer_cfg.copy()\n    paramwise_options = optimizer_cfg.pop(\'paramwise_options\', None)\n    assert paramwise_options is None\n    return obj_from_dict(optimizer_cfg, torch.optim,\n                         dict(params=model.parameters()))\n\n\ndef _dist_train(model, data_loaders, batch_processor, cfg):\n    # put model on gpus\n    model = MMDistributedDataParallel(model.cuda())\n    # build runner\n    optimizer = build_optimizer(model, cfg.optimizer)\n    runner = Runner(model, batch_processor, optimizer, cfg.work_dir,\n                    cfg.log_level)\n    # register hooks\n    optimizer_config = DistOptimizerHook(**cfg.optimizer_config)\n    runner.register_training_hooks(cfg.lr_config, optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n    runner.register_hook(DistSamplerSeedHook())\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n\n\ndef _single_train(model, data_loaders, batch_processor, cfg):\n    if cfg.gpus > 1:\n        raise NotImplemented\n    # put model on gpus\n    model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()\n    # build runner\n    optimizer = build_optimizer(model, cfg.optimizer)\n    runner = Runner(model,\n                    batch_processor,\n                    optimizer,\n                    cfg.work_dir,\n                    cfg.log_level,\n                    iter_size=cfg.iter_size)\n    runner.register_training_hooks(cfg.lr_config, cfg.optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n'"
dsgcn/train_cluster_det.py,0,"b""from collections import OrderedDict\n\nfrom dsgcn.train import train_cluster\n\n\ndef batch_processor(model, data, train_mode):\n    assert train_mode\n    _, loss = model(data, return_loss=True)\n    log_vars = OrderedDict()\n    log_vars['loss'] = loss.item()\n\n    outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data[-1]))\n\n    return outputs\n\n\ndef train_cluster_det(model, cfg, logger):\n    train_cluster(model, cfg, logger, batch_processor)\n"""
dsgcn/train_cluster_seg.py,1,"b""import torch\n\nfrom collections import OrderedDict\n\nfrom dsgcn.train import train_cluster\nfrom evaluation import accuracy\n\n\ndef batch_processor(model, data, train_mode):\n    assert train_mode\n    pred, loss = model(data, return_loss=True)\n\n    log_vars = OrderedDict()\n    log_vars['loss'] = loss.item()\n    _, _, gt_labels = data\n    # TODO: remove pad_label when computing batch accuracy\n    pred_labels = torch.argmax(pred.cpu(), dim=1).long()\n    gt_labels = gt_labels.cpu().numpy()\n    pred_labels = pred_labels.numpy()\n    log_vars['acc'] = accuracy(gt_labels, pred_labels)\n\n    outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data[-1]))\n\n    return outputs\n\n\ndef train_cluster_seg(model, cfg, logger):\n    train_cluster(model, cfg, logger, batch_processor)\n"""
evaluation/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom .metrics import *\nfrom .evaluate import evaluate\n'
evaluation/evaluate.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport inspect\nimport argparse\nimport numpy as np\n\nimport evaluation.metrics as metrics\nfrom utils import Timer, TextColors\n\n\ndef _read_meta(fn):\n    labels = list()\n    lb_set = set()\n    with open(fn) as f:\n        for lb in f.readlines():\n            lb = int(lb.strip())\n            labels.append(lb)\n            lb_set.add(lb)\n    return np.array(labels), lb_set\n\n\ndef evaluate(gt_labels, pred_labels, metric='pairwise'):\n    if isinstance(gt_labels, str) and isinstance(pred_labels, str):\n        print('[gt_labels] {}'.format(gt_labels))\n        print('[pred_labels] {}'.format(pred_labels))\n        gt_labels, gt_lb_set = _read_meta(gt_labels)\n        pred_labels, pred_lb_set = _read_meta(pred_labels)\n\n        print('#inst: gt({}) vs pred({})'.format(len(gt_labels),\n                                                 len(pred_labels)))\n        print('#cls: gt({}) vs pred({})'.format(len(gt_lb_set),\n                                                len(pred_lb_set)))\n\n    metric_func = metrics.__dict__[metric]\n\n    with Timer('evaluate with {}{}{}'.format(TextColors.FATAL, metric,\n                                             TextColors.ENDC)):\n        result = metric_func(gt_labels, pred_labels)\n    if isinstance(result, np.float):\n        print('{}{}: {:.4f}{}'.format(TextColors.OKGREEN, metric, result,\n                                      TextColors.ENDC))\n    else:\n        ave_pre, ave_rec, fscore = result\n        print('{}ave_pre: {:.4f}, ave_rec: {:.4f}, fscore: {:.4f}{}'.format(\n            TextColors.OKGREEN, ave_pre, ave_rec, fscore, TextColors.ENDC))\n\n\nif __name__ == '__main__':\n    metric_funcs = inspect.getmembers(metrics, inspect.isfunction)\n    metric_names = [n for n, _ in metric_funcs]\n\n    parser = argparse.ArgumentParser(description='Evaluate Cluster')\n    parser.add_argument('--gt_labels', type=str, required=True)\n    parser.add_argument('--pred_labels', type=str, required=True)\n    parser.add_argument('--metric', default='pairwise', choices=metric_names)\n    args = parser.parse_args()\n\n    evaluate(args.gt_labels, args.pred_labels, args.metric)\n"""
evaluation/metrics.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import division\n\nimport numpy as np\nfrom sklearn.metrics.cluster import (contingency_matrix,\n                                     normalized_mutual_info_score)\nfrom sklearn.metrics import (precision_score, recall_score)\n\n__all__ = [\'pairwise\', \'bcubed\', \'nmi\', \'precision\', \'recall\', \'accuracy\']\n\n\ndef _check(gt_labels, pred_labels):\n    if gt_labels.ndim != 1:\n        raise ValueError(""gt_labels must be 1D: shape is %r"" %\n                         (gt_labels.shape, ))\n    if pred_labels.ndim != 1:\n        raise ValueError(""pred_labels must be 1D: shape is %r"" %\n                         (pred_labels.shape, ))\n    if gt_labels.shape != pred_labels.shape:\n        raise ValueError(\n            ""gt_labels and pred_labels must have same size, got %d and %d"" %\n            (gt_labels.shape[0], pred_labels.shape[0]))\n    return gt_labels, pred_labels\n\n\ndef _get_lb2idxs(labels):\n    lb2idxs = {}\n    for idx, lb in enumerate(labels):\n        if lb not in lb2idxs:\n            lb2idxs[lb] = []\n        lb2idxs[lb].append(idx)\n    return lb2idxs\n\n\ndef _compute_fscore(pre, rec):\n    return 2. * pre * rec / (pre + rec)\n\n\ndef fowlkes_mallows_score(gt_labels, pred_labels, sparse=True):\n    \'\'\' The original function is from `sklearn.metrics.fowlkes_mallows_score`.\n        We output the pairwise precision, pairwise recall and F-measure,\n        instead of calculating the geometry mean of precision and recall.\n    \'\'\'\n    n_samples, = gt_labels.shape\n\n    c = contingency_matrix(gt_labels, pred_labels, sparse=sparse)\n    tk = np.dot(c.data, c.data) - n_samples\n    pk = np.sum(np.asarray(c.sum(axis=0)).ravel()**2) - n_samples\n    qk = np.sum(np.asarray(c.sum(axis=1)).ravel()**2) - n_samples\n\n    avg_pre = tk / pk\n    avg_rec = tk / qk\n    fscore = _compute_fscore(avg_pre, avg_rec)\n\n    return avg_pre, avg_rec, fscore\n\n\ndef pairwise(gt_labels, pred_labels, sparse=True):\n    _check(gt_labels, pred_labels)\n    return fowlkes_mallows_score(gt_labels, pred_labels, sparse)\n\n\ndef bcubed(gt_labels, pred_labels):\n    _check(gt_labels, pred_labels)\n\n    gt_lb2idxs = _get_lb2idxs(gt_labels)\n    pred_lb2idxs = _get_lb2idxs(pred_labels)\n\n    num_lbs = len(gt_lb2idxs)\n    pre = np.zeros(num_lbs)\n    rec = np.zeros(num_lbs)\n    gt_num = np.zeros(num_lbs)\n\n    for i, gt_idxs in enumerate(gt_lb2idxs.values()):\n        all_pred_lbs = np.unique(pred_labels[gt_idxs])\n        gt_num[i] = len(gt_idxs)\n        for pred_lb in all_pred_lbs:\n            pred_idxs = pred_lb2idxs[pred_lb]\n            n = 1. * np.intersect1d(gt_idxs, pred_idxs).size\n            pre[i] += n**2 / len(pred_idxs)\n            rec[i] += n**2 / gt_num[i]\n\n    gt_num = gt_num.sum()\n    avg_pre = pre.sum() / gt_num\n    avg_rec = rec.sum() / gt_num\n    fscore = _compute_fscore(avg_pre, avg_rec)\n\n    return avg_pre, avg_rec, fscore\n\n\ndef nmi(gt_labels, pred_labels):\n    return normalized_mutual_info_score(pred_labels, gt_labels)\n\n\ndef precision(gt_labels, pred_labels):\n    return precision_score(gt_labels, pred_labels)\n\n\ndef recall(gt_labels, pred_labels):\n    return recall_score(gt_labels, pred_labels)\n\n\ndef accuracy(gt_labels, pred_labels):\n    return np.mean(gt_labels == pred_labels)\n'"
lgcn/__init__.py,0,"b'from .test_lgcn import test_lgcn\nfrom .train_lgcn import train_lgcn\n\n__factory__ = {\n    \'test_lgcn\': test_lgcn,\n    \'train_lgcn\': train_lgcn,\n}\n\n\ndef build_handler(phase):\n    key_handler = \'{}_lgcn\'.format(phase)\n    if key_handler not in __factory__:\n        raise KeyError(""Unknown op:"", key_handler)\n    return __factory__[key_handler]\n'"
lgcn/main.py,3,"b""from __future__ import division\n\nimport os\nimport torch\nimport argparse\n\nfrom mmcv import Config\n\nfrom utils import (create_logger, set_random_seed, rm_suffix,\n                   mkdir_if_no_exists)\n\nfrom lgcn.models import build_model\nfrom lgcn import build_handler\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Linkage-based Face Clustering via GCN')\n    parser.add_argument('--config', help='config file path')\n    parser.add_argument('--seed', type=int, default=1, help='random seed')\n    parser.add_argument('--phase', choices=['test', 'train'], default='test')\n    parser.add_argument('--work_dir', help='the dir to save logs and models')\n    parser.add_argument('--load_from',\n                        default=None,\n                        help='the checkpoint file to load from')\n    parser.add_argument('--resume_from',\n                        default=None,\n                        help='the checkpoint file to resume from')\n    parser.add_argument(\n        '--gpus',\n        type=int,\n        default=1,\n        help='number of gpus(only applicable to non-distributed training)')\n    parser.add_argument('--distributed', action='store_true', default=False)\n    parser.add_argument('--save_output', action='store_true', default=False)\n    parser.add_argument('--no_cuda', action='store_true', default=False)\n    parser.add_argument('--force', action='store_true', default=False)\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n    cfg = Config.fromfile(args.config)\n\n    # set cuda\n    cfg.cuda = not args.no_cuda and torch.cuda.is_available()\n\n    # set cudnn_benchmark & cudnn_deterministic\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    if cfg.get('cudnn_deterministic', False):\n        torch.backends.cudnn.deterministic = True\n\n    # update configs according to args\n    if not hasattr(cfg, 'work_dir'):\n        if args.work_dir is not None:\n            cfg.work_dir = args.work_dir\n        else:\n            cfg_name = rm_suffix(os.path.basename(args.config))\n            cfg.work_dir = os.path.join('./data/work_dir', cfg_name)\n    mkdir_if_no_exists(cfg.work_dir, is_folder=True)\n\n    cfg.load_from = args.load_from\n    cfg.resume_from = args.resume_from\n\n    cfg.gpus = args.gpus\n    cfg.distributed = args.distributed\n    cfg.save_output = args.save_output\n    cfg.force = args.force\n\n    logger = create_logger()\n\n    # set random seeds\n    if args.seed is not None:\n        logger.info('Set random seed to {}'.format(args.seed))\n        set_random_seed(args.seed)\n\n    model = build_model(cfg.model['type'], **cfg.model['kwargs'])\n    handler = build_handler(args.phase)\n\n    handler(model, cfg, logger)\n\n\nif __name__ == '__main__':\n    main()\n"""
lgcn/online_evaluation.py,1,"b'import torch\n\nfrom evaluation import precision, recall, accuracy\n\n\ndef online_evaluate(gtmat, pred):\n    pred_labels = torch.argmax(pred.cpu(), dim=1).long()\n    gt_labels = gtmat.view(-1).cpu().numpy()\n    pred_labels = pred_labels.numpy()\n    acc = accuracy(gt_labels, pred_labels)\n    pre = precision(gt_labels, pred_labels)\n    rec = recall(gt_labels, pred_labels)\n    return acc, pre, rec\n'"
lgcn/test_lgcn.py,2,"b""from __future__ import division\n\nimport os\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\n\nfrom mmcv.runner import load_checkpoint\nfrom mmcv.parallel import MMDataParallel\n\nfrom lgcn.datasets import build_dataset, build_dataloader\nfrom lgcn.online_evaluation import online_evaluate\n\nfrom utils import (clusters2labels, intdict2ndarray, get_cluster_idxs,\n                   write_meta)\nfrom proposals.graph import graph_clustering_dynamic_th\nfrom evaluation import evaluate\n\n\ndef test(model, dataset, cfg, logger):\n    if cfg.load_from:\n        print('load from {}'.format(cfg.load_from))\n        load_checkpoint(model, cfg.load_from, strict=True, logger=logger)\n\n    losses = []\n    edges = []\n    scores = []\n\n    if cfg.gpus == 1:\n        data_loader = build_dataloader(dataset,\n                                       cfg.batch_size_per_gpu,\n                                       cfg.workers_per_gpu,\n                                       train=False)\n\n        model = MMDataParallel(model, device_ids=range(cfg.gpus))\n        if cfg.cuda:\n            model.cuda()\n\n        model.eval()\n        for i, (data, cid, node_list) in enumerate(data_loader):\n            with torch.no_grad():\n                _, _, h1id, gtmat = data\n                pred, loss = model(data, return_loss=True)\n                losses += [loss.item()]\n                pred = F.softmax(pred, dim=1)\n                if i % cfg.log_config.interval == 0:\n                    if dataset.ignore_label:\n                        logger.info('[Test] Iter {}/{}'.format(\n                            i, len(data_loader)))\n                    else:\n                        acc, p, r = online_evaluate(gtmat, pred)\n                        logger.info(\n                            '[Test] Iter {}/{}: Loss {:.4f}, '\n                            'Accuracy {:.4f}, Precision {:.4f}, Recall {:.4f}'.\n                            format(i, len(data_loader), loss, acc, p, r))\n\n                node_list = node_list.numpy()\n                bs = len(cid)\n                h1id_num = len(h1id[0])\n                for b in range(bs):\n                    cidb = cid[b].int().item()\n                    nlst = node_list[b]\n                    center_idx = nlst[cidb]\n                    for j, n in enumerate(h1id[b]):\n                        edges.append([center_idx, nlst[n.item()]])\n                        scores.append(pred[b * h1id_num + j, 1].item())\n    else:\n        raise NotImplementedError\n\n    if not dataset.ignore_label:\n        avg_loss = sum(losses) / len(losses)\n        logger.info('[Test] Overall Loss {:.4f}'.format(avg_loss))\n\n    return np.array(edges), np.array(scores), len(dataset)\n\n\ndef test_lgcn(model, cfg, logger):\n    for k, v in cfg.model['kwargs'].items():\n        setattr(cfg.test_data, k, v)\n    dataset = build_dataset(cfg.test_data)\n\n    ofn_pred = os.path.join(cfg.work_dir, 'pred_edges_scores.npz')\n    if os.path.isfile(ofn_pred) and not cfg.force:\n        data = np.load(ofn_pred)\n        edges = data['edges']\n        scores = data['scores']\n        inst_num = data['inst_num']\n        if inst_num != len(dataset):\n            logger.warn(\n                'instance number in {} is different from dataset: {} vs {}'.\n                format(ofn_pred, inst_num, len(dataset)))\n    else:\n        edges, scores, inst_num = test(model, dataset, cfg, logger)\n\n    # produce predicted labels\n    clusters = graph_clustering_dynamic_th(edges,\n                                           scores,\n                                           max_sz=cfg.max_sz,\n                                           step=cfg.step,\n                                           pool=cfg.pool)\n    pred_idx2lb = clusters2labels(clusters)\n    pred_labels = intdict2ndarray(pred_idx2lb)\n\n    if cfg.save_output:\n        print('save predicted edges and scores to {}'.format(ofn_pred))\n        np.savez_compressed(ofn_pred,\n                            edges=edges,\n                            scores=scores,\n                            inst_num=inst_num)\n        ofn_meta = os.path.join(cfg.work_dir, 'pred_labels.txt')\n        write_meta(ofn_meta, pred_idx2lb, inst_num=inst_num)\n\n    # evaluation\n    if not dataset.ignore_label:\n        print('==> evaluation')\n        gt_labels = dataset.labels\n        for metric in cfg.metrics:\n            evaluate(gt_labels, pred_labels, metric)\n\n        single_cluster_idxs = get_cluster_idxs(clusters, size=1)\n        print('==> evaluation (removing {} single clusters)'.format(\n            len(single_cluster_idxs)))\n        remain_idxs = np.setdiff1d(np.arange(len(dataset)),\n                                   np.array(single_cluster_idxs))\n        remain_idxs = np.array(remain_idxs)\n        for metric in cfg.metrics:\n            evaluate(gt_labels[remain_idxs], pred_labels[remain_idxs], metric)\n"""
lgcn/train_lgcn.py,1,"b'from __future__ import division\n\nfrom collections import OrderedDict\n\nimport torch\nfrom mmcv.runner import Runner, obj_from_dict\nfrom mmcv.parallel import MMDataParallel\nfrom lgcn.datasets import build_dataset, build_dataloader\nfrom lgcn.online_evaluation import online_evaluate\n\n\ndef batch_processor(model, data, train_mode):\n    assert train_mode\n\n    pred, loss = model(data, return_loss=True)\n\n    log_vars = OrderedDict()\n    _, _, _, gtmat = data\n    acc, p, r = online_evaluate(gtmat, pred)\n    log_vars[\'loss\'] = loss.item()\n    log_vars[\'accuracy\'] = acc\n    log_vars[\'precision\'] = p\n    log_vars[\'recall\'] = r\n\n    outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(gtmat))\n\n    return outputs\n\n\ndef train_lgcn(model, cfg, logger):\n    # prepare data loaders\n    for k, v in cfg.model[\'kwargs\'].items():\n        setattr(cfg.train_data, k, v)\n    dataset = build_dataset(cfg.train_data)\n    data_loaders = [\n        build_dataloader(dataset,\n                         cfg.batch_size_per_gpu,\n                         cfg.workers_per_gpu,\n                         train=True,\n                         shuffle=True)\n    ]\n\n    # train\n    if cfg.distributed:\n        raise NotImplementedError\n    else:\n        _single_train(model, data_loaders, cfg)\n\n\ndef build_optimizer(model, optimizer_cfg):\n    """"""Build optimizer from configs.\n    """"""\n    if hasattr(model, \'module\'):\n        model = model.module\n\n    optimizer_cfg = optimizer_cfg.copy()\n    paramwise_options = optimizer_cfg.pop(\'paramwise_options\', None)\n    assert paramwise_options is None\n    return obj_from_dict(optimizer_cfg, torch.optim,\n                         dict(params=model.parameters()))\n\n\ndef _single_train(model, data_loaders, cfg):\n    if cfg.gpus > 1:\n        raise NotImplemented\n    # put model on gpus\n    model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()\n    # build runner\n    optimizer = build_optimizer(model, cfg.optimizer)\n    runner = Runner(model, batch_processor, optimizer, cfg.work_dir,\n                    cfg.log_level)\n    runner.register_training_hooks(cfg.lr_config, cfg.optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n'"
post_process/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom .nms import nms\nfrom .deoverlap import deoverlap\n'
post_process/deoverlap.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport glob\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom utils import load_data, write_meta\nfrom post_process import nms\n\n\ndef deoverlap(scores,\n              proposals,\n              tot_inst_num,\n              th_pos=-1,\n              th_iou=1,\n              pred_label_fn=None,\n              outlier_scores=None,\n              th_outlier=0.5,\n              keep_outlier=False):\n    print('avg_score(mean: {:.2f}, max: {:.2f}, min: {:.2f})'.format(\n        scores.mean(), scores.max(), scores.min()))\n\n    assert len(proposals) == len(scores), '{} vs {}'.format(\n        len(proposals), len(scores))\n    assert (outlier_scores is None) or isinstance(outlier_scores, dict)\n\n    pos_lst = []\n    for idx, prob in enumerate(scores):\n        if prob < th_pos:\n            continue\n        pos_lst.append([idx, prob])\n    pos_lst = sorted(pos_lst, key=lambda x: x[1], reverse=True)\n\n    # get all clusters\n    clusters = []\n    if keep_outlier:\n        o_clusters = []\n    for idx, _ in tqdm(pos_lst):\n        fn_node = proposals[idx]\n        cluster = load_data(fn_node)\n        cluster, o_cluster = filter_outlier(cluster, fn_node, outlier_scores, th_outlier)\n        clusters.append(cluster)\n        if keep_outlier and len(o_cluster) > 0:\n            o_clusters.append(o_cluster)\n\n    if keep_outlier:\n        print('#outlier_clusters: {}'.format(len(o_clusters)))\n        clusters.extend(o_clusters)\n\n    idx2lb, idx2lbs = nms(clusters, th_iou)\n\n    # output stats\n    multi_lb_num = 0\n    for _, lbs in idx2lbs.items():\n        if len(lbs) > 1:\n            multi_lb_num += 1\n    inst_num = len(idx2lb)\n    cls_num = len(set(idx2lb.values()))\n\n    print('#inst: {}, #class: {}, #multi-label: {}'.format(\n        inst_num, cls_num, multi_lb_num))\n    print('#inst-coverage: {:.2f}'.format(1. * inst_num / tot_inst_num))\n\n    # save to file\n    pred_labels = write_meta(pred_label_fn, idx2lb, inst_num=tot_inst_num)\n\n    return pred_labels\n\n\ndef filter_outlier(cluster, fn_node, outlier_scores, th_outlier):\n    if outlier_scores is None or fn_node not in outlier_scores:\n        return cluster, []\n    outlier_prob = outlier_scores[fn_node]\n\n    # `outlier_prob` may have large size due to padding\n    size = len(cluster)\n    if len(outlier_prob) > size:\n        outlier_prob = outlier_prob[:size]\n\n    comp = outlier_prob > th_outlier\n    clean_idxs = np.where(comp)[0]\n    outlier_idxs = np.where(~comp)[0]\n    return cluster[clean_idxs], cluster[outlier_idxs]\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Super-vertex Deoverlap')\n    parser.add_argument('--pred_score', type=str)\n    parser.add_argument('--th_pos', default=-1, type=float)\n    parser.add_argument('--th_iou', default=1, type=float)\n    parser.add_argument('--output_name', default='', type=str)\n    parser.add_argument('--force', action='store_true')\n    args = parser.parse_args()\n\n    assert args.th_iou >= 0\n\n    assert args.pred_score.endswith('.npz')\n    if args.output_name == '':\n        pos = args.pred_score.rfind('.npz')\n        pred_label_fn = '{}_th_iou_{}_pos_{}_pred_label.txt'.format(\n            args.pred_score[:pos], args.th_iou, args.th_pos)\n    else:\n        pred_label_fn = args.output_name\n\n    print('th_pos={}, th_iou={}, pred_score={}, pred_label_fn={}'.format(\n        args.th_pos, args.th_iou, args.pred_score, pred_label_fn))\n\n    d = np.load(args.pred_score, allow_pickle=True)\n    scores = d['data']\n    meta = d['meta'].item()\n    proposal_folders = meta['proposal_folders']\n    tot_inst_num = meta['tot_inst_num']\n\n    # read proposals\n    proposals = []\n    fn_node_pattern = '*_node.npz'\n    for proposal_folder in proposal_folders:\n        fn_clusters = sorted(\n            glob.glob(os.path.join(proposal_folder, fn_node_pattern)))\n        proposals.extend([fn_node for fn_node in fn_clusters])\n\n    deoverlap(scores, proposals, tot_inst_num, args.th_pos, args.th_iou,\n              pred_label_fn)\n"""
post_process/nms.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport time\n\nfrom proposals import compute_iou\n\n\ndef nms(clusters, th=1.):\n    # nms\n    t0 = time.time()\n    suppressed = set()\n    if th < 1:\n        start_idx = 0\n        tot_size = len(clusters)\n        while start_idx < tot_size:\n            if start_idx in suppressed:\n                start_idx += 1\n                continue\n            cluster = clusters[start_idx]\n            for j in range(start_idx + 1, tot_size):\n                if j in suppressed:\n                    continue\n                if compute_iou(cluster, clusters[j]) > th:\n                    suppressed.add(j)\n            start_idx += 1\n    else:\n        print('th={} >= 1, skip the nms'.format(th))\n    print('nms consumes {} s'.format(time.time() - t0))\n\n    # assign label\n    lb = 0\n    idx2lbs = {}\n    for i, cluster in enumerate(clusters):\n        if i in suppressed:\n            continue\n        for v in cluster:\n            if v not in idx2lbs:\n                idx2lbs[v] = []\n            idx2lbs[v].append(lb)\n        lb += 1\n\n    # deoverlap (choose the one belongs to the highest predicted iou)\n    idx2lb = {}\n    for idx, lbs in idx2lbs.items():\n        idx2lb[idx] = lbs[0]\n\n    return idx2lb, idx2lbs\n"""
proposals/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom .metrics import *\nfrom .super_vertex import super_vertex\nfrom .stat_cluster import get_majority\nfrom .generate_basic_proposals import (filter_clusters, save_proposals,\n                                       generate_basic_proposals)\nfrom .generate_iter_proposals import generate_iter_proposals\nfrom .generate_proposals import generate_proposals\n'"
proposals/generate_basic_proposals.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport argparse\nimport os.path as osp\nfrom tqdm import tqdm\n\nfrom utils import (dump_data, read_meta, write_meta, build_knns,\n                   filter_clusters, labels2clusters, clusters2labels,\n                   BasicDataset, Timer)\nfrom proposals import super_vertex\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Generate Basic Proposals\')\n    parser.add_argument(""--name"",\n                        type=str,\n                        default=\'part1_test\',\n                        help=""image features"")\n    parser.add_argument(""--prefix"",\n                        type=str,\n                        default=\'./data\',\n                        help=""prefix of dataset"")\n    parser.add_argument(""--oprefix"",\n                        type=str,\n                        default=\'./data/cluster_proposals\',\n                        help=""prefix of saving super vertx"")\n    parser.add_argument(""--dim"",\n                        type=int,\n                        default=256,\n                        help=""dimension of feature"")\n    parser.add_argument(""--no_normalize"",\n                        action=\'store_true\',\n                        help=""normalize feature by default"")\n    parser.add_argument(\'--k\', default=80, type=int)\n    parser.add_argument(\'--th_knn\', default=0.7, type=float)\n    parser.add_argument(\'--th_step\', default=0.05, type=float)\n    parser.add_argument(\'--knn_method\',\n                        default=\'faiss\',\n                        choices=[\'faiss\', \'hnsw\'])\n    parser.add_argument(\'--maxsz\', default=300, type=int)\n    parser.add_argument(\'--minsz\', default=3, type=int)\n    parser.add_argument(\'--is_rebuild\', action=\'store_true\')\n    parser.add_argument(\'--is_save_proposals\', action=\'store_true\')\n    parser.add_argument(\'--force\', action=\'store_true\')\n    args = parser.parse_args()\n\n    return args\n\n\ndef save_proposals(clusters, knns, ofolder, force=False):\n    for lb, nodes in enumerate(tqdm(clusters)):\n        opath_node = osp.join(ofolder, \'{}_node.npz\'.format(lb))\n        opath_edge = osp.join(ofolder, \'{}_edge.npz\'.format(lb))\n        if not force and osp.exists(opath_node) and osp.exists(opath_edge):\n            continue\n        nodes = set(nodes)\n        edges = []\n        visited = set()\n        # get edges from knn\n        for idx in nodes:\n            ners, dists = knns[idx]\n            for n, dist in zip(ners, dists):\n                if n == idx or n not in nodes:\n                    continue\n                idx1, idx2 = (idx, n) if idx < n else (n, idx)\n                key = \'{}-{}\'.format(idx1, idx2)\n                if key not in visited:\n                    visited.add(key)\n                    edges.append([idx1, idx2, dist])\n        # save to npz file\n        nodes = list(nodes)\n        dump_data(opath_node, data=nodes, force=force)\n        dump_data(opath_edge, data=edges, force=force)\n\n\ndef generate_basic_proposals(oprefix,\n                             knn_prefix,\n                             feats,\n                             feat_dim=256,\n                             knn_method=\'faiss\',\n                             k=80,\n                             th_knn=0.6,\n                             th_step=0.05,\n                             minsz=3,\n                             maxsz=300,\n                             is_rebuild=False,\n                             is_save_proposals=True,\n                             force=False,\n                             **kwargs):\n    print(\'k={}, th_knn={}, th_step={}, maxsz={}, is_rebuild={}\'.format(\n        k, th_knn, th_step, maxsz, is_rebuild))\n\n    # build knns\n    knns = build_knns(knn_prefix, feats, knn_method, k, is_rebuild)\n\n    # obtain cluster proposals\n    ofolder = osp.join(\n        oprefix, \'{}_k_{}_th_{}_step_{}_minsz_{}_maxsz_{}_iter_0\'.format(\n            knn_method, k, th_knn, th_step, minsz, maxsz))\n    ofn_pred_labels = osp.join(ofolder, \'pred_labels.txt\')\n    if not osp.exists(ofolder):\n        os.makedirs(ofolder)\n    if not osp.isfile(ofn_pred_labels) or is_rebuild:\n        with Timer(\'build super vertices\'):\n            clusters = super_vertex(knns, k, th_knn, th_step, maxsz)\n        with Timer(\'dump clustering to {}\'.format(ofn_pred_labels)):\n            labels = clusters2labels(clusters)\n            write_meta(ofn_pred_labels, labels)\n    else:\n        print(\'read clusters from {}\'.format(ofn_pred_labels))\n        lb2idxs, _ = read_meta(ofn_pred_labels)\n        clusters = labels2clusters(lb2idxs)\n    clusters = filter_clusters(clusters, minsz)\n\n    # output cluster proposals\n    ofolder_proposals = osp.join(ofolder, \'proposals\')\n    if is_save_proposals:\n        print(\'saving cluster proposals to {}\'.format(ofolder_proposals))\n        if not osp.exists(ofolder_proposals):\n            os.makedirs(ofolder_proposals)\n        save_proposals(clusters, knns, ofolder=ofolder_proposals, force=force)\n\n    return ofolder_proposals, ofn_pred_labels\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    ds = BasicDataset(name=args.name,\n                      prefix=args.prefix,\n                      dim=args.dim,\n                      normalize=not args.no_normalize)\n    ds.info()\n\n    generate_basic_proposals(osp.join(args.oprefix, args.name),\n                             osp.join(args.prefix, \'knns\', args.name),\n                             ds.features,\n                             args.dim,\n                             args.knn_method,\n                             args.k,\n                             args.th_knn,\n                             args.th_step,\n                             args.minsz,\n                             args.maxsz,\n                             is_rebuild=args.is_rebuild,\n                             is_save_proposals=args.is_save_proposals,\n                             force=args.force)\n'"
proposals/generate_iter_proposals.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport argparse\nimport numpy as np\n\nfrom utils import (read_meta, write_meta, build_knns, labels2clusters,\n                   clusters2labels, BasicDataset, Timer)\nfrom proposals import super_vertex, filter_clusters, save_proposals\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Generate Iterative Proposals\')\n    parser.add_argument(""--name"",\n                        type=str,\n                        default=\'part1_test\',\n                        help=""image features"")\n    parser.add_argument(""--prefix"",\n                        type=str,\n                        default=\'./data\',\n                        help=""prefix of dataset"")\n    parser.add_argument(""--oprefix"",\n                        type=str,\n                        default=\'./data/cluster_proposals\',\n                        help=""prefix of saving super vertx"")\n    parser.add_argument(""--dim"",\n                        type=int,\n                        default=256,\n                        help=""dimension of feature"")\n    parser.add_argument(""--no_normalize"",\n                        action=\'store_true\',\n                        help=""normalize feature by default"")\n    parser.add_argument(\'--k\', default=3, type=int)\n    parser.add_argument(\'--th_knn\', default=0.6, type=float)\n    parser.add_argument(\'--th_step\', default=0.05, type=float)\n    parser.add_argument(\'--knn_method\',\n                        default=\'faiss\',\n                        choices=[\'faiss\', \'hnsw\'])\n    parser.add_argument(\'--minsz\', default=3, type=int)\n    parser.add_argument(\'--maxsz\', default=500, type=int)\n    parser.add_argument(\'--sv_minsz\', default=2, type=int)\n    parser.add_argument(\'--sv_maxsz\', default=5, type=int)\n    parser.add_argument(""--sv_labels"",\n                        type=str,\n                        default=None,\n                        help=""super vertex labels"")\n    parser.add_argument(""--sv_knn_prefix"",\n                        type=str,\n                        default=None,\n                        help=""super vertex precomputed knn"")\n    parser.add_argument(\'--is_rebuild\', action=\'store_true\')\n    parser.add_argument(\'--is_save_proposals\', action=\'store_true\')\n    parser.add_argument(\'--force\', action=\'store_true\')\n    args = parser.parse_args()\n\n    return args\n\n\ndef parse_path(s):\n    s = os.path.dirname(s)\n    s = s.split(\'/\')[-1]\n    lst = s.split(\'_\')\n    lst.insert(0, \'knn_method\')\n    dic1 = {}\n    for i in range(0, len(lst), 2):\n        dic1[lst[i]] = lst[i + 1]\n    dic = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n    assert dic == dic1\n    return dic\n\n\ndef get_iter_from_path(s):\n    return int(parse_path(s)[\'iter\'])\n\n\ndef get_knns_from_path(s, knn_prefix, feats):\n    dic = parse_path(s)\n    k = int(dic[\'k\'])\n    knn_method = dic[\'knn_method\']\n    knns = build_knns(knn_prefix, feats, knn_method, k, is_rebuild=False)\n    return knns\n\n\ndef generate_iter_proposals(oprefix,\n                            knn_prefix,\n                            feats,\n                            feat_dim=256,\n                            knn_method=\'faiss\',\n                            k=80,\n                            th_knn=0.6,\n                            th_step=0.05,\n                            minsz=3,\n                            maxsz=300,\n                            sv_minsz=2,\n                            sv_maxsz=5,\n                            sv_labels=None,\n                            sv_knn_prefix=None,\n                            is_rebuild=False,\n                            is_save_proposals=True,\n                            force=False,\n                            **kwargs):\n\n    assert sv_minsz >= 2, ""sv_minsz >= 2 to avoid duplicated proposals""\n    print(\'k={}, th_knn={}, th_step={}, minsz={}, maxsz={}, \'\n          \'sv_minsz={}, sv_maxsz={}, is_rebuild={}\'.format(\n              k, th_knn, th_step, minsz, maxsz, sv_minsz, sv_maxsz,\n              is_rebuild))\n\n    if not os.path.exists(sv_labels):\n        raise FileNotFoundError(\'{} not found.\'.format(sv_labels))\n\n    if sv_knn_prefix is None:\n        sv_knn_prefix = knn_prefix\n\n    # get iter and knns from super vertex path\n    _iter = get_iter_from_path(sv_labels) + 1\n    knns_inst = get_knns_from_path(sv_labels, sv_knn_prefix, feats)\n    print(\'read sv_clusters from {}\'.format(sv_labels))\n    sv_lb2idxs, sv_idx2lb = read_meta(sv_labels)\n    inst_num = len(sv_idx2lb)\n    sv_clusters = labels2clusters(sv_lb2idxs)\n    # sv_clusters = filter_clusters(sv_clusters, minsz)\n    feats = np.array([feats[c, :].mean(axis=0) for c in sv_clusters])\n    print(\'average feature of super vertices:\', feats.shape)\n\n    # build knns\n    knns = build_knns(knn_prefix, feats, knn_method, k, is_rebuild)\n\n    # obtain cluster proposals\n    ofolder = os.path.join(\n        oprefix,\n        \'{}_k_{}_th_{}_step_{}_minsz_{}_maxsz_{}_sv_minsz_{}_maxsz_{}_iter_{}\'.\n        format(knn_method, k, th_knn, th_step, minsz, maxsz, sv_minsz,\n               sv_maxsz, _iter))\n    ofn_pred_labels = os.path.join(ofolder, \'pred_labels.txt\')\n    if not os.path.exists(ofolder):\n        os.makedirs(ofolder)\n    if not os.path.isfile(ofn_pred_labels) or is_rebuild:\n        with Timer(\'build super vertices (iter={})\'.format(_iter)):\n            clusters = super_vertex(knns, k, th_knn, th_step, sv_maxsz)\n            clusters = filter_clusters(clusters, sv_minsz)\n            clusters = [[x for c in cluster for x in sv_clusters[c]]\n                        for cluster in clusters]\n        with Timer(\'dump clustering to {}\'.format(ofn_pred_labels)):\n            labels = clusters2labels(clusters)\n            write_meta(ofn_pred_labels, labels, inst_num=inst_num)\n    else:\n        print(\'read clusters from {}\'.format(ofn_pred_labels))\n        lb2idxs, _ = read_meta(ofn_pred_labels)\n        clusters = labels2clusters(lb2idxs)\n    clusters = filter_clusters(clusters, minsz, maxsz)\n\n    # output cluster proposals\n    ofolder_proposals = os.path.join(ofolder, \'proposals\')\n    if is_save_proposals:\n        print(\'saving cluster proposals to {}\'.format(ofolder_proposals))\n        if not os.path.exists(ofolder_proposals):\n            os.makedirs(ofolder_proposals)\n        save_proposals(clusters,\n                       knns_inst,\n                       ofolder=ofolder_proposals,\n                       force=force)\n\n    return ofolder_proposals, ofn_pred_labels\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    ds = BasicDataset(name=args.name,\n                      prefix=args.prefix,\n                      dim=args.dim,\n                      normalize=not args.no_normalize)\n    ds.info()\n\n    sv_folder = os.path.dirname(args.sv_labels)\n    generate_iter_proposals(sv_folder,\n                            os.path.join(sv_folder, \'knns\'),\n                            ds.features,\n                            args.dim,\n                            args.knn_method,\n                            args.k,\n                            args.th_knn,\n                            args.th_step,\n                            args.minsz,\n                            args.maxsz,\n                            args.sv_minsz,\n                            args.sv_maxsz,\n                            sv_labels=args.sv_labels,\n                            sv_knn_prefix=args.sv_knn_prefix,\n                            is_rebuild=args.is_rebuild,\n                            is_save_proposals=args.is_save_proposals,\n                            force=args.force)\n'"
proposals/generate_proposals.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os.path as osp\nimport argparse\n\nfrom utils import BasicDataset\nfrom proposals import generate_basic_proposals, generate_iter_proposals\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Generate Proposals\')\n    parser.add_argument(""--name"",\n                        type=str,\n                        default=\'part1_test\',\n                        help=""image features"")\n    parser.add_argument(""--prefix"",\n                        type=str,\n                        default=\'./data\',\n                        help=""prefix of dataset"")\n    parser.add_argument(""--oprefix"",\n                        type=str,\n                        default=\'./data/cluster_proposals\',\n                        help=""prefix of saving super vertx"")\n    parser.add_argument(""--dim"",\n                        type=int,\n                        default=256,\n                        help=""dimension of feature"")\n    parser.add_argument(""--no_normalize"",\n                        action=\'store_true\',\n                        help=""normalize feature by default"")\n    args = parser.parse_args()\n\n    return args\n\n\ndef generate_proposals(params, prefix, oprefix, name, dim, no_normalize=False):\n    ds = BasicDataset(name=name,\n                      prefix=prefix,\n                      dim=dim,\n                      normalize=not no_normalize)\n    ds.info()\n\n    folders = []\n    for param in params:\n        oprefix_i0 = osp.join(oprefix, name)\n        knn_prefix_i0 = osp.join(prefix, \'knns\', name)\n        folder_i0, pred_labels_i0 = generate_basic_proposals(\n            oprefix=oprefix_i0,\n            knn_prefix=knn_prefix_i0,\n            feats=ds.features,\n            feat_dim=dim,\n            **param)\n\n        iter0 = param.get(\'iter0\', True)\n        if iter0:\n            folders.append(folder_i0)\n\n        iter1_params = param.get(\'iter1_params\', [])\n        for param_i1 in iter1_params:\n            oprefix_i1 = osp.dirname(folder_i0)\n            knn_prefix_i1 = osp.join(oprefix_i1, \'knns\')\n            folder_i1, _ = generate_iter_proposals(oprefix=oprefix_i1,\n                                                   knn_prefix=knn_prefix_i1,\n                                                   feats=ds.features,\n                                                   feat_dim=dim,\n                                                   sv_labels=pred_labels_i0,\n                                                   sv_knn_prefix=knn_prefix_i0,\n                                                   **param_i1)\n            folders.append(folder_i1)\n\n    return folders\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    k = 80\n    knn_method = \'faiss\'\n\n    step_i0 = 0.05\n    minsz_i0 = 3\n    maxsz_i0 = 300\n\n    th_i1 = 0.4\n    step_i1 = 0.05\n    minsz_i1 = 3\n    maxsz_i1 = 500\n    sv_minsz_i1 = 2\n\n    params = [\n        dict(k=k,\n             knn_method=knn_method,\n             th_knn=0.6,\n             th_step=step_i0,\n             minsz=minsz_i0,\n             maxsz=maxsz_i0,\n             iter1_params=[\n                 dict(k=2,\n                      knn_method=knn_method,\n                      th_knn=th_i1,\n                      th_step=step_i1,\n                      minsz=minsz_i1,\n                      maxsz=maxsz_i1,\n                      sv_minsz=sv_minsz_i1,\n                      sv_maxsz=8),\n                 dict(k=3,\n                      knn_method=knn_method,\n                      th_knn=th_i1,\n                      th_step=step_i1,\n                      minsz=minsz_i1,\n                      maxsz=maxsz_i1,\n                      sv_minsz=sv_minsz_i1,\n                      sv_maxsz=5)\n             ]),\n        dict(\n            k=k,\n            knn_method=knn_method,\n            th_knn=0.7,\n            th_step=step_i0,\n            minsz=minsz_i0,\n            maxsz=maxsz_i0,\n            iter0=False,  # do not include the iter0 proposals\n            iter1_params=[\n                dict(k=2,\n                     knn_method=knn_method,\n                     th_knn=th_i1,\n                     th_step=step_i1,\n                     minsz=minsz_i1,\n                     maxsz=maxsz_i1,\n                     sv_minsz=sv_minsz_i1,\n                     sv_maxsz=8),\n                dict(k=3,\n                     knn_method=knn_method,\n                     th_knn=th_i1,\n                     th_step=step_i1,\n                     minsz=minsz_i1,\n                     maxsz=maxsz_i1,\n                     sv_minsz=sv_minsz_i1,\n                     sv_maxsz=5)\n            ]),\n    ]\n    folders = generate_proposals(params,\n                                 args.prefix,\n                                 args.oprefix,\n                                 args.name,\n                                 args.dim,\n                                 no_normalize=args.no_normalize)\n    print(\'generate proposals in the following {} folders: {}\'.format(\n        len(folders), folders))\n'"
proposals/graph.py,0,"b'# Graph Toolbox.\n# By Xiaohang Zhan (xiaohangzhan@outlook.com)\n# Refer [CDP](https://github.com/XiaohangZhan/cdp/) for more details.\n\nimport numpy as np\n\n\nclass Data(object):\n    def __init__(self, name):\n        self.__name = name\n        self.__links = set()\n\n    @property\n    def name(self):\n        return self.__name\n\n    @property\n    def links(self):\n        return set(self.__links)\n\n    def add_link(self, other, score):\n        self.__links.add(other)\n        other.__links.add(self)\n\n\ndef connected_components(nodes, edges):\n    result = []\n    nodes = set(nodes)\n    node2ners = {n: set() for n in nodes}\n    for n1, n2, _ in edges:\n        node2ners[n1].add(n2)\n        node2ners[n2].add(n1)\n    while nodes:\n        n = nodes.pop()\n        group = {n}\n        queue = [n]\n        while queue:\n            n = queue.pop(0)\n            neighbors = node2ners[n]\n            neighbors.difference_update(group)\n            nodes.difference_update(neighbors)\n            group.update(neighbors)\n            queue.extend(neighbors)\n        result.append(group)\n    return result\n\n\ndef connected_components_constraint(nodes, max_sz, score_dict=None, th=None):\n    \'\'\'\n        1. Edges with scores smaller than `th` will be pruned.\n        2. If a component is larger than `max_sz`, all the nodes\n        in the component are added to `remain` and returned for next iteration.\n    \'\'\'\n    result = []\n    remain = set()\n    nodes = set(nodes)\n    while nodes:\n        n = nodes.pop()\n        group = {n}\n        queue = [n]\n        valid = True\n        while queue:\n            n = queue.pop(0)\n            if th is not None:\n                neighbors = {\n                    l\n                    for l in n.links\n                    if score_dict[tuple(sorted([n.name, l.name]))] >= th\n                }\n            else:\n                neighbors = n.links\n            neighbors.difference_update(group)\n            nodes.difference_update(neighbors)\n            group.update(neighbors)\n            queue.extend(neighbors)\n            if len(group) > max_sz or len(remain.intersection(neighbors)) > 0:\n                # if this group exceeds `max_sz`, add the nodes into `remain`\n                valid = False\n                remain.update(group)\n                break\n        if valid:\n            # if this group is smaller than or equal to `max_sz`, finalize it.\n            result.append(group)\n    return result, remain\n\n\ndef graph_clustering_dynamic_th(edges,\n                                score,\n                                max_sz,\n                                step=0.05,\n                                pool=None,\n                                max_iter=100):\n    edges = np.sort(edges, axis=1)\n    th = score.min()\n\n    # construct graph\n    score_dict = {}  # score lookup table\n    if pool is None:\n        for i, e in enumerate(edges):\n            score_dict[e[0], e[1]] = score[i]\n    elif pool == \'avg\':\n        cnt_dict = {}\n        for i, e in enumerate(edges):\n            if (e[0], e[1]) in score_dict:\n                cnt = cnt_dict[e[0], e[1]]\n                score_dict[e[0], e[1]] = 1. * (cnt * score_dict[e[0], e[1]] +\n                                               score[i]) / (cnt + 1)\n                cnt_dict[e[0], e[1]] += 1\n            else:\n                score_dict[e[0], e[1]] = score[i]\n                cnt_dict[e[0], e[1]] = 1\n        del cnt_dict\n\n    elif pool == \'max\':\n        for i, e in enumerate(edges):\n            if (e[0], e[1]) in score_dict:\n                score_dict[e[0], e[1]] = max(score_dict[e[0], e[1]], score[i])\n            else:\n                score_dict[e[0], e[1]] = score[i]\n    else:\n        raise ValueError(\'Pooling operation not supported\')\n\n    nodes = np.sort(np.unique(edges.flatten()))\n    mapping = -1 * np.ones((nodes.max() + 1), dtype=np.int)\n    mapping[nodes] = np.arange(nodes.shape[0])\n    link_idx = mapping[edges]\n    vertex = [Data(n) for n in nodes]\n    for l, s in zip(link_idx, score):\n        vertex[l[0]].add_link(vertex[l[1]], s)\n\n    # first iteration\n    comps, remain = connected_components_constraint(vertex, max_sz)\n\n    # iteration\n    components = comps[:]\n    _iter = 0\n    while remain:\n        th = th + (1 - th) * step\n        comps, remain = connected_components_constraint(\n            remain, max_sz, score_dict, th)\n        components.extend(comps)\n        _iter += 1\n        if _iter >= max_iter:\n            print(""\\t Force stopping at: th {}, remain {}"".format(\n                th, len(remain)))\n            components.append(remain)\n            remain = {}\n\n    components = [sorted([n.name for n in c]) for c in components]\n    return components\n'"
proposals/metrics.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\ndef convert2set(x):\n    if isinstance(x, set):\n        return x\n    elif isinstance(x, list):\n        return set(x)\n    else:\n        return set(list(x))\n\n\ndef compute_iop(pred, label):\n    s1 = convert2set(pred)\n    s2 = convert2set(label)\n    return 1. * len(s1 & s2) / len(s1)\n\n\ndef compute_iog(pred, label):\n    s1 = convert2set(pred)\n    s2 = convert2set(label)\n    return 1. * len(s1 & s2) / len(s2)\n\n\ndef compute_iou(pred, label):\n    s1 = convert2set(pred)\n    s2 = convert2set(label)\n    return 1. * len(s1 & s2) / len(s1 | s2)\n'"
proposals/stat_cluster.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport argparse\nimport numpy as np\n\nfrom utils import read_meta, load_data\nfrom proposals.metrics import compute_iou, compute_iop, compute_iog\n\n\ndef get_majority(lb2cnt):\n    max_cnt = 0\n    max_lb = None\n    for lb, cnt in lb2cnt.items():\n        if cnt > max_cnt:\n            max_cnt = cnt\n            max_lb = lb\n    return max_lb, max_cnt\n\n\ndef compute_avg_size(lst):\n    if len(lst) == 0:\n        return 0\n    return sum(lst) / len(lst)\n\n\ndef coverage(s, size):\n    if not isinstance(s, set):\n        raise TypeError('s should be set type')\n    return 1. * len(s) / size\n\n\ndef inst2cls(inst_sets, idx2lb):\n    cls_sets = []\n    for inst_set in inst_sets:\n        cls_set = set()\n        for idx in inst_set:\n            cls_set.add(idx2lb[idx])\n        cls_sets.append(cls_set)\n    return cls_sets\n\n\ndef analyze_clusters(clusters, idx2lb, lb2idxs, th_pos, th_neg):\n    pos_set, neg_set = set(), set()\n    pos_idx_set, neg_idx_set = set(), set()\n    num_nodes = []\n    ious = []\n    iops = []\n    iogs = []\n    for nodes in clusters:\n        lb2cnt = {}\n        nodes = set(nodes)\n        # take majority as label of the graph\n        for idx in nodes:\n            lb = idx2lb[idx]\n            if lb not in lb2cnt:\n                lb2cnt[lb] = 0\n            lb2cnt[lb] += 1\n        lb, _ = get_majority(lb2cnt)\n        idxs = lb2idxs[lb]\n        # compute stat\n        iou = compute_iou(nodes, idxs)\n        iop = compute_iop(nodes, idxs)\n        iog = compute_iog(nodes, idxs)\n        ious.append(iou)\n        iops.append(iop)\n        iogs.append(iog)\n        num_nodes.append(len(nodes))\n        if iou > th_pos:\n            pos_set.add(lb)\n            pos_idx_set |= nodes\n        elif iou < th_neg:\n            neg_set.add(lb)\n            neg_idx_set |= nodes\n        else:\n            pass\n\n    ious = np.array(ious)\n    iops = np.array(iops)\n    iogs = np.array(iogs)\n    num_nodes = np.array(num_nodes)\n    return num_nodes, ious, iops, iogs, pos_set, neg_set, pos_idx_set, neg_idx_set\n\n\ndef mse_error(arr, n):\n    return np.dot(n - arr, n - arr) / arr.size\n\n\ndef stat_cluster(clusters, idx2lb, lb2idxs, inst_num, cls_num, th_pos, th_neg):\n    print('#clusters:', len(clusters))\n    num_nodes, ious, iops, iogs, pos_set, neg_set, pos_idx_set, neg_idx_set = \\\n            analyze_clusters(clusters, idx2lb, lb2idxs, th_pos, th_neg)\n\n    ## compute statistics\n    print('isolated anchor: ', len(np.where(num_nodes == 1)[0]))\n    avg_node_size = compute_avg_size(num_nodes)\n    print('#all_avg_node: {}'.format(int(avg_node_size)))\n    pos_num_nodes = num_nodes[np.where(ious > th_pos)]\n    if len(pos_num_nodes) > 0:\n        avg_node_size = compute_avg_size(pos_num_nodes)\n        print('#pos_avg_node: {}, #max_node: {}, #min_node: {}'.format(\n            int(avg_node_size), pos_num_nodes.max(), pos_num_nodes.min()))\n    neg_num_nodes = num_nodes[np.where(ious < th_pos)]\n    if len(neg_num_nodes) > 0:\n        avg_node_size = compute_avg_size(neg_num_nodes)\n        print('#neg_avg_node: {}, #max_node: {}, #min_node: {}'.format(\n            int(avg_node_size), neg_num_nodes.max(), neg_num_nodes.min()))\n\n    pos_g_labels = np.where(ious > th_pos)[0]\n    neg_g_labels = np.where(ious < th_neg)[0]\n    print('#tot: {}, #pos: {}, #neg: {}'.format(len(ious), pos_g_labels.size,\n                                                neg_g_labels.size))\n\n    err0 = mse_error(ious, 0)\n    err1 = mse_error(ious, 1)\n    print('random guess error: 0({}), 1({})'.format(err0, err1))\n\n    pos_c = coverage(pos_idx_set, inst_num)\n    neg_c = coverage(neg_idx_set, inst_num)\n    all_c = coverage(pos_idx_set | neg_idx_set, inst_num)\n    print(\n        '[instance-level] pos coverage: {:.2f}, neg coverage: {:.2f}, all coverage: {:.2f}'\n        .format(pos_c, neg_c, all_c))\n\n    pos_c = coverage(pos_set, cls_num)\n    neg_c = coverage(neg_set, cls_num)\n    all_c = coverage(pos_set | neg_set, cls_num)\n    print(\n        '[class-level] pos coverage: {:.2f}, neg coverage: {:.2f}, all coverage: {:.2f}'\n        .format(pos_c, neg_c, all_c))\n"""
proposals/super_vertex.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom proposals.graph import graph_clustering_dynamic_th\nfrom utils import filter_knns\n\n\ndef super_vertex(knns, k, th, th_step, max_sz):\n    pairs, scores = filter_knns(knns, k, th)\n    assert len(pairs) == len(scores)\n    if len(pairs) == 0:\n        return []\n    components = graph_clustering_dynamic_th(pairs, scores, max_sz, th_step)\n    return components\n'"
tools/analyze_proposals.py,0,"b""import os\nimport glob\nfrom utils import load_data\nfrom tqdm import tqdm\n\nif __name__ == '__main__':\n    fn_node_pattern = '*_node.npz'\n    folder_pattern = './data/cluster_proposals/part1_test/{}/proposals/'\n    proposal_folders = [\n        folder_pattern.format(\n            'faiss_k_30_th_0.7_step_0.05_minsz_3_maxsz_300_iter_0'),\n        folder_pattern.format(\n            'faiss_k_80_th_0.7_step_0.05_minsz_3_maxsz_300_iter_0')\n    ]\n    nodes = []\n    for proposal_folder in tqdm(proposal_folders):\n        fn_nodes = sorted(\n            glob.glob(os.path.join(proposal_folder, fn_node_pattern)))\n        nodes.append([set(load_data(fn_node)) for fn_node in fn_nodes])\n\n    cnt = 0\n    print(len(nodes[0]), len(nodes[1]))\n    for n1 in tqdm(nodes[0]):\n        for n2 in tqdm(nodes[1]):\n            inter = len(n1 & n2)\n            # if inter > 0:\n            if inter > 0 and inter < len(n1) and inter < len(n2):\n                cnt += 1\n                print('cnt:', cnt, inter, len(n1), len(n2))\n                # break\n                # exit()\n    print('cnt:', cnt)\n"""
tools/baseline_cluster.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport inspect\nimport argparse\n\nimport baseline\nfrom utils import (write_meta, set_random_seed, mkdir_if_no_exists,\n                   BasicDataset, Timer)\n\nfuncs = inspect.getmembers(baseline, inspect.isfunction)\nmethod_names = [n for n, _ in funcs]\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Baseline Clustering\')\n    parser.add_argument(""--name"",\n                        type=str,\n                        default=\'part1_test\',\n                        help=""image features"")\n    parser.add_argument(""--prefix"",\n                        type=str,\n                        default=\'./data\',\n                        help=""prefix of dataset"")\n    parser.add_argument(""--oprefix"",\n                        type=str,\n                        default=\'./data/baseline_results\',\n                        help=""prefix of saving clustering results"")\n    parser.add_argument(""--dim"",\n                        type=int,\n                        default=256,\n                        help=""dimension of feature"")\n    parser.add_argument(""--no_normalize"",\n                        action=\'store_true\',\n                        help=""whether to normalize feature"")\n    parser.add_argument(\'--method\', choices=method_names, required=True)\n    # args for different methods\n    parser.add_argument(\'--n_clusters\',\n                        default=2,\n                        type=int,\n                        help=""KMeans, HAC"")\n    parser.add_argument(\'--seed\', type=int, default=42, help=\'random seed\')\n    parser.add_argument(\'--batch_size\', default=100, type=int)\n    parser.add_argument(\'--eps\', default=0.7, type=float)\n    parser.add_argument(\'--distance\', default=0.7, type=float)\n    parser.add_argument(\'--min_samples\', default=10, type=int)\n    parser.add_argument(\'--hmethod\', default=\'single\', type=str)\n    parser.add_argument(\'--radius\', default=0.1, type=float)\n    parser.add_argument(\'--min_conn\', default=1, type=int)\n    parser.add_argument(\'--bw\', default=1, type=float)\n    parser.add_argument(\'--min_bin_freq\', default=1, type=int)\n    parser.add_argument(\'--knn\', default=80, type=int)\n    parser.add_argument(\'--th_sim\', default=0.7, type=float)\n    parser.add_argument(\'--iters\', default=20, type=int)\n    parser.add_argument(\'--knn_method\',\n                        default=\'faiss\',\n                        choices=[\'faiss\', \'faiss_gpu\', \'hnsw\'])\n    parser.add_argument(\'--num_process\', default=1, type=int)\n    parser.add_argument(\'--force\', action=\'store_true\')\n    args = parser.parse_args()\n\n    return args\n\n\ndef get_output_path(args, ofn=\'pred_labels.txt\'):\n    method2name = {\n        \'aro\':\n        \'k_{}_th_{}\'.format(args.knn, args.th_sim),\n        \'knn_aro\':\n        \'k_{}_th_{}\'.format(args.knn, args.th_sim),\n        \'dbscan\':\n        \'eps_{}_min_{}\'.format(args.eps, args.min_samples),\n        \'knn_dbscan\':\n        \'eps_{}_min_{}_{}_k_{}_th_{}\'.format(args.eps, args.min_samples,\n                                             args.knn_method, args.knn,\n                                             args.th_sim),\n        \'our_dbscan\':\n        \'min_{}_k_{}_th_{}\'.format(args.min_samples, args.knn, args.th_sim),\n        \'hdbscan\':\n        \'min_{}\'.format(args.min_samples),\n        \'fast_hierarchy\':\n        \'dist_{}_hmethod_{}\'.format(args.distance, args.hmethod),\n        \'hierarchy\':\n        \'n_{}_k_{}\'.format(args.n_clusters, args.knn),\n        \'knn_hierarchy\':\n        \'n_{}_k_{}_th_{}\'.format(args.n_clusters, args.knn, args.th_sim),\n        \'mini_batch_kmeans\':\n        \'n_{}_bs_{}\'.format(args.n_clusters, args.batch_size),\n        \'kmeans\':\n        \'n_{}\'.format(args.n_clusters),\n        \'spectral\':\n        \'n_{}\'.format(args.n_clusters),\n        \'dask_spectral\':\n        \'n_{}\'.format(args.n_clusters),\n        \'knn_spectral\':\n        \'n_{}_k_{}_th_{}\'.format(args.n_clusters, args.knn, args.th_sim),\n        \'densepeak\':\n        \'k_{}_th_{}_r_{}_m_{}\'.format(args.knn, args.th_sim, args.radius,\n                                      args.min_conn),\n        \'meanshift\':\n        \'bw_{}_bin_{}\'.format(args.bw, args.min_bin_freq),\n        \'chinese_whispers\':\n        \'{}_k_{}_th_{}_iters_{}\'.format(args.knn_method, args.knn, args.th_sim,\n                                        args.iters),\n        \'chinese_whispers_fast\':\n        \'{}_k_{}_th_{}_iters_{}\'.format(args.knn_method, args.knn, args.th_sim,\n                                        args.iters),\n    }\n\n    if args.method in method2name:\n        name = \'{}_{}_{}\'.format(args.name, args.method,\n                                 method2name[args.method])\n    else:\n        name = \'{}_{}\'.format(args.name, args.method)\n\n    opath = os.path.join(args.oprefix, name, ofn)\n    if os.path.exists(opath) and not args.force:\n        raise FileExistsError(\n            \'{} has already existed. Please set force=True to overwrite.\'.\n            format(opath))\n    mkdir_if_no_exists(opath)\n\n    return opath\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    set_random_seed(args.seed)\n\n    cluster_func = baseline.__dict__[args.method]\n\n    ds = BasicDataset(name=args.name,\n                      prefix=args.prefix,\n                      dim=args.dim,\n                      normalize=not args.no_normalize)\n    ds.info()\n    feats = ds.features\n\n    opath = get_output_path(args)\n\n    with Timer(\'{}\'.format(args.method)):\n        pred_labels = cluster_func(feats, **args.__dict__)\n\n    # save clustering results\n    idx2lb = {}\n    for idx, lb in enumerate(pred_labels):\n        if lb == -1:\n            continue\n        idx2lb[idx] = lb\n    inst_num = len(pred_labels)\n    print(\'coverage: {} / {} = {:.4f}\'.format(len(idx2lb), inst_num,\n                                              1. * len(idx2lb) / inst_num))\n    write_meta(opath, idx2lb, inst_num=inst_num)\n'"
tools/download_data.py,0,"b""import gdown\nimport tarfile\nimport argparse\nimport os.path as osp\n\ndata2url = {\n    'part1':\n    'https://drive.google.com/uc?id=16WD4orcF9dqjNPLzST2U3maDh2cpzxAY',\n    'benchmark':\n    'https://drive.google.com/uc?id=10boLBiYq-6wKC_N_71unlMyNrimRjpVa',\n    'youtube_face':\n    'https://drive.google.com/uc?id=1zrckFOx5fDnvDSK3ZeT2Di6HLaxZPnoG',\n    'deepfashion':\n    'https://drive.google.com/uc?id=15B5Ypj8_U9rhcuvkrkCZQAgV4cfes7aV',\n}\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Download Dataset')\n    parser.add_argument('--data', default='part1', choices=data2url.keys())\n    parser.add_argument('--tar_path', default='data.tar.gz', type=str)\n    parser.add_argument('--force', action='store_true')\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    url = data2url[args.data]\n    tar_path = args.tar_path\n    if osp.exists(tar_path) and not args.force:\n        print('{} already exists.'\n              'Modify --tar_path or run with --force to overwrite.'.format(\n                  tar_path))\n    else:\n        gdown.download(url, tar_path, quiet=False)\n    print('untar {}'.format(tar_path))\n    tar = tarfile.open(tar_path)\n    for member in tar.getmembers():\n        if member.isreg():\n            tar.extract(member)\n    tar.close()\n    print('download data successfully!')\n"""
tools/dsgcn_upper_bound.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport argparse\nimport numpy as np\n\nfrom utils import load_data, read_meta, write_meta, labels2clusters\nfrom proposals import get_majority, compute_iou\nfrom post_process import nms\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='GCN Upper Bound')\n    parser.add_argument('--cluster_path', nargs='+')\n    parser.add_argument('--th_pos', default=-1, type=float)\n    parser.add_argument('--th_iou', default=1, type=float)\n    parser.add_argument('--gt_labels', type=str, required=True)\n    parser.add_argument('--output_name', default='', type=str)\n    parser.add_argument('--output_folder',\n                        default='./data/results/gcn_ub/',\n                        type=str)\n    parser.add_argument('--force', action='store_true')\n    args = parser.parse_args()\n\n    assert args.th_iou >= 0\n\n    if not os.path.exists(args.output_folder):\n        os.makedirs(args.output_folder)\n\n    cluster_name = args.output_name + '_' if args.output_name != '' else ''\n    pred_label_fn = os.path.join(\n        args.output_folder,\n        '{}th_iou_{}_pos_{}_pred_labels.txt'.format(cluster_name, args.th_iou,\n                                                    args.th_pos))\n\n    if os.path.exists(pred_label_fn) and not args.force:\n        print('{} has already existed. Please set force=True to overwrite.'.\n              format(pred_label_fn))\n        exit()\n\n    # read label\n    lb2idxs, idx2lb = read_meta(args.gt_labels)\n    tot_inst_num = len(idx2lb)\n\n    clusters = []\n    for path in args.cluster_path:\n        path = path.replace('\\\\', '')\n        if path.endswith('.npz'):\n            clusters.extend(load_data(path))\n        elif path.endswith('.txt'):\n            lb2idxs_, _ = read_meta(path)\n            clusters.extend(labels2clusters(lb2idxs_))\n        else:\n            raise ValueError('Unkown suffix', path)\n\n    # get ground-truth iou\n    ious = []\n    for cluster in clusters:\n        lb2cnt = {}\n        cluster = set(cluster)\n        # take majority as label of the graph\n        for idx in cluster:\n            if idx not in idx2lb:\n                print('[warn] {} is not found'.format(idx))\n                continue\n            lb = idx2lb[idx]\n            if lb not in lb2cnt:\n                lb2cnt[lb] = 0\n            lb2cnt[lb] += 1\n        lb, _ = get_majority(lb2cnt)\n        if lb is None:\n            iou = -1e6\n        else:\n            idxs = lb2idxs[lb]\n            iou = compute_iou(cluster, idxs)\n        ious.append(iou)\n    ious = np.array(ious)\n\n    # rank by iou\n    pos_g_labels = np.where(ious > args.th_pos)[0]\n    clusters = [[clusters[i], ious[i]] for i in pos_g_labels]\n    clusters = sorted(clusters, key=lambda x: x[1], reverse=True)\n    clusters = [n for n, _ in clusters]\n\n    inst_num = len(idx2lb)\n    pos_idx_set = set()\n    for c in clusters:\n        pos_idx_set |= set(c)\n    print('inst-coverage before nms: {}'.format(1. * len(pos_idx_set) /\n                                                inst_num))\n\n    # nms\n    idx2lb, _ = nms(clusters, args.th_iou)\n\n    # output stats\n    inst_num = len(idx2lb)\n    cls_num = len(idx2lb.values())\n\n    print('#inst: {}, #class: {}'.format(inst_num, cls_num))\n    print('#inst-coverage: {:.2f}'.format(1. * inst_num / tot_inst_num))\n\n    # save to file\n    write_meta(pred_label_fn, idx2lb, inst_num=tot_inst_num)\n"""
tools/test_knn.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport argparse\n\nfrom utils import (BasicDataset, build_knns, knns2spmat, fast_knns2spmat,\n                   is_spmat_eq, Timer)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Test KNN\')\n    parser.add_argument(""--name"",\n                        type=str,\n                        default=\'part1_test\',\n                        help=""image features"")\n    parser.add_argument(""--prefix"",\n                        type=str,\n                        default=\'./data\',\n                        help=""prefix of dataset"")\n    parser.add_argument(""--dim"",\n                        type=int,\n                        default=256,\n                        help=""dimension of feature"")\n    parser.add_argument(\'--knn\', default=80, type=int)\n    parser.add_argument(\'--th_sim\', default=0.6, type=float)\n    parser.add_argument(\'--knn_method\',\n                        default=\'faiss\',\n                        choices=[\'faiss\', \'faiss_gpu\', \'hnsw\'])\n    parser.add_argument(\'--num_process\', default=None, type=int)\n    parser.add_argument(""--no_normalize"",\n                        action=\'store_true\',\n                        help=""normalize feature by default"")\n    parser.add_argument(""--test_all"", action=\'store_true\')\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    ds = BasicDataset(name=args.name,\n                      prefix=args.prefix,\n                      dim=args.dim,\n                      normalize=args.no_normalize)\n    ds.info()\n\n    with Timer(\'[{}] build_knns\'.format(args.knn_method)):\n        if args.num_process is None:\n            import multiprocessing as mp\n            args.num_process = mp.cpu_count()\n        print(\'use {} CPU for computation\'.format(args.num_process))\n        knn_prefix = os.path.join(args.prefix, \'knns\', args.name)\n        knns = build_knns(knn_prefix,\n                          ds.features,\n                          args.knn_method,\n                          args.knn,\n                          num_process=args.num_process)\n\n    if args.test_all:\n        with Timer(\'knns2spmat\'):\n            adj1 = knns2spmat(knns, args.knn, args.th_sim, use_sim=True)\n\n        with Timer(\'fast_knns2spmat\'):\n            adj2 = fast_knns2spmat(knns, args.knn, args.th_sim, use_sim=True)\n\n        print(\'#adj: {}, #adj2: {}, #non-eq: {}\'.format(\n            adj1.nnz, adj2.nnz, (adj1 != adj2).nnz))\n\n        assert is_spmat_eq(adj1, adj2), ""adj1 and adj2 are not equal""\n        print(\'Output of knns2spmat and fast_knns2spmat are equal\')\n'"
utils/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom .misc import *\nfrom .knn import *\nfrom .misc_cluster import *\nfrom .adjacency import *\nfrom .dataset import BasicDataset\nfrom .logger import create_logger\nfrom .faiss_search import faiss_search_knn\nfrom .faiss_gpu import faiss_search_approx_knn\n'
utils/adjacency.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport scipy.sparse as sp\n\n\ndef row_normalize(mx):\n    """"""Row-normalize sparse matrix""""""\n    rowsum = np.array(mx.sum(1))\n    # if rowsum <= 0, keep its previous value\n    rowsum[rowsum <= 0] = 1\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\n\ndef build_symmetric_adj(adj, self_loop=True):\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n    if self_loop:\n        adj = adj + sp.eye(adj.shape[0])\n    return adj\n\n\ndef sparse_mx_to_indices_values(sparse_mx):\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n    values = sparse_mx.data\n    shape = np.array(sparse_mx.shape)\n    return indices, values, shape\n\n\ndef indices_values_to_sparse_tensor(indices, values, shape):\n    import torch\n    indices = torch.from_numpy(indices)\n    values = torch.from_numpy(values)\n    shape = torch.Size(shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    """"""Convert a scipy sparse matrix to a torch sparse tensor.""""""\n    indices, values, shape = sparse_mx_to_indices_values(sparse_mx)\n    return indices_values_to_sparse_tensor(indices, values, shape)\n'"
utils/dataset.py,0,"b'#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport os\r\nimport numpy as np\r\n\r\nfrom utils.misc import TextColors, l2norm, read_probs, read_meta\r\n\r\n\r\nclass BasicDataset():\r\n    def __init__(self,\r\n                 name,\r\n                 prefix=\'data\',\r\n                 dim=256,\r\n                 normalize=True,\r\n                 verbose=True):\r\n        self.name = name\r\n        self.dtype = np.float32\r\n        self.dim = dim\r\n        self.normalize = normalize\r\n        if not os.path.exists(prefix):\r\n            raise FileNotFoundError(\r\n                \'folder({}) does not exist.\'.format(prefix))\r\n        self.prefix = prefix\r\n        self.label_path = os.path.join(prefix, \'labels\', name + \'.meta\')\r\n        if os.path.isfile(self.label_path):\r\n            self.lb2idxs, self.idx2lb = read_meta(self.label_path,\r\n                                                  verbose=verbose)\r\n            self.inst_num = len(self.idx2lb)\r\n            self.cls_num = len(self.lb2idxs)\r\n        else:\r\n            print(\r\n                \'meta file not found: {}.\\n\'\r\n                \'init `lb2idxs` and `idx2lb` as None.\'\r\n                .format(self.label_path))\r\n            self.lb2idxs, self.idx2lb = None, None\r\n            self.inst_num, self.cls_num = -1, -1\r\n        self.feat_path = os.path.join(prefix, \'features\', name + \'.bin\')\r\n        self.features = read_probs(self.feat_path,\r\n                                   self.inst_num,\r\n                                   dim,\r\n                                   self.dtype,\r\n                                   verbose=verbose)\r\n        if self.normalize:\r\n            self.features = l2norm(self.features)\r\n\r\n    def info(self):\r\n        print(\'name:{}{}{}\\ninst_num:{}\\ncls_num:{}\\ndim:{}\\n\'\r\n              \'feat_path:{}\\nnormalization:{}{}{}\\ndtype:{}\'.format(\r\n                  TextColors.OKGREEN, self.name, TextColors.ENDC,\r\n                  self.inst_num, self.cls_num, self.dim, self.feat_path,\r\n                  TextColors.FATAL, self.normalize, TextColors.ENDC,\r\n                  self.dtype))\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    import argparse\r\n    parser = argparse.ArgumentParser(description=""example"")\r\n    parser.add_argument(""--name"",\r\n                        type=str,\r\n                        default=\'part1_test\',\r\n                        help=""image features"")\r\n    parser.add_argument(""--prefix"",\r\n                        type=str,\r\n                        default=\'./data\',\r\n                        help=""prefix of dataset"")\r\n    parser.add_argument(""--dim"",\r\n                        type=int,\r\n                        default=256,\r\n                        help=""dimension of feature"")\r\n    parser.add_argument(""--no_normalize"",\r\n                        action=\'store_true\',\r\n                        help=""whether to normalize feature"")\r\n    args = parser.parse_args()\r\n\r\n    ds = BasicDataset(name=args.name,\r\n                      prefix=args.prefix,\r\n                      dim=args.dim,\r\n                      normalize=not args.no_normalize)\r\n    ds.info()\r\n'"
utils/dist.py,4,"b""import os\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef init_dist(launcher, backend='nccl', **kwargs):\n    if mp.get_start_method(allow_none=True) is None:\n        mp.set_start_method('spawn')\n    if launcher == 'pytorch':\n        _init_dist_pytorch(backend, **kwargs)\n    else:\n        raise ValueError('Invalid launcher type: {}'.format(launcher))\n\n\ndef _init_dist_pytorch(backend, **kwargs):\n    rank = int(os.environ['RANK'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n"""
utils/draw.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport random\nfrom igraph import Graph, plot\n\nfrom utils import load_data\n\n\ndef draw_graph(ofolder, idx2lb, g_label, idx, prob):\n    fpath = os.path.join(ofolder, \'{}.npz\'.format(idx))\n    ograph_folder = \'graph/\' + ofolder.split(\'/\')[-1]\n    if not os.path.exists(ograph_folder):\n        os.makedirs(ograph_folder)\n    color_dict = {1: ""red"", 0: ""lightblue""}\n    vertices, raw_edges = load_data(fpath)\n    vertices = list(vertices)\n    lb = idx2lb[idx]\n    abs2rel = {}\n    for i, v in enumerate(vertices):\n        abs2rel[v] = i\n    edges = [(abs2rel[p1], abs2rel[p2]) for p1, p2, _ in raw_edges]\n    g = Graph(vertex_attrs={""label"": vertices}, edges=edges, directed=False)\n    edge_weights = [1 - d for _, _, d in raw_edges]\n    if len(edge_weights) > 0:\n        w_mean = sum(edge_weights) / len(edge_weights)\n        w_max = max(edge_weights)\n        w_min = min(edge_weights)\n    else:\n        w_mean, w_max, w_min = 1, 1, 1\n\n    visual_style = {}\n    visual_style[""vertex_color""] = [\n        color_dict[lb == idx2lb[v]] for v in vertices\n    ]\n    visual_style[\'edge_width\'] = [5 * w for w in edge_weights]\n\n    plot(g,\n         **visual_style,\n         target=""{}/{}_{}_{:.2f}_{:.2f}_{:.2f}_{:.2f}.png"".format(\n             ograph_folder, g_label, idx, prob, w_mean, w_min, w_max))\n\n\ndef draw_graphs(err, idx2lb, gt_folder, draw_err_num=10):\n    for lb in err:\n        lst = err[lb]\n        random.shuffle(lst)\n        for idx, prob in lst[:draw_err_num]:\n            print(idx, prob[lb])\n            draw_graph(gt_folder, idx2lb, lb, idx, prob[lb])\n'"
utils/faiss_gpu.py,0,"b'import os\nimport gc\nimport numpy as np\nfrom tqdm import tqdm\n\nimport faiss\n\n__all__ = [\'faiss_search_approx_knn\']\n\n\nclass faiss_index_wrapper():\n    def __init__(self,\n                 target,\n                 nprobe=128,\n                 index_factory_str=None,\n                 verbose=False,\n                 mode=\'proxy\',\n                 using_gpu=True):\n        self._res_list = []\n\n        num_gpu = faiss.get_num_gpus()\n        print(\'[faiss gpu] #GPU: {}\'.format(num_gpu))\n\n        size, dim = target.shape\n        assert size > 0, ""size: {}"".format(size)\n        index_factory_str = ""IVF{},PQ{}"".format(\n            min(8192, 16 * round(np.sqrt(size))),\n            32) if index_factory_str is None else index_factory_str\n        cpu_index = faiss.index_factory(dim, index_factory_str)\n        cpu_index.nprobe = nprobe\n\n        if mode == \'proxy\':\n            co = faiss.GpuClonerOptions()\n            co.useFloat16 = True\n            co.usePrecomputed = False\n\n            index = faiss.IndexProxy()\n            for i in range(num_gpu):\n                res = faiss.StandardGpuResources()\n                self._res_list.append(res)\n                sub_index = faiss.index_cpu_to_gpu(\n                    res, i, cpu_index, co) if using_gpu else cpu_index\n                index.addIndex(sub_index)\n        elif mode == \'shard\':\n            co = faiss.GpuMultipleClonerOptions()\n            co.useFloat16 = True\n            co.usePrecomputed = False\n            co.shard = True\n            index = faiss.index_cpu_to_all_gpus(cpu_index,\n                                                co,\n                                                ngpu=num_gpu)\n        else:\n            raise KeyError(""Unknown index mode"")\n\n        index = faiss.IndexIDMap(index)\n        index.verbose = verbose\n\n        # get nlist to decide how many samples used for training\n        nlist = int([\n            item for item in index_factory_str.split("","") if \'IVF\' in item\n        ][0].replace(""IVF"", """"))\n\n        # training\n        if not index.is_trained:\n            indexes_sample_for_train = np.random.randint(\n                0, size, nlist * 256)\n            index.train(target[indexes_sample_for_train])\n\n        # add with ids\n        target_ids = np.arange(0, size)\n        index.add_with_ids(target, target_ids)\n        self.index = index\n\n    def search(self, *args, **kargs):\n        return self.index.search(*args, **kargs)\n\n    def __del__(self):\n        self.index.reset()\n        del self.index\n        for res in self._res_list:\n            del res\n\n\ndef batch_search(index, query, k, bs, verbose=False):\n    n = len(query)\n    dists = np.zeros((n, k), dtype=np.float32)\n    nbrs = np.zeros((n, k), dtype=np.int64)\n\n    for sid in tqdm(range(0, n, bs),\n                    desc=""faiss searching..."",\n                    disable=not verbose):\n        eid = min(n, sid + bs)\n        dists[sid:eid], nbrs[sid:eid] = index.search(query[sid:eid], k)\n    return dists, nbrs\n\n\ndef faiss_search_approx_knn(query,\n                            target,\n                            k,\n                            nprobe=128,\n                            bs=int(1e6),\n                            index_factory_str=None,\n                            verbose=False):\n    index = faiss_index_wrapper(target,\n                                nprobe=nprobe,\n                                index_factory_str=index_factory_str,\n                                verbose=verbose)\n    dists, nbrs = batch_search(index, query, k=k, bs=bs, verbose=verbose)\n\n    del index\n    gc.collect()\n    return dists, nbrs\n'"
utils/faiss_search.py,7,"b""import gc\nfrom tqdm import tqdm\n\nfrom .faiss_gpu import faiss_search_approx_knn\n\n__all__ = ['faiss_search_knn']\n\n\ndef precise_dist(feat, nbrs, num_process=4, sort=True, verbose=False):\n    import torch\n    feat_share = torch.from_numpy(feat).share_memory_()\n    nbrs_share = torch.from_numpy(nbrs).share_memory_()\n    dist_share = torch.zeros_like(nbrs_share).share_memory_()\n\n    precise_dist_share_mem(feat_share,\n                           nbrs_share,\n                           dist_share,\n                           num_process=num_process,\n                           sort=sort,\n                           verbose=verbose)\n\n    del feat_share\n    gc.collect()\n    return dist_share.numpy(), nbrs_share.numpy()\n\n\ndef precise_dist_share_mem(feat,\n                           nbrs,\n                           dist,\n                           num_process=16,\n                           sort=True,\n                           process_unit=4000,\n                           verbose=False):\n    from torch import multiprocessing as mp\n    num, _ = feat.shape\n    num_per_proc = int(num / num_process) + 1\n\n    processes = []\n    for pi in range(num_process):\n        sid = pi * num_per_proc\n        eid = min(sid + num_per_proc, num)\n        p = mp.Process(target=bmm,\n                       kwargs={\n                           'feat': feat,\n                           'nbrs': nbrs,\n                           'dist': dist,\n                           'sid': sid,\n                           'eid': eid,\n                           'sort': sort,\n                           'process_unit': process_unit,\n                           'verbose': verbose,\n                       })\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n\n\ndef bmm(feat,\n        nbrs,\n        dist,\n        sid,\n        eid,\n        sort=True,\n        process_unit=4000,\n        verbose=False):\n    import torch\n    _, cols = dist.shape\n    batch_sim = torch.zeros((eid - sid, cols), dtype=torch.float32)\n    for s in tqdm(range(sid, eid, process_unit),\n                  desc='bmm',\n                  disable=not verbose):\n        e = min(eid, s + process_unit)\n        query = feat[s:e].unsqueeze(1)\n        gallery = feat[nbrs[s:e]].permute(0, 2, 1)\n        batch_sim[s - sid:e - sid] = torch.bmm(query, gallery).view(-1, cols)\n\n    if sort:\n        sort_unit = int(1e6)\n        batch_nbr = nbrs[sid:eid]\n        for s in range(0, batch_sim.shape[0], sort_unit):\n            e = min(s + sort_unit, eid)\n            batch_sim[s:e], indices = torch.sort(batch_sim[s:e],\n                                                 descending=True)\n            batch_nbr[s:e] = torch.gather(batch_nbr[s:e], 1, indices)\n        nbrs[sid:eid] = batch_nbr\n    dist[sid:eid] = 1. - batch_sim\n\n\ndef faiss_search_knn(feat,\n                     k,\n                     nprobe=128,\n                     num_process=4,\n                     is_precise=True,\n                     sort=True,\n                     verbose=False):\n\n    dists, nbrs = faiss_search_approx_knn(query=feat,\n                                          target=feat,\n                                          k=k,\n                                          nprobe=nprobe,\n                                          verbose=verbose)\n\n    if is_precise:\n        print('compute precise dist among k={} nearest neighbors'.format(k))\n        dists, nbrs = precise_dist(feat,\n                                   nbrs,\n                                   num_process=num_process,\n                                   sort=sort,\n                                   verbose=verbose)\n\n    return dists, nbrs\n"""
utils/knn.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport math\nimport numpy as np\nimport multiprocessing as mp\nfrom tqdm import tqdm\n\nfrom utils import (load_data, dump_data, mkdir_if_no_exists, Timer)\nfrom .faiss_search import faiss_search_knn\n\n__all__ = [\n    \'knn_brute_force\', \'knn_hnsw\', \'knn_faiss\', \'knn_faiss_gpu\', \'knns2spmat\',\n    \'fast_knns2spmat\', \'knns2sub_spmat\', \'build_knns\', \'filter_knns\',\n    \'knns2ordered_nbrs\'\n]\n\n\ndef knns_recall(nbrs, idx2lb, lb2idxs):\n    with Timer(\'compute recall\'):\n        recs = []\n        cnt = 0\n        for idx, (n, _) in enumerate(nbrs):\n            lb = idx2lb[idx]\n            idxs = lb2idxs[lb]\n            n = list(n)\n            if len(n) == 1:\n                cnt += 1\n            s = set(idxs) & set(n)\n            recs += [1. * len(s) / len(idxs)]\n        print(\'there are {} / {} = {:.3f} isolated anchors.\'.format(\n            cnt, len(nbrs), 1. * cnt / len(nbrs)))\n    recall = np.mean(recs)\n    return recall\n\n\ndef filter_knns(knns, k, th):\n    pairs = []\n    scores = []\n    n = len(knns)\n    nbrs = np.zeros([n, k], dtype=np.int32) - 1\n    simi = np.zeros([n, k]) - 1\n    for i, (nbr, dist) in enumerate(knns):\n        assert len(nbr) == len(dist)\n        nbrs[i, :len(nbr)] = nbr\n        simi[i, :len(nbr)] = 1. - dist\n    anchor = np.tile(np.arange(n).reshape(n, 1), (1, k))\n\n    # filter\n    selidx = np.where((simi >= th) & (nbrs != -1) & (nbrs != anchor))\n    pairs = np.hstack((anchor[selidx].reshape(-1,\n                                              1), nbrs[selidx].reshape(-1, 1)))\n    scores = simi[selidx]\n\n    if len(pairs) > 0:\n        # keep uniq pairs\n        pairs = np.sort(pairs, axis=1)\n        pairs, unique_idx = np.unique(pairs, return_index=True, axis=0)\n        scores = scores[unique_idx]\n    return pairs, scores\n\n\ndef knns2ordered_nbrs(knns, sort=True):\n    if isinstance(knns, list):\n        knns = np.array(knns)\n    nbrs = knns[:, 0, :].astype(np.int32)\n    dists = knns[:, 1, :]\n    if sort:\n        # sort dists from low to high\n        nb_idx = np.argsort(dists, axis=1)\n        idxs = np.arange(nb_idx.shape[0]).reshape(-1, 1)\n        dists = dists[idxs, nb_idx]\n        nbrs = nbrs[idxs, nb_idx]\n    return dists, nbrs\n\n\ndef knns2spmat(knns, k, th_sim=0.7, use_sim=False):\n    # convert knns to symmetric sparse matrix\n    from scipy.sparse import csr_matrix\n    eps = 1e-5\n    n = len(knns)\n    row, col, data = [], [], []\n    for row_i, knn in enumerate(knns):\n        nbrs, dists = knn\n        for nbr, dist in zip(nbrs, dists):\n            assert -eps <= dist <= 1 + eps, ""{}: {}"".format(row_i, dist)\n            w = dist\n            if 1 - w < th_sim or nbr == -1:\n                continue\n            if row_i == nbr:\n                assert abs(dist) < eps\n                continue\n            row.append(row_i)\n            col.append(nbr)\n            if use_sim:\n                w = 1 - w\n            data.append(w)\n    assert len(row) == len(col) == len(data)\n    spmat = csr_matrix((data, (row, col)), shape=(n, n))\n    return spmat\n\n\ndef fast_knns2spmat(knns, k, th_sim=0.7, use_sim=False, fill_value=None):\n    # convert knns to symmetric sparse matrix\n    from scipy.sparse import csr_matrix\n    eps = 1e-5\n    n = len(knns)\n    if isinstance(knns, list):\n        knns = np.array(knns)\n    if len(knns.shape) == 2:\n        # knns saved by hnsw has different shape\n        n = len(knns)\n        ndarr = np.ones([n, 2, k])\n        ndarr[:, 0, :] = -1  # assign unknown dist to 1 and nbr to -1\n        for i, (nbr, dist) in enumerate(knns):\n            size = len(nbr)\n            assert size == len(dist)\n            ndarr[i, 0, :size] = nbr[:size]\n            ndarr[i, 1, :size] = dist[:size]\n        knns = ndarr\n    nbrs = knns[:, 0, :]\n    dists = knns[:, 1, :]\n    assert -eps <= dists.min() <= dists.max(\n    ) <= 1 + eps, ""min: {}, max: {}"".format(dists.min(), dists.max())\n    if use_sim:\n        sims = 1. - dists\n    else:\n        sims = dists\n    if fill_value is not None:\n        print(\'[fast_knns2spmat] edge fill value:\', fill_value)\n        sims.fill(fill_value)\n    row, col = np.where(sims >= th_sim)\n    # remove the self-loop\n    idxs = np.where(row != nbrs[row, col])\n    row = row[idxs]\n    col = col[idxs]\n    data = sims[row, col]\n    col = nbrs[row, col]  # convert to absolute column\n    assert len(row) == len(col) == len(data)\n    spmat = csr_matrix((data, (row, col)), shape=(n, n))\n    return spmat\n\n\ndef knns2sub_spmat(idxs, knns, th_sim=0.7, use_sim=False):\n    # convert knns to symmetric sparse sub-matrix\n    from scipy.sparse import csr_matrix\n    n = len(idxs)\n    row, col, data = [], [], []\n    abs2rel = {}\n    for rel_i, abs_i in enumerate(idxs):\n        assert abs_i not in abs2rel\n        abs2rel[abs_i] = rel_i\n\n    for row_i, idx in enumerate(idxs):\n        nbrs, dists = knns[idx]\n        for nbr, dist in zip(nbrs, dists):\n            if idx == nbr:\n                assert abs(dist) < 1e-6, ""{}: {}"".format(idx, dist)\n                continue\n            if nbr not in abs2rel:\n                continue\n            col_i = abs2rel[nbr]\n            assert -1e-6 <= dist <= 1\n            w = dist\n            if 1 - w < th_sim or nbr == -1:\n                continue\n            row.append(row_i)\n            col.append(col_i)\n            if use_sim:\n                w = 1 - w\n            data.append(w)\n    assert len(row) == len(col) == len(data)\n    spmat = csr_matrix((data, (row, col)), shape=(n, n))\n    return spmat\n\n\ndef build_knns(knn_prefix,\n               feats,\n               knn_method,\n               k,\n               num_process=None,\n               is_rebuild=False,\n               feat_create_time=None):\n    knn_prefix = os.path.join(knn_prefix, \'{}_k_{}\'.format(knn_method, k))\n    mkdir_if_no_exists(knn_prefix)\n    knn_path = knn_prefix + \'.npz\'\n    if os.path.isfile(\n            knn_path) and not is_rebuild and feat_create_time is not None:\n        knn_create_time = os.path.getmtime(knn_path)\n        if knn_create_time <= feat_create_time:\n            print(\'[warn] knn is created before feats ({} vs {})\'.format(\n                format_time(knn_create_time), format_time(feat_create_time)))\n            is_rebuild = True\n    if not os.path.isfile(knn_path) or is_rebuild:\n        index_path = knn_prefix + \'.index\'\n        with Timer(\'build index\'):\n            if knn_method == \'hnsw\':\n                index = knn_hnsw(feats, k, index_path)\n            elif knn_method == \'faiss\':\n                index = knn_faiss(feats,\n                                  k,\n                                  index_path,\n                                  omp_num_threads=num_process,\n                                  rebuild_index=True)\n            elif knn_method == \'faiss_gpu\':\n                index = knn_faiss_gpu(feats,\n                                      k,\n                                      index_path,\n                                      num_process=num_process)\n            else:\n                raise KeyError(\n                    \'Only support hnsw and faiss currently ({}).\'.format(\n                        knn_method))\n            knns = index.get_knns()\n        with Timer(\'dump knns to {}\'.format(knn_path)):\n            dump_data(knn_path, knns, force=True)\n    else:\n        print(\'read knn from {}\'.format(knn_path))\n        knns = load_data(knn_path)\n    return knns\n\n\nclass knn():\n    def __init__(self, feats, k, index_path=\'\', verbose=True):\n        pass\n\n    def filter_by_th(self, i):\n        th_nbrs = []\n        th_dists = []\n        nbrs, dists = self.knns[i]\n        for n, dist in zip(nbrs, dists):\n            if 1 - dist < self.th:\n                continue\n            th_nbrs.append(n)\n            th_dists.append(dist)\n        th_nbrs = np.array(th_nbrs)\n        th_dists = np.array(th_dists)\n        return (th_nbrs, th_dists)\n\n    def get_knns(self, th=None):\n        if th is None or th <= 0.:\n            return self.knns\n        # TODO: optimize the filtering process by numpy\n        # nproc = mp.cpu_count()\n        nproc = 1\n        with Timer(\'filter edges by th {} (CPU={})\'.format(th, nproc),\n                   self.verbose):\n            self.th = th\n            self.th_knns = []\n            tot = len(self.knns)\n            if nproc > 1:\n                pool = mp.Pool(nproc)\n                th_knns = list(\n                    tqdm(pool.imap(self.filter_by_th, range(tot)), total=tot))\n                pool.close()\n            else:\n                th_knns = [self.filter_by_th(i) for i in range(tot)]\n            return th_knns\n\n\nclass knn_brute_force(knn):\n    def __init__(self, feats, k, index_path=\'\', verbose=True):\n        self.verbose = verbose\n        with Timer(\'[brute force] build index\', verbose):\n            feats = feats.astype(\'float32\')\n            sim = feats.dot(feats.T)\n        with Timer(\'[brute force] query topk {}\'.format(k), verbose):\n            nbrs = np.argpartition(-sim, kth=k)[:, :k]\n            idxs = np.array([i for i in range(nbrs.shape[0])])\n            dists = 1 - sim[idxs.reshape(-1, 1), nbrs]\n            self.knns = [(np.array(nbr, dtype=np.int32),\n                          np.array(dist, dtype=np.float32))\n                         for nbr, dist in zip(nbrs, dists)]\n\n\nclass knn_hnsw(knn):\n    def __init__(self, feats, k, index_path=\'\', verbose=True, **kwargs):\n        import nmslib\n        self.verbose = verbose\n        with Timer(\'[hnsw] build index\', verbose):\n            \'\'\' higher ef leads to better accuracy, but slower search\n                higher M leads to higher accuracy/run_time at fixed ef,\n                but consumes more memory\n            \'\'\'\n            # space_params = {\n            #     \'ef\': 100,\n            #     \'M\': 16,\n            # }\n            # index = nmslib.init(method=\'hnsw\',\n            #                     space=\'cosinesimil\',\n            #                     space_params=space_params)\n            index = nmslib.init(method=\'hnsw\', space=\'cosinesimil\')\n            if index_path != \'\' and os.path.isfile(index_path):\n                index.loadIndex(index_path)\n            else:\n                index.addDataPointBatch(feats)\n                index.createIndex({\n                    \'post\': 2,\n                    \'indexThreadQty\': 1\n                },\n                                  print_progress=verbose)\n                if index_path:\n                    print(\'[hnsw] save index to {}\'.format(index_path))\n                    mkdir_if_no_exists(index_path)\n                    index.saveIndex(index_path)\n        with Timer(\'[hnsw] query topk {}\'.format(k), verbose):\n            knn_ofn = index_path + \'.npz\'\n            if os.path.exists(knn_ofn):\n                print(\'[hnsw] read knns from {}\'.format(knn_ofn))\n                self.knns = np.load(knn_ofn)[\'data\']\n            else:\n                self.knns = index.knnQueryBatch(feats, k=k)\n\n\nclass knn_faiss(knn):\n    def __init__(self,\n                 feats,\n                 k,\n                 index_path=\'\',\n                 index_key=\'\',\n                 nprobe=128,\n                 omp_num_threads=None,\n                 rebuild_index=True,\n                 verbose=True,\n                 **kwargs):\n        import faiss\n        if omp_num_threads is not None:\n            faiss.omp_set_num_threads(omp_num_threads)\n        self.verbose = verbose\n        with Timer(\'[faiss] build index\', verbose):\n            if index_path != \'\' and not rebuild_index and os.path.exists(\n                    index_path):\n                print(\'[faiss] read index from {}\'.format(index_path))\n                index = faiss.read_index(index_path)\n            else:\n                feats = feats.astype(\'float32\')\n                size, dim = feats.shape\n                index = faiss.IndexFlatIP(dim)\n                if index_key != \'\':\n                    assert index_key.find(\n                        \'HNSW\') < 0, \'HNSW returns distances insted of sims\'\n                    metric = faiss.METRIC_INNER_PRODUCT\n                    nlist = min(4096, 8 * round(math.sqrt(size)))\n                    if index_key == \'IVF\':\n                        quantizer = index\n                        index = faiss.IndexIVFFlat(quantizer, dim, nlist,\n                                                   metric)\n                    else:\n                        index = faiss.index_factory(dim, index_key, metric)\n                    if index_key.find(\'Flat\') < 0:\n                        assert not index.is_trained\n                    index.train(feats)\n                    index.nprobe = min(nprobe, nlist)\n                    assert index.is_trained\n                    print(\'nlist: {}, nprobe: {}\'.format(nlist, nprobe))\n                index.add(feats)\n                if index_path != \'\':\n                    print(\'[faiss] save index to {}\'.format(index_path))\n                    mkdir_if_no_exists(index_path)\n                    faiss.write_index(index, index_path)\n        with Timer(\'[faiss] query topk {}\'.format(k), verbose):\n            knn_ofn = index_path + \'.npz\'\n            if os.path.exists(knn_ofn):\n                print(\'[faiss] read knns from {}\'.format(knn_ofn))\n                self.knns = np.load(knn_ofn)[\'data\']\n            else:\n                sims, nbrs = index.search(feats, k=k)\n                self.knns = [(np.array(nbr, dtype=np.int32),\n                              1 - np.array(sim, dtype=np.float32))\n                             for nbr, sim in zip(nbrs, sims)]\n\n\nclass knn_faiss_gpu(knn):\n    def __init__(self,\n                 feats,\n                 k,\n                 index_path=\'\',\n                 index_key=\'\',\n                 nprobe=128,\n                 num_process=4,\n                 is_precise=True,\n                 sort=True,\n                 verbose=True,\n                 **kwargs):\n        with Timer(\'[faiss_gpu] query topk {}\'.format(k), verbose):\n            knn_ofn = index_path + \'.npz\'\n            if os.path.exists(knn_ofn):\n                print(\'[faiss_gpu] read knns from {}\'.format(knn_ofn))\n                self.knns = np.load(knn_ofn)[\'data\']\n            else:\n                dists, nbrs = faiss_search_knn(feats,\n                                               k=k,\n                                               nprobe=nprobe,\n                                               num_process=num_process,\n                                               is_precise=is_precise,\n                                               sort=sort,\n                                               verbose=False)\n\n                self.knns = [(np.array(nbr, dtype=np.int32),\n                              np.array(dist, dtype=np.float32))\n                             for nbr, dist in zip(nbrs, dists)]\n\n\nif __name__ == \'__main__\':\n    from utils import l2norm\n\n    k = 30\n    d = 256\n    nfeat = 10000\n    np.random.seed(42)\n\n    feats = np.random.random((nfeat, d)).astype(\'float32\')\n    feats = l2norm(feats)\n\n    index1 = knn_hnsw(feats, k)\n    index2 = knn_faiss(feats, k)\n    index3 = knn_faiss(feats, k, index_key=\'Flat\')\n    index4 = knn_faiss(feats, k, index_key=\'IVF\')\n    index5 = knn_faiss(feats, k, index_key=\'IVF100,PQ32\')\n\n    print(index1.knns[0])\n    print(index2.knns[0])\n    print(index3.knns[0])\n    print(index4.knns[0])\n    print(index5.knns[0])\n'"
utils/logger.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport logging\n\n\ndef create_logger(name=\'global_logger\', log_file=None):\n    """""" use different log level for file and stream\n    """"""\n    logger = logging.getLogger(name)\n    formatter = logging.Formatter(\'[%(asctime)s] %(message)s\')\n    logger.setLevel(logging.DEBUG)\n\n    sh = logging.StreamHandler()\n    sh.setFormatter(formatter)\n    sh.setLevel(logging.INFO)\n    logger.addHandler(sh)\n\n    if log_file is not None:\n        fh = logging.FileHandler(log_file)\n        fh.setFormatter(formatter)\n        fh.setLevel(logging.DEBUG)\n        logger.addHandler(fh)\n\n    return logger\n\n\nif __name__ == \'__main__\':\n    logger = create_logger(\'test\')\n    logger = create_logger(\'test\', \'log.txt\')\n    logger.info(\'output to file and stream\')\n    logger.debug(\'output to file\')\n'"
utils/misc.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport time\nimport json\nimport pickle\nimport random\nimport numpy as np\n\n\nclass TextColors:\n    HEADER = \'\\033[35m\'\n    OKBLUE = \'\\033[34m\'\n    OKGREEN = \'\\033[32m\'\n    WARNING = \'\\033[33m\'\n    FATAL = \'\\033[31m\'\n    ENDC = \'\\033[0m\'\n    BOLD = \'\\033[1m\'\n    UNDERLINE = \'\\033[4m\'\n\n\nclass Timer():\n    def __init__(self, name=\'task\', verbose=True):\n        self.name = name\n        self.verbose = verbose\n\n    def __enter__(self):\n        self.start = time.time()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.verbose:\n            print(\'[Time] {} consumes {:.4f} s\'.format(\n                self.name,\n                time.time() - self.start))\n        return exc_type is None\n\n\ndef set_random_seed(seed, cuda=False):\n    import torch\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if cuda:\n        torch.cuda.manual_seed_all(seed)\n\n\ndef l2norm(vec):\n    vec /= np.linalg.norm(vec, axis=1).reshape(-1, 1)\n    return vec\n\n\ndef is_l2norm(features, size):\n    rand_i = random.choice(range(size))\n    norm_ = np.dot(features[rand_i, :], features[rand_i, :])\n    return abs(norm_ - 1) < 1e-6\n\n\ndef is_spmat_eq(a, b):\n    return (a != b).nnz == 0\n\n\ndef aggregate(features, adj, times):\n    dtype = features.dtype\n    for i in range(times):\n        features = adj * features\n    return features.astype(dtype)\n\n\ndef read_probs(path, inst_num, feat_dim, dtype=np.float32, verbose=False):\n    assert (inst_num > 0 or inst_num == -1) and feat_dim > 0\n    count = -1\n    if inst_num > 0:\n        count = inst_num * feat_dim\n    probs = np.fromfile(path, dtype=dtype, count=count)\n    if feat_dim > 1:\n        probs = probs.reshape(inst_num, feat_dim)\n    if verbose:\n        print(\'[{}] shape: {}\'.format(path, probs.shape))\n    return probs\n\n\ndef read_meta(fn_meta, start_pos=0, verbose=True):\n    lb2idxs = {}\n    idx2lb = {}\n    with open(fn_meta) as f:\n        for idx, x in enumerate(f.readlines()[start_pos:]):\n            lb = int(x.strip())\n            if lb not in lb2idxs:\n                lb2idxs[lb] = []\n            lb2idxs[lb] += [idx]\n            idx2lb[idx] = lb\n\n    inst_num = len(idx2lb)\n    cls_num = len(lb2idxs)\n    if verbose:\n        print(\'[{}] #cls: {}, #inst: {}\'.format(fn_meta, cls_num, inst_num))\n    return lb2idxs, idx2lb\n\n\ndef write_meta(ofn, idx2lb, inst_num=None):\n    if len(idx2lb) == 0:\n        print(\'[warn] idx2lb is empty! skip write idx2lb to {}\'.format(ofn))\n        return\n\n    if inst_num is None:\n        inst_num = max(idx2lb.keys()) + 1\n    cls_num = len(set(idx2lb.values()))\n\n    idx2newlb = {}\n    current_lb = 0\n    discard_lb = 0\n    map2newlb = {}\n    for idx in range(inst_num):\n        if idx in idx2lb:\n            lb = idx2lb[idx]\n            if lb in map2newlb:\n                newlb = map2newlb[lb]\n            else:\n                newlb = current_lb\n                map2newlb[lb] = newlb\n                current_lb += 1\n        else:\n            newlb = cls_num + discard_lb\n            discard_lb += 1\n        idx2newlb[idx] = newlb\n    assert current_lb == cls_num, \'{} vs {}\'.format(current_lb, cls_num)\n\n    print(\'#discard: {}, #lbs: {}\'.format(discard_lb, current_lb))\n    print(\'#inst: {}, #class: {}\'.format(inst_num, cls_num))\n    if ofn is not None:\n        print(\'save label to\', ofn)\n        with open(ofn, \'w\') as of:\n            for idx in range(inst_num):\n                of.write(str(idx2newlb[idx]) + \'\\n\')\n\n    pred_labels = intdict2ndarray(idx2newlb)\n    return pred_labels\n\n\ndef write_feat(ofn, features):\n    print(\'save features to\', ofn)\n    features.tofile(ofn)\n\n\ndef dump2npz(ofn, data, force=False):\n    if os.path.exists(ofn) and not force:\n        return\n    np.savez_compressed(ofn, data=data)\n\n\ndef dump2json(ofn, data, force=False):\n    if os.path.exists(ofn) and not force:\n        return\n\n    def default(obj):\n        if isinstance(obj, np.int32):\n            return int(obj)\n        elif isinstance(obj, np.int64):\n            return int(obj)\n        elif isinstance(obj, np.float32):\n            return float(obj)\n        elif isinstance(obj, set) or isinstance(obj, np.ndarray):\n            return list(obj)\n        else:\n            raise TypeError(""Unserializable object {} of type {}"".format(\n                obj, type(obj)))\n\n    with open(ofn, \'w\') as of:\n        json.dump(data, of, default=default)\n\n\ndef dump2pkl(ofn, data, force=False):\n    if os.path.exists(ofn) and not force:\n        return\n    with open(ofn, \'wb\') as of:\n        pickle.dump(data, of)\n\n\ndef dump_data(ofn, data, force=False, verbose=False):\n    if os.path.exists(ofn) and not force:\n        if verbose:\n            print(\n                \'{} already exists. Set force=True to overwrite.\'.format(ofn))\n        return\n    mkdir_if_no_exists(ofn)\n    if ofn.endswith(\'.json\'):\n        dump2json(ofn, data, force=force)\n    elif ofn.endswith(\'.pkl\'):\n        dump2pkl(ofn, data, force=force)\n    else:\n        dump2npz(ofn, data, force=force)\n\n\ndef load_npz(fn):\n    return np.load(fn, allow_pickle=True)[\'data\']\n\n\ndef load_pkl(fn):\n    return pickle.load(open(fn, \'rb\'))\n\n\ndef load_json(fn):\n    return json.load(open(fn, \'r\'))\n\n\ndef load_data(ofn):\n    if ofn.endswith(\'.json\'):\n        return load_json(ofn)\n    elif ofn.endswith(\'.pkl\'):\n        return load_pkl(ofn)\n    else:\n        return load_npz(ofn)\n\n\ndef labels2clusters(lb2idxs):\n    clusters = [idxs for _, idxs in lb2idxs.items()]\n    return clusters\n\n\ndef clusters2labels(clusters):\n    idx2lb = {}\n    for lb, cluster in enumerate(clusters):\n        for v in cluster:\n            idx2lb[v] = lb\n    return idx2lb\n\n\ndef intdict2ndarray(d, default_val=-1):\n    arr = np.zeros(len(d)) + default_val\n    for k, v in d.items():\n        arr[k] = v\n    return arr\n\n\ndef list2dict(labels, ignore_value=-1):\n    idx2lb = {}\n    for idx, lb in enumerate(labels):\n        if lb == ignore_value:\n            continue\n        idx2lb[idx] = lb\n    return idx2lb\n\n\ndef mkdir_if_no_exists(path, subdirs=[\'\'], is_folder=False):\n    if path == \'\':\n        return\n    for sd in subdirs:\n        if sd != \'\' or is_folder:\n            d = os.path.dirname(os.path.join(path, sd))\n        else:\n            d = os.path.dirname(path)\n        if not os.path.exists(d):\n            os.makedirs(d)\n\n\ndef rm_suffix(s, suffix=None):\n    if suffix is None:\n        return s[:s.rfind(\'.\')]\n    else:\n        return s[:s.rfind(suffix)]\n\n\ndef rand_argmax(v):\n    assert len(v.squeeze().shape) == 1\n    return np.random.choice(np.flatnonzero(v == v.max()))\n\n\ndef create_temp_file_if_exist(path, suffix=\'\'):\n    path_with_suffix = path + suffix\n    if not os.path.exists(path_with_suffix):\n        return path_with_suffix\n    else:\n        i = 0\n        while i < 1000:\n            temp_path = \'{}_{}\'.format(path, i) + suffix\n            i += 1\n            if not os.path.exists(temp_path):\n                return temp_path\n'"
utils/misc_cluster.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\ndef filter_clusters(clusters, min_size=None, max_size=None):\n    if min_size is not None:\n        clusters = [c for c in clusters if len(c) >= min_size]\n    if max_size is not None:\n        clusters = [c for c in clusters if len(c) <= max_size]\n    return clusters\n\n\ndef get_cluster_idxs(clusters, size=1):\n    idxs = []\n    for c in clusters:\n        if len(c) == size:\n            idxs.extend(c)\n    return idxs\n'"
vegcn/__init__.py,0,"b'from .test_gcn_v import test_gcn_v\nfrom .test_gcn_e import test_gcn_e\nfrom .train_gcn_v import train_gcn_v\nfrom .train_gcn_e import train_gcn_e\n\n__factory__ = {\n    \'test_gcn_v\': test_gcn_v,\n    \'test_gcn_e\': test_gcn_e,\n    \'train_gcn_v\': train_gcn_v,\n    \'train_gcn_e\': train_gcn_e,\n}\n\n\ndef build_handler(phase, model):\n    key_handler = \'{}_{}\'.format(phase, model)\n    if key_handler not in __factory__:\n        raise KeyError(""Unknown op:"", key_handler)\n    return __factory__[key_handler]\n'"
vegcn/confidence.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom itertools import groupby\n\n__all__ = [\'density\', \'confidence\', \'confidence_to_peaks\']\n\n\ndef density(dists, radius=0.3, use_weight=True):\n    row, col = (dists < radius).nonzero()\n\n    num, _ = dists.shape\n    if use_weight:\n        density = np.zeros((num, ), dtype=np.float32)\n        for r, c in zip(row, col):\n            density[r] += 1 - dists[r, c]\n    else:\n        density = np.zeros((num, ), dtype=np.int32)\n        for k, g in groupby(row):\n            density[k] = len(list(g))\n    return density\n\n\ndef s_nbr(dists, nbrs, idx2lb, **kwargs):\n    \'\'\' use supervised confidence defined on neigborhood\n    \'\'\'\n    num, _ = dists.shape\n    conf = np.zeros((num, ), dtype=np.float32)\n    contain_neg = 0\n    for i, (nbr, dist) in enumerate(zip(nbrs, dists)):\n        lb = idx2lb[i]\n        pos, neg = 0, 0\n        for j, n in enumerate(nbr):\n            if idx2lb[n] == lb:\n                pos += 1 - dist[j]\n            else:\n                neg += 1 - dist[j]\n        conf[i] = pos - neg\n        if neg > 0:\n            contain_neg += 1\n    print(\'#contain_neg:\', contain_neg)\n    conf /= np.abs(conf).max()\n    return conf\n\n\ndef s_nbr_size_norm(dists, nbrs, idx2lb, **kwargs):\n    \'\'\' use supervised confidence defined on neigborhood (norm by size)\n    \'\'\'\n    num, _ = dists.shape\n    conf = np.zeros((num, ), dtype=np.float32)\n    contain_neg = 0\n    max_size = 0\n    for i, (nbr, dist) in enumerate(zip(nbrs, dists)):\n        size = 0\n        pos, neg = 0, 0\n        lb = idx2lb[i]\n        for j, n in enumerate(nbr):\n            if idx2lb[n] == lb:\n                pos += 1 - dist[j]\n            else:\n                neg += 1 - dist[j]\n            size += 1\n        conf[i] = pos - neg\n        max_size = max(max_size, size)\n        if neg > 0:\n            contain_neg += 1\n    print(\'#contain_neg:\', contain_neg)\n    print(\'max_size: {}\'.format(max_size))\n    conf /= max_size\n    return conf\n\n\ndef s_avg(feats, idx2lb, lb2idxs, **kwargs):\n    \'\'\' use average similarity of intra-nodes\n    \'\'\'\n    num = len(idx2lb)\n    conf = np.zeros((num, ), dtype=np.float32)\n    for i in range(num):\n        lb = idx2lb[i]\n        idxs = lb2idxs[lb]\n        idxs.remove(i)\n        if len(idxs) == 0:\n            continue\n        feat = feats[i, :]\n        conf[i] = feat.dot(feats[idxs, :].T).mean()\n    eps = 1e-6\n    assert -1 - eps <= conf.min() <= conf.max(\n    ) <= 1 + eps, ""min: {}, max: {}"".format(conf.min(), conf.max())\n    return conf\n\n\ndef s_center(feats, idx2lb, lb2idxs, **kwargs):\n    \'\'\' use average similarity of intra-nodes\n    \'\'\'\n    num = len(idx2lb)\n    conf = np.zeros((num, ), dtype=np.float32)\n    for i in range(num):\n        lb = idx2lb[i]\n        idxs = lb2idxs[lb]\n        if len(idxs) == 0:\n            continue\n        feat = feats[i, :]\n        feat_center = feats[idxs, :].mean(axis=0)\n        conf[i] = feat.dot(feat_center.T)\n    eps = 1e-6\n    assert -1 - eps <= conf.min() <= conf.max(\n    ) <= 1 + eps, ""min: {}, max: {}"".format(conf.min(), conf.max())\n    return conf\n\n\ndef confidence(metric=\'s_nbr\', **kwargs):\n    metric2func = {\n        \'s_nbr\': s_nbr,\n        \'s_nbr_size_norm\': s_nbr_size_norm,\n        \'s_avg\': s_avg,\n        \'s_center\': s_center,\n    }\n    if metric in metric2func:\n        func = metric2func[metric]\n    else:\n        raise KeyError(\'Only support confidence metircs: {}\'.format(\n            metric2func.keys()))\n\n    conf = func(**kwargs)\n    return conf\n\n\ndef confidence_to_peaks(dists, nbrs, confidence, max_conn=1):\n    # Note that dists has been sorted in ascending order\n    assert dists.shape[0] == confidence.shape[0]\n    assert dists.shape == nbrs.shape\n\n    num, _ = dists.shape\n    dist2peak = {i: [] for i in range(num)}\n    peaks = {i: [] for i in range(num)}\n\n    for i, nbr in tqdm(enumerate(nbrs)):\n        nbr_conf = confidence[nbr]\n        for j, c in enumerate(nbr_conf):\n            nbr_idx = nbr[j]\n            if i == nbr_idx or c <= confidence[i]:\n                continue\n            dist2peak[i].append(dists[i, j])\n            peaks[i].append(nbr_idx)\n            if len(dist2peak[i]) >= max_conn:\n                break\n    return dist2peak, peaks\n'"
vegcn/deduce.py,0,"b""import numpy as np\n\n__all__ = ['peaks_to_labels']\n\n\ndef _find_parent(parent, u):\n    idx = []\n    # parent is a fixed point\n    while (u != parent[u]):\n        idx.append(u)\n        u = parent[u]\n    for i in idx:\n        parent[i] = u\n    return u\n\n\ndef edge_to_connected_graph(edges, num):\n    parent = list(range(num))\n    for u, v in edges:\n        p_u = _find_parent(parent, u)\n        p_v = _find_parent(parent, v)\n        parent[p_u] = p_v\n\n    for i in range(num):\n        parent[i] = _find_parent(parent, i)\n    remap = {}\n    uf = np.unique(np.array(parent))\n    for i, f in enumerate(uf):\n        remap[f] = i\n    cluster_id = np.array([remap[f] for f in parent])\n    return cluster_id\n\n\ndef peaks_to_edges(peaks, dist2peak, tau):\n    edges = []\n    for src in peaks:\n        dsts = peaks[src]\n        dists = dist2peak[src]\n        for dst, dist in zip(dsts, dists):\n            if src == dst or dist >= 1 - tau:\n                continue\n            edges.append([src, dst])\n    return edges\n\n\ndef peaks_to_labels(peaks, dist2peak, tau, inst_num):\n    edges = peaks_to_edges(peaks, dist2peak, tau)\n    pred_labels = edge_to_connected_graph(edges, inst_num)\n    return pred_labels\n"""
vegcn/extract.py,1,"b""from __future__ import division\n\nimport torch\nimport os.path as osp\nimport numpy as np\n\nfrom vegcn.datasets import build_dataset\nfrom vegcn.models import build_model\nfrom vegcn.test_gcn_v import test\n\nfrom utils import create_logger, write_feat, mkdir_if_no_exists\n\n\ndef extract_gcn_v(opath_feat, opath_pred_confs, data_name, cfg):\n    if osp.isfile(opath_feat) and osp.isfile(opath_pred_confs):\n        print('{} and {} already exist.'.format(opath_feat, opath_pred_confs))\n        return\n    cfg.cuda = torch.cuda.is_available()\n\n    logger = create_logger()\n\n    model = build_model(cfg.model['type'], **cfg.model['kwargs'])\n\n    for k, v in cfg.model['kwargs'].items():\n        setattr(cfg[data_name], k, v)\n    cfg[data_name].eval_interim = False\n\n    dataset = build_dataset(cfg.model['type'], cfg[data_name])\n\n    pred_confs, gcn_feat = test(model, dataset, cfg, logger)\n\n    logger.info('save predicted confs to {}'.format(opath_pred_confs))\n    mkdir_if_no_exists(opath_pred_confs)\n    np.savez_compressed(opath_pred_confs,\n                        pred_confs=pred_confs,\n                        inst_num=dataset.inst_num)\n\n    logger.info('save gcn features to {}'.format(opath_feat))\n    mkdir_if_no_exists(opath_feat)\n    write_feat(opath_feat, gcn_feat)\n"""
vegcn/main.py,3,"b""from __future__ import division\n\nimport os\nimport torch\nimport argparse\n\nfrom mmcv import Config\n\nfrom utils import (create_logger, set_random_seed, rm_suffix,\n                   mkdir_if_no_exists)\n\nfrom vegcn.models import build_model\nfrom vegcn import build_handler\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='LTC via Confidence and Connectivity Estimation')\n    parser.add_argument('--config', help='config file path')\n    parser.add_argument('--seed', type=int, default=42, help='random seed')\n    parser.add_argument('--phase', choices=['test', 'train'], default='test')\n    parser.add_argument('--work_dir', help='the dir to save logs and models')\n    parser.add_argument('--load_from',\n                        default=None,\n                        help='the checkpoint file to load from')\n    parser.add_argument('--resume_from',\n                        default=None,\n                        help='the checkpoint file to resume from')\n    parser.add_argument(\n        '--gpus',\n        type=int,\n        default=1,\n        help='number of gpus(only applicable to non-distributed training)')\n    parser.add_argument('--random_conns', action='store_true', default=False)\n    parser.add_argument('--distributed', action='store_true', default=False)\n    parser.add_argument('--eval_interim', action='store_true', default=False)\n    parser.add_argument('--save_output', action='store_true', default=False)\n    parser.add_argument('--no_cuda', action='store_true', default=False)\n    parser.add_argument('--force', action='store_true', default=False)\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n    cfg = Config.fromfile(args.config)\n\n    # set cuda\n    cfg.cuda = not args.no_cuda and torch.cuda.is_available()\n\n    # set cudnn_benchmark & cudnn_deterministic\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    if cfg.get('cudnn_deterministic', False):\n        torch.backends.cudnn.deterministic = True\n\n    # update configs according to args\n    if not hasattr(cfg, 'work_dir'):\n        if args.work_dir is not None:\n            cfg.work_dir = args.work_dir\n        else:\n            cfg_name = rm_suffix(os.path.basename(args.config))\n            cfg.work_dir = os.path.join('./data/work_dir', cfg_name)\n    mkdir_if_no_exists(cfg.work_dir, is_folder=True)\n\n    cfg.load_from = args.load_from\n    cfg.resume_from = args.resume_from\n\n    cfg.gpus = args.gpus\n    cfg.distributed = args.distributed\n\n    cfg.random_conns = args.random_conns\n    cfg.eval_interim = args.eval_interim\n    cfg.save_output = args.save_output\n    cfg.force = args.force\n\n    for data in ['train_data', 'test_data']:\n        if not hasattr(cfg, data):\n            continue\n        cfg[data].eval_interim = cfg.eval_interim\n        if not hasattr(cfg[data], 'knn_graph_path') or not os.path.isfile(\n                cfg[data].knn_graph_path):\n            cfg[data].prefix = cfg.prefix\n            cfg[data].knn = cfg.knn\n            cfg[data].knn_method = cfg.knn_method\n            name = 'train_name' if data == 'train_data' else 'test_name'\n            cfg[data].name = cfg[name]\n\n    logger = create_logger()\n\n    # set random seeds\n    if args.seed is not None:\n        logger.info('Set random seed to {}'.format(args.seed))\n        set_random_seed(args.seed)\n\n    model = build_model(cfg.model['type'], **cfg.model['kwargs'])\n    handler = build_handler(args.phase, cfg.model['type'])\n\n    handler(model, cfg, logger)\n\n\nif __name__ == '__main__':\n    main()\n"""
vegcn/test_gcn_e.py,1,"b""from __future__ import division\n\nimport torch\nimport numpy as np\nimport os.path as osp\n\nfrom mmcv.runner import load_checkpoint\nfrom mmcv.parallel import MMDataParallel\n\nfrom vegcn.datasets import build_dataset\nfrom vegcn.deduce import peaks_to_labels\nfrom lgcn.datasets import build_dataloader\n\nfrom utils import (list2dict, write_meta, mkdir_if_no_exists, Timer)\nfrom evaluation import evaluate, accuracy\n\n\ndef output_accuracy(output, labels):\n    preds = output.max(1)[1].type_as(labels)\n    correct = preds.eq(labels).double()\n    correct = correct.sum()\n    return correct / len(labels)\n\n\ndef test(model, dataset, cfg, logger):\n    if cfg.load_from:\n        print('load from {}'.format(cfg.load_from))\n        load_checkpoint(model, cfg.load_from, strict=True, logger=logger)\n\n    losses = []\n    accs = []\n    pred_conns = []\n\n    max_lst = []\n    multi_max = []\n\n    if cfg.gpus == 1:\n        data_loader = build_dataloader(dataset,\n                                       cfg.batch_size_per_gpu,\n                                       cfg.workers_per_gpu,\n                                       train=False)\n        size = len(data_loader)\n\n        model = MMDataParallel(model, device_ids=range(cfg.gpus))\n        if cfg.cuda:\n            model.cuda()\n\n        model.eval()\n        for i, data in enumerate(data_loader):\n            with torch.no_grad():\n                output, loss = model(data, return_loss=True)\n                if not dataset.ignore_label:\n                    labels = data[2].view(-1)\n                    if not cfg.regressor:\n                        acc = output_accuracy(output, labels)\n                        accs += [acc.item()]\n                    losses += [loss.item()]\n                if not cfg.regressor:\n                    output = output[:, 1]\n                if cfg.max_conn == 1:\n                    output_max = output.max()\n                    pred = (output == output_max).nonzero().view(-1)\n                    pred_size = len(pred)\n                    if pred_size > 1:\n                        multi_max.append(pred_size)\n                        pred_i = np.random.choice(np.arange(pred_size))\n                    else:\n                        pred_i = 0\n                    pred = [int(pred[pred_i].detach().cpu().numpy())]\n                    max_lst.append(output_max.detach().cpu().numpy())\n                elif cfg.max_conn > 1:\n                    output = output.detach().cpu().numpy()\n                    pred = output.argpartition(cfg.max_conn)[:cfg.max_conn]\n                pred_conns.append(pred)\n                if i % cfg.log_config.interval == 0:\n                    if dataset.ignore_label:\n                        logger.info('[Test] Iter {}/{}'.format(i, size))\n                    else:\n                        logger.info('[Test] Iter {}/{}: Loss {:.4f}'.format(\n                            i, size, loss))\n    else:\n        raise NotImplementedError\n\n    if not dataset.ignore_label:\n        avg_loss = sum(losses) / len(losses)\n        logger.info('[Test] Overall Loss {:.4f}'.format(avg_loss))\n        if not cfg.regressor:\n            avg_acc = sum(accs) / len(accs)\n            logger.info('[Test] Overall Accuracy {:.4f}'.format(avg_acc))\n    if size > 0:\n        logger.info('max val: mean({:.2f}), max({:.2f}), min({:.2f})'.format(\n            sum(max_lst) / size, max(max_lst), min(max_lst)))\n    multi_max_size = len(multi_max)\n    if multi_max_size > 0:\n        logger.info('multi-max({:.2f}): mean({:.1f}), max({}), min({})'.format(\n            1. * multi_max_size / size,\n            sum(multi_max) / multi_max_size, max(multi_max), min(multi_max)))\n\n    return np.array(pred_conns)\n\n\ndef test_gcn_e(model, cfg, logger):\n    for k, v in cfg.model['kwargs'].items():\n        setattr(cfg.test_data, k, v)\n    dataset = build_dataset(cfg.model['type'], cfg.test_data)\n\n    pred_peaks = dataset.peaks\n    pred_dist2peak = dataset.dist2peak\n\n    ofn_pred = osp.join(cfg.work_dir, 'pred_conns.npz')\n    if osp.isfile(ofn_pred) and not cfg.force:\n        data = np.load(ofn_pred)\n        pred_conns = data['pred_conns']\n        inst_num = data['inst_num']\n        if inst_num != dataset.inst_num:\n            logger.warn(\n                'instance number in {} is different from dataset: {} vs {}'.\n                format(ofn_pred, inst_num, len(dataset)))\n    else:\n        if cfg.random_conns:\n            pred_conns = []\n            for nbr, dist, idx in zip(dataset.subset_nbrs,\n                                      dataset.subset_dists,\n                                      dataset.subset_idxs):\n                for _ in range(cfg.max_conn):\n                    pred_rel_nbr = np.random.choice(np.arange(len(nbr)))\n                    pred_abs_nbr = nbr[pred_rel_nbr]\n                    pred_peaks[idx].append(pred_abs_nbr)\n                    pred_dist2peak[idx].append(dist[pred_rel_nbr])\n                    pred_conns.append(pred_rel_nbr)\n            pred_conns = np.array(pred_conns)\n        else:\n            pred_conns = test(model, dataset, cfg, logger)\n            for pred_rel_nbr, nbr, dist, idx in zip(pred_conns,\n                                                    dataset.subset_nbrs,\n                                                    dataset.subset_dists,\n                                                    dataset.subset_idxs):\n                pred_abs_nbr = nbr[pred_rel_nbr]\n                pred_peaks[idx].extend(pred_abs_nbr)\n                pred_dist2peak[idx].extend(dist[pred_rel_nbr])\n        inst_num = dataset.inst_num\n\n    if len(pred_conns) > 0:\n        logger.info(\n            'pred_conns (nbr order): mean({:.1f}), max({}), min({})'.format(\n                pred_conns.mean(), pred_conns.max(), pred_conns.min()))\n\n    if not dataset.ignore_label and cfg.eval_interim:\n        subset_gt_labels = dataset.subset_gt_labels\n        for i in range(cfg.max_conn):\n            pred_peaks_labels = np.array([\n                dataset.idx2lb[pred_peaks[idx][i]]\n                for idx in dataset.subset_idxs\n            ])\n\n            acc = accuracy(pred_peaks_labels, subset_gt_labels)\n            logger.info(\n                '[{}-th] accuracy of pred_peaks labels ({}): {:.4f}'.format(\n                    i, len(pred_peaks_labels), acc))\n\n            # the rule for nearest nbr is only appropriate when nbrs is sorted\n            nearest_idxs = np.where(pred_conns[:, i] == 0)[0]\n            acc = accuracy(pred_peaks_labels[nearest_idxs],\n                           subset_gt_labels[nearest_idxs])\n            logger.info(\n                '[{}-th] accuracy of pred labels (nearest: {}): {:.4f}'.format(\n                    i, len(nearest_idxs), acc))\n\n            not_nearest_idxs = np.where(pred_conns[:, i] > 0)[0]\n            acc = accuracy(pred_peaks_labels[not_nearest_idxs],\n                           subset_gt_labels[not_nearest_idxs])\n            logger.info(\n                '[{}-th] accuracy of pred labels (not nearest: {}): {:.4f}'.\n                format(i, len(not_nearest_idxs), acc))\n\n    with Timer('Peaks to clusters (th_cut={})'.format(cfg.tau)):\n        pred_labels = peaks_to_labels(pred_peaks, pred_dist2peak, cfg.tau,\n                                      inst_num)\n\n    if cfg.save_output:\n        logger.info(\n            'save predicted connectivity and labels to {}'.format(ofn_pred))\n        if not osp.isfile(ofn_pred) or cfg.force:\n            np.savez_compressed(ofn_pred,\n                                pred_conns=pred_conns,\n                                inst_num=inst_num)\n\n        # save clustering results\n        idx2lb = list2dict(pred_labels, ignore_value=-1)\n\n        folder = '{}_gcne_k_{}_th_{}_ig_{}'.format(cfg.test_name, cfg.knn,\n                                                   cfg.th_sim,\n                                                   cfg.test_data.ignore_ratio)\n        opath_pred_labels = osp.join(cfg.work_dir, folder,\n                                     'tau_{}_pred_labels.txt'.format(cfg.tau))\n        mkdir_if_no_exists(opath_pred_labels)\n        write_meta(opath_pred_labels, idx2lb, inst_num=inst_num)\n\n    # evaluation\n    if not dataset.ignore_label:\n        print('==> evaluation')\n        for metric in cfg.metrics:\n            evaluate(dataset.gt_labels, pred_labels, metric)\n"""
vegcn/test_gcn_v.py,3,"b""from __future__ import division\n\nimport torch\nimport numpy as np\nimport os.path as osp\nimport torch.nn.functional as F\n\nfrom mmcv.runner import load_checkpoint\n\nfrom vegcn.datasets import build_dataset\nfrom vegcn.confidence import confidence_to_peaks\nfrom vegcn.deduce import peaks_to_labels\n\nfrom utils import (sparse_mx_to_torch_sparse_tensor, list2dict, write_meta,\n                   write_feat, mkdir_if_no_exists, rm_suffix, build_knns,\n                   knns2ordered_nbrs, BasicDataset, Timer)\nfrom evaluation import evaluate, accuracy\n\n\ndef test(model, dataset, cfg, logger):\n    if cfg.load_from:\n        logger.info('load from {}'.format(cfg.load_from))\n        load_checkpoint(model, cfg.load_from, strict=True, logger=logger)\n\n    features = torch.FloatTensor(dataset.features)\n    adj = sparse_mx_to_torch_sparse_tensor(dataset.adj)\n    if not dataset.ignore_label:\n        labels = torch.FloatTensor(dataset.labels)\n\n    if cfg.cuda:\n        model.cuda()\n        features = features.cuda()\n        adj = adj.cuda()\n        labels = labels.cuda()\n\n    model.eval()\n    output, gcn_feat = model((features, adj), output_feat=True)\n    if not dataset.ignore_label:\n        loss = F.mse_loss(output, labels)\n        loss_test = float(loss)\n        logger.info('[Test] loss = {:.4f}'.format(loss_test))\n\n    pred_confs = output.detach().cpu().numpy()\n    gcn_feat = gcn_feat.detach().cpu().numpy()\n    return pred_confs, gcn_feat\n\n\ndef test_gcn_v(model, cfg, logger):\n    for k, v in cfg.model['kwargs'].items():\n        setattr(cfg.test_data, k, v)\n    dataset = build_dataset(cfg.model['type'], cfg.test_data)\n\n    folder = '{}_gcnv_k_{}_th_{}'.format(cfg.test_name, cfg.knn, cfg.th_sim)\n    oprefix = osp.join(cfg.work_dir, folder)\n    oname = osp.basename(rm_suffix(cfg.load_from))\n    opath_pred_confs = osp.join(oprefix, 'pred_confs', '{}.npz'.format(oname))\n\n    if osp.isfile(opath_pred_confs) and not cfg.force:\n        data = np.load(opath_pred_confs)\n        pred_confs = data['pred_confs']\n        inst_num = data['inst_num']\n        if inst_num != dataset.inst_num:\n            logger.warn(\n                'instance number in {} is different from dataset: {} vs {}'.\n                format(opath_pred_confs, inst_num, len(dataset)))\n    else:\n        pred_confs, gcn_feat = test(model, dataset, cfg, logger)\n        inst_num = dataset.inst_num\n\n    logger.info('pred_confs: mean({:.4f}). max({:.4f}), min({:.4f})'.format(\n        pred_confs.mean(), pred_confs.max(), pred_confs.min()))\n\n    logger.info('Convert to cluster')\n    with Timer('Predition to peaks'):\n        pred_dist2peak, pred_peaks = confidence_to_peaks(\n            dataset.dists, dataset.nbrs, pred_confs, cfg.max_conn)\n\n    if not dataset.ignore_label and cfg.eval_interim:\n        # evaluate the intermediate results\n        for i in range(cfg.max_conn):\n            num = len(dataset.peaks)\n            pred_peaks_i = np.arange(num)\n            peaks_i = np.arange(num)\n            for j in range(num):\n                if len(pred_peaks[j]) > i:\n                    pred_peaks_i[j] = pred_peaks[j][i]\n                if len(dataset.peaks[j]) > i:\n                    peaks_i[j] = dataset.peaks[j][i]\n            acc = accuracy(pred_peaks_i, peaks_i)\n            logger.info('[{}-th conn] accuracy of peak match: {:.4f}'.format(\n                i + 1, acc))\n            acc = 0.\n            for idx, peak in enumerate(pred_peaks_i):\n                acc += int(dataset.idx2lb[peak] == dataset.idx2lb[idx])\n            acc /= len(pred_peaks_i)\n            logger.info(\n                '[{}-th conn] accuracy of peak label match: {:.4f}'.format(\n                    i + 1, acc))\n\n    with Timer('Peaks to clusters (th_cut={})'.format(cfg.tau_0)):\n        pred_labels = peaks_to_labels(pred_peaks, pred_dist2peak, cfg.tau_0,\n                                      inst_num)\n\n    if cfg.save_output:\n        logger.info('save predicted confs to {}'.format(opath_pred_confs))\n        mkdir_if_no_exists(opath_pred_confs)\n        np.savez_compressed(opath_pred_confs,\n                            pred_confs=pred_confs,\n                            inst_num=inst_num)\n\n        # save clustering results\n        idx2lb = list2dict(pred_labels, ignore_value=-1)\n\n        opath_pred_labels = osp.join(\n            cfg.work_dir, folder, 'tau_{}_pred_labels.txt'.format(cfg.tau_0))\n        logger.info('save predicted labels to {}'.format(opath_pred_labels))\n        mkdir_if_no_exists(opath_pred_labels)\n        write_meta(opath_pred_labels, idx2lb, inst_num=inst_num)\n\n    # evaluation\n    if not dataset.ignore_label:\n        print('==> evaluation')\n        for metric in cfg.metrics:\n            evaluate(dataset.gt_labels, pred_labels, metric)\n\n    if cfg.use_gcn_feat:\n        # gcn_feat is saved to disk for GCN-E\n        opath_feat = osp.join(oprefix, 'features', '{}.bin'.format(oname))\n        if not osp.isfile(opath_feat) or cfg.force:\n            mkdir_if_no_exists(opath_feat)\n            write_feat(opath_feat, gcn_feat)\n\n        name = rm_suffix(osp.basename(opath_feat))\n        prefix = oprefix\n        ds = BasicDataset(name=name,\n                          prefix=prefix,\n                          dim=cfg.model['kwargs']['nhid'],\n                          normalize=True)\n        ds.info()\n\n        # use top embedding of GCN to rebuild the kNN graph\n        with Timer('connect to higher confidence with use_gcn_feat'):\n            knn_prefix = osp.join(prefix, 'knns', name)\n            knns = build_knns(knn_prefix,\n                              ds.features,\n                              cfg.knn_method,\n                              cfg.knn,\n                              is_rebuild=True)\n            dists, nbrs = knns2ordered_nbrs(knns)\n\n            pred_dist2peak, pred_peaks = confidence_to_peaks(\n                dists, nbrs, pred_confs, cfg.max_conn)\n            pred_labels = peaks_to_labels(pred_peaks, pred_dist2peak, cfg.tau,\n                                          inst_num)\n\n        # save clustering results\n        if cfg.save_output:\n            oname_meta = '{}_gcn_feat'.format(name)\n            opath_pred_labels = osp.join(\n                oprefix, oname_meta, 'tau_{}_pred_labels.txt'.format(cfg.tau))\n            mkdir_if_no_exists(opath_pred_labels)\n\n            idx2lb = list2dict(pred_labels, ignore_value=-1)\n            write_meta(opath_pred_labels, idx2lb, inst_num=inst_num)\n\n        # evaluation\n        if not dataset.ignore_label:\n            print('==> evaluation')\n            for metric in cfg.metrics:\n                evaluate(dataset.gt_labels, pred_labels, metric)\n"""
vegcn/train_gcn_e.py,0,"b""from __future__ import division\n\nfrom collections import OrderedDict\n\nfrom dsgcn.runner import Runner\nfrom mmcv.parallel import MMDataParallel\n\nfrom dsgcn.train import build_optimizer\nfrom vegcn.datasets import build_dataset\nfrom lgcn.datasets import build_dataloader\n\n\ndef batch_processor(model, data, train_mode):\n    assert train_mode\n\n    _, loss = model(data, return_loss=True)\n\n    log_vars = OrderedDict()\n    log_vars['loss'] = loss.item()\n\n    outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data[2]))\n\n    return outputs\n\n\ndef train_gcn_e(model, cfg, logger):\n    # prepare data loaders\n    for k, v in cfg.model['kwargs'].items():\n        setattr(cfg.train_data, k, v)\n\n    dataset = build_dataset(cfg.model['type'], cfg.train_data)\n    data_loaders = [\n        build_dataloader(dataset,\n                         cfg.batch_size_per_gpu,\n                         cfg.workers_per_gpu,\n                         train=True,\n                         shuffle=True)\n    ]\n\n    # train\n    if cfg.distributed:\n        raise NotImplementedError\n    else:\n        _single_train(model, data_loaders, cfg)\n\n\ndef _single_train(model, data_loaders, cfg):\n    if cfg.gpus > 1:\n        raise NotImplemented\n    # put model on gpus\n    model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()\n    # build runner\n    optimizer = build_optimizer(model, cfg.optimizer)\n    runner = Runner(model,\n                    batch_processor,\n                    optimizer,\n                    cfg.work_dir,\n                    cfg.log_level,\n                    iter_size=cfg.iter_size)\n    runner.register_training_hooks(cfg.lr_config, cfg.optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n"""
vegcn/train_gcn_v.py,2,"b""from __future__ import division\n\nfrom collections import OrderedDict\n\nimport torch\nfrom vegcn.runner import Runner\nfrom dsgcn.train import build_optimizer\nfrom vegcn.datasets import build_dataset\nfrom utils import sparse_mx_to_torch_sparse_tensor\n\n\ndef batch_processor(model, data, train_mode):\n    assert train_mode\n\n    _, loss = model(data, return_loss=True)\n\n    log_vars = OrderedDict()\n    log_vars['loss'] = loss.item()\n\n    outputs = dict(loss=loss, log_vars=log_vars, num_samples=len(data[2]))\n\n    return outputs\n\n\ndef train_gcn_v(model, cfg, logger):\n    # prepare dataset\n    for k, v in cfg.model['kwargs'].items():\n        setattr(cfg.train_data, k, v)\n    dataset = build_dataset(cfg.model['type'], cfg.train_data)\n\n    # train\n    if cfg.distributed:\n        raise NotImplementedError\n    else:\n        _single_train(model, dataset, cfg)\n\n\ndef _single_train(model, dataset, cfg):\n    if cfg.gpus > 1:\n        raise NotImplemented\n\n    # build runner\n    optimizer = build_optimizer(model, cfg.optimizer)\n    runner = Runner(model, batch_processor, optimizer, cfg.work_dir,\n                    cfg.log_level)\n    runner.register_training_hooks(cfg.lr_config, cfg.optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n\n    features = torch.FloatTensor(dataset.features)\n    adj = sparse_mx_to_torch_sparse_tensor(dataset.adj)\n    labels = torch.FloatTensor(dataset.labels)\n\n    if cfg.cuda:\n        model.cuda()\n        features = features.cuda()\n        adj = adj.cuda()\n        labels = labels.cuda()\n\n    train_data = [[features, adj, labels]]\n    runner.run(train_data, cfg.workflow, cfg.total_epochs)\n"""
dsgcn/configs/cfg_test_det_fashion_20_prpsls.py,0,"b""# On 1 TitanX, it takes around 1 min for testing\n# test on pretrained model: (pre, rec, fscore) = (32.9, 33.61, 33.25)\n\nimport os.path as osp\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 256\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'deepfashion_test'\nk = 5\nknn_method = 'hnsw'\n\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 50\nth_iter0_lst_i0 = [(0.55, True), (0.6, True), (0.65, True)]\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 200\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (2, 12), (2, 16), (3, 5), (3, 10), (4, 4)]\n\nproposal_params = [\n    dict(k=k,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter0=iter0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for th_i0, iter0 in th_iter0_lst_i0\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=generate_proposals(\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_test_det_fashion_2_prpsls.py,0,"b""# On 1 TitanX, it takes around 10s for testing\n# test on pretrained model: (pre, rec, fscore) = (26.43, 32.47, 29.14)\n\nimport os.path as osp\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 256\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'deepfashion_test'\nk = 5\nknn_method = 'faiss'\n\nstep = 0.05\nminsz = 3\nmaxsz = 100\nthresholds = [0.55, 0.6]\n\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in thresholds\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=generate_proposals(\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_test_det_fashion_8_prpsls.py,0,"b""# On 1 TitanX, it takes around 30s for testing\n# test on pretrained model: (pre, rec, fscore) = (33.73, 31.4, 32.52)\n\nimport os.path as osp\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 256\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'deepfashion_test'\nk = 5\nknn_method = 'hnsw'\n\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 50\nth_iter0_lst_i0 = [(0.55, True), (0.6, True), (0.65, True)]\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 200\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (3, 5)]\n\nproposal_params = [\n    dict(k=k,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter0=iter0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for th_i0, iter0 in th_iter0_lst_i0\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=generate_proposals(\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_test_det_ms1m_20_prpsls.py,0,"b""# the same result as dsgcn/configs/yaml/cfg_test_hnsw_2_i0_18_i1.yaml\n# On 1 TitanX, it takes around 40min for testing (exclude the proposal generation)\n# test on pretrained model: (pre, rec, fscore) = (94.54, 81.62, 87.61)\n\nimport os.path as osp\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 256\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\nk = 80\nknn_method = 'hnsw'\n\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 300\nth_iter0_lst_i0 = [(0.6, True), (0.7, True), (0.75, False)]\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 500\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (2, 12), (2, 16), (3, 5), (3, 10), (4, 4)]\n\nproposal_params = [\n    dict(k=k,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter0=iter0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for th_i0, iter0 in th_iter0_lst_i0\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=generate_proposals(\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_test_det_ms1m_2_prpsls.py,0,"b""# the same result as dsgcn/configs/yaml/cfg_test_0.7_0.75.yaml\n# On 1 TitanX, it takes around 8 min for testing (exclude the proposal generation)\n# test on pretrained model: (pre, rec, fscore) = (95.41, 67.77, 79.25)\n\nimport os.path as osp\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 256\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\nk = 80\nknn_method = 'faiss'\n\nstep = 0.05\nminsz = 3\nmaxsz = 300\nthresholds = [0.7, 0.75]\n\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in thresholds\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=generate_proposals(\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_test_det_ms1m_5_prpsls.py,0,"b""# the same result as dsgcn/configs/yaml/cfg_test_0.7_0.75.yaml\n# On 1 TitanX, it takes around 2hr for testing (exclude the proposal generation)\n# test on pretrained model: (pre, rec, fscore) = (94.62, 72.59, 82.15)\n\nimport os.path as osp\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 256\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\nk = 80\nknn_method = 'faiss'\n\nstep = 0.05\nminsz = 3\nmaxsz = 300\nthresholds = [0.55, 0.6, 0.65, 0.7, 0.75]\n\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in thresholds\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=generate_proposals(\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_test_det_ms1m_8_prpsls.py,0,"b""# the same result as dsgcn/configs/yaml/cfg_test_hnsw_2_i0_6_i1.yaml\n# On 1 TitanX, it takes around 15min for testing (exclude the proposal generation)\n# test on pretrained model: (pre, rec, fscore) = (94.23, 79.69, 86.35)\n\nimport os.path as osp\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 256\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\nk = 80\nknn_method = 'hnsw'\n\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 300\nth_iter0_lst_i0 = [(0.6, True), (0.7, True), (0.75, False)]\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 500\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (3, 5)]\n\nproposal_params = [\n    dict(k=k,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter0=iter0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for th_i0, iter0 in th_iter0_lst_i0\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=generate_proposals(\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_test_det_ytb_4_prpsls.py,0,"b""# On 1 TitanX, it takes around 10 min for testing\n# test on pretrained model: (pre, rec, fscore) = (96.74, 92.25, 94.44)\n\nimport os.path as osp\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 256\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'ytb_test'\nk = 160\nknn_method = 'hnsw'\n\nstep = 0.05\nminsz = 3\nmaxsz = 1600\nthresholds = [0.65, 0.68, 0.7, 0.72]\n\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in thresholds\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=generate_proposals(\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_test_seg_ms1m_20_prpsls.py,0,"b""# On 1 TitanX, it takes around 11 min for testing (exclude the proposal generation)\n# metircs (pre, rec, pairwise fscore)\n# test on pretrained model (gt iop, 0.2-0.8): (97.98, 81.14, 88.77)\n# test on pretrained model (pred iop, 0.1-0.9): (97.91, 80.86, 88.57)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\nuse_random_seed = True\nfeatureless = False\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=featureless,\n                         stage='seg',\n                         reduce_method='no_pool',\n                         use_random_seed=use_random_seed,\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 1\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\nth_outlier = 0.5\nkeep_outlier = True\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\n\nk = 80\nknn_method = 'hnsw'\n\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 300\nth_iter0_lst_i0 = [(0.6, True), (0.7, True), (0.75, False)]\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 500\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (2, 12), (2, 16), (3, 5), (3, 10), (4, 4)]\n\nproposal_params = [\n    dict(k=k,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter0=iter0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for th_i0, iter0 in th_iter0_lst_i0\n]\n# use the same proposal params as training\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 use_random_seed=use_random_seed,\n                 use_max_degree_seed=False,\n                 featureless=featureless,\n                 th_iop_min=0.1,\n                 th_iop_max=0.9,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/configs/cfg_test_seg_ms1m_2_prpsls.py,0,"b""# On 1 TitanX, it takes around 1 min for testing (exclude the proposal generation)\n# metircs (pre, rec, pairwise fscore)\n# test on pretrained model (gt iop, 0.2-0.8): (99.1, 67.29, 80.16)\n# test on pretrained model (pred iop, 0.1-0.9): (99.07, 67.22, 80.1)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\nuse_random_seed = True\nfeatureless = False\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=featureless,\n                         stage='seg',\n                         reduce_method='no_pool',\n                         use_random_seed=use_random_seed,\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 1\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\nth_outlier = 0.5\nkeep_outlier = True\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\n\nknn_method = 'faiss'\nk = 80\nstep = 0.05\nminsz = 3\nmaxsz = 300\n\ntest_thresholds = [0.7, 0.75]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in test_thresholds\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 use_random_seed=use_random_seed,\n                 use_max_degree_seed=False,\n                 featureless=featureless,\n                 th_iop_min=0.1,\n                 th_iop_max=0.9,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/configs/cfg_test_seg_ms1m_5_prpsls.py,0,"b""# On 1 TitanX, it takes around 3 min for testing (exclude the proposal generation)\n# metircs (pre, rec, pairwise fscore)\n# test on pretrained model (gt iop, 0.2-0.8): (98.98, 72.13, 83.44)\n# test on pretrained model (pred iop, 0.1-0.9): (98.84, 72.01, 83.31)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\nuse_random_seed = True\nfeatureless = False\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=featureless,\n                         stage='seg',\n                         reduce_method='no_pool',\n                         use_random_seed=use_random_seed,\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 1\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\nth_outlier = 0.5\nkeep_outlier = True\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\n\nknn_method = 'faiss'\nk = 80\nstep = 0.05\nminsz = 3\nmaxsz = 300\n\nthresholds = [0.55, 0.6, 0.65, 0.7, 0.75]\n\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in thresholds\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 use_random_seed=use_random_seed,\n                 use_max_degree_seed=False,\n                 featureless=featureless,\n                 th_iop_min=0.1,\n                 th_iop_max=0.9,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/configs/cfg_test_seg_ms1m_8_prpsls.py,0,"b""# On 1 TitanX, it takes around 5 min for testing (exclude the proposal generation)\n# metircs (pre, rec, pairwise fscore)\n# test on pretrained model (gt iop, 0.2-0.8): (98.12, 79.18, 87.64)\n# test on pretrained model (pred iop, 0.1-0.9): (97.93, 78.98, 87.44)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\nuse_random_seed = True\nfeatureless = False\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=featureless,\n                         stage='seg',\n                         reduce_method='no_pool',\n                         use_random_seed=use_random_seed,\n                         hidden_dims=[512, 64]))\n\nbatch_size_per_gpu = 1\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\nth_outlier = 0.5\nkeep_outlier = True\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\nk = 80\nknn_method = 'hnsw'\n\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 300\nth_iter0_lst_i0 = [(0.6, True), (0.7, True), (0.75, False)]\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 500\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (3, 5)]\n\nproposal_params = [\n    dict(k=k,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter0=iter0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for th_i0, iter0 in th_iter0_lst_i0\n]\n\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 use_random_seed=use_random_seed,\n                 use_max_degree_seed=False,\n                 featureless=featureless,\n                 th_iop_min=0.1,\n                 th_iop_max=0.9,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/configs/cfg_train_det_fashion_4_prpsls.py,0,"b""# On 1 TitanX, it takes around 7min for training\n# test on 2 proposal params: (pre, rec, fscore) = (26.51, 32.42, 29.17)\n# test on 20 proposal params: (pre, rec, fscore) = (33.98, 30.03, 31.88)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[15, 24, 28],\n)\n\niter_size = 1\nbatch_size_per_gpu = 32\ntest_batch_size_per_gpu = 256\ntotal_epochs = 30\nworkflow = [('train', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntrain_name = 'deepfashion_train'\ntest_name = 'deepfashion_test'\nk = 5\nknn_method = 'faiss'\nstep = 0.05\nminsz = 3\nmaxsz = 100\n\ntrain_thresholds = [0.5, 0.55, 0.6, 0.65]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in train_thresholds\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(train_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(train_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntrain_data = dict(wo_weight=False,\n                  feat_path=feat_path,\n                  label_path=label_path,\n                  proposal_folders=partial(generate_proposals,\n                      params=proposal_params,\n                      prefix=prefix,\n                      oprefix=proposal_path,\n                      name=train_name,\n                      dim=model['kwargs']['feature_dim'],\n                      no_normalize=False))\n\ntest_thresholds = [0.55, 0.6]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in test_thresholds\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=partial(generate_proposals,\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_train_det_fashion_84_prpsls.py,0,"b""# On 1 TitanX, it takes around 1.5 hours for training\n# test on 2 proposal params: (pre, rec, fscore) = (26.43, 32.47, 29.14)\n# test on 20 proposal params: (pre, rec, fscore) = (32.9, 33.61, 33.25)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[15, 24, 28],\n)\n\niter_size = 1\nbatch_size_per_gpu = 32\ntest_batch_size_per_gpu = 256\ntotal_epochs = 30\nworkflow = [('train', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntrain_name = 'deepfashion_train'\ntest_name = 'deepfashion_test'\nknn_method = 'hnsw'\n\nk_lst_i0 = [2, 3, 5]\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 100\nth_lst_i0 = [0.5, 0.55, 0.6, 0.65]\n\nk_th_lst_i0 = []\nfor k in k_lst_i0:\n    for th in th_lst_i0:\n        k_th_lst_i0.append((k, th))\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 200\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (2, 12), (2, 16), (3, 5), (3, 10), (4, 4)]\n\nproposal_params = [\n    dict(k=k_i0,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for k_i0, th_i0 in k_th_lst_i0\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(train_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(train_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntrain_data = dict(wo_weight=False,\n                  feat_path=feat_path,\n                  label_path=label_path,\n                  proposal_folders=partial(generate_proposals,\n                                           params=proposal_params,\n                                           prefix=prefix,\n                                           oprefix=proposal_path,\n                                           name=train_name,\n                                           dim=model['kwargs']['feature_dim'],\n                                           no_normalize=False))\n\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 50\nth_iter0_lst_i0 = [(0.55, True), (0.6, True), (0.65, True)]\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 200\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (2, 12), (2, 16), (3, 5), (3, 10), (4, 4)]\n\nproposal_params = [\n    dict(k=k,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter0=iter0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for th_i0, iter0 in th_iter0_lst_i0\n]\n# use the same proposal params as training\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/configs/cfg_train_det_fashion_8_prpsls.py,0,"b""# On 1 TitanX, it takes around 15 min for training\n# test on 2 proposal params: (pre, rec, fscore) = (34.95, 27.7, 30.91)\n# test on 2 proposal params: (pre, rec, fscore) = (33.11, 32.88, 33.0)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[15, 24, 28],\n)\n\niter_size = 1\nbatch_size_per_gpu = 32\ntest_batch_size_per_gpu = 256\ntotal_epochs = 30\nworkflow = [('train', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntrain_name = 'deepfashion_train'\ntest_name = 'deepfashion_test'\nknn_method = 'faiss'\nstep = 0.05\nminsz = 3\nmaxsz = 100\n\nk_th_lst = [(2, 0.5), (2, 0.6), (3, 0.5), (3, 0.6), (5, 0.5), (5, 0.55),\n            (5, 0.6), (5, 0.65)]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for k, th_knn in k_th_lst\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(train_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(train_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntrain_data = dict(wo_weight=False,\n                  feat_path=feat_path,\n                  label_path=label_path,\n                  proposal_folders=partial(generate_proposals,\n                                           params=proposal_params,\n                                           prefix=prefix,\n                                           oprefix=proposal_path,\n                                           name=train_name,\n                                           dim=model['kwargs']['feature_dim'],\n                                           no_normalize=False))\n\nk = 5\nmaxsz = 50\ntest_thresholds = [0.55, 0.6]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in test_thresholds\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/configs/cfg_train_det_ms1m_4_prpsls.py,0,"b""# the same result as dsgcn/configs/yaml/cfg_train_0.7_0.75.yaml\n# On 1 TitanX, it takes around 7.5 hours for training\n# test on 2 proposal params: (pre, rec, fscore) = (96.64, 66.8, 79)\n# test on 20 proposal params: (pre, rec, fscore) = (96.59, 72.28, 82.69)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[15, 24, 28],\n)\n\niter_size = 1\nbatch_size_per_gpu = 32\ntest_batch_size_per_gpu = 256\ntotal_epochs = 30\nworkflow = [('train', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntrain_name = 'part0_train'\ntest_name = 'part1_test'\nk = 80\nknn_method = 'faiss'\nstep = 0.05\nminsz = 3\nmaxsz = 300\n\ntrain_thresholds = [0.6, 0.65, 0.7, 0.75]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in train_thresholds\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(train_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(train_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntrain_data = dict(wo_weight=False,\n                  feat_path=feat_path,\n                  label_path=label_path,\n                  proposal_folders=partial(generate_proposals,\n                      params=proposal_params,\n                      prefix=prefix,\n                      oprefix=proposal_path,\n                      name=train_name,\n                      dim=model['kwargs']['feature_dim'],\n                      no_normalize=False))\n\ntest_thresholds = [0.7, 0.75]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in test_thresholds\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=partial(generate_proposals,\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_train_det_ms1m_84_prpsls.py,0,"b""# On 1 TitanX, it takes around 70 hours for training\n# test on 2 proposal params: (pre, rec, fscore) = (96.63, 66.89, 79.06)\n# test on 20 proposal params: (pre, rec, fscore) = (96.39, 79.57, 87.18)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[15, 24, 28],\n)\n\niter_size = 1\nbatch_size_per_gpu = 32\ntest_batch_size_per_gpu = 256\ntotal_epochs = 30\nworkflow = [('train', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntrain_name = 'part0_train'\ntest_name = 'part1_test'\nknn_method = 'hnsw'\n\nk_lst_i0 = [30, 60, 80]\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 300\nth_lst_i0 = [0.6, 0.65, 0.7, 0.75]\n\nk_th_lst_i0 = []\nfor k in k_lst_i0:\n    for th in th_lst_i0:\n        k_th_lst_i0.append((k, th))\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 500\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (2, 12), (2, 16), (3, 5), (3, 10), (4, 4)]\n\nproposal_params = [\n    dict(k=k_i0,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for k_i0, th_i0 in k_th_lst_i0\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(train_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(train_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntrain_data = dict(wo_weight=False,\n                  feat_path=feat_path,\n                  label_path=label_path,\n                  proposal_folders=partial(generate_proposals,\n                                           params=proposal_params,\n                                           prefix=prefix,\n                                           oprefix=proposal_path,\n                                           name=train_name,\n                                           dim=model['kwargs']['feature_dim'],\n                                           no_normalize=False))\n\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 300\nth_iter0_lst_i0 = [(0.6, True), (0.7, True), (0.75, False)]\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 500\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (2, 12), (2, 16), (3, 5), (3, 10), (4, 4)]\n\nproposal_params = [\n    dict(k=k,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter0=iter0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for th_i0, iter0 in th_iter0_lst_i0\n]\n# use the same proposal params as training\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/configs/cfg_train_det_ms1m_8_prpsls.py,0,"b""# the same result as dsgcn/configs/yaml/cfg_train_8_prpsl.yaml\n# On 1 TitanX, it takes around 10.5 hours for training\n# test on 2 proposal params: (pre, rec, fscore) = (96.4, 67.07, 79.1)\n# test on 20 proposal params: (pre, rec, fscore) = (96.38, 72.37, 82.67)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=False,\n                         reduce_method='max',\n                         hidden_dims=[512, 64]))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[15, 24, 28],\n)\n\niter_size = 1\nbatch_size_per_gpu = 32\ntest_batch_size_per_gpu = 256\ntotal_epochs = 30\nworkflow = [('train', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntrain_name = 'part0_train'\ntest_name = 'part1_test'\nknn_method = 'faiss'\nstep = 0.05\nminsz = 3\nmaxsz = 300\n\nk_th_lst = [(30, 0.6), (30, 0.7), (60, 0.6), (60, 0.7), (80, 0.6), (80, 0.65),\n            (80, 0.7), (80, 0.75)]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for k, th_knn in k_th_lst\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(train_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(train_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntrain_data = dict(wo_weight=False,\n                  feat_path=feat_path,\n                  label_path=label_path,\n                  proposal_folders=partial(generate_proposals,\n                                           params=proposal_params,\n                                           prefix=prefix,\n                                           oprefix=proposal_path,\n                                           name=train_name,\n                                           dim=model['kwargs']['feature_dim'],\n                                           no_normalize=False))\n\nk = 80\ntest_thresholds = [0.7, 0.75]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in test_thresholds\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/configs/cfg_train_seg_ms1m_4_prpsls.py,0,"b""# On 1 TitanX, it takes around 2 hours for training\n# metircs (pre, rec, pairwise fscore)\n# test on 2 proposal params (gt iop, 0.2-0.8): (99.31, 67.24, 80.19)\n# test on 20 proposal params (gt iop, 0.2-0.8): (98.1, 80.87, 88.66)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\nuse_random_seed = True\nfeatureless=True\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=featureless,\n                         stage='seg',\n                         reduce_method='no_pool',\n                         use_random_seed=use_random_seed,\n                         hidden_dims=[512, 64]))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[15, 24, 28],\n)\n\niter_size = 32\nbatch_size_per_gpu = 1\ntest_batch_size_per_gpu = 256\ntotal_epochs = 30\nworkflow = [('train', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\nth_outlier = 0.5\nkeep_outlier = True\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntrain_name = 'part0_train'\ntest_name = 'part1_test'\n\nknn_method = 'faiss'\nk = 80\nstep = 0.05\nminsz = 3\nmaxsz = 300\n\ntrain_thresholds = [0.6, 0.65, 0.7, 0.75]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in train_thresholds\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(train_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(train_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntrain_data = dict(wo_weight=False,\n                  feat_path=feat_path,\n                  label_path=label_path,\n                  use_random_seed=use_random_seed,\n                  featureless=featureless,\n                  th_iop_min=0.3,\n                  th_iop_max=0.7,\n                  proposal_folders=partial(generate_proposals,\n                      params=proposal_params,\n                      prefix=prefix,\n                      oprefix=proposal_path,\n                      name=train_name,\n                      dim=model['kwargs']['feature_dim'],\n                      no_normalize=False))\n\ntest_thresholds = [0.7, 0.75]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in test_thresholds\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 use_random_seed=use_random_seed,\n                 use_max_degree_seed=False,\n                 featureless=featureless,\n                 th_iop_min=0.2,\n                 th_iop_max=0.8,\n                 proposal_folders=partial(generate_proposals,\n                     params=proposal_params,\n                     prefix=prefix,\n                     oprefix=proposal_path,\n                     name=test_name,\n                     dim=model['kwargs']['feature_dim'],\n                     no_normalize=False))\n"""
dsgcn/configs/cfg_train_seg_ms1m_84_prpsls.py,0,"b""# On 1 TitanX, it takes around 22 hours for training\n# metircs (pre, rec, pairwise fscore)\n# test on 2 proposal params (gt iop, 0.2-0.8): (99.1, 67.29, 80.16)\n# test on 20 proposal params (gt iop, 0.2-0.8): (97.83, 81.2, 88.74)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\nuse_random_seed = True\nfeatureless = False\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=featureless,\n                         stage='seg',\n                         reduce_method='no_pool',\n                         use_random_seed=use_random_seed,\n                         hidden_dims=[512, 64]))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[15, 24, 28],\n)\n\niter_size = 32\nbatch_size_per_gpu = 1\ntest_batch_size_per_gpu = 256\ntotal_epochs = 30\nworkflow = [('train', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\nth_outlier = 0.5\nkeep_outlier = True\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntrain_name = 'part0_train'\ntest_name = 'part1_test'\n\nth_iop_min=0.2\nth_iop_max=0.8\n\nknn_method = 'hnsw'\n\nk_lst_i0 = [30, 60, 80]\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 300\nth_lst_i0 = [0.6, 0.65, 0.7, 0.75]\n\nk_th_lst_i0 = []\nfor k in k_lst_i0:\n    for th in th_lst_i0:\n        k_th_lst_i0.append((k, th))\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 500\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (2, 12), (2, 16), (3, 5), (3, 10), (4, 4)]\n\nproposal_params = [\n    dict(k=k_i0,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for k_i0, th_i0 in k_th_lst_i0\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(train_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(train_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntrain_data = dict(wo_weight=False,\n                  feat_path=feat_path,\n                  label_path=label_path,\n                  use_random_seed=use_random_seed,\n                  featureless=featureless,\n                  th_iop_min=th_iop_min,\n                  th_iop_max=th_iop_max,\n                  proposal_folders=partial(generate_proposals,\n                                           params=proposal_params,\n                                           prefix=prefix,\n                                           oprefix=proposal_path,\n                                           name=train_name,\n                                           dim=model['kwargs']['feature_dim'],\n                                           no_normalize=False))\n\nstep_i0 = 0.05\nminsz_i0 = 3\nmaxsz_i0 = 300\nth_iter0_lst_i0 = [(0.6, True), (0.7, True), (0.75, False)]\n\nth_i1 = 0.4\nstep_i1 = 0.05\nminsz_i1 = 3\nmaxsz_i1 = 500\nsv_minsz_i1 = 2\nk_maxsz_lst_i1 = [(2, 8), (2, 12), (2, 16), (3, 5), (3, 10), (4, 4)]\n\nproposal_params = [\n    dict(k=k,\n         knn_method=knn_method,\n         th_knn=th_i0,\n         th_step=step_i0,\n         minsz=minsz_i0,\n         maxsz=maxsz_i0,\n         iter0=iter0,\n         iter1_params=[\n             dict(k=k_i1,\n                  knn_method=knn_method,\n                  th_knn=th_i1,\n                  th_step=step_i1,\n                  minsz=minsz_i1,\n                  maxsz=maxsz_i1,\n                  sv_minsz=sv_minsz_i1,\n                  sv_maxsz=sv_maxsz_i1) for k_i1, sv_maxsz_i1 in k_maxsz_lst_i1\n         ]) for th_i0, iter0 in th_iter0_lst_i0\n]\n# use the same proposal params as training\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\ntest_data = dict(wo_weight=True,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 use_random_seed=use_random_seed,\n                 use_max_degree_seed=True,\n                 featureless=featureless,\n                 th_iop_min=th_iop_min,\n                 th_iop_max=th_iop_max,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/configs/cfg_train_seg_ms1m_8_prpsls.py,0,"b""# On 1 TitanX, it takes around 3 hours for training\n# metircs (pre, rec, pairwise fscore)\n# test on 2 proposal params (gt iop, 0.2-0.8): (99.23, 67.31, 80.21)\n# test on 20 proposal params (gt iop, 0.2-0.8): (98.02, 81.16, 88.79)\n\nimport os.path as osp\nfrom functools import partial\nfrom proposals import generate_proposals\n\nuse_random_seed = True\nfeatureless = False\n\n# model\nmodel = dict(type='dsgcn',\n             kwargs=dict(feature_dim=256,\n                         featureless=featureless,\n                         stage='seg',\n                         reduce_method='no_pool',\n                         use_random_seed=use_random_seed,\n                         hidden_dims=[512, 64]))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[15, 24, 28],\n)\n\niter_size = 32\nbatch_size_per_gpu = 1\ntest_batch_size_per_gpu = 256\ntotal_epochs = 30\nworkflow = [('train', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n\n# post_process\nth_pos = -1\nth_iou = 1\nth_outlier = 0.5\nkeep_outlier = True\n\n# testing metrics\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# data locations\nprefix = './data'\ntrain_name = 'part0_train'\ntest_name = 'part1_test'\n\nth_iop_min=0.2\nth_iop_max=0.8\n\nknn_method = 'faiss'\nstep = 0.05\nminsz = 3\nmaxsz = 300\n\nk_th_lst = [(30, 0.6), (30, 0.7), (60, 0.6), (60, 0.7), (80, 0.6), (80, 0.65),\n            (80, 0.7), (80, 0.75)]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for k, th_knn in k_th_lst\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(train_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(train_name))\nproposal_path = osp.join(prefix, 'cluster_proposals')\ntrain_data = dict(wo_weight=False,\n                  feat_path=feat_path,\n                  label_path=label_path,\n                  use_random_seed=use_random_seed,\n                  featureless=featureless,\n                  th_iop_min=th_iop_min,\n                  th_iop_max=th_iop_max,\n                  proposal_folders=partial(generate_proposals,\n                                           params=proposal_params,\n                                           prefix=prefix,\n                                           oprefix=proposal_path,\n                                           name=train_name,\n                                           dim=model['kwargs']['feature_dim'],\n                                           no_normalize=False))\n\nk = 80\ntest_thresholds = [0.7, 0.75]\nproposal_params = [\n    dict(\n        k=k,\n        knn_method=knn_method,\n        th_knn=th_knn,\n        th_step=step,\n        minsz=minsz,\n        maxsz=maxsz,\n    ) for th_knn in test_thresholds\n]\nfeat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\nlabel_path = osp.join(prefix, 'labels', '{}.meta'.format(test_name))\ntest_data = dict(wo_weight=False,\n                 feat_path=feat_path,\n                 label_path=label_path,\n                 use_random_seed=use_random_seed,\n                 use_max_degree_seed=False,\n                 featureless=featureless,\n                 th_iop_min=th_iop_min,\n                 th_iop_max=th_iop_max,\n                 proposal_folders=partial(generate_proposals,\n                                          params=proposal_params,\n                                          prefix=prefix,\n                                          oprefix=proposal_path,\n                                          name=test_name,\n                                          dim=model['kwargs']['feature_dim'],\n                                          no_normalize=False))\n"""
dsgcn/datasets/__init__.py,0,"b'from .cluster_dataset import ClusterDataset\nfrom .cluster_det_processor import ClusterDetProcessor\nfrom .cluster_seg_processor import ClusterSegProcessor\nfrom .build_dataloader import build_dataloader\n\n__factory__ = {\n    \'det\': ClusterDetProcessor,\n    \'seg\': ClusterSegProcessor,\n}\n\n\ndef build_dataset(cfg):\n    return ClusterDataset(cfg)\n\n\ndef build_processor(name):\n    if name not in __factory__:\n        raise KeyError(""Unknown processor:"", name)\n    return __factory__[name]\n'"
dsgcn/datasets/build_dataloader.py,8,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom mmcv.runner import get_dist_info\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataloader import default_collate\n\nfrom dsgcn.datasets.sampler import (DistributedSampler,\n                                    DistributedSequentialSampler)\n\n\ndef collate_graphs(batch):\n    bs = len(batch)\n    if bs > 1:\n        feat, adj, lb = zip(*batch)\n        sizes = [f.shape[0] for f in feat]\n        max_size = max(sizes)\n        # pad to [X, 0]\n        pad_feat = [\n            F.pad(torch.from_numpy(f), (0, 0, 0, max_size - s), value=0)\n            for f, s in zip(feat, sizes)\n        ]\n        # pad to [[A, 0], [0, 0]]\n        pad_adj = [\n            F.pad(torch.from_numpy(a), (0, max_size - s, 0, max_size - s),\n                  value=0) for a, s in zip(adj, sizes)\n        ]\n        # pad to [[A, 0], [0, I]]\n        pad_adj = [\n            a + F.pad(torch.eye(max_size - s),\n                      (s, 0, s, 0), value=0) if s < max_size else a\n            for a, s in zip(pad_adj, sizes)\n        ]\n        pad_feat = default_collate(pad_feat)\n        pad_adj = default_collate(pad_adj)\n        lb_sizes = [_lb.size for _lb in lb]\n        lb_max_size = max(lb_sizes)\n        if lb_max_size > 1:\n            # pad to [X, -100]\n            # note that pad_value should be equal to ignore_index in NLLLoss\n            pad_lb = [\n                F.pad(torch.from_numpy(_lb), (0, lb_max_size - s), value=-100)\n                for _lb, s in zip(lb, lb_sizes)\n            ]\n            pad_lb = default_collate(pad_lb)\n        else:\n            pad_lb = torch.from_numpy(np.array(lb))\n        return pad_feat, pad_adj, pad_lb\n    else:\n        return default_collate(batch)\n\n\ndef build_dataloader(dataset,\n                     processor,\n                     batch_size_per_gpu,\n                     workers_per_gpu,\n                     shuffle=False,\n                     train=False,\n                     **kwargs):\n    rank, world_size = get_dist_info()\n    if train:\n        sampler = DistributedSampler(dataset, world_size, rank, shuffle)\n    else:\n        sampler = DistributedSequentialSampler(dataset, world_size, rank)\n    batch_size = batch_size_per_gpu\n    num_workers = workers_per_gpu\n\n    data_loader = DataLoader(processor(dataset),\n                             batch_size=batch_size,\n                             sampler=sampler,\n                             num_workers=num_workers,\n                             collate_fn=collate_graphs,\n                             pin_memory=False,\n                             **kwargs)\n\n    return data_loader\n'"
dsgcn/datasets/cluster_dataset.py,0,"b'import glob\nimport os.path as osp\nimport numpy as np\n\nfrom utils import (read_meta, read_probs, l2norm, load_data, intdict2ndarray,\n                   Timer)\nfrom proposals import compute_iop, get_majority\n\n\nclass ClusterDataset(object):\n    def __init__(self, cfg):\n        self.fn_node_pattern = \'*_node.npz\'\n        self.fn_edge_pattern = \'*_edge.npz\'\n\n        feat_path = cfg[\'feat_path\']\n        label_path = cfg.get(\'label_path\', None)\n        proposal_folders = cfg[\'proposal_folders\']\n\n        self.feature_dim = cfg[\'feature_dim\']\n        self.featureless = cfg.get(\'featureless\', False)\n        self.is_norm_adj = cfg.get(\'is_norm_adj\', True)\n        self.num_class = cfg.get(\'num_class\', 1)\n        self.th_iop_min = cfg.get(\'th_iop_min\', None)\n        self.th_iop_max = cfg.get(\'th_iop_max\', None)\n        self.wo_weight = cfg.get(\'wo_weight\', False)\n        self.det_label = cfg.get(\'det_label\', \'iou\')\n        self.use_random_seed = cfg.get(\'use_random_seed\', True)\n        self.use_max_degree_seed = cfg.get(\'use_max_degree_seed\', False)\n        self.pred_iop_score = cfg.get(\'pred_iop_score\', \'\')\n\n        if self.th_iop_min is not None and self.th_iop_max is not None:\n            assert 0 <= self.th_iop_min < self.th_iop_max <= 1\n            self.do_iop_check = True\n        else:\n            assert self.th_iop_min is None and self.th_iop_max is None\n            self.do_iop_check = False\n\n        self.fn2iop = None\n        if self.pred_iop_score != \'\' and self.pred_iop_score is not None:\n            assert osp.isfile(self.pred_iop_score), \'{} is not a file\'.format(\n                self.pred_iop_score)\n            print(\'read predicted iop from {}\'.format(self.pred_iop_score))\n            d = np.load(self.pred_iop_score, allow_pickle=True)\n            pred_scores = d[\'data\']\n            meta = d[\'meta\'].item()\n            _proposals = []\n            _proposal_folders = meta[\'proposal_folders\']\n            if callable(_proposal_folders):\n                _proposal_folders = _proposal_folders()\n            for _proposal_folder in _proposal_folders:\n                fn_clusters = sorted(\n                    glob.glob(osp.join(_proposal_folder,\n                                       self.fn_node_pattern)))\n                _proposals.extend([fn_node for fn_node in fn_clusters])\n            self.fn2iop = {}\n            for fn, iop in zip(_proposals, pred_scores):\n                self.fn2iop[fn] = iop\n\n        self._read(feat_path, label_path, proposal_folders)\n\n        print(\'#cluster: {}, #num_class: {}, feature shape: {}, \'\n              \'norm_adj: {}, wo_weight: {}\'.format(self.size, self.num_class,\n                                                   self.features.shape,\n                                                   self.is_norm_adj,\n                                                   self.wo_weight))\n\n    def _read(self, feat_path, label_path, proposal_folders):\n        with Timer(\'read meta and feature\'):\n            if label_path is not None:\n                self.lb2idxs, self.idx2lb = read_meta(label_path)\n                self.labels = intdict2ndarray(self.idx2lb)\n                self.inst_num = len(self.idx2lb)\n                self.ignore_label = False\n            else:\n                self.lb2idxs, self.idx2lb = None, None\n                self.labels = None\n                self.inst_num = -1\n                self.ignore_label = True\n            if not self.featureless:\n                features = read_probs(feat_path, self.inst_num,\n                                      self.feature_dim)\n                self.features = l2norm(features)\n                if self.inst_num == -1:\n                    self.inst_num = features.shape[0]\n            else:\n                assert self.inst_num > 0\n                self.feature_dim = 1\n                self.features = np.ones(self.inst_num).reshape(-1, 1)\n\n        with Timer(\'read proposal list\'):\n            self.lst = []\n            self.tot_lst = []\n            if callable(proposal_folders):\n                proposal_folders = proposal_folders()\n            for proposal_folder in proposal_folders:\n                print(\'read proposals from folder: \', proposal_folder)\n                fn_nodes = sorted(\n                    glob.glob(osp.join(proposal_folder, self.fn_node_pattern)))\n                fn_edges = sorted(\n                    glob.glob(osp.join(proposal_folder, self.fn_edge_pattern)))\n                assert len(fn_nodes) == len(\n                    fn_edges), ""node files({}) vs edge files({})"".format(\n                        len(fn_nodes), len(fn_edges))\n                assert len(fn_nodes) > 0, \'files under {} is 0\'.format(\n                    proposal_folder)\n                for fn_node, fn_edge in zip(fn_nodes, fn_edges):\n                    # sanity check\n                    assert fn_node[:fn_node.rfind(\n                        \'_\')] == fn_edge[:fn_edge.rfind(\'_\'\n                                                        )], ""{} vs {}"".format(\n                                                            fn_node, fn_edge)\n                    if self._check_iop(fn_node):\n                        self.lst.append([fn_node, fn_edge])\n                    self.tot_lst.append([fn_node, fn_edge])\n\n            self.size = len(self.lst)\n            self.tot_size = len(self.tot_lst)\n            assert self.size <= self.tot_size\n\n            if self.size < self.tot_size:\n                print(\'select {} / {} = {:.2f} proposals \'\n                      \'with iop between ({:.2f}, {:.2f})\'.format(\n                          self.size, self.tot_size,\n                          1. * self.size / self.tot_size, self.th_iop_min,\n                          self.th_iop_max))\n\n    def _check_iop(self, fn_node):\n        if not self.do_iop_check:\n            return True\n        node = load_data(fn_node)\n        if not self.ignore_label and not self.fn2iop:\n            lb2cnt = {}\n            for idx in node:\n                if idx not in self.idx2lb:\n                    continue\n                lb = self.idx2lb[idx]\n                if lb not in lb2cnt:\n                    lb2cnt[lb] = 0\n                lb2cnt[lb] += 1\n            gt_lb, _ = get_majority(lb2cnt)\n            gt_node = self.lb2idxs[gt_lb]\n            iop = compute_iop(node, gt_node)\n        else:\n            iop = self.fn2iop[fn_node]\n        return (iop >= self.th_iop_min) and (iop <= self.th_iop_max)\n\n    def __len__(self):\n        return self.size\n'"
dsgcn/datasets/cluster_det_processor.py,0,"b""import numpy as np\n\nfrom utils import load_data\nfrom proposals import compute_iou, compute_iop, get_majority\nfrom .cluster_processor import ClusterProcessor\n\n\nclass ClusterDetProcessor(ClusterProcessor):\n    def __init__(self, dataset):\n        super().__init__(dataset)\n\n    def build_graph(self, fn_node, fn_edge):\n        ''' build graph from graph file\n            - nodes: NxD,\n                     each row represents the feature of a node\n            - adj:   NxN,\n                     a symmetric similarity matrix with self-connection\n        '''\n        node = load_data(fn_node)\n        edge = load_data(fn_edge)\n        assert len(node) > 1, '#node of {}: {}'.format(fn_node, len(node))\n        # take majority as label of the graph\n        if not self.dataset.ignore_label:\n            lb2cnt = {}\n            for idx in node:\n                if idx not in self.dataset.idx2lb:\n                    continue\n                lb = self.dataset.idx2lb[idx]\n                if lb not in lb2cnt:\n                    lb2cnt[lb] = 0\n                lb2cnt[lb] += 1\n            gt_lb, _ = get_majority(lb2cnt)\n            gt_node = self.dataset.lb2idxs[gt_lb]\n            if self.dataset.det_label == 'iou':\n                label = compute_iou(node, gt_node)\n            elif self.dataset.det_label == 'iop':\n                label = compute_iop(node, gt_node)\n            else:\n                raise KeyError('Unknown det_label type: {}'.format(\n                    self.dataset.det_label))\n        else:\n            label = -1.\n\n        adj, _, _ = self.build_adj(node, edge)\n        features = self.build_features(node)\n        return features, adj, label\n\n    def __getitem__(self, idx):\n        ''' each features is a NxD matrix,\n            each adj is a NxN matrix,\n            each label is a floating point number,\n            which indicates the quality of the proposal.\n        '''\n        if idx is None or idx > self.dataset.size:\n            raise ValueError('idx({}) is not in the range of {}'.format(\n                idx, self.dataset.size))\n        fn_node, fn_edge = self.dataset.lst[idx]\n        ret = self.build_graph(fn_node, fn_edge)\n        assert ret is not None\n        features, adj, label = ret\n        return features.astype(self.dtype), adj.astype(self.dtype), np.array(\n            label, dtype=self.dtype)\n"""
dsgcn/datasets/cluster_processor.py,0,"b""import numpy as np\n\n\nclass ClusterProcessor(object):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.dtype = np.float32\n\n    def __len__(self):\n        return self.dataset.size\n\n    def build_adj(self, node, edge):\n        node = list(node)\n        abs2rel = {}\n        rel2abs = {}\n        for i, n in enumerate(node):\n            abs2rel[n] = i\n            rel2abs[i] = n\n        size = len(node)\n        adj = np.eye(size)\n        for e in edge:\n            w = 1.\n            if len(e) == 2:\n                e1, e2 = e\n            elif len(e) == 3:\n                e1, e2, dist = e\n                if not self.dataset.wo_weight:\n                    w = 1. - dist\n            else:\n                raise ValueError('Unknown length of e: {}'.format(e))\n            v1 = abs2rel[e1]\n            v2 = abs2rel[e2]\n            adj[v1][v2] = w\n            adj[v2][v1] = w\n        if self.dataset.is_norm_adj:\n            adj /= adj.sum(axis=1, keepdims=True)\n        return adj, abs2rel, rel2abs\n\n    def build_features(self, node):\n        if self.dataset.featureless:\n            features = np.ones(len(node)).reshape(-1, 1)\n        else:\n            features = self.dataset.features[node, :]\n        return features\n\n    def __getitem__(self, idx):\n        raise NotImplementedError\n"""
dsgcn/datasets/cluster_seg_processor.py,0,"b""import random\nimport numpy as np\n\nfrom utils import load_data\nfrom proposals import compute_iou, get_majority\nfrom .cluster_processor import ClusterProcessor\n\n\nclass ClusterSegProcessor(ClusterProcessor):\n    def __init__(self, dataset):\n        super().__init__(dataset)\n\n    @classmethod\n    def get_node_lb(cls, pred, label):\n        gt_set = set(label)\n        lbs = []\n        for node in pred:\n            if node in gt_set:\n                lbs.append(1)\n            else:\n                lbs.append(0)\n        return np.array(lbs)\n\n    def build_graph(self, fn_node, fn_edge):\n        ''' build graph from graph file\n            - nodes: NxD,\n                     each row represents the feature of a node\n            - adj:   NxN,\n                     a symmetric similarity matrix with self-connection\n        '''\n        node = load_data(fn_node)\n        edge = load_data(fn_edge)\n        assert len(node) > 1, '#node of {}: {}'.format(fn_node, len(node))\n\n        adj, abs2rel, rel2abs = self.build_adj(node, edge)\n        # compute label & mask\n        if self.dataset.use_random_seed:\n            ''' except using node with max degree as seed,\n                you can explore more creative designs.\n                e.g., applying random seed for multiple times,\n                and take the best results.\n            '''\n            if self.dataset.use_max_degree_seed:\n                s = adj.sum(axis=1, keepdims=True)\n                rel_center_idx = np.argmax(s)\n                center_idx = rel2abs[rel_center_idx]\n            else:\n                center_idx = random.choice(node)\n                rel_center_idx = abs2rel[center_idx]\n            mask = np.zeros(len(node))\n            mask[rel_center_idx] = 1\n            mask = mask.reshape(-1, 1)\n            if not self.dataset.ignore_label:\n                lb = self.dataset.idx2lb[center_idx]\n                gt_node = self.dataset.lb2idxs[lb]\n        else:\n            # do not use mask\n            if not self.dataset.ignore_label:\n                lb2cnt = {}\n                for idx in node:\n                    if idx not in self.dataset.idx2lb:\n                        continue\n                    lb = self.dataset.idx2lb[idx]\n                    if lb not in lb2cnt:\n                        lb2cnt[lb] = 0\n                    lb2cnt[lb] += 1\n                gt_lb, _ = get_majority(lb2cnt)\n                gt_node = self.dataset.lb2idxs[gt_lb]\n\n        if not self.dataset.ignore_label:\n            g_label = self.get_node_lb(node, gt_node)\n        else:\n            g_label = np.zeros_like(node)\n\n        features = self.build_features(node)\n        if self.dataset.use_random_seed:\n            features = np.concatenate((features, mask), axis=1)\n        return features, adj, g_label\n\n    def __getitem__(self, idx):\n        ''' each features is a NxD matrix,\n            each adj is a NxN matrix,\n            each label is a Nx2 matrix,\n            which indicates the quality of the proposal.\n        '''\n        if idx is None or idx > self.dataset.size:\n            raise ValueError('idx({}) is not in the range of {}'.format(\n                idx, self.dataset.size))\n        fn_node, fn_edge = self.dataset.lst[idx]\n        ret = self.build_graph(fn_node, fn_edge)\n        assert ret is not None\n        features, adj, label = ret\n        return features.astype(self.dtype), adj.astype(self.dtype), label\n"""
dsgcn/datasets/sampler.py,5,"b'import math\nimport torch\nfrom torch.utils.data.sampler import Sampler\nfrom torch.utils.data.distributed import (DistributedSampler as\n                                          _DistributedSampler)\n\n__all__ = [""DistributedSampler"", ""DistributedSequentialSampler""]\n\n\nclass DistributedSampler(_DistributedSampler):\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank)\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        if self.shuffle:\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n\nclass DistributedSequentialSampler(Sampler):\n    def __init__(self, dataset, world_size, rank):\n        assert rank >= 0\n        assert dataset.size >= world_size, \'{} vs {}\'.format(\n            dataset.size, world_size)\n        sub_num = int(math.ceil(1. * dataset.size / world_size))\n        # add extra samples to make it evenly divisible\n        tot_num = sub_num * world_size\n        self.beg = sub_num * rank\n        self.end = min(self.beg + sub_num, tot_num)\n\n    def __iter__(self):\n        indices = list(range(self.beg, self.end))\n        return iter(indices)\n\n    def __len__(self):\n        return self.end - self.beg\n'"
dsgcn/models/__init__.py,0,"b'from .dsgcn import dsgcn\n\n__factory__ = {\n    \'dsgcn\': dsgcn,\n}\n\n\ndef build_model(name, *args, **kwargs):\n    if name not in __factory__:\n        raise KeyError(""Unknown model:"", name)\n    return __factory__[name](*args, **kwargs)\n'"
dsgcn/models/dsgcn.py,17,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\n\n__all__ = [\'dsgcn\']\n\'\'\'\nOriginal implementation can be referred to:\n    - GCN: https://github.com/tkipf/pygcn\n    - SGC: https://github.com/Tiiiger/SGC\n\'\'\'\n\n\nclass GraphConv(nn.Module):\n    def __init__(self, in_features, out_features, bias=False):\n        super(GraphConv, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, x, adj, D=None):\n        if x.dim() == 3:\n            xw = torch.matmul(x, self.weight)\n            output = torch.bmm(adj, xw)\n        elif x.dim() == 2:\n            xw = torch.mm(x, self.weight)\n            output = torch.spmm(adj, xw)\n        if D is not None:\n            output = output * 1. / D\n        return output\n\n    def __repr__(self):\n        return \'{} ({} -> {})\'.format(self.__class__.__name__,\n                                      self.in_features, self.out_features)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes, planes, dropout=0.0):\n        super(BasicBlock, self).__init__()\n        self.gc = GraphConv(inplanes, planes)\n        self.relu = nn.ReLU(inplace=True)\n        if dropout > 0:\n            self.dropout = nn.Dropout2d(p=dropout)\n        else:\n            self.dropout = None\n\n    def forward(self, x, adj, D=None):\n        x = self.gc(x, adj, D)\n        x = self.relu(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        return x\n\n\nclass GNN(nn.Module):\n    """""" input is (bs, N, D), for featureless, D=1\n        dev output is (bs, num_classes)\n        seg output is (bs, N, num_classes)\n    """"""\n\n    def __init__(self,\n                 planes,\n                 feature_dim,\n                 featureless,\n                 num_classes=1,\n                 dropout=0.0,\n                 reduce_method=\'max\',\n                 stage=\'det\',\n                 use_random_seed=False,\n                 **kwargs):\n        assert feature_dim > 0\n        assert dropout >= 0 and dropout < 1\n        super(GNN, self).__init__()\n        if featureless:\n            self.inplanes = 1\n        else:\n            self.inplanes = feature_dim\n        self.num_classes = num_classes\n        self.reduce_method = reduce_method\n        self.stage = stage\n        if self.stage == \'det\':\n            self.loss = torch.nn.MSELoss()\n        elif self.stage == \'seg\':\n            self.num_classes = 2\n            if use_random_seed:\n                self.inplanes += 1\n            self.loss = torch.nn.NLLLoss(ignore_index=-100)\n        else:\n            raise KeyError(\'Unknown stage: {}\'.format(stage))\n\n    def pool(self, x):\n        # use global op to reduce N\n        # make sure isomorphic graphs output the same representation\n        if self.reduce_method == \'sum\':\n            return torch.sum(x, dim=1)\n        elif self.reduce_method == \'mean\':\n            return torch.mean(x, dim=1)\n        elif self.reduce_method == \'max\':\n            return torch.max(x, dim=1)[0]\n        elif self.reduce_method == \'no_pool\':\n            return x  # wo global pooling\n        else:\n            raise KeyError(\'Unkown reduce method\', self.reduce_method)\n\n    def forward(self, data, return_loss=False):\n        x = self.extract(data[0], data[1])\n        if return_loss:\n            label = data[2]\n            if self.stage == \'det\':\n                loss = self.loss(x.view(-1), label)\n            elif self.stage == \'seg\':\n                loss = self.loss(x, label)\n            return x, loss\n        else:\n            return x\n\n\nclass GCN(GNN):\n    def __init__(self,\n                 planes,\n                 feature_dim,\n                 featureless,\n                 num_classes=1,\n                 dropout=0.0,\n                 reduce_method=\'max\',\n                 stage=\'det\',\n                 **kwargs):\n        super().__init__(planes, feature_dim, featureless, num_classes,\n                         dropout, reduce_method, stage, **kwargs)\n\n        self.layers = self._make_layer(BasicBlock, planes, dropout)\n        self.classifier = nn.Linear(self.inplanes, self.num_classes)\n\n    def _make_layer(self, block, planes, dropout=0.0):\n        layers = nn.ModuleList([])\n        for i, plane in enumerate(planes):\n            layers.append(block(self.inplanes, plane, dropout))\n            self.inplanes = plane\n        return layers\n\n    def extract(self, x, adj):\n        bs = x.size(0)\n        adj.detach_()\n        D = adj.sum(dim=2, keepdim=True)\n        D.detach_()\n        assert (\n            D >\n            0).all(), ""D should larger than 0, otherwise gradient will be NaN.""\n        for layer in self.layers:\n            x = layer(x, adj, D)\n        x = self.pool(x)\n        x = x.view(-1, self.inplanes)\n        x = self.classifier(x)\n        if self.reduce_method == \'no_pool\':\n            if self.num_classes > 1:\n                x = x.view(bs, -1, self.num_classes)\n                x = torch.transpose(x, 1, 2).contiguous()\n                x = F.log_softmax(x, dim=1)\n            else:\n                x = x.view(bs, -1)\n        return x\n\n\nclass SGC(GNN):\n    def __init__(self,\n                 planes,\n                 feature_dim,\n                 featureless,\n                 num_classes=1,\n                 dropout=0.0,\n                 reduce_method=\'max\',\n                 stage=\'det\',\n                 **kwargs):\n        super().__init__(planes, feature_dim, featureless, num_classes,\n                         dropout, reduce_method, stage, **kwargs)\n\n        assert stage == \'det\'\n        self.degree = len(planes)\n        self.classifier = nn.Linear(self.inplanes, num_classes)\n\n    def extract(self, x, adj):\n        adj.detach_()\n        D = adj.sum(dim=2, keepdim=True)\n        D.detach_()\n        assert (\n            D >\n            0).all(), ""D should larger than 0, otherwise gradient will be NaN.""\n        for _ in range(self.degree):\n            if x.dim() == 3:\n                x = torch.bmm(adj, x) / D\n            elif x.dim() == 2:\n                x = torch.spmm(adj, x) / D\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n\ndef _build_model(gcn_type):\n    __gcn_type__ = {\n        \'gcn\': GCN,\n        \'sgc\': SGC,\n    }\n    if gcn_type not in __gcn_type__:\n        raise KeyError(""Unknown gcn_type:"", gcn_type)\n    return __gcn_type__[gcn_type]\n\n\ndef dsgcn(feature_dim,\n          hidden_dims=[],\n          featureless=False,\n          gcn_type=\'gcn\',\n          reduce_method=\'max\',\n          stage=\'det\',\n          dropout=0.5,\n          num_classes=1,\n          **kwargs):\n    model = _build_model(gcn_type)\n    return model(planes=hidden_dims,\n                 feature_dim=feature_dim,\n                 featureless=featureless,\n                 reduce_method=reduce_method,\n                 stage=stage,\n                 dropout=dropout,\n                 num_classes=num_classes,\n                 **kwargs)\n'"
dsgcn/runner/__init__.py,0,b'from .runner import Runner\n'
dsgcn/runner/runner.py,0,"b""import logging\nfrom mmcv.runner import Runner as _Runner\n\n\nclass Runner(_Runner):\n    def __init__(self,\n                 model,\n                 batch_processor,\n                 optimizer=None,\n                 work_dir=None,\n                 log_level=logging.INFO,\n                 logger=None,\n                 iter_size=1):\n        super().__init__(model, batch_processor, optimizer, work_dir,\n                         log_level, logger)\n        assert isinstance(iter_size, int) and iter_size >= 1\n        self.iter_size = iter_size\n\n    def train_iter_size(self, data_loader, **kwargs):\n        self.model.train()\n        self.mode = 'train'\n        self.data_loader = data_loader\n        self._max_iters = self._max_epochs * len(data_loader)\n        self.call_hook('before_train_epoch')\n        self._loss = 0\n        self._iter_size_cnt = 0\n        for i, data_batch in enumerate(data_loader):\n            self._inner_iter = i\n            self.call_hook('before_train_iter')\n            _outputs = self.batch_processor(self.model,\n                                            data_batch,\n                                            train_mode=True,\n                                            **kwargs)\n            if not isinstance(_outputs, dict):\n                raise TypeError('batch_processor() must return a dict')\n            if 'log_vars' in _outputs:\n                self.log_buffer.update(_outputs['log_vars'],\n                                       _outputs['num_samples'])\n            self._loss += _outputs['loss']\n            self._iter_size_cnt += 1\n            if (i + 1) % self.iter_size == 0 or i == len(data_loader) - 1:\n                self.outputs = {'loss': self._loss / self._iter_size_cnt}\n                self.call_hook('after_train_iter')\n                self._loss = 0\n                self._iter_size_cnt = 0\n            self._iter += 1\n\n        self.call_hook('after_train_epoch')\n        self._epoch += 1\n"""
lgcn/configs/cfg_test_lgcn_fashion.py,0,"b""import os.path as osp\n\n# data locations\nprefix = './data'\ntest_name = 'deepfashion_test'\nknn = 5\nknn_method = 'faiss'\n\ntest_data = dict(\n    feat_path=osp.join(prefix, 'features', '{}.bin'.format(test_name)),\n    label_path=osp.join(prefix, 'labels', '{}.meta'.format(test_name)),\n    knn_graph_path=osp.join(prefix, 'knns', test_name,\n                            '{}_k_{}.npz'.format(knn_method, knn)),\n    k_at_hop=[5, 5],\n    active_connection=5,\n    is_norm_feat=True,\n    is_sort_knns=True,\n    is_test=True,\n)\n\n# model\nmodel = dict(type='lgcn', kwargs=dict(feature_dim=256))\n\nbatch_size_per_gpu = 16\n\n# testing args\nmax_sz = 50\nstep = 0.5\npool = 'avg'\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=200, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
lgcn/configs/cfg_test_lgcn_ms1m.py,0,"b""import os.path as osp\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\nknn = 80\nknn_method = 'faiss'\n\ntest_data = dict(\n    feat_path=osp.join(prefix, 'features', '{}.bin'.format(test_name)),\n    label_path=osp.join(prefix, 'labels', '{}.meta'.format(test_name)),\n    knn_graph_path=osp.join(prefix, 'knns', test_name,\n                            '{}_k_{}.npz'.format(knn_method, knn)),\n    k_at_hop=[80, 10],\n    active_connection=10,\n    is_norm_feat=True,\n    is_sort_knns=True,\n    is_test=True,\n)\n\n# model\nmodel = dict(type='lgcn', kwargs=dict(feature_dim=256))\n\nbatch_size_per_gpu = 16\n\n# testing args\nmax_sz = 300\nstep = 0.6\npool = 'avg'\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=200, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
lgcn/configs/cfg_train_lgcn_fashion.py,0,"b""import os.path as osp\n\n# data locations\nprefix = './data'\ntrain_name = 'deepfashion_train'\ntest_name = 'deepfashion_test'\nknn = 5\nknn_method = 'faiss'\n\ntrain_data = dict(\n    feat_path=osp.join(prefix, 'features', '{}.bin'.format(train_name)),\n    label_path=osp.join(prefix, 'labels', '{}.meta'.format(train_name)),\n    knn_graph_path=osp.join(prefix, 'knns', train_name,\n                            '{}_k_{}.npz'.format(knn_method, knn)),\n    k_at_hop=[5, 5],\n    active_connection=5,\n    is_norm_feat=True,\n    is_sort_knns=True,\n)\n\ntest_data = dict(\n    feat_path=osp.join(prefix, 'features', '{}.bin'.format(test_name)),\n    label_path=osp.join(prefix, 'labels', '{}.meta'.format(test_name)),\n    knn_graph_path=osp.join(prefix, 'knns', test_name,\n                            '{}_k_{}.npz'.format(knn_method, knn)),\n    k_at_hop=[5, 5],\n    active_connection=5,\n    is_norm_feat=True,\n    is_sort_knns=True,\n    is_test=True,\n)\n\n# model\nmodel = dict(type='lgcn', kwargs=dict(feature_dim=256))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[1, 2, 3],\n)\n\nbatch_size_per_gpu = 16\ntotal_epochs = 4\nworkflow = [('train', 1)]\n\n# testing args\nmax_sz = 50\nstep = 0.5\npool = 'avg'\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=200, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
lgcn/configs/cfg_train_lgcn_ms1m.py,0,"b""import os.path as osp\n\n# data locations\nprefix = './data'\ntrain_name = 'part0_train'\ntest_name = 'part1_test'\nknn = 80\nknn_method = 'faiss'\n\ntrain_data = dict(\n    feat_path=osp.join(prefix, 'features', '{}.bin'.format(train_name)),\n    label_path=osp.join(prefix, 'labels', '{}.meta'.format(train_name)),\n    knn_graph_path=osp.join(prefix, 'knns', train_name,\n                            '{}_k_{}.npz'.format(knn_method, knn)),\n    k_at_hop=[200, 10],\n    active_connection=10,\n    is_norm_feat=True,\n    is_sort_knns=True,\n)\n\ntest_data = dict(\n    feat_path=osp.join(prefix, 'features', '{}.bin'.format(test_name)),\n    label_path=osp.join(prefix, 'labels', '{}.meta'.format(test_name)),\n    knn_graph_path=osp.join(prefix, 'knns', test_name,\n                            '{}_k_{}.npz'.format(knn_method, knn)),\n    k_at_hop=[80, 10],\n    active_connection=10,\n    is_norm_feat=True,\n    is_sort_knns=True,\n    is_test=True,\n)\n\n# model\nmodel = dict(type='lgcn', kwargs=dict(feature_dim=256))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\nlr_config = dict(\n    policy='step',\n    step=[1, 2, 3],\n)\n\nbatch_size_per_gpu = 16\ntotal_epochs = 4\nworkflow = [('train', 1)]\n\n# testing args\nmax_sz = 300\nstep = 0.6\npool = 'avg'\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=200, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
lgcn/datasets/__init__.py,0,b'from .cluster_dataset import ClusterDataset\nfrom .build_dataloader import build_dataloader\n\n\ndef build_dataset(cfg):\n    return ClusterDataset(cfg)\n'
lgcn/datasets/build_dataloader.py,1,"b'from mmcv.runner import get_dist_info\nfrom torch.utils.data import DataLoader\n\nfrom dsgcn.datasets.sampler import (DistributedSampler,\n                                    DistributedSequentialSampler)\n\n\ndef build_dataloader(dataset,\n                     batch_size_per_gpu,\n                     workers_per_gpu,\n                     shuffle=False,\n                     train=False,\n                     **kwargs):\n    rank, world_size = get_dist_info()\n    if train:\n        sampler = DistributedSampler(dataset, world_size, rank, shuffle)\n    else:\n        sampler = DistributedSequentialSampler(dataset, world_size, rank)\n    batch_size = batch_size_per_gpu\n    num_workers = workers_per_gpu\n\n    data_loader = DataLoader(dataset,\n                             batch_size=batch_size,\n                             sampler=sampler,\n                             num_workers=num_workers,\n                             pin_memory=False,\n                             **kwargs)\n\n    return data_loader\n'"
lgcn/datasets/cluster_dataset.py,0,"b""import numpy as np\n\nfrom utils import (read_meta, read_probs, l2norm, knns2ordered_nbrs,\n                   intdict2ndarray, Timer)\n\n\nclass ClusterDataset(object):\n    def __init__(self, cfg):\n        feat_path = cfg['feat_path']\n        label_path = cfg.get('label_path', None)\n        knn_graph_path = cfg['knn_graph_path']\n\n        self.k_at_hop = cfg['k_at_hop']\n        self.depth = len(self.k_at_hop)\n        self.active_connection = cfg['active_connection']\n        self.feature_dim = cfg['feature_dim']\n        self.is_norm_feat = cfg.get('is_norm_feat', True)\n        self.is_sort_knns = cfg.get('is_sort_knns', True)\n        self.is_test = cfg.get('is_test', False)\n\n        with Timer('read meta and feature'):\n            if label_path is not None:\n                _, idx2lb = read_meta(label_path)\n                self.inst_num = len(idx2lb)\n                self.labels = intdict2ndarray(idx2lb)\n                self.ignore_label = False\n            else:\n                self.labels = None\n                self.inst_num = -1\n                self.ignore_label = True\n            self.features = read_probs(feat_path, self.inst_num,\n                                       self.feature_dim)\n            if self.is_norm_feat:\n                self.features = l2norm(self.features)\n            if self.inst_num == -1:\n                self.inst_num = self.features.shape[0]\n            self.size = self.inst_num\n\n        with Timer('read knn graph'):\n            knns = np.load(knn_graph_path)['data']\n            _, self.knn_graph = knns2ordered_nbrs(knns, sort=self.is_sort_knns)\n        assert np.mean(self.k_at_hop) >= self.active_connection\n\n        print('feature shape: {}, norm_feat: {}, sort_knns: {} '\n              'k_at_hop: {}, active_connection: {}'.format(\n                  self.features.shape, self.is_norm_feat, self.is_sort_knns,\n                  self.k_at_hop, self.active_connection))\n\n    def __getitem__(self, index):\n        '''\n        return the vertex feature and the adjacent matrix A, together\n        with the indices of the center node and its 1-hop nodes\n        '''\n        if index is None or index > self.size:\n            raise ValueError('index({}) is not in the range of {}'.format(\n                index, self.size))\n\n        center_node = index\n\n        # hops[0] for 1-hop neighbors, hops[1] for 2-hop neighbors\n        hops = []\n        hops.append(set(self.knn_graph[center_node][1:]))\n\n        # Actually we dont need the loop since the depth is fixed here,\n        # But we still remain the code for further revision\n        for d in range(1, self.depth):\n            hops.append(set())\n            for h in hops[-2]:\n                hops[-1].update(set(self.knn_graph[h][1:self.k_at_hop[d] + 1]))\n\n        hops_set = set([h for hop in hops for h in hop])\n        hops_set.update([\n            center_node,\n        ])\n        uniq_nodes = np.array(list(hops_set), dtype=np.int64)\n        uniq_nodes_map = {j: i for i, j in enumerate(uniq_nodes)}\n\n        center_idx = np.array([uniq_nodes_map[center_node]], dtype=np.int64)\n        one_hop_idxs = np.array([uniq_nodes_map[i] for i in hops[0]],\n                                dtype=np.int64)\n        center_feat = self.features[center_node]\n        feat = self.features[uniq_nodes]\n        feat = feat - center_feat\n\n        max_num_nodes = self.k_at_hop[0] * (self.k_at_hop[1] + 1) + 1\n        num_nodes = len(uniq_nodes)\n        A = np.zeros([num_nodes, num_nodes], dtype=feat.dtype)\n\n        res_num_nodes = max_num_nodes - num_nodes\n        if res_num_nodes > 0:\n            pad_feat = np.zeros([res_num_nodes, self.feature_dim],\n                                dtype=feat.dtype)\n            feat = np.concatenate([feat, pad_feat], axis=0)\n\n        for node in uniq_nodes:\n            neighbors = self.knn_graph[node, 1:self.active_connection + 1]\n            for n in neighbors:\n                if n in uniq_nodes:\n                    i, j = uniq_nodes_map[node], uniq_nodes_map[n]\n                    A[i, j] = 1\n                    A[j, i] = 1\n\n        D = A.sum(1, keepdims=True)\n        A = A / D\n        A_ = np.zeros([max_num_nodes, max_num_nodes], dtype=A.dtype)\n        A_[:num_nodes, :num_nodes] = A\n\n        if self.ignore_label:\n            return (feat, A_, center_idx, one_hop_idxs)\n\n        labels = self.labels[uniq_nodes]\n        one_hop_labels = labels[one_hop_idxs]\n        center_label = labels[center_idx]\n        edge_labels = (center_label == one_hop_labels).astype(np.int64)\n\n        if self.is_test:\n            if res_num_nodes > 0:\n                pad_nodes = np.zeros(res_num_nodes, dtype=uniq_nodes.dtype)\n                uniq_nodes = np.concatenate([uniq_nodes, pad_nodes], axis=0)\n            return (feat, A_, one_hop_idxs,\n                    edge_labels), center_idx, uniq_nodes\n        else:\n            return (feat, A_, one_hop_idxs, edge_labels)\n\n    def __len__(self):\n        return self.size\n"""
lgcn/models/__init__.py,0,"b'from .lgcn import lgcn\n\n__factory__ = {\n    \'lgcn\': lgcn,\n}\n\n\ndef build_model(name, *args, **kwargs):\n    if name not in __factory__:\n        raise KeyError(""Unknown model:"", name)\n    return __factory__[name](*args, **kwargs)\n'"
lgcn/models/lgcn.py,9,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\n\nclass MeanAggregator(nn.Module):\n    def __init__(self):\n        super(MeanAggregator, self).__init__()\n\n    def forward(self, features, A):\n        x = torch.bmm(A, features)\n        return x\n\n\nclass GraphConv(nn.Module):\n    def __init__(self, in_dim, out_dim, agg):\n        super(GraphConv, self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.weight = nn.Parameter(torch.FloatTensor(in_dim * 2, out_dim))\n        self.bias = nn.Parameter(torch.FloatTensor(out_dim))\n        init.xavier_uniform_(self.weight)\n        init.constant_(self.bias, 0)\n        self.agg = agg()\n\n    def forward(self, features, A):\n        b, n, d = features.shape\n        assert (d == self.in_dim)\n        agg_feats = self.agg(features, A)\n        cat_feats = torch.cat([features, agg_feats], dim=2)\n        out = torch.einsum('bnd,df->bnf', (cat_feats, self.weight))\n        out = F.relu(out + self.bias)\n        return out\n\n\nclass lgcn(nn.Module):\n    def __init__(self, feature_dim):\n        super(lgcn, self).__init__()\n        self.bn0 = nn.BatchNorm1d(feature_dim, affine=False)\n        self.conv1 = GraphConv(feature_dim, 512, MeanAggregator)\n        self.conv2 = GraphConv(512, 512, MeanAggregator)\n        self.conv3 = GraphConv(512, 256, MeanAggregator)\n        self.conv4 = GraphConv(256, 256, MeanAggregator)\n\n        self.classifier = nn.Sequential(nn.Linear(256, 256), nn.PReLU(256),\n                                        nn.Linear(256, 2))\n        self.loss = nn.CrossEntropyLoss()\n\n    def extract(self, x, A, one_hop_idxs):\n        # data normalization l2 -> bn\n        B, N, D = x.shape\n        # xnorm = x.norm(2,2,keepdim=True) + 1e-8\n        # xnorm = xnorm.expand_as(x)\n        # x = x.div(xnorm)\n\n        x = x.view(-1, D)\n        x = self.bn0(x)\n        x = x.view(B, N, D)\n\n        x = self.conv1(x, A)\n        x = self.conv2(x, A)\n        x = self.conv3(x, A)\n        x = self.conv4(x, A)\n        k1 = one_hop_idxs.size(-1)\n        dout = x.size(-1)\n        edge_feat = torch.zeros(B, k1, dout).cuda()\n        for b in range(B):\n            edge_feat[b, :, :] = x[b, one_hop_idxs[b]]\n        edge_feat = edge_feat.view(-1, dout)\n        pred = self.classifier(edge_feat)\n\n        # shape: (B*k1, 2)\n        return pred\n\n    def forward(self, data, return_loss=False):\n        x, A, one_hop_idxs, labels = data\n        x = self.extract(x, A, one_hop_idxs)\n        if return_loss:\n            loss = self.loss(x, labels.view(-1))\n            return x, loss\n        else:\n            return x\n"""
vegcn/configs/cfg_test_gcne_fashion.py,0,"b""import os.path as osp\nfrom mmcv import Config\nfrom utils import rm_suffix\n\n# data locations\nprefix = './data'\ntest_name = 'deepfashion_test'\nknn = 80\nknn_method = 'faiss'\nth_sim = 0.  # cut edges with similarity smaller than th_sim\n\n# gcn_v configs\n_work_dir = 'work_dir'\nckpt_name = 'pretrained_gcn_v_fashion'\ngcnv_cfg = './vegcn/configs/cfg_test_gcnv_fashion.py'\ngcnv_cfg_name = rm_suffix(osp.basename(gcnv_cfg))\ngcnv_cfg = Config.fromfile(gcnv_cfg)\n\nuse_gcn_feat = True\ngcnv_prefix = '{}/{}/{}/{}_gcnv_k_{}_th_{}'.format(prefix, _work_dir,\n                                                   gcnv_cfg_name, test_name,\n                                                   gcnv_cfg.knn,\n                                                   gcnv_cfg.th_sim)\nif use_gcn_feat:\n    gcnv_nhid = gcnv_cfg.model.kwargs.nhid\n    feat_path = osp.join(gcnv_prefix, 'features', '{}.bin'.format(ckpt_name))\nelse:\n    gcnv_nhid = gcnv_cfg.model.kwargs.feature_dim\n    feat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\n\n# testing args\nmax_conn = 1\ntau = 0.85\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# if `knn_graph_path` is not passed, it will build knn_graph automatically\ntest_data = dict(feat_path=feat_path,\n                 label_path=osp.join(prefix, 'labels',\n                                     '{}.meta'.format(test_name)),\n                 pred_confs=osp.join(gcnv_prefix, 'pred_confs',\n                                     '{}.npz'.format(ckpt_name)),\n                 k=knn,\n                 is_norm_feat=True,\n                 th_sim=th_sim,\n                 max_conn=max_conn,\n                 ignore_ratio=0.8)\n\n# model\nregressor = False\nnclass = 1 if regressor else 2\nmodel = dict(type='gcn_e',\n             kwargs=dict(feature_dim=gcnv_nhid,\n                         nhid=512,\n                         nclass=nclass,\n                         dropout=0.))\n\nbatch_size_per_gpu = 1\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=200, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
vegcn/configs/cfg_test_gcne_ms1m.py,0,"b""import os.path as osp\nfrom mmcv import Config\nfrom utils import rm_suffix\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\nknn = 160\nknn_method = 'faiss'\nth_sim = 0.  # cut edges with similarity smaller than th_sim\n\n# gcn_v configs\n_work_dir = 'work_dir'\nckpt_name = 'pretrained_gcn_v_ms1m'\ngcnv_cfg = './vegcn/configs/cfg_test_gcnv_ms1m.py'\ngcnv_cfg_name = rm_suffix(osp.basename(gcnv_cfg))\ngcnv_cfg = Config.fromfile(gcnv_cfg)\n\nuse_gcn_feat = True\ngcnv_prefix = '{}/{}/{}/{}_gcnv_k_{}_th_{}'.format(prefix, _work_dir,\n                                                   gcnv_cfg_name, test_name,\n                                                   gcnv_cfg.knn,\n                                                   gcnv_cfg.th_sim)\nif use_gcn_feat:\n    gcnv_nhid = gcnv_cfg.model.kwargs.nhid\n    feat_path = osp.join(gcnv_prefix, 'features', '{}.bin'.format(ckpt_name))\nelse:\n    gcnv_nhid = gcnv_cfg.model.kwargs.feature_dim\n    feat_path = osp.join(prefix, 'features', '{}.bin'.format(test_name))\n\n# testing args\nmax_conn = 1\ntau = 0.8\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# if `knn_graph_path` is not passed, it will build knn_graph automatically\ntest_data = dict(feat_path=feat_path,\n                 label_path=osp.join(prefix, 'labels',\n                                     '{}.meta'.format(test_name)),\n                 pred_confs=osp.join(gcnv_prefix, 'pred_confs',\n                                     '{}.npz'.format(ckpt_name)),\n                 k=knn,\n                 is_norm_feat=True,\n                 th_sim=th_sim,\n                 max_conn=max_conn,\n                 ignore_ratio=0.8)\n\n# model\nregressor = False\nnclass = 1 if regressor else 2\nmodel = dict(type='gcn_e',\n             kwargs=dict(feature_dim=gcnv_nhid,\n                         nhid=512,\n                         nclass=nclass,\n                         dropout=0.))\n\nbatch_size_per_gpu = 1\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=2000, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
vegcn/configs/cfg_test_gcnv_fashion.py,0,"b""import os.path as osp\n\n# data locations\nprefix = './data'\ntest_name = 'deepfashion_test'\nknn = 5\nknn_method = 'faiss'\nth_sim = 0.  # cut edges with similarity smaller than th_sim\n\n# if `knn_graph_path` is not passed, it will build knn_graph automatically\ntest_data = dict(feat_path=osp.join(prefix, 'features',\n                                    '{}.bin'.format(test_name)),\n                 label_path=osp.join(prefix, 'labels',\n                                     '{}.meta'.format(test_name)),\n                 knn_graph_path=osp.join(prefix, 'knns', test_name,\n                                         '{}_k_{}.npz'.format(knn_method,\n                                                              knn)),\n                 k=knn,\n                 is_norm_feat=True,\n                 th_sim=th_sim,\n                 conf_metric='s_nbr')\n\n# model\nmodel = dict(type='gcn_v',\n             kwargs=dict(feature_dim=256, nhid=512, nclass=1, dropout=0.))\n\nbatch_size_per_gpu = 1\n\n# testing args\nuse_gcn_feat = True\nmax_conn = 1\ntau_0 = 0.8\ntau = 0.85\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=1, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
vegcn/configs/cfg_test_gcnv_ms1m.py,0,"b""import os.path as osp\n\n# data locations\nprefix = './data'\ntest_name = 'part1_test'\nknn = 80\nknn_method = 'faiss'\nth_sim = 0.  # cut edges with similarity smaller than th_sim\n\n# if `knn_graph_path` is not passed, it will build knn_graph automatically\ntest_data = dict(feat_path=osp.join(prefix, 'features',\n                                    '{}.bin'.format(test_name)),\n                 label_path=osp.join(prefix, 'labels',\n                                     '{}.meta'.format(test_name)),\n                 knn_graph_path=osp.join(prefix, 'knns', test_name,\n                                         '{}_k_{}.npz'.format(knn_method,\n                                                              knn)),\n                 k=knn,\n                 is_norm_feat=True,\n                 th_sim=th_sim,\n                 conf_metric='s_nbr')\n\n# model\nmodel = dict(type='gcn_v',\n             kwargs=dict(feature_dim=256, nhid=512, nclass=1, dropout=0.))\n\nbatch_size_per_gpu = 16\n\n# testing args\nuse_gcn_feat = True\nmax_conn = 1\ntau_0 = 0.65\ntau = 0.8\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# misc\nworkers_per_gpu = 1\n\nlog_level = 'INFO'\nlog_config = dict(interval=200, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
vegcn/configs/cfg_train_gcne_fashion.py,0,"b""# On 1 TitanX, it takes around 7min for training\n# train with PyTorch (0.5.0a0+e31ab99), the performance is:\n# (pre, rec, pairwise fscore) = (37.41, 41.05, 39.15)\n# (pre, rec, bcubed fscore) = (63.42, 57.05, 60.06)\n# nmi = 90.45\n\nimport os.path as osp\nfrom mmcv import Config\nfrom utils import rm_suffix\nfrom vegcn.extract import extract_gcn_v\n\n# data locations\nprefix = './data'\ntrain_name = 'deepfashion_train'\ntest_name = 'deepfashion_test'\nknn = 80\nknn_method = 'faiss'\nth_sim = 0.  # cut edges with similarity smaller than th_sim\n\n# testing args\nmax_conn = 1\ntau = 0.85\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# gcn_v configs\n_work_dir = 'work_dir'\nckpt_name = 'latest'  # epoch_20000\ngcnv_cfg = './vegcn/configs/cfg_train_gcnv_fashion.py'\ngcnv_cfg_name = rm_suffix(osp.basename(gcnv_cfg))\ngcnv_cfg = Config.fromfile(gcnv_cfg)\ngcnv_cfg.load_from = '{}/{}/{}/{}.pth'.format(prefix, _work_dir, gcnv_cfg_name,\n                                              ckpt_name)\n\nuse_gcn_feat = True\nfeat_paths = []\npred_conf_paths = []\ngcnv_nhid = gcnv_cfg.model.kwargs.nhid\nfor name in [train_name, test_name]:\n    gcnv_prefix = '{}/{}/{}/{}_gcnv_k_{}_th_{}'.format(prefix, _work_dir,\n                                                       gcnv_cfg_name, name,\n                                                       gcnv_cfg.knn,\n                                                       gcnv_cfg.th_sim)\n    feat_paths.append(\n        osp.join(gcnv_prefix, 'features', '{}.bin'.format(ckpt_name)))\n    pred_conf_paths.append(\n        osp.join(gcnv_prefix, 'pred_confs', '{}.npz'.format(ckpt_name)))\n\nif not use_gcn_feat:\n    gcnv_nhid = gcnv_cfg.model.kwargs.feature_dim\n    feat_paths = []\n    for name in [train_name, test_name]:\n        feat_paths.append(osp.join(prefix, 'features', '{}.bin'.format(name)))\n\ntrain_feat_path, test_feat_path = feat_paths\ntrain_pred_conf_path, test_pred_conf_path = pred_conf_paths\n\nextract_gcn_v(train_feat_path, train_pred_conf_path, 'train_data', gcnv_cfg)\nextract_gcn_v(test_feat_path, test_pred_conf_path, 'test_data', gcnv_cfg)\n\n# if `knn_graph_path` is not passed, it will build knn_graph automatically\ntrain_data = dict(feat_path=train_feat_path,\n                  label_path=osp.join(prefix, 'labels',\n                                      '{}.meta'.format(train_name)),\n                  pred_confs=train_pred_conf_path,\n                  k=knn,\n                  is_norm_feat=True,\n                  th_sim=th_sim,\n                  max_conn=max_conn,\n                  ignore_ratio=0.9)\n\ntest_data = dict(feat_path=test_feat_path,\n                 label_path=osp.join(prefix, 'labels',\n                                     '{}.meta'.format(test_name)),\n                 pred_confs=test_pred_conf_path,\n                 k=knn,\n                 is_norm_feat=True,\n                 th_sim=th_sim,\n                 max_conn=max_conn,\n                 ignore_ratio=0.8)\n\n# model\nregressor = False\nnclass = 1 if regressor else 2\nmodel = dict(type='gcn_e',\n             kwargs=dict(feature_dim=gcnv_nhid,\n                         nhid=512,\n                         nclass=nclass,\n                         dropout=0.))\n\n# training args\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=1e-5)\noptimizer_config = {}\n\ntotal_epochs = 40\nlr_config = dict(policy='step',\n                 step=[int(r * total_epochs) for r in [0.5, 0.8, 0.9]])\n\nbatch_size_per_gpu = 1\niter_size = 64\nworkflow = [('train_iter_size', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
vegcn/configs/cfg_train_gcne_ms1m.py,0,"b""# On 1 TitanX, it takes around 10hr for training\n# train with PyTorch (0.5.0a0+e31ab99), the performance is:\n# (pre, rec, pairwise fscore) = (90.55, 84.83, 87.6)\n# (pre, rec, bcubed fscore) = (93.82, 79.79, 86.23)\n# nmi = 96.45\n\nimport os.path as osp\nfrom mmcv import Config\nfrom utils import rm_suffix\nfrom vegcn.extract import extract_gcn_v\n\n# data locations\nprefix = './data'\ntrain_name = 'part0_train'\ntest_name = 'part1_test'\nknn = 160\nknn_method = 'faiss'\nth_sim = 0.  # cut edges with similarity smaller than th_sim\n\n# testing args\nmax_conn = 1\ntau = 0.8\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# gcn_v configs\n_work_dir = 'work_dir'\nckpt_name = 'latest'  # epoch_80000\ngcnv_cfg = './vegcn/configs/cfg_train_gcnv_ms1m.py'\ngcnv_cfg_name = rm_suffix(osp.basename(gcnv_cfg))\ngcnv_cfg = Config.fromfile(gcnv_cfg)\ngcnv_cfg.load_from = '{}/{}/{}/{}.pth'.format(prefix, _work_dir, gcnv_cfg_name,\n                                              ckpt_name)\n\nuse_gcn_feat = True\nfeat_paths = []\npred_conf_paths = []\ngcnv_nhid = gcnv_cfg.model.kwargs.nhid\nfor name in [train_name, test_name]:\n    gcnv_prefix = '{}/{}/{}/{}_gcnv_k_{}_th_{}'.format(prefix, _work_dir,\n                                                       gcnv_cfg_name, name,\n                                                       gcnv_cfg.knn,\n                                                       gcnv_cfg.th_sim)\n    feat_paths.append(\n        osp.join(gcnv_prefix, 'features', '{}.bin'.format(ckpt_name)))\n    pred_conf_paths.append(\n        osp.join(gcnv_prefix, 'pred_confs', '{}.npz'.format(ckpt_name)))\n\nif not use_gcn_feat:\n    gcnv_nhid = gcnv_cfg.model.kwargs.feature_dim\n    feat_paths = []\n    for name in [train_name, test_name]:\n        feat_paths.append(osp.join(prefix, 'features', '{}.bin'.format(name)))\n\ntrain_feat_path, test_feat_path = feat_paths\ntrain_pred_conf_path, test_pred_conf_path = pred_conf_paths\n\nextract_gcn_v(train_feat_path, train_pred_conf_path, 'train_data', gcnv_cfg)\nextract_gcn_v(test_feat_path, test_pred_conf_path, 'test_data', gcnv_cfg)\n\n# if `knn_graph_path` is not passed, it will build knn_graph automatically\ntrain_data = dict(feat_path=train_feat_path,\n                  label_path=osp.join(prefix, 'labels',\n                                      '{}.meta'.format(train_name)),\n                  pred_confs=train_pred_conf_path,\n                  k=knn,\n                  is_norm_feat=True,\n                  th_sim=th_sim,\n                  max_conn=max_conn,\n                  ignore_ratio=0.7)\n\ntest_data = dict(feat_path=test_feat_path,\n                 label_path=osp.join(prefix, 'labels',\n                                     '{}.meta'.format(test_name)),\n                 pred_confs=test_pred_conf_path,\n                 k=knn,\n                 is_norm_feat=True,\n                 th_sim=th_sim,\n                 max_conn=max_conn,\n                 ignore_ratio=0.8)\n\n# model\nregressor = False\nnclass = 1 if regressor else 2\nmodel = dict(type='gcn_e',\n             kwargs=dict(feature_dim=gcnv_nhid,\n                         nhid=512,\n                         nclass=nclass,\n                         dropout=0.))\n\n# training args\noptimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=1e-5)\noptimizer_config = {}\n\ntotal_epochs = 40\nlr_config = dict(policy='step',\n                 step=[int(r * total_epochs) for r in [0.5, 0.8, 0.9]])\n\nbatch_size_per_gpu = 1\niter_size = 32\nworkflow = [('train_iter_size', 1)]\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1)\n\nlog_level = 'INFO'\nlog_config = dict(interval=100, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
vegcn/configs/cfg_train_gcnv_fashion.py,0,"b""import os.path as osp\n\n# data locations\nprefix = './data'\ntrain_name = 'deepfashion_train'\ntest_name = 'deepfashion_test'\nknn = 5\nknn_method = 'faiss'\nth_sim = 0.  # cut edges with similarity smaller than th_sim\n\n# if `knn_graph_path` is not passed, it will build knn_graph automatically\ntrain_data = dict(\n    feat_path=osp.join(prefix, 'features', '{}.bin'.format(train_name)),\n    label_path=osp.join(prefix, 'labels', '{}.meta'.format(train_name)),\n    knn_graph_path=osp.join(prefix, 'knns', train_name,\n                            '{}_k_{}.npz'.format(knn_method, knn)),\n    k=knn,\n    is_norm_feat=True,\n    th_sim=th_sim,\n    conf_metric='s_nbr')\n\ntest_data = dict(feat_path=osp.join(prefix, 'features',\n                                    '{}.bin'.format(test_name)),\n                 label_path=osp.join(prefix, 'labels',\n                                     '{}.meta'.format(test_name)),\n                 knn_graph_path=osp.join(prefix, 'knns', test_name,\n                                         '{}_k_{}.npz'.format(knn_method,\n                                                              knn)),\n                 k=knn,\n                 is_norm_feat=True,\n                 th_sim=th_sim,\n                 conf_metric='s_nbr')\n\n# model\nmodel = dict(type='gcn_v',\n             kwargs=dict(feature_dim=256, nhid=512, nclass=1, dropout=0.))\n\n# training args\noptimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=1e-4)\noptimizer_config = {}\n\ntotal_epochs = 20000\nlr_config = dict(policy='step',\n                 step=[int(r * total_epochs) for r in [0.5, 0.8, 0.9]])\n\nbatch_size_per_gpu = 1\nworkflow = [('train_gcnv', 1)]\n\n# testing args\nuse_gcn_feat = True\nmax_conn = 1\ntau_0 = 0.8\ntau = 0.85\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=1000)\n\nlog_level = 'INFO'\nlog_config = dict(interval=1, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
vegcn/configs/cfg_train_gcnv_ms1m.py,0,"b""import os.path as osp\n\n# data locations\nprefix = './data'\ntrain_name = 'part0_train'\ntest_name = 'part1_test'\nknn = 80\nknn_method = 'faiss'\nth_sim = 0.  # cut edges with similarity smaller than th_sim\n\n# if `knn_graph_path` is not passed, it will build knn_graph automatically\ntrain_data = dict(feat_path=osp.join(prefix, 'features',\n                                    '{}.bin'.format(train_name)),\n                 label_path=osp.join(prefix, 'labels',\n                                     '{}.meta'.format(train_name)),\n                 knn_graph_path=osp.join(prefix, 'knns', train_name,\n                                         '{}_k_{}.npz'.format(knn_method,\n                                                              knn)),\n                 k=knn,\n                 is_norm_feat=True,\n                 th_sim=th_sim,\n                 conf_metric='s_nbr')\n\ntest_data = dict(feat_path=osp.join(prefix, 'features',\n                                    '{}.bin'.format(test_name)),\n                 label_path=osp.join(prefix, 'labels',\n                                     '{}.meta'.format(test_name)),\n                 knn_graph_path=osp.join(prefix, 'knns', test_name,\n                                         '{}_k_{}.npz'.format(knn_method,\n                                                              knn)),\n                 k=knn,\n                 is_norm_feat=True,\n                 th_sim=th_sim,\n                 conf_metric='s_nbr')\n\n# model\nmodel = dict(type='gcn_v',\n             kwargs=dict(feature_dim=256, nhid=512, nclass=1, dropout=0.))\n\n# training args\noptimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=1e-5)\noptimizer_config = {}\n\ntotal_epochs = 80000\nlr_config = dict(\n    policy='step',\n    step = [int(r * total_epochs) for r in [0.5, 0.8, 0.9]]\n)\n\nbatch_size_per_gpu = 1\nworkflow = [('train_gcnv', 1)]\n\n# testing args\nuse_gcn_feat = True\nmax_conn = 1\ntau_0 = 0.65\ntau = 0.8\n\nmetrics = ['pairwise', 'bcubed', 'nmi']\n\n# misc\nworkers_per_gpu = 1\n\ncheckpoint_config = dict(interval=5000)\n\nlog_level = 'INFO'\nlog_config = dict(interval=1, hooks=[\n    dict(type='TextLoggerHook'),\n])\n"""
vegcn/datasets/__init__.py,0,"b'from .gcn_v_dataset import GCNVDataset\nfrom .gcn_e_dataset import GCNEDataset\n\n__factory__ = {\n    \'gcn_v\': GCNVDataset,\n    \'gcn_e\': GCNEDataset,\n}\n\n\ndef build_dataset(model_type, cfg):\n    if model_type not in __factory__:\n        raise KeyError(""Unknown dataset type:"", model_type)\n    return __factory__[model_type](cfg)\n'"
vegcn/datasets/gcn_e_dataset.py,0,"b'import numpy as np\nimport os.path as osp\nfrom tqdm import tqdm\n\nfrom utils import (read_meta, read_probs, l2norm, build_knns,\n                   knns2ordered_nbrs, fast_knns2spmat, row_normalize,\n                   build_symmetric_adj, intdict2ndarray, rm_suffix, Timer)\n\n\nclass GCNEDataset(object):\n    def __init__(self, cfg):\n        feat_path = cfg[\'feat_path\']\n        label_path = cfg.get(\'label_path\', None)\n        knn_graph_path = cfg.get(\'knn_graph_path\', None)\n\n        self.k = cfg[\'k\']\n        self.feature_dim = cfg[\'feature_dim\']\n        self.is_norm_feat = cfg.get(\'is_norm_feat\', True)\n\n        self.th_sim = cfg.get(\'th_sim\', 0.)\n        self.max_conn = cfg.get(\'max_conn\', 1)\n\n        self.ignore_ratio = cfg.get(\'ignore_ratio\', 0.8)\n        self.ignore_small_confs = cfg.get(\'ignore_small_confs\', True)\n        self.use_candidate_set = cfg.get(\'use_candidate_set\', True)\n\n        self.nproc = cfg.get(\'nproc\', 1)\n        self.max_qsize = cfg.get(\'max_qsize\', int(1e5))\n\n        with Timer(\'read meta and feature\'):\n            if label_path is not None:\n                self.lb2idxs, self.idx2lb = read_meta(label_path)\n                self.inst_num = len(self.idx2lb)\n                self.gt_labels = intdict2ndarray(self.idx2lb)\n                self.ignore_label = False\n            else:\n                self.inst_num = -1\n                self.ignore_label = True\n            self.features = read_probs(feat_path, self.inst_num,\n                                       self.feature_dim)\n            if self.is_norm_feat:\n                self.features = l2norm(self.features)\n            if self.inst_num == -1:\n                self.inst_num = self.features.shape[0]\n            self.size = self.inst_num\n            assert self.size == self.features.shape[0]\n\n        print(\'feature shape: {}, k: {}, norm_feat: {}\'.format(\n            self.features.shape, self.k, self.is_norm_feat))\n\n        with Timer(\'read knn graph\'):\n            if knn_graph_path is not None:\n                knns = np.load(knn_graph_path)[\'data\']\n            else:\n                prefix = osp.dirname(feat_path)\n                name = rm_suffix(osp.basename(feat_path))\n                # find root folder of `features`\n                prefix = osp.dirname(prefix)\n                knn_prefix = osp.join(prefix, \'knns\', name)\n                knns = build_knns(knn_prefix, self.features, cfg.knn_method,\n                                  cfg.knn)\n            assert self.inst_num == len(knns), ""{} vs {}"".format(\n                self.inst_num, len(knns))\n\n            adj = fast_knns2spmat(knns, self.k, self.th_sim, use_sim=True)\n\n            # build symmetric adjacency matrix\n            adj = build_symmetric_adj(adj, self_loop=True)\n            self.adj = row_normalize(adj)\n\n            # convert knns to (dists, nbrs)\n            self.dists, self.nbrs = knns2ordered_nbrs(knns, sort=True)\n\n            if cfg.pred_confs != \'\':\n                print(\'read estimated confidence from {}\'.format(\n                    cfg.pred_confs))\n                self.confs = np.load(cfg.pred_confs)[\'pred_confs\']\n            else:\n                print(\'use unsupervised density as confidence\')\n                assert self.radius\n                from vegcn.confidence import density\n                self.confs = density(self.dists, radius=self.radius)\n\n            assert 0 <= self.ignore_ratio <= 1\n            if self.ignore_ratio == 1:\n                self.ignore_set = set(np.arange(len(self.confs)))\n            else:\n                num = int(len(self.confs) * self.ignore_ratio)\n                confs = self.confs\n                if not self.ignore_small_confs:\n                    confs = -confs\n                self.ignore_set = set(np.argpartition(confs, num)[:num])\n\n        print(\n            \'ignore_ratio: {}, ignore_small_confs: {}, use_candidate_set: {}\'.\n            format(self.ignore_ratio, self.ignore_small_confs,\n                   self.use_candidate_set))\n        print(\'#ignore_set: {} / {} = {:.3f}\'.format(\n            len(self.ignore_set), self.inst_num,\n            1. * len(self.ignore_set) / self.inst_num))\n\n        with Timer(\'Prepare sub-graphs\'):\n            # construct subgraphs with larger confidence\n            self.peaks = {i: [] for i in range(self.inst_num)}\n            self.dist2peak = {i: [] for i in range(self.inst_num)}\n\n            if self.nproc > 1:\n                # multi-process\n                import multiprocessing as mp\n                pool = mp.Pool(self.nproc)\n                results = []\n                num = int(self.inst_num / self.max_qsize) + 1\n                for i in tqdm(range(num)):\n                    beg = int(i * self.max_qsize)\n                    end = min(beg + self.max_qsize, self.inst_num)\n                    lst = [j for j in range(beg, end)]\n                    results.extend(\n                        list(\n                            tqdm(pool.map(self.get_subgraph, lst),\n                                 total=len(lst))))\n                pool.close()\n                pool.join()\n            else:\n                results = [\n                    self.get_subgraph(i) for i in tqdm(range(self.inst_num))\n                ]\n\n            self.adj_lst = []\n            self.feat_lst = []\n            self.lb_lst = []\n            self.subset_gt_labels = []\n            self.subset_idxs = []\n            self.subset_nbrs = []\n            self.subset_dists = []\n            for result in results:\n                if result is None:\n                    continue\n                elif len(result) == 3:\n                    i, nbr, dist = result\n                    self.peaks[i].extend(nbr)\n                    self.dist2peak[i].extend(dist)\n                    continue\n                i, nbr, dist, feat, adj, lb = result\n                self.subset_idxs.append(i)\n                self.subset_nbrs.append(nbr)\n                self.subset_dists.append(dist)\n                self.feat_lst.append(feat)\n                self.adj_lst.append(adj)\n                if not self.ignore_label:\n                    self.subset_gt_labels.append(self.idx2lb[i])\n                    self.lb_lst.append(lb)\n            self.subset_gt_labels = np.array(self.subset_gt_labels)\n\n            self.size = len(self.feat_lst)\n            assert self.size == len(self.adj_lst)\n            if not self.ignore_label:\n                assert self.size == len(self.lb_lst)\n\n    def get_subgraph(self, i):\n        nbr = self.nbrs[i]\n        dist = self.dists[i]\n        idxs = np.where(self.confs[nbr] > self.confs[i])[0]\n\n        if len(idxs) == 0:\n            return None\n        elif len(idxs) == 1 or i in self.ignore_set:\n            nbr_lst = []\n            dist_lst = []\n            for j in idxs[:self.max_conn]:\n                nbr_lst.append(nbr[j])\n                dist_lst.append(self.dists[i, j])\n            return i, nbr_lst, dist_lst\n\n        if self.use_candidate_set:\n            nbr = nbr[idxs]\n            dist = dist[idxs]\n\n        # present `direction`\n        feat = self.features[nbr] - self.features[i]\n        adj = self.adj[nbr, :][:, nbr]\n        adj = row_normalize(adj).toarray().astype(np.float32)\n\n        if not self.ignore_label:\n            lb = [int(self.idx2lb[i] == self.idx2lb[n]) for n in nbr]\n        else:\n            lb = [0 for _ in nbr]  # dummy labels\n        lb = np.array(lb)\n\n        return i, nbr, dist, feat, adj, lb\n\n    def __getitem__(self, index):\n        features = self.feat_lst[index]\n        adj = self.adj_lst[index]\n        if not self.ignore_label:\n            labels = self.lb_lst[index]\n        else:\n            labels = -1\n        return features, adj, labels\n\n    def __len__(self):\n        return self.size\n'"
vegcn/datasets/gcn_v_dataset.py,0,"b""import os\nimport numpy as np\n\nfrom utils import (read_meta, read_probs, l2norm, build_knns,\n                   knns2ordered_nbrs, fast_knns2spmat, row_normalize,\n                   build_symmetric_adj, sparse_mx_to_indices_values,\n                   intdict2ndarray, Timer)\nfrom vegcn.confidence import (confidence, confidence_to_peaks)\n\n\nclass GCNVDataset(object):\n    def __init__(self, cfg):\n        feat_path = cfg['feat_path']\n        label_path = cfg.get('label_path', None)\n        knn_graph_path = cfg.get('knn_graph_path', None)\n\n        self.k = cfg['k']\n        self.feature_dim = cfg['feature_dim']\n        self.is_norm_feat = cfg.get('is_norm_feat', True)\n        self.save_decomposed_adj = cfg.get('save_decomposed_adj', False)\n\n        self.th_sim = cfg.get('th_sim', 0.)\n        self.max_conn = cfg.get('max_conn', 1)\n        self.conf_metric = cfg.get('conf_metric')\n\n        with Timer('read meta and feature'):\n            if label_path is not None:\n                self.lb2idxs, self.idx2lb = read_meta(label_path)\n                self.inst_num = len(self.idx2lb)\n                self.gt_labels = intdict2ndarray(self.idx2lb)\n                self.ignore_label = False\n            else:\n                self.inst_num = -1\n                self.ignore_label = True\n            self.features = read_probs(feat_path, self.inst_num,\n                                       self.feature_dim)\n            if self.is_norm_feat:\n                self.features = l2norm(self.features)\n            if self.inst_num == -1:\n                self.inst_num = self.features.shape[0]\n            self.size = 1 # take the entire graph as input\n\n        with Timer('read knn graph'):\n            if os.path.isfile(knn_graph_path):\n                knns = np.load(knn_graph_path)['data']\n            else:\n                if knn_graph_path is not None:\n                    print('knn_graph_path does not exist: {}'.format(\n                        knn_graph_path))\n                knn_prefix = os.path.join(cfg.prefix, 'knns', cfg.name)\n                knns = build_knns(knn_prefix, self.features, cfg.knn_method,\n                                  cfg.knn)\n\n            adj = fast_knns2spmat(knns, self.k, self.th_sim, use_sim=True)\n\n            # build symmetric adjacency matrix\n            adj = build_symmetric_adj(adj, self_loop=True)\n            adj = row_normalize(adj)\n            if self.save_decomposed_adj:\n                adj = sparse_mx_to_indices_values(adj)\n                self.adj_indices, self.adj_values, self.adj_shape = adj\n            else:\n                self.adj = adj\n\n            # convert knns to (dists, nbrs)\n            self.dists, self.nbrs = knns2ordered_nbrs(knns)\n\n        print('feature shape: {}, k: {}, norm_feat: {}'.format(\n            self.features.shape, self.k, self.is_norm_feat))\n\n        if not self.ignore_label:\n            with Timer('Prepare ground-truth label'):\n                self.labels = confidence(feats=self.features,\n                                         dists=self.dists,\n                                         nbrs=self.nbrs,\n                                         metric=self.conf_metric,\n                                         idx2lb=self.idx2lb,\n                                         lb2idxs=self.lb2idxs)\n                if cfg.eval_interim:\n                    _, self.peaks = confidence_to_peaks(\n                        self.dists, self.nbrs, self.labels, self.max_conn)\n\n    def __getitem__(self, index):\n        ''' return the entire graph for training.\n        To accelerate training or cope with larger graph,\n        we can sample sub-graphs in this function.\n        '''\n\n        assert index == 0\n        return (self.features, self.adj_indices, self.adj_values,\n                self.adj_shape, self.labels)\n\n    def __len__(self):\n        return self.size\n"""
vegcn/models/__init__.py,0,"b'from .gcn_v import gcn_v\nfrom .gcn_e import gcn_e\n\n__factory__ = {\n    \'gcn_v\': gcn_v,\n    \'gcn_e\': gcn_e,\n}\n\n\ndef build_model(name, *args, **kwargs):\n    if name not in __factory__:\n        raise KeyError(""Unknown model:"", name)\n    return __factory__[name](*args, **kwargs)\n'"
vegcn/models/gcn_e.py,2,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom vegcn.models.utils import GraphConv, MeanAggregator\n\n\nclass GCN_E(nn.Module):\n    def __init__(self, feature_dim, nhid, nclass, dropout=0):\n        super(GCN_E, self).__init__()\n        nhid_half = int(nhid / 2)\n        self.conv1 = GraphConv(feature_dim, nhid, MeanAggregator, dropout)\n        self.conv2 = GraphConv(nhid, nhid, MeanAggregator, dropout)\n        self.conv3 = GraphConv(nhid, nhid_half, MeanAggregator, dropout)\n        self.conv4 = GraphConv(nhid_half, nhid_half, MeanAggregator, dropout)\n\n        self.nclass = nclass\n        self.classifier = nn.Sequential(nn.Linear(nhid_half, nhid_half),\n                                        nn.PReLU(nhid_half),\n                                        nn.Linear(nhid_half, self.nclass))\n        if nclass == 1:\n            self.loss = nn.MSELoss()\n        elif nclass == 2:\n            self.loss = nn.NLLLoss()\n        else:\n            raise ValueError('nclass should be 1 or 2')\n\n    def forward(self, data, return_loss=False):\n        x, adj = data[0], data[1]\n        x = self.conv1(x, adj)\n        x = self.conv2(x, adj)\n        x = self.conv3(x, adj)\n        x = self.conv4(x, adj)\n        x = x.view(-1, x.shape[-1])\n        pred = self.classifier(x)\n        pred = F.log_softmax(pred, dim=-1)\n\n        if return_loss:\n            label = data[2].view(-1)\n            loss = self.loss(pred, label)\n            return pred, loss\n\n        return pred\n\n\ndef gcn_e(feature_dim, nhid=512, nclass=1, dropout=0., **kwargs):\n    model = GCN_E(feature_dim=feature_dim,\n                  nhid=nhid,\n                  nclass=nclass,\n                  dropout=dropout)\n    return model\n"""
vegcn/models/gcn_v.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\n\nfrom vegcn.models.utils import GraphConv, MeanAggregator\n\n\nclass GCN_V(nn.Module):\n    def __init__(self, feature_dim, nhid, nclass, dropout=0):\n        super(GCN_V, self).__init__()\n        self.conv1 = GraphConv(feature_dim, nhid, MeanAggregator, dropout)\n\n        self.nclass = nclass\n        self.classifier = nn.Sequential(nn.Linear(nhid, nhid), nn.PReLU(nhid),\n                                        nn.Linear(nhid, self.nclass))\n        self.loss = torch.nn.MSELoss()\n\n    def forward(self, data, output_feat=False, return_loss=False):\n        assert not output_feat or not return_loss\n        x, adj = data[0], data[1]\n        x = self.conv1(x, adj)\n        pred = self.classifier(x).view(-1)\n\n        if output_feat:\n            return pred, x\n\n        if return_loss:\n            label = data[2]\n            loss = self.loss(pred, label)\n            return pred, loss\n\n        return pred\n\n\ndef gcn_v(feature_dim, nhid, nclass=1, dropout=0., **kwargs):\n    model = GCN_V(feature_dim=feature_dim,\n                  nhid=nhid,\n                  nclass=nclass,\n                  dropout=dropout)\n    return model\n'"
vegcn/models/utils.py,9,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\n\nclass MeanAggregator(nn.Module):\n    def __init__(self):\n        super(MeanAggregator, self).__init__()\n\n    def forward(self, features, A):\n        if features.dim() == 2:\n            x = torch.spmm(A, features)\n        elif features.dim() == 3:\n            x = torch.bmm(A, features)\n        else:\n            raise RuntimeError('the dimension of features should be 2 or 3')\n        return x\n\n\nclass GraphConv(nn.Module):\n    def __init__(self, in_dim, out_dim, agg, dropout=0):\n        super(GraphConv, self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.weight = nn.Parameter(torch.FloatTensor(in_dim * 2, out_dim))\n        self.bias = nn.Parameter(torch.FloatTensor(out_dim))\n        init.xavier_uniform_(self.weight)\n        init.constant_(self.bias, 0)\n        self.agg = agg()\n        self.dropout = dropout\n\n    def forward(self, features, A):\n        feat_dim = features.shape[-1]\n        assert (feat_dim == self.in_dim)\n        agg_feats = self.agg(features, A)\n        cat_feats = torch.cat([features, agg_feats], dim=-1)\n        if features.dim() == 2:\n            op = 'nd,df->nf'\n        elif features.dim() == 3:\n            op = 'bnd,df->bnf'\n        else:\n            raise RuntimeError('the dimension of features should be 2 or 3')\n        out = torch.einsum(op, (cat_feats, self.weight))\n        out = F.relu(out + self.bias)\n        if self.dropout > 0:\n            out = F.dropout(out, self.dropout, training=self.training)\n        return out\n"""
vegcn/runner/__init__.py,0,b'from .runner import Runner\n'
vegcn/runner/runner.py,0,"b""from mmcv.runner import Runner as _Runner\n\n\nclass Runner(_Runner):\n    def train_gcnv(self, dataset, **kwargs):\n        self.model.train()\n        self.mode = 'train'\n        self.data_loader = [None]  # dummy data_loader\n        self._max_iters = self._max_epochs\n        self.call_hook('before_train_epoch')\n        self._loss = 0\n        data_batch = dataset\n        self.call_hook('before_train_iter')\n        _outputs = self.batch_processor(self.model,\n                                        data_batch,\n                                        train_mode=True,\n                                        **kwargs)\n        if not isinstance(_outputs, dict):\n            raise TypeError('batch_processor() must return a dict')\n        if 'log_vars' in _outputs:\n            self.log_buffer.update(_outputs['log_vars'],\n                                   _outputs['num_samples'])\n        self._loss += _outputs['loss']\n        self.outputs = {'loss': self._loss}\n\n        self.call_hook('after_train_iter')\n        self._loss = 0\n        self._iter += 1\n\n        self.call_hook('after_train_epoch')\n        self._epoch += 1\n"""
