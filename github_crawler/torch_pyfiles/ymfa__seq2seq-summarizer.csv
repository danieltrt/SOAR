file_path,api_count,code
model.py,32,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport random\nfrom params import Params\nfrom utils import Vocab, Hypothesis, word_detector\nfrom typing import Union, List\n\nDEVICE = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\neps = 1e-31\n\n\nclass EncoderRNN(nn.Module):\n\n  def __init__(self, embed_size, hidden_size, bidi=True, *, rnn_drop: float=0):\n    super(EncoderRNN, self).__init__()\n    self.hidden_size = hidden_size\n    self.num_directions = 2 if bidi else 1\n    self.gru = nn.GRU(embed_size, hidden_size, bidirectional=bidi, dropout=rnn_drop)\n\n  def forward(self, embedded, hidden, input_lengths=None):\n    """"""\n    :param embedded: (src seq len, batch size, embed size)\n    :param hidden: (num directions, batch size, encoder hidden size)\n    :param input_lengths: list containing the non-padded length of each sequence in this batch;\n                          if set, we use `PackedSequence` to skip the PAD inputs and leave the\n                          corresponding encoder states as zeros\n    :return: (src seq len, batch size, hidden size * num directions = decoder hidden size)\n\n    Perform multi-step encoding.\n    """"""\n    if input_lengths is not None:\n      embedded = pack_padded_sequence(embedded, input_lengths)\n\n    output, hidden = self.gru(embedded, hidden)\n\n    if input_lengths is not None:\n      output, _ = pad_packed_sequence(output)\n\n    if self.num_directions > 1:\n      # hidden: (num directions, batch, hidden) => (1, batch, hidden * 2)\n      batch_size = hidden.size(1)\n      hidden = hidden.transpose(0, 1).contiguous().view(1, batch_size,\n                                                        self.hidden_size * self.num_directions)\n    return output, hidden\n\n  def init_hidden(self, batch_size):\n    return torch.zeros(self.num_directions, batch_size, self.hidden_size, device=DEVICE)\n\n\nclass DecoderRNN(nn.Module):\n\n  def __init__(self, vocab_size, embed_size, hidden_size, *, enc_attn=True, dec_attn=True,\n               enc_attn_cover=True, pointer=True, tied_embedding=None, out_embed_size=None,\n               in_drop: float=0, rnn_drop: float=0, out_drop: float=0, enc_hidden_size=None):\n    super(DecoderRNN, self).__init__()\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.combined_size = self.hidden_size\n    self.enc_attn = enc_attn\n    self.dec_attn = dec_attn\n    self.enc_attn_cover = enc_attn_cover\n    self.pointer = pointer\n    self.out_embed_size = out_embed_size\n    if tied_embedding is not None and self.out_embed_size and embed_size != self.out_embed_size:\n      print(""Warning: Output embedding size %d is overriden by its tied embedding size %d.""\n            % (self.out_embed_size, embed_size))\n      self.out_embed_size = embed_size\n\n    self.in_drop = nn.Dropout(in_drop) if in_drop > 0 else None\n    self.gru = nn.GRU(embed_size, self.hidden_size, dropout=rnn_drop)\n\n    if enc_attn:\n      if not enc_hidden_size: enc_hidden_size = self.hidden_size\n      self.enc_bilinear = nn.Bilinear(self.hidden_size, enc_hidden_size, 1)\n      self.combined_size += enc_hidden_size\n      if enc_attn_cover:\n        self.cover_weight = nn.Parameter(torch.rand(1))\n\n    if dec_attn:\n      self.dec_bilinear = nn.Bilinear(self.hidden_size, self.hidden_size, 1)\n      self.combined_size += self.hidden_size\n\n    self.out_drop = nn.Dropout(out_drop) if out_drop > 0 else None\n    if pointer:\n      self.ptr = nn.Linear(self.combined_size, 1)\n\n    if tied_embedding is not None and embed_size != self.combined_size:\n      # use pre_out layer if combined size is different from embedding size\n      self.out_embed_size = embed_size\n\n    if self.out_embed_size:  # use pre_out layer\n      self.pre_out = nn.Linear(self.combined_size, self.out_embed_size)\n      size_before_output = self.out_embed_size\n    else:  # don\'t use pre_out layer\n      size_before_output = self.combined_size\n\n    self.out = nn.Linear(size_before_output, vocab_size)\n    if tied_embedding is not None:\n      self.out.weight = tied_embedding.weight\n\n  def forward(self, embedded, hidden, encoder_states=None, decoder_states=None, coverage_vector=None, *,\n              encoder_word_idx=None, ext_vocab_size: int=None, log_prob: bool=True):\n    """"""\n    :param embedded: (batch size, embed size)\n    :param hidden: (1, batch size, decoder hidden size)\n    :param encoder_states: (src seq len, batch size, hidden size), for attention mechanism\n    :param decoder_states: (past dec steps, batch size, hidden size), for attention mechanism\n    :param encoder_word_idx: (src seq len, batch size), for pointer network\n    :param ext_vocab_size: the dynamic vocab size, determined by the max num of OOV words contained\n                           in any src seq in this batch, for pointer network\n    :param log_prob: return log probability instead of probability\n    :return: tuple of four things:\n             1. word prob or log word prob, (batch size, dynamic vocab size);\n             2. RNN hidden state after this step, (1, batch size, decoder hidden size);\n             3. attention weights over encoder states, (batch size, src seq len);\n             4. prob of copying by pointing as opposed to generating, (batch size, 1)\n\n    Perform single-step decoding.\n    """"""\n    batch_size = embedded.size(0)\n    combined = torch.zeros(batch_size, self.combined_size, device=DEVICE)\n\n    if self.in_drop: embedded = self.in_drop(embedded)\n\n    output, hidden = self.gru(embedded.unsqueeze(0), hidden)  # unsqueeze and squeeze are necessary\n    combined[:, :self.hidden_size] = output.squeeze(0)        # as RNN expects a 3D tensor (step=1)\n    offset = self.hidden_size\n    enc_attn, prob_ptr = None, None  # for visualization\n\n    if self.enc_attn or self.pointer:\n      # energy and attention: (num encoder states, batch size, 1)\n      num_enc_steps = encoder_states.size(0)\n      enc_total_size = encoder_states.size(2)\n      enc_energy = self.enc_bilinear(hidden.expand(num_enc_steps, batch_size, -1).contiguous(),\n                                     encoder_states)\n      if self.enc_attn_cover and coverage_vector is not None:\n        enc_energy += self.cover_weight * torch.log(coverage_vector.transpose(0, 1).unsqueeze(2) + eps)\n      # transpose => (batch size, num encoder states, 1)\n      enc_attn = F.softmax(enc_energy, dim=0).transpose(0, 1)\n      if self.enc_attn:\n        # context: (batch size, encoder hidden size, 1)\n        enc_context = torch.bmm(encoder_states.permute(1, 2, 0), enc_attn)\n        combined[:, offset:offset+enc_total_size] = enc_context.squeeze(2)\n        offset += enc_total_size\n      enc_attn = enc_attn.squeeze(2)\n\n    if self.dec_attn:\n      if decoder_states is not None and len(decoder_states) > 0:\n        dec_energy = self.dec_bilinear(hidden.expand_as(decoder_states).contiguous(),\n                                       decoder_states)\n        dec_attn = F.softmax(dec_energy, dim=0).transpose(0, 1)\n        dec_context = torch.bmm(decoder_states.permute(1, 2, 0), dec_attn)\n        combined[:, offset:offset + self.hidden_size] = dec_context.squeeze(2)\n      offset += self.hidden_size\n\n    if self.out_drop: combined = self.out_drop(combined)\n\n    # generator\n    if self.out_embed_size:\n      out_embed = self.pre_out(combined)\n    else:\n      out_embed = combined\n    logits = self.out(out_embed)  # (batch size, vocab size)\n\n    # pointer\n    if self.pointer:\n      output = torch.zeros(batch_size, ext_vocab_size, device=DEVICE)\n      # distribute probabilities between generator and pointer\n      prob_ptr = F.sigmoid(self.ptr(combined))  # (batch size, 1)\n      prob_gen = 1 - prob_ptr\n      # add generator probabilities to output\n      gen_output = F.softmax(logits, dim=1)  # can\'t use log_softmax due to adding probabilities\n      output[:, :self.vocab_size] = prob_gen * gen_output\n      # add pointer probabilities to output\n      ptr_output = enc_attn\n      output.scatter_add_(1, encoder_word_idx.transpose(0, 1), prob_ptr * ptr_output)\n      if log_prob: output = torch.log(output + eps)\n    else:\n      if log_prob: output = F.log_softmax(logits, dim=1)\n      else: output = F.softmax(logits, dim=1)\n\n    return output, hidden, enc_attn, prob_ptr\n\n\nclass Seq2SeqOutput(object):\n\n  def __init__(self, encoder_outputs: torch.Tensor, encoder_hidden: torch.Tensor,\n               decoded_tokens: torch.Tensor, loss: Union[torch.Tensor, float]=0,\n               loss_value: float=0, enc_attn_weights: torch.Tensor=None,\n               ptr_probs: torch.Tensor=None):\n    self.encoder_outputs = encoder_outputs\n    self.encoder_hidden = encoder_hidden\n    self.decoded_tokens = decoded_tokens  # (out seq len, batch size)\n    self.loss = loss  # scalar\n    self.loss_value = loss_value  # float value, excluding coverage loss\n    self.enc_attn_weights = enc_attn_weights  # (out seq len, batch size, src seq len)\n    self.ptr_probs = ptr_probs  # (out seq len, batch size)\n\n\nclass Seq2Seq(nn.Module):\n\n  def __init__(self, vocab: Vocab, params: Params, max_dec_steps=None):\n    """"""\n    :param vocab: mainly for info about special tokens and vocab size\n    :param params: model hyper-parameters\n    :param max_dec_steps: max num of decoding steps (only effective at test time, as during\n                          training the num of steps is determined by the `target_tensor`); it is\n                          safe to change `self.max_dec_steps` as the network architecture is\n                          independent of src/tgt seq lengths\n\n    Create the seq2seq model; its encoder and decoder will be created automatically.\n    """"""\n    super(Seq2Seq, self).__init__()\n    self.vocab = vocab\n    self.vocab_size = len(vocab)\n    if vocab.embeddings is not None:\n      self.embed_size = vocab.embeddings.shape[1]\n      if params.embed_size is not None and self.embed_size != params.embed_size:\n        print(""Warning: Model embedding size %d is overriden by pre-trained embedding size %d.""\n              % (params.embed_size, self.embed_size))\n      embedding_weights = torch.from_numpy(vocab.embeddings)\n    else:\n      self.embed_size = params.embed_size\n      embedding_weights = None\n    self.max_dec_steps = params.max_tgt_len + 1 if max_dec_steps is None else max_dec_steps\n    self.enc_attn = params.enc_attn\n    self.enc_attn_cover = params.enc_attn_cover\n    self.dec_attn = params.dec_attn\n    self.pointer = params.pointer\n    self.cover_loss = params.cover_loss\n    self.cover_func = params.cover_func\n    enc_total_size = params.hidden_size * 2 if params.enc_bidi else params.hidden_size\n    if params.dec_hidden_size:\n      dec_hidden_size = params.dec_hidden_size\n      self.enc_dec_adapter = nn.Linear(enc_total_size, dec_hidden_size)\n    else:\n      dec_hidden_size = enc_total_size\n      self.enc_dec_adapter = None\n\n    self.embedding = nn.Embedding(self.vocab_size, self.embed_size, padding_idx=vocab.PAD,\n                                  _weight=embedding_weights)\n    self.encoder = EncoderRNN(self.embed_size, params.hidden_size, params.enc_bidi,\n                              rnn_drop=params.enc_rnn_dropout)\n    self.decoder = DecoderRNN(self.vocab_size, self.embed_size, dec_hidden_size,\n                              enc_attn=params.enc_attn, dec_attn=params.dec_attn,\n                              pointer=params.pointer, out_embed_size=params.out_embed_size,\n                              tied_embedding=self.embedding if params.tie_embed else None,\n                              in_drop=params.dec_in_dropout, rnn_drop=params.dec_rnn_dropout,\n                              out_drop=params.dec_out_dropout, enc_hidden_size=enc_total_size)\n\n  def filter_oov(self, tensor, ext_vocab_size):\n    """"""Replace any OOV index in `tensor` with UNK""""""\n    if ext_vocab_size and ext_vocab_size > self.vocab_size:\n      result = tensor.clone()\n      result[tensor >= self.vocab_size] = self.vocab.UNK\n      return result\n    return tensor\n\n  def get_coverage_vector(self, enc_attn_weights):\n    """"""Combine the past attention weights into one vector""""""\n    if self.cover_func == \'max\':\n      coverage_vector, _ = torch.max(torch.cat(enc_attn_weights), dim=0)\n    elif self.cover_func == \'sum\':\n      coverage_vector = torch.sum(torch.cat(enc_attn_weights), dim=0)\n    else:\n      raise ValueError(\'Unrecognized cover_func: \' + self.cover_func)\n    return coverage_vector\n\n  def forward(self, input_tensor, target_tensor=None, input_lengths=None, criterion=None, *,\n              forcing_ratio=0, partial_forcing=True, ext_vocab_size=None, sample=False,\n              saved_out: Seq2SeqOutput=None, visualize: bool=None, include_cover_loss: bool=False)\\\n          -> Seq2SeqOutput:\n    """"""\n    :param input_tensor: tensor of word indices, (src seq len, batch size)\n    :param target_tensor: tensor of word indices, (tgt seq len, batch size)\n    :param input_lengths: see explanation in `EncoderRNN`\n    :param criterion: the loss function; if set, loss will be returned\n    :param forcing_ratio: see explanation in `Params` (requires `target_tensor`, training only)\n    :param partial_forcing: see explanation in `Params` (training only)\n    :param ext_vocab_size: see explanation in `DecoderRNN`\n    :param sample: if True, the returned `decoded_tokens` will be based on random sampling instead\n                   of greedily selecting the token of the highest probability at each step\n    :param saved_out: the output of this function in a previous run; if set, the encoding step will\n                      be skipped and we reuse the encoder states saved in this object\n    :param visualize: whether to return data for attention and pointer visualization; if None,\n                      return if no `criterion` is provided\n    :param include_cover_loss: whether to include coverage loss in the returned `loss_value`\n\n    Run the seq2seq model for training or testing.\n    """"""\n    input_length = input_tensor.size(0)\n    batch_size = input_tensor.size(1)\n    log_prob = not (sample or self.decoder.pointer)  # don\'t apply log too soon in these cases\n    if visualize is None:\n      visualize = criterion is None\n    if visualize and not (self.enc_attn or self.pointer):\n      visualize = False  # nothing to visualize\n\n    if target_tensor is None:\n      target_length = self.max_dec_steps\n    else:\n      target_length = target_tensor.size(0)\n\n    if forcing_ratio == 1:\n      # if fully teacher-forced, it may be possible to eliminate the for-loop over decoder steps\n      # for generality, this optimization is not investigated\n      use_teacher_forcing = True\n    elif forcing_ratio > 0:\n      if partial_forcing:\n        use_teacher_forcing = None  # decide later individually in each step\n      else:\n        use_teacher_forcing = random.random() < forcing_ratio\n    else:\n      use_teacher_forcing = False\n\n    if saved_out:  # reuse encoder states of a previous run\n      encoder_outputs = saved_out.encoder_outputs\n      encoder_hidden = saved_out.encoder_hidden\n      assert input_length == encoder_outputs.size(0)\n      assert batch_size == encoder_outputs.size(1)\n    else:  # run the encoder\n      encoder_hidden = self.encoder.init_hidden(batch_size)\n      # encoder_embedded: (input len, batch size, embed size)\n      encoder_embedded = self.embedding(self.filter_oov(input_tensor, ext_vocab_size))\n      encoder_outputs, encoder_hidden = \\\n        self.encoder(encoder_embedded, encoder_hidden, input_lengths)\n\n    # initialize return values\n    r = Seq2SeqOutput(encoder_outputs, encoder_hidden,\n                      torch.zeros(target_length, batch_size, dtype=torch.long))\n    if visualize:\n      r.enc_attn_weights = torch.zeros(target_length, batch_size, input_length)\n      if self.pointer:\n        r.ptr_probs = torch.zeros(target_length, batch_size)\n\n    decoder_input = torch.tensor([self.vocab.SOS] * batch_size, device=DEVICE)\n    if self.enc_dec_adapter is None:\n      decoder_hidden = encoder_hidden\n    else:\n      decoder_hidden = self.enc_dec_adapter(encoder_hidden)\n    decoder_states = []\n    enc_attn_weights = []\n\n    for di in range(target_length):\n      decoder_embedded = self.embedding(self.filter_oov(decoder_input, ext_vocab_size))\n      if enc_attn_weights:\n        coverage_vector = self.get_coverage_vector(enc_attn_weights)\n      else:\n        coverage_vector = None\n      decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = \\\n        self.decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n                     torch.cat(decoder_states) if decoder_states else None, coverage_vector,\n                     encoder_word_idx=input_tensor, ext_vocab_size=ext_vocab_size,\n                     log_prob=log_prob)\n      if self.dec_attn:\n        decoder_states.append(decoder_hidden)\n      # save the decoded tokens\n      if not sample:\n        _, top_idx = decoder_output.data.topk(1)  # top_idx shape: (batch size, k=1)\n      else:\n        prob_distribution = torch.exp(decoder_output) if log_prob else decoder_output\n        top_idx = torch.multinomial(prob_distribution, 1)\n      top_idx = top_idx.squeeze(1).detach()  # detach from history as input\n      r.decoded_tokens[di] = top_idx\n      # compute loss\n      if criterion:\n        if target_tensor is None:\n          gold_standard = top_idx  # for sampling\n        else:\n          gold_standard = target_tensor[di]\n        if not log_prob:\n          decoder_output = torch.log(decoder_output + eps)  # necessary for NLLLoss\n        nll_loss = criterion(decoder_output, gold_standard)\n        r.loss += nll_loss\n        r.loss_value += nll_loss.item()\n      # update attention history and compute coverage loss\n      if self.enc_attn_cover or (criterion and self.cover_loss > 0):\n        if coverage_vector is not None and criterion and self.cover_loss > 0:\n          coverage_loss = torch.sum(torch.min(coverage_vector, dec_enc_attn)) / batch_size \\\n                          * self.cover_loss\n          r.loss += coverage_loss\n          if include_cover_loss: r.loss_value += coverage_loss.item()\n        enc_attn_weights.append(dec_enc_attn.unsqueeze(0))\n      # save data for visualization\n      if visualize:\n        r.enc_attn_weights[di] = dec_enc_attn.data\n        if self.pointer:\n          r.ptr_probs[di] = dec_prob_ptr.squeeze(1).data\n      # decide the next input\n      if use_teacher_forcing or (use_teacher_forcing is None and random.random() < forcing_ratio):\n        decoder_input = target_tensor[di]  # teacher forcing\n      else:\n        decoder_input = top_idx\n    \n    return r\n\n  def beam_search(self, input_tensor, input_lengths=None, ext_vocab_size=None, beam_size=4, *,\n                  min_out_len=1, max_out_len=None, len_in_words=True) -> List[Hypothesis]:\n    """"""\n    :param input_tensor: tensor of word indices, (src seq len, batch size); for now, batch size has\n                         to be 1\n    :param input_lengths: see explanation in `EncoderRNN`\n    :param ext_vocab_size: see explanation in `DecoderRNN`\n    :param beam_size: the beam size\n    :param min_out_len: required minimum output length\n    :param max_out_len: required maximum output length (if None, use the model\'s own value)\n    :param len_in_words: if True, count output length in words instead of tokens (i.e. do not count\n                         punctuations)\n    :return: list of the best decoded sequences, in descending order of probability\n\n    Use beam search to generate summaries.\n    """"""\n    batch_size = input_tensor.size(1)\n    assert batch_size == 1\n    if max_out_len is None:\n      max_out_len = self.max_dec_steps - 1  # max_out_len doesn\'t count EOS\n\n    # encode\n    encoder_hidden = self.encoder.init_hidden(batch_size)\n    # encoder_embedded: (input len, batch size, embed size)\n    encoder_embedded = self.embedding(self.filter_oov(input_tensor, ext_vocab_size))\n    encoder_outputs, encoder_hidden = \\\n      self.encoder(encoder_embedded, encoder_hidden, input_lengths)\n    if self.enc_dec_adapter is None:\n      decoder_hidden = encoder_hidden\n    else:\n      decoder_hidden = self.enc_dec_adapter(encoder_hidden)\n    # turn batch size from 1 to beam size (by repeating)\n    # if we want dynamic batch size, the following must be created for all possible batch sizes\n    encoder_outputs = encoder_outputs.expand(-1, beam_size, -1).contiguous()\n    input_tensor = input_tensor.expand(-1, beam_size).contiguous()\n\n    # decode\n    hypos = [Hypothesis([self.vocab.SOS], [], decoder_hidden, [], [], 1)]\n    results, backup_results = [], []\n    step = 0\n    while hypos and step < 2 * max_out_len:  # prevent infinitely generating punctuations\n      # make batch size equal to beam size (n_hypos <= beam size)\n      n_hypos = len(hypos)\n      if n_hypos < beam_size:\n        hypos.extend(hypos[-1] for _ in range(beam_size - n_hypos))\n      # assemble existing hypotheses into a batch\n      decoder_input = torch.tensor([h.tokens[-1] for h in hypos], device=DEVICE)\n      decoder_hidden = torch.cat([h.dec_hidden for h in hypos], 1)\n      if self.dec_attn and step > 0:  # dim 0 is decoding step, dim 1 is beam batch\n        decoder_states = torch.cat([torch.cat(h.dec_states, 0) for h in hypos], 1)\n      else:\n        decoder_states = None\n      if self.enc_attn_cover:\n        enc_attn_weights = [torch.cat([h.enc_attn_weights[i] for h in hypos], 1)\n                            for i in range(step)]\n      else:\n        enc_attn_weights = []\n      if enc_attn_weights:\n        coverage_vector = self.get_coverage_vector(enc_attn_weights)  # shape: (beam size, src len)\n      else:\n        coverage_vector = None\n      # run the decoder over the assembled batch\n      decoder_embedded = self.embedding(self.filter_oov(decoder_input, ext_vocab_size))\n      decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = \\\n        self.decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n                     decoder_states, coverage_vector,\n                     encoder_word_idx=input_tensor, ext_vocab_size=ext_vocab_size)\n      top_v, top_i = decoder_output.data.topk(beam_size)  # shape of both: (beam size, beam size)\n      # create new hypotheses\n      new_hypos = []\n      for in_idx in range(n_hypos):\n        for out_idx in range(beam_size):\n          new_tok = top_i[in_idx][out_idx].item()\n          new_prob = top_v[in_idx][out_idx].item()\n          if len_in_words:\n            non_word = not self.vocab.is_word(new_tok)\n          else:\n            non_word = new_tok == self.vocab.EOS  # only SOS & EOS don\'t count\n          new_hypo = hypos[in_idx].create_next(new_tok, new_prob,\n                                               decoder_hidden[0][in_idx].unsqueeze(0).unsqueeze(0),\n                                               self.dec_attn,\n                                               dec_enc_attn[in_idx].unsqueeze(0).unsqueeze(0)\n                                               if dec_enc_attn is not None else None, non_word)\n          new_hypos.append(new_hypo)\n      # process the new hypotheses\n      new_hypos = sorted(new_hypos, key=lambda h: -h.avg_log_prob)\n      hypos = []\n      new_complete_results, new_incomplete_results = [], []\n      for nh in new_hypos:\n        length = len(nh)\n        if nh.tokens[-1] == self.vocab.EOS:  # a complete hypothesis\n          if len(new_complete_results) < beam_size and min_out_len <= length <= max_out_len:\n            new_complete_results.append(nh)\n        elif len(hypos) < beam_size and length < max_out_len:  # an incomplete hypothesis\n          hypos.append(nh)\n        elif length == max_out_len and len(new_incomplete_results) < beam_size:\n          new_incomplete_results.append(nh)\n      if new_complete_results:\n        results.extend(new_complete_results)\n      elif new_incomplete_results:\n        backup_results.extend(new_incomplete_results)\n      step += 1\n    if not results:  # if no sequence ends with EOS within desired length, fallback to sequences\n      results = backup_results  # that are ""truncated"" at the end to max_out_len\n    return sorted(results, key=lambda h: -h.avg_log_prob)[:beam_size]\n'"
params.py,0,"b'from typing import Optional, Union, List\n\n\nclass Params:\n  # Model architecture\n  vocab_size: int = 30000\n  hidden_size: int = 150  # of the encoder; default decoder size is doubled if encoder is bidi\n  dec_hidden_size: Optional[int] = 200  # if set, a matrix will transform enc state into dec state\n  embed_size: int = 100\n  enc_bidi: bool = True\n  enc_attn: bool = True  # decoder has attention over encoder states?\n  dec_attn: bool = False  # decoder has attention over previous decoder states?\n  pointer: bool = True  # use pointer network (copy mechanism) in addition to word generator?\n  out_embed_size: Optional[int] = None  # if set, use an additional layer before decoder output\n  tie_embed: bool = True  # tie the decoder output layer to the input embedding layer?\n\n  # Coverage (to turn on/off, change both `enc_attn_cover` and `cover_loss`)\n  enc_attn_cover: bool = True  # provide coverage as input when computing enc attn?\n  cover_func: str = \'max\'  # how to aggregate previous attention distributions? sum or max\n  cover_loss: float = 1  # add coverage loss if > 0; weight of coverage loss as compared to NLLLoss\n  show_cover_loss: bool = False  # include coverage loss in the loss shown in the progress bar?\n\n  # Regularization\n  enc_rnn_dropout: float = 0\n  dec_in_dropout: float = 0\n  dec_rnn_dropout: float = 0\n  dec_out_dropout: float = 0\n\n  # Training\n  optimizer: str = \'adam\'  # adam or adagrad\n  lr: float = 0.001  # learning rate\n  adagrad_accumulator: float = 0.1\n  lr_decay_step: int = 5  # decay lr every how many epochs?\n  lr_decay: Optional[float] = None  # decay lr by multiplying this factor\n  batch_size: int = 32\n  n_batches: int = 1000  # how many batches per epoch\n  val_batch_size: int = 32\n  n_val_batches: int = 100  # how many validation batches per epoch\n  n_epochs: int = 75\n  pack_seq: bool = True  # use packed sequence to skip PAD inputs?\n  forcing_ratio: float = 0.75  # initial percentage of using teacher forcing\n  partial_forcing: bool = True  # in a seq, can some steps be teacher forced and some not?\n  forcing_decay_type: Optional[str] = \'exp\'  # linear, exp, sigmoid, or None\n  forcing_decay: float = 0.9999\n  sample: bool = True  # are non-teacher forced inputs based on sampling or greedy selection?\n  grad_norm: float = 1  # use gradient clipping if > 0; max gradient norm\n  # note: enabling reinforcement learning can significantly slow down training\n  rl_ratio: float = 0  # use mixed objective if > 0; ratio of RL in the loss function\n  rl_ratio_power: float = 1  # increase rl_ratio by **= rl_ratio_power after each epoch; (0, 1]\n  rl_start_epoch: int = 1  # start RL at which epoch (later start can ensure a strong baseline)?\n\n  # Data\n  embed_file: Optional[str] = \'data/.vector_cache/glove.6B.100d.txt\'  # use pre-trained embeddings\n  data_path: str = \'data/cnndm.gz\'\n  val_data_path: Optional[str] = \'data/cnndm.val.gz\'\n  max_src_len: int = 400  # exclusive of special tokens such as EOS\n  max_tgt_len: int = 100  # exclusive of special tokens such as EOS\n  truncate_src: bool = True  # truncate to max_src_len? if false, drop example if too long\n  truncate_tgt: bool = True  # truncate to max_tgt_len? if false, drop example if too long\n\n  # Saving model automatically during training\n  model_path_prefix: Optional[str] = \'checkpoints/cnndm05\'\n  keep_every_epoch: bool = False  # save all epochs, or only the best and the latest one?\n\n  # Testing\n  beam_size: int = 4\n  min_out_len: int = 60\n  max_out_len: Optional[int] = 100\n  out_len_in_words: bool = False\n  test_data_path: str = \'data/cnndm.test.gz\'\n  test_sample_ratio: float = 1  # what portion of the test data is used? (1 for all data)\n  test_save_results: bool = False\n\n  def update(self, cmd_args: List[str]):\n    """"""Update configuration by a list of command line arguments""""""\n    arg_name = None\n    for arg_text in cmd_args:\n      if arg_name is None:\n        assert arg_text.startswith(\'--\')  # the arg name has to start with ""--""\n        arg_name = arg_text[2:]\n      else:\n        arg_curr_value = getattr(self, arg_name)\n        if arg_text.lower() == \'none\':\n          arg_new_value = None\n        elif arg_text.lower() == \'true\':\n          arg_new_value = True\n        elif arg_text.lower() == \'false\':\n          arg_new_value = False\n        else:\n          arg_type = self.__annotations__[arg_name]\n          if type(arg_type) is not type:  # support only Optional[T], where T is a basic type\n            assert arg_type.__origin__ is Union\n            arg_types = [t for t in arg_type.__args__ if t is not type(None)]\n            assert len(arg_types) == 1\n            arg_type = arg_types[0]\n            assert type(arg_type) is type\n          arg_new_value = arg_type(arg_text)\n        setattr(self, arg_name, arg_new_value)\n        print(""Hyper-parameter %s = %s (was %s)"" % (arg_name, arg_new_value, arg_curr_value))\n        arg_name = None\n    if arg_name is not None:\n      print(""Warning: Argument %s lacks a value and is ignored."" % arg_name)\n'"
test.py,8,"b'import torch\nimport tarfile\nfrom io import BytesIO\nfrom typing import Dict, Tuple, List, Union, Optional\nfrom utils import rouge, Vocab, OOVDict, Batch, format_tokens, format_rouge_scores, Dataset\nfrom model import DEVICE, Seq2SeqOutput, Seq2Seq\nfrom params import Params\nfrom tqdm import tqdm\n\n\ndef decode_batch_output(decoded_tokens, vocab: Vocab, oov_dict: OOVDict) -> List[List[str]]:\n  """"""Convert word indices to strings.""""""\n  decoded_batch = []\n  if not isinstance(decoded_tokens, list):\n    decoded_tokens = decoded_tokens.transpose(0, 1).tolist()\n  for i, doc in enumerate(decoded_tokens):\n    decoded_doc = []\n    for word_idx in doc:\n      if word_idx >= len(vocab):\n        word = oov_dict.index2word.get((i, word_idx), \'<UNK>\')\n      else:\n        word = vocab[word_idx]\n      decoded_doc.append(word)\n      if word_idx == vocab.EOS:\n        break\n    decoded_batch.append(decoded_doc)\n  return decoded_batch\n\n\ndef decode_batch(batch: Batch, model: Seq2Seq, vocab: Vocab, criterion=None, *, pack_seq=True,\n                 show_cover_loss=False) -> Tuple[List[List[str]], Seq2SeqOutput]:\n  """"""Test the `model` on the `batch`, return the decoded textual tokens and the Seq2SeqOutput.""""""\n  if not pack_seq:\n    input_lengths = None\n  else:\n    input_lengths = batch.input_lengths\n  with torch.no_grad():\n    input_tensor = batch.input_tensor.to(DEVICE)\n    if batch.target_tensor is None or criterion is None:\n      target_tensor = None\n    else:\n      target_tensor = batch.target_tensor.to(DEVICE)\n    out = model(input_tensor, target_tensor, input_lengths, criterion,\n                ext_vocab_size=batch.ext_vocab_size, include_cover_loss=show_cover_loss)\n    decoded_batch = decode_batch_output(out.decoded_tokens, vocab, batch.oov_dict)\n  target_length = batch.target_tensor.size(0)\n  out.loss_value /= target_length\n  return decoded_batch, out\n\n\ndef decode_one(*args, **kwargs):\n  """"""\n  Same as `decode_batch()` but because batch size is 1, the batch dim in visualization data is\n  eliminated.\n  """"""\n  decoded_batch, out = decode_batch(*args, **kwargs)\n  decoded_doc = decoded_batch[0]\n  if out.enc_attn_weights is not None:\n    out.enc_attn_weights = out.enc_attn_weights[:len(decoded_doc), 0, :]\n  if out.ptr_probs is not None:\n    out.ptr_probs = out.ptr_probs[:len(decoded_doc), 0]\n  return decoded_doc, out\n\n\ndef eval_batch(batch: Batch, model: Seq2Seq, vocab: Vocab, criterion=None, *, pack_seq=True,\n               show_cover_loss=False) -> Tuple[float, float]:\n  """"""Test the `model` on the `batch`, return the ROUGE score and the loss.""""""\n  decoded_batch, out = decode_batch(batch, model, vocab, criterion=criterion, pack_seq=pack_seq,\n                                    show_cover_loss=show_cover_loss)\n  examples = batch[0]\n  gold_summaries = [ex.tgt for ex in examples]\n  scores = rouge(gold_summaries, decoded_batch)\n  return out.loss_value, scores[0][\'l_f\']\n\n\ndef eval_batch_output(tgt_tensor_or_tokens: Union[torch.Tensor, List[List[str]]], vocab: Vocab,\n                      oov_dict: OOVDict, *pred_tensors: torch.Tensor) -> List[Dict[str, float]]:\n  """"""\n  :param tgt_tensor_or_tokens: the gold standard, either as indices or textual tokens\n  :param vocab: the fixed-size vocab\n  :param oov_dict: out-of-vocab dict\n  :param pred_tensors: one or more systems\' prediction (output tensors)\n  :return: two-level score lookup (system index => ROUGE metric => value)\n\n  Evaluate one or more systems\' output.\n  """"""\n  decoded_batch = [decode_batch_output(pred_tensor, vocab, oov_dict)\n                   for pred_tensor in pred_tensors]\n  if isinstance(tgt_tensor_or_tokens, torch.Tensor):\n    gold_summaries = decode_batch_output(tgt_tensor_or_tokens, vocab, oov_dict)\n  else:\n    gold_summaries = tgt_tensor_or_tokens\n  scores = rouge(gold_summaries, *decoded_batch)\n  return scores\n\n\ndef eval_bs_batch(batch: Batch, model: Seq2Seq, vocab: Vocab, *, pack_seq=True, beam_size=4,\n                  min_out_len=1, max_out_len=None, len_in_words=True, best_only=True,\n                  details: bool=True) -> Tuple[Optional[List[Dict[str, float]]], Optional[str]]:\n  """"""\n  :param batch: a test batch of a single example\n  :param model: a trained summarizer\n  :param vocab: vocabulary of the trained summarizer\n  :param pack_seq: currently has no effect as batch size is 1\n  :param beam_size: the beam size\n  :param min_out_len: required minimum output length\n  :param max_out_len: required maximum output length (if None, use the model\'s own value)\n  :param len_in_words: if True, count output length in words instead of tokens (i.e. do not count\n                       punctuations)\n  :param best_only: if True, run ROUGE only on the best hypothesis instead of all `beam size` many\n  :param details: if True, also return a string containing the result of this document\n  :return: two-level score lookup (hypothesis index => ROUGE metric => value)\n\n  Test a trained summarizer on a document using the beam search decoder.\n  """"""\n  assert len(batch.examples) == 1\n  with torch.no_grad():\n    input_tensor = batch.input_tensor.to(DEVICE)\n    hypotheses = model.beam_search(input_tensor, batch.input_lengths if pack_seq else None,\n                                   batch.ext_vocab_size, beam_size, min_out_len=min_out_len,\n                                   max_out_len=max_out_len, len_in_words=len_in_words)\n  if best_only:\n    to_decode = [hypotheses[0].tokens]\n  else:\n    to_decode = [h.tokens for h in hypotheses]\n  decoded_batch = decode_batch_output(to_decode, vocab, batch.oov_dict)\n  if details:\n    file_content = ""[System Summary]\\n"" + format_tokens(decoded_batch[0])\n  else:\n    file_content = None\n  if batch.examples[0].tgt is not None:  # run ROUGE if gold standard summary exists\n    gold_summaries = [batch.examples[0].tgt for _ in range(len(decoded_batch))]\n    scores = rouge(gold_summaries, decoded_batch)\n    if details:\n      file_content += ""\\n\\n\\n[Reference Summary]\\n"" + format_tokens(batch.examples[0].tgt)\n      file_content += ""\\n\\n\\n[ROUGE Scores]\\n"" + format_rouge_scores(scores[0]) + ""\\n""\n  else:\n    scores = None\n  if details:\n    file_content += ""\\n\\n\\n[Source Text]\\n"" + format_tokens(batch.examples[0].src)\n  return scores, file_content\n\n\ndef eval_bs(test_set: Dataset, vocab: Vocab, model: Seq2Seq, params: Params):\n  test_gen = test_set.generator(1, vocab, None, True if params.pointer else False)\n  n_samples = int(params.test_sample_ratio * len(test_set.pairs))\n\n  if params.test_save_results and params.model_path_prefix:\n    result_file = tarfile.open(params.model_path_prefix + "".results.tgz"", \'w:gz\')\n  else:\n    result_file = None\n\n  model.eval()\n  r1, r2, rl, rsu4 = 0, 0, 0, 0\n  prog_bar = tqdm(range(1, n_samples + 1))\n  for i in prog_bar:\n    batch = next(test_gen)\n    scores, file_content = eval_bs_batch(batch, model, vocab, pack_seq=params.pack_seq,\n                                         beam_size=params.beam_size,\n                                         min_out_len=params.min_out_len,\n                                         max_out_len=params.max_out_len,\n                                         len_in_words=params.out_len_in_words,\n                                         details=result_file is not None)\n    if file_content:\n      file_content = file_content.encode(\'utf-8\')\n      file_info = tarfile.TarInfo(name=\'%06d.txt\' % i)\n      file_info.size = len(file_content)\n      result_file.addfile(file_info, fileobj=BytesIO(file_content))\n    if scores:\n      r1 += scores[0][\'1_f\']\n      r2 += scores[0][\'2_f\']\n      rl += scores[0][\'l_f\']\n      rsu4 += scores[0][\'su4_f\']\n      prog_bar.set_postfix(R1=\'%.4g\' % (r1 / i * 100), R2=\'%.4g\' % (r2 / i * 100),\n                           RL=\'%.4g\' % (rl / i * 100), RSU4=\'%.4g\' % (rsu4 / i * 100))\n\n\nif __name__ == ""__main__"":\n  import argparse\n  import os.path\n\n  parser = argparse.ArgumentParser(description=\'Evaluate a summarization model.\')\n  parser.add_argument(\'--model\', type=str, metavar=\'M\', help=\'path to the model to be evaluated\')\n  args, unknown_args = parser.parse_known_args()\n\n  p = Params()\n  if unknown_args:  # allow command line args to override params.py\n    p.update(unknown_args)\n\n  if args.model:  # evaluate a specific model\n    filename = args.model\n  else:  # evaluate the best model\n    train_status = torch.load(p.model_path_prefix + "".train.pt"")\n    filename = \'%s.%02d.pt\' % (p.model_path_prefix, train_status[\'best_epoch_so_far\'])\n\n  print(""Evaluating %s..."" % filename)\n  m = torch.load(filename)  # use map_location=\'cpu\' if you are testing a CUDA model using CPU\n\n  m.encoder.gru.flatten_parameters()\n  m.decoder.gru.flatten_parameters()\n\n  if hasattr(m, \'vocab\'):\n    v = m.vocab\n  else:  # fixes for models trained by a previous version of the summarizer\n    filename, _ = os.path.splitext(p.data_path)\n    if p.vocab_size:\n      filename += "".%d"" % p.vocab_size\n    filename += \'.vocab\'\n    v = torch.load(filename)\n    m.vocab = v\n    m.max_dec_steps = m.max_output_length\n\n  d = Dataset(p.test_data_path)\n  eval_bs(d, v, m, p)\n'"
train.py,6,"b'import torch\nimport torch.nn as nn\nimport math\nimport os\nfrom torch import optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom tqdm import tqdm\nfrom utils import Dataset, show_plot, Vocab, Batch\nfrom model import Seq2Seq, DEVICE\nfrom params import Params\nfrom test import eval_batch, eval_batch_output\n\n\ndef train_batch(batch: Batch, model: Seq2Seq, criterion, optimizer, *,\n                pack_seq=True, forcing_ratio=0.5, partial_forcing=True, sample=False,\n                rl_ratio: float=0, vocab=None, grad_norm: float=0, show_cover_loss=False):\n  if not pack_seq:\n    input_lengths = None\n  else:\n    input_lengths = batch.input_lengths\n\n  optimizer.zero_grad()\n  input_tensor = batch.input_tensor.to(DEVICE)\n  target_tensor = batch.target_tensor.to(DEVICE)\n  ext_vocab_size = batch.ext_vocab_size\n\n  out = model(input_tensor, target_tensor, input_lengths, criterion,\n              forcing_ratio=forcing_ratio, partial_forcing=partial_forcing, sample=sample,\n              ext_vocab_size=ext_vocab_size, include_cover_loss=show_cover_loss)\n\n  if rl_ratio > 0:\n    assert vocab is not None\n    sample_out = model(input_tensor, saved_out=out, criterion=criterion, sample=True,\n                       ext_vocab_size=ext_vocab_size)\n    baseline_out = model(input_tensor, saved_out=out, visualize=False,\n                         ext_vocab_size=ext_vocab_size)\n    scores = eval_batch_output([ex.tgt for ex in batch.examples], vocab, batch.oov_dict,\n                               sample_out.decoded_tokens, baseline_out.decoded_tokens)\n    greedy_rouge = scores[1][\'l_f\']\n    neg_reward = greedy_rouge - scores[0][\'l_f\']\n    # if sample > baseline, the reward is positive (i.e. good exploration), rl_loss is negative\n    rl_loss = neg_reward * sample_out.loss\n    rl_loss_value = neg_reward * sample_out.loss_value\n    loss = (1 - rl_ratio) * out.loss + rl_ratio * rl_loss\n    loss_value = (1 - rl_ratio) * out.loss_value + rl_ratio * rl_loss_value\n  else:\n    loss = out.loss\n    loss_value = out.loss_value\n    greedy_rouge = None\n\n  loss.backward()\n  if grad_norm > 0:\n    clip_grad_norm_(model.parameters(), grad_norm)\n  optimizer.step()\n\n  target_length = target_tensor.size(0)\n  return loss_value / target_length, greedy_rouge\n\n\ndef train(train_generator, vocab: Vocab, model: Seq2Seq, params: Params, valid_generator=None,\n          saved_state: dict=None):\n  # variables for plotting\n  plot_points_per_epoch = max(math.log(params.n_batches, 1.6), 1.)\n  plot_every = round(params.n_batches / plot_points_per_epoch)\n  plot_losses, cached_losses = [], []\n  plot_val_losses, plot_val_metrics = [], []\n\n  total_parameters = sum(parameter.numel() for parameter in model.parameters()\n                         if parameter.requires_grad)\n  print(""Training %d trainable parameters..."" % total_parameters)\n  model.to(DEVICE)\n  if saved_state is None:\n    if params.optimizer == \'adagrad\':\n      optimizer = optim.Adagrad(model.parameters(), lr=params.lr,\n                                initial_accumulator_value=params.adagrad_accumulator)\n    else:\n      optimizer = optim.Adam(model.parameters(), lr=params.lr)\n    past_epochs = 0\n    total_batch_count = 0\n  else:\n    optimizer = saved_state[\'optimizer\']\n    past_epochs = saved_state[\'epoch\']\n    total_batch_count = saved_state[\'total_batch_count\']\n  if params.lr_decay:\n    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, params.lr_decay_step, params.lr_decay,\n                                             past_epochs - 1)\n  criterion = nn.NLLLoss(ignore_index=vocab.PAD)\n  best_avg_loss, best_epoch_id = float(""inf""), None\n\n  for epoch_count in range(1 + past_epochs, params.n_epochs + 1):\n    if params.lr_decay:\n      lr_scheduler.step()\n    rl_ratio = params.rl_ratio if epoch_count >= params.rl_start_epoch else 0\n    epoch_loss, epoch_metric = 0, 0\n    epoch_avg_loss, valid_avg_loss, valid_avg_metric = None, None, None\n    prog_bar = tqdm(range(1, params.n_batches + 1), desc=\'Epoch %d\' % epoch_count)\n    model.train()\n\n    for batch_count in prog_bar:  # training batches\n      if params.forcing_decay_type:\n        if params.forcing_decay_type == \'linear\':\n          forcing_ratio = max(0, params.forcing_ratio - params.forcing_decay * total_batch_count)\n        elif params.forcing_decay_type == \'exp\':\n          forcing_ratio = params.forcing_ratio * (params.forcing_decay ** total_batch_count)\n        elif params.forcing_decay_type == \'sigmoid\':\n          forcing_ratio = params.forcing_ratio * params.forcing_decay / (\n                  params.forcing_decay + math.exp(total_batch_count / params.forcing_decay))\n        else:\n          raise ValueError(\'Unrecognized forcing_decay_type: \' + params.forcing_decay_type)\n      else:\n        forcing_ratio = params.forcing_ratio\n\n      batch = next(train_generator)\n      loss, metric = train_batch(batch, model, criterion, optimizer, pack_seq=params.pack_seq,\n                                 forcing_ratio=forcing_ratio,\n                                 partial_forcing=params.partial_forcing, sample=params.sample,\n                                 rl_ratio=rl_ratio, vocab=vocab, grad_norm=params.grad_norm,\n                                 show_cover_loss=params.show_cover_loss)\n\n      epoch_loss += float(loss)\n      epoch_avg_loss = epoch_loss / batch_count\n      if metric is not None:  # print ROUGE as well if reinforcement learning is enabled\n        epoch_metric += metric\n        epoch_avg_metric = epoch_metric / batch_count\n        prog_bar.set_postfix(loss=\'%g\' % epoch_avg_loss, rouge=\'%.4g\' % (epoch_avg_metric * 100))\n      else:\n        prog_bar.set_postfix(loss=\'%g\' % epoch_avg_loss)\n\n      cached_losses.append(loss)\n      total_batch_count += 1\n      if total_batch_count % plot_every == 0:\n        period_avg_loss = sum(cached_losses) / len(cached_losses)\n        plot_losses.append(period_avg_loss)\n        cached_losses = []\n\n    if valid_generator is not None:  # validation batches\n      valid_loss, valid_metric = 0, 0\n      prog_bar = tqdm(range(1, params.n_val_batches + 1), desc=\'Valid %d\' % epoch_count)\n      model.eval()\n\n      for batch_count in prog_bar:\n        batch = next(valid_generator)\n        loss, metric = eval_batch(batch, model, vocab, criterion, pack_seq=params.pack_seq,\n                                  show_cover_loss=params.show_cover_loss)\n        valid_loss += loss\n        valid_metric += metric\n        valid_avg_loss = valid_loss / batch_count\n        valid_avg_metric = valid_metric / batch_count\n        prog_bar.set_postfix(loss=\'%g\' % valid_avg_loss, rouge=\'%.4g\' % (valid_avg_metric * 100))\n\n      plot_val_losses.append(valid_avg_loss)\n      plot_val_metrics.append(valid_avg_metric)\n\n      metric_loss = -valid_avg_metric  # choose the best model by ROUGE instead of loss\n      if metric_loss < best_avg_loss:\n        best_epoch_id = epoch_count\n        best_avg_loss = metric_loss\n\n    else:  # no validation, ""best"" is defined by training loss\n      if epoch_avg_loss < best_avg_loss:\n        best_epoch_id = epoch_count\n        best_avg_loss = epoch_avg_loss\n\n    if params.model_path_prefix:\n      # save model\n      filename = \'%s.%02d.pt\' % (params.model_path_prefix, epoch_count)\n      torch.save(model, filename)\n      if not params.keep_every_epoch:  # clear previously saved models\n        for epoch_id in range(1 + past_epochs, epoch_count):\n          if epoch_id != best_epoch_id:\n            try:\n              prev_filename = \'%s.%02d.pt\' % (params.model_path_prefix, epoch_id)\n              os.remove(prev_filename)\n            except FileNotFoundError:\n              pass\n      # save training status\n      torch.save({\n        \'epoch\': epoch_count,\n        \'total_batch_count\': total_batch_count,\n        \'train_avg_loss\': epoch_avg_loss,\n        \'valid_avg_loss\': valid_avg_loss,\n        \'valid_avg_metric\': valid_avg_metric,\n        \'best_epoch_so_far\': best_epoch_id,\n        \'params\': params,\n        \'optimizer\': optimizer\n      }, \'%s.train.pt\' % params.model_path_prefix)\n\n    if rl_ratio > 0:\n      params.rl_ratio **= params.rl_ratio_power\n\n    show_plot(plot_losses, plot_every, plot_val_losses, plot_val_metrics, params.n_batches,\n              params.model_path_prefix)\n\n\nif __name__ == ""__main__"":\n  import argparse\n\n  parser = argparse.ArgumentParser(description=\'Train the seq2seq abstractive summarizer.\')\n  parser.add_argument(\'--resume_from\', type=str, metavar=\'R\',\n                      help=\'path to a saved training status (*.train.pt)\')\n  args, unknown_args = parser.parse_known_args()\n\n  if args.resume_from:\n    print(""Resuming from %s..."" % args.resume_from)\n    train_status = torch.load(args.resume_from)\n    m = torch.load(\'%s.%02d.pt\' % (args.resume_from[:-9], train_status[\'epoch\']))\n    p = train_status[\'params\']\n  else:\n    p = Params()\n    m = None\n    train_status = None\n\n  if unknown_args:  # allow command line args to override params.py\n    p.update(unknown_args)\n\n  dataset = Dataset(p.data_path, max_src_len=p.max_src_len, max_tgt_len=p.max_tgt_len,\n                    truncate_src=p.truncate_src, truncate_tgt=p.truncate_tgt)\n  if m is None:\n    v = dataset.build_vocab(p.vocab_size, embed_file=p.embed_file)\n    m = Seq2Seq(v, p)\n  else:\n    v = dataset.build_vocab(p.vocab_size)\n\n  train_gen = dataset.generator(p.batch_size, v, v, True if p.pointer else False)\n  if p.val_data_path:\n    val_dataset = Dataset(p.val_data_path, max_src_len=p.max_src_len, max_tgt_len=p.max_tgt_len,\n                          truncate_src=p.truncate_src, truncate_tgt=p.truncate_tgt)\n    val_gen = val_dataset.generator(p.val_batch_size, v, v, True if p.pointer else False)\n  else:\n    val_gen = None\n\n  train(train_gen, v, m, p, val_gen, train_status)\n'"
utils.py,6,"b'import os\nimport re\nfrom tempfile import TemporaryDirectory\nimport subprocess\nfrom multiprocessing.dummy import Pool\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom typing import NamedTuple, List, Callable, Dict, Tuple, Optional\nfrom collections import Counter\nfrom random import shuffle\nfrom functools import lru_cache\nimport torch\nimport gzip\n\n\nplt.switch_backend(\'agg\')\n\nword_detector = re.compile(\'\\w\')\n\n\nclass Vocab(object):\n\n  PAD = 0\n  SOS = 1\n  EOS = 2\n  UNK = 3\n\n  def __init__(self):\n    self.word2index = {}\n    self.word2count = Counter()\n    self.reserved = [\'<PAD>\', \'<SOS>\', \'<EOS>\', \'<UNK>\']\n    self.index2word = self.reserved[:]\n    self.embeddings = None\n\n  def add_words(self, words: List[str]):\n    for word in words:\n      if word not in self.word2index:\n        self.word2index[word] = len(self.index2word)\n        self.index2word.append(word)\n    self.word2count.update(words)\n\n  def trim(self, *, vocab_size: int=None, min_freq: int=1):\n    if min_freq <= 1 and (vocab_size is None or vocab_size >= len(self.word2index)):\n      return\n    ordered_words = sorted(((c, w) for (w, c) in self.word2count.items()), reverse=True)\n    if vocab_size:\n      ordered_words = ordered_words[:vocab_size]\n    self.word2index = {}\n    self.word2count = Counter()\n    self.index2word = self.reserved[:]\n    for count, word in ordered_words:\n      if count < min_freq: break\n      self.word2index[word] = len(self.index2word)\n      self.word2count[word] = count\n      self.index2word.append(word)\n\n  def load_embeddings(self, file_path: str, dtype=np.float32) -> int:\n    num_embeddings = 0\n    vocab_size = len(self)\n    with open(file_path, \'rb\') as f:\n      for line in f:\n        line = line.split()\n        word = line[0].decode(\'utf-8\')\n        idx = self.word2index.get(word)\n        if idx is not None:\n          vec = np.array(line[1:], dtype=dtype)\n          if self.embeddings is None:\n            n_dims = len(vec)\n            self.embeddings = np.random.normal(np.zeros((vocab_size, n_dims))).astype(dtype)\n            self.embeddings[self.PAD] = np.zeros(n_dims)\n          self.embeddings[idx] = vec\n          num_embeddings += 1\n    return num_embeddings\n\n  def __getitem__(self, item):\n    if type(item) is int:\n      return self.index2word[item]\n    return self.word2index.get(item, self.UNK)\n\n  def __len__(self):\n    return len(self.index2word)\n\n  @lru_cache(maxsize=None)\n  def is_word(self, token_id: int) -> bool:\n    """"""Return whether the token at `token_id` is a word; False for punctuations.""""""\n    if token_id < 4: return False\n    if token_id >= len(self): return True  # OOV is assumed to be words\n    token_str = self.index2word[token_id]\n    if not word_detector.search(token_str) or token_str == \'<P>\':\n      return False\n    return True\n\n\nclass Example(NamedTuple):\n  src: List[str]\n  tgt: List[str]\n  src_len: int  # inclusive of EOS, so that it corresponds to tensor shape\n  tgt_len: int  # inclusive of EOS, so that it corresponds to tensor shape\n\n\nclass OOVDict(object):\n\n  def __init__(self, base_oov_idx):\n    self.word2index = {}  # type: Dict[Tuple[int, str], int]\n    self.index2word = {}  # type: Dict[Tuple[int, int], str]\n    self.next_index = {}  # type: Dict[int, int]\n    self.base_oov_idx = base_oov_idx\n    self.ext_vocab_size = base_oov_idx\n\n  def add_word(self, idx_in_batch, word) -> int:\n    key = (idx_in_batch, word)\n    index = self.word2index.get(key)\n    if index is not None: return index\n    index = self.next_index.get(idx_in_batch, self.base_oov_idx)\n    self.next_index[idx_in_batch] = index + 1\n    self.word2index[key] = index\n    self.index2word[(idx_in_batch, index)] = word\n    self.ext_vocab_size = max(self.ext_vocab_size, index + 1)\n    return index\n\n\nclass Batch(NamedTuple):\n  examples: List[Example]\n  input_tensor: Optional[torch.Tensor]\n  target_tensor: Optional[torch.Tensor]\n  input_lengths: Optional[List[int]]\n  oov_dict: Optional[OOVDict]\n\n  @property\n  def ext_vocab_size(self):\n    if self.oov_dict is not None:\n      return self.oov_dict.ext_vocab_size\n    return None\n\ndef simple_tokenizer(text: str, lower: bool=False, newline: str=None) -> List[str]:\n  """"""Split an already tokenized input `text`.""""""\n  if lower:\n    text = text.lower()\n  if newline is not None:  # replace newline by a token\n    text = text.replace(\'\\n\', \' \' + newline + \' \')\n  return text.split()\n\n\nclass Dataset(object):\n\n  def __init__(self, filename: str, tokenize: Callable=simple_tokenizer, max_src_len: int=None,\n               max_tgt_len: int=None, truncate_src: bool=False, truncate_tgt: bool=False):\n    print(""Reading dataset %s..."" % filename, end=\' \', flush=True)\n    self.filename = filename\n    self.pairs = []\n    self.src_len = 0\n    self.tgt_len = 0\n    if filename.endswith(\'.gz\'):\n      open = gzip.open\n    with open(filename, \'rt\', encoding=\'utf-8\') as f:\n      for i, line in enumerate(f):\n        pair = line.strip().split(\'\\t\')\n        if len(pair) != 2:\n          print(""Line %d of %s is malformed."" % (i, filename))\n          continue\n        src = tokenize(pair[0])\n        if max_src_len and len(src) > max_src_len:\n          if truncate_src:\n            src = src[:max_src_len]\n          else:\n            continue\n        tgt = tokenize(pair[1])\n        if max_tgt_len and len(tgt) > max_tgt_len:\n          if truncate_tgt:\n            tgt = tgt[:max_tgt_len]\n          else:\n            continue\n        src_len = len(src) + 1  # EOS\n        tgt_len = len(tgt) + 1  # EOS\n        self.src_len = max(self.src_len, src_len)\n        self.tgt_len = max(self.tgt_len, tgt_len)\n        self.pairs.append(Example(src, tgt, src_len, tgt_len))\n    print(""%d pairs."" % len(self.pairs))\n\n  def build_vocab(self, vocab_size: int=None, src: bool=True, tgt: bool=True,\n                  embed_file: str=None) -> Vocab:\n    filename, _ = os.path.splitext(self.filename)\n    if vocab_size:\n      filename += "".%d"" % vocab_size\n    filename += \'.vocab\'\n    if os.path.isfile(filename):\n      vocab = torch.load(filename)\n      print(""Vocabulary loaded, %d words."" % len(vocab))\n    else:\n      print(""Building vocabulary..."", end=\' \', flush=True)\n      vocab = Vocab()\n      for example in self.pairs:\n        if src:\n          vocab.add_words(example.src)\n        if tgt:\n          vocab.add_words(example.tgt)\n      vocab.trim(vocab_size=vocab_size)\n      print(""%d words."" % len(vocab))\n      torch.save(vocab, filename)\n    if embed_file:\n      count = vocab.load_embeddings(embed_file)\n      print(""%d pre-trained embeddings loaded."" % count)\n    return vocab\n\n  def generator(self, batch_size: int, src_vocab: Vocab=None, tgt_vocab: Vocab=None,\n                ext_vocab: bool=False):\n    ptr = len(self.pairs)  # make sure to shuffle at first run\n    if ext_vocab:\n      assert src_vocab is not None\n      base_oov_idx = len(src_vocab)\n    while True:\n      if ptr + batch_size > len(self.pairs):\n        shuffle(self.pairs)  # shuffle inplace to save memory\n        ptr = 0\n      examples = self.pairs[ptr:ptr + batch_size]\n      ptr += batch_size\n      src_tensor, tgt_tensor = None, None\n      lengths, oov_dict = None, None\n      if src_vocab or tgt_vocab:\n        # initialize tensors\n        if src_vocab:\n          examples.sort(key=lambda x: -x.src_len)\n          lengths = [x.src_len for x in examples]\n          max_src_len = lengths[0]\n          src_tensor = torch.zeros(max_src_len, batch_size, dtype=torch.long)\n          if ext_vocab:\n            oov_dict = OOVDict(base_oov_idx)\n        if tgt_vocab:\n          max_tgt_len = max(x.tgt_len for x in examples)\n          tgt_tensor = torch.zeros(max_tgt_len, batch_size, dtype=torch.long)\n        # fill up tensors by word indices\n        for i, example in enumerate(examples):\n          if src_vocab:\n            for j, word in enumerate(example.src):\n              idx = src_vocab[word]\n              if ext_vocab and idx == src_vocab.UNK:\n                idx = oov_dict.add_word(i, word)\n              src_tensor[j, i] = idx\n            src_tensor[example.src_len - 1, i] = src_vocab.EOS\n          if tgt_vocab:\n            for j, word in enumerate(example.tgt):\n              idx = tgt_vocab[word]\n              if ext_vocab and idx == src_vocab.UNK:\n                idx = oov_dict.word2index.get((i, word), idx)\n              tgt_tensor[j, i] = idx\n            tgt_tensor[example.tgt_len - 1, i] = tgt_vocab.EOS\n      yield Batch(examples, src_tensor, tgt_tensor, lengths, oov_dict)\n\n\nclass Hypothesis(object):\n\n  def __init__(self, tokens, log_probs, dec_hidden, dec_states, enc_attn_weights, num_non_words):\n    self.tokens = tokens  # type: List[int]\n    self.log_probs = log_probs  # type: List[float]\n    self.dec_hidden = dec_hidden  # shape: (1, 1, hidden_size)\n    self.dec_states = dec_states  # list of dec_hidden\n    self.enc_attn_weights = enc_attn_weights  # list of shape: (1, 1, src_len)\n    self.num_non_words = num_non_words  # type: int\n\n  def __repr__(self):\n    return repr(self.tokens)\n\n  def __len__(self):\n    return len(self.tokens) - self.num_non_words\n\n  @property\n  def avg_log_prob(self):\n    return sum(self.log_probs) / len(self.log_probs)\n\n  def create_next(self, token, log_prob, dec_hidden, add_dec_states, enc_attn, non_word):\n    return Hypothesis(tokens=self.tokens + [token], log_probs=self.log_probs + [log_prob],\n                      dec_hidden=dec_hidden, dec_states=\n                      self.dec_states + [dec_hidden] if add_dec_states else self.dec_states,\n                      enc_attn_weights=self.enc_attn_weights + [enc_attn]\n                      if enc_attn is not None else self.enc_attn_weights,\n                      num_non_words=self.num_non_words + 1 if non_word else self.num_non_words)\n\n\ndef show_plot(loss, step=1, val_loss=None, val_metric=None, val_step=1, file_prefix=None):\n  plt.figure()\n  fig, ax = plt.subplots(figsize=(12, 8))\n  # this locator puts ticks at regular intervals\n  loc = ticker.MultipleLocator(base=0.2)\n  ax.yaxis.set_major_locator(loc)\n  ax.set_ylabel(\'Loss\', color=\'b\')\n  ax.set_xlabel(\'Batch\')\n  plt.plot(range(step, len(loss) * step + 1, step), loss, \'b\')\n  if val_loss:\n    plt.plot(range(val_step, len(val_loss) * val_step + 1, val_step), val_loss, \'g\')\n  if val_metric:\n    ax2 = ax.twinx()\n    ax2.plot(range(val_step, len(val_metric) * val_step + 1, val_step), val_metric, \'r\')\n    ax2.set_ylabel(\'ROUGE\', color=\'r\')\n  if file_prefix:\n    plt.savefig(file_prefix + \'.png\')\n    plt.close()\n\n\ndef show_attention_map(src_words, pred_words, attention, pointer_ratio=None):\n  fig, ax = plt.subplots(figsize=(16, 4))\n  im = plt.pcolormesh(np.flipud(attention), cmap=""GnBu"")\n  # set ticks and labels\n  ax.set_xticks(np.arange(len(src_words)) + 0.5)\n  ax.set_xticklabels(src_words, fontsize=14)\n  ax.set_yticks(np.arange(len(pred_words)) + 0.5)\n  ax.set_yticklabels(reversed(pred_words), fontsize=14)\n  if pointer_ratio is not None:\n    ax1 = ax.twinx()\n    ax1.set_yticks(np.concatenate([np.arange(0.5, len(pred_words)), [len(pred_words)]]))\n    ax1.set_yticklabels(\'%.3f\' % v for v in np.flipud(pointer_ratio))\n    ax1.set_ylabel(\'Copy probability\', rotation=-90, va=""bottom"")\n  # let the horizontal axes labelling appear on top\n  ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n  # rotate the tick labels and set their alignment\n  plt.setp(ax.get_xticklabels(), rotation=-45, ha=""right"", rotation_mode=""anchor"")\n\n\nnon_word_char_in_word = re.compile(r""(?<=\\w)\\W(?=\\w)"")\nnot_for_output = {\'<PAD>\', \'<SOS>\', \'<EOS>\', \'<UNK>\'}\n\ndef format_tokens(tokens: List[str], newline: str= \'<P>\', for_rouge: bool=False) -> str:\n  """"""Join output `tokens` for ROUGE evaluation.""""""\n  tokens = filter(lambda t: t not in not_for_output, tokens)\n  if for_rouge:\n    tokens = [non_word_char_in_word.sub("""", t) for t in tokens]  # ""n\'t"" => ""nt""\n  if newline is None:\n    s = \' \'.join(tokens)\n  else:  # replace newline tokens by newlines\n    lines, line = [], []\n    for tok in tokens:\n      if tok == newline:\n        if line: lines.append("" "".join(line))\n        line = []\n      else:\n        line.append(tok)\n    if line: lines.append("" "".join(line))\n    s = \'\\n\'.join(lines)\n  return s\n\ndef format_rouge_scores(rouge_result: Dict[str, float]) -> str:\n  lines = []\n  line, prev_metric = [], None\n  for key in sorted(rouge_result.keys()):\n    metric = key.rsplit(""_"", maxsplit=1)[0]\n    if metric != prev_metric and prev_metric is not None:\n      lines.append(""\\t"".join(line))\n      line = []\n    line.append(""%s %s"" % (key, rouge_result[key]))\n    prev_metric = metric\n  lines.append(""\\t"".join(line))\n  return ""\\n"".join(lines)\n\n\nthis_dir = os.path.dirname(os.path.abspath(__file__))\n\nrouge_pattern = re.compile(rb""(\\d+) ROUGE-(.+) Average_([RPF]): ([\\d.]+) ""\n                           rb""\\(95%-conf\\.int\\. ([\\d.]+) - ([\\d.]+)\\)"")\n\ndef rouge(target: List[List[str]], *predictions: List[List[str]]) -> List[Dict[str, float]]:\n  """"""Perform single-reference ROUGE evaluation of one or more systems\' predictions.""""""\n  results = [dict() for _ in range(len(predictions))]  # e.g. 0 => \'su4_f\' => 0.35\n  with TemporaryDirectory() as folder:  # on my server, /tmp is a RAM disk\n    # write SPL files\n    eval_entries = []\n    for i, tgt_tokens in enumerate(target):\n      sys_entries = []\n      for j, pred_docs in enumerate(predictions):\n        sys_file = \'sys%d_%d.spl\' % (j, i)\n        sys_entries.append(\'\\n    <P ID=""%d"">%s</P>\' % (j, sys_file))\n        with open(os.path.join(folder, sys_file), \'wt\') as f:\n          f.write(format_tokens(pred_docs[i], for_rouge=True))\n      ref_file = \'ref_%d.spl\' % i\n      with open(os.path.join(folder, ref_file), \'wt\') as f:\n        f.write(format_tokens(tgt_tokens, for_rouge=True))\n      eval_entry = """"""\n<EVAL ID=""{1}"">\n  <PEER-ROOT>{0}</PEER-ROOT>\n  <MODEL-ROOT>{0}</MODEL-ROOT>\n  <INPUT-FORMAT TYPE=""SPL""></INPUT-FORMAT>\n  <PEERS>{2}\n  </PEERS>\n  <MODELS>\n    <M ID=""A"">{3}</M>\n  </MODELS>\n</EVAL>"""""".format(folder, i, \'\'.join(sys_entries), ref_file)\n      eval_entries.append(eval_entry)\n    # write config file\n    xml = \'<ROUGE-EVAL version=""1.0"">{0}\\n</ROUGE-EVAL>\'.format("""".join(eval_entries))\n    config_path = os.path.join(folder, \'task.xml\')\n    with open(config_path, \'wt\') as f:\n      f.write(xml)\n    # run ROUGE\n    out = subprocess.check_output(\'./ROUGE-1.5.5.pl -e data -a -n 2 -2 4 -u \' + config_path,\n                                  shell=True, cwd=os.path.join(this_dir, \'data\'))\n  # parse ROUGE output\n  for line in out.split(b\'\\n\'):\n    match = rouge_pattern.match(line)\n    if match:\n      sys_id, metric, rpf, value, low, high = match.groups()\n      results[int(sys_id)][(metric + b\'_\' + rpf).decode(\'utf-8\').lower()] = float(value)\n  return results\n\n\ndef rouge_single(example: List[List[str]]) -> List[Dict[str, float]]:\n  """"""Helper for `rouge_parallel()`.""""""\n  return rouge(*example)\n\n\ndef rouge_parallel(target: List[List[str]], *predictions: List[List[str]]) \\\n        -> List[List[Dict[str, float]]]:\n  """"""\n  Run ROUGE tests in parallel (by Python multi-threading, i.e. multiprocessing.dummy) to obtain\n  per-document scores. Depending on batch size and hardware, this may be slower or faster than\n  `rouge()`.\n  """"""\n  with Pool() as p:\n    return p.map(rouge_single, zip(target, *predictions))\n'"
data/make_cnndm_data.py,0,"b'""""""\nPre-process the CNN/Daily Mail dataset. Before using this script, please download the following\nfiles and put all of them under `data/cnndm`:\n\n* cnn_stories_tokenized.zip, dm_stories_tokenized.zip -- These can be obtained from\n  https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail\n* all_test.txt, all_train.txt, all_val.txt -- These are the indices of documents in See et al\'s\n  training/validation/testing sets, used here to ensure the same data split. They can be found in\n  https://github.com/abisee/cnn-dailymail/tree/master/url_lists\n\nThis script will generate `cnndm.gz`, `cnndm.val.gz`, and `cnndm.test.gz`. Each file is a gzipped\ntext file containing one example per line.\n""""""\nimport re\nimport os\nimport gzip\nfrom zipfile import ZipFile\nfrom hashlib import sha1\n\n\nsplitter = re.compile(r\'(-)\')\nword_recognizer = re.compile(r\'^\\w[\\w\\-]+\\w$\')\ncontractions = {""s"", ""d"", ""ve"", ""ll"", ""m"", ""re"", ""em""}\nptb_unescape = {\'-LRB-\': \'(\', \'-RRB-\': \')\', \'-LCB-\': \'{\', \'-RCB-\': \'}\'}\n\nprint_every = 1000  # print progress every 1000 documents\ndata_path = os.path.dirname(os.path.abspath(__file__))\ncorpus_path = os.path.join(data_path, \'cnndm\')\n\n\ndef split_example(filename: str, data: str, eop: str=\'<P>\') -> tuple:\n  text, summary = [], []\n  highlight_mode = False\n  for paragraph in data.split(\'\\n\\n\'):\n    if paragraph == \'@highlight\':\n      highlight_mode = True\n    else:\n      original_tokens = paragraph.split()\n      tokens, next_prefix = [], None\n      for i, tok in enumerate(original_tokens):\n        if tok == \'\xc2\xbf\':  # convert \xc2\xbf into \'\n          if i + 1 < len(original_tokens):\n            if original_tokens[i+1] == \'t\' and len(tokens) > 0 and tokens[-1][-1] == \'n\':\n              tokens[-1] = tokens[-1][:-1]\n              next_prefix = ""n\'""\n            elif original_tokens[i+1] in contractions:\n              next_prefix = ""\'""\n            elif len(tokens) > 0 and tokens[-1] == \'o\':  # o \' clock => o\'clock\n              tokens.pop()\n              next_prefix = ""o\'""\n            elif len(tokens) > 0 and tokens[-1] == \'y\':  # y \' all => y\' all\n              tokens[-1] = ""y\'""\n            else:\n              tokens.append(""\'"")\n          else:\n            tokens.append(""\'"")\n        elif tok in ptb_unescape:\n          assert next_prefix is None\n          tokens.append(ptb_unescape[tok])\n        elif tok == \'|\':\n          assert next_prefix is None\n        else:\n          tok = tok.lower()\n          if next_prefix is not None:\n            tok = next_prefix + tok\n          if tok == \'-\':\n            tokens.append(\'--\')\n          elif \'-\' in tok and not \'--\' in tok and word_recognizer.match(tok):\n            tokens.extend(t for t in splitter.split(tok) if t)\n          else:\n            tokens.append(tok)\n          next_prefix = None\n      if not tokens:\n        continue  # skip empty paragraphs\n      if eop: tokens.append(eop)\n      if highlight_mode is False:\n        text.extend(tokens)\n      else:\n        if highlight_mode is True:\n          summary.extend(tokens)\n          highlight_mode = None\n        else:\n          print(""A paragraph in %s is dropped because it is not text or summary."" % filename)\n  return text, summary\n\n\ndef get_story_set(filename: str) -> set:\n  story_names = set()\n  with open(os.path.join(corpus_path, filename), \'rb\') as f:\n    for line in f:\n      story_names.add(sha1(line.strip()).hexdigest())\n  return story_names\n\n\ntrain_set = get_story_set(\'all_train.txt\')\nvalid_set = get_story_set(\'all_val.txt\')\ntest_set = get_story_set(\'all_test.txt\')\ntrain_out = gzip.open(os.path.join(data_path, \'cnndm.gz\'), \'wt\')\nvalid_out = gzip.open(os.path.join(data_path, \'cnndm.val.gz\'), \'wt\')\ntest_out = gzip.open(os.path.join(data_path, \'cnndm.test.gz\'), \'wt\')\n\ncount = 0\nfor download_file in [\'cnn_stories_tokenized.zip\', \'dm_stories_tokenized.zip\']:\n  with ZipFile(os.path.join(corpus_path, download_file), \'r\') as archive:\n    for filename in archive.namelist():\n      if not filename.endswith(\'.story\'): continue\n      story_name = filename[-46:-6]\n      if story_name in train_set:\n        fout = train_out\n      elif story_name in valid_set:\n        fout = valid_out\n      elif story_name in test_set:\n        fout = test_out\n      else:\n        print(""Error: filename %s is not found in train, valid, or test set."" % filename)\n        continue\n      with archive.open(filename, \'r\') as f:\n        content = f.read().decode(\'utf-8\')\n        text, summary = split_example(filename, content)\n        if not text:\n          print(""Skipped: %s has no text."" % filename)\n          continue\n        if not summary:\n          print(""Skipped: %s has no summary."" % filename)\n          continue\n        if len(text) < len(summary):\n          print(""Skipped: the text of %s is shorter than its summary."" % filename)\n          continue\n        fout.write("" "".join(text) + ""\\t"" + "" "".join(summary) + ""\\n"")\n        count += 1\n        if count % print_every == 0:\n          print(count)\n          fout.flush()\n\ntrain_out.close()\nvalid_out.close()\ntest_out.close()\n'"
data/make_google_data.py,0,"b'""""""\nDownload and tokenize the Google Sentence Compression Data. This script will create the data file\n`sentences.txt` for use in the summarizer, and a directory `google-sentence-compression-data` to\nhold the downloaded raw files.\n\nOne may use shell commands to randomly split `sentences.txt` into training, validation, and testing\nsets. I use 80% training (sent.txt) and 20% validation (sent.val.txt).\n\nAbout this dataset: https://github.com/google-research-datasets/sentence-compression\n""""""\nimport gzip\nimport json\nimport unicodedata\nimport re\nimport os\nimport urllib.request\nfrom nltk import word_tokenize\n\n\nsplitter = re.compile(r\'(-)\')\ncontractions = {""\'s"", ""\'d"", ""\'ve"", ""\'ll"", ""\'m"", ""\'re""}\n\n\nprint_every = 1000  # print progress every 1000 sentences\ndata_path = os.path.dirname(os.path.abspath(__file__))\ncorpus_path = os.path.join(data_path, \'google-sentence-compression-data\')\nif not os.path.isdir(corpus_path):\n  os.mkdir(corpus_path)\n\n\ndef tokenize(text):\n  # de-accent and lower\n  text = \'\'.join(c for c in unicodedata.normalize(\'NFKD\', text) if unicodedata.category(c) != \'Mn\')\n  text = unicodedata.normalize(\'NFC\', text).lower()\n  # split hyphens\n  tokens = []\n  for token in word_tokenize(text):\n    if \'-\' in token and not \'--\' in token:\n      tokens.extend(t for t in splitter.split(token) if t)\n    else:\n      tokens.append(token)\n  # separate leading apostrophe from words e.g. ""\'apple""\n  new_tokens = []\n  for token in tokens:\n    if len(token) > 1 and token.startswith(""\'"") and ""\'\'"" not in token \\\n            and token not in contractions:\n      new_tokens.append(""\'"")\n      new_tokens.append(token[1:])\n    else:\n      new_tokens.append(token)\n  return \' \'.join(new_tokens)\n\n\ncount = 0\nwith open(os.path.join(data_path, \'sentences.txt\'), \'wt\') as fout:\n  for volume_id in range(1, 11):\n    filename = \'sent-comp.train%02d.json.gz\' % volume_id\n    file_path = os.path.join(corpus_path, filename)\n    if not os.path.isfile(file_path):\n      url = ""https://github.com/google-research-datasets/sentence-compression/raw/master/data/"" \\\n            + filename\n      print(""Downloading %s..."" % url)\n      urllib.request.urlretrieve(url, file_path)\n    print(""Processing %s..."" % filename)\n    with gzip.open(file_path, \'rt\', encoding=\'utf-8\') as fin:\n      lines = []\n      for line in fin:\n        line = line.strip()\n        if not line:\n          if lines:\n            obj = json.loads(\'\\n\'.join(lines))\n            original = obj[\'source_tree\'][\'sentence\']\n            summary = obj[\'headline\']\n            entry = \'%s\\t%s\' % (tokenize(original), tokenize(summary))\n            fout.write(entry + ""\\n"")\n            count += 1\n            if count % print_every == 0:\n              print(count)\n            lines = []\n        else:\n          lines.append(line)\n'"
