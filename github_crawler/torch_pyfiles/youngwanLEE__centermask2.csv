file_path,api_count,code
train_net.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# Modified by Youngwan Lee (ETRI), 2020. All Rights Reserved.\nimport logging\nimport os\nfrom collections import OrderedDict\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel\n\nimport detectron2.utils.comm as comm\nfrom detectron2.data import MetadataCatalog, build_detection_train_loader\nfrom detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, hooks, launch\nfrom detectron2.utils.events import EventStorage\nfrom detectron2.evaluation import (\n    CityscapesEvaluator,\n    COCOPanopticEvaluator,\n    DatasetEvaluators,\n    LVISEvaluator,\n    PascalVOCDetectionEvaluator,\n    SemSegEvaluator,\n    verify_results,\n)\nfrom centermask.evaluation import COCOEvaluator\nfrom detectron2.modeling import GeneralizedRCNNWithTTA\n\nfrom detectron2.data.dataset_mapper import DatasetMapper\nfrom centermask.config import get_cfg\nfrom centermask.checkpoint import AdetCheckpointer\n\n\nclass Trainer(DefaultTrainer):\n    """"""\n    This is the same Trainer except that we rewrite the\n    `build_train_loader` method.\n    """"""\n\n    def __init__(self, cfg):\n        """"""\n        Args:\n            cfg (CfgNode):\n        Use the custom checkpointer, which loads other backbone models\n        with matching heuristics.\n        """"""\n        # Assume these objects must be constructed in this order.\n        model = self.build_model(cfg)\n        optimizer = self.build_optimizer(cfg, model)\n        data_loader = self.build_train_loader(cfg)\n\n        # For training, wrap with DDP. But don\'t need this for inference.\n        if comm.get_world_size() > 1:\n            model = DistributedDataParallel(\n                model, device_ids=[comm.get_local_rank()], broadcast_buffers=False\n            )\n        super(DefaultTrainer, self).__init__(model, data_loader, optimizer)\n\n        self.scheduler = self.build_lr_scheduler(cfg, optimizer)\n        # Assume no other objects need to be checkpointed.\n        # We can later make it checkpoint the stateful hooks\n        self.checkpointer = AdetCheckpointer(\n            # Assume you want to save checkpoints together with logs/statistics\n            model,\n            cfg.OUTPUT_DIR,\n            optimizer=optimizer,\n            scheduler=self.scheduler,\n        )\n        self.start_iter = 0\n        self.max_iter = cfg.SOLVER.MAX_ITER\n        self.cfg = cfg\n\n        self.register_hooks(self.build_hooks())\n\n    def train_loop(self, start_iter: int, max_iter: int):\n        """"""\n        Args:\n            start_iter, max_iter (int): See docs above\n        """"""\n        logger = logging.getLogger(__name__)\n        logger.info(""Starting training from iteration {}"".format(start_iter))\n\n        self.iter = self.start_iter = start_iter\n        self.max_iter = max_iter\n\n        with EventStorage(start_iter) as self.storage:\n            self.before_train()\n            for self.iter in range(start_iter, max_iter):\n                self.before_step()\n                self.run_step()\n                self.after_step()\n            self.after_train()\n\n    def train(self):\n        """"""\n        Run training.\n\n        Returns:\n            OrderedDict of results, if evaluation is enabled. Otherwise None.\n        """"""\n        self.train_loop(self.start_iter, self.max_iter)\n        if hasattr(self, ""_last_eval_results"") and comm.is_main_process():\n            verify_results(self.cfg, self._last_eval_results)\n            return self._last_eval_results\n\n    @classmethod\n    def build_train_loader(cls, cfg):\n        """"""\n        Returns:\n            iterable\n\n        It calls :func:`detectron2.data.build_detection_train_loader` with a customized\n        DatasetMapper, which adds categorical labels as a semantic mask.\n        """"""\n        mapper = DatasetMapper(cfg, True)\n        return build_detection_train_loader(cfg, mapper)\n\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        """"""\n        Create evaluator(s) for a given dataset.\n        This uses the special metadata ""evaluator_type"" associated with each builtin dataset.\n        For your own dataset, you can simply create an evaluator manually in your\n        script and do not have to worry about the hacky if-else logic here.\n        """"""\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, ""inference"")\n        evaluator_list = []\n        evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n        if evaluator_type in [""sem_seg"", ""coco_panoptic_seg""]:\n            evaluator_list.append(\n                SemSegEvaluator(\n                    dataset_name,\n                    distributed=True,\n                    num_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES,\n                    ignore_label=cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,\n                    output_dir=output_folder,\n                )\n            )\n        if evaluator_type in [""coco"", ""coco_panoptic_seg""]:\n            evaluator_list.append(COCOEvaluator(dataset_name, cfg, True, output_folder))\n        if evaluator_type == ""coco_panoptic_seg"":\n            evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n        if evaluator_type == ""cityscapes"":\n            assert (\n                torch.cuda.device_count() >= comm.get_rank()\n            ), ""CityscapesEvaluator currently do not work with multiple machines.""\n            return CityscapesEvaluator(dataset_name)\n        if evaluator_type == ""pascal_voc"":\n            return PascalVOCDetectionEvaluator(dataset_name)\n        if evaluator_type == ""lvis"":\n            return LVISEvaluator(dataset_name, cfg, True, output_folder)\n        if len(evaluator_list) == 0:\n            raise NotImplementedError(\n                ""no Evaluator for the dataset {} with the type {}"".format(\n                    dataset_name, evaluator_type\n                )\n            )\n        if len(evaluator_list) == 1:\n            return evaluator_list[0]\n        return DatasetEvaluators(evaluator_list)\n\n    @classmethod\n    def test_with_TTA(cls, cfg, model):\n        logger = logging.getLogger(""detectron2.trainer"")\n        # In the end of training, run an evaluation with TTA\n        # Only support some R-CNN models.\n        logger.info(""Running inference with test-time augmentation ..."")\n        model = GeneralizedRCNNWithTTA(cfg, model)\n        evaluators = [\n            cls.build_evaluator(\n                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, ""inference_TTA"")\n            )\n            for name in cfg.DATASETS.TEST\n        ]\n        res = cls.test(cfg, model, evaluators)\n        res = OrderedDict({k + ""_TTA"": v for k, v in res.items()})\n        return res\n\n\ndef setup(args):\n    """"""\n    Create configs and perform basic setups.\n    """"""\n    cfg = get_cfg()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n    default_setup(cfg, args)\n    return cfg\n\n\ndef main(args):\n    cfg = setup(args)\n\n    if args.eval_only:\n        model = Trainer.build_model(cfg)\n        AdetCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n            cfg.MODEL.WEIGHTS, resume=args.resume\n        )\n        evaluators = [\n            Trainer.build_evaluator(cfg, name)\n            for name in cfg.DATASETS.TEST\n        ]\n        res = Trainer.test(cfg, model, evaluators)\n        if comm.is_main_process():\n            verify_results(cfg, res)\n        if cfg.TEST.AUG.ENABLED:\n            res.update(Trainer.test_with_TTA(cfg, model))\n        return res\n\n    """"""\n    If you\'d like to do anything fancier than the standard training logic,\n    consider writing your own training loop or subclassing the trainer.\n    """"""\n    trainer = Trainer(cfg)\n    trainer.resume_or_load(resume=args.resume)\n    if cfg.TEST.AUG.ENABLED:\n        trainer.register_hooks(\n            [hooks.EvalHook(0, lambda: trainer.test_with_TTA(cfg, trainer.model))]\n        )\n    return trainer.train()\n\n\nif __name__ == ""__main__"":\n    args = default_argument_parser().parse_args()\n    print(""Command Line Args:"", args)\n    launch(\n        main,\n        args.num_gpus,\n        num_machines=args.num_machines,\n        machine_rank=args.machine_rank,\n        dist_url=args.dist_url,\n        args=(args,),\n    )\n'"
centermask/__init__.py,0,"b'from centermask import modeling\n\n__version__ = ""0.1""\n'"
datasets/prepare_panoptic_fpn.py,0,"b'# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\nimport time\nimport functools\nimport json\nimport multiprocessing as mp\nimport numpy as np\nimport os\nfrom PIL import Image\n\nfrom detectron2.data.datasets.builtin_meta import COCO_CATEGORIES\nfrom fvcore.common.download import download\n\nfrom panopticapi.utils import rgb2id\n\n\ndef _process_panoptic_to_semantic(input_panoptic, output_semantic, segments, id_map):\n    panoptic = np.asarray(Image.open(input_panoptic), dtype=np.uint32)\n    panoptic = rgb2id(panoptic)\n    output = np.zeros_like(panoptic, dtype=np.uint8) + 255\n    for seg in segments:\n        cat_id = seg[""category_id""]\n        new_cat_id = id_map[cat_id]\n        output[panoptic == seg[""id""]] = new_cat_id\n    Image.fromarray(output).save(output_semantic)\n\n\ndef separate_coco_semantic_from_panoptic(panoptic_json, panoptic_root, sem_seg_root, categories):\n    """"""\n    Create semantic segmentation annotations from panoptic segmentation\n    annotations, to be used by PanopticFPN.\n\n    It maps all thing categories to class 0, and maps all unlabeled pixels to class 255.\n    It maps all stuff categories to contiguous ids starting from 1.\n\n    Args:\n        panoptic_json (str): path to the panoptic json file, in COCO\'s format.\n        panoptic_root (str): a directory with panoptic annotation files, in COCO\'s format.\n        sem_seg_root (str): a directory to output semantic annotation files\n        categories (list[dict]): category metadata. Each dict needs to have:\n            ""id"": corresponds to the ""category_id"" in the json annotations\n            ""isthing"": 0 or 1\n    """"""\n    os.makedirs(sem_seg_root, exist_ok=True)\n\n    stuff_ids = [k[""id""] for k in categories if k[""isthing""] == 0]\n    thing_ids = [k[""id""] for k in categories if k[""isthing""] == 1]\n    id_map = {}  # map from category id to id in the output semantic annotation\n    assert len(stuff_ids) <= 254\n    for i, stuff_id in enumerate(stuff_ids):\n        id_map[stuff_id] = i + 1\n    for thing_id in thing_ids:\n        id_map[thing_id] = 0\n    id_map[0] = 255\n\n    with open(panoptic_json) as f:\n        obj = json.load(f)\n\n    pool = mp.Pool(processes=max(mp.cpu_count() // 2, 4))\n\n    def iter_annotations():\n        for anno in obj[""annotations""]:\n            file_name = anno[""file_name""]\n            segments = anno[""segments_info""]\n            input = os.path.join(panoptic_root, file_name)\n            output = os.path.join(sem_seg_root, file_name)\n            yield input, output, segments\n\n    print(""Start writing to {} ..."".format(sem_seg_root))\n    start = time.time()\n    pool.starmap(\n        functools.partial(_process_panoptic_to_semantic, id_map=id_map),\n        iter_annotations(),\n        chunksize=100,\n    )\n    print(""Finished. time: {:.2f}s"".format(time.time() - start))\n\n\nif __name__ == ""__main__"":\n    dataset_dir = os.path.join(os.path.dirname(__file__), ""coco"")\n    for s in [""val2017"", ""train2017""]:\n        separate_coco_semantic_from_panoptic(\n            os.path.join(dataset_dir, ""annotations/panoptic_{}.json"".format(s)),\n            os.path.join(dataset_dir, ""panoptic_{}"".format(s)),\n            os.path.join(dataset_dir, ""panoptic_stuff_{}"".format(s)),\n            COCO_CATEGORIES,\n        )\n\n    # Prepare val2017_100 for quick testing:\n\n    dest_dir = os.path.join(dataset_dir, ""annotations/"")\n    URL_PREFIX = ""https://dl.fbaipublicfiles.com/detectron2/""\n    download(URL_PREFIX + ""annotations/coco/panoptic_val2017_100.json"", dest_dir)\n    with open(os.path.join(dest_dir, ""panoptic_val2017_100.json"")) as f:\n        obj = json.load(f)\n\n    def link_val100(dir_full, dir_100):\n        print(""Creating "" + dir_100 + "" ..."")\n        os.makedirs(dir_100, exist_ok=True)\n        for img in obj[""images""]:\n            basename = os.path.splitext(img[""file_name""])[0]\n            src = os.path.join(dir_full, basename + "".png"")\n            dst = os.path.join(dir_100, basename + "".png"")\n            src = os.path.relpath(src, start=dir_100)\n            os.symlink(src, dst)\n\n    link_val100(\n        os.path.join(dataset_dir, ""panoptic_val2017""),\n        os.path.join(dataset_dir, ""panoptic_val2017_100""),\n    )\n\n    link_val100(\n        os.path.join(dataset_dir, ""panoptic_stuff_val2017""),\n        os.path.join(dataset_dir, ""panoptic_stuff_val2017_100""),\n    )\n'"
demo/demo.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport argparse\nimport glob\nimport multiprocessing as mp\nimport os\nimport time\nimport cv2\nimport tqdm\nimport sys\n\n#TODO : this is a temporary expedient\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')))\n\nfrom detectron2.data.detection_utils import read_image\nfrom detectron2.utils.logger import setup_logger\n\nfrom predictor import VisualizationDemo\nfrom centermask.config import get_cfg\n\n# constants\nWINDOW_NAME = ""COCO detections""\n\n\ndef setup_cfg(args):\n    # load config from file and command-line arguments\n    cfg = get_cfg()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    # Set score_threshold for builtin models\n    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = args.confidence_threshold\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = args.confidence_threshold\n    cfg.MODEL.FCOS.INFERENCE_TH_TEST = args.confidence_threshold\n    cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = args.confidence_threshold\n    cfg.freeze()\n    return cfg\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=""Detectron2 Demo"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/quick_schedules/e2e_mask_rcnn_R_50_FPN_inference_acc_test.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n    )\n    parser.add_argument(""--webcam"", action=""store_true"", help=""Take inputs from webcam."")\n    parser.add_argument(""--video-input"", help=""Path to video file."")\n    parser.add_argument(""--input"", nargs=""+"", help=""A list of space separated input images"")\n    parser.add_argument(\n        ""--output"",\n        help=""A file or directory to save output visualizations. ""\n        ""If not given, will show output in an OpenCV window."",\n    )\n\n    parser.add_argument(\n        ""--confidence-threshold"",\n        type=float,\n        default=0.5,\n        help=""Minimum score for instance predictions to be shown"",\n    )\n    parser.add_argument(\n        ""--opts"",\n        help=""Modify config options using the command-line \'KEY VALUE\' pairs"",\n        default=[],\n        nargs=argparse.REMAINDER,\n    )\n    return parser\n\n\nif __name__ == ""__main__"":\n    mp.set_start_method(""spawn"", force=True)\n    args = get_parser().parse_args()\n    logger = setup_logger()\n    logger.info(""Arguments: "" + str(args))\n\n    cfg = setup_cfg(args)\n\n    demo = VisualizationDemo(cfg)\n\n    if args.input:\n        if os.path.isdir(args.input[0]):\n            args.input = [os.path.join(args.input[0], fname) for fname in os.listdir(args.input[0])]\n        elif len(args.input) == 1:\n            args.input = glob.glob(os.path.expanduser(args.input[0]))\n            assert args.input, ""The input path(s) was not found""\n        for path in tqdm.tqdm(args.input, disable=not args.output):\n            # use PIL, to be consistent with evaluation\n            img = read_image(path, format=""BGR"")\n            start_time = time.time()\n            predictions, visualized_output = demo.run_on_image(img)\n            logger.info(\n                ""{}: detected {} instances in {:.2f}s"".format(\n                    path, len(predictions[""instances""]), time.time() - start_time\n                )\n            )\n\n            if args.output:\n                if os.path.isdir(args.output):\n                    assert os.path.isdir(args.output), args.output\n                    out_filename = os.path.join(args.output, os.path.basename(path))\n                else:\n                    assert len(args.input) == 1, ""Please specify a directory with args.output""\n                    out_filename = args.output\n                visualized_output.save(out_filename)\n            else:\n                cv2.imshow(WINDOW_NAME, visualized_output.get_image()[:, :, ::-1])\n                if cv2.waitKey(0) == 27:\n                    break  # esc to quit\n    elif args.webcam:\n        assert args.input is None, ""Cannot have both --input and --webcam!""\n        cam = cv2.VideoCapture(0)\n        for vis in tqdm.tqdm(demo.run_on_video(cam)):\n            cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n            cv2.imshow(WINDOW_NAME, vis)\n            if cv2.waitKey(1) == 27:\n                break  # esc to quit\n        cv2.destroyAllWindows()\n    elif args.video_input:\n        video = cv2.VideoCapture(args.video_input)\n        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        frames_per_second = video.get(cv2.CAP_PROP_FPS)\n        num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n        basename = os.path.basename(args.video_input)\n\n        if args.output:\n            if os.path.isdir(args.output):\n                output_fname = os.path.join(args.output, basename)\n                output_fname = os.path.splitext(output_fname)[0] + "".mkv""\n            else:\n                output_fname = args.output\n            assert not os.path.isfile(output_fname), output_fname\n            output_file = cv2.VideoWriter(\n                filename=output_fname,\n                # some installation of opencv may not support x264 (due to its license),\n                # you can try other format (e.g. MPEG)\n                fourcc=cv2.VideoWriter_fourcc(*""x264""),\n                fps=float(frames_per_second),\n                frameSize=(width, height),\n                isColor=True,\n            )\n        assert os.path.isfile(args.video_input)\n        for vis_frame in tqdm.tqdm(demo.run_on_video(video), total=num_frames):\n            if args.output:\n                output_file.write(vis_frame)\n            else:\n                cv2.namedWindow(basename, cv2.WINDOW_NORMAL)\n                cv2.imshow(basename, vis_frame)\n                if cv2.waitKey(1) == 27:\n                    break  # esc to quit\n        video.release()\n        if args.output:\n            output_file.release()\n        else:\n            cv2.destroyAllWindows()\n'"
demo/predictor.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport numpy as np\nimport atexit\nimport bisect\nimport multiprocessing as mp\nfrom collections import deque\nimport cv2\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.engine.defaults import DefaultPredictor\nfrom detectron2.utils.video_visualizer import VideoVisualizer\nfrom detectron2.utils.visualizer import ColorMode, Visualizer\n\n\nclass VisualizationDemo(object):\n    def __init__(self, cfg, instance_mode=ColorMode.IMAGE, parallel=False):\n        """"""\n        Args:\n            cfg (CfgNode):\n            instance_mode (ColorMode):\n            parallel (bool): whether to run the model in different processes from visualization.\n                Useful since the visualization logic can be slow.\n        """"""\n        self.metadata = MetadataCatalog.get(\n            cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else ""__unused""\n        )\n        self.cpu_device = torch.device(""cpu"")\n        self.instance_mode = instance_mode\n\n        self.parallel = parallel\n        if parallel:\n            num_gpu = torch.cuda.device_count()\n            self.predictor = AsyncPredictor(cfg, num_gpus=num_gpu)\n        else:\n            self.predictor = DefaultPredictor(cfg)\n\n    def run_on_image(self, image):\n        """"""\n        Args:\n            image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n                This is the format used by OpenCV.\n\n        Returns:\n            predictions (dict): the output of the model.\n            vis_output (VisImage): the visualized image output.\n        """"""\n        vis_output = None\n        predictions = self.predictor(image)\n        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n        image = image[:, :, ::-1]\n        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n        if ""inst"" in predictions:\n            visualizer.vis_inst(predictions[""inst""])\n        if ""bases"" in predictions:\n            self.vis_bases(predictions[""bases""])\n        if ""panoptic_seg"" in predictions:\n            panoptic_seg, segments_info = predictions[""panoptic_seg""]\n            vis_output = visualizer.draw_panoptic_seg_predictions(\n                panoptic_seg.to(self.cpu_device), segments_info\n            )\n        else:\n            if ""sem_seg"" in predictions:\n                vis_output = visualizer.draw_sem_seg(\n                    predictions[""sem_seg""].argmax(dim=0).to(self.cpu_device))\n            if ""instances"" in predictions:\n                instances = predictions[""instances""].to(self.cpu_device)\n                vis_output = visualizer.draw_instance_predictions(predictions=instances)\n\n        return predictions, vis_output\n\n    def _frame_from_video(self, video):\n        while video.isOpened():\n            success, frame = video.read()\n            if success:\n                yield frame\n            else:\n                break\n\n    def vis_bases(self, bases):\n        basis_colors = [[2, 200, 255], [107, 220, 255], [30, 200, 255], [60, 220, 255]]\n        bases = bases[0].squeeze()\n        bases = (bases / 8).tanh().cpu().numpy()\n        num_bases = len(bases)\n        fig, axes = plt.subplots(nrows=num_bases // 2, ncols=2)\n        for i, basis in enumerate(bases):\n            basis = (basis + 1) / 2\n            basis = basis / basis.max()\n            basis_viz = np.zeros((basis.shape[0], basis.shape[1], 3), dtype=np.uint8)\n            basis_viz[:, :, 0] = basis_colors[i][0]\n            basis_viz[:, :, 1] = basis_colors[i][1]\n            basis_viz[:, :, 2] = np.uint8(basis * 255)\n            basis_viz = cv2.cvtColor(basis_viz, cv2.COLOR_HSV2RGB)\n            axes[i // 2][i % 2].imshow(basis_viz)\n        plt.show()\n\n    def run_on_video(self, video):\n        """"""\n        Visualizes predictions on frames of the input video.\n\n        Args:\n            video (cv2.VideoCapture): a :class:`VideoCapture` object, whose source can be\n                either a webcam or a video file.\n\n        Yields:\n            ndarray: BGR visualizations of each video frame.\n        """"""\n        video_visualizer = VideoVisualizer(self.metadata, self.instance_mode)\n\n        def process_predictions(frame, predictions):\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n            if ""panoptic_seg"" in predictions:\n                panoptic_seg, segments_info = predictions[""panoptic_seg""]\n                vis_frame = video_visualizer.draw_panoptic_seg_predictions(\n                    frame, panoptic_seg.to(self.cpu_device), segments_info\n                )\n            elif ""instances"" in predictions:\n                predictions = predictions[""instances""].to(self.cpu_device)\n                vis_frame = video_visualizer.draw_instance_predictions(frame, predictions)\n            elif ""sem_seg"" in predictions:\n                vis_frame = video_visualizer.draw_sem_seg(\n                    frame, predictions[""sem_seg""].argmax(dim=0).to(self.cpu_device)\n                )\n\n            # Converts Matplotlib RGB format to OpenCV BGR format\n            vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)\n            return vis_frame\n\n        frame_gen = self._frame_from_video(video)\n        if self.parallel:\n            buffer_size = self.predictor.default_buffer_size\n\n            frame_data = deque()\n\n            for cnt, frame in enumerate(frame_gen):\n                frame_data.append(frame)\n                self.predictor.put(frame)\n\n                if cnt >= buffer_size:\n                    frame = frame_data.popleft()\n                    predictions = self.predictor.get()\n                    yield process_predictions(frame, predictions)\n\n            while len(frame_data):\n                frame = frame_data.popleft()\n                predictions = self.predictor.get()\n                yield process_predictions(frame, predictions)\n        else:\n            for frame in frame_gen:\n                yield process_predictions(frame, self.predictor(frame))\n\n\nclass AsyncPredictor:\n    """"""\n    A predictor that runs the model asynchronously, possibly on >1 GPUs.\n    Because rendering the visualization takes considerably amount of time,\n    this helps improve throughput when rendering videos.\n    """"""\n\n    class _StopToken:\n        pass\n\n    class _PredictWorker(mp.Process):\n        def __init__(self, cfg, task_queue, result_queue):\n            self.cfg = cfg\n            self.task_queue = task_queue\n            self.result_queue = result_queue\n            super().__init__()\n\n        def run(self):\n            predictor = DefaultPredictor(self.cfg)\n\n            while True:\n                task = self.task_queue.get()\n                if isinstance(task, AsyncPredictor._StopToken):\n                    break\n                idx, data = task\n                result = predictor(data)\n                self.result_queue.put((idx, result))\n\n    def __init__(self, cfg, num_gpus: int = 1):\n        """"""\n        Args:\n            cfg (CfgNode):\n            num_gpus (int): if 0, will run on CPU\n        """"""\n        num_workers = max(num_gpus, 1)\n        self.task_queue = mp.Queue(maxsize=num_workers * 3)\n        self.result_queue = mp.Queue(maxsize=num_workers * 3)\n        self.procs = []\n        for gpuid in range(max(num_gpus, 1)):\n            cfg = cfg.clone()\n            cfg.defrost()\n            cfg.MODEL.DEVICE = ""cuda:{}"".format(gpuid) if num_gpus > 0 else ""cpu""\n            self.procs.append(\n                AsyncPredictor._PredictWorker(cfg, self.task_queue, self.result_queue)\n            )\n\n        self.put_idx = 0\n        self.get_idx = 0\n        self.result_rank = []\n        self.result_data = []\n\n        for p in self.procs:\n            p.start()\n        atexit.register(self.shutdown)\n\n    def put(self, image):\n        self.put_idx += 1\n        self.task_queue.put((self.put_idx, image))\n\n    def get(self):\n        self.get_idx += 1  # the index needed for this request\n        if len(self.result_rank) and self.result_rank[0] == self.get_idx:\n            res = self.result_data[0]\n            del self.result_data[0], self.result_rank[0]\n            return res\n\n        while True:\n            # make sure the results are returned in the correct order\n            idx, res = self.result_queue.get()\n            if idx == self.get_idx:\n                return res\n            insert = bisect.bisect(self.result_rank, idx)\n            self.result_rank.insert(insert, idx)\n            self.result_data.insert(insert, res)\n\n    def __len__(self):\n        return self.put_idx - self.get_idx\n\n    def __call__(self, image):\n        self.put(image)\n        return self.get()\n\n    def shutdown(self):\n        for _ in self.procs:\n            self.task_queue.put(AsyncPredictor._StopToken())\n\n    @property\n    def default_buffer_size(self):\n        return len(self.procs) * 5\n'"
centermask/checkpoint/__init__.py,0,"b'from .adet_checkpoint import AdetCheckpointer\n\n__all__ = [""AdetCheckpointer""]\n'"
centermask/checkpoint/adet_checkpoint.py,0,"b'import pickle\nfrom fvcore.common.file_io import PathManager\nfrom detectron2.checkpoint import DetectionCheckpointer\n\n\nclass AdetCheckpointer(DetectionCheckpointer):\n    def _load_file(self, filename):\n        if filename.endswith("".pkl""):\n            with PathManager.open(filename, ""rb"") as f:\n                data = pickle.load(f, encoding=""latin1"")\n            if ""model"" in data and ""__author__"" in data:\n                # file is in Detectron2 model zoo format\n                self.logger.info(""Reading a file from \'{}\'"".format(data[""__author__""]))\n                return data\n            else:\n                # assume file is from Caffe2 / Detectron1 model zoo\n                if ""blobs"" in data:\n                    # Detection models have ""blobs"", but ImageNet models don\'t\n                    data = data[""blobs""]\n                data = {k: v for k, v in data.items() if not k.endswith(""_momentum"")}\n                return {""model"": data, ""__author__"": ""Caffe2"", ""matching_heuristics"": True}\n\n        loaded = super()._load_file(filename)  # load native pth checkpoint\n        if ""model"" not in loaded:\n            loaded = {""model"": loaded}\n        if ""lpf"" in filename:\n            loaded[""matching_heuristics""] = True\n        return loaded\n'"
centermask/config/__init__.py,0,"b'from .config import get_cfg\n\n__all__ = [\n    ""get_cfg"",\n]\n'"
centermask/config/config.py,0,"b'from detectron2.config import CfgNode\n\n\ndef get_cfg() -> CfgNode:\n    """"""\n    Get a copy of the default config.\n\n    Returns:\n        a detectron2 CfgNode instance.\n    """"""\n    from .defaults import _C\n\n    return _C.clone()\n'"
centermask/config/defaults.py,0,"b'# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\nfrom detectron2.config.defaults import _C\nfrom detectron2.config import CfgNode as CN\n\n\n# ---------------------------------------------------------------------------- #\n# Additional Configs\n# ---------------------------------------------------------------------------- #\n_C.MODEL.MOBILENET = False\n\n# ---------------------------------------------------------------------------- #\n# FCOS Head\n# ---------------------------------------------------------------------------- #\n_C.MODEL.FCOS = CN()\n\n# This is the number of foreground classes.\n_C.MODEL.FCOS.NUM_CLASSES = 80\n_C.MODEL.FCOS.IN_FEATURES = [""p3"", ""p4"", ""p5"", ""p6"", ""p7""]\n_C.MODEL.FCOS.FPN_STRIDES = [8, 16, 32, 64, 128]\n_C.MODEL.FCOS.PRIOR_PROB = 0.01\n_C.MODEL.FCOS.INFERENCE_TH_TRAIN = 0.05\n_C.MODEL.FCOS.INFERENCE_TH_TEST = 0.05\n_C.MODEL.FCOS.NMS_TH = 0.6\n_C.MODEL.FCOS.PRE_NMS_TOPK_TRAIN = 1000\n_C.MODEL.FCOS.PRE_NMS_TOPK_TEST = 1000\n_C.MODEL.FCOS.POST_NMS_TOPK_TRAIN = 100\n_C.MODEL.FCOS.POST_NMS_TOPK_TEST = 100\n_C.MODEL.FCOS.TOP_LEVELS = 2\n_C.MODEL.FCOS.NORM = ""GN""  # Support GN or none\n_C.MODEL.FCOS.USE_SCALE = True\n\n# Multiply centerness before threshold\n# This will affect the final performance by about 0.05 AP but save some time\n_C.MODEL.FCOS.THRESH_WITH_CTR = False\n\n# Focal loss parameters\n_C.MODEL.FCOS.LOSS_ALPHA = 0.25\n_C.MODEL.FCOS.LOSS_GAMMA = 2.0\n_C.MODEL.FCOS.SIZES_OF_INTEREST = [64, 128, 256, 512]\n_C.MODEL.FCOS.USE_RELU = True\n_C.MODEL.FCOS.USE_DEFORMABLE = False\n\n# the number of convolutions used in the cls and bbox tower\n_C.MODEL.FCOS.NUM_CLS_CONVS = 4\n_C.MODEL.FCOS.NUM_BOX_CONVS = 4\n_C.MODEL.FCOS.NUM_SHARE_CONVS = 0\n_C.MODEL.FCOS.CENTER_SAMPLE = True\n_C.MODEL.FCOS.POS_RADIUS = 1.5\n_C.MODEL.FCOS.LOC_LOSS_TYPE = \'giou\'\n\n\n# ---------------------------------------------------------------------------- #\n# VoVNet backbone\n# ---------------------------------------------------------------------------- #\n\n_C.MODEL.VOVNET = CN()\n\n_C.MODEL.VOVNET.CONV_BODY = ""V-39-eSE""\n_C.MODEL.VOVNET.OUT_FEATURES = [""stage2"", ""stage3"", ""stage4"", ""stage5""]\n# Options: FrozenBN, GN, ""SyncBN"", ""BN""\n_C.MODEL.VOVNET.NORM = ""FrozenBN""\n_C.MODEL.VOVNET.OUT_CHANNELS = 256\n_C.MODEL.VOVNET.BACKBONE_OUT_CHANNELS = 256\n_C.MODEL.VOVNET.STAGE_WITH_DCN = (False, False, False, False)\n_C.MODEL.VOVNET.WITH_MODULATED_DCN = False\n_C.MODEL.VOVNET.DEFORMABLE_GROUPS = 1\n\n\n# ---------------------------------------------------------------------------- #\n# CenterMask\n# ---------------------------------------------------------------------------- #\n_C.MODEL.ROI_MASK_HEAD.ASSIGN_CRITERION = ""area""\n_C.MODEL.MASKIOU_ON = False\n_C.MODEL.MASKIOU_LOSS_WEIGHT = 1.0\n\n_C.MODEL.ROI_MASKIOU_HEAD = CN()\n_C.MODEL.ROI_MASKIOU_HEAD.NAME = ""MaskIoUHead""\n_C.MODEL.ROI_MASKIOU_HEAD.CONV_DIM = 256\n_C.MODEL.ROI_MASKIOU_HEAD.NUM_CONV = 4\n\n\n# ---------------------------------------------------------------------------- #\n# Keypoint Head\n# ---------------------------------------------------------------------------- #\n_C.MODEL.ROI_KEYPOINT_HEAD.IN_FEATURES = [""p2"", ""p3"", ""p4"", ""p5""]\n_C.MODEL.ROI_KEYPOINT_HEAD.ASSIGN_CRITERION = ""ratio""'"
centermask/evaluation/__init__.py,0,b'from .coco_evaluation import COCOEvaluator'
centermask/evaluation/coco_evaluation.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# Modified by Sangrok Lee and Youngwan Lee (ETRI), 2020. All Rights Reserved.\nimport contextlib\nimport copy\nimport io\nimport itertools\nimport json\nimport logging\nimport numpy as np\nimport os\nimport pickle\nfrom collections import OrderedDict\nimport pycocotools.mask as mask_util\nimport torch\nfrom fvcore.common.file_io import PathManager\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom tabulate import tabulate\n\nimport detectron2.utils.comm as comm\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.data.datasets.coco import convert_to_coco_json\nfrom detectron2.structures import Boxes, BoxMode, pairwise_iou\nfrom detectron2.utils.logger import create_small_table\n\nfrom detectron2.evaluation.evaluator import DatasetEvaluator\n\n\nclass COCOEvaluator(DatasetEvaluator):\n    """"""\n    Evaluate object proposal, instance detection/segmentation, keypoint detection\n    outputs using COCO\'s metrics and APIs.\n    """"""\n\n    def __init__(self, dataset_name, cfg, distributed, output_dir=None):\n        """"""\n        Args:\n            dataset_name (str): name of the dataset to be evaluated.\n                It must have either the following corresponding metadata:\n                    ""json_file"": the path to the COCO format annotation\n                Or it must be in detectron2\'s standard dataset format\n                so it can be converted to COCO format automatically.\n            cfg (CfgNode): config instance\n            distributed (True): if True, will collect results from all ranks for evaluation.\n                Otherwise, will evaluate the results in the current process.\n            output_dir (str): optional, an output directory to dump all\n                results predicted on the dataset. The dump contains two files:\n                1. ""instance_predictions.pth"" a file in torch serialization\n                   format that contains all the raw original predictions.\n                2. ""coco_instances_results.json"" a json file in COCO\'s result\n                   format.\n        """"""\n        self._tasks = self._tasks_from_config(cfg)\n        self._distributed = distributed\n        self._output_dir = output_dir\n\n        self._cpu_device = torch.device(""cpu"")\n        self._logger = logging.getLogger(__name__)\n\n        self._metadata = MetadataCatalog.get(dataset_name)\n        if not hasattr(self._metadata, ""json_file""):\n            self._logger.warning(\n                f""json_file was not found in MetaDataCatalog for \'{dataset_name}\'.""\n                "" Trying to convert it to COCO format ...""\n            )\n\n            cache_path = os.path.join(output_dir, f""{dataset_name}_coco_format.json"")\n            self._metadata.json_file = cache_path\n            convert_to_coco_json(dataset_name, cache_path)\n\n        json_file = PathManager.get_local_path(self._metadata.json_file)\n        with contextlib.redirect_stdout(io.StringIO()):\n            self._coco_api = COCO(json_file)\n\n        self._kpt_oks_sigmas = cfg.TEST.KEYPOINT_OKS_SIGMAS\n        # Test set json files do not contain annotations (evaluation must be\n        # performed using the COCO evaluation server).\n        self._do_evaluation = ""annotations"" in self._coco_api.dataset\n\n    def reset(self):\n        self._predictions = []\n\n    def _tasks_from_config(self, cfg):\n        """"""\n        Returns:\n            tuple[str]: tasks that can be evaluated under the given configuration.\n        """"""\n        tasks = (""bbox"",)\n        if cfg.MODEL.MASK_ON:\n            tasks = tasks + (""segm"",)\n        if cfg.MODEL.KEYPOINT_ON:\n            tasks = tasks + (""keypoints"",)\n        return tasks\n\n    def process(self, inputs, outputs):\n        """"""\n        Args:\n            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).\n                It is a list of dict. Each dict corresponds to an image and\n                contains keys like ""height"", ""width"", ""file_name"", ""image_id"".\n            outputs: the outputs of a COCO model. It is a list of dicts with key\n                ""instances"" that contains :class:`Instances`.\n        """"""\n        for input, output in zip(inputs, outputs):\n            prediction = {""image_id"": input[""image_id""]}\n\n            # TODO this is ugly\n            if ""instances"" in output:\n                instances = output[""instances""].to(self._cpu_device)\n                prediction[""instances""] = instances_to_coco_json(instances, input[""image_id""])\n            if ""proposals"" in output:\n                prediction[""proposals""] = output[""proposals""].to(self._cpu_device)\n            self._predictions.append(prediction)\n\n    def evaluate(self):\n        if self._distributed:\n            comm.synchronize()\n            predictions = comm.gather(self._predictions, dst=0)\n            predictions = list(itertools.chain(*predictions))\n\n            if not comm.is_main_process():\n                return {}\n        else:\n            predictions = self._predictions\n\n        if len(predictions) == 0:\n            self._logger.warning(""[COCOEvaluator] Did not receive valid predictions."")\n            return {}\n\n        if self._output_dir:\n            PathManager.mkdirs(self._output_dir)\n            file_path = os.path.join(self._output_dir, ""instances_predictions.pth"")\n            with PathManager.open(file_path, ""wb"") as f:\n                torch.save(predictions, f)\n\n        self._results = OrderedDict()\n        if ""proposals"" in predictions[0]:\n            self._eval_box_proposals(predictions)\n        if ""instances"" in predictions[0]:\n            self._eval_predictions(set(self._tasks), predictions)\n        # Copy so the caller can do whatever with results\n        return copy.deepcopy(self._results)\n\n    def _eval_predictions(self, tasks, predictions):\n        """"""\n        Evaluate predictions on the given tasks.\n        Fill self._results with the metrics of the tasks.\n        """"""\n        self._logger.info(""Preparing results for COCO format ..."")\n        coco_results = list(itertools.chain(*[x[""instances""] for x in predictions]))\n\n        # unmap the category ids for COCO\n        if hasattr(self._metadata, ""thing_dataset_id_to_contiguous_id""):\n            reverse_id_mapping = {\n                v: k for k, v in self._metadata.thing_dataset_id_to_contiguous_id.items()\n            }\n            for result in coco_results:\n                category_id = result[""category_id""]\n                assert (\n                    category_id in reverse_id_mapping\n                ), ""A prediction has category_id={}, which is not available in the dataset."".format(\n                    category_id\n                )\n                result[""category_id""] = reverse_id_mapping[category_id]\n\n        if self._output_dir:\n            file_path = os.path.join(self._output_dir, ""coco_instances_results.json"")\n            self._logger.info(""Saving results to {}"".format(file_path))\n            with PathManager.open(file_path, ""w"") as f:\n                f.write(json.dumps(coco_results))\n                f.flush()\n\n        if not self._do_evaluation:\n            self._logger.info(""Annotations are not available for evaluation."")\n            return\n\n        self._logger.info(""Evaluating predictions ..."")\n        for task in sorted(tasks):\n            coco_eval = (\n                _evaluate_predictions_on_coco(\n                    self._coco_api, coco_results, task, kpt_oks_sigmas=self._kpt_oks_sigmas\n                )\n                if len(coco_results) > 0\n                else None  # cocoapi does not handle empty results very well\n            )\n\n            res = self._derive_coco_results(\n                coco_eval, task, class_names=self._metadata.get(""thing_classes"")\n            )\n            self._results[task] = res\n\n    def _eval_box_proposals(self, predictions):\n        """"""\n        Evaluate the box proposals in predictions.\n        Fill self._results with the metrics for ""box_proposals"" task.\n        """"""\n        if self._output_dir:\n            # Saving generated box proposals to file.\n            # Predicted box_proposals are in XYXY_ABS mode.\n            bbox_mode = BoxMode.XYXY_ABS.value\n            ids, boxes, objectness_logits = [], [], []\n            for prediction in predictions:\n                ids.append(prediction[""image_id""])\n                boxes.append(prediction[""proposals""].proposal_boxes.tensor.numpy())\n                objectness_logits.append(prediction[""proposals""].objectness_logits.numpy())\n\n            proposal_data = {\n                ""boxes"": boxes,\n                ""objectness_logits"": objectness_logits,\n                ""ids"": ids,\n                ""bbox_mode"": bbox_mode,\n            }\n            with PathManager.open(os.path.join(self._output_dir, ""box_proposals.pkl""), ""wb"") as f:\n                pickle.dump(proposal_data, f)\n\n        if not self._do_evaluation:\n            self._logger.info(""Annotations are not available for evaluation."")\n            return\n\n        self._logger.info(""Evaluating bbox proposals ..."")\n        res = {}\n        areas = {""all"": """", ""small"": ""s"", ""medium"": ""m"", ""large"": ""l""}\n        for limit in [100, 1000]:\n            for area, suffix in areas.items():\n                stats = _evaluate_box_proposals(predictions, self._coco_api, area=area, limit=limit)\n                key = ""AR{}@{:d}"".format(suffix, limit)\n                res[key] = float(stats[""ar""].item() * 100)\n        self._logger.info(""Proposal metrics: \\n"" + create_small_table(res))\n        self._results[""box_proposals""] = res\n\n    def _derive_coco_results(self, coco_eval, iou_type, class_names=None):\n        """"""\n        Derive the desired score numbers from summarized COCOeval.\n        Args:\n            coco_eval (None or COCOEval): None represents no predictions from model.\n            iou_type (str):\n            class_names (None or list[str]): if provided, will use it to predict\n                per-category AP.\n        Returns:\n            a dict of {metric name: score}\n        """"""\n\n        metrics = {\n            ""bbox"": [""AP"", ""AP50"", ""AP75"", ""APs"", ""APm"", ""APl""],\n            ""segm"": [""AP"", ""AP50"", ""AP75"", ""APs"", ""APm"", ""APl""],\n            ""keypoints"": [""AP"", ""AP50"", ""AP75"", ""APm"", ""APl""],\n        }[iou_type]\n\n        if coco_eval is None:\n            self._logger.warn(""No predictions from the model!"")\n            return {metric: float(""nan"") for metric in metrics}\n\n        # the standard metrics\n        results = {\n            metric: float(coco_eval.stats[idx] * 100 if coco_eval.stats[idx] >= 0 else ""nan"")\n            for idx, metric in enumerate(metrics)\n        }\n        self._logger.info(\n            ""Evaluation results for {}: \\n"".format(iou_type) + create_small_table(results)\n        )\n        if not np.isfinite(sum(results.values())):\n            self._logger.info(""Note that some metrics cannot be computed."")\n\n        if class_names is None or len(class_names) <= 1:\n            return results\n        # Compute per-category AP\n        # from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa\n        precisions = coco_eval.eval[""precision""]\n        # precision has dims (iou, recall, cls, area range, max dets)\n        assert len(class_names) == precisions.shape[2]\n\n        results_per_category = []\n        for idx, name in enumerate(class_names):\n            # area range index 0: all area ranges\n            # max dets index -1: typically 100 per image\n            precision = precisions[:, :, idx, 0, -1]\n            precision = precision[precision > -1]\n            ap = np.mean(precision) if precision.size else float(""nan"")\n            results_per_category.append((""{}"".format(name), float(ap * 100)))\n\n        # tabulate it\n        N_COLS = min(6, len(results_per_category) * 2)\n        results_flatten = list(itertools.chain(*results_per_category))\n        results_2d = itertools.zip_longest(*[results_flatten[i::N_COLS] for i in range(N_COLS)])\n        table = tabulate(\n            results_2d,\n            tablefmt=""pipe"",\n            floatfmt="".3f"",\n            headers=[""category"", ""AP""] * (N_COLS // 2),\n            numalign=""left"",\n        )\n        self._logger.info(""Per-category {} AP: \\n"".format(iou_type) + table)\n\n        results.update({""AP-"" + name: ap for name, ap in results_per_category})\n        return results\n\n\ndef instances_to_coco_json(instances, img_id):\n    """"""\n    Dump an ""Instances"" object to a COCO-format json that\'s used for evaluation.\n    Args:\n        instances (Instances):\n        img_id (int): the image id\n    Returns:\n        list[dict]: list of json annotations in COCO format.\n    """"""\n    num_instance = len(instances)\n    if num_instance == 0:\n        return []\n\n    boxes = instances.pred_boxes.tensor.numpy()\n    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n    boxes = boxes.tolist()\n    scores = instances.scores.tolist()\n    classes = instances.pred_classes.tolist()\n\n    has_mask = instances.has(""pred_masks"")\n    has_mask_scores = instances.has(""mask_scores"")\n    if has_mask:\n        # use RLE to encode the masks, because they are too large and takes memory\n        # since this evaluator stores outputs of the entire dataset\n        rles = [\n            mask_util.encode(np.array(mask[:, :, None], order=""F"", dtype=""uint8""))[0]\n            for mask in instances.pred_masks\n        ]\n        for rle in rles:\n            # ""counts"" is an array encoded by mask_util as a byte-stream. Python3\'s\n            # json writer which always produces strings cannot serialize a bytestream\n            # unless you decode it. Thankfully, utf-8 works out (which is also what\n            # the pycocotools/_mask.pyx does).\n            rle[""counts""] = rle[""counts""].decode(""utf-8"")\n        \n        if has_mask_scores:\n            mask_scores = instances.mask_scores.tolist()\n\n    has_keypoints = instances.has(""pred_keypoints"")\n    if has_keypoints:\n        keypoints = instances.pred_keypoints\n\n    results = []\n    for k in range(num_instance):\n        result = {\n            ""image_id"": img_id,\n            ""category_id"": classes[k],\n            ""bbox"": boxes[k],\n            ""score"": scores[k],\n        }\n        if has_mask:\n            result[""segmentation""] = rles[k]\n            if has_mask_scores:\n                result[""mask_score""] = mask_scores[k]\n\n        if has_keypoints:\n            # In COCO annotations,\n            # keypoints coordinates are pixel indices.\n            # However our predictions are floating point coordinates.\n            # Therefore we subtract 0.5 to be consistent with the annotation format.\n            # This is the inverse of data loading logic in `datasets/coco.py`.\n            keypoints[k][:, :2] -= 0.5\n            result[""keypoints""] = keypoints[k].flatten().tolist()\n        results.append(result)\n    return results\n\n\n# inspired from Detectron:\n# https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L255 # noqa\ndef _evaluate_box_proposals(dataset_predictions, coco_api, thresholds=None, area=""all"", limit=None):\n    """"""\n    Evaluate detection proposal recall metrics. This function is a much\n    faster alternative to the official COCO API recall evaluation code. However,\n    it produces slightly different results.\n    """"""\n    # Record max overlap value for each gt box\n    # Return vector of overlap values\n    areas = {\n        ""all"": 0,\n        ""small"": 1,\n        ""medium"": 2,\n        ""large"": 3,\n        ""96-128"": 4,\n        ""128-256"": 5,\n        ""256-512"": 6,\n        ""512-inf"": 7,\n    }\n    area_ranges = [\n        [0 ** 2, 1e5 ** 2],  # all\n        [0 ** 2, 32 ** 2],  # small\n        [32 ** 2, 96 ** 2],  # medium\n        [96 ** 2, 1e5 ** 2],  # large\n        [96 ** 2, 128 ** 2],  # 96-128\n        [128 ** 2, 256 ** 2],  # 128-256\n        [256 ** 2, 512 ** 2],  # 256-512\n        [512 ** 2, 1e5 ** 2],\n    ]  # 512-inf\n    assert area in areas, ""Unknown area range: {}"".format(area)\n    area_range = area_ranges[areas[area]]\n    gt_overlaps = []\n    num_pos = 0\n\n    for prediction_dict in dataset_predictions:\n        predictions = prediction_dict[""proposals""]\n\n        # sort predictions in descending order\n        # TODO maybe remove this and make it explicit in the documentation\n        inds = predictions.objectness_logits.sort(descending=True)[1]\n        predictions = predictions[inds]\n\n        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict[""image_id""])\n        anno = coco_api.loadAnns(ann_ids)\n        gt_boxes = [\n            BoxMode.convert(obj[""bbox""], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n            for obj in anno\n            if obj[""iscrowd""] == 0\n        ]\n        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes\n        gt_boxes = Boxes(gt_boxes)\n        gt_areas = torch.as_tensor([obj[""area""] for obj in anno if obj[""iscrowd""] == 0])\n\n        if len(gt_boxes) == 0 or len(predictions) == 0:\n            continue\n\n        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])\n        gt_boxes = gt_boxes[valid_gt_inds]\n\n        num_pos += len(gt_boxes)\n\n        if len(gt_boxes) == 0:\n            continue\n\n        if limit is not None and len(predictions) > limit:\n            predictions = predictions[:limit]\n\n        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)\n\n        _gt_overlaps = torch.zeros(len(gt_boxes))\n        for j in range(min(len(predictions), len(gt_boxes))):\n            # find which proposal box maximally covers each gt box\n            # and get the iou amount of coverage for each gt box\n            max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n\n            # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n            gt_ovr, gt_ind = max_overlaps.max(dim=0)\n            assert gt_ovr >= 0\n            # find the proposal box that covers the best covered gt box\n            box_ind = argmax_overlaps[gt_ind]\n            # record the iou coverage of this gt box\n            _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n            assert _gt_overlaps[j] == gt_ovr\n            # mark the proposal box and the gt box as used\n            overlaps[box_ind, :] = -1\n            overlaps[:, gt_ind] = -1\n\n        # append recorded iou coverage level\n        gt_overlaps.append(_gt_overlaps)\n    gt_overlaps = torch.cat(gt_overlaps, dim=0)\n    gt_overlaps, _ = torch.sort(gt_overlaps)\n\n    if thresholds is None:\n        step = 0.05\n        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)\n    recalls = torch.zeros_like(thresholds)\n    # compute recall for each iou threshold\n    for i, t in enumerate(thresholds):\n        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)\n    # ar = 2 * np.trapz(recalls, thresholds)\n    ar = recalls.mean()\n    return {\n        ""ar"": ar,\n        ""recalls"": recalls,\n        ""thresholds"": thresholds,\n        ""gt_overlaps"": gt_overlaps,\n        ""num_pos"": num_pos,\n    }\n\n\ndef _evaluate_predictions_on_coco(coco_gt, coco_results, iou_type, kpt_oks_sigmas=None):\n    """"""\n    Evaluate the coco results using COCOEval API.\n    """"""\n    assert len(coco_results) > 0\n\n    if iou_type == ""segm"":\n        coco_results = copy.deepcopy(coco_results)\n        # When evaluating mask AP, if the results contain bbox, cocoapi will\n        # use the box area as the area of the instance, instead of the mask area.\n        # This leads to a different definition of small/medium/large.\n        # We remove the bbox field to let mask AP use mask area.\n        # We also replace `score` with `mask_score` when using mask scoring.\n        has_mask_scores = ""mask_score"" in coco_results[0]\n        \n        for c in coco_results:\n            c.pop(""bbox"", None)\n            if has_mask_scores:\n                c[""score""] = c[""mask_score""]\n                del c[""mask_score""]\n\n    coco_dt = coco_gt.loadRes(coco_results)\n    coco_eval = COCOeval(coco_gt, coco_dt, iou_type)\n    # Use the COCO default keypoint OKS sigmas unless overrides are specified\n    if kpt_oks_sigmas:\n        coco_eval.params.kpt_oks_sigmas = np.array(kpt_oks_sigmas)\n\n    if iou_type == ""keypoints"":\n        num_keypoints = len(coco_results[0][""keypoints""]) // 3\n        assert len(coco_eval.params.kpt_oks_sigmas) == num_keypoints, (\n            ""[COCOEvaluator] The length of cfg.TEST.KEYPOINT_OKS_SIGMAS (default: 17) ""\n            ""must be equal to the number of keypoints. However the prediction has {} ""\n            ""keypoints! For more information please refer to ""\n            ""http://cocodataset.org/#keypoints-eval."".format(num_keypoints)\n        )\n\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n\n    return coco_eval'"
centermask/layers/__init__.py,0,"b'from .deform_conv import DFConv2d\nfrom .ml_nms import ml_nms\nfrom .iou_loss import IOULoss\nfrom .conv_with_kaiming_uniform import conv_with_kaiming_uniform\nfrom .wrappers import MaxPool2d, Linear, Max\n\n__all__ = [k for k in globals().keys() if not k.startswith(""_"")]\n'"
centermask/layers/conv_with_kaiming_uniform.py,0,"b'from torch import nn\n\nfrom detectron2.layers import Conv2d\nfrom .deform_conv import DFConv2d\nfrom detectron2.layers.batch_norm import get_norm\n\n\ndef conv_with_kaiming_uniform(\n        norm=None, activation=None,\n        use_deformable=False, use_sep=False):\n    def make_conv(\n        in_channels, out_channels, kernel_size, stride=1, dilation=1\n    ):\n        if use_deformable:\n            conv_func = DFConv2d\n        else:\n            conv_func = Conv2d\n        if use_sep:\n            assert in_channels == out_channels\n            groups = in_channels\n        else:\n            groups = 1\n        conv = conv_func(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=dilation * (kernel_size - 1) // 2,\n            dilation=dilation,\n            groups=groups,\n            bias=(norm is None)\n        )\n        if not use_deformable:\n            # Caffe2 implementation uses XavierFill, which in fact\n            # corresponds to kaiming_uniform_ in PyTorch\n            nn.init.kaiming_uniform_(conv.weight, a=1)\n            if norm is None:\n                nn.init.constant_(conv.bias, 0)\n        module = [conv,]\n        if norm is not None:\n            if norm == ""GN"":\n                norm_module = nn.GroupNorm(32, out_channels)\n            else:\n                norm_module = get_norm(norm, out_channels)\n            module.append(norm_module)\n        if activation is not None:\n            module.append(nn.ReLU(inplace=True))\n        if len(module) > 1:\n            return nn.Sequential(*module)\n        return conv\n\n    return make_conv\n'"
centermask/layers/deform_conv.py,2,"b'import torch\nfrom torch import nn\n\nfrom detectron2.layers import Conv2d\n\n\nclass _NewEmptyTensorOp(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, new_shape):\n        ctx.shape = x.shape\n        return x.new_empty(new_shape)\n\n    @staticmethod\n    def backward(ctx, grad):\n        shape = ctx.shape\n        return _NewEmptyTensorOp.apply(grad, shape), None\n\n\nclass DFConv2d(nn.Module):\n    """"""Deformable convolutional layer""""""\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            with_modulated_dcn=True,\n            kernel_size=3,\n            stride=1,\n            groups=1,\n            dilation=1,\n            deformable_groups=1,\n            bias=False,\n            padding=None\n    ):\n        super(DFConv2d, self).__init__()\n        if isinstance(kernel_size, (list, tuple)):\n            assert isinstance(stride, (list, tuple))\n            assert isinstance(dilation, (list, tuple))\n            assert len(kernel_size) == 2\n            assert len(stride) == 2\n            assert len(dilation) == 2\n            padding = (\n                dilation[0] * (kernel_size[0] - 1) // 2,\n                dilation[1] * (kernel_size[1] - 1) // 2\n            )\n            offset_base_channels = kernel_size[0] * kernel_size[1]\n        else:\n            padding = dilation * (kernel_size - 1) // 2\n            offset_base_channels = kernel_size * kernel_size\n        if with_modulated_dcn:\n            from .deform_conv import ModulatedDeformConv\n            offset_channels = offset_base_channels * 3  # default: 27\n            conv_block = ModulatedDeformConv\n        else:\n            from .deform_conv import DeformConv\n            offset_channels = offset_base_channels * 2  # default: 18\n            conv_block = DeformConv\n        self.offset = Conv2d(\n            in_channels,\n            deformable_groups * offset_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=1,\n            dilation=dilation\n        )\n        for l in [self.offset, ]:\n            nn.init.kaiming_uniform_(l.weight, a=1)\n            torch.nn.init.constant_(l.bias, 0.)\n        self.conv = conv_block(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            deformable_groups=deformable_groups,\n            bias=bias\n        )\n        self.with_modulated_dcn = with_modulated_dcn\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.offset_split = offset_base_channels * deformable_groups * 2\n\n    def forward(self, x, return_offset=False):\n        if x.numel() > 0:\n            if not self.with_modulated_dcn:\n                offset_mask = self.offset(x)\n                x = self.conv(x, offset_mask)\n            else:\n                offset_mask = self.offset(x)\n                offset = offset_mask[:, :self.offset_split, :, :]\n                mask = offset_mask[:, self.offset_split:, :, :].sigmoid()\n                x = self.conv(x, offset, mask)\n            if return_offset:\n                return x, offset_mask\n            return x\n        # get output shape\n        output_shape = [\n            (i + 2 * p - (di * (k - 1) + 1)) // d + 1\n            for i, p, di, k, d in zip(\n                x.shape[-2:],\n                self.padding,\n                self.dilation,\n                self.kernel_size,\n                self.stride\n            )\n        ]\n        output_shape = [x.shape[0], self.conv.weight.shape[0]] + output_shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\n'"
centermask/layers/iou_loss.py,9,"b""import torch\nfrom torch import nn\n\n\nclass IOULoss(nn.Module):\n    def __init__(self, loc_loss_type='iou'):\n        super(IOULoss, self).__init__()\n        self.loc_loss_type = loc_loss_type\n\n    def forward(self, pred, target, weight=None):\n        pred_left = pred[:, 0]\n        pred_top = pred[:, 1]\n        pred_right = pred[:, 2]\n        pred_bottom = pred[:, 3]\n\n        target_left = target[:, 0]\n        target_top = target[:, 1]\n        target_right = target[:, 2]\n        target_bottom = target[:, 3]\n\n        target_aera = (target_left + target_right) * \\\n                      (target_top + target_bottom)\n        pred_aera = (pred_left + pred_right) * \\\n                    (pred_top + pred_bottom)\n\n        w_intersect = torch.min(pred_left, target_left) + \\\n                      torch.min(pred_right, target_right)\n        h_intersect = torch.min(pred_bottom, target_bottom) + \\\n                      torch.min(pred_top, target_top)\n\n        g_w_intersect = torch.max(pred_left, target_left) + \\\n                        torch.max(pred_right, target_right)\n        g_h_intersect = torch.max(pred_bottom, target_bottom) + \\\n                        torch.max(pred_top, target_top)\n        ac_uion = g_w_intersect * g_h_intersect\n\n        area_intersect = w_intersect * h_intersect\n        area_union = target_aera + pred_aera - area_intersect\n\n        ious = (area_intersect + 1.0) / (area_union + 1.0)\n        gious = ious - (ac_uion - area_union) / ac_uion\n        if self.loc_loss_type == 'iou':\n            losses = -torch.log(ious)\n        elif self.loc_loss_type == 'linear_iou':\n            losses = 1 - ious\n        elif self.loc_loss_type == 'giou':\n            losses = 1 - gious\n        else:\n            raise NotImplementedError\n\n        if weight is not None:\n            return (losses * weight).sum()\n        else:\n            return losses.sum()\n"""
centermask/layers/ml_nms.py,0,"b'from detectron2.layers import batched_nms\n\n\ndef ml_nms(boxlist, nms_thresh, max_proposals=-1,\n           score_field=""scores"", label_field=""labels""):\n    """"""\n    Performs non-maximum suppression on a boxlist, with scores specified\n    in a boxlist field via score_field.\n    Arguments:\n        boxlist(BoxList)\n        nms_thresh (float)\n        max_proposals (int): if > 0, then only the top max_proposals are kept\n            after non-maximum suppression\n        score_field (str)\n    """"""\n    if nms_thresh <= 0:\n        return boxlist\n    boxes = boxlist.pred_boxes.tensor\n    scores = boxlist.scores\n    labels = boxlist.pred_classes\n    keep = batched_nms(boxes, scores, labels, nms_thresh)\n    if max_proposals > 0:\n        keep = keep[: max_proposals]\n    boxlist = boxlist[keep]\n    return boxlist\n'"
centermask/layers/wrappers.py,7,"b'# Author, Sangrok Lee, github.com/lsrock1\n\nimport torch\n\n\n# from facebook detectron2\nclass _NewEmptyTensorOp(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, new_shape):\n        ctx.shape = x.shape\n        return x.new_empty(new_shape)\n\n    @staticmethod\n    def backward(ctx, grad):\n        shape = ctx.shape\n        return _NewEmptyTensorOp.apply(grad, shape), None\n\n\nclass MaxPool2d(torch.nn.MaxPool2d):\n    """"""\n    A wrapper around :class:`torch.nn.MaxPool2d` to support empty inputs and more features.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._make_iteratable()\n\n    def forward(self, x):\n        if x.numel() == 0:\n            # When input is empty, we want to return a empty tensor with ""correct"" shape,\n            # So that the following operations will not panic\n            # if they check for the shape of the tensor.\n            # This computes the height and width of the output tensor\n            \n            output_shape = [\n                (i + 2 * p - (di * (k - 1) + 1)) // s + 1\n                for i, p, di, k, s in zip(\n                    x.shape[-2:], self.padding, self.dilation, self.kernel_size, self.stride\n                )\n            ]\n            output_shape = [x.shape[0], x.shape[1]] + output_shape\n            empty = _NewEmptyTensorOp.apply(x, output_shape)\n            \n            return empty\n\n        x = super().forward(x)\n        \n        return x\n\n    def _make_iteratable(self):\n        if not isinstance(self.padding, list):\n            self.padding = [self.padding, self.padding]\n\n        if not isinstance(self.dilation, list):\n            self.dilation = [self.dilation, self.dilation]\n\n        if not isinstance(self.kernel_size, list):\n            self.kernel_size = [self.kernel_size, self.kernel_size]\n\n        if not isinstance(self.stride, list):\n            self.stride = [self.stride, self.stride]\n\n\nclass Linear(torch.nn.Linear):\n    """"""\n    A wrapper around :class:`torch.nn.Linear` to support empty inputs and more features.\n    Because of https://github.com/pytorch/pytorch/issues/34202\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, x):\n        if x.numel() == 0:\n            output_shape = [x.shape[0], self.weight.shape[0]]\n            \n            empty = _NewEmptyTensorOp.apply(x, output_shape)\n            if self.training:\n                # This is to make DDP happy.\n                # DDP expects all workers to have gradient w.r.t the same set of parameters.\n                _dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0\n                return empty + _dummy\n            else:\n                return empty\n\n        x = super().forward(x)\n        return x\n\n\ndef Max(x):\n    """"""\n    A wrapper around torch.max in Spatial Attention Module (SAM) to support empty inputs and more features.\n    """"""\n    if x.numel() == 0:\n        output_shape = [x.shape[0], 1, x.shape[2], x.shape[3]]\n        empty = _NewEmptyTensorOp.apply(x, output_shape)\n        return empty\n    return torch.max(x, dim=1, keepdim=True)[0]'"
centermask/modeling/__init__.py,0,b'from .fcos import FCOS\nfrom .backbone import build_fcos_resnet_fpn_backbone\nfrom .centermask import CenterROIHeads\n'
centermask/utils/comm.py,1,"b'import torch.distributed as dist\nfrom detectron2.utils.comm import get_world_size\n\n\ndef reduce_sum(tensor):\n    world_size = get_world_size()\n    if world_size < 2:\n        return tensor\n    tensor = tensor.clone()\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    return tensor\n'"
centermask/utils/measures.py,0,"b""# coding: utf-8\n# Adapted from https://github.com/ShichenLiu/CondenseNet/blob/master/utils.py\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport operator\n\nfrom functools import reduce\n\n\ndef get_num_gen(gen):\n    return sum(1 for x in gen)\n\n\ndef is_pruned(layer):\n    try:\n        layer.mask\n        return True\n    except AttributeError:\n        return False\n\n\ndef is_leaf(model):\n    return get_num_gen(model.children()) == 0\n\n\ndef get_layer_info(layer):\n    layer_str = str(layer)\n    type_name = layer_str[:layer_str.find('(')].strip()\n    return type_name\n\n\ndef get_layer_param(model):\n    return sum([reduce(operator.mul, i.size(), 1) for i in model.parameters()])\n\n\n### The input batch size should be 1 to call this function\ndef measure_layer(layer, *args):\n    global count_ops, count_params\n\n    for x in args:\n        delta_ops = 0\n        delta_params = 0\n        multi_add = 1\n        type_name = get_layer_info(layer)\n\n        ### ops_conv\n        if type_name in ['Conv2d']:\n            out_h = int((x.size()[2] + 2 * layer.padding[0] / layer.dilation[0] - layer.kernel_size[0]) /\n                        layer.stride[0] + 1)\n            out_w = int((x.size()[3] + 2 * layer.padding[1] / layer.dilation[1] - layer.kernel_size[1]) /\n                        layer.stride[1] + 1)\n            delta_ops = layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add\n            delta_params = get_layer_param(layer)\n\n        elif type_name in ['ConvTranspose2d']:\n            _, _, in_h, in_w = x.size()\n            out_h = int((in_h-1)*layer.stride[0] - 2 * layer.padding[0] + layer.kernel_size[0] + layer.output_padding[0])\n            out_w = int((in_w-1)*layer.stride[1] - 2 * layer.padding[1] + layer.kernel_size[1] + layer.output_padding[1])\n            delta_ops = layer.in_channels * layer.out_channels * layer.kernel_size[0] *  \\\n                        layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add\n            delta_params = get_layer_param(layer)\n\n        ### ops_learned_conv\n        elif type_name in ['LearnedGroupConv']:\n            measure_layer(layer.relu, x)\n            measure_layer(layer.norm, x)\n            conv = layer.conv\n            out_h = int((x.size()[2] + 2 * conv.padding[0] - conv.kernel_size[0]) /\n                        conv.stride[0] + 1)\n            out_w = int((x.size()[3] + 2 * conv.padding[1] - conv.kernel_size[1]) /\n                        conv.stride[1] + 1)\n            delta_ops = conv.in_channels * conv.out_channels * conv.kernel_size[0] * conv.kernel_size[1] * out_h * out_w / layer.condense_factor * multi_add\n            delta_params = get_layer_param(conv) / layer.condense_factor\n\n        ### ops_nonlinearity\n        elif type_name in ['ReLU', 'ReLU6']:\n            delta_ops = x.numel()\n            delta_params = get_layer_param(layer)\n\n        ### ops_pooling\n        elif type_name in ['AvgPool2d', 'MaxPool2d']:\n            in_w = x.size()[2]\n            kernel_ops = layer.kernel_size * layer.kernel_size\n            out_w = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n            out_h = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n            delta_ops = x.size()[0] * x.size()[1] * out_w * out_h * kernel_ops\n            delta_params = get_layer_param(layer)\n\n        elif type_name in ['LastLevelMaxPool']:\n            pass\n\n        elif type_name in ['AdaptiveAvgPool2d']:\n            delta_ops = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]\n            delta_params = get_layer_param(layer)\n\n        elif type_name in ['ZeroPad2d', 'RetinaNetPostProcessor']:\n            pass\n            #delta_ops = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]\n            #delta_params = get_layer_param(layer)\n\n        ### ops_linear\n        elif type_name in ['Linear']:\n            weight_ops = layer.weight.numel() * multi_add\n            bias_ops = layer.bias.numel()\n            delta_ops = x.size()[0] * (weight_ops + bias_ops)\n            delta_params = get_layer_param(layer)\n\n        ### ops_nothing\n        elif type_name in ['BatchNorm2d', 'Dropout2d', 'DropChannel', 'Dropout', 'FrozenBatchNorm2d', 'GroupNorm']:\n            delta_params = get_layer_param(layer)\n\n        elif type_name in ['SumTwo']:\n            delta_ops = x.numel()\n\n        elif type_name in ['AggregateCell']:\n            if not layer.pre_transform:\n                delta_ops = 2 * x.numel() # twice for each input\n            else:\n                measure_layer(layer.branch_1, x)\n                measure_layer(layer.branch_2, x)\n                delta_params = get_layer_param(layer)\n\n        elif type_name in ['Identity', 'Zero']:\n            pass\n\n        elif type_name in ['Scale']:\n            delta_params = get_layer_param(layer)\n            delta_ops = x.numel()\n\n        elif type_name in ['FCOSPostProcessor', 'RPNPostProcessor', 'KeypointPostProcessor',\n                           'ROIAlign', 'PostProcessor', 'KeypointRCNNPredictor', \n                           'NaiveSyncBatchNorm', 'Upsample', 'Sequential']:\n            pass\n\n        elif type_name in ['DeformConv']:\n            # don't count bilinear\n            offset_conv = list(layer.parameters())[0]\n            delta_ops = reduce(operator.mul, offset_conv.size(), x.size()[2] * x.size()[3])\n            out_h = int((x.size()[2] + 2 * layer.padding[0] / layer.dilation[0]\n                         - layer.kernel_size[0]) / layer.stride[0] + 1)\n            out_w = int((x.size()[3] + 2 * layer.padding[1] / layer.dilation[1]\n                         - layer.kernel_size[1]) / layer.stride[1] + 1)\n            delta_ops += layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add\n            delta_params = get_layer_param(layer)\n\n        ### unknown layer type\n        else:\n            raise TypeError('unknown layer type: %s' % type_name)\n\n        count_ops += delta_ops\n        count_params += delta_params\n    return\n\n\ndef measure_model(model, x):\n    global count_ops, count_params\n    count_ops = 0\n    count_params = 0\n\n    def should_measure(x):\n        return is_leaf(x) or is_pruned(x)\n\n    def modify_forward(model):\n        for child in model.children():\n            if should_measure(child):\n                def new_forward(m):\n                    def lambda_forward(*args):\n                        measure_layer(m, *args)\n                        return m.old_forward(*args)\n                    return lambda_forward\n                child.old_forward = child.forward\n                child.forward = new_forward(child)\n            else:\n                modify_forward(child)\n\n    def restore_forward(model):\n        for child in model.children():\n            # leaf node\n            if is_leaf(child) and hasattr(child, 'old_forward'):\n                child.forward = child.old_forward\n                child.old_forward = None\n            else:\n                restore_forward(child)\n\n    modify_forward(model)\n    out = model.forward(x)\n    restore_forward(model)\n\n    return out, count_ops, count_params\n"""
centermask/modeling/backbone/__init__.py,0,"b'# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\nfrom .fpn import build_fcos_resnet_fpn_backbone, LastLevelP6P7, LastLevelP6\nfrom .vovnet import build_vovnet_fpn_backbone, build_vovnet_backbone, build_fcos_vovnet_fpn_backbone\nfrom .mobilenet import build_mnv2_backbone, build_mobilenetv2_fpn_backbone, build_fcos_mobilenetv2_fpn_backbone\n'"
centermask/modeling/backbone/fpn.py,1,"b'from torch import nn\nimport torch.nn.functional as F\nimport fvcore.nn.weight_init as weight_init\n\nfrom detectron2.modeling.backbone import FPN, build_resnet_backbone\nfrom detectron2.layers import ShapeSpec\nfrom detectron2.modeling.backbone.build import BACKBONE_REGISTRY\n\n\n__all__ = [\n    ""FPN"",\n    ""LastLevelP6P7"",\n    ""LastLevelP6"",\n    ""build_fcos_resnet_fpn_backbone""\n]\n\nclass LastLevelP6P7(nn.Module):\n    """"""\n    This module is used in RetinaNet and FCOS to generate extra layers, P6 and P7 from\n    C5 or P5 feature.\n    """"""\n\n    def __init__(self, in_channels, out_channels, in_features=""res5""):\n        super().__init__()\n        self.num_levels = 2\n        self.in_feature = in_features\n        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)\n        self.p7 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)\n        for module in [self.p6, self.p7]:\n            weight_init.c2_xavier_fill(module)\n\n    def forward(self, x):\n        p6 = self.p6(x)\n        p7 = self.p7(F.relu(p6))\n        return [p6, p7]\n\n\nclass LastLevelP6(nn.Module):\n    """"""\n    This module is used in FCOS to generate extra layers\n    """"""\n\n    def __init__(self, in_channels, out_channels, in_features=""res5""):\n        super().__init__()\n        self.num_levels = 1\n        self.in_feature = in_features\n        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)\n        for module in [self.p6]:\n            weight_init.c2_xavier_fill(module)\n\n    def forward(self, x):\n        p6 = self.p6(x)\n        return [p6]\n\n\n@BACKBONE_REGISTRY.register()\ndef build_fcos_resnet_fpn_backbone(cfg, input_shape: ShapeSpec):\n    """"""\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    """"""\n    if cfg.MODEL.MOBILENET:\n        bottom_up = build_mnv2_backbone(cfg, input_shape)\n    else:\n        bottom_up = build_resnet_backbone(cfg, input_shape)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    top_levels = cfg.MODEL.FCOS.TOP_LEVELS\n    in_channels_top = out_channels\n    if top_levels == 2:\n        top_block = LastLevelP6P7(in_channels_top, out_channels, ""p5"")\n    if top_levels == 1:\n        top_block = LastLevelP6(in_channels_top, out_channels, ""p5"")\n    elif top_levels == 0:\n        top_block = None\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=top_block,\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone\n'"
centermask/modeling/backbone/mobilenet.py,1,"b'# taken from https://github.com/tonylins/pytorch-mobilenet-v2/\n# Published by Ji Lin, tonylins\n# licensed under the  Apache License, Version 2.0, January 2004\n# Modified by Youngwan Lee, Feburary 2020\n\nfrom torch import nn\nfrom torch.nn import BatchNorm2d\nfrom detectron2.layers import Conv2d, FrozenBatchNorm2d, ShapeSpec\nfrom detectron2.modeling.backbone.build import BACKBONE_REGISTRY\nfrom detectron2.modeling.backbone import Backbone\nfrom detectron2.modeling.backbone.fpn import FPN, LastLevelMaxPool\n\nfrom .fpn import LastLevelP6, LastLevelP6P7\n\n__all__ = [\n    ""MobileNetV2"",\n    ""build_mnv2_backbone"",\n    ""build_mobilenetv2_fpn_backbone"",\n    ""build_fcos_mobilenetv2_fpn_backbone""\n]\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        Conv2d(inp, oup, 3, stride, 1, bias=False),\n        FrozenBatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        Conv2d(inp, oup, 1, 1, 0, bias=False),\n        FrozenBatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                FrozenBatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                FrozenBatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                FrozenBatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                FrozenBatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                FrozenBatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(Backbone):\n    """"""\n    Should freeze bn\n    """"""\n    def __init__(self, cfg, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.return_features_indices = [3, 6, 13, 17]\n        self.return_features_num_channels = []\n        self.features = nn.ModuleList([conv_bn(3, input_channel, 2)])\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n                if len(self.features) - 1 in self.return_features_indices:\n                    self.return_features_num_channels.append(output_channel)\n\n        self._initialize_weights()\n        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_AT)\n\n    def _freeze_backbone(self, freeze_at):\n        for layer_index in range(freeze_at):\n            for p in self.features[layer_index].parameters():\n                p.requires_grad = False\n\n    def forward(self, x):\n        res = []\n        for i, m in enumerate(self.features):\n            x = m(x)\n            if i in self.return_features_indices:\n                res.append(x)\n        return {\'res{}\'.format(i + 2): r for i, r in enumerate(res)}\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, (2. / n) ** 0.5)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n@BACKBONE_REGISTRY.register()\ndef build_mnv2_backbone(cfg, input_shape):\n    """"""\n    Create a MobileNetV2 instance from config.\n    Returns:\n        MobileNetV2: a :class:`MobileNetV2` instance.\n    """"""\n    out_features = cfg.MODEL.RESNETS.OUT_FEATURES\n\n    out_feature_channels = {""res2"": 24, ""res3"": 32,\n                            ""res4"": 96, ""res5"": 320}\n    out_feature_strides = {""res2"": 4, ""res3"": 8, ""res4"": 16, ""res5"": 32}\n    model = MobileNetV2(cfg)\n    model._out_features = out_features\n    model._out_feature_channels = out_feature_channels\n    model._out_feature_strides = out_feature_strides\n    return model\n\n\n@BACKBONE_REGISTRY.register()\ndef build_mobilenetv2_fpn_backbone(cfg, input_shape: ShapeSpec):\n    """"""\n    Args:\n        cfg: a detectron2 CfgNode\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    """"""\n    bottom_up = build_mnv2_backbone(cfg, input_shape)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=LastLevelMaxPool(),\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone\n\n\n@BACKBONE_REGISTRY.register()\ndef build_fcos_mobilenetv2_fpn_backbone(cfg, input_shape: ShapeSpec):\n    """"""\n    Args:\n        cfg: a detectron2 CfgNode\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    """"""\n    bottom_up = build_mnv2_backbone(cfg, input_shape)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    top_levels = cfg.MODEL.FCOS.TOP_LEVELS\n    in_channels_top = out_channels\n    if top_levels == 2:\n        top_block = LastLevelP6P7(in_channels_top, out_channels, ""p5"")\n    if top_levels == 1:\n        top_block = LastLevelP6(in_channels_top, out_channels, ""p5"")\n    elif top_levels == 0:\n        top_block = None\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=top_block,\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone'"
centermask/modeling/backbone/vovnet.py,5,"b'# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport fvcore.nn.weight_init as weight_init\nfrom detectron2.modeling.backbone import Backbone\nfrom detectron2.modeling.backbone.build import BACKBONE_REGISTRY\nfrom detectron2.modeling.backbone.fpn import FPN\nfrom detectron2.layers import (\n    Conv2d,\n    DeformConv,\n    ModulatedDeformConv,\n    FrozenBatchNorm2d,\n    ShapeSpec,\n    get_norm,\n)\nfrom .fpn import LastLevelP6, LastLevelP6P7\n\n__all__ = [\n    ""VoVNet"",\n    ""build_vovnet_backbone"",\n    ""build_vovnet_fpn_backbone"",\n    ""build_fcos_vovnet_fpn_backbone""\n]\n\n_NORM = False\n\nVoVNet19_slim_dw_eSE = {\n    \'stem\': [64, 64, 64],\n    \'stage_conv_ch\': [64, 80, 96, 112],\n    \'stage_out_ch\': [112, 256, 384, 512],\n    ""layer_per_block"": 3,\n    ""block_per_stage"": [1, 1, 1, 1],\n    ""eSE"": True,\n    ""dw"" : True\n}\n\nVoVNet19_dw_eSE = {\n    \'stem\': [64, 64, 64],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 3,\n    ""block_per_stage"": [1, 1, 1, 1],\n    ""eSE"": True,\n    ""dw"" : True\n}\n\nVoVNet19_slim_eSE = {\n    \'stem\': [64, 64, 128],\n    \'stage_conv_ch\': [64, 80, 96, 112],\n    \'stage_out_ch\': [112, 256, 384, 512],\n    \'layer_per_block\': 3,\n    \'block_per_stage\': [1, 1, 1, 1],\n    \'eSE\' : True,\n    ""dw"" : False\n}\n\nVoVNet19_eSE = {\n    \'stem\': [64, 64, 128],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 3,\n    ""block_per_stage"": [1, 1, 1, 1],\n    ""eSE"": True,\n    ""dw"" : False\n}\n\nVoVNet39_eSE = {\n    \'stem\': [64, 64, 128],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 5,\n    ""block_per_stage"": [1, 1, 2, 2],\n    ""eSE"": True,\n    ""dw"" : False\n}\n\nVoVNet57_eSE = {\n    \'stem\': [64, 64, 128],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 5,\n    ""block_per_stage"": [1, 1, 4, 3],\n    ""eSE"": True,\n    ""dw"" : False\n}\n\nVoVNet99_eSE = {\n    \'stem\': [64, 64, 128],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 5,\n    ""block_per_stage"": [1, 3, 9, 3],\n    ""eSE"": True,\n    ""dw"" : False\n}\n\n_STAGE_SPECS = {\n    ""V-19-slim-dw-eSE"": VoVNet19_slim_dw_eSE,\n    ""V-19-dw-eSE"": VoVNet19_dw_eSE,\n    ""V-19-slim-eSE"": VoVNet19_slim_eSE,\n    ""V-19-eSE"": VoVNet19_eSE,\n    ""V-39-eSE"": VoVNet39_eSE,\n    ""V-57-eSE"": VoVNet57_eSE,\n    ""V-99-eSE"": VoVNet99_eSE,\n}\n\ndef dw_conv3x3(in_channels, out_channels, module_name, postfix,\n            stride=1, kernel_size=3, padding=1):\n    """"""3x3 convolution with padding""""""\n    return [\n        (\'{}_{}/dw_conv3x3\'.format(module_name, postfix),\n            nn.Conv2d(in_channels, out_channels,\n                      kernel_size=kernel_size,\n                      stride=stride,\n                      padding=padding,\n                      groups=out_channels,\n                      bias=False)),\n        (\'{}_{}/pw_conv1x1\'.format(module_name, postfix),\n            nn.Conv2d(in_channels, out_channels,\n                      kernel_size=1,\n                      stride=1,\n                      padding=0,\n                      groups=1,\n                      bias=False)),\n        (\'{}_{}/pw_norm\'.format(module_name, postfix), get_norm(_NORM, out_channels)),\n        (\'{}_{}/pw_relu\'.format(module_name, postfix), nn.ReLU(inplace=True)),\n    ]\n\nclass DFConv3x3(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        module_name,\n        postfix,\n        dilation=1,\n        groups=1,\n        with_modulated_dcn=None,\n        deformable_groups=1\n        ):\n        super(DFConv3x3, self).__init__()\n        self.module_names = []\n        self.with_modulated_dcn = with_modulated_dcn\n        if self.with_modulated_dcn:\n            deform_conv_op = ModulatedDeformConv\n            # offset channels are 2 or 3 (if with modulated) * kernel_size * kernel_size\n            offset_channels = 27\n        else:\n            deform_conv_op = DeformConv\n            offset_channels = 18\n\n        unit_name = f""{module_name}_{postfix}/conv_offset""\n        self.module_names.append(unit_name)\n        self.add_module(unit_name, Conv2d(\n            in_channels,\n            offset_channels * deformable_groups,\n            kernel_size=3,\n            stride=1,\n            padding=1 * dilation,\n            dilation=dilation,\n        ))\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.constant_(m.weight, 0)\n                nn.init.constant_(m.bias, 0)\n\n        unit_name = f""{module_name}_{postfix}/conv""\n        self.module_names.append(unit_name)\n        self.add_module(f""{module_name}_{postfix}/conv"", deform_conv_op(\n                    in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1 * dilation,\n                    bias=False,\n                    groups=groups,\n                    dilation=1,\n                    deformable_groups=deformable_groups,\n                ))\n        unit_name = f""{module_name}_{postfix}/norm""\n        self.module_names.append(unit_name)\n        self.add_module(unit_name, get_norm(_NORM, out_channels))\n\n\n    def forward(self, x):\n        if self.with_modulated_dcn:\n            #offset conv\n            offset_mask = getattr(self, self.module_names[0])(x)\n            offset_x, offset_y, mask = torch.chunk(offset_mask, 3, dim=1)\n            offset = torch.cat((offset_x, offset_y), dim=1)\n            mask = mask.sigmoid()\n            #conv\n            out = getattr(self, self.module_names[1])(x, offset, mask)\n        else:\n            offset = getattr(self, self.module_names[0])(x)\n            out = getattr(self, self.module_names[1])(x, offset)\n\n        return F.relu_(getattr(self, self.module_names[2])(out))\n\n\n\ndef conv3x3(in_channels, out_channels, module_name, postfix, \n              stride=1, groups=1, kernel_size=3, padding=1):\n    """"""3x3 convolution with padding""""""\n    return [\n        (f\'{module_name}_{postfix}/conv\',\n         nn.Conv2d(in_channels, \n                    out_channels, \n                    kernel_size=kernel_size, \n                    stride=stride, \n                    padding=padding, \n                    groups=groups, \n                    bias=False)),\n        (f\'{module_name}_{postfix}/norm\', get_norm(_NORM, out_channels)),\n        (f\'{module_name}_{postfix}/relu\', nn.ReLU(inplace=True))\n    ]\n\n\ndef conv1x1(in_channels, out_channels, module_name, postfix, \n              stride=1, groups=1, kernel_size=1, padding=0):\n    """"""1x1 convolution with padding""""""\n    return [\n        (f\'{module_name}_{postfix}/conv\',\n         nn.Conv2d(in_channels, \n                    out_channels, \n                    kernel_size=kernel_size, \n                    stride=stride, \n                    padding=padding, \n                    groups=groups,\n                    bias=False)),\n        (f\'{module_name}_{postfix}/norm\', get_norm(_NORM, out_channels)),\n        (f\'{module_name}_{postfix}/relu\', nn.ReLU(inplace=True))\n    ]\n\nclass Hsigmoid(nn.Module):\n    def __init__(self, inplace=True):\n        super(Hsigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return F.relu6(x + 3., inplace=self.inplace) / 6.\n\n\nclass eSEModule(nn.Module):\n    def __init__(self, channel, reduction=4):\n        super(eSEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(channel,channel, kernel_size=1,\n                             padding=0)\n        self.hsigmoid = Hsigmoid()\n\n    def forward(self, x):\n        input = x\n        x = self.avg_pool(x)\n        x = self.fc(x)\n        x = self.hsigmoid(x)\n        return input * x\n\n\nclass _OSA_module(nn.Module):\n\n    def __init__(self, \n                 in_ch, \n                 stage_ch, \n                 concat_ch, \n                 layer_per_block, \n                 module_name, \n                 SE=False,\n                 identity=False,\n                 depthwise=False,\n                 dcn_config={},\n                 ):\n\n        super(_OSA_module, self).__init__()\n\n        self.identity = identity\n        self.depthwise = depthwise\n        self.isReduced = False\n        self.layers = nn.ModuleList()\n        in_channel = in_ch\n        if self.depthwise and in_channel != stage_ch:\n            self.isReduced = True\n            self.conv_reduction = nn.Sequential(\n                OrderedDict(conv1x1(in_channel, stage_ch, \n                  ""{}_reduction"".format(module_name), ""0"")))\n        with_dcn = dcn_config.get(""stage_with_dcn"", False)\n        for i in range(layer_per_block):\n            if self.depthwise:\n                self.layers.append(\n                    nn.Sequential(OrderedDict(dw_conv3x3(stage_ch, stage_ch, module_name, i))))\n            elif with_dcn:\n                deformable_groups = dcn_config.get(""deformable_groups"", 1)\n                with_modulated_dcn = dcn_config.get(""with_modulated_dcn"", False)\n                self.layers.append(DFConv3x3(in_channel, stage_ch, module_name, i, \n                    with_modulated_dcn=with_modulated_dcn, deformable_groups=deformable_groups))\n            else:\n                self.layers.append(nn.Sequential(OrderedDict(conv3x3(in_channel, stage_ch, module_name, i))))\n            in_channel = stage_ch\n\n        # feature aggregation\n        in_channel = in_ch + layer_per_block * stage_ch\n        self.concat = nn.Sequential(OrderedDict(conv1x1(in_channel, concat_ch, module_name, ""concat"")))\n\n        self.ese = eSEModule(concat_ch)\n\n\n    def forward(self, x):\n\n        identity_feat = x\n\n        output = []\n        output.append(x)\n\n        if self.depthwise and self.isReduced:\n            x = self.conv_reduction(x)\n        \n        for layer in self.layers:\n            x = layer(x)\n            output.append(x)\n\n        x = torch.cat(output, dim=1)\n        xt = self.concat(x)\n\n        xt = self.ese(xt)\n\n        if self.identity:\n            xt = xt + identity_feat\n\n        return xt\n\n\nclass _OSA_stage(nn.Sequential):\n\n    def __init__(self, \n                 in_ch, \n                 stage_ch, \n                 concat_ch, \n                 block_per_stage, \n                 layer_per_block, \n                 stage_num,\n                 SE=False,\n                 depthwise=False,\n                 dcn_config={}):\n        super(_OSA_stage, self).__init__()\n\n        if not stage_num == 2:\n            self.add_module(""Pooling"", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True))\n\n        if block_per_stage != 1:\n            SE = False\n        module_name = f""OSA{stage_num}_1""\n        self.add_module(module_name, _OSA_module(in_ch, \n                                                 stage_ch, \n                                                 concat_ch, \n                                                 layer_per_block, \n                                                 module_name,\n                                                 SE,\n                                                 depthwise=depthwise,\n                                                 dcn_config=dcn_config))\n        for i in range(block_per_stage - 1):\n            if i != block_per_stage - 2: #last block\n                SE = False\n            module_name = f""OSA{stage_num}_{i + 2}""\n            self.add_module(module_name,\n                            _OSA_module(concat_ch, \n                                        stage_ch, \n                                        concat_ch, \n                                        layer_per_block, \n                                        module_name, \n                                        SE,\n                                        identity=True,\n                                        depthwise=depthwise,\n                                        dcn_config=dcn_config))\n\n\n\nclass VoVNet(Backbone):\n\n    def __init__(self, cfg, input_ch, out_features=None):\n        """"""\n        Args:\n            input_ch(int) : the number of input channel\n            out_features (list[str]): name of the layers whose outputs should\n                be returned in forward. Can be anything in ""stem"", ""stage2"" ...\n        """"""\n        super(VoVNet, self).__init__()\n\n        global _NORM\n        _NORM = cfg.MODEL.VOVNET.NORM\n            \n        stage_specs = _STAGE_SPECS[cfg.MODEL.VOVNET.CONV_BODY]\n\n        stem_ch = stage_specs[""stem""]\n        config_stage_ch = stage_specs[""stage_conv_ch""]\n        config_concat_ch = stage_specs[""stage_out_ch""]\n        block_per_stage = stage_specs[""block_per_stage""]\n        layer_per_block = stage_specs[""layer_per_block""]\n        SE = stage_specs[""eSE""]\n        depthwise = stage_specs[""dw""]\n\n        self._out_features = out_features\n\n\n        # Stem module\n        conv_type = dw_conv3x3 if depthwise else conv3x3\n        stem = conv3x3(input_ch, stem_ch[0], ""stem"", ""1"", 2)\n        stem += conv_type(stem_ch[0], stem_ch[1], ""stem"", ""2"", 1)\n        stem += conv_type(stem_ch[1], stem_ch[2], ""stem"", ""3"", 2)\n        self.add_module(""stem"", nn.Sequential((OrderedDict(stem))))\n        current_stirde = 4\n        self._out_feature_strides = {""stem"": current_stirde, ""stage2"": current_stirde}\n        self._out_feature_channels = {""stem"": stem_ch[2]}\n\n        stem_out_ch = [stem_ch[2]]\n        in_ch_list = stem_out_ch + config_concat_ch[:-1]\n        # OSA stages\n        self.stage_names = []\n        for i in range(4):  # num_stages\n            name = ""stage%d"" % (i + 2) # stage 2 ... stage 5\n            self.stage_names.append(name)\n            self.add_module(name, _OSA_stage(in_ch_list[i],\n                                             config_stage_ch[i],\n                                             config_concat_ch[i],\n                                             block_per_stage[i],\n                                             layer_per_block,\n                                             i + 2,\n                                             SE,\n                                             depthwise,\n                                             dcn_config = {\n                                                 ""stage_with_dcn"": cfg.MODEL.VOVNET.STAGE_WITH_DCN[i],\n                                                 ""with_modulated_dcn"": cfg.MODEL.VOVNET.WITH_MODULATED_DCN,\n                                                 ""deformable_groups"": cfg.MODEL.VOVNET.DEFORMABLE_GROUPS,\n                                             }\n            ))\n            \n            self._out_feature_channels[name] = config_concat_ch[i]\n            if not i == 0:\n                self._out_feature_strides[name] = current_stirde = int(\n                    current_stirde * 2) \n\n        # initialize weights\n        # self._initialize_weights()\n        # Optionally freeze (requires_grad=False) parts of the backbone\n        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_AT)\n\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n\n    def _freeze_backbone(self, freeze_at):\n        if freeze_at < 0:\n            return\n        # freeze BN layers\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                freeze_bn_params(m)\n        for stage_index in range(freeze_at):\n            if stage_index == 0:\n                m = self.stem # stage 0 is the stem\n            else:\n                m = getattr(self, ""stage"" + str(stage_index+1))\n            for p in m.parameters():\n                p.requires_grad = False\n                FrozenBatchNorm2d.convert_frozen_batchnorm(self)\n\n    def forward(self, x):\n        outputs = {}\n        x = self.stem(x)\n        if ""stem"" in self._out_features:\n            outputs[""stem""] = x\n        for name in self.stage_names:\n            x = getattr(self, name)(x)\n            if name in self._out_features:\n                outputs[name] = x\n\n        return outputs\n\n    def output_shape(self):\n        return {\n            name: ShapeSpec(\n                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n            )\n            for name in self._out_features\n        }\n\n\n@BACKBONE_REGISTRY.register()\ndef build_vovnet_backbone(cfg, input_shape):\n    """"""\n    Create a VoVNet instance from config.\n\n    Returns:\n        VoVNet: a :class:`VoVNet` instance.\n    """"""\n    out_features = cfg.MODEL.VOVNET.OUT_FEATURES\n    return VoVNet(cfg, input_shape.channels, out_features=out_features)\n\n\n@BACKBONE_REGISTRY.register()\ndef build_vovnet_fpn_backbone(cfg, input_shape: ShapeSpec):\n    """"""\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    """"""\n    bottom_up = build_vovnet_backbone(cfg, input_shape)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=LastLevelMaxPool(),\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone\n\n\n@BACKBONE_REGISTRY.register()\ndef build_fcos_vovnet_fpn_backbone(cfg, input_shape: ShapeSpec):\n    """"""\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    """"""\n    bottom_up = build_vovnet_backbone(cfg, input_shape)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    top_levels = cfg.MODEL.FCOS.TOP_LEVELS\n    in_channels_top = out_channels\n    if top_levels == 2:\n        top_block = LastLevelP6P7(in_channels_top, out_channels, ""p5"")\n    if top_levels == 1:\n        top_block = LastLevelP6(in_channels_top, out_channels, ""p5"")\n    elif top_levels == 0:\n        top_block = None\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=top_block,\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone'"
centermask/modeling/centermask/__init__.py,0,"b'# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\nfrom .center_heads import CenterROIHeads\nfrom .proposal_utils import (\n\tadd_ground_truth_to_proposals,\n\tadd_ground_truth_to_proposals_single_image\n)\nfrom .sam import SpatialAttentionMaskHead\nfrom .pooler import ROIPooler\nfrom. mask_head import build_mask_head, mask_rcnn_loss, mask_rcnn_inference\nfrom .maskiou_head import build_maskiou_head, mask_iou_loss, mask_iou_inference\nfrom .keypoint_head import build_keypoint_head, keypoint_rcnn_loss, keypoint_rcnn_inference'"
centermask/modeling/centermask/center_heads.py,12,"b'# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\nimport torch\nfrom torch import nn\nfrom typing import Dict, List, Optional, Tuple, Union\nimport numpy as np\n\nfrom detectron2.modeling.roi_heads import (\n    ROI_HEADS_REGISTRY,\n)\nfrom detectron2.structures import Boxes, Instances, pairwise_iou, ImageList\nfrom detectron2.utils.events import get_event_storage\nfrom detectron2.modeling.matcher import Matcher\nfrom detectron2.modeling.sampling import subsample_labels\nfrom detectron2.layers import ShapeSpec\n# from detectron2.modeling.roi_heads.keypoint_head import build_keypoint_head\nfrom .keypoint_head import build_keypoint_head\n\n\nfrom .mask_head import build_mask_head, mask_rcnn_loss, mask_rcnn_inference\nfrom .maskiou_head import build_maskiou_head, mask_iou_loss, mask_iou_inference\nfrom .proposal_utils import add_ground_truth_to_proposals\nfrom .pooler import ROIPooler\n\n\n\n__all__ = [""CenterROIHeads""]\n\n\ndef select_foreground_proposals(proposals, bg_label):\n    """"""\n    Given a list of N Instances (for N images), each containing a `gt_classes` field,\n    return a list of Instances that contain only instances with `gt_classes != -1 &&\n    gt_classes != bg_label`.\n\n    Args:\n        proposals (list[Instances]): A list of N Instances, where N is the number of\n            images in the batch.\n        bg_label: label index of background class.\n\n    Returns:\n        list[Instances]: N Instances, each contains only the selected foreground instances.\n        list[Tensor]: N boolean vector, correspond to the selection mask of\n            each Instances object. True for selected instances.\n    """"""\n    assert isinstance(proposals, (list, tuple))\n    assert isinstance(proposals[0], Instances)\n    assert proposals[0].has(""gt_classes"")\n    fg_proposals = []\n    fg_selection_masks = []\n    for proposals_per_image in proposals:\n        gt_classes = proposals_per_image.gt_classes\n        fg_selection_mask = (gt_classes != -1) & (gt_classes != bg_label)\n        fg_idxs = fg_selection_mask.nonzero().squeeze(1)\n        fg_proposals.append(proposals_per_image[fg_idxs])\n        fg_selection_masks.append(fg_selection_mask)\n    return fg_proposals, fg_selection_masks\n\n\ndef select_proposals_with_visible_keypoints(proposals: List[Instances]) -> List[Instances]:\n    """"""\n    Args:\n        proposals (list[Instances]): a list of N Instances, where N is the\n            number of images.\n\n    Returns:\n        proposals: only contains proposals with at least one visible keypoint.\n\n    Note that this is still slightly different from Detectron.\n    In Detectron, proposals for training keypoint head are re-sampled from\n    all the proposals with IOU>threshold & >=1 visible keypoint.\n\n    Here, the proposals are first sampled from all proposals with\n    IOU>threshold, then proposals with no visible keypoint are filtered out.\n    This strategy seems to make no difference on Detectron and is easier to implement.\n    """"""\n    ret = []\n    all_num_fg = []\n    for proposals_per_image in proposals:\n        # If empty/unannotated image (hard negatives), skip filtering for train\n        if len(proposals_per_image) == 0:\n            ret.append(proposals_per_image)\n            continue\n        gt_keypoints = proposals_per_image.gt_keypoints.tensor\n        # #fg x K x 3\n        vis_mask = gt_keypoints[:, :, 2] >= 1\n        xs, ys = gt_keypoints[:, :, 0], gt_keypoints[:, :, 1]\n        proposal_boxes = proposals_per_image.proposal_boxes.tensor.unsqueeze(dim=1)  # #fg x 1 x 4\n        kp_in_box = (\n            (xs >= proposal_boxes[:, :, 0])\n            & (xs <= proposal_boxes[:, :, 2])\n            & (ys >= proposal_boxes[:, :, 1])\n            & (ys <= proposal_boxes[:, :, 3])\n        )\n        selection = (kp_in_box & vis_mask).any(dim=1)\n        selection_idxs = torch.nonzero(selection).squeeze(1)\n        all_num_fg.append(selection_idxs.numel())\n        ret.append(proposals_per_image[selection_idxs])\n\n    storage = get_event_storage()\n    storage.put_scalar(""keypoint_head/num_fg_samples"", np.mean(all_num_fg))\n    return ret\n\n\nclass ROIHeads(nn.Module):\n    """"""\n    ROIHeads perform all per-region computation in an R-CNN.\n\n    It contains logic of cropping the regions, extract per-region features,\n    and make per-region predictions.\n\n    It can have many variants, implemented as subclasses of this class.\n    """"""\n\n    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):\n        super(ROIHeads, self).__init__()\n\n        # fmt: off\n        self.batch_size_per_image     = cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE\n        self.positive_sample_fraction = cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION\n        self.test_score_thresh        = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n        self.test_nms_thresh          = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n        self.test_detections_per_img  = cfg.TEST.DETECTIONS_PER_IMAGE\n        self.in_features              = cfg.MODEL.ROI_HEADS.IN_FEATURES\n        self.num_classes              = cfg.MODEL.ROI_HEADS.NUM_CLASSES\n        self.proposal_append_gt       = cfg.MODEL.ROI_HEADS.PROPOSAL_APPEND_GT\n        self.feature_strides          = {k: v.stride for k, v in input_shape.items()}\n        self.feature_channels         = {k: v.channels for k, v in input_shape.items()}\n        # fmt: on\n\n        # Matcher to assign box proposals to gt boxes\n        self.proposal_matcher = Matcher(\n            cfg.MODEL.ROI_HEADS.IOU_THRESHOLDS,\n            cfg.MODEL.ROI_HEADS.IOU_LABELS,\n            allow_low_quality_matches=False,\n        )\n\n    def _sample_proposals(self, matched_idxs, matched_labels, gt_classes):\n        """"""\n        Based on the matching between N proposals and M groundtruth,\n        sample the proposals and set their classification labels.\n\n        Args:\n            matched_idxs (Tensor): a vector of length N, each is the best-matched\n                gt index in [0, M) for each proposal.\n            matched_labels (Tensor): a vector of length N, the matcher\'s label\n                (one of cfg.MODEL.ROI_HEADS.IOU_LABELS) for each proposal.\n            gt_classes (Tensor): a vector of length M.\n\n        Returns:\n            Tensor: a vector of indices of sampled proposals. Each is in [0, N).\n            Tensor: a vector of the same length, the classification label for\n                each sampled proposal. Each sample is labeled as either a category in\n                [0, num_classes) or the background (num_classes).\n        """"""\n        has_gt = gt_classes.numel() > 0\n        # Get the corresponding GT for each proposal\n        if has_gt:\n            gt_classes = gt_classes[matched_idxs]\n            # Label unmatched proposals (0 label from matcher) as background (label=num_classes)\n            gt_classes[matched_labels == 0] = self.num_classes\n            # Label ignore proposals (-1 label)\n            gt_classes[matched_labels == -1] = -1\n        else:\n            gt_classes = torch.zeros_like(matched_idxs) + self.num_classes\n\n        sampled_fg_idxs, sampled_bg_idxs = subsample_labels(\n            gt_classes, self.batch_size_per_image, self.positive_sample_fraction, self.num_classes\n        )\n\n        sampled_idxs = torch.cat([sampled_fg_idxs, sampled_bg_idxs], dim=0)\n        return sampled_idxs, gt_classes[sampled_idxs]\n\n    @torch.no_grad()\n    def label_and_sample_proposals(self, proposals, targets):\n        """"""\n        Prepare some proposals to be used to train the ROI heads.\n        It performs box matching between `proposals` and `targets`, and assigns\n        training labels to the proposals.\n        It returns ``self.batch_size_per_image`` random samples from proposals and groundtruth\n        boxes, with a fraction of positives that is no larger than\n        ``self.positive_sample_fraction``.\n\n        Args:\n            See :meth:`ROIHeads.forward`\n\n        Returns:\n            list[Instances]:\n                length `N` list of `Instances`s containing the proposals\n                sampled for training. Each `Instances` has the following fields:\n\n                - proposal_boxes: the proposal boxes\n                - gt_boxes: the ground-truth box that the proposal is assigned to\n                  (this is only meaningful if the proposal has a label > 0; if label = 0\n                  then the ground-truth box is random)\n\n                Other fields such as ""gt_classes"", ""gt_masks"", that\'s included in `targets`.\n        """"""\n        # ywlee for using targets.gt_classes\n        # in add_ground_truth_to_proposal()\n        # gt_boxes = [x.gt_boxes for x in targets]\n\n        # Augment proposals with ground-truth boxes.\n        # In the case of learned proposals (e.g., RPN), when training starts\n        # the proposals will be low quality due to random initialization.\n        # It\'s possible that none of these initial\n        # proposals have high enough overlap with the gt objects to be used\n        # as positive examples for the second stage components (box head,\n        # cls head, mask head). Adding the gt boxes to the set of proposals\n        # ensures that the second stage components will have some positive\n        # examples from the start of training. For RPN, this augmentation improves\n        # convergence and empirically improves box AP on COCO by about 0.5\n        # points (under one tested configuration).\n        if self.proposal_append_gt:\n            proposals = add_ground_truth_to_proposals(targets, proposals)\n\n        proposals_with_gt = []\n\n        num_fg_samples = []\n        num_bg_samples = []\n        for proposals_per_image, targets_per_image in zip(proposals, targets):\n            has_gt = len(targets_per_image) > 0\n            match_quality_matrix = pairwise_iou(\n                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes\n            )\n            matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)\n            sampled_idxs, gt_classes = self._sample_proposals(\n                matched_idxs, matched_labels, targets_per_image.gt_classes\n            )\n\n            # Set target attributes of the sampled proposals:\n            proposals_per_image = proposals_per_image[sampled_idxs]\n            proposals_per_image.gt_classes = gt_classes\n\n            # We index all the attributes of targets that start with ""gt_""\n            # and have not been added to proposals yet (=""gt_classes"").\n            if has_gt:\n                sampled_targets = matched_idxs[sampled_idxs]\n                # NOTE: here the indexing waste some compute, because heads\n                # like masks, keypoints, etc, will filter the proposals again,\n                # (by foreground/background, or number of keypoints in the image, etc)\n                # so we essentially index the data twice.\n                for (trg_name, trg_value) in targets_per_image.get_fields().items():\n                    if trg_name.startswith(""gt_"") and not proposals_per_image.has(trg_name):\n                        proposals_per_image.set(trg_name, trg_value[sampled_targets])\n            else:\n                gt_boxes = Boxes(\n                    targets_per_image.gt_boxes.tensor.new_zeros((len(sampled_idxs), 4))\n                )\n                proposals_per_image.gt_boxes = gt_boxes\n\n            num_bg_samples.append((gt_classes == self.num_classes).sum().item())\n            num_fg_samples.append(gt_classes.numel() - num_bg_samples[-1])\n            proposals_with_gt.append(proposals_per_image)\n\n        # Log the number of fg/bg samples that are selected for training ROI heads\n        storage = get_event_storage()\n        storage.put_scalar(""roi_head/num_fg_samples"", np.mean(num_fg_samples))\n        storage.put_scalar(""roi_head/num_bg_samples"", np.mean(num_bg_samples))\n\n        return proposals_with_gt\n\n    def forward(self, images, features, proposals, targets=None):\n        """"""\n        Args:\n            images (ImageList):\n            features (dict[str: Tensor]): input data as a mapping from feature\n                map name to tensor. Axis 0 represents the number of images `N` in\n                the input data; axes 1-3 are channels, height, and width, which may\n                vary between feature maps (e.g., if a feature pyramid is used).\n            proposals (list[Instances]): length `N` list of `Instances`s. The i-th\n                `Instances` contains object proposals for the i-th input image,\n                with fields ""proposal_boxes"" and ""objectness_logits"".\n            targets (list[Instances], optional): length `N` list of `Instances`s. The i-th\n                `Instances` contains the ground-truth per-instance annotations\n                for the i-th input image.  Specify `targets` during training only.\n                It may have the following fields:\n\n                - gt_boxes: the bounding box of each instance.\n                - gt_classes: the label for each instance with a category ranging in [0, #class].\n                - gt_masks: PolygonMasks or BitMasks, the ground-truth masks of each instance.\n                - gt_keypoints: NxKx3, the groud-truth keypoints for each instance.\n\n        Returns:\n            results (list[Instances]): length `N` list of `Instances`s containing the\n            detected instances. Returned during inference only; may be [] during training.\n\n            losses (dict[str->Tensor]):\n            mapping from a named loss to a tensor storing the loss. Used during training only.\n        """"""\n        raise NotImplementedError()\n\n\n\n\n@ROI_HEADS_REGISTRY.register()\nclass CenterROIHeads(ROIHeads):\n    """"""\n    It\'s ""standard"" in a sense that there is no ROI transform sharing\n    or feature sharing between tasks.\n    The cropped rois go to separate branches  masks directly.\n    This way, it is easier to make separate abstractions for different branches.\n\n    This class is used by most models, such as FPN and C5.\n    To implement more models, you can subclass it and implement a different\n    :meth:`forward()` or a head.\n    """"""\n\n    def __init__(self, cfg, input_shape):\n        super(CenterROIHeads, self).__init__(cfg, input_shape)\n        self._init_mask_head(cfg)\n        self._init_mask_iou_head(cfg)\n        self._init_keypoint_head(cfg, input_shape)\n\n\n    def _init_mask_head(self, cfg):\n        # fmt: off\n        self.mask_on           = cfg.MODEL.MASK_ON\n        if not self.mask_on:\n            return\n        pooler_resolution = cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\n        pooler_scales     = tuple(1.0 / self.feature_strides[k] for k in self.in_features)\n        sampling_ratio    = cfg.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO\n        pooler_type       = cfg.MODEL.ROI_MASK_HEAD.POOLER_TYPE\n        assign_crit       = cfg.MODEL.ROI_MASK_HEAD.ASSIGN_CRITERION\n\n        # fmt: on\n\n        in_channels = [self.feature_channels[f] for f in self.in_features][0]\n\n        self.mask_pooler = ROIPooler(\n            output_size=pooler_resolution,\n            scales=pooler_scales,\n            sampling_ratio=sampling_ratio,\n            pooler_type=pooler_type,\n            assign_crit=assign_crit,\n        )\n        self.mask_head = build_mask_head(\n            cfg, ShapeSpec(channels=in_channels, width=pooler_resolution, height=pooler_resolution)\n        )\n\n\n    def _init_mask_iou_head(self, cfg):\n        # fmt: off\n        self.maskiou_on     = cfg.MODEL.MASKIOU_ON\n        if not self.maskiou_on:\n            return\n        in_channels         = cfg.MODEL.ROI_MASK_HEAD.CONV_DIM\n        pooler_resolution   = cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\n        self.maskiou_weight = cfg.MODEL.MASKIOU_LOSS_WEIGHT\n\n        # fmt : on\n\n        self.maskiou_head = build_maskiou_head(\n            cfg, ShapeSpec(channels=in_channels, width=pooler_resolution, height=pooler_resolution)\n        )\n\n\n    def _init_keypoint_head(self, cfg, input_shape):\n        # fmt: off\n        self.keypoint_on  = cfg.MODEL.KEYPOINT_ON\n        if not self.keypoint_on:\n            return\n        self.kp_in_features = cfg.MODEL.ROI_KEYPOINT_HEAD.IN_FEATURES\n        pooler_resolution   = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_RESOLUTION\n        pooler_scales       = tuple(1.0 / input_shape[k].stride for k in self.kp_in_features)  # noqa\n        sampling_ratio      = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_SAMPLING_RATIO\n        pooler_type         = cfg.MODEL.ROI_KEYPOINT_HEAD.POOLER_TYPE\n        assign_crit         = cfg.MODEL.ROI_KEYPOINT_HEAD.ASSIGN_CRITERION\n        # fmt: on\n\n        in_channels = [input_shape[f].channels for f in self.kp_in_features][0]\n\n        self.keypoint_pooler = ROIPooler(\n            output_size=pooler_resolution,\n            scales=pooler_scales,\n            sampling_ratio=sampling_ratio,\n            pooler_type=pooler_type,\n            assign_crit=assign_crit,\n        )\n        self.keypoint_head = build_keypoint_head(\n            cfg, ShapeSpec(channels=in_channels, width=pooler_resolution, height=pooler_resolution)\n        )\n\n    def forward(\n        self,\n        images: ImageList,\n        features: Dict[str, torch.Tensor],\n        proposals: List[Instances],\n        targets: Optional[List[Instances]] = None,\n    ) -> Tuple[List[Instances], Dict[str, torch.Tensor]]:\n        """"""\n        See :class:`ROIHeads.forward`.\n        """"""\n        del images\n        if self.training:\n            proposals = self.label_and_sample_proposals(proposals, targets)\n        del targets\n\n        if self.training:\n            if self.maskiou_on:\n                losses, mask_features, selected_mask, labels, maskiou_targets = self._forward_mask(features, proposals)\n                losses.update(self._forward_maskiou(mask_features, proposals, selected_mask, labels, maskiou_targets))\n            else:\n                losses = self._forward_mask(features, proposals)\n            losses.update(self._forward_keypoint(features, proposals))\n            return proposals, losses\n        else:\n            # During inference cascaded prediction is used: the mask and keypoints heads are only\n            # applied to the top scoring box detections.\n            pred_instances = self.forward_with_given_boxes(features, proposals)\n            return pred_instances, {}\n\n    def forward_with_given_boxes(\n        self, features: Dict[str, torch.Tensor], instances: List[Instances]\n    ) -> List[Instances]:\n        """"""\n        Use the given boxes in `instances` to produce other (non-box) per-ROI outputs.\n\n        This is useful for downstream tasks where a box is known, but need to obtain\n        other attributes (outputs of other heads).\n        Test-time augmentation also uses this.\n\n        Args:\n            features: same as in `forward()`\n            instances (list[Instances]): instances to predict other outputs. Expect the keys\n                ""pred_boxes"" and ""pred_classes"" to exist.\n\n        Returns:\n            instances (list[Instances]):\n                the same `Instances` objects, with extra\n                fields such as `pred_masks` or `pred_keypoints`.\n        """"""\n        assert not self.training\n        assert instances[0].has(""pred_boxes"") and instances[0].has(""pred_classes"")\n\n        if self.maskiou_on:\n            instances, mask_features = self._forward_mask(features, instances)\n            instances = self._forward_maskiou(mask_features, instances)\n        else:\n            instances = self._forward_mask(features, instances)\n\n        instances = self._forward_keypoint(features, instances)\n\n        return instances\n\n\n    def _forward_mask(\n        self, features: Dict[str, torch.Tensor], instances: List[Instances]\n    ) -> Union[Dict[str, torch.Tensor], List[Instances]]:\n        """"""\n        Forward logic of the mask prediction branch.\n\n        Args:\n            features (dict[str, Tensor]): mapping from feature map names to tensor.\n                Same as in :meth:`ROIHeads.forward`.\n            instances (list[Instances]): the per-image instances to train/predict masks.\n                In training, they can be the proposals.\n                In inference, they can be the predicted boxes.\n\n        Returns:\n            In training, a dict of losses.\n            In inference, update `instances` with new fields ""pred_masks"" and return it.\n        """"""\n        if not self.mask_on:\n            return {} if self.training else instances\n\n        features = [features[f] for f in self.in_features]\n\n        if self.training:\n            # The loss is only defined on positive proposals.\n            proposals, _ = select_foreground_proposals(instances, self.num_classes)\n            # proposal_boxes = [x.proposal_boxes for x in proposals]\n            mask_features = self.mask_pooler(features, proposals, self.training)\n            mask_logits = self.mask_head(mask_features)\n            if self.maskiou_on:\n                loss, selected_mask, labels, maskiou_targets = mask_rcnn_loss(mask_logits, proposals, self.maskiou_on)\n                return {""loss_mask"": loss}, mask_features, selected_mask, labels, maskiou_targets\n            else:\n                return {""loss_mask"": mask_rcnn_loss(mask_logits, proposals, self.maskiou_on)}\n        else:\n            # pred_boxes = [x.pred_boxes for x in instances]\n            mask_features = self.mask_pooler(features, instances)\n            mask_logits = self.mask_head(mask_features)\n            mask_rcnn_inference(mask_logits, instances)\n\n            if self.maskiou_on:\n                return instances, mask_features\n            else:\n                return instances\n\n\n    def _forward_maskiou(self, mask_features, instances, selected_mask=None, labels=None, maskiou_targets=None):\n        """"""\n        Forward logic of the mask iou prediction branch.\n        Args:\n            features (list[Tensor]): #level input features for mask prediction\n            instances (list[Instances]): the per-image instances to train/predict masks.\n                In training, they can be the proposals.\n                In inference, they can be the predicted boxes.\n        Returns:\n            In training, a dict of losses.\n            In inference, calibrate instances\' scores.\n        """"""\n        if not self.maskiou_on:\n            return {} if self.training else instances\n\n        if self.training:\n            pred_maskiou = self.maskiou_head(mask_features, selected_mask)\n            return {""loss_maskiou"": mask_iou_loss(labels, pred_maskiou, maskiou_targets, self.maskiou_weight)}\n\n        else:\n            selected_mask = torch.cat([i.pred_masks for i in instances], 0)\n            if selected_mask.shape[0] == 0:\n                return instances\n            pred_maskiou = self.maskiou_head(mask_features, selected_mask)\n            mask_iou_inference(instances, pred_maskiou)\n            return instances\n\n\n    def _forward_keypoint(\n        self, features: Dict[str, torch.Tensor], instances: List[Instances]\n    ) -> Union[Dict[str, torch.Tensor], List[Instances]]:\n        """"""\n        Forward logic of the keypoint prediction branch.\n\n        Args:\n            features (dict[str, Tensor]): mapping from feature map names to tensor.\n                Same as in :meth:`ROIHeads.forward`.\n            instances (list[Instances]): the per-image instances to train/predict keypoints.\n                In training, they can be the proposals.\n                In inference, they can be the predicted boxes.\n\n        Returns:\n            In training, a dict of losses.\n            In inference, update `instances` with new fields ""pred_keypoints"" and return it.\n        """"""\n        if not self.keypoint_on:\n            return {} if self.training else instances\n\n        features = [features[f] for f in self.kp_in_features]\n\n        if self.training:\n            # The loss is defined on positive proposals with at >=1 visible keypoints.\n            proposals, _ = select_foreground_proposals(instances, self.num_classes)\n            proposals = select_proposals_with_visible_keypoints(proposals)\n            # proposal_boxes = [x.proposal_boxes for x in proposals]\n\n            keypoint_features = self.keypoint_pooler(features, proposals, self.training)\n            return self.keypoint_head(keypoint_features, proposals)\n        else:\n            # pred_boxes = [x.pred_boxes for x in instances]\n            keypoint_features = self.keypoint_pooler(features, instances)\n            return self.keypoint_head(keypoint_features, instances)\n'"
centermask/modeling/centermask/keypoint_head.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nfrom typing import List\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom detectron2.layers import Conv2d, ConvTranspose2d, ShapeSpec, cat, interpolate\nfrom detectron2.structures import Instances, heatmaps_to_keypoints\nfrom detectron2.utils.events import get_event_storage\nfrom detectron2.utils.registry import Registry\n\n_TOTAL_SKIPPED = 0\n\nROI_KEYPOINT_HEAD_REGISTRY = Registry(""ROI_KEYPOINT_HEAD"")\nROI_KEYPOINT_HEAD_REGISTRY.__doc__ = """"""\nRegistry for keypoint heads, which make keypoint predictions from per-region features.\n\nThe registered object will be called with `obj(cfg, input_shape)`.\n""""""\n\n\ndef build_keypoint_head(cfg, input_shape):\n    """"""\n    Build a keypoint head from `cfg.MODEL.ROI_KEYPOINT_HEAD.NAME`.\n    """"""\n    name = cfg.MODEL.ROI_KEYPOINT_HEAD.NAME\n    return ROI_KEYPOINT_HEAD_REGISTRY.get(name)(cfg, input_shape)\n\n\ndef keypoint_rcnn_loss(pred_keypoint_logits, instances, normalizer):\n    """"""\n    Arguments:\n        pred_keypoint_logits (Tensor): A tensor of shape (N, K, S, S) where N is the total number\n            of instances in the batch, K is the number of keypoints, and S is the side length\n            of the keypoint heatmap. The values are spatial logits.\n        instances (list[Instances]): A list of M Instances, where M is the batch size.\n            These instances are predictions from the model\n            that are in 1:1 correspondence with pred_keypoint_logits.\n            Each Instances should contain a `gt_keypoints` field containing a `structures.Keypoint`\n            instance.\n        normalizer (float): Normalize the loss by this amount.\n            If not specified, we normalize by the number of visible keypoints in the minibatch.\n\n    Returns a scalar tensor containing the loss.\n    """"""\n    heatmaps = []\n    valid = []\n\n    keypoint_side_len = pred_keypoint_logits.shape[2]\n    for instances_per_image in instances:\n        if len(instances_per_image) == 0:\n            continue\n        keypoints = instances_per_image.gt_keypoints\n        heatmaps_per_image, valid_per_image = keypoints.to_heatmap(\n            instances_per_image.proposal_boxes.tensor, keypoint_side_len\n        )\n        heatmaps.append(heatmaps_per_image.view(-1))\n        valid.append(valid_per_image.view(-1))\n\n    if len(heatmaps):\n        keypoint_targets = cat(heatmaps, dim=0)\n        valid = cat(valid, dim=0).to(dtype=torch.uint8)\n        valid = torch.nonzero(valid).squeeze(1)\n\n    # torch.mean (in binary_cross_entropy_with_logits) doesn\'t\n    # accept empty tensors, so handle it separately\n    if len(heatmaps) == 0 or valid.numel() == 0:\n        global _TOTAL_SKIPPED\n        _TOTAL_SKIPPED += 1\n        storage = get_event_storage()\n        storage.put_scalar(""kpts_num_skipped_batches"", _TOTAL_SKIPPED, smoothing_hint=False)\n        return pred_keypoint_logits.sum() * 0\n\n    N, K, H, W = pred_keypoint_logits.shape\n    pred_keypoint_logits = pred_keypoint_logits.view(N * K, H * W)\n\n    keypoint_loss = F.cross_entropy(\n        pred_keypoint_logits[valid], keypoint_targets[valid], reduction=""sum""\n    )\n\n    # If a normalizer isn\'t specified, normalize by the number of visible keypoints in the minibatch\n    if normalizer is None:\n        normalizer = valid.numel()\n    keypoint_loss /= normalizer\n\n    return keypoint_loss\n\n\ndef keypoint_rcnn_inference(pred_keypoint_logits, pred_instances):\n    """"""\n    Post process each predicted keypoint heatmap in `pred_keypoint_logits` into (x, y, score)\n        and add it to the `pred_instances` as a `pred_keypoints` field.\n\n    Args:\n        pred_keypoint_logits (Tensor): A tensor of shape (R, K, S, S) where R is the total number\n           of instances in the batch, K is the number of keypoints, and S is the side length of\n           the keypoint heatmap. The values are spatial logits.\n        pred_instances (list[Instances]): A list of N Instances, where N is the number of images.\n\n    Returns:\n        None. Each element in pred_instances will contain an extra ""pred_keypoints"" field.\n            The field is a tensor of shape (#instance, K, 3) where the last\n            dimension corresponds to (x, y, score).\n            The scores are larger than 0.\n    """"""\n    # flatten all bboxes from all images together (list[Boxes] -> Rx4 tensor)\n    bboxes_flat = cat([b.pred_boxes.tensor for b in pred_instances], dim=0)\n\n    keypoint_results = heatmaps_to_keypoints(pred_keypoint_logits.detach(), bboxes_flat.detach())\n    num_instances_per_image = [len(i) for i in pred_instances]\n    keypoint_results = keypoint_results[:, :, [0, 1, 3]].split(num_instances_per_image, dim=0)\n\n    for keypoint_results_per_image, instances_per_image in zip(keypoint_results, pred_instances):\n        # keypoint_results_per_image is (num instances)x(num keypoints)x(x, y, score)\n        instances_per_image.pred_keypoints = keypoint_results_per_image\n\n\nclass BaseKeypointRCNNHead(nn.Module):\n    """"""\n    Implement the basic Keypoint R-CNN losses and inference logic.\n    """"""\n\n    def __init__(self, cfg, input_shape):\n        super().__init__()\n        # fmt: off\n        self.loss_weight                    = cfg.MODEL.ROI_KEYPOINT_HEAD.LOSS_WEIGHT\n        self.normalize_by_visible_keypoints = cfg.MODEL.ROI_KEYPOINT_HEAD.NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS  # noqa\n        self.num_keypoints                  = cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS\n        batch_size_per_image                = cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE\n        positive_sample_fraction            = cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION\n        # fmt: on\n        self.normalizer_per_img = (\n            self.num_keypoints * batch_size_per_image * positive_sample_fraction\n        )\n\n    def forward(self, x, instances: List[Instances]):\n        """"""\n        Args:\n            x: input region feature(s) provided by :class:`ROIHeads`.\n            instances (list[Instances]): contains the boxes & labels corresponding\n                to the input features.\n                Exact format is up to its caller to decide.\n                Typically, this is the foreground instances in training, with\n                ""proposal_boxes"" field and other gt annotations.\n                In inference, it contains boxes that are already predicted.\n\n        Returns:\n            A dict of losses if in training. The predicted ""instances"" if in inference.\n        """"""\n        x = self.layers(x)\n        if self.training:\n            num_images = len(instances)\n            normalizer = (\n                None\n                if self.normalize_by_visible_keypoints\n                else num_images * self.normalizer_per_img\n            )\n            return {\n                ""loss_keypoint"": keypoint_rcnn_loss(x, instances, normalizer=normalizer)\n                * self.loss_weight\n            }\n        else:\n            keypoint_rcnn_inference(x, instances)\n            return instances\n\n    def layers(self, x):\n        """"""\n        Neural network layers that makes predictions from regional input features.\n        """"""\n        raise NotImplementedError\n\n\n@ROI_KEYPOINT_HEAD_REGISTRY.register()\nclass KRCNNConvDeconvUpsampleHead(BaseKeypointRCNNHead):\n    """"""\n    A standard keypoint head containing a series of 3x3 convs, followed by\n    a transpose convolution and bilinear interpolation for upsampling.\n    """"""\n\n    def __init__(self, cfg, input_shape: ShapeSpec):\n        """"""\n        The following attributes are parsed from config:\n            conv_dims: an iterable of output channel counts for each conv in the head\n                         e.g. (512, 512, 512) for three convs outputting 512 channels.\n            num_keypoints: number of keypoint heatmaps to predicts, determines the number of\n                           channels in the final output.\n        """"""\n        super().__init__(cfg, input_shape)\n\n        # fmt: off\n        # default up_scale to 2 (this can eventually be moved to config)\n        up_scale      = 2\n        conv_dims     = cfg.MODEL.ROI_KEYPOINT_HEAD.CONV_DIMS\n        num_keypoints = cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS\n        in_channels   = input_shape.channels\n        # fmt: on\n\n        self.blocks = []\n        for idx, layer_channels in enumerate(conv_dims, 1):\n            module = Conv2d(in_channels, layer_channels, 3, stride=1, padding=1)\n            self.add_module(""conv_fcn{}"".format(idx), module)\n            self.blocks.append(module)\n            in_channels = layer_channels\n\n        deconv_kernel = 4\n        self.score_lowres = ConvTranspose2d(\n            in_channels, num_keypoints, deconv_kernel, stride=2, padding=deconv_kernel // 2 - 1\n        )\n        self.up_scale = up_scale\n\n        for name, param in self.named_parameters():\n            if ""bias"" in name:\n                nn.init.constant_(param, 0)\n            elif ""weight"" in name:\n                # Caffe2 implementation uses MSRAFill, which in fact\n                # corresponds to kaiming_normal_ in PyTorch\n                nn.init.kaiming_normal_(param, mode=""fan_out"", nonlinearity=""relu"")\n\n    def layers(self, x):\n        for layer in self.blocks:\n            x = F.relu(layer(x))\n        x = self.score_lowres(x)\n        x = interpolate(x, scale_factor=self.up_scale, mode=""bilinear"", align_corners=False)\n        return x\n'"
centermask/modeling/centermask/mask_head.py,19,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# Modified by Sangrok Lee and Youngwan Lee (ETRI), 2020. All Rights Reserved.\nimport fvcore.nn.weight_init as weight_init\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport numpy as np\nfrom typing import List\nimport copy\nimport pycocotools.mask as mask_utils\n\nfrom detectron2.layers import Conv2d, ConvTranspose2d, ShapeSpec, cat, get_norm\nfrom detectron2.utils.events import get_event_storage\nfrom detectron2.utils.registry import Registry\nfrom detectron2.structures.masks import PolygonMasks\n\nROI_MASK_HEAD_REGISTRY = Registry(""ROI_MASK_HEAD"")\nROI_MASK_HEAD_REGISTRY.__doc__ = """"""\nRegistry for mask heads, which predicts instance masks given\nper-region features.\n\nThe registered object will be called with `obj(cfg, input_shape)`.\n""""""\n\n\ndef crop(polygons: List[List[np.ndarray]], boxes: torch.Tensor) -> ""PolygonMasks"":\n    boxes = boxes.to(torch.device(""cpu"")).numpy()\n    results = [\n        _crop(polygon, box) for polygon, box in zip(polygons, boxes)\n    ]\n\n    return PolygonMasks(results)\n\n\ndef _crop(polygons: np.ndarray, box: np.ndarray) -> List[np.ndarray]:\n    w, h = box[2] - box[0], box[3] - box[1]\n\n    polygons = copy.deepcopy(polygons)\n    for p in polygons:\n        p[0::2] = p[0::2] - box[0]  # .clamp(min=0, max=w)\n        p[1::2] = p[1::2] - box[1]  # .clamp(min=0, max=h)\n\n    return polygons\n\n\ndef mask_rcnn_loss(pred_mask_logits, instances, maskiou_on):\n    """"""\n    Compute the mask prediction loss defined in the Mask R-CNN paper.\n\n    Args:\n        pred_mask_logits (Tensor): A tensor of shape (B, C, Hmask, Wmask) or (B, 1, Hmask, Wmask)\n            for class-specific or class-agnostic, where B is the total number of predicted masks\n            in all images, C is the number of foreground classes, and Hmask, Wmask are the height\n            and width of the mask predictions. The values are logits.\n        instances (list[Instances]): A list of N Instances, where N is the number of images\n            in the batch. These instances are in 1:1\n            correspondence with the pred_mask_logits. The ground-truth labels (class, box, mask,\n            ...) associated with each instance are stored in fields.\n\n    Returns:\n        mask_loss (Tensor): A scalar tensor containing the loss.\n    """"""\n    cls_agnostic_mask = pred_mask_logits.size(1) == 1\n    total_num_masks = pred_mask_logits.size(0)\n    mask_side_len = pred_mask_logits.size(2)\n    assert pred_mask_logits.size(2) == pred_mask_logits.size(3), ""Mask prediction must be square!""\n\n    gt_classes = []\n    gt_masks = []\n    mask_ratios = []\n    for instances_per_image in instances:\n        if len(instances_per_image) == 0:\n            continue\n\n        if not cls_agnostic_mask:\n            gt_classes_per_image = instances_per_image.gt_classes.to(dtype=torch.int64)\n            gt_classes.append(gt_classes_per_image)\n\n        if maskiou_on:\n            cropped_mask = crop(instances_per_image.gt_masks.polygons, instances_per_image.proposal_boxes.tensor)\n            cropped_mask = torch.tensor(\n                [mask_utils.area(mask_utils.frPyObjects([p for p in obj], box[3]-box[1], box[2]-box[0])).sum().astype(float)\n                for obj, box in zip(cropped_mask.polygons, instances_per_image.proposal_boxes.tensor)]\n                )\n                \n            mask_ratios.append(\n                (cropped_mask / instances_per_image.gt_masks.area())\n                .to(device=pred_mask_logits.device).clamp(min=0., max=1.)\n            )\n        \n        gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(\n            instances_per_image.proposal_boxes.tensor, mask_side_len\n        ).to(device=pred_mask_logits.device)\n        # A tensor of shape (N, M, M), N=#instances in the image; M=mask_side_len\n        gt_masks.append(gt_masks_per_image)\n\n    #gt_classes = cat(gt_classes, dim=0)\n\n    if len(gt_masks) == 0:\n        gt_classes = torch.LongTensor(gt_classes)\n        if maskiou_on:\n            selected_index = torch.arange(pred_mask_logits.shape[0], device=pred_mask_logits.device)\n            if cls_agnostic_mask:\n                selected_mask = pred_mask_logits[:, 0]\n            else:\n                # gt_classes = torch.LongTensor(gt_classes)\n                selected_mask = pred_mask_logits[selected_index, gt_classes]\n            mask_num, mask_h, mask_w = selected_mask.shape\n            selected_mask = selected_mask.reshape(mask_num, 1, mask_h, mask_w)\n            return pred_mask_logits.sum() * 0, selected_mask, gt_classes, None\n        \n        else:\n            return pred_mask_logits.sum() * 0\n\n    gt_masks = cat(gt_masks, dim=0)\n\n    if cls_agnostic_mask:\n        pred_mask_logits = pred_mask_logits[:, 0]\n        gt_classes = torch.zeros(total_num_masks, dtype=torch.int64)\n    else:\n        indices = torch.arange(total_num_masks)\n        gt_classes = cat(gt_classes, dim=0) #ywlee\n        pred_mask_logits = pred_mask_logits[indices, gt_classes] # (num_mask, Hmask, Wmask)\n\n    if gt_masks.dtype == torch.bool:\n        gt_masks_bool = gt_masks\n    else:\n        # Here we allow gt_masks to be float as well (depend on the implementation of rasterize())\n        gt_masks_bool = gt_masks > 0.5\n\n    # Log the training accuracy (using gt classes and 0.5 threshold)\n    mask_incorrect = (pred_mask_logits > 0.0) != gt_masks_bool\n    mask_accuracy = 1 - (mask_incorrect.sum().item() / max(mask_incorrect.numel(), 1.0))\n    num_positive = gt_masks_bool.sum().item()\n    false_positive = (mask_incorrect & ~gt_masks_bool).sum().item() / max(\n        gt_masks_bool.numel() - num_positive, 1.0\n    )\n    false_negative = (mask_incorrect & gt_masks_bool).sum().item() / max(num_positive, 1.0)\n\n    storage = get_event_storage()\n    storage.put_scalar(""mask_rcnn/accuracy"", mask_accuracy)\n    storage.put_scalar(""mask_rcnn/false_positive"", false_positive)\n    storage.put_scalar(""mask_rcnn/false_negative"", false_negative)\n\n    mask_loss = F.binary_cross_entropy_with_logits(\n        pred_mask_logits, gt_masks.to(dtype=torch.float32), reduction=""mean""\n    )\n    \n    if maskiou_on:\n        mask_ratios = cat(mask_ratios, dim=0)\n\n        value_eps = 1e-10 * torch.ones(gt_masks.shape[0], device=gt_masks.device)\n        mask_ratios = torch.max(mask_ratios, value_eps)\n\n        pred_masks = pred_mask_logits > 0\n\n        mask_targets_full_area = gt_masks.sum(dim=[1,2]) / mask_ratios\n\n        mask_ovr_area = (pred_masks * gt_masks).sum(dim=[1,2]).float()\n        mask_union_area = pred_masks.sum(dim=[1,2]) + mask_targets_full_area - mask_ovr_area\n        value_1 = torch.ones(pred_masks.shape[0], device=gt_masks.device)\n        value_0 = torch.zeros(pred_masks.shape[0], device=gt_masks.device)\n        mask_union_area = torch.max(mask_union_area, value_1)\n        mask_ovr_area = torch.max(mask_ovr_area, value_0)\n        maskiou_targets = mask_ovr_area / mask_union_area\n        mask_num, mask_h, mask_w = pred_mask_logits.shape\n        selected_mask = pred_mask_logits.reshape(mask_num, 1, mask_h, mask_w)\n        selected_mask = selected_mask.sigmoid()\n        return mask_loss, selected_mask, gt_classes, maskiou_targets.detach()\n    else:\n        return mask_loss\n\n\ndef mask_rcnn_inference(pred_mask_logits, pred_instances):\n    """"""\n    Convert pred_mask_logits to estimated foreground probability masks while also\n    extracting only the masks for the predicted classes in pred_instances. For each\n    predicted box, the mask of the same class is attached to the instance by adding a\n    new ""pred_masks"" field to pred_instances.\n\n    Args:\n        pred_mask_logits (Tensor): A tensor of shape (B, C, Hmask, Wmask) or (B, 1, Hmask, Wmask)\n            for class-specific or class-agnostic, where B is the total number of predicted masks\n            in all images, C is the number of foreground classes, and Hmask, Wmask are the height\n            and width of the mask predictions. The values are logits.\n        pred_instances (list[Instances]): A list of N Instances, where N is the number of images\n            in the batch. Each Instances must have field ""pred_classes"".\n\n    Returns:\n        None. pred_instances will contain an extra ""pred_masks"" field storing a mask of size (Hmask,\n            Wmask) for predicted class. Note that the masks are returned as a soft (non-quantized)\n            masks the resolution predicted by the network; post-processing steps, such as resizing\n            the predicted masks to the original image resolution and/or binarizing them, is left\n            to the caller.\n    """"""\n    cls_agnostic_mask = pred_mask_logits.size(1) == 1\n\n    if cls_agnostic_mask:\n        mask_probs_pred = pred_mask_logits.sigmoid()\n    else:\n        # Select masks corresponding to the predicted classes\n        num_masks = pred_mask_logits.shape[0]\n        class_pred = cat([i.pred_classes for i in pred_instances])\n        indices = torch.arange(num_masks, device=class_pred.device)\n        mask_probs_pred = pred_mask_logits[indices, class_pred][:, None].sigmoid()\n    # mask_probs_pred.shape: (B, 1, Hmask, Wmask)\n\n    num_boxes_per_image = [len(i) for i in pred_instances]\n    mask_probs_pred = mask_probs_pred.split(num_boxes_per_image, dim=0)\n\n    for prob, instances in zip(mask_probs_pred, pred_instances):\n        instances.pred_masks = prob  # (1, Hmask, Wmask)\n\n\n@ROI_MASK_HEAD_REGISTRY.register()\nclass MaskRCNNConvUpsampleHead(nn.Module):\n    """"""\n    A mask head with several conv layers, plus an upsample layer (with `ConvTranspose2d`).\n    """"""\n\n    def __init__(self, cfg, input_shape: ShapeSpec):\n        """"""\n        The following attributes are parsed from config:\n            num_conv: the number of conv layers\n            conv_dim: the dimension of the conv layers\n            norm: normalization for the conv layers\n        """"""\n        super(MaskRCNNConvUpsampleHead, self).__init__()\n\n        # fmt: off\n        num_classes       = cfg.MODEL.ROI_HEADS.NUM_CLASSES\n        conv_dims         = cfg.MODEL.ROI_MASK_HEAD.CONV_DIM\n        self.norm         = cfg.MODEL.ROI_MASK_HEAD.NORM\n        num_conv          = cfg.MODEL.ROI_MASK_HEAD.NUM_CONV\n        input_channels    = input_shape.channels\n        cls_agnostic_mask = cfg.MODEL.ROI_MASK_HEAD.CLS_AGNOSTIC_MASK\n        # fmt: on\n\n        self.conv_norm_relus = []\n\n        for k in range(num_conv):\n            conv = Conv2d(\n                input_channels if k == 0 else conv_dims,\n                conv_dims,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=not self.norm,\n                norm=get_norm(self.norm, conv_dims),\n                activation=F.relu,\n            )\n            self.add_module(""mask_fcn{}"".format(k + 1), conv)\n            self.conv_norm_relus.append(conv)\n\n        self.deconv = ConvTranspose2d(\n            conv_dims if num_conv > 0 else input_channels,\n            conv_dims,\n            kernel_size=2,\n            stride=2,\n            padding=0,\n        )\n\n        num_mask_classes = 1 if cls_agnostic_mask else num_classes\n        self.predictor = Conv2d(conv_dims, num_mask_classes, kernel_size=1, stride=1, padding=0)\n\n        for layer in self.conv_norm_relus + [self.deconv]:\n            weight_init.c2_msra_fill(layer)\n        # use normal distribution initialization for mask prediction layer\n        nn.init.normal_(self.predictor.weight, std=0.001)\n        if self.predictor.bias is not None:\n            nn.init.constant_(self.predictor.bias, 0)\n\n    def forward(self, x):\n        for layer in self.conv_norm_relus:\n            x = layer(x)\n        x = F.relu(self.deconv(x))\n        return self.predictor(x)\n\n\ndef build_mask_head(cfg, input_shape):\n    """"""\n    Build a mask head defined by `cfg.MODEL.ROI_MASK_HEAD.NAME`.\n    """"""\n    name = cfg.MODEL.ROI_MASK_HEAD.NAME\n    return ROI_MASK_HEAD_REGISTRY.get(name)(cfg, input_shape)'"
centermask/modeling/centermask/maskiou_head.py,7,"b'# Copyright (c) Sangrok Lee and Youngwan Lee (ETRI) All Rights Reserved.\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom detectron2.layers import Conv2d, ShapeSpec, cat\nfrom detectron2.utils.registry import Registry\nfrom centermask.layers import MaxPool2d, Linear\n\nROI_MASKIOU_HEAD_REGISTRY = Registry(""ROI_MASKIOU_HEAD"")\nROI_MASKIOU_HEAD_REGISTRY.__doc__ = """"""\nRegistry for maskiou heads, which predicts predicted mask iou.\n\nThe registered object will be called with `obj(cfg, input_shape)`.\n""""""\n\n\ndef mask_iou_loss(labels, pred_maskiou, gt_maskiou, loss_weight):\n    """"""\n    Compute the maskiou loss.\n\n    Args:\n        labels (Tensor): Given mask labels (num of instance,)\n        pred_maskiou (Tensor):  A tensor of shape (num of instance, C)\n        gt_maskiou (Tensor): Ground Truth IOU generated in mask head (num of instance,)\n    """"""\n    def l2_loss(input, target):\n        """"""\n        very similar to the smooth_l1_loss from pytorch, but with\n        the extra beta parameter\n        """"""\n        pos_inds = torch.nonzero(target > 0.0).squeeze(1)\n        if pos_inds.shape[0] > 0:\n            cond = torch.abs(input[pos_inds] - target[pos_inds])\n            loss = 0.5 * cond**2 / pos_inds.shape[0]\n        else:\n            loss = input * 0.0\n        return loss.sum()\n\n    if labels.numel() == 0:\n        return pred_maskiou.sum() * 0\n    \n    index = torch.arange(pred_maskiou.shape[0]).to(device=pred_maskiou.device)\n    maskiou_loss = l2_loss(pred_maskiou[index, labels], gt_maskiou)\n    maskiou_loss = loss_weight * maskiou_loss\n    \n    return maskiou_loss\n\n\ndef mask_iou_inference(pred_instances, pred_maskiou):\n    labels = cat([i.pred_classes for i in pred_instances])\n    num_masks = pred_maskiou.shape[0]\n    index = torch.arange(num_masks, device=labels.device)\n    num_boxes_per_image = [len(i) for i in pred_instances]\n    maskious = pred_maskiou[index, labels].split(num_boxes_per_image, dim=0)\n    for maskiou, box in zip(maskious, pred_instances):\n        box.mask_scores = box.scores * maskiou\n\n\n@ROI_MASKIOU_HEAD_REGISTRY.register()\nclass MaskIoUHead(nn.Module):\n    def __init__(self, cfg, input_shape: ShapeSpec):\n        super(MaskIoUHead, self).__init__()\n\n        # fmt: off\n        num_classes       = cfg.MODEL.ROI_HEADS.NUM_CLASSES\n        conv_dims         = cfg.MODEL.ROI_MASKIOU_HEAD.CONV_DIM\n        num_conv          = cfg.MODEL.ROI_MASKIOU_HEAD.NUM_CONV\n        input_channels    = input_shape.channels + 1\n        resolution        = input_shape.width // 2\n        # fmt: on\n\n        self.conv_relus = []\n        stride = 1\n        for k in range(num_conv):\n            if (k+1) == num_conv:\n                stride = 2\n            conv = Conv2d(\n                input_channels if k == 0 else conv_dims,\n                conv_dims,\n                kernel_size=3,\n                stride=stride,\n                padding=1,\n                activation=F.relu\n            )\n            self.add_module(""maskiou_fcn{}"".format(k+1), conv)\n            self.conv_relus.append(conv)\n        self.maskiou_fc1 = Linear(conv_dims*resolution**2, 1024)\n        self.maskiou_fc2 = Linear(1024, 1024)\n        self.maskiou = Linear(1024, num_classes)\n        self.pooling = MaxPool2d(kernel_size=2, stride=2)\n\n\n        for l in self.conv_relus:\n            nn.init.kaiming_normal_(l.weight, mode=""fan_out"", nonlinearity=""relu"")\n            nn.init.constant_(l.bias, 0)\n        for l in [self.maskiou_fc1, self.maskiou_fc2]:\n            nn.init.kaiming_normal_(l.weight, mode=""fan_out"", nonlinearity=""relu"")\n            nn.init.constant_(l.bias, 0)\n\n\n        nn.init.normal_(self.maskiou.weight, mean=0, std=0.01)\n        nn.init.constant_(self.maskiou.bias, 0)\n\n    def forward(self, x, mask):\n        mask_pool = self.pooling(mask)\n        x = torch.cat((x, mask_pool), 1)\n\n        for layer in self.conv_relus:\n            x = layer(x)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.maskiou_fc1(x))\n        x = F.relu(self.maskiou_fc2(x))\n        x = self.maskiou(x)\n        return x\n\n\ndef build_maskiou_head(cfg, input_shape):\n    """"""\n    Build a mask iou head defined by `cfg.MODEL.ROI_MASKIOU_HEAD.NAME`.\n    """"""\n    name = cfg.MODEL.ROI_MASKIOU_HEAD.NAME\n    return ROI_MASKIOU_HEAD_REGISTRY.get(name)(cfg, input_shape)'"
centermask/modeling/centermask/pooler.py,14,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# Modified by Youngwan Lee (ETRI), 2020. All Rights Reserved.\nimport math\nimport sys\nimport torch\nfrom torch import nn\nfrom torchvision.ops import RoIPool\n\nfrom detectron2.layers import ROIAlign, ROIAlignRotated, cat\nfrom detectron2.modeling.poolers import (\n        convert_boxes_to_pooler_format, assign_boxes_to_levels\n)\n\ndef _img_area(instance):\n\n    device = instance.pred_classes.device\n    image_size = instance.image_size\n    area = torch.as_tensor(image_size[0] * image_size[1], dtype=torch.float, device=device)\n    tmp = torch.zeros((len(instance.pred_classes), 1), dtype=torch.float, device=device)\n\n    return (area + tmp).squeeze(1)\n\n\ndef assign_boxes_to_levels_by_ratio(instances, min_level, max_level, is_train=False):\n    """"""\n    Map each box in `instances` to a feature map level index by adaptive ROI mapping function \n    in CenterMask paper and return the assignment\n    vector.\n\n    Args:\n        instances (list[Instances]): the per-image instances to train/predict masks.\n        min_level (int): Smallest feature map level index. The input is considered index 0,\n            the output of stage 1 is index 1, and so.\n        max_level (int): Largest feature map level index.\n\n    Returns:\n        A tensor of length M, where M is the total number of boxes aggregated over all\n            N batch images. The memory layout corresponds to the concatenation of boxes\n            from all images. Each element is the feature map index, as an offset from\n            `self.min_level`, for the corresponding box (so value i means the box is at\n            `self.min_level + i`).\n    """"""\n    eps = sys.float_info.epsilon\n    if is_train:\n        box_lists = [x.proposal_boxes for x in instances]\n    else:\n        box_lists = [x.pred_boxes for x in instances]\n    box_areas = cat([boxes.area() for boxes in box_lists])\n    img_areas = cat([_img_area(instance_i) for instance_i in instances])\n\n    # Eqn.(2) in the CenterMask paper\n    level_assignments = torch.ceil(\n        max_level - torch.log2(img_areas / box_areas + eps)\n    )\n\n    # clamp level to (min, max), in case the box size is too large or too small\n    # for the available feature maps\n    level_assignments = torch.clamp(level_assignments, min=min_level, max=max_level)\n    return level_assignments.to(torch.int64) - min_level\n\n\ndef assign_boxes_to_levels(box_lists, min_level, max_level, canonical_box_size, canonical_level):\n    """"""\n    Map each box in `box_lists` to a feature map level index and return the assignment\n    vector.\n\n    Args:\n        box_lists (list[Boxes] | list[RotatedBoxes]): A list of N Boxes or N RotatedBoxes,\n            where N is the number of images in the batch.\n        min_level (int): Smallest feature map level index. The input is considered index 0,\n            the output of stage 1 is index 1, and so.\n        max_level (int): Largest feature map level index.\n        canonical_box_size (int): A canonical box size in pixels (sqrt(box area)).\n        canonical_level (int): The feature map level index on which a canonically-sized box\n            should be placed.\n\n    Returns:\n        A tensor of length M, where M is the total number of boxes aggregated over all\n            N batch images. The memory layout corresponds to the concatenation of boxes\n            from all images. Each element is the feature map index, as an offset from\n            `self.min_level`, for the corresponding box (so value i means the box is at\n            `self.min_level + i`).\n    """"""\n    eps = sys.float_info.epsilon\n    box_sizes = torch.sqrt(cat([boxes.area() for boxes in box_lists]))\n    # Eqn.(1) in FPN paper\n    level_assignments = torch.floor(\n        canonical_level + torch.log2(box_sizes / canonical_box_size + eps)\n    )\n    # clamp level to (min, max), in case the box size is too large or too small\n    # for the available feature maps\n    level_assignments = torch.clamp(level_assignments, min=min_level, max=max_level)\n    return level_assignments.to(torch.int64) - min_level\n\n\ndef convert_boxes_to_pooler_format(box_lists):\n    """"""\n    Convert all boxes in `box_lists` to the low-level format used by ROI pooling ops\n    (see description under Returns).\n\n    Args:\n        box_lists (list[Boxes] | list[RotatedBoxes]):\n            A list of N Boxes or N RotatedBoxes, where N is the number of images in the batch.\n\n    Returns:\n        When input is list[Boxes]:\n            A tensor of shape (M, 5), where M is the total number of boxes aggregated over all\n            N batch images.\n            The 5 columns are (batch index, x0, y0, x1, y1), where batch index\n            is the index in [0, N) identifying which batch image the box with corners at\n            (x0, y0, x1, y1) comes from.\n        When input is list[RotatedBoxes]:\n            A tensor of shape (M, 6), where M is the total number of boxes aggregated over all\n            N batch images.\n            The 6 columns are (batch index, x_ctr, y_ctr, width, height, angle_degrees),\n            where batch index is the index in [0, N) identifying which batch image the\n            rotated box (x_ctr, y_ctr, width, height, angle_degrees) comes from.\n    """"""\n\n    def fmt_box_list(box_tensor, batch_index):\n        repeated_index = torch.full(\n            (len(box_tensor), 1), batch_index, dtype=box_tensor.dtype, device=box_tensor.device\n        )\n        return cat((repeated_index, box_tensor), dim=1)\n\n    pooler_fmt_boxes = cat(\n        [fmt_box_list(box_list.tensor, i) for i, box_list in enumerate(box_lists)], dim=0\n    )\n\n    return pooler_fmt_boxes\n\n\nclass ROIPooler(nn.Module):\n    """"""\n    Region of interest feature map pooler that supports pooling from one or more\n    feature maps.\n    """"""\n\n    def __init__(\n        self,\n        output_size,\n        scales,\n        sampling_ratio,\n        pooler_type,\n        canonical_box_size=224,\n        canonical_level=4,\n        assign_crit=""area"",\n    ):\n        """"""\n        Args:\n            output_size (int, tuple[int] or list[int]): output size of the pooled region,\n                e.g., 14 x 14. If tuple or list is given, the length must be 2.\n            scales (list[float]): The scale for each low-level pooling op relative to\n                the input image. For a feature map with stride s relative to the input\n                image, scale is defined as a 1 / s. The stride must be power of 2.\n                When there are multiple scales, they must form a pyramid, i.e. they must be\n                a monotically decreasing geometric sequence with a factor of 1/2.\n            sampling_ratio (int): The `sampling_ratio` parameter for the ROIAlign op.\n            pooler_type (string): Name of the type of pooling operation that should be applied.\n                For instance, ""ROIPool"" or ""ROIAlignV2"".\n            canonical_box_size (int): A canonical box size in pixels (sqrt(box area)). The default\n                is heuristically defined as 224 pixels in the FPN paper (based on ImageNet\n                pre-training).\n            canonical_level (int): The feature map level index from which a canonically-sized box\n                should be placed. The default is defined as level 4 (stride=16) in the FPN paper,\n                i.e., a box of size 224x224 will be placed on the feature with stride=16.\n                The box placement for all boxes will be determined from their sizes w.r.t\n                canonical_box_size. For example, a box whose area is 4x that of a canonical box\n                should be used to pool features from feature level ``canonical_level+1``.\n\n                Note that the actual input feature maps given to this module may not have\n                sufficiently many levels for the input boxes. If the boxes are too large or too\n                small for the input feature maps, the closest level will be used.\n        """"""\n        super().__init__()\n\n        if isinstance(output_size, int):\n            output_size = (output_size, output_size)\n        assert len(output_size) == 2\n        assert isinstance(output_size[0], int) and isinstance(output_size[1], int)\n        self.output_size = output_size\n\n        if pooler_type == ""ROIAlign"":\n            self.level_poolers = nn.ModuleList(\n                ROIAlign(\n                    output_size, spatial_scale=scale, sampling_ratio=sampling_ratio, aligned=False\n                )\n                for scale in scales\n            )\n        elif pooler_type == ""ROIAlignV2"":\n            self.level_poolers = nn.ModuleList(\n                ROIAlign(\n                    output_size, spatial_scale=scale, sampling_ratio=sampling_ratio, aligned=True\n                )\n                for scale in scales\n            )\n        elif pooler_type == ""ROIPool"":\n            self.level_poolers = nn.ModuleList(\n                RoIPool(output_size, spatial_scale=scale) for scale in scales\n            )\n        elif pooler_type == ""ROIAlignRotated"":\n            self.level_poolers = nn.ModuleList(\n                ROIAlignRotated(output_size, spatial_scale=scale, sampling_ratio=sampling_ratio)\n                for scale in scales\n            )\n        else:\n            raise ValueError(""Unknown pooler type: {}"".format(pooler_type))\n\n        # Map scale (defined as 1 / stride) to its feature map level under the\n        # assumption that stride is a power of 2.\n        min_level = -math.log2(scales[0])\n        max_level = -math.log2(scales[-1])\n        assert math.isclose(min_level, int(min_level)) and math.isclose(\n            max_level, int(max_level)\n        ), ""Featuremap stride is not power of 2!""\n        self.min_level = int(min_level)\n        self.max_level = int(max_level)\n        assert (\n            len(scales) == self.max_level - self.min_level + 1\n        ), ""[ROIPooler] Sizes of input featuremaps do not form a pyramid!""\n        assert 0 < self.min_level and self.min_level <= self.max_level\n        if len(scales) > 1:\n            # When there is only one feature map, canonical_level is redundant and we should not\n            # require it to be a sensible value. Therefore we skip this assertion\n            assert self.min_level <= canonical_level and canonical_level <= self.max_level\n        self.canonical_level = canonical_level\n        assert canonical_box_size > 0\n        self.canonical_box_size = canonical_box_size\n        self.assign_crit = assign_crit #ywlee\n\n    def forward(self, x, instances, is_train=False):\n        """"""\n        Args:\n            x (list[Tensor]): A list of feature maps of NCHW shape, with scales matching those\n                used to construct this module.\n            instances (list[Instances]): the per-image instances to train/predict masks.\n                In training, they can be the proposals.\n                In inference, they can be the predicted boxes.\n            is_train (True/False)\n\n        Returns:\n            Tensor:\n                A tensor of shape (M, C, output_size, output_size) where M is the total number of\n                boxes aggregated over all N batch images and C is the number of channels in `x`.\n        """"""\n        if is_train:\n            box_lists = [x.proposal_boxes for x in instances]\n        else:\n            box_lists = [x.pred_boxes for x in instances]\n\n        num_level_assignments = len(self.level_poolers)\n\n        assert isinstance(x, list) and isinstance(\n            box_lists, list\n        ), ""Arguments to pooler must be lists""\n        assert (\n            len(x) == num_level_assignments\n        ), ""unequal value, num_level_assignments={}, but x is list of {} Tensors"".format(\n            num_level_assignments, len(x)\n        )\n\n        assert len(box_lists) == x[0].size(\n            0\n        ), ""unequal value, x[0] batch dim 0 is {}, but box_list has length {}"".format(\n            x[0].size(0), len(box_lists)\n        )\n\n        pooler_fmt_boxes = convert_boxes_to_pooler_format(box_lists)\n\n        if num_level_assignments == 1:\n            return self.level_poolers[0](x[0], pooler_fmt_boxes)\n\n        if self.assign_crit == ""ratio"":\n            level_assignments = assign_boxes_to_levels_by_ratio(\n                instances, self.min_level, self.max_level, is_train\n            )\n        else: #default\n            level_assignments = assign_boxes_to_levels(\n                box_lists, self.min_level, self.max_level, self.canonical_box_size, self.canonical_level\n            )\n\n        num_boxes = len(pooler_fmt_boxes)\n        num_channels = x[0].shape[1]\n        output_size = self.output_size[0]\n\n        dtype, device = x[0].dtype, x[0].device\n        output = torch.zeros(\n            (num_boxes, num_channels, output_size, output_size), dtype=dtype, device=device\n        )\n\n        for level, (x_level, pooler) in enumerate(zip(x, self.level_poolers)):\n            inds = torch.nonzero(level_assignments == level).squeeze(1)\n            pooler_fmt_boxes_level = pooler_fmt_boxes[inds]\n            output[inds] = pooler(x_level, pooler_fmt_boxes_level)\n\n        return output\n'"
centermask/modeling/centermask/proposal_utils.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# Modified by Youngwan Lee (ETRI) in 28/01/2020.\nimport math\nimport torch\n\nfrom detectron2.structures import Instances\n\n\ndef add_ground_truth_to_proposals(targets, proposals):\n    """"""\n    Call `add_ground_truth_to_proposals_single_image` for all images.\n\n    Args:\n        targets(list[Instances]): list of N elements. Element i is a Boxes\n            representing the gound-truth for image i.\n        proposals (list[Instances]): list of N elements. Element i is a Instances\n            representing the proposals for image i.\n\n    Returns:\n        list[Instances]: list of N Instances. Each is the proposals for the image,\n            with field ""proposal_boxes"" and ""objectness_logits"".\n    """"""\n    assert targets is not None\n\n    assert len(proposals) == len(targets)\n    if len(proposals) == 0:\n        return proposals\n\n    return [\n        add_ground_truth_to_proposals_single_image(tagets_i, proposals_i)\n        for tagets_i, proposals_i in zip(targets, proposals)\n    ]\n\n\ndef add_ground_truth_to_proposals_single_image(targets_i, proposals):\n    """"""\n    Augment `proposals` with ground-truth boxes from `gt_boxes`.\n\n    Args:\n        Same as `add_ground_truth_to_proposals`, but with targets and proposals\n        per image.\n\n    Returns:\n        Same as `add_ground_truth_to_proposals`, but for only one image.\n    """"""\n    device = proposals.scores.device\n    proposals.proposal_boxes = proposals.pred_boxes\n    proposals.remove(""pred_boxes"")\n    # Concatenating gt_boxes with proposals requires them to have the same fields\n    # Assign all ground-truth boxes an objectness logit corresponding to P(object) \\approx 1.\n    gt_logit_value = math.log((1.0 - 1e-10) / (1 - (1.0 - 1e-10)))\n    gt_logits = gt_logit_value * torch.ones(len(targets_i), device=device)\n    gt_proposal = Instances(proposals.image_size)\n    gt_proposal.proposal_boxes = targets_i.gt_boxes\n    # to have the same fields with proposals\n    gt_proposal.scores = gt_logits\n    gt_proposal.pred_classes = targets_i.gt_classes\n    gt_proposal.locations = torch.ones((len(targets_i), 2), device=device)\n\n    new_proposals = Instances.cat([proposals, gt_proposal])\n\n    return new_proposals\n'"
centermask/modeling/centermask/sam.py,3,"b'# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\nimport fvcore.nn.weight_init as weight_init\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom detectron2.layers import Conv2d, ConvTranspose2d, ShapeSpec, get_norm\nfrom .mask_head import ROI_MASK_HEAD_REGISTRY\nfrom centermask.layers import Max\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=3):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), \'kernel size must be 3 or 7\'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv = Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        weight_init.c2_msra_fill(self.conv)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out = Max(x)\n        scale = torch.cat([avg_out, max_out], dim=1)\n        scale = self.conv(scale)\n        return x * self.sigmoid(scale)\n\n\n@ROI_MASK_HEAD_REGISTRY.register()\nclass SpatialAttentionMaskHead(nn.Module):\n    """"""\n    A mask head with several conv layers and spatial attention module \n    in CenterMask paper, plus an upsample layer (with `ConvTranspose2d`).\n    """"""\n\n    def __init__(self, cfg, input_shape: ShapeSpec):\n        """"""\n        The following attributes are parsed from config:\n            num_conv: the number of conv layers\n            conv_dim: the dimension of the conv layers\n            norm: normalization for the conv layers\n        """"""\n        super(SpatialAttentionMaskHead, self).__init__()\n\n        # fmt: off\n        num_classes       = cfg.MODEL.ROI_HEADS.NUM_CLASSES\n        conv_dims         = cfg.MODEL.ROI_MASK_HEAD.CONV_DIM\n        self.norm         = cfg.MODEL.ROI_MASK_HEAD.NORM\n        num_conv          = cfg.MODEL.ROI_MASK_HEAD.NUM_CONV\n        input_channels    = input_shape.channels\n        cls_agnostic_mask = cfg.MODEL.ROI_MASK_HEAD.CLS_AGNOSTIC_MASK\n        # fmt: on\n\n        self.conv_norm_relus = []\n\n        for k in range(num_conv):\n            conv = Conv2d(\n                input_channels if k == 0 else conv_dims,\n                conv_dims,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=not self.norm,\n                norm=get_norm(self.norm, conv_dims),\n                activation=F.relu,\n            )\n            self.add_module(""mask_fcn{}"".format(k + 1), conv)\n            self.conv_norm_relus.append(conv)\n\n        self.spatialAtt = SpatialAttention()\n\n        self.deconv = ConvTranspose2d(\n            conv_dims if num_conv > 0 else input_channels,\n            conv_dims,\n            kernel_size=2,\n            stride=2,\n            padding=0,\n        )\n\n        num_mask_classes = 1 if cls_agnostic_mask else num_classes\n        self.predictor = Conv2d(conv_dims, num_mask_classes, kernel_size=1, stride=1, padding=0)\n\n        for layer in self.conv_norm_relus + [self.deconv]:\n            weight_init.c2_msra_fill(layer)\n        # use normal distribution initialization for mask prediction layer\n        nn.init.normal_(self.predictor.weight, std=0.001)\n        if self.predictor.bias is not None:\n            nn.init.constant_(self.predictor.bias, 0)\n\n    def forward(self, x):\n        for layer in self.conv_norm_relus:\n            x = layer(x)\n        x = self.spatialAtt(x)\n        x = F.relu(self.deconv(x))\n        return self.predictor(x)'"
centermask/modeling/fcos/__init__.py,0,b'from .fcos import FCOS\n'
centermask/modeling/fcos/fcos.py,11,"b'import math\nfrom typing import List, Dict\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom detectron2.layers import ShapeSpec\nfrom detectron2.modeling.proposal_generator.build import PROPOSAL_GENERATOR_REGISTRY\n\nfrom centermask.layers import DFConv2d, IOULoss\nfrom .fcos_outputs import FCOSOutputs\n\n\n__all__ = [""FCOS""]\n\nINF = 100000000\n\n\nclass Scale(nn.Module):\n    def __init__(self, init_value=1.0):\n        super(Scale, self).__init__()\n        self.scale = nn.Parameter(torch.FloatTensor([init_value]))\n\n    def forward(self, input):\n        return input * self.scale\n\n\n@PROPOSAL_GENERATOR_REGISTRY.register()\nclass FCOS(nn.Module):\n    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):\n        super().__init__()\n        # fmt: off\n        self.in_features          = cfg.MODEL.FCOS.IN_FEATURES\n        self.fpn_strides          = cfg.MODEL.FCOS.FPN_STRIDES\n        self.focal_loss_alpha     = cfg.MODEL.FCOS.LOSS_ALPHA\n        self.focal_loss_gamma     = cfg.MODEL.FCOS.LOSS_GAMMA\n        self.center_sample        = cfg.MODEL.FCOS.CENTER_SAMPLE\n        self.strides              = cfg.MODEL.FCOS.FPN_STRIDES\n        self.radius               = cfg.MODEL.FCOS.POS_RADIUS\n        self.pre_nms_thresh_train = cfg.MODEL.FCOS.INFERENCE_TH_TRAIN\n        self.pre_nms_thresh_test  = cfg.MODEL.FCOS.INFERENCE_TH_TEST\n        self.pre_nms_topk_train   = cfg.MODEL.FCOS.PRE_NMS_TOPK_TRAIN\n        self.pre_nms_topk_test    = cfg.MODEL.FCOS.PRE_NMS_TOPK_TEST\n        self.nms_thresh           = cfg.MODEL.FCOS.NMS_TH\n        self.post_nms_topk_train  = cfg.MODEL.FCOS.POST_NMS_TOPK_TRAIN\n        self.post_nms_topk_test   = cfg.MODEL.FCOS.POST_NMS_TOPK_TEST\n        self.thresh_with_ctr      = cfg.MODEL.FCOS.THRESH_WITH_CTR\n        self.mask_on              = cfg.MODEL.MASK_ON #ywlee\n        # fmt: on\n        self.iou_loss = IOULoss(cfg.MODEL.FCOS.LOC_LOSS_TYPE)\n        # generate sizes of interest\n        soi = []\n        prev_size = -1\n        for s in cfg.MODEL.FCOS.SIZES_OF_INTEREST:\n            soi.append([prev_size, s])\n            prev_size = s\n        soi.append([prev_size, INF])\n        self.sizes_of_interest = soi\n        self.fcos_head = FCOSHead(cfg, [input_shape[f] for f in self.in_features])\n\n    def forward(self, images, features, gt_instances):\n        """"""\n        Arguments:\n            images (list[Tensor] or ImageList): images to be processed\n            targets (list[BoxList]): ground-truth boxes present in the image (optional)\n\n        Returns:\n            result (list[BoxList] or dict[Tensor]): the output from the model.\n                During training, it returns a dict[Tensor] which contains the losses.\n                During testing, it returns list[BoxList] contains additional fields\n                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n\n        """"""\n        features = [features[f] for f in self.in_features]\n        locations = self.compute_locations(features)\n        logits_pred, reg_pred, ctrness_pred, bbox_towers = self.fcos_head(features)\n\n        if self.training:\n            pre_nms_thresh = self.pre_nms_thresh_train\n            pre_nms_topk = self.pre_nms_topk_train\n            post_nms_topk = self.post_nms_topk_train\n        else:\n            pre_nms_thresh = self.pre_nms_thresh_test\n            pre_nms_topk = self.pre_nms_topk_test\n            post_nms_topk = self.post_nms_topk_test\n\n        outputs = FCOSOutputs(\n            images,\n            locations,\n            logits_pred,\n            reg_pred,\n            ctrness_pred,\n            self.focal_loss_alpha,\n            self.focal_loss_gamma,\n            self.iou_loss,\n            self.center_sample,\n            self.sizes_of_interest,\n            self.strides,\n            self.radius,\n            self.fcos_head.num_classes,\n            pre_nms_thresh,\n            pre_nms_topk,\n            self.nms_thresh,\n            post_nms_topk,\n            self.thresh_with_ctr,\n            gt_instances,\n        )\n\n        if self.training:\n            losses, _ = outputs.losses()\n            if self.mask_on:\n                proposals = outputs.predict_proposals()\n                return proposals, losses\n            else:\n                return None, losses\n        else:\n            proposals = outputs.predict_proposals()\n            return proposals, {}\n\n    def compute_locations(self, features):\n        locations = []\n        for level, feature in enumerate(features):\n            h, w = feature.size()[-2:]\n            locations_per_level = self.compute_locations_per_level(\n                h, w, self.fpn_strides[level],\n                feature.device\n            )\n            locations.append(locations_per_level)\n        return locations\n\n    def compute_locations_per_level(self, h, w, stride, device):\n        shifts_x = torch.arange(\n            0, w * stride, step=stride,\n            dtype=torch.float32, device=device\n        )\n        shifts_y = torch.arange(\n            0, h * stride, step=stride,\n            dtype=torch.float32, device=device\n        )\n        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n        shift_x = shift_x.reshape(-1)\n        shift_y = shift_y.reshape(-1)\n        locations = torch.stack((shift_x, shift_y), dim=1) + stride // 2\n        return locations\n\n\nclass FCOSHead(nn.Module):\n    def __init__(self, cfg, input_shape: List[ShapeSpec]):\n        """"""\n        Arguments:\n            in_channels (int): number of channels of the input feature\n        """"""\n        super().__init__()\n        # TODO: Implement the sigmoid version first.\n        self.num_classes = cfg.MODEL.FCOS.NUM_CLASSES\n        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES\n        head_configs = {""cls"": (cfg.MODEL.FCOS.NUM_CLS_CONVS,\n                                False),\n                        ""bbox"": (cfg.MODEL.FCOS.NUM_BOX_CONVS,\n                                 cfg.MODEL.FCOS.USE_DEFORMABLE),\n                        ""share"": (cfg.MODEL.FCOS.NUM_SHARE_CONVS,\n                                  cfg.MODEL.FCOS.USE_DEFORMABLE)}\n        norm = None if cfg.MODEL.FCOS.NORM == ""none"" else cfg.MODEL.FCOS.NORM\n\n        in_channels = [s.channels for s in input_shape]\n        assert len(set(in_channels)) == 1, ""Each level must have the same channel!""\n        in_channels = in_channels[0]\n\n        for head in head_configs:\n            tower = []\n            num_convs, use_deformable = head_configs[head]\n            if use_deformable:\n                conv_func = DFConv2d\n            else:\n                conv_func = nn.Conv2d\n            for i in range(num_convs):\n                tower.append(conv_func(\n                        in_channels, in_channels,\n                        kernel_size=3, stride=1,\n                        padding=1, bias=True\n                ))\n                if norm == ""GN"":\n                    tower.append(nn.GroupNorm(32, in_channels))\n                tower.append(nn.ReLU())\n            self.add_module(\'{}_tower\'.format(head),\n                            nn.Sequential(*tower))\n\n        self.cls_logits = nn.Conv2d(\n            in_channels, self.num_classes,\n            kernel_size=3, stride=1,\n            padding=1\n        )\n        self.bbox_pred = nn.Conv2d(\n            in_channels, 4, kernel_size=3,\n            stride=1, padding=1\n        )\n        self.ctrness = nn.Conv2d(\n            in_channels, 1, kernel_size=3,\n            stride=1, padding=1\n        )\n\n        if cfg.MODEL.FCOS.USE_SCALE:\n            self.scales = nn.ModuleList([Scale(init_value=1.0) for _ in self.fpn_strides])\n        else:\n            self.scales = None\n\n        for modules in [\n            self.cls_tower, self.bbox_tower,\n            self.share_tower, self.cls_logits,\n            self.bbox_pred, self.ctrness\n        ]:\n            for l in modules.modules():\n                if isinstance(l, nn.Conv2d):\n                    torch.nn.init.normal_(l.weight, std=0.01)\n                    torch.nn.init.constant_(l.bias, 0)\n\n        # initialize the bias for focal loss\n        prior_prob = cfg.MODEL.FCOS.PRIOR_PROB\n        bias_value = -math.log((1 - prior_prob) / prior_prob)\n        torch.nn.init.constant_(self.cls_logits.bias, bias_value)\n\n    def forward(self, x):\n        logits = []\n        bbox_reg = []\n        ctrness = []\n        bbox_towers = []\n        for l, feature in enumerate(x):\n            feature = self.share_tower(feature)\n            cls_tower = self.cls_tower(feature)\n            bbox_tower = self.bbox_tower(feature)\n\n            logits.append(self.cls_logits(cls_tower))\n            ctrness.append(self.ctrness(bbox_tower))\n            reg = self.bbox_pred(bbox_tower)\n            if self.scales is not None:\n                reg = self.scales[l](reg)\n            # Note that we use relu, as in the improved FCOS, instead of exp.\n            bbox_reg.append(F.relu(reg))\n\n        return logits, bbox_reg, ctrness, bbox_towers\n'"
centermask/modeling/fcos/fcos_outputs.py,19,"b'import logging\nimport torch\nimport torch.nn.functional as F\n\nfrom detectron2.layers import cat\nfrom detectron2.structures import Instances, Boxes\nfrom centermask.utils.comm import get_world_size\nfrom fvcore.nn import sigmoid_focal_loss_jit\n\nfrom centermask.utils.comm import reduce_sum\nfrom centermask.layers import ml_nms\n\n\nlogger = logging.getLogger(__name__)\n\nINF = 100000000\n\n""""""\nShape shorthand in this module:\n\n    N: number of images in the minibatch\n    L: number of feature maps per image on which RPN is run\n    Hi, Wi: height and width of the i-th feature map\n    4: size of the box parameterization\n\nNaming convention:\n\n    labels: refers to the ground-truth class of an position.\n\n    reg_targets: refers to the 4-d (left, top, right, bottom) distances that parameterize the ground-truth box.\n\n    logits_pred: predicted classification scores in [-inf, +inf];\n    \n    reg_pred: the predicted (left, top, right, bottom), corresponding to reg_targets \n\n    ctrness_pred: predicted centerness scores\n    \n""""""\n\n\ndef compute_ctrness_targets(reg_targets):\n    if len(reg_targets) == 0:\n        return reg_targets.new_zeros(len(reg_targets))\n    left_right = reg_targets[:, [0, 2]]\n    top_bottom = reg_targets[:, [1, 3]]\n    ctrness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\\n                 (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n    return torch.sqrt(ctrness)\n\n\ndef fcos_losses(\n        labels,\n        reg_targets,\n        logits_pred,\n        reg_pred,\n        ctrness_pred,\n        focal_loss_alpha,\n        focal_loss_gamma,\n        iou_loss,\n):\n    num_classes = logits_pred.size(1)\n    labels = labels.flatten()\n\n    pos_inds = torch.nonzero(labels != num_classes).squeeze(1)\n    num_pos_local = pos_inds.numel()\n    num_gpus = get_world_size()\n    total_num_pos = reduce_sum(pos_inds.new_tensor([num_pos_local])).item()\n    num_pos_avg = max(total_num_pos / num_gpus, 1.0)\n\n    # prepare one_hot\n    class_target = torch.zeros_like(logits_pred)\n    class_target[pos_inds, labels[pos_inds]] = 1\n\n    class_loss = sigmoid_focal_loss_jit(\n        logits_pred,\n        class_target,\n        alpha=focal_loss_alpha,\n        gamma=focal_loss_gamma,\n        reduction=""sum"",\n    ) / num_pos_avg\n\n    reg_pred = reg_pred[pos_inds]\n    reg_targets = reg_targets[pos_inds]\n    ctrness_pred = ctrness_pred[pos_inds]\n\n    ctrness_targets = compute_ctrness_targets(reg_targets)\n    ctrness_targets_sum = ctrness_targets.sum()\n    ctrness_norm = max(reduce_sum(ctrness_targets_sum).item() / num_gpus, 1e-6)\n\n    reg_loss = iou_loss(\n        reg_pred,\n        reg_targets,\n        ctrness_targets\n    ) / ctrness_norm\n\n    ctrness_loss = F.binary_cross_entropy_with_logits(\n        ctrness_pred,\n        ctrness_targets,\n        reduction=""sum""\n    ) / num_pos_avg\n\n    losses = {\n        ""loss_fcos_cls"": class_loss,\n        ""loss_fcos_loc"": reg_loss,\n        ""loss_fcos_ctr"": ctrness_loss\n    }\n    return losses, {}\n\n\nclass FCOSOutputs(object):\n    def __init__(\n            self,\n            images,\n            locations,\n            logits_pred,\n            reg_pred,\n            ctrness_pred,\n            focal_loss_alpha,\n            focal_loss_gamma,\n            iou_loss,\n            center_sample,\n            sizes_of_interest,\n            strides,\n            radius,\n            num_classes,\n            pre_nms_thresh,\n            pre_nms_top_n,\n            nms_thresh,\n            fpn_post_nms_top_n,\n            thresh_with_ctr,\n            gt_instances=None,\n    ):\n        self.logits_pred = logits_pred\n        self.reg_pred = reg_pred\n        self.ctrness_pred = ctrness_pred\n        self.locations = locations\n\n        self.gt_instances = gt_instances\n        self.num_feature_maps = len(logits_pred)\n        self.num_images = len(images)\n        self.image_sizes = images.image_sizes\n        self.focal_loss_alpha = focal_loss_alpha\n        self.focal_loss_gamma = focal_loss_gamma\n        self.iou_loss = iou_loss\n        self.center_sample = center_sample\n        self.sizes_of_interest = sizes_of_interest\n        self.strides = strides\n        self.radius = radius\n        self.num_classes = num_classes\n        self.pre_nms_thresh = pre_nms_thresh\n        self.pre_nms_top_n = pre_nms_top_n\n        self.nms_thresh = nms_thresh\n        self.fpn_post_nms_top_n = fpn_post_nms_top_n\n        self.thresh_with_ctr = thresh_with_ctr\n\n    def _transpose(self, training_targets, num_loc_list):\n        \'\'\'\n        This function is used to transpose image first training targets to level first ones\n        :return: level first training targets\n        \'\'\'\n        for im_i in range(len(training_targets)):\n            training_targets[im_i] = torch.split(\n                training_targets[im_i], num_loc_list, dim=0\n            )\n\n        targets_level_first = []\n        for targets_per_level in zip(*training_targets):\n            targets_level_first.append(\n                torch.cat(targets_per_level, dim=0)\n            )\n        return targets_level_first\n\n    def _get_ground_truth(self):\n        num_loc_list = [len(loc) for loc in self.locations]\n        self.num_loc_list = num_loc_list\n\n        # compute locations to size ranges\n        loc_to_size_range = []\n        for l, loc_per_level in enumerate(self.locations):\n            loc_to_size_range_per_level = loc_per_level.new_tensor(self.sizes_of_interest[l])\n            loc_to_size_range.append(\n                loc_to_size_range_per_level[None].expand(num_loc_list[l], -1)\n            )\n\n        loc_to_size_range = torch.cat(loc_to_size_range, dim=0)\n        locations = torch.cat(self.locations, dim=0)\n\n        training_targets = self.compute_targets_for_locations(\n            locations, self.gt_instances, loc_to_size_range\n        )\n\n        # transpose im first training_targets to level first ones\n        training_targets = {\n            k: self._transpose(v, num_loc_list) for k, v in training_targets.items()\n        }\n\n        # we normalize reg_targets by FPN\'s strides here\n        reg_targets = training_targets[""reg_targets""]\n        for l in range(len(reg_targets)):\n            reg_targets[l] = reg_targets[l] / float(self.strides[l])\n\n        return training_targets\n\n    def get_sample_region(self, gt, strides, num_loc_list, loc_xs, loc_ys, radius=1):\n        num_gts = gt.shape[0]\n        K = len(loc_xs)\n        gt = gt[None].expand(K, num_gts, 4)\n        center_x = (gt[..., 0] + gt[..., 2]) / 2\n        center_y = (gt[..., 1] + gt[..., 3]) / 2\n        center_gt = gt.new_zeros(gt.shape)\n        # no gt\n        if center_x.numel() == 0 or center_x[..., 0].sum() == 0:\n            return loc_xs.new_zeros(loc_xs.shape, dtype=torch.uint8)\n        beg = 0\n        for level, num_loc in enumerate(num_loc_list):\n            end = beg + num_loc\n            stride = strides[level] * radius\n            xmin = center_x[beg:end] - stride\n            ymin = center_y[beg:end] - stride\n            xmax = center_x[beg:end] + stride\n            ymax = center_y[beg:end] + stride\n            # limit sample region in gt\n            center_gt[beg:end, :, 0] = torch.where(xmin > gt[beg:end, :, 0], xmin, gt[beg:end, :, 0])\n            center_gt[beg:end, :, 1] = torch.where(ymin > gt[beg:end, :, 1], ymin, gt[beg:end, :, 1])\n            center_gt[beg:end, :, 2] = torch.where(xmax > gt[beg:end, :, 2], gt[beg:end, :, 2], xmax)\n            center_gt[beg:end, :, 3] = torch.where(ymax > gt[beg:end, :, 3], gt[beg:end, :, 3], ymax)\n            beg = end\n        left = loc_xs[:, None] - center_gt[..., 0]\n        right = center_gt[..., 2] - loc_xs[:, None]\n        top = loc_ys[:, None] - center_gt[..., 1]\n        bottom = center_gt[..., 3] - loc_ys[:, None]\n        center_bbox = torch.stack((left, top, right, bottom), -1)\n        inside_gt_bbox_mask = center_bbox.min(-1)[0] > 0\n        return inside_gt_bbox_mask\n\n    def compute_targets_for_locations(self, locations, targets, size_ranges):\n        labels = []\n        reg_targets = []\n        xs, ys = locations[:, 0], locations[:, 1]\n\n        for im_i in range(len(targets)):\n            targets_per_im = targets[im_i]\n            bboxes = targets_per_im.gt_boxes.tensor\n            labels_per_im = targets_per_im.gt_classes\n\n            # no gt\n            if bboxes.numel() == 0:\n                labels.append(labels_per_im.new_zeros(locations.size(0)) + self.num_classes)\n                reg_targets.append(locations.new_zeros((locations.size(0), 4)))\n                continue\n\n            area = targets_per_im.gt_boxes.area()\n\n            l = xs[:, None] - bboxes[:, 0][None]\n            t = ys[:, None] - bboxes[:, 1][None]\n            r = bboxes[:, 2][None] - xs[:, None]\n            b = bboxes[:, 3][None] - ys[:, None]\n            reg_targets_per_im = torch.stack([l, t, r, b], dim=2)\n\n            if self.center_sample:\n                is_in_boxes = self.get_sample_region(\n                    bboxes, self.strides, self.num_loc_list,\n                    xs, ys, radius=self.radius\n                )\n            else:\n                is_in_boxes = reg_targets_per_im.min(dim=2)[0] > 0\n\n            max_reg_targets_per_im = reg_targets_per_im.max(dim=2)[0]\n            # limit the regression range for each location\n            is_cared_in_the_level = \\\n                (max_reg_targets_per_im >= size_ranges[:, [0]]) & \\\n                (max_reg_targets_per_im <= size_ranges[:, [1]])\n\n            locations_to_gt_area = area[None].repeat(len(locations), 1)\n            locations_to_gt_area[is_in_boxes == 0] = INF\n            locations_to_gt_area[is_cared_in_the_level == 0] = INF\n\n            # if there are still more than one objects for a location,\n            # we choose the one with minimal area\n            locations_to_min_area, locations_to_gt_inds = locations_to_gt_area.min(dim=1)\n\n            reg_targets_per_im = reg_targets_per_im[range(len(locations)), locations_to_gt_inds]\n\n            labels_per_im = labels_per_im[locations_to_gt_inds]\n            labels_per_im[locations_to_min_area == INF] = self.num_classes\n\n            labels.append(labels_per_im)\n            reg_targets.append(reg_targets_per_im)\n\n        return {""labels"": labels, ""reg_targets"": reg_targets}\n\n    def losses(self):\n        """"""\n        Return the losses from a set of FCOS predictions and their associated ground-truth.\n\n        Returns:\n            dict[loss name -> loss value]: A dict mapping from loss name to loss value.\n        """"""\n\n        training_targets = self._get_ground_truth()\n        labels, reg_targets = training_targets[""labels""], training_targets[""reg_targets""]\n\n        # Collect all logits and regression predictions over feature maps\n        # and images to arrive at the same shape as the labels and targets\n        # The final ordering is L, N, H, W from slowest to fastest axis.\n        logits_pred = cat(\n            [\n                # Reshape: (N, C, Hi, Wi) -> (N, Hi, Wi, C) -> (N*Hi*Wi, C)\n                x.permute(0, 2, 3, 1).reshape(-1, self.num_classes)\n                for x in self.logits_pred\n            ], dim=0,)\n        reg_pred = cat(\n            [\n                # Reshape: (N, B, Hi, Wi) -> (N, Hi, Wi, B) -> (N*Hi*Wi, B)\n                x.permute(0, 2, 3, 1).reshape(-1, 4)\n                for x in self.reg_pred\n            ], dim=0,)\n        ctrness_pred = cat(\n            [\n                # Reshape: (N, 1, Hi, Wi) -> (N*Hi*Wi,)\n                x.reshape(-1) for x in self.ctrness_pred\n            ], dim=0,)\n\n        labels = cat(\n            [\n                # Reshape: (N, 1, Hi, Wi) -> (N*Hi*Wi,)\n                x.reshape(-1) for x in labels\n            ], dim=0,)\n\n        reg_targets = cat(\n            [\n                # Reshape: (N, Hi, Wi, 4) -> (N*Hi*Wi, 4)\n                x.reshape(-1, 4) for x in reg_targets\n            ], dim=0,)\n\n        return fcos_losses(\n            labels,\n            reg_targets,\n            logits_pred,\n            reg_pred,\n            ctrness_pred,\n            self.focal_loss_alpha,\n            self.focal_loss_gamma,\n            self.iou_loss\n        )\n\n    def predict_proposals(self):\n        sampled_boxes = []\n\n        bundle = (\n            self.locations, self.logits_pred,\n            self.reg_pred, self.ctrness_pred,\n            self.strides\n        )\n\n        for i, (l, o, r, c, s) in enumerate(zip(*bundle)):\n            # recall that during training, we normalize regression targets with FPN\'s stride.\n            # we denormalize them here.\n            r = r * s\n            sampled_boxes.append(\n                self.forward_for_single_feature_map(\n                    l, o, r, c, self.image_sizes\n                )\n            )\n\n        boxlists = list(zip(*sampled_boxes))\n        boxlists = [Instances.cat(boxlist) for boxlist in boxlists]\n        boxlists = self.select_over_all_levels(boxlists)\n        return boxlists\n\n    def forward_for_single_feature_map(\n            self, locations, box_cls,\n            reg_pred, ctrness, image_sizes\n    ):\n        N, C, H, W = box_cls.shape\n\n        # put in the same format as locations\n        box_cls = box_cls.view(N, C, H, W).permute(0, 2, 3, 1)\n        box_cls = box_cls.reshape(N, -1, C).sigmoid()\n        box_regression = reg_pred.view(N, 4, H, W).permute(0, 2, 3, 1)\n        box_regression = box_regression.reshape(N, -1, 4)\n        ctrness = ctrness.view(N, 1, H, W).permute(0, 2, 3, 1)\n        ctrness = ctrness.reshape(N, -1).sigmoid()\n\n        # if self.thresh_with_ctr is True, we multiply the classification\n        # scores with centerness scores before applying the threshold.\n        if self.thresh_with_ctr:\n            box_cls = box_cls * ctrness[:, :, None]\n        candidate_inds = box_cls > self.pre_nms_thresh\n        pre_nms_top_n = candidate_inds.view(N, -1).sum(1)\n        pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)\n\n        if not self.thresh_with_ctr:\n            box_cls = box_cls * ctrness[:, :, None]\n\n        results = []\n        for i in range(N):\n            per_box_cls = box_cls[i]\n            per_candidate_inds = candidate_inds[i]\n            per_box_cls = per_box_cls[per_candidate_inds]\n\n            per_candidate_nonzeros = per_candidate_inds.nonzero()\n            per_box_loc = per_candidate_nonzeros[:, 0]\n            per_class = per_candidate_nonzeros[:, 1]\n\n            per_box_regression = box_regression[i]\n            per_box_regression = per_box_regression[per_box_loc]\n            per_locations = locations[per_box_loc]\n\n            per_pre_nms_top_n = pre_nms_top_n[i]\n\n            if per_candidate_inds.sum().item() > per_pre_nms_top_n.item():\n                per_box_cls, top_k_indices = \\\n                    per_box_cls.topk(per_pre_nms_top_n, sorted=False)\n                per_class = per_class[top_k_indices]\n                per_box_regression = per_box_regression[top_k_indices]\n                per_locations = per_locations[top_k_indices]\n\n            detections = torch.stack([\n                per_locations[:, 0] - per_box_regression[:, 0],\n                per_locations[:, 1] - per_box_regression[:, 1],\n                per_locations[:, 0] + per_box_regression[:, 2],\n                per_locations[:, 1] + per_box_regression[:, 3],\n            ], dim=1)\n\n            boxlist = Instances(image_sizes[i])\n            boxlist.pred_boxes = Boxes(detections)\n            boxlist.scores = torch.sqrt(per_box_cls)\n            boxlist.pred_classes = per_class\n            boxlist.locations = per_locations\n\n            results.append(boxlist)\n\n        return results\n\n    def select_over_all_levels(self, boxlists):\n        num_images = len(boxlists)\n        results = []\n        for i in range(num_images):\n            # multiclass nms\n            result = ml_nms(boxlists[i], self.nms_thresh)\n            number_of_detections = len(result)\n\n            # Limit to max_per_image detections **over all classes**\n            if number_of_detections > self.fpn_post_nms_top_n > 0:\n                cls_scores = result.scores\n                image_thresh, _ = torch.kthvalue(\n                    cls_scores.cpu(),\n                    number_of_detections - self.fpn_post_nms_top_n + 1\n                )\n                keep = cls_scores >= image_thresh.item()\n                keep = torch.nonzero(keep).squeeze(1)\n                result = result[keep]\n            results.append(result)\n        return results\n'"
