file_path,api_count,code
train_net.py,0,"b'# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\n""""""\nVoVNet Training Script.\n\nThis script is a simplified version of the training script in detectron2/tools.\n""""""\n\nimport os\n\nimport detectron2.utils.comm as comm\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, launch\nfrom detectron2.evaluation import (\n    COCOEvaluator, \n    COCOPanopticEvaluator,\n    SemSegEvaluator,\n    DatasetEvaluators,\n    verify_results\n    )\nfrom detectron2.data import MetadataCatalog\n\nfrom vovnet import add_vovnet_config\n\n\nclass Trainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, ""inference"")\n        evaluator_list = []\n        evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n        if evaluator_type in [""sem_seg"", ""coco_panoptic_seg""]:\n            evaluator_list.append(\n                SemSegEvaluator(\n                    dataset_name,\n                    distributed=True,\n                    num_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES,\n                    ignore_label=cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,\n                    output_dir=output_folder,\n                )\n            )\n        if evaluator_type in [""coco"", ""coco_panoptic_seg""]:\n            evaluator_list.append(COCOEvaluator(dataset_name, cfg, True, output_folder))\n        if evaluator_type == ""coco_panoptic_seg"":\n            evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n\n        if len(evaluator_list) == 0:\n            raise NotImplementedError(\n                ""no Evaluator for the dataset {} with the type {}"".format(\n                    dataset_name, evaluator_type\n                )\n            )\n        elif len(evaluator_list) == 1:\n            return evaluator_list[0]\n        \n        return DatasetEvaluators(evaluator_list)\n\n\ndef setup(args):\n    """"""\n    Create configs and perform basic setups.\n    """"""\n    cfg = get_cfg()\n    add_vovnet_config(cfg)\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n    default_setup(cfg, args)\n    return cfg\n\n\ndef main(args):\n    cfg = setup(args)\n\n    if args.eval_only:\n        model = Trainer.build_model(cfg)\n        DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n            cfg.MODEL.WEIGHTS, resume=args.resume\n        )\n        res = Trainer.test(cfg, model)\n        if comm.is_main_process():\n            verify_results(cfg, res)\n        return res\n\n    trainer = Trainer(cfg)\n    trainer.resume_or_load(resume=args.resume)\n    return trainer.train()\n\n\nif __name__ == ""__main__"":\n    args = default_argument_parser().parse_args()\n    print(""Command Line Args:"", args)\n    launch(\n        main,\n        args.num_gpus,\n        num_machines=args.num_machines,\n        machine_rank=args.machine_rank,\n        dist_url=args.dist_url,\n        args=(args,),\n    )\n'"
vovnet/__init__.py,0,"b'# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\nfrom .config import add_vovnet_config\nfrom .vovnet import build_vovnet_fpn_backbone, build_vovnet_backbone\nfrom .mobilenet import build_mobilenetv2_fpn_backbone, build_mnv2_backbone'"
vovnet/config.py,0,"b'# -*- coding: utf-8 -*-\n# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\n\nfrom detectron2.config import CfgNode as CN\n\n\ndef add_vovnet_config(cfg):\n    """"""\n    Add config for VoVNet.\n    """"""\n    _C = cfg\n\n    _C.MODEL.VOVNET = CN()\n\n    _C.MODEL.VOVNET.CONV_BODY = ""V-39-eSE""\n    _C.MODEL.VOVNET.OUT_FEATURES = [""stage2"", ""stage3"", ""stage4"", ""stage5""]\n\n    # Options: FrozenBN, GN, ""SyncBN"", ""BN""\n    _C.MODEL.VOVNET.NORM = ""FrozenBN""\n\n    _C.MODEL.VOVNET.OUT_CHANNELS = 256\n\n    _C.MODEL.VOVNET.BACKBONE_OUT_CHANNELS = 256\n'"
vovnet/mobilenet.py,1,"b'# taken from https://github.com/tonylins/pytorch-mobilenet-v2/\n# Published by Ji Lin, tonylins\n# licensed under the  Apache License, Version 2.0, January 2004\n\nfrom torch import nn\nfrom torch.nn import BatchNorm2d\nfrom detectron2.layers import Conv2d, FrozenBatchNorm2d, ShapeSpec\nfrom detectron2.modeling.backbone.build import BACKBONE_REGISTRY\nfrom detectron2.modeling.backbone import Backbone\nfrom detectron2.modeling.backbone.fpn import FPN, LastLevelMaxPool\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        Conv2d(inp, oup, 3, stride, 1, bias=False),\n        FrozenBatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        Conv2d(inp, oup, 1, 1, 0, bias=False),\n        FrozenBatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                FrozenBatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                FrozenBatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                FrozenBatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                FrozenBatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                FrozenBatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(Backbone):\n    """"""\n    Should freeze bn\n    """"""\n    def __init__(self, cfg, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.return_features_indices = [3, 6, 13, 17]\n        self.return_features_num_channels = []\n        self.features = nn.ModuleList([conv_bn(3, input_channel, 2)])\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n                if len(self.features) - 1 in self.return_features_indices:\n                    self.return_features_num_channels.append(output_channel)\n\n        self._initialize_weights()\n        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_AT)\n\n    def _freeze_backbone(self, freeze_at):\n        for layer_index in range(freeze_at):\n            for p in self.features[layer_index].parameters():\n                p.requires_grad = False\n\n    def forward(self, x):\n        res = []\n        for i, m in enumerate(self.features):\n            x = m(x)\n            if i in self.return_features_indices:\n                res.append(x)\n        return {\'res{}\'.format(i + 2): r for i, r in enumerate(res)}\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, (2. / n) ** 0.5)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n@BACKBONE_REGISTRY.register()\ndef build_mnv2_backbone(cfg, input_shape):\n    """"""\n    Create a MobileNetV2 instance from config.\n    Returns:\n        MobileNetV2: a :class:`MobileNetV2` instance.\n    """"""\n    out_features = cfg.MODEL.RESNETS.OUT_FEATURES\n\n    out_feature_channels = {""res2"": 24, ""res3"": 32,\n                            ""res4"": 96, ""res5"": 320}\n    out_feature_strides = {""res2"": 4, ""res3"": 8, ""res4"": 16, ""res5"": 32}\n    model = MobileNetV2(cfg)\n    model._out_features = out_features\n    model._out_feature_channels = out_feature_channels\n    model._out_feature_strides = out_feature_strides\n    return model\n\n\n@BACKBONE_REGISTRY.register()\ndef build_mobilenetv2_fpn_backbone(cfg, input_shape: ShapeSpec):\n    """"""\n    Args:\n        cfg: a detectron2 CfgNode\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    """"""\n    bottom_up = build_mnv2_backbone(cfg, input_shape)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=LastLevelMaxPool(),\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone'"
vovnet/vovnet.py,3,"b'# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom detectron2.layers import FrozenBatchNorm2d, ShapeSpec, get_norm\nfrom detectron2.modeling.backbone import Backbone\nfrom detectron2.modeling.backbone.build import BACKBONE_REGISTRY\nfrom detectron2.modeling.backbone.fpn import FPN, LastLevelMaxPool\n\n__all__ = [""VoVNet"", ""build_vovnet_backbone"", ""build_vovnet_fpn_backbone""]\n\n_NORM = False\n\nVoVNet19_slim_dw_eSE = {\n    \'stem\': [64, 64, 64],\n    \'stage_conv_ch\': [64, 80, 96, 112],\n    \'stage_out_ch\': [112, 256, 384, 512],\n    ""layer_per_block"": 3,\n    ""block_per_stage"": [1, 1, 1, 1],\n    ""eSE"": True,\n    ""dw"" : True\n}\n\nVoVNet19_dw_eSE = {\n    \'stem\': [64, 64, 64],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 3,\n    ""block_per_stage"": [1, 1, 1, 1],\n    ""eSE"": True,\n    ""dw"" : True\n}\n\nVoVNet19_slim_eSE = {\n    \'stem\': [64, 64, 128],\n    \'stage_conv_ch\': [64, 80, 96, 112],\n    \'stage_out_ch\': [112, 256, 384, 512],\n    \'layer_per_block\': 3,\n    \'block_per_stage\': [1, 1, 1, 1],\n    \'eSE\' : True,\n    ""dw"" : False\n}\n\nVoVNet19_eSE = {\n    \'stem\': [64, 64, 128],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 3,\n    ""block_per_stage"": [1, 1, 1, 1],\n    ""eSE"": True,\n    ""dw"" : False\n}\n\nVoVNet39_eSE = {\n    \'stem\': [64, 64, 128],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 5,\n    ""block_per_stage"": [1, 1, 2, 2],\n    ""eSE"": True,\n    ""dw"" : False\n}\n\nVoVNet57_eSE = {\n    \'stem\': [64, 64, 128],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 5,\n    ""block_per_stage"": [1, 1, 4, 3],\n    ""eSE"": True,\n    ""dw"" : False\n}\n\nVoVNet99_eSE = {\n    \'stem\': [64, 64, 128],\n    ""stage_conv_ch"": [128, 160, 192, 224],\n    ""stage_out_ch"": [256, 512, 768, 1024],\n    ""layer_per_block"": 5,\n    ""block_per_stage"": [1, 3, 9, 3],\n    ""eSE"": True,\n    ""dw"" : False\n}\n\n_STAGE_SPECS = {\n    ""V-19-slim-dw-eSE"": VoVNet19_slim_dw_eSE,\n    ""V-19-dw-eSE"": VoVNet19_dw_eSE,\n    ""V-19-slim-eSE"": VoVNet19_slim_eSE,\n    ""V-19-eSE"": VoVNet19_eSE,\n    ""V-39-eSE"": VoVNet39_eSE,\n    ""V-57-eSE"": VoVNet57_eSE,\n    ""V-99-eSE"": VoVNet99_eSE,\n}\n\ndef dw_conv3x3(in_channels, out_channels, module_name, postfix,\n            stride=1, kernel_size=3, padding=1):\n    """"""3x3 convolution with padding""""""\n    return [\n        (\'{}_{}/dw_conv3x3\'.format(module_name, postfix),\n            nn.Conv2d(in_channels, out_channels,\n                      kernel_size=kernel_size,\n                      stride=stride,\n                      padding=padding,\n                      groups=out_channels,\n                      bias=False)),\n        (\'{}_{}/pw_conv1x1\'.format(module_name, postfix),\n            nn.Conv2d(in_channels, out_channels,\n                      kernel_size=1,\n                      stride=1,\n                      padding=0,\n                      groups=1,\n                      bias=False)),\n        (\'{}_{}/pw_norm\'.format(module_name, postfix), get_norm(_NORM, out_channels)),\n        (\'{}_{}/pw_relu\'.format(module_name, postfix), nn.ReLU(inplace=True)),\n    ]\n\ndef conv3x3(\n    in_channels, out_channels, module_name, postfix, stride=1, groups=1, kernel_size=3, padding=1\n):\n    """"""3x3 convolution with padding""""""\n    return [\n        (\n            f""{module_name}_{postfix}/conv"",\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                groups=groups,\n                bias=False,\n            ),\n        ),\n        (f""{module_name}_{postfix}/norm"", get_norm(_NORM, out_channels)),\n        (f""{module_name}_{postfix}/relu"", nn.ReLU(inplace=True)),\n    ]\n\n\ndef conv1x1(\n    in_channels, out_channels, module_name, postfix, stride=1, groups=1, kernel_size=1, padding=0\n):\n    """"""1x1 convolution with padding""""""\n    return [\n        (\n            f""{module_name}_{postfix}/conv"",\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                groups=groups,\n                bias=False,\n            ),\n        ),\n        (f""{module_name}_{postfix}/norm"", get_norm(_NORM, out_channels)),\n        (f""{module_name}_{postfix}/relu"", nn.ReLU(inplace=True)),\n    ]\n\n\nclass Hsigmoid(nn.Module):\n    def __init__(self, inplace=True):\n        super(Hsigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return F.relu6(x + 3.0, inplace=self.inplace) / 6.0\n\n\nclass eSEModule(nn.Module):\n    def __init__(self, channel, reduction=4):\n        super(eSEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(channel, channel, kernel_size=1, padding=0)\n        self.hsigmoid = Hsigmoid()\n\n    def forward(self, x):\n        input = x\n        x = self.avg_pool(x)\n        x = self.fc(x)\n        x = self.hsigmoid(x)\n        return input * x\n\n\nclass _OSA_module(nn.Module):\n    def __init__(\n        self, in_ch, stage_ch, concat_ch, layer_per_block, module_name, SE=False, identity=False, depthwise=False\n    ):\n\n        super(_OSA_module, self).__init__()\n\n        self.identity = identity\n        self.depthwise = depthwise\n        self.isReduced = False\n        self.layers = nn.ModuleList()\n        in_channel = in_ch\n        if self.depthwise and in_channel != stage_ch:\n            self.isReduced = True\n            self.conv_reduction = nn.Sequential(\n                OrderedDict(conv1x1(in_channel, stage_ch, \n                  ""{}_reduction"".format(module_name), ""0"")))            \n        for i in range(layer_per_block):\n            if self.depthwise:\n                self.layers.append(\n                    nn.Sequential(OrderedDict(dw_conv3x3(stage_ch, stage_ch, module_name, i))))\n            else:\n                self.layers.append(\n                    nn.Sequential(OrderedDict(conv3x3(in_channel, stage_ch, module_name, i)))\n                )\n            in_channel = stage_ch\n\n        # feature aggregation\n        in_channel = in_ch + layer_per_block * stage_ch\n        self.concat = nn.Sequential(\n            OrderedDict(conv1x1(in_channel, concat_ch, module_name, ""concat""))\n        )\n\n        self.ese = eSEModule(concat_ch)\n\n    def forward(self, x):\n\n        identity_feat = x\n\n        output = []\n        output.append(x)\n        if self.depthwise and self.isReduced:\n            x = self.conv_reduction(x)\n        for layer in self.layers:\n            x = layer(x)\n            output.append(x)\n\n        x = torch.cat(output, dim=1)\n        xt = self.concat(x)\n\n        xt = self.ese(xt)\n\n        if self.identity:\n            xt = xt + identity_feat\n\n        return xt\n\n\nclass _OSA_stage(nn.Sequential):\n    def __init__(\n        self, \n        in_ch, \n        stage_ch, \n        concat_ch, \n        block_per_stage, \n        layer_per_block, \n        stage_num, SE=False, \n        depthwise=False):\n\n        super(_OSA_stage, self).__init__()\n\n        if not stage_num == 2:\n            self.add_module(""Pooling"", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True))\n\n        if block_per_stage != 1:\n            SE = False\n        module_name = f""OSA{stage_num}_1""\n        self.add_module(\n            module_name, _OSA_module(in_ch, stage_ch, concat_ch, layer_per_block, module_name, SE, depthwise=depthwise)\n        )\n        for i in range(block_per_stage - 1):\n            if i != block_per_stage - 2:  # last block\n                SE = False\n            module_name = f""OSA{stage_num}_{i + 2}""\n            self.add_module(\n                module_name,\n                _OSA_module(\n                    concat_ch, stage_ch, concat_ch, layer_per_block, module_name, SE, identity=True, depthwise=depthwise\n                ),\n            )\n\n\nclass VoVNet(Backbone):\n    def __init__(self, cfg, input_ch, out_features=None):\n        """"""\n        Args:\n            input_ch(int) : the number of input channel\n            out_features (list[str]): name of the layers whose outputs should\n                be returned in forward. Can be anything in ""stem"", ""stage2"" ...\n        """"""\n        super(VoVNet, self).__init__()\n\n        global _NORM\n        _NORM = cfg.MODEL.VOVNET.NORM\n\n        stage_specs = _STAGE_SPECS[cfg.MODEL.VOVNET.CONV_BODY]\n\n        stem_ch = stage_specs[""stem""]\n        config_stage_ch = stage_specs[""stage_conv_ch""]\n        config_concat_ch = stage_specs[""stage_out_ch""]\n        block_per_stage = stage_specs[""block_per_stage""]\n        layer_per_block = stage_specs[""layer_per_block""]\n        SE = stage_specs[""eSE""]\n        depthwise = stage_specs[""dw""]\n\n        self._out_features = out_features\n\n        # Stem module\n        conv_type = dw_conv3x3 if depthwise else conv3x3\n        stem = conv3x3(input_ch, stem_ch[0], ""stem"", ""1"", 2)\n        stem += conv_type(stem_ch[0], stem_ch[1], ""stem"", ""2"", 1)\n        stem += conv_type(stem_ch[1], stem_ch[2], ""stem"", ""3"", 2)\n        self.add_module(""stem"", nn.Sequential((OrderedDict(stem))))\n        current_stirde = 4\n        self._out_feature_strides = {""stem"": current_stirde, ""stage2"": current_stirde}\n        self._out_feature_channels = {""stem"": stem_ch[2]}\n\n        stem_out_ch = [stem_ch[2]]\n        in_ch_list = stem_out_ch + config_concat_ch[:-1]\n        # OSA stages\n        self.stage_names = []\n        for i in range(4):  # num_stages\n            name = ""stage%d"" % (i + 2)  # stage 2 ... stage 5\n            self.stage_names.append(name)\n            self.add_module(\n                name,\n                _OSA_stage(\n                    in_ch_list[i],\n                    config_stage_ch[i],\n                    config_concat_ch[i],\n                    block_per_stage[i],\n                    layer_per_block,\n                    i + 2,\n                    SE,\n                    depthwise,\n                ),\n            )\n\n            self._out_feature_channels[name] = config_concat_ch[i]\n            if not i == 0:\n                self._out_feature_strides[name] = current_stirde = int(current_stirde * 2)\n\n        # initialize weights\n        self._initialize_weights()\n        # Optionally freeze (requires_grad=False) parts of the backbone\n        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_AT)\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n\n    def _freeze_backbone(self, freeze_at):\n        if freeze_at < 0:\n            return\n\n        for stage_index in range(freeze_at):\n            if stage_index == 0:\n                m = self.stem  # stage 0 is the stem\n            else:\n                m = getattr(self, ""stage"" + str(stage_index + 1))\n            for p in m.parameters():\n                p.requires_grad = False\n                FrozenBatchNorm2d.convert_frozen_batchnorm(self)\n\n    def forward(self, x):\n        outputs = {}\n        x = self.stem(x)\n        if ""stem"" in self._out_features:\n            outputs[""stem""] = x\n        for name in self.stage_names:\n            x = getattr(self, name)(x)\n            if name in self._out_features:\n                outputs[name] = x\n\n        return outputs\n\n    def output_shape(self):\n        return {\n            name: ShapeSpec(\n                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n            )\n            for name in self._out_features\n        }\n\n\n@BACKBONE_REGISTRY.register()\ndef build_vovnet_backbone(cfg, input_shape):\n    """"""\n    Create a VoVNet instance from config.\n\n    Returns:\n        VoVNet: a :class:`VoVNet` instance.\n    """"""\n    out_features = cfg.MODEL.VOVNET.OUT_FEATURES\n    return VoVNet(cfg, input_shape.channels, out_features=out_features)\n\n\n@BACKBONE_REGISTRY.register()\ndef build_vovnet_fpn_backbone(cfg, input_shape: ShapeSpec):\n    """"""\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    """"""\n    bottom_up = build_vovnet_backbone(cfg, input_shape)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=LastLevelMaxPool(),\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone\n'"
