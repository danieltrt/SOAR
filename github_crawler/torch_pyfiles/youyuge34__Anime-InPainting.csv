file_path,api_count,code
main.py,10,"b'import os\nimport cv2\nimport random\nimport numpy as np\nimport torch\nimport argparse\nfrom shutil import copyfile\nfrom src.config import Config\nfrom src.edge_connect import EdgeConnect\n\n\ndef main(mode=None, config=None):\n    r""""""starts the model\n\n    Args:\n        mode (int): 1: train, 2: test, 3: eval, reads from config file if not specified\n                    4: demo_patch,\n    """"""\n\n    if mode == 4:\n        config = load_config_demo(mode, config=config)\n    else:\n        config = load_config(mode)\n\n    # init environment\n    if (config.DEVICE == 1 or config.DEVICE is None) and torch.cuda.is_available():\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = \',\'.join(str(e) for e in config.GPU)\n        config.DEVICE = torch.device(""cuda"")\n        torch.backends.cudnn.benchmark = True  # cudnn auto-tuner\n    else:\n        config.DEVICE = torch.device(""cpu"")\n    # print(torch.cuda.is_available())\n    print(\'DEVICE is:\', config.DEVICE)\n\n    # set cv2 running threads to 1 (prevents deadlocks with pytorch dataloader)\n    cv2.setNumThreads(0)\n\n    # initialize random seed\n    torch.manual_seed(config.SEED)\n    torch.cuda.manual_seed_all(config.SEED)\n    np.random.seed(config.SEED)\n    random.seed(config.SEED)\n\n    # enable the cudnn auto-tuner for hardware.\n    torch.backends.cudnn.benchmark = True\n\n    # build the model and initialize\n    model = EdgeConnect(config)\n    model.load()\n\n    # model training\n    if config.MODE == 1:\n        config.print()\n        print(\'\\nstart training...\\n\')\n        model.train()\n\n    # model test\n    elif config.MODE == 2:\n        print(\'\\nstart testing...\\n\')\n        # import time\n        # start = time.time()\n        with torch.no_grad():\n            model.test()\n        # print(time.time() - start)\n\n    # eval mode\n    elif config.MODE == 3:\n        print(\'\\nstart eval...\\n\')\n        with torch.no_grad():\n            model.eval()\n\n    elif config.MODE == 4:\n        if config.DEBUG:\n            config.print()\n        print(\'model prepared.\')\n        return model\n\n\ndef load_config(mode=None):\n    r""""""loads model config \n\n    Args:\n        mode (int): 1: train, 2: test, 3: eval, reads from config file if not specified\n    """"""\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--path\', \'--checkpoints\', type=str, default=\'./checkpoints\',\n                        help=\'model checkpoints path (default: ./checkpoints)\')\n    parser.add_argument(\'--model\', type=int, choices=[1, 2, 3, 4],\n                        help=\'1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model\')\n\n    # test mode\n    if mode == 2:\n        parser.add_argument(\'--input\', type=str, help=\'path to the input images directory or an input image\')\n        parser.add_argument(\'--mask\', type=str, help=\'path to the masks directory or a mask file\')\n        parser.add_argument(\'--edge\', type=str, help=\'path to the edges directory or an edge file\')\n        parser.add_argument(\'--output\', type=str, help=\'path to the output directory\')\n\n    args = parser.parse_args()\n    config_path = os.path.join(args.path, \'config.yml\')\n\n    # create checkpoints path if does\'t exist\n    if not os.path.exists(args.path):\n        os.makedirs(args.path)\n\n    # copy config template if does\'t exist\n    if not os.path.exists(config_path):\n        copyfile(\'./config.yml.example\', config_path)\n\n    # load config file\n    config = Config(config_path)\n\n    # train mode\n    if mode == 1:\n        config.MODE = 1\n        if args.model:\n            config.MODEL = args.model \n\n        if config.SKIP_PHASE2 is None:\n            config.SKIP_PHASE2 = 0\n        if config.MODEL == 2 and config.SKIP_PHASE2 == 1:\n            raise Exception(""MODEL is 2, cannot skip phase2! trun config.SKIP_PHASE2 off or just use MODEL 3."")\n\n    # test mode\n    elif mode == 2:\n        config.MODE = 2\n        config.MODEL = args.model if args.model is not None else 3\n        config.INPUT_SIZE = 0\n\n        if args.input is not None:\n            config.TEST_FLIST = args.input\n\n        if args.mask is not None:\n            config.TEST_MASK_FLIST = args.mask\n\n        if args.edge is not None:\n            config.TEST_EDGE_FLIST = args.edge\n\n        if args.output is not None:\n            config.RESULTS = args.output\n\n    # eval mode\n    elif mode == 3:\n        config.MODE = 3\n        config.MODEL = args.model if args.model is not None else 3\n\n    return config\n\n\ndef load_config_demo(mode, config):\n    r""""""loads model config\n\n    Args:\n        mode (int): 4: demo_patch\n    """"""\n    print(\'load_config_demo----->\')\n    if mode == 4:\n        config.MODE = 4\n        config.MODEL = 3\n        config.INPUT_SIZE = 0\n\n    return config\n\n\nif __name__ == ""__main__"":\n    main()\n'"
test.py,0,"b""from main import main\n\nif __name__ == '__main__':\n\tmain(mode=2)"""
tool_patch.py,0,"b'#!/usr/bin/env python\n# author: youyuge34@github\n""""""\n===============================================================================\nInteractive Image Patching Tool using Edge-Connect algorithm.\n\n\nUSAGE:\n    python tool_patch.py --path <your weights directory path>\n                         --edge (optional to open edge window)\n\n\nREADME FIRST:\n    Two windows will show up, one for input and one for output.\n    if `--edge` arg is input, another edge window will show up.\n    [Important] Switch your typewriting into ENG first.\n\n    At first, in input window, draw black in the missing part using\nmouse left button. Then press \'n\' to patch the image (once or a few times)\nFor any finer touch-ups, you can press any of the keys below and draw lines on\nthe areas you want. Then again press \'n\' for updating the output.\n    In input window, left mouse is to draw the black mask which means the defect.\n    In edge window, left mouse is to draw the edge while right mouse is to erase the edges.\n    Press \'[\' and \']\' to resize the brush thickness, as in PhotoShop.\n    Finally, press \'s\' to save the output.\n\nKey \'[\' - To make the brush thickness smaller\nKey \']\' - To make the brush thickness larger\nKey \'0\' - Todo\nKey \'1\' - Todo\n\nKey \'n\' - To patch the black part of image, just use input image\nKey \'e\' - To patch the black part of image, use the input image and edit edge\nKey \'r\' - To reset the setup\nKey \'s\' - To save the output\nKey \'q\' - To quit\n===============================================================================\n""""""\n\n# Python 2/3 compatibility\nfrom __future__ import print_function\n\nimport argparse\nimport glob\n\nfrom easygui import *\nimport numpy as np\nimport cv2 as cv\nimport sys\nimport os\nimport shutil\nfrom src.config import Config\nfrom main import main\n\nBLUE = [255, 0, 0]  # rectangle color\nRED = [0, 0, 255]  # PR BG\nGREEN = [0, 255, 0]  # PR FG\nBLACK = [0, 0, 0]  # sure BG\nWHITE = [255, 255, 255]  # sure FG\n\nDRAW_MASK = {\'color\': BLACK, \'val\': 255}\n\nradius = 3  # brush radius\ndrawing = False\ndrawing_edge_l = False\ndrawing_edge_r = False\nvalue = DRAW_MASK\nTHICKNESS = -1  # solid brush circle \xe5\xae\x9e\xe5\xbf\x83\xe5\x9c\x86\n\n\ndef onmouse_input(event, x, y, flags, param):\n    """"""\n    mouse callback function, whenever mouse move or click in input window this function is called.\n    \xe5\x8f\xaa\xe8\xa6\x81\xe9\xbc\xa0\xe6\xa0\x87\xe5\x9c\xa8input\xe7\xaa\x97\xe5\x8f\xa3\xe4\xb8\x8a\xe7\xa7\xbb\xe5\x8a\xa8(\xe7\x82\xb9\xe5\x87\xbb\xef\xbc\x89\xef\xbc\x8c\xe6\xad\xa4\xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\xb1\xe4\xbc\x9a\xe8\xa2\xab\xe5\x9b\x9e\xe8\xb0\x83\xe6\x89\xa7\xe8\xa1\x8c\n    """"""\n    # to change the variable outside of the function\n    # \xe4\xb8\xba\xe6\x96\xb9\xe6\xb3\x95\xe4\xbd\x93\xe5\xa4\x96\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xe8\xb5\x8b\xe5\x80\xbc\xef\xbc\x8c\xe5\xa3\xb0\xe6\x98\x8eglobal\n    global img, img2, drawing, value, mask, ix, iy, rect_over\n    # print(x,y)\n\n    # draw touchup curves\n    if event == cv.EVENT_LBUTTONDOWN:\n        drawing = True\n        cv.circle(img, (x, y), radius, value[\'color\'], THICKNESS, lineType=cv.LINE_AA)\n        cv.circle(mask, (x, y), radius, value[\'val\'], THICKNESS, lineType=cv.LINE_AA)\n\n    elif drawing is True and event == cv.EVENT_MOUSEMOVE:\n        cv.circle(img, (x, y), radius, value[\'color\'], THICKNESS, lineType=cv.LINE_AA)\n        cv.circle(mask, (x, y), radius, value[\'val\'], THICKNESS, lineType=cv.LINE_AA)\n\n    elif drawing is True and event == cv.EVENT_LBUTTONUP:\n        drawing = False\n        cv.circle(img, (x, y), radius, value[\'color\'], THICKNESS, lineType=cv.LINE_AA)\n        cv.circle(mask, (x, y), radius, value[\'val\'], THICKNESS, lineType=cv.LINE_AA)\n\n\ndef onmouse_edge(event, x, y, flags, param):\n    """"""\n    mouse callback function, whenever mouse move or click in edge window this function is called.\n    \xe5\x8f\xaa\xe8\xa6\x81\xe9\xbc\xa0\xe6\xa0\x87\xe5\x9c\xa8edge\xe7\xaa\x97\xe5\x8f\xa3\xe4\xb8\x8a\xe7\xa7\xbb\xe5\x8a\xa8(\xe7\x82\xb9\xe5\x87\xbb\xef\xbc\x89\xef\xbc\x8c\xe6\xad\xa4\xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\xb1\xe4\xbc\x9a\xe8\xa2\xab\xe5\x9b\x9e\xe8\xb0\x83\xe6\x89\xa7\xe8\xa1\x8c\n    """"""\n    # to change the variable outside of the function\n    # \xe4\xb8\xba\xe6\x96\xb9\xe6\xb3\x95\xe4\xbd\x93\xe5\xa4\x96\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xe8\xb5\x8b\xe5\x80\xbc\xef\xbc\x8c\xe5\xa3\xb0\xe6\x98\x8eglobal\n    global img, img2, drawing_edge_l, drawing_edge_r, value, mask, ix, iy, edge\n    # print(x, y)\n\n    # draw touchup curves\n    if event == cv.EVENT_LBUTTONDOWN:\n        drawing_edge_l = True\n        # cv.circle(edge, (x, y), 1, WHITE, THICKNESS, lineType=cv.LINE_AA)\n        edge[y, x] = 255\n\n    elif drawing_edge_l is True and event == cv.EVENT_MOUSEMOVE:\n        # cv.circle(edge, (x, y), 1, WHITE, THICKNESS, lineType=4)\n        edge[y, x] = 255\n\n    elif drawing_edge_l is True and event == cv.EVENT_LBUTTONUP:\n        drawing_edge_l = False\n        # cv.circle(edge, (x, y), 1, WHITE, THICKNESS, lineType=cv.LINE_AA)\n        edge[y, x] = 255\n\n    elif event == cv.EVENT_RBUTTONDOWN:\n        drawing_edge_r = True\n        cv.circle(edge, (x, y), radius, BLACK, THICKNESS, lineType=cv.LINE_AA)\n\n    elif drawing_edge_r is True and event == cv.EVENT_MOUSEMOVE:\n        cv.circle(edge, (x, y), radius, BLACK, THICKNESS, lineType=cv.LINE_AA)\n\n    elif drawing_edge_r is True and event == cv.EVENT_RBUTTONUP:\n        drawing_edge_r = False\n        cv.circle(edge, (x, y), radius, BLACK, THICKNESS, lineType=cv.LINE_AA)\n\n\ndef check_load(args):\n    """"""\n    Check the directory and weights files. Load the config file.\n    """"""\n    if not os.path.exists(args.path):\n        raise NotADirectoryError(\'Path <\' + str(args.path) + \'> does not exist!\')\n\n    edge_weight_files = list(glob.glob(os.path.join(args.path, \'EdgeModel_gen*.pth\')))\n    if len(edge_weight_files) == 0:\n        raise FileNotFoundError(\'Weights file <EdgeModel_gen*.pth> cannot be found under path: \' + args.path)\n    inpaint_weight_files = list(glob.glob(os.path.join(args.path, \'InpaintingModel_gen*.pth\')))\n    if len(inpaint_weight_files) == 0:\n        raise FileNotFoundError(\'Weights file <InpaintingModel_gen*.pth> cannot be found under path: \' + args.path)\n\n    config_path = os.path.join(args.path, \'config.yml\')\n    # copy config template if does\'t exist\n    if not os.path.exists(config_path):\n        shutil.copyfile(\'./config.yml.example\', config_path)\n\n    # load config file\n    config = Config(config_path)\n\n    return config\n\n\ndef load_model(config):\n    """"""\n    Load model, the key function to interact with backend.\n    """"""\n    model = main(mode=4, config=config)\n    return model\n\n\ndef model_process(img, mask, edge=None):\n    """"""\n    Patch the image with mask. Key function.\n    :param img: Input dimension 3\n    :param mask: Mask dimension 2\n    :return:\n    """"""\n    # print(img.shape, mask.shape)\n    mask[mask > 0] = 255\n    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n    if edge is None:\n        result, edges = model.test_img_with_mask(img, mask)\n    else:\n        result, edges = model.test_img_with_mask(img, mask, edge_edit=edge)\n    result = cv.cvtColor(result, cv.COLOR_RGB2BGR)\n    return result, edges\n\n\nif __name__ == \'__main__\':\n\n    # print documentation\n    print(__doc__)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-p\', \'--path\', type=str, help=\'path of model weights files <.pth>\')\n    parser.add_argument(\'-e\', \'--edge\', action=\'store_true\', help=\'open the edge edit window\')\n    args = parser.parse_args()\n\n    # check the exist of path and the weights files\n    config = check_load(args)\n    model = load_model(config)\n\n    # image: absolute image path\n    image = fileopenbox(msg=\'Select a image\', title=\'Select\', filetypes=[[\'*.png\', \'*.jpg\', \'*.jpeg\', \'Image Files\']])\n    if image is None:\n        print(\'\\nPlease select a image.\')\n        exit()\n    else:\n        print(\'Image selected: \' + image)\n\n    img = cv.imread(image)\n\n    # resize the photo if it is too small\n    if max(img.shape[0], img.shape[1]) < 256:\n        img = cv.resize(img, dsize=(0, 0), fx=1.5, fy=1.5, interpolation=cv.INTER_CUBIC)\n    img2 = img.copy()  # a copy of original image\n    mask = np.zeros(img.shape[:2], dtype=np.uint8)  # mask initialized to PR_BG\n    output = np.zeros(img.shape, np.uint8)  # output image to be shown\n\n    # input and output windows\n    cv.namedWindow(\'output\')\n    cv.namedWindow(\'input\')\n    cv.setMouseCallback(\'input\', onmouse_input)\n    cv.moveWindow(\'input\', img.shape[1] + 20, 90)  # move input window\n\n    if args.edge:\n        edge = np.zeros(img.shape, np.uint8)\n        cv.namedWindow(\'edge\')\n        cv.setMouseCallback(\'edge\', onmouse_edge)\n        cv.moveWindow(\'edge\', img.shape[1] + 40, 90)\n\n    while 1:\n        cv.imshow(\'output\', output)\n        cv.imshow(\'input\', img)\n        if args.edge:\n            cv.imshow(\'edge\', edge)\n        k = cv.waitKey(200)\n\n        # key bindings\n        if k == 27 or k == ord(\'q\'):  # esc to exit\n            break\n        elif k == ord(\'0\'):  # BG drawing\n            print("" mark background regions with left mouse button \\n"")\n            value = DRAW_MASK\n\n        # TODO: edge model\n        # elif k == ord(\'1\'):  # FG drawing\n        #     print("" mark foreground regions with left mouse button \\n"")\n        #     value = DRAW_FG\n\n        elif k == ord(\'r\'):  # reset everything\n            print(""resetting \\n"")\n            drawing = False\n            value = DRAW_MASK\n            img = img2.copy()\n            mask = np.zeros(img.shape[:2], dtype=np.uint8)  # mask initialized to PR_BG\n            output = np.zeros(img.shape, np.uint8)  # output image to be shown\n            if args.edge:\n                edge = np.zeros(img.shape, np.uint8)\n                drawing_edge_r = False\n                drawing_edge_l = False\n\n        elif k == ord(\'n\'):  # begin to path the image\n            print(""\\nPatching using input image..."")\n            output, edge = model_process(img, mask)\n            print(""\\nPatched!"")\n        elif k == ord(\'e\') and args.edge:  # begin to path the image\n            print(""\\nPatching using input and edge..."")\n            output, _ = model_process(img, mask, edge)\n            print(""\\nPatched!"")\n        elif k == ord(\'[\'):\n            radius = 1 if radius == 1 else radius - 1\n            print(\'Brush thickness is\', radius)\n        elif k == ord(\']\'):\n            radius += 1\n            print(\'Brush thickness is\', radius)\n        elif k == ord(\'s\'):\n            path = filesavebox(\'save\', \'save the output.\', default=\'patched_\' + os.path.basename(image),\n                               filetypes=[[\'*.jpg\', \'jpg\'], [\'*.png\', \'png\']])\n            if path:\n                if not path.endswith(\'.jpg\') and not path.endswith(\'.png\'):\n                    path = str(path) + \'.png\'\n                cv.imwrite(path, output)\n                print(\'Patched image is saved to\', path)\n    cv.destroyAllWindows()\n'"
train.py,0,"b""from main import main\n\nif __name__ == '__main__':\n\tmain(mode=1)"""
scripts/fid_score.py,3,"b'#!/usr/bin/env python3\n""""""Calculates the Frechet Inception Distance (FID) to evalulate GANs\nThe FID metric calculates the distance between two distributions of images.\nTypically, we have summary statistics (mean & covariance matrix) of one\nof these distributions, while the 2nd distribution is given by a GAN.\nWhen run as a stand-alone program, it compares the distribution of\nimages that are stored as PNG/JPEG at a specified location with a\ndistribution given by summary statistics (in pickle format).\nThe FID is calculated by assuming that X_1 and X_2 are the activations of\nthe pool_3 layer of the inception net for generated samples and real world\nsamples respectivly.\nSee --help to see further details.\nCode apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\nof Tensorflow\nCopyright 2018 Institute of Bioinformatics, JKU Linz\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n   http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nimport pathlib\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\nimport torch\nimport numpy as np\nfrom scipy.misc import imread\nfrom scipy import linalg\nfrom torch.autograd import Variable\nfrom torch.nn.functional import adaptive_avg_pool2d\n\nfrom inception import InceptionV3\n\n\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--path\', type=str, nargs=2, help=(\'Path to the generated images or to .npz statistic files\'))\nparser.add_argument(\'--batch-size\', type=int, default=64, help=\'Batch size to use\')\nparser.add_argument(\'--dims\', type=int, default=2048, choices=list(InceptionV3.BLOCK_INDEX_BY_DIM), help=(\'Dimensionality of Inception features to use. By default, uses pool3 features\'))\nparser.add_argument(\'-c\', \'--gpu\', default=\'\', type=str, help=\'GPU to use (leave blank for CPU only)\')\n\n\ndef get_activations(images, model, batch_size=64, dims=2048,\n                    cuda=False, verbose=False):\n    """"""Calculates the activations of the pool_3 layer for all images.\n    Params:\n    -- images      : Numpy array of dimension (n_images, 3, hi, wi). The values\n                     must lie between 0 and 1.\n    -- model       : Instance of inception model\n    -- batch_size  : the images numpy array is split into batches with\n                     batch size batch_size. A reasonable batch size depends\n                     on the hardware.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the number\n                     of calculated batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, dims) that contains the\n       activations of the given tensor when feeding inception with the\n       query tensor.\n    """"""\n    model.eval()\n\n    d0 = images.shape[0]\n    if batch_size > d0:\n        print((\'Warning: batch size is bigger than the data size. \'\n               \'Setting batch size to data size\'))\n        batch_size = d0\n\n    n_batches = d0 // batch_size\n    n_used_imgs = n_batches * batch_size\n\n    pred_arr = np.empty((n_used_imgs, dims))\n    for i in range(n_batches):\n        if verbose:\n            print(\'\\rPropagating batch %d/%d\' % (i + 1, n_batches),\n                  end=\'\', flush=True)\n        start = i * batch_size\n        end = start + batch_size\n\n        batch = torch.from_numpy(images[start:end]).type(torch.FloatTensor)\n        batch = Variable(batch, volatile=True)\n        if cuda:\n            batch = batch.cuda()\n\n        pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n        if pred.shape[2] != 1 or pred.shape[3] != 1:\n            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n        pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)\n\n    if verbose:\n        print(\' done\')\n\n    return pred_arr\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    """"""Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n    Stable version by Dougal J. Sutherland.\n    Params:\n    -- mu1   : Numpy array containing the activations of a layer of the\n               inception net (like returned by the function \'get_predictions\')\n               for generated samples.\n    -- mu2   : The sample mean over activations, precalculated on an \n               representive data set.\n    -- sigma1: The covariance matrix over activations for generated samples.\n    -- sigma2: The covariance matrix over activations, precalculated on an \n               representive data set.\n    Returns:\n    --   : The Frechet Distance.\n    """"""\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        \'Training and test mean vectors have different lengths\'\n    assert sigma1.shape == sigma2.shape, \\\n        \'Training and test covariances have different dimensions\'\n\n    diff = mu1 - mu2\n\n    # Product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = (\'fid calculation produces singular product; \'\n               \'adding %s to diagonal of cov estimates\') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    # Numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError(\'Imaginary component {}\'.format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) +\n            np.trace(sigma2) - 2 * tr_covmean)\n\n\ndef calculate_activation_statistics(images, model, batch_size=64,\n                                    dims=2048, cuda=False, verbose=False):\n    """"""Calculation of the statistics used by the FID.\n    Params:\n    -- images      : Numpy array of dimension (n_images, 3, hi, wi). The values\n                     must lie between 0 and 1.\n    -- model       : Instance of inception model\n    -- batch_size  : The images numpy array is split into batches with\n                     batch size batch_size. A reasonable batch size\n                     depends on the hardware.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the\n                     number of calculated batches is reported.\n    Returns:\n    -- mu    : The mean over samples of the activations of the pool_3 layer of\n               the inception model.\n    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n               the inception model.\n    """"""\n    act = get_activations(images, model, batch_size, dims, cuda, verbose)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma\n\n\ndef _compute_statistics_of_path(path, model, batch_size, dims, cuda):\n    npz_file = os.path.join(path, \'statistics.npz\')\n    if os.path.exists(npz_file):\n        f = np.load(npz_file)\n        m, s = f[\'mu\'][:], f[\'sigma\'][:]\n        f.close()\n    else:\n        path = pathlib.Path(path)\n        files = list(path.glob(\'*.jpg\')) + list(path.glob(\'*.png\'))\n\n        imgs = np.array([imread(str(fn)).astype(np.float32) for fn in files])\n\n        # Bring images to shape (B, 3, H, W)\n        imgs = imgs.transpose((0, 3, 1, 2))\n\n        # Rescale images to be between 0 and 1\n        imgs /= 255\n\n        m, s = calculate_activation_statistics(imgs, model, batch_size, dims, cuda)\n        np.savez(npz_file, mu=m, sigma=s)\n\n    return m, s\n\n\ndef calculate_fid_given_paths(paths, batch_size, cuda, dims):\n    """"""Calculates the FID of two paths""""""\n    for p in paths:\n        if not os.path.exists(p):\n            raise RuntimeError(\'Invalid path: %s\' % p)\n\n    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n\n    model = InceptionV3([block_idx])\n    if cuda:\n        model.cuda()\n\n    print(\'calculate path1 statistics...\')\n    m1, s1 = _compute_statistics_of_path(paths[0], model, batch_size, dims, cuda)\n    print(\'calculate path2 statistics...\')\n    m2, s2 = _compute_statistics_of_path(paths[1], model, batch_size, dims, cuda)\n    print(\'calculate frechet distance...\')\n    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n\n    return fid_value\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    fid_value = calculate_fid_given_paths(args.path,\n                                          args.batch_size,\n                                          args.gpu != \'\',\n                                          args.dims)\n    print(\'FID: \', round(fid_value, 4))\n'"
scripts/flist.py,0,"b""import os\nimport argparse\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--path', type=str, help='path to the dataset')\nparser.add_argument('--output', type=str, help='path to the file list')\nargs = parser.parse_args()\n\next = {'.jpg', '.png'}\n\nimages = []\nfor root, dirs, files in os.walk(args.path):\n    print('loading ' + root)\n    for file in files:\n        if os.path.splitext(file)[1] in ext:\n            images.append(os.path.join(root, file))\n\nimages = sorted(images)\nnp.savetxt(args.output, images, fmt='%s')"""
scripts/flist_train_split.py,0,"b""import os\nimport argparse\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--path', type=str, help='path to the dataset')\nparser.add_argument('--train', type=int, default=28, help='number of train in a iter')\nparser.add_argument('--val', type=int, default=1, help='number of val in a iter')\nparser.add_argument('--test', type=int, default=1, help='number of test in a iter')\nparser.add_argument('--output', type=str, help='path to the three file lists')\nargs = parser.parse_args()\n\next = {'.jpg', '.png'}\ntotal = args.train + args.val + args.test\nimages_train = []\nimages_val = []\nimages_test = []\n\nnum = 1\nfor root, dirs, files in os.walk(args.path):\n    print('loading ' + root)\n    for file in files:\n        if os.path.splitext(file)[1] in ext:\n            path = os.path.join(root, file)\n            if num % total > (args.val + args.test) or num % total == 0:\n                images_train.append(path)\n            elif num % total <= args.val and num % total > 0:\n                images_val.append(path)\n            else:\n                images_test.append(path)\n            num += 1\n\nimages_train.sort()\nimages_val.sort()\nimages_test.sort()\n\nprint('train number =', len(images_train))\nprint('val number =', len(images_val))\nprint('test number =', len(images_test))\n\nif not os.path.exists(args.output):\n    os.mkdir(args.output)\nnp.savetxt(os.path.join(args.output, 'train.flist'), images_train, fmt='%s')\nnp.savetxt(os.path.join(args.output, 'val.flist'), images_val, fmt='%s')\nnp.savetxt(os.path.join(args.output, 'test.flist'), images_test, fmt='%s')\n"""
scripts/getchu_crawler.py,0,"b'import requests\nimport os\nimport traceback\nfrom bs4 import BeautifulSoup\n\nHEADERS = {\n    ""User-Agent"": ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:47.0) Gecko/20100101 Firefox/47.0"",\n    \'Connection\': \'keep-alive\'\n}\n\nPROXIES = {\n    \'http\': \'socks5://127.0.0.1:1080\',\n    \'https\': \'socks5://127.0.0.1:1080\',\n}\n\nif __name__ == ""__main__"":\n\turl = \'http://www.getchu.com\'\n\tr = requests.get(url,proxies = PROXIES, timeout=20, headers=HEADERS)\n\t# r.encoding = \'utf-8\'\n\tprint(r.text[:10])\n\tprint(r.status_code)'"
scripts/inception.py,4,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n\nclass InceptionV3(nn.Module):\n    """"""Pretrained InceptionV3 network returning feature maps""""""\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=[DEFAULT_BLOCK_INDEX],\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False):\n        """"""Build pretrained InceptionV3\n        Parameters\n        ----------\n        output_blocks : list of int\n            Indices of blocks to return features of. Possible values are:\n                - 0: corresponds to output of first max pooling\n                - 1: corresponds to output of second max pooling\n                - 2: corresponds to output which is fed to aux classifier\n                - 3: corresponds to output of final average pooling\n        resize_input : bool\n            If true, bilinearly resizes input to width and height 299 before\n            feeding input to model. As the network without fully connected\n            layers is fully convolutional, it should be able to handle inputs\n            of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n            If true, normalizes the input to the statistics the pretrained\n            Inception network expects\n        requires_grad : bool\n            If true, parameters of the model require gradient. Possibly useful\n            for finetuning the network\n        """"""\n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            \'Last possible output block index is 3\'\n\n        self.blocks = nn.ModuleList()\n\n        inception = models.inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        """"""Get Inception feature maps\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in \n            range (0, 1)\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output \n        block, sorted ascending by index\n        """"""\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.upsample(x, size=(299, 299), mode=\'bilinear\')\n\n        if self.normalize_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp\n'"
scripts/metrics.py,0,"b'import os\nimport numpy as np\nimport argparse\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nfrom ntpath import basename\nfrom scipy.misc import imread\nfrom skimage.measure import compare_ssim\nfrom skimage.measure import compare_psnr\nfrom skimage.color import rgb2gray\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'script to compute all statistics\')\n    parser.add_argument(\'--data-path\', help=\'Path to ground truth data\', type=str)\n    parser.add_argument(\'--output-path\', help=\'Path to output data\', type=str)\n    parser.add_argument(\'--debug\', default=0, help=\'Debug\', type=int)\n    args = parser.parse_args()\n    return args\n\n\ndef compare_mae(img_true, img_test):\n    img_true = img_true.astype(np.float32)\n    img_test = img_test.astype(np.float32)\n    return np.sum(np.abs(img_true - img_test)) / np.sum(img_true + img_test)\n\n\nargs = parse_args()\nfor arg in vars(args):\n    print(\'[%s] =\' % arg, getattr(args, arg))\n\npath_true = args.data_path\npath_pred = args.output_path\n\npsnr = []\nssim = []\nmae = []\nnames = []\nindex = 1\n\nfiles = list(glob(path_true + \'/*.jpg\')) + list(glob(path_true + \'/*.png\'))\nfor fn in sorted(files):\n    name = basename(str(fn))\n    names.append(name)\n\n    img_gt = (imread(str(fn)) / 255.0).astype(np.float32)\n    img_pred = (imread(path_pred + \'/\' + basename(str(fn))) / 255.0).astype(np.float32)\n\n    img_gt = rgb2gray(img_gt)\n    img_pred = rgb2gray(img_pred)\n\n    if args.debug != 0:\n        plt.subplot(\'121\')\n        plt.imshow(img_gt)\n        plt.title(\'Groud truth\')\n        plt.subplot(\'122\')\n        plt.imshow(img_pred)\n        plt.title(\'Output\')\n        plt.show()\n\n    psnr.append(compare_psnr(img_gt, img_pred, data_range=1))\n    ssim.append(compare_ssim(img_gt, img_pred, data_range=1, win_size=51))\n    mae.append(compare_mae(img_gt, img_pred))\n    if np.mod(index, 100) == 0:\n        print(\n            str(index) + \' images processed\',\n            ""PSNR: %.4f"" % round(np.mean(psnr), 4),\n            ""SSIM: %.4f"" % round(np.mean(ssim), 4),\n            ""MAE: %.4f"" % round(np.mean(mae), 4),\n        )\n    index += 1\n\nnp.savez(args.output_path + \'/metrics.npz\', psnr=psnr, ssim=ssim, mae=mae, names=names)\nprint(\n    ""PSNR: %.4f"" % round(np.mean(psnr), 4),\n    ""PSNR Variance: %.4f"" % round(np.var(psnr), 4),\n    ""SSIM: %.4f"" % round(np.mean(ssim), 4),\n    ""SSIM Variance: %.4f"" % round(np.var(ssim), 4),\n    ""MAE: %.4f"" % round(np.mean(mae), 4),\n    ""MAE Variance: %.4f"" % round(np.var(mae), 4)\n)\n'"
src/__init__.py,0,b'# empty'
src/config.py,0,"b""import os\nimport yaml\n\nclass Config(dict):\n    def __init__(self, config_path):\n        with open(config_path, 'r') as f:\n            self._yaml = f.read()\n            self._dict = yaml.load(self._yaml)\n            self._dict['PATH'] = os.path.dirname(config_path)\n \n    def __getattr__(self, name):\n        if self._dict.get(name) is not None:\n            return self._dict[name]\n\n        if DEFAULT_CONFIG.get(name) is not None:\n            return DEFAULT_CONFIG[name]\n\n        return None\n\n    def print(self):\n        print('Model configurations:')\n        print('---------------------------------')\n        print(self._yaml)\n        print('')\n        print('---------------------------------')\n        print('')\n\n\nDEFAULT_CONFIG = {\n    'MODE': 1,                      # 1: train, 2: test, 3: eval\n    'MODEL': 1,                     # 1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model\n    'MASK': 3,                      # 1: random block, 2: half, 3: external, 4: (external, random block), 5: (external, random block, half)\n    'EDGE': 1,                      # 1: canny, 2: external\n    'NMS': 1,                       # 0: no non-max-suppression, 1: applies non-max-suppression on the external edges by multiplying by Canny\n    'SEED': 10,                     # random seed\n    'GPU': [0],                     # list of gpu ids\n    'DEBUG': 0,                     # turns on debugging mode\n    'VERBOSE': 0,                   # turns on verbose mode in the output console\n\n    'LR': 0.0001,                   # learning rate\n    'D2G_LR': 0.1,                  # discriminator/generator learning rate ratio\n    'BETA1': 0.0,                   # adam optimizer beta1\n    'BETA2': 0.9,                   # adam optimizer beta2\n    'BATCH_SIZE': 8,                # input batch size for training\n    'INPUT_SIZE': 256,              # input image size for training 0 for original size\n    'SIGMA': 2,                     # standard deviation of the Gaussian filter used in Canny edge detector (0: random, -1: no edge)\n    'MAX_ITERS': 2e6,               # maximum number of iterations to train the model\n\n    'EDGE_THRESHOLD': 0.5,          # edge detection threshold\n    'L1_LOSS_WEIGHT': 1,            # l1 loss weight\n    'FM_LOSS_WEIGHT': 10,           # feature-matching loss weight\n    'STYLE_LOSS_WEIGHT': 1,         # style loss weight\n    'CONTENT_LOSS_WEIGHT': 1,       # perceptual loss weight\n    'INPAINT_ADV_LOSS_WEIGHT': 0.01,# adversarial loss weight\n\n    'GAN_LOSS': 'nsgan',            # nsgan | lsgan | hinge\n    'GAN_POOL_SIZE': 0,             # fake images pool size\n\n    'SAVE_INTERVAL': 1000,          # how many iterations to wait before saving model (0: never)\n    'SAMPLE_INTERVAL': 1000,        # how many iterations to wait before sampling (0: never)\n    'SAMPLE_SIZE': 12,              # number of images to sample\n    'EVAL_INTERVAL': 0,             # how many iterations to wait before model evaluation (0: never)\n    'LOG_INTERVAL': 10,             # how many iterations to wait before logging training status (0: never)\n}"""
src/dataset.py,2,"b""import os\nimport glob\nimport scipy\nimport torch\nimport random\nimport numpy as np\nimport torchvision.transforms.functional as F\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nfrom scipy.misc import imread\nfrom skimage.feature import canny\nfrom skimage.color import rgb2gray\nfrom .utils import create_mask\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, config, flist, edge_flist, mask_flist, augment=True, training=True):\n        super(Dataset, self).__init__()\n        self.augment = augment\n        self.training = training\n        self.data = self.load_flist(flist)\n        self.edge_data = self.load_flist(edge_flist)\n        self.mask_data = self.load_flist(mask_flist)\n\n        self.input_size = config.INPUT_SIZE\n        self.sigma = config.SIGMA\n        self.edge = config.EDGE\n        self.mask = config.MASK\n        self.nms = config.NMS\n\n        # in test mode, there's a one-to-one relationship between mask and image\n        # masks are loaded non random\n        if config.MODE == 2:\n            self.mask = 6\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        try:\n            item = self.load_item(index)\n        except:\n            print('loading error: ' + self.data[index])\n            item = self.load_item(0)\n\n        return item\n\n    def load_name(self, index):\n        name = self.data[index]\n        return os.path.basename(name)\n\n    def load_item(self, index):\n\n        size = self.input_size\n\n        # load image\n        img = imread(self.data[index])\n\n        # resize/crop if needed\n        if size != 0:\n            img = self.resize(img, size, size)\n\n        # create grayscale image\n        img_gray = rgb2gray(img)\n\n        # load mask\n        mask = self.load_mask(img, index)\n\n        # load edge\n        edge = self.load_edge(img_gray, index, mask)\n\n        # augment data\n        if self.augment and np.random.binomial(1, 0.5) > 0:\n            img = img[:, ::-1, ...]\n            img_gray = img_gray[:, ::-1, ...]\n            edge = edge[:, ::-1, ...]\n            mask = mask[:, ::-1, ...]\n\n        return self.to_tensor(img), self.to_tensor(img_gray), self.to_tensor(edge), self.to_tensor(mask)\n\n    def load_edge(self, img, index, mask):\n        sigma = self.sigma\n\n        # in test mode images are masked (with masked regions), \n        # using 'mask' parameter prevents canny to detect edges for the masked regions\n        mask = None if self.training else (1 - mask / 255).astype(np.bool)\n\n        # canny \n        if self.edge == 1:\n            # no edge\n            if sigma == -1:\n                return np.zeros(img.shape).astype(np.float)\n\n            # random sigma\n            if sigma == 0:\n                sigma = random.randint(1, 4)\n\n            return canny(img, sigma=sigma, mask=mask).astype(np.float)\n\n        # external\n        else:\n            imgh, imgw = img.shape[0:2]\n            edge = imread(self.edge_data[index])\n            edge = self.resize(edge, imgh, imgw)\n\n            # non-max suppression\n            if self.nms == 1:\n                edge = edge * canny(img, sigma=sigma, mask=mask)\n\n            return edge\n\n    def load_mask(self, img, index):\n        imgh, imgw = img.shape[0:2]\n        mask_type = self.mask\n\n        # external + random block\n        if mask_type == 4:\n            mask_type = 1 if np.random.binomial(1, 0.5) == 1 else 3\n\n        # external + random block + half\n        elif mask_type == 5:\n            mask_type = np.random.randint(1, 4)\n\n        # random block\n        if mask_type == 1:\n            return create_mask(imgw, imgh, imgw // 2, imgh // 2)\n\n        # half\n        if mask_type == 2:\n            # randomly choose right or left\n            return create_mask(imgw, imgh, imgw // 2, imgh, 0 if random.random() < 0.5 else imgw // 2, 0)\n\n        # external\n        if mask_type == 3:\n            mask_index = random.randint(0, len(self.mask_data) - 1)\n            mask = imread(self.mask_data[mask_index])\n            mask = self.resize(mask, imgh, imgw)\n            mask = (mask > 0).astype(np.uint8) * 255  # threshold due to interpolation\n            return mask\n\n        # test mode: load mask non random\n        if mask_type == 6:\n            mask = imread(self.mask_data[index])\n            mask = self.resize(mask, imgh, imgw, centerCrop=False)\n            mask = rgb2gray(mask)\n            mask = (mask > 0).astype(np.uint8) * 255\n            return mask\n\n    def to_tensor(self, img):\n        img = Image.fromarray(img)\n        img_t = F.to_tensor(img).float()\n        return img_t\n\n    def resize(self, img, height, width, centerCrop=True):\n        imgh, imgw = img.shape[0:2]\n\n        if centerCrop and imgh != imgw:\n            # center crop\n            side = np.minimum(imgh, imgw)\n            j = (imgh - side) // 2\n            i = (imgw - side) // 2\n            img = img[j:j + side, i:i + side, ...]\n\n        img = scipy.misc.imresize(img, [height, width])\n\n        return img\n\n    def load_flist(self, flist):\n        if isinstance(flist, list):\n            return flist\n\n        # flist: image file path, image directory path, text file flist path\n        if isinstance(flist, str):\n            if os.path.isdir(flist):\n                flist = list(glob.glob(flist + '/*.jpg')) + list(glob.glob(flist + '/*.png'))\n                flist.sort()\n                return flist\n\n            if os.path.isfile(flist):\n                try:\n                    return np.genfromtxt(flist, dtype=np.str, encoding='utf-8')\n                except:\n                    return [flist]\n\n        return []\n\n    def create_iterator(self, batch_size):\n        while True:\n            sample_loader = DataLoader(\n                dataset=self,\n                batch_size=batch_size,\n                drop_last=True\n            )\n\n            for item in sample_loader:\n                yield item\n"""
src/edge_connect.py,10,"b'import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom .dataset import Dataset\nfrom .models import EdgeModel, InpaintingModel\nfrom .utils import Progbar, create_dir, stitch_images, imsave, canny_edge, to_tensor, output_align\nfrom .metrics import PSNR, EdgeAccuracy\nfrom skimage.color import rgb2gray\n\n\nclass EdgeConnect():\n    def __init__(self, config):\n        self.config = config\n\n        if config.MODEL == 1:\n            model_name = \'edge\'\n        elif config.MODEL == 2:\n            model_name = \'inpaint\'\n        elif config.MODEL == 3:\n            model_name = \'edge_inpaint\'\n        elif config.MODEL == 4:\n            model_name = \'joint\'\n\n        self.debug = False\n        self.model_name = model_name\n        self.edge_model = EdgeModel(config).to(config.DEVICE)\n        self.inpaint_model = InpaintingModel(config).to(config.DEVICE)\n\n        self.psnr = PSNR(255.0).to(config.DEVICE)\n        self.edgeacc = EdgeAccuracy(config.EDGE_THRESHOLD).to(config.DEVICE)\n\n        # test mode\n        if self.config.MODE == 2:\n            self.test_dataset = Dataset(config, config.TEST_FLIST, config.TEST_EDGE_FLIST, config.TEST_MASK_FLIST,\n                                        augment=False, training=False)\n        else:\n            self.train_dataset = Dataset(config, config.TRAIN_FLIST, config.TRAIN_EDGE_FLIST, config.TRAIN_MASK_FLIST,\n                                         augment=True, training=True)\n            self.val_dataset = Dataset(config, config.VAL_FLIST, config.VAL_EDGE_FLIST, config.VAL_MASK_FLIST,\n                                       augment=False, training=True)\n            self.sample_iterator = self.val_dataset.create_iterator(config.SAMPLE_SIZE)\n\n        self.samples_path = os.path.join(config.PATH, \'samples\')\n        self.results_path = os.path.join(config.PATH, \'results\')\n\n        if config.RESULTS is not None:\n            self.results_path = os.path.join(config.RESULTS)\n\n        if config.DEBUG is not None and config.DEBUG != 0:\n            self.debug = True\n\n        self.log_file = os.path.join(config.PATH, \'log_\' + model_name + \'.dat\')\n\n    def load(self):\n        if self.config.MODEL == 1:\n            self.edge_model.load()\n\n        elif self.config.MODEL == 2:\n            self.inpaint_model.load()\n\n        else:\n            self.edge_model.load()\n            self.inpaint_model.load()\n\n    def save(self):\n        if self.config.MODEL == 1:\n            self.edge_model.save()\n\n        elif self.config.MODEL == 2 or self.config.MODEL == 3:\n            self.inpaint_model.save()\n\n        else:\n            self.edge_model.save()\n            self.inpaint_model.save()\n\n    def train(self):\n        train_loader = DataLoader(\n            dataset=self.train_dataset,\n            batch_size=self.config.BATCH_SIZE,\n            num_workers=4,\n            drop_last=True,\n            shuffle=True\n        )\n\n        epoch = 0\n        keep_training = True\n        model = self.config.MODEL\n        max_iteration = int(float((self.config.MAX_ITERS)))\n        total = len(self.train_dataset)\n        skip_phase2 = self.config.SKIP_PHASE2\n\n        while (keep_training):\n            epoch += 1\n            print(\'\\n\\nTraining epoch: %d\' % epoch)\n\n            # [\'epoch\', \'iter\'] will not be auto-averaged during training, others will\n            progbar = Progbar(total, width=20, stateful_metrics=[\'epoch\', \'iter\'])\n\n            for items in train_loader:\n                self.edge_model.train()\n                self.inpaint_model.train()\n\n                images, images_gray, edges, masks = self.cuda(*items)\n\n                # edge model\n                if model == 1:\n                    # train\n                    outputs, gen_loss, dis_loss, logs = self.edge_model.process(images_gray, edges, masks)\n\n                    # metrics\n                    precision, recall = self.edgeacc(edges * masks, outputs * masks)\n                    logs.append((\'precision\', precision.item()))\n                    logs.append((\'recall\', recall.item()))\n\n                    # backward\n                    self.edge_model.backward(gen_loss, dis_loss)\n                    iteration = self.edge_model.iteration\n                    # print(\'iter = \',iteration)\n\n\n                # inpaint model\n                elif model == 2:\n                    # train\n                    outputs, gen_loss, dis_loss, logs = self.inpaint_model.process(images, edges, masks)\n                    outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n                    # metrics\n                    psnr = self.psnr(self.postprocess(images), self.postprocess(outputs_merged))\n                    mae = (torch.sum(torch.abs(images - outputs_merged)) / torch.sum(images)).float()\n                    logs.append((\'psnr\', psnr.item()))\n                    logs.append((\'mae\', mae.item()))\n\n                    # backward\n                    self.inpaint_model.backward(gen_loss, dis_loss)\n                    iteration = self.inpaint_model.iteration\n\n\n                # inpaint with edge model\n                elif model == 3:\n                    # train\n                    if not skip_phase2 or np.random.binomial(1, 0.5) > 0:\n                        outputs = self.edge_model(images_gray, edges, masks)\n                        outputs = outputs * masks + edges * (1 - masks)\n                    else:\n                        outputs = edges\n\n                    outputs, gen_loss, dis_loss, logs = self.inpaint_model.process(images, outputs.detach(), masks)\n                    outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n                    # metrics\n                    psnr = self.psnr(self.postprocess(images), self.postprocess(outputs_merged))\n                    mae = (torch.sum(torch.abs(images - outputs_merged)) / torch.sum(images)).float()\n                    logs.append((\'psnr\', psnr.item()))\n                    logs.append((\'mae\', mae.item()))\n\n                    # backward\n                    self.inpaint_model.backward(gen_loss, dis_loss)\n                    iteration = self.inpaint_model.iteration\n\n\n                # joint model\n                else:\n                    # train\n                    e_outputs, e_gen_loss, e_dis_loss, e_logs = self.edge_model.process(images_gray, edges, masks)\n                    e_outputs = e_outputs * masks + edges * (1 - masks)\n                    i_outputs, i_gen_loss, i_dis_loss, i_logs = self.inpaint_model.process(images, e_outputs, masks)\n                    outputs_merged = (i_outputs * masks) + (images * (1 - masks))\n\n                    # metrics\n                    psnr = self.psnr(self.postprocess(images), self.postprocess(outputs_merged))\n                    mae = (torch.sum(torch.abs(images - outputs_merged)) / torch.sum(images)).float()\n                    precision, recall = self.edgeacc(edges * masks, e_outputs * masks)\n                    e_logs.append((\'pre\', precision.item()))\n                    e_logs.append((\'rec\', recall.item()))\n                    i_logs.append((\'psnr\', psnr.item()))\n                    i_logs.append((\'mae\', mae.item()))\n                    logs = e_logs + i_logs\n\n                    # backward\n                    self.inpaint_model.backward(i_gen_loss, i_dis_loss)\n                    self.edge_model.backward(e_gen_loss, e_dis_loss)\n                    iteration = self.inpaint_model.iteration\n\n                if iteration >= max_iteration:\n                    keep_training = False\n                    break\n\n                logs = [\n                           (""epoch"", epoch),\n                           (""iter"", iteration),\n                       ] + logs\n\n                # terminal prints\n                if self.config.PRINT_INTERVAL and iteration % self.config.PRINT_INTERVAL == 0:\n                    progbar.add(len(images) * int(self.config.PRINT_INTERVAL),\n                                values=logs if self.config.VERBOSE else [x for x in logs if not x[0].startswith(\'l_\')])\n\n                # log model at checkpoints\n                if self.config.LOG_INTERVAL and iteration % self.config.LOG_INTERVAL == 0:\n                    self.log(logs)\n\n                # sample model at checkpoints\n                if self.config.SAMPLE_INTERVAL and iteration % self.config.SAMPLE_INTERVAL == 0:\n                    print(\'\\nstart sampling...\\n\')\n                    with torch.no_grad():\n                        self.sample()\n\n                # evaluate model at checkpoints\n                if self.config.EVAL_INTERVAL and iteration % self.config.EVAL_INTERVAL == 0:\n                    print(\'\\nstart eval...\\n\')\n                    with torch.no_grad():\n                        self.eval()\n\n                # save model at checkpoints\n                if self.config.SAVE_INTERVAL and iteration % self.config.SAVE_INTERVAL == 0:\n                    self.save()\n\n        print(\'\\nEnd training....\')\n\n    def eval(self):\n        val_loader = DataLoader(\n            dataset=self.val_dataset,\n            batch_size=self.config.BATCH_SIZE,\n            drop_last=True,\n            shuffle=True\n        )\n\n        model = self.config.MODEL\n        total = len(self.val_dataset)\n\n        self.edge_model.eval()\n        self.inpaint_model.eval()\n\n        progbar = Progbar(total, width=20, stateful_metrics=[\'it\'])\n        iteration = 0\n\n        for items in val_loader:\n            iteration += 1\n            images, images_gray, edges, masks = self.cuda(*items)\n\n            # edge model\n            if model == 1:\n                # eval\n                outputs, gen_loss, dis_loss, logs = self.edge_model.process(images_gray, edges, masks)\n\n                # metrics\n                precision, recall = self.edgeacc(edges * masks, outputs * masks)\n                logs.append((\'precision\', precision.item()))\n                logs.append((\'recall\', recall.item()))\n\n\n            # inpaint model\n            elif model == 2:\n                # eval\n                outputs, gen_loss, dis_loss, logs = self.inpaint_model.process(images, edges, masks)\n                outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n                # metrics\n                psnr = self.psnr(self.postprocess(images), self.postprocess(outputs_merged))\n                mae = (torch.sum(torch.abs(images - outputs_merged)) / torch.sum(images)).float()\n                logs.append((\'psnr\', psnr.item()))\n                logs.append((\'mae\', mae.item()))\n\n\n            # inpaint with edge model\n            elif model == 3:\n                # eval\n                outputs = self.edge_model(images_gray, edges, masks)\n                outputs = outputs * masks + edges * (1 - masks)\n\n                outputs, gen_loss, dis_loss, logs = self.inpaint_model.process(images, outputs.detach(), masks)\n                outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n                # metrics\n                psnr = self.psnr(self.postprocess(images), self.postprocess(outputs_merged))\n                mae = (torch.sum(torch.abs(images - outputs_merged)) / torch.sum(images)).float()\n                logs.append((\'psnr\', psnr.item()))\n                logs.append((\'mae\', mae.item()))\n\n\n            # joint model\n            else:\n                # eval\n                e_outputs, e_gen_loss, e_dis_loss, e_logs = self.edge_model.process(images_gray, edges, masks)\n                e_outputs = e_outputs * masks + edges * (1 - masks)\n                i_outputs, i_gen_loss, i_dis_loss, i_logs = self.inpaint_model.process(images, e_outputs, masks)\n                outputs_merged = (i_outputs * masks) + (images * (1 - masks))\n\n                # metrics\n                psnr = self.psnr(self.postprocess(images), self.postprocess(outputs_merged))\n                mae = (torch.sum(torch.abs(images - outputs_merged)) / torch.sum(images)).float()\n                precision, recall = self.edgeacc(edges * masks, e_outputs * masks)\n                e_logs.append((\'pre\', precision.item()))\n                e_logs.append((\'rec\', recall.item()))\n                i_logs.append((\'psnr\', psnr.item()))\n                i_logs.append((\'mae\', mae.item()))\n                logs = e_logs + i_logs\n\n            logs = [(""it"", iteration), ] + logs\n            progbar.add(len(images), values=logs)\n\n    def test(self):\n        self.edge_model.eval()\n        self.inpaint_model.eval()\n\n        model = self.config.MODEL\n        create_dir(self.results_path)\n\n        test_loader = DataLoader(\n            dataset=self.test_dataset,\n            batch_size=1,\n        )\n\n        index = 0\n        for items in test_loader:\n            name = self.test_dataset.load_name(index)\n            images, images_gray, edges, masks = self.cuda(*items)\n            # print(\'images size is {}, \\n edges size is {}, \\n masks size is {}\'.format(images.size(), edges.size(), masks.size()))\n            index += 1\n\n            # edge model\n            if model == 1:\n                outputs = self.edge_model(images_gray, edges, masks)\n                outputs = output_align(images, outputs)\n                outputs_merged = (outputs * masks) + (edges * (1 - masks))\n\n            # inpaint model\n            elif model == 2:\n                outputs = self.inpaint_model(images, edges, masks)\n                outputs = output_align(images, outputs)\n                outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n            # inpaint with edge model / joint model\n            else:\n                edges = self.edge_model(images_gray, edges, masks).detach()\n                outputs = self.inpaint_model(images, edges, masks)\n                outputs = output_align(images, outputs)\n                outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n            output = self.postprocess(outputs_merged)[0]\n            path = os.path.join(self.results_path, name)\n            print(index, name)\n\n            imsave(output, path)\n\n            if self.debug:\n                edges = self.postprocess(1 - edges)[0]\n                masked = self.postprocess(images * (1 - masks) + masks)[0]\n                fname, fext = name.split(\'.\')\n\n                imsave(edges, os.path.join(self.results_path, fname + \'_edge.\' + fext))\n                imsave(masked, os.path.join(self.results_path, fname + \'_masked.\' + fext))\n\n        print(\'\\nEnd test....\')\n\n    def sample(self, it=None):\n        self.edge_model.eval()\n        self.inpaint_model.eval()\n\n        model = self.config.MODEL\n        items = next(self.sample_iterator)\n        images, images_gray, edges, masks = self.cuda(*items)\n\n        # edge model\n        if model == 1:\n            iteration = self.edge_model.iteration\n            inputs = (images_gray * (1 - masks)) + masks\n            outputs = self.edge_model(images_gray, edges, masks)\n            outputs_merged = (outputs * masks) + (edges * (1 - masks))\n\n        # inpaint model\n        elif model == 2:\n            iteration = self.inpaint_model.iteration\n            inputs = (images * (1 - masks)) + masks\n            outputs = self.inpaint_model(images, edges, masks)\n            outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n        # inpaint with edge model / joint model\n        else:\n            iteration = self.inpaint_model.iteration\n            inputs = (images * (1 - masks)) + masks\n            outputs = self.edge_model(images_gray, edges, masks).detach()\n            edges = (outputs * masks + edges * (1 - masks)).detach()\n            outputs = self.inpaint_model(images, edges, masks)\n            outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n        if it is not None:\n            iteration = it\n\n        image_per_row = 2\n        if self.config.SAMPLE_SIZE <= 6:\n            image_per_row = 1\n\n        images = stitch_images(\n            self.postprocess(images),\n            self.postprocess(inputs),\n            self.postprocess(edges),\n            self.postprocess(outputs),\n            self.postprocess(outputs_merged),\n            img_per_row=image_per_row\n        )\n\n        path = os.path.join(self.samples_path, self.model_name)\n        name = os.path.join(path, str(iteration).zfill(5) + "".png"")\n        create_dir(path)\n        print(\'\\nsaving sample \' + name)\n        images.save(name)\n\n    def test_img_with_mask(self, img, mask, edge_edit=None):\n        self.edge_model.eval()\n        self.inpaint_model.eval()\n\n        model = self.config.MODEL  # 3\n        image_gray = rgb2gray(img)\n        if edge_edit is None:\n            edge = canny_edge(image_gray, mask, sigma=self.config.SIGMA, training=False)\n        else:\n            edge = rgb2gray(edge_edit)\n        img = to_tensor(img)\n        image_gray = to_tensor(image_gray)\n        edge = to_tensor(edge)\n        mask = to_tensor(mask)\n\n        images, images_gray, edges, masks = self.cuda(img, image_gray, edge, mask)\n        if self.config.DEBUG:\n            print(\'images size is {}, \\n edges size is {}, \\n masks size is {}\'.format(images.size(), edges.size(),\n                                                                                       masks.size()))\n\n        # edge model\n        if model == 1:\n            outputs = self.edge_model(images_gray, edges, masks)\n            outputs_merged = (outputs * masks) + (edges * (1 - masks))\n\n        # inpaint model\n        elif model == 2:\n            outputs = self.inpaint_model(images, edges, masks)\n            outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n        # inpaint with edge model / joint model\n        else:\n            if edge_edit is None:\n                edges = self.edge_model(images_gray, edges, masks).detach()\n                edges = output_align(images, edges)\n            print(\'edges:\', edges.size())\n            outputs = self.inpaint_model(images, edges, masks)\n            outputs = output_align(images, outputs)\n            # print(\'outputs:\', outputs.size())\n            # print(\'edges:\', edges.size())\n            # print(\'images:\', images.size())\n            # print(\'masks:\', masks.size())\n            outputs_merged = (outputs * masks) + (images * (1 - masks))\n\n        output = self.postprocess(outputs_merged)[0]\n        output = output.cpu().numpy().astype(np.uint8).squeeze()\n        edges = self.postprocess(edges)[0]\n        edges = edges.cpu().numpy().astype(np.uint8).squeeze()\n        return output, edges\n\n    def log(self, logs):\n        with open(self.log_file, \'a\') as f:\n            f.write(\'%s\\n\' % \' \'.join([str(item[1]) for item in logs]))\n\n    def cuda(self, *args):\n        return (item.to(self.config.DEVICE) for item in args)\n\n    def postprocess(self, img):\n        # [0, 1] => [0, 255]\n        img = img * 255.0\n        img = img.permute(0, 2, 3, 1)\n        return img.int()\n'"
src/loss.py,22,"b'import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n\nclass AdversarialLoss(nn.Module):\n    r""""""\n    Adversarial loss\n    https://arxiv.org/abs/1711.10337\n    """"""\n\n    def __init__(self, type=\'nsgan\', target_real_label=1.0, target_fake_label=0.0):\n        r""""""\n        type = nsgan | lsgan | hinge\n        """"""\n        super(AdversarialLoss, self).__init__()\n\n        self.type = type\n        self.register_buffer(\'real_label\', torch.tensor(target_real_label))\n        self.register_buffer(\'fake_label\', torch.tensor(target_fake_label))\n\n        if type == \'nsgan\':\n            self.criterion = nn.BCELoss()\n\n        elif type == \'lsgan\':\n            self.criterion = nn.MSELoss()\n\n        elif type == \'hinge\':\n            self.criterion = nn.ReLU()\n\n    def __call__(self, outputs, is_real, is_disc=None):\n        if self.type == \'hinge\':\n            if is_disc:\n                if is_real:\n                    outputs = -outputs\n                return self.criterion(1 + outputs).mean()\n            else:\n                return (-outputs).mean()\n\n        else:\n            labels = (self.real_label if is_real else self.fake_label).expand_as(outputs)\n            loss = self.criterion(outputs, labels)\n            return loss\n\n\nclass StyleLoss(nn.Module):\n    r""""""\n    Perceptual loss, VGG-based\n    https://arxiv.org/abs/1603.08155\n    https://github.com/dxyang/StyleTransfer/blob/master/utils.py\n    """"""\n\n    def __init__(self):\n        super(StyleLoss, self).__init__()\n        self.add_module(\'vgg\', VGG19())\n        self.criterion = torch.nn.L1Loss()\n\n    def compute_gram(self, x):\n        b, ch, h, w = x.size()\n        f = x.view(b, ch, w * h)\n        f_T = f.transpose(1, 2)\n        G = f.bmm(f_T) / (h * w * ch)\n\n        return G\n\n    def __call__(self, x, y):\n        # Compute features\n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n\n        # Compute loss\n        style_loss = 0.0\n        style_loss += self.criterion(self.compute_gram(x_vgg[\'relu2_2\']), self.compute_gram(y_vgg[\'relu2_2\']))\n        style_loss += self.criterion(self.compute_gram(x_vgg[\'relu3_4\']), self.compute_gram(y_vgg[\'relu3_4\']))\n        style_loss += self.criterion(self.compute_gram(x_vgg[\'relu4_4\']), self.compute_gram(y_vgg[\'relu4_4\']))\n        style_loss += self.criterion(self.compute_gram(x_vgg[\'relu5_2\']), self.compute_gram(y_vgg[\'relu5_2\']))\n\n        return style_loss\n\n\n\nclass PerceptualLoss(nn.Module):\n    r""""""\n    Perceptual loss, VGG-based\n    https://arxiv.org/abs/1603.08155\n    https://github.com/dxyang/StyleTransfer/blob/master/utils.py\n    """"""\n\n    def __init__(self, weights=[1.0, 1.0, 1.0, 1.0, 1.0]):\n        super(PerceptualLoss, self).__init__()\n        self.add_module(\'vgg\', VGG19())\n        self.criterion = torch.nn.L1Loss()\n        self.weights = weights\n\n    def __call__(self, x, y):\n        # Compute features\n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n\n        content_loss = 0.0\n        content_loss += self.weights[0] * self.criterion(x_vgg[\'relu1_1\'], y_vgg[\'relu1_1\'])\n        content_loss += self.weights[1] * self.criterion(x_vgg[\'relu2_1\'], y_vgg[\'relu2_1\'])\n        content_loss += self.weights[2] * self.criterion(x_vgg[\'relu3_1\'], y_vgg[\'relu3_1\'])\n        content_loss += self.weights[3] * self.criterion(x_vgg[\'relu4_1\'], y_vgg[\'relu4_1\'])\n        content_loss += self.weights[4] * self.criterion(x_vgg[\'relu5_1\'], y_vgg[\'relu5_1\'])\n\n\n        return content_loss\n\n\n\nclass VGG19(torch.nn.Module):\n    def __init__(self):\n        super(VGG19, self).__init__()\n        features = models.vgg19(pretrained=True).features\n        self.relu1_1 = torch.nn.Sequential()\n        self.relu1_2 = torch.nn.Sequential()\n\n        self.relu2_1 = torch.nn.Sequential()\n        self.relu2_2 = torch.nn.Sequential()\n\n        self.relu3_1 = torch.nn.Sequential()\n        self.relu3_2 = torch.nn.Sequential()\n        self.relu3_3 = torch.nn.Sequential()\n        self.relu3_4 = torch.nn.Sequential()\n\n        self.relu4_1 = torch.nn.Sequential()\n        self.relu4_2 = torch.nn.Sequential()\n        self.relu4_3 = torch.nn.Sequential()\n        self.relu4_4 = torch.nn.Sequential()\n\n        self.relu5_1 = torch.nn.Sequential()\n        self.relu5_2 = torch.nn.Sequential()\n        self.relu5_3 = torch.nn.Sequential()\n        self.relu5_4 = torch.nn.Sequential()\n\n        for x in range(2):\n            self.relu1_1.add_module(str(x), features[x])\n\n        for x in range(2, 4):\n            self.relu1_2.add_module(str(x), features[x])\n\n        for x in range(4, 7):\n            self.relu2_1.add_module(str(x), features[x])\n\n        for x in range(7, 9):\n            self.relu2_2.add_module(str(x), features[x])\n\n        for x in range(9, 12):\n            self.relu3_1.add_module(str(x), features[x])\n\n        for x in range(12, 14):\n            self.relu3_2.add_module(str(x), features[x])\n\n        for x in range(14, 16):\n            self.relu3_2.add_module(str(x), features[x])\n\n        for x in range(16, 18):\n            self.relu3_4.add_module(str(x), features[x])\n\n        for x in range(18, 21):\n            self.relu4_1.add_module(str(x), features[x])\n\n        for x in range(21, 23):\n            self.relu4_2.add_module(str(x), features[x])\n\n        for x in range(23, 25):\n            self.relu4_3.add_module(str(x), features[x])\n\n        for x in range(25, 27):\n            self.relu4_4.add_module(str(x), features[x])\n\n        for x in range(27, 30):\n            self.relu5_1.add_module(str(x), features[x])\n\n        for x in range(30, 32):\n            self.relu5_2.add_module(str(x), features[x])\n\n        for x in range(32, 34):\n            self.relu5_3.add_module(str(x), features[x])\n\n        for x in range(34, 36):\n            self.relu5_4.add_module(str(x), features[x])\n\n        # don\'t need the gradients, just want the features\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        relu1_1 = self.relu1_1(x)\n        relu1_2 = self.relu1_2(relu1_1)\n\n        relu2_1 = self.relu2_1(relu1_2)\n        relu2_2 = self.relu2_2(relu2_1)\n\n        relu3_1 = self.relu3_1(relu2_2)\n        relu3_2 = self.relu3_2(relu3_1)\n        relu3_3 = self.relu3_3(relu3_2)\n        relu3_4 = self.relu3_4(relu3_3)\n\n        relu4_1 = self.relu4_1(relu3_4)\n        relu4_2 = self.relu4_2(relu4_1)\n        relu4_3 = self.relu4_3(relu4_2)\n        relu4_4 = self.relu4_4(relu4_3)\n\n        relu5_1 = self.relu5_1(relu4_4)\n        relu5_2 = self.relu5_2(relu5_1)\n        relu5_3 = self.relu5_3(relu5_2)\n        relu5_4 = self.relu5_4(relu5_3)\n\n        out = {\n            \'relu1_1\': relu1_1,\n            \'relu1_2\': relu1_2,\n\n            \'relu2_1\': relu2_1,\n            \'relu2_2\': relu2_2,\n\n            \'relu3_1\': relu3_1,\n            \'relu3_2\': relu3_2,\n            \'relu3_3\': relu3_3,\n            \'relu3_4\': relu3_4,\n\n            \'relu4_1\': relu4_1,\n            \'relu4_2\': relu4_2,\n            \'relu4_3\': relu4_3,\n            \'relu4_4\': relu4_4,\n\n            \'relu5_1\': relu5_1,\n            \'relu5_2\': relu5_2,\n            \'relu5_3\': relu5_3,\n            \'relu5_4\': relu5_4,\n        }\n        return out\n'"
src/metrics.py,10,"b'import torch\nimport torch.nn as nn\n\n\nclass EdgeAccuracy(nn.Module):\n    """"""\n    Measures the accuracy of the edge map\n    """"""\n    def __init__(self, threshold=0.5):\n        super(EdgeAccuracy, self).__init__()\n        self.threshold = threshold\n\n    def __call__(self, inputs, outputs):\n        labels = (inputs > self.threshold)\n        outputs = (outputs > self.threshold)\n\n        relevant = torch.sum(labels.float())\n        selected = torch.sum(outputs.float())\n\n        if relevant == 0 and selected == 0:\n            return 1, 1\n\n        true_positive = ((outputs == labels) * labels).float()\n        recall = torch.sum(true_positive) / (relevant + 1e-8)\n        precision = torch.sum(true_positive) / (selected + 1e-8)\n\n        return precision, recall\n\n\nclass PSNR(nn.Module):\n    def __init__(self, max_val):\n        super(PSNR, self).__init__()\n\n        base10 = torch.log(torch.tensor(10.0))\n        max_val = torch.tensor(max_val).float()\n\n        self.register_buffer(\'base10\', base10)\n        self.register_buffer(\'max_val\', 20 * torch.log(max_val) / base10)\n\n    def __call__(self, a, b):\n        mse = torch.mean((a.float() - b.float()) ** 2)\n    \n        if mse == 0:\n            return 0\n\n        return self.max_val - 10 * torch.log(mse) / self.base10\n'"
src/models.py,17,"b'import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom .networks import InpaintGenerator, EdgeGenerator, Discriminator\nfrom .dataset import Dataset\nfrom .loss import AdversarialLoss, PerceptualLoss, StyleLoss\nfrom .utils import get_model_list\n\n\nclass BaseModel(nn.Module):\n    def __init__(self, name, config):\n        super(BaseModel, self).__init__()\n\n        self.name = name\n        self.config = config\n        self.iteration = 0\n\n    def load(self):\n        gen_path = get_model_list(self.config.PATH, self.name, \'gen\')\n        dis_path = get_model_list(self.config.PATH, self.name, \'dis\')\n        if gen_path is not None:\n            print(\'Loading {} generator weights file: {}...\'.format(self.name, gen_path))\n            if self.config.DEVICE == torch.device(\'cuda\'):  # gpu\n                data = torch.load(gen_path)\n            else:  # cpu\n                data = torch.load(gen_path, map_location=lambda storage, loc: storage)\n            self.generator.load_state_dict(data[\'generator\'])\n            self.iteration = data[\'iteration\']\n\n        # load discriminator only when training\n        if self.config.MODE == 1 and dis_path is not None:\n            print(\'Loading {} discriminator weights file: {}...\'.format(self.name, dis_path))\n            if self.config.DEVICE == torch.device(\'cuda\'):\n                data = torch.load(dis_path)\n            else:\n                data = torch.load(dis_path, map_location=lambda storage, loc: storage)\n            self.discriminator.load_state_dict(data[\'discriminator\'])\n\n    def save(self):\n        print(\'\\nsaving %s...\\n\' % self.name)\n        torch.save({\n            \'iteration\': self.iteration,\n            \'generator\': self.generator.state_dict()\n        }, os.path.join(self.config.PATH, \'{}_gen_{}.pth\'.format(self.name, self.iteration)))\n\n        torch.save({\n            \'discriminator\': self.discriminator.state_dict()\n        }, os.path.join(self.config.PATH, \'{}_dis_{}.pth\'.format(self.name, self.iteration)))\n\n\nclass EdgeModel(BaseModel):\n    def __init__(self, config):\n        super(EdgeModel, self).__init__(\'EdgeModel\', config)\n\n        # generator input: [grayscale(1) + edge(1) + mask(1)]\n        # discriminator input: (grayscale(1) + edge(1))\n        generator = EdgeGenerator(use_spectral_norm=True)\n        discriminator = Discriminator(in_channels=2, use_sigmoid=config.GAN_LOSS != \'hinge\')\n\n        # multi-gpu\n        if len(config.GPU) > 1:\n            generator = nn.DataParallel(generator, config.GPU)\n            discriminator = nn.DataParallel(discriminator, config.GPU)\n\n        l1_loss = nn.L1Loss()\n        adversarial_loss = AdversarialLoss(type=config.GAN_LOSS)\n\n        self.add_module(\'generator\', generator)\n        self.add_module(\'discriminator\', discriminator)\n\n        self.add_module(\'l1_loss\', l1_loss)\n        self.add_module(\'adversarial_loss\', adversarial_loss)\n\n        self.gen_optimizer = optim.Adam(\n            params=generator.parameters(),\n            lr=float(config.LR),\n            betas=(config.BETA1, config.BETA2)\n        )\n\n        self.dis_optimizer = optim.Adam(\n            params=discriminator.parameters(),\n            lr=float(config.LR) * float(config.D2G_LR),\n            betas=(config.BETA1, config.BETA2)\n        )\n\n    def process(self, images, edges, masks):\n        self.iteration += 1\n\n        # zero optimizers\n        self.gen_optimizer.zero_grad()\n        self.dis_optimizer.zero_grad()\n\n        # process outputs\n        outputs = self(images, edges, masks)\n        gen_loss = 0\n        dis_loss = 0\n\n        # discriminator loss\n        dis_input_real = torch.cat((images, edges), dim=1)\n        dis_input_fake = torch.cat((images, outputs.detach()), dim=1)\n        dis_real, dis_real_feat = self.discriminator(dis_input_real)  # in: (grayscale(1) + edge(1))\n        dis_fake, dis_fake_feat = self.discriminator(dis_input_fake)  # in: (grayscale(1) + edge(1))\n        dis_real_loss = self.adversarial_loss(dis_real, True, True)\n        dis_fake_loss = self.adversarial_loss(dis_fake, False, True)\n        dis_loss += (dis_real_loss + dis_fake_loss) / 2\n\n        # generator adversarial loss\n        gen_input_fake = torch.cat((images, outputs), dim=1)\n        gen_fake, gen_fake_feat = self.discriminator(gen_input_fake)  # in: (grayscale(1) + edge(1))\n        gen_gan_loss = self.adversarial_loss(gen_fake, True, False)\n        gen_loss += gen_gan_loss\n\n        # generator feature matching loss\n        gen_fm_loss = 0\n        for i in range(len(dis_real_feat)):\n            gen_fm_loss += self.l1_loss(gen_fake_feat[i], dis_real_feat[i].detach())\n        gen_fm_loss = gen_fm_loss * self.config.FM_LOSS_WEIGHT\n        gen_loss += gen_fm_loss\n\n        # create logs\n        logs = [\n            (""l_d1"", dis_loss.item()),\n            (""l_g1"", gen_gan_loss.item()),\n            (""l_fm"", gen_fm_loss.item()),\n        ]\n\n        return outputs, gen_loss, dis_loss, logs\n\n    def forward(self, images, edges, masks):\n        edges_masked = (edges * (1 - masks))\n        images_masked = (images * (1 - masks)) + masks\n        inputs = torch.cat((images_masked, edges_masked, masks), dim=1)\n        outputs = self.generator(inputs)  # in: [grayscale(1) + edge(1) + mask(1)]\n        return outputs\n\n    def backward(self, gen_loss=None, dis_loss=None):\n        if dis_loss is not None:\n            dis_loss.backward()\n        self.dis_optimizer.step()\n\n        if gen_loss is not None:\n            gen_loss.backward()\n        self.gen_optimizer.step()\n\n\nclass InpaintingModel(BaseModel):\n    def __init__(self, config):\n        super(InpaintingModel, self).__init__(\'InpaintingModel\', config)\n\n        # generator input: [rgb(3) + edge(1)]\n        # discriminator input: [rgb(3)]\n        generator = InpaintGenerator()\n        discriminator = Discriminator(in_channels=3, use_sigmoid=config.GAN_LOSS != \'hinge\')\n\n        # multi-gpu\n        if len(config.GPU) > 1:\n            generator = nn.DataParallel(generator, config.GPU)\n            discriminator = nn.DataParallel(discriminator, config.GPU)\n\n        l1_loss = nn.L1Loss()\n        perceptual_loss = PerceptualLoss()\n        style_loss = StyleLoss()\n        adversarial_loss = AdversarialLoss(type=config.GAN_LOSS)\n\n        self.add_module(\'generator\', generator)\n        self.add_module(\'discriminator\', discriminator)\n\n        self.add_module(\'l1_loss\', l1_loss)\n        self.add_module(\'perceptual_loss\', perceptual_loss)\n        self.add_module(\'style_loss\', style_loss)\n        self.add_module(\'adversarial_loss\', adversarial_loss)\n\n        self.gen_optimizer = optim.Adam(\n            params=generator.parameters(),\n            lr=float(config.LR),\n            betas=(config.BETA1, config.BETA2)\n        )\n\n        self.dis_optimizer = optim.Adam(\n            params=discriminator.parameters(),\n            lr=float(config.LR) * float(config.D2G_LR),\n            betas=(config.BETA1, config.BETA2)\n        )\n\n    def process(self, images, edges, masks):\n        self.iteration += 1\n\n        # zero optimizers\n        self.gen_optimizer.zero_grad()\n        self.dis_optimizer.zero_grad()\n\n        # process outputs\n        outputs = self(images, edges, masks)\n        gen_loss = 0\n        dis_loss = 0\n\n        # discriminator loss\n        dis_input_real = images\n        dis_input_fake = outputs.detach()\n        dis_real, _ = self.discriminator(dis_input_real)  # in: [rgb(3)]\n        dis_fake, _ = self.discriminator(dis_input_fake)  # in: [rgb(3)]\n        dis_real_loss = self.adversarial_loss(dis_real, True, True)\n        dis_fake_loss = self.adversarial_loss(dis_fake, False, True)\n        dis_loss += (dis_real_loss + dis_fake_loss) / 2\n\n        # generator adversarial loss\n        gen_input_fake = outputs\n        gen_fake, _ = self.discriminator(gen_input_fake)  # in: [rgb(3)]\n        gen_gan_loss = self.adversarial_loss(gen_fake, True, False) * self.config.INPAINT_ADV_LOSS_WEIGHT\n        gen_loss += gen_gan_loss\n\n        # generator l1 loss\n        gen_l1_loss = self.l1_loss(outputs, images) * self.config.L1_LOSS_WEIGHT / torch.mean(masks)\n        gen_loss += gen_l1_loss\n\n        # generator perceptual loss\n        gen_content_loss = self.perceptual_loss(outputs, images)\n        gen_content_loss = gen_content_loss * self.config.CONTENT_LOSS_WEIGHT\n        gen_loss += gen_content_loss\n\n        # generator style loss\n        gen_style_loss = self.style_loss(outputs * masks, images * masks)\n        gen_style_loss = gen_style_loss * self.config.STYLE_LOSS_WEIGHT\n        gen_loss += gen_style_loss\n\n        # create logs\n        logs = [\n            (""l_d2"", dis_loss.item()),\n            (""l_g2"", gen_gan_loss.item()),\n            (""l_l1"", gen_l1_loss.item()),\n            (""l_per"", gen_content_loss.item()),\n            (""l_sty"", gen_style_loss.item()),\n        ]\n\n        return outputs, gen_loss, dis_loss, logs\n\n    def forward(self, images, edges, masks):\n        images_masked = (images * (1 - masks).float()) + masks\n        # print(edges.size(),images_masked.size())\n        inputs = torch.cat((images_masked, edges), dim=1)\n        outputs = self.generator(inputs)  # in: [rgb(3) + edge(1)]\n        return outputs\n\n    def backward(self, gen_loss=None, dis_loss=None):\n        dis_loss.backward()\n        self.dis_optimizer.step()\n\n        gen_loss.backward()\n        self.gen_optimizer.step()\n'"
src/networks.py,5,"b""import torch\nimport torch.nn as nn\n\n\nclass BaseNetwork(nn.Module):\n    def __init__(self):\n        super(BaseNetwork, self).__init__()\n\n    def init_weights(self, init_type='normal', gain=0.02):\n        '''\n        initialize network's weights\n        init_type: normal | xavier | kaiming | orthogonal\n        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39\n        '''\n\n        def init_func(m):\n            classname = m.__class__.__name__\n            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n                if init_type == 'normal':\n                    nn.init.normal_(m.weight.data, 0.0, gain)\n                elif init_type == 'xavier':\n                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n                elif init_type == 'kaiming':\n                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n                elif init_type == 'orthogonal':\n                    nn.init.orthogonal_(m.weight.data, gain=gain)\n\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias.data, 0.0)\n\n            elif classname.find('BatchNorm2d') != -1:\n                nn.init.normal_(m.weight.data, 1.0, gain)\n                nn.init.constant_(m.bias.data, 0.0)\n\n        self.apply(init_func)\n\n\nclass InpaintGenerator(BaseNetwork):\n    def __init__(self, residual_blocks=8, init_weights=True):\n        super(InpaintGenerator, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels=4, out_channels=64, kernel_size=7, padding=0),\n            nn.InstanceNorm2d(64, track_running_stats=False),\n            nn.ReLU(True),\n\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(128, track_running_stats=False),\n            nn.ReLU(True),\n\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(256, track_running_stats=False),\n            nn.ReLU(True)\n        )\n\n        blocks = []\n        for _ in range(residual_blocks):\n            block = ResnetBlock(256, 2)\n            blocks.append(block)\n\n        self.middle = nn.Sequential(*blocks)\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(128, track_running_stats=False),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n            nn.InstanceNorm2d(64, track_running_stats=False),\n            nn.ReLU(True),\n\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, padding=0),\n        )\n\n        if init_weights:\n            self.init_weights()\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.middle(x)\n        x = self.decoder(x)\n        x = (torch.tanh(x) + 1) / 2\n\n        return x\n\n\nclass EdgeGenerator(BaseNetwork):\n    def __init__(self, residual_blocks=8, use_spectral_norm=True, init_weights=True):\n        super(EdgeGenerator, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            spectral_norm(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, padding=0), use_spectral_norm),\n            nn.InstanceNorm2d(64, track_running_stats=False),\n            nn.ReLU(True),\n\n            spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n            nn.InstanceNorm2d(128, track_running_stats=False),\n            nn.ReLU(True),\n\n            spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n            nn.InstanceNorm2d(256, track_running_stats=False),\n            nn.ReLU(True)\n        )\n\n        blocks = []\n        for _ in range(residual_blocks):\n            block = ResnetBlock(256, 2, use_spectral_norm=use_spectral_norm)\n            blocks.append(block)\n\n        self.middle = nn.Sequential(*blocks)\n\n        self.decoder = nn.Sequential(\n            spectral_norm(nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n            nn.InstanceNorm2d(128, track_running_stats=False),\n            nn.ReLU(True),\n\n            spectral_norm(nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n            nn.InstanceNorm2d(64, track_running_stats=False),\n            nn.ReLU(True),\n\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=7, padding=0),\n        )\n\n        if init_weights:\n            self.init_weights()\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.middle(x)\n        x = self.decoder(x)\n        x = torch.sigmoid(x)\n        return x\n\n\nclass Discriminator(BaseNetwork):\n    def __init__(self, in_channels, use_sigmoid=True, use_spectral_norm=True, init_weights=True):\n        super(Discriminator, self).__init__()\n        self.use_sigmoid = use_sigmoid\n\n        self.conv1 = self.features = nn.Sequential(\n            spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.conv2 = nn.Sequential(\n            spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.conv3 = nn.Sequential(\n            spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.conv4 = nn.Sequential(\n            spectral_norm(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.conv5 = nn.Sequential(\n            spectral_norm(nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n        )\n\n        if init_weights:\n            self.init_weights()\n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n        conv4 = self.conv4(conv3)\n        conv5 = self.conv5(conv4)\n\n        outputs = conv5\n        if self.use_sigmoid:\n            outputs = torch.sigmoid(conv5)\n\n        return outputs, [conv1, conv2, conv3, conv4, conv5]\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, dilation=1, use_spectral_norm=False):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.ReflectionPad2d(dilation),\n            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n            nn.InstanceNorm2d(dim, track_running_stats=False),\n            nn.ReLU(True),\n\n            nn.ReflectionPad2d(1),\n            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n            nn.InstanceNorm2d(dim, track_running_stats=False),\n        )\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n\n        # Remove ReLU at the end of the residual block\n        # http://torch.ch/blog/2016/02/04/resnets.html\n\n        return out\n\n\ndef spectral_norm(module, mode=True):\n    if mode:\n        return nn.utils.spectral_norm(module)\n\n    return module\n"""
src/utils.py,1,"b'import os\nimport sys\nimport time\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom skimage.feature import canny\nimport torchvision.transforms.functional as F\nimport torch.nn as nn\n\n\ndef create_dir(dir):\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\n\ndef create_mask(width, height, mask_width, mask_height, x=None, y=None):\n    mask = np.zeros((height, width))\n    mask_x = x if x is not None else random.randint(0, width - mask_width)\n    mask_y = y if y is not None else random.randint(0, height - mask_height)\n    mask[mask_y:mask_y + mask_height, mask_x:mask_x + mask_width] = 1\n    return mask\n\n\n# Get model list for resume, key_phase = \'EdgeModel\' or \'InpaintModel\', key_model = \'gen\' or \'dis\'\ndef get_model_list(dirname, key_phase, key_model):\n    if os.path.exists(dirname) is False:\n        return None\n    gen_models = [os.path.join(dirname, f) for f in os.listdir(dirname) if\n                  os.path.isfile(os.path.join(dirname, f)) and key_phase in f and key_model in f and "".pth"" in f]\n    if len(gen_models) == 0:\n        return None\n    gen_models.sort()\n    last_model_name = gen_models[-1]\n    return last_model_name\n\n\ndef stitch_images(inputs, *outputs, img_per_row=2):\n    gap = 5\n    columns = len(outputs) + 1\n\n    width, height = inputs[0][:, :, 0].shape\n    img = Image.new(\'RGB\',\n                    (width * img_per_row * columns + gap * (img_per_row - 1), height * int(len(inputs) / img_per_row)))\n    images = [inputs, *outputs]\n\n    for ix in range(len(inputs)):\n        xoffset = int(ix % img_per_row) * width * columns + int(ix % img_per_row) * gap\n        yoffset = int(ix / img_per_row) * height\n\n        for cat in range(len(images)):\n            im = np.array((images[cat][ix]).cpu()).astype(np.uint8).squeeze()\n            im = Image.fromarray(im)\n            img.paste(im, (xoffset + cat * width, yoffset))\n\n    return img\n\n\ndef imshow(img, title=\'\'):\n    fig = plt.gcf()\n    fig.canvas.set_window_title(title)\n    plt.axis(\'off\')\n    plt.imshow(img, interpolation=\'none\')\n    plt.show()\n\n\ndef imsave(img, path):\n    im = Image.fromarray(img.cpu().numpy().astype(np.uint8).squeeze())\n    im.save(path)\n\n\ndef canny_edge(img, mask, sigma=2, training=False):\n    # in test mode images are masked (with masked regions),\n    # using \'mask\' parameter prevents canny to detect edges for the masked regions\n    mask = None if training else (1 - mask / 255).astype(np.bool)\n\n    # canny\n    # no edge\n    if sigma == -1:\n        return np.zeros(img.shape).astype(np.float)\n\n    # random sigma\n    if sigma == 0:\n        sigma = random.randint(1, 4)\n\n    return canny(img, sigma=sigma, mask=mask).astype(np.float)\n\n\ndef output_align(input, output):\n    """"""\n    In testing, sometimes output is several pixels less than irregular-size input,\n    here is to fill them\n    """"""\n    if output.size() != input.size():\n        diff_width = input.size(-1) - output.size(-1)\n        diff_height = input.size(-2) - output.size(-2)\n        m = nn.ReplicationPad2d((0, diff_width, 0, diff_height))\n        output = m(output)\n\n    return output\n\n\ndef to_tensor(img):\n    img = Image.fromarray(img)\n    img_t = F.to_tensor(img).float()\n    img_t = img_t.unsqueeze(0)\n    return img_t\n\n\nclass Progbar(object):\n    """"""Displays a progress bar.\n\n    Arguments:\n        target: Total number of steps expected, None if unknown.\n        width: Progress bar width on screen.\n        verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n        stateful_metrics: Iterable of string names of metrics that\n            should *not* be averaged over time. Metrics in this list\n            will be displayed as-is. All others will be averaged\n            by the progbar before display.\n        interval: Minimum visual progress update interval (in seconds).\n    """"""\n\n    def __init__(self, target, width=25, verbose=1, interval=0.05,\n                 stateful_metrics=None):\n        self.target = target\n        self.width = width\n        self.verbose = verbose\n        self.interval = interval\n        if stateful_metrics:\n            self.stateful_metrics = set(stateful_metrics)\n        else:\n            self.stateful_metrics = set()\n\n        self._dynamic_display = ((hasattr(sys.stdout, \'isatty\') and\n                                  sys.stdout.isatty()) or\n                                 \'ipykernel\' in sys.modules or\n                                 \'posix\' in sys.modules)\n        self._total_width = 0\n        self._seen_so_far = 0\n        # We use a dict + list to avoid garbage collection\n        # issues found in OrderedDict\n        self._values = {}\n        self._values_order = []\n        self._start = time.time()\n        self._last_update = 0\n\n    def update(self, current, values=None):\n        """"""Updates the progress bar.\n\n        Arguments:\n            current: Index of current step.\n            values: List of tuples:\n                `(name, value_for_last_step)`.\n                If `name` is in `stateful_metrics`,\n                `value_for_last_step` will be displayed as-is.\n                Else, an average of the metric over time will be displayed.\n        """"""\n        values = values or []\n        for k, v in values:\n            if k not in self._values_order:\n                self._values_order.append(k)\n            if k not in self.stateful_metrics:\n                if k not in self._values:\n                    self._values[k] = [v * (current - self._seen_so_far),\n                                       current - self._seen_so_far]\n                else:\n                    self._values[k][0] += v * (current - self._seen_so_far)\n                    self._values[k][1] += (current - self._seen_so_far)\n            else:\n                self._values[k] = v\n        self._seen_so_far = current\n\n        now = time.time()\n        info = \' - %.0fs\' % (now - self._start)\n        if self.verbose == 1:\n            if (now - self._last_update < self.interval and\n                    self.target is not None and current < self.target):\n                return\n\n            prev_total_width = self._total_width\n            if self._dynamic_display:\n                sys.stdout.write(\'\\b\' * prev_total_width)\n                sys.stdout.write(\'\\r\')\n            else:\n                sys.stdout.write(\'\\n\')\n\n            if self.target is not None:\n                numdigits = int(np.floor(np.log10(self.target))) + 1\n                barstr = \'%%%dd/%d [\' % (numdigits, self.target)\n                bar = barstr % current\n                prog = float(current) / self.target\n                prog_width = int(self.width * prog)\n                if prog_width > 0:\n                    bar += (\'=\' * (prog_width - 1))\n                    if current < self.target:\n                        bar += \'>\'\n                    else:\n                        bar += \'=\'\n                bar += (\'.\' * (self.width - prog_width))\n                bar += \']\'\n            else:\n                bar = \'%7d/Unknown\' % current\n\n            self._total_width = len(bar)\n            sys.stdout.write(bar)\n\n            if current:\n                time_per_unit = (now - self._start) / current\n            else:\n                time_per_unit = 0\n            if self.target is not None and current < self.target:\n                eta = time_per_unit * (self.target - current)\n                if eta > 3600:\n                    eta_format = \'%d:%02d:%02d\' % (eta // 3600,\n                                                   (eta % 3600) // 60,\n                                                   eta % 60)\n                elif eta > 60:\n                    eta_format = \'%d:%02d\' % (eta // 60, eta % 60)\n                else:\n                    eta_format = \'%ds\' % eta\n\n                info = \' - ETA: %s\' % eta_format\n            else:\n                if time_per_unit >= 1:\n                    info += \' %.0fs/step\' % time_per_unit\n                elif time_per_unit >= 1e-3:\n                    info += \' %.0fms/step\' % (time_per_unit * 1e3)\n                else:\n                    info += \' %.0fus/step\' % (time_per_unit * 1e6)\n\n            for k in self._values_order:\n                info += \' - %s:\' % k\n                if isinstance(self._values[k], list):\n                    avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))\n                    if abs(avg) > 1e-3:\n                        info += \' %.4f\' % avg\n                    else:\n                        info += \' %.4e\' % avg\n                else:\n                    info += \' %s\' % self._values[k]\n\n            self._total_width += len(info)\n            if prev_total_width > self._total_width:\n                info += (\' \' * (prev_total_width - self._total_width))\n\n            if self.target is not None and current >= self.target:\n                info += \'\\n\'\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n        elif self.verbose == 2:\n            if self.target is None or current >= self.target:\n                for k in self._values_order:\n                    info += \' - %s:\' % k\n                    avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))\n                    if avg > 1e-3:\n                        info += \' %.4f\' % avg\n                    else:\n                        info += \' %.4e\' % avg\n                info += \'\\n\'\n\n                sys.stdout.write(info)\n                sys.stdout.flush()\n\n        self._last_update = now\n\n    def add(self, n, values=None):\n        self.update(self._seen_so_far + n, values)\n'"
